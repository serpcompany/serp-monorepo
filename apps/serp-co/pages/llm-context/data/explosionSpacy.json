[
  {
    "owner": "explosion",
    "repo": "spacy",
    "content": "TITLE: Analyzing Text Tokens with spaCy in Python\nDESCRIPTION: Demonstrates how to use spaCy to analyze text and extract various linguistic features including lemmatization, part-of-speech tags, dependencies, and token characteristics. The code loads the English language model, processes a sample sentence, and prints detailed token information.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_pos-deps.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n\nfor token in doc:\n    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n            token.shape_, token.is_alpha, token.is_stop)\n```\n\n----------------------------------------\n\nTITLE: Accessing Morphological Features in spaCy\nDESCRIPTION: This snippet demonstrates how to load a spaCy model, process text, and access morphological features of tokens. It shows how to inspect the morphological analysis and retrieve specific morphological features.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\nprint(\"Pipeline:\", nlp.pipe_names)\ndoc = nlp(\"I was reading the paper.\")\ntoken = doc[0]  # 'I'\nprint(token.morph)  # 'Case=Nom|Number=Sing|Person=1|PronType=Prs'\nprint(token.morph.get(\"PronType\"))  # ['Prs']\n```\n\n----------------------------------------\n\nTITLE: Installing and Loading spaCy Model\nDESCRIPTION: Shows how to download a spaCy model using the CLI and load it in Python code. The example uses the English small model (en_core_web_sm).\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/spacy-101.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy download en_core_web_sm\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> import spacy\n>>> nlp = spacy.load(\"en_core_web_sm\")\n```\n\n----------------------------------------\n\nTITLE: Implementing spaCy's Tokenization Algorithm in Python\nDESCRIPTION: A Python implementation of spaCy's tokenization algorithm, optimized for readability rather than performance. It demonstrates how the tokenizer handles special cases, prefixes, suffixes, and infixes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef tokenizer_pseudo_code(\n    text,\n    special_cases,\n    prefix_search,\n    suffix_search,\n    infix_finditer,\n    token_match,\n    url_match\n):\n    tokens = []\n    for substring in text.split():\n        suffixes = []\n        while substring:\n            if substring in special_cases:\n                tokens.extend(special_cases[substring])\n                substring = \"\"\n                continue\n            while prefix_search(substring) or suffix_search(substring):\n                if token_match(substring):\n                    tokens.append(substring)\n                    substring = \"\"\n                    break\n                if substring in special_cases:\n                    tokens.extend(special_cases[substring])\n                    substring = \"\"\n                    break\n                if prefix_search(substring):\n                    split = prefix_search(substring).end()\n                    tokens.append(substring[:split])\n                    substring = substring[split:]\n                    if substring in special_cases:\n                        continue\n                if suffix_search(substring):\n                    split = suffix_search(substring).start()\n                    suffixes.append(substring[split:])\n                    substring = substring[:split]\n            if token_match(substring):\n                tokens.append(substring)\n                substring = \"\"\n            elif url_match(substring):\n                tokens.append(substring)\n                substring = \"\"\n            elif substring in special_cases:\n                tokens.extend(special_cases[substring])\n                substring = \"\"\n            elif list(infix_finditer(substring)):\n                infixes = infix_finditer(substring)\n                offset = 0\n                for match in infixes:\n                    if offset == 0 and match.start() == 0:\n                        continue\n                    tokens.append(substring[offset : match.start()])\n                    tokens.append(substring[match.start() : match.end()])\n                    offset = match.end()\n                if substring[offset:]:\n                    tokens.append(substring[offset:])\n                substring = \"\"\n            elif substring:\n                tokens.append(substring)\n                substring = \"\"\n        tokens.extend(reversed(suffixes))\n    for match in matcher(special_cases, text):\n        tokens.replace(match, special_cases[match])\n    return tokens\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy Pipeline Models\nDESCRIPTION: Shows different ways to load spaCy models using util.load_model(), including loading from a package name, with component exclusions, and from a file path. Supports customizing which components are enabled or disabled.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nnlp = util.load_model(\"en_core_web_sm\")\nnlp = util.load_model(\"en_core_web_sm\", exclude=[\"ner\"])\nnlp = util.load_model(\"/path/to/data\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Noun Chunks with spaCy\nDESCRIPTION: Demonstrates how to extract noun chunks from text using spaCy's Doc.noun_chunks feature. Shows how to analyze the root text, dependency relations, and head text of each chunk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\nfor chunk in doc.noun_chunks:\n    print(chunk.text, chunk.root.text, chunk.root.dep_,\n            chunk.root.head.text)\n```\n\n----------------------------------------\n\nTITLE: Using Statistical Sentence Segmenter in spaCy\nDESCRIPTION: Shows how to use spaCy's SentenceRecognizer component (senter) by excluding the parser and enabling the senter. This is a faster alternative to the parser when only sentence boundaries are needed, not full dependency parses.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])\nnlp.enable_pipe(\"senter\")\ndoc = nlp(\"This is a sentence. This is another sentence.\")\nfor sent in doc.sents:\n    print(sent.text)\n```\n\n----------------------------------------\n\nTITLE: Debugging Training Data in spaCy\nDESCRIPTION: This command uses spaCy's CLI to analyze and validate training and development data. It provides useful stats and identifies potential issues in the data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug data config.cfg\n```\n\n----------------------------------------\n\nTITLE: Loading and Using spaCy Models in Python\nDESCRIPTION: These Python snippets demonstrate how to load and use spaCy models in your code, both by using spacy.load() and by importing the model directly.\nSOURCE: https://github.com/explosion/spacy/blob/master/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a sentence.\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nimport en_core_web_sm\n\nnlp = en_core_web_sm.load()\ndoc = nlp(\"This is a sentence.\")\n```\n\n----------------------------------------\n\nTITLE: Tokenizing Text with spaCy in Python\nDESCRIPTION: Demonstrates basic text tokenization using spaCy's English language model to split a sentence into individual tokens. The code loads the English language model, processes a sample sentence, and prints each token.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_tokenization.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\nfor token in doc:\n    print(token.text)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Neural Network Architecture in Python\nDESCRIPTION: Creates a custom neural network architecture for a spaCy pipeline component, demonstrating how to register and configure custom model architectures.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_31\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[components.tagger]\nfactory = \"tagger\"\n\n[components.tagger.model]\n@architectures = \"custom_neural_network.v1\"\noutput_width = 512\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom thinc.types import Floats2d\nfrom thinc.api import Model\nimport spacy\nfrom spacy.tokens import Doc\n\n@spacy.registry.architectures(\"custom_neural_network.v1\")\ndef custom_neural_network(output_width: int) -> Model[List[Doc], List[Floats2d]]:\n    return create_model(output_width)\n```\n\n----------------------------------------\n\nTITLE: Using AttributeRuler to Override POS Tags and Tags in spaCy\nDESCRIPTION: This example demonstrates how to use the AttributeRuler to specify custom tag and POS values for the phrase \"The Who\", overriding the default tags provided by the statistical tagger. The example shows both the default behavior and the modified behavior after adding attribute rules.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ntext = \"I saw The Who perform. Who did you see?\"\ndoc1 = nlp(text)\nprint(doc1[2].tag_, doc1[2].pos_)  # DT DET\nprint(doc1[3].tag_, doc1[3].pos_)  # WP PRON\n\n# Add attribute ruler with exception for \"The Who\" as NNP/PROPN NNP/PROPN\nruler = nlp.get_pipe(\"attribute_ruler\")\n# Pattern to match \"The Who\"\npatterns = [[{\"LOWER\": \"the\"}, {\"TEXT\": \"Who\"}]]\n# The attributes to assign to the matched token\nattrs = {\"TAG\": \"NNP\", \"POS\": \"PROPN\"}\n# Add rules to the attribute ruler\nruler.add(patterns=patterns, attrs=attrs, index=0)  # \"The\" in \"The Who\"\nruler.add(patterns=patterns, attrs=attrs, index=1)  # \"Who\" in \"The Who\"\n\ndoc2 = nlp(text)\nprint(doc2[2].tag_, doc2[2].pos_)  # NNP PROPN\nprint(doc2[3].tag_, doc2[3].pos_)  # NNP PROPN\n# The second \"Who\" remains unmodified\nprint(doc2[5].tag_, doc2[5].pos_)  # WP PRON\n```\n\n----------------------------------------\n\nTITLE: Updating spaCy Language Models with Training Data in Python\nDESCRIPTION: This code snippet demonstrates how to update spaCy Language models using the update method. It processes raw text and entity offsets to create Example objects for training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfor raw_text, entity_offsets in train_data:\n    doc = nlp.make_doc(raw_text)\n    example = Example.from_dict(doc, {\"entities\": entity_offsets})\n    nlp.update([example], sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Processing Text with spaCy's Language.__call__ Method (Python)\nDESCRIPTION: Shows how to use the Language.__call__ method to process text and create a Doc object. This method applies the full NLP pipeline to the input text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"An example sentence. Another sentence.\")\nassert (doc[0].text, doc[0].head.tag_) == (\"An\", \"NN\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Using spaCy Language Models\nDESCRIPTION: Shows how to load a downloaded spaCy language model and use it to process text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a sentence.\")\n```\n\n----------------------------------------\n\nTITLE: Adding Individual Word Vectors to a spaCy Vocabulary\nDESCRIPTION: This example shows how to add word vectors individually to a spaCy Vocab object. It creates random vectors for three words and adds them to a new vocabulary, demonstrating the manual approach to vector addition for custom applications.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.vocab import Vocab\n\nvector_data = {\n    \"dog\": numpy.random.uniform(-1, 1, (300,)),\n    \"cat\": numpy.random.uniform(-1, 1, (300,)),\n    \"orange\": numpy.random.uniform(-1, 1, (300,))\n}\nvocab = Vocab()\nfor word, vector in vector_data.items():\n    vocab.set_vector(word, vector)\n```\n\n----------------------------------------\n\nTITLE: Basic Text Processing with spaCy\nDESCRIPTION: Demonstrates how to process text using spaCy to get linguistic annotations like part-of-speech tags and dependency relations for each token in a sentence.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/spacy-101.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\nfor token in doc:\n    print(token.text, token.pos_, token.dep_)\n```\n\n----------------------------------------\n\nTITLE: Example spaCy Pipeline meta.json Structure\nDESCRIPTION: An example of a complete meta.json file for a spaCy pipeline, showing all available fields including language specification, version requirements, labels, and performance metrics. This metadata is available as nlp.meta and exported when saving an nlp object to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"example_pipeline\",\n  \"lang\": \"en\",\n  \"version\": \"1.0.0\",\n  \"spacy_version\": \">=3.0.0,<3.1.0\",\n  \"parent_package\": \"spacy\",\n  \"requirements\": [\"spacy-transformers>=1.0.0,<1.1.0\"],\n  \"description\": \"Example pipeline for spaCy\",\n  \"author\": \"You\",\n  \"email\": \"you@example.com\",\n  \"url\": \"https://example.com\",\n  \"license\": \"CC BY-SA 3.0\",\n  \"sources\": [{ \"name\": \"My Corpus\", \"license\": \"MIT\" }],\n  \"vectors\": { \"width\": 0, \"vectors\": 0, \"keys\": 0, \"name\": null },\n  \"pipeline\": [\"tok2vec\", \"ner\", \"textcat\"],\n  \"labels\": {\n    \"ner\": [\"PERSON\", \"ORG\", \"PRODUCT\"],\n    \"textcat\": [\"POSITIVE\", \"NEGATIVE\"]\n  },\n  \"performance\": {\n    \"ents_f\": 82.7300930714,\n    \"ents_p\": 82.135523614,\n    \"ents_r\": 83.3333333333,\n    \"textcat_score\": 88.364323811\n  },\n  \"speed\": { \"cpu\": 7667.8, \"gpu\": null, \"nwords\": 10329 },\n  \"spacy_git_version\": \"61dfdd9fb\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example Training Loop\nDESCRIPTION: Implements a complete training loop for a spaCy model, including initialization, shuffling training data, creating examples from text and entity offsets, updating the model, and saving it to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_49\n\nLANGUAGE: python\nCODE:\n```\noptimizer = nlp.initialize()\nfor itn in range(100):\n    random.shuffle(train_data)\n    for raw_text, entity_offsets in train_data:\n        doc = nlp.make_doc(raw_text)\n        example = Example.from_dict(doc, {\"entities\": entity_offsets})\n        nlp.update([example], sgd=optimizer)\nnlp.to_disk(\"/output\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Token Matcher in spaCy\nDESCRIPTION: Complete example showing how to initialize a Matcher, add patterns, and process matches with a document, including extracting matched spans and their IDs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab)\n# Add match ID \"HelloWorld\" with no callback and one pattern\npattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\nmatcher.add(\"HelloWorld\", [pattern])\n\ndoc = nlp(\"Hello, world! Hello world!\")\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    string_id = nlp.vocab.strings[match_id]  # Get string representation\n    span = doc[start:end]  # The matched span\n    print(match_id, string_id, start, end, span.text)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Trainable Component in spaCy\nDESCRIPTION: This code snippet shows the basic structure for implementing a custom trainable component in spaCy. It includes a TrainablePipe subclass with predict and set_annotations methods, and a component factory registered with @Language.factory.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline import TrainablePipe\nfrom spacy.language import Language\n\nclass TrainableComponent(TrainablePipe):\n    def predict(self, docs):\n        ...\n\n    def set_annotations(self, docs, scores):\n        ...\n\n@Language.factory(\"my_trainable_component\")\ndef make_component(nlp, name, model):\n    return TrainableComponent(nlp.vocab, model, name=name)\n```\n\n----------------------------------------\n\nTITLE: Adding On-Match Rules in spaCy\nDESCRIPTION: Demonstrates how to add custom callback functions to matcher patterns. Example shows matching 'Google I/O' and adding it as an entity to the document.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\nfrom spacy.matcher import Matcher\nfrom spacy.tokens import Span\n\nnlp = English()\nmatcher = Matcher(nlp.vocab)\n\ndef add_event_ent(matcher, doc, i, matches):\n    match_id, start, end = matches[i]\n    entity = Span(doc, start, end, label=\"EVENT\")\n    doc.ents += (entity,)\n    print(entity.text)\n\npattern = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\nmatcher.add(\"GoogleIO\", [pattern], on_match=add_event_ent)\ndoc = nlp(\"This is a text about Google I/O\")\nmatches = matcher(doc)\n```\n\n----------------------------------------\n\nTITLE: Creating Doc Objects in spaCy (Python)\nDESCRIPTION: Shows two methods of constructing a Doc object: using the nlp pipeline directly and manually creating a Doc by providing words and spaces arrays to the constructor.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Construction 1\ndoc = nlp(\"Some text\")\n\n# Construction 2\nfrom spacy.tokens import Doc\n\nwords = [\"hello\", \"world\", \"!\"]\nspaces = [True, False, False]\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\n```\n\n----------------------------------------\n\nTITLE: Loading a spaCy Pipeline from Disk\nDESCRIPTION: Demonstrates how to load a trained spaCy pipeline from a local directory using spacy.load(). This initializes a Language class with the proper pipeline components and model data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"/path/to/pipeline\")\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy via pip\nDESCRIPTION: Shows the basic pip installation commands for spaCy and its dependencies.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U pip setuptools wheel\n$ pip install -U %%SPACY_PKG_NAME%%SPACY_PKG_FLAGS\n```\n\n----------------------------------------\n\nTITLE: Constructing LLM Pipeline Components in spaCy\nDESCRIPTION: Examples showing different ways to create and configure LLM components in spaCy pipelines, including using default or custom models, task-specific factories, and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with the default GPT 3.5 model and an explicitly defined task\nconfig = {\"task\": {\"@llm_tasks\": \"spacy.NER.v3\", \"labels\": [\"PERSON\", \"ORGANISATION\", \"LOCATION\"]}}\nllm = nlp.add_pipe(\"llm\", config=config)\n\n# Construction via add_pipe with a task-specific factory and default GPT3.5 model\nllm = nlp.add_pipe(\"llm_ner\")\n\n# Construction via add_pipe with a task-specific factory and custom model\nllm = nlp.add_pipe(\"llm_ner\", config={\"model\": {\"@llm_models\": \"spacy.Dolly.v1\", \"name\": \"dolly-v2-12b\"}})\n\n# Construction from class\nfrom spacy_llm.pipeline import LLMWrapper\nllm = LLMWrapper(vocab=nlp.vocab, task=task, model=model, cache=cache, save_io=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Language-Specific Factories in Python\nDESCRIPTION: This snippet demonstrates how to create language-specific factories for English and German. It includes a TokenNormalizer class and shows how to register and use language-specific factories.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\nfrom spacy.lang.de import German\n\nclass TokenNormalizer:\n    def __init__(self, norm_table):\n        self.norm_table = norm_table\n\n    def __call__(self, doc):\n        for token in doc:\n            # Overwrite the token.norm_ if there's an entry in the data\n            token.norm_ = self.norm_table.get(token.text, token.norm_)\n        return doc\n\n@English.factory(\"token_normalizer\")\ndef create_en_normalizer(nlp, name):\n    return TokenNormalizer({\"realise\": \"realize\", \"colour\": \"color\"})\n\n@German.factory(\"token_normalizer\")\ndef create_de_normalizer(nlp, name):\n    return TokenNormalizer({\"daß\": \"dass\", \"wußte\": \"wusste\"})\n\nnlp_en = English()\nnlp_en.add_pipe(\"token_normalizer\")  # uses the English factory\nprint([token.norm_ for token in nlp_en(\"realise colour daß wußte\")])\n\nnlp_de = German()\nnlp_de.add_pipe(\"token_normalizer\")  # uses the German factory\nprint([token.norm_ for token in nlp_de(\"realise colour daß wußte\")])\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Pipeline Component Factory in spaCy (Python)\nDESCRIPTION: Demonstrates how to register a custom pipeline component factory using the @Language.factory decorator or the Language.factory() function. This allows initializing the component by name and defining default configurations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\n# Usage as a decorator\n@Language.factory(\n   \"my_component\",\n   default_config={\"some_setting\": True},\n)\ndef create_my_component(nlp, name, some_setting):\n     return MyComponent(some_setting)\n\n# Usage as function\nLanguage.factory(\n    \"my_component\",\n    default_config={\"some_setting\": True},\n    func=create_my_component\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Blank Language Pipelines\nDESCRIPTION: Examples of creating blank language pipelines using spacy.blank() for different languages.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnlp_en = spacy.blank(\"en\")   # equivalent to English()\nnlp_de = spacy.blank(\"de\")   # equivalent to German()\n```\n\n----------------------------------------\n\nTITLE: Wrapping Custom Entity Recognizer in spaCy Pipeline\nDESCRIPTION: Shows how to integrate a custom entity recognizer into a spaCy pipeline by creating a wrapper function that converts BILUO tags to entity spans.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nimport your_custom_entity_recognizer\nfrom spacy.training import biluo_tags_to_spans\nfrom spacy.language import Language\n\n@Language.component(\"custom_ner_wrapper\")\ndef custom_ner_wrapper(doc):\n    words = [token.text for token in doc]\n    custom_entities = your_custom_entity_recognizer(words)\n    doc.ents = biluo_tags_to_spans(doc, custom_entities)\n    return doc\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom BPEmb Vectors Class in Python for spaCy\nDESCRIPTION: A complete implementation of a custom vectors class that supports BPEmb subword embeddings by extending spaCy's BaseVectors abstract class. This implementation includes methods for vector lookup, batch processing, and integration with the spaCy registry system.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# requires: pip install bpemb\nimport warnings\nfrom pathlib import Path\nfrom typing import Callable, Optional, cast\n\nfrom bpemb import BPEmb\nfrom thinc.api import Ops, get_current_ops\nfrom thinc.backends import get_array_ops\nfrom thinc.types import Floats2d\n\nfrom spacy.strings import StringStore\nfrom spacy.util import registry\nfrom spacy.vectors import BaseVectors\nfrom spacy.vocab import Vocab\n\n\nclass BPEmbVectors(BaseVectors):\n    def __init__(\n        self,\n        *,\n        strings: Optional[StringStore] = None,\n        lang: Optional[str] = None,\n        vs: Optional[int] = None,\n        dim: Optional[int] = None,\n        cache_dir: Optional[Path] = None,\n        encode_extra_options: Optional[str] = None,\n        model_file: Optional[Path] = None,\n        emb_file: Optional[Path] = None,\n    ):\n        kwargs = {}\n        if lang is not None:\n            kwargs[\"lang\"] = lang\n        if vs is not None:\n            kwargs[\"vs\"] = vs\n        if dim is not None:\n            kwargs[\"dim\"] = dim\n        if cache_dir is not None:\n            kwargs[\"cache_dir\"] = cache_dir\n        if encode_extra_options is not None:\n            kwargs[\"encode_extra_options\"] = encode_extra_options\n        if model_file is not None:\n            kwargs[\"model_file\"] = model_file\n        if emb_file is not None:\n            kwargs[\"emb_file\"] = emb_file\n        self.bpemb = BPEmb(**kwargs)\n        self.strings = strings\n        self.name = repr(self.bpemb)\n        self.n_keys = -1\n        self.mode = \"BPEmb\"\n        self.to_ops(get_current_ops())\n\n    def __contains__(self, key):\n        return True\n\n    def is_full(self):\n        return True\n\n    def add(self, key, *, vector=None, row=None):\n        warnings.warn(\n            (\n                \"Skipping BPEmbVectors.add: the bpemb vector table cannot be \"\n                \"modified. Vectors are calculated from bytepieces.\"\n            )\n        )\n        return -1\n\n    def __getitem__(self, key):\n        return self.get_batch([key])[0]\n\n    def get_batch(self, keys):\n        keys = [self.strings.as_string(key) for key in keys]\n        bp_ids = self.bpemb.encode_ids(keys)\n        ops = get_array_ops(self.bpemb.emb.vectors)\n        indices = ops.asarray(ops.xp.hstack(bp_ids), dtype=\"int32\")\n        lengths = ops.asarray([len(x) for x in bp_ids], dtype=\"int32\")\n        vecs = ops.reduce_mean(cast(Floats2d, self.bpemb.emb.vectors[indices]), lengths)\n        return vecs\n\n    @property\n    def shape(self):\n        return self.bpemb.vectors.shape\n\n    def __len__(self):\n        return self.shape[0]\n\n    @property\n    def vectors_length(self):\n        return self.shape[1]\n\n    @property\n    def size(self):\n        return self.bpemb.vectors.size\n\n    def to_ops(self, ops: Ops):\n        self.bpemb.emb.vectors = ops.asarray(self.bpemb.emb.vectors)\n\n\n@registry.vectors(\"BPEmbVectors.v1\")\ndef create_bpemb_vectors(\n    lang: Optional[str] = \"multi\",\n    vs: Optional[int] = None,\n    dim: Optional[int] = None,\n    cache_dir: Optional[Path] = None,\n    encode_extra_options: Optional[str] = None,\n    model_file: Optional[Path] = None,\n    emb_file: Optional[Path] = None,\n) -> Callable[[Vocab], BPEmbVectors]:\n    def bpemb_vectors_factory(vocab: Vocab) -> BPEmbVectors:\n        return BPEmbVectors(\n            strings=vocab.strings,\n            lang=lang,\n            vs=vs,\n            dim=dim,\n            cache_dir=cache_dir,\n            encode_extra_options=encode_extra_options,\n            model_file=model_file,\n            emb_file=emb_file,\n        )\n\n    return bpemb_vectors_factory\n```\n\n----------------------------------------\n\nTITLE: Extracting Named Entities using spaCy in Python\nDESCRIPTION: Demonstrates how to load a spaCy model, process text, and extract named entities with their properties (text, position, and label). The example shows recognition of organization (ORG), geopolitical entity (GPE), and monetary value (MONEY) entities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_named-entities.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Pipeline Component for Person-Organization Extraction\nDESCRIPTION: Implements a full spaCy pipeline component that extracts relationships between people and their affiliated organizations. This example includes merging entities and visualizing the parsed sentence structure with displaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.language import Language\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n@Language.component(\"extract_person_orgs\")\ndef extract_person_orgs(doc):\n    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n    for ent in person_entities:\n        head = ent.root.head\n        if head.lemma_ == \"work\":\n            preps = [token for token in head.children if token.dep_ == \"prep\"]\n            for prep in preps:\n                orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \"VBD\"})\n    return doc\n\n# To make the entities easier to work with, we'll merge them into single tokens\nnlp.add_pipe(\"merge_entities\")\nnlp.add_pipe(\"extract_person_orgs\")\n\ndoc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n# If you're not in a Jupyter / IPython environment, use displacy.serve\ndisplacy.render(doc, options={\"fine_grained\": True})\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy Language Models via pip\nDESCRIPTION: Demonstrates how to install spaCy language models using pip, including options for installing from URLs or local files.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U %%SPACY_PKG_NAME%%SPACY_PKG_FLAGS\n$ python -m spacy download en_core_web_sm\n```\n\n----------------------------------------\n\nTITLE: Lemmatization in spaCy\nDESCRIPTION: This snippet demonstrates how to use spaCy's lemmatization capabilities. It loads an English model, retrieves the lemmatizer component, and shows how to access lemmas for each token in a sentence.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\n# English pipelines include a rule-based lemmatizer\nnlp = spacy.load(\"en_core_web_sm\")\nlemmatizer = nlp.get_pipe(\"lemmatizer\")\nprint(lemmatizer.mode)  # 'rule'\n\ndoc = nlp(\"I was reading the paper.\")\nprint([token.lemma_ for token in doc])\n# ['I', 'be', 'read', 'the', 'paper', '.']\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Tokenizer in spaCy with Regular Expressions\nDESCRIPTION: This example demonstrates how to create a custom tokenizer by defining special cases, prefix, suffix, and infix rules using regular expressions. It handles special cases like emoticons and customizes how punctuation and hyphens are processed.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport re\nimport spacy\nfrom spacy.tokenizer import Tokenizer\n\nspecial_cases = {\":)\": [{\"ORTH\": \":)\"}]}\nprefix_re = re.compile(r'''^[\\\\[\\\\(\"']''')\nsuffix_re = re.compile(r'''[\\\\]\\\\)\"']$''')\ninfix_re = re.compile(r'''[-~]''')\nsimple_url_re = re.compile(r'''^https?://''')\n\ndef custom_tokenizer(nlp):\n    return Tokenizer(nlp.vocab, rules=special_cases,\n                                prefix_search=prefix_re.search,\n                                suffix_search=suffix_re.search,\n                                infix_finditer=infix_re.finditer,\n                                url_match=simple_url_re.match)\n\nnlp = spacy.load(\"en_core_web_sm\")\nnlp.tokenizer = custom_tokenizer(nlp)\ndoc = nlp(\"hello-world. :)\")\nprint([t.text for t in doc]) # ['hello', '-', 'world.', ':)']\n```\n\n----------------------------------------\n\nTITLE: Training Command with Config Overrides\nDESCRIPTION: Example of running spaCy training with command-line overrides for config settings, demonstrating how to specify custom paths and training parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy train config.cfg --paths.train ./corpus/train.spacy --paths.dev ./corpus/dev.spacy --training.max_epochs 3\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy Pipelines with Disabled or Excluded Components in Python\nDESCRIPTION: Examples of loading a spaCy pipeline while disabling or excluding specific components. The 'disable' parameter loads components but doesn't run them in the pipeline, while 'exclude' doesn't load the components at all.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Load the pipeline without the entity recognizer\nnlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n\n# Load the tagger and parser but don't enable them\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])\n# Explicitly enable the tagger later on\nnlp.enable_pipe(\"tagger\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a RESTCountries Pipeline Component in Python for spaCy\nDESCRIPTION: This code implements a spaCy pipeline component that fetches country metadata from the REST Countries API. It uses PhraseMatcher to identify country mentions, annotates them as entities, and adds custom attributes to Doc and Span objects containing country information like capitals, coordinates, and flags.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom spacy.lang.en import English\nfrom spacy.language import Language\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.tokens import Doc, Span, Token\n\n@Language.factory(\"rest_countries\")\nclass RESTCountriesComponent:\n    def __init__(self, nlp, name, label=\"GPE\"):\n        r = requests.get(\"https://restcountries.com/v2/all\")\n        r.raise_for_status()  # make sure requests raises an error if it fails\n        countries = r.json()\n        # Convert API response to dict keyed by country name for easy lookup\n        self.countries = {c[\"name\"]: c for c in countries}\n        self.label = label\n        # Set up the PhraseMatcher with Doc patterns for each country name\n        self.matcher = PhraseMatcher(nlp.vocab)\n        self.matcher.add(\"COUNTRIES\", [nlp.make_doc(c) for c in self.countries.keys()])\n        # Register attributes on the Span. We'll be overwriting this based on\n        # the matches, so we're only setting a default value, not a getter.\n        Span.set_extension(\"is_country\", default=None)\n        Span.set_extension(\"country_capital\", default=None)\n        Span.set_extension(\"country_latlng\", default=None)\n        Span.set_extension(\"country_flag\", default=None)\n        # Register attribute on Doc via a getter that checks if the Doc\n        # contains a country entity\n        Doc.set_extension(\"has_country\", getter=self.has_country)\n\n    def __call__(self, doc):\n        spans = []  # keep the spans for later so we can merge them afterwards\n        for _, start, end in self.matcher(doc):\n            # Generate Span representing the entity & set label\n            entity = Span(doc, start, end, label=self.label)\n            # Set custom attributes on entity. Can be extended with other data\n            # returned by the API, like currencies, country code, calling code etc.\n            entity._.set(\"is_country\", True)\n            entity._.set(\"country_capital\", self.countries[entity.text][\"capital\"])\n            entity._.set(\"country_latlng\", self.countries[entity.text][\"latlng\"])\n            entity._.set(\"country_flag\", self.countries[entity.text][\"flag\"])\n            spans.append(entity)\n        # Overwrite doc.ents and add entity – be careful not to replace!\n        doc.ents = list(doc.ents) + spans\n        return doc  # don't forget to return the Doc!\n\n    def has_country(self, doc):\n        \"\"\"Getter for Doc attributes. Since the getter is only called\n        when we access the attribute, we can refer to the Span's 'is_country'\n        attribute here, which is already set in the processing step.\"\"\"\n        return any([entity._.get(\"is_country\") for entity in doc.ents])\n\nnlp = English()\nnlp.add_pipe(\"rest_countries\", config={\"label\": \"GPE\"})\ndoc = nlp(\"Some text about Colombia and the Czech Republic\")\nprint(\"Pipeline\", nlp.pipe_names)  # pipeline contains component name\nprint(\"Doc has countries\", doc._.has_country)  # Doc contains countries\nfor ent in doc.ents:\n    if ent._.is_country:\n        print(ent.text, ent.label_, ent._.country_capital, ent._.country_latlng, ent._.country_flag)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Matcher Results with displaCy\nDESCRIPTION: Shows how to visualize entity matches using displaCy, generating HTML visualizations of named entities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy import displacy\nhtml = displacy.render(doc, style=\"ent\", page=True,\n                    options={\"ents\": [\"EVENT\"]})\n```\n\n----------------------------------------\n\nTITLE: Basic Text Processing with spaCy\nDESCRIPTION: Simple example of processing a single text string using spaCy's NLP pipeline\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a text\")\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Attributes Globally in Python\nDESCRIPTION: Illustrates the correct way to add custom attributes to global Doc, Token, or Span objects in spaCy, ensuring early detection of namespace collisions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_43\n\nLANGUAGE: diff\nCODE:\n```\n+ from spacy.tokens import Doc\n+ def __init__(attr=\"my_attr\"):\n+     Doc.set_extension(attr, getter=self.get_doc_attr)\n\n- def __call__(doc):\n-     doc.set_extension(\"my_attr\", getter=self.get_doc_attr)\n```\n\n----------------------------------------\n\nTITLE: Registering a Basic Component Factory in Python\nDESCRIPTION: This snippet demonstrates how to register a basic component factory using the @Language.factory decorator. The factory function takes the nlp object and component name as arguments and returns a component instance.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\n@Language.factory(\"my_component\")\ndef my_component(nlp, name):\n    return MyComponent()\n```\n\n----------------------------------------\n\nTITLE: Identifying Named Entities in Text with spaCy\nDESCRIPTION: Loads the small English language model in spaCy and extracts named entities from a sample sentence. This code identifies a person (Alex Smith) and organization (Acme Corp Inc.) in the text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\nprint([(ent.text, ent.label_) for ent in doc.ents])\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Pipeline Components\nDESCRIPTION: Demonstration of defining a custom component, adding it to a pipeline, and analyzing pipeline components.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@Language.component(\"my_component\")\ndef my_component(doc):\n    return doc\n\nnlp.add_pipe(\"my_component\")\nnlp.add_pipe(\"ner\", source=other_nlp)\nnlp.analyze_pipes(pretty=True)\n```\n\n----------------------------------------\n\nTITLE: Accessing Tokens and Spans in a Doc (Python)\nDESCRIPTION: Demonstrates how to access individual tokens using integer indexing and spans using Python's slice notation. Negative indices and open-ended ranges are supported.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\nassert doc[0].text == \"Give\"\nassert doc[-1].text == \".\"\nspan = doc[1:3]\nassert span.text == \"it back\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Classification with GPT-3.5 in spaCy\nDESCRIPTION: INI configuration for setting up a text classification pipeline using GPT-3.5 model from OpenAI in spaCy. It defines the pipeline, task, and model settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\nlang = \"en\"\npipeline = [\"llm\"]\n\n[components]\n\n[components.llm]\nfactory = \"llm\"\n\n[components.llm.task]\n@llm_tasks = \"spacy.TextCat.v2\"\nlabels = [\"COMPLIMENT\", \"INSULT\"]\n\n[components.llm.model]\n@llm_models = \"spacy.GPT-3-5.v1\"\nconfig = {\"temperature\": 0.0}\n```\n\n----------------------------------------\n\nTITLE: Disabling Pipeline Components in spaCy v2.0\nDESCRIPTION: Demonstrates the new approach for disabling specific pipeline components when loading a model or temporarily using the disable_pipes context manager.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_12\n\nLANGUAGE: diff\nCODE:\n```\n- nlp = spacy.load(\"en_core_web_sm\", tagger=False, entity=False)\n- doc = nlp(\"I don't want parsed\", parse=False)\n\n+ nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"ner\"])\n+ with nlp.disable_pipes(\"parser\"):\n+    doc = nlp(\"I don't want parsed\")\n```\n\n----------------------------------------\n\nTITLE: Custom Embedding Architecture Example\nDESCRIPTION: Example of registering a custom embedding model architecture with spaCy's registry system.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.ml.staticvectors import StaticVectors\nfrom spacy.util import registry\n\nprint(\"I was imported!\")\n\n@registry.architectures(\"my_example.MyEmbedding.v1\")\ndef MyEmbedding(output_width: int) -> Model[List[Doc], List[Floats2d]]:\n    print(\"I was called!\")\n    return StaticVectors(nO=output_width)\n```\n\n----------------------------------------\n\nTITLE: Using GPT-3.5 for Text Classification in Python\nDESCRIPTION: Python code to assemble and use a spaCy pipeline with GPT-3.5 for text classification. It loads the configuration, processes text, and prints the resulting categories.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy_llm.util import assemble\n\nnlp = assemble(\"config.cfg\")\ndoc = nlp(\"You look gorgeous!\")\nprint(doc.cats)\n```\n\n----------------------------------------\n\nTITLE: Disabling Pipeline Components with select_pipes in Python\nDESCRIPTION: Demonstrates the usage of the select_pipes method to disable specific pipeline components or enable only certain components. It can be used as a context manager or to create a DisabledPipes object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nwith nlp.select_pipes(disable=[\"tagger\", \"parser\"]):\n   nlp.initialize()\n\nwith nlp.select_pipes(enable=\"ner\"):\n    nlp.initialize()\n\ndisabled = nlp.select_pipes(disable=[\"tagger\", \"parser\"])\nnlp.initialize()\ndisabled.restore()\n```\n\n----------------------------------------\n\nTITLE: Debugging the spaCy Tokenizer\nDESCRIPTION: Shows how to use the nlp.tokenizer.explain() method to debug the tokenization process. This method returns a list of tuples showing which tokenizer rule or pattern was matched for each token.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\n\nnlp = English()\ntext = '\"Let\\'s go!\"'\ndoc = nlp(text)\ntok_exp = nlp.tokenizer.explain(text)\nassert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]\nfor t in tok_exp:\n    print(t[1], \"\\t\", t[0])\n```\n\n----------------------------------------\n\nTITLE: Customizing spaCy Pipeline Components\nDESCRIPTION: Examples showing how to set custom attributes on Doc/Token objects and add custom components to the spaCy processing pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Set custom attributes\nDoc.set_extension(\"my_attr\", default=False)\nToken.set_extension(\"my_attr\", getter=my_token_getter)\nassert doc._.my_attr, token._.my_attr\n\n# Add components to the pipeline\nmy_component = lambda doc: doc\nnlp.add_pipe(my_component)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Component with Language.factory in spaCy v3.0\nDESCRIPTION: Demonstrates how to register a custom pipeline component using the @Language.factory decorator. This approach allows components to be referenced by string name and properly serialized in the pipeline. The factory function accepts the nlp object, component name, and custom configuration parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\n@Language.factory(\"my_component\", default_config={\"some_setting\": False})\ndef create_component(nlp: Language, name: str, some_setting: bool):\n    return MyCoolComponent(some_setting=some_setting)\n\n\nclass MyCoolComponent:\n    def __init__(self, some_setting):\n        self.some_setting = some_setting\n\n    def __call__(self, doc):\n        # Do something to the doc\n        return doc\n```\n\n----------------------------------------\n\nTITLE: Configuring TransformerModel in spaCy\nDESCRIPTION: Configuration for loading a transformer model from HuggingFace, specifying RoBERTa base model with fast tokenizer, mixed precision support, and strided span processing for handling longer documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_10\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy-transformers.TransformerModel.v3\"\nname = \"roberta-base\"\ntokenizer_config = {\"use_fast\": true}\ntransformer_config = {}\nmixed_precision = true\ngrad_scaler_config = {\"init_scale\": 32768}\n\n[model.get_spans]\n@span_getters = \"spacy-transformers.strided_spans.v1\"\nwindow = 128\nstride = 96\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Whitespace Tokenizer in spaCy\nDESCRIPTION: This example demonstrates how to create a custom whitespace tokenizer class that splits text on spaces. It shows proper handling of empty strings and trailing spaces, and how to integrate it into spaCy's pipeline by replacing the default tokenizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.tokens import Doc\n\nclass WhitespaceTokenizer:\n    def __init__(self, vocab):\n        self.vocab = vocab\n\n    def __call__(self, text):\n        words = text.split(\" \")\n        spaces = [True] * len(words)\n        # Avoid zero-length tokens\n        for i, word in enumerate(words):\n            if word == \"\":\n                words[i] = \" \"\n                spaces[i] = False\n        # Remove the final trailing space\n        if words[-1] == \" \":\n            words = words[0:-1]\n            spaces = spaces[0:-1]\n        else:\n           spaces[-1] = False\n\n        return Doc(self.vocab, words=words, spaces=spaces)\n\nnlp = spacy.blank(\"en\")\nnlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\ndoc = nlp(\"What's happened to me? he thought. It wasn't a dream.\")\nprint([token.text for token in doc])\n```\n\n----------------------------------------\n\nTITLE: Inspecting spaCy Pipeline Components in Python\nDESCRIPTION: Example showing how to inspect available components and active components in a spaCy pipeline using properties like pipe_names, component_names, and disabled.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\"ner\")\nnlp.add_pipe(\"textcat\")\nassert nlp.pipe_names == [\"ner\", \"textcat\"]\nnlp.disable_pipe(\"ner\")\nassert nlp.pipe_names == [\"textcat\"]\nassert nlp.component_names == [\"ner\", \"textcat\"]\nassert nlp.disabled == [\"ner\"]\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateless Function Components in Python\nDESCRIPTION: Shows how to create a simple stateless pipeline component using the @Language.component decorator. The component takes a Doc object and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\n@Language.component(\"my_component\")\ndef my_component(doc):\n    return doc\n```\n\n----------------------------------------\n\nTITLE: Temporarily Disabling spaCy Pipeline Components in Python\nDESCRIPTION: Using nlp.select_pipes() as a context manager or with manual restoration to temporarily disable specific pipeline components for a block of code. This avoids unnecessary processing for specific tasks.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# 1. Use as a context manager\nwith nlp.select_pipes(disable=[\"tagger\", \"parser\", \"lemmatizer\"]):\n    doc = nlp(\"I won't be tagged and parsed\")\ndoc = nlp(\"I will be tagged and parsed\")\n\n# 2. Restore manually\ndisabled = nlp.select_pipes(disable=\"ner\")\ndoc = nlp(\"I won't have named entities\")\ndisabled.restore()\n```\n\n----------------------------------------\n\nTITLE: Navigating Parse Tree Dependencies\nDESCRIPTION: Shows how to traverse the dependency parse tree to analyze syntactic relations between tokens, including head-child relationships and dependency labels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\nfor token in doc:\n    print(token.text, token.dep_, token.head.text, token.head.pos_,\n            [child for child in token.children])\n```\n\n----------------------------------------\n\nTITLE: Migrating add_pipe Usage from spaCy v2 to v3\nDESCRIPTION: Illustrates the changes required when upgrading code that adds custom components to a spaCy pipeline. In v3, components are added by string name rather than passing the component instance directly, with configuration passed as a separate argument.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_34\n\nLANGUAGE: diff\nCODE:\n```\nimport spacy\nfrom your_plugin import MyCoolComponent\n\nnlp = spacy.load(\"en_core_web_sm\")\n- component = MyCoolComponent(some_setting=True)\n- nlp.add_pipe(component)\n+ nlp.add_pipe(\"my_component\", config={\"some_setting\": True})\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy Pipeline Components\nDESCRIPTION: Example configuration file (config.cfg) showing how to specify language and pipeline components with their settings\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\nlang = \"en\"\npipeline = [\"tok2vec\", \"parser\"]\n\n[components]\n\n[components.tok2vec]\nfactory = \"tok2vec\"\n# Settings for the tok2vec component\n\n[components.parser]\nfactory = \"parser\"\n# Settings for the parser component\n```\n\n----------------------------------------\n\nTITLE: Downloading a transformer-based pipeline in Python\nDESCRIPTION: Example of how to download the English transformer-based pipeline using the spaCy CLI.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy download en_core_web_trf\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Callback in Python\nDESCRIPTION: Python implementation of a custom callback function to add a stop word to the language defaults. The function is registered using the @spacy.registry.callbacks decorator.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\n@spacy.registry.callbacks(\"customize_language_data\")\ndef create_callback():\n    def customize_language_data(lang_cls):\n        lang_cls.Defaults.stop_words.add(\"good\")\n        return lang_cls\n\n    return customize_language_data\n```\n\n----------------------------------------\n\nTITLE: Implementing BERT Word Piece Tokenizer in spaCy\nDESCRIPTION: Demonstrates how to integrate the BERT word piece tokenizer from the tokenizers library with spaCy. The custom tokenizer creates a Doc object with tokens that match the BERT word pieces, preserving spaces between tokens.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom tokenizers import BertWordPieceTokenizer\nfrom spacy.tokens import Doc\nimport spacy\n\nclass BertTokenizer:\n    def __init__(self, vocab, vocab_file, lowercase=True):\n        self.vocab = vocab\n        self._tokenizer = BertWordPieceTokenizer(vocab_file, lowercase=lowercase)\n\n    def __call__(self, text):\n        tokens = self._tokenizer.encode(text)\n        words = []\n        spaces = []\n        for i, (text, (start, end)) in enumerate(zip(tokens.tokens, tokens.offsets)):\n            words.append(text)\n            if i < len(tokens.tokens) - 1:\n                # If next start != current end we assume a space in between\n                next_start, next_end = tokens.offsets[i + 1]\n                spaces.append(next_start > end)\n            else:\n                spaces.append(True)\n        return Doc(self.vocab, words=words, spaces=spaces)\n\nnlp = spacy.blank(\"en\")\nnlp.tokenizer = BertTokenizer(nlp.vocab, \"bert-base-uncased-vocab.txt\")\ndoc = nlp(\"Justin Drew Bieber is a Canadian singer, songwriter, and actor.\")\nprint(doc.text, [token.text for token in doc])\n# [CLS]justin drew bi##eber is a canadian singer, songwriter, and actor.[SEP]\n# ['[CLS]', 'justin', 'drew', 'bi', '##eber', 'is', 'a', 'canadian', 'singer',\n#  ',', 'songwriter', ',', 'and', 'actor', '.', '[SEP]']\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Language Class in spaCy\nDESCRIPTION: This snippet shows how to register a custom language class using the @spacy.registry.languages decorator. This allows the custom language to be used with spacy.blank() and in training configs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.lang.en import English\n\nclass CustomEnglishDefaults(English.Defaults):\n    stop_words = set([\"custom\", \"stop\"])\n\n@spacy.registry.languages(\"custom_en\")\nclass CustomEnglish(English):\n    lang = \"custom_en\"\n    Defaults = CustomEnglishDefaults\n\n# This now works! 🎉\nnlp = spacy.blank(\"custom_en\")\n```\n\n----------------------------------------\n\nTITLE: Rule-based Morphology Analysis in spaCy\nDESCRIPTION: This example illustrates spaCy's rule-based approach to assigning morphological features for languages with simpler morphological systems like English. It shows how to access morphological features and part-of-speech tags.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Where are you?\")\nprint(doc[2].morph)  # 'Case=Nom|Person=2|PronType=Prs'\nprint(doc[2].pos_)  # 'PRON'\n```\n\n----------------------------------------\n\nTITLE: Enabling Selected spaCy Pipeline Components in Python\nDESCRIPTION: Using the 'enable' keyword with select_pipes to run only specific components while temporarily disabling all others.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Enable only the parser\nwith nlp.select_pipes(enable=\"parser\"):\n    doc = nlp(\"I will only be parsed\")\n```\n\n----------------------------------------\n\nTITLE: Using Disable Parameter with nlp.pipe() Method in Python\nDESCRIPTION: Disabling specific components when processing multiple documents with the nlp.pipe() method, which is useful for batch processing when certain components aren't needed.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfor doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"lemmatizer\"]):\n    # Do something with the doc here\n```\n\n----------------------------------------\n\nTITLE: Analyzing spaCy Pipeline Components\nDESCRIPTION: Example of using nlp.analyze_pipes() to analyze components in the current pipeline, which provides information about their attributes, requirements, and whether they retokenize the Doc.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\"tagger\")\n```\n\n----------------------------------------\n\nTITLE: Creating Training Data for NER in spaCy (Python)\nDESCRIPTION: Script for creating a .spacy training data file from custom NER annotations. It creates Doc objects with entity spans and saves them in the binary DocBin format used for training spaCy models.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.tokens import DocBin\n\nnlp = spacy.blank(\"en\")\ntraining_data = [\n  (\"Tokyo Tower is 333m tall.\", [(0, 11, \"BUILDING\")]),\n]\n# the DocBin will store the example documents\ndb = DocBin()\nfor text, annotations in training_data:\n    doc = nlp(text)\n    ents = []\n    for start, end, label in annotations:\n        span = doc.char_span(start, end, label=label)\n        ents.append(span)\n    doc.ents = ents\n    db.add(doc)\ndb.to_disk(\"./train.spacy\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Lexeme Properties in spaCy\nDESCRIPTION: Shows how to access various properties of lexemes in spaCy's vocabulary, including text, hash value, shape, prefix, suffix, and boolean flags.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/spacy-101.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"I love coffee\")\nfor word in doc:\n    lexeme = doc.vocab[word.text]\n    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)\n```\n\n----------------------------------------\n\nTITLE: Creating a custom PyTorch model wrapper in Python\nDESCRIPTION: Example of how to create a custom PyTorch model and wrap it for use in spaCy using the Thinc library.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import nn\nfrom thinc.api import PyTorchWrapper\n\ntorch_model = nn.Sequential(\n    nn.Linear(32, 32),\n    nn.ReLU(),\n    nn.Softmax(dim=1)\n)\nmodel = PyTorchWrapper(torch_model)\n```\n\n----------------------------------------\n\nTITLE: Implementing TrainablePipe Skeleton for Relation Extraction\nDESCRIPTION: Basic structure of the RelationExtractor class that inherits from TrainablePipe, defining core methods needed for a custom spaCy pipeline component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline import TrainablePipe\n\nclass RelationExtractor(TrainablePipe):\n     def __init__(self, vocab, model, name=\"rel\"):\n        \"\"\"Create a component instance.\"\"\"\n        self.model = model\n        self.vocab = vocab\n        self.name = name\n\n    def update(self, examples, drop=0.0, sgd=None, losses=None):\n        \"\"\"Learn from a batch of Example objects.\"\"\"\n        ...\n\n    def predict(self, docs):\n        \"\"\"Apply the model to a batch of Doc objects.\"\"\"\n        ...\n\n    def set_annotations(self, docs, predictions):\n        \"\"\"Modify a batch of Doc objects using the predictions.\"\"\"\n         ...\n\n    def initialize(self, get_examples, nlp=None, labels=None):\n        \"\"\"Initialize the model before training.\"\"\"\n        ...\n\n    def add_label(self, label):\n        \"\"\"Add a label to the component.\"\"\"\n        ...\n```\n\n----------------------------------------\n\nTITLE: Running spaCy's debug data command\nDESCRIPTION: This command analyzes and validates training data using settings from the config file. It reports statistics and identifies potential issues in the data that might affect model performance.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug data [config_path] [--code] [--ignore-warnings] [--verbose] [--no-format] [overrides]\n```\n\n----------------------------------------\n\nTITLE: Using spaCy's Default Dependency Parser for Sentence Segmentation\nDESCRIPTION: Demonstrates the default sentence segmentation method using the dependency parser. This approach requires a trained pipeline but typically provides the most accurate results for standard text formats.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a sentence. This is another sentence.\")\nfor sent in doc.sents:\n    print(sent.text)\n```\n\n----------------------------------------\n\nTITLE: Decorating Custom Pipeline Components in Python\nDESCRIPTION: Custom pipeline components in spaCy v3.0 need to be decorated with @Language.component or @Language.factory. This change affects how custom components are defined and registered.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@Language.component\ndef custom_component(doc):\n    # component logic here\n    return doc\n```\n\n----------------------------------------\n\nTITLE: Using spaCy Train Helper Function in Python\nDESCRIPTION: Example of running spaCy training from a Python script by calling the train helper function directly with a config file path and optional overrides for training and development data paths.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.cli.train import train\n\ntrain(\"./config.cfg\", overrides={\"paths.train\": \"./train.spacy\", \"paths.dev\": \"./dev.spacy\"})\n```\n\n----------------------------------------\n\nTITLE: Accessing Sentence Boundaries in spaCy Doc Object (Python)\nDESCRIPTION: Shows how to iterate over sentences in a document using the sents property. This requires sentence boundaries to have been set by a parser, senter, or sentencizer component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence. Here's another...\")\nsents = list(doc.sents)\nassert len(sents) == 2\nassert [s.root.text for s in sents] == [\"is\", \"'s\"]\n```\n\n----------------------------------------\n\nTITLE: Adding Pipeline Components\nDESCRIPTION: Example demonstrating how to add built-in components to a blank pipeline\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\"sentencizer\")\n# add_pipe returns the added component\nruler = nlp.add_pipe(\"entity_ruler\")\n```\n\n----------------------------------------\n\nTITLE: Training Loop with Example Objects\nDESCRIPTION: Python code showing a complete training loop using the new Example objects, including initialization and batch updates for an NER model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nTRAIN_DATA = [\n    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n    (\"I like London.\", {\"entities\": [(7, 13, \"LOC\")]}),\n]\nexamples = []\nfor text, annots in TRAIN_DATA:\n    examples.append(Example.from_dict(nlp.make_doc(text), annots))\nnlp.initialize(lambda: examples)\nfor i in range(20):\n    random.shuffle(examples)\n    for batch in minibatch(examples, size=8):\n        nlp.update(batch)\n```\n\n----------------------------------------\n\nTITLE: Training spaCy Model with Custom Data Paths\nDESCRIPTION: This command trains a spaCy model using a configuration file and specifies custom paths for training and development data. It also sets an output directory for the trained model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom LLM Task in Python\nDESCRIPTION: Python code showing how to implement a custom LLM task by creating a task class with generate_prompts and parse_responses methods, and registering it using a decorator.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Iterable, List\nfrom spacy.tokens import Doc\nfrom spacy_llm.registry import registry\nfrom spacy_llm.util import split_labels\n\n@registry.llm_tasks(\"my_namespace.MyTask.v1\")\ndef make_my_task(labels: str, my_other_config_val: float) -> \"MyTask\":\n    labels_list = split_labels(labels)\n    return MyTask(labels=labels_list, my_other_config_val=my_other_config_val)\n\nclass MyTask:\n    def __init__(self, labels: List[str], my_other_config_val: float):\n        ...\n\n    def generate_prompts(self, docs: Iterable[Doc]) -> Iterable[str]:\n        ...\n\n    def parse_responses(\n        self, docs: Iterable[Doc], responses: Iterable[str]\n    ) -> Iterable[Doc]:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Downloading spaCy Language Models via Command Line\nDESCRIPTION: Shows how to download spaCy language models using the command line interface, including options for specific versions and direct downloads.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# Download best-matching version of a package for your spaCy installation\n$ python -m spacy download en_core_web_sm\n\n# Download exact package version\n$ python -m spacy download en_core_web_sm-3.0.0 --direct\n```\n\n----------------------------------------\n\nTITLE: Initializing spaCy Language Pipeline for Training in Python\nDESCRIPTION: This code snippet demonstrates how to initialize a spaCy Language pipeline for training using the initialize method. It takes a function that returns training examples and returns an optimizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nget_examples = lambda: examples\noptimizer = nlp.initialize(get_examples)\n```\n\n----------------------------------------\n\nTITLE: Accessing Named Entity Annotations in spaCy in Python\nDESCRIPTION: This code demonstrates how to access named entity recognition (NER) annotations at both document and token levels, showing entity text, position, and type information.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n\n# document level\nents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\nprint(ents)\n\n# token level\nent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\nent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\nprint(ent_san)  # ['San', 'B', 'GPE']\nprint(ent_francisco)  # ['Francisco', 'I', 'GPE']\n```\n\n----------------------------------------\n\nTITLE: Configuring Shared Embedding Layers in spaCy\nDESCRIPTION: This snippet demonstrates how to set up a shared tok2vec component in a spaCy pipeline configuration. It shows the configuration for a tok2vec component and an NER component that listens to the shared embeddings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[components.tok2vec]\nfactory = \"tok2vec\"\n\n[components.tok2vec.model]\n@architectures = \"spacy.Tok2Vec.v2\"\n\n[components.tok2vec.model.embed]\n@architectures = \"spacy.MultiHashEmbed.v2\"\n\n[components.tok2vec.model.encode]\n@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n\n[components.ner]\nfactory = \"ner\"\n\n[components.ner.model]\n@architectures = \"spacy.TransitionBasedParser.v1\"\n\n[components.ner.model.tok2vec]\n@architectures = \"spacy.Tok2VecListener.v1\"\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy Pipeline Examples\nDESCRIPTION: Examples of loading spaCy pipelines using different methods including from package name, file path, and with component exclusions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_sm\") # package\nnlp = spacy.load(\"/path/to/pipeline\") # string path\nnlp = spacy.load(Path(\"/path/to/pipeline\")) # pathlib Path\n\nnlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\", \"tagger\"])\n```\n\n----------------------------------------\n\nTITLE: Finding Verbs with Subjects\nDESCRIPTION: Demonstrates efficient pattern matching for finding verbs with subjects in the dependency tree by traversing from below.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.symbols import nsubj, VERB\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n\n# Finding a verb with a subject from below — good\nverbs = set()\nfor possible_subject in doc:\n    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n        verbs.add(possible_subject.head)\nprint(verbs)\n```\n\n----------------------------------------\n\nTITLE: Serializing and Deserializing spaCy Pipeline\nDESCRIPTION: Demonstrates how to serialize a spaCy pipeline's binary data and configuration, and restore it later. The process involves saving both the binary component data and the config dictionary containing pipeline settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconfig = nlp.config\nbytes_data = nlp.to_bytes()\n```\n\nLANGUAGE: python\nCODE:\n```\nlang_cls = spacy.util.get_lang_class(config[\"nlp\"][\"lang\"])\nnlp = lang_cls.from_config(config)\nnlp.from_bytes(bytes_data)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Named Entities with displaCy in Python\nDESCRIPTION: This snippet demonstrates how to use spaCy and displaCy to visualize named entities in a given text. It loads a pre-trained English model, processes the text, and serves the visualization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\n\ntext = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(text)\ndisplacy.serve(doc, style=\"ent\")\n```\n\n----------------------------------------\n\nTITLE: Packaging a spaCy Pipeline with CLI Command\nDESCRIPTION: Shows how to use spaCy's CLI to create an installable package from a trained pipeline. This command generates all necessary files and a binary wheel or tarball that can be installed with pip.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy package ./en_example_pipeline ./packages\n```\n\n----------------------------------------\n\nTITLE: Efficient Phrase Matching with spaCy PhraseMatcher\nDESCRIPTION: This code demonstrates the use of spaCy's PhraseMatcher for efficient matching of large terminology lists. It creates Doc objects for patterns and matches them against the input text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.matcher import PhraseMatcher\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = PhraseMatcher(nlp.vocab)\nterms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n# Only run nlp.make_doc to speed things up\npatterns = [nlp.make_doc(text) for text in terms]\nmatcher.add(\"TerminologyList\", patterns)\n\ndoc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n          \"converse in the Oval Office inside the White House in Washington, D.C.\")\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(span.text)\n```\n\n----------------------------------------\n\nTITLE: Configuring EntityRecognizer Pipeline Component\nDESCRIPTION: Example showing how to configure and add the NER component to the spaCy pipeline with custom settings including moves, update oracle size, model and incorrect spans handling.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.ner import DEFAULT_NER_MODEL\nconfig = {\n   \"moves\": None,\n   \"update_with_oracle_cut_size\": 100,\n   \"model\": DEFAULT_NER_MODEL,\n   \"incorrect_spans_key\": \"incorrect_spans\",\n}\nnlp.add_pipe(\"ner\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Using Rule-based Sentencizer Component in spaCy\nDESCRIPTION: Demonstrates how to use the Sentencizer component, which is a simple rule-based approach that splits sentences on punctuation. This is useful when you need sentence boundaries without requiring a full trained pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.lang.en import English\n\nnlp = English()  # just the language with no pipeline\nnlp.add_pipe(\"sentencizer\")\ndoc = nlp(\"This is a sentence. This is another sentence.\")\nfor sent in doc.sents:\n    print(sent.text)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Data Reader with Text Variants\nDESCRIPTION: This Python function implements a custom corpus reader that streams data and creates random lexical variants of texts. It's registered with spaCy's registry system to be usable in configuration files.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Callable, Iterator, List\nimport spacy\nfrom spacy.training import Example\nfrom spacy.language import Language\nimport random\n\n@spacy.registry.readers(\"corpus_variants.v1\")\ndef stream_data(source: str) -> Callable[[Language], Iterator[Example]]:\n    def generate_stream(nlp):\n        for text, cats in read_custom_data(source):\n            # Create a random variant of the example text\n            i = random.randint(0, len(text) - 1)\n            variant = text[:i] + text[i].upper() + text[i + 1:]\n            doc = nlp.make_doc(variant)\n            example = Example.from_dict(doc, {\"cats\": cats})\n            yield example\n\n    return generate_stream\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Vectors in Python when Creating a Pipeline\nDESCRIPTION: Code snippet demonstrating how to configure custom vectors programmatically when creating a blank spaCy pipeline in Python. This allows for using BPEmb vectors without a config file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.blank(\"en\", config={\"nlp.vectors\": {\"@vectors\": \"BPEmbVectors.v1\", \"lang\": \"en\"}})\n```\n\n----------------------------------------\n\nTITLE: Adding Pipeline Components in spaCy\nDESCRIPTION: Demonstrates registering and adding custom components to the spaCy processing pipeline using Language.add_pipe. Shows component registration, adding with different positions, and copying from source pipelines.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@Language.component(\"component\")\ndef component_func(doc):\n    # modify Doc and return it\n    return doc\n\nnlp.add_pipe(\"component\", before=\"ner\")\ncomponent = nlp.add_pipe(\"component\", name=\"custom_name\", last=True)\n\n# Add component from source pipeline\nsource_nlp = spacy.load(\"en_core_web_sm\")\nnlp.add_pipe(\"ner\", source=source_nlp)\n```\n\n----------------------------------------\n\nTITLE: Configuring Independent Embedding Layers in spaCy\nDESCRIPTION: This snippet shows how to configure independent embedding layers for each component in a spaCy pipeline. It demonstrates the configuration for an NER component with its own tok2vec instance.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[components.ner]\nfactory = \"ner\"\n\n[components.ner.model]\n@architectures = \"spacy.TransitionBasedParser.v1\"\n\n[components.ner.model.tok2vec]\n@architectures = \"spacy.Tok2Vec.v2\"\n\n[components.ner.model.tok2vec.embed]\n@architectures = \"spacy.MultiHashEmbed.v2\"\n\n[components.ner.model.tok2vec.encode]\n@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n```\n\n----------------------------------------\n\nTITLE: Extracting Person-Organization Relationships Using Dependency Parsing\nDESCRIPTION: Demonstrates how to use spaCy's dependency parsing to find relationships between people and organizations. The code identifies when a person worked at an organization by analyzing the syntactic structure of the sentence and verb tense.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nperson_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\nfor ent in person_entities:\n    # Because the entity is a span, we need to use its root token. The head\n    # is the syntactic governor of the person, e.g. the verb\n    head = ent.root.head\n    if head.lemma_ == \"work\":\n        # Check if the children contain a preposition\n        preps = [token for token in head.children if token.dep_ == \"prep\"]\n        for prep in preps:\n            # Check if tokens part of ORG entities are in the preposition's\n            # children, e.g. at -> Acme Corp Inc.\n            orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n            # If the verb is in past tense, the company was a previous company\n            print({\"person\": ent, \"orgs\": orgs, \"past\": head.tag_ == \"VBD\"})\n```\n\n----------------------------------------\n\nTITLE: Integrating Custom Model for POS, Dependency Parsing in spaCy\nDESCRIPTION: Demonstrates how to wrap a custom model that provides POS tags, dependency labels, and head indices into a spaCy pipeline component, creating a new Doc object with the custom annotations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nimport your_custom_model\nfrom spacy.language import Language\nfrom spacy.symbols import POS, TAG, DEP, HEAD\nfrom spacy.tokens import Doc\nimport numpy\n\n@Language.component(\"custom_model_wrapper\")\ndef custom_model_wrapper(doc):\n    words = [token.text for token in doc]\n    spaces = [token.whitespace for token in doc]\n    pos, tags, deps, heads = your_custom_model(words)\n    # Convert the strings to integers and add them to the string store\n    pos = [doc.vocab.strings.add(label) for label in pos]\n    tags = [doc.vocab.strings.add(label) for label in tags]\n    deps = [doc.vocab.strings.add(label) for label in deps]\n    # Create a new Doc from a numpy array\n    attrs = [POS, TAG, DEP, HEAD]\n    arr = numpy.array(list(zip(pos, tags, deps, heads)), dtype=\"uint64\")\n    new_doc = Doc(doc.vocab, words=words, spaces=spaces).from_array(attrs, arr)\n    return new_doc\n```\n\n----------------------------------------\n\nTITLE: Creating Doc Objects with Pre-tokenized Text in spaCy\nDESCRIPTION: Shows how to create spaCy Doc objects directly from pre-tokenized text by providing lists of words and spaces. This approach allows working with pre-existing tokenization while maintaining the correct text representation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.tokens import Doc\n\nnlp = spacy.blank(\"en\")\nwords = [\"Hello\", \",\", \"world\", \"!\"]\nspaces = [False, True, False, False]\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\nprint(doc.text)\nprint([(t.text, t.text_with_ws, t.whitespace_) for t in doc])\n```\n\n----------------------------------------\n\nTITLE: Initializing SpanCategorizer Component in Python\nDESCRIPTION: Example of how to initialize the SpanCategorizer component in spaCy. It demonstrates adding the component to the pipeline and initializing it with examples and the nlp object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\nspancat.initialize(lambda: examples, nlp=nlp)\n```\n\n----------------------------------------\n\nTITLE: Expanding Person Entities with Titles in spaCy (Python)\nDESCRIPTION: This function expands person entities by including titles like 'Dr.' or 'Mr.' It checks the token before a person entity and extends the entity span if a title is found.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\nfrom spacy.tokens import Span\n\n@Language.component(\"expand_person_entities\")\ndef expand_person_entities(doc):\n    new_ents = []\n    for ent in doc.ents:\n        # Only check for title if it's a person and not the first token\n        if ent.label_ == \"PERSON\" and ent.start != 0:\n            prev_token = doc[ent.start - 1]\n            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n                new_ents.append(new_ent)\n            else:\n                new_ents.append(ent)\n        else:\n            new_ents.append(ent)\n    doc.ents = new_ents\n    return doc\n```\n\n----------------------------------------\n\nTITLE: Loading and Saving spaCy Models in Python\nDESCRIPTION: Demonstrates various ways to load spaCy models using shortcut links, package names, or file paths. Also shows how to save and load models to/from disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en\") # shortcut link\nnlp = spacy.load(\"en_core_web_sm\") # package\nnlp = spacy.load(\"/path/to/en\") # unicode path\nnlp = spacy.load(Path(\"/path/to/en\")) # pathlib Path\n\nnlp.to_disk(\"/path/to/nlp\")\nnlp = English().from_disk(\"/path/to/nlp\")\n```\n\n----------------------------------------\n\nTITLE: Pattern Matching on Doc with spaCy Matcher\nDESCRIPTION: Example of creating a Matcher instance, adding a simple pattern to match 'hello world', and finding matches in a document.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/matcher.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.matcher import Matcher\n\nmatcher = Matcher(nlp.vocab)\npattern = [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\nmatcher.add(\"HelloWorld\", [pattern])\ndoc = nlp(\"hello world!\")\nmatches = matcher(doc)\n```\n\n----------------------------------------\n\nTITLE: Training spaCy Model with GPU\nDESCRIPTION: This command demonstrates how to train a spaCy model using a GPU. It specifies the GPU ID to be used for training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy train config.cfg --gpu-id 0\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy Tok2Vec Model Architecture\nDESCRIPTION: Example configuration for the spaCy Tok2Vec model architecture. It defines the embedding and encoding components of the Tok2Vec model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.Tok2Vec.v2\"\n\n[model.embed]\n@architectures = \"spacy.CharacterEmbed.v2\"\n# ...\n\n[model.encode]\n@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n# ...\n```\n\n----------------------------------------\n\nTITLE: Efficient Batch Processing with nlp.pipe\nDESCRIPTION: Comparison showing how to efficiently process multiple texts using nlp.pipe instead of list comprehension\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntexts = [\"This is a text\", \"These are lots of texts\", \"...\"]\n- docs = [nlp(text) for text in texts]\n+ docs = list(nlp.pipe(texts))\n```\n\n----------------------------------------\n\nTITLE: Initializing - spaCy Tokenizer Construction Examples\nDESCRIPTION: Examples showing two ways to construct a spaCy tokenizer: creating a blank tokenizer with just the vocabulary, or creating one with default language settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tokenizer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction 1\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English\nnlp = English()\n# Create a blank Tokenizer with just the English vocab\ntokenizer = Tokenizer(nlp.vocab)\n\n# Construction 2\nfrom spacy.lang.en import English\nnlp = English()\n# Create a Tokenizer with the default settings for English\n# including punctuation rules and exceptions\ntokenizer = nlp.tokenizer\n```\n\n----------------------------------------\n\nTITLE: FastAPI Web Service with spaCy Memory Zones\nDESCRIPTION: Shows how to implement memory zones in a FastAPI web service to manage spaCy's memory usage efficiently. Includes setup of the FastAPI application, dependency injection for the NLP model, and text processing within memory zones.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/memory-management.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI, APIRouter, Depends, Request\nimport spacy\nfrom spacy.language import Language\n\nrouter = APIRouter()\n\n\ndef make_app():\n    app = FastAPI()\n    app.state.NLP = spacy.load(\"en_core_web_sm\")\n    app.include_router(router)\n    return app\n\n\ndef get_nlp(request: Request) -> Language:\n    return request.app.state.NLP\n\n\n@router.post(\"/parse\")\ndef parse_texts(\n    *, text_batch: list[str], nlp: Language = Depends(get_nlp)\n) -> list[dict]:\n    with nlp.memory_zone():\n        # Put the spaCy call within a separate function, so we can't\n        # leak the Doc objects outside the scope of the memory zone.\n        output = _process_text(nlp, text_batch)\n    return output\n\n\ndef _process_text(nlp: Language, texts: list[str]) -> list[dict]:\n    # Call spaCy, and transform the output into our own data\n    # structures. This function is called from inside a memory\n    # zone, so must not return the spaCy objects.\n    docs = list(nlp.pipe(texts))\n    return [\n        {\n            \"tokens\": [{\"text\": t.text} for t in doc],\n            \"entities\": [\n                {\"start\": e.start, \"end\": e.end, \"label\": e.label_} for e in doc.ents\n            ],\n        }\n        for doc in docs\n    ]\n\n\napp = make_app()\n```\n\n----------------------------------------\n\nTITLE: Saving spaCy Language Model to Disk (Python)\nDESCRIPTION: Example demonstrating how to save the current state of a spaCy Language object to a directory using the to_disk method. This saves all components and their weights if a trained pipeline is loaded.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nnlp.to_disk(\"/path/to/pipeline\")\n```\n\n----------------------------------------\n\nTITLE: Loading SpaCy Pipeline Packages\nDESCRIPTION: Example showing how to import and load a SpaCy pipeline package using Python's native import syntax, which is recommended for larger codebases as it provides better error handling and linting support.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n```\n\n----------------------------------------\n\nTITLE: Configuring NER.v3 Basic Component in spaCy\nDESCRIPTION: Basic configuration for the spaCy NER.v3 component that specifies the task and entity labels to extract.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_24\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.NER.v3\"\nlabels = [\"PERSON\", \"ORGANISATION\", \"LOCATION\"]\n```\n\n----------------------------------------\n\nTITLE: Applying spaCy Pipeline to Text Files (Bash)\nDESCRIPTION: Demonstrates the usage of the new 'apply' CLI command to process text files with a spaCy pipeline and save the annotated documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-5.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ spacy apply en_core_web_sm my_texts/ output.spacy\n```\n\n----------------------------------------\n\nTITLE: Initializing TextCategorizer with Label Data in Python\nDESCRIPTION: This code shows how to access the label data of the TextCategorizer and use it to initialize the model with a pre-defined label set. This is useful for initializing the model with existing labels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nlabels = textcat.label_data\ntextcat.initialize(lambda: [], nlp=nlp, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Complete Dependency Matcher Pattern Implementation\nDESCRIPTION: Demonstrates the full dependency matcher pattern that finds sentences about founders and companies, showing how multiple token patterns can be linked together to form a complex match pattern.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.matcher import DependencyMatcher\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = DependencyMatcher(nlp.vocab)\n\npattern = [\n    {\n        \"RIGHT_ID\": \"anchor_founded\",\n        \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n    },\n    {\n        \"LEFT_ID\": \"anchor_founded\",\n        \"REL_OP\": \">\",\n        \"RIGHT_ID\": \"founded_subject\",\n        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},\n    },\n    {\n        \"LEFT_ID\": \"anchor_founded\",\n        \"REL_OP\": \">\",\n        \"RIGHT_ID\": \"founded_object\",\n        \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"},\n    },\n    {\n        \"LEFT_ID\": \"founded_object\",\n        \"REL_OP\": \">\",\n        \"RIGHT_ID\": \"founded_object_modifier\",\n        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n    }\n]\n\nmatcher.add(\"FOUNDED\", [pattern])\ndoc = nlp(\"Lee, an experienced CEO, has founded two AI startups.\")\nmatches = matcher(doc)\n\nprint(matches) # [(4851363122962674176, [6, 0, 10, 9])]\n# Each token_id corresponds to one pattern dict\nmatch_id, token_ids = matches[0]\nfor i in range(len(token_ids)):\n    print(pattern[i][\"RIGHT_ID\"] + \":\", doc[token_ids[i]].text)\n```\n\n----------------------------------------\n\nTITLE: Implementing Tokenizer Customization in Python\nDESCRIPTION: Python implementation of a custom callback function to modify tokenizer settings, including removing a suffix and adding a special case.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.util import registry, compile_suffix_regex\n\n@registry.callbacks(\"customize_tokenizer\")\ndef make_customize_tokenizer():\n    def customize_tokenizer(nlp):\n        # remove a suffix\n        suffixes = list(nlp.Defaults.suffixes)\n        suffixes.remove(\"\\\\[\")\n        suffix_regex = compile_suffix_regex(suffixes)\n        nlp.tokenizer.suffix_search = suffix_regex.search\n\n        # add a special case\n        nlp.tokenizer.add_special_case(\"_SPECIAL_\", [{\"ORTH\": \"_SPECIAL_\"}])\n    return customize_tokenizer\n```\n\n----------------------------------------\n\nTITLE: Adding Special Case Tokenization Rules in spaCy\nDESCRIPTION: Demonstrates how to add a special case rule to an existing Tokenizer instance in spaCy. This example shows how to tokenize the word 'gimme' into 'gim' and 'me'.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.symbols import ORTH\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"gimme that\")  # phrase to tokenize\nprint([w.text for w in doc])  # ['gimme', 'that']\n\n# Add special case rule\nspecial_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\nnlp.tokenizer.add_special_case(\"gimme\", special_case)\n\n# Check new tokenization\nprint([w.text for w in nlp(\"gimme that\")])  # ['gim', 'me', 'that']\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Update Method\nDESCRIPTION: Implementation of the update method that handles model training by calculating loss and gradients for backpropagation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef update(\n    self,\n    examples: Iterable[Example],\n    *,\n    drop: float = 0.0,\n    sgd: Optional[Optimizer] = None,\n    losses: Optional[Dict[str, float]] = None,\n) -> Dict[str, float]:\n    # ...\n    docs = [eg.predicted for eg in examples]\n    predictions, backprop = self.model.begin_update(docs)\n    loss, gradient = self.get_loss(examples, predictions)\n    backprop(gradient)\n    losses[self.name] += loss\n    # ...\n    return losses\n```\n\n----------------------------------------\n\nTITLE: Pipeline Initialization Process\nDESCRIPTION: Abstract example showing how spacy.load works under the hood, demonstrating the steps of initializing a language pipeline\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlang = \"en\"\npipeline = [\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"attribute_ruler\", \"lemmatizer\"]\ndata_path = \"path/to/en_core_web_sm/en_core_web_sm-3.0.0\"\n\ncls = spacy.util.get_lang_class(lang)  # 1. Get Language class, e.g. English\nnlp = cls()                            # 2. Initialize it\nfor name in pipeline:\n    nlp.add_pipe(name, config={...})   # 3. Add the component to the pipeline\nnlp.from_disk(data_path)               # 4. Load in the binary data\n```\n\n----------------------------------------\n\nTITLE: Registering a Configurable BERT Tokenizer in spaCy\nDESCRIPTION: Demonstrates registering a BERT tokenizer with configurable parameters (vocabulary file and lowercase option). This function can be referenced in the training configuration with specific settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n@spacy.registry.tokenizers(\"bert_word_piece_tokenizer\")\ndef create_bert_tokenizer(vocab_file: str, lowercase: bool):\n    def create_tokenizer(nlp):\n        return BertTokenizer(nlp.vocab, vocab_file, lowercase)\n\n    return create_tokenizer\n```\n\n----------------------------------------\n\nTITLE: Downloading Assets from URL in spaCy project.yml\nDESCRIPTION: Configuration for downloading assets from a public HTTPS URL and optionally from Google Cloud Storage. Specifies destination paths, URLs, checksums, and optional flags.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nassets:\n  # Download from public HTTPS URL\n  - dest: 'assets/training.spacy'\n    url: 'https://example.com/data.spacy'\n    checksum: '63373dd656daa1fd3043ce166a59474c'\n  # Optional download from Google Cloud Storage bucket\n  - dest: 'assets/development.spacy'\n    extra: True\n    url: 'gs://your-bucket/corpora'\n    checksum: '5113dc04e03f079525edd8df3f4f39e3'\n```\n\n----------------------------------------\n\nTITLE: Batch Processing with Sentencizer\nDESCRIPTION: Example showing how to process multiple documents using the Sentencizer's pipe method with batching.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencizer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsentencizer = nlp.add_pipe(\"sentencizer\")\nfor doc in sentencizer.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Packaging spaCy Models with Custom Code\nDESCRIPTION: Command-line example for packaging a trained spaCy model with custom code. It shows how to include custom functions in the packaged model so they're available when the model is loaded.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n### Packaging\n$ python -m spacy package ./model-best ./packages --code functions.py\n```\n\n----------------------------------------\n\nTITLE: Custom Vector Embedding Implementation\nDESCRIPTION: Implementation of a custom embedding model that combines static vectors with learned embeddings through summation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom thinc.api import add, chain, remap_ids, Embed\nfrom spacy.ml.staticvectors import StaticVectors\nfrom spacy.ml.featureextractor import FeatureExtractor\nfrom spacy.util import registry\n\n@registry.architectures(\"my_example.MyEmbedding.v1\")\ndef MyCustomVectors(\n    output_width: int,\n    vector_width: int,\n    embed_rows: int,\n    key2row: Dict[int, int]\n) -> Model[List[Doc], List[Floats2d]]:\n    return add(\n        StaticVectors(nO=output_width),\n        chain(\n           FeatureExtractor([\"ORTH\"]),\n           remap_ids(key2row),\n           Embed(nO=output_width, nV=embed_rows)\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Extension Attributes in spaCy\nDESCRIPTION: Complete example showing how to create custom extension attributes with getters for Token, Doc, and Span objects to detect fruits in text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc, Span, Token\n\nfruits = [\"apple\", \"pear\", \"banana\", \"orange\", \"strawberry\"]\nis_fruit_getter = lambda token: token.text in fruits\nhas_fruit_getter = lambda obj: any([t.text in fruits for t in obj])\n\nToken.set_extension(\"is_fruit\", getter=is_fruit_getter)\nDoc.set_extension(\"has_fruit\", getter=has_fruit_getter)\nSpan.set_extension(\"has_fruit\", getter=has_fruit_getter)\n```\n\n----------------------------------------\n\nTITLE: Calculating Similarity Between Texts in spaCy\nDESCRIPTION: Example showing how to calculate similarity between documents, spans, and tokens using spaCy's similarity method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_vectors-similarity.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_md\")  # make sure to use larger package!\ndoc1 = nlp(\"I like salty fries and hamburgers.\")\ndoc2 = nlp(\"Fast food tastes very good.\")\n\n# Similarity of two documents\nprint(doc1, \"<->\", doc2, doc1.similarity(doc2))\n# Similarity of tokens and spans\nfrench_fries = doc1[2:4]\nburgers = doc1[5]\nprint(french_fries, \"<->\", burgers, french_fries.similarity(burgers))\n```\n\n----------------------------------------\n\nTITLE: Creating a Relation Model Architecture in Python\nDESCRIPTION: Defines the main architecture for a relation extraction model using spaCy's registry system. This function creates a model that takes a list of documents as input and outputs predictions as a 2D matrix of floats.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n@spacy.registry.architectures(\"rel_model.v1\")\ndef create_relation_model(...) -> Model[List[Doc], Floats2d]:\n    model = ...  # 👈 model will go here\n    return model\n```\n\n----------------------------------------\n\nTITLE: Enhanced Person-Organization Extraction with Auxiliary Verb Detection\nDESCRIPTION: An improved version of the extraction component that handles complex verb constructions. This code detects past tense not only from the main verb but also from auxiliary verbs like \"was working\".\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_55\n\nLANGUAGE: python\nCODE:\n```\n@Language.component(\"extract_person_orgs\")\ndef extract_person_orgs(doc):\n    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n    for ent in person_entities:\n        head = ent.root.head\n        if head.lemma_ == \"work\":\n            preps = [token for token in head.children if token.dep_ == \"prep\"]\n            for prep in preps:\n                orgs = [t for t in prep.children if t.ent_type_ == \"ORG\"]\n                aux = [token for token in head.children if token.dep_ == \"aux\"]\n                past_aux = any(t.tag_ == \"VBD\" for t in aux)\n                past = head.tag_ == \"VBD\" or head.tag_ == \"VBG\" and past_aux\n                print({'person': ent, 'orgs': orgs, 'past': past})\n    return doc\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Binary Training Format in Python\nDESCRIPTION: Example of creating a binary training format using DocBin and saving it to disk, then reading it back as a Corpus. This format is efficient for storage and is the main data format used in spaCy v3.0.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import DocBin\nfrom spacy.training import Corpus\n\ndoc_bin = DocBin(docs=docs)\ndoc_bin.to_disk(\"./data.spacy\")\nreader = Corpus(\"./data.spacy\")\n```\n\n----------------------------------------\n\nTITLE: Serializing a spaCy Doc to Bytes in Python\nDESCRIPTION: Demonstrates how to serialize a spaCy Doc object to a binary string using the to_bytes() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\ndoc_bytes = doc.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Processing Text Streams with spaCy's Language.pipe Method (Python)\nDESCRIPTION: Demonstrates the use of Language.pipe method to efficiently process multiple texts as a stream, yielding Doc objects. This method is more efficient for processing large numbers of texts.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntexts = [\"One document.\", \"...\", \"Lots of documents\"]\nfor doc in nlp.pipe(texts, batch_size=50):\n    assert doc.has_annotation(\"DEP\")\n```\n\n----------------------------------------\n\nTITLE: Abstract Pipeline Loading Implementation\nDESCRIPTION: Abstract example showing the internal steps of how spacy.load() works by getting the language class, initializing it, adding pipeline components and loading data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncls = spacy.util.get_lang_class(lang)  # 1. Get Language class, e.g. English\nnlp = cls()                            # 2. Initialize it\nfor name in pipeline:\n    nlp.add_pipe(name, config={...})   # 3. Add the component to the pipeline\nnlp.from_disk(data_path)               # 4. Load in the binary data\n```\n\n----------------------------------------\n\nTITLE: Saving a spaCy Pipeline to Disk in Python\nDESCRIPTION: Demonstrates how to save a trained spaCy pipeline to disk using the Language.to_disk method. This creates a directory containing all pipeline data, meta information, and configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nnlp.to_disk(\"./en_example_pipeline\")\n```\n\n----------------------------------------\n\nTITLE: Token Shape Matching with spaCy PhraseMatcher\nDESCRIPTION: This code demonstrates how to use spaCy's PhraseMatcher to match tokens based on their shape. It's particularly useful for matching patterns like IP addresses without worrying about specific tokenization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\nfrom spacy.matcher import PhraseMatcher\n\nnlp = English()\nmatcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\nmatcher.add(\"IP\", [nlp(\"127.0.0.1\"), nlp(\"127.127.0.0\")])\n\ndoc = nlp(\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\")\nfor match_id, start, end in matcher(doc):\n    print(\"Matched based on token shape:\", doc[start:end])\n```\n\n----------------------------------------\n\nTITLE: Sourcing Components from Existing spaCy Pipelines\nDESCRIPTION: Example of reusing trained components across pipelines by sourcing them from existing models. This demonstrates adding only the named entity recognizer from a pre-trained pipeline to a new blank pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\n# The source pipeline with different components\nsource_nlp = spacy.load(\"en_core_web_sm\")\nprint(source_nlp.pipe_names)\n\n# Add only the entity recognizer to the new blank pipeline\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\"ner\", source=source_nlp)\nprint(nlp.pipe_names)\n```\n\n----------------------------------------\n\nTITLE: Creating Training Data from Doc Objects in Python\nDESCRIPTION: This snippet demonstrates how to manually create spaCy training data by constructing Doc objects with words, spaces, and entity annotations, then saving them to a binary .spacy file using DocBin.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.tokens import Doc, DocBin\n\nnlp = spacy.blank(\"en\")\ndocbin = DocBin()\nwords = [\"Apple\", \"is\", \"looking\", \"at\", \"buying\", \"U.K.\", \"startup\", \".\"]\nspaces = [True, True, True, True, True, True, True, False]\nents = [\"B-ORG\", \"O\", \"O\", \"O\", \"O\", \"B-GPE\", \"O\", \"O\"]\ndoc = Doc(nlp.vocab, words=words, spaces=spaces, ents=ents)\ndocbin.add(doc)\ndocbin.to_disk(\"./train.spacy\")\n```\n\n----------------------------------------\n\nTITLE: Adding Rules to DependencyMatcher with Callback in Python\nDESCRIPTION: This snippet illustrates how to add a rule to a DependencyMatcher, including a pattern and a callback function. The callback function is executed when a match is found.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencymatcher.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef on_match(matcher, doc, id, matches):\n    print('Matched!', matches)\n\nmatcher = DependencyMatcher(nlp.vocab)\nmatcher.add(\"FOUNDED\", patterns, on_match=on_match)\n```\n\n----------------------------------------\n\nTITLE: Calling spaCy Train Function from Python\nDESCRIPTION: Example of how to programmatically call the spaCy training function from Python code instead of using the command line. This allows for training to be incorporated into larger Python applications.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.cli.train import train\n\ntrain(\"./config.cfg\", overrides={\"paths.train\": \"./train.spacy\", \"paths.dev\": \"./dev.spacy\"})\n\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dependencies with spaCy's displaCy in Python\nDESCRIPTION: This code demonstrates how to visualize dependency parsing results using spaCy's built-in displaCy module. It loads a pre-trained English model, processes a sentence, and renders the dependency structure.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n# Since this is an interactive Jupyter environment, we can use displacy.render here\ndisplacy.render(doc, style='dep')\n```\n\n----------------------------------------\n\nTITLE: Using displaCy in Jupyter Notebooks\nDESCRIPTION: This example demonstrates how to use displaCy within Jupyter notebooks for both dependency parsing and entity recognition visualizations. The code shows how displaCy automatically detects Jupyter environments to render appropriate HTML output.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Don't forget to install a trained pipeline, e.g.: python -m spacy download en\n\n# In[1]:\nimport spacy\nfrom spacy import displacy\n\n# In[2]:\ndoc = nlp(\"Rats are various medium-sized, long-tailed rodents.\")\ndisplacy.render(doc, style=\"dep\")\n\n# In[3]:\ndoc2 = nlp(LONG_NEWS_ARTICLE)\ndisplacy.render(doc2, style=\"ent\")\n```\n\n----------------------------------------\n\nTITLE: Accessing String Hashes in spaCy Vocabulary\nDESCRIPTION: Demonstrates how to access string hashes in spaCy's vocabulary using the StringStore. It shows bidirectional lookup between strings and their hash values.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/spacy-101.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"I love coffee\")\nprint(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\nprint(doc.vocab.strings[3197928453018144401])  # 'coffee'\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Batch Filter for Duplicate Examples\nDESCRIPTION: This Python function implements a custom batching strategy that filters out duplicate examples with the same text within each training batch, demonstrating how to customize the data flow during training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Callable, Iterable, Iterator, List\nimport spacy\nfrom spacy.training import Example\n\n@spacy.registry.batchers(\"filtering_batch.v1\")\ndef filter_batch(size: int) -> Callable[[Iterable[Example]], Iterator[List[Example]]]:\n    def create_filtered_batches(examples):\n        batch = []\n        for eg in examples:\n            # Remove duplicate examples with the same text from batch\n            if eg.text not in [x.text for x in batch]:\n                batch.append(eg)\n            if len(batch) == size:\n                yield batch\n                batch = []\n\n    return create_filtered_batches\n```\n\n----------------------------------------\n\nTITLE: Fuzzy and Regex Matching with Lists in spaCy (Python)\nDESCRIPTION: Shows how to use FUZZY and REGEX operators with lists in spaCy's pattern matching, allowing for more flexible and powerful matching capabilities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-5.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npattern = [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"awesome\", \"cool\", \"wonderful\"]}}}]\npattern = [{\"TEXT\": {\"REGEX\": {\"NOT_IN\": [\"^awe(some)?$\", \"^wonder(ful)?\"]}}}]\n```\n\n----------------------------------------\n\nTITLE: Getting Aligned Token Attributes in spaCy (Python)\nDESCRIPTION: Shows how to get aligned views of token attributes between predicted and reference documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npredicted = Doc(vocab, words=[\"Apply\", \"some\", \"sunscreen\"])\ntoken_ref = [\"Apply\", \"some\", \"sun\", \"screen\"]\ntags_ref = [\"VERB\", \"DET\", \"NOUN\", \"NOUN\"]\nexample = Example.from_dict(predicted, {\"words\": token_ref, \"tags\": tags_ref})\nassert example.get_aligned(\"TAG\", as_string=True) == [\"VERB\", \"DET\", \"NOUN\"]\n```\n\n----------------------------------------\n\nTITLE: Batch Processing with spaCy Tagger\nDESCRIPTION: Example demonstrating how to process multiple documents in batches using the tagger's pipe method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\nfor doc in tagger.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Evaluating spaCy Pipeline Components\nDESCRIPTION: Demonstrates how to evaluate pipeline components using the Language.evaluate method. Takes a batch of Example objects and returns evaluation scores.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nscores = nlp.evaluate(examples)\nprint(scores)\n```\n\n----------------------------------------\n\nTITLE: Configuring Annotating Components in spaCy Pipeline (INI)\nDESCRIPTION: This snippet demonstrates how to configure annotating components in a spaCy pipeline. It shows how to set up the parser to annotate during training, allowing the tagger to use dependency features.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_14\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\npipeline = [\"parser\", \"tagger\"]\n\n[components.tagger.model.tok2vec.embed]\n@architectures = \"spacy.MultiHashEmbed.v1\"\nwidth = ${components.tagger.model.tok2vec.encode.width}\nattrs = [\"NORM\",\"DEP\"]\nrows = [5000,2500]\ninclude_static_vectors = false\n\n[training]\nannotating_components = [\"parser\"]\n```\n\n----------------------------------------\n\nTITLE: Initializing a spaCy Configuration File with CLI\nDESCRIPTION: Creates a new configuration file with recommended settings for a specific language and pipeline. This example initializes an English model with named entity recognition and text categorization components, optimized for accuracy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init config config.cfg --lang en --pipeline ner,textcat --optimize accuracy\n```\n\n----------------------------------------\n\nTITLE: Exporting spaCy Pipeline Configuration (Python)\nDESCRIPTION: Example showing how to export a trainable config.cfg for the current nlp object using the config property. The config includes pipeline components and training settings, and can be saved to disk or converted to a string.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nnlp.config.to_disk(\"./config.cfg\")\nprint(nlp.config.to_str())\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Data Augmenter in spaCy\nDESCRIPTION: Python implementation of a custom data augmentation callback that produces text variants in 'SpOnGeBoB cAsE', registered with spaCy's augmenters registry. The augmenter yields both original and augmented examples.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nimport random\n\n@spacy.registry.augmenters(\"spongebob_augmenter.v1\")\ndef create_augmenter(randomize: bool = False):\n    def augment(nlp, example):\n        text = example.text\n        if randomize:\n            # Randomly uppercase/lowercase characters\n            chars = [c.lower() if random.random() < 0.5 else c.upper() for c in text]\n        else:\n            # Uppercase followed by lowercase\n            chars = [c.lower() if i % 2 else c.upper() for i, c in enumerate(text)]\n        # Create augmented training example\n        example_dict = example.to_dict()\n        doc = nlp.make_doc(\"\".join(chars))\n        example_dict[\"token_annotation\"][\"ORTH\"] = [t.text for t in doc]\n        # Original example followed by augmented example\n        yield example\n        yield example.from_dict(doc, example_dict)\n\n    return augment\n```\n\n----------------------------------------\n\nTITLE: Initializing EntityRecognizer Component in spaCy\nDESCRIPTION: Shows how to initialize the named entity recognition component and configure it via config file. The component is added to the pipeline and initialized with training examples.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\nner.initialize(lambda: examples, nlp=nlp)\n```\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[initialize.components.ner]\n\n[initialize.components.ner.labels]\n@readers = \"spacy.read_labels.v1\"\npath = \"corpus/labels/ner.json\n```\n\n----------------------------------------\n\nTITLE: Setting Named Entities in spaCy\nDESCRIPTION: Demonstrates how to set named entities in a document using Span objects with entity labels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Span\ndoc = nlp(\"Mr. Best flew to New York on Saturday morning.\")\ndoc.set_ents([Span(doc, 0, 2, \"PERSON\")])\nents = list(doc.ents)\nassert ents[0].label_ == \"PERSON\"\nassert ents[0].text == \"Mr. Best\"\n```\n\n----------------------------------------\n\nTITLE: Efficient Doc Object Serialization with DocBin\nDESCRIPTION: Shows how to serialize multiple Doc objects efficiently using DocBin class. Includes options for selecting specific attributes to save and handling user data. The example demonstrates both serialization and deserialization processes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.tokens import DocBin\n\ndoc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\ntexts = [\"Some text\", \"Lots of texts...\", \"...\"]\nnlp = spacy.load(\"en_core_web_sm\")\nfor doc in nlp.pipe(texts):\n    doc_bin.add(doc)\nbytes_data = doc_bin.to_bytes()\n\n# Deserialize later, e.g. in a new process\nnlp = spacy.blank(\"en\")\ndoc_bin = DocBin().from_bytes(bytes_data)\ndocs = list(doc_bin.get_docs(nlp.vocab))\n```\n\n----------------------------------------\n\nTITLE: Token Pattern Example in spaCy\nDESCRIPTION: Examples of token pattern attribute usage in spaCy's Matcher, showing both uppercase and lowercase attribute formats. These patterns can be used to match specific token characteristics in text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n{\"LOWER\": \"text\"}\n```\n\nLANGUAGE: python\nCODE:\n```\n{\"lower\": \"text\"}\n```\n\n----------------------------------------\n\nTITLE: Using Doc Input in spaCy Pipeline\nDESCRIPTION: Example of creating a custom Doc and processing it through the spaCy pipeline, allowing for custom tokenization or setting extensions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-2.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp.make_doc(\"This is text 500.\")\ndoc._.text_id = 500\ndoc = nlp(doc)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Sentence Boundary Detection in spaCy\nDESCRIPTION: Shows how to create a custom pipeline component that defines sentence boundaries based on specific rules. This example splits sentences on \"...\" tokens, adding these custom rules before the parser runs for additional segmentation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\nimport spacy\n\ntext = \"this is a sentence...hello...and another sentence.\"\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(text)\nprint(\"Before:\", [sent.text for sent in doc.sents])\n\n@Language.component(\"set_custom_boundaries\")\ndef set_custom_boundaries(doc):\n    for token in doc[:-1]:\n        if token.text == \"...\":\n            doc[token.i + 1].is_sent_start = True\n    return doc\n\nnlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\ndoc = nlp(text)\nprint(\"After:\", [sent.text for sent in doc.sents])\n```\n\n----------------------------------------\n\nTITLE: Merging Entities and Processing Financial Text with spaCy in Python\nDESCRIPTION: This code adds entity and noun chunk merging pipes to a spaCy pipeline, then processes financial texts to identify money entities and their relationships to subjects in the sentences.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Merge noun phrases and entities for easier analysis\nnlp.add_pipe(\"merge_entities\")\nnlp.add_pipe(\"merge_noun_chunks\")\n\nTEXTS = [\n    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n]\nfor doc in nlp.pipe(TEXTS):\n    for token in doc:\n        if token.ent_type_ == \"MONEY\":\n            # We have an attribute and direct object, so check for subject\n            if token.dep_ in (\"attr\", \"dobj\"):\n                subj = [w for w in token.head.lefts if w.dep_ == \"nsubj\"]\n                if subj:\n                    print(subj[0], \"-->\", token)\n            # We have a prepositional object with a preposition\n            elif token.dep_ == \"pobj\" and token.head.dep_ == \"prep\":\n                print(token.head.head, \"-->\", token)\n```\n\n----------------------------------------\n\nTITLE: Creating training examples for various spaCy NLP tasks\nDESCRIPTION: Examples of creating training data for part-of-speech tagging, named entity recognition, and text classification using the Example.from_dict method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Training data for a part-of-speech tagger\ndoc = Doc(vocab, words=[\"I\", \"like\", \"stuff\"])\ngold_dict = {\"tags\": [\"NOUN\", \"VERB\", \"NOUN\"]}\nexample = Example.from_dict(doc, gold_dict)\n\n# Training data for an entity recognizer (option 1)\ndoc = nlp(\"Laura flew to Silicon Valley.\")\ngold_dict = {\"entities\": [\"U-PERS\", \"O\", \"O\", \"B-LOC\", \"L-LOC\"]}\nexample = Example.from_dict(doc, gold_dict)\n\n# Training data for an entity recognizer (option 2)\ndoc = nlp(\"Laura flew to Silicon Valley.\")\ngold_dict = {\"entities\": [(0, 5, \"PERSON\"), (14, 28, \"LOC\")]}\nexample = Example.from_dict(doc, gold_dict)\n\n# Training data for text categorization\ndoc = nlp(\"I'm pretty happy about that!\")\ngold_dict = {\"cats\": {\"POSITIVE\": 1.0, \"NEGATIVE\": 0.0}}\nexample = Example.from_dict(doc, gold_dict)\n```\n\n----------------------------------------\n\nTITLE: Using DependencyMatcher for Pattern Matching\nDESCRIPTION: Example of creating and using a DependencyMatcher to match patterns within dependency parse trees using Semgrex operators.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.matcher import DependencyMatcher\n\nmatcher = DependencyMatcher(nlp.vocab)\npattern = [\n    {\"RIGHT_ID\": \"anchor_founded\", \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}},\n    {\"LEFT_ID\": \"anchor_founded\", \"REL_OP\": \">\", \"RIGHT_ID\": \"subject\", \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}}\n]\nmatcher.add(\"FOUNDED\", [pattern])\n```\n\n----------------------------------------\n\nTITLE: Disabling Components for NER-only Processing in spaCy (Python)\nDESCRIPTION: This snippet shows how to load a spaCy model while disabling all components except for Named Entity Recognition (NER). It provides examples for both non-transformer and transformer models.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/models/index.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n```\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Component with Serialization Methods in Python\nDESCRIPTION: Example of a custom spaCy pipeline component that implements to_disk and from_disk methods for serializing JSON data. The component can store arbitrary JSON-serializable data and save/load it from disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom spacy import Language\nfrom spacy.util import ensure_path\n\n@Language.factory(\"my_component\")\nclass CustomComponent:\n    def __init__(self, nlp: Language, name: str = \"my_component\"):\n        self.name = name\n        self.data = []\n\n    def __call__(self, doc):\n        # Do something to the doc here\n        return doc\n\n    def add(self, data):\n        # Add something to the component's data\n        self.data.append(data)\n\n    def to_disk(self, path, exclude=tuple()):\n        # This will receive the directory path + /my_component\n        path = ensure_path(path)\n        if not path.exists():\n            path.mkdir()\n        data_path = path / \"data.json\"\n        with data_path.open(\"w\", encoding=\"utf8\") as f:\n            f.write(json.dumps(self.data))\n\n    def from_disk(self, path, exclude=tuple()):\n        # This will receive the directory path + /my_component\n        data_path = path / \"data.json\"\n        with data_path.open(\"r\", encoding=\"utf8\") as f:\n            self.data = json.load(f)\n        return self\n```\n\n----------------------------------------\n\nTITLE: Updating TrainablePipe Model in Python\nDESCRIPTION: Learn from a batch of Example objects and update the component's model. This method takes examples, an optional dropout rate, an optimizer, and a losses dictionary.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\noptimizer = nlp.initialize()\nlosses = pipe.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Displaying Named Entities in spaCy (Python)\nDESCRIPTION: This snippet loads a pre-trained English model, processes a sample text, and prints the recognized named entities with their labels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Dr. Alex Smith chaired first board meeting of Acme Corp Inc.\")\nprint([(ent.text, ent.label_) for ent in doc.ents])\n```\n\n----------------------------------------\n\nTITLE: Updating Training Workflow in spaCy v2.0\nDESCRIPTION: Illustrates changes to the training API including using begin_training to get an optimizer, the new update method that accepts strings and dictionaries, and the to_disk method for saving models.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_14\n\nLANGUAGE: diff\nCODE:\n```\n- for itn in range(1000):\n-     for text, entities in train_data:\n-         doc = Doc(text)\n-         gold = GoldParse(doc, entities=entities)\n-         nlp.update(doc, gold)\n- nlp.end_training()\n- nlp.save_to_directory(\"/model\")\n\n+ nlp.begin_training()\n+ for itn in range(1000):\n+     for texts, annotations in train_data:\n+         nlp.update(texts, annotations)\n+ nlp.to_disk(\"/model\")\n```\n\n----------------------------------------\n\nTITLE: Predicting with TrainablePipe Component in Python\nDESCRIPTION: Example of using the predict method of a TrainablePipe component to apply the model to a batch of documents without modifying them.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\nscores = pipe.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Creating a Configurable Component Factory in Python\nDESCRIPTION: Implementation of a more complex pipeline component using the @Language.factory decorator to create a configurable component that can accept settings from the config file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nSNEKS = {\"basic\": snek, \"cute\": cute_snek}  # collection of sneks\n\n@Language.factory(\"snek\", default_config={\"snek_style\": \"basic\"})\nclass SnekFactory:\n    def __init__(self, nlp: Language, name: str, snek_style: str):\n        self.nlp = nlp\n        self.snek_style = snek_style\n        self.snek = SNEKS[self.snek_style]\n\n    def __call__(self, doc):\n        print(self.snek)\n        return doc\n```\n\n----------------------------------------\n\nTITLE: Including Custom Functions in Pipeline Package\nDESCRIPTION: Shows how to package a spaCy pipeline with custom functions using the CLI's --code parameter. This ensures custom components are properly registered when the pipeline is loaded.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy package ./en_example_pipeline ./packages --code functions.py\n```\n\n----------------------------------------\n\nTITLE: Initializing English and German Language Models in spaCy\nDESCRIPTION: This code snippet demonstrates how to import and initialize English and German language models in spaCy. It shows the creation of language-specific NLP objects that include the respective language data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_language-data.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\nfrom spacy.lang.de import German\n\nnlp_en = English()  # Includes English data\nnlp_de = German()  # Includes German data\n```\n\n----------------------------------------\n\nTITLE: Setting Coreference Annotations in Python with spaCy\nDESCRIPTION: This snippet demonstrates how to add the experimental coreference resolver to the NLP pipeline, predict coreference clusters, and set annotations on documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncoref = nlp.add_pipe(\"experimental_coref\")\nclusters = coref.predict([doc1, doc2])\ncoref.set_annotations([doc1, doc2], clusters)\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy Pipeline Components in INI\nDESCRIPTION: This snippet demonstrates how to specify the components of a spaCy processing pipeline in a configuration file. It includes the tok2vec, tagger, parser, and named entity recognizer components.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_pipelines.mdx#2025-04-21_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\npipeline = [\"tok2vec\", \"tagger\", \"parser\", \"ner\"]\n```\n\n----------------------------------------\n\nTITLE: Merging Tokens in spaCy Using Doc.retokenize\nDESCRIPTION: Shows how to use spaCy's Doc.retokenize context manager to merge multiple tokens into a single token. The example merges \"New York\" into a single token and demonstrates how to set custom attributes on the merged token.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"I live in New York\")\nprint(\"Before:\", [token.text for token in doc])\n\nwith doc.retokenize() as retokenizer:\n    retokenizer.merge(doc[3:5], attrs={\"LEMMA\": \"new york\"})\nprint(\"After:\", [token.text for token in doc])\n```\n\n----------------------------------------\n\nTITLE: Performing Rehearsal Update on spaCy Language Models in Python\nDESCRIPTION: This code snippet shows how to perform a rehearsal update on spaCy Language models using the rehearse method. It helps address the catastrophic forgetting problem in model training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\noptimizer = nlp.resume_training()\nlosses = nlp.rehearse(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Basic Sentence Segmentation with spaCy's Dependency Parser\nDESCRIPTION: Shows how to access sentences from a Doc object using doc.sents. This example demonstrates spaCy's default approach to sentence segmentation using the dependency parser, which provides the most accurate sentence boundaries based on the full parse.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a sentence. This is another sentence.\")\nassert doc.has_annotation(\"SENT_START\")\nfor sent in doc.sents:\n    print(sent.text)\n```\n\n----------------------------------------\n\nTITLE: Converting IOB Tags to BILUO Tags in spaCy\nDESCRIPTION: Demonstrates how to convert a sequence of IOB tags to BILUO tags, which is useful for using IOB tags with models that only support BILUO format for named entity recognition.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import iob_to_biluo\n\ntags = [\"O\", \"O\", \"B-LOC\", \"I-LOC\", \"O\"]\nbiluo_tags = iob_to_biluo(tags)\nassert biluo_tags == [\"O\", \"O\", \"B-LOC\", \"L-LOC\", \"O\"]\n```\n\n----------------------------------------\n\nTITLE: Initializing spaCy Tagger Component\nDESCRIPTION: Initialize the tagger component with training examples and optional configuration. The initialization sets up the model and label scheme.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\ntagger.initialize(lambda: examples, nlp=nlp)\n```\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[initialize.components.tagger]\n\n[initialize.components.tagger.labels]\n@readers = \"spacy.read_labels.v1\"\npath = \"corpus/labels/tagger.json\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy Language Models\nDESCRIPTION: Shows different methods to load spaCy language models, including using the package name or a path to the data directory.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")           # load package \"en_core_web_sm\"\nnlp = spacy.load(\"/path/to/en_core_web_sm\")  # load package from a directory\n\ndoc = nlp(\"This is a sentence.\")\n```\n\n----------------------------------------\n\nTITLE: spaCy Pretrain Command Syntax\nDESCRIPTION: The full syntax for the pretrain command with all available options, including configuration path, output directory, code import path, resume options, and GPU settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy pretrain [config_path] [output_dir] [--code] [--resume-path] [--epoch-resume] [--gpu-id] [overrides]\n```\n\n----------------------------------------\n\nTITLE: Running Project Command\nDESCRIPTION: Command to execute a specific preprocessing task defined in project.yml.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project run preprocess\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dependencies with spaCy displaCy\nDESCRIPTION: Basic example of using spaCy's displaCy to visualize dependency parsing of a simple sentence. Loads the English language model and serves the visualization through a local web server.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a sentence.\")\ndisplacy.serve(doc, style=\"dep\")\n```\n\n----------------------------------------\n\nTITLE: Training spaCy Models with Custom Code\nDESCRIPTION: Command-line example showing how to train a spaCy model with custom code. It demonstrates using the --code parameter to include a Python file with custom functions during training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n### Training\n$ python -m spacy train config.cfg --code functions.py\n```\n\n----------------------------------------\n\nTITLE: Using Custom Extension Attributes in spaCy\nDESCRIPTION: Shows how to access custom extension attributes on Token, Doc, and Span objects after they have been registered.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I have an apple and a melon\")\nassert doc[3]._.is_fruit      # get Token attributes\nassert not doc[0]._.is_fruit\nassert doc._.has_fruit        # get Doc attributes\nassert doc[1:4]._.has_fruit   # get Span attributes\n```\n\n----------------------------------------\n\nTITLE: Visualizing Long Texts with spaCy displaCy\nDESCRIPTION: Advanced example demonstrating how to visualize longer texts by breaking them into sentences and visualizing each separately using displaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ntext = \"\"\"In ancient Rome, some neighbors live in three adjacent houses. In the center is the house of Senex, who lives there with wife Domina, son Hero, and several slaves, including head slave Hysterium and the musical's main character Pseudolus. A slave belonging to Hero, Pseudolus wishes to buy, win, or steal his freedom. One of the neighboring houses is owned by Marcus Lycus, who is a buyer and seller of beautiful women; the other belongs to the ancient Erronius, who is abroad searching for his long-lost children (stolen in infancy by pirates). One day, Senex and Domina go on a trip and leave Pseudolus in charge of Hero. Hero confides in Pseudolus that he is in love with the lovely Philia, one of the courtesans in the House of Lycus (albeit still a virgin).\"\"\"\ndoc = nlp(text)\nsentence_spans = list(doc.sents)\ndisplacy.serve(sentence_spans, style=\"dep\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom English Subclass in spaCy\nDESCRIPTION: This snippet demonstrates how to create a custom English language subclass in spaCy. It defines custom stop words and shows the difference in behavior between the standard English class and the custom subclass.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\n\nclass CustomEnglishDefaults(English.Defaults):\n    stop_words = set([\"custom\", \"stop\"])\n\nclass CustomEnglish(English):\n    lang = \"custom_en\"\n    Defaults = CustomEnglishDefaults\n\nnlp1 = English()\nnlp2 = CustomEnglish()\n\nprint(nlp1.lang, [token.is_stop for token in nlp1(\"custom stop\")])\nprint(nlp2.lang, [token.is_stop for token in nlp2(\"custom stop\")])\n```\n\n----------------------------------------\n\nTITLE: Using displaCy.serve for Visualization\nDESCRIPTION: Example demonstrating how to serve dependency parse visualizations in a browser using displaCy's serve function with multiple documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc1 = nlp(\"This is a sentence.\")\ndoc2 = nlp(\"This is another sentence.\")\ndisplacy.serve([doc1, doc2], style=\"dep\")\n```\n\n----------------------------------------\n\nTITLE: Using EntityRuler in spaCy Pipeline\nDESCRIPTION: Illustrates how to use the EntityRuler in a spaCy pipeline to add entities to a Doc object based on defined patterns.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"entity_ruler\")\nruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\n\ndoc = nlp(\"A text about Apple.\")\nents = [(ent.text, ent.label_) for ent in doc.ents]\nassert ents == [(\"Apple\", \"ORG\")]\n```\n\n----------------------------------------\n\nTITLE: Importing spaCy Language Models as Modules\nDESCRIPTION: Demonstrates how to import and load spaCy language models as Python modules.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport en_core_web_sm\n\nnlp = en_core_web_sm.load()\ndoc = nlp(\"This is a sentence.\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Saving EntityRuler Patterns\nDESCRIPTION: Demonstrates how to save and load EntityRuler patterns to/from disk using to_disk and from_disk methods.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nruler.to_disk(\"./patterns.jsonl\")\nnew_ruler = nlp.add_pipe(\"entity_ruler\").from_disk(\"./patterns.jsonl\")\n```\n\n----------------------------------------\n\nTITLE: Listing Installed spaCy Pipeline Models\nDESCRIPTION: Lists all pipeline packages installed in the current environment. This includes any spaCy pipeline packaged with 'spacy package'. Uses Python entry points without loading the nlp object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nnames = util.get_installed_models()\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy in Virtual Environment\nDESCRIPTION: Instructions for installing spaCy in a Python virtual environment for isolation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m venv .env\n$ source .env/bin/activate\n$ pip install -U pip setuptools wheel\n$ pip install -U %%SPACY_PKG_NAME%%SPACY_PKG_FLAGS\n```\n\n----------------------------------------\n\nTITLE: Creating Language Object from Config in Python\nDESCRIPTION: Demonstrates how to create a Language object from a loaded configuration file using the from_config classmethod.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom thinc.api import Config\nfrom spacy.language import Language\n\nconfig = Config().from_disk(\"./config.cfg\")\nnlp = Language.from_config(config)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Word Vectors for Semantic Similarity in spaCy\nDESCRIPTION: Example showing how to load a model with word vectors and compare the semantic similarity between two Latin sentences. This demonstrates the practical application of word vectors after they have been initialized.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nnlp_latin = spacy.load(\"/tmp/la_vectors_wiki_lg\")\ndoc1 = nlp_latin(\"Caecilius est in horto\")\ndoc2 = nlp_latin(\"servus est in atrio\")\ndoc1.similarity(doc2)\n```\n\n----------------------------------------\n\nTITLE: Pipeline Components Inspection\nDESCRIPTION: Example showing how to inspect the current pipeline components and their names\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(nlp.pipeline)\n# [('tok2vec', <spacy.pipeline.Tok2Vec>), ('tagger', <spacy.pipeline.Tagger>), ('parser', <spacy.pipeline.DependencyParser>), ('ner', <spacy.pipeline.EntityRecognizer>), ('attribute_ruler', <spacy.pipeline.AttributeRuler>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer>)]\nprint(nlp.pipe_names)\n# ['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n```\n\n----------------------------------------\n\nTITLE: Parsing Spans with displaCy in Python\nDESCRIPTION: Example showing how to parse and render spans using spaCy's displaCy module. Demonstrates loading a model, creating spans, and generating HTML visualization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"But Google is starting from behind.\")\ndoc.spans['orgs'] = [doc[1:2]]\nents_parse = displacy.parse_spans(doc, options={\"spans_key\" : \"orgs\"})\nhtml = displacy.render(ents_parse, style=\"span\", manual=True)\n```\n\n----------------------------------------\n\nTITLE: Calculating Semantic Similarity in spaCy\nDESCRIPTION: Shows how to calculate semantic similarity between two documents using cosine similarity of word vectors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\napples = nlp(\"I like apples\")\noranges = nlp(\"I like oranges\")\napples_oranges = apples.similarity(oranges)\noranges_apples = oranges.similarity(apples)\nassert apples_oranges == oranges_apples\n```\n\n----------------------------------------\n\nTITLE: Visualizing Named Entities with spaCy's displaCy in Python\nDESCRIPTION: This snippet demonstrates how to use spaCy to process text, identify named entities, and visualize them using the displaCy visualizer with the 'ent' style. It loads a pre-trained model, processes text to identify entities, and serves the visualization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\n\ntext = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(text)\ndisplacy.serve(doc, style=\"ent\")\n```\n\n----------------------------------------\n\nTITLE: Serializing Document Collections with DocBin\nDESCRIPTION: Example showing how to use the new DocBin class to efficiently serialize and deserialize collections of Doc objects, including specific attributes and user data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-2.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import DocBin\ndoc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"], store_user_data=True)\nfor doc in nlp.pipe(texts):\n    doc_bin.add(doc)\nbytes_data = doc_bin.to_bytes()\n# Deserialize later, e.g. in a new process\nnlp = spacy.blank(\"en\")\ndoc_bin = DocBin().from_bytes(bytes_data)\ndocs = list(doc_bin.get_docs(nlp.vocab))\n```\n\n----------------------------------------\n\nTITLE: Resuming Training of spaCy Language Pipeline in Python\nDESCRIPTION: This code snippet shows how to resume training of a spaCy Language pipeline using the resume_training method. It returns an optimizer and initializes rehearsal for pipeline components.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\noptimizer = nlp.resume_training()\nnlp.rehearse(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Using spacy-llm Pipeline for Text Classification in Python\nDESCRIPTION: This code snippet shows how to use a configured spacy-llm pipeline for text classification. It assembles the pipeline from a config file and processes a sample text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy_llm.util import assemble\n\n\nnlp = assemble(\"config.cfg\")\ndoc = nlp(\"You look gorgeous!\")\nprint(doc.cats)\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Pipeline Components in Python\nDESCRIPTION: Shows how to register a custom pipeline component using the @Language.component decorator or as a function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\n# Usage as a decorator\n@Language.component(\"my_component\")\ndef my_component(doc):\n   # Do something to the doc\n   return doc\n\n# Usage as a function\nLanguage.component(\"my_component2\", func=my_component)\n```\n\n----------------------------------------\n\nTITLE: Advanced Model Inspection with debug model Command\nDESCRIPTION: Examines specific layers (5 and 15) of a model, showing dimensions, parameters, and how values change during initialization and training. Demonstrates how the softmax layer's weight matrix updates during training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug model ./config.cfg tagger -l \"5,15\" -DIM -PAR -P0 -P1 -P2\n```\n\n----------------------------------------\n\nTITLE: Configuring DependencyParser in spaCy (Python)\nDESCRIPTION: Example of how to configure and add the DependencyParser to a spaCy pipeline. It demonstrates setting various parameters such as moves, update_with_oracle_cut_size, learn_tokens, min_action_freq, and the model architecture.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.dep_parser import DEFAULT_PARSER_MODEL\nconfig = {\n   \"moves\": None,\n   \"update_with_oracle_cut_size\": 100,\n   \"learn_tokens\": False,\n   \"min_action_freq\": 30,\n   \"model\": DEFAULT_PARSER_MODEL,\n}\nnlp.add_pipe(\"parser\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Running spaCy Pretrain Command\nDESCRIPTION: Command to pretrain a tok2vec component on raw text using a configuration file. This pretraining helps improve model performance, especially with limited labeled data, by learning vector representations of tokens.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy pretrain config.cfg ./output_pretrain --paths.raw_text ./data.jsonl\n```\n\n----------------------------------------\n\nTITLE: Updating spaCy Tagger Model\nDESCRIPTION: Train the tagger model using examples and an optimizer, returning loss metrics.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\noptimizer = nlp.initialize()\nlosses = tagger.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Pipeline Speed with spaCy CLI\nDESCRIPTION: Command to benchmark the processing speed of a trained spaCy pipeline with a 95% confidence interval. The pipeline is warmed up before measurements are taken and the results show words processed per second.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_35\n\nLANGUAGE: cli\nCODE:\n```\n$ python -m spacy benchmark speed [model] [data_path] [--code] [--batch_size] [--no-shuffle] [--gpu-id] [--batches] [--warmup]\n```\n\n----------------------------------------\n\nTITLE: Configuring Sequence-Based Batching Strategy in spaCy using INI Format\nDESCRIPTION: Example configuration for batch_by_sequence batcher that creates batches of a specified size. This simpler batching strategy is useful when dealing with fixed batch sizes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_29\n\nLANGUAGE: ini\nCODE:\n```\n[training.batcher]\n@batchers = \"spacy.batch_by_sequence.v1\"\nsize = 32\nget_length = null\n```\n\n----------------------------------------\n\nTITLE: Multi-Language Support Implementation\nDESCRIPTION: Code showing how to initialize multi-language support using both standard import and lazy-loading approaches.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Standard import\nfrom spacy.lang.xx import MultiLanguage\nnlp = MultiLanguage()\n\n# With lazy-loading\nnlp = spacy.blank(\"xx\")\n```\n\n----------------------------------------\n\nTITLE: Scoring TextCategorizer Examples in Python\nDESCRIPTION: This snippet demonstrates how to score a batch of examples using the TextCategorizer's score method. It returns a dictionary of scores produced by the Scorer.score_cats method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nscores = textcat.score(examples)\n```\n\n----------------------------------------\n\nTITLE: Integrating EntityRuler with spaCy's NER Component\nDESCRIPTION: Shows how EntityRuler can enhance spaCy's existing named entity recognizer by adding custom entities while respecting the model's predictions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\nruler = nlp.add_pipe(\"entity_ruler\")\npatterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\nruler.add_patterns(patterns)\n\ndoc = nlp(\"MyCorp Inc. is a company in the U.S.\")\nprint([(ent.text, ent.label_) for ent in doc.ents])\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy with CUDA and Transformer Support\nDESCRIPTION: Commands to install spaCy with CUDA 11.3 support and transformer capabilities. Includes setting CUDA_PATH environment variable.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport CUDA_PATH=\"/opt/nvidia/cuda\"\npip install -U %%SPACY_PKG_NAME[cuda113,transformers]%%SPACY_PKG_FLAGS\n```\n\n----------------------------------------\n\nTITLE: Using Sentencizer in Pipeline\nDESCRIPTION: Example demonstrating how to use the Sentencizer in a spaCy pipeline to detect sentence boundaries.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencizer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\n\nnlp = English()\nnlp.add_pipe(\"sentencizer\")\ndoc = nlp(\"This is a sentence. This is another sentence.\")\nassert len(list(doc.sents)) == 2\n```\n\n----------------------------------------\n\nTITLE: Extended Match Pattern API in Python\nDESCRIPTION: Demonstrates new token pattern matching capabilities including lemma lists, token length constraints, and regex matching on custom attributes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-1.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Matches \"love cats\" or \"likes flowers\"\npattern1 = [{\"LEMMA\": {\"IN\": [\"like\", \"love\"]}}, {\"POS\": \"NOUN\"}]\n# Matches tokens of length >= 10\npattern2 = [{\"LENGTH\": {\">=\": 10}}]\n# Matches custom attribute with regex\npattern3 = [{\"_\": {\"country\": {\"REGEX\": \"^([Uu](\\.?|nited) ?[Ss](\\.?|tates)\"}}}]\n```\n\n----------------------------------------\n\nTITLE: Using Getter Functions for Token Attributes in Python\nDESCRIPTION: Shows how to use getter functions instead of setting values explicitly when writing to Doc, Token, or Span objects in spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_42\n\nLANGUAGE: diff\nCODE:\n```\n+ is_fruit = lambda token: token.text in (\"apple\", \"orange\")\n+ Token.set_extension(\"is_fruit\", getter=is_fruit)\n\n- token._.set_extension(\"is_fruit\", default=False)\n- if token.text in (\"\\\"apple\\\"\", \"\\\"orange\\\"):\n-     token._.set(\"is_fruit\", True)\n```\n\n----------------------------------------\n\nTITLE: Managing Word Vectors\nDESCRIPTION: Example demonstrating how to manage word vectors, including setting individual vectors, loading GloVe vectors, and pruning vector tables.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor word, vector in vector_data:\n    nlp.vocab.set_vector(word, vector)\nnlp.vocab.vectors.from_glove(\"/path/to/vectors\")\n# Keep 10000 unique vectors and remap the rest\nnlp.vocab.prune_vectors(10000)\nnlp.to_disk(\"/model\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Transformer Hidden States\nDESCRIPTION: Example showing how to access the last hidden layer output for a specific token from transformer model outputs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Get the last hidden layer output for \"is\" (token index 1)\ndoc = nlp(\"This is a text.\")\nindices = doc._.trf_data.align[1].data.flatten()\nlast_hidden_state = doc._.trf_data.model_output.last_hidden_state\ndim = last_hidden_state.shape[-1]\ntensors = last_hidden_state.reshape(-1, dim)[indices]\n```\n\n----------------------------------------\n\nTITLE: Defining Token Patterns in JSON for spaCy Matcher\nDESCRIPTION: Example of a token pattern using JSON format. It demonstrates matching 'I like/love' followed by one or more nouns.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/matcher.mdx#2025-04-21_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\"LOWER\": \"i\"},\n  {\"LEMMA\": {\"IN\": [\"like\", \"love\"]}},\n  {\"POS\": \"NOUN\", \"OP\": \"+\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Basic Transformer Model Usage in spaCy\nDESCRIPTION: Example showing how to use transformer models in spaCy with GPU support, including loading a model and accessing transformer data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom thinc.api import set_gpu_allocator, require_gpu\n\nset_gpu_allocator(\"pytorch\")\nrequire_gpu(0)\n\nnlp = spacy.load(\"en_core_web_trf\")\nfor doc in nlp.pipe([\"some text\", \"some other text\"]):\n    tokvecs = doc._.trf_data.tensors[-1]\n```\n\n----------------------------------------\n\nTITLE: Initializing XLM-RoBERTa Transformer Model in Python\nDESCRIPTION: Constructs an XLM-RoBERTa transformer model with configurable parameters such as vocabulary size, dropout probabilities, hidden layer dimensions, and more. This model is designed for use in natural language processing tasks.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nspacy-curated-transformers.XlmrTransformer.v1\n```\n\n----------------------------------------\n\nTITLE: Combining Regex and Fuzzy Matching with Lists in spaCy\nDESCRIPTION: Shows how to combine REGEX and FUZZY matching with IN and NOT_IN attributes for more flexible token matching in spaCy v3.5+.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npattern = [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"awesome\", \"cool\", \"wonderful\"]}}}]\n\npattern = [{\"TEXT\": {\"REGEX\": {\"NOT_IN\": [\"^awe(some)?$\", \"^wonder(ful)?\"]}}}]\n```\n\n----------------------------------------\n\nTITLE: Defining Pipeline Components in spaCy Configuration\nDESCRIPTION: This snippet shows how to define a pipeline component (textcat) in a spaCy configuration file, including its factory and model architecture settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[components.textcat]\nfactory = \"textcat\"\n\n[components.textcat.model]\n@architectures = \"spacy.TextCatBOW.v2\"\nexclusive_classes = true\nngram_size = 1\nno_output_layer = false\n```\n\n----------------------------------------\n\nTITLE: Using Dropout and Normalization in Thinc Model\nDESCRIPTION: Shows how to incorporate dropout and layer normalization directly in Thinc layer definitions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nwith Model.define_operators({\">>\":chain}):\n    layers = (\n        Relu(hidden_width, dropout=dropout, normalize=False)\n        >> Relu(hidden_width, dropout=dropout, normalize=False)\n        >> Softmax()\n    )\n    model = char_embed >> with_array(layers)\n    model.initialize(X=input_sample, Y=output_sample)\n```\n\n----------------------------------------\n\nTITLE: Using Regular Expressions in Token Patterns with spaCy\nDESCRIPTION: Demonstrates how to use regular expressions (REGEX) in token patterns to match different spellings or custom attributes in spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npattern = [{\"TEXT\": {\"REGEX\": \"^[Uu](\\.?|nited)$\"}},\n           {\"TEXT\": {\"REGEX\": \"^[Ss](\\.?|tates)$\"}},\n           {\"LOWER\": \"president\"}]\n```\n\nLANGUAGE: python\nCODE:\n```\n# Match different spellings of token texts\npattern = [{\"TEXT\": {\"REGEX\": \"deff?in[ia]tely\"}}]\n\n# Match tokens with fine-grained POS tags starting with 'V'\npattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]\n\n# Match custom attribute values with regular expressions\npattern = [{\"_\": {\"country\": {\"REGEX\": \"^[Uu](nited|\\.?) ?[Ss](tates|\\.?)$\"}}}]\n```\n\n----------------------------------------\n\nTITLE: Pruning Word Vectors to Optimize Memory Usage in spaCy\nDESCRIPTION: This example shows how to reduce the number of vectors in a spaCy model to optimize memory usage while maintaining coverage. It loads a large vectors model, prunes it to a specified number of vectors, and demonstrates how to inspect the remapped words.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_lg\")\nn_vectors = 105000  # number of vectors to keep\nremoved_words = nlp.vocab.prune_vectors(n_vectors)\n\nassert len(nlp.vocab.vectors) <= n_vectors  # unique vectors have been pruned\nassert nlp.vocab.vectors.n_keys > n_vectors  # but not the total entries\n```\n\n----------------------------------------\n\nTITLE: Concatenating Multiple spaCy Doc Objects (Python)\nDESCRIPTION: Illustrates how to concatenate multiple spaCy Doc objects into a single Doc. This static method ensures that all Docs share the same Vocab and preserves sentence boundaries and named entities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc\ntexts = [\"London is the capital of the United Kingdom.\",\n         \"The River Thames flows through London.\",\n         \"The famous Tower Bridge crosses the River Thames.\"]\ndocs = list(nlp.pipe(texts))\nc_doc = Doc.from_docs(docs)\nassert str(c_doc) == \" \".join(texts)\nassert len(list(c_doc.sents)) == len(docs)\nassert [str(ent) for ent in c_doc.ents] == \\\n       [str(ent) for doc in docs for ent in doc.ents]\n```\n\n----------------------------------------\n\nTITLE: Using Dolly Model for Named Entity Recognition in Python\nDESCRIPTION: Python code to assemble and use a spaCy pipeline with the Dolly model for Named Entity Recognition. It loads the configuration, processes text, and prints the recognized entities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy_llm.util import assemble\n\nnlp = assemble(\"config.cfg\")\ndoc = nlp(\"Jack and Jill rode up the hill in Les Deux Alpes\")\nprint([(ent.text, ent.label_) for ent in doc.ents])\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Component with Type Hints in spaCy\nDESCRIPTION: Example of creating a custom pipeline component factory with type hints and pydantic validation in spaCy v3.0. The example demonstrates how to use Language.factory decorator with typed parameters, including strict type validation with pydantic's StrictBool.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\nfrom pydantic import StrictBool\n\n@Language.factory(\"my_component\")\ndef create_my_component(\n    nlp: Language,\n    name: str,\n    custom: StrictBool\n):\n   ...\n```\n\n----------------------------------------\n\nTITLE: Basic Model Configuration in INI Format\nDESCRIPTION: Configuration example showing how to specify a tagger component and its model architecture in spaCy's config file format.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[components.tagger]\nfactory = \"tagger\"\n\n[components.tagger.model]\n@architectures = \"model.v1\"\nwidth = 512\nclasses = 16\n```\n\n----------------------------------------\n\nTITLE: NER Component Configuration with Transformer\nDESCRIPTION: Configuration for NER component using TransformerListener architecture with mean pooling.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_11\n\nLANGUAGE: ini\nCODE:\n```\n[components.ner]\nfactory = \"ner\"\n\n[nlp.pipeline.ner.model]\n@architectures = \"spacy.TransitionBasedParser.v1\"\nstate_type = \"ner\"\nextra_state_tokens = false\nhidden_width = 128\nmaxout_pieces = 3\nuse_upper = false\n\n[nlp.pipeline.ner.model.tok2vec]\n@architectures = \"spacy-transformers.TransformerListener.v1\"\ngrad_factor = 1.0\n\n[nlp.pipeline.ner.model.tok2vec.pooling]\n@layers = \"reduce_mean.v1\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Spans with spaCy's displaCy\nDESCRIPTION: This example demonstrates how to use spaCy's span visualizer to highlight overlapping spans in text. It creates a blank English language model, defines spans with entity types 'ORG' and 'GPE', and serves the visualization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\nfrom spacy.tokens import Span\n\ntext = \"Welcome to the Bank of China.\"\n\nnlp = spacy.blank(\"en\")\ndoc = nlp(text)\n\ndoc.spans[\"sc\"] = [\n    Span(doc, 3, 6, \"ORG\"),\n    Span(doc, 5, 6, \"GPE\"),\n]\n\ndisplacy.serve(doc, style=\"span\")\n```\n\n----------------------------------------\n\nTITLE: Adding Labels to spaCy Tagger\nDESCRIPTION: Demonstrates how to add a custom label to a spaCy tagger component. The method raises an error if the output dimension is already set or if the model is initialized.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\ntagger.add_label(\"MY_LABEL\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Language Defaults in spaCy\nDESCRIPTION: Example showing how to customize default language settings by extending the Language.Defaults class. Includes configuration for tokenizer, stop words, lexical attributes, and writing system settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import language\nfrom spacy.lang.tokenizer_exceptions import URL_MATCH\nfrom thinc.api import Config\n\nDEFAULT_CONFIFG = \"\"\"\n[nlp.tokenizer]\n@tokenizers = \"MyCustomTokenizer.v1\"\n\"\"\"\n\nclass Defaults(Language.Defaults):\n   stop_words = set()\n   tokenizer_exceptions = {}\n   prefixes = tuple()\n   suffixes = tuple()\n   infixes = tuple()\n   token_match = None\n   url_match = URL_MATCH\n   lex_attr_getters = {}\n   syntax_iterators = {}\n   writing_system = {\"direction\": \"ltr\", \"has_case\": True, \"has_letters\": True}\n   config = Config().from_str(DEFAULT_CONFIG)\n```\n\n----------------------------------------\n\nTITLE: Implementing Factory Function for Class Components\nDESCRIPTION: Shows how to create a factory function for initializing class-based components, using the @Language.factory decorator to register the component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\n@Language.factory(\"my_component\")\ndef create_my_component(nlp, name):\n    return MyComponent(nlp)\n\nclass MyComponent:\n    def __init__(self, nlp):\n        self.nlp = nlp\n\n    def __call__(self, doc):\n        return doc\n```\n\n----------------------------------------\n\nTITLE: Implementing Modular Relation Model with Chain Architecture\nDESCRIPTION: Implements the relation model using a modular approach that chains two layers: an instance tensor generator and a classification layer. This architecture allows for flexibility in model configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@spacy.registry.architectures(\"rel_model.v1\")\ndef create_relation_model(\n    create_instance_tensor: Model[List[Doc], Floats2d],\n    classification_layer: Model[Floats2d, Floats2d],\n) -> Model[List[Doc], Floats2d]:\n    model = chain(create_instance_tensor, classification_layer)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Creating Entity Linking Training Data - Python\nDESCRIPTION: Demonstrates how to create training data for spaCy's entity linking component. Shows example of annotating a document with entity spans, candidate entity links and sentence boundaries\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Russ Cochran his reprints include EC Comics.\")\ngold_dict = {\"entities\": [(0, 12, \"PERSON\")],\n             \"links\": {(0, 12): {\"Q7381115\": 1.0, \"Q2146908\": 0.0}},\n             \"sent_starts\": [1, -1, -1, -1, -1, -1, -1, -1]}\nexample = Example.from_dict(doc, gold_dict)\n```\n\n----------------------------------------\n\nTITLE: Defining Command Dependencies and Outputs in spaCy project.yml\nDESCRIPTION: Configuration for a training command that specifies required dependencies (config and corpus files) and outputs (trained model). This enables intelligent re-running of commands based on changed dependencies.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\ncommands:\n  - name: train\n    help: 'Train a spaCy pipeline using the specified corpus and config'\n    script:\n      - 'python -m spacy train ./configs/config.cfg -o training/ --paths.train ./corpus/training.spacy --paths.dev ./corpus/evaluation.spacy'\n    deps:\n      - 'configs/config.cfg'\n      - 'corpus/training.spacy'\n      - 'corpus/evaluation.spacy'\n    outputs:\n      - 'training/model-best'\n```\n\n----------------------------------------\n\nTITLE: Processing a Stream of Documents with CuratedTransformer\nDESCRIPTION: Example of how to apply the CuratedTransformer component to a stream of documents using the pipe method. This method yields processed documents in order.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntrf = nlp.add_pipe(\"curated_transformer\")\nfor doc in trf.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Implementing Method Extensions in spaCy\nDESCRIPTION: Demonstrates how to create method extensions that become available as object methods. Method extensions are immutable and can accept arguments when called.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nDoc.set_extension(\"hello\", method=lambda doc, name: f\"Hi {name}!\")\nassert doc._.hello(\"Bob\") == \"Hi Bob!\"\n```\n\n----------------------------------------\n\nTITLE: Scoring Categories Example\nDESCRIPTION: Demonstrates how to evaluate document-level categories with multiple labels, calculating micro/macro PRF scores and AUC.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/scorer.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlabels = [\"LABEL_A\", \"LABEL_B\", \"LABEL_C\"]\nscores = Scorer.score_cats(\n    examples,\n    \"cats\",\n    labels=labels\n)\nprint(scores[\"cats_macro_auc\"])\n```\n\n----------------------------------------\n\nTITLE: Pickle Serialization of spaCy Objects\nDESCRIPTION: Demonstrates efficient pickle serialization of multiple Doc objects that share the same vocabulary. Shows how pickling objects together can significantly reduce data size compared to pickling separately.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc1 = nlp(\"Hello world\")\ndoc2 = nlp(\"This is a test\")\n\ndoc1_data = pickle.dumps(doc1)\ndoc2_data = pickle.dumps(doc2)\nprint(len(doc1_data) + len(doc2_data))  # 6636116 😞\n\ndoc_data = pickle.dumps([doc1, doc2])\nprint(len(doc_data))  # 3319761 😃\n```\n\n----------------------------------------\n\nTITLE: Configuring Doc Cleaner in spaCy Pipeline\nDESCRIPTION: Shows how to add the doc_cleaner component to a spaCy pipeline to clean up transformer tensors and other Doc attributes, helping prevent GPU memory exhaustion.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/memory-management.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnlp.add_pipe(\"doc_cleaner\", config={\"attrs\": {\"tensor\": None}})\n```\n\n----------------------------------------\n\nTITLE: Aligning Tokenization Between Different Systems in Python\nDESCRIPTION: Demonstrates how to use spaCy's Alignment class to map between different tokenization schemes. The example shows how to align tokens that are segmented differently (e.g., \"'s\" as one token vs. \"'\" and \"s\" as separate tokens) and how to interpret the alignment data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import Alignment\n\nother_tokens = [\"i\", \"listened\", \"to\", \"obama\", \"'\", \"s\", \"podcasts\", \".\"]\nspacy_tokens = [\"i\", \"listened\", \"to\", \"obama\", \"'s\", \"podcasts\", \".\"]\nalign = Alignment.from_strings(other_tokens, spacy_tokens)\nprint(f\"a -> b, lengths: {align.x2y.lengths}\")  # array([1, 1, 1, 1, 1, 1, 1, 1])\nprint(f\"a -> b, mapping: {align.x2y.data}\")  # array([0, 1, 2, 3, 4, 4, 5, 6]) : two tokens both refer to \"'s\"\nprint(f\"b -> a, lengths: {align.y2x.lengths}\")  # array([1, 1, 1, 1, 2, 1, 1])   : the token \"'s\" refers to two tokens\nprint(f\"b -> a, mappings: {align.y2x.data}\")  # array([0, 1, 2, 3, 4, 5, 6, 7])\n```\n\n----------------------------------------\n\nTITLE: Defining a training command in project.yml\nDESCRIPTION: Example of how to define a training command in project.yml, including the command script, dependencies, and expected outputs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_29\n\nLANGUAGE: yaml\nCODE:\n```\n- name: train\n  help: 'Train a spaCy pipeline using the specified corpus and config'\n  script:\n    - 'spacy train ./config.cfg --output training/'\n  deps:\n    - 'corpus/train'\n    - 'corpus/dev'\n    - 'config.cfg'\n  outputs:\n    - 'training/model-best'\n```\n\n----------------------------------------\n\nTITLE: Using spaCy Tagger Label Data for Initialization\nDESCRIPTION: Demonstrates how to access and use label data for initializing a tagger model with a pre-defined label set.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlabels = tagger.label_data\ntagger.initialize(lambda: [], nlp=nlp, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Managing Vocabulary and String Hashes in spaCy\nDESCRIPTION: Illustrates vocabulary management in spaCy, including creating new Doc objects with empty or shared vocabularies, and adding new strings to the vocabulary.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/spacy-101.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.tokens import Doc\nfrom spacy.vocab import Vocab\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"I love coffee\")  # Original Doc\nprint(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\nprint(doc.vocab.strings[3197928453018144401])  # 'coffee' 👍\n\nempty_doc = Doc(Vocab())  # New Doc with empty Vocab\n# empty_doc.vocab.strings[3197928453018144401] will raise an error :(\n\nempty_doc.vocab.strings.add(\"coffee\")  # Add \"coffee\" and generate hash\nprint(empty_doc.vocab.strings[3197928453018144401])  # 'coffee' 👍\n\nnew_doc = Doc(doc.vocab)  # Create new doc with first doc's vocab\nprint(new_doc.vocab.strings[3197928453018144401])  # 'coffee' 👍\n```\n\n----------------------------------------\n\nTITLE: Adding Pipeline Components in spaCy\nDESCRIPTION: Example of adding a lemmatizer component to a blank English language pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# pip install -U %%SPACY_PKG_NAME[lookups]%%SPACY_PKG_FLAGS\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\"lemmatizer\")\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanCat v3 in spaCy\nDESCRIPTION: Configuration example for the SpanCat v3 task component. Demonstrates basic setup with entity labels for person, organization, and location detection.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_33\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.SpanCat.v3\"\nlabels = [\"PERSON\", \"ORGANISATION\", \"LOCATION\"]\nexamples = null\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Language Class in Python\nDESCRIPTION: Defines a custom spaCy Language subclass with language-specific defaults like stop words. This allows creating new language support in spaCy without modifying core code.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\nclass SnekDefaults(Language.Defaults):\n    stop_words = set([\"sss\", \"hiss\"])\n\nclass SnekLanguage(Language):\n    lang = \"snk\"\n    Defaults = SnekDefaults\n```\n\n----------------------------------------\n\nTITLE: Retokenizing a spaCy Doc in Python\nDESCRIPTION: Shows how to use the retokenize context manager to merge tokens in a spaCy Doc.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Hello world!\")\nwith doc.retokenize() as retokenizer:\n    retokenizer.merge(doc[0:2])\n```\n\n----------------------------------------\n\nTITLE: Implementing Fuzzy Matching in spaCy Token Patterns\nDESCRIPTION: Demonstrates how to use fuzzy matching in token patterns to match alternate spellings, typos, and similar variations of words in spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Matches \"favourite\", \"favorites\", \"gavorite\", \"theatre\", \"theatr\", ...\npattern = [{\"TEXT\": {\"FUZZY\": \"favorite\"}},\n           {\"TEXT\": {\"FUZZY\": \"theater\"}}]\n```\n\nLANGUAGE: python\nCODE:\n```\n# Match lowercase with fuzzy matching (allows 3 edits)\npattern = [{\"LOWER\": {\"FUZZY\": \"definitely\"}}]\n\n# Match custom attribute values with fuzzy matching (allows 3 edits)\npattern = [{\"_\": {\"country\": {\"FUZZY\": \"Kyrgyzstan\"}}}]\n\n# Match with exact Levenshtein edit distance limits (allows 4 edits)\npattern = [{\"_\": {\"country\": {\"FUZZY4\": \"Kyrgyzstan\"}}}]\n```\n\n----------------------------------------\n\nTITLE: Printing spaCy Installation Information\nDESCRIPTION: Command to print information about the spaCy installation, trained pipelines, and local setup. It can generate Markdown-formatted output for GitHub issues and allows excluding specific information.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy info [--markdown] [--silent] [--exclude]\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy info [model] [--markdown] [--silent] [--exclude]\n```\n\n----------------------------------------\n\nTITLE: Implementing Last Transformer Layer Listener in Python\nDESCRIPTION: Constructs a listener layer similar to TransformerLayersListener, but specifically for the last transformer layer. It extracts the output and performs pooling over individual pieces of each Doc token, with configurable width, pooling method, and upstream component name.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nspacy-curated-transformers.LastTransformerLayerListener.v1\n```\n\n----------------------------------------\n\nTITLE: Pipeline Processing Flow\nDESCRIPTION: Example showing how the pipeline processes text by creating a Doc object and applying each component sequentially\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp.make_doc(\"This is a sentence\")  # Create a Doc from raw text\nfor name, proc in nlp.pipeline:           # Iterate over components in order\n    doc = proc(doc)                       # Apply each component\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanRuler Pipeline Component in spaCy\nDESCRIPTION: Example configuration for adding the span_ruler pipeline component with custom settings for spans_key, validation, and overwrite behavior.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n   \"spans_key\": \"my_spans\",\n   \"validate\": True,\n   \"overwrite\": False,\n}\nnlp.add_pipe(\"span_ruler\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Creating Scalar Weighting Listener for Transformer Layers in Python\nDESCRIPTION: Builds a listener layer that calculates a weighted representation of all transformer layer outputs and performs pooling. It requires upstream Transformer components to return all layer outputs and allows configuration of width, weighting model, pooling method, and upstream component name.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nspacy-curated-transformers.ScalarWeightingListener.v1\n```\n\n----------------------------------------\n\nTITLE: Initializing EntityRuler in SpaCy\nDESCRIPTION: Examples demonstrating two ways to initialize an EntityRuler: via nlp.add_pipe() or by directly instantiating the EntityRuler class. The second approach shows how to create an EntityRuler with overwrite_ents set to True.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe\nruler = nlp.add_pipe(\"entity_ruler\")\n\n# Construction from class\nfrom spacy.pipeline import EntityRuler\nruler = EntityRuler(nlp, overwrite_ents=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Transformer Layers Listener in Python\nDESCRIPTION: Creates a listener layer that communicates with upstream Transformer components, extracting the output of the last transformer layer and performing pooling over individual pieces of each Doc token. It allows specification of layers, width, pooling method, and upstream component name.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nspacy-curated-transformers.TransformerLayersListener.v1\n```\n\n----------------------------------------\n\nTITLE: Setting Annotations with TrainablePipe Component in Python\nDESCRIPTION: Demonstrates how to use the set_annotations method of a TrainablePipe component to modify documents using pre-computed scores.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\nscores = pipe.predict(docs)\npipe.set_annotations(docs, scores)\n```\n\n----------------------------------------\n\nTITLE: Removing Rules from spaCy's Default Tokenizer\nDESCRIPTION: This example demonstrates how to remove specific characters from the default suffix rules. It shows how to convert the default suffixes to a list, remove a specific character, and recompile the modified rule set.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nsuffixes = list(nlp.Defaults.suffixes)\nsuffixes.remove(\"\\\\\\\\[\")\nsuffix_regex = spacy.util.compile_suffix_regex(suffixes)\nnlp.tokenizer.suffix_search = suffix_regex.search\n```\n\n----------------------------------------\n\nTITLE: Loading EntityRuler from Disk\nDESCRIPTION: Illustrates how to load EntityRuler patterns from disk, either from a JSONL file or a directory containing patterns and config.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"entity_ruler\")\nruler.from_disk(\"/path/to/patterns.jsonl\")  # loads patterns only\nruler.from_disk(\"/path/to/entity_ruler\")    # loads patterns and config\n```\n\n----------------------------------------\n\nTITLE: Building Custom Model Architecture in Python\nDESCRIPTION: Example showing how to build a custom model architecture using Thinc's Model class and the spaCy registry. The function builds a model by chaining tok2vec and output layers.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom thinc.api import Model, chain\n\n@spacy.registry.architectures(\"model.v1\")\ndef build_model(width: int, classes: int) -> Model:\n    tok2vec = build_tok2vec(width)\n    output_layer = build_output_layer(width, classes)\n    model = chain(tok2vec, output_layer)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Initializing EntityLinker in spaCy Python\nDESCRIPTION: Different ways to construct and add an EntityLinker component to a spaCy pipeline, including via add_pipe with default or custom model configurations, or direct initialization from the class.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\nentity_linker = nlp.add_pipe(\"entity_linker\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_el.v1\"}}\nentity_linker = nlp.add_pipe(\"entity_linker\", config=config)\n\n# Construction from class\nfrom spacy.pipeline import EntityLinker\nentity_linker = EntityLinker(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Modifying Pipeline Components in spaCy\nDESCRIPTION: Examples of removing, renaming, and replacing pipeline components in an existing spaCy pipeline using methods like remove_pipe, rename_pipe, and replace_pipe.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nnlp.remove_pipe(\"parser\")\nnlp.rename_pipe(\"ner\", \"entityrecognizer\")\nnlp.replace_pipe(\"tagger\", \"my_custom_tagger\")\n```\n\n----------------------------------------\n\nTITLE: Transformer Component Configuration\nDESCRIPTION: Configuration example for the transformer component in spaCy's config.cfg file, showing component settings and model architecture.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_7\n\nLANGUAGE: ini\nCODE:\n```\n[components.transformer]\nfactory = \"transformer\"\nmax_batch_items = 4096\n\n[components.transformer.model]\n@architectures = \"spacy-transformers.TransformerModel.v3\"\nname = \"bert-base-cased\"\ntokenizer_config = {\"use_fast\": true}\n\n[components.transformer.model.get_spans]\n@span_getters = \"spacy-transformers.doc_spans.v1\"\n\n[components.transformer.set_extra_annotations]\n@annotation_setters = \"spacy-transformers.null_annotation_setter.v1\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Entry Points in setup.py\nDESCRIPTION: Demonstrates how to expose a custom spaCy component as an entry point in setup.py. This allows spaCy to discover and use the component without explicit imports.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools import setup\n\nsetup(\n    name=\"snek\",\n    entry_points={\n        \"spacy_factories\": [\"snek = snek:snek_component\"]\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Text Categorizer Configuration with Ensemble Architecture\nDESCRIPTION: Complete configuration example for a TextCategorizer component using the TextCatEnsemble architecture with tok2vec and linear model specifications.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[components.textcat]\nfactory = \"textcat\"\nlabels = []\n\n[components.textcat.model]\n@architectures = \"spacy.TextCatEnsemble.v2\"\nnO = null\n\n[components.textcat.model.tok2vec]\n@architectures = \"spacy.Tok2Vec.v2\"\n\n[components.textcat.model.tok2vec.embed]\n@architectures = \"spacy.MultiHashEmbed.v2\"\nwidth = 64\nrows = [2000, 2000, 1000, 1000, 1000, 1000]\nattrs = [\"ORTH\", \"LOWER\", \"PREFIX\", \"SUFFIX\", \"SHAPE\", \"ID\"]\ninclude_static_vectors = false\n\n[components.textcat.model.tok2vec.encode]\n@architectures = \"spacy.MaxoutWindowEncoder.v2\"\nwidth = ${components.textcat.model.tok2vec.embed.width}\nwindow_size = 1\nmaxout_pieces = 3\ndepth = 2\n\n[components.textcat.model.linear_model]\n@architectures = \"spacy.TextCatBOW.v3\"\nexclusive_classes = true\nlength = 262144\nngram_size = 1\nno_output_layer = false\n```\n\n----------------------------------------\n\nTITLE: Aligning Named Entity Recognition in spaCy (Python)\nDESCRIPTION: Shows how to get aligned BILUO tags for named entity recognition between documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwords = [\"Mrs\", \"Smith\", \"flew\", \"to\", \"New York\"]\ndoc = Doc(en_vocab, words=words)\nentities = [(0, 9, \"PERSON\"), (18, 26, \"LOC\")]\ngold_words = [\"Mrs Smith\", \"flew\", \"to\", \"New\", \"York\"]\nexample = Example.from_dict(doc, {\"words\": gold_words, \"entities\": entities})\nner_tags = example.get_aligned_ner()\nassert ner_tags == [\"B-PERSON\", \"L-PERSON\", \"O\", \"O\", \"U-LOC\"]\n```\n\n----------------------------------------\n\nTITLE: Updating CuratedTransformer Model\nDESCRIPTION: Update the transformer model during training using examples and an optimizer. Handles backpropagation for connected downstream components.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"curated_transformer\")\noptimizer = nlp.initialize()\nlosses = trf.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Span Groups to a spaCy Doc in Python\nDESCRIPTION: Demonstrates how to add custom span groups to a spaCy Doc using the spans property.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Their goi ng home\")\ndoc.spans[\"errors\"] = [doc[0:1], doc[1:3]]\n```\n\n----------------------------------------\n\nTITLE: Loading TextCategorizer from Disk in Python\nDESCRIPTION: This code shows how to load a TextCategorizer from disk. It modifies the object in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\ntextcat.from_disk(\"/path/to/textcat\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Pattern with Linguistic Annotations in spaCy\nDESCRIPTION: This pattern matches 'Facebook' followed by a form of 'be', an optional adverb, and an adjective. It uses linguistic annotations like LOWER for case-insensitive matching, LEMMA for word forms, and POS for part-of-speech tags.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n[{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"ADJ\"}]\n```\n\n----------------------------------------\n\nTITLE: Initializing EntityRecognizer Component\nDESCRIPTION: Examples demonstrating different ways to initialize the EntityRecognizer component, including via add_pipe with default or custom models, and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\nner = nlp.add_pipe(\"ner\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_ner\"}}\nparser = nlp.add_pipe(\"ner\", config=config)\n\n# Construction from class\nfrom spacy.pipeline import EntityRecognizer\nner = EntityRecognizer(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Training Logger in Python\nDESCRIPTION: Creates a custom logger that writes training results to a tab-separated file. The logger tracks steps, scores, and losses for each pipeline component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_29\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg (excerpt)\n[training.logger]\n@loggers = \"my_custom_logger.v1\"\nlog_path = \"my_file.tab\"\n```\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nfrom typing import IO, Tuple, Callable, Dict, Any, Optional\nimport spacy\nfrom spacy import Language\nfrom pathlib import Path\n\n@spacy.registry.loggers(\"my_custom_logger.v1\")\ndef custom_logger(log_path):\n    def setup_logger(\n        nlp: Language,\n        stdout: IO=sys.stdout,\n        stderr: IO=sys.stderr\n    ) -> Tuple[Callable, Callable]:\n        stdout.write(f\"Logging to {log_path}\\n\")\n        log_file = Path(log_path).open(\"w\", encoding=\"utf8\")\n        log_file.write(\"step\\t\")\n        log_file.write(\"score\\t\")\n        for pipe in nlp.pipe_names:\n            log_file.write(f\"loss_{pipe}\\t\")\n        log_file.write(\"\\n\")\n\n        def log_step(info: Optional[Dict[str, Any]]):\n            if info:\n                log_file.write(f\"{info['step']}\\t\")\n                log_file.write(f\"{info['score']}\\t\")\n                for pipe in nlp.pipe_names:\n                    log_file.write(f\"{info['losses'][pipe]}\\t\")\n                log_file.write(\"\\n\")\n\n        def finalize():\n            log_file.close()\n\n        return log_step, finalize\n\n    return setup_logger\n```\n\n----------------------------------------\n\nTITLE: Splitting Tokens in spaCy Using Doc.retokenize\nDESCRIPTION: Demonstrates how to split a token into multiple subtokens using spaCy's retokenizer. The example splits \"NewYork\" into \"New\" and \"York\", showing how to specify attachment heads and token attributes for the resulting subtokens.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"I live in NewYork\")\nprint(\"Before:\", [token.text for token in doc])\ndisplacy.render(doc)  # displacy.serve if you're not in a Jupyter environment\n\nwith doc.retokenize() as retokenizer:\n    heads = [(doc[3], 1), doc[2]]\n    attrs = {\"POS\": [\"PROPN\", \"PROPN\"], \"DEP\": [\"pobj\", \"compound\"]}\n    retokenizer.split(doc[3], [\"New\", \"York\"], heads=heads, attrs=attrs)\nprint(\"After:\", [token.text for token in doc])\ndisplacy.render(doc)  # displacy.serve if you're not in a Jupyter environment\n```\n\n----------------------------------------\n\nTITLE: Initializing a Custom TrainablePipe Component in Python\nDESCRIPTION: Example of creating a custom pipeline component that inherits from TrainablePipe and registering it as a factory method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline import TrainablePipe\nfrom spacy.language import Language\n\nclass CustomPipe(TrainablePipe):\n    ...\n\n@Language.factory(\"your_custom_pipe\", default_config={\"model\": MODEL})\ndef make_custom_pipe(nlp, name, model):\n    return CustomPipe(nlp.vocab, model, name)\n```\n\n----------------------------------------\n\nTITLE: Processing a Stream of Documents with TrainablePipe in Python\nDESCRIPTION: Shows how to apply a TrainablePipe component to a stream of documents, which is typically done automatically in the spaCy pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\nfor doc in pipe.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Initializing SpanFinder Component in Python\nDESCRIPTION: Examples demonstrating different ways to construct the SpanFinder component, including via add_pipe with default/custom models and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\nspan_finder = nlp.add_pipe(\"span_finder\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_span_finder\"}}\nspan_finder = nlp.add_pipe(\"span_finder\", config=config)\n\n# Construction from class\nfrom spacy.pipeline import SpanFinder\nspan_finder = SpanFinder(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Running spaCy debug-data CLI Command\nDESCRIPTION: This command runs the spaCy debug-data tool on English language training and development data files. It analyzes the data for potential issues and provides detailed statistics.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-2.mdx#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug-data en train.json dev.json\n```\n\n----------------------------------------\n\nTITLE: Initializing DependencyParser in spaCy (Python)\nDESCRIPTION: Shows different ways to construct a DependencyParser component, including via add_pipe with default/custom models and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\nparser = nlp.add_pipe(\"parser\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_parser\"}}\nparser = nlp.add_pipe(\"parser\", config=config)\n\n# Construction from class\nfrom spacy.pipeline import DependencyParser\nparser = DependencyParser(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Hashtag Labeling with spaCy Custom Token Attributes\nDESCRIPTION: This snippet shows how to label hashtags using custom token attributes in spaCy. It uses the Matcher to identify hashtags, merges them into single tokens, and sets a custom attribute to mark them as hashtags.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy.tokens import Token\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab)\n\n# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\nmatcher.add(\"HASHTAG\", [[{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}]])\n\n# Register token extension\nToken.set_extension(\"is_hashtag\", default=False)\n\ndoc = nlp(\"Hello world 😀 #MondayMotivation\")\nmatches = matcher(doc)\nhashtags = []\nfor match_id, start, end in matches:\n    if doc.vocab.strings[match_id] == \"HASHTAG\":\n        hashtags.append(doc[start:end])\nwith doc.retokenize() as retokenizer:\n    for span in hashtags:\n        retokenizer.merge(span)\n        for token in span:\n            token._.is_hashtag = True\n\nfor token in doc:\n    print(token.text, token._.is_hashtag)\n```\n\n----------------------------------------\n\nTITLE: Using PhraseMatcher to Find Token Sequences\nDESCRIPTION: Demonstrates how to add patterns and find matches in a document using the PhraseMatcher. The example shows matching 'Barack Obama' in text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/phrasematcher.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.matcher import PhraseMatcher\n\nmatcher = PhraseMatcher(nlp.vocab)\nmatcher.add(\"OBAMA\", [nlp(\"Barack Obama\")])\ndoc = nlp(\"Barack Obama lifts America one last time in emotional farewell\")\nmatches = matcher(doc)\n```\n\n----------------------------------------\n\nTITLE: Replacing Token-to-Vector Listeners in spaCy Pipeline (Python)\nDESCRIPTION: Example showing how to replace listeners of a token-to-vector component for a specific pipeline component using the replace_listeners method. This helps prevent performance degradation when some components are frozen during training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n### Example\nnlp = spacy.load(\"en_core_web_sm\")\nnlp.replace_listeners(\"tok2vec\", \"tagger\", [\"model.tok2vec\"])\n```\n\n----------------------------------------\n\nTITLE: Initializing Transformer Component for Training in Python\nDESCRIPTION: This example shows how to initialize the Transformer component for training. It uses a function that returns example objects to set up the model and label scheme.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"transformer\")\ntrf.initialize(lambda: examples, nlp=nlp)\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCategorizer for Multi-Label Classification in Python\nDESCRIPTION: Example of configuring the TextCategorizer for multi-label classification with a custom threshold and default model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL\nconfig = {\n   \"threshold\": 0.5,\n   \"model\": DEFAULT_MULTI_TEXTCAT_MODEL,\n}\nnlp.add_pipe(\"textcat_multilabel\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Exporting Token Attributes to NumPy Array in spaCy Doc (Python)\nDESCRIPTION: Demonstrates how to export specified token attributes from a spaCy Doc to a NumPy array. The method can handle single or multiple attributes and returns a 1D or 2D array accordingly.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\ndoc = nlp(text)\n# All strings mapped to integers, for easy export to numpy\nnp_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\nnp_array = doc.to_array(\"POS\")\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy Candidate Selector\nDESCRIPTION: Configuration example for the spacy.CandidateSelector.v1 component showing initialization and KB loader setup.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_20\n\nLANGUAGE: ini\nCODE:\n```\n[initialize]\n[initialize.components]\n[initialize.components.llm]\n[initialize.components.llm.candidate_selector]\n@llm_misc = \"spacy.CandidateSelector.v1\"\n\n# Load a KB from a KB file. For loading KBs from spaCy pipelines see spacy.KBObjectLoader.v1.\n[initialize.components.llm.candidate_selector.kb_loader]\n@llm_misc = \"spacy.KBFileLoader.v1\"\n# Path to knowledge base .yaml file.\npath = ${paths.el_kb}\n```\n\n----------------------------------------\n\nTITLE: Using a Custom Component in spaCy\nDESCRIPTION: Example code showing how to add and use a custom component that was registered via entry points. The component can be added by name without importing it directly.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> from spacy.lang.en import English\n>>> nlp = English()\n>>> nlp.add_pipe(\"snek\")  # this now works! 🐍🎉\n>>> doc = nlp(\"I am snek\")\n    --..,_                     _,.--.\\n       `'.'.                .'`__ o  `;__. I am snek\\n          '.'.            .'.'`  '---'`  `\\n            '.`'--....--'`.'\\n              `'--....--'`\n```\n\n----------------------------------------\n\nTITLE: Configuring Sentiment Analysis Task in spaCy LLM\nDESCRIPTION: Example configuration for the spaCy Sentiment task that performs sentiment analysis on text using LLMs without few-shot examples.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_49\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.Sentiment.v1\"\nexamples = null\n```\n\n----------------------------------------\n\nTITLE: Using EntityRuler for Basic Entity Recognition\nDESCRIPTION: Demonstrates how to use EntityRuler to add named entities based on pattern dictionaries, combining both exact string matches and token patterns in a single ruleset.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\n\nnlp = English()\nruler = nlp.add_pipe(\"entity_ruler\")\npatterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\nruler.add_patterns(patterns)\n\ndoc = nlp(\"Apple is opening its first big office in San Francisco.\")\nprint([(ent.text, ent.label_) for ent in doc.ents])\n```\n\n----------------------------------------\n\nTITLE: Custom Transformer Annotation Setting\nDESCRIPTION: Example demonstrating how to customize transformer annotations by implementing a custom annotation setter function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef custom_annotation_setter(docs, trf_data):\n    doc_data = list(trf_data.doc_data)\n    for doc, data in zip(docs, doc_data):\n        doc._.custom_attr = data\n\nnlp = spacy.load(\"en_core_web_trf\")\nnlp.get_pipe(\"transformer\").set_extra_annotations = custom_annotation_setter\ndoc = nlp(\"This is a text\")\nassert isinstance(doc._.custom_attr, TransformerData)\nprint(doc._.custom_attr.tensors)\n```\n\n----------------------------------------\n\nTITLE: Configuring Tok2VecTransformer in spaCy\nDESCRIPTION: Example configuration for the Tok2VecTransformer architecture in spaCy. It specifies the model name, tokenizer settings, and other parameters for using a transformer as a Tok2Vec layer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_12\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy-transformers.Tok2VecTransformer.v3\"\nname = \"albert-base-v2\"\ntokenizer_config = {\"use_fast\": false}\ntransformer_config = {}\ngrad_factor = 1.0\nmixed_precision = true\ngrad_scaler_config = {\"init_scale\": 32768}\n```\n\n----------------------------------------\n\nTITLE: Predicting Named Entities in spaCy\nDESCRIPTION: Demonstrates how to apply the NER model to documents for prediction without modifying them. Takes multiple Doc objects as input and returns prediction scores.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\nscores = ner.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Noun Chunks in spaCy Doc Object (Python)\nDESCRIPTION: Demonstrates how to iterate over base noun phrases (noun chunks) in a document using the noun_chunks property. The example extracts and validates noun chunks from a sample sentence.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"A phrase with another phrase occurs.\")\nchunks = list(doc.noun_chunks)\nassert len(chunks) == 2\nassert chunks[0].text == \"A phrase\"\nassert chunks[1].text == \"another phrase\"\n```\n\n----------------------------------------\n\nTITLE: Configuring and Adding AttributeRuler to spaCy Pipeline\nDESCRIPTION: Example showing how to configure the AttributeRuler component with validation enabled and add it to a spaCy pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributeruler.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\"validate\": True}\nnlp.add_pipe(\"attribute_ruler\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Adding SpanResolver to Pipeline\nDESCRIPTION: Example showing how to configure and add the SpanResolver component to a spaCy pipeline. This demonstrates setting up the model and configuring input/output prefixes for the span groups.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy_experimental.coref.span_resolver_component import DEFAULT_SPAN_RESOLVER_MODEL\nfrom spacy_experimental.coref.coref_util import DEFAULT_CLUSTER_PREFIX, DEFAULT_CLUSTER_HEAD_PREFIX\nconfig={\n    \"model\": DEFAULT_SPAN_RESOLVER_MODEL,\n    \"input_prefix\": DEFAULT_CLUSTER_HEAD_PREFIX,\n    \"output_prefix\": DEFAULT_CLUSTER_PREFIX,\n},\nnlp.add_pipe(\"experimental_span_resolver\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Phone Number Matcher in spaCy\nDESCRIPTION: This example demonstrates how to use the Matcher to find phone numbers in text. It defines a pattern for numbers in the format (123) 456 789, applies it to a document, and extracts the matched spans.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab)\npattern = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddd\"},\n           {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"ddd\"}]\nmatcher.add(\"PHONE_NUMBER\", [pattern])\n\ndoc = nlp(\"Call me at (123) 456 789 or (123) 456 789!\")\nprint([t.text for t in doc])\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(span.text)\n```\n\n----------------------------------------\n\nTITLE: Detecting Hashtags with spaCy Matcher\nDESCRIPTION: This snippet demonstrates how to use spaCy's Matcher to detect hashtags in text. It adds a pattern for valid hashtags and prints the matched spans.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\nmatcher.add(\"HASHTAG\", [[{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}]])\n\ndoc = nlp(\"Hello world 😀 #MondayMotivation\")\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    string_id = doc.vocab.strings[match_id]  # Look up string ID\n    span = doc[start:end]\n    print(string_id, span.text)\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for Tok2Vec Component in Python\nDESCRIPTION: Demonstrates how to add a Tok2Vec component to the NLP pipeline and create an optimizer for it. This is typically used for training or fine-tuning the component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntok2vec = nlp.add_pipe(\"tok2vec\")\noptimizer = tok2vec.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Configuring Transformer Component in spaCy\nDESCRIPTION: Example showing how to configure and add the transformer component to the spaCy pipeline with custom settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy_transformers import Transformer\nfrom spacy_transformers.pipeline_component import DEFAULT_CONFIG\n\nnlp.add_pipe(\"transformer\", config=DEFAULT_CONFIG[\"transformer\"])\n```\n\n----------------------------------------\n\nTITLE: Adding Label to TextCategorizer in Python\nDESCRIPTION: This code shows how to add a new label to the TextCategorizer. It's important to note that this should be done before the model is fully initialized, and it's not necessary if a representative data sample is provided to the initialize method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\ntextcat.add_label(\"MY_LABEL\")\n```\n\n----------------------------------------\n\nTITLE: Processing Document Stream with SpanCategorizer in spaCy (Python)\nDESCRIPTION: Demonstrates how to apply the SpanCategorizer to a stream of documents using the pipe method, which is typically used for batch processing.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\nfor doc in spancat.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Converting Word Vectors for spaCy Usage\nDESCRIPTION: This command converts word vectors for use with spaCy. It exports an 'nlp' object that can be used to initialize a model with vectors. The command supports various options for pruning, truncating, and specifying vector modes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init vectors [lang] [vectors_loc] [output_dir] [--prune] [--truncate] [--name] [--verbose]\n```\n\n----------------------------------------\n\nTITLE: Retrieving word vectors from spaCy's Vocab\nDESCRIPTION: Demonstrates how to get a vector for a specific word in the vocabulary. This is crucial for various NLP tasks that rely on word embeddings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nnlp.vocab.get_vector(\"apple\")\n```\n\n----------------------------------------\n\nTITLE: Configuring TransitionBasedParser in spaCy\nDESCRIPTION: Example configuration for spaCy's TransitionBasedParser.v2 architecture, which can be used for NER or dependency parsing. The configuration defines model parameters including state type, hidden layer width, and token vectorization settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_22\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.TransitionBasedParser.v2\"\nstate_type = \"ner\"\nextra_state_tokens = false\nhidden_width = 64\nmaxout_pieces = 2\nuse_upper = true\n\n[model.tok2vec]\n@architectures = \"spacy.HashEmbedCNN.v2\"\npretrained_vectors = null\nwidth = 96\ndepth = 4\nembed_size = 2000\nwindow_size = 1\nmaxout_pieces = 3\nsubword_features = true\n```\n\n----------------------------------------\n\nTITLE: Using Custom Component in spaCy Pipeline\nDESCRIPTION: Example showing how to add the custom component to a spaCy pipeline, add data to it, and serialize the pipeline to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_sm\")\nmy_component = nlp.add_pipe(\"my_component\")\nmy_component.add({\"hello\": \"world\"})\nnlp.to_disk(\"/path/to/pipeline\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Attribute Extensions in spaCy\nDESCRIPTION: Demonstrates how to set and use attribute extensions on Doc objects. Attribute extensions allow setting default values that can be overwritten, providing a simple way to store arbitrary information.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nDoc.set_extension(\"hello\", default=True)\nassert doc._.hello\ndoc._.hello = False\n```\n\n----------------------------------------\n\nTITLE: Analyzing Sentiment with Emoji in Social Media Using spaCy\nDESCRIPTION: This example shows how to match emoji tokens in social media posts and assign sentiment scores based on positive or negative emoji. It uses a callback function to modify the document's sentiment attribute when emoji are found.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\nfrom spacy.matcher import Matcher\n\nnlp = English()  # We only want the tokenizer, so no need to load a pipeline\nmatcher = Matcher(nlp.vocab)\n\npos_emoji = [\"😀\", \"😃\", \"😂\", \"🤣\", \"😊\", \"😍\"]  # Positive emoji\nneg_emoji = [\"😞\", \"😠\", \"😩\", \"😢\", \"😭\", \"😒\"]  # Negative emoji\n\n# Add patterns to match one or more emoji tokens\npos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\nneg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n\n# Function to label the sentiment\ndef label_sentiment(matcher, doc, i, matches):\n    match_id, start, end = matches[i]\n    if doc.vocab.strings[match_id] == \"HAPPY\":  # Don't forget to get string!\n        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\n    elif doc.vocab.strings[match_id] == \"SAD\":\n        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\n\nmatcher.add(\"HAPPY\", pos_patterns, on_match=label_sentiment)  # Add positive pattern\nmatcher.add(\"SAD\", neg_patterns, on_match=label_sentiment)  # Add negative pattern\n```\n\n----------------------------------------\n\nTITLE: Scoring Entity Links Example\nDESCRIPTION: Shows how to evaluate named entity linking performance by calculating PRF scores for predicted links on overlapping entities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/scorer.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nscores = Scorer.score_links(\n    examples,\n    negative_labels=[\"NIL\", \"\"]\n)\nprint(scores[\"nel_micro_f\"])\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Documents with DependencyParser (Python)\nDESCRIPTION: Shows how to process a stream of documents using the pipe method with batch processing capabilities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\nfor doc in parser.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Emoji Sentiment Analysis with spaCy Custom Attributes\nDESCRIPTION: This code snippet shows how to implement emoji sentiment analysis using spaCy's custom attributes. It uses the emoji library to get emoji descriptions and adjusts document sentiment based on emoji matches.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport emoji  # Installation: pip install emoji\nfrom spacy.tokens import Span  # Get the global Span object\n\nSpan.set_extension(\"emoji_desc\", default=None)  # Register the custom attribute\n\ndef label_sentiment(matcher, doc, i, matches):\n    match_id, start, end = matches[i]\n    if doc.vocab.strings[match_id] == \"HAPPY\":  # Don't forget to get string!\n        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\n    elif doc.vocab.strings[match_id] == \"SAD\":\n        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\n    span = doc[start:end]\n    # Verify if it is an emoji and set the extension attribute correctly.\n    if emoji.is_emoji(span[0].text):\n        span._.emoji_desc = emoji.demojize(span[0].text, delimiters=(\"\", \"\"), language=doc.lang_).replace(\"_\", \" \")\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy NER.v2 Task\nDESCRIPTION: Example configuration for the NER.v2 task which supports defining labels with custom descriptions for better LLM performance. This version supports explicitly defining labels with descriptions and supports zero-shot and few-shot prompting.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_28\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.NER.v2\"\nlabels = [\"PERSON\", \"ORGANISATION\", \"LOCATION\"]\nexamples = null\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy and Dependencies with pip\nDESCRIPTION: These commands install spaCy and its dependencies using pip. It's recommended to use a virtual environment to avoid modifying system state.\nSOURCE: https://github.com/explosion/spacy/blob/master/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U pip setuptools wheel\npip install spacy\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .env\nsource .env/bin/activate\npip install -U pip setuptools wheel\npip install spacy\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parameters with Transformer Component in Python\nDESCRIPTION: This snippet demonstrates how to temporarily modify the Transformer's model to use given parameter values, typically for saving the best model during training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"transformer\")\nwith trf.use_params(optimizer.averages):\n    trf.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Initializing a Vocab instance in Python\nDESCRIPTION: Creates a new Vocab object with optional initial strings. This is the foundation for storing and accessing lexemes in spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.vocab import Vocab\nvocab = Vocab(strings=[\"hello\", \"world\"])\n```\n\n----------------------------------------\n\nTITLE: Applying Transformer Pipe to Document Stream in Python\nDESCRIPTION: This snippet demonstrates how to apply the Transformer pipe to a stream of documents using the pipe method. It processes documents in batches for efficiency.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"transformer\")\nfor doc in trf.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Annotation Setter in spacy-transformers\nDESCRIPTION: Example of registering a custom annotation setter for the spacy-transformers package. The annotation setter takes documents and transformer data and applies custom annotations to the documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport spacy_transformers\n\n@spacy_transformers.registry.annotation_setters(\"my_annotation_setter.v1\")\ndef configure_custom_annotation_setter():\n    def annotation_setter(docs, trf_data) -> None:\n       # Set annotations on the docs\n\n    return annotation_setter\n```\n\n----------------------------------------\n\nTITLE: Initializing Language Objects in Python\nDESCRIPTION: Examples of initializing Language objects, either from a subclass or from scratch using a Vocab object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Construction from subclass\nfrom spacy.lang.en import English\nnlp = English()\n\n# Construction from scratch\nfrom spacy.vocab import Vocab\nfrom spacy.language import Language\nnlp = Language(Vocab())\n```\n\n----------------------------------------\n\nTITLE: Configuring Label Definitions in NER.v2\nDESCRIPTION: Configuration example showing how to define custom descriptions for each entity label in NER.v2. This allows giving the LLM more context about what to extract for each entity type.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_30\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.NER.v2\"\nlabels = PERSON,SPORTS_TEAM\n\n[components.llm.task.label_definitions]\nPERSON = \"Extract any named individual in the text.\"\nSPORTS_TEAM = \"Extract the names of any professional sports team. e.g. Golden State Warriors, LA Lakers, Man City, Real Madrid\"\n```\n\n----------------------------------------\n\nTITLE: Using Matcher and PhraseMatcher in spaCy\nDESCRIPTION: Demonstrates how to use the revised Matcher API and the new PhraseMatcher. The Matcher allows adding patterns with callbacks, while PhraseMatcher efficiently matches large terminology lists.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.matcher import Matcher, PhraseMatcher\n\nmatcher = Matcher(nlp.vocab)\nmatcher.add('HEARTS', None, [{\"ORTH\": \"❤️\", \"OP\": '+'}])\n\nphrasematcher = PhraseMatcher(nlp.vocab)\nphrasematcher.add(\"OBAMA\", None, nlp(\"Barack Obama\"))\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Extensions on a Doc (Python)\nDESCRIPTION: Shows how to define custom attributes on Doc objects using set_extension() with a getter function, making them available through the doc._ namespace.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc\ncity_getter = lambda doc: any(city in doc.text for city in (\"New York\", \"Paris\", \"Berlin\"))\nDoc.set_extension(\"has_city\", getter=city_getter)\ndoc = nlp(\"I like New York\")\nassert doc._.has_city\n```\n\n----------------------------------------\n\nTITLE: Setting Annotations with CuratedTransformer\nDESCRIPTION: Assign extracted features to Doc objects, writing DocTransformerOutput to the Doc._.trf_data attribute.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"curated_transformer\")\nscores = trf.predict(docs)\ntrf.set_annotations(docs, scores)\n```\n\n----------------------------------------\n\nTITLE: Configuring Tok2Vec Pipeline Component\nDESCRIPTION: Example showing how to configure and add the tok2vec component to the pipeline using the default model configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.tok2vec import DEFAULT_TOK2VEC_MODEL\nconfig = {\"model\": DEFAULT_TOK2VEC_MODEL}\nnlp.add_pipe(\"tok2vec\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanCategorizer for Multi-label Classification in Python\nDESCRIPTION: Example configuration for the 'spancat' component in spaCy, which performs multi-label classification on spans. It sets up parameters like threshold, spans_key, and suggester function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.spancat import DEFAULT_SPANCAT_MODEL\nconfig = {\n    \"threshold\": 0.5,\n    \"spans_key\": \"labeled_spans\",\n    \"max_positive\": None,\n    \"model\": DEFAULT_SPANCAT_MODEL,\n    \"suggester\": {\"@misc\": \"spacy.ngram_suggester.v1\", \"sizes\": [1, 2, 3]},\n}\nnlp.add_pipe(\"spancat\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Using Lookup Tables in spaCy\nDESCRIPTION: Demonstration of the new Lookups API for adding and accessing dictionary data in the vocabulary, useful for custom components and extension attributes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-2.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata = {\"foo\": \"bar\"}\nnlp.vocab.lookups.add_table(\"my_dict\", data)\n\ndef custom_component(doc):\n   table = doc.vocab.lookups.get_table(\"my_dict\")\n   print(table.get(\"foo\"))  # look something up\n   return doc\n```\n\n----------------------------------------\n\nTITLE: Package Installation Example\nDESCRIPTION: Example showing how to package and install a spaCy pipeline using the package command and pip.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_42\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy package /input /output\n$ cd /output/en_pipeline-0.0.0\n$ pip install dist/en_pipeline-0.0.0.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Configuring CoreferenceResolver Component\nDESCRIPTION: Example showing how to configure and add the CoreferenceResolver component with custom settings including model and cluster prefix.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy_experimental.coref.coref_component import DEFAULT_COREF_MODEL\nfrom spacy_experimental.coref.coref_util import DEFAULT_CLUSTER_PREFIX\nconfig={\n    \"model\": DEFAULT_COREF_MODEL,\n    \"span_cluster_prefix\": DEFAULT_CLUSTER_PREFIX,\n}\nnlp.add_pipe(\"experimental_coref\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory Zones for Word Counting in Python\nDESCRIPTION: Demonstrates how to use spaCy's memory_zone context manager to count words while ensuring memory cleanup. The function processes texts within a memory zone and returns word frequency counts without retaining Doc objects.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/memory-management.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import Counter\n\ndef count_words(nlp, texts):\n    counts = Counter()\n    with nlp.memory_zone():\n        for doc in nlp.pipe(texts):\n            for token in doc:\n                counts[token.text] += 1\n    return counts\n```\n\n----------------------------------------\n\nTITLE: Using spaCy debug profile Command for Performance Analysis\nDESCRIPTION: Profiles which functions take the most time in a spaCy pipeline. Input should be formatted as one JSON object per line with a \"text\" key. If no input file is specified, the IMDB dataset is loaded automatically.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug profile [model] [inputs] [--n-texts]\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for spacy-llm in Python\nDESCRIPTION: This snippet demonstrates how to set up logging for spacy-llm using Python's built-in logging module. It adds a StreamHandler and sets the logging level to DEBUG.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport spacy_llm\n\n\nspacy_llm.logger.addHandler(logging.StreamHandler())\nspacy_llm.logger.setLevel(logging.DEBUG)\n```\n\n----------------------------------------\n\nTITLE: Configuring training optimizer and learning rate schedule in INI\nDESCRIPTION: Example of a training configuration file snippet showing how to set up the optimizer and learning rate schedule using the new config system.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[training]\naccumulate_gradient = 3\n\n[training.optimizer]\n@optimizers = \"Adam.v1\"\n\n[training.optimizer.learn_rate]\n@schedules = \"warmup_linear.v1\"\nwarmup_steps = 250\ntotal_steps = 20000\ninitial_rate = 0.01\n```\n\n----------------------------------------\n\nTITLE: Using EntityLinker.__call__ Method in spaCy Python\nDESCRIPTION: Example of directly applying the EntityLinker to a document. This method modifies the document in place by adding entity links and returns the processed document.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nentity_linker = nlp.add_pipe(\"entity_linker\")\n# This usually happens under the hood\nprocessed = entity_linker(doc)\n```\n\n----------------------------------------\n\nTITLE: Initializing TextCategorizer in Python\nDESCRIPTION: Examples of initializing the TextCategorizer using different methods, including add_pipe and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\n# Use 'textcat_multilabel' for multi-label classification\ntextcat = nlp.add_pipe(\"textcat\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_textcat\"}}\nparser = nlp.add_pipe(\"textcat\", config=config)\n\n# Construction from class\n# Use 'MultiLabel_TextCategorizer' for multi-label classification\nfrom spacy.pipeline import TextCategorizer\ntextcat = TextCategorizer(nlp.vocab, model, threshold=0.5)\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Initialization\nDESCRIPTION: Implementation of the initialize method that sets up labels either explicitly or from training data and performs model initialization with shape inference.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom itertools import islice\n\ndef initialize(\n    self,\n    get_examples: Callable[[], Iterable[Example]],\n    *,\n    nlp: Language = None,\n    labels: Optional[List[str]] = None,\n):\n    if labels is not None:\n        for label in labels:\n            self.add_label(label)\n    else:\n        for example in get_examples():\n            relations = example.reference._.rel\n            for indices, label_dict in relations.items():\n                for label in label_dict.keys():\n                    self.add_label(label)\n    subbatch = list(islice(get_examples(), 10))\n    doc_sample = [eg.reference for eg in subbatch]\n    label_sample = self._examples_to_truth(subbatch)\n    self.model.initialize(X=doc_sample, Y=label_sample)\n```\n\n----------------------------------------\n\nTITLE: Accessing Individual Tokens in Span\nDESCRIPTION: Shows how to access individual Token objects within a Span using index notation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\nspan = doc[1:4]\nassert span[1].text == \"back\"\n```\n\n----------------------------------------\n\nTITLE: Adding Patterns to EntityRuler\nDESCRIPTION: Example of adding multiple patterns to an EntityRuler, including both token patterns and phrase patterns.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npatterns = [\n    {\"label\": \"ORG\", \"pattern\": \"Apple\"},\n    {\"label\": \"GPE\", \"pattern\": [{\"lower\": \"san\"}, {\"lower\": \"francisco\"}]}\n]\nruler = nlp.add_pipe(\"entity_ruler\")\nruler.add_patterns(patterns)\n```\n\n----------------------------------------\n\nTITLE: Custom Configuration for Streaming Training Data\nDESCRIPTION: This configuration snippet demonstrates how to set up a training pipeline for streaming data that doesn't fit in memory. It defines train and dev corpora with different limits and sets training parameters accordingly.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_36\n\nLANGUAGE: ini\nCODE:\n```\n[corpora.dev]\n@readers = \"even_odd.v1\"\nlimit = 100\n\n[corpora.train]\n@readers = \"even_odd.v1\"\nlimit = -1\n\n[training]\nmax_epochs = -1\npatience = 500\nmax_steps = 2000\n```\n\n----------------------------------------\n\nTITLE: Training spaCy Model with Custom Code\nDESCRIPTION: Bash command to train a spaCy model using a custom configuration and additional code file containing custom functions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy train config.cfg --output ./output --code ./functions.py\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Train Command with Overrides\nDESCRIPTION: Example of running the train command with configuration overrides to specify custom paths for training and development data while outputting to a specific directory.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy train config.cfg --output ./output --paths.train ./train --paths.dev ./dev\n```\n\n----------------------------------------\n\nTITLE: Predicting Entity Links with EntityLinker in spaCy (Python)\nDESCRIPTION: Shows how to apply the EntityLinker model to a batch of Doc objects to predict KB IDs for entities without modifying the documents. Returns a list of KB identifiers, including 'NIL' for entities without a prediction.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nentity_linker = nlp.add_pipe(\"entity_linker\")\nkb_ids = entity_linker.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Configuring Shared Tok2Vec Listener Replacement in spaCy (INI)\nDESCRIPTION: Configuration example showing how to replace a shared token-to-vector listener with an independent copy for frozen components, ensuring their performance isn't degraded when training other components with the same underlying Tok2Vec instance.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_13\n\nLANGUAGE: ini\nCODE:\n```\n[training]\nfrozen_components = [\"tagger\"]\n\n[components.tagger]\nsource = \"en_core_web_sm\"\nreplace_listeners = [\"model.tok2vec\"]\n```\n\n----------------------------------------\n\nTITLE: Initializing BERT WordPiece Encoder in Python\nDESCRIPTION: Constructs a WordPiece piece encoder model for BERT-like models. It accepts token sequences or documents and returns piece identifiers, splitting tokens on punctuation characters. This model requires separate initialization using an appropriate loader.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nspacy-curated-transformers.BertWordpieceEncoder.v1\n```\n\n----------------------------------------\n\nTITLE: Adding Lemmatizer to spaCy Pipeline with Custom Config\nDESCRIPTION: Example of adding the Lemmatizer component to a spaCy pipeline with custom configuration settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lemmatizer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\"mode\": \"rule\"}\nnlp.add_pipe(\"lemmatizer\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Removing Patterns from SpanRuler by Label\nDESCRIPTION: Demonstrates how to remove patterns from a SpanRuler using a label.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npatterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"}]\nruler = nlp.add_pipe(\"span_ruler\")\nruler.add_patterns(patterns)\nruler.remove(\"ORG\")\n```\n\n----------------------------------------\n\nTITLE: Checking Token Characteristics in Python using spaCy\nDESCRIPTION: This snippet demonstrates various methods to check token characteristics in spaCy. It includes checks for alphabetic, ASCII, digit, case, punctuation, and other lexical features.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\ntoken.text.isalpha()  # is_alpha\nall(ord(c) < 128 for c in token.text)  # is_ascii\ntoken.text.isdigit()  # is_digit\ntoken.text.islower()  # is_lower\ntoken.text.isupper()  # is_upper\ntoken.text.istitle()  # is_title\ntoken.text.isspace()  # is_space\n```\n\n----------------------------------------\n\nTITLE: Modifying spaCy's Infix Rules to Prevent Hyphen Splitting\nDESCRIPTION: This example shows how to customize the infix tokenization rules to prevent splitting compound words with hyphens. It modifies the default infix patterns by commenting out the rule that splits on hyphens between letters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\nfrom spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\nfrom spacy.util import compile_infix_regex\n\n# Default tokenizer\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"mother-in-law\")\nprint([t.text for t in doc]) # ['mother', '-', 'in', '-', 'law']\n\n# Modify tokenizer infix patterns\ninfixes = (\n    LIST_ELLIPSES\n    + LIST_ICONS\n    + [\n        r\"(?<=[0-9])[+\\\\-\\\\*^](?=[0-9-])\",\n        r\"(?<=[{al}{q}])\\\\.(?=[{au}{q}])\".format(\n            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n        ),\n        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n        # ✅ Commented out regex that splits on hyphens between letters:\n        # r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n    ]\n)\n\ninfix_re = compile_infix_regex(infixes)\nnlp.tokenizer.infix_finditer = infix_re.finditer\ndoc = nlp(\"mother-in-law\")\nprint([t.text for t in doc]) # ['mother-in-law']\n```\n\n----------------------------------------\n\nTITLE: Creating Spans from Matcher Results\nDESCRIPTION: Demonstrates two methods of creating Span objects from matcher results: manually and using as_spans=True parameter.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy.tokens import Span\n\nnlp = spacy.blank(\"en\")\nmatcher = Matcher(nlp.vocab)\nmatcher.add(\"PERSON\", [[{\"lower\": \"barack\"}, {\"lower\": \"obama\"}]])\ndoc = nlp(\"Barack Obama was the 44th president of the United States\")\n\n# 1. Return (match_id, start, end) tuples\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = Span(doc, start, end, label=match_id)\n    print(span.text, span.label_)\n\n# 2. Return Span objects directly\nmatches = matcher(doc, as_spans=True)\nfor span in matches:\n    print(span.text, span.label_)\n```\n\n----------------------------------------\n\nTITLE: Configuring an EntityLinker Pipeline Component in spaCy\nDESCRIPTION: Example showing how to configure an entity linker component with custom settings before adding it to the spaCy pipeline. The configuration includes settings for discarding specific labels, context inclusion, model architecture, and candidate generation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.entity_linker import DEFAULT_NEL_MODEL\nconfig = {\n   \"labels_discard\": [],\n   \"n_sents\": 0,\n   \"incl_prior\": True,\n   \"incl_context\": True,\n   \"model\": DEFAULT_NEL_MODEL,\n   \"entity_vector_length\": 64,\n   \"get_candidates\": {'@misc': 'spacy.CandidateGenerator.v1'},\n   \"threshold\": None,\n}\nnlp.add_pipe(\"entity_linker\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Predicting with Transformer Component in Python\nDESCRIPTION: This code snippet demonstrates how to apply the Transformer model to a batch of Doc objects without modifying them, using the predict method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"transformer\")\nscores = trf.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Example Pattern for DependencyMatcher in Python\nDESCRIPTION: A pattern example for matching \"[subject] ... initially founded\" showing the structure of DependencyMatcher patterns. The pattern defines an anchor token and its relationships with other tokens using dependency operators.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencymatcher.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# pattern: \"[subject] ... initially founded\"\n[\n  # anchor token: founded\n  {\n    \"RIGHT_ID\": \"founded\",\n    \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n  },\n  # founded -> subject\n  {\n    \"LEFT_ID\": \"founded\",\n    \"REL_OP\": \">\",\n    \"RIGHT_ID\": \"subject\",\n    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}\n  },\n  # \"founded\" follows \"initially\"\n  {\n    \"LEFT_ID\": \"founded\",\n    \"REL_OP\": \";\",\n    \"RIGHT_ID\": \"initially\",\n    \"RIGHT_ATTRS\": {\"ORTH\": \"initially\"}\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring MultiHashEmbed for Token Attribute Embedding in spaCy\nDESCRIPTION: This snippet demonstrates the configuration for the MultiHashEmbed architecture in spaCy. It sets up an embedding layer that combines multiple lexical attributes using hash embedding, with options for including static word vectors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.MultiHashEmbed.v2\"\nwidth = 64\nattrs = [\"NORM\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"]\nrows = [2000, 1000, 1000, 1000]\ninclude_static_vectors = true\n```\n\n----------------------------------------\n\nTITLE: Using spaCy debug model Command for Neural Network Inspection\nDESCRIPTION: Debugs a Thinc Model by running it on sample text and checking how it updates internal weights and parameters. Provides various options to examine different aspects of the model during the debugging process.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug model [config_path] [component] [--layers] [--dimensions] [--parameters] [--gradients] [--attributes] [--print-step0] [--print-step1] [--print-step2] [--print-step3] [--gpu-id]\n```\n\n----------------------------------------\n\nTITLE: Defining Project Structure and Assets in YAML for spaCy\nDESCRIPTION: This YAML snippet demonstrates the structure of a project.yml file used in spaCy projects. It includes sections for variables, assets, workflows, and commands, showing how to define project dependencies, data processing steps, and evaluation metrics.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n%%GITHUB_PROJECTS/pipelines/tagger_parser_ud/project.yml\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Pipeline Component in Python\nDESCRIPTION: Creates a custom 'snek' pipeline component that prints ASCII art when processing a document. The component is defined using the @Language.component decorator to register it with spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\nsnek = \"\"\"\n    --..,_                     _,.--.\\n       `'.'.                .'`__ o  `;__. {text}\\n          '.'.            .'.'`  '---'`  `\\n            '.`'--....--'`.'\\n              `'--....--'`\\n\"\"\"\n\n@Language.component(\"snek\")\ndef snek_component(doc):\n    print(snek.format(text=doc.text))\n    return doc\n```\n\n----------------------------------------\n\nTITLE: Updating Coreference Resolver Model in Python with spaCy\nDESCRIPTION: This code shows how to update the coreference resolver model using a batch of examples and an optimizer. It returns the updated losses dictionary.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncoref = nlp.add_pipe(\"experimental_coref\")\noptimizer = nlp.initialize()\nlosses = coref.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Accessing Vector Values in spaCy\nDESCRIPTION: Demonstrates iterating over vector values that have been assigned to keys.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfor vector in nlp.vocab.vectors.values():\n    print(vector)\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanFinder Pipeline Component in Python\nDESCRIPTION: Example showing how to configure and add the span_finder pipeline component with custom settings including threshold, spans_key, and length constraints.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.span_finder import DEFAULT_SPAN_FINDER_MODEL\nconfig = {\n    \"threshold\": 0.5,\n    \"spans_key\": \"my_spans\",\n    \"max_length\": None,\n    \"min_length\": None,\n    \"model\": DEFAULT_SPAN_FINDER_MODEL,\n}\nnlp.add_pipe(\"span_finder\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Migrating from v2 to v3 Training Command\nDESCRIPTION: Comparison showing the difference between spaCy v2 and v3 training commands, highlighting the shift from command-line arguments to configuration-based training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_22\n\nLANGUAGE: diff\nCODE:\n```\n- python -m spacy train en ./output ./train.json ./dev.json\n--pipeline tagger,parser --cnn-window 1 --bilstm-depth 0\n+ python -m spacy train ./config.cfg --output ./output\n```\n\n----------------------------------------\n\nTITLE: Initializing Lemmatizer with Custom Lookups\nDESCRIPTION: Example of initializing the Lemmatizer component with custom lookups data and corresponding config.cfg snippet.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lemmatizer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"lemmatizer\")\nlemmatizer.initialize(lookups=lookups)\n```\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[initialize.components.lemmatizer]\n\n[initialize.components.lemmatizer.lookups]\n@misc = \"load_my_lookups.v1\"\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Labels to LLM Tasks in spaCy\nDESCRIPTION: Example showing how to add custom labels to an LLM-based component, which can be used for custom entity types or other task-specific labels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nllm_ner = nlp.add_pipe(\"llm_ner\")\nllm_ner.add_label(\"MY_LABEL\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Debug Component in spaCy\nDESCRIPTION: This code snippet demonstrates how to create a custom debug component in spaCy using type hints and pydantic validation. The component logs information about the nlp object and the Doc that passes through, with a configurable log level.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.language import Language\nfrom spacy.tokens import Doc\nfrom pydantic import StrictStr\nimport logging\n\n@Language.factory(\"debug\", default_config={\"log_level\": \"DEBUG\"})\nclass DebugComponent:\n    def __init__(self, nlp: Language, name: str, log_level: StrictStr):\n        self.logger = logging.getLogger(f\"spacy.{name}\")\n        self.logger.setLevel(log_level)\n        self.logger.info(f\"Pipeline: {nlp.pipe_names}\")\n\n    def __call__(self, doc: Doc) -> Doc:\n        is_tagged = doc.has_annotation(\"TAG\")\n        self.logger.debug(f\"Doc: {len(doc)} tokens, is tagged: {is_tagged}\")\n        return doc\n\nnlp = spacy.load(\"en_core_web_sm\")\nnlp.add_pipe(\"debug\", config={\"log_level\": \"DEBUG\"})\ndoc = nlp(\"This is a text...\")\n```\n\n----------------------------------------\n\nTITLE: Checking Token Attribute Annotation in spaCy Doc (Python)\nDESCRIPTION: Illustrates how to check if a spaCy Doc contains annotation for a specific Token attribute. This method replaces previous boolean attributes like is_tagged, is_parsed, etc.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a text\")\n- assert doc.is_parsed\n+ assert doc.has_annotation(\"DEP\")\n```\n\n----------------------------------------\n\nTITLE: Customizing - Adding Special Case Tokenization Rules\nDESCRIPTION: Example demonstrating how to add custom tokenization rules for special cases like contractions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tokenizer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs import ORTH, NORM\ncase = [{ORTH: \"do\"}, {ORTH: \"n't\", NORM: \"not\"}]\ntokenizer.add_special_case(\"don't\", case)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Batch Size Using Registered Function in spaCy (INI)\nDESCRIPTION: This config demonstrates how to use a registered function to create a dynamic batch size schedule for training in spaCy. It uses the compounding function to gradually increase the batch size during training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_17\n\nLANGUAGE: ini\nCODE:\n```\n[training.batcher.size]\n@schedules = \"compounding.v1\"\nstart = 100\nstop = 1000\ncompound = 1.001\n```\n\n----------------------------------------\n\nTITLE: Saving EntityRuler with Pipeline\nDESCRIPTION: Shows how to save an NLP pipeline that includes an EntityRuler with patterns to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_sm\")\nruler = nlp.add_pipe(\"entity_ruler\")\nruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\nnlp.to_disk(\"/path/to/pipeline\")\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Morphologizer\nDESCRIPTION: Shows how to apply the morphologizer model to documents for predictions without modifying them.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\nscores = morphologizer.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Initializing Curated Transformer Configuration in spaCy\nDESCRIPTION: This command auto-fills the Hugging Face model hyperparameters and loader parameters for a Curated Transformer pipeline component in a .cfg file. It can read model details from command-line arguments or the config file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init fill-curated-transformer [base_path] [output_file] [--model-name] [--model-revision] [--pipe-name] [--code]\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parameters in SentenceRecognizer\nDESCRIPTION: Temporarily modifies the model's parameters using a context manager.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\nwith senter.use_params(optimizer.averages):\n    senter.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Token Pattern Definition in spaCy\nDESCRIPTION: Demonstrates how to define a pattern for matching a sequence of tokens that includes 'hello', punctuation, and 'world' using lowercase matching and punctuation flags.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n```\n\n----------------------------------------\n\nTITLE: Accessing Named Entities in a spaCy Doc in Python\nDESCRIPTION: Shows how to access named entities (ents) in a spaCy Doc and their properties.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Mr. Best flew to New York on Saturday morning.\")\nents = list(doc.ents)\nassert ents[0].label_ == \"PERSON\"\nassert ents[0].text == \"Mr. Best\"\n```\n\n----------------------------------------\n\nTITLE: Defining Token Patterns with Extended Attributes in Python\nDESCRIPTION: Examples of using extended pattern syntax with attributes like LEMMA, LENGTH, and MORPH to create more complex token matching patterns in spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Matches \"love cats\" or \"likes flowers\"\npattern1 = [{\"LEMMA\": {\"IN\": [\"like\", \"love\"]}},\n            {\"POS\": \"NOUN\"}]\n\n# Matches tokens of length >= 10\npattern2 = [{\"LENGTH\": {\">=\": 10}}]\n\n# Match based on morph attributes\npattern3 = [{\"MORPH\": {\"IS_SUBSET\": [\"Number=Sing\", \"Gender=Neut\"]}}]\n# \"\", \"Number=Sing\" and \"Number=Sing|Gender=Neut\" will match as subsets\n# \"Number=Plur|Gender=Neut\" will not match\n# \"Number=Sing|Gender=Neut|Polite=Infm\" will not match because it's a superset\n```\n\n----------------------------------------\n\nTITLE: Merging Tokens in a spaCy Doc in Python\nDESCRIPTION: Demonstrates how to merge tokens and set custom attributes using the Retokenizer.merge() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like David Bowie\")\nwith doc.retokenize() as retokenizer:\n    attrs = {\"LEMMA\": \"David Bowie\"}\n    retokenizer.merge(doc[2:4], attrs=attrs)\n```\n\n----------------------------------------\n\nTITLE: Configuring Label Readers for spaCy Model Initialization\nDESCRIPTION: Example configuration for using spaCy's label reader to load pre-generated label sets during model initialization. This helps speed up the initialization process by providing pre-generated label sets for components like the named entity recognizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_25\n\nLANGUAGE: ini\nCODE:\n```\n[initialize.components]\n\n[initialize.components.ner]\n\n[initialize.components.ner.labels]\n@readers = \"spacy.read_labels.v1\"\npath = \"corpus/labels/ner.json\"\n```\n\n----------------------------------------\n\nTITLE: Configuring EntityLinker Model for Entity Disambiguation in spaCy\nDESCRIPTION: Configuration for the EntityLinker model architecture which helps disambiguate textual mentions to unique identifiers. It uses a tok2vec layer followed by a linear output layer to assign entities to their most likely identifiers from a knowledge base.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_29\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.EntityLinker.v2\"\nnO = null\n\n[model.tok2vec]\n@architectures = \"spacy.HashEmbedCNN.v2\"\npretrained_vectors = null\nwidth = 96\ndepth = 2\nembed_size = 2000\nwindow_size = 1\nmaxout_pieces = 3\nsubword_features = true\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Similarity Hooks in spaCy's Pipeline\nDESCRIPTION: This example shows how to create custom hooks for the similarity methods in spaCy. It implements a component that overrides the default similarity calculation for Doc, Span, and Token objects, replacing it with a simpler vector-based computation based on a specific vector index.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\n\nclass SimilarityModel:\n    def __init__(self, name: str, index: int):\n        self.name = name\n        self.index = index\n\n    def __call__(self, doc):\n        doc.user_hooks[\"similarity\"] = self.similarity\n        doc.user_span_hooks[\"similarity\"] = self.similarity\n        doc.user_token_hooks[\"similarity\"] = self.similarity\n        return doc\n\n    def similarity(self, obj1, obj2):\n        return obj1.vector[self.index] + obj2.vector[self.index]\n\n\n@Language.factory(\"similarity_component\", default_config={\"index\": 0})\ndef create_similarity_component(nlp, name, index: int):\n    return SimilarityModel(name, index)\n```\n\n----------------------------------------\n\nTITLE: Updating Tok2Vec Model During Training\nDESCRIPTION: Example of updating the Tok2Vec model during training using examples and an optimizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntok2vec = nlp.add_pipe(\"tok2vec\")\noptimizer = nlp.initialize()\nlosses = tok2vec.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Executing spaCy Train Command\nDESCRIPTION: Command-line syntax for training a spaCy pipeline using a configuration file. The command supports various options for specifying output location, custom code, verbosity, and GPU usage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy train [config_path] [--output] [--code] [--verbose] [--gpu-id] [overrides]\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy HashEmbedCNN Model Architecture\nDESCRIPTION: Example configuration for the spaCy HashEmbedCNN model architecture. It defines various parameters for the embedding and encoding layers of the model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.HashEmbedCNN.v2\"\npretrained_vectors = null\nwidth = 96\ndepth = 4\nembed_size = 2000\nwindow_size = 1\nmaxout_pieces = 3\nsubword_features = true\n```\n\n----------------------------------------\n\nTITLE: Using spaCy Tagger Component\nDESCRIPTION: Example showing how to process a document using the tagger component directly.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\ntagger = nlp.add_pipe(\"tagger\")\n# This usually happens under the hood\nprocessed = tagger(doc)\n```\n\n----------------------------------------\n\nTITLE: Configuring Frozen and Annotating Components in spaCy Pipeline (INI)\nDESCRIPTION: This config excerpt shows how to set up frozen and annotating components in a spaCy pipeline. It demonstrates using a frozen NER component and a sentencizer to provide required annotations for an entity linker during training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_15\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\npipeline = [\"sentencizer\", \"ner\", \"entity_linker\"]\n\n[components.ner]\nsource = \"en_core_web_sm\"\n\n[training]\nfrozen_components = [\"ner\"]\nannotating_components = [\"sentencizer\", \"ner\"]\n```\n\n----------------------------------------\n\nTITLE: Excluding Components from spaCy Serialization\nDESCRIPTION: Example demonstrating how to exclude specific components when serializing and loading spaCy models using to_bytes() and from_disk() methods.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndata = nlp.to_bytes(exclude=[\"tokenizer\", \"vocab\"])\nnlp.from_disk(\"/pipeline\", exclude=[\"ner\"])\n```\n\n----------------------------------------\n\nTITLE: Setting Entity Annotations in spaCy Documents in Python\nDESCRIPTION: This snippet demonstrates two methods for adding or modifying entity annotations in a spaCy document by creating spans and using either doc.set_ents or directly assigning to doc.ents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.tokens import Span\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"fb is hiring a new vice president of global policy\")\nents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\nprint('Before', ents)\n# The model didn't recognize \"fb\" as an entity :(\n\n# Create a span for the new entity\nfb_ent = Span(doc, 0, 1, label=\"ORG\")\norig_ents = list(doc.ents)\n\n# Option 1: Modify the provided entity spans, leaving the rest unmodified\ndoc.set_ents([fb_ent], default=\"unmodified\")\n\n# Option 2: Assign a complete list of ents to doc.ents\ndoc.ents = orig_ents + [fb_ent]\n\nents = [(e.text, e.start, e.end, e.label_) for e in doc.ents]\nprint('After', ents)\n# [('fb', 0, 1, 'ORG')] 🎉\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy Pipeline Configuration from File\nDESCRIPTION: Loads a pipeline's config.cfg file from a specified path. The config includes details about components, creation settings, and training hyperparameters. Supports optional overrides and variable interpolation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nconfig = util.load_config(\"/path/to/config.cfg\")\nprint(config.to_str())\n```\n\n----------------------------------------\n\nTITLE: Migrating Model Loading Code in spaCy v3.0\nDESCRIPTION: Example showing how to update the code that loads spaCy models from using shortcut names like 'en' to using the full model name like 'en_core_web_sm' as required in spaCy v3.0.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_14\n\nLANGUAGE: diff\nCODE:\n```\n- nlp = spacy.load(\"en\")\n+ nlp = spacy.load(\"en_core_web_sm\")\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy Language Model from Disk (Python)\nDESCRIPTION: Example showing how to load a serialized state from a directory into a Language object using from_disk. This requires the correct language class to be initialized with pipeline components added beforehand.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\nnlp = Language().from_disk(\"/path/to/pipeline\")\n\n# Using language-specific subclass\nfrom spacy.lang.en import English\nnlp = English().from_disk(\"/path/to/pipeline\")\n```\n\n----------------------------------------\n\nTITLE: Finishing Update for TrainablePipe in Python\nDESCRIPTION: Update parameters using the current parameter gradients. This method typically calls the model's finish_update method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\noptimizer = nlp.initialize()\nlosses = pipe.update(examples, sgd=None)\npipe.finish_update(sgd)\n```\n\n----------------------------------------\n\nTITLE: Adding Vectors to Store in Python\nDESCRIPTION: Examples of adding new vectors to the store, either with new values or mapping to existing rows.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nvector = numpy.random.uniform(-1, 1, (300,))\ncat_id = nlp.vocab.strings[\"cat\"]\nnlp.vocab.vectors.add(cat_id, vector=vector)\nnlp.vocab.vectors.add(\"dog\", row=0)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sentiment Task with Few-Shot Examples\nDESCRIPTION: Configuration for the Sentiment task that includes few-shot examples from a YAML file using the FewShotReader.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_51\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.Sentiment.v1\"\n[components.llm.task.examples]\n@misc = \"spacy.FewShotReader.v1\"\npath = \"sentiment_examples.yml\"\n```\n\n----------------------------------------\n\nTITLE: Converting Data Formats in spaCy\nDESCRIPTION: Command line syntax for converting various file formats into spaCy's binary training data format, with options for sentence segmentation, morphology, and language specification.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy convert [input_file] [output_dir] [--converter] [--file-type] [--n-sents] [--seg-sents] [--base] [--morphology] [--merge-subtokens] [--ner-map] [--lang]\n```\n\n----------------------------------------\n\nTITLE: Initializing Sentencizer\nDESCRIPTION: Examples showing different ways to initialize the Sentencizer component, either via add_pipe or direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencizer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe\nsentencizer = nlp.add_pipe(\"sentencizer\")\n\n# Construction from class\nfrom spacy.pipeline import Sentencizer\nsentencizer = Sentencizer()\n```\n\n----------------------------------------\n\nTITLE: Deserializing spaCy Language Model from Bytes in Python\nDESCRIPTION: Shows how to load a spaCy language model from serialized binary data. This example creates an English language model and loads the state from previously serialized bytes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\nnlp_bytes = nlp.to_bytes()\nnlp2 = English()\nnlp2.from_bytes(nlp_bytes)\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Token Children\nDESCRIPTION: Shows how to iterate through token's left and right children in the parse tree using Token.lefts and Token.rights attributes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"bright red apples on the tree\")\nprint([token.text for token in doc[2].lefts])  # ['bright', 'red']\nprint([token.text for token in doc[2].rights])  # ['on']\nprint(doc[2].n_lefts)  # 2\nprint(doc[2].n_rights)  # 1\n```\n\n----------------------------------------\n\nTITLE: Fetching Project Assets Command in spaCy\nDESCRIPTION: Command to fetch project assets like datasets and pretrained weights defined in project.yml. Supports checksum validation and handles both remote downloads and local file copying.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_45\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project assets [project_dir]\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project assets [--sparse]\n```\n\n----------------------------------------\n\nTITLE: Adding Patterns with Callback to spaCy Matcher\nDESCRIPTION: Example of adding multiple patterns to a Matcher with a callback function that executes when matches are found.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/matcher.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef on_match(matcher, doc, id, matches):\n    print('Matched!', matches)\n\nmatcher = Matcher(nlp.vocab)\npatterns = [\n   [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}],\n   [{\"ORTH\": \"Google\"}, {\"ORTH\": \"Maps\"}]\n]\nmatcher.add(\"TEST_PATTERNS\", patterns, on_match=on_match)\ndoc = nlp(\"HELLO WORLD on Google Maps.\")\nmatches = matcher(doc)\n```\n\n----------------------------------------\n\nTITLE: Replacing Pipeline Components in spaCy\nDESCRIPTION: Example of replacing a pipeline component with a new one using replace_pipe method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nnew_parser = nlp.replace_pipe(\"parser\", \"my_custom_parser\")\n```\n\n----------------------------------------\n\nTITLE: Accessing lexemes in spaCy's Vocab\nDESCRIPTION: Shows how to retrieve a lexeme from the vocabulary using either an integer ID or a string. This method creates new lexemes for unseen strings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napple = nlp.vocab.strings[\"apple\"]\nassert nlp.vocab[apple] == nlp.vocab[\"apple\"]\n```\n\n----------------------------------------\n\nTITLE: Deserializing a spaCy Doc from JSON in Python\nDESCRIPTION: Demonstrates how to deserialize a JSON representation back into a spaCy Doc object using the from_json() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc\ndoc = nlp(\"All we have to decide is what to do with the time that is given us.\")\ndoc_json = doc.to_json()\ndeserialized_doc = Doc(nlp.vocab).from_json(doc_json)\nassert deserialized_doc.text == doc.text == doc_json[\"text\"]\n```\n\n----------------------------------------\n\nTITLE: Using Alignment for Token Mapping in spaCy\nDESCRIPTION: Shows how to use the Alignment class to calculate alignment between different tokenization schemes, specifically aligning BERT-style tokens with spaCy tokens.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import Alignment\n\nbert_tokens = [\"obama\", \"'\", \"s\", \"podcast\"]\nspacy_tokens = [\"obama\", \"'s\", \"podcast\"]\nalignment = Alignment.from_strings(bert_tokens, spacy_tokens)\na2b = alignment.x2y\nassert list(a2b.data) == [0, 1, 1, 2]\n```\n\n----------------------------------------\n\nTITLE: Analyzing Pipeline Components with analyze_pipes in Python\nDESCRIPTION: Shows how to use the analyze_pipes method to get a summary of the current pipeline components, including their assigned and required attributes, scores, and potential problems. The method returns a dictionary with detailed analysis.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\"tagger\")\nnlp.add_pipe(\"entity_linker\")\nanalysis = nlp.analyze_pipes()\n```\n\n----------------------------------------\n\nTITLE: Knowledge Base YAML Structure Example\nDESCRIPTION: Example structure for a knowledge base YAML file showing entity definitions with IDs, names, descriptions, and alias mappings with probabilities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\nentities:\n  # The key should be whatever ID identifies this entity uniquely in your knowledge base.\n  ID1:\n      name: \"...\"\n      desc: \"...\"\n  ID2:\n      ...\n# Data on aliases in your knowledge base - e. g. \"Apple\" for the entity \"Apple Inc.\".\naliases:\n  - alias: \"...\"\n    # List of all entities that this alias refers to.\n    entities: [\"ID1\", \"ID2\", ...]\n    # Optional: prior probabilities that this alias refers to the n-th entity in the \"entities\" attribute.\n    probabilities: [0.5, 0.2, ...]\n  - alias: \"...\"\n    entities: [...]\n    probabilities: [...]\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing and Running spaCy NER Demo Project\nDESCRIPTION: Commands to clone, setup and run a Named Entity Recognition demo project in spaCy. Includes project initialization, dependency installation, and workflow execution.\nSOURCE: https://github.com/explosion/spacy/blob/master/examples/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy project clone pipelines/ner_demo\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd ner_demo\npython -m pip install -r requirements.txt\npython -m spacy project assets\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy project run all\n```\n\n----------------------------------------\n\nTITLE: Configuring EntityRuler in SpaCy Pipeline\nDESCRIPTION: Example showing how to configure the entity_ruler component when adding it to the SpaCy pipeline. This demonstrates setting various configuration options like phrase_matcher_attr, validate, overwrite_ents, and ent_id_sep.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\n   \"phrase_matcher_attr\": None,\n   \"validate\": True,\n   \"overwrite_ents\": False,\n   \"ent_id_sep\": \"||\",\n}\nnlp.add_pipe(\"entity_ruler\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Configuring EntityRuler Initialization in INI\nDESCRIPTION: Configuration example for initializing the EntityRuler component using a config file. It shows how to specify pattern loading from a JSON file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[initialize.components.entity_ruler]\n\n[initialize.components.entity_ruler.patterns]\n@readers = \"srsly.read_jsonl.v1\"\npath = \"corpus/entity_ruler_patterns.jsonl\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Tok2Vec\nDESCRIPTION: Example of using Tok2Vec to make predictions on documents without modifying them.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntok2vec = nlp.add_pipe(\"tok2vec\")\nscores = tok2vec.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Calculating Semantic Similarity between Spans in Python using spaCy\nDESCRIPTION: Shows how to compute semantic similarity between two spans using the similarity method. The default estimate uses cosine similarity with average word vectors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"green apples and red oranges\")\ngreen_apples = doc[:2]\nred_oranges = doc[3:]\napples_oranges = green_apples.similarity(red_oranges)\noranges_apples = red_oranges.similarity(green_apples)\nassert apples_oranges == oranges_apples\n```\n\n----------------------------------------\n\nTITLE: Basic Matcher Usage\nDESCRIPTION: Shows the basic usage of applying a matcher to a document.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(YOUR_TEXT_HERE)\nmatcher(doc)\n```\n\n----------------------------------------\n\nTITLE: Initializing EditTreeLemmatizer Component\nDESCRIPTION: Examples demonstrating different ways to initialize the EditTreeLemmatizer component, including via add_pipe with default model, custom model configuration, and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\n\n# Construction via create_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_tagger\"}}\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", config=config, name=\"lemmatizer\")\n\n# Construction from class\nfrom spacy.pipeline import EditTreeLemmatizer\nlemmatizer = EditTreeLemmatizer(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Setting Entity Annotations from NumPy Array in spaCy in Python\nDESCRIPTION: This code demonstrates how to use NumPy arrays and the doc.from_array method to set entity annotations by directly modifying the underlying token attributes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nimport spacy\nfrom spacy.attrs import ENT_IOB, ENT_TYPE\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp.make_doc(\"London is a big city in the United Kingdom.\")\nprint(\"Before\", doc.ents)  # []\n\nheader = [ENT_IOB, ENT_TYPE]\nattr_array = numpy.zeros((len(doc), len(header)), dtype=\"uint64\")\nattr_array[0, 0] = 3  # B\nattr_array[0, 1] = doc.vocab.strings[\"GPE\"]\ndoc.from_array(header, attr_array)\nprint(\"After\", doc.ents)  # [London]\n```\n\n----------------------------------------\n\nTITLE: Analyzing Token Subtrees and Ancestors\nDESCRIPTION: Shows how to work with token subtrees and ancestors in the parse tree, including verification of ancestor relationships.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Credit and mortgage account holders must submit their requests\")\n\nroot = [token for token in doc if token.head == token][0]\nsubject = list(root.lefts)[0]\nfor descendant in subject.subtree:\n    assert subject is descendant or subject.is_ancestor(descendant)\n    print(descendant.text, descendant.dep_, descendant.n_lefts,\n            descendant.n_rights,\n            [ancestor.text for ancestor in descendant.ancestors])\n```\n\n----------------------------------------\n\nTITLE: Applying Lemmatizer to a Document\nDESCRIPTION: Example of applying the Lemmatizer component to a single document, which modifies the document in place.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lemmatizer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nlemmatizer = nlp.add_pipe(\"lemmatizer\")\n# This usually happens under the hood\nprocessed = lemmatizer(doc)\n```\n\n----------------------------------------\n\nTITLE: Creating TextCategorizer Optimizer in Python\nDESCRIPTION: This code shows how to create an optimizer for the TextCategorizer pipeline component. It adds the TextCategorizer to the NLP pipeline and then creates an optimizer for it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\noptimizer = textcat.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: FastAPI Service Configuration\nDESCRIPTION: YAML configuration for serving spaCy models via FastAPI REST endpoint.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_36\n\nLANGUAGE: yaml\nCODE:\n```\n  - name: \"serve\"\n    help: \"Serve the models via a FastAPI REST API using the given host and port\"\n    script:\n      - \"uvicorn scripts.main:app --reload --host 127.0.0.1 --port 5000\"\n    deps:\n      - \"scripts/main.py\"\n    no_skip: true\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy NLP Object from Disk\nDESCRIPTION: Demonstrates how to load a spaCy NLP object from disk using the from_disk() method. This allows restoring a previously saved NLP pipeline from a specified file path.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_serialization.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnlp.from_disk(\"/path\")\n```\n\n----------------------------------------\n\nTITLE: Predicting Spans with SpanResolver in Python\nDESCRIPTION: Apply the SpanResolver model to a batch of Doc objects to predict mention clusters without modifying the documents. Returns a list of MentionClusters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\nspans = span_resolver.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Downloading and Initializing Word Vectors in spaCy\nDESCRIPTION: This bash example demonstrates how to download FastText word vectors for Latin and initialize them as a spaCy model. The command creates a blank spaCy pipeline with the specified vectors that can be loaded for semantic similarity comparisons.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_43\n\nLANGUAGE: bash\nCODE:\n```\n$ wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.la.300.vec.gz\n$ python -m spacy init vectors en cc.la.300.vec.gz /tmp/la_vectors_wiki_lg\n```\n\n----------------------------------------\n\nTITLE: Implementing Serialization Methods for Custom Components\nDESCRIPTION: Adds to_disk and from_disk methods to a custom component to enable serialization and deserialization of component data when saving or loading a pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.util import ensure_path\n\ndef to_disk(self, path, exclude=tuple()):\n    path = ensure_path(path)\n    if not path.exists():\n        path.mkdir()\n    snek_path = path / \"snek.txt\"\n    with snek_path.open(\"w\", encoding=\"utf8\") as snek_file:\n        snek_file.write(self.snek)\n\ndef from_disk(self, path, exclude=tuple()):\n    snek_path = path / \"snek.txt\"\n    with snek_path.open(\"r\", encoding=\"utf8\") as snek_file:\n        self.snek = snek_file.read()\n    return self\n```\n\n----------------------------------------\n\nTITLE: Initializing CuratedTransformer Component in spaCy\nDESCRIPTION: Examples of how to initialize the CuratedTransformer component in spaCy, including adding it to the pipeline with default or custom configurations, and direct construction from the class.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Construction via add_pipe with default model\ntrf = nlp.add_pipe(\"curated_transformer\")\n\n# Construction via add_pipe with custom config\nconfig = {\n    \"model\": {\n        \"@architectures\": \"spacy-curated-transformers.XlmrTransformer.v1\",\n        \"vocab_size\": 250002,\n        \"num_hidden_layers\": 12,\n        \"hidden_width\": 768,\n        \"piece_encoder\": {\n            \"@architectures\": \"spacy-curated-transformers.XlmrSentencepieceEncoder.v1\"\n        }\n    }\n}\ntrf = nlp.add_pipe(\"curated_transformer\", config=config)\n\n# Construction from class\nfrom spacy_curated_transformers import CuratedTransformer\ntrf = CuratedTransformer(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Adding Label to SpanCategorizer in Python\nDESCRIPTION: Example of adding a new label to the SpanCategorizer component. This is typically not needed if a representative data sample is provided during initialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\nspancat.add_label(\"MY_LABEL\")\n```\n\n----------------------------------------\n\nTITLE: Resizing Vector Arrays in spaCy\nDESCRIPTION: Demonstrates how to resize the vectors array to a new shape of 10000 rows and 300 dimensions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nremoved = nlp.vocab.vectors.resize((10000, 300))\n```\n\n----------------------------------------\n\nTITLE: REL Task Full Configuration\nDESCRIPTION: Complete configuration for REL task including few-shot example loading from a JSONL file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_44\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.REL.v1\"\nlabels = [\"LivesIn\", \"Visits\"]\n\n[components.llm.task.examples]\n@misc = \"spacy.FewShotReader.v1\"\npath = \"rel_examples.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy Model with Word Vectors\nDESCRIPTION: Command showing how to switch from a small spaCy model to a larger one that includes word vectors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_vectors-similarity.mdx#2025-04-21_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- python -m spacy download en_core_web_sm\n+ python -m spacy download en_core_web_lg\n```\n\n----------------------------------------\n\nTITLE: Applying SpanCategorizer to a Document in spaCy (Python)\nDESCRIPTION: Shows how to apply the SpanCategorizer to a single document. This method modifies the document in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nspancat = nlp.add_pipe(\"spancat\")\n# This usually happens under the hood\nprocessed = spancat(doc)\n```\n\n----------------------------------------\n\nTITLE: Processing a Document with SpanResolver\nDESCRIPTION: Example of how to directly apply the SpanResolver component to a document. This would typically happen automatically when processing text through the full NLP pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\n# This usually happens under the hood\nprocessed = span_resolver(doc)\n```\n\n----------------------------------------\n\nTITLE: Using PhraseMatcher for Terminology Lists in spaCy v2.0\nDESCRIPTION: Demonstrates the new PhraseMatcher which is more efficient for matching large terminology lists by accepting Doc objects as patterns instead of token rules.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_19\n\nLANGUAGE: diff\nCODE:\n```\n- matcher = Matcher(nlp.vocab)\n- matcher.add_entity(\"PRODUCT\")\n- for text in large_terminology_list\n-     matcher.add_pattern(\"PRODUCT\", [{ORTH: text}])\n\n+ from spacy.matcher import PhraseMatcher\n+ matcher = PhraseMatcher(nlp.vocab)\n+ patterns = [nlp.make_doc(text) for text in large_terminology_list]\n+ matcher.add(\"PRODUCT\", None, *patterns)\n```\n\n----------------------------------------\n\nTITLE: Using Scorer.score to evaluate Examples in spaCy\nDESCRIPTION: Calculates scores for a list of Example objects using scoring methods provided by pipeline components. Returns a dictionary containing various evaluation metrics for different NLP tasks.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/scorer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nscorer = Scorer()\nscores = scorer.score(examples)\n```\n\n----------------------------------------\n\nTITLE: Adding Entity IDs to EntityRuler Patterns\nDESCRIPTION: Demonstrates how to add entity IDs to patterns, allowing multiple patterns to be associated with the same entity, which is useful for recognizing different forms of the same entity.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\n\nnlp = English()\nruler = nlp.add_pipe(\"entity_ruler\")\npatterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}], \"id\": \"san-francisco\"},\n            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"fran\"}], \"id\": \"san-francisco\"}]\nruler.add_patterns(patterns)\n\ndoc1 = nlp(\"Apple is opening its first big office in San Francisco.\")\nprint([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])\n\ndoc2 = nlp(\"Apple is opening its first big office in San Fran.\")\nprint([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])\n```\n\n----------------------------------------\n\nTITLE: Predicting Sentence Boundaries with SentenceRecognizer in spaCy\nDESCRIPTION: Example of how to apply the SentenceRecognizer's model to a batch of documents to predict sentence boundaries without modifying the documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\nscores = senter.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Removing Pipeline Components in spaCy\nDESCRIPTION: Example of removing a component from the pipeline using remove_pipe method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nname, component = nlp.remove_pipe(\"parser\")\nassert name == \"parser\"\n```\n\n----------------------------------------\n\nTITLE: Component Configuration Example\nDESCRIPTION: Configuration example showing how to set up the relation extractor component in spaCy's config system.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_35\n\nLANGUAGE: ini\nCODE:\n```\n[components.relation_extractor]\nfactory = \"relation_extractor\"\n\n[components.relation_extractor.model]\n@architectures = \"rel_model.v1\"\n# ...\n\n[training.score_weights]\nrel_micro_p = 0.0\nrel_micro_r = 0.0\nrel_micro_f = 1.0\n```\n\n----------------------------------------\n\nTITLE: Analyzing Pipeline Dependencies with analyze_pipes in spaCy\nDESCRIPTION: This snippet shows adding an entity_linker component to the pipeline and then analyzing it with analyze_pipes() to identify missing dependencies. It demonstrates that the entity_linker requires entities and sentence boundaries that haven't been provided.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# This is a problem because it needs entities and sentence boundaries\nnlp.add_pipe(\"entity_linker\")\nanalysis = nlp.analyze_pipes(pretty=True)\n```\n\n----------------------------------------\n\nTITLE: Checking word existence in spaCy's Vocab\nDESCRIPTION: Shows how to check if a word exists in the vocabulary using the 'in' operator. This is useful for distinguishing between known and out-of-vocabulary words.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnlp(\"I'm eating an apple\")\napple = nlp.vocab.strings[\"apple\"]\noov = nlp.vocab.strings[\"dskfodkfos\"]\nassert apple in nlp.vocab\nassert oov not in nlp.vocab\n```\n\n----------------------------------------\n\nTITLE: Named Entity Recognition with Disabled Components\nDESCRIPTION: Example of processing texts while disabling unnecessary pipeline components for efficiency, focusing only on NER\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\ntexts = [\n    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n]\n\nnlp = spacy.load(\"en_core_web_sm\")\nfor doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n    # Do something with the doc here\n    print([(ent.text, ent.label_) for ent in doc.ents])\n```\n\n----------------------------------------\n\nTITLE: Implementing Tokenizer Customization Callback\nDESCRIPTION: Python function that customizes a tokenizer by modifying its suffixes and adding special cases, registered as a callback that can be referenced in the training config.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.util import registry, compile_suffix_regex\n\n@registry.callbacks(\"customize_tokenizer\")\ndef make_customize_tokenizer():\n    def customize_tokenizer(nlp):\n        # remove a suffix\n        suffixes = list(nlp.Defaults.suffixes)\n        suffixes.remove(\"\\\\[\")\n        suffix_regex = compile_suffix_regex(suffixes)\n        nlp.tokenizer.suffix_search = suffix_regex.search\n\n        # add a special case\n        nlp.tokenizer.add_special_case(\"_SPECIAL_\", [{\"ORTH\": \"_SPECIAL_\"}])\n    return customize_tokenizer\n```\n\n----------------------------------------\n\nTITLE: Managing spaCy Projects with CLI Commands\nDESCRIPTION: Example showing how to clone a project template, download assets and run workflows using spaCy's project management CLI commands.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Clone a project template\n$ python -m spacy project clone pipelines/tagger_parser_ud\n$ cd tagger_parser_ud\n# Download data assets\n$ python -m spacy project assets\n# Run a workflow\n$ python -m spacy project run all\n```\n\n----------------------------------------\n\nTITLE: Updating NER Model with Training Examples\nDESCRIPTION: Demonstrates how to train the NER model using Example objects and an optimizer. Updates the model's parameters based on training data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\noptimizer = nlp.initialize()\nlosses = ner.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Pipeline Directory Structure\nDESCRIPTION: YAML representation of the directory structure after serializing a spaCy pipeline with a custom component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n└── /path/to/pipeline\n    ├── my_component     # data serialized by \"my_component\"\n    │   └── data.json\n    ├── ner              # data for \"ner\" component\n    ├── parser           # data for \"parser\" component\n    ├── tagger           # data for \"tagger\" component\n    ├── vocab            # pipeline vocabulary\n    ├── meta.json        # pipeline meta.json\n    ├── config.cfg       # pipeline config\n    └── tokenizer        # tokenization rules\n```\n\n----------------------------------------\n\nTITLE: Accessing Named Entities within a Span in Python using spaCy\nDESCRIPTION: Demonstrates how to access named entities that fall completely within a Span. Returns a tuple of Span objects representing the entities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Mr. Best flew to New York on Saturday morning.\")\nspan = doc[0:6]\nents = list(span.ents)\nassert ents[0].label == 346\nassert ents[0].label_ == \"PERSON\"\nassert ents[0].text == \"Mr. Best\"\n```\n\n----------------------------------------\n\nTITLE: Applying CuratedTransformer to a Single Document\nDESCRIPTION: Example of how to apply the CuratedTransformer component to a single document. This method modifies the document in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\ntrf = nlp.add_pipe(\"curated_transformer\")\n# This usually happens under the hood\nprocessed = trf(doc)\n```\n\n----------------------------------------\n\nTITLE: Initializing Morphologizer in spaCy\nDESCRIPTION: Examples demonstrating different ways to initialize a morphologizer pipeline component, including via add_pipe with default model, with custom model configuration, or directly from the class.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\nmorphologizer = nlp.add_pipe(\"morphologizer\")\n\n# Construction via create_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_morphologizer\"}}\nmorphologizer = nlp.add_pipe(\"morphologizer\", config=config)\n\n# Construction from class\nfrom spacy.pipeline import Morphologizer\nmorphologizer = Morphologizer(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Setting Error Handler for spaCy's Language Processing (Python)\nDESCRIPTION: Shows how to set a custom error handler for the Language class. This handler will be called when an error occurs during the processing of documents in any pipeline component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef warn_error(proc_name, proc, docs, e):\n    print(f\"An error occurred when applying component {proc_name}.\")\n\nnlp.set_error_handler(warn_error)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dependency Parse with spaCy displacy\nDESCRIPTION: Uses spaCy's displacy visualizer to render the dependency parse for text. This helps in understanding the dependency structure for pattern creation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Smith founded a healthcare company\")\ndisplacy.serve(doc)\n```\n\n----------------------------------------\n\nTITLE: Configuring Named Entity Visualizer Options in spaCy displaCy\nDESCRIPTION: Example showing how to customize entity visualization by filtering displayed entity types to PERSON, ORG, and PRODUCT, while also setting a custom color for ORG entities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\noptions = {\"ents\": [\"PERSON\", \"ORG\", \"PRODUCT\"],\n           \"colors\": {\"ORG\": \"yellow\"}}\ndisplacy.serve(doc, style=\"ent\", options=options)\n```\n\n----------------------------------------\n\nTITLE: Fuzzy Matching in spaCy (Python)\nDESCRIPTION: Demonstrates the new fuzzy matching feature in spaCy, allowing for flexible pattern matching with specified edit distances.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-5.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Match lowercase with fuzzy matching (allows up to 3 edits)\npattern = [{\"LOWER\": {\"FUZZY\": \"definitely\"}}]\n\n# Match custom attribute values with fuzzy matching (allows up to 3 edits)\npattern = [{\"_\": {\"country\": {\"FUZZY\": \"Kyrgyzstan\"}}}]\n\n# Match with exact Levenshtein edit distance limits (allows up to 4 edits)\npattern = [{\"_\": {\"country\": {\"FUZZY4\": \"Kyrgyzstan\"}}}]\n```\n\n----------------------------------------\n\nTITLE: Setting a Custom Knowledge Base for EntityLinker in spaCy Python\nDESCRIPTION: Example of using the set_kb method to provide a custom knowledge base to the EntityLinker. The kb_loader function creates and configures a KnowledgeBase instance that will be used for entity linking.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef create_kb(vocab):\n    kb = InMemoryLookupKB(vocab, entity_vector_length=128)\n    kb.add_entity(...)\n    kb.add_alias(...)\n    return kb\nentity_linker = nlp.add_pipe(\"entity_linker\")\nentity_linker.set_kb(create_kb)\n```\n\n----------------------------------------\n\nTITLE: Scoring Examples with TrainablePipe in Python\nDESCRIPTION: Score a batch of examples using the TrainablePipe component. This method returns a dictionary of scores.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nscores = pipe.score(examples)\n```\n\n----------------------------------------\n\nTITLE: Adding Patterns with Callbacks to PhraseMatcher\nDESCRIPTION: Shows how to add patterns with custom callback functions that are triggered on matches. Includes multiple patterns and callback handling.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/phrasematcher.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef on_match(matcher, doc, id, matches):\n    print('Matched!', matches)\n\nmatcher = PhraseMatcher(nlp.vocab)\nmatcher.add(\"OBAMA\", [nlp(\"Barack Obama\")], on_match=on_match)\nmatcher.add(\"HEALTH\", [nlp(\"health care reform\"), nlp(\"healthcare reform\")], on_match=on_match)\ndoc = nlp(\"Barack Obama urges Congress to find courage to defend his healthcare reforms\")\nmatches = matcher(doc)\n```\n\n----------------------------------------\n\nTITLE: Processing Text with Context Using as_tuples\nDESCRIPTION: Demonstrates how to process texts while maintaining associated metadata using the as_tuples option and custom document extensions\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.tokens import Doc\n\nif not Doc.has_extension(\"text_id\"):\n    Doc.set_extension(\"text_id\", default=None)\n\ntext_tuples = [\n    (\"This is the first text.\", {\"text_id\": \"text1\"}),\n    (\"This is the second text.\", {\"text_id\": \"text2\"})\n]\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc_tuples = nlp.pipe(text_tuples, as_tuples=True)\n\ndocs = []\nfor doc, context in doc_tuples:\n    doc._.text_id = context[\"text_id\"]\n    docs.append(doc)\n\nfor doc in docs:\n    print(f\"{doc._.text_id}: {doc.text}\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Classification Layer for Relation Prediction\nDESCRIPTION: Implements a classification layer using a linear transformation followed by a logistic activation function. This layer processes the instance tensor and outputs relation predictions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n@spacy.registry.architectures(\"rel_classification_layer.v1\")\ndef create_classification_layer(\n    nO: int = None, nI: int = None\n) -> Model[Floats2d, Floats2d]:\n    return chain(Linear(nO=nO, nI=nI), Logistic())\n```\n\n----------------------------------------\n\nTITLE: Using displaCy.parse_deps for Manual Dependency Parsing\nDESCRIPTION: Example demonstrating how to generate dependency parse data in manual format for custom rendering.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a sentence.\")\ndeps_parse = displacy.parse_deps(doc)\nhtml = displacy.render(deps_parse, style=\"dep\", manual=True)\n```\n\n----------------------------------------\n\nTITLE: Accessing Span Vector in spaCy\nDESCRIPTION: Demonstrates how to access the vector representation of a span.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like apples\")\nassert doc[1:].vector.dtype == \"float32\"\nassert doc[1:].vector.shape == (300,)\n```\n\n----------------------------------------\n\nTITLE: Entity Pattern File Format in JSONL\nDESCRIPTION: Example of pattern definitions in JSONL format for EntityRuler, showing both exact string match and token-based patterns.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_41\n\nLANGUAGE: json\nCODE:\n```\n{\"label\": \"ORG\", \"pattern\": \"Apple\"}\n{\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}\n```\n\n----------------------------------------\n\nTITLE: Adding Pipeline Components in spaCy v2.0\nDESCRIPTION: Shows how to add built-in pipeline components using the new create_pipe method instead of directly importing and instantiating pipeline classes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_13\n\nLANGUAGE: diff\nCODE:\n```\n- from spacy.pipeline import Tagger\n- tagger = Tagger(nlp.vocab)\n- nlp.pipeline.insert(0, tagger)\n\n+ tagger = nlp.create_pipe(\"tagger\")\n+ nlp.add_pipe(tagger, first=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Individual Corpora in spaCy Project Configuration\nDESCRIPTION: Example of configuring multiple corpora in a spaCy project file. Defines train, dev, pretrain, and custom corpus readers with their respective paths and reader functions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[corpora]\n\n[corpora.train]\n@readers = \"spacy.Corpus.v1\"\npath = ${paths:train}\n\n[corpora.dev]\n@readers = \"spacy.Corpus.v1\"\npath = ${paths:dev}\n\n[corpora.pretrain]\n@readers = \"spacy.JsonlCorpus.v1\"\npath = ${paths.raw}\n\n[corpora.my_custom_data]\n@readers = \"my_custom_reader.v1\"\n```\n\n----------------------------------------\n\nTITLE: Counting Token Attribute Frequencies in spaCy Doc (Python)\nDESCRIPTION: Demonstrates how to count frequencies of a given attribute in a spaCy Doc object. The method returns a dictionary of attribute values and their counts.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs import ORTH\ndoc = nlp(\"apple apple orange banana\")\nassert doc.count_by(ORTH) == {7024: 1, 119552: 1, 2087: 2}\ndoc.to_array([ORTH])\n# array([[11880], [11880], [7561], [12800]])\n```\n\n----------------------------------------\n\nTITLE: Configuring Named Entity Recognition with Dolly Model\nDESCRIPTION: INI configuration for setting up a Named Entity Recognition pipeline using the Dolly model from Hugging Face in spaCy. It defines the pipeline, task, and model settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\nlang = \"en\"\npipeline = [\"llm\"]\n\n[components]\n\n[components.llm]\nfactory = \"llm\"\n\n[components.llm.task]\n@llm_tasks = \"spacy.NER.v3\"\nlabels = [\"PERSON\", \"ORGANISATION\", \"LOCATION\"]\n\n[components.llm.model]\n@llm_models = \"spacy.Dolly.v1\"\nname = \"dolly-v2-3b\"\n```\n\n----------------------------------------\n\nTITLE: Batch Exporting Multiple Dependency Parse SVGs\nDESCRIPTION: This complete example shows how to process multiple sentences, generate dependency parse visualizations for each one, and export them as separate SVG files with filenames derived from the sentence text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\nfrom pathlib import Path\n\nnlp = spacy.load(\"en_core_web_sm\")\nsentences = [\"This is an example.\", \"This is another one.\"]\nfor sent in sentences:\n    doc = nlp(sent)\n    svg = displacy.render(doc, style=\"dep\", jupyter=False)\n    file_name = '-'.join([w.text for w in doc if not w.is_punct]) + \".svg\"\n    output_path = Path(\"/images/\" + file_name)\n    output_path.open(\"w\", encoding=\"utf-8\").write(svg)\n```\n\n----------------------------------------\n\nTITLE: Serializing spaCy Doc Objects with Field Exclusion (Python)\nDESCRIPTION: Shows how to exclude specific fields when serializing a Doc object to bytes or disk. This example demonstrates excluding text and tensor fields during serialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndata = doc.to_bytes(exclude=[\"text\", \"tensor\"])\ndoc.from_disk(\"./doc.bin\", exclude=[\"user_data\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring StaticVectors Architecture in spaCy\nDESCRIPTION: Configuration for StaticVectors.v2 that embeds Doc objects using vocab's vectors table with learned linear projection. Includes output width, vector width, dropout and initialization parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_8\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.StaticVectors.v2\"\nnO = null\nnM = null\ndropout = 0.2\nkey_attr = \"ORTH\"\n\n[model.init_W]\n@initializers = \"glorot_uniform_init.v1\"\n```\n\n----------------------------------------\n\nTITLE: Serializing SpanGroup to Bytes in Python using spaCy\nDESCRIPTION: This snippet demonstrates how to serialize a SpanGroup object to a bytestring using the to_bytes() method. It creates a document, adds spans to it, and then serializes the span group.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spangroup.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Their goi ng home\")\ndoc.spans[\"errors\"] = [doc[0:1], doc[1:3]]\ngroup_bytes = doc.spans[\"errors\"].to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Configuring CharacterEmbed for Character-based Word Representation in spaCy\nDESCRIPTION: This configuration sets up the CharacterEmbed architecture in spaCy. It creates an embedded representation based on character embeddings using a feed-forward network, with parameters for controlling the embedding dimensions and character usage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.CharacterEmbed.v2\"\nwidth = 128\nrows = 7000\nnM = 64\nnC = 8\n```\n\n----------------------------------------\n\nTITLE: Configuring NGram Suggester\nDESCRIPTION: Configuration example for the NGram suggester that suggests spans of specified lengths.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_22\n\nLANGUAGE: ini\nCODE:\n```\n[components.spancat.suggester]\n@misc = \"spacy.ngram_suggester.v1\"\nsizes = [1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Vector by Key in Python\nDESCRIPTION: Method to get a vector by its key. Raises a KeyError if the key is not found in the table.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef __getitem__(self, key: Union[int, str]) -> numpy.ndarray[ndim=1, dtype=float32]:\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Vectors in spaCy INI Format\nDESCRIPTION: Example configuration snippet showing how to specify a custom vectors implementation in a spaCy config file using the INI format. This enables the use of BPEmb vectors for English in a pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_16\n\nLANGUAGE: ini\nCODE:\n```\n[nlp.vectors]\n@vectors = \"BPEmbVectors.v1\"\nlang = \"en\"\n```\n\n----------------------------------------\n\nTITLE: Creating Stateful Class Components in Python\nDESCRIPTION: Demonstrates how to implement a stateful class-based pipeline component using the @Language.factory decorator. The component requires nlp and name parameters for initialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\n@Language.factory(\"my_component\")\nclass MyComponent:\n    def __init__(self, nlp, name):\n        self.nlp = nlp\n\n    def __call__(self, doc):\n        return doc\n```\n\n----------------------------------------\n\nTITLE: Saving spaCy NLP Object to Disk\nDESCRIPTION: Illustrates how to save a spaCy NLP object to disk using the to_disk() method. This method allows persisting the entire NLP pipeline to a specified file path.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_serialization.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnlp.to_disk(\"/path\")\n```\n\n----------------------------------------\n\nTITLE: Merging Tokens with Custom Extensions in spaCy\nDESCRIPTION: Demonstrates how to merge tokens (\"David Bowie\") while setting a custom extension attribute (is_musician). First registers a custom token extension with a default value, then uses the retokenizer to merge tokens and update the extension value.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.tokens import Token\n\n# Register a custom token attribute, token._.is_musician\nToken.set_extension(\"is_musician\", default=False)\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"I like David Bowie\")\nprint(\"Before:\", [(token.text, token._.is_musician) for token in doc])\n\nwith doc.retokenize() as retokenizer:\n    retokenizer.merge(doc[2:4], attrs={\"_\": {\"is_musician\": True}})\nprint(\"After:\", [(token.text, token._.is_musician) for token in doc])\n```\n\n----------------------------------------\n\nTITLE: Using EntityRuler for Rule-Based NER in Python\nDESCRIPTION: Example of adding named entities based on pattern dictionaries using the new EntityRuler component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-1.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline import EntityRuler\nruler = EntityRuler(nlp)\nruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\nnlp.add_pipe(ruler, before=\"ner\")\n```\n\n----------------------------------------\n\nTITLE: Defining Corpora in spaCy Configuration File\nDESCRIPTION: This INI configuration snippet shows how to define a training corpus in a spaCy config file. It specifies the corpus reader, file path, and processing parameters like maximum document length and preprocessing options.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_34\n\nLANGUAGE: ini\nCODE:\n```\n[corpora]\n\n[corpora.train]\n@readers = \"spacy.Corpus.v1\"\npath = ${paths.train}\ngold_preproc = false\nmax_length = 0\nlimit = 0\naugmenter = null\n\n[training]\ntrain_corpus = \"corpora.train\"\n```\n\n----------------------------------------\n\nTITLE: Processing Document Stream with SpanFinder in Python\nDESCRIPTION: Example demonstrating how to process a stream of documents using the SpanFinder pipe method with batching.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nspan_finder = nlp.add_pipe(\"span_finder\")\nfor doc in span_finder.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Documents with EntityRecognizer\nDESCRIPTION: Example demonstrating how to process multiple documents in batches using the EntityRecognizer's pipe method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\nfor doc in ner.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring SpaCy LLM Cache\nDESCRIPTION: Configuration for SpaCy's BatchCache component that stores document batches on disk to avoid reprocessing the same documents multiple times. Includes settings for cache path, batch size, and memory management.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_63\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.cache]\n@llm_misc = \"spacy.BatchCache.v1\"\npath = \"path/to/cache\"\nbatch_size = 64\nmax_batches_in_mem = 4\n```\n\n----------------------------------------\n\nTITLE: Advanced Token Pattern Matching in spaCy\nDESCRIPTION: Example of more complex token patterns using list membership and numeric comparisons. It matches lemmas from a list followed by a proper noun with length >= 10.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/matcher.mdx#2025-04-21_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\"LEMMA\": {\"IN\": [\"like\", \"love\", \"enjoy\"]}},\n  {\"POS\": \"PROPN\", \"LENGTH\": {\">=\": 10}},\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCatParametricAttention Neural Network in spaCy\nDESCRIPTION: Configuration for the TextCatParametricAttention architecture which uses a neural network with parametric attention to focus on tokens relevant to text classification. The model is built on top of a configurable Tok2Vec layer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_26\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.TextCatParametricAttention.v1\"\nexclusive_classes = true\nnO = null\n\n[model.tok2vec]\n@architectures = \"spacy.Tok2Vec.v2\"\n\n[model.tok2vec.embed]\n@architectures = \"spacy.MultiHashEmbed.v2\"\nwidth = 64\nrows = [2000, 2000, 1000, 1000, 1000, 1000]\nattrs = [\"ORTH\", \"LOWER\", \"PREFIX\", \"SUFFIX\", \"SHAPE\", \"ID\"]\ninclude_static_vectors = false\n\n[model.tok2vec.encode]\n@architectures = \"spacy.MaxoutWindowEncoder.v2\"\nwidth = ${model.tok2vec.embed.width}\nwindow_size = 1\nmaxout_pieces = 3\ndepth = 2\n```\n\n----------------------------------------\n\nTITLE: Configuring Tagger in spaCy\nDESCRIPTION: Example configuration for spaCy's Tagger.v2 architecture, which adds a linear layer with softmax activation to predict POS tags. The configuration includes options for the output size and probability normalization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_23\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.Tagger.v2\"\nnO = null\nnormalize = false\n\n[model.tok2vec]\n# ...\n```\n\n----------------------------------------\n\nTITLE: Pretraining spaCy Models with Raw Text (Bash)\nDESCRIPTION: Example of using the new spacy pretrain CLI command to initialize models with information from raw text, similar to BERT pretraining.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-1.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy pretrain ./raw_text.jsonl\\nen_core_web_lg ./pretrained-model\n```\n\n----------------------------------------\n\nTITLE: Computing Token Similarity - Python\nDESCRIPTION: Demonstrates how to compute semantic similarity between tokens using vectors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\napples, _, oranges = nlp(\"apples and oranges\")\napples_oranges = apples.similarity(oranges)\noranges_apples = oranges.similarity(apples)\nassert apples_oranges == oranges_apples\n```\n\n----------------------------------------\n\nTITLE: Creating Example from Dictionary in spaCy (Python)\nDESCRIPTION: Demonstrates creating an Example object using the from_dict classmethod with predicted document and reference annotations dictionary.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc\nfrom spacy.training import Example\n\npredicted = Doc(vocab, words=[\"Apply\", \"some\", \"sunscreen\"])\ntoken_ref = [\"Apply\", \"some\", \"sun\", \"screen\"]\ntags_ref = [\"VERB\", \"DET\", \"NOUN\", \"NOUN\"]\nexample = Example.from_dict(predicted, {\"words\": token_ref, \"tags\": tags_ref})\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy KB Object Loader\nDESCRIPTION: Configuration example for the spacy.KBObjectLoader.v1 showing paths setup for knowledge base, NLP pipeline, and entity descriptions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_21\n\nLANGUAGE: ini\nCODE:\n```\n[initialize.components.llm.candidate_selector.kb_loader]\n@llm_misc = \"spacy.KBObjectLoader.v1\"\n# Path to knowledge base directory in serialized spaCy pipeline.\npath = ${paths.el_kb}\n# Path to spaCy pipeline. If this is not specified, spacy-llm tries to determine this automatically (but may fail).\nnlp_path = ${paths.el_nlp}\n# Path to file with descriptions for entity.\ndesc_path = ${paths.el_desc}\n```\n\n----------------------------------------\n\nTITLE: Adding Person Title as Custom Attribute in spaCy (Python)\nDESCRIPTION: This snippet defines a function to get a person's title and adds it as a custom extension attribute to spaCy's Span objects. It allows accessing the title without modifying the original entity span.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.tokens import Span\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef get_person_title(span):\n    if span.label_ == \"PERSON\" and span.start != 0:\n        prev_token = span.doc[span.start - 1]\n        if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n            return prev_token.text\n\n# Register the Span extension as 'person_title'\nSpan.set_extension(\"person_title\", getter=get_person_title)\n\ndoc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\nprint([(ent.text, ent.label_, ent._.person_title) for ent in doc.ents])\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Pipeline Components in spaCy v2.0\nDESCRIPTION: Demonstrates how to add custom pipeline components that automatically assign metadata to Doc objects during processing instead of doing it separately after parsing.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_16\n\nLANGUAGE: diff\nCODE:\n```\n- doc = nlp(\"Doc with a standard pipeline\")\n- meta = get_meta(doc)\n\n+ nlp.add_pipe(meta_component)\n+ doc = nlp(\"Doc with a custom pipeline that assigns meta\")\n+ meta = doc._.meta\n```\n\n----------------------------------------\n\nTITLE: Applying Trained spaCy Pipeline to Data\nDESCRIPTION: Command to apply a trained spaCy pipeline to input data (spaCy binary format, JSONL, or plain text) and store the resulting annotated documents in a DocBin file. Supports processing single files or entire directories.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy apply [model] [data-path] [output-file] [--code] [--text-key] [--force-overwrite] [--gpu-id] [--batch-size] [--n-process]\n```\n\n----------------------------------------\n\nTITLE: Rendering Raw Entity Data with displaCy\nDESCRIPTION: Example of directly providing raw entity annotation data to displaCy in the required format. This demonstrates how to structure entity data with text, entity positions, and labels for manual rendering.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nex = [{\"text\": \"But Google is starting from behind.\",\n       \"ents\": [{\"start\": 4, \"end\": 10, \"label\": \"ORG\"}],\n       \"title\": None}]\nhtml = displacy.render(ex, style=\"ent\", manual=True)\n```\n\n----------------------------------------\n\nTITLE: Obtaining Vector Norms of Lexemes in spaCy\nDESCRIPTION: Example of how to get the L2 norm of lexeme word vectors in spaCy. The example compares vector norms between different lexemes ('apple' and 'pasta') to show they have different values.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lexeme.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\napple = nlp.vocab[\"apple\"]\npasta = nlp.vocab[\"pasta\"]\napple.vector_norm  # 7.1346845626831055\npasta.vector_norm  # 7.759851932525635\nassert apple.vector_norm != pasta.vector_norm\n```\n\n----------------------------------------\n\nTITLE: Creating Byte-BPE Encoder in Python\nDESCRIPTION: Constructs a Byte-BPE piece encoder model that processes token sequences or documents and returns corresponding piece identifiers. This model also requires separate initialization using an appropriate loader.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nspacy-curated-transformers.ByteBpeEncoder.v1\n```\n\n----------------------------------------\n\nTITLE: Accessing Entity Identifiers in spaCy's Entity Linking\nDESCRIPTION: This example shows how to access entity identifiers after performing entity linking in spaCy. It demonstrates accessing KB identifiers at both the document and token levels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\n# \"my_custom_el_pipeline\" is assumed to be a custom NLP pipeline that was trained and serialized to disk\nnlp = spacy.load(\"my_custom_el_pipeline\")\ndoc = nlp(\"Ada Lovelace was born in London\")\n\n# Document level\nents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]\nprint(ents)  # [('Ada Lovelace', 'PERSON', 'Q7259'), ('London', 'GPE', 'Q84')]\n\n# Token level\nent_ada_0 = [doc[0].text, doc[0].ent_type_, doc[0].ent_kb_id_]\nent_ada_1 = [doc[1].text, doc[1].ent_type_, doc[1].ent_kb_id_]\nent_london_5 = [doc[5].text, doc[5].ent_type_, doc[5].ent_kb_id_]\nprint(ent_ada_0)  # ['Ada', 'PERSON', 'Q7259']\nprint(ent_ada_1)  # ['Lovelace', 'PERSON', 'Q7259']\nprint(ent_london_5)  # ['London', 'GPE', 'Q84']\n```\n\n----------------------------------------\n\nTITLE: Predicting with SpanCategorizer in Python\nDESCRIPTION: Example of using the SpanCategorizer's predict method to apply the model to a batch of Doc objects without modifying them.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\nscores = spancat.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Using DependencyMatcher with Semgrex Operators in spaCy\nDESCRIPTION: Example of creating a DependencyMatcher pattern that matches a subject followed by 'initially founded'. The pattern demonstrates defining an anchor token and specifying relations between tokens in the dependency parse tree.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.matcher import DependencyMatcher\n\n# \"[subject] ... initially founded\"\npattern = [\n  # anchor token: founded\n  {\n    \"RIGHT_ID\": \"founded\",\n    \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n  },\n  # founded -> subject\n  {\n    \"LEFT_ID\": \"founded\",\n    \"REL_OP\": \">\",\n    \"RIGHT_ID\": \"subject\",\n    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}\n  },\n  # \"founded\" follows \"initially\"\n  {\n    \"LEFT_ID\": \"founded\",\n    \"REL_OP\": \";\",\n    \"RIGHT_ID\": \"initially\",\n    \"RIGHT_ATTRS\": {\"ORTH\": \"initially\"}\n  }\n]\n\nmatcher = DependencyMatcher(nlp.vocab)\nmatcher.add(\"FOUNDED\", [pattern])\nmatches = matcher(doc)\n```\n\n----------------------------------------\n\nTITLE: Applying TextCategorizer to a Single Document in Python\nDESCRIPTION: Example of applying the TextCategorizer to a single document using the __call__ method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\ntextcat = nlp.add_pipe(\"textcat\")\n# This usually happens under the hood\nprocessed = textcat(doc)\n```\n\n----------------------------------------\n\nTITLE: Initializing spaCy Tagger Component\nDESCRIPTION: Examples demonstrating different ways to initialize the tagger component: via add_pipe with default model, with custom model configuration, and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\ntagger = nlp.add_pipe(\"tagger\")\n\n# Construction via create_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_tagger\"}}\ntagger = nlp.add_pipe(\"tagger\", config=config)\n\n# Construction from class\nfrom spacy.pipeline import Tagger\ntagger = Tagger(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Configuring Frozen Components and Listener Replacement (INI)\nDESCRIPTION: Configuration excerpt showing how to specify frozen components during training and replace listeners for a component sourced from an existing model. This is used in spaCy's config.cfg file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_28\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg (excerpt)\n[training]\nfrozen_components = [\"tagger\"]\n\n[components]\n\n[components.tagger]\nsource = \"en_core_web_sm\"\nreplace_listeners = [\"model.tok2vec\"]\n```\n\n----------------------------------------\n\nTITLE: Creating a Span from Character Indices in Python using spaCy\nDESCRIPTION: Demonstrates how to create a Span object from character indices of a Doc object. The method returns None if the indices don't map to a valid span.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York\")\nspan = doc[1:4].char_span(5, 13, label=\"GPE\")\nassert span.text == \"New York\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanCategorizer for Single-label Classification in Python\nDESCRIPTION: Example configuration for the 'spancat_singlelabel' component in spaCy, which performs single-label classification on spans. It sets up parameters like spans_key, model, suggester function, and additional single-label specific options.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.spancat import DEFAULT_SPANCAT_SINGLELABEL_MODEL\nconfig = {\n    \"spans_key\": \"labeled_spans\",\n    \"model\": DEFAULT_SPANCAT_SINGLELABEL_MODEL,\n    \"suggester\": {\"@misc\": \"spacy.ngram_suggester.v1\", \"sizes\": [1, 2, 3]},\n    # Additional spancat_singlelabel parameters\n    \"negative_weight\": 0.8,\n    \"allow_overlap\": True,\n}\nnlp.add_pipe(\"spancat_singlelabel\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss for TextCategorizer in spaCy\nDESCRIPTION: Demonstrates how to calculate the loss and gradient of loss for a batch of documents and their predicted scores.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\nscores = textcat.predict([eg.predicted for eg in examples])\nloss, d_loss = textcat.get_loss(examples, scores)\n```\n\n----------------------------------------\n\nTITLE: Iterating Over MorphAnalysis Features in Python\nDESCRIPTION: Shows how to iterate over all feature/value pairs in a MorphAnalysis object using the __iter__ method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfeats = \"Feat1=Val1,Val3|Feat2=Val2\"\nmorph = MorphAnalysis(nlp.vocab, feats)\nassert list(morph) == [\"Feat1=Va1\", \"Feat1=Val3\", \"Feat2=Val2\"]\n```\n\n----------------------------------------\n\nTITLE: Splitting Tokens in a spaCy Doc in Python\nDESCRIPTION: Illustrates how to split tokens and specify heads and attributes using the Retokenizer.split() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I live in NewYork\")\nwith doc.retokenize() as retokenizer:\n    heads = [(doc[3], 1), doc[2]]\n    attrs = {\"POS\": [\"PROPN\", \"PROPN\"],\n             \"DEP\": [\"pobj\", \"compound\"]}\n    retokenizer.split(doc[3], [\"New\", \"York\"], heads=heads, attrs=attrs)\n```\n\n----------------------------------------\n\nTITLE: Using Transformer Component\nDESCRIPTION: Example showing how to process a document using the transformer component in a spaCy pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\ntrf = nlp.add_pipe(\"transformer\")\n# This usually happens under the hood\nprocessed = transformer(doc)\n```\n\n----------------------------------------\n\nTITLE: Initializing SpanRuler in Python\nDESCRIPTION: Example of initializing a SpanRuler component and adding it to the spaCy pipeline. It demonstrates how to use the initialize method with custom patterns.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspan_ruler = nlp.add_pipe(\"span_ruler\")\nspan_ruler.initialize(lambda: [], nlp=nlp, patterns=patterns)\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCategorizer for Single-Label Classification in Python\nDESCRIPTION: Example of configuring the TextCategorizer for single-label classification using the default model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL\nconfig = {\n   \"model\": DEFAULT_SINGLE_TEXTCAT_MODEL,\n}\nnlp.add_pipe(\"textcat\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Default spaCy Pretraining Configuration\nDESCRIPTION: The default configuration for spaCy's pretraining functionality, included as a reference. This shows the standard settings for corpus loading, batching, and optimization that are used during pretraining.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_19\n\nLANGUAGE: ini\nCODE:\n```\n%%GITHUB_SPACY/spacy/default_config_pretraining.cfg\n```\n\n----------------------------------------\n\nTITLE: Initializing a Custom KnowledgeBase in Python\nDESCRIPTION: Example of creating a custom KnowledgeBase subclass and initializing it with a vocabulary and entity vector length.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/kb.mdx#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom spacy.kb import KnowledgeBase\nfrom spacy.vocab import Vocab\n\nclass FullyImplementedKB(KnowledgeBase):\n  def __init__(self, vocab: Vocab, entity_vector_length: int):\n      super().__init__(vocab, entity_vector_length)\n      ...\nvocab = nlp.vocab\nkb = FullyImplementedKB(vocab=vocab, entity_vector_length=64)\n```\n\n----------------------------------------\n\nTITLE: Accessing SpanGroup Properties in Python\nDESCRIPTION: Demonstrates how to access the document reference and check for overlapping spans in a SpanGroup.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spangroup.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Their goi ng home\")\ndoc.spans[\"errors\"] = [doc[0:1], doc[1:3]]\nassert doc.spans[\"errors\"].doc == doc\nassert not doc.spans[\"errors\"].has_overlap\ndoc.spans[\"errors\"].append(doc[2:4])\nassert doc.spans[\"errors\"].has_overlap\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy in Editable Mode for Developers in Bash\nDESCRIPTION: Commands for installing spaCy in editable mode for developers. This allows changes to .py files to be reflected immediately, while changes to Cython files require re-running the installation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -r requirements.txt\n$ pip install --no-build-isolation --editable .\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCatEnsemble for Text Classification in spaCy\nDESCRIPTION: Configuration for the TextCatEnsemble architecture which stacks a linear bag-of-words model with a neural network model. The model uses a Tok2Vec layer with attention and can handle multi-label or single-label classification.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_24\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.TextCatEnsemble.v2\"\nnO = null\n\n[model.linear_model]\n@architectures = \"spacy.TextCatBOW.v3\"\nexclusive_classes = true\nlength = 262144\nngram_size = 1\nno_output_layer = false\n\n[model.tok2vec]\n@architectures = \"spacy.Tok2Vec.v2\"\n\n[model.tok2vec.embed]\n@architectures = \"spacy.MultiHashEmbed.v2\"\nwidth = 64\nrows = [2000, 2000, 1000, 1000, 1000, 1000]\nattrs = [\"ORTH\", \"LOWER\", \"PREFIX\", \"SUFFIX\", \"SHAPE\", \"ID\"]\ninclude_static_vectors = false\n\n[model.tok2vec.encode]\n@architectures = \"spacy.MaxoutWindowEncoder.v2\"\nwidth = ${model.tok2vec.embed.width}\nwindow_size = 1\nmaxout_pieces = 3\ndepth = 2\n```\n\n----------------------------------------\n\nTITLE: Setting Annotations with Transformer Component in Python\nDESCRIPTION: This example shows how to assign extracted features to Doc objects using the set_annotations method after prediction.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"transformer\")\nscores = trf.predict(docs)\ntrf.set_annotations(docs, scores)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Batch Size Schedule in Python\nDESCRIPTION: Defines a custom batch size scheduling function that increases batch size gradually during training based on a starting value and growth factor.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\n@spacy.registry.schedules(\"my_custom_schedule.v1\")\ndef my_custom_schedule(start: int = 1, factor: float = 1.001):\n   while True:\n      yield start\n      start = start * factor\n```\n\nLANGUAGE: ini\nCODE:\n```\n[training.batcher.size]\n@schedules = \"my_custom_schedule.v1\"\nstart = 2\nfactor = 1.005\n```\n\n----------------------------------------\n\nTITLE: Processing Single Document with Tok2Vec\nDESCRIPTION: Example of processing a single document with the Tok2Vec component, which adds context-sensitive embeddings to Doc.tensor.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\ntok2vec = nlp.add_pipe(\"tok2vec\")\n# This usually happens under the hood\nprocessed = tok2vec(doc)\n```\n\n----------------------------------------\n\nTITLE: Manipulating SpanGroup in Python\nDESCRIPTION: Shows how to get the length of a SpanGroup, access individual spans, set new spans, delete spans, and concatenate SpanGroups.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spangroup.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Their goi ng home\")\ndoc.spans[\"errors\"] = [doc[0:1], doc[1:3]]\nassert len(doc.spans[\"errors\"]) == 2\nspan = doc.spans[\"errors\"][1]\nassert span.text == \"goi ng\"\ndoc.spans[\"errors\"][0] = doc[0:2]\nassert doc.spans[\"errors\"][0].text == \"Their goi\"\ndel doc.spans[\"errors\"][0]\nassert len(doc.spans[\"errors\"]) == 1\ndoc.spans[\"other\"] = [doc[0:2], doc[2:4]]\nspan_group = doc.spans[\"errors\"] + doc.spans[\"other\"]\nassert len(span_group) == 3\n```\n\n----------------------------------------\n\nTITLE: Exporting Span Attributes to NumPy Array in Python using spaCy\nDESCRIPTION: Shows how to export token attributes of a Span to a NumPy array. The method accepts a list of attribute IDs and returns a 2D array of integer values.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\ndoc = nlp(\"I like New York in Autumn.\")\nspan = doc[2:3]\n# All strings mapped to integers, for easy export to numpy\nnp_array = span.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n```\n\n----------------------------------------\n\nTITLE: Evaluating tokenization accuracy in spaCy\nDESCRIPTION: Uses the static method score_tokenization to evaluate tokenization quality, calculating token accuracy, precision, recall, and F-score for token character spans. Documents with unknown spaces are skipped.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/scorer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nscores = Scorer.score_tokenization(examples)\n```\n\n----------------------------------------\n\nTITLE: Running Project Commands in spaCy\nDESCRIPTION: Command to execute named commands or workflows defined in project.yml with support for dependency tracking and conditional re-running based on state changes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_46\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project run [subcommand] [project_dir] [--force] [--dry]\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project run train\n```\n\n----------------------------------------\n\nTITLE: Converting JSON Training Data to Binary .spacy Format\nDESCRIPTION: Command for converting existing JSON-formatted training data to the new binary .spacy format using the spacy convert command, which creates more efficient storage for training documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy convert ./training.json ./output\n```\n\n----------------------------------------\n\nTITLE: Adding Entities to Knowledge Base\nDESCRIPTION: Demonstrates adding individual entities to the knowledge base with their frequency and vector representations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/inmemorylookupkb.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nkb.add_entity(entity=\"Q42\", freq=32, entity_vector=vector1)\nkb.add_entity(entity=\"Q463035\", freq=111, entity_vector=vector2)\n```\n\n----------------------------------------\n\nTITLE: Accessing Right Dependencies of Span in spaCy\nDESCRIPTION: Shows how to access tokens to the right of a span whose heads are within the span.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York in Autumn.\")\nrights = [t.text for t in doc[2:4].rights]\nassert rights == [\"in\"]\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parameters in TrainablePipe in Python\nDESCRIPTION: Temporarily modify the pipe's model to use given parameter values. Original parameters are restored at the end of the context.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\nwith pipe.use_params(optimizer.averages):\n    pipe.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Token Children in spaCy\nDESCRIPTION: Demonstrates accessing immediate syntactic children of a token in the dependency parse tree.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\ngive_children = doc[0].children\nassert [t.text for t in give_children] == [\"it\", \"back\", \"!\"]\n```\n\n----------------------------------------\n\nTITLE: Using Temporary Parameters for Tok2Vec Component in Python\nDESCRIPTION: Shows how to temporarily modify the Tok2Vec component's parameters using a context manager. This is useful for operations like saving the best model during training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntok2vec = nlp.add_pipe(\"tok2vec\")\nwith tok2vec.use_params(optimizer.averages):\n    tok2vec.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Computing Loss for EditTreeLemmatizer\nDESCRIPTION: Example demonstrating how to calculate loss and gradient for model predictions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\nscores = lemmatizer.model.begin_update([eg.predicted for eg in examples])\nloss, d_loss = lemmatizer.get_loss(examples, scores)\n```\n\n----------------------------------------\n\nTITLE: Predicting Text Categories with spaCy\nDESCRIPTION: Demonstrates how to apply the text categorizer model to documents for prediction without modifying them.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\nscores = textcat.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Using SpanRuler in spaCy Pipeline\nDESCRIPTION: Demonstrates how to use the SpanRuler in a spaCy pipeline to identify and label spans in a document.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"span_ruler\")\nruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\n\ndoc = nlp(\"A text about Apple.\")\nspans = [(span.text, span.label_) for span in doc.spans[\"ruler\"]]\nassert spans == [(\"Apple\", \"ORG\")]\n```\n\n----------------------------------------\n\nTITLE: Processing Single Document with SpanFinder in Python\nDESCRIPTION: Example showing how to process a single document using the SpanFinder component directly.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nspan_finder = nlp.add_pipe(\"span_finder\")\n# This usually happens under the hood\nprocessed = span_finder(doc)\n```\n\n----------------------------------------\n\nTITLE: Configuring Chinese Language Processing with pkuseg\nDESCRIPTION: Example showing how to initialize Chinese language processing with pkuseg tokenizer and update the user dictionary.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-3.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.zh import Chinese\n\n# Load with \"default\" model provided by pkuseg\ncfg = {\"pkuseg_model\": \"default\", \"require_pkuseg\": True}\nnlp = Chinese(meta={\"tokenizer\": {\"config\": cfg}})\n\n# Append words to user dict\nnlp.tokenizer.pkuseg_update_user_dict([\"中国\", \"ABC\"])\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Dependency Matcher Pattern in spaCy\nDESCRIPTION: Demonstrates the simplest dependency matcher pattern that identifies and names a single token in the dependency tree. The pattern matches the token 'founded'.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.matcher import DependencyMatcher\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = DependencyMatcher(nlp.vocab)\npattern = [\n  {\n    \"RIGHT_ID\": \"anchor_founded\",       # unique name\n    \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}  # token pattern for \"founded\"\n  }\n]\nmatcher.add(\"FOUNDED\", [pattern])\ndoc = nlp(\"Smith founded two companies.\")\nmatches = matcher(doc)\nprint(matches) # [(4851363122962674176, [1])]\n```\n\n----------------------------------------\n\nTITLE: Initializing tok2vec Component with Pretrained Weights in spaCy Configuration\nDESCRIPTION: Configuration snippet showing how to specify the path to a pretrained model file (.bin) to initialize the tok2vec layer in a spaCy pipeline. The configuration uses paths.init_tok2vec to define the location of the pretrained weights file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_21\n\nLANGUAGE: ini\nCODE:\n```\n[paths]\ninit_tok2vec = \"pretrain/model-last.bin\"\n\n[initialize]\ninit_tok2vec = ${paths.init_tok2vec}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Pipe Metadata with get_pipe_meta in Python\nDESCRIPTION: Demonstrates the use of the get_pipe_meta method to retrieve metadata for a specific pipeline component instance. The method returns a FactoryMeta object containing information about the component and its default configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nnlp.add_pipe(\"ner\", name=\"entity_recognizer\")\nfactory_meta = nlp.get_pipe_meta(\"entity_recognizer\")\nassert factory_meta.factory == \"ner\"\nprint(factory_meta.default_config)\n```\n\n----------------------------------------\n\nTITLE: Accessing Left Children of Token in spaCy\nDESCRIPTION: Shows how to access the leftward immediate children of a token in the syntactic dependency parse.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York in Autumn.\")\nlefts = [t.text for t in doc[3].lefts]\nassert lefts == [\"New\"]\n```\n\n----------------------------------------\n\nTITLE: spaCy NER Training Output Log\nDESCRIPTION: Sample output showing the training progress of a Named Entity Recognition model, including loss metrics and entity scoring across training iterations.\nSOURCE: https://github.com/explosion/spacy/blob/master/examples/README.md#2025-04-21_snippet_1\n\nLANGUAGE: none\nCODE:\n```\nℹ Running workflow 'all'\n\n================================== convert ==================================\nRunning command: /home/user/venv/bin/python scripts/convert.py en assets/train.json corpus/train.spacy\nRunning command: /home/user/venv/bin/python scripts/convert.py en assets/dev.json corpus/dev.spacy\n\n=============================== create-config ===============================\nRunning command: /home/user/venv/bin/python -m spacy init config --lang en --pipeline ner configs/config.cfg --force\nℹ Generated config template specific for your use case\n- Language: en\n- Pipeline: ner\n- Optimize for: efficiency\n- Hardware: CPU\n- Transformer: None\n✔ Auto-filled config with all values\n✔ Saved config\nconfigs/config.cfg\nYou can now add your data and train your pipeline:\npython -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n\n=================================== train ===================================\nRunning command: /home/user/venv/bin/python -m spacy train configs/config.cfg --output training/ --paths.train corpus/train.spacy --paths.dev corpus/dev.spacy --training.eval_frequency 10 --training.max_steps 100 --gpu-id -1\nℹ Using CPU\n\n=========================== Initializing pipeline ===========================\n[2021-03-11 19:34:59,101] [INFO] Set up nlp object from config\n[2021-03-11 19:34:59,109] [INFO] Pipeline: ['tok2vec', 'ner']\n[2021-03-11 19:34:59,113] [INFO] Created vocabulary\n[2021-03-11 19:34:59,113] [INFO] Finished initializing nlp object\n[2021-03-11 19:34:59,265] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n✔ Initialized pipeline\n\n============================= Training pipeline =============================\nℹ Pipeline: ['tok2vec', 'ner']\nℹ Initial learn rate: 0.001\nE    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n---  ------  ------------  --------  ------  ------  ------  ------\n  0       0          0.00      7.90    0.00    0.00    0.00    0.00\n 10      10          0.11     71.07    0.00    0.00    0.00    0.00\n 20      20          0.65     22.44   50.00   50.00   50.00    0.50\n 30      30          0.22      6.38   80.00   66.67  100.00    0.80\n 40      40          0.00      0.00   80.00   66.67  100.00    0.80\n 50      50          0.00      0.00   80.00   66.67  100.00    0.80\n 60      60          0.00      0.00  100.00  100.00  100.00    1.00\n 70      70          0.00      0.00  100.00  100.00  100.00    1.00\n 80      80          0.00      0.00  100.00  100.00  100.00    1.00\n 90      90          0.00      0.00  100.00  100.00  100.00    1.00\n100     100          0.00      0.00  100.00  100.00  100.00    1.00\n✔ Saved pipeline to output directory\ntraining/model-last\n```\n\n----------------------------------------\n\nTITLE: Creating Example object from dictionary for spaCy training\nDESCRIPTION: Code snippet showing how to create an Example object from a dictionary of gold-standard annotations and a Doc object for training spaCy models.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nexample = Example.from_dict(doc, gold_dict)\n```\n\n----------------------------------------\n\nTITLE: Initializing SpanCategorizer in spaCy (Python)\nDESCRIPTION: Demonstrates different ways to initialize the SpanCategorizer component in spaCy, including via add_pipe with default and custom models, and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\n# Replace 'spancat' with 'spancat_singlelabel' for exclusive classes\nspancat = nlp.add_pipe(\"spancat\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_spancat\"}}\nspancat = nlp.add_pipe(\"spancat\", config=config)\n\n# Construction from class\nfrom spacy.pipeline import SpanCategorizer\nspancat = SpanCategorizer(nlp.vocab, model, suggester)\n```\n\n----------------------------------------\n\nTITLE: Training Text Classification Model\nDESCRIPTION: Example demonstrating how to create, train and use a text categorization model in spaCy's pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.create_pipe(\"textcat\")\nnlp.add_pipe(textcat, last=True)\nnlp.begin_training()\nfor itn in range(100):\n   for doc, gold in train_data:\n       nlp.update([doc], [gold])\ndoc = nlp(\"This is a text.\")\nprint(doc.cats)\n```\n\n----------------------------------------\n\nTITLE: Configuring Transformer Settings in INI\nDESCRIPTION: Configuration settings for the transformer component specifying custom span getter with maximum length parameter.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_8\n\nLANGUAGE: ini\nCODE:\n```\n[components.transformer.model.get_spans]\n@span_getters = \"custom_sent_spans\"\nmax_length = 25\n```\n\n----------------------------------------\n\nTITLE: Entity Recognition with Knowledge Base Links\nDESCRIPTION: Enhanced entity input format for displaCy that includes knowledge base identifiers and URLs. This allows linking entities to external knowledge bases like Wikidata.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"text\": \"But Google is starting from behind.\",\n    \"ents\": [{\"start\": 4, \"end\": 10, \"label\": \"ORG\", \"kb_id\": \"Q95\", \"kb_url\": \"https://www.wikidata.org/entity/Q95\"}],\n    \"title\": None\n}\n```\n\n----------------------------------------\n\nTITLE: Serializing spaCy Language Model to Bytes in Python\nDESCRIPTION: Demonstrates how to serialize a spaCy language model to binary data using the to_bytes() method. This allows saving the current state of the model for later use.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nnlp_bytes = nlp.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Removing Rules from spaCy Matcher\nDESCRIPTION: Demonstrates how to remove a matching rule from the Matcher.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/matcher.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmatcher.add(\"Rule\", [[{\"ORTH\": \"test\"}]])\nassert \"Rule\" in matcher\nmatcher.remove(\"Rule\")\nassert \"Rule\" not in matcher\n```\n\n----------------------------------------\n\nTITLE: Updating Transformer Component during Training in Python\nDESCRIPTION: This snippet demonstrates how to prepare for an update to the Transformer component during training. It runs the model and communicates output to downstream components.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"transformer\")\noptimizer = nlp.initialize()\nlosses = trf.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Matching Phone Numbers Using Token Shape Patterns in spaCy\nDESCRIPTION: This pattern matches phone numbers in formats like (123) 4567 8901 using the SHAPE attribute which represents digit sequences. The pattern looks for patterns of digits (represented by 'd') surrounded by specific punctuation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n[{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"dddd\"},\n {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]\n```\n\n----------------------------------------\n\nTITLE: Processing Document Streams with LLM Components in spaCy\nDESCRIPTION: Example demonstrating how to process a stream of documents with an LLM component using the pipe method, which allows for efficient batch processing with a configurable batch size.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nllm_ner = nlp.add_pipe(\"llm_ner\")\nfor doc in llm_ner.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Initializing EditTreeLemmatizer Component\nDESCRIPTION: Examples demonstrating how to initialize the lemmatizer component with training examples and configuration settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\nlemmatizer.initialize(lambda: examples, nlp=nlp)\n```\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[initialize.components.lemmatizer]\n\n[initialize.components.lemmatizer.labels]\n@readers = \"spacy.read_labels.v1\"\npath = \"corpus/labels/lemmatizer.json\n```\n\n----------------------------------------\n\nTITLE: Predicting Tags with spaCy Tagger\nDESCRIPTION: Apply the tagger model to documents without modifying them, returning prediction scores.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\nscores = tagger.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Uploading spaCy Pipelines to Hugging Face Hub with CLI Commands\nDESCRIPTION: Steps to install the spacy-huggingface-hub package, log in to Hugging Face, package a spaCy model, and upload it to the Hugging Face Hub. This allows sharing models with others and accessing them via pip.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-1.mdx#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install spacy-huggingface-hub\n$ huggingface-cli login\n$ python -m spacy package ./en_ner_fashion ./output --build wheel\n$ cd ./output/en_ner_fashion-0.0.0/dist\n$ python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Working with Token Conjuncts in spaCy\nDESCRIPTION: Shows how to access coordinated tokens using the conjuncts property, which returns tokens connected by coordinating conjunctions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like apples and oranges\")\napples_conjuncts = doc[2].conjuncts\nassert [t.text for t in apples_conjuncts] == [\"oranges\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring PretrainCharacters in spaCy\nDESCRIPTION: Configuration example for the PretrainCharacters pretraining objective which predicts leading and trailing UTF-8 bytes of tokens. The example shows setup for pretraining a tok2vec component with character prediction.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_21\n\nLANGUAGE: ini\nCODE:\n```\n[pretraining]\ncomponent = \"tok2vec\"\n...\n\n[pretraining.objective]\n@architectures = \"spacy.PretrainCharacters.v1\"\nmaxout_pieces = 3\nhidden_size = 300\nn_characters = 4\n```\n\n----------------------------------------\n\nTITLE: Creating Character-Based Spans in spaCy\nDESCRIPTION: Shows how to create a Span object from character indices in the document text, including setting a label for named entities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York\")\nspan = doc.char_span(7, 15, label=\"GPE\")\nassert span.text == \"New York\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Blank Language Pipeline\nDESCRIPTION: Examples of initializing a blank language pipeline either directly or using spacy.blank() for the Yoruba language.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.yo import Yoruba\nnlp = Yoruba()  # use directly\nnlp = spacy.blank(\"yo\")  # blank instance\n```\n\n----------------------------------------\n\nTITLE: Pushing Pipelines to Hugging Face Hub\nDESCRIPTION: Command to push a packaged spaCy pipeline (.whl file) to the Hugging Face Hub.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_52\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--verbose]\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Deduplicating vectors in spaCy's Vocab\nDESCRIPTION: Shows how to remove duplicate rows from the vector table while maintaining mappings for all words. This helps in optimizing the vector storage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnlp.vocab.deduplicate_vectors()\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Span Extensions\nDESCRIPTION: Shows how to define custom attributes on Span objects using extension methods.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Span\ncity_getter = lambda span: any(city in span.text for city in (\"New York\", \"Paris\", \"Berlin\"))\nSpan.set_extension(\"has_city\", getter=city_getter)\ndoc = nlp(\"I like New York in Autumn\")\nassert doc[1:4]._.has_city\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss for TrainablePipe in Python\nDESCRIPTION: Find the loss and gradient of loss for a batch of documents and their predicted scores. This method needs to be overwritten with a custom implementation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\nscores = ner.predict([eg.predicted for eg in examples])\nloss, d_loss = ner.get_loss(examples, scores)\n```\n\n----------------------------------------\n\nTITLE: Updating TextCategorizer Model in spaCy\nDESCRIPTION: Demonstrates how to update the text categorizer model using training examples and an optimizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\noptimizer = nlp.initialize()\nlosses = textcat.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Updating EditTreeLemmatizer Model\nDESCRIPTION: Example showing how to update the lemmatizer model using training examples and an optimizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\noptimizer = nlp.initialize()\nlosses = lemmatizer.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy Language Models with CLI\nDESCRIPTION: Commands to download the new neural network models for different languages using spaCy's command-line interface.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy download en_core_web_sm\npython -m spacy download de_core_news_sm\npython -m spacy download xx_ent_wiki_sm\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Initialize Method for AcronymComponent in Python\nDESCRIPTION: This snippet shows how to implement a custom initialize method for the AcronymComponent to handle data initialization before training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nclass AcronymComponent:\n    def __init__(self):\n        self.data = {}\n\n    def initialize(self, get_examples=None, nlp=None, data={}):\n        self.data = data\n```\n\n----------------------------------------\n\nTITLE: Accessing Transformer Outputs for a Doc in Python\nDESCRIPTION: Illustrates how to retrieve the last hidden layer output for a specific token from the DocTransformerOutput object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a text.\")\ntensors = doc._.trf_data.last_hidden_layer_state[1]\n```\n\n----------------------------------------\n\nTITLE: Computing Semantic Similarity Between Lexemes in spaCy\nDESCRIPTION: Example of how to compute semantic similarity between two lexemes in spaCy. The example compares 'apple' and 'orange' lexemes and demonstrates that similarity is symmetric.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lexeme.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napple = nlp.vocab[\"apple\"]\norange = nlp.vocab[\"orange\"]\napple_orange = apple.similarity(orange)\norange_apple = orange.similarity(apple)\nassert apple_orange == orange_apple\n```\n\n----------------------------------------\n\nTITLE: Loading Language Classes in spaCy\nDESCRIPTION: Shows how to import and load a Language class using the two-letter language code. This allows for lazy-loading of language data and importing languages without directly importing their classes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nfor lang_id in [\"en\", \"de\"]:\n    lang_class = util.get_lang_class(lang_id)\n    lang = lang_class()\n```\n\n----------------------------------------\n\nTITLE: JSONL Format for Raw Text in spaCy Pretraining\nDESCRIPTION: Example showing the JSONL (newline-delimited JSON) format used for providing raw text to spaCy's pretraining functionality. Each line contains a JSON object with a 'text' field.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\"text\": \"Can I ask where you work now and what you do, and if you enjoy it?\"}\n{\"text\": \"They may just pull out of the Seattle market completely, at least until they have autonomous vehicles.\"}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Overlapping Spans with displaCy in Python\nDESCRIPTION: This snippet demonstrates how to use displaCy to visualize overlapping spans in a spaCy document. It creates custom spans and renders them using the 'span' style.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-3.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\nfrom spacy.tokens import Span\n\nnlp = spacy.blank(\"en\")\ntext = \"Welcome to the Bank of China.\"\ndoc = nlp(text)\ndoc.spans[\"custom\"] = [Span(doc, 3, 6, \"ORG\"), Span(doc, 5, 6, \"GPE\")]\ndisplacy.serve(doc, style=\"span\", options={\"spans_key\": \"custom\"})\n```\n\n----------------------------------------\n\nTITLE: Adding Aliases with Probabilities\nDESCRIPTION: Demonstrates adding an alias with multiple possible entity references and their probabilities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/inmemorylookupkb.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nkb.add_alias(alias=\"Douglas\", entities=[\"Q42\", \"Q463035\"], probabilities=[0.6, 0.3])\n```\n\n----------------------------------------\n\nTITLE: Processing Single Document with DependencyParser (Python)\nDESCRIPTION: Demonstrates how to apply the dependency parser to a single document, which modifies the document in-place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nparser = nlp.add_pipe(\"parser\")\n# This usually happens under the hood\nprocessed = parser(doc)\n```\n\n----------------------------------------\n\nTITLE: Checking and Retrieving Word Vector (Python)\nDESCRIPTION: Shows how to check if a word has a vector and retrieve it if available using the has_vector and get_vector methods.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nif nlp.vocab.has_vector(\"apple\"):\n    vector = nlp.vocab.get_vector(\"apple\")\n```\n\n----------------------------------------\n\nTITLE: JSON Format for Few-Shot Examples in NER.v3\nDESCRIPTION: Example JSON structure for providing few-shot examples to the NER.v3 task, including spans with entity information and reasoning.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"text\": \"You can't get a great chocolate flavor with carob.\",\n    \"spans\": [\n      {\n        \"text\": \"chocolate\",\n        \"is_entity\": false,\n        \"label\": \"==NONE==\",\n        \"reason\": \"is a flavor in this context, not an ingredient\"\n      },\n      {\n        \"text\": \"carob\",\n        \"is_entity\": true,\n        \"label\": \"INGREDIENT\",\n        \"reason\": \"is an ingredient to add chocolate flavor\"\n      }\n    ]\n  },\n  ...\n]\n```\n\n----------------------------------------\n\nTITLE: Wrapping PyTorch Model with Thinc in Python\nDESCRIPTION: Demonstrates how to wrap a PyTorch model using Thinc's PyTorchWrapper to make it compatible with spaCy's architecture.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom thinc.api import PyTorchWrapper\n\nwrapped_pt_model = PyTorchWrapper(torch_model)\n```\n\n----------------------------------------\n\nTITLE: Using Enable and Disable Parameters with spaCy.load() in Python\nDESCRIPTION: Examples showing how to use the 'enable' parameter in spacy.load(), which disables all components not listed. When combined with 'disable', conflicts between the lists will raise an error.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Load the complete pipeline, but disable all components except for tok2vec and tagger\nnlp = spacy.load(\"en_core_web_sm\", enable=[\"tok2vec\", \"tagger\"])\n# Has the same effect, as NER is already not part of enabled set of components\nnlp = spacy.load(\"en_core_web_sm\", enable=[\"tok2vec\", \"tagger\"], disable=[\"ner\"])\n# Will raise an error, as the sets of enabled and disabled components are conflicting\nnlp = spacy.load(\"en_core_web_sm\", enable=[\"ner\"], disable=[\"ner\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring NER.v3 with Custom Entity Description and Label Definitions\nDESCRIPTION: Advanced configuration for the NER.v3 component including custom entity labels, description, and detailed label definitions to guide the LLM.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_25\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.NER.v3\"\nlabels = [\"DISH\", \"INGREDIENT\", \"EQUIPMENT\"]\ndescription = Entities are the names food dishes,\n    ingredients, and any kind of cooking equipment.\n    Adjectives, verbs, adverbs are not entities.\n    Pronouns are not entities.\n\n[components.llm.task.label_definitions]\nDISH = \"Known food dishes, e.g. Lobster Ravioli, garlic bread\"\nINGREDIENT = \"Individual parts of a food dish, including herbs and spices.\"\nEQUIPMENT = \"Any kind of cooking equipment. e.g. oven, cooking pot, grill\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Pretraining Layers in spaCy\nDESCRIPTION: Examples of how to configure the pretraining target in spaCy by specifying which component and layer to pretrain. Shows both pretraining the entire tok2vec component or just the tok2vec subnetwork of another component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_20\n\nLANGUAGE: ini\nCODE:\n```\n# 1. Use the whole model of the \"tok2vec\" component\n[pretraining]\ncomponent = \"tok2vec\"\nlayer = \"\"\n\n# 2. Pretrain the \"tok2vec\" node of the \"textcat\" component\n[pretraining]\ncomponent = \"textcat\"\nlayer = \"tok2vec\"\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for Transformer Component in Python\nDESCRIPTION: This example shows how to create an optimizer specifically for the Transformer pipeline component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"transformer\")\noptimizer = trf.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Creating a Stateful Component for Acronym Detection in Python\nDESCRIPTION: This example demonstrates a stateful pipeline component for handling acronyms. It uses a PhraseMatcher to detect acronyms and their expanded forms, and adds them to a custom doc extension. The factory function includes a case_sensitive configuration option.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\nfrom spacy.tokens import Doc\nfrom spacy.matcher import PhraseMatcher\nimport spacy\n\nDICTIONARY = {\"lol\": \"laughing out loud\", \"brb\": \"be right back\"}\nDICTIONARY.update({value: key for key, value in DICTIONARY.items()})\n\n@Language.factory(\"acronyms\", default_config={\"case_sensitive\": False})\ndef create_acronym_component(nlp: Language, name: str, case_sensitive: bool):\n    return AcronymComponent(nlp, case_sensitive)\n\nclass AcronymComponent:\n    def __init__(self, nlp: Language, case_sensitive: bool):\n        # Create the matcher and match on Token.lower if case-insensitive\n        matcher_attr = \"TEXT\" if case_sensitive else \"LOWER\"\n        self.matcher = PhraseMatcher(nlp.vocab, attr=matcher_attr)\n        self.matcher.add(\"ACRONYMS\", [nlp.make_doc(term) for term in DICTIONARY])\n        self.case_sensitive = case_sensitive\n        # Register custom extension on the Doc\n        if not Doc.has_extension(\"acronyms\"):\n            Doc.set_extension(\"acronyms\", default=[])\n\n    def __call__(self, doc: Doc) -> Doc:\n        # Add the matched spans when doc is processed\n        for _, start, end in self.matcher(doc):\n            span = doc[start:end]\n            acronym = DICTIONARY.get(span.text if self.case_sensitive else span.text.lower())\n            doc._.acronyms.append((span, acronym))\n        return doc\n\n# Add the component to the pipeline and configure it\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\"acronyms\", config={\"case_sensitive\": False})\n\n# Process a doc and see the results\ndoc = nlp(\"LOL, be right back\")\nprint(doc._.acronyms)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Serialization Methods for AcronymComponent in Python\nDESCRIPTION: This snippet demonstrates how to implement custom to_disk and from_disk methods for the AcronymComponent to handle data serialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport srsly\nfrom spacy.util import ensure_path\n\nclass AcronymComponent:\n    # other methods here...\n\n    def to_disk(self, path, exclude=tuple()):\n        path = ensure_path(path)\n        if not path.exists():\n            path.mkdir()\n        srsly.write_json(path / \"data.json\", self.data)\n\n    def from_disk(self, path, exclude=tuple()):\n        self.data = srsly.read_json(path / \"data.json\")\n        return self\n```\n\n----------------------------------------\n\nTITLE: Configuring Morphologizer with Default Model in spaCy\nDESCRIPTION: Example showing how to add a morphologizer pipeline component with the default morphology model configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.morphologizer import DEFAULT_MORPH_MODEL\nconfig = {\"model\": DEFAULT_MORPH_MODEL}\nnlp.add_pipe(\"morphologizer\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Filtering Overlapping Spans in spaCy\nDESCRIPTION: Shows how to filter and remove duplicate or overlapping spans from a sequence of Span objects, preferring longer spans when there are overlaps.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_56\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nspans = [doc[0:2], doc[0:2], doc[0:4]]\nfiltered = filter_spans(spans)\n```\n\n----------------------------------------\n\nTITLE: Configuring batch_by_padded Batcher in spaCy\nDESCRIPTION: Configuration for batch_by_padded batcher that creates minibatches based on padded sequence size. It accepts parameters for size, buffer, discard_oversize policy, and optional length calculation function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_30\n\nLANGUAGE: ini\nCODE:\n```\n[training.batcher]\n@batchers = \"spacy.batch_by_padded.v1\"\nsize = 100\nbuffer = 256\ndiscard_oversize = false\nget_length = null\n```\n\n----------------------------------------\n\nTITLE: Initializing Labels Configuration in spaCy\nDESCRIPTION: Configuration example for initializing NER labels using spacy.read_labels.v1 reader to load labels from a JSON file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_9\n\nLANGUAGE: ini\nCODE:\n```\n[initialize.components.ner]\n\n[initialize.components.ner.labels]\n@readers = \"spacy.read_labels.v1\"\npath = \"corpus/labels/ner.json\n```\n\n----------------------------------------\n\nTITLE: Processing Single Document with EntityRecognizer\nDESCRIPTION: Example showing how to process a single document using the EntityRecognizer component directly.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nner = nlp.add_pipe(\"ner\")\n# This usually happens under the hood\nprocessed = ner(doc)\n```\n\n----------------------------------------\n\nTITLE: Adding Patterns to AttributeRuler\nDESCRIPTION: Example showing how to add patterns and corresponding attributes to the AttributeRuler. This example adds a pattern to match verbs with the VB tag and assign them the VERB POS tag.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributeruler.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"attribute_ruler\")\npatterns = [[{\"TAG\": \"VB\"}]]\nattrs = {\"POS\": \"VERB\"}\nruler.add(patterns=patterns, attrs=attrs)\n```\n\n----------------------------------------\n\nTITLE: Statistical Morphology Analysis in spaCy\nDESCRIPTION: This code snippet shows how to use spaCy's statistical Morphologizer component to assign morphological features and coarse-grained part-of-speech tags. It demonstrates this using a German language model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"de_core_news_sm\")\ndoc = nlp(\"Wo bist du?\") # English: 'Where are you?'\nprint(doc[2].morph)  # 'Case=Nom|Number=Sing|Person=2|PronType=Prs'\nprint(doc[2].pos_) # 'PRON'\n```\n\n----------------------------------------\n\nTITLE: Initializing Token Object - Python\nDESCRIPTION: Demonstrates creation and basic usage of a Token object from a Doc object using spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\ntoken = doc[0]\nassert token.text == \"Give\"\n```\n\n----------------------------------------\n\nTITLE: Checking Span Extensions\nDESCRIPTION: Shows how to check if a specific extension is registered on the Span class.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Span\nSpan.set_extension(\"is_city\", default=False)\nassert Span.has_extension(\"is_city\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Sentence Span Getter in Python\nDESCRIPTION: Example of creating a custom span getter that returns sentence spans from docs. Uses the spaCy registry decorator to register the custom function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@spacy.registry.span_getters(\"custom_sent_spans\")\ndef configure_get_sent_spans() -> Callable:\n    def get_sent_spans(docs: Iterable[Doc]) -> List[List[Span]]:\n        return [list(doc.sents) for doc in docs]\n\n    return get_sent_spans\n```\n\n----------------------------------------\n\nTITLE: Serializing spaCy NLP Object to Bytes\nDESCRIPTION: Demonstrates how to serialize a spaCy NLP object to bytes using the to_bytes() method. This allows saving the entire state of the NLP pipeline for later use or transfer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_serialization.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndata = nlp.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Installing a Custom Component Package\nDESCRIPTION: Command to install a custom spaCy component package in the current environment, making it available to spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m pip install .\n```\n\n----------------------------------------\n\nTITLE: Setting Annotations with spaCy Tagger\nDESCRIPTION: Modify documents using pre-computed prediction scores from the tagger model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\nscores = tagger.predict([doc1, doc2])\ntagger.set_annotations([doc1, doc2], scores)\n```\n\n----------------------------------------\n\nTITLE: Accessing Token Vector in spaCy\nDESCRIPTION: Demonstrates how to access a token's word vector representation and verify its properties.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like apples\")\napples = doc[2]\nassert apples.vector.dtype == \"float32\"\nassert apples.vector.shape == (300,)\n```\n\n----------------------------------------\n\nTITLE: Using Example.get_aligned_spans_x2y in spaCy\nDESCRIPTION: Demonstrates how to align predicted entity spans to reference tokenization using the get_aligned_spans_x2y method. Shows adding a NER pipeline and converting entity indices between different tokenizations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nnlp.add_pipe(\"my_ner\")\ndoc = nlp(\"Mr and Mrs Smith flew to New York\")\ntokens_ref = [\"Mr and Mrs\", \"Smith\", \"flew\", \"to\", \"New York\"]\nexample = Example.from_dict(doc, {\"words\": tokens_ref})\nents_pred = example.predicted.ents\n# Assume the NER model has found \"Mr and Mrs Smith\" as a named entity\nassert [(ent.start, ent.end) for ent in ents_pred] == [(0, 4)]\nents_x2y = example.get_aligned_spans_x2y(ents_pred)\nassert [(ent.start, ent.end) for ent in ents_x2y] == [(0, 2)]\n```\n\n----------------------------------------\n\nTITLE: Loading Coreference Resolver from Disk in Python with spaCy\nDESCRIPTION: This code shows how to load a previously serialized coreference resolver from disk, modifying the object in place and returning it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncoref = nlp.add_pipe(\"experimental_coref\")\ncoref.from_disk(\"/path/to/coref\")\n```\n\n----------------------------------------\n\nTITLE: Configuring FewShotReader\nDESCRIPTION: Configuration for SpaCy's FewShotReader component that reads examples from YAML, JSON, or JSONL files using the srsly library.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_64\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task.examples]\n@misc = \"spacy.FewShotReader.v1\"\npath = \"ner_examples.yml\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanCategorizer Model Architecture in spaCy\nDESCRIPTION: Configuration for the SpanCategorizer model which powers the SpanCategorizer component. It includes a token-to-vector model, a reducer model to map sequence vectors for each span to a single vector, and a scorer model for classification.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_28\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.SpanCategorizer.v1\"\nscorer = {\"@layers\": \"spacy.LinearLogistic.v1\"}\n\n[model.reducer]\n@layers = spacy.mean_max_reducer.v1\"\nhidden_size = 128\n\n[model.tok2vec]\n@architectures = \"spacy.Tok2Vec.v1\"\n\n[model.tok2vec.embed]\n@architectures = \"spacy.MultiHashEmbed.v1\"\n# ...\n\n[model.tok2vec.encode]\n@architectures = \"spacy.MaxoutWindowEncoder.v1\"\n# ...\n```\n\n----------------------------------------\n\nTITLE: Retrieving Candidates for Entity Linking in Python\nDESCRIPTION: Example of using the get_candidates method to retrieve candidate entities for a given text span.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/kb.mdx#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom spacy.lang.en import English\nnlp = English()\ndoc = nlp(\"Douglas Adams wrote 'The Hitchhiker's Guide to the Galaxy'.\")\ncandidates = kb.get_candidates(doc[0:2])\n```\n\n----------------------------------------\n\nTITLE: Filling a Partial spaCy Config File with Default Values\nDESCRIPTION: Auto-fills a partial configuration file with all default values and generates a diff to show the changes. This is useful for completing configs generated with the quickstart widget.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init fill-config base.cfg config.cfg --diff\n```\n\n----------------------------------------\n\nTITLE: Checking Label Presence in EntityRuler\nDESCRIPTION: Shows how to check if a specific label is present in the EntityRuler patterns using the 'in' operator.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"entity_ruler\")\nruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\nassert \"ORG\" in ruler\nassert not \"PERSON\" in ruler\n```\n\n----------------------------------------\n\nTITLE: Initializing EntityLinker in spaCy (Python)\nDESCRIPTION: Demonstrates how to initialize the EntityLinker component with custom examples and a knowledge base loader. This method sets up the component for training, including network validation and label scheme setup.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nentity_linker = nlp.add_pipe(\"entity_linker\")\nentity_linker.initialize(lambda: examples, nlp=nlp, kb_loader=my_kb)\n```\n\n----------------------------------------\n\nTITLE: Initializing Transformer Component\nDESCRIPTION: Examples demonstrating different ways to initialize the transformer component, including default configuration, custom config, and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\ntrf = nlp.add_pipe(\"transformer\")\n\n# Construction via add_pipe with custom config\nconfig = {\n    \"model\": {\n        \"@architectures\": \"spacy-transformers.TransformerModel.v3\",\n        \"name\": \"bert-base-uncased\",\n        \"tokenizer_config\": {\"use_fast\": True},\n        \"transformer_config\": {\"output_attentions\": True},\n        \"mixed_precision\": True,\n        \"grad_scaler_config\": {\"init_scale\": 32768}\n    }\n}\ntrf = nlp.add_pipe(\"transformer\", config=config)\n\n# Construction from class\nfrom spacy_transformers import Transformer\ntrf = Transformer(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Llama2 Model Configuration Example\nDESCRIPTION: Configuration example for using Llama2 model in spaCy's LLM component\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_58\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.model]\n@llm_models = \"spacy.Llama2.v1\"\nname = \"Llama-2-7b-hf\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Component Factory with Custom Config in Python\nDESCRIPTION: This example shows how to create a component factory with a custom configuration. It uses the @Language.factory decorator with a default_config and demonstrates how to add the component to the pipeline with a custom configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.language import Language\n\n@Language.factory(\"my_component\", default_config={\"some_setting\": True})\ndef my_component(nlp, name, some_setting: bool):\n    return MyComponent(some_setting=some_setting)\n\nnlp = spacy.blank(\"en\")\nnlp.add_pipe(\"my_component\", config={\"some_setting\": False})\n```\n\n----------------------------------------\n\nTITLE: Configuring EditTreeLemmatizer Pipeline Component\nDESCRIPTION: Example showing how to configure and add the EditTreeLemmatizer to the spaCy pipeline using default model settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.edit_tree_lemmatizer import DEFAULT_EDIT_TREE_LEMMATIZER_MODEL\nconfig = {\"model\": DEFAULT_EDIT_TREE_LEMMATIZER_MODEL}\nnlp.add_pipe(\"trainable_lemmatizer\", config=config, name=\"lemmatizer\")\n```\n\n----------------------------------------\n\nTITLE: Finding Vector Keys and Rows\nDESCRIPTION: Examples of looking up vector keys and rows using different search methods.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrow = nlp.vocab.vectors.find(key=\"cat\")\nrows = nlp.vocab.vectors.find(keys=[\"cat\", \"dog\"])\nkey = nlp.vocab.vectors.find(row=256)\nkeys = nlp.vocab.vectors.find(rows=[18, 256, 985])\n```\n\n----------------------------------------\n\nTITLE: Initializing a Training Config File\nDESCRIPTION: Command for generating a starter training configuration file using the init config command, specifying language and pipeline components to include.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init config ./config.cfg --lang en --pipeline tagger,parser\n```\n\n----------------------------------------\n\nTITLE: Initializing SentenceRecognizer in spaCy\nDESCRIPTION: Examples of different ways to initialize the SentenceRecognizer, including via add_pipe, create_pipe with custom model, and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\nsenter = nlp.add_pipe(\"senter\")\n\n# Construction via create_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_senter\"}}\nsenter = nlp.add_pipe(\"senter\", config=config)\n\n# Construction from class\nfrom spacy.pipeline import SentenceRecognizer\nsenter = SentenceRecognizer(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy NER.v1 Task\nDESCRIPTION: Example configuration for the original NER.v1 task that supports both zero-shot and few-shot prompting. This version requires specifying entity labels and provides various configuration options for entity matching.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_29\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.NER.v1\"\nlabels = PERSON,ORGANISATION,LOCATION\nexamples = null\n```\n\n----------------------------------------\n\nTITLE: Initializing Span from Doc Slice\nDESCRIPTION: Creates a Span object from a slice of a Doc object, demonstrating basic span creation and token access.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\nspan = doc[1:4]\nassert [t.text for t in span] ==  [\"it\", \"back\", \"!\"]\n```\n\n----------------------------------------\n\nTITLE: Initializing CoreferenceResolver Component\nDESCRIPTION: Different ways to initialize the CoreferenceResolver component, including via add_pipe with default/custom models and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\ncoref = nlp.add_pipe(\"experimental_coref\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_coref.v1\"}}\ncoref = nlp.add_pipe(\"experimental_coref\", config=config)\n\n# Construction from class\nfrom spacy_experimental.coref.coref_component import CoreferenceResolver\ncoref = CoreferenceResolver(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Deserializing Binary Data to spaCy Vectors\nDESCRIPTION: Shows how to create a new Vectors object and load serialized vector data using the from_bytes() method. This example initializes a new Vectors object with a StringStore and loads previously serialized vector data into it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.vectors import Vectors\nvectors_bytes = vectors.to_bytes()\nnew_vectors = Vectors(StringStore())\nnew_vectors.from_bytes(vectors_bytes)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Factory Metadata with get_factory_meta in Python\nDESCRIPTION: Shows how to use the get_factory_meta class method to retrieve metadata for a given pipeline component factory. The method returns a FactoryMeta object containing information about the component and its default configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nfactory_meta = Language.get_factory_meta(\"ner\")\nassert factory_meta.factory == \"ner\"\nprint(factory_meta.default_config)\n```\n\n----------------------------------------\n\nTITLE: Using displaCy.render for HTML Generation\nDESCRIPTION: Example showing how to generate HTML markup for dependency parse visualization using displaCy's render function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a sentence.\")\nhtml = displacy.render(doc, style=\"dep\")\n```\n\n----------------------------------------\n\nTITLE: Converting Entity Offsets to BILUO Tags\nDESCRIPTION: Example demonstrating how to convert character offset-based entity annotations to BILUO tagging scheme using spaCy's training utilities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import offsets_to_biluo_tags\n\ndoc = nlp(\"I like London.\")\nentities = [(7, 13, \"LOC\")]\ntags = offsets_to_biluo_tags(doc, entities)\nassert tags == [\"O\", \"O\", \"U-LOC\", \"O\"]\n```\n\n----------------------------------------\n\nTITLE: Calculating Lowest Common Ancestor Matrix for spaCy Doc (Python)\nDESCRIPTION: Shows how to calculate the lowest common ancestor matrix for a spaCy Doc object. The method returns a numpy array representing the LCA matrix.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a test\")\nmatrix = doc.get_lca_matrix()\n# array([[0, 1, 1, 1], [1, 1, 1, 1], [1, 1, 2, 3], [1, 1, 3, 3]], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Applying TextCategorizer to a Stream of Documents in Python\nDESCRIPTION: Example of applying the TextCategorizer to a stream of documents using the pipe method with batching.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\nfor doc in textcat.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCatBOW Bag-of-Words Model in spaCy\nDESCRIPTION: Configuration for the TextCatBOW architecture which implements an n-gram bag-of-words model for text categorization. This model prioritizes speed over accuracy and is configurable for exclusive or non-exclusive classes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_25\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.TextCatBOW.v3\"\nexclusive_classes = false\nlength = 262144\nngram_size = 1\nno_output_layer = false\nnO = null\n```\n\n----------------------------------------\n\nTITLE: Visualizing Matched Patterns with displaCy in spaCy\nDESCRIPTION: This example demonstrates how to find and visualize matches for a specific pattern using spaCy's Matcher and displaCy visualizer. It collects sentences containing matches of 'Facebook is/was [adj]' pattern and renders them visually.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab)\nmatched_sents = []  # Collect data of matched sentences to be visualized\n\ndef collect_sents(matcher, doc, i, matches):\n    match_id, start, end = matches[i]\n    span = doc[start:end]  # Matched span\n    sent = span.sent  # Sentence containing matched span\n    # Append mock entity for match in displaCy style to matched_sents\n    # get the match span by ofsetting the start and end of the span with the\n    # start and end of the sentence in the doc\n    match_ents = [{\n        \"start\": span.start_char - sent.start_char,\n        \"end\": span.end_char - sent.start_char,\n        \"label\": \"MATCH\",\n    }]\n    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n\npattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n           {\"POS\": \"ADJ\"}]\nmatcher.add(\"FacebookIs\", [pattern], on_match=collect_sents)  # add pattern\ndoc = nlp(\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\nmatches = matcher(doc)\n\n# Serve visualization of sentences containing match with displaCy\n# set manual=True to make displaCy render straight from a dictionary\n# (if you're not running the code within a Jupyter environment, you can\n# use displacy.serve instead)\ndisplacy.render(matched_sents, style=\"ent\", manual=True)\n```\n\n----------------------------------------\n\nTITLE: Saving EntityRuler Patterns to Disk\nDESCRIPTION: Shows how to save EntityRuler patterns to disk, either as a JSONL file or as a directory containing patterns and config.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"entity_ruler\")\nruler.to_disk(\"/path/to/patterns.jsonl\")  # saves patterns only\nruler.to_disk(\"/path/to/entity_ruler\")    # saves patterns and config\n```\n\n----------------------------------------\n\nTITLE: Serializing DependencyParser to Bytes in Python\nDESCRIPTION: This code demonstrates how to serialize a DependencyParser component to a bytestring. It adds a parser to the NLP pipeline and then converts it to bytes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\nparser_bytes = parser.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Predicting with CuratedTransformer\nDESCRIPTION: Apply the transformer model to a batch of Doc objects to generate predictions without modifying the documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"curated_transformer\")\nscores = trf.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Changing Output Dimension of DependencyParser in Python\nDESCRIPTION: This snippet demonstrates how to change the output dimension of a DependencyParser component in spaCy. It adds a parser to the NLP pipeline and sets its output dimension to 512.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\nparser.set_output(512)\n```\n\n----------------------------------------\n\nTITLE: Using spacy.explain() Function\nDESCRIPTION: Examples of using spacy.explain() to get descriptions of linguistic annotations and processing tokens with explanations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nspacy.explain(\"NORP\")\n# Nationalities or religious or political groups\n\ndoc = nlp(\"Hello world\")\nfor word in doc:\n   print(word.text, word.tag_, spacy.explain(word.tag_))\n# Hello UH interjection\n# world NN noun, singular or mass\n```\n\n----------------------------------------\n\nTITLE: Initializing CoreferenceResolver for Training\nDESCRIPTION: Example showing how to initialize the component for training with example data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncoref = nlp.add_pipe(\"experimental_coref\")\ncoref.initialize(lambda: examples, nlp=nlp)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Entity Vector from KnowledgeBase in Python\nDESCRIPTION: Example of using the get_vector method to retrieve a pretrained entity vector for a given entity ID.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/kb.mdx#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nvector = kb.get_vector(\"Q42\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Scorer in spaCy Pipeline\nDESCRIPTION: Example of specifying a custom scoring function for a tagger component in the spaCy config file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-2.mdx#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[components.tagger]\nfactory = \"tagger\"\nscorer = {\"@scorers\":\"spacy.tagger_scorer.v1\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Example from Dictionary with POS Tags\nDESCRIPTION: Demonstrates a more concise way to create an Example object with gold-standard part-of-speech tags using the Example.from_dict method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nwords = [\"I\", \"like\", \"stuff\"]\ntags = [\"NOUN\", \"VERB\", \"NOUN\"]\npredicted = Doc(nlp.vocab, words=words)\nexample = Example.from_dict(predicted, {\"tags\": tags})\n```\n\n----------------------------------------\n\nTITLE: Calculating Parser Loss\nDESCRIPTION: Shows how to compute loss and gradient for parser predictions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\nscores = parser.predict([eg.predicted for eg in examples])\nloss, d_loss = parser.get_loss(examples, scores)\n```\n\n----------------------------------------\n\nTITLE: Initializing Korean Tokenizer in spaCy\nDESCRIPTION: Demonstrates how to initialize a blank Korean language model in spaCy using the default MeCab-based tokenizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.blank(\"ko\")\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Pattern Dictionaries to AttributeRuler\nDESCRIPTION: Example demonstrating how to add multiple patterns to the AttributeRuler using a list of pattern dictionaries. It includes a pattern for verb tags and one for the phrase 'two apples' with index specification.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributeruler.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"attribute_ruler\")\npatterns = [\n  {\n    \"patterns\": [[{\"TAG\": \"VB\"}]], \"attrs\": {\"POS\": \"VERB\"}\n  },\n  {\n    \"patterns\": [[{\"LOWER\": \"two\"}, {\"LOWER\": \"apples\"}]],\n    \"attrs\": {\"LEMMA\": \"apple\"},\n    \"index\": -1\n  },\n]\nruler.add_patterns(patterns)\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss for SentenceRecognizer\nDESCRIPTION: Calculates loss and gradient for a batch of examples and their predicted scores.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\nscores = senter.predict([eg.predicted for eg in examples])\nloss, d_loss = senter.get_loss(examples, scores)\n```\n\n----------------------------------------\n\nTITLE: Processing Documents with CoreferenceResolver\nDESCRIPTION: Example showing how to process a single document using the CoreferenceResolver component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\ncoref = nlp.add_pipe(\"experimental_coref\")\n# This usually happens under the hood\nprocessed = coref(doc)\n```\n\n----------------------------------------\n\nTITLE: Initializing PhraseMatcher in Python\nDESCRIPTION: Creates a new PhraseMatcher instance with a shared vocabulary object. The matcher can be configured to match on different token attributes and includes optional pattern validation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/phrasematcher.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.matcher import PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab)\n```\n\n----------------------------------------\n\nTITLE: Accessing Text Categories in spaCy Doc Object (Python)\nDESCRIPTION: Shows how to access the document categories (doc.cats) property which stores scores for text categories applied to the document. This property is typically set by the TextCategorizer component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a text about football.\")\nprint(doc.cats)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Evaluation Script with Typer in Python\nDESCRIPTION: Python script that uses Typer to create a command-line interface for a custom evaluation function, with typed arguments that automatically become CLI parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport typer\n\ndef custom_evaluation(batch_size: int = 128, model_path: str, data_path: str):\n    # The arguments are now available as positional CLI arguments\n    print(batch_size, model_path, data_path)\n\nif __name__ == \"__main__\":\n    typer.run(custom_evaluation)\n```\n\n----------------------------------------\n\nTITLE: Setting Candidates for SpanCategorizer in Python\nDESCRIPTION: Demonstration of using the suggester to add a list of Span candidates to a list of Doc objects for debugging purposes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\nspancat.set_candidates(docs, \"candidates\")\n```\n\n----------------------------------------\n\nTITLE: Using spacy.info() Function\nDESCRIPTION: Examples of using spacy.info() to get information about spaCy installation and models in different formats.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nspacy.info()\nspacy.info(\"en_core_web_sm\")\nmarkdown = spacy.info(markdown=True, silent=True)\n```\n\n----------------------------------------\n\nTITLE: Running Training with Custom Code\nDESCRIPTION: Command for training a spaCy pipeline with a config file and custom code containing registered functions, such as tokenizer customization callbacks.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy train config.cfg --code ./functions.py\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Vector Keys in Python\nDESCRIPTION: Example showing how to iterate over all keys in the vector store.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor key in nlp.vocab.vectors:\n    print(key, nlp.vocab.strings[key])\n```\n\n----------------------------------------\n\nTITLE: Converting a Span to a Doc Object in Python using spaCy\nDESCRIPTION: Demonstrates how to create a new Doc object from a Span, copying the data. Optionally allows passing precomputed array representations for efficiency when processing multiple spans.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York in Autumn.\")\nspan = doc[2:4]\ndoc2 = span.as_doc()\nassert doc2.text == \"New York\"\n```\n\n----------------------------------------\n\nTITLE: Initializing SpanRuler Component in spaCy\nDESCRIPTION: Examples showing two methods of constructing a SpanRuler: via add_pipe method and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe\nruler = nlp.add_pipe(\"span_ruler\")\n\n# Construction from class\nfrom spacy.pipeline import SpanRuler\nruler = SpanRuler(nlp, overwrite=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanCat v1 Component in spaCy\nDESCRIPTION: Configuration example for the SpanCat v1 component that handles overlapping entities and stores annotations in doc.spans. Specifies entity labels and optional example generation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_35\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.SpanCat.v1\"\nlabels = PERSON,ORGANISATION,LOCATION\nexamples = null\n```\n\n----------------------------------------\n\nTITLE: Benchmarking spaCy Pipeline Speed (Bash)\nDESCRIPTION: Shows how to use the new 'benchmark speed' CLI command to measure the processing speed of a spaCy pipeline on a given dataset.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-5.mdx#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ spacy benchmark speed my_pipeline data.spacy\n```\n\n----------------------------------------\n\nTITLE: Project Command Configuration\nDESCRIPTION: YAML configuration defining a preprocessing command with dependencies and outputs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncommands:\n  - name: preprocess\n    help: \"Convert the input data to spaCy's format\"\n    script:\n      - 'python -m spacy convert assets/train.conllu corpus/'\n      - 'python -m spacy convert assets/eval.conllu corpus/'\n    deps:\n      - 'assets/train.conllu'\n      - 'assets/eval.conllu'\n    outputs:\n      - 'corpus/train.spacy'\n      - 'corpus/eval.spacy'\n```\n\n----------------------------------------\n\nTITLE: Aligning Spans in spaCy (Python)\nDESCRIPTION: Demonstrates aligning span objects between reference and predicted document tokenizations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwords = [\"Mr and Mrs Smith\", \"flew\", \"to\", \"New York\"]\ndoc = Doc(en_vocab, words=words)\nentities = [(0, 16, \"PERSON\")]\ntokens_ref = [\"Mr\", \"and\", \"Mrs\", \"Smith\", \"flew\", \"to\", \"New\", \"York\"]\nexample = Example.from_dict(doc, {\"words\": tokens_ref, \"entities\": entities})\nents_ref = example.reference.ents\nassert [(ent.start, ent.end) for ent in ents_ref] == [(0, 4)]\nents_y2x = example.get_aligned_spans_y2x(ents_ref)\nassert [(ent.start, ent.end) for ent in ents_y2x] == [(0, 1)]\n```\n\n----------------------------------------\n\nTITLE: Configuring Tagger Pipeline Component in spaCy\nDESCRIPTION: Example showing how to configure and add a tagger component to the spaCy pipeline using the default tagger model configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.tagger import DEFAULT_TAGGER_MODEL\nconfig = {\"model\": DEFAULT_TAGGER_MODEL}\nnlp.add_pipe(\"tagger\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Processing Multiple Documents with SentenceRecognizer in spaCy\nDESCRIPTION: Example of how to apply the SentenceRecognizer to a stream of documents using the pipe method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\nfor doc in senter.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring TransformerListener in spaCy\nDESCRIPTION: Configuration for creating a TransformerListener layer that connects to a Transformer component in the pipeline, specifying gradient factor and mean pooling for token vector calculation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_11\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy-transformers.TransformerListener.v1\"\ngrad_factor = 1.0\n\n[model.pooling]\n@layers = \"reduce_mean.v1\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Docs from DocBin\nDESCRIPTION: Demonstrates how to recover Doc objects from DocBin using a vocabulary.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/docbin.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndocs = list(doc_bin.get_docs(nlp.vocab))\n```\n\n----------------------------------------\n\nTITLE: Loading DependencyParser from Bytes in Python\nDESCRIPTION: This snippet shows how to load a DependencyParser component from a bytestring. It serializes a parser to bytes, adds a new parser to the NLP pipeline, and then loads it from the bytes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nparser_bytes = parser.to_bytes()\nparser = nlp.add_pipe(\"parser\")\nparser.from_bytes(parser_bytes)\n```\n\n----------------------------------------\n\nTITLE: Disabling POS Tagging and Lemmatization in spaCy Pipelines\nDESCRIPTION: This snippet demonstrates how to disable part-of-speech tagging and lemmatization components when loading spaCy pipelines. It shows examples for both small and transformer-based English models.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/models/index.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"attribute_ruler\", \"lemmatizer\"])\nnlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n```\n\n----------------------------------------\n\nTITLE: Generate Installable Package Command\nDESCRIPTION: Command to create an installable Python package from a pipeline data directory, with options for including code files, meta data, and building distribution formats.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_41\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy package [input_dir] [output_dir] [--code] [--meta-path] [--create-meta] [--build] [--name] [--version] [--force]\n```\n\n----------------------------------------\n\nTITLE: Setting Error Handler for TrainablePipe Component in Python\nDESCRIPTION: Example of defining and setting a custom error handler for a TrainablePipe component to handle errors during document processing.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef warn_error(proc_name, proc, docs, e):\n    print(f\"An error occurred when applying component {proc_name}.\")\n\npipe = nlp.add_pipe(\"ner\")\npipe.set_error_handler(warn_error)\n```\n\n----------------------------------------\n\nTITLE: Saving TrainablePipe to Disk\nDESCRIPTION: Demonstrates how to serialize a custom pipeline component to disk. The method saves the pipe to a specified directory path, which will be created if it doesn't exist.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\npipe.to_disk(\"/path/to/pipe\")\n```\n\n----------------------------------------\n\nTITLE: Defining Named Entities with BILUO Tags\nDESCRIPTION: Shows how to create an Example with gold-standard named entity annotations using the BILUO tagging scheme.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ndoc = Doc(nlp.vocab, words=[\"Facebook\", \"released\", \"React\", \"in\", \"2014\"])\nexample = Example.from_dict(doc, {\"entities\": [\"U-ORG\", \"O\", \"U-TECHNOLOGY\", \"O\", \"U-DATE\"]})\n```\n\n----------------------------------------\n\nTITLE: Using DependencyMatcher to Find Matches in Python\nDESCRIPTION: Example demonstrating how to add patterns to a DependencyMatcher and find matches in a document. The pattern searches for the word \"founded\" in the text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencymatcher.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.matcher import DependencyMatcher\n\nmatcher = DependencyMatcher(nlp.vocab)\npattern = [{\"RIGHT_ID\": \"founded_id\",\n  \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}}]\nmatcher.add(\"FOUNDED\", [pattern])\ndoc = nlp(\"Bill Gates founded Microsoft.\")\nmatches = matcher(doc)\n```\n\n----------------------------------------\n\nTITLE: Matching International German Phone Numbers with spaCy Patterns\nDESCRIPTION: A pattern to match German international phone numbers with the country code +49. It uses a combination of ORTH for exact text matching, SHAPE for digit patterns, and optional punctuation with the OP operator.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n[{\"ORTH\": \"+\"}, {\"ORTH\": \"49\"}, {\"ORTH\": \"(\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"},\n {\"ORTH\": \")\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\", \"LENGTH\": 6}]\n```\n\n----------------------------------------\n\nTITLE: Running find-threshold Command for spancat in spaCy\nDESCRIPTION: Command for finding optimal threshold values for a spancat component. This runs prediction trials with different thresholds to maximize the spans_sc_f metric.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_38\n\nLANGUAGE: bash\nCODE:\n```\n# For spancat:\n$ python -m spacy find-threshold my_nlp data.spacy spancat threshold spans_sc_f\n```\n\n----------------------------------------\n\nTITLE: Handling Reference Documents in spaCy (Python)\nDESCRIPTION: Shows how to work with reference documents to extract gold standard labels for training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor i, eg in enumerate(examples):\n    for j, label in enumerate(all_labels):\n        gold_labels[i][j] = eg.reference.cats.get(label, 0.0)\n```\n\n----------------------------------------\n\nTITLE: Serializing LLM Components to Bytes in spaCy\nDESCRIPTION: Example demonstrating how to serialize an LLM component to a bytestring, which can be useful for storage or transmission without using the filesystem.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nllm_ner = nlp.add_pipe(\"llm_ner\")\nner_bytes = llm_ner.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Getting Span Extensions\nDESCRIPTION: Demonstrates how to retrieve information about registered Span extensions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Span\nSpan.set_extension(\"is_city\", default=False)\nextension = Span.get_extension(\"is_city\")\nassert extension == (False, None, None, None)\n```\n\n----------------------------------------\n\nTITLE: Generating HTML with displaCy.render\nDESCRIPTION: This example shows how to use displaCy.render to generate HTML markup for dependency visualizations without starting a web server. It processes multiple sentences and renders them as a complete HTML page.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc1 = nlp(\"This is a sentence.\")\ndoc2 = nlp(\"This is another sentence.\")\nhtml = displacy.render([doc1, doc2], style=\"dep\", page=True)\n```\n\n----------------------------------------\n\nTITLE: Scoring Spans Example\nDESCRIPTION: Demonstrates how to calculate precision, recall, and F-score for labeled or unlabeled spans like named entities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/scorer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nscores = Scorer.score_spans(examples, \"ents\")\nprint(scores[\"ents_f\"])\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Labels to NER Component\nDESCRIPTION: Shows how to manually add new entity labels to the NER component. This is typically not needed if using initialize() with representative samples.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\nner.add_label(\"MY_LABEL\")\n```\n\n----------------------------------------\n\nTITLE: Loading Transformer Component from Disk in Python\nDESCRIPTION: This snippet demonstrates how to load a previously saved Transformer component from disk, modifying the existing object in place.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"transformer\")\ntrf.from_disk(\"/path/to/transformer\")\n```\n\n----------------------------------------\n\nTITLE: Loading Only Binary Data from a spaCy Pipeline\nDESCRIPTION: Shows how to load only the binary data from a spaCy pipeline without the pipeline configuration, by creating a blank Language instance and calling from_disk().\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.blank(\"en\").from_disk(\"/path/to/data\")\n```\n\n----------------------------------------\n\nTITLE: Loading Vocab State from Disk (Python)\nDESCRIPTION: Demonstrates how to load the state of a Vocab object from a directory using the from_disk method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.vocab import Vocab\nvocab = Vocab().from_disk(\"/path/to/vocab\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Pipeline and Frozen Components in spaCy (INI)\nDESCRIPTION: Configuration example showing how to define the pipeline order and specify which components should be frozen (not updated) during training. Frozen components are included in the final pipeline as-is.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_12\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\nlang = \"en\"\npipeline = [\"parser\", \"ner\", \"textcat\", \"custom\"]\n\n[training]\nfrozen_components = [\"parser\", \"custom\"]\n```\n\n----------------------------------------\n\nTITLE: Checking Feature/Value Presence in MorphAnalysis in Python\nDESCRIPTION: Demonstrates how to check if a specific feature/value pair is contained in a MorphAnalysis object using the __contains__ method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfeats = \"Feat1=Val1,Val2|Feat2=Val2\"\nmorph = MorphAnalysis(nlp.vocab, feats)\nassert \"Feat1=Val1\" in morph\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parser Parameters\nDESCRIPTION: Shows how to temporarily modify the parser's model parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nparser = DependencyParser(nlp.vocab)\nwith parser.use_params(optimizer.averages):\n    parser.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Finding Similar Vectors\nDESCRIPTION: Demonstrates finding the most similar vectors using cosine similarity.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nqueries = numpy.asarray([numpy.random.uniform(-1, 1, (300,))])\nmost_similar = nlp.vocab.vectors.most_similar(queries, n=10)\n```\n\n----------------------------------------\n\nTITLE: Scoring Span Predictions in Python using spaCy\nDESCRIPTION: This snippet shows how to use the score_span_predictions function to calculate accuracy for reconstructions of spans from single tokens. It takes examples and an output prefix as input and returns a dictionary containing the accuracy score.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/scorer.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nscores = score_span_predictions(\n    examples,\n    output_prefix=\"coref_clusters\",\n)\nprint(scores[\"span_coref_clusters_accuracy\"])\n```\n\n----------------------------------------\n\nTITLE: Example Lemma LLM Response Format\nDESCRIPTION: Example of the expected response format from an LLM for the Lemma task. Shows how tokens are mapped to their lemmatized forms.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_46\n\nLANGUAGE: text\nCODE:\n```\nI: I\n'm: be\nbuying: buy\nice: ice\ncream: cream\nfor: for\nmy: my\nfriends: friend\n.: .\n```\n\n----------------------------------------\n\nTITLE: Updating Default Config for Registered Function in Python\nDESCRIPTION: This snippet shows how to update the default_config to use the registered function for the acronym dictionary.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n- default_config = {\"dictionary:\" DICTIONARY}\n+ default_config = {\"dictionary\": {\"@misc\": \"acronyms.slang_dict.v1\"}}\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for a spaCy Project\nDESCRIPTION: Command to auto-generate a Markdown README file based on the project.yml configuration, documenting commands, workflows and assets.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project document --output README.md\n```\n\n----------------------------------------\n\nTITLE: Initializing DependencyParser with Pre-defined Labels in Python\nDESCRIPTION: This snippet shows how to initialize a DependencyParser with pre-defined labels. It retrieves the label data from the parser and uses it to initialize the model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlabels = parser.label_data\nparser.initialize(lambda: [], nlp=nlp, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Adding Patterns to SpanRuler\nDESCRIPTION: Shows how to add multiple patterns to a SpanRuler, including both token patterns and phrase patterns.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npatterns = [\n    {\"label\": \"ORG\", \"pattern\": \"Apple\"},\n    {\"label\": \"GPE\", \"pattern\": [{\"lower\": \"san\"}, {\"lower\": \"francisco\"}]}\n]\nruler = nlp.add_pipe(\"span_ruler\")\nruler.add_patterns(patterns)\n```\n\n----------------------------------------\n\nTITLE: Running spaCy Project Commands with Environment Variables\nDESCRIPTION: Bash commands demonstrating how to set environment variables and run a spaCy project command, allowing for command-line overrides of settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nexport GPU_ID=1\nBATCH_SIZE=128 python -m spacy project run evaluate\n```\n\n----------------------------------------\n\nTITLE: Applying SentenceRecognizer to a Document in spaCy\nDESCRIPTION: Example of how to apply the SentenceRecognizer to a single document, which modifies the document in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nsenter = nlp.add_pipe(\"senter\")\n# This usually happens under the hood\nprocessed = senter(doc)\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Learning Examples for Sentiment Task in YAML\nDESCRIPTION: YAML file format for providing few-shot examples to the Sentiment task. Each example contains text and its corresponding sentiment score from 0 to 1.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_50\n\nLANGUAGE: yaml\nCODE:\n```\n- text: 'This is horrifying.'\n  score: 0\n- text: 'This is underwhelming.'\n  score: 0.25\n- text: 'This is ok.'\n  score: 0.5\n- text: \"I'm looking forward to this!\"\n  score: 1.0\n```\n\n----------------------------------------\n\nTITLE: Getting the Length of a Doc (Python)\nDESCRIPTION: Demonstrates how to get the number of tokens in a Doc object using the len() function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\nassert len(doc) == 7\n```\n\n----------------------------------------\n\nTITLE: Serializing EditTreeLemmatizer to Bytes in Python\nDESCRIPTION: Illustrates how to serialize the EditTreeLemmatizer pipe to a bytestring using the to_bytes method. This method returns the serialized form of the lemmatizer as bytes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\nlemmatizer_bytes = lemmatizer.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Customizing Span Visualization Options in spaCy\nDESCRIPTION: This snippet shows how to customize the span visualizer by specifying a custom spans key. It creates a span with the label 'BANK' and uses the options parameter to tell displaCy to use this custom span collection.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndoc.spans[\"custom\"] = [Span(doc, 3, 6, \"BANK\")]\noptions = {\"spans_key\": \"custom\"}\ndisplacy.serve(doc, style=\"span\", options=options)\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Annotation Setter in spaCy Transformers\nDESCRIPTION: This snippet demonstrates how to create and register a custom annotation setter function using the @registry.annotation_setters decorator. The example shows a null annotation setter that doesn't perform any operations on the documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n@registry.annotation_setters(\"spacy-transformers.null_annotation_setter.v1\")\ndef configure_null_annotation_setter() -> Callable:\n    def setter(docs: List[Doc], trf_data: FullTransformerBatch) -> None:\n        pass\n\n    return setter\n```\n\n----------------------------------------\n\nTITLE: Loading KnowledgeBase from Disk in Python\nDESCRIPTION: Example of restoring the state of the knowledge base from a directory using the from_disk method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/kb.mdx#2025-04-21_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom spacy.vocab import Vocab\nvocab = Vocab().from_disk(\"/path/to/vocab\")\nkb = FullyImplementedKB(vocab=vocab, entity_vector_length=64)\nkb.from_disk(\"/path/to/kb\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Span Root Token in spaCy\nDESCRIPTION: Demonstrates how to access the root token of a span, which is the token with the shortest path to the root of the sentence.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York in Autumn.\")\ni, like, new, york, in_, autumn, dot = range(len(doc))\nassert doc[new].head.text == \"York\"\nassert doc[york].head.text == \"like\"\nnew_york = doc[new:york+1]\nassert new_york.root.text == \"York\"\n```\n\n----------------------------------------\n\nTITLE: Initializing TrainablePipe Component for Training in Python\nDESCRIPTION: Shows how to initialize a TrainablePipe component for training, including setting up the model and label scheme.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\npipe.initialize(lambda: [], pipeline=nlp.pipeline)\n```\n\n----------------------------------------\n\nTITLE: Initializing Morphologizer Component\nDESCRIPTION: Shows how to initialize the morphologizer component for training with examples and configuration settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\nmorphologizer.initialize(lambda: examples, nlp=nlp)\n```\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[initialize.components.morphologizer]\n\n[initialize.components.morphologizer.labels]\n@readers = \"spacy.read_labels.v1\"\npath = \"corpus/labels/morphologizer.json\n```\n\n----------------------------------------\n\nTITLE: Applying Morphologizer to a Document in spaCy\nDESCRIPTION: Example showing how to manually apply the morphologizer to a document, though this typically happens automatically when processing text through the spaCy pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nmorphologizer = nlp.add_pipe(\"morphologizer\")\n# This usually happens under the hood\nprocessed = morphologizer(doc)\n```\n\n----------------------------------------\n\nTITLE: Creating LLM-powered NER Component Directly in Python\nDESCRIPTION: Python code demonstrating how to create an LLM-powered Named Entity Recognition component directly in Python without using a configuration file. It adds labels and processes text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.blank(\"en\")\nllm_ner = nlp.add_pipe(\"llm_ner\")\nllm_ner.add_label(\"PERSON\")\nllm_ner.add_label(\"LOCATION\")\nnlp.initialize()\ndoc = nlp(\"Jack and Jill rode up the hill in Les Deux Alpes\")\nprint([(ent.text, ent.label_) for ent in doc.ents])\n```\n\n----------------------------------------\n\nTITLE: Splitting Example into Sentences in spaCy\nDESCRIPTION: Demonstrates how to split a single Example object into multiple Examples based on sentence boundaries using the split_sents method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I went yesterday had lots of fun\")\ntokens_ref = [\"I\", \"went\", \"yesterday\", \"had\", \"lots\", \"of\", \"fun\"]\nsents_ref = [True, False, False, True, False, False, False]\nexample = Example.from_dict(doc, {\"words\": tokens_ref, \"sent_starts\": sents_ref})\nsplit_examples = example.split_sents()\nassert split_examples[0].text == \"I went yesterday \"\nassert split_examples[1].text == \"had lots of fun\"\n```\n\n----------------------------------------\n\nTITLE: Serializing Tok2Vec Component to Bytes in Python\nDESCRIPTION: Demonstrates how to serialize the Tok2Vec component to a bytestring. This is useful for storing the component in databases or transmitting it over networks.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntok2vec = nlp.add_pipe(\"tok2vec\")\ntok2vec_bytes = tok2vec.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Getting Vector Shape in Python\nDESCRIPTION: Property to get the shape of the vector table as a tuple of (rows, dimensions).\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef shape(self) -> Tuple[int, int]:\n```\n\n----------------------------------------\n\nTITLE: Converting BILUO Tags to IOB Tags in spaCy\nDESCRIPTION: Demonstrates how to convert a sequence of BILUO tags to IOB tags, useful for integrating BILUO tags with models that only support the IOB format for named entity recognition.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import biluo_to_iob\n\ntags = [\"O\", \"O\", \"B-LOC\", \"I-LOC\", \"L-LOC\", \"O\"]\niob_tags = biluo_to_iob(tags)\nassert iob_tags == [\"O\", \"O\", \"B-LOC\", \"I-LOC\", \"I-LOC\", \"O\"]\n```\n\n----------------------------------------\n\nTITLE: Accessing Language Meta Information in spaCy (Python)\nDESCRIPTION: Example showing how to access the meta property which contains metadata about the Language class or loaded pipeline, including name, version, data sources, license, and author information.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nprint(nlp.meta)\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCat v3 Component in spaCy\nDESCRIPTION: Configuration for TextCat v3 that includes label definitions for text classification. Demonstrates setting up compliment and insult categories with their definitions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_36\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.TextCat.v3\"\nlabels = [\"COMPLIMENT\", \"INSULT\"]\n\n[components.llm.task.label_definitions]\n\"COMPLIMENT\" = \"a polite expression of praise or admiration.\",\n\"INSULT\" = \"a disrespectful or scornfully abusive remark or act.\"\nexamples = null\n```\n\n----------------------------------------\n\nTITLE: Iterating and Modifying SpanGroup in Python\nDESCRIPTION: Demonstrates how to iterate over spans in a SpanGroup, append new spans, and extend the group with multiple spans.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spangroup.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Their goi ng home\")\ndoc.spans[\"errors\"] = [doc[0:1], doc[1:3]]\nfor error_span in doc.spans[\"errors\"]:\n    print(error_span)\ndoc.spans[\"errors\"].append(doc[3:4])\ndoc.spans[\"errors\"].extend([doc[1:3], doc[0:1]])\nassert len(doc.spans[\"errors\"]) == 4\n```\n\n----------------------------------------\n\nTITLE: Example output of debug data command\nDESCRIPTION: Comprehensive output from the debug data command showing validation results, training statistics, vocabulary information, entity recognition details, part-of-speech tags, and dependency parsing analysis.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_19\n\nLANGUAGE: text\nCODE:\n```\n=========================== Data format validation ===========================\n✔ Corpus is loadable\n✔ Pipeline can be initialized with data\n\n=============================== Training stats ===============================\nTraining pipeline: tagger, parser, ner\nStarting with blank model 'en'\n18127 training docs\n2939 evaluation docs\n⚠ 34 training examples also in evaluation data\n\n============================== Vocab & Vectors ==============================\nℹ 2083156 total words in the data (56962 unique)\n⚠ 13020 misaligned tokens in the training data\n⚠ 2423 misaligned tokens in the dev data\n10 most common words: 'the' (98429), ',' (91756), '.' (87073), 'to' (50058),\n'of' (49559), 'and' (44416), 'a' (34010), 'in' (31424), 'that' (22792), 'is'\n(18952)\nℹ No word vectors present in the model\n\n========================== Named Entity Recognition ==========================\nℹ 18 new labels, 0 existing labels\n528978 missing values (tokens with '-' label)\nNew: 'ORG' (23860), 'PERSON' (21395), 'GPE' (21193), 'DATE' (18080), 'CARDINAL'\n(10490), 'NORP' (9033), 'MONEY' (5164), 'PERCENT' (3761), 'ORDINAL' (2122),\n'LOC' (2113), 'TIME' (1616), 'WORK_OF_ART' (1229), 'QUANTITY' (1150), 'FAC'\n(1134), 'EVENT' (974), 'PRODUCT' (935), 'LAW' (444), 'LANGUAGE' (338)\n✔ Good amount of examples for all labels\n✔ Examples without occurrences available for all labels\n✔ No entities consisting of or starting/ending with whitespace\n\n=========================== Part-of-speech Tagging ===========================\nℹ 49 labels in data\n'NN' (266331), 'IN' (227365), 'DT' (185600), 'NNP' (164404), 'JJ' (119830),\n'NNS' (110957), '.' (101482), ',' (92476), 'RB' (90090), 'PRP' (90081), 'VB'\n(74538), 'VBD' (68199), 'CC' (62862), 'VBZ' (50712), 'VBP' (43420), 'VBN'\n(42193), 'CD' (40326), 'VBG' (34764), 'TO' (31085), 'MD' (25863), 'PRP$'\n(23335), 'HYPH' (13833), 'POS' (13427), 'UH' (13322), 'WP' (10423), 'WDT'\n(9850), 'RP' (8230), 'WRB' (8201), ':' (8168), '''' (7392), '``' (6984), 'NNPS'\n(5817), 'JJR' (5689), '$' (3710), 'EX' (3465), 'JJS' (3118), 'RBR' (2872),\n'-RRB-' (2825), '-LRB-' (2788), 'PDT' (2078), 'XX' (1316), 'RBS' (1142), 'FW'\n(794), 'NFP' (557), 'SYM' (440), 'WP$' (294), 'LS' (293), 'ADD' (191), 'AFX'\n(24)\n\n============================= Dependency Parsing =============================\nℹ Found 111703 sentences with an average length of 18.6 words.\nℹ Found 2251 nonprojective train sentences\nℹ Found 303 nonprojective dev sentences\nℹ 47 labels in train data\nℹ 211 labels in projectivized train data\n'punct' (236796), 'prep' (188853), 'pobj' (182533), 'det' (172674), 'nsubj'\n(169481), 'compound' (116142), 'ROOT' (111697), 'amod' (107945), 'dobj' (93540),\n'aux' (86802), 'advmod' (86197), 'cc' (62679), 'conj' (59575), 'poss' (36449),\n'ccomp' (36343), 'advcl' (29017), 'mark' (27990), 'nummod' (24582), 'relcl'\n(21359), 'xcomp' (21081), 'attr' (18347), 'npadvmod' (17740), 'acomp' (17204),\n'auxpass' (15639), 'appos' (15368), 'neg' (15266), 'nsubjpass' (13922), 'case'\n(13408), 'acl' (12574), 'pcomp' (10340), 'nmod' (9736), 'intj' (9285), 'prt'\n(8196), 'quantmod' (7403), 'dep' (4300), 'dative' (4091), 'agent' (3908), 'expl'\n(3456), 'parataxis' (3099), 'oprd' (2326), 'predet' (1946), 'csubj' (1494),\n'subtok' (1147), 'preconj' (692), 'meta' (469), 'csubjpass' (64), 'iobj' (1)\n⚠ Low number of examples for label 'iobj' (1)\n⚠ Low number of examples for 130 labels in the projectivized dependency\ntrees used for training. You may want to projectivize labels such as punct\nbefore training in order to improve parser performance.\n⚠ Projectivized labels with low numbers of examples: appos||attr: 12\nadvmod||dobj: 13 prep||ccomp: 12 nsubjpass||ccomp: 15 pcomp||prep: 14\namod||dobj: 9 attr||xcomp: 14 nmod||nsubj: 17 prep||advcl: 2 prep||prep: 5\nnsubj||conj: 12 advcl||advmod: 18 ccomp||advmod: 11 ccomp||pcomp: 5 acl||pobj:\n10 npadvmod||acomp: 7 dobj||pcomp: 14 nsubjpass||pcomp: 1 nmod||pobj: 8\namod||attr: 6 nmod||dobj: 12 aux||conj: 1 neg||conj: 1 dative||xcomp: 11\npobj||dative: 3 xcomp||acomp: 19 advcl||pobj: 2 nsubj||advcl: 2 csubj||ccomp: 1\nadvcl||acl: 1 relcl||nmod: 2 dobj||advcl: 10 advmod||advcl: 3 nmod||nsubjpass: 6\namod||pobj: 5 cc||neg: 1 attr||ccomp: 16 advcl||xcomp: 3 nmod||attr: 4\nadvcl||nsubjpass: 5 advcl||ccomp: 4 ccomp||conj: 1 punct||acl: 1 meta||acl: 1\nparataxis||acl: 1 prep||acl: 1 amod||nsubj: 7 ccomp||ccomp: 3 acomp||xcomp: 5\ndobj||acl: 5 prep||oprd: 6 advmod||acl: 2 dative||advcl: 1 pobj||agent: 5\nxcomp||amod: 1 dep||advcl: 1 prep||amod: 8 relcl||compound: 1 advcl||csubj: 3\nnpadvmod||conj: 2 npadvmod||xcomp: 4 advmod||nsubj: 3 ccomp||amod: 7\nadvcl||conj: 1 nmod||conj: 2 advmod||nsubjpass: 2 dep||xcomp: 2 appos||ccomp: 1\nadvmod||dep: 1 advmod||advmod: 5 aux||xcomp: 8 dep||advmod: 1 dative||ccomp: 2\nprep||dep: 1 conj||conj: 1 dep||ccomp: 4 cc||ROOT: 1 prep||ROOT: 1 nsubj||pcomp:\n3 advmod||prep: 2 relcl||dative: 1 acl||conj: 1 advcl||attr: 4 prep||npadvmod: 1\nnsubjpass||xcomp: 1 neg||advmod: 1 xcomp||oprd: 1 advcl||advcl: 1 dobj||dep: 3\nnsubjpass||parataxis: 1 attr||pcomp: 1 ccomp||parataxis: 1 advmod||attr: 1\nnmod||oprd: 1 appos||nmod: 2 advmod||relcl: 1 appos||npadvmod: 1 appos||conj: 1\nprep||expl: 1 nsubjpass||conj: 1 punct||pobj: 1 cc||pobj: 1 conj||pobj: 1\npunct||conj: 1 ccomp||dep: 1 oprd||xcomp: 3 ccomp||xcomp: 1 ccomp||nsubj: 1\nnmod||dep: 1 xcomp||ccomp: 1 acomp||advcl: 1 intj||advmod: 1 advmod||acomp: 2\nrelcl||oprd: 1 advmod||prt: 1 advmod||pobj: 1 appos||nummod: 1 relcl||npadvmod:\n3 mark||advcl: 1 aux||ccomp: 1 amod||nsubjpass: 1 npadvmod||advmod: 1 conj||dep:\n1 nummod||pobj: 1 amod||npadvmod: 1 intj||pobj: 1 nummod||npadvmod: 1\nxcomp||xcomp: 1 aux||dep: 1 advcl||relcl: 1\n⚠ The following labels were found only in the train data: xcomp||amod,\nadvcl||relcl, prep||nsubjpass, acl||nsubj, nsubjpass||conj, xcomp||oprd,\nadvmod||conj, advmod||advmod, iobj, advmod||nsubjpass, dobj||conj, ccomp||amod,\nmeta||acl, xcomp||xcomp, prep||attr, prep||ccomp, advcl||acomp, acl||dobj,\nadvcl||advcl, pobj||agent, prep||advcl, nsubjpass||xcomp, prep||dep,\nacomp||xcomp, aux||ccomp, ccomp||dep, conj||dep, relcl||compound,\nnsubjpass||ccomp, nmod||dobj, advmod||advcl, advmod||acl, dobj||advcl,\ndative||xcomp, prep||nsubj, ccomp||ccomp, nsubj||ccomp, xcomp||acomp,\nprep||acomp, dep||advmod, acl||pobj, appos||dobj, npadvmod||acomp, cc||ROOT,\nrelcl||nsubj, nmod||pobj, acl||nsubjpass, ccomp||advmod, pcomp||prep,\namod||dobj, advmod||attr, advcl||csubj, appos||attr, dobj||pcomp, prep||ROOT,\nrelcl||pobj, advmod||pobj, amod||nsubj, ccomp||xcomp, prep||oprd,\nnpadvmod||advmod, appos||nummod, advcl||pobj, neg||advmod, acl||attr,\nappos||nsubjpass, csubj||ccomp, amod||nsubjpass, intj||pobj, dep||advcl,\ncc||neg, xcomp||ccomp, dative||ccomp, nmod||oprd, pobj||dative, prep||dobj,\ndep||ccomp, relcl||attr, ccomp||nsubj, advcl||xcomp, nmod||dep, advcl||advmod,\nccomp||conj, pobj||prep, advmod||acomp, advmod||relcl, attr||pcomp,\nccomp||parataxis, oprd||xcomp, intj||advmod, nmod||nsubjpass, prep||npadvmod,\nparataxis||acl, prep||pobj, advcl||dobj, amod||pobj, prep||acl, conj||pobj,\nadvmod||dep, punct||pobj, ccomp||acomp, acomp||advcl, nummod||npadvmod,\ndobj||dep, npadvmod||xcomp, advcl||conj, relcl||npadvmod, punct||acl,\nrelcl||dobj, dobj||xcomp, nsubjpass||parataxis, dative||advcl, relcl||nmod,\nadvcl||ccomp, appos||npadvmod, ccomp||pcomp, prep||amod, mark||advcl,\nprep||advmod, prep||xcomp, appos||nsubj, attr||ccomp, advmod||prt, dobj||ccomp,\naux||conj, advcl||nsubj, conj||conj, advmod||ccomp, advcl||nsubjpass,\nattr||xcomp, nmod||conj, npadvmod||conj, relcl||dative, prep||expl,\nnsubjpass||pcomp, advmod||xcomp, advmod||dobj, appos||pobj, nsubj||conj,\nrelcl||nsubjpass, advcl||attr, appos||ccomp, advmod||prep, prep||conj,\nnmod||attr, punct||conj, neg||conj, dep||xcomp, aux||xcomp, dobj||acl,\nnummod||pobj, amod||npadvmod, nsubj||pcomp, advcl||acl, appos||nmod,\nrelcl||oprd, prep||prep, cc||pobj, nmod||nsubj, amod||attr, aux||dep,\nappos||conj, advmod||nsubj, nsubj||advcl, acl||conj\nTo train a parser, your data should include at least 20 instances of each label.\n⚠ Multiple root labels (ROOT, nsubj, aux, npadvmod, prep) found in\ntraining data. spaCy's parser uses a single root label ROOT so this distinction\nwill not be available.\n\n================================== Summary ==================================\n✔ 5 checks passed\n⚠ 8 warnings\n```\n\n----------------------------------------\n\nTITLE: Retrieving Rules from DependencyMatcher in Python\nDESCRIPTION: This example demonstrates how to retrieve a rule from a DependencyMatcher using the 'get' method. It returns the callback function and patterns associated with the rule.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencymatcher.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmatcher.add(\"FOUNDED\", patterns, on_match=on_match)\non_match, patterns = matcher.get(\"FOUNDED\")\n```\n\n----------------------------------------\n\nTITLE: Loading Lemmatizer from Disk\nDESCRIPTION: Example showing how to load a serialized lemmatizer pipeline component from disk using the from_disk() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lemmatizer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"lemmatizer\")\nlemmatizer.from_disk(\"/path/to/lemmatizer\")\n```\n\n----------------------------------------\n\nTITLE: Updating EntityLinker Model in spaCy (Python)\nDESCRIPTION: Demonstrates how to update the EntityLinker model using a batch of Example objects. This method trains both the entity linking model and the context encoder, using an optimizer for gradient updates.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nentity_linker = nlp.add_pipe(\"entity_linker\")\noptimizer = nlp.initialize()\nlosses = entity_linker.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Configuring MaxoutWindowEncoder for Context Encoding in spaCy\nDESCRIPTION: This snippet shows the configuration for the MaxoutWindowEncoder architecture in spaCy. It sets up a context encoder using convolutions with maxout activation, layer normalization, and residual connections, allowing customization of the network's width, depth, and window size.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.MaxoutWindowEncoder.v2\"\nwidth = 128\nwindow_size = 1\nmaxout_pieces = 3\ndepth = 4\n```\n\n----------------------------------------\n\nTITLE: Configuring PretrainCharacters Objective in spaCy\nDESCRIPTION: Configuration for the Characters pretraining objective in spaCy that predicts leading and trailing UTF-8 bytes of words. This objective uses maxout pieces, hidden size parameters, and specifies the number of characters to predict.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_22\n\nLANGUAGE: ini\nCODE:\n```\n### Characters objective\n[pretraining.objective]\n@architectures = \"spacy.PretrainCharacters.v1\"\nmaxout_pieces = 3\nhidden_size = 300\nn_characters = 4\n```\n\n----------------------------------------\n\nTITLE: Retrieving Multiple Entity Vectors from KnowledgeBase in Python\nDESCRIPTION: Example of using the get_vectors method to retrieve pretrained entity vectors for multiple entity IDs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/kb.mdx#2025-04-21_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nvectors = kb.get_vectors((\"Q42\", \"Q3107329\"))\n```\n\n----------------------------------------\n\nTITLE: Configuration for Initializing Component Labels\nDESCRIPTION: This configuration snippet shows how to provide explicit labels for model components when streaming data, by specifying a path to a JSON file with the label definitions that will be used during initialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_38\n\nLANGUAGE: ini\nCODE:\n```\n[initialize.components.textcat.labels]\n@readers = \"spacy.read_labels.v1\"\npath = \"labels/textcat.json\"\nrequire = true\n```\n\n----------------------------------------\n\nTITLE: Adding Doc to DocBin\nDESCRIPTION: Shows how to add a Doc object to DocBin for serialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/docbin.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc_bin = DocBin(attrs=[\"LEMMA\"])\ndoc = nlp(\"This is a document to serialize.\")\ndoc_bin.add(doc)\n```\n\n----------------------------------------\n\nTITLE: Adding Morphological Analysis in Python\nDESCRIPTION: Inserts a morphological analysis in the morphology table if not already present. The features can be provided in Universal Dependencies FEATS format as a string. Returns the hash of the new analysis.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfeats = \"Feat1=Val1|Feat2=Val2\"\nhash = nlp.vocab.morphology.add(feats)\nassert hash == nlp.vocab.strings[feats]\n```\n\n----------------------------------------\n\nTITLE: Configuring remotes in project.yml\nDESCRIPTION: Example of how to define remote storage locations in the project.yml file, including an S3 bucket and a local filesystem path.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_27\n\nLANGUAGE: yaml\nCODE:\n```\nremotes:\n  default: 's3://my-spacy-bucket'\n  local: '/mnt/scratch/cache'\n```\n\n----------------------------------------\n\nTITLE: Adding Sentencizer with Custom Config\nDESCRIPTION: Example showing how to add the sentencizer to the pipeline with custom configuration for punctuation characters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencizer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\"punct_chars\": None}\nnlp.add_pipe(\"sentencizer\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Configuring SentenceRecognizer in spaCy Pipeline\nDESCRIPTION: Example of how to configure and add the SentenceRecognizer to the spaCy pipeline using the default senter model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.pipeline.senter import DEFAULT_SENTER_MODEL\nconfig = {\"model\": DEFAULT_SENTER_MODEL,}\nnlp.add_pipe(\"senter\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Accessing Example Text Property (Python)\nDESCRIPTION: Shows how to access the text of the predicted document in an Example object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nraw_text = example.text\n```\n\n----------------------------------------\n\nTITLE: Loading EditTreeLemmatizer from Bytes in Python\nDESCRIPTION: Demonstrates how to load the EditTreeLemmatizer pipe from a bytestring using the from_bytes method. This method modifies the object in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer_bytes = lemmatizer.to_bytes()\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\nlemmatizer.from_bytes(lemmatizer_bytes)\n```\n\n----------------------------------------\n\nTITLE: Initializing AttributeRuler via add_pipe\nDESCRIPTION: Example demonstrating how to initialize an AttributeRuler component by adding it to a spaCy pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributeruler.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe\nruler = nlp.add_pipe(\"attribute_ruler\")\n```\n\n----------------------------------------\n\nTITLE: Initializing SpanFinder Component\nDESCRIPTION: Shows how to initialize a SpanFinder component with training examples. The method requires a function that returns Example objects and optionally takes an nlp object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nspan_finder = nlp.add_pipe(\"span_finder\")\nspan_finder.initialize(lambda: examples, nlp=nlp)\n```\n\n----------------------------------------\n\nTITLE: Accessing Multiple Sentences for Span in spaCy\nDESCRIPTION: Demonstrates how to access all sentences that a span belongs to.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\nspan = doc[2:4]\nassert len(span.sents) == 2\n```\n\n----------------------------------------\n\nTITLE: Loading LLM Components from Disk in spaCy\nDESCRIPTION: Example showing how to load a previously saved LLM component from disk, restoring its configuration and state.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nllm_ner = nlp.add_pipe(\"llm_ner\")\nllm_ner.from_disk(\"/path/to/llm_ner\")\n```\n\n----------------------------------------\n\nTITLE: Deserializing LLM Components from Bytes in spaCy\nDESCRIPTION: Example showing how to load an LLM component from a previously serialized bytestring, restoring its configuration and state.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nner_bytes = llm_ner.to_bytes()\nllm_ner = nlp.add_pipe(\"llm_ner\")\nllm_ner.from_bytes(ner_bytes)\n```\n\n----------------------------------------\n\nTITLE: Checking if a Language Class is Loaded in spaCy\nDESCRIPTION: Demonstrates how to check if a Language subclass is already loaded. Language classes are loaded lazily to avoid expensive setup code associated with language data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nlang_cls = util.get_lang_class(\"en\")\nassert util.lang_class_is_loaded(\"en\") is True\nassert util.lang_class_is_loaded(\"de\") is False\n```\n\n----------------------------------------\n\nTITLE: Setting Text Category Annotations in spaCy\nDESCRIPTION: Shows how to modify documents with pre-computed text category scores using the set_annotations method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\nscores = textcat.predict(docs)\ntextcat.set_annotations(docs, scores)\n```\n\n----------------------------------------\n\nTITLE: Setting Annotations with Tok2Vec\nDESCRIPTION: Example of applying pre-computed Tok2Vec scores to modify a batch of documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntok2vec = nlp.add_pipe(\"tok2vec\")\nscores = tok2vec.predict(docs)\ntok2vec.set_annotations(docs, scores)\n```\n\n----------------------------------------\n\nTITLE: Applying LLM Components to Documents in spaCy\nDESCRIPTION: Example showing how to apply an LLM-based named entity recognition component to a document in spaCy, which typically happens automatically when the nlp pipeline processes text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Ingrid visited Paris.\")\nllm_ner = nlp.add_pipe(\"llm_ner\")\n# This usually happens under the hood\nprocessed = llm_ner(doc)\n```\n\n----------------------------------------\n\nTITLE: Setting Annotations with SpanResolver in Python\nDESCRIPTION: Modify a batch of documents by saving predicted spans using the output prefix in Doc.spans.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\nspans = span_resolver.predict([doc1, doc2])\nspan_resolver.set_annotations([doc1, doc2], spans)\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for EditTreeLemmatizer\nDESCRIPTION: Example showing how to create an optimizer for the lemmatizer component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\noptimizer = lemmatizer.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Creating SpanFinder Optimizer\nDESCRIPTION: Demonstrates creation of an optimizer for the SpanFinder component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nspan_finder = nlp.add_pipe(\"span_finder\")\noptimizer = span_finder.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Configuring GPT-4 Model in spaCy\nDESCRIPTION: Example configuration for setting up GPT-4 model in spaCy's components. Shows basic setup with temperature parameter set to 0.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_53\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.model]\n@llm_models = \"spacy.GPT-4.v1\"\nname = \"gpt-4\"\nconfig = {\"temperature\": 0.0}\n```\n\n----------------------------------------\n\nTITLE: Predicting with SpanFinder\nDESCRIPTION: Demonstrates how to apply the SpanFinder model to documents without modifying them. Returns prediction scores for the input documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nspan_finder = nlp.add_pipe(\"span_finder\")\nscores = span_finder.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Initializing spaCy Configuration in INI Format\nDESCRIPTION: Example of initializing spaCy configuration using INI format. It demonstrates how to specify paths for vectors and tok2vec initialization, as well as custom component initialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\n[initialize]\nvectors = \"/path/to/vectors_nlp\"\ninit_tok2vec = \"/path/to/pretrain.bin\"\n\n[initialize_components]\n\n[initialize.components.my_component]\ndata_path = \"/path/to/component_data\"\n```\n\n----------------------------------------\n\nTITLE: Batch Processing with spaCy\nDESCRIPTION: Demonstrates how to iterate over batches of training data using the minibatch utility function for efficient processing.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nbatches = minibatch(train_data)\nfor batch in batches:\n    nlp.update(batch)\n```\n\n----------------------------------------\n\nTITLE: Scoring Dependencies Example\nDESCRIPTION: Shows how to calculate UAS (Unlabeled Attachment Score) and LAS (Labeled Attachment Score) for dependency parsing, with custom getter function and label filtering.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/scorer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef dep_getter(token, attr):\n    dep = getattr(token, attr)\n    dep = token.vocab.strings.as_string(dep).lower()\n    return dep\n\nscores = Scorer.score_deps(\n    examples,\n    \"dep\",\n    getter=dep_getter,\n    ignore_labels=(\"p\", \"punct\")\n)\nprint(scores[\"dep_uas\"], scores[\"dep_las\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing Annotation Setting\nDESCRIPTION: Implementation of set_annotations method that stores model predictions in the custom doc._.rel attribute for each entity pair.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef set_annotations(self, docs: Iterable[Doc], predictions: Floats2d):\n    c = 0\n    get_instances = self.model.attrs[\"get_instances\"]\n    for doc in docs:\n        for (e1, e2) in get_instances(doc):\n            offset = (e1.start, e2.start)\n            if offset not in doc._.rel:\n                doc._.rel[offset] = {}\n            for j, label in enumerate(self.labels):\n                doc._.rel[offset][label] = predictions[c, j]\n            c += 1\n```\n\n----------------------------------------\n\nTITLE: Initializing EntityRuler in Python\nDESCRIPTION: Example of how to initialize an EntityRuler component and add it to the spaCy pipeline. It demonstrates setting up the EntityRuler with patterns.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nentity_ruler = nlp.add_pipe(\"entity_ruler\")\nentity_ruler.initialize(lambda: [], nlp=nlp, patterns=patterns)\n```\n\n----------------------------------------\n\nTITLE: Saving EntityRecognizer to Disk\nDESCRIPTION: Demonstrates how to serialize a named entity recognition model to disk using the to_disk() method. Accepts a path parameter and optional exclude list for serialization fields.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\nner.to_disk(\"/path/to/ner\")\n```\n\n----------------------------------------\n\nTITLE: Updating Model Loading in spaCy 2.0\nDESCRIPTION: Modify spacy.load() calls to remove the 'path' keyword argument and use Language.from_disk() for loading binary data in spaCy 2.0.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n- nlp = spacy.load(\"en\", path=\"/model\")\n\n+ nlp = spacy.load(\"/model\")\n+ nlp = spacy.blank(\"en\").from_disk(\"/model/data\")\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Suffixes to spaCy's Tokenizer\nDESCRIPTION: This snippet shows how to add additional suffixes to the default suffix rules in spaCy's tokenizer. It demonstrates how to extend the existing rules with a custom pattern for handling trailing hyphens followed by plus signs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nsuffixes = nlp.Defaults.suffixes + [r'''-+$''',]\nsuffix_regex = spacy.util.compile_suffix_regex(suffixes)\nnlp.tokenizer.suffix_search = suffix_regex.search\n```\n\n----------------------------------------\n\nTITLE: Running find-threshold Command for textcat_multilabel in spaCy\nDESCRIPTION: Commands for finding optimal threshold values for a textcat_multilabel component. This command runs prediction trials with different thresholds to maximize the cats_macro_f metric.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_37\n\nLANGUAGE: bash\nCODE:\n```\n# For textcat_multilabel:\n$ python -m spacy find-threshold my_nlp data.spacy textcat_multilabel threshold cats_macro_f\n```\n\n----------------------------------------\n\nTITLE: Using EditTreeLemmatizer in Pipeline\nDESCRIPTION: Example showing how to process a document through the EditTreeLemmatizer component, which modifies the document in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\n# This usually happens under the hood\nprocessed = lemmatizer(doc)\n```\n\n----------------------------------------\n\nTITLE: Configuring Tokenizer Customization in INI\nDESCRIPTION: Example of configuring a callback function in the spaCy config file to customize the tokenizer before initialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_26\n\nLANGUAGE: ini\nCODE:\n```\n[initialize]\n\n[initialize.before_init]\n@callbacks = \"customize_tokenizer\"\n```\n\n----------------------------------------\n\nTITLE: Accessing TextCategorizer Labels in Python\nDESCRIPTION: This snippet demonstrates how to access the labels currently added to the TextCategorizer component. It shows adding a label and then checking if it's in the labels property.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntextcat.add_label(\"MY_LABEL\")\nassert \"MY_LABEL\" in textcat.labels\n```\n\n----------------------------------------\n\nTITLE: Updating SpanCategorizer Model in Python\nDESCRIPTION: Example of updating the SpanCategorizer model using a batch of Example objects and an optimizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\noptimizer = nlp.initialize()\nlosses = spancat.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Aligning Parse Trees in spaCy (Python)\nDESCRIPTION: Demonstrates getting aligned dependency parse trees with optional projectivization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"He pretty quickly walks away\")\nexample = Example.from_dict(doc, {\"heads\": [3, 2, 3, 0, 2]})\nproj_heads, proj_labels = example.get_aligned_parse(projectivize=True)\nassert proj_heads == [3, 2, 3, 0, 3]\n```\n\n----------------------------------------\n\nTITLE: Setting Vector for Word in Vocabulary (Python)\nDESCRIPTION: Demonstrates how to set a vector for a word in the spaCy vocabulary using the set_vector method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nnlp.vocab.set_vector(\"apple\", array([...]))\n```\n\n----------------------------------------\n\nTITLE: Updating SpanResolver Model with Examples in Python\nDESCRIPTION: Train the SpanResolver model using a batch of Example objects. Uses the predict method and an optimizer for learning.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\noptimizer = nlp.initialize()\nlosses = span_resolver.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Prodigy Train Curve Configuration\nDESCRIPTION: YAML configuration for running Prodigy's train-curve recipe with spaCy to evaluate model performance with different portions of training data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_35\n\nLANGUAGE: yaml\nCODE:\n```\n- name: \"train_curve\"\n    help: \"Train the model with Prodigy by using different portions of training examples to evaluate if more annotations can potentially improve the performance\"\n    script:\n      - \"python -m prodigy train-curve --ner ${vars.prodigy.train_dataset},eval:${vars.prodigy.eval_dataset} --config configs/${vars.config} --show-plot\"\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for Coreference Resolver in Python with spaCy\nDESCRIPTION: This snippet demonstrates how to create an optimizer for the coreference resolver pipeline component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncoref = nlp.add_pipe(\"experimental_coref\")\noptimizer = coref.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Loading SentenceRecognizer from Bytes\nDESCRIPTION: Loads a SentenceRecognizer component from a bytestring representation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsenter_bytes = senter.to_bytes()\nsenter = nlp.add_pipe(\"senter\")\nsenter.from_bytes(senter_bytes)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Patterns from spaCy Matcher\nDESCRIPTION: Shows how to retrieve stored patterns and callback functions for a specific rule.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/matcher.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmatcher.add(\"Rule\", [[{\"ORTH\": \"test\"}]])\non_match, patterns = matcher.get(\"Rule\")\n```\n\n----------------------------------------\n\nTITLE: Working with EntityRecognizer Label Data\nDESCRIPTION: Demonstrates accessing and using label metadata for model initialization with pre-defined label sets.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlabels = ner.label_data\nner.initialize(lambda: [], nlp=nlp, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Checking Token Flags - Python\nDESCRIPTION: Example of checking boolean flags on tokens, such as title case.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs import IS_TITLE\ndoc = nlp(\"Give it back! He pleaded.\")\ntoken = doc[0]\nassert token.check_flag(IS_TITLE) == True\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Learning Example Format for NER in YAML\nDESCRIPTION: Example of how to structure few-shot learning examples for the NER task in YAML format. The examples include text samples with annotated entities that will be injected into the LLM prompt.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_31\n\nLANGUAGE: yaml\nCODE:\n```\n- text: Jack and Jill went up the hill.\n  entities:\n    PERSON:\n      - Jack\n      - Jill\n    LOCATION:\n      - hill\n- text: Jack fell down and broke his crown.\n  entities:\n    PERSON:\n      - Jack\n```\n\n----------------------------------------\n\nTITLE: Running a training command and pushing results to remote storage\nDESCRIPTION: Example of how to run a training command and then push the outputs to the configured remote storage using the spaCy CLI.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project run train\n$ python -m spacy project push\n```\n\n----------------------------------------\n\nTITLE: Checking Boolean Flags on Lexemes in spaCy\nDESCRIPTION: Example of how to add and check a custom boolean flag on a lexeme in spaCy's vocabulary. The example creates a flag that identifies library names and tests it on the 'spaCy' lexeme.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lexeme.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nis_my_library = lambda text: text in [\"spaCy\", \"Thinc\"]\nMY_LIBRARY = nlp.vocab.add_flag(is_my_library)\nassert nlp.vocab[\"spaCy\"].check_flag(MY_LIBRARY) == True\n```\n\n----------------------------------------\n\nTITLE: Using Operators in spaCy Matcher Token Patterns\nDESCRIPTION: Example showing the use of operators in token patterns. It matches zero or more adjectives, followed by one or more nouns, and exactly two proper nouns.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/matcher.mdx#2025-04-21_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\"POS\": \"ADJ\", \"OP\": \"*\"},\n  {\"POS\": \"NOUN\", \"OP\": \"+\"}\n  {\"POS\": \"PROPN\", \"OP\": \"{2}\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Processing Document Stream with Tok2Vec\nDESCRIPTION: Example of processing a stream of documents with the Tok2Vec component using batching for better performance.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntok2vec = nlp.add_pipe(\"tok2vec\")\nfor doc in tok2vec.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Deserializing a spaCy Doc from Bytes in Python\nDESCRIPTION: Shows how to deserialize a binary string back into a spaCy Doc object using the from_bytes() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc\ndoc = nlp(\"Give it back! He pleaded.\")\ndoc_bytes = doc.to_bytes()\ndoc2 = Doc(doc.vocab).from_bytes(doc_bytes)\nassert doc.text == doc2.text\n```\n\n----------------------------------------\n\nTITLE: Loading SpanResolver from Bytes in Python\nDESCRIPTION: Load a SpanResolver component from a bytestring, modifying the object in place.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nspan_resolver_bytes = span_resolver.to_bytes()\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\nspan_resolver.from_bytes(span_resolver_bytes)\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields During Tok2Vec Serialization in Python\nDESCRIPTION: Demonstrates how to exclude specific fields when serializing the Tok2Vec component. This can be useful for reducing the size of the serialized data or for excluding certain information.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndata = tok2vec.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Deserializing Vocab State from Bytes (Python)\nDESCRIPTION: Illustrates how to load the state of a Vocab object from a binary string using the from_bytes method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.vocab import Vocab\nvocab_bytes = nlp.vocab.to_bytes()\nvocab = Vocab()\nvocab.from_bytes(vocab_bytes)\n```\n\n----------------------------------------\n\nTITLE: Configuring Orthographic Variants Augmenter in spaCy\nDESCRIPTION: Configuration snippet showing how to set up the built-in orth_variants augmenter in the training corpus section of a spaCy config file, with settings for augmentation level and lowercasing percentage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_41\n\nLANGUAGE: ini\nCODE:\n```\n[corpora.train]\n@readers = \"spacy.Corpus.v1\"\npath = ${paths.train}\ngold_preproc = false\nmax_length = 0\nlimit = 0\n\n[corpora.train.augmenter]\n@augmenters = \"spacy.orth_variants.v1\"\n# Percentage of texts that will be augmented / lowercased\nlevel = 0.1\nlower = 0.5\n\n[corpora.train.augmenter.orth_variants]\n@readers = \"srsly.read_json.v1\"\npath = \"corpus/orth_variants.json\"\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Span Tokens\nDESCRIPTION: Shows how to iterate over Token objects within a Span.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\nspan = doc[1:4]\nassert [t.text for t in span] == [\"it\", \"back\", \"!\"]\n```\n\n----------------------------------------\n\nTITLE: Accessing Right Children of Token in spaCy\nDESCRIPTION: Shows how to access the rightward immediate children of a token in the syntactic dependency parse.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York in Autumn.\")\nrights = [t.text for t in doc[3].rights]\nassert rights == [\"in\"]\n```\n\n----------------------------------------\n\nTITLE: Implementing an Infinite Corpus Generator for Even/Odd Numbers\nDESCRIPTION: This Python code creates a custom corpus that generates examples with even or odd numbers for text classification. It provides both limited and unlimited streams of examples, which is useful for training and evaluation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Callable, Iterable, Iterator\nfrom spacy import util\nimport random\nfrom spacy.training import Example\nfrom spacy import Language\n\n\n@util.registry.readers(\"even_odd.v1\")\ndef create_even_odd_corpus(limit: int = -1) -> Callable[[Language], Iterable[Example]]:\n    return EvenOddCorpus(limit)\n\n\nclass EvenOddCorpus:\n    def __init__(self, limit):\n        self.limit = limit\n\n    def __call__(self, nlp: Language) -> Iterator[Example]:\n        i = 0\n        while i < self.limit or self.limit < 0:\n            r = random.randint(0, 1000)\n            cat = r % 2 == 0\n            text = \"This is sentence \" + str(r)\n            yield Example.from_dict(\n                nlp.make_doc(text), {\"cats\": {\"EVEN\": cat, \"ODD\": not cat}}\n            )\n            i += 1\n```\n\n----------------------------------------\n\nTITLE: Accessing Token Subtree in spaCy\nDESCRIPTION: Demonstrates how to access a token's complete syntactic subtree, including all descendants.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\ngive_subtree = doc[0].subtree\nassert [t.text for t in give_subtree] == [\"Give\", \"it\", \"back\", \"!\"]\n```\n\n----------------------------------------\n\nTITLE: Removing Rules from DependencyMatcher in Python\nDESCRIPTION: This snippet shows how to remove a rule from a DependencyMatcher using the 'remove' method. It demonstrates adding a rule, checking its existence, removing it, and verifying its absence.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencymatcher.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmatcher.add(\"FOUNDED\", patterns)\nassert \"FOUNDED\" in matcher\nmatcher.remove(\"FOUNDED\")\nassert \"FOUNDED\" not in matcher\n```\n\n----------------------------------------\n\nTITLE: Accessing EditTreeLemmatizer Label Data in Python\nDESCRIPTION: Shows how to access the label data of the EditTreeLemmatizer and use it to initialize the model with a pre-defined label set. This is useful for initializing the model with specific labels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlabels = lemmatizer.label_data\nlemmatizer.initialize(lambda: [], nlp=nlp, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: AttributeRuler Configuration in config.cfg\nDESCRIPTION: Example of how to configure the AttributeRuler initialization in a spaCy configuration file, specifying pattern loading from a JSON file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributeruler.mdx#2025-04-21_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[initialize.components.attribute_ruler]\n\n[initialize.components.attribute_ruler.patterns]\n@readers = \"srsly.read_json.v1\"\npath = \"corpus/attribute_ruler_patterns.json\n```\n\n----------------------------------------\n\nTITLE: Creating Sub-spans from Span\nDESCRIPTION: Demonstrates how to create a new Span from an existing Span using slice notation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\nspan = doc[1:4]\nassert span[1:3].text == \"back!\"\n```\n\n----------------------------------------\n\nTITLE: Entity Offset to BILUO Tags Conversion\nDESCRIPTION: Demonstrates the stricter validation of entity offsets when converting to BILUO tags, requiring non-overlapping entities\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-2.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I live in Berlin Kreuzberg\")\nentities = [(10, 16, \"GPE\"), (17, 26, \"LOC\")]\ntags = get_biluo_tags_from_offsets(doc, entities)\n```\n\n----------------------------------------\n\nTITLE: Compiling Suffix Regex Patterns in Python\nDESCRIPTION: Demonstrates how to compile suffix rules into a regex object and assign it to the tokenizer's suffix search. Used for customizing tokenization patterns.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nsuffixes = (\"'s\", \"'S\", r\"(?<=[0-9])\\+\")\nsuffix_regex = util.compile_suffix_regex(suffixes)\nnlp.tokenizer.suffix_search = suffix_regex.search\n```\n\n----------------------------------------\n\nTITLE: Basic Debug Config Command\nDESCRIPTION: The basic command to debug a spaCy configuration file and show validation errors. Creates and validates all objects in the configuration tree.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy debug config [config_path] [--code] [--show-functions] [--show-variables] [overrides]\n```\n\n----------------------------------------\n\nTITLE: Setting Token Morphology - Python\nDESCRIPTION: Example of setting morphological features on a token using UD FEATS format.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\ndoc[0].set_morph(\"Mood=Imp|VerbForm=Fin\")\nassert \"Mood=Imp\" in doc[0].morph\nassert doc[0].morph.get(\"Mood\") == [\"Imp\"]\n```\n\n----------------------------------------\n\nTITLE: Custom Pipeline Component for HTML Processing\nDESCRIPTION: Implements a custom pipeline component that merges HTML break tags and flags them using custom token extensions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.language import Language\nfrom spacy.matcher import Matcher\nfrom spacy.tokens import Token\n\n@Language.factory(\"html_merger\")\ndef create_bad_html_merger(nlp, name):\n    return BadHTMLMerger(nlp.vocab)\n\nclass BadHTMLMerger:\n    def __init__(self, vocab):\n        patterns = [\n            [{\"ORTH\": \"<\"}, {\"LOWER\": \"br\"}, {\"ORTH\": \">\"}],\n            [{\"ORTH\": \"<\"}, {\"LOWER\": \"br/\"}, {\"ORTH\": \">\"}],\n        ]\n        Token.set_extension(\"bad_html\", default=False)\n        self.matcher = Matcher(vocab)\n        self.matcher.add(\"BAD_HTML\", patterns)\n\n    def __call__(self, doc):\n        matches = self.matcher(doc)\n        spans = []\n        for match_id, start, end in matches:\n            spans.append(doc[start:end])\n        with doc.retokenize() as retokenizer:\n            for span in spans:\n                retokenizer.merge(span)\n                for token in span:\n                    token._.bad_html = True\n        return doc\n\nnlp = spacy.load(\"en_core_web_sm\")\nnlp.add_pipe(\"html_merger\", last=True)\ndoc = nlp(\"Hello<br>world! <br/> This is a test.\")\nfor token in doc:\n    print(token.text, token._.bad_html)\n```\n\n----------------------------------------\n\nTITLE: Setting Entity Annotations with EntityLinker in spaCy (Python)\nDESCRIPTION: Illustrates how to modify a batch of documents using pre-computed entity IDs. This method applies the predicted KB identifiers to the named entities in the provided documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nentity_linker = nlp.add_pipe(\"entity_linker\")\nkb_ids = entity_linker.predict([doc1, doc2])\nentity_linker.set_annotations([doc1, doc2], kb_ids)\n```\n\n----------------------------------------\n\nTITLE: Merging DocBin Objects\nDESCRIPTION: Shows how to merge two DocBin objects with matching attributes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/docbin.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndoc_bin1 = DocBin(attrs=[\"LEMMA\", \"POS\"])\ndoc_bin1.add(nlp(\"Hello world\"))\ndoc_bin2 = DocBin(attrs=[\"LEMMA\", \"POS\"])\ndoc_bin2.add(nlp(\"This is a sentence\"))\ndoc_bin1.merge(doc_bin2)\nassert len(doc_bin1) == 2\n```\n\n----------------------------------------\n\nTITLE: Defining Tokenizer Customization in Config File\nDESCRIPTION: Configuration snippet showing how to specify a callback function for customizing the tokenizer in the initialization section of the config file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_23\n\nLANGUAGE: ini\nCODE:\n```\n[initialize]\n\n[initialize.before_init]\n@callbacks = \"customize_tokenizer\"\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields in spaCy Tagger Serialization\nDESCRIPTION: Shows how to exclude specific data fields when serializing a tagger component to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndata = tagger.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Checking DependencyMatcher Length in Python\nDESCRIPTION: This snippet demonstrates how to check the number of rules in a DependencyMatcher object. It shows adding a pattern and verifying the length before and after addition.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencymatcher.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmatcher = DependencyMatcher(nlp.vocab)\nassert len(matcher) == 0\npattern = [{\"RIGHT_ID\": \"founded_id\",\n  \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}}]\nmatcher.add(\"FOUNDED\", [pattern])\nassert len(matcher) == 1\n```\n\n----------------------------------------\n\nTITLE: Initializing Lemmatizer with Custom Settings\nDESCRIPTION: Examples of initializing the Lemmatizer component with default and custom settings using nlp.add_pipe().\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lemmatizer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\nlemmatizer = nlp.add_pipe(\"lemmatizer\")\n\n# Construction via add_pipe with custom settings\nconfig = {\"mode\": \"rule\", \"overwrite\": True}\nlemmatizer = nlp.add_pipe(\"lemmatizer\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Initializing Morphologizer with Label Data in Python\nDESCRIPTION: Shows how to access the label data of the Morphologizer component and use it to initialize the model with a pre-defined label set.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlabels = morphologizer.label_data\nmorphologizer.initialize(lambda: [], nlp=nlp, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Defining Few-Shot Examples for Translation in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define few-shot learning examples for the Translation task. It includes three example translations from English to Spanish.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\n- text: 'Top of the morning to you!'\n  translation: '¡Muy buenos días!'\n- text: 'The weather is great today.'\n  translation: 'El clima está fantástico hoy.'\n- text: 'Do you know what will happen tomorrow?'\n  translation: '¿Sabes qué pasará mañana?'\n```\n\n----------------------------------------\n\nTITLE: Converting Doc Objects to displaCy Format with parse_ents\nDESCRIPTION: Example of converting a spaCy Doc object to displaCy's entity visualization format using the parse_ents helper function. This allows rendering of entity annotations with manual=True setting.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"But Google is starting from behind.\")\nex = displacy.parse_ents(doc)\nhtml = displacy.render(ex, style=\"ent\", manual=True)\n```\n\n----------------------------------------\n\nTITLE: Using Morphologizer.pipe for Document Processing\nDESCRIPTION: Demonstrates how to process a stream of documents using the morphologizer pipe with batch processing.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\nfor doc in morphologizer.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Case-Insensitive Phrase Matching with spaCy PhraseMatcher\nDESCRIPTION: This snippet shows how to perform case-insensitive phrase matching using spaCy's PhraseMatcher. It matches on the lowercase token text by setting the 'attr' argument to 'LOWER'.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\nfrom spacy.matcher import PhraseMatcher\n\nnlp = English()\nmatcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\npatterns = [nlp.make_doc(name) for name in [\"Angela Merkel\", \"Barack Obama\"]]\nmatcher.add(\"Names\", patterns)\n\ndoc = nlp(\"angela merkel and us president barack Obama\")\nfor match_id, start, end in matcher(doc):\n    print(\"Matched based on lowercase token text:\", doc[start:end])\n```\n\n----------------------------------------\n\nTITLE: Implementing Prediction Method\nDESCRIPTION: Implementation of the predict method that processes documents and returns model predictions as a Floats2d array.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef predict(self, docs: Iterable[Doc]) -> Floats2d:\n    predictions = self.model.predict(docs)\n    return self.model.ops.asarray(predictions)\n```\n\n----------------------------------------\n\nTITLE: Adding String to StringStore in Python\nDESCRIPTION: Use the add method to add a new string to the StringStore and get its hash.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/stringstore.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstringstore = StringStore([\"apple\", \"orange\"])\nbanana_hash = stringstore.add(\"banana\")\nassert len(stringstore) == 3\nassert banana_hash == 2525716904149915114\nassert stringstore[banana_hash] == \"banana\"\nassert stringstore[\"banana\"] == banana_hash\n```\n\n----------------------------------------\n\nTITLE: Deserializing Vector Data from Bytes in Python\nDESCRIPTION: Method to load vector data from a binary string.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndef from_bytes(self, data: bytes) -> BaseVectors:\n```\n\n----------------------------------------\n\nTITLE: Initializing Japanese Tokenizer in spaCy\nDESCRIPTION: Demonstrates how to load the Japanese language class with different SudachiPy split modes. It shows the default setup and how to configure the split mode using the tokenizer config.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.ja import Japanese\n\n# Load SudachiPy with split mode A (default)\nnlp = Japanese()\n# Load SudachiPy with split mode B\ncfg = {\"split_mode\": \"B\"}\nnlp = Japanese.from_config({\"nlp\": {\"tokenizer\": cfg}})\n```\n\n----------------------------------------\n\nTITLE: Initializing DependencyParser Component\nDESCRIPTION: Example showing how to initialize a dependency parser component and configure it via the config file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\nparser.initialize(lambda: examples, nlp=nlp)\n```\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[initialize.components.parser]\n\n[initialize.components.parser.labels]\n@readers = \"spacy.read_labels.v1\"\npath = \"corpus/labels/parser.json\n```\n\n----------------------------------------\n\nTITLE: Iterating over Noun Chunks in a Span in Python using spaCy\nDESCRIPTION: Shows how to iterate over base noun phrases (NP chunks) within a Span. Yields Span objects representing noun chunks if the document has been syntactically parsed.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"A phrase with another phrase occurs.\")\nspan = doc[3:5]\nchunks = list(span.noun_chunks)\nassert len(chunks) == 1\nassert chunks[0].text == \"another phrase\"\n```\n\n----------------------------------------\n\nTITLE: Serializing Tok2Vec Component to Disk in Python\nDESCRIPTION: Demonstrates how to save the Tok2Vec component to disk. This is used for persisting the trained component for later use or distribution.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntok2vec = nlp.add_pipe(\"tok2vec\")\ntok2vec.to_disk(\"/path/to/tok2vec\")\n```\n\n----------------------------------------\n\nTITLE: Saving DocBin to Disk\nDESCRIPTION: Demonstrates how to save a DocBin to a file with .spacy extension.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/docbin.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndocs = [nlp(\"Hello world!\")]\ndoc_bin = DocBin(docs=docs)\ndoc_bin.to_disk(\"./data.spacy\")\n```\n\n----------------------------------------\n\nTITLE: German Language Parse Tree Navigation\nDESCRIPTION: Demonstrates parse tree navigation in German, showing left and right children of tokens.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"de_core_news_sm\")\ndoc = nlp(\"schöne rote Äpfel auf dem Baum\")\nprint([token.text for token in doc[2].lefts])  # ['schöne', 'rote']\nprint([token.text for token in doc[2].rights])  # ['auf']\n```\n\n----------------------------------------\n\nTITLE: Serializing DocBin to Bytes\nDESCRIPTION: Demonstrates how to serialize DocBin annotations to a bytestring.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/docbin.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndocs = [nlp(\"Hello world!\")]\ndoc_bin = DocBin(docs=docs)\ndoc_bin_bytes = doc_bin.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Converting BILUO Tags to IOB Format\nDESCRIPTION: Example showing how to convert entity annotations from BILUO tagging scheme to IOB format.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import biluo_to_iob\n\ntags = [\"O\", \"O\", \"B-LOC\", \"I-LOC\", \"L-LOC\", \"O\"]\niob_tags = biluo_to_iob(tags)\nassert iob_tags == [\"O\", \"O\", \"B-LOC\", \"I-LOC\", \"I-LOC\", \"O\"]\n```\n\n----------------------------------------\n\nTITLE: Creating Entity Spans from Character Offsets in spaCy in Python\nDESCRIPTION: This single-line example shows how to create an entity span using character offsets rather than token indices with the Doc.char_span method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfb_ent = doc.char_span(0, 2, label=\"ORG\")\n```\n\n----------------------------------------\n\nTITLE: Installing New Language Models in spaCy v2.2\nDESCRIPTION: Commands to download and install the new language models for Dutch, Norwegian and Lithuanian using spaCy's command-line interface.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-2.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy download nl_core_news_sm\npython -m spacy download nb_core_news_sm\npython -m spacy download lt_core_news_sm\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for NER Component\nDESCRIPTION: Creates an optimizer instance for training the NER component. Used for updating model parameters during training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\noptimizer = ner.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Defining Pipeline Component in INI Configuration\nDESCRIPTION: Example of how a pipeline component is defined in a spaCy configuration file. It shows the structure for specifying a component factory and its arguments, including variable interpolation.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Language.md#2025-04-21_snippet_0\n\nLANGUAGE: INI\nCODE:\n```\n[components.my_component]\nfactory = \"foo\"\nsome_arg = \"bar\"\nother_arg = ${paths.some_path}\n```\n\n----------------------------------------\n\nTITLE: Checking if a Lexeme Has a Word Vector in spaCy\nDESCRIPTION: Example of how to check if a lexeme has an associated word vector in spaCy. This property returns a boolean indicating whether vector data is available for the lexeme.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lexeme.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napple = nlp.vocab[\"apple\"]\nassert apple.has_vector\n```\n\n----------------------------------------\n\nTITLE: Removing Patterns from SpanRuler by ID\nDESCRIPTION: Shows how to remove patterns from a SpanRuler using a pattern ID.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npatterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"}]\nruler = nlp.add_pipe(\"span_ruler\")\nruler.add_patterns(patterns)\nruler.remove_by_id(\"apple\")\n```\n\n----------------------------------------\n\nTITLE: Compiling Infix Regex Patterns in Python\nDESCRIPTION: Shows how to compile infix rules into a regex object and assign it to the tokenizer's infix finditer. Used for customizing internal token splitting patterns.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_54\n\nLANGUAGE: python\nCODE:\n```\ninfixes = (\"…\", \"-\", \"—\", r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")\ninfix_regex = util.compile_infix_regex(infixes)\nnlp.tokenizer.infix_finditer = infix_regex.finditer\n```\n\n----------------------------------------\n\nTITLE: Documenting Python Functions with Docstrings\nDESCRIPTION: Example of how to write a docstring for a function in the spaCy project. It includes a summary, argument descriptions, return value, and a link to the full documentation.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef has_pipe(self, name: str) -> bool:\n    \"\"\"Check if a component name is present in the pipeline. Equivalent to\n    `name in nlp.pipe_names`.\n\n    name (str): Name of the component.\n    RETURNS (bool): Whether a component of the name exists in the pipeline.\n\n    DOCS: https://spacy.io/api/language#has_pipe\n    \"\"\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Loading Token Attributes from NumPy Array in spaCy Doc (Python)\nDESCRIPTION: Shows how to load token attributes into a spaCy Doc from a NumPy array. This method allows writing attribute values to a Doc object from an (M, N) array of attributes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\nfrom spacy.tokens import Doc\ndoc = nlp(\"Hello world!\")\nnp_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\ndoc2 = Doc(doc.vocab, words=[t.text for t in doc])\ndoc2.from_array([LOWER, POS, ENT_TYPE, IS_ALPHA], np_array)\nassert doc[0].pos_ == doc2[0].pos_\n```\n\n----------------------------------------\n\nTITLE: Retrieving Candidates in Batch for Entity Linking in Python\nDESCRIPTION: Example of using the get_candidates_batch method to retrieve candidate entities for multiple text spans.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/kb.mdx#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom spacy.lang.en import English\nnlp = English()\ndoc = nlp(\"Douglas Adams wrote 'The Hitchhiker's Guide to the Galaxy'.\")\ncandidates = kb.get_candidates_batch((doc[0:2], doc[3:]))\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for EntityLinker in spaCy (Python)\nDESCRIPTION: Shows how to create an optimizer specifically for the EntityLinker component. This method returns an Optimizer object that can be used for model updates.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nentity_linker = nlp.add_pipe(\"entity_linker\")\noptimizer = entity_linker.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Predicting Coreference Clusters\nDESCRIPTION: Example showing how to generate predictions for multiple documents without modifying them.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncoref = nlp.add_pipe(\"experimental_coref\")\nclusters = coref.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Configuring a few-shot example file for LLM tasks in spaCy\nDESCRIPTION: This snippet shows how to configure an LLM component to use a file containing examples for few-shot prompting with the NER.v2 task. It specifies the task type, entity labels, and the path to the examples file that will be used in prompt generation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_10\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.NER.v2\"\nlabels = PERSON,ORGANISATION,LOCATION\n[components.llm.task.examples]\n@misc = \"spacy.FewShotReader.v1\"\npath = \"ner_examples.yml\"\n```\n\n----------------------------------------\n\nTITLE: Using TextCategorizer Parameters in Python\nDESCRIPTION: This snippet demonstrates how to modify the TextCategorizer's model to use given parameter values. It uses a context manager to temporarily apply the optimizer's averaged parameters and save the model to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\nwith textcat.use_params(optimizer.averages):\n    textcat.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Initializing SpanCategorizer with Label Data\nDESCRIPTION: Example showing how to initialize a SpanCategorizer with pre-defined label data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nlabels = spancat.label_data\nspancat.initialize(lambda: [], nlp=nlp, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Simple BOW Text Categorizer Configuration\nDESCRIPTION: Configuration example showing how to use the simpler TextCatBOW architecture instead of the ensemble model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[components.textcat]\nfactory = \"textcat\"\nlabels = []\n\n[components.textcat.model]\n@architectures = \"spacy.TextCatBOW.v3\"\nexclusive_classes = true\nlength = 262144\nngram_size = 1\nno_output_layer = false\nnO = null\n```\n\n----------------------------------------\n\nTITLE: Defining Orthographic Variants in JSON for spaCy\nDESCRIPTION: JSON structure for defining orthographic variant rules, including single token replacements (like ellipses) and paired token replacements (like quotes), to be used by the spaCy orth_variants augmenter.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_42\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"single\": [{ \"tags\": [\"NFP\"], \"variants\": [\"…\", \"...\"] }],\n  \"paired\": [\n    {\n      \"tags\": [\"``, \"''\"],\n      \"variants\": [\n        [\"'\", \"'\"],\n        [\"'\", \"'\"]\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Morphologizer from Bytes in Python\nDESCRIPTION: Shows how to load a Morphologizer pipe from a bytestring, modifying the object in place and returning it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer_bytes = morphologizer.to_bytes()\nmorphologizer = nlp.add_pipe(\"morphologizer\")\nmorphologizer.from_bytes(morphologizer_bytes)\n```\n\n----------------------------------------\n\nTITLE: TextCat Few-Shot Reader Configuration\nDESCRIPTION: Configuration for loading few-shot examples from a JSON file using the FewShotReader component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_40\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task.examples]\n@misc = \"spacy.FewShotReader.v1\"\npath = \"textcat_examples.json\"\n```\n\n----------------------------------------\n\nTITLE: Managing Model Weights with use_params\nDESCRIPTION: Shows how to temporarily replace model weights using the use_params context manager. Useful for checkpointing model states.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwith nlp.use_params(optimizer.averages):\n    nlp.to_disk(\"/tmp/checkpoint\")\n```\n\n----------------------------------------\n\nTITLE: Initializing SpanGroup in Python\nDESCRIPTION: Creates a SpanGroup object with optional name, attributes, and initial spans. Demonstrates two ways of constructing a SpanGroup: directly and through Doc.spans assignment.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spangroup.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Their goi ng home\")\nspans = [doc[0:1], doc[1:3]]\n\n# Construction 1\nfrom spacy.tokens import SpanGroup\n\ngroup = SpanGroup(doc, name=\"errors\", spans=spans, attrs={\"annotator\": \"matt\"})\ndoc.spans[\"errors\"] = group\n\n# Construction 2\ndoc.spans[\"errors\"] = spans\nassert isinstance(doc.spans[\"errors\"], SpanGroup)\n```\n\n----------------------------------------\n\nTITLE: Finding Tokens by End Character Position in spaCy\nDESCRIPTION: Demonstrates how to use the token_by_end function to find a token in a TokenC array by its final character offset. The function returns the index of the token or -1 if not found.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-structs.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens.doc cimport Doc, token_by_end\nfrom spacy.vocab cimport Vocab\n\ndoc = Doc(Vocab(), words=[\"hello\", \"world\"])\nassert token_by_end(doc.c, doc.length, 5) == 0\nassert token_by_end(doc.c, doc.length, 1) == -1\n```\n\n----------------------------------------\n\nTITLE: Setting Document Annotations with Morphologizer\nDESCRIPTION: Demonstrates how to modify documents using pre-computed morphological scores.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\nscores = morphologizer.predict([doc1, doc2])\nmorphologizer.set_annotations([doc1, doc2], scores)\n```\n\n----------------------------------------\n\nTITLE: Saving StringStore to Disk in Python\nDESCRIPTION: Use the to_disk method to save the current state of StringStore to a directory.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/stringstore.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstringstore.to_disk(\"/path/to/strings\")\n```\n\n----------------------------------------\n\nTITLE: Serializing Vector Data to Bytes in Python\nDESCRIPTION: Method to serialize vector data to a binary string.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ndef to_bytes(self) -> bytes:\n```\n\n----------------------------------------\n\nTITLE: Computing Vector Norm for spaCy Doc Objects (Python)\nDESCRIPTION: Demonstrates how to access the L2 norm of a document's vector representation using the vector_norm property, and compares norms between different documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndoc1 = nlp(\"I like apples\")\ndoc2 = nlp(\"I like oranges\")\ndoc1.vector_norm  # 4.54232424414368\ndoc2.vector_norm  # 3.304373298575751\nassert doc1.vector_norm != doc2.vector_norm\n```\n\n----------------------------------------\n\nTITLE: Loading EntityLinker from Disk in spaCy (Python)\nDESCRIPTION: Shows how to load a previously saved EntityLinker component from disk. This method modifies the existing object in place with the loaded data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nentity_linker = nlp.add_pipe(\"entity_linker\")\nentity_linker.from_disk(\"/path/to/entity_linker\")\n```\n\n----------------------------------------\n\nTITLE: Loading AttributeRuler from Disk - Python\nDESCRIPTION: Example demonstrating how to load an AttributeRuler pipe from disk using the from_disk() method. Requires adding the ruler to the pipeline first.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributeruler.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"attribute_ruler\")\nruler.from_disk(\"/path/to/attribute_ruler\")\n```\n\n----------------------------------------\n\nTITLE: Generating Project Documentation with spaCy CLI\nDESCRIPTION: Command to auto-generate a Markdown-formatted README based on project.yml. Creates documentation sections for commands, workflows and assets between hidden markers.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_49\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project document [project_dir] [--output] [--no-emoji]\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project document --output README.md\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields in Morphologizer Serialization in Python\nDESCRIPTION: Demonstrates how to exclude specific data fields when serializing the Morphologizer component to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndata = morphologizer.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Adding Subject Dependency to Pattern\nDESCRIPTION: Extends the basic pattern to include the subject of 'founded' with the dependency relation 'nsubj', representing the founder in the sentence.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_34\n\nLANGUAGE: python\nCODE:\n```\npattern = [\n    {\n        \"RIGHT_ID\": \"anchor_founded\",\n        \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n    },\n    {\n        \"LEFT_ID\": \"anchor_founded\",\n        \"REL_OP\": \">\",\n        \"RIGHT_ID\": \"founded_subject\",\n        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},\n    }\n    # ...\n]\n```\n\n----------------------------------------\n\nTITLE: Using EntityLinker.pipe Method for Batch Processing in spaCy Python\nDESCRIPTION: Example of using the pipe method to process a stream of documents with batch processing. This is more efficient than processing documents one by one when working with large amounts of text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nentity_linker = nlp.add_pipe(\"entity_linker\")\nfor doc in entity_linker.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Setting Annotations with SpanCategorizer in Python\nDESCRIPTION: Demonstration of how to modify a batch of Doc objects using pre-computed scores from the SpanCategorizer component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\nscores = spancat.predict(docs)\nspancat.set_annotations(docs, scores)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Vectors by Key in Python\nDESCRIPTION: Example showing how to retrieve a vector by its key from the vocabulary's vector store.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncat_id = nlp.vocab.strings[\"cat\"]\ncat_vector = nlp.vocab.vectors[cat_id]\nassert cat_vector == nlp.vocab[\"cat\"].vector\n```\n\n----------------------------------------\n\nTITLE: Renaming Pipeline Components in spaCy\nDESCRIPTION: Example of renaming a pipeline component using rename_pipe method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nnlp.rename_pipe(\"parser\", \"spacy_parser\")\n```\n\n----------------------------------------\n\nTITLE: Using Parameters with SpanFinder\nDESCRIPTION: Shows how to temporarily modify the model's parameters using a context manager.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nspan_finder = nlp.add_pipe(\"span_finder\")\nwith span_finder.use_params(optimizer.averages):\n    span_finder.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Constructor and Label Management\nDESCRIPTION: Extended constructor implementation with label management functionality, including methods to store and retrieve labels used by the relation extractor.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n    def __init__(self, vocab, model, name=\"rel\"):\n        \"\"\"Create a component instance.\"\"\"\n        # ...\n        self.cfg = {\"labels\": []}\n\n    @property\n    def labels(self) -> Tuple[str, ...]:\n        \"\"\"Returns the labels currently added to the component.\"\"\"\n        return tuple(self.cfg[\"labels\"])\n\n    def add_label(self, label: str):\n        \"\"\"Add a new label to the pipe.\"\"\"\n        self.cfg[\"labels\"] = list(self.labels) + [label]\n```\n\n----------------------------------------\n\nTITLE: Removing Token Extensions - Python\nDESCRIPTION: Shows how to remove a previously registered extension from the Token class.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Token\nToken.set_extension(\"is_fruit\", default=False)\nremoved = Token.remove_extension(\"is_fruit\")\nassert not Token.has_extension(\"is_fruit\")\n```\n\n----------------------------------------\n\nTITLE: Executing spaCy Debug Pieces Command\nDESCRIPTION: Command-line syntax for analyzing word- or sentencepiece stats in spaCy. This command helps debug tokenization by providing statistical information about token lengths in training and development corpora.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug pieces [config_path] [--code] [--name] [overrides]\n```\n\n----------------------------------------\n\nTITLE: Scoring individual token attributes in spaCy\nDESCRIPTION: Evaluates the accuracy of a single token attribute such as part-of-speech tags. Tokens with missing values in the reference document are skipped during evaluation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/scorer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nscores = Scorer.score_token_attr(examples, \"pos\")\nprint(scores[\"pos_acc\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing Instance Tensor Layer with Custom Forward Pass\nDESCRIPTION: Creates a custom Thinc model that generates instance tensors from documents. Includes forward pass logic and initialization method to prepare the model for training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n@spacy.registry.architectures(\"rel_instance_tensor.v1\")\ndef create_tensors(\n    tok2vec: Model[List[Doc], List[Floats2d]],\n    pooling: Model[Ragged, Floats2d],\n    get_instances: Callable[[Doc], List[Tuple[Span, Span]]],\n) -> Model[List[Doc], Floats2d]:\n\n    return Model(\n        \"instance_tensors\",\n        instance_forward,\n        init=instance_init,\n        layers=[tok2vec, pooling],\n        refs={\"tok2vec\": tok2vec, \"pooling\": pooling},\n        attrs={\"get_instances\": get_instances},\n    )\n\n\n# The custom forward function\ndef instance_forward(\n    model: Model[List[Doc], Floats2d],\n    docs: List[Doc],\n    is_train: bool,\n) -> Tuple[Floats2d, Callable]:\n    tok2vec = model.get_ref(\"tok2vec\")\n    tokvecs, bp_tokvecs = tok2vec(docs, is_train)\n    get_instances = model.attrs[\"get_instances\"]\n    all_instances = [get_instances(doc) for doc in docs]\n    pooling = model.get_ref(\"pooling\")\n    relations = ...\n\n    def backprop(d_relations: Floats2d) -> List[Doc]:\n        d_tokvecs = ...\n        return bp_tokvecs(d_tokvecs)\n\n    return relations, backprop\n\n\n# The custom initialization method\ndef instance_init(\n    model: Model,\n    X: List[Doc] = None,\n    Y: Floats2d = None,\n) -> Model:\n    tok2vec = model.get_ref(\"tok2vec\")\n    tok2vec.initialize(X)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Using prefer_gpu in spaCy\nDESCRIPTION: Example showing how to enable GPU processing in spaCy before loading a model. The function attempts to allocate data and perform operations on GPU if available.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nactivated = spacy.prefer_gpu()\nnlp = spacy.load(\"en_core_web_sm\")\n```\n\n----------------------------------------\n\nTITLE: Batch Processing with CoreferenceResolver\nDESCRIPTION: Example demonstrating how to process multiple documents in a stream using the pipe method with batch processing.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncoref = nlp.add_pipe(\"experimental_coref\")\nfor doc in coref.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring Summarization Task Component\nDESCRIPTION: Basic configuration for the Summarization.v1 task component without examples.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_16\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.Summarization.v1\"\nexamples = null\nmax_n_words = null\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for CuratedTransformer\nDESCRIPTION: Create an optimizer instance for the transformer component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"curated_transformer\")\noptimizer = trf.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parameters with Morphologizer\nDESCRIPTION: Demonstrates how to temporarily modify the model's parameters using a context manager.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\nwith morphologizer.use_params(optimizer.averages):\n    morphologizer.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Configuring MishWindowEncoder Architecture in spaCy\nDESCRIPTION: Configuration for MishWindowEncoder.v2 architecture that uses convolutions with Mish activation, layer normalization and residual connections. Specifies width, window size and depth parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.MishWindowEncoder.v2\"\nwidth = 64\nwindow_size = 1\ndepth = 4\n```\n\n----------------------------------------\n\nTITLE: Setting Annotations with EditTreeLemmatizer\nDESCRIPTION: Example demonstrating how to modify documents using pre-computed tree identifiers.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\ntree_ids = lemmatizer.predict([doc1, doc2])\nlemmatizer.set_annotations([doc1, doc2], tree_ids)\n```\n\n----------------------------------------\n\nTITLE: Accessing Lexeme Struct in spaCy\nDESCRIPTION: Demonstrates how to access a LexemeC struct through a token. LexemeC structs contain lexical information and are typically owned by the Vocab with tokens holding read-only pointers to them.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-structs.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlex = doc.c[3].lex\n```\n\n----------------------------------------\n\nTITLE: Converting IOB to spaCy v3 Format\nDESCRIPTION: This command converts an IOB file to .spacy (DocBin) format for use with spaCy v3. It utilizes the spacy convert utility with parameters for IOB conversion format, sentence segmentation, batch size, and a base model.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/example_data/ner_example_data/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy convert -c iob -s -n 10 -b en_core_web_sm file.iob .\n```\n\n----------------------------------------\n\nTITLE: Computing Loss for spaCy Tagger\nDESCRIPTION: Calculate loss and gradient for a batch of examples and their predictions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\nscores = tagger.predict([eg.predicted for eg in examples])\nloss, d_loss = tagger.get_loss(examples, scores)\n```\n\n----------------------------------------\n\nTITLE: Accessing Word Vectors of Lexemes in spaCy\nDESCRIPTION: Example of how to access the word vector of a lexeme in spaCy. The example demonstrates that vectors are float32 arrays with a specific shape (300 dimensions in this case).\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lexeme.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\napple = nlp.vocab[\"apple\"]\nassert apple.vector.dtype == \"float32\"\nassert apple.vector.shape == (300,)\n```\n\n----------------------------------------\n\nTITLE: Project Assets Configuration\nDESCRIPTION: YAML configuration showing how to define project assets with URLs and checksums for downloading training data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nassets:\n  - dest: 'assets/training.spacy'\n    url: 'https://example.com/data.spacy'\n    checksum: '63373dd656daa1fd3043ce166a59474c'\n  - dest: 'assets/development.spacy'\n    git:\n      repo: 'https://github.com/example/repo'\n      branch: 'master'\n      path: 'path/development.spacy'\n    checksum: '5113dc04e03f079525edd8df3f4f39e3'\n```\n\n----------------------------------------\n\nTITLE: Saving Vectors to Disk\nDESCRIPTION: Shows how to save vector data to disk storage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nvectors.to_disk(\"/path/to/vectors\")\n\n```\n\n----------------------------------------\n\nTITLE: Serializing Sentencizer Settings\nDESCRIPTION: Examples showing how to serialize and deserialize Sentencizer settings to/from bytes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencizer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\"punct_chars\": [\".\", \"?\", \"!\", \"。\"]}\nsentencizer = nlp.add_pipe(\"sentencizer\", config=config)\nsentencizer_bytes = sentencizer.to_bytes()\n\nsentencizer_bytes = sentencizer.to_bytes()\nsentencizer = nlp.add_pipe(\"sentencizer\")\nsentencizer.from_bytes(sentencizer_bytes)\n```\n\n----------------------------------------\n\nTITLE: Implementing Example Creation in spaCy Training\nDESCRIPTION: Code snippet showing the beginning of creating an Example object for training, which holds both the predicted document and gold-standard annotations. This is part of the internal training API explanation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nwords = [\"I\", \"like\", \"stuff\"]\npredicted = Doc(vocab, words=words)\n```\n\n----------------------------------------\n\nTITLE: Running Project Workflow\nDESCRIPTION: Command to execute a complete workflow of multiple commands.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project run all\n```\n\n----------------------------------------\n\nTITLE: Accessing EntityRecognizer Labels\nDESCRIPTION: Shows how to add and check for labels in the EntityRecognizer component using the labels property.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nner.add_label(\"MY_LABEL\")\nassert \"MY_LABEL\" in ner.labels\n```\n\n----------------------------------------\n\nTITLE: Creating MorphAnalysis from Hash ID in Python using spaCy\nDESCRIPTION: This snippet demonstrates how to create a MorphAnalysis object from a hash ID using the from_id class method. It shows the process of generating a hash from a features string and then reconstructing the MorphAnalysis object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfeats = \"Feat1=Val1|Feat2=Val2\"\nhash = nlp.vocab.strings[feats]\nmorph = MorphAnalysis.from_id(nlp.vocab, hash)\nassert str(morph) == feats\n```\n\n----------------------------------------\n\nTITLE: Configuring Lemma Task in spaCy LLM\nDESCRIPTION: Example configuration for the spaCy Lemma task that lemmatizes text using LLMs. This basic configuration doesn't include examples for few-shot learning.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_45\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.Lemma.v1\"\nexamples = null\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural Network with Thinc in Python\nDESCRIPTION: Shows how to implement the same neural network architecture using Thinc's native layers and combinators.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom thinc.api import chain, with_array, Model, Relu, Dropout, Softmax\nfrom spacy.ml import CharacterEmbed\n\nchar_embed = CharacterEmbed(width, embed_size, nM, nC)\nwith Model.define_operators({\">>\":chain}):\n    layers = (\n        Relu(hidden_width, width)\n        >> Dropout(dropout)\n        >> Relu(hidden_width, hidden_width)\n        >> Dropout(dropout)\n        >> Softmax(nO, hidden_width)\n    )\n    model = char_embed >> with_array(layers)\n```\n\n----------------------------------------\n\nTITLE: Accessing Morphologizer Labels in Python\nDESCRIPTION: Demonstrates how to access and verify the labels added to the Morphologizer component, including morphological features and POS tags.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer.add_label(\"Mood=Ind|POS=VERB|Tense=Past|VerbForm=Fin\")\nassert \"Mood=Ind|POS=VERB|Tense=Past|VerbForm=Fin\" in morphologizer.labels\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy Doc State from Disk (Python)\nDESCRIPTION: Shows how to load a spaCy Doc object's state from a directory on disk. This method modifies the object in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc\nfrom spacy.vocab import Vocab\ndoc = Doc(Vocab()).from_disk(\"/path/to/doc\")\n```\n\n----------------------------------------\n\nTITLE: Initializing CuratedTransformer Component\nDESCRIPTION: Initialize the transformer component with training examples and optional NLP object. Sets up the model and label scheme based on provided examples.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"curated_transformer\")\ntrf.initialize(lambda: examples, nlp=nlp)\n```\n\n----------------------------------------\n\nTITLE: Creating Parser Optimizer\nDESCRIPTION: Demonstrates creating an optimizer for the parser component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\noptimizer = parser.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Working with SpanCategorizer Labels\nDESCRIPTION: Example showing how to add and check labels in a SpanCategorizer component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nspancat.add_label(\"MY_LABEL\")\nassert \"MY_LABEL\" in spancat.labels\n```\n\n----------------------------------------\n\nTITLE: Dictionary structure for spaCy training data\nDESCRIPTION: Example structure of the dictionary format used to create training examples for spaCy, including various linguistic annotations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n{\n   \"text\": str,\n   \"words\": List[str],\n   \"lemmas\": List[str],\n   \"spaces\": List[bool],\n   \"tags\": List[str],\n   \"pos\": List[str],\n   \"morphs\": List[str],\n   \"sent_starts\": List[Optional[bool]],\n   \"deps\": List[str],\n   \"heads\": List[int],\n   \"entities\": List[str],\n   \"entities\": List[(int, int, str)],\n   \"cats\": Dict[str, float],\n   \"links\": Dict[(int, int), dict],\n   \"spans\": Dict[str, List[Tuple]],\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Morphologizer from Disk in Python\nDESCRIPTION: Illustrates how to load a previously saved Morphologizer pipe from disk, modifying the object in place.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\nmorphologizer.from_disk(\"/path/to/morphologizer\")\n```\n\n----------------------------------------\n\nTITLE: Default Punctuation Characters\nDESCRIPTION: List of default punctuation characters used by the Sentencizer for sentence boundary detection.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencizer.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n['!', '.', '?', '։', '؟', '۔', '܀', '܁', '܂', '߹', '।', '॥', '၊', '။', '።', '፧', '፨', '᙮', '᜵', '᜶', '᠃', '᠉', '᥄', '᥅', '᪨', '᪩', '᪪', '᪫', '᭚', '᭛', '᭞', '᭟', '᰻', '᰼', '᱾', '᱿', '‼', '‽', '⁇', '⁈', '⁉', '⸮', '⸼', '꓿', '꘎', '꘏', '꛳', '꛷', '꡶', '꡷', '꣎', '꣏', '꤯', '꧈', '꧉', '꩝', '꩞', '꩟', '꫰', '꫱', '꯫', '﹒', '﹖', '﹗', '！', '．', '？', '𐩖', '𐩗', '𑁇', '𑁈', '𑂾', '𑂿', '𑃀', '𑃁', '𑅁', '𑅂', '𑅃', '𑇅', '𑇆', '𑇍', '𑇞', '𑇟', '𑈸', '𑈹', '𑈻', '𑈼', '𑊩', '𑑋', '𑑌', '𑗂', '𑗃', '𑗉', '𑗊', '𑗋', '𑗌', '𑗍', '𑗎', '𑗏', '𑗐', '𑗑', '𑗒', '𑗓', '𑗔', '𑗕', '𑗖', '𑗗', '𑙁', '𑙂', '𑜼', '𑜽', '𑜾', '𑩂', '𑩃', '𑪛', '𑪜', '𑱁', '𑱂', '𖩮', '𖩯', '𖫵', '𖬷', '𖬸', '𖭄', '𛲟', '𝪈', '｡', '。']\n```\n\n----------------------------------------\n\nTITLE: Configuring Weights & Biases Logger in INI Format\nDESCRIPTION: This code snippet demonstrates how to configure the WandbLogger in the training section of a spaCy config file. It specifies the logger, project name, config values to remove, and logging intervals.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_38\n\nLANGUAGE: ini\nCODE:\n```\n[training.logger]\n@loggers = \"spacy.WandbLogger.v3\"\nproject_name = \"monitor_spacy_training\"\nremove_config_values = [\"paths.train\", \"paths.dev\", \"corpora.train.path\", \"corpora.dev.path\"]\nlog_dataset_dir = \"corpus\"\nmodel_log_interval = 1000\n```\n\n----------------------------------------\n\nTITLE: Configuring ConsoleLogger.v3 in spaCy Training\nDESCRIPTION: Example configuration for the ConsoleLogger.v3 component in spaCy training. This updated version allows specifying the type of progress bar and offers more customization options than v2.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_23\n\nLANGUAGE: ini\nCODE:\n```\n[training.logger]\n@loggers = \"spacy.ConsoleLogger.v3\"\nprogress_bar = \"eval\"\nconsole_output = true\noutput_file = \"training_log.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Checking Pipeline Component Presence in spaCy\nDESCRIPTION: Example showing how to check if a component exists in the pipeline using has_pipe method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n@Language.component(\"component\")\ndef component(doc):\n    return doc\n\nnlp.add_pipe(\"component\", name=\"my_component\")\nassert \"my_component\" in nlp.pipe_names\nassert nlp.has_pipe(\"my_component\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Raw Task Component in spaCy\nDESCRIPTION: Configuration for the Raw.v1 task component that processes arbitrary information using LLM. Defines basic setup without examples.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_13\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.Raw.v1\"\nexamples = null\n```\n\n----------------------------------------\n\nTITLE: Checking if Vector Table is Full in Python\nDESCRIPTION: Property to check if the vector table is full and no new keys can be added.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef is_full(self) -> bool:\n```\n\n----------------------------------------\n\nTITLE: Instance Tensor Creation Layer Configuration\nDESCRIPTION: Configuration for the instance tensor creation layer, including token vector generation, pooling strategy, and instance generation components.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_21\n\nLANGUAGE: ini\nCODE:\n```\n[model.create_instance_tensor]\n@architectures = \"rel_instance_tensor.v1\"\n\n[model.create_instance_tensor.tok2vec]\n@architectures = \"spacy.HashEmbedCNN.v2\"\n# ...\n\n[model.create_instance_tensor.pooling]\n@layers = \"reduce_mean.v1\"\n\n[model.create_instance_tensor.get_instances]\n# ...\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parameters with CuratedTransformer\nDESCRIPTION: Temporarily modify model parameters using a context manager, restoring original parameters afterwards.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"curated_transformer\")\nwith trf.use_params(optimizer.averages):\n    trf.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Batch Vector Retrieval\nDESCRIPTION: Shows how to efficiently retrieve vectors for multiple keys as a batch.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nwords = [\"cat\", \"dog\"]\nvectors = nlp.vocab.vectors.get_batch(words)\n```\n\n----------------------------------------\n\nTITLE: Saving SpanRuler Patterns to Disk\nDESCRIPTION: Shows how to save SpanRuler patterns to a directory on disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"span_ruler\")\nruler.to_disk(\"/path/to/span_ruler\")\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields in Serialization\nDESCRIPTION: Demonstrates how to exclude specific fields during SentenceRecognizer serialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndata = senter.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Creating a Candidate Object for Entity Linking in Python\nDESCRIPTION: Example of constructing a Candidate object, which represents a potential entity match for a given text mention.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/kb.mdx#2025-04-21_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom spacy.kb import Candidate\ncandidate = Candidate(kb, entity_hash, entity_freq, entity_vector, alias_hash, prior_prob)\n```\n\n----------------------------------------\n\nTITLE: Initializing Tok2Vec for Training\nDESCRIPTION: Example showing how to initialize the Tok2Vec component for training with example data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntok2vec = nlp.add_pipe(\"tok2vec\")\ntok2vec.initialize(lambda: examples, nlp=nlp)\n```\n\n----------------------------------------\n\nTITLE: Initializing TextCategorizer Component in spaCy\nDESCRIPTION: Shows how to initialize a text categorizer component and configure it through the config file. The initialization includes adding the component to the pipeline and setting up training parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\ntextcat.initialize(lambda: examples, nlp=nlp)\n```\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[initialize.components.textcat]\npositive_label = \"POS\"\n\n[initialize.components.textcat.labels]\n@readers = \"spacy.read_labels.v1\"\npath = \"corpus/labels/textcat.json\n```\n\n----------------------------------------\n\nTITLE: Initializing AttributeRuler with Patterns\nDESCRIPTION: Example showing how to initialize the AttributeRuler component with patterns, either programmatically or via configuration file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributeruler.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"attribute_ruler\")\nruler.initialize(lambda: [], nlp=nlp, patterns=patterns)\n```\n\n----------------------------------------\n\nTITLE: Setting Token Extensions - Python\nDESCRIPTION: Example of defining custom attributes on Token objects using set_extension with a getter function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Token\nfruit_getter = lambda token: token.text in (\"apple\", \"pear\", \"banana\")\nToken.set_extension(\"is_fruit\", getter=fruit_getter)\ndoc = nlp(\"I have an apple\")\nassert doc[3]._.is_fruit\n```\n\n----------------------------------------\n\nTITLE: Serializing SpanRuler to Bytes\nDESCRIPTION: Shows how to serialize a SpanRuler object to a bytestring.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"span_ruler\")\nruler_bytes = ruler.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Word Vector Representation in Python\nDESCRIPTION: Example showing the numerical vector representation of a word (banana) in multi-dimensional space. The vector consists of float values representing different dimensions of meaning.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_vectors-similarity.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\narray([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,\n       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,\n      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,\n       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,\n      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,\n       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,\n       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,\n       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,\n       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,\n       # ... and so on ...\n       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,\n      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,\n      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Counting Left Children in spaCy\nDESCRIPTION: Demonstrates how to get the count of leftward immediate children of a token.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York in Autumn.\")\nassert doc[3].n_lefts == 1\n```\n\n----------------------------------------\n\nTITLE: Accessing Labels of LLM Components in spaCy\nDESCRIPTION: Example demonstrating how to access the labels that have been added to an LLM component, which is useful for inspecting the component's configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nllm_ner.add_label(\"MY_LABEL\")\nassert \"MY_LABEL\" in llm_ner.labels\n```\n\n----------------------------------------\n\nTITLE: Span Visualization Input Format for displaCy\nDESCRIPTION: Structure of input data for token span visualization using displaCy. The format includes text, token list, and span definitions with start/end token indices and labels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"text\": \"Welcome to the Bank of China.\",\n    \"spans\": [\n        {\"start_token\": 3, \"end_token\": 6, \"label\": \"ORG\"},\n        {\"start_token\": 5, \"end_token\": 6, \"label\": \"GPE\"},\n    ],\n    \"tokens\": [\"Welcome\", \"to\", \"the\", \"Bank\", \"of\", \"China\", \".\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Serializing EntityLinker to Bytes in spaCy (Python)\nDESCRIPTION: Illustrates how to serialize the EntityLinker component to a bytestring, including the KnowledgeBase. This is useful for compact storage or transmission of the component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nentity_linker = nlp.add_pipe(\"entity_linker\")\nentity_linker_bytes = entity_linker.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Installing HuggingFace Dependencies\nDESCRIPTION: Commands to install required dependencies for using HuggingFace models with spaCy\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_59\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install \"spacy-llm[transformers]\" \"transformers[sentencepiece]\"\n```\n\n----------------------------------------\n\nTITLE: TextCat Few-Shot Learning Examples\nDESCRIPTION: JSON examples for text classification including compliments and insults for few-shot learning.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_39\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"text\": \"You look great!\",\n    \"answer\": \"Compliment\"\n  },\n  {\n    \"text\": \"You are not very clever at all.\",\n    \"answer\": \"Insult\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Getting Candidates for Entity Linking\nDESCRIPTION: Shows how to retrieve candidate entities for a text span using the knowledge base.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/inmemorylookupkb.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\nnlp = English()\ndoc = nlp(\"Douglas Adams wrote 'The Hitchhiker's Guide to the Galaxy'.\")\ncandidates = kb.get_candidates(doc[0:2])\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss with SpanFinder\nDESCRIPTION: Shows how to compute loss and gradient of loss for a batch of documents and their predicted scores.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nspan_finder = nlp.add_pipe(\"span_finder\")\nscores = span_finder.predict([eg.predicted for eg in examples])\nloss, d_loss = span_finder.get_loss(examples, scores)\n```\n\n----------------------------------------\n\nTITLE: Accessing Left Dependencies of Span in spaCy\nDESCRIPTION: Shows how to access tokens to the left of a span whose heads are within the span.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York in Autumn.\")\nlefts = [t.text for t in doc[3:7].lefts]\nassert lefts == [\"New\"]\n```\n\n----------------------------------------\n\nTITLE: Creating Table from Dictionary\nDESCRIPTION: Shows how to create a Table object from an existing dictionary using the from_dict class method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lookups import Table\ndata = {\"foo\": \"bar\", \"baz\": 100}\ntable = Table.from_dict(data, name=\"some_table\")\n```\n\n----------------------------------------\n\nTITLE: Example of Inspecting Layer Names with debug model Command\nDESCRIPTION: Prints the name of each layer after creation of the model (\"Step 0\"), showing the internal structure of the Neural Network. This helps focus on specific layers for further inspection.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug model ./config.cfg tagger -P0\n```\n\n----------------------------------------\n\nTITLE: Serializing Morphologizer to Bytes in Python\nDESCRIPTION: Demonstrates how to serialize the Morphologizer pipe to a bytestring for compact storage or transmission.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\nmorphologizer_bytes = morphologizer.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Learning Examples for Lemma Task in YAML\nDESCRIPTION: YAML file format for providing few-shot examples to the Lemma task. Each example contains text and its corresponding lemma mappings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_47\n\nLANGUAGE: yaml\nCODE:\n```\n- text: I'm buying ice cream.\n  lemmas:\n    - 'I': 'I'\n    - \"'m\": 'be'\n    - 'buying': 'buy'\n    - 'ice': 'ice'\n    - 'cream': 'cream'\n    - '.': '.'\n\n- text: I've watered the plants.\n  lemmas:\n    - 'I': 'I'\n    - \"'ve\": 'have'\n    - 'watered': 'water'\n    - 'the': 'the'\n    - 'plants': 'plant'\n    - '.': '.'\n```\n\n----------------------------------------\n\nTITLE: Installing and Verifying spacy-huggingface-hub in Bash\nDESCRIPTION: These bash commands show how to install the spacy-huggingface-hub package and verify that the huggingface-hub command is registered with the spaCy CLI.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_39\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install spacy-huggingface-hub\n# Check that the CLI is registered\n$ python -m spacy huggingface-hub --help\n```\n\n----------------------------------------\n\nTITLE: Saving Knowledge Base State to Disk in Python\nDESCRIPTION: Shows how to save the current state of a knowledge base to a directory using the to_disk method. Accepts a path parameter that can be string or Path object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/inmemorylookupkb.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nkb.to_disk(path)\n```\n\n----------------------------------------\n\nTITLE: Serializing LLM Components to Disk in spaCy\nDESCRIPTION: Example demonstrating how to save an LLM component to disk, which persists the component's configuration and state for later reuse.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nllm_ner = nlp.add_pipe(\"llm_ner\")\nllm_ner.to_disk(\"/path/to/llm_ner\")\n```\n\n----------------------------------------\n\nTITLE: Loading TrainablePipe from Disk\nDESCRIPTION: Shows how to load a previously saved pipeline component from disk. The method modifies the pipe object in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\npipe.from_disk(\"/path/to/pipe\")\n```\n\n----------------------------------------\n\nTITLE: Converting Example to Dictionary in spaCy\nDESCRIPTION: Shows how to convert a spaCy Example object to its dictionary representation using the to_dict method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\neg_dict = example.to_dict()\n```\n\n----------------------------------------\n\nTITLE: Converting BILUO Tags to Entity Offsets\nDESCRIPTION: Example showing how to convert BILUO tag annotations back to character offset-based entity annotations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import biluo_tags_to_offsets\n\ndoc = nlp(\"I like London.\")\ntags = [\"O\", \"O\", \"U-LOC\", \"O\"]\nentities = biluo_tags_to_offsets(doc, tags)\nassert entities == [(7, 13, \"LOC\")]\n```\n\n----------------------------------------\n\nTITLE: Implementing Relation Candidate Generator\nDESCRIPTION: Creates a function that generates candidate entity pairs for relation prediction. This implementation considers all pairs of entities within a specified maximum token distance.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n@spacy.registry.misc(\"rel_instance_generator.v1\")\ndef create_instances(max_length: int) -> Callable[[Doc], List[Tuple[Span, Span]]]:\n    def get_candidates(doc: \"Doc\") -> List[Tuple[Span, Span]]:\n        candidates = []\n        for ent1 in doc.ents:\n            for ent2 in doc.ents:\n                if ent1 != ent2:\n                    if max_length and abs(ent2.start - ent1.start) <= max_length:\n                        candidates.append((ent1, ent2))\n        return candidates\n    return get_candidates\n```\n\n----------------------------------------\n\nTITLE: Loading Tok2Vec Component from Disk in Python\nDESCRIPTION: Shows how to load a previously saved Tok2Vec component from disk. This is used to restore a trained component for use in a pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntok2vec = nlp.add_pipe(\"tok2vec\")\ntok2vec.from_disk(\"/path/to/tok2vec\")\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Tokens in a Doc (Python)\nDESCRIPTION: Shows how to iterate through all tokens in a Doc object to access token-level annotations and attributes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back\")\nassert [t.text for t in doc] == [\"Give\", \"it\", \"back\"]\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Entity Candidates\nDESCRIPTION: Demonstrates retrieving candidates for multiple mentions simultaneously.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/inmemorylookupkb.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\nnlp = English()\ndoc = nlp(\"Douglas Adams wrote 'The Hitchhiker's Guide to the Galaxy'.\")\ncandidates = kb.get_candidates((doc[0:2], doc[3:]))\n```\n\n----------------------------------------\n\nTITLE: Debugging - Tokenizer Explanation Example\nDESCRIPTION: Example showing how to use the explain method to debug tokenization rules and patterns.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tokenizer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntok_exp = nlp.tokenizer.explain(\"(don't)\")\nassert [t[0] for t in tok_exp] == [\"PREFIX\", \"SPECIAL-1\", \"SPECIAL-2\", \"SUFFIX\"]\nassert [t[1] for t in tok_exp] == [\"(\", \"do\", \"n't\", \")\"]\n```\n\n----------------------------------------\n\nTITLE: Loading EditTreeLemmatizer from Disk in Python\nDESCRIPTION: Shows how to load the EditTreeLemmatizer pipe from disk using the from_disk method. This method loads the lemmatizer from a specified directory path and modifies the object in place.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\nlemmatizer.from_disk(\"/path/to/lemmatizer\")\n```\n\n----------------------------------------\n\nTITLE: Using Language.add_pipe with String Component Name in Python\nDESCRIPTION: In spaCy v3.0, Language.add_pipe now takes the string name of the component factory instead of the component function. This change affects how custom pipeline components are added.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nLanguage.add_pipe(\"component_name\")\n```\n\n----------------------------------------\n\nTITLE: Validating spaCy Pipeline Compatibility\nDESCRIPTION: Command to check compatibility of installed pipeline packages with the current version of spaCy. It lists packages, versions, and provides update information if needed.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy validate\n```\n\n----------------------------------------\n\nTITLE: Serializing DependencyParser to Disk in Python\nDESCRIPTION: This code shows how to serialize a DependencyParser component to disk. It adds a parser to the NLP pipeline and then saves it to a specified file path.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\nparser.to_disk(\"/path/to/parser\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Test Command in spaCy project.yml\nDESCRIPTION: YAML configuration for a custom test command that runs pytest and generates an HTML report. It specifies dependencies, outputs, and ensures tests always run with no_skip: true option.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\ncommands:\n  - name: test\n    help: 'Test the trained pipeline'\n    script:\n      - 'pip install pytest pytest-html'\n      - 'python -m pytest ./scripts/tests --html=metrics/test-report.html'\n    deps:\n      - 'training/model-best'\n    outputs:\n      - 'metrics/test-report.html'\n    no_skip: true\n```\n\n----------------------------------------\n\nTITLE: Serializing spaCy Tagger to Disk\nDESCRIPTION: Shows how to save a tagger component to disk. The component is serialized to a specified directory path.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\ntagger.to_disk(\"/path/to/tagger\")\n```\n\n----------------------------------------\n\nTITLE: Loading StringStore from Disk in Python\nDESCRIPTION: Use the from_disk method to load a StringStore state from a directory.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/stringstore.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.strings import StringStore\nstringstore = StringStore().from_disk(\"/path/to/strings\")\n```\n\n----------------------------------------\n\nTITLE: Registering Custom PyTorch Architecture in spaCy\nDESCRIPTION: Demonstrates how to register a custom architecture that includes a PyTorch subnetwork using spaCy's registry system.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom thinc.types import Floats2d\nfrom thinc.api import Model, PyTorchWrapper, chain, with_array\nimport spacy\nfrom spacy.tokens.doc import Doc\nfrom spacy.ml import CharacterEmbed\nfrom torch import nn\n\n@spacy.registry.architectures(\"CustomTorchModel.v1\")\ndef create_torch_model(\n    nO: int,\n    width: int,\n    hidden_width: int,\n    embed_size: int,\n    nM: int,\n    nC: int,\n    dropout: float,\n) -> Model[List[Doc], List[Floats2d]]:\n    char_embed = CharacterEmbed(width, embed_size, nM, nC)\n    torch_model = nn.Sequential(\n        nn.Linear(width, hidden_width),\n        nn.ReLU(),\n        nn.Dropout2d(dropout),\n        nn.Linear(hidden_width, nO),\n        nn.ReLU(),\n        nn.Dropout2d(dropout),\n        nn.Softmax(dim=1)\n    )\n    wrapped_pt_model = PyTorchWrapper(torch_model)\n    model = chain(char_embed, with_array(wrapped_pt_model))\n    return model\n```\n\n----------------------------------------\n\nTITLE: Using PlainTextCorpus with spaCy in Python\nDESCRIPTION: Demonstrates how to use PlainTextCorpus with a spaCy Language object to yield examples from the data. This is typically used in training or processing pipelines.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/corpus.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import PlainTextCorpus\nimport spacy\n\ncorpus = PlainTextCorpus(\"./docs.txt\")\nnlp = spacy.blank(\"en\")\ndata = corpus(nlp)\n```\n\n----------------------------------------\n\nTITLE: Entity Recognition Input Format for displaCy\nDESCRIPTION: Structure of input data for named entity visualization using displaCy. The format includes text content and entity positions with labels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"text\": \"But Google is starting from behind.\",\n    \"ents\": [{\"start\": 4, \"end\": 10, \"label\": \"ORG\"}],\n    \"title\": None\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Corpus with Training Data\nDESCRIPTION: Example showing how to initialize a Corpus object with training data file or directory.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/corpus.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import Corpus\n\n# With a single file\ncorpus = Corpus(\"./data/train.spacy\")\n\n# With a directory\ncorpus = Corpus(\"./data\", limit=10)\n```\n\n----------------------------------------\n\nTITLE: Configuring WithStridedSpans Span Getter in INI\nDESCRIPTION: Demonstrates how to configure the WithStridedSpans span getter in the pipeline configuration file, specifying window and stride sizes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_15\n\nLANGUAGE: ini\nCODE:\n```\n[transformer.model.with_spans]\n@architectures = \"spacy-curated-transformers.WithStridedSpans.v1\"\nstride = 96\nwindow = 128\n```\n\n----------------------------------------\n\nTITLE: Configuring Few-Shot Learning with Example Reader for NER.v3\nDESCRIPTION: Configuration for providing few-shot examples to the NER.v3 component using the FewShotReader.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_26\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task.examples]\n@misc = \"spacy.FewShotReader.v1\"\npath = \"${paths.examples}\"\n```\n\n----------------------------------------\n\nTITLE: Finding Function Location in spaCy\nDESCRIPTION: Example command showing how to locate the source file and line number of a registered spaCy function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy find-function spacy.TextCatBOW.v1\n```\n\n----------------------------------------\n\nTITLE: Creating Pipeline Component in spaCy\nDESCRIPTION: Example of creating a pipeline component using the create_pipe method. Note: As of v3.0, Language.add_pipe is preferred for creating and adding components.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.create_pipe(\"parser\")\n```\n\n----------------------------------------\n\nTITLE: Checking Token Vector Existence in spaCy\nDESCRIPTION: Shows how to check if a token has an associated word vector.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like apples\")\napples = doc[2]\nassert apples.has_vector\n```\n\n----------------------------------------\n\nTITLE: Retrieving Doc Extensions in spaCy\nDESCRIPTION: Demonstrates how to retrieve a previously registered extension on a Doc object, returning a tuple containing the default value and associated methods.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc\nDoc.set_extension(\"has_city\", default=False)\nextension = Doc.get_extension(\"has_city\")\nassert extension == (False, None, None, None)\n```\n\n----------------------------------------\n\nTITLE: Updating Parser Model\nDESCRIPTION: Example of training the parser model using examples and an optimizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\noptimizer = nlp.initialize()\nlosses = parser.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Adding a Label to Morphologizer in Python\nDESCRIPTION: Demonstrates how to add a new label to the Morphologizer pipe. The label includes morphological features and part-of-speech information.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\nmorphologizer.add_label(\"Mood=Ind|POS=VERB|Tense=Past|VerbForm=Fin\")\n```\n\n----------------------------------------\n\nTITLE: Saving Tokenizer to Disk\nDESCRIPTION: Demonstrates how to serialize a spaCy Tokenizer object to disk storage. Requires an initialized tokenizer with a vocabulary.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tokenizer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntokenizer = Tokenizer(nlp.vocab)\ntokenizer.to_disk(\"/path/to/tokenizer\")\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields during EditTreeLemmatizer Serialization in Python\nDESCRIPTION: Demonstrates how to exclude specific fields during the serialization of the EditTreeLemmatizer. This example shows excluding the 'vocab' field when saving to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndata = lemmatizer.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Getting Path to Installed spaCy Package\nDESCRIPTION: Resolves the file system location of an installed package, primarily used for finding pipeline packages. Currently imports the package to determine its path.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nutil.get_package_path(\"en_core_web_sm\")\n# /usr/lib/python3.6/site-packages/en_core_web_sm\n```\n\n----------------------------------------\n\nTITLE: Loading Knowledge Base State from Disk in Python\nDESCRIPTION: Demonstrates loading a knowledge base state from disk using from_disk method. Requires matching Vocab object and initialized KB with specified entity vector length.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/inmemorylookupkb.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.vocab import Vocab\nvocab = Vocab().from_disk(\"/path/to/vocab\")\nkb = InMemoryLookupKB(vocab=vocab, entity_vector_length=64)\nkb.from_disk(\"/path/to/kb\")\n```\n\n----------------------------------------\n\nTITLE: Saving Vocab State to Disk (Python)\nDESCRIPTION: Illustrates how to save the current state of the Vocab object to a directory using the to_disk method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nnlp.vocab.to_disk(\"/path/to/vocab\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Validating spaCy Pipeline Metadata\nDESCRIPTION: Gets a pipeline's meta.json file from a specified path and validates its contents. Meta data typically includes details about author, licensing, data sources, and version information.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nmeta = util.load_meta(\"/path/to/meta.json\")\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Config Override\nDESCRIPTION: Example showing how to override configuration settings using environment variables, useful for automated training processes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ SPACY_CONFIG_OVERRIDES=\"--system.gpu_allocator pytorch --training.max_epochs 3\" ./your_script.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Span Visualizer Options in spaCy displaCy\nDESCRIPTION: Example showing how to customize span visualization by specifying a custom spans key 'sc' to render spans from that collection in the document.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\noptions = {\"spans_key\": \"sc\"}\ndisplacy.serve(doc, style=\"span\", options=options)\n```\n\n----------------------------------------\n\nTITLE: Component Configuration in spaCy v3.0 Config File\nDESCRIPTION: Shows how a custom component appears in the spaCy configuration file (config.cfg). The factory name and settings are automatically included in the configuration, enabling reproducible pipeline creation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_33\n\nLANGUAGE: ini\nCODE:\n```\n[components.my_component]\nfactory = \"my_component\"\nsome_setting = true\n```\n\n----------------------------------------\n\nTITLE: Serializing a spaCy Doc to JSON in Python\nDESCRIPTION: Illustrates how to serialize a spaCy Doc object to JSON format using the to_json() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"All we have to decide is what to do with the time that is given us.\")\nassert doc.to_json()[\"text\"] == doc.text\n```\n\n----------------------------------------\n\nTITLE: Initializing Scorer in spaCy\nDESCRIPTION: Creates a new Scorer instance, which can be initialized with a default pipeline or a provided one. The Scorer is used to evaluate the performance of spaCy models.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/scorer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.scorer import Scorer\n\n# Default scoring pipeline\nscorer = Scorer()\n\n# Provided scoring pipeline\nnlp = spacy.load(\"en_core_web_sm\")\nscorer = Scorer(nlp)\n```\n\n----------------------------------------\n\nTITLE: Using Corpus with spaCy Pipeline\nDESCRIPTION: Example demonstrating how to use a Corpus object with a spaCy language pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/corpus.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import Corpus\nimport spacy\n\ncorpus = Corpus(\"./train.spacy\")\nnlp = spacy.blank(\"en\")\ntrain_data = corpus(nlp)\n```\n\n----------------------------------------\n\nTITLE: Implementing Callback with Arguments in Python\nDESCRIPTION: Python implementation of a custom callback function that accepts arguments for extra stop words and debug mode. It uses type hints for better type checking.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport spacy\n\n@spacy.registry.callbacks(\"customize_language_data\")\ndef create_callback(extra_stop_words: List[str] = [], debug: bool = False):\n    def customize_language_data(lang_cls):\n        lang_cls.Defaults.stop_words.update(extra_stop_words)\n        if debug:\n            print(\"Updated stop words\")\n        return lang_cls\n\n    return customize_language_data\n```\n\n----------------------------------------\n\nTITLE: Loading EntityRecognizer from Bytes\nDESCRIPTION: Shows how to load a named entity recognition model from a bytestring using from_bytes(). Returns the modified EntityRecognizer object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nner_bytes = ner.to_bytes()\nner = nlp.add_pipe(\"ner\")\nner.from_bytes(ner_bytes)\n```\n\n----------------------------------------\n\nTITLE: Example Project Directory Structure in spaCy\nDESCRIPTION: Example layout of a typical spaCy project directory, showing the organizational structure with the project configuration files and various directories for assets, configs, corpora, and outputs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\n├── project.yml          # the project settings\n├── project.lock         # lockfile that tracks inputs/outputs\n├── assets/              # downloaded data assets\n├── configs/             # pipeline config.cfg files used for training\n├── corpus/              # output directory for training corpus\n├── metas/               # pipeline meta.json templates used for packaging\n├── metrics/             # output directory for evaluation metrics\n├── notebooks/           # directory for Jupyter notebooks\n├── packages/            # output directory for pipeline Python packages\n├── scripts/             # directory for scripts, e.g. referenced in commands\n├── training/            # output directory for trained pipelines\n└── ...                  # any other files, like a requirements.txt etc.\n```\n\n----------------------------------------\n\nTITLE: Checking Rule Existence in DependencyMatcher in Python\nDESCRIPTION: This example shows how to check if a specific rule exists in a DependencyMatcher object using the 'in' operator. It demonstrates checking before and after adding a rule.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencymatcher.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmatcher = DependencyMatcher(nlp.vocab)\nassert \"FOUNDED\" not in matcher\nmatcher.add(\"FOUNDED\", [pattern])\nassert \"FOUNDED\" in matcher\n```\n\n----------------------------------------\n\nTITLE: Serializing Transformer Component to Disk in Python\nDESCRIPTION: This example shows how to save the Transformer component to disk for later use or distribution.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"transformer\")\ntrf.to_disk(\"/path/to/transformer\")\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCatReduce Model for Text Classification in spaCy\nDESCRIPTION: Configuration for the TextCatReduce model which pools token hidden representations of each document using various reduction methods (first, max, mean) and applies a classification layer. It's a generalization of the older TextCatCNN model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_27\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.TextCatReduce.v1\"\nexclusive_classes = false\nuse_reduce_first = false\nuse_reduce_last = false\nuse_reduce_max = false\nuse_reduce_mean = true\nnO = null\n\n[model.tok2vec]\n@architectures = \"spacy.HashEmbedCNN.v2\"\npretrained_vectors = null\nwidth = 96\ndepth = 4\nembed_size = 2000\nwindow_size = 1\nmaxout_pieces = 3\nsubword_features = true\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy Tagger from Disk\nDESCRIPTION: Demonstrates loading a previously saved tagger component from disk. The method modifies the tagger object in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\ntagger.from_disk(\"/path/to/tagger\")\n```\n\n----------------------------------------\n\nTITLE: Converting MorphAnalysis to Dictionary in Python\nDESCRIPTION: Shows how to convert a MorphAnalysis object to a dictionary representation using the to_dict method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfeats = \"Feat1=Val1,Val2|Feat2=Val2\"\nmorph = MorphAnalysis(nlp.vocab, feats)\nassert morph.to_dict() == {\"Feat1\": \"Val1,Val2\", \"Feat2\": \"Val2\"}\n```\n\n----------------------------------------\n\nTITLE: Customizing Entity Visualization Options in spaCy\nDESCRIPTION: This example shows how to customize the entity visualizer by filtering which entity types to display and overriding their default colors. The code filters to show only organization entities and applies a custom gradient background to them.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncolors = {\"ORG\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\"}\noptions = {\"ents\": [\"ORG\"], \"colors\": colors}\ndisplacy.serve(doc, style=\"ent\", options=options)\n```\n\n----------------------------------------\n\nTITLE: Initializing DocBin Instance\nDESCRIPTION: Shows how to create a new DocBin object with specific attributes to serialize.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/docbin.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import DocBin\ndoc_bin = DocBin(attrs=[\"ENT_IOB\", \"ENT_TYPE\"])\n```\n\n----------------------------------------\n\nTITLE: Loading Lemmatizer from Bytes\nDESCRIPTION: Example showing how to load a serialized lemmatizer pipeline component from a bytestring using the from_bytes() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lemmatizer.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer_bytes = lemmatizer.to_bytes()\nlemmatizer = nlp.add_pipe(\"lemmatizer\")\nlemmatizer.from_bytes(lemmatizer_bytes)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Tok2Vec Model from Pipeline Component\nDESCRIPTION: Code showing how to fetch the original Model from a Tok2Vec component in the pipeline. This is the initial step in the replace_listeners process to access the embedding model.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Listeners.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntok2vec = self.get_pipe(tok2vec_name)\ntok2vec_model = tok2vec.model\n```\n\n----------------------------------------\n\nTITLE: Configuring Lemma Task with Few-Shot Examples\nDESCRIPTION: Configuration for the Lemma task that includes few-shot examples from a YAML file using the FewShotReader.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_48\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.Lemma.v1\"\n[components.llm.task.examples]\n@misc = \"spacy.FewShotReader.v1\"\npath = \"lemma_examples.yml\"\n```\n\n----------------------------------------\n\nTITLE: Excluding fields during EntityLinker serialization in Python\nDESCRIPTION: This example shows how to exclude specific serialization fields when saving an EntityLinker to disk. In this case, the 'vocab' field is excluded from the serialization process.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndata = entity_linker.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Initializing Tok2Vec Component\nDESCRIPTION: Different ways to construct and initialize the Tok2Vec component, including via add_pipe with default/custom models and direct class instantiation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\ntok2vec = nlp.add_pipe(\"tok2vec\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_tok2vec\"}}\nparser = nlp.add_pipe(\"tok2vec\", config=config)\n\n# Construction from class\nfrom spacy.pipeline import Tok2Vec\ntok2vec = Tok2Vec(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Configuring Pipeline Components in INI Format\nDESCRIPTION: Example configuration for pipeline components using the config.cfg file format. Shows how to specify factory and component settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_18\n\nLANGUAGE: ini\nCODE:\n```\n[components.sentencizer]\nfactory = \"sentencizer\"\npunct_chars = [\"!\", \".\", \"?\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Project Directories in spaCy project.yml\nDESCRIPTION: Configuration for defining standard directories that should be created within a spaCy project. These directories will be automatically created and available for commands to use.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\ndirectories: ['assets', 'configs', 'corpus', 'metas', 'metrics', 'notebooks', 'packages', 'scripts', 'training']\n```\n\n----------------------------------------\n\nTITLE: Debug Pieces Output Example\nDESCRIPTION: Sample output from the debug pieces command showing token length statistics for both training and development corpora, including median and mean token lengths and token length ranges.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_28\n\nLANGUAGE: text\nCODE:\n```\n========================= Training corpus statistics =========================\nMedian token length: 1.0\nMean token length: 1.54\nToken length range: [1, 13]\n\n======================= Development corpus statistics =======================\nMedian token length: 1.0\nMean token length: 1.44\nToken length range: [1, 8]\n```\n\n----------------------------------------\n\nTITLE: Serializing Lemmatizer to Disk\nDESCRIPTION: Example showing how to serialize a lemmatizer pipeline component to disk using the to_disk() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lemmatizer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"lemmatizer\")\nlemmatizer.to_disk(\"/path/to/lemmatizer\")\n```\n\n----------------------------------------\n\nTITLE: Defining Project Entry in JSON for spaCy Universe\nDESCRIPTION: This JSON structure defines the format for adding a new project to the spaCy Universe. It includes fields for project metadata, code examples, author information, and categorization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/UNIVERSE.md#2025-04-21_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"unique-project-id\",\n    \"title\": \"Project title\",\n    \"slogan\": \"A short summary\",\n    \"description\": \"A longer description – *Markdown allowed!*\",\n    \"github\": \"user/repo\",\n    \"pip\": \"package-name\",\n    \"code_example\": [\n        \"import spacy\",\n        \"import package_name\",\n        \"\",\n        \"nlp = spacy.load('en')\",\n        \"nlp.add_pipe(package_name)\"\n    ],\n    \"code_language\": \"python\",\n    \"url\": \"https://example.com\",\n    \"thumb\": \"https://example.com/thumb.jpg\",\n    \"image\": \"https://example.com/image.jpg\",\n    \"author\": \"Your Name\",\n    \"author_links\": {\n        \"twitter\": \"username\",\n        \"github\": \"username\",\n        \"website\": \"https://example.com\"\n    },\n    \"category\": [\"pipeline\", \"standalone\"],\n    \"tags\": [\"some-tag\", \"etc\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Document Spans in INI\nDESCRIPTION: Configuration example for using doc_spans.v1 span getter in the transformer model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_18\n\nLANGUAGE: ini\nCODE:\n```\n[transformer.model.get_spans]\n@span_getters = \"spacy-transformers.doc_spans.v1\"\n```\n\n----------------------------------------\n\nTITLE: Loading Probability Tables in spaCy\nDESCRIPTION: Example of loading probability tables from spacy-lookups-data into an existing spaCy model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Requirements: pip install spacy-lookups-data\nimport spacy\nfrom spacy.lookups import load_lookups\nnlp = spacy.load(\"en_core_web_sm\")\nlookups = load_lookups(\"en\", [\"lexeme_prob\"])\nnlp.vocab.lookups.add_table(\"lexeme_prob\", lookups.get_table(\"lexeme_prob\"))\n```\n\n----------------------------------------\n\nTITLE: Referencing spaCy Default Configuration File\nDESCRIPTION: Reference to the default configuration file in spaCy, which contains the base settings and structure for training pipelines.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n%%GITHUB_SPACY/spacy/default_config.cfg\n```\n\n----------------------------------------\n\nTITLE: Converting FEATS String to Dictionary in Python\nDESCRIPTION: Static method that converts a Universal Dependencies FEATS string to a dictionary of features and values.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.morphology import Morphology\nd = Morphology.feats_to_dict(\"Feat1=Val1|Feat2=Val2\")\nassert d == {\"Feat1\": \"Val1\", \"Feat2\": \"Val2\"}\n```\n\n----------------------------------------\n\nTITLE: Checking Token Vector Properties in spaCy\nDESCRIPTION: Script demonstrating how to check vector properties of tokens including vector presence, vector norm, and out-of-vocabulary status.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_vectors-similarity.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_md\")\ntokens = nlp(\"dog cat banana afskfsd\")\n\nfor token in tokens:\n    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n```\n\n----------------------------------------\n\nTITLE: Serializing EntityLinker to Disk in spaCy (Python)\nDESCRIPTION: Demonstrates how to save the EntityLinker component to disk. This method serializes all the component data, including the model and configurations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nentity_linker = nlp.add_pipe(\"entity_linker\")\nentity_linker.to_disk(\"/path/to/entity_linker\")\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy Corpus Training\nDESCRIPTION: Configuration example for setting up a spaCy corpus reader with training path and parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/corpus.mdx#2025-04-21_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[paths]\ntrain = \"corpus/train.spacy\"\n\n[corpora.train]\n@readers = \"spacy.Corpus.v1\"\npath = ${paths.train}\ngold_preproc = false\nmax_length = 0\nlimit = 0\naugmenter = null\n```\n\n----------------------------------------\n\nTITLE: Scoring Coreference Clusters in Python using spaCy\nDESCRIPTION: This snippet demonstrates how to use the score_coref_clusters function to compute LEA (Link-Based Entity-Aware) PRF scores for coreference clusters. It takes examples and a span cluster prefix as input and returns a dictionary of scores.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/scorer.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nscores = score_coref_clusters(\n    examples,\n    span_cluster_prefix=\"coref_clusters\",\n)\nprint(scores[\"coref_f\"])\n```\n\n----------------------------------------\n\nTITLE: Counting Right Dependencies in spaCy\nDESCRIPTION: Demonstrates how to count the number of right dependencies of a span.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York in Autumn.\")\nassert doc[2:4].n_rights == 1\n```\n\n----------------------------------------\n\nTITLE: Remote Storage Configuration\nDESCRIPTION: YAML configuration for defining remote storage locations for project assets.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nremotes:\n  default: 's3://my-spacy-bucket'\n  local: '/mnt/scratch/cache'\n```\n\n----------------------------------------\n\nTITLE: Configuring Score Weights in spaCy Training\nDESCRIPTION: Example of a configuration snippet for customizing score weights in spaCy's training evaluation. It defines how different metrics like dependency parsing accuracy, NER F-score, and tagging accuracy should be weighted to determine the best model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_18\n\nLANGUAGE: ini\nCODE:\n```\n[training.score_weights]\ndep_las = 0.4\ndep_uas = null\nents_f = 0.4\ntag_acc = 0.2\ntoken_acc = 0.0\nspeed = 0.0\n```\n\n----------------------------------------\n\nTITLE: Adding custom flags to spaCy's Vocab\nDESCRIPTION: Demonstrates how to add a custom boolean flag to words in the vocabulary. This allows for custom categorization of words based on user-defined criteria.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef is_my_product(text):\n    products = [\"spaCy\", \"Thinc\", \"displaCy\"]\n    return text in products\n\nMY_PRODUCT = nlp.vocab.add_flag(is_my_product)\ndoc = nlp(\"I like spaCy\")\nassert doc[2].check_flag(MY_PRODUCT) == True\n```\n\n----------------------------------------\n\nTITLE: Pulling Project Outputs from Remote Storage\nDESCRIPTION: Command to download project outputs from remote storage with smart dependency tracking and version management. Supports various cloud storage protocols through Pathy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_48\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project pull [remote] [project_dir]\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project pull my_bucket\n```\n\nLANGUAGE: yaml\nCODE:\n```\n### project.yml\nremotes:\n  my_bucket: 's3://my-spacy-bucket'\n```\n\n----------------------------------------\n\nTITLE: Scoring morphological features per attribute in spaCy\nDESCRIPTION: Evaluates morphological features in Universal Dependencies FEATS format, calculating per-feature precision, recall, and F-scores. This method provides detailed scoring for complex linguistic features.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/scorer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nscores = Scorer.score_token_attr_per_feat(examples, \"morph\")\nprint(scores[\"morph_per_feat\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanFinder Pipeline with Spancat in INI\nDESCRIPTION: Configuration example for setting up a pipeline with span_finder and spancat components, including the required training settings for annotating components.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-6.mdx#2025-04-21_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\npipeline = [\"tok2vec\",\"span_finder\",\"spancat\"]\n\n[training]\nannotating_components = [\"tok2vec\",\"span_finder\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Japanese Tokenizer in spaCy\nDESCRIPTION: Shows how to configure the Japanese tokenizer in the spaCy config file, specifying the tokenizer class and split mode.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\n[nlp.tokenizer]\n@tokenizers = \"spacy.ja.JapaneseTokenizer\"\nsplit_mode = \"A\"\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy Tagger from Bytes\nDESCRIPTION: Demonstrates deserializing a tagger component from a bytestring. The method modifies the object in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntagger_bytes = tagger.to_bytes()\ntagger = nlp.add_pipe(\"tagger\")\ntagger.from_bytes(tagger_bytes)\n```\n\n----------------------------------------\n\nTITLE: Checking Label Presence in SpanRuler\nDESCRIPTION: Shows how to check if a specific label is present in the SpanRuler patterns using the __contains__ method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"span_ruler\")\nruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\nassert \"ORG\" in ruler\nassert not \"PERSON\" in ruler\n```\n\n----------------------------------------\n\nTITLE: Configuring FeatureExtractor Architecture in spaCy\nDESCRIPTION: Configuration for FeatureExtractor.v1 that extracts arrays of input features from Doc objects based on specified token attributes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_9\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.FeatureExtractor.v1\"\ncolumns = [\"NORM\", \"PREFIX\", \"SUFFIX\", \"SHAPE\", \"ORTH\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Corpus Reader in spaCy using INI Format\nDESCRIPTION: Example configuration for the Corpus reader that manages annotated corpora in DocBin format. The configuration specifies the path to training data, preprocessing options, and other corpus parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_26\n\nLANGUAGE: ini\nCODE:\n```\n[paths]\ntrain = \"corpus/train.spacy\"\n\n[corpora.train]\n@readers = \"spacy.Corpus.v1\"\npath = ${paths.train}\ngold_preproc = false\nmax_length = 0\nlimit = 0\n```\n\n----------------------------------------\n\nTITLE: Serializing SpanCategorizer to Bytes\nDESCRIPTION: Example showing how to serialize a SpanCategorizer component to a bytestring.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\nspancat_bytes = spancat.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Accessing Span Subtree in spaCy\nDESCRIPTION: Shows how to access all tokens within a span and their descendants.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\nsubtree = [t.text for t in doc[:3].subtree]\nassert subtree == [\"Give\", \"it\", \"back\", \"!\"]\n```\n\n----------------------------------------\n\nTITLE: String Store Management in spaCy v2.0\nDESCRIPTION: Shows the updated string handling with explicit string addition to the vocabulary and consistent hash values across different instances.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_17\n\nLANGUAGE: diff\nCODE:\n```\n- nlp.vocab.strings[\"coffee\"]       # 3672\n- other_nlp.vocab.strings[\"coffee\"] # 40259\n\n+ nlp.vocab.strings.add(\"coffee\")\n+ nlp.vocab.strings[\"coffee\"]       # 3197928453018144401\n+ other_nlp.vocab.strings[\"coffee\"] # 3197928453018144401\n```\n\n----------------------------------------\n\nTITLE: LangChain OpenAI Configuration Example\nDESCRIPTION: Configuration example for using OpenAI models through LangChain integration\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_62\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.model]\n@llm_models = \"langchain.OpenAI.v1\"\nname = \"gpt-3.5-turbo\"\nquery = {\"@llm_queries\": \"spacy.CallLangChain.v1\"}\nconfig = {\"temperature\": 0.0}\n```\n\n----------------------------------------\n\nTITLE: Migrating Document Processing in spaCy 2.0\nDESCRIPTION: Update document processing to use Language.pipe() method for batching and improved performance in spaCy 2.0.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n- docs = (nlp(text) for text in texts)\n\n+ docs = nlp.pipe(texts)\n```\n\n----------------------------------------\n\nTITLE: Saving spaCy Doc State to Disk (Python)\nDESCRIPTION: Demonstrates how to save the current state of a spaCy Doc object to a directory on disk. This method allows for persisting Doc objects.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndoc.to_disk(\"/path/to/doc\")\n```\n\n----------------------------------------\n\nTITLE: Computing Loss for NER Training\nDESCRIPTION: Shows how to calculate loss and gradient for a batch of examples during NER training. Uses predicted scores to compute the model's error.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\nscores = ner.predict([eg.predicted for eg in examples])\nloss, d_loss = ner.get_loss(examples, scores)\n```\n\n----------------------------------------\n\nTITLE: Loading SpanResolver from Disk in Python\nDESCRIPTION: Load a previously saved SpanResolver component from a directory on disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\nspan_resolver.from_disk(\"/path/to/span_resolver\")\n```\n\n----------------------------------------\n\nTITLE: Defining spaCy Entry Points in Python\nDESCRIPTION: Shows how to define custom components and languages as entry points in a package setup file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-1.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools import setup\nsetup(\n    name=\"custom_extension_package\",\n    entry_points={\n        \"spacy_factories\": [\"your_component = component:ComponentFactory\"]\n        \"spacy_languages\": [\"xyz = language:XYZLanguage\"]\n   }\n)\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Debug Pieces Command\nDESCRIPTION: Example of running the debug pieces command with a configuration file to analyze piece statistics in a spaCy model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug pieces ./config.cfg\n```\n\n----------------------------------------\n\nTITLE: Checking String Presence in StringStore in Python\nDESCRIPTION: Use the 'in' operator to check if a string is present in the StringStore.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/stringstore.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstringstore = StringStore([\"apple\", \"orange\"])\nassert \"apple\" in stringstore\nassert not \"cherry\" in stringstore\n```\n\n----------------------------------------\n\nTITLE: Updating spacy-transformers Version Requirement in meta.json\nDESCRIPTION: This diff shows how to update the spacy-transformers version requirement in the meta.json file when upgrading from v1.0 to v1.1. It changes the version constraint to ensure compatibility with the new version.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-2.mdx#2025-04-21_snippet_7\n\nLANGUAGE: diff\nCODE:\n```\n  \"requirements\": [\n-    \"spacy-transformers>=1.0.3,<1.1.0\"\n+    \"spacy-transformers>=1.1.2,<1.2.0\"\n  ]\n```\n\n----------------------------------------\n\nTITLE: Serializing Transformer Model to Bytes\nDESCRIPTION: Example showing how to serialize a spaCy transformer pipeline component to bytes using the to_bytes() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"transformer\")\ntrf_bytes = trf.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Converting MorphAnalysis to String in Python\nDESCRIPTION: Shows how to convert a MorphAnalysis object to a string in the Universal Dependencies FEATS format using the __str__ method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfeats = \"Feat1=Val1,Val2|Feat2=Val2\"\nmorph = MorphAnalysis(nlp.vocab, feats)\nassert str(morph) == feats\n```\n\n----------------------------------------\n\nTITLE: Loading AttributeRuler from Bytes - Python\nDESCRIPTION: Example demonstrating how to load an AttributeRuler pipe from a bytestring using from_bytes(). Shows full process of serializing and deserializing.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributeruler.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nruler_bytes = ruler.to_bytes()\nruler = nlp.add_pipe(\"attribute_ruler\")\nruler.from_bytes(ruler_bytes)\n```\n\n----------------------------------------\n\nTITLE: Updating Span Creation for SpanGroup Compatibility\nDESCRIPTION: Code diff showing the required changes for creating spans in Example objects to ensure compatibility with the new SpanGroup doc validation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-6.mdx#2025-04-21_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n     doc = Doc(nlp.vocab, words=tokens)  # predicted doc\n     example = Example.from_dict(doc, {\"ner\": iob_tags})\n     # use the reference doc when creating reference spans\n-    span = Span(doc, 0, 5, \"ORG\")\n+    span = Span(example.reference, 0, 5, \"ORG\")\n     example.reference.spans[spans_key] = [span]\n```\n\n----------------------------------------\n\nTITLE: Using Temporary Parameters for Coreference Resolver in Python with spaCy\nDESCRIPTION: This code shows how to temporarily modify the coreference resolver's model parameters using a context manager, typically used for saving the best model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncoref = nlp.add_pipe(\"experimental_coref\")\nwith coref.use_params(optimizer.averages):\n    coref.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Updating Knowledge Base Usage in spaCy (Python)\nDESCRIPTION: Demonstrates the change in instantiating a knowledge base in spaCy v3.5, using the new InMemoryLookupKB class instead of the abstract KnowledgeBase class.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-5.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n- kb = KnowledgeBase()\n+ kb = InMemoryLookupKB()\n```\n\n----------------------------------------\n\nTITLE: Defining Custom displaCy Colors for Entity Visualization\nDESCRIPTION: Creates a dictionary mapping custom named entity labels to color values that will be used by the displaCy visualizer when rendering those entities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndisplacy_colors = {\"SNEK\": \"#3dff74\", \"HUMAN\": \"#cfc5ff\"}\n```\n\n----------------------------------------\n\nTITLE: Using displaCy Visualizer in Python\nDESCRIPTION: Shows how to use the displaCy visualizer to serve visualizations via a web server or generate HTML markup. It supports both dependency and named entity visualization styles.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy import displacy\ndoc = nlp(\"This is a sentence about Facebook.\")\ndisplacy.serve(doc, style=\"dep\") # run the web server\nhtml = displacy.render(doc, style=\"ent\") # generate HTML\n```\n\n----------------------------------------\n\nTITLE: Using require_cpu in spaCy\nDESCRIPTION: Example showing how to force CPU processing in spaCy by explicitly requiring CPU allocation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nspacy.require_cpu()\nnlp = spacy.load(\"en_core_web_sm\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Morphology Class in Python\nDESCRIPTION: Creates a new Morphology object by providing a StringStore. The Morphology class stores possible morphological analyses for a language and indexes them by hash.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.morphology import Morphology\n\nmorphology = Morphology(strings)\n```\n\n----------------------------------------\n\nTITLE: Defining Pipeline Components in spaCy Configuration (INI)\nDESCRIPTION: Configuration example showing how to define pipeline components in spaCy, including sourcing components from existing trained pipelines and creating new components from factories with custom settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_11\n\nLANGUAGE: ini\nCODE:\n```\n[components]\n\n# \"parser\" and \"ner\" are sourced from a trained pipeline\n[components.parser]\nsource = \"en_core_web_sm\"\n\n[components.ner]\nsource = \"en_core_web_sm\"\n\n# \"textcat\" and \"custom\" are created blank from a built-in / custom factory\n[components.textcat]\nfactory = \"textcat\"\n\n[components.custom]\nfactory = \"your_custom_factory\"\nyour_custom_setting = true\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for TrainablePipe in Python\nDESCRIPTION: Create an optimizer for the pipeline component. By default, it uses Adam optimizer with default settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\noptimizer = pipe.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Iterating over lexemes in spaCy's Vocab\nDESCRIPTION: Demonstrates how to iterate over all lexemes in the vocabulary, useful for operations like filtering stop words.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstop_words = (lex for lex in nlp.vocab if lex.is_stop)\n```\n\n----------------------------------------\n\nTITLE: Initializing DependencyMatcher in Python\nDESCRIPTION: Example showing how to create a DependencyMatcher instance with the vocabulary object that must be shared with the documents it will operate on.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencymatcher.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.matcher import DependencyMatcher\nmatcher = DependencyMatcher(nlp.vocab)\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Vector Keys in spaCy\nDESCRIPTION: Shows how to iterate through vector keys and print both the key and its corresponding string value.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfor key in nlp.vocab.vectors.keys():\n    print(key, nlp.vocab.strings[key])\n```\n\n----------------------------------------\n\nTITLE: Customizing displaCy Visualization Options\nDESCRIPTION: Example showing how to customize the displaCy visualization with options like compact mode, background color, text color, and font settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noptions = {\"compact\": True, \"bg\": \"#09a3d5\",\n           \"color\": \"white\", \"font\": \"Source Sans Pro\"}\ndisplacy.serve(doc, style=\"dep\", options=options)\n```\n\n----------------------------------------\n\nTITLE: Accessing spaCy Tagger Labels\nDESCRIPTION: Shows how to check for the presence of a label in the tagger component after adding it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntagger.add_label(\"MY_LABEL\")\nassert \"MY_LABEL\" in tagger.labels\n```\n\n----------------------------------------\n\nTITLE: Downloading and Installing spaCy Models\nDESCRIPTION: These commands demonstrate different ways to download and install spaCy models, including using the spacy download command and pip install from various sources.\nSOURCE: https://github.com/explosion/spacy/blob/master/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy download en_core_web_sm\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install /Users/you/en_core_web_sm-3.0.0.tar.gz\npip install /Users/you/en_core_web_sm-3.0.0-py3-none-any.whl\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Assembling spaCy Pipeline from Config\nDESCRIPTION: CLI command demonstrating how to assemble a pipeline from a config file without training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-1.mdx#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy assemble config.cfg ./output\n```\n\n----------------------------------------\n\nTITLE: Checking Factory Registration in spaCy\nDESCRIPTION: Example demonstrating how to check if a factory is registered on Language class or subclass using has_factory method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\nfrom spacy.lang.en import English\n\n@English.component(\"component\")\ndef component(doc):\n    return doc\n\nassert English.has_factory(\"component\")\nassert not Language.has_factory(\"component\")\n```\n\n----------------------------------------\n\nTITLE: Error: Model Package Import Failure - Python\nDESCRIPTION: Error occurring when a specific language model package is not installed in the current environment.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nImportError: No module named 'en_core_web_sm'\n```\n\n----------------------------------------\n\nTITLE: Setting Boolean Flags on Lexemes in spaCy\nDESCRIPTION: Example of how to add and set a custom boolean flag on a lexeme in spaCy's vocabulary. The example creates a flag called COOL_FLAG and sets it to True for the 'spaCy' lexeme.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lexeme.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nCOOL_FLAG = nlp.vocab.add_flag(lambda text: False)\nnlp.vocab[\"spaCy\"].set_flag(COOL_FLAG, True)\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy Entity Linker Component\nDESCRIPTION: Configuration example for the spacy.EntityLinker.v1 component showing paths, component initialization and candidate selector setup.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_19\n\nLANGUAGE: ini\nCODE:\n```\n[paths]\nel_nlp = null\n\n...\n\n[components.llm.task]\n@llm_tasks = \"spacy.EntityLinker.v1\"\n\n[initialize]\n[initialize.components]\n[initialize.components.llm]\n[initialize.components.llm.candidate_selector]\n@llm_misc = \"spacy.CandidateSelector.v1\"\n\n# Load a KB from a KB file. For loading KBs from spaCy pipelines see spacy.KBObjectLoader.v1.\n[initialize.components.llm.candidate_selector.kb_loader]\n@llm_misc = \"spacy.KBFileLoader.v1\"\n# Path to knowledge base .yaml file.\npath = ${paths.el_kb}\n```\n\n----------------------------------------\n\nTITLE: Downloading spaCy Pipeline Models via CLI\nDESCRIPTION: Command to download trained pipeline models for spaCy. It finds the best compatible version and uses pip to install the package. Allows for direct downloads and source distribution options.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy download [model] [--direct] [--sdist] [pip_args]\n```\n\n----------------------------------------\n\nTITLE: Defining LLM Model Function Signature in Python\nDESCRIPTION: Specifies the function signature for LLM models in spaCy, which take an iterable of prompts and return an iterable of responses. The function can have a general signature or a more specific one for string inputs and outputs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nCallable[[Iterable[Any]], Iterable[Any]]\n```\n\nLANGUAGE: python\nCODE:\n```\nCallable[[Iterable[str]], Iterable[str]]\n```\n\n----------------------------------------\n\nTITLE: Configuring Strided Spans in INI\nDESCRIPTION: Configuration example for using strided_spans.v1 span getter with window and stride parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_20\n\nLANGUAGE: ini\nCODE:\n```\n[transformer.model.get_spans]\n@span_getters = \"spacy-transformers.strided_spans.v1\"\nwindow = 128\nstride = 96\n```\n\n----------------------------------------\n\nTITLE: Working with Vector Shapes\nDESCRIPTION: Demonstrates creating vectors with specific shapes and accessing dimension information.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nvectors = Vectors(shape(1, 300))\nvectors.add(\"cat\", numpy.random.uniform(-1, 1, (300,)))\nrows, dims = vectors.shape\nassert rows == 1\nassert dims == 300\n```\n\n----------------------------------------\n\nTITLE: Using Hash Values for String Storage\nDESCRIPTION: Example showing how strings are stored and accessed using hash values instead of integer IDs in the vocabulary system.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I love coffee\")\nassert doc.vocab.strings[\"coffee\"] == 3197928453018144401\nassert doc.vocab.strings[3197928453018144401] == \"coffee\"\n\nbeer_hash = doc.vocab.strings.add(\"beer\")\nassert doc.vocab.strings[\"beer\"] == beer_hash\nassert doc.vocab.strings[beer_hash] == \"beer\"\n```\n\n----------------------------------------\n\nTITLE: Updating spaCy Version Requirements in meta.json\nDESCRIPTION: Shows how to update the spaCy version requirements in meta.json to make a pipeline compatible with spaCy v3.4.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-4.mdx#2025-04-21_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- \"spacy_version\": \">=3.3.0,<3.4.0\",\n+ \"spacy_version\": \">=3.3.0,<3.5.0\",\n```\n\n----------------------------------------\n\nTITLE: Pipeline Package __init__.py for Custom Functions\nDESCRIPTION: Example of how the __init__.py file in a pipeline package imports custom functions to ensure they're properly registered when the pipeline is loaded.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom . import functions\n\ndef load(**overrides):\n   ...\n```\n\n----------------------------------------\n\nTITLE: Loading SpanFinder from Disk\nDESCRIPTION: Demonstrates loading a SpanFinder component from disk storage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nspan_finder = nlp.add_pipe(\"span_finder\")\nspan_finder.from_disk(\"/path/to/span_finder\")\n```\n\n----------------------------------------\n\nTITLE: Removing Patterns from EntityRuler\nDESCRIPTION: Demonstrates how to remove a specific pattern from the EntityRuler using its ID.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npatterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"}]\nruler = nlp.add_pipe(\"entity_ruler\")\nruler.add_patterns(patterns)\nruler.remove(\"apple\")\n```\n\n----------------------------------------\n\nTITLE: Loading DocBin from Disk\nDESCRIPTION: Shows how to load a DocBin from a saved file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/docbin.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndoc_bin = DocBin().from_disk(\"./data.spacy\")\n```\n\n----------------------------------------\n\nTITLE: Updating Doc.from_docs Usage with Tensor Exclusion\nDESCRIPTION: Shows how to exclude tensor data when merging docs to reduce memory usage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-3.mdx#2025-04-21_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n-merged_doc = Doc.from_docs(docs)\n+merged_doc = Doc.from_docs(docs, exclude=[\"tensor\"])\n```\n\n----------------------------------------\n\nTITLE: Checking Document Annotations in Python\nDESCRIPTION: The Doc flags like Doc.is_parsed or Doc.is_tagged have been replaced by Doc.has_annotation in spaCy v3.0. This change affects how you check for the presence of specific annotations on a Doc object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndoc.has_annotation(\"TAG\")\n```\n\n----------------------------------------\n\nTITLE: Working with Private Assets in spaCy project.yml\nDESCRIPTION: Configuration for handling private assets that aren't publicly available. Only destination paths and checksums are specified, allowing teammates to manually place files in the proper locations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nassets:\n  - dest: 'assets/private_training_data.json'\n    checksum: '63373dd656daa1fd3043ce166a59474c'\n  - dest: 'assets/private_vectors.bin'\n    checksum: '5113dc04e03f079525edd8df3f4f39e3'\n```\n\n----------------------------------------\n\nTITLE: Updating Entry Points for Factory Component\nDESCRIPTION: Shows how to update the entry point in setup.py to use a factory class instead of a simple component function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_13\n\nLANGUAGE: diff\nCODE:\n```\nentry_points={\n-   \"spacy_factories\": [\"snek = snek:snek_component\"]\n+   \"spacy_factories\": [\"snek = snek:SnekFactory\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Coreference Resolver from Bytes in Python with spaCy\nDESCRIPTION: This code shows how to load a previously serialized coreference resolver from a bytestring, modifying the object in place and returning it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncoref_bytes = coref.to_bytes()\ncoref = nlp.add_pipe(\"experimental_coref\")\ncoref.from_bytes(coref_bytes)\n```\n\n----------------------------------------\n\nTITLE: Serializing Lemmatizer to Bytes\nDESCRIPTION: Example showing how to serialize a lemmatizer pipeline component to a bytestring using the to_bytes() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lemmatizer.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"lemmatizer\")\nlemmatizer_bytes = lemmatizer.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Using Variable Interpolation in Strings with spaCy Config (INI)\nDESCRIPTION: Shows how to use variables inside strings in spaCy configuration files, similar to Python f-strings. Non-string values are automatically converted to strings when interpolated.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_9\n\nLANGUAGE: ini\nCODE:\n```\n[paths]\nversion = 5\nroot = \"/Users/you/data\"\ntrain = \"${paths.root}/train_${paths.version}.spacy\"\n# Result: /Users/you/data/train_5.spacy\n```\n\n----------------------------------------\n\nTITLE: Configuring PretrainVectors in spaCy\nDESCRIPTION: Configuration example for the PretrainVectors pretraining objective which predicts word vectors from a static embeddings table. The example shows how to set up pretraining for a tok2vec component with cosine loss.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_20\n\nLANGUAGE: ini\nCODE:\n```\n[pretraining]\ncomponent = \"tok2vec\"\n\n[initialize]\nvectors = \"en_core_web_lg\"\n...\n\n[pretraining.objective]\n@architectures = \"spacy.PretrainVectors.v1\"\nmaxout_pieces = 3\nhidden_size = 300\nloss = \"cosine\"\n```\n\n----------------------------------------\n\nTITLE: Configuring PretrainVectors Objective in spaCy\nDESCRIPTION: Configuration for the Vectors pretraining objective in spaCy that trains the model to predict word vectors from a static embeddings table. This objective defines maxout pieces, hidden size, and sets cosine as the loss function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_23\n\nLANGUAGE: ini\nCODE:\n```\n### Vectors objective\n[pretraining.objective]\n@architectures = \"spacy.PretrainVectors.v1\"\nmaxout_pieces = 3\nhidden_size = 300\nloss = \"cosine\"\n```\n\n----------------------------------------\n\nTITLE: Initializing BaseVectors in Python\nDESCRIPTION: Constructor method for creating a new BaseVectors instance. It takes an optional StringStore as a parameter.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef __init__(self, *, strings: Optional[StringStore] = None):\n```\n\n----------------------------------------\n\nTITLE: Using Quantifiers in spaCy Token Patterns\nDESCRIPTION: Demonstrates the use of quantifiers (OP) in token patterns to specify optional tokens or sequences of tokens in spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npattern = [{\"LOWER\": \"hello\"},\n           {\"IS_PUNCT\": True, \"OP\": \"?\"}]\n```\n\n----------------------------------------\n\nTITLE: Pointer Access from Different Data Structures in Cython\nDESCRIPTION: Demonstrates how to obtain pointers from numpy arrays, C++ vectors, and memory views in Cython. Shows proper type declarations and memory layout considerations for C-contiguous data.\nSOURCE: https://github.com/explosion/spacy/blob/master/CONTRIBUTING.md#2025-04-21_snippet_4\n\nLANGUAGE: cython\nCODE:\n```\ncdef void get_pointers(np.ndarray[int, mode='c'] numpy_array, vector[int] cpp_vector, int[::1] memory_view) nogil:\n    pointer1 = <int*>numpy_array.data\n    pointer2 = cpp_vector.data()\n    pointer3 = &memory_view[0]\n```\n\n----------------------------------------\n\nTITLE: Rehearsing TextCategorizer Model in spaCy\nDESCRIPTION: Shows how to perform a rehearsal update to address catastrophic forgetting in the model. This is an experimental feature.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\noptimizer = nlp.resume_training()\nlosses = textcat.rehearse(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Configuring Word-Based Batching Strategy in spaCy using INI Format\nDESCRIPTION: Example configuration for batch_by_words batcher that creates minibatches with a target number of words. This batching strategy handles oversized examples based on configuration parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_28\n\nLANGUAGE: ini\nCODE:\n```\n[training.batcher]\n@batchers = \"spacy.batch_by_words.v1\"\nsize = 100\ntolerance = 0.2\ndiscard_oversize = false\nget_length = null\n```\n\n----------------------------------------\n\nTITLE: Configuring NLP Object and Pipeline in spaCy\nDESCRIPTION: This code block demonstrates how to configure the nlp object, including language, pipeline components, and tokenizer settings in a spaCy training configuration file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\nlang = \"en\"\npipeline = [\"tagger\", \"parser\", \"ner\"]\nbefore_creation = null\nafter_creation = null\nafter_pipeline_creation = null\nbatch_size = 1000\n\n[nlp.tokenizer]\n@tokenizers = \"spacy.Tokenizer.v1\"\n```\n\n----------------------------------------\n\nTITLE: Configuring JsonlCorpus Reader in spaCy using INI Format\nDESCRIPTION: Example configuration for the JsonlCorpus reader that creates Example objects from a JSONL file. This is commonly used for reading raw text corpus for language model pretraining.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_27\n\nLANGUAGE: ini\nCODE:\n```\n[paths]\npretrain = \"corpus/raw_text.jsonl\"\n\n[corpora.pretrain]\n@readers = \"spacy.JsonlCorpus.v1\"\npath = ${paths.pretrain}\nmin_length = 0\nmax_length = 0\nlimit = 0\n```\n\n----------------------------------------\n\nTITLE: Deserializing SpanGroup from Bytes in Python using spaCy\nDESCRIPTION: This snippet shows how to deserialize a SpanGroup object from a bytestring using the from_bytes() method. It creates a document, serializes a span group, creates a new SpanGroup object, and then loads the serialized data into it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spangroup.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import SpanGroup\n\ndoc = nlp(\"Their goi ng home\")\ndoc.spans[\"errors\"] = [doc[0:1], doc[1:3]]\ngroup_bytes = doc.spans[\"errors\"].to_bytes()\nnew_group = SpanGroup()\nnew_group.from_bytes(group_bytes)\n```\n\n----------------------------------------\n\nTITLE: Setting Values in a Table\nDESCRIPTION: Demonstrates how to set key-value pairs in a Table using the set method. String keys will be hashed automatically.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lookups import Table\ntable = Table()\ntable.set(\"foo\", \"bar\")\nassert table[\"foo\"] == \"bar\"\n```\n\n----------------------------------------\n\nTITLE: Updating Language Class Imports in spaCy v2.0\nDESCRIPTION: Shows how to update import statements for Language classes from the old format to the new spacy.lang.xx structure introduced in spaCy v2.0.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_11\n\nLANGUAGE: diff\nCODE:\n```\n- from spacy.en import English\n\n+ from spacy.lang.en import English\n```\n\n----------------------------------------\n\nTITLE: Performing Rehearsal Update in TrainablePipe in Python\nDESCRIPTION: Perform a 'rehearsal' update to address catastrophic forgetting. This experimental method takes examples, an optimizer, and a losses dictionary.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\noptimizer = nlp.resume_training()\nlosses = pipe.rehearse(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Accessing TokenC struct in spaCy\nDESCRIPTION: Example showing how to access a TokenC struct from a Doc object in spaCy. The TokenC struct can be accessed via the 'c' attribute of a Doc object, with indexing to get a specific token.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-structs.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntoken = &doc.c[3]\ntoken_ptr = &doc.c[3]\n```\n\n----------------------------------------\n\nTITLE: Checking for Vector Representation in spaCy Doc Object (Python)\nDESCRIPTION: Demonstrates how to check if a document has an associated word vector using the has_vector property.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like apples\")\nassert doc.has_vector\n```\n\n----------------------------------------\n\nTITLE: Adding Parser Labels\nDESCRIPTION: Example of manually adding a new label to the parser component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\nparser.add_label(\"MY_LABEL\")\n```\n\n----------------------------------------\n\nTITLE: Working with Span Conjuncts in spaCy\nDESCRIPTION: Shows how to access coordinated tokens (conjuncts) of a span's root token.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like apples and oranges\")\napples_conjuncts = doc[2:3].conjuncts\nassert [t.text for t in apples_conjuncts] == [\"oranges\"]\n```\n\n----------------------------------------\n\nTITLE: Loading EntityRecognizer from Disk\nDESCRIPTION: Shows how to load a previously saved named entity recognition model from disk using from_disk(). Returns the modified EntityRecognizer object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\nner.from_disk(\"/path/to/ner\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Vector Batch in Python\nDESCRIPTION: Method to efficiently get vectors for multiple keys as a batch. Required for use with StaticVectors in training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef get_batch(self, keys: Iterable[Union[int, str]]):\n```\n\n----------------------------------------\n\nTITLE: Defining Hugging Face Hub Upload Command in project.yml\nDESCRIPTION: This YAML snippet shows how to define a command in the project.yml file for uploading a trained and packaged spaCy pipeline to the Hugging Face Hub. It includes the command script, dependencies, and variables.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_41\n\nLANGUAGE: yaml\nCODE:\n```\n- name: \"push_to_hub\"\n  help: \"Upload the trained model to the Hugging Face Hub\"\n  script:\n    - \"python -m spacy huggingface-hub push packages/en_${vars.name}-${vars.version}/dist/en_${vars.name}-${vars.version}-py3-none-any.whl\"\n  deps:\n    - \"packages/en_${vars.name}-${vars.version}/dist/en_${vars.name}-${vars.version}-py3-none-any.whl\"\n```\n\n----------------------------------------\n\nTITLE: Calculating Token Vector Norm in spaCy\nDESCRIPTION: Shows how to access and compare the L2 norms of token vectors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like apples and pasta\")\napples = doc[2]\npasta = doc[4]\napples.vector_norm  # 6.89589786529541\npasta.vector_norm  # 7.759851932525635\nassert apples.vector_norm != pasta.vector_norm\n```\n\n----------------------------------------\n\nTITLE: Applying Entity Expansion in spaCy Pipeline (Python)\nDESCRIPTION: This snippet demonstrates how to add the entity expansion component to the spaCy pipeline and use it to process text, expanding person entities to include titles.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.language import Language\nfrom spacy.tokens import Span\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n@Language.component(\"expand_person_entities\")\ndef expand_person_entities(doc):\n    new_ents = []\n    for ent in doc.ents:\n        if ent.label_ == \"PERSON\" and ent.start != 0:\n            prev_token = doc[ent.start - 1]\n            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n                new_ents.append(new_ent)\n            else:\n                new_ents.append(ent)\n        else:\n            new_ents.append(ent)\n    doc.ents = new_ents\n    return doc\n\n# Add the component after the named entity recognizer\nnlp.add_pipe(\"expand_person_entities\", after=\"ner\")\n\ndoc = nlp(\"Dr. Alex Smith chaired first board meeting of Acme Corp Inc.\")\nprint([(ent.text, ent.label_) for ent in doc.ents])\n```\n\n----------------------------------------\n\nTITLE: Custom Vector Model Configuration\nDESCRIPTION: Configuration example for using a custom embedding model in the tagger component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_13\n\nLANGUAGE: ini\nCODE:\n```\n[tagger.model.tok2vec.embed]\n@architectures = \"my_example.MyEmbedding.v1\"\noutput_width = 128\n```\n\n----------------------------------------\n\nTITLE: Updating SpanFinder Model\nDESCRIPTION: Demonstrates model training using examples and an optimizer. Updates the model based on predictions and gold-standard annotations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nspan_finder = nlp.add_pipe(\"span_finder\")\noptimizer = nlp.initialize()\nlosses = span_finder.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: SpanRuler with Entity Recognition\nDESCRIPTION: Shows how to configure SpanRuler to work with entity recognition and customize entity annotation behavior.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n# only annotate doc.ents, not doc.spans\nconfig = {\"spans_key\": None, \"annotate_ents\": True, \"overwrite\": False}\nruler = nlp.add_pipe(\"span_ruler\", config=config)\npatterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\nruler.add_patterns(patterns)\n\ndoc = nlp(\"MyCorp Inc. is a company in the U.S.\")\nprint([(ent.text, ent.label_) for ent in doc.ents])\n```\n\n----------------------------------------\n\nTITLE: Adding and Checking Labels in DependencyParser in Python\nDESCRIPTION: This code demonstrates how to add a label to a DependencyParser component and check if it exists in the labels property. It adds a custom label and then verifies its presence.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nparser.add_label(\"MY_LABEL\")\nassert \"MY_LABEL\" in parser.labels\n```\n\n----------------------------------------\n\nTITLE: Loading SpanCategorizer from Disk\nDESCRIPTION: Example showing how to load a SpanCategorizer component from disk using the from_disk() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\nspancat.from_disk(\"/path/to/spancat\")\n```\n\n----------------------------------------\n\nTITLE: Registering Custom displaCy Colors via Entry Points\nDESCRIPTION: Adds custom entity visualization colors to setup.py using the spacy_displacy_colors entry point, allowing displaCy to automatically use these colors for the specified entity labels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_18\n\nLANGUAGE: diff\nCODE:\n```\nfrom setuptools import setup\n\nsetup(\n    name=\"snek\",\n    entry_points={\n+       \"spacy_displacy_colors\": [\"colors = snek:displacy_colors\"]\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Vector Data to Disk in Python\nDESCRIPTION: Method to save vector data with the pipeline to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndef to_disk(self, path: Union[str, Path]):\n```\n\n----------------------------------------\n\nTITLE: Retrieving String or Hash from StringStore in Python\nDESCRIPTION: Use the __getitem__ method to retrieve a string from a hash or vice versa.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/stringstore.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstringstore = StringStore([\"apple\", \"orange\"])\napple_hash = stringstore[\"apple\"]\nassert apple_hash == 8566208034543834098\nassert stringstore[apple_hash] == \"apple\"\n```\n\n----------------------------------------\n\nTITLE: Using Variable Interpolation in spaCy Configuration Files (INI)\nDESCRIPTION: Demonstrates how to use variable interpolation in spaCy config files to reference values across different sections using the ${section.value} syntax. The example shows reusing the seed value and reusing entire blocks like the optimizer configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_8\n\nLANGUAGE: ini\nCODE:\n```\n[system]\nseed = 0\n\n[training]\nseed = ${system.seed}\n\n[training.optimizer]\n@optimizers = \"Adam.v1\"\nbeta1 = 0.9\nbeta2 = 0.999\nL2_is_weight_decay = true\nL2 = 0.01\ngrad_clip = 1.0\nuse_averages = false\neps = 1e-8\n\n[pretraining]\noptimizer = ${training.optimizer}\n```\n\n----------------------------------------\n\nTITLE: Example of the spaCy assemble Command\nDESCRIPTION: Command for assembling a spaCy pipeline from a configuration file without additional training. The command takes a config file path and an output directory.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_39\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy assemble config.cfg ./output\n```\n\n----------------------------------------\n\nTITLE: Configuring copy_from_base_model Callback in spaCy\nDESCRIPTION: Configuration for copy_from_base_model callback that copies the tokenizer and/or vocabulary from specified models. Useful for fine-tuning an existing pipeline when used in combination with sourced components.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_33\n\nLANGUAGE: ini\nCODE:\n```\n[initialize.before_init]\n@callbacks = \"spacy.copy_from_base_model.v1\"\ntokenizer = \"en_core_sci_md\"\nvocab = \"en_core_sci_md\"\n```\n\n----------------------------------------\n\nTITLE: Setting Multiple Entities\nDESCRIPTION: Shows how to define multiple entities at once with their frequencies and vectors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/inmemorylookupkb.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nkb.set_entities(entity_list=[\"Q42\", \"Q463035\"], freq_list=[32, 111], vector_list=[vector1, vector2])\n```\n\n----------------------------------------\n\nTITLE: Loading SpanFinder from Bytes\nDESCRIPTION: Demonstrates loading a SpanFinder component from a bytestring representation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nspan_finder_bytes = span_finder.to_bytes()\nspan_finder = nlp.add_pipe(\"span_finder\")\nspan_finder.from_bytes(span_finder_bytes)\n```\n\n----------------------------------------\n\nTITLE: Parametrizing tests with a single argument in Python\nDESCRIPTION: Shows how to use pytest's parametrize decorator to run the same test function with different input examples, maintaining separation between test cases and test logic.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize(\"words\", [[\"hello\", \"world\"], [\"this\", \"is\", \"a\", \"test\"]])\ndef test_doc_length(words):\n    doc = Doc(Vocab(), words=words)\n    assert len(doc) == len(words)\n```\n\n----------------------------------------\n\nTITLE: Loading TextCategorizer from Bytes in Python\nDESCRIPTION: This code shows how to load a TextCategorizer from a bytestring. It modifies the object in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntextcat_bytes = textcat.to_bytes()\ntextcat = nlp.add_pipe(\"textcat\")\ntextcat.from_bytes(textcat_bytes)\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy with Lookups Data\nDESCRIPTION: Command to install spaCy with the lookups option for languages that support lemmatization rules.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U %%SPACY_PKG_NAME[lookups]%%SPACY_PKG_FLAGS\n```\n\n----------------------------------------\n\nTITLE: Specifying SpaCy Pipeline Dependencies\nDESCRIPTION: Example requirements.txt configuration showing how to specify SpaCy and model dependencies using direct URLs for pipeline packages.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_18\n\nLANGUAGE: text\nCODE:\n```\nspacy>=3.0.0,<4.0.0\nen_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Accessing Token Ancestors in spaCy\nDESCRIPTION: Demonstrates how to access a token's syntactic ancestors in the dependency parse tree using the ancestors property.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\nit_ancestors = doc[1].ancestors\nassert [t.text for t in it_ancestors] == [\"Give\"]\nhe_ancestors = doc[4].ancestors\nassert [t.text for t in he_ancestors] == [\"pleaded\"]\n```\n\n----------------------------------------\n\nTITLE: Loading Vector Data from Disk in Python\nDESCRIPTION: Method to load vector data from a saved pipeline on disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef from_disk(self, path: Union[str, Path]) -> BaseVectors:\n```\n\n----------------------------------------\n\nTITLE: Registering Pipeline Component\nDESCRIPTION: Basic registration of the relation extractor component using the Language.factory decorator.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\n@Language.factory(\"relation_extractor\")\ndef make_relation_extractor(nlp, name, model):\n    return RelationExtractor(nlp.vocab, model, name)\n```\n\n----------------------------------------\n\nTITLE: Cloning spaCy Project Template\nDESCRIPTION: Command to clone a spaCy project template for training a tagger and parser using the CLI.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy project clone pipelines/tagger_parser_ud\n```\n\n----------------------------------------\n\nTITLE: Updating Morphologizer Model\nDESCRIPTION: Shows how to update the morphologizer model using training examples and an optimizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\noptimizer = nlp.initialize()\nlosses = morphologizer.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Finding Optimal Threshold for spaCy Component (Bash)\nDESCRIPTION: Illustrates the use of the new 'find-threshold' CLI command to determine the best threshold for a spaCy component based on a specified metric.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-5.mdx#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ spacy find-threshold my_pipeline data.spacy spancat threshold spans_sc_f --n_trials 20\n```\n\n----------------------------------------\n\nTITLE: Combining Wrapped PyTorch Model with spaCy Components in Python\nDESCRIPTION: Shows how to combine a wrapped PyTorch model with spaCy's CharacterEmbed using Thinc's chain combinator.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom thinc.api import chain, with_array, PyTorchWrapper\nfrom spacy.ml import CharacterEmbed\n\nwrapped_pt_model = PyTorchWrapper(torch_model)\nchar_embed = CharacterEmbed(width, embed_size, nM, nC)\nmodel = chain(char_embed, with_array(wrapped_pt_model))\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCat Component\nDESCRIPTION: Configuration example for the TextCat.v1 component showing basic setup with labels for compliment and insult classification.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_38\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.TextCat.v1\"\nlabels = COMPLIMENT,INSULT\nexamples = null\n```\n\n----------------------------------------\n\nTITLE: Retrieving Morphological Analysis in Python\nDESCRIPTION: Gets the FEATS string representation for a given hash of morphological analysis from the Morphology object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfeats = \"Feat1=Val1|Feat2=Val2\"\nhash = nlp.vocab.morphology.add(feats)\nassert nlp.vocab.morphology.get(hash) == feats\n```\n\n----------------------------------------\n\nTITLE: Serializing SentenceRecognizer to Bytes\nDESCRIPTION: Converts the SentenceRecognizer component to a bytestring representation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\nsenter_bytes = senter.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Checking Vector Table Fullness\nDESCRIPTION: Demonstrates how to check if a vectors table is full and no slots are available.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nvectors = Vectors(shape=(1, 300))\nvectors.add(\"cat\", numpy.random.uniform(-1, 1, (300,)))\nassert vectors.is_full\n```\n\n----------------------------------------\n\nTITLE: Migrating from Doc and GoldParse to Example\nDESCRIPTION: Comparison showing how to migrate from the v2 GoldParse API to the v3 Example-based API for entity annotations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_26\n\nLANGUAGE: diff\nCODE:\n```\ndoc = nlp.make_doc(\"Mark Zuckerberg is the CEO of Facebook\")\nentities = [(0, 15, \"PERSON\"), (30, 38, \"ORG\")]\n- gold = GoldParse(doc, entities=entities)\n+ example = Example.from_dict(doc, {\"entities\": entities})\n```\n\n----------------------------------------\n\nTITLE: Character Embedding Configuration\nDESCRIPTION: Configuration example showing how to swap the embedding layer to use character embeddings instead of hash embeddings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\n[components.tok2vec.model.embed]\n@architectures = \"spacy.CharacterEmbed.v2\"\n\n[components.tok2vec.model.encode]\n@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Call Method\nDESCRIPTION: Implementation of the __call__ method that processes a single document by getting predictions and setting annotations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef __call__(self, doc: Doc):\n    predictions = self.predict([doc])\n    self.set_annotations([doc], predictions)\n    return doc\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variables in spaCy Project Commands\nDESCRIPTION: This YAML snippet demonstrates how to use a defined environment variable within a spaCy project command. It shows an example of echoing the custom ENV_PATH variable in a project script.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n- name: 'echo-path'\n  script:\n    - 'echo ${env.ENV_PATH}'\n```\n\n----------------------------------------\n\nTITLE: Checking Resizability of TrainablePipe in Python\nDESCRIPTION: Check if the output dimension of the component's model can be resized. Returns a boolean value.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ncan_resize = pipe.is_resizable\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields in Vocab Serialization (Python)\nDESCRIPTION: Shows how to exclude specific fields during serialization and deserialization of the Vocab object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndata = vocab.to_bytes(exclude=[\"strings\", \"vectors\"])\nvocab.from_disk(\"./vocab\", exclude=[\"strings\"])\n```\n\n----------------------------------------\n\nTITLE: Disabling the Parser Component in spaCy Pipeline in Python\nDESCRIPTION: This snippet shows how to disable the parser component when loading a spaCy model to improve processing speed when syntactic information is not needed.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n```\n\n----------------------------------------\n\nTITLE: Deserializing Lookups from Bytes\nDESCRIPTION: Shows how to load a lookups object from a serialized bytestring.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlookup_bytes = lookups.to_bytes()\nlookups = Lookups()\nlookups.from_bytes(lookup_bytes)\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields During DependencyParser Serialization in Python\nDESCRIPTION: This code demonstrates how to exclude specific fields when serializing a DependencyParser to disk. It excludes the 'vocab' field from serialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndata = parser.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanResolver Architecture in spaCy\nDESCRIPTION: Example configuration for the spaCy experimental SpanResolver model architecture. This model handles the resolution of span mentions in coreference clusters. It defines parameters for embedding sizes, CNN configuration, window size, and maximum span distance, using a transformer for token vector representation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_31\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy-experimental.SpanResolver.v1\"\nhidden_size = 1024\ndistance_embedding_size = 64\nconv_channels = 4\nwindow_size = 1\nmax_distance = 128\nprefix = \"coref_head_clusters\"\n\n[model.tok2vec]\n@architectures = \"spacy-transformers.TransformerListener.v1\"\ngrad_factor = 1.0\nupstream = \"transformer\"\npooling = {\"@layers\":\"reduce_mean.v1\"}\n```\n\n----------------------------------------\n\nTITLE: Hashing String in Python using spaCy\nDESCRIPTION: Use the hash_string utility function to get a 64-bit hash for a given string.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/stringstore.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.strings import hash_string\nassert hash_string(\"apple\") == 8566208034543834098\n```\n\n----------------------------------------\n\nTITLE: Serializing Coreference Resolver to Bytes in Python with spaCy\nDESCRIPTION: This snippet demonstrates how to serialize the coreference resolver pipeline component to a bytestring, including the KnowledgeBase.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncoref = nlp.add_pipe(\"experimental_coref\")\ncoref_bytes = coref.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Adding Titles to Entity Visualizations in spaCy\nDESCRIPTION: This snippet demonstrates how to add a title to an entity visualization by storing it in the document's user_data dictionary. This is useful when displaying multiple documents to distinguish between different visualizations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence about Google.\")\ndoc.user_data[\"title\"] = \"This is a title\"\ndisplacy.serve(doc, style=\"ent\")\n```\n\n----------------------------------------\n\nTITLE: Loading SentenceRecognizer from Disk\nDESCRIPTION: Loads a saved SentenceRecognizer component from disk storage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\nsenter.from_disk(\"/path/to/senter\")\n```\n\n----------------------------------------\n\nTITLE: Running flake8 for Linting Python Code\nDESCRIPTION: Command to run flake8 for linting the spaCy codebase. This tool helps enforce code style and identify potential errors or inconsistencies in the code.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nflake8 spacy\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields in TextCategorizer Serialization in Python\nDESCRIPTION: This snippet demonstrates how to exclude specific fields during the serialization of the TextCategorizer. It shows how to exclude the 'vocab' field when saving the component to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndata = textcat.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parameters with SpanCategorizer in Python\nDESCRIPTION: Demonstration of modifying the SpanCategorizer's model to use given parameter values, typically used for saving the best model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\nwith spancat.use_params(optimizer.averages):\n    spancat.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Using spaCy debug diff-config Command\nDESCRIPTION: This command shows differences between a config file and spaCy's defaults or another config file. It's useful for debugging and sharing configurations with maintainers on the discussion forum.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug diff-config [config_path] [--compare-to] [--optimize] [--gpu] [--pretraining] [--markdown]\n```\n\n----------------------------------------\n\nTITLE: Parametrizing Tests in Python with pytest\nDESCRIPTION: A pytest example showing how to use the parametrize decorator to run the same test with different input values. This allows testing multiple cases without duplicating code.\nSOURCE: https://github.com/explosion/spacy/blob/master/spacy/tests/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize('text', [\"google.com\", \"spacy.io\"])\ndef test_tokenizer_keep_urls(tokenizer, text):\n    tokens = tokenizer(text)\n    assert len(tokens) == 1\n```\n\n----------------------------------------\n\nTITLE: Getting Token Extensions - Python\nDESCRIPTION: Shows how to retrieve information about registered token extensions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Token\nToken.set_extension(\"is_fruit\", default=False)\nextension = Token.get_extension(\"is_fruit\")\nassert extension == (False, None, None, None)\n```\n\n----------------------------------------\n\nTITLE: Pruning vectors in spaCy's Vocab\nDESCRIPTION: Demonstrates how to reduce the size of the vector table while maintaining mappings for all words. This is useful for optimizing memory usage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnlp.vocab.prune_vectors(10000)\nassert len(nlp.vocab.vectors) <= 10000\n```\n\n----------------------------------------\n\nTITLE: Checking Pattern Existence in PhraseMatcher\nDESCRIPTION: Demonstrates how to check if a specific pattern ID exists in the matcher using the in operator.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/phrasematcher.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmatcher = PhraseMatcher(nlp.vocab)\nassert \"OBAMA\" not in matcher\nmatcher.add(\"OBAMA\", [nlp(\"Barack Obama\")])\nassert \"OBAMA\" in matcher\n```\n\n----------------------------------------\n\nTITLE: Loading and Saving Probability Lookups in SpaCy\nDESCRIPTION: Demonstrates how to load and save probability tables in a SpaCy English model. The probability table is loaded lazily when first accessed and saved with the model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-3.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.en import English\nnlp = English()\ndoc = nlp(\"the\")\nprint(doc[0].prob) # lazily loads extra prob table\nnlp.to_disk(\"/path/to/model\") # includes prob table\n```\n\n----------------------------------------\n\nTITLE: Warning Implementation Pattern\nDESCRIPTION: Demonstrates how to efficiently implement warnings by collecting issues before raising a single warning.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nn_empty = 0\nfor spans in lots_of_annotations:\n    if len(spans) == 0:\n        n_empty += 1\nwarnings.warn(Warnings.456.format(count=n_empty))\n```\n\n----------------------------------------\n\nTITLE: Serializing Coreference Resolver to Disk in Python with spaCy\nDESCRIPTION: This snippet demonstrates how to serialize the coreference resolver pipeline component to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncoref = nlp.add_pipe(\"experimental_coref\")\ncoref.to_disk(\"/path/to/coref\")\n```\n\n----------------------------------------\n\nTITLE: Registering a Function for Acronym Dictionary in Python\nDESCRIPTION: This snippet demonstrates how to register a function that returns a custom dictionary using the @spacy.registry.misc decorator.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n@spacy.registry.misc(\"acronyms.slang_dict.v1\")\ndef create_acronyms_slang_dict():\n    dictionary = {\"lol\": \"laughing out loud\", \"brb\": \"be right back\"}\n    dictionary.update({value: key for key, value in dictionary.items()})\n    return dictionary\n```\n\n----------------------------------------\n\nTITLE: Getting Span Length\nDESCRIPTION: Demonstrates how to get the number of tokens in a Span using len().\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\nspan = doc[1:4]\nassert len(span) == 3\n```\n\n----------------------------------------\n\nTITLE: Defining Environment Variables in spaCy Project YAML\nDESCRIPTION: This YAML snippet shows how to define environment variables in a spaCy project configuration file. It demonstrates making the system PATH available as a custom environment variable for use in project commands.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  ENV_PATH: PATH\n```\n\n----------------------------------------\n\nTITLE: Changing Embedding Matrix Ops in Python\nDESCRIPTION: Method to change the embedding matrix to use different Thinc ops.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef to_ops(self, ops: Ops):\n```\n\n----------------------------------------\n\nTITLE: Converting JSON data to spaCy format using CLI\nDESCRIPTION: Command to convert JSON training data to the new .spacy format using the spacy convert CLI tool.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy convert ./data.json .\n```\n\n----------------------------------------\n\nTITLE: Reconstructing Tokens with Spaces in spaCy\nDESCRIPTION: Demonstrates how to reconstruct original tokens with proper spacing information from a list of words and original text.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_57\n\nLANGUAGE: python\nCODE:\n```\norig_words = [\"Hey\", \",\", \"what\", \"'s\", \"up\", \"?\"]\norig_text = \"Hey, what's up?\"\nwords, spaces = get_words_and_spaces(orig_words, orig_text)\n# ['Hey', ',', 'what', \"'s\", 'up', '?']\n# [False, True, False, True, False, False]\n```\n\n----------------------------------------\n\nTITLE: Creating Reference Doc with Gold-Standard TAG Annotations\nDESCRIPTION: Creates a reference Doc object with gold-standard part-of-speech tags by adding tags to the vocabulary and converting them to an array.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n# create the reference Doc with gold-standard TAG annotations\ntags = [\"NOUN\", \"VERB\", \"NOUN\"]\ntag_ids = [vocab.strings.add(tag) for tag in tags]\nreference = Doc(vocab, words=words).from_array(\"TAG\", numpy.array(tag_ids, dtype=\"uint64\"))\nexample = Example(predicted, reference)\n```\n\n----------------------------------------\n\nTITLE: Serializing EditTreeLemmatizer to Disk in Python\nDESCRIPTION: Demonstrates how to serialize the EditTreeLemmatizer pipe to disk using the to_disk method. This method saves the lemmatizer to a specified directory path.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\nlemmatizer.to_disk(\"/path/to/lemmatizer\")\n```\n\n----------------------------------------\n\nTITLE: Rehearsing spaCy Tagger Model\nDESCRIPTION: Perform rehearsal updates to prevent catastrophic forgetting during model training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\noptimizer = nlp.resume_training()\nlosses = tagger.rehearse(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Predicting with EditTreeLemmatizer\nDESCRIPTION: Example showing how to apply the lemmatizer model to documents without modifying them.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\ntree_ids = lemmatizer.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Enabling Pipeline Components in spaCy\nDESCRIPTION: Example demonstrating how to re-enable a previously disabled pipeline component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nnlp.disable_pipe(\"ner\")\nassert \"ner\" in nlp.disabled\nassert not \"ner\" in nlp.pipe_names\nnlp.enable_pipe(\"ner\")\nassert not \"ner\" in nlp.disabled\nassert \"ner\" in nlp.pipe_names\n```\n\n----------------------------------------\n\nTITLE: Configuring File Readers for spaCy Data Loading\nDESCRIPTION: Example configuration for using srsly's JSON reader to load data in a spaCy pipeline. This demonstrates how to configure a file reader for loading orthographic variants for data augmentation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_24\n\nLANGUAGE: ini\nCODE:\n```\n[corpora.train.augmenter.orth_variants]\n@readers = \"srsly.read_json.v1\"\npath = \"corpus/en_orth_variants.json\"\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Extension Attribute for Relations\nDESCRIPTION: Sets up a custom extension attribute on the Doc object to store relation data between entities. The attribute is a dictionary with entity offset tuples as keys and relation scores as values.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc\nDoc.set_extension(\"rel\", default={})\n```\n\n----------------------------------------\n\nTITLE: Creating Scalar Weight Model for Transformer Layers in Python\nDESCRIPTION: Constructs a model that accepts a list of transformer layer outputs and returns a weighted representation. It allows configuration of the number of layers, dropout probability, and precision settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nspacy-curated-transformers.ScalarWeight.v1\n```\n\n----------------------------------------\n\nTITLE: Excluding Serialization Fields\nDESCRIPTION: Shows how to exclude specific data fields during EntityRecognizer serialization using the exclude parameter.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndata = ner.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: spaCy Benchmark Accuracy Command Syntax\nDESCRIPTION: The syntax for spaCy's benchmark accuracy command (formerly 'evaluate') that evaluates trained pipeline performance. It supports various options for output formats, visualization, and component-specific evaluation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy benchmark accuracy [model] [data_path] [--output] [--code] [--gold-preproc] [--gpu-id] [--displacy-path] [--displacy-limit] [--per-component] [--spans-key]\n```\n\n----------------------------------------\n\nTITLE: Configuring NGram Range Suggester\nDESCRIPTION: Configuration example for the NGram range suggester that suggests spans within a size range.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_23\n\nLANGUAGE: ini\nCODE:\n```\n[components.spancat.suggester]\n@misc = \"spacy.ngram_range_suggester.v1\"\nmin_size = 2\nmax_size = 4\n```\n\n----------------------------------------\n\nTITLE: Saving KnowledgeBase to Disk in Python\nDESCRIPTION: Example of saving the current state of the knowledge base to a directory using the to_disk method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/kb.mdx#2025-04-21_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nkb.to_disk(path)\n```\n\n----------------------------------------\n\nTITLE: Serializing Table to Bytes\nDESCRIPTION: Shows how to serialize a Table object to a bytestring for storage or transmission.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntable_bytes = table.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Removing Tables from Lookups\nDESCRIPTION: Demonstrates how to remove a table from the lookups object by name. This raises an error if the table doesn't exist.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlookups = Lookups()\nlookups.add_table(\"some_table\")\nremoved_table = lookups.remove_table(\"some_table\")\nassert \"some_table\" not in lookups\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields During Serialization - Python\nDESCRIPTION: Example showing how to exclude specific data fields during serialization using the exclude parameter when saving to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributeruler.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndata = ruler.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Initializing SentenceRecognizer for Training in spaCy\nDESCRIPTION: Example of how to initialize the SentenceRecognizer component for training using a function that returns examples.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\nsenter.initialize(lambda: examples, nlp=nlp)\n```\n\n----------------------------------------\n\nTITLE: Appending a Token to a Doc using push_back in Cython\nDESCRIPTION: Example demonstrating how to use the Doc.push_back method to append a token to a Doc object. This example creates a new Doc, gets a lexeme from the vocabulary, and appends it with trailing whitespace.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-classes.mdx#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom spacy.tokens cimport Doc\nfrom spacy.vocab cimport Vocab\n\ndoc = Doc(Vocab())\nlexeme = doc.vocab.get(\"hello\")\ndoc.push_back(lexeme, True)\nassert doc.text == \"hello \"\n```\n\n----------------------------------------\n\nTITLE: Generating DVC Configuration with spaCy CLI\nDESCRIPTION: Command to generate Data Version Control config file. Requires DVC to be installed and initialized. Creates dvc.yaml based on project workflows.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_50\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--quiet]\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ git init\n$ dvc init\n$ python -m spacy project dvc all\n```\n\n----------------------------------------\n\nTITLE: Accessing Span's Sentence in spaCy\nDESCRIPTION: Shows how to access the sentence that contains a span.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\nspan = doc[1:3]\nassert span.sent.text == \"Give it back!\"\n```\n\n----------------------------------------\n\nTITLE: Resetting vectors in spaCy's Vocab\nDESCRIPTION: Shows how to reset the vector table in the vocabulary. This is necessary when changing the size of word vectors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnlp.vocab.reset_vectors(width=300)\n```\n\n----------------------------------------\n\nTITLE: Removing Patterns from PhraseMatcher\nDESCRIPTION: Demonstrates how to remove a pattern from the matcher using its ID. Throws KeyError if pattern doesn't exist.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/phrasematcher.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmatcher = PhraseMatcher(nlp.vocab)\nmatcher.add(\"OBAMA\", [nlp(\"Barack Obama\")])\nassert \"OBAMA\" in matcher\nmatcher.remove(\"OBAMA\")\nassert \"OBAMA\" not in matcher\n```\n\n----------------------------------------\n\nTITLE: Retrieving Values for a Feature in MorphAnalysis in Python\nDESCRIPTION: Demonstrates how to get values for a specific feature from a MorphAnalysis object using the get method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfeats = \"Feat1=Val1,Val2\"\nmorph = MorphAnalysis(nlp.vocab, feats)\nassert morph.get(\"Feat1\") == [\"Val1\", \"Val2\"]\n```\n\n----------------------------------------\n\nTITLE: Configuration for Streaming Training Data\nDESCRIPTION: Configuration setting to enable streaming of large or infinite corpora during training by setting max_epochs to -1.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-1.mdx#2025-04-21_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[training]\nmax_epochs = -1\n```\n\n----------------------------------------\n\nTITLE: Getting Vector Count in Python\nDESCRIPTION: Method to return the number of vectors in the table.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef __len__(self) -> int:\n```\n\n----------------------------------------\n\nTITLE: Setting Lexeme Attribute Values in spaCy\nDESCRIPTION: Demonstrates how to set attribute values on a LexemeC struct using the set_struct_attr static method. This example sets the NORM attribute to the value of the lower attribute.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-structs.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs cimport NORM\nfrom spacy.lexeme cimport Lexeme\n\nlexeme = doc.c[3].lex\nLexeme.set_struct_attr(lexeme, NORM, lexeme.lower)\n```\n\n----------------------------------------\n\nTITLE: Getting struct attribute values from TokenC in spaCy\nDESCRIPTION: Example demonstrating how to get an attribute value from a TokenC struct using the Token.get_struct_attr static method. This example retrieves the IS_ALPHA attribute from a token.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-structs.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs cimport IS_ALPHA\nfrom spacy.tokens cimport Token\n\nis_alpha = Token.get_struct_attr(&doc.c[3], IS_ALPHA)\n```\n\n----------------------------------------\n\nTITLE: Changing Vector Operations Backend\nDESCRIPTION: Demonstrates changing the embedding matrix to use different Thinc ops.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom thinc.api import NumpyOps\n\nvectors.to_ops(NumpyOps())\n\n```\n\n----------------------------------------\n\nTITLE: Getting Vector Size in Python\nDESCRIPTION: Property to get the total size of the vector table (rows * dimensions).\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef size(self) -> int:\n```\n\n----------------------------------------\n\nTITLE: Deserializing Tokenizer from Bytes\nDESCRIPTION: Loads a tokenizer from a bytestring representation, modifying the object in place.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tokenizer.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntokenizer_bytes = tokenizer.to_bytes()\ntokenizer = Tokenizer(nlp.vocab)\ntokenizer.from_bytes(tokenizer_bytes)\n```\n\n----------------------------------------\n\nTITLE: Training Text Classification Models via CLI\nDESCRIPTION: Command to train a text classification model using spaCy's CLI, demonstrating the usage of the textcat pipeline with a simple CNN architecture and multilabel classification.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-2.mdx#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy train en /output /train /dev \\\n--pipeline textcat --textcat-arch simple_cnn \\\n--textcat-multilabel\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variables in spaCy project.yml\nDESCRIPTION: YAML configuration that references environment variables for batch size and GPU ID, making them available to commands for flexible configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  batch_size: BATCH_SIZE\n  gpu_id: GPU_ID\n\ncommands:\n  - name: evaluate\n    script:\n      - 'python scripts/custom_evaluation.py ${env.batch_size}'\n```\n\n----------------------------------------\n\nTITLE: Serializing SpanFinder to Disk\nDESCRIPTION: Shows how to save the SpanFinder component to disk storage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nspan_finder = nlp.add_pipe(\"span_finder\")\nspan_finder.to_disk(\"/path/to/span_finder\")\n```\n\n----------------------------------------\n\nTITLE: Type-Annotated Model Chaining in Python\nDESCRIPTION: Example demonstrating how to chain models with explicit type annotations using Thinc's typing system, showing input/output type specifications for different layers.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom thinc.api import Model, chain\nfrom thinc.types import Floats2d\ndef chain_model(\n    tok2vec: Model[List[Doc], List[Floats2d]],\n    layer1: Model[List[Floats2d], Floats2d],\n    layer2: Model[Floats2d, Floats2d]\n) -> Model[List[Doc], Floats2d]:\n    model = chain(tok2vec, layer1, layer2)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Disabling Pipeline Components in spaCy\nDESCRIPTION: Example showing how to temporarily disable a pipeline component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nnlp.add_pipe(\"ner\")\nnlp.add_pipe(\"textcat\")\nassert nlp.pipe_names == [\"ner\", \"textcat\"]\nnlp.disable_pipe(\"ner\")\nassert nlp.pipe_names == [\"textcat\"]\nassert nlp.component_names == [\"ner\", \"textcat\"]\nassert nlp.disabled == [\"ner\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring models_with_nvtx_range Callback in spaCy\nDESCRIPTION: Configuration for models_with_nvtx_range callback that wraps model operations with NVTX range markers for GPU profiling. These markers help attribute specific operations to a model's forward or backprop passes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_34\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\nafter_pipeline_creation = {\"@callbacks\":\"spacy.models_with_nvtx_range.v1\"}\n```\n\n----------------------------------------\n\nTITLE: Switching to Lookup Lemmatization in spaCy\nDESCRIPTION: This snippet demonstrates how to replace a trainable or rule-based lemmatizer with a lookup lemmatizer in spaCy, using an English language model as an example.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/models/index.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_sm\")\nnlp.remove_pipe(\"lemmatizer\")\nnlp.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"}).initialize()\n```\n\n----------------------------------------\n\nTITLE: Optimizing Large Pattern Sets\nDESCRIPTION: Demonstrates how to optimize performance when adding large numbers of patterns by selectively enabling pipeline components.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"entity_ruler\")\npatterns = [{\"label\": \"TEST\", \"pattern\": str(i)} for i in range(100000)]\nwith nlp.select_pipes(enable=\"tagger\"):\n    ruler.add_patterns(patterns)\n```\n\n----------------------------------------\n\nTITLE: Saving SentenceRecognizer to Disk\nDESCRIPTION: Serializes the SentenceRecognizer component to disk storage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\nsenter.to_disk(\"/path/to/senter\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Preset Spans Suggester\nDESCRIPTION: Configuration example for the preset spans suggester that uses existing spans from doc.spans.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_24\n\nLANGUAGE: ini\nCODE:\n```\n[components.spancat.suggester]\n@misc = \"spacy.preset_spans_suggester.v1\"\nspans_key = \"my_spans\"\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parameters with EditTreeLemmatizer\nDESCRIPTION: Example demonstrating how to temporarily modify the model parameters using a context manager.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\nwith lemmatizer.use_params(optimizer.averages):\n    lemmatizer.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Serializing TextCategorizer to Disk in Python\nDESCRIPTION: This snippet demonstrates how to serialize the TextCategorizer to disk. It saves the component to a specified directory path.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\ntextcat.to_disk(\"/path/to/textcat\")\n```\n\n----------------------------------------\n\nTITLE: Serializing Lookups to Bytes\nDESCRIPTION: Demonstrates how to serialize the lookups object to a bytestring for storage or transmission.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlookup_bytes = lookups.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Checking Rule Existence in spaCy Matcher\nDESCRIPTION: Shows how to verify if a specific rule exists in the Matcher using the in operator.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/matcher.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmatcher = Matcher(nlp.vocab)\nassert \"Rule\" not in matcher\nmatcher.add(\"Rule\", [[{'ORTH': 'test'}]])\nassert \"Rule\" in matcher\n```\n\n----------------------------------------\n\nTITLE: Configuration for Probability Tables\nDESCRIPTION: Configuration snippet for specifying probability tables in spaCy's config.cfg when training a model from scratch.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_7\n\nLANGUAGE: ini\nCODE:\n```\n[initialize.lookups]\n@misc = \"spacy.LookupsDataLoader.v1\"\nlang = ${nlp.lang}\ntables = [\"lexeme_prob\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Entity Annotations in Cython for spaCy in Python\nDESCRIPTION: This Cython code example shows how to efficiently set entity annotations by writing directly to the underlying C structs of spaCy tokens, handling both entity type and IOB annotation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# cython: infer_types=True\nfrom spacy.typedefs cimport attr_t\nfrom spacy.tokens.doc cimport Doc\n\ncpdef set_entity(Doc doc, int start, int end, attr_t ent_type):\n    for i in range(start, end):\n        doc.c[i].ent_type = ent_type\n    doc.c[start].ent_iob = 3\n    for i in range(start+1, end):\n        doc.c[i].ent_iob = 2\n```\n\n----------------------------------------\n\nTITLE: Using Parameters with spaCy Tagger\nDESCRIPTION: Temporarily modify the tagger's model parameters within a context manager.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\nwith tagger.use_params(optimizer.averages):\n    tagger.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Example spaCy Training Command and Console Output\nDESCRIPTION: Demonstration of the console output from a spaCy training run, showing the progress of training with metrics like loss and accuracy displayed in a tabular format across steps and epochs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy train config.cfg\n```\n\nLANGUAGE: bash\nCODE:\n```\nℹ Using CPU\nℹ Loading config and nlp from: config.cfg\nℹ Pipeline: ['tok2vec', 'tagger']\nℹ Start training\nℹ Training. Initial learn rate: 0.0\nℹ Saving results to training_log.jsonl\n\nE     #        LOSS TOK2VEC   LOSS TAGGER   TAG_ACC   SCORE\n---   ------   ------------   -----------   -------   ------\n  0        0           0.00         86.20      0.22     0.00\n  0      200           3.08      18968.78     34.00     0.34\n  0      400          31.81      22539.06     33.64     0.34\n  0      600          92.13      22794.91     43.80     0.44\n  0      800         183.62      21541.39     56.05     0.56\n  0     1000         352.49      25461.82     65.15     0.65\n  0     1200         422.87      23708.82     71.84     0.72\n  0     1400         601.92      24994.79     76.57     0.77\n  0     1600         662.57      22268.02     80.20     0.80\n  0     1800        1101.50      28413.77     82.56     0.83\n  0     2000        1253.43      28736.36     85.00     0.85\n  0     2200        1411.02      28237.53     87.42     0.87\n  0     2400        1605.35      28439.95     88.70     0.89\n```\n\n----------------------------------------\n\nTITLE: Getting Neighboring Tokens - Python\nDESCRIPTION: Shows how to access neighboring tokens in a document.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\ngive_nbor = doc[0].nbor()\nassert give_nbor.text == \"it\"\n```\n\n----------------------------------------\n\nTITLE: Custom Batch Filtering Configuration\nDESCRIPTION: This configuration snippet shows how to specify a custom batching strategy in the training pipeline, pointing to a registered batcher function with a specific batch size.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_39\n\nLANGUAGE: ini\nCODE:\n```\n[training.batcher]\n@batchers = \"filtering_batch.v1\"\nsize = 150\n```\n\n----------------------------------------\n\nTITLE: Retrieving Error Handler for TrainablePipe Component in Python\nDESCRIPTION: Demonstrates how to retrieve the current error handler function for a TrainablePipe component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"ner\")\nerror_handler = pipe.get_error_handler()\n```\n\n----------------------------------------\n\nTITLE: Initializing MorphAnalysis Class in Python\nDESCRIPTION: Creates a new MorphAnalysis object to store a single morphological analysis. It can be initialized from a FEATS string or a dictionary of morphological features.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import MorphAnalysis\n\nfeats = \"Feat1=Val1|Feat2=Val2\"\nm = MorphAnalysis(nlp.vocab, feats)\n```\n\n----------------------------------------\n\nTITLE: Checking Number of Vector Keys\nDESCRIPTION: Shows how to check the number of keys in the vectors table.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nvectors = Vectors(shape=(10, 300))\nassert len(vectors) == 10\nassert vectors.n_keys == 0\n```\n\n----------------------------------------\n\nTITLE: Setting Vector Values in Python\nDESCRIPTION: Demonstration of setting a vector value for a given key in the vector store.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncat_id = nlp.vocab.strings[\"cat\"]\nvector = numpy.random.uniform(-1, 1, (300,))\nnlp.vocab.vectors[cat_id] = vector\n```\n\n----------------------------------------\n\nTITLE: Saving Lookups to Disk\nDESCRIPTION: Demonstrates how to save the lookups object to a directory as lookups.bin. The directory will be created if it doesn't exist.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlookups.to_disk(\"/path/to/lookups\")\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields During Serialization in Python\nDESCRIPTION: Shows how to exclude specific serialization fields when saving a CuratedTransformer instance to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndata = trf.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Setting struct attribute values in TokenC in spaCy\nDESCRIPTION: Example showing how to set an attribute value in a TokenC struct using the Token.set_struct_attr static method. This example sets the TAG attribute of a token to 0.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-structs.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs cimport TAG\nfrom spacy.tokens cimport Token\n\ntoken = &doc.c[3]\nToken.set_struct_attr(token, TAG, 0)\n```\n\n----------------------------------------\n\nTITLE: Configuring PlainTextCorpus in INI format\nDESCRIPTION: Example configuration for PlainTextCorpus in INI format. It specifies the reader, path to the corpus, and optional length constraints for documents.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/corpus.mdx#2025-04-21_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\n### Example config\n[corpora.pretrain]\n@readers = \"spacy.PlainTextCorpus.v1\"\npath = \"corpus/raw_text.txt\"\nmin_length = 0\nmax_length = 0\n```\n\n----------------------------------------\n\nTITLE: Debug Config with Validation Error Output\nDESCRIPTION: Example output showing validation errors in the configuration file, including missing required fields and extra fields not permitted.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n✘ Config validation error\ndropout     field required\noptimizer   field required\noptimize    extra fields not permitted\n\n{'seed': 0, 'accumulate_gradient': 1, 'dev_corpus': 'corpora.dev', 'train_corpus': 'corpora.train', 'gpu_allocator': None, 'patience': 1600, 'max_epochs': 0, 'max_steps': 20000, 'eval_frequency': 200, 'frozen_components': [], 'optimize': None, 'before_to_disk': None, 'batcher': {'@batchers': 'spacy.batch_by_words.v1', 'discard_oversize': False, 'tolerance': 0.2, 'get_length': None, 'size': {'@schedules': 'compounding.v1', 'start': 100, 'stop': 1000, 'compound': 1.001, 't': 0.0}}, 'logger': {'@loggers': 'spacy.ConsoleLogger.v1', 'progress_bar': False}, 'score_weights': {'tag_acc': 0.5, 'dep_uas': 0.25, 'dep_las': 0.25, 'sents_f': 0.0}}\n```\n\n----------------------------------------\n\nTITLE: Getting Pipeline Components in spaCy\nDESCRIPTION: Example of retrieving pipeline components using the get_pipe method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.get_pipe(\"parser\")\ncustom_component = nlp.get_pipe(\"custom_component\")\n```\n\n----------------------------------------\n\nTITLE: Referencing FastAPI Integration Script in Python\nDESCRIPTION: This snippet shows the GitHub URL for a FastAPI integration script that creates a REST API with a POST endpoint for batch text processing and named entity recognition.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nhttps://github.com/explosion/projects/blob/v3/integrations/fastapi/scripts/main.py\n```\n\n----------------------------------------\n\nTITLE: Example of running debug data command\nDESCRIPTION: Simple example showing how to run the debug data command with a specific configuration file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug data ./config.cfg\n```\n\n----------------------------------------\n\nTITLE: Uploading spaCy Model to Hugging Face Hub using Bash Commands\nDESCRIPTION: This bash script demonstrates the process of logging into Hugging Face, packaging a spaCy model, and uploading it to the Hugging Face Hub using the spacy huggingface-hub command.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_40\n\nLANGUAGE: bash\nCODE:\n```\n$ huggingface-cli login\n$ python -m spacy package ./en_ner_fashion ./output --build wheel\n$ cd ./output/en_ner_fashion-0.0.0/dist\n$ python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Configuring Acronym Component in INI Format\nDESCRIPTION: This snippet shows how to configure the acronym component in the config.cfg file using INI format.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_29\n\nLANGUAGE: ini\nCODE:\n```\n[components.acronyms]\nfactory = \"acronyms\"\n\n[components.acronyms.data]\n@misc = \"acronyms.slang_dict.v1\"\n```\n\n----------------------------------------\n\nTITLE: Thinc Model Type Hints\nDESCRIPTION: Example of using Thinc's custom types for model architectures with specific array and input/output types.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef build_tagger_model(\n    tok2vec: Model[List[Doc], List[Floats2d]], nO: Optional[int] = None\n) -> Model[List[Doc], List[Floats2d]]:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Updating SentenceRecognizer Model\nDESCRIPTION: Updates the SentenceRecognizer model using Example objects for training. Requires an optimizer and returns loss metrics.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\noptimizer = nlp.initialize()\nlosses = senter.update(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Installing Hugging Face Hub Integration\nDESCRIPTION: Commands to install and set up spaCy's Hugging Face Hub integration package and login to Hugging Face.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_51\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install spacy-huggingface-hub\n$ huggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Vocabulary Entry Structure - Python\nDESCRIPTION: Comprehensive schema for vocabulary entries defining word properties including orthographic form, normalization, shape, and various boolean flags for word characteristics\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"orth\": \"string\",     # the word text\n    \"id\": \"int\",          # can correspond to row in vectors table\n    \"lower\": \"string\",\n    \"norm\": \"string\",\n    \"shape\": \"string\"\n    \"prefix\": \"string\",\n    \"suffix\": \"string\",\n    \"length\": \"int\",\n    \"cluster\": \"string\",\n    \"prob\": \"float\",\n    \"is_alpha\": \"bool\",\n    \"is_ascii\": \"bool\",\n    \"is_digit\": \"bool\",\n    \"is_lower\": \"bool\",\n    \"is_punct\": \"bool\",\n    \"is_space\": \"bool\",\n    \"is_title\": \"bool\",\n    \"is_upper\": \"bool\",\n    \"like_url\": \"bool\",\n    \"like_num\": \"bool\",\n    \"like_email\": \"bool\",\n    \"is_stop\": \"bool\",\n    \"is_oov\": \"bool\",\n    \"is_quote\": \"bool\",\n    \"is_left_punct\": \"bool\",\n    \"is_right_punct\": \"bool\"\n}\n```\n\n----------------------------------------\n\nTITLE: Loading CuratedTransformer from Bytes in Python\nDESCRIPTION: Demonstrates how to load a CuratedTransformer instance from a bytestring, modifying the object in place and returning it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrf_bytes = trf.to_bytes()\ntrf = nlp.add_pipe(\"curated_transformer\")\ntrf.from_bytes(trf_bytes)\n```\n\n----------------------------------------\n\nTITLE: Serializing spaCy Tagger to Bytes\nDESCRIPTION: Shows how to serialize a tagger component to a bytestring for binary storage or transmission.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\ntagger_bytes = tagger.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - NLP Libraries Speed Comparison\nDESCRIPTION: Comparison table showing processing speed in words per second (WPS) for different NLP libraries on both CPU and GPU, tested with 10,000 Reddit comments.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/facts-figures.mdx#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Library | Pipeline                                        | WPS CPU <Help>words per second on CPU, higher is better</Help> | WPS GPU <Help>words per second on GPU, higher is better</Help> |\n| ------- | ----------------------------------------------- | -------------------------------------------------------------: | -------------------------------------------------------------: |\n| spaCy   | [`en_core_web_lg`](/models/en#en_core_web_lg)   |                                                         10,014 |                                                         14,954 |\n| spaCy   | [`en_core_web_trf`](/models/en#en_core_web_trf) |                                                            684 |                                                          3,768 |\n| Stanza  | `en_ewt`                                        |                                                            878 |                                                          2,180 |\n| Flair   | `pos`(`-fast`) & `ner`(`-fast`)                 |                                                            323 |                                                          1,184 |\n| UDPipe  | `english-ewt-ud-2.5`                            |                                                          1,101 |                                                          _n/a_ |\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss for Morphologizer\nDESCRIPTION: Demonstrates how to compute loss and gradient for model predictions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\nscores = morphologizer.predict([eg.predicted for eg in examples])\nloss, d_loss = morphologizer.get_loss(examples, scores)\n```\n\n----------------------------------------\n\nTITLE: Creating SentenceRecognizer Optimizer\nDESCRIPTION: Creates an optimizer instance for the SentenceRecognizer component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\noptimizer = senter.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Setting Output Dimension of TrainablePipe in Python\nDESCRIPTION: Change the output dimension of the component's model if it is resizable. Raises NotImplementedError if not resizable.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nif pipe.is_resizable:\n    pipe.set_output(512)\n```\n\n----------------------------------------\n\nTITLE: Processing - Batch Tokenization Example\nDESCRIPTION: Example showing how to tokenize multiple texts using the pipe method for batch processing.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tokenizer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntexts = [\"One document.\", \"...\", \"Lots of documents\"]\nfor doc in tokenizer.pipe(texts, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Comparing Pre/Post spaCy 3.7 Listener Replacement Callbacks\nDESCRIPTION: Code demonstrating the callback signature differences between spaCy versions before and after 3.7. Newer versions support additional arguments (the replaced listener and tok2vec pipe) while maintaining backward compatibility.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Listeners.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef replace_listener_pre_37(copied_tok2vec_model):\n  ...\n\ndef replace_listener_post_37(copied_tok2vec_model, replaced_listener, tok2vec_pipe):\n  ...\n```\n\n----------------------------------------\n\nTITLE: Pattern Validation in spaCy Matcher\nDESCRIPTION: Shows how to validate patterns against a JSON schema using the validate=True option in Matcher. Helps catch unsupported attributes during development.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab, validate=True)\n# Add match ID \"HelloWorld\" with unsupported attribute CASEINSENSITIVE\npattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"CASEINSENSITIVE\": \"world\"}]\nmatcher.add(\"HelloWorld\", [pattern])\n```\n\n----------------------------------------\n\nTITLE: Moving NER to the End of spaCy Pipeline (Python)\nDESCRIPTION: This snippet demonstrates how to move the Named Entity Recognition (NER) component to the end of the spaCy pipeline. It's useful for accessing POS and LEMMA features in an entity_ruler. This applies to v3.0.x models only, as v3.1+ models have NER at the end by default.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/models/index.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# load without NER\nnlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n\n# source NER from the same pipeline package as the last component\nnlp.add_pipe(\"ner\", source=spacy.load(\"en_core_web_sm\"))\n\n# insert the entity ruler\nnlp.add_pipe(\"entity_ruler\", before=\"ner\")\n```\n\n----------------------------------------\n\nTITLE: Project Workflow Configuration\nDESCRIPTION: YAML configuration defining a workflow that combines multiple commands in sequence.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nworkflows:\n  all:\n    - preprocess\n    - train\n    - package\n```\n\n----------------------------------------\n\nTITLE: Updating Sentence Splitter Creation in Python\nDESCRIPTION: Shows the change from deprecated 'sbd' name to the new 'sentencizer' name for the rule-based sentence boundary detector.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-1.mdx#2025-04-21_snippet_8\n\nLANGUAGE: diff\nCODE:\n```\n- sentence_splitter = nlp.create_pipe(\"sbd\")\n+ sentence_splitter = nlp.create_pipe(\"sentencizer\")\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields in SpanResolver Serialization in Python\nDESCRIPTION: Demonstrate how to exclude specific serialization fields when saving the SpanResolver component to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndata = span_resolver.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Loading Tokenizer from Disk\nDESCRIPTION: Shows how to load a previously saved tokenizer from disk storage. Returns the modified Tokenizer object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tokenizer.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntokenizer = Tokenizer(nlp.vocab)\ntokenizer.from_disk(\"/path/to/tokenizer\")\n```\n\n----------------------------------------\n\nTITLE: Setting GPU Memory Allocator for PyTorch in spaCy Config\nDESCRIPTION: Demonstrates how to set the GPU memory allocator for PyTorch in the spaCy training config to prevent OOM errors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_12\n\nLANGUAGE: ini\nCODE:\n```\n[training]\ngpu_allocator = \"pytorch\"\n```\n\n----------------------------------------\n\nTITLE: Proper Error Handling Pattern\nDESCRIPTION: Shows how to replace try/except blocks with explicit condition checks for better error handling.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_15\n\nLANGUAGE: diff\nCODE:\n```\n- try:\n-     token = doc[i]\n- except IndexError:\n-     token = doc[-1]\n\n+ if i < len(doc):\n+     token = doc[i]\n+ else:\n+     token = doc[-1]\n```\n\n----------------------------------------\n\nTITLE: Working with Predicted Documents in spaCy (Python)\nDESCRIPTION: Demonstrates processing multiple examples' predicted documents with a model and updating annotations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndocs = [eg.predicted for eg in examples]\npredictions, _ = model.begin_update(docs)\nset_annotations(docs, predictions)\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanCategorizer Initialization in INI\nDESCRIPTION: Configuration snippet for initializing the SpanCategorizer component. It shows how to specify the path for reading labels using a JSON file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[initialize.components.spancat]\n\n[initialize.components.spancat.labels]\n@readers = \"spacy.read_labels.v1\"\npath = \"corpus/labels/spancat.json\n```\n\n----------------------------------------\n\nTITLE: Serializing CuratedTransformer to Bytes\nDESCRIPTION: Convert the transformer component to a bytestring representation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"curated_transformer\")\ntrf_bytes = trf.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Serializing TextCategorizer to Bytes in Python\nDESCRIPTION: This snippet demonstrates how to serialize the TextCategorizer to a bytestring. It converts the component into a byte representation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/textcategorizer.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntextcat = nlp.add_pipe(\"textcat\")\ntextcat_bytes = textcat.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy KB File Loader\nDESCRIPTION: Configuration example for the spacy.KBFileLoader.v1 showing knowledge base file path setup.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_23\n\nLANGUAGE: ini\nCODE:\n```\n[initialize.components.llm.candidate_selector.kb_loader]\n@llm_misc = \"spacy.KBFileLoader.v1\"\n# Path to knowledge base file.\npath = ${paths.el_kb}\n```\n\n----------------------------------------\n\nTITLE: Configuring NVTX Range Markers in spaCy\nDESCRIPTION: Configuration example showing how to enable NVTX range markers for model and pipeline methods after pipeline creation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_35\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\nafter_pipeline_creation = {\"@callbacks\":\"spacy.models_and_pipes_with_nvtx_range.v1\"}\n```\n\n----------------------------------------\n\nTITLE: Updating Serialization Method Parameters in Python\nDESCRIPTION: Demonstrates the change from 'disable' to 'exclude' parameter in serialization methods and updates to the vocabulary exclusion syntax.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-1.mdx#2025-04-21_snippet_7\n\nLANGUAGE: diff\nCODE:\n```\n- nlp.to_disk(\"/path\", disable=[\"parser\", \"ner\"])\n+ nlp.to_disk(\"/path\", exclude=[\"parser\", \"ner\"])\n- data = nlp.tokenizer.to_bytes(vocab=False)\n+ data = nlp.tokenizer.to_bytes(exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Sentence Span Getter in Python\nDESCRIPTION: Custom function that returns sentence spans based on maximum length constraints, registered with spacy_transformers span_getters registry.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport spacy_transformers\n\n@spacy_transformers.registry.span_getters(\"custom_sent_spans\")\ndef configure_custom_sent_spans(max_length: int):\n    def get_custom_sent_spans(docs):\n        spans = []\n        for doc in docs:\n            spans.append([])\n            for sent in doc.sents:\n                start = 0\n                end = max_length\n                while end <= len(sent):\n                    spans[-1].append(sent[start:end])\n                    start += max_length\n                    end += max_length\n                if start < len(sent):\n                    spans[-1].append(sent[start:len(sent)])\n        return spans\n\n    return get_custom_sent_spans\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanCat v2 in spaCy\nDESCRIPTION: Configuration example for the SpanCat v2 task component. Shows the same basic entity recognition setup as v3 but for the older version.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_34\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.SpanCat.v2\"\nlabels = [\"PERSON\", \"ORGANISATION\", \"LOCATION\"]\nexamples = null\n```\n\n----------------------------------------\n\nTITLE: Registering a Whitespace Tokenizer Function in spaCy\nDESCRIPTION: Shows how to register a custom whitespace tokenizer function using spaCy's registry system. This allows the tokenizer to be referenced in training configurations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n@spacy.registry.tokenizers(\"whitespace_tokenizer\")\ndef create_whitespace_tokenizer():\n    def create_tokenizer(nlp):\n        return WhitespaceTokenizer(nlp.vocab)\n\n    return create_tokenizer\n```\n\n----------------------------------------\n\nTITLE: Full spaCy assemble Command Signature\nDESCRIPTION: Complete command signature for the spaCy assemble command, showing all possible arguments including config_path, output_dir, code imports, verbosity options, and config overrides.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_40\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy assemble [config_path] [output_dir] [--code] [--verbose] [overrides]\n```\n\n----------------------------------------\n\nTITLE: Updating Doc Retokenization Syntax in Python\nDESCRIPTION: Shows how to update code from using deprecated Doc.merge() and Span.merge() methods to the new Doc.retokenize() context manager for more efficient bulk merging operations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-1.mdx#2025-04-21_snippet_6\n\nLANGUAGE: diff\nCODE:\n```\n- doc[1:5].merge()\n- doc[6:8].merge()\n+ with doc.retokenize() as retokenizer:\n+     retokenizer.merge(doc[1:5])\n+     retokenizer.merge(doc[6:8])\n```\n\n----------------------------------------\n\nTITLE: Configuring Few-Shot Reader for NER Examples\nDESCRIPTION: Configuration that specifies how to load few-shot examples for NER from an external YAML file. This allows providing examples to the LLM to demonstrate the expected output format.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_32\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task.examples]\n@misc = \"spacy.FewShotReader.v1\"\npath = \"ner_examples.yml\"\n```\n\n----------------------------------------\n\nTITLE: Serializing SpanCategorizer to Disk\nDESCRIPTION: Example showing how to save a SpanCategorizer component to disk using the to_disk() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\nspancat.to_disk(\"/path/to/spancat\")\n```\n\n----------------------------------------\n\nTITLE: Example meta.json Structure for spaCy Pipeline\nDESCRIPTION: Shows the structure of a meta.json file that contains metadata about a spaCy pipeline, including name, language, version compatibility, description, and author information.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"example_pipeline\",\n  \"lang\": \"en\",\n  \"version\": \"1.0.0\",\n  \"spacy_version\": \">=2.0.0,<3.0.0\",\n  \"description\": \"Example pipeline for spaCy\",\n  \"author\": \"You\",\n  \"email\": \"you@example.com\",\n  \"license\": \"CC BY-SA 3.0\"\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Lexeme from the Vocabulary by Orth ID in Cython\nDESCRIPTION: Example showing how to retrieve a LexemeC pointer from the vocabulary using an orth ID (hash of the text content). This demonstrates accessing lexical entries by their normalized form.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-classes.mdx#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nlexeme = vocab.get_by_orth(doc[0].lex.norm)\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Schedule Function in spaCy\nDESCRIPTION: Example of registering a custom schedule function called 'waltzing.v1' that yields a repeating pattern of values 1, 2, 3. The function uses the @spacy.registry.schedules decorator to register it in spaCy's schedules registry.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Iterator\nimport spacy\n\n@spacy.registry.schedules(\"waltzing.v1\")\ndef waltzing() -> Iterator[float]:\n    i = 0\n    while True:\n        yield i % 3 + 1\n        i += 1\n```\n\n----------------------------------------\n\nTITLE: Example of Using debug diff-config\nDESCRIPTION: A simple example showing how to run the diff-config command on a configuration file to compare it with spaCy's default settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy debug diff-config ./config.cfg\n```\n\n----------------------------------------\n\nTITLE: Deserializing DocBin from Bytes\nDESCRIPTION: Shows how to deserialize DocBin annotations from a bytestring.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/docbin.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndoc_bin_bytes = doc_bin.to_bytes()\nnew_doc_bin = DocBin().from_bytes(doc_bin_bytes)\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Vector Key-Value Pairs\nDESCRIPTION: Shows how to iterate over key-vector pairs and print key, string representation, and vector values.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfor key, vector in nlp.vocab.vectors.items():\n   print(key, nlp.vocab.strings[key], vector)\n```\n\n----------------------------------------\n\nTITLE: Initializing Labels Command Usage in spaCy\nDESCRIPTION: Command line syntax for initializing labels using spaCy CLI, including options for config path, output path, code import, verbosity and GPU selection.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init labels [config_path] [output_path] [--code] [--verbose] [--gpu-id] [overrides]\n```\n\n----------------------------------------\n\nTITLE: Checking Lexeme Flag Values in spaCy\nDESCRIPTION: Shows how to check binary flag attributes on a LexemeC struct using the c_check_flag static method. The method returns a boolean indicating whether the specified flag is set.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-structs.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs cimport IS_STOP\nfrom spacy.lexeme cimport Lexeme\n\nlexeme = doc.c[3].lex\nis_stop = Lexeme.c_check_flag(lexeme, IS_STOP)\n```\n\n----------------------------------------\n\nTITLE: Configuring NLP Callbacks in INI\nDESCRIPTION: Example of configuring a callback function in the spaCy config file to customize language data before NLP object creation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_21\n\nLANGUAGE: ini\nCODE:\n```\n[nlp.before_creation]\n@callbacks = \"customize_language_data\"\n```\n\n----------------------------------------\n\nTITLE: Span Classification Data Structure\nDESCRIPTION: JSON structure for span visualization, including text content, token-based span annotations, and token list.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"text\": \"Welcome to the Bank of China.\",\n  \"spans\": [\n    { \"start_token\": 3, \"end_token\": 6, \"label\": \"ORG\" },\n    { \"start_token\": 5, \"end_token\": 6, \"label\": \"GPE\" }\n  ],\n  \"tokens\": [\"Welcome\", \"to\", \"the\", \"Bank\", \"of\", \"China\", \".\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Using Alignment in spaCy (Python)\nDESCRIPTION: Demonstrates token alignment between predicted and reference documents using the Alignment object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntokens_x = [\"Apply\", \"some\", \"sunscreen\"]\nx = Doc(vocab, words=tokens_x)\ntokens_y = [\"Apply\", \"some\", \"sun\", \"screen\"]\nexample = Example.from_dict(x, {\"words\": tokens_y})\nalignment = example.alignment\nassert list(alignment.y2x.data) == [[0], [1], [2], [2]]\n```\n\n----------------------------------------\n\nTITLE: Updating Model Initialization Command Syntax\nDESCRIPTION: Shows the change in init-model command syntax from separate frequency and cluster files to a single JSONL file for lexical entries.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-1.mdx#2025-04-21_snippet_10\n\nLANGUAGE: diff\nCODE:\n```\n- $ spacy init-model en ./model --freqs-loc ./freqs.txt --clusters-loc ./clusters.txt\n+ $ spacy init-model en ./model --jsonl-loc ./vocab.jsonl\n```\n\n----------------------------------------\n\nTITLE: Checking Table Existence with has_table\nDESCRIPTION: Shows how to check if a table exists in the lookups object using the has_table method, which is equivalent to using the in operator.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlookups = Lookups()\nlookups.add_table(\"some_table\")\nassert lookups.has_table(\"some_table\")\n```\n\n----------------------------------------\n\nTITLE: Serializing Tokenizer to Bytes\nDESCRIPTION: Converts a tokenizer object to a bytestring representation for serialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tokenizer.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntokenizer = tokenizer(nlp.vocab)\ntokenizer_bytes = tokenizer.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Dependency Parsing Input Format for displaCy\nDESCRIPTION: Structure of input data for dependency visualization using displaCy. The format includes words with part-of-speech tags and arcs representing grammatical relationships between words.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"words\": [\n        {\"text\": \"This\", \"tag\": \"DT\"},\n        {\"text\": \"is\", \"tag\": \"VBZ\"},\n        {\"text\": \"a\", \"tag\": \"DT\"},\n        {\"text\": \"sentence\", \"tag\": \"NN\"}\n    ],\n    \"arcs\": [\n        {\"start\": 0, \"end\": 1, \"label\": \"nsubj\", \"dir\": \"left\"},\n        {\"start\": 2, \"end\": 3, \"label\": \"det\", \"dir\": \"left\"},\n        {\"start\": 1, \"end\": 3, \"label\": \"attr\", \"dir\": \"right\"}\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Vector Store Length in Python\nDESCRIPTION: Example demonstrating how to get the number of vectors in the store.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvectors = Vectors(shape=(3, 300))\nassert len(vectors) == 3\n```\n\n----------------------------------------\n\nTITLE: Serializing TrainablePipe to Bytes\nDESCRIPTION: Demonstrates how to serialize a pipeline component to a bytestring format.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\npipe_bytes = pipe.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields from Serialization\nDESCRIPTION: Example showing how to exclude specific fields when serializing the lemmatizer component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lemmatizer.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndata = lemmatizer.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Loading CuratedTransformer from Disk\nDESCRIPTION: Load a serialized transformer component from disk storage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"curated_transformer\")\ntrf.from_disk(\"/path/to/transformer\")\n```\n\n----------------------------------------\n\nTITLE: Processing Documents with EditTreeLemmatizer.pipe\nDESCRIPTION: Example showing how to add a trainable lemmatizer to the pipeline and process a stream of documents with batch processing.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/edittreelemmatizer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"trainable_lemmatizer\", name=\"lemmatizer\")\nfor doc in lemmatizer.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Deserializing EntityRuler from Bytes\nDESCRIPTION: Demonstrates how to load EntityRuler patterns from a bytestring, modifying the object in place.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nruler_bytes = ruler.to_bytes()\nruler = nlp.add_pipe(\"entity_ruler\")\nruler.from_bytes(ruler_bytes)\n```\n\n----------------------------------------\n\nTITLE: Plain Text Corpus Example Format\nDESCRIPTION: Example of plain text format used for raw corpus data with one document per line.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/corpus.mdx#2025-04-21_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nCan I ask where you work now and what you do, and if you enjoy it?\nThey may just pull out of the Seattle market completely, at least until they have autonomous vehicles.\nMy cynical view on this is that it will never be free to the public. Reason: what would be the draw of joining the military? Right now their selling point is free Healthcare and Education. Ironically both are run horribly and most, that I've talked to, come out wishing they never went in.\n```\n\n----------------------------------------\n\nTITLE: Configuring Vector Initialization for Sourced Components\nDESCRIPTION: Configuration example showing how to explicitly specify vector initialization when sourcing components from other pipelines. This is required in spaCy v3.1 when sourcing components that use static vectors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-1.mdx#2025-04-21_snippet_8\n\nLANGUAGE: ini\nCODE:\n```\n[components.ner]\nsource = \"en_core_web_md\"\n\n[initialize]\nvectors = \"en_core_web_md\"\n```\n\n----------------------------------------\n\nTITLE: Setting Parser Annotations\nDESCRIPTION: Shows how to modify documents using pre-computed parser scores.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\nscores = parser.predict([doc1, doc2])\nparser.set_annotations([doc1, doc2], scores)\n```\n\n----------------------------------------\n\nTITLE: Training Command with Custom Code\nDESCRIPTION: Command line instruction for training spaCy with custom code implementation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy train ./config.cfg --code ./code.py\n```\n\n----------------------------------------\n\nTITLE: Configuring FileReader\nDESCRIPTION: Configuration for SpaCy's FileReader component that reads and returns the contents of a file as a string. Commonly used for reading Jinja template files.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_65\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task.template]\n@misc = \"spacy.FileReader.v1\"\npath = \"ner_template.jinja2\"\n```\n\n----------------------------------------\n\nTITLE: Parametrizing Tests with Multiple Values per Test Case\nDESCRIPTION: An example of using pytest's parametrize decorator with tuples to provide multiple related values for each test case, allowing for more complex test scenarios.\nSOURCE: https://github.com/explosion/spacy/blob/master/spacy/tests/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize('text,length', [(\"U.S.\", 1), (\"us.\", 2), (\"(U.S.\", 2)])\n```\n\n----------------------------------------\n\nTITLE: Overriding Variables in spaCy Project CLI\nDESCRIPTION: This command demonstrates how to override variables when running a spaCy project from the command line. It shows the syntax for specifying a project directory placeholder and setting custom variable values.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy project run test . --vars.foo bar\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy with Lookups Data\nDESCRIPTION: Command to install spaCy with additional lookups data package that includes lemmatization and normalization tables.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-3.mdx#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install spacy[lookups]\n```\n\n----------------------------------------\n\nTITLE: Configuring Legacy MaxoutWindowEncoder in spaCy\nDESCRIPTION: Configuration for the spacy.MaxoutWindowEncoder.v1 architecture that produces a Model[Floats2D, Floats2D]. Implements context encoding using convolutions with maxout activation, layer normalization and residual connections.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/legacy.mdx#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.MaxoutWindowEncoder.v1\"\nwidth = 128\nwindow_size = 1\nmaxout_pieces = 3\ndepth = 4\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Parameter Sets with pytest\nDESCRIPTION: An example showing how to combine multiple parametrize decorators to test all combinations of different parameter sets, useful for comprehensive testing of interactions between parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/spacy/tests/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize('text', [\"A test sentence\", \"Another sentence\"])\n@pytest.mark.parametrize('punct', ['.', '!', '?'])\n```\n\n----------------------------------------\n\nTITLE: Pipeline Accuracy Table in Markdown\nDESCRIPTION: Markdown table comparing accuracy scores of different spaCy models (v2 and v3) across Parser, Tagger and NER tasks on OntoNotes 5.0 corpus\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/_benchmarks-models.mdx#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Pipeline                                                   | Parser | Tagger |  NER |\n| ---------------------------------------------------------- | -----: | -----: | ---: |\n| [`en_core_web_trf`](/models/en#en_core_web_trf) (spaCy v3) |   95.1 |   97.8 | 89.8 |\n| [`en_core_web_lg`](/models/en#en_core_web_lg) (spaCy v3)   |   92.0 |   97.4 | 85.5 |\n| `en_core_web_lg` (spaCy v2)                                |   91.9 |   97.2 | 85.5 |\n```\n\n----------------------------------------\n\nTITLE: Examining Removed Words After Vector Pruning in spaCy\nDESCRIPTION: Example output showing the dictionary of removed words after pruning vectors. Each entry shows the original word, the word it was remapped to, and the similarity score between them, illustrating how semantic similarity guides the pruning process.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"Shore\": (\"coast\", 0.732257),\n    \"Precautionary\": (\"caution\", 0.490973),\n    \"hopelessness\": (\"sadness\", 0.742366),\n    \"Continuous\": (\"continuous\", 0.732549),\n    \"Disemboweled\": (\"corpse\", 0.499432),\n    \"biostatistician\": (\"scientist\", 0.339724),\n    \"somewheres\": (\"somewheres\", 0.402736),\n    \"observing\": (\"observe\", 0.823096),\n    \"Leaving\": (\"leaving\", 1.0),\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Rule-based Korean Tokenizer in spaCy\nDESCRIPTION: Shows how to configure a Korean pipeline with a rule-based tokenizer instead of the default MeCab-based tokenizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\"nlp\": {\"tokenizer\": {\"@tokenizers\": \"spacy.Tokenizer.v1\"}}}\nnlp = spacy.blank(\"ko\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Serializing SpanResolver to Bytes in Python\nDESCRIPTION: Convert the SpanResolver component to a bytestring for serialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\nspan_resolver_bytes = span_resolver.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Dependency Tree Children in spaCy\nDESCRIPTION: Shows how to set up attributes for syntactic parse tree navigation using set_children_from_heads. This function must be called after modifying TokenC.head attributes to maintain tree consistency.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-structs.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens.doc cimport Doc, set_children_from_heads\nfrom spacy.vocab cimport Vocab\n\ndoc = Doc(Vocab(), words=[\"Baileys\", \"from\", \"a\", \"shoe\"])\ndoc.c[0].head = 0\ndoc.c[1].head = 0\ndoc.c[2].head = 3\ndoc.c[3].head = 1\nset_children_from_heads(doc.c, doc.length)\nassert doc.c[3].l_kids == 1\n```\n\n----------------------------------------\n\nTITLE: Initializing spaCy Matcher Object\nDESCRIPTION: Python code snippet showing how to create a Matcher object. It requires the vocabulary from the NLP pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/matcher.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.matcher import Matcher\nmatcher = Matcher(nlp.vocab)\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy with GPU Support\nDESCRIPTION: Instructions for installing spaCy with CUDA GPU support and activating it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U %%SPACY_PKG_NAME[cuda113]%%SPACY_PKG_FLAGS\n```\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nspacy.prefer_gpu()\nnlp = spacy.load(\"en_core_web_sm\")\n```\n\n----------------------------------------\n\nTITLE: Chinese Language Configuration\nDESCRIPTION: Manual setup examples for different Chinese word segmentation options including character segmentation, Jieba, and PKUSeg.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lang.zh import Chinese\n\n# Character segmentation (default)\nnlp = Chinese()\n# Jieba\ncfg = {\"segmenter\": \"jieba\"}\nnlp = Chinese.from_config({\"nlp\": {\"tokenizer\": cfg}})\n# PKUSeg with \"mixed\" model provided by pkuseg\ncfg = {\"segmenter\": \"pkuseg\"}\nnlp = Chinese.from_config({\"nlp\": {\"tokenizer\": cfg}})\nnlp.tokenizer.initialize(pkuseg_model=\"mixed\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependency Visualizer Options in spaCy displaCy\nDESCRIPTION: Example showing how to set custom options for dependency visualization with displaCy. This snippet demonstrates setting compact mode and changing the arrow color to blue.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\noptions = {\"compact\": True, \"color\": \"blue\"}\ndisplacy.serve(doc, style=\"dep\", options=options)\n```\n\n----------------------------------------\n\nTITLE: Configuration - spaCy Tokenizer Default Config\nDESCRIPTION: Default INI configuration for the spaCy tokenizer showing the tokenizer class assignment.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tokenizer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[nlp.tokenizer]\n@tokenizers = \"spacy.Tokenizer.v1\"\n```\n\n----------------------------------------\n\nTITLE: Adding Modifier to Object in Pattern\nDESCRIPTION: Completes the pattern by specifying that the direct object should have a modifier with dependency relation 'amod' or 'compound', representing the kind of company.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_36\n\nLANGUAGE: python\nCODE:\n```\npattern = [\n    # ...\n    {\n        \"LEFT_ID\": \"founded_object\",\n        \"REL_OP\": \">\",\n        \"RIGHT_ID\": \"founded_object_modifier\",\n        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Checking Matcher Length in spaCy\nDESCRIPTION: Demonstrates how to check the number of rules added to a Matcher instance.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/matcher.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmatcher = Matcher(nlp.vocab)\nassert len(matcher) == 0\nmatcher.add(\"Rule\", [[{\"ORTH\": \"test\"}]])\nassert len(matcher) == 1\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Language via Entry Points\nDESCRIPTION: Updates setup.py to register a custom Language subclass via the spacy_languages entry point, allowing spaCy to discover and use the language via its language code.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_16\n\nLANGUAGE: diff\nCODE:\n```\nfrom setuptools import setup\n\nsetup(\n    name=\"snek\",\n    entry_points={\n        \"spacy_factories\": [\"snek = snek:SnekFactory\"],\n+       \"spacy_languages\": [\"snk = snek:SnekLanguage\"]\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Incorrect Span Annotations in Python\nDESCRIPTION: Python code showing how to add incorrect span annotations to a document for training the entity recognizer with partial annotations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-1.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_doc = nlp.make_doc(\"Barack Obama was born in Hawaii.\")\n# The doc.spans key can be defined in the config\ntrain_doc.spans[\"incorrect_spans\"] = [\n  Span(doc, 0, 2, label=\"ORG\"),\n  Span(doc, 5, 6, label=\"PRODUCT\")\n]\n```\n\n----------------------------------------\n\nTITLE: Raw Task Few-Shot Examples in YAML\nDESCRIPTION: YAML configuration showing examples for few-shot learning in Raw.v1 task, including arithmetic, creative writing, and sentiment analysis cases.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\n- text: \"3 + 5 = x. What's x?\"\n  reply: '8'\n\n- text: 'Write me a limerick.'\n  reply: \"There was an Old Man with a beard, Who said, 'It is just as I feared! Two Owls and a Hen, Four Larks and a Wren, Have all built their nests in my beard!\"\n\n- text: \"Analyse the sentiment of the text 'This is great'.\"\n  reply: \"'This is great' expresses a very positive sentiment.\"\n```\n\n----------------------------------------\n\nTITLE: Tag Component Implementation\nDESCRIPTION: Examples of implementing different types of tags with variants for method, version, and model indicators.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_3\n\nLANGUAGE: jsx\nCODE:\n```\n<Tag>method</Tag>\n<Tag variant=\"version\">4</Tag>\n<Tag variant=\"model\">tagger, parser</Tag>\n```\n\n----------------------------------------\n\nTITLE: REL Task Configuration\nDESCRIPTION: Configuration example for the REL.v1 component showing setup with relation labels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_42\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.REL.v1\"\nlabels = [\"LivesIn\", \"Visits\"]\n```\n\n----------------------------------------\n\nTITLE: Checking Table Existence in Lookups\nDESCRIPTION: Demonstrates how to check if a table exists in the lookups object using the in operator, which delegates to the has_table method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlookups = Lookups()\nlookups.add_table(\"some_table\")\nassert \"some_table\" in lookups\n```\n\n----------------------------------------\n\nTITLE: Serializing SpanFinder to Bytes\nDESCRIPTION: Shows how to serialize the SpanFinder component to a bytestring.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nspan_finder = nlp.add_pipe(\"span_finder\")\nspan_finder_bytes = span_finder.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Creating a Token Object from a TokenC Pointer in Cython\nDESCRIPTION: Example showing how to initialize a Token object from a TokenC pointer. This demonstrates accessing an existing token from a Doc's internal C array and creating a new Token reference to it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-classes.mdx#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ntoken = Token.cinit(&doc.c[3], doc, 3)\n```\n\n----------------------------------------\n\nTITLE: Configuring lower_case Data Augmenter in spaCy\nDESCRIPTION: Configuration for lower_case augmenter that lowercases documents to make the model less sensitive to capitalization. It takes a level parameter to control the percentage of texts that will be augmented.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_32\n\nLANGUAGE: ini\nCODE:\n```\n[corpora.train.augmenter]\n@augmenters = \"spacy.lower_case.v1\"\nlevel = 0.3\n```\n\n----------------------------------------\n\nTITLE: Basic spaCy Fill-Config Command Syntax\nDESCRIPTION: The full command syntax for filling a partial spaCy configuration file with default values, showing all available parameters and options.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init fill-config [base_path] [output_file] [--diff]\n```\n\n----------------------------------------\n\nTITLE: Named Entity Recognition Data Structure\nDESCRIPTION: JSON structure for entity visualization, showing text content and entity annotations with character offsets.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"text\": \"But Google is starting from behind.\",\n  \"ents\": [{ \"start\": 4, \"end\": 10, \"label\": \"ORG\" }]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Rule-based Korean Tokenizer in spaCy Config\nDESCRIPTION: Demonstrates how to set up a rule-based tokenizer for Korean in the spaCy config file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_9\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\nlang = \"ko\"\ntokenizer = {\"@tokenizers\" = \"spacy.Tokenizer.v1\"}\n```\n\n----------------------------------------\n\nTITLE: Checking Token Ancestry - Python\nDESCRIPTION: Demonstrates how to check dependency relationships between tokens.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\ngive = doc[0]\nit = doc[1]\nassert give.is_ancestor(it)\n```\n\n----------------------------------------\n\nTITLE: Saving AttributeRuler to Disk - Python\nDESCRIPTION: Example showing how to save an AttributeRuler pipe to disk using the to_disk() method. The ruler must first be added to the nlp pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributeruler.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"attribute_ruler\")\nruler.to_disk(\"/path/to/attribute_ruler\")\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields During Serialization\nDESCRIPTION: Example showing how to exclude specific fields when serializing a SpanCategorizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndata = spancat.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Defining TransformerListener Model Chain Architecture\nDESCRIPTION: Code demonstrating how the TransformerListener architecture chains the listener with the trfs2arrays component. This shows the model structure when a component is listening to a transformer.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Listeners.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = chain(\n    TransformerListener(upstream_name=upstream)\n    trfs2arrays(pooling, grad_factor),\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Vector Size\nDESCRIPTION: Shows how to check the total size of vectors (rows * dimensions).\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nvectors = Vectors(shape=(500, 300))\nassert vectors.size == 150000\n```\n\n----------------------------------------\n\nTITLE: Checking PhraseMatcher Length\nDESCRIPTION: Shows how to check the number of rules added to the matcher using the len() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/phrasematcher.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmatcher = PhraseMatcher(nlp.vocab)\nassert len(matcher) == 0\nmatcher.add(\"OBAMA\", [nlp(\"Barack Obama\")])\nassert len(matcher) == 1\n```\n\n----------------------------------------\n\nTITLE: Serializing StringStore to Bytes in Python\nDESCRIPTION: Use the to_bytes method to serialize the StringStore state to a binary string.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/stringstore.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nstore_bytes = stringstore.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Migrating Simple Training Style\nDESCRIPTION: Comparison showing how to migrate from the v2 simple training style to the v3 Example-based API for creating training examples.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_27\n\nLANGUAGE: diff\nCODE:\n```\ntext = \"Mark Zuckerberg is the CEO of Facebook\"\nannotations = {\"entities\": [(0, 15, \"PERSON\"), (30, 38, \"ORG\")]}\n+ doc = nlp.make_doc(text)\n+ example = Example.from_dict(doc, annotations)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom PyTorch Model in spaCy Config\nDESCRIPTION: Shows how to configure the custom PyTorch model in spaCy's config file, specifying architecture and parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_11\n\nLANGUAGE: ini\nCODE:\n```\n[components.tagger]\nfactory = \"tagger\"\n\n[components.tagger.model]\n@architectures = \"CustomTorchModel.v1\"\nnO = 50\nwidth = 96\nhidden_width = 48\nembed_size = 2000\nnM = 64\nnC = 8\ndropout = 0.2\n```\n\n----------------------------------------\n\nTITLE: Deserializing SpanRuler from Bytes\nDESCRIPTION: Demonstrates how to load a SpanRuler object from a bytestring.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nruler_bytes = ruler.to_bytes()\nruler = nlp.add_pipe(\"span_ruler\")\nruler.from_bytes(ruler_bytes)\n```\n\n----------------------------------------\n\nTITLE: Multiple Pattern Matching in spaCy\nDESCRIPTION: Shows how to add multiple patterns to a matcher to match different variations of the same phrase, with and without punctuation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npatterns = [\n    [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}],\n    [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\n]\nmatcher.add(\"HelloWorld\", patterns)\n```\n\n----------------------------------------\n\nTITLE: List Comprehension Over Lambda\nDESCRIPTION: Demonstrates replacing lambda functions with more readable list comprehensions and helper functions.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_16\n\nLANGUAGE: diff\nCODE:\n```\n- split_string: Callable[[str], List[str]] = lambda value: [v.strip() for v in value.split(\",\")]\n\n+ def split_string(value: str) -> List[str]:\n+     return [v.strip() for v in value.split(\",\")]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Prompt Sharding in Python\nDESCRIPTION: This snippet demonstrates how prompts are split (sharded) when they exceed the model's context length. It shows two example prompts that would be generated for a sentiment analysis task.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nEstimate the sentiment of this text:\n\"This has been amazing - I can't remember \"\nEstimated sentiment:\n```\n\nLANGUAGE: python\nCODE:\n```\nEstimate the sentiment of this text:\n\"the last time I left the cinema so impressed.\"\nEstimated sentiment:\n```\n\n----------------------------------------\n\nTITLE: Defining Training Configuration in INI Format for spaCy\nDESCRIPTION: This snippet shows the structure of a spaCy training configuration file. It includes sections for defining the nlp object, tokenizer, pipeline components, and various settings for language model training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n%%GITHUB_SPACY/spacy/default_config.cfg\n```\n\n----------------------------------------\n\nTITLE: Variable Reassignment Pattern in Python\nDESCRIPTION: Example showing the correct way to reassign variables while maintaining type consistency and avoiding namespace clutter.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nents = get_a_list_of_entities()\nents = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\nent_mappings = {(ent.start, ent.end): ent.label_ for ent in ents}\n```\n\n----------------------------------------\n\nTITLE: Loading Model from __init__.py in spaCy Pipeline Package\nDESCRIPTION: Helper function to use in the load() method of a pipeline package's __init__.py file. This allows for proper loading of the model with configurable options for disabling or excluding components.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.util import load_model_from_init_py\n\ndef load(**overrides):\n    return load_model_from_init_py(__file__, **overrides)\n```\n\n----------------------------------------\n\nTITLE: Initializing Lexeme Tables Configuration\nDESCRIPTION: Configuration settings for initializing lexeme tables when training a model from scratch. Specifies language and required lookup tables.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_19\n\nLANGUAGE: ini\nCODE:\n```\n[initialize.lookups]\n@misc = \"spacy.LookupsDataLoader.v1\"\nlang = ${nlp.lang}\ntables = [\"lexeme_norm\"]\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for Morphologizer\nDESCRIPTION: Shows how to create an optimizer for the morphologizer component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\noptimizer = morphologizer.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Running spaCy Tests with pytest in Bash\nDESCRIPTION: Commands for running spaCy's test suite using pytest. This includes options for running basic tests or including additional slow tests for more comprehensive testing.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m pip install -U pytest               # update pytest\n$ python -m pytest --pyargs spacy               # basic tests\n$ python -m pytest --pyargs spacy --slow        # basic and slow tests\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Anthropic API\nDESCRIPTION: Shell command to set required environment variable for using Anthropic API with spaCy\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_56\n\nLANGUAGE: shell\nCODE:\n```\nexport ANTHROPIC_API_KEY=\"...\"\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy Language Models in Jupyter Notebooks\nDESCRIPTION: Demonstrates how to install spaCy language models within a Jupyter notebook environment.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n!python -m spacy download en_core_web_sm\n```\n\n----------------------------------------\n\nTITLE: Using require_gpu in spaCy\nDESCRIPTION: Example demonstrating how to require GPU processing in spaCy, which will raise an error if no GPU is available.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nspacy.require_gpu()\nnlp = spacy.load(\"en_core_web_sm\")\n```\n\n----------------------------------------\n\nTITLE: Remote storage path example\nDESCRIPTION: Example showing the path structure in an S3 bucket after pushing a trained model, with URL encoding and hash-based organization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\n└── s3://my-spacy-bucket/training%2Fmodel-best\n    └── 1d8cb33a06cc345ad3761c6050934a1b\n        └── d8e20c3537a084c5c10d95899fe0b1ff\n```\n\n----------------------------------------\n\nTITLE: Serializing EntityRecognizer to Bytes\nDESCRIPTION: Demonstrates serializing a named entity recognition model to a bytestring using to_bytes(). Useful for in-memory serialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\nner_bytes = ner.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Processing Stream of Documents with Lemmatizer\nDESCRIPTION: Example of applying the Lemmatizer component to a stream of documents using the pipe method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lemmatizer.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlemmatizer = nlp.add_pipe(\"lemmatizer\")\nfor doc in lemmatizer.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Serializing SpanResolver to Disk in Python\nDESCRIPTION: Save the SpanResolver component to a directory on disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\nspan_resolver.to_disk(\"/path/to/span_resolver\")\n```\n\n----------------------------------------\n\nTITLE: Example Output of Relation Prediction\nDESCRIPTION: Demonstrates how to access and interpret the relation predictions stored in the custom document extension attribute.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Amsterdam is the capital of the Netherlands.\")\nprint(\"spans\", [(e.start, e.text, e.label_) for e in doc.ents])\nfor value, rel_dict in doc._.rel.items():\n    print(f\"{value}: {rel_dict}\")\n\n# spans [(0, 'Amsterdam', 'LOC'), (6, 'Netherlands', 'LOC')]\n# (0, 6): {'CAPITAL_OF': 0.89, 'LOCATED_IN': 0.75, 'UNRELATED': 0.002}\n# (6, 0): {'CAPITAL_OF': 0.01, 'LOCATED_IN': 0.13, 'UNRELATED': 0.017}\n```\n\n----------------------------------------\n\nTITLE: Running DVC with spaCy Project\nDESCRIPTION: Command to create DVC configuration from a spaCy project workflow.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project dvc [project_dir] [workflow_name]\n```\n\n----------------------------------------\n\nTITLE: Serializing EntityRuler to Bytes\nDESCRIPTION: Example of serializing EntityRuler patterns to a bytestring for storage or transmission.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"entity_ruler\")\nruler_bytes = ruler.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Dependency Visualization Data Structure\nDESCRIPTION: JSON structure for dependency parsing visualization, showing word tokens and their relationships through arcs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"words\": [\n    { \"text\": \"This\", \"tag\": \"DT\" },\n    { \"text\": \"is\", \"tag\": \"VBZ\" },\n    { \"text\": \"a\", \"tag\": \"DT\" },\n    { \"text\": \"sentence\", \"tag\": \"NN\" }\n  ],\n  \"arcs\": [\n    { \"start\": 0, \"end\": 1, \"label\": \"nsubj\", \"dir\": \"left\" },\n    { \"start\": 2, \"end\": 3, \"label\": \"det\", \"dir\": \"left\" },\n    { \"start\": 1, \"end\": 3, \"label\": \"attr\", \"dir\": \"right\" }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Checking vocabulary size in Python\nDESCRIPTION: Demonstrates how to get the current number of lexemes in the vocabulary using the len() function on a spaCy nlp object's vocab.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\nassert len(nlp.vocab) > 0\n```\n\n----------------------------------------\n\nTITLE: Configuring Legacy Tok2Vec Architecture in spaCy\nDESCRIPTION: Configuration example for the spacy.Tok2Vec.v1 architecture that expects an encode model of type Model[Floats2D, Floats2D]. Used with embedding and encoding subnetworks.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/legacy.mdx#2025-04-21_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.Tok2Vec.v1\"\n\n[model.embed]\n@architectures = \"spacy.CharacterEmbed.v1\"\n# ...\n\n[model.encode]\n@architectures = \"spacy.MaxoutWindowEncoder.v1\"\n# ...\n```\n\n----------------------------------------\n\nTITLE: Merging Token Spans Using Edge Detection\nDESCRIPTION: Demonstrates how to create and merge spans using left and right edge detection in the parse tree.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Credit and mortgage account holders must submit their requests\")\nspan = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\nwith doc.retokenize() as retokenizer:\n    retokenizer.merge(span)\nfor token in doc:\n    print(token.text, token.pos_, token.dep_, token.head.text)\n```\n\n----------------------------------------\n\nTITLE: Remote storage directory structure\nDESCRIPTION: Illustration of how spaCy organizes files in remote storage using a hierarchical structure based on file paths, command hashes, and content hashes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_28\n\nLANGUAGE: yaml\nCODE:\n```\n└── urlencoded_file_path            # Path of original file\n    ├── some_command_hash           # Hash of command you ran\n    │   ├── some_content_hash       # Hash of file content\n    │   └── another_content_hash\n    └── another_command_hash\n        └── third_content_hash\n```\n\n----------------------------------------\n\nTITLE: Loading Lookups from Disk\nDESCRIPTION: Shows how to load a lookups object from a directory containing a lookups.bin file. Loading will be skipped if the file doesn't exist.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lookups import Lookups\nlookups = Lookups()\nlookups.from_disk(\"/path/to/lookups\")\n```\n\n----------------------------------------\n\nTITLE: Checking if String Maps to Installed pip Package\nDESCRIPTION: Verifies if a given string corresponds to a package installed via pip. Primarily used to validate pipeline packages in spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nutil.is_package(\"en_core_web_sm\") # True\nutil.is_package(\"xyz\") # False\n```\n\n----------------------------------------\n\nTITLE: Using spaCy Matcher\nDESCRIPTION: Example of using spaCy's pattern matcher to find text matches.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load('en_core_web_sm')\nmatcher = Matcher(nlp.vocab)\npattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\nmatcher.add(\"HelloWorld\", None, pattern)\ndoc = nlp(\"Hello, world! Hello world!\")\nmatches = matcher(doc)\n```\n\n----------------------------------------\n\nTITLE: Removing Span Extensions\nDESCRIPTION: Demonstrates how to remove a previously registered extension from the Span class.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Span\nSpan.set_extension(\"is_city\", default=False)\nremoved = Span.remove_extension(\"is_city\")\nassert not Span.has_extension(\"is_city\")\n```\n\n----------------------------------------\n\nTITLE: Initializing spaCy LLM component with few-shot examples callback\nDESCRIPTION: This configuration snippet demonstrates how to initialize a spaCy pipeline with a get_examples callback for the LLM component. It sets the number of examples to use for few-shot learning to 3, which will be automatically fetched during initialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_11\n\nLANGUAGE: ini\nCODE:\n```\n[initialize.components.llm]\nn_prompt_examples = 3\n```\n\n----------------------------------------\n\nTITLE: Loading TrainablePipe from Bytes\nDESCRIPTION: Shows how to deserialize a pipeline component from a bytestring format. The method modifies the pipe object in place and returns it.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npipe_bytes = pipe.to_bytes()\npipe = nlp.add_pipe(\"your_custom_pipe\")\npipe.from_bytes(pipe_bytes)\n```\n\n----------------------------------------\n\nTITLE: Checking SpanRuler Length in Python\nDESCRIPTION: Demonstrates how to check the number of patterns in a SpanRuler using the __len__ method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"span_ruler\")\nassert len(ruler) == 0\nruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\nassert len(ruler) == 1\n```\n\n----------------------------------------\n\nTITLE: Relation Instance Generator Configuration\nDESCRIPTION: Configuration for the relation instance generator component that creates candidate entity pairs for relation prediction, with a maximum distance parameter.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_23\n\nLANGUAGE: ini\nCODE:\n```\n[model.create_instance_tensor.get_instances]\n@misc = \"rel_instance_generator.v1\"\nmax_length = 100\n```\n\n----------------------------------------\n\nTITLE: Transformers GPU Testing Command\nDESCRIPTION: Command for executing spaCy Transformers GPU tests using a specific repository and branch configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/ExplosionBot.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n@explosion-bot please test_gpu --run-on spacy-transformers --run-on-branch master --spacy-branch current_pr\n```\n\n----------------------------------------\n\nTITLE: Updating Vector Key Iteration Pattern in Python\nDESCRIPTION: Shows how to modify code that iterates over vector keys when using floret vectors instead of default vectors. The new approach uses an external word list rather than vocabulary vectors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-4.mdx#2025-04-21_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\n- lexemes = [nlp.vocab[orth] for orth in nlp.vocab.vectors]\n+ lexemes = [nlp.vocab[word] for word in external_word_list]\n```\n\n----------------------------------------\n\nTITLE: Using Senter for Fast Sentence Segmentation in spaCy\nDESCRIPTION: This code shows how to disable the parser and enable the senter component for faster sentence segmentation without dependency parsing in spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/models/index.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_sm\")\nnlp.disable_pipe(\"parser\")\nnlp.enable_pipe(\"senter\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Type Imports\nDESCRIPTION: Shows how to handle circular imports in type hints using TYPE_CHECKING conditional import.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom typing TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .language import Language\n\ndef load_model(name: str) -> \"Language\":\n    ...\n```\n\n----------------------------------------\n\nTITLE: Binary Classification Examples\nDESCRIPTION: JSON examples for binary classification showing positive and negative cases for spam detection.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_41\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"text\": \"You won the lottery! Wire a fee of 200$ to be able to withdraw your winnings.\",\n    \"answer\": \"POS\"\n  },\n  {\n    \"text\": \"Your order #123456789 has arrived\",\n    \"answer\": \"NEG\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Cohere API\nDESCRIPTION: Shell command to set required environment variable for using Cohere API with spaCy\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_55\n\nLANGUAGE: shell\nCODE:\n```\nexport CO_API_KEY=\"...\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Coref Architecture in spaCy\nDESCRIPTION: Example configuration for the spaCy experimental Coref model architecture. It defines parameters for a coreference resolution model including embedding size, dropout rate, network dimensions, and antecedent handling settings. The model uses a transformer for token vector representation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_30\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy-experimental.Coref.v1\"\ndistance_embedding_size = 20\ndropout = 0.3\nhidden_size = 1024\ndepth = 2\nantecedent_limit = 50\nantecedent_batch_size = 512\n\n[model.tok2vec]\n@architectures = \"spacy-transformers.TransformerListener.v1\"\ngrad_factor = 1.0\nupstream = \"transformer\"\npooling = {\"@layers\":\"reduce_mean.v1\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Descriptive Component Names in Python\nDESCRIPTION: Demonstrates the importance of choosing specific and descriptive names for custom pipeline components to avoid conflicts.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_41\n\nLANGUAGE: diff\nCODE:\n```\n+ name = \"myapp_lemmatizer\"\n- name = \"lemmatizer\"\n```\n\n----------------------------------------\n\nTITLE: Downloading spaCy Language Models in Python\nDESCRIPTION: Demonstrates how to programmatically download spaCy language models using Python code.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nspacy.cli.download(\"en_core_web_sm\")\n```\n\n----------------------------------------\n\nTITLE: Using Token Length - Python\nDESCRIPTION: Shows how to get the length of a token in unicode characters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"Give it back! He pleaded.\")\ntoken = doc[0]\nassert len(token) == 4\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray for Parallel Training\nDESCRIPTION: Commands for installing spaCy with Ray support and running distributed training across multiple workers.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U %%SPACY_PKG_NAME[ray]%%SPACY_PKG_FLAGS\n# Check that the CLI is registered\n$ python -m spacy ray --help\n# Train a pipeline\n$ python -m spacy ray train config.cfg --n-workers 2\n```\n\n----------------------------------------\n\nTITLE: Checking Vector Availability in spaCy\nDESCRIPTION: Shows how to check if a span has an associated word vector.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like apples\")\nassert doc[1:].has_vector\n```\n\n----------------------------------------\n\nTITLE: Checking Key Existence in Python\nDESCRIPTION: Method to check whether a vector entry exists for a given key.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef __contains__(self, key: int) -> bool:\n```\n\n----------------------------------------\n\nTITLE: Cloning a Custom spaCy Project Repository\nDESCRIPTION: Command to clone a spaCy project from a custom Git repository using the spacy project clone command with the --repo option.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy project clone your_project --repo https://github.com/you/repo\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy Transformers Extension\nDESCRIPTION: Command to install the spacy-transformers extension package using pip.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U %%SPACY_PKG_NAME[transformers] %%SPACY_PKG_FLAGS\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for SpanCategorizer in Python\nDESCRIPTION: Example of creating an optimizer for the SpanCategorizer pipeline component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\noptimizer = spancat.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Calculating Lowest Common Ancestor Matrix for a Span in Python using spaCy\nDESCRIPTION: Demonstrates how to compute the lowest common ancestor matrix for a given Span. Returns a matrix with integer indices of ancestors or -1 if no common ancestor is found.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York in Autumn\")\nspan = doc[1:4]\nmatrix = span.get_lca_matrix()\n# array([[0, 0, 0], [0, 1, 2], [0, 2, 2]], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Initializing a Table\nDESCRIPTION: Demonstrates how to initialize a new Table object with optional name and data. Table is a subclass of OrderedDict with additional functionality.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lookups import Table\ndata = {\"foo\": \"bar\", \"baz\": 100}\ntable = Table(name=\"some_table\", data=data)\nassert \"foo\" in table\nassert table[\"foo\"] == \"bar\"\n```\n\n----------------------------------------\n\nTITLE: Summarization Few-Shot Example in YAML\nDESCRIPTION: YAML example showing a few-shot learning case for text summarization of UN description.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\n- text: >-\n    The United Nations, referred to informally as the UN, is an intergovernmental organization whose stated purposes are  to maintain international peace and security, develop friendly relations among nations, achieve international cooperation, and serve as a centre for harmonizing the actions of nations. It is the world's largest international organization. The UN is headquartered on international territory in New York City, and the organization has other offices in Geneva, Nairobi, Vienna, and The Hague, where the International Court of Justice is headquartered.\\n\\n The UN was established after World War II with the aim of preventing future world wars, and succeeded the League of  Nations, which was characterized as ineffective.\n  summary: 'The UN is an international organization that promotes global peace, cooperation, and harmony. Established after WWII, its purpose is to prevent future world wars.'\n```\n\n----------------------------------------\n\nTITLE: Counting Left Dependencies in spaCy\nDESCRIPTION: Demonstrates how to count the number of left dependencies of a span.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York in Autumn.\")\nassert doc[3:7].n_lefts == 1\n```\n\n----------------------------------------\n\nTITLE: Using xfail to mark expected test failures in Python\nDESCRIPTION: Shows how to mark tests that should pass but currently fail with pytest's xfail decorator, adding a reason to explain why the test is expected to fail.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.xfail(reason=\"Issue #225 - not yet implemented\")\ndef test_en_tokenizer_splits_em_dash_infix(en_tokenizer):\n    doc = en_tokenizer(\"Will this road take me to Puddleton?\\u2014No.\")\n    assert doc[8].text == \"\\u2014\"\n```\n\n----------------------------------------\n\nTITLE: NER System Comparison Table in Markdown\nDESCRIPTION: Markdown table comparing named entity recognition accuracy between spaCy RoBERTa, Stanza, and Flair on OntoNotes 5.0 and CoNLL '03 datasets\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/_benchmarks-models.mdx#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Named Entity Recognition System  | OntoNotes | CoNLL '03 |\n| -------------------------------- | --------: | --------: |\n| spaCy RoBERTa (2020)             |      89.8 |      91.6 |\n| Stanza (StanfordNLP)<sup>1</sup> |      88.8 |      92.1 |\n| Flair<sup>2</sup>                |      89.7 |      93.1 |\n```\n\n----------------------------------------\n\nTITLE: Project Clone Examples\nDESCRIPTION: Examples demonstrating how to clone a default spaCy project template and from a custom repository.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_44\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project clone pipelines/ner_wikiner\n\n$ python -m spacy project clone template --repo https://github.com/your_org/your_repo\n```\n\n----------------------------------------\n\nTITLE: Table Formatting Examples\nDESCRIPTION: Examples of table formatting in both Markdown and JSX syntax, including special formatting for divider rows.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n| Header 1 | Header 2 |\n| -------- | -------- |\n| Column 1 | Column 2 |\n```\n\nLANGUAGE: markup\nCODE:\n```\n<Table>\n    <Tr><Th>Header 1</Th><Th>Header 2</Th></Tr></thead>\n    <Tr><Td>Column 1</Td><Td>Column 2</Td></Tr>\n</Table>\n```\n\n----------------------------------------\n\nTITLE: Saving CuratedTransformer to Disk\nDESCRIPTION: Serialize the transformer component to disk storage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/curatedtransformer.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntrf = nlp.add_pipe(\"curated_transformer\")\ntrf.to_disk(\"/path/to/transformer\")\n```\n\n----------------------------------------\n\nTITLE: Updating Config Files with Fill-Config Command\nDESCRIPTION: Command to update a spaCy v3.3 configuration file to include new v3.4 settings using the fill-config CLI command.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-4.mdx#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init fill-config config-v3.3.cfg config-v3.4.cfg\n```\n\n----------------------------------------\n\nTITLE: Using displaCy.parse_ents for Manual Entity Parsing\nDESCRIPTION: Example showing how to generate named entity data in manual format for custom visualization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"But Google is starting from behind.\")\nents_parse = displacy.parse_ents(doc)\nhtml = displacy.render(ents_parse, style=\"ent\", manual=True)\n```\n\n----------------------------------------\n\nTITLE: Switching to Non-Trainable Lemmatizer in spaCy\nDESCRIPTION: This code shows how to replace a trainable lemmatizer with a non-trainable one in a spaCy pipeline, using a German language model as an example.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/models/index.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"de_core_web_sm\")\nnlp.remove_pipe(\"lemmatizer\")\nnlp.add_pipe(\"lemmatizer\").initialize()\n```\n\n----------------------------------------\n\nTITLE: Deserializing Table from Bytes\nDESCRIPTION: Demonstrates how to load a Table object from a serialized bytestring.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntable_bytes = table.to_bytes()\ntable = Table()\ntable.from_bytes(table_bytes)\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanRuler Initialization in INI\nDESCRIPTION: Example of a configuration file (config.cfg) showing how to set up the SpanRuler component with patterns loaded from a JSON file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n### config.cfg\n[initialize.components.span_ruler]\n\n[initialize.components.span_ruler.patterns]\n@readers = \"srsly.read_jsonl.v1\"\npath = \"corpus/span_ruler_patterns.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Using spaCy Executable PEX File in Bash\nDESCRIPTION: Examples of how to use a spaCy executable .pex file for running Python scripts or spaCy CLI commands. This method allows for easy deployment of spaCy with all its dependencies.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ ./spacy.pex my_script.py\n$ ./spacy.pex -m spacy info\n```\n\n----------------------------------------\n\nTITLE: Project Clone Command\nDESCRIPTION: Command to clone a project template from a Git repository, with options for specifying repository source, branch, and using sparse checkout.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_43\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project clone [name] [dest] [--repo] [--branch] [--sparse]\n```\n\n----------------------------------------\n\nTITLE: Downloading Project Assets\nDESCRIPTION: Commands to navigate to project directory and download required assets.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ cd some_example_project\n$ python -m spacy project assets\n```\n\n----------------------------------------\n\nTITLE: Implementing Scoring Method\nDESCRIPTION: Implementation of the score method that calculates precision, recall, and F-score metrics for model evaluation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef score(self, examples: Iterable[Example]) -> Dict[str, Any]:\n    prf = PRFScore()\n    for example in examples:\n        ...\n\n    return {\n        \"rel_micro_p\": prf.precision,\n        \"rel_micro_r\": prf.recall,\n        \"rel_micro_f\": prf.fscore,\n    }\n```\n\n----------------------------------------\n\nTITLE: Using project pull command with spaCy\nDESCRIPTION: Example of how to pull project outputs from a remote storage named 'local' using the spaCy CLI.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project pull local\n```\n\n----------------------------------------\n\nTITLE: Using Wildcard Token Patterns in spaCy\nDESCRIPTION: Demonstrates how to use an empty dictionary as a wildcard pattern to match any token in a sequence. Useful for matching patterns where some token contents are unknown but the context is known.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n[{\"ORTH\": \"User\"}, {\"ORTH\": \"name\"}, {\"ORTH\": \":\"},{}]\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy Tok2VecListener Model Architecture\nDESCRIPTION: Example configuration for the spaCy Tok2VecListener model architecture. It demonstrates how to set up a listener that connects to a shared Tok2Vec component in the pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[components.tok2vec]\nfactory = \"tok2vec\"\n\n[components.tok2vec.model]\n@architectures = \"spacy.HashEmbedCNN.v2\"\nwidth = 342\n\n[components.tagger]\nfactory = \"tagger\"\n\n[components.tagger.model]\n@architectures = \"spacy.Tagger.v2\"\n\n[components.tagger.model.tok2vec]\n@architectures = \"spacy.Tok2VecListener.v1\"\nwidth = ${components.tok2vec.model.width}\n```\n\n----------------------------------------\n\nTITLE: Declaring Python Package Dependencies\nDESCRIPTION: Lists the required Python packages with version constraints. Jinja2 templating engine version 3.1.0 or higher is required, along with the srsly package for data serialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/setup/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\njinja2>=3.1.0\nsrsly\n```\n\n----------------------------------------\n\nTITLE: Configuring Translation Task with Few-Shot Examples in INI\nDESCRIPTION: This INI configuration snippet shows how to set up the Translation task with few-shot learning examples. It specifies the task, target language, and the path to the examples file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_12\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.Translation.v1\"\ntarget_lang = \"Spanish\"\n[components.llm.task.examples]\n@misc = \"spacy.FewShotReader.v1\"\npath = \"translation_examples.yml\"\n```\n\n----------------------------------------\n\nTITLE: Initializing AcronymComponent with Problematic Data Handling in Python\nDESCRIPTION: This snippet shows an initial attempt to create an AcronymComponent factory, which has the issue of including data directly in the config file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n@Language.factory(\"acronyms\", default_config={\"data\": {}, \"case_sensitive\": False})\ndef create_acronym_component(nlp: Language, name: str, data: Dict[str, str], case_sensitive: bool):\n    # 🚨 Problem: data ends up in the config file\n    return AcronymComponent(nlp, data, case_sensitive)\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields During Serialization in Python with spaCy\nDESCRIPTION: This snippet demonstrates how to exclude specific serialization fields when saving the coreference resolver to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndata = coref.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Iterating Over StringStore in Python\nDESCRIPTION: Use a for loop to iterate over all strings in the StringStore.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/stringstore.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstringstore = StringStore([\"apple\", \"orange\"])\nall_strings = [s for s in stringstore]\nassert all_strings == [\"apple\", \"orange\"]\n```\n\n----------------------------------------\n\nTITLE: Loading SpanRuler Patterns from Disk\nDESCRIPTION: Demonstrates how to load SpanRuler patterns from a directory on disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"span_ruler\")\nruler.from_disk(\"/path/to/span_ruler\")\n```\n\n----------------------------------------\n\nTITLE: Using Replacement Functions for Transformer Listeners\nDESCRIPTION: Code showing how replace_listeners uses model attributes to make appropriate changes to the transformer listener configuration and model. This ensures the correct architecture is maintained when replacing listeners.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Listeners.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nreplace_func = tok2vec_model.attrs[\"replace_listener_cfg\"]\nnew_config = replace_func(tok2vec_cfg[\"model\"], pipe_cfg[\"model\"][\"tok2vec\"])\n...\nnew_model = tok2vec_model.attrs[\"replace_listener\"](new_model)\n```\n\n----------------------------------------\n\nTITLE: Initializing Acronym Component with JSON Data in INI Format\nDESCRIPTION: This snippet demonstrates how to configure the initialization of the acronym component with JSON data in the config.cfg file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_30\n\nLANGUAGE: ini\nCODE:\n```\n[initialize.components.my_component]\n\n[initialize.components.my_component.data]\n# ✅ This only runs on initialization\n@readers = \"srsly.read_json.v1\"\npath = \"/path/to/slang_dict.json\"\n```\n\n----------------------------------------\n\nTITLE: Detecting Jupyter Notebook Environment\nDESCRIPTION: Checks if the user is running spaCy from a Jupyter notebook by detecting the IPython kernel. Used primarily for visualization features like displacy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nhtml = \"<h1>Hello world!</h1>\"\nif util.is_in_jupyter():\n    from IPython.core.display import display, HTML\n    display(HTML(html))\n```\n\n----------------------------------------\n\nTITLE: Serializing Vocab State to Bytes (Python)\nDESCRIPTION: Shows how to serialize the current state of the Vocab object to a binary string using the to_bytes method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nvocab_bytes = nlp.vocab.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Array Iteration Pattern in Cython\nDESCRIPTION: Shows the preferred way to iterate over C arrays in Cython using slice notation with explicit length specification. Demonstrates type inference and nogil function declaration.\nSOURCE: https://github.com/explosion/spacy/blob/master/CONTRIBUTING.md#2025-04-21_snippet_5\n\nLANGUAGE: cython\nCODE:\n```\ncdef int c_total(const int* int_array, int length) nogil:\n    total = 0\n    for item in int_array[:length]:\n        total += item\n    return total\n```\n\n----------------------------------------\n\nTITLE: Serializing Morphologizer to Disk in Python\nDESCRIPTION: Shows how to serialize the Morphologizer pipe to disk, saving its data to a specified directory path.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphologizer.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmorphologizer = nlp.add_pipe(\"morphologizer\")\nmorphologizer.to_disk(\"/path/to/morphologizer\")\n```\n\n----------------------------------------\n\nTITLE: Improving Dictionary Type Hints in Python\nDESCRIPTION: Demonstrates how to use more specific type hints for dictionary arguments, replacing generic 'dict' with explicit 'Dict[str, Any]'.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_7\n\nLANGUAGE: diff\nCODE:\n```\n- def func(some_arg: dict) -> None:\n+ def func(some_arg: Dict[str, Any]) -> None:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Defining DocBin msgpack Structure\nDESCRIPTION: Shows the internal structure of the msgpack object used by DocBin for serialization, including version, attributes, tokens, spaces, lengths and strings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/docbin.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"version\": str,           # DocBin version number\n    \"attrs\": List[uint64],    # e.g. [TAG, HEAD, ENT_IOB, ENT_TYPE]\n    \"tokens\": bytes,          # Serialized numpy uint64 array with the token data\n    \"spaces\": bytes,         # Serialized numpy boolean array with spaces data\n    \"lengths\": bytes,        # Serialized numpy int32 array with the doc lengths\n    \"strings\": List[str]     # List of unique strings in the token data\n}\n```\n\n----------------------------------------\n\nTITLE: Testing for expected errors with pytest.raises in Python\nDESCRIPTION: Demonstrates how to test that a function raises the expected error using pytest.raises context manager, verifying error handling for invalid entity tags.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nwords = [\"a\", \"b\", \"c\", \"d\", \"e\"]\nents = [\"Q-PERSON\", \"I-PERSON\", \"O\", \"I-PERSON\", \"I-GPE\"]\nwith pytest.raises(ValueError):\n    Doc(Vocab(), words=words, ents=ents)\n```\n\n----------------------------------------\n\nTITLE: Updating spaCy Config from v3.1 to v3.2\nDESCRIPTION: Command to update a spaCy v3.1 config file to include new v3.2 settings using the init fill-config command.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-2.mdx#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init fill-config config-v3.1.cfg config-v3.2.cfg\n```\n\n----------------------------------------\n\nTITLE: Migrating Model Download Commands in spaCy v3.0\nDESCRIPTION: Example showing how to update the model download command from using shortcut names like 'en' to using the full model name like 'en_core_web_sm' as required in spaCy v3.0.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_13\n\nLANGUAGE: diff\nCODE:\n```\n- python -m spacy download en\n+ python -m spacy download en_core_web_sm\n```\n\n----------------------------------------\n\nTITLE: Updating spaCy Version Requirements in meta.json\nDESCRIPTION: Example showing how to update the spaCy version requirements in a pipeline's meta.json file to make it compatible with v3.7\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-7.mdx#2025-04-21_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\n- \"spacy_version\": \">=3.6.0,<3.7.0\",\n+ \"spacy_version\": \">=3.6.0,<3.8.0\",\n```\n\n----------------------------------------\n\nTITLE: Serialization Fields Example\nDESCRIPTION: Demonstrates basic usage of serialization with optional field exclusion.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndata = pipe.to_disk(\"/path\")\n```\n\n----------------------------------------\n\nTITLE: Config for Relation Model Architecture\nDESCRIPTION: Configuration snippet for the relation model architecture, specifying the main model and its subcomponents in the spaCy config format.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_18\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"rel_model.v1\"\n\n[model.create_instance_tensor]\n# ...\n\n[model.classification_layer]\n# ...\n```\n\n----------------------------------------\n\nTITLE: Installing Additional Transformer Dependencies\nDESCRIPTION: Command to install extra dependencies for specific transformer models that require SentencePiece tokenizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install transformers[sentencepiece]\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCatCNN.v1 in spaCy\nDESCRIPTION: Example configuration for the TextCatCNN.v1 architecture in spaCy. This model uses a CNN for token vector calculation and mean pooling for classification.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/legacy.mdx#2025-04-21_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.TextCatCNN.v1\"\nexclusive_classes = false\nnO = null\n\n[model.tok2vec]\n@architectures = \"spacy.HashEmbedCNN.v1\"\npretrained_vectors = null\nwidth = 96\ndepth = 4\nembed_size = 2000\nwindow_size = 1\nmaxout_pieces = 3\nsubword_features = true\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCatBOW v2 Architecture\nDESCRIPTION: Configuration for the TextCatBOW v2 model architecture, which adds support for resizable models but uses an erroneous sparse linear layer that only used a small number of allocated parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/legacy.mdx#2025-04-21_snippet_7\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.TextCatBOW.v2\"\nexclusive_classes = false\nngram_size = 1\nno_output_layer = false\nnO = null\n```\n\n----------------------------------------\n\nTITLE: Counting Right Children in spaCy\nDESCRIPTION: Shows how to get the count of rightward immediate children of a token.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like New York in Autumn.\")\nassert doc[3].n_rights == 1\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy with Build Constraints in Shell\nDESCRIPTION: Command to install spaCy using build constraints to specify an older version of numpy during compilation. This helps avoid binary incompatibility issues when numpy is downgraded later.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nPIP_CONSTRAINT=https://raw.githubusercontent.com/explosion/spacy/master/build-constraints.txt \\\npip install spacy --no-cache-dir\n```\n\n----------------------------------------\n\nTITLE: Checking Doc Extensions in spaCy\nDESCRIPTION: Shows how to verify if a specific extension has been registered on the Doc class.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc\nDoc.set_extension(\"has_city\", default=False)\nassert Doc.has_extension(\"has_city\")\n```\n\n----------------------------------------\n\nTITLE: Defining Candidate Attributes Table in Markdown\nDESCRIPTION: This snippet presents a markdown table that defines the attributes of candidate entities in spaCy's knowledge base. It includes the attribute names, their descriptions, and data types.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/kb.mdx#2025-04-21_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n| Name            | Description                                                              |\n| --------------- | ------------------------------------------------------------------------ |\n| `entity`        | The entity's unique KB identifier. ~~int~~                               |\n| `entity_`       | The entity's unique KB identifier. ~~str~~                               |\n| `alias`         | The alias or textual mention. ~~int~~                                    |\n| `alias_`        | The alias or textual mention. ~~str~~                                    |\n| `prior_prob`    | The prior probability of the `alias` referring to the `entity`. ~~long~~ |\n| `entity_freq`   | The frequency of the entity in a typical corpus. ~~long~~                |\n| `entity_vector` | The pretrained vector of the entity. ~~numpy.ndarray~~                   |\n```\n\n----------------------------------------\n\nTITLE: Setting Output Dimension of NER Model\nDESCRIPTION: Demonstrates how to change the output dimension of the NER model. This resizes the model's output layer to accommodate a different number of labels.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\nner.set_output(512)\n```\n\n----------------------------------------\n\nTITLE: Loading SpanCategorizer from Bytes\nDESCRIPTION: Example showing how to load a SpanCategorizer component from a bytestring.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nspancat_bytes = spancat.to_bytes()\nspancat = nlp.add_pipe(\"spancat\")\nspancat.from_bytes(spancat_bytes)\n```\n\n----------------------------------------\n\nTITLE: Initializing Vector Store in Python\nDESCRIPTION: Examples of creating new vector stores using the Vectors class, both empty and pre-populated with data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.vectors import Vectors\n\nempty_vectors = Vectors(shape=(10000, 300))\n\ndata = numpy.zeros((3, 300), dtype='f')\nkeys = [\"cat\", \"dog\", \"rat\"]\nvectors = Vectors(data=data, keys=keys)\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCatEnsemble.v1 in spaCy\nDESCRIPTION: Example configuration for the TextCatEnsemble.v1 architecture in spaCy. This model combines a bag-of-words approach with a neural network using CNN and attention.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/legacy.mdx#2025-04-21_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.TextCatEnsemble.v1\"\nexclusive_classes = false\npretrained_vectors = null\nwidth = 64\nembed_size = 2000\nconv_depth = 2\nwindow_size = 1\nngram_size = 1\ndropout = null\nnO = null\n```\n\n----------------------------------------\n\nTITLE: Pushing to Remote Storage\nDESCRIPTION: Command to push project outputs to configured remote storage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project push\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - Dependency Parsing Benchmarks\nDESCRIPTION: Comparison table showing dependency parsing accuracy metrics (UAS and LAS scores) between spaCy and other parsing systems on the Penn Treebank dataset.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/facts-figures.mdx#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Dependency Parsing System                                                      |  UAS |  LAS |\n| ------------------------------------------------------------------------------ | ---: | ---: |\n| spaCy RoBERTa (2020)                                                           | 95.1 | 93.7 |\n| [Mrini et al.](https://khalilmrini.github.io/Label_Attention_Layer.pdf) (2019) | 97.4 | 96.3 |\n| [Zhou and Zhao](https://www.aclweb.org/anthology/P19-1230/) (2019)             | 97.2 | 95.7 |\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields During Serialization\nDESCRIPTION: Example showing how to exclude specific fields when serializing a transformer model to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndata = trf.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Checking EntityRuler Length in Python\nDESCRIPTION: Demonstrates how to check the number of patterns in an EntityRuler instance using the len() function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"entity_ruler\")\nassert len(ruler) == 0\nruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\nassert len(ruler) == 1\n```\n\n----------------------------------------\n\nTITLE: Configuring a Tok2VecListener for NER Component\nDESCRIPTION: Configuration for setting up a Tok2VecListener in the NER component that references the upstream Tok2Vec component. This allows the NER component to use embeddings produced by the shared Tok2Vec component.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Listeners.md#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[components.ner.model.tok2vec]\n@architectures = \"spacy.Tok2VecListener.v1\"\nupstream = \"tok2vec\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Predicted Annotations in spaCy Training Pipeline\nDESCRIPTION: Configuration snippet showing how to set up the parser predictions to be used as features for the tagger component during training. Demonstrates setting up MultiHashEmbed architecture with DEP attributes.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-1.mdx#2025-04-21_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[nlp]\npipeline = [\"parser\", \"tagger\"]\n\n[components.tagger.model.tok2vec.embed]\n@architectures = \"spacy.MultiHashEmbed.v1\"\nwidth = ${components.tagger.model.tok2vec.encode.width}\nattrs = [\"NORM\",\"DEP\"]\nrows = [5000,2500]\ninclude_static_vectors = false\n\n[training]\nannotating_components = [\"parser\"]\n```\n\n----------------------------------------\n\nTITLE: Updating spaCy Version Requirements in meta.json\nDESCRIPTION: Shows how to update the spaCy version requirements in the meta.json file for custom pipelines.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-3.mdx#2025-04-21_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n- \"spacy_version\": \">=3.2.0,<3.3.0\",\n+ \"spacy_version\": \">=3.2.0,<3.4.0\",\n```\n\n----------------------------------------\n\nTITLE: Configuring Overwrite Setting in spaCy Pipeline\nDESCRIPTION: Example of setting the overwrite option for a tagger component in the spaCy config file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-2.mdx#2025-04-21_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[components.tagger]\nfactory = \"tagger\"\noverwrite = false\n```\n\n----------------------------------------\n\nTITLE: Pushing Project Outputs to Remote Storage\nDESCRIPTION: Command to upload project outputs to remote storage with content-based versioning and compression. Supports various cloud storage protocols through cloudpathlib.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_47\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project push [remote] [project_dir]\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy project push my_bucket\n```\n\nLANGUAGE: yaml\nCODE:\n```\n### project.yml\nremotes:\n  my_bucket: 's3://my-spacy-bucket'\n```\n\n----------------------------------------\n\nTITLE: Checking for Trainable Lemmatizer in spaCy Pipeline\nDESCRIPTION: This snippet demonstrates how to check if the lemmatizer in a spaCy pipeline is trainable, using a German language model as an example.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/models/index.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"de_core_web_sm\")\nassert nlp.get_pipe(\"lemmatizer\").is_trainable\n```\n\n----------------------------------------\n\nTITLE: Prodigy Integration Configuration\nDESCRIPTION: YAML configuration for integrating Prodigy annotation tool with spaCy, including workflow definitions for data processing and model training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_34\n\nLANGUAGE: yaml\nCODE:\n```\nvars:\n  prodigy:\n    train_dataset: \"fashion_brands_training\"\n    eval_dataset: \"fashion_brands_eval\"\n\nworkflows:\n  all:\n    - data-to-spacy\n    - train_spacy\n\ncommands:\n  - name: \"data-to-spacy\"\n    help: \"Merge your annotations and create data in spaCy's binary format\"\n    script:\n      - \"python -m prodigy data-to-spacy corpus/ --ner ${vars.prodigy.train_dataset},eval:${vars.prodigy.eval_dataset}\"\n    outputs:\n      - \"corpus/train.spacy\"\n      - \"corpus/dev.spacy\"\n  - name: \"train_spacy\"\n    help: \"Train a named entity recognition model with spaCy\"\n    script:\n      - \"python -m spacy train configs/config.cfg --output training/ --paths.train corpus/train.spacy --paths.dev corpus/dev.spacy\"\n    deps:\n      - \"corpus/train.spacy\"\n      - \"corpus/dev.spacy\"\n    outputs:\n      - \"training/model-best\"\n```\n\n----------------------------------------\n\nTITLE: Checking DocBin Length\nDESCRIPTION: Demonstrates how to check the number of Doc objects added to a DocBin instance.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/docbin.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc_bin = DocBin(attrs=[\"LEMMA\"])\ndoc = nlp(\"This is a document to serialize.\")\ndoc_bin.add(doc)\nassert len(doc_bin) == 1\n```\n\n----------------------------------------\n\nTITLE: Ignoring Linter Rules in Python\nDESCRIPTION: Examples of how to ignore specific linter rules using '# noqa' comments. This is useful for cases where linter warnings are not applicable or need to be suppressed for valid reasons.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# The imported class isn't used in this file, but imported here, so it can be\n# imported *from* here by another module.\nfrom .submodule import SomeClass  # noqa: F401\n\ntry:\n    do_something()\nexcept:  # noqa: E722\n    # This bare except is justified, for some specific reason\n    do_something_else()\n```\n\n----------------------------------------\n\nTITLE: Installing DVC with Git Integration\nDESCRIPTION: Commands for setting up DVC (Data Version Control) with Git in a spaCy project environment. Initializes both Git and DVC repositories.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install dvc   # Install DVC\n$ git init          # Initialize a Git repo\n$ dvc init         # Initialize a DVC project\n```\n\n----------------------------------------\n\nTITLE: Updated Lemmatizer Initialization\nDESCRIPTION: Example showing the new way to initialize a custom Lemmatizer using Lookups class instead of direct dictionaries\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-2.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lemmatizer import Lemmatizer\nfrom spacy.lookups import Lookups\n\nlemma_index = {\"verb\": (\"cope\", \"cop\")}\nlemma_exc = {\"verb\": {\"coping\": (\"cope\",)}}\nlemma_rules = {\"verb\": [[\"ing\", \"\"]]}\nlookups = Lookups()\nlookups.add_table(\"lemma_index\", lemma_index)\nlookups.add_table(\"lemma_exc\", lemma_exc)\nlookups.add_table(\"lemma_rules\", lemma_rules)\nlemmatizer = Lemmatizer(lookups)\n```\n\n----------------------------------------\n\nTITLE: Exporting Tag Maps from SpaCy Models\nDESCRIPTION: Demonstrates how to export a tag map from a provided model to JSON format, converting internal integer IDs to strings for compatibility and readability.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-3.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nimport srsly\n\nnlp = spacy.load(\"en_core_web_sm\")\ntag_map = {}\n\n# convert any integer IDs to strings for JSON\nfor tag, morph in nlp.vocab.morphology.tag_map.items():\n    tag_map[tag] = {}\n    for feat, val in morph.items():\n        feat = nlp.vocab.strings.as_string(feat)\n        if not isinstance(val, bool):\n            val = nlp.vocab.strings.as_string(val)\n        tag_map[tag][feat] = val\n\nsrsly.write_json(\"tag_map.json\", tag_map)\n```\n\n----------------------------------------\n\nTITLE: Using StringStore in spaCy\nDESCRIPTION: This example demonstrates how to use spaCy's StringStore to convert between strings and hash values. It shows that strings must be explicitly added to the store before retrieving them by hash, and illustrates how to iterate through all keys in the StringStore.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/StringStore-Vocab.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.strings import StringStore\n\nss = StringStore()\nhashval = ss[\"spacy\"] # 10639093010105930009\ntry:\n    # this won't work\n    ss[hashval]\nexcept KeyError:\n    print(f\"key {hashval} unknown in the StringStore.\")\n\nss.add(\"spacy\")\nassert ss[hashval] == \"spacy\" # it works now\n\n# There is no `.keys` property, but you can iterate over keys\n# The empty string will never be in the list of keys\nfor key in ss:\n    print(key)\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Neural Network in Python\nDESCRIPTION: Creates a simple PyTorch neural network with linear layers, ReLU activation, dropout, and softmax output.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import nn\n\ntorch_model = nn.Sequential(\n    nn.Linear(width, hidden_width),\n    nn.ReLU(),\n    nn.Dropout2d(dropout),\n    nn.Linear(hidden_width, nO),\n    nn.ReLU(),\n    nn.Dropout2d(dropout),\n    nn.Softmax(dim=1)\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Vectors from Disk\nDESCRIPTION: Demonstrates loading vector data from disk storage.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nvectors = Vectors(StringStore())\nvectors.from_disk(\"/path/to/vectors\")\n```\n\n----------------------------------------\n\nTITLE: PhraseMatcher with Custom Token Attributes in Python\nDESCRIPTION: Demonstrates using PhraseMatcher with different token attributes like POS for flexible matching.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-1.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmatcher = PhraseMatcher(nlp.vocab, attr=\"POS\")\nmatcher.add(\"PATTERN\", None, nlp(\"I love cats\"))\ndoc = nlp(\"You like dogs\")\nmatches = matcher(doc)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sentence Spans in INI\nDESCRIPTION: Configuration example for using sent_spans.v1 span getter in the transformer model.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_19\n\nLANGUAGE: ini\nCODE:\n```\n[transformer.model.get_spans]\n@span_getters = \"spacy-transformers.sent_spans.v1\"\n```\n\n----------------------------------------\n\nTITLE: Updating CLI Training Command Syntax\nDESCRIPTION: Demonstrates the new syntax for specifying pipeline components in the spacy train command using comma-separated lists instead of individual flags.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-1.mdx#2025-04-21_snippet_9\n\nLANGUAGE: diff\nCODE:\n```\n- $ spacy train en /output train_data.json dev_data.json --no-parser\n+ $ spacy train en /output train_data.json dev_data.json --pipeline tagger,ner\n```\n\n----------------------------------------\n\nTITLE: Running Specific Tests in Bash\nDESCRIPTION: Commands for running tests in specific directories, files, or individual test functions using pytest. Demonstrates the path syntax for targeting different scopes of tests.\nSOURCE: https://github.com/explosion/spacy/blob/master/spacy/tests/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npy.test spacy/tests/tokenizer  # run all tests in directory\npy.test spacy/tests/tokenizer/test_exceptions.py # run all tests in file\npy.test spacy/tests/tokenizer/test_exceptions.py::test_tokenizer_handles_emoji # run specific test\n```\n\n----------------------------------------\n\nTITLE: Copying SpanGroup in Python\nDESCRIPTION: Shows how to create a copy of a SpanGroup object.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spangroup.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import SpanGroup\n\ndoc = nlp(\"Their goi ng home\")\ndoc.spans[\"errors\"] = [doc[1:3], doc[0:3]]\nnew_group = doc.spans[\"errors\"].copy()\n```\n\n----------------------------------------\n\nTITLE: Creating Infoboxes\nDESCRIPTION: Examples of creating different types of infoboxes using JSX syntax.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_11\n\nLANGUAGE: jsx\nCODE:\n```\n<Infobox title=\"Information\">Regular infobox</Infobox>\n<Infobox title=\"Important note\" variant=\"warning\">This is a warning.</Infobox>\n<Infobox title=\"Be careful!\" variant=\"danger\">This is dangerous.</Infobox>\n```\n\n----------------------------------------\n\nTITLE: Initializing InMemoryLookupKB\nDESCRIPTION: Creates a new instance of InMemoryLookupKB with a vocabulary and specified entity vector length.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/inmemorylookupkb.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.kb import InMemoryLookupKB\nvocab = nlp.vocab\nkb = InMemoryLookupKB(vocab=vocab, entity_vector_length=64)\n```\n\n----------------------------------------\n\nTITLE: Tokenization Check Example\nDESCRIPTION: Shows how to verify spaCy's tokenization of complex text to ensure pattern matching will work as expected.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"A complex-example,!\")\nprint([token.text for token in doc])\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy Language Models via pip with URLs\nDESCRIPTION: Shows various methods to install spaCy language models using pip with direct URLs or local files.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n# With external URL\n$ pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl\n$ pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n\n# Using spacy info to get the external URL\n$ pip install $(spacy info en_core_web_sm --url)\n\n# With local file\n$ pip install /Users/you/en_core_web_sm-3.0.0-py3-none-any.whl\n$ pip install /Users/you/en_core_web_sm-3.0.0.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Processing Document Stream with SpanResolver\nDESCRIPTION: How to process a stream of documents using the pipe method of SpanResolver. This approach allows batch processing with a configurable batch size for efficiency.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\nfor doc in span_resolver.pipe(docs, batch_size=50):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Retrieving Pointers from Different Data Structures in Cython\nDESCRIPTION: Demonstrates how to extract C pointers from numpy arrays, C++ vectors, and memory views for efficient data access in nogil functions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncdef void get_pointers(np.ndarray[int, mode='c'] numpy_array, vector[int] cpp_vector, int[::1] memory_view) nogil:\n    pointer1 = <int*>numpy_array.data\n    pointer2 = cpp_vector.data()\n    pointer3 = &memory_view[0]\n```\n\n----------------------------------------\n\nTITLE: Parametrizing tests with multiple arguments in Python\nDESCRIPTION: Demonstrates parametrizing a test with multiple arguments using pytest, showing how to test tokenization with different expected output lengths while using a fixture (en_tokenizer).\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize(\"text,expected_len\", [(\"hello world\", 2), (\"I can't!\", 4)])\ndef test_token_length(en_tokenizer, text, expected_len):  # en_tokenizer is a fixture\n    doc = en_tokenizer(text)\n    assert len(doc) == expected_len\n```\n\n----------------------------------------\n\nTITLE: Error: No Compatible Model Found - Python\nDESCRIPTION: Error message shown when attempting to download a trained pipeline that either doesn't exist or isn't compatible with the installed spaCy version.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nNo compatible package found for [lang] (spaCy vX.X.X).\n```\n\n----------------------------------------\n\nTITLE: Installing spacy-llm Package\nDESCRIPTION: Command to install the spacy-llm package using pip. This package integrates Large Language Models into spaCy pipelines.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install spacy-llm\n```\n\n----------------------------------------\n\nTITLE: Defining Training Configuration Parameters in spaCy (Markdown)\nDESCRIPTION: This code snippet is a markdown table that defines and explains various configuration parameters for training in spaCy. It includes settings such as accumulate_gradient, batcher, dropout rate, evaluation frequency, and optimizer settings among others.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n| Name                                                 | Description                                                                                                                                                                                                                                                                                                                         |\n| ---------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `accumulate_gradient`                                | Whether to divide the batch up into substeps. Defaults to `1`. ~~int~~                                                                                                                                                                                                                                                              |\n| `batcher`                                            | Callable that takes an iterator of [`Doc`](/api/doc) objects and yields batches of `Doc`s. Defaults to [`batch_by_words`](/api/top-level#batch_by_words). ~~Callable[[Iterator[Doc], Iterator[List[Doc]]]]~~                                                                                                                        |\n| `before_to_disk`                                     | Optional callback to modify `nlp` object right before it is saved to disk during and after training. Can be used to remove or reset config values or disable components. Defaults to `null`. ~~Optional[Callable[[Language], Language]]~~                                                                                           |\n| `before_update` <Tag variant=\"new\">3.5</Tag>         | Optional callback that is invoked at the start of each training step with the `nlp` object and a `Dict` containing the following entries: `step`, `epoch`. Can be used to make deferred changes to components. Defaults to `null`. ~~Optional[Callable[[Language, Dict[str, Any]], None]]~~                                         |\n| `dev_corpus`                                         | Dot notation of the config location defining the dev corpus. Defaults to `corpora.dev`. ~~str~~                                                                                                                                                                                                                                     |\n| `dropout`                                            | The dropout rate. Defaults to `0.1`. ~~float~~                                                                                                                                                                                                                                                                                      |\n| `eval_frequency`                                     | How often to evaluate during training (steps). Defaults to `200`. ~~int~~                                                                                                                                                                                                                                                           |\n| `frozen_components`                                  | Pipeline component names that are \"frozen\" and shouldn't be initialized or updated during training. See [here](/usage/training#config-components) for details. Defaults to `[]`. ~~List[str]~~                                                                                                                                      |\n| `annotating_components` <Tag variant=\"new\">3.1</Tag> | Pipeline component names that should set annotations on the predicted docs during training. See [here](/usage/training#annotating-components) for details. Defaults to `[]`. ~~List[str]~~                                                                                                                                          |\n| `gpu_allocator`                                      | Library for cupy to route GPU memory allocation to. Can be `\"pytorch\"` or `\"tensorflow\"`. Defaults to variable `${system.gpu_allocator}`. ~~str~~                                                                                                                                                                                   |\n| `logger`                                             | Callable that takes the `nlp` and stdout and stderr `IO` objects, sets up the logger, and returns two new callables to log a training step and to finalize the logger. Defaults to [`ConsoleLogger`](/api/top-level#ConsoleLogger). ~~Callable[[Language, IO, IO], [Tuple[Callable[[Dict[str, Any]], None], Callable[[], None]]]]~~ |\n| `max_epochs`                                         | Maximum number of epochs to train for. `0` means an unlimited number of epochs. `-1` means that the train corpus should be streamed rather than loaded into memory with no shuffling within the training loop. Defaults to `0`. ~~int~~                                                                                             |\n| `max_steps`                                          | Maximum number of update steps to train for. `0` means an unlimited number of steps. Defaults to `20000`. ~~int~~                                                                                                                                                                                                                   |\n| `optimizer`                                          | The optimizer. The learning rate schedule and other settings can be configured as part of the optimizer. Defaults to [`Adam`](https://thinc.ai/docs/api-optimizers#adam). ~~Optimizer~~                                                                                                                                             |\n| `patience`                                           | How many steps to continue without improvement in evaluation score. `0` disables early stopping. Defaults to `1600`. ~~int~~                                                                                                                                                                                                        |\n| `score_weights`                                      | Score names shown in metrics mapped to their weight towards the final weighted score. See [here](/usage/training#metrics) for details. Defaults to `{}`. ~~Dict[str, float]~~                                                                                                                                                       |\n| `seed`                                               | The random seed. Defaults to variable `${system.seed}`. ~~int~~                                                                                                                                                                                                                                                                     |\n| `train_corpus`                                       | Dot notation of the config location defining the train corpus. Defaults to `corpora.train`. ~~str~~                                                                                                                                                                                                                                 |\n```\n\n----------------------------------------\n\nTITLE: JSONL Example Data Format\nDESCRIPTION: Example of JSONL format used for raw text corpus data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/corpus.mdx#2025-04-21_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"text\": \"Can I ask where you work now and what you do, and if you enjoy it?\"}\n{\"text\": \"They may just pull out of the Seattle market completely, at least until they have autonomous vehicles.\"}\n{\"text\": \"My cynical view on this is that it will never be free to the public. Reason: what would be the draw of joining the military? Right now their selling point is free Healthcare and Education. Ironically both are run horribly and most, that I've talked to, come out wishing they never went in.\"}\n```\n\n----------------------------------------\n\nTITLE: Installing spacy-experimental Package\nDESCRIPTION: Command to install the spacy-experimental package that contains the CoreferenceResolver component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/coref.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U spacy-experimental\n```\n\n----------------------------------------\n\nTITLE: Executing Config Update Command\nDESCRIPTION: Command to update a spaCy v3.2 config file to v3.3 settings using the init fill-config command.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-3.mdx#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init fill-config config-v3.2.cfg config-v3.3.cfg\n```\n\n----------------------------------------\n\nTITLE: Installing Floret for Vector Integration\nDESCRIPTION: Command to install the Floret package for compact, full-coverage vectors in spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-2.mdx#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install floret\n```\n\n----------------------------------------\n\nTITLE: Error: spaCy Command Not Found - Bash\nDESCRIPTION: Command line error when the spaCy command is not recognized, typically due to PATH environment settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncommand not found: spacy\n```\n\n----------------------------------------\n\nTITLE: Finding Tokens by Start Character Position in spaCy\nDESCRIPTION: Demonstrates how to use the token_by_start function to find a token in a TokenC array by its first character offset. The function returns the index of the token or -1 if not found.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-structs.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens.doc cimport Doc, token_by_start\nfrom spacy.vocab cimport Vocab\n\ndoc = Doc(Vocab(), words=[\"hello\", \"world\"])\nassert token_by_start(doc.c, doc.length, 6) == 1\nassert token_by_start(doc.c, doc.length, 4) == -1\n```\n\n----------------------------------------\n\nTITLE: Migrating Packaging Commands\nDESCRIPTION: Comparison showing the simplified pipeline packaging process in v3, which now automatically builds the installable package.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_30\n\nLANGUAGE: diff\nCODE:\n```\npython -m spacy package ./output ./packages\n- cd /output/en_pipeline-0.0.0\n- python setup.py sdist\n```\n\n----------------------------------------\n\nTITLE: Converting Dictionary to FEATS String in Python\nDESCRIPTION: Static method that converts a dictionary of features and values to a Universal Dependencies FEATS string representation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.morphology import Morphology\nf = Morphology.dict_to_feats({\"Feat1\": \"Val1\", \"Feat2\": \"Val2\"})\nassert f == \"Feat1=Val1|Feat2=Val2\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Example Object in spaCy (Python)\nDESCRIPTION: Creates an Example object from predicted and reference documents with word tokens, spaces, and tags. Shows basic construction of training instance.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/example.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc\nfrom spacy.training import Example\npred_words = [\"Apply\", \"some\", \"sunscreen\"]\npred_spaces = [True, True, False]\ngold_words = [\"Apply\", \"some\", \"sun\", \"screen\"]\ngold_spaces = [True, True, False, False]\ngold_tags = [\"VERB\", \"DET\", \"NOUN\", \"NOUN\"]\npredicted = Doc(nlp.vocab, words=pred_words, spaces=pred_spaces)\nreference = Doc(nlp.vocab, words=gold_words, spaces=gold_spaces, tags=gold_tags)\nexample = Example(predicted, reference)\n```\n\n----------------------------------------\n\nTITLE: Displaying spaCy Badge in Markdown\nDESCRIPTION: Shows how to include a 'Built with spaCy' badge in Markdown format, which can be used in project documentation or README files to indicate the use of spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/spacy-101.mdx#2025-04-21_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n[![Built with spaCy](https://img.shields.io/badge/built%20with-spaCy-09a3d5.svg)](https://spacy.io)\n```\n\n----------------------------------------\n\nTITLE: Installing spacy-experimental Package\nDESCRIPTION: Command to install the spacy-experimental package which contains the SpanResolver component. This package is required to use the experimental_span_resolver component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U spacy-experimental\n```\n\n----------------------------------------\n\nTITLE: Lemmatization Usage with Turkish Model\nDESCRIPTION: Example showing lemmatization usage that now requires explicit installation of lookups data\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-2.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnlp = Turkish()\ndoc = nlp(\"Bu bir cümledir.\")\n# 🚨 This now requires the lookups data to be installed explicitly\nprint([token.lemma_ for token in doc])\n```\n\n----------------------------------------\n\nTITLE: Debug Config with Full Options\nDESCRIPTION: Extended debug command showing both functions and variables in the configuration\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy debug config ./config.cfg --show-functions --show-variables\n```\n\n----------------------------------------\n\nTITLE: Creating Lists in Markdown and JSX\nDESCRIPTION: Examples of creating ordered lists in both Markdown and JSX formats.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n1. One\n2. Two\n```\n\nLANGUAGE: markup\nCODE:\n```\n<Ol>\n    <Li>One</Li>\n    <Li>Two</Li>\n</Ol>\n```\n\n----------------------------------------\n\nTITLE: Downloading Assets from Git Repository in spaCy project.yml\nDESCRIPTION: Configuration for downloading assets from a Git repository using sparse checkout. Specifies the repository URL, branch, file path, and optional checksum verification.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nassets:\n  - dest: 'assets/training.spacy'\n    git:\n      repo: 'https://github.com/example/repo'\n      branch: 'master'\n      path: 'path/training.spacy'\n    checksum: '63373dd656daa1fd3043ce166a59474c'\n    description: 'The training data (5000 examples)'\n```\n\n----------------------------------------\n\nTITLE: Deserializing StringStore from Bytes in Python\nDESCRIPTION: Use the from_bytes method to load a StringStore state from a binary string.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/stringstore.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.strings import StringStore\nstore_bytes = stringstore.to_bytes()\nnew_store = StringStore().from_bytes(store_bytes)\n```\n\n----------------------------------------\n\nTITLE: Testing for expected warnings in Python\nDESCRIPTION: Shows how to test that functions raise the expected warnings using pytest.warns context manager, verifying warning handling in the PhraseMatcher with validation.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef test_phrase_matcher_validation(en_vocab):\n    doc1 = Doc(en_vocab, words=[\"Test\"], deps=[\"ROOT\"])\n    doc2 = Doc(en_vocab, words=[\"Test\"])\n    matcher = PhraseMatcher(en_vocab, validate=True)\n    with pytest.warns(UserWarning):\n        # Warn about unnecessarily parsed document\n        matcher.add(\"TEST1\", [doc1])\n    with pytest.warns(None) as record:\n        matcher.add(\"TEST2\", [docs])\n        assert not record.list\n```\n\n----------------------------------------\n\nTITLE: Updating spaCy Config Files Using CLI\nDESCRIPTION: Command to update a configuration file from spaCy v3.6 to v3.7 using the init fill-config command\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-7.mdx#2025-04-21_snippet_1\n\nLANGUAGE: cli\nCODE:\n```\n$ python -m spacy init fill-config config-v3.6.cfg config-v3.7.cfg\n```\n\n----------------------------------------\n\nTITLE: Accessing Document Vector Representation in spaCy (Python)\nDESCRIPTION: Shows how to access the vector property of a Doc object, which provides a real-valued meaning representation of the document. By default, this is the average of the token vectors.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like apples\")\nassert doc.vector.dtype == \"float32\"\nassert doc.vector.shape == (300,)\n```\n\n----------------------------------------\n\nTITLE: Button Component Examples\nDESCRIPTION: Implementation of button components with primary and secondary variants.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_4\n\nLANGUAGE: jsx\nCODE:\n```\n<Button to=\"#\" variant=\"primary\">Primary small</Button>\n<Button to=\"#\" variant=\"secondary\">Secondary small</Button>\n```\n\n----------------------------------------\n\nTITLE: Auto-formatting Python Code with Black\nDESCRIPTION: Example of refactoring code to make it more concise and compatible with Black auto-formatting. It demonstrates how to split long lines and improve readability.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\n- range_suggester = registry.misc.get(\"spacy.ngram_range_suggester.v1\")(\n-     min_size=1, max_size=3\n- )\n+ suggester_factory = registry.misc.get(\"spacy.ngram_range_suggester.v1\")\n+ range_suggester = suggester_factory(min_size=1, max_size=3)\n```\n\n----------------------------------------\n\nTITLE: Simple Debug Config Example\nDESCRIPTION: A simple example showing how to debug a configuration file named config.cfg\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy debug config config.cfg\n```\n\n----------------------------------------\n\nTITLE: Configuring Legacy MishWindowEncoder in spaCy\nDESCRIPTION: Configuration for the spacy.MishWindowEncoder.v1 architecture that produces a Model[Floats2D, Floats2D]. Implements context encoding using convolutions with Mish activation, layer normalization and residual connections.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/legacy.mdx#2025-04-21_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.MishWindowEncoder.v1\"\nwidth = 64\nwindow_size = 1\ndepth = 4\n```\n\n----------------------------------------\n\nTITLE: Setting NER Annotations in spaCy\nDESCRIPTION: Shows how to modify Doc objects with pre-computed NER scores. Takes documents and corresponding prediction scores to update entity annotations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nner = nlp.add_pipe(\"ner\")\nscores = ner.predict([doc1, doc2])\nner.set_annotations([doc1, doc2], scores)\n```\n\n----------------------------------------\n\nTITLE: Basic spaCy Config Initialization Command Syntax\nDESCRIPTION: The full command syntax for initializing a spaCy configuration file, showing all available parameters and options.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cli.mdx#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init config [output_file] [--lang] [--pipeline] [--optimize] [--gpu] [--pretraining] [--force]\n```\n\n----------------------------------------\n\nTITLE: Configuring Console Logger\nDESCRIPTION: Configuration for the spaCy console logger that outputs training progress in a tabular format, with options to enable/disable the progress bar.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/legacy.mdx#2025-04-21_snippet_8\n\nLANGUAGE: ini\nCODE:\n```\n[training.logger]\n@loggers = \"spacy.ConsoleLogger.v1\"\nprogress_bar = true\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy train config.cfg\n\nℹ Using CPU\nℹ Loading config and nlp from: config.cfg\nℹ Pipeline: ['tok2vec', 'tagger']\nℹ Start training\nℹ Training. Initial learn rate: 0.0\n\nE     #        LOSS TOK2VEC   LOSS TAGGER   TAG_ACC   SCORE\n---   ------   ------------   -----------   -------   ------\n  0        0           0.00         86.20      0.22     0.00\n  0      200           3.08      18968.78     34.00     0.34\n  0      400          31.81      22539.06     33.64     0.34\n  0      600          92.13      22794.91     43.80     0.44\n  0      800         183.62      21541.39     56.05     0.56\n  0     1000         352.49      25461.82     65.15     0.65\n  0     1200         422.87      23708.82     71.84     0.72\n  0     1400         601.92      24994.79     76.57     0.77\n  0     1600         662.57      22268.02     80.20     0.80\n  0     1800        1101.50      28413.77     82.56     0.83\n  0     2000        1253.43      28736.36     85.00     0.85\n  0     2200        1411.02      28237.53     87.42     0.87\n  0     2400        1605.35      28439.95     88.70     0.89\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCatCNN.v2 in spaCy\nDESCRIPTION: Example configuration for the TextCatCNN.v2 architecture in spaCy. This version is resizable, allowing addition of labels after training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/legacy.mdx#2025-04-21_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.TextCatCNN.v2\"\nexclusive_classes = false\nnO = null\n\n[model.tok2vec]\n@architectures = \"spacy.HashEmbedCNN.v2\"\npretrained_vectors = null\nwidth = 96\ndepth = 4\nembed_size = 2000\nwindow_size = 1\nmaxout_pieces = 3\nsubword_features = true\n```\n\n----------------------------------------\n\nTITLE: Building and Running the spaCy Website with Docker\nDESCRIPTION: Instructions for building a Docker image for the spaCy website and running it in a container. Includes commands for building the image and running the website in development mode.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t spacy-io .\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it \\\n  --rm \\\n  -v $(pwd):/home/node/website \\\n  -p 3000:3000 \\\n  spacy-io \\\n  npm run dev -- -H 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Creating a Doc object with POS tags in Python\nDESCRIPTION: Demonstrates how to construct a Doc object manually with words and part-of-speech tags for testing purposes, avoiding dependencies on other library functionality.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef test_doc_creation_with_pos():\n    doc = Doc(Vocab(), words=[\"hello\", \"world\"], pos=[\"NOUN\", \"VERB\"])\n    assert doc[0].pos_ == \"NOUN\"\n    assert doc[1].pos_ == \"VERB\"\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy with Lookups Data\nDESCRIPTION: Command to install spaCy with additional lookups data package for lemmatization support\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-2.mdx#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install spacy[lookups]\n```\n\n----------------------------------------\n\nTITLE: Building spaCy in Parallel for Developers in Bash\nDESCRIPTION: Commands for building spaCy in parallel using multiple build jobs. This can significantly speed up the compilation process for developers working with the source code.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -r requirements.txt\n$ SPACY_NUM_BUILD_JOBS=4 pip install --no-build-isolation --editable .\n```\n\n----------------------------------------\n\nTITLE: Pipeline Configuration via Standard Input\nDESCRIPTION: Command demonstrating how to pipe a configuration from init config to training, allowing for dynamic config generation without saving to disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init config - --lang en --pipeline ner,textcat --optimize accuracy | python -m spacy train - --paths.train ./corpus/train.spacy --paths.dev ./corpus/dev.spacy\n```\n\n----------------------------------------\n\nTITLE: Checking StringStore Length in Python\nDESCRIPTION: Get the number of strings in the StringStore using the len() function.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/stringstore.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstringstore = StringStore([\"apple\", \"orange\"])\nassert len(stringstore) == 2\n```\n\n----------------------------------------\n\nTITLE: Updating Saving Methods in spaCy 2.0\nDESCRIPTION: Replace saving methods with to_disk() or to_bytes() for consistent API across all containers in spaCy 2.0.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n- nlp.save_to_directory(\"/model\")\n- nlp.vocab.dump(\"/vocab\")\n\n+ nlp.to_disk(\"/model\")\n+ nlp.vocab.to_disk(\"/vocab\")\n```\n\n----------------------------------------\n\nTITLE: Exporting displaCy Visualization as SVG\nDESCRIPTION: This snippet demonstrates how to generate an SVG file from a displaCy dependency parse visualization. It creates the SVG markup and saves it to a file, which is useful for creating standalone graphics.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsvg = displacy.render(doc, style=\"dep\")\noutput_path = Path(\"/images/sentence.svg\")\noutput_path.open(\"w\", encoding=\"utf-8\").write(svg)\n```\n\n----------------------------------------\n\nTITLE: Defining Entity Ruler Attributes in Markdown\nDESCRIPTION: This snippet defines a table of attributes for the Entity Ruler component in spaCy. It includes the attribute names, their descriptions, and their types.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityruler.mdx#2025-04-21_snippet_13\n\nLANGUAGE: markdown\nCODE:\n```\n| Name              | Description                                                                                                           |\n| ----------------- | --------------------------------------------------------------------------------------------------------------------- |\n| `matcher`         | The underlying matcher used to process token patterns. ~~Matcher~~                                                    |\n| `phrase_matcher`  | The underlying phrase matcher used to process phrase patterns. ~~PhraseMatcher~~                                      |\n| `token_patterns`  | The token patterns present in the entity ruler, keyed by label. ~~Dict[str, List[Dict[str, Union[str, List[dict]]]]~~ |\n| `phrase_patterns` | The phrase patterns present in the entity ruler, keyed by label. ~~Dict[str, List[Doc]]~~                             |\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenAI API\nDESCRIPTION: Shell commands to set required environment variables for using OpenAI API with spaCy\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_54\n\nLANGUAGE: shell\nCODE:\n```\nexport OPENAI_API_KEY=\"sk-...\"\nexport OPENAI_API_ORG=\"org-...\"\n```\n\n----------------------------------------\n\nTITLE: Migrating from gold to training Module\nDESCRIPTION: Comparison showing the renamed module and utility functions for working with entity annotations and BILUO tag formats.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_31\n\nLANGUAGE: diff\nCODE:\n```\n- from spacy.gold import biluo_tags_from_offsets, offsets_from_biluo_tags, spans_from_biluo_tags\n+ from spacy.training import offsets_to_biluo_tags, biluo_tags_to_offsets, biluo_tags_to_spans\n```\n\n----------------------------------------\n\nTITLE: Error: Module Load Attribute Error - Python\nDESCRIPTION: Error occurring when a file or directory name shadows the spaCy module name.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nAttributeError: 'module' object has no attribute 'load'\n```\n\n----------------------------------------\n\nTITLE: Deserializing spaCy NLP Object from Bytes\nDESCRIPTION: Shows how to deserialize a spaCy NLP object from bytes using the from_bytes() method. This allows loading a previously saved NLP pipeline state.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/101/_serialization.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnlp.from_bytes(data)\n```\n\n----------------------------------------\n\nTITLE: Initializing StringStore in Python\nDESCRIPTION: Create a new StringStore instance with optional initial strings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/stringstore.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.strings import StringStore\nstringstore = StringStore([\"apple\", \"orange\"])\n```\n\n----------------------------------------\n\nTITLE: Disabling Black Formatting in Python Code\nDESCRIPTION: Example of using fmt directives to disable black formatting for specific code blocks where automatic formatting would reduce readability, particularly in language data files or test cases.\nSOURCE: https://github.com/explosion/spacy/blob/master/CONTRIBUTING.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# fmt: off\ntext = \"I look forward to using Thingamajig.  I've been told it will make my life easier...\"\nheads = [1, 1, 1, 1, 3, 4, 1, 6, 11, 11, 11, 11, 14, 14, 11, 16, 17, 14, 11]\ndeps = [\"nsubj\", \"ROOT\", \"advmod\", \"prep\", \"pcomp\", \"dobj\", \"punct\", \"\",\n        \"nsubjpass\", \"aux\", \"auxpass\", \"ROOT\", \"nsubj\", \"aux\", \"ccomp\",\n        \"poss\", \"nsubj\", \"ccomp\", \"punct\"]\n# fmt: on\n```\n\n----------------------------------------\n\nTITLE: Allowing Customizable Attribute Names in Python\nDESCRIPTION: Demonstrates how to allow users to customize attribute names in extensions to avoid namespace collisions and accommodate different naming preferences.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_44\n\nLANGUAGE: diff\nCODE:\n```\n+ Doc.set_extension(self.doc_attr, default=\"some value\")\n- Doc.set_extension(\"my_doc_attr\", default=\"some value\")\n```\n\n----------------------------------------\n\nTITLE: Bot Information Command\nDESCRIPTION: Command to request general information about supported bot commands.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/ExplosionBot.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n@explosion-bot please info\n```\n\n----------------------------------------\n\nTITLE: Rehearsing SentenceRecognizer Model\nDESCRIPTION: Performs a rehearsal update to prevent catastrophic forgetting. Uses Example objects and an optimizer for training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\noptimizer = nlp.resume_training()\nlosses = senter.rehearse(examples, sgd=optimizer)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for PaLM API\nDESCRIPTION: Shell command to set required environment variable for using PaLM API with spaCy\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_57\n\nLANGUAGE: shell\nCODE:\n```\nexport PALM_API_KEY=\"...\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Batch Size in spaCy Training (INI)\nDESCRIPTION: This snippet shows how to configure a static batch size for training in spaCy using the config file. It sets a fixed size of 3000 for the batch_by_words batcher.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_16\n\nLANGUAGE: ini\nCODE:\n```\n[training.batcher]\n@batchers = \"spacy.batch_by_words.v1\"\nsize = 3000\n```\n\n----------------------------------------\n\nTITLE: Updating spaCy Version Requirements in meta.json\nDESCRIPTION: Changes required in the meta.json file to update the spaCy version compatibility range from v3.0 to v3.1. This allows pipelines trained with v3.0 to work with v3.1.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-1.mdx#2025-04-21_snippet_6\n\nLANGUAGE: diff\nCODE:\n```\n- \"spacy_version\": \">=3.0.0,<3.1.0\",\n+ \"spacy_version\": \">=3.0.0,<3.2.0\",\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCatBOW v1 Architecture\nDESCRIPTION: Configuration for the TextCatBOW v1 model architecture, which implements an n-gram bag-of-words text classification model. This version doesn't support resizable models for adding labels after training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/legacy.mdx#2025-04-21_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.TextCatBOW.v1\"\nexclusive_classes = false\nngram_size = 1\nno_output_layer = false\nnO = null\n```\n\n----------------------------------------\n\nTITLE: Entity Recognizer Configuration with Incorrect Spans\nDESCRIPTION: Configuration example for the EntityRecognizer showing how to set up incorrect spans handling with custom parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-1.mdx#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[components.ner]\nfactory = \"ner\"\nincorrect_spans_key = \"incorrect_spans\"\nmoves = null\nupdate_with_oracle_cut_size = 100\n```\n\n----------------------------------------\n\nTITLE: Chinese Tokenizer Configuration\nDESCRIPTION: Configuration settings for Chinese tokenizer using ini format.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/models.mdx#2025-04-21_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[nlp.tokenizer]\n@tokenizers = \"spacy.zh.ChineseTokenizer\"\nsegmenter = \"char\"\n```\n\n----------------------------------------\n\nTITLE: Initializing SpanResolver for Training\nDESCRIPTION: Example showing how to initialize the SpanResolver component for training with examples. This is typically called by Language.initialize to set up the model architecture and label scheme.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\nspan_resolver.initialize(lambda: examples, nlp=nlp)\n```\n\n----------------------------------------\n\nTITLE: Logging Implementation Example\nDESCRIPTION: Shows how to implement logging statements for debugging and training information.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nlogger.info(\"Set up nlp object from config\")\nconfig = nlp.config.interpolate()\n```\n\n----------------------------------------\n\nTITLE: Setting up and Installing the spaCy Website\nDESCRIPTION: Instructions for cloning the repository, switching to the correct Node version, installing dependencies, and starting the development server.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngit clone https://github.com/explosion/spaCy\ncd spaCy/website\n\n# Switch to the correct Node version\n#\n# If you don't have NVM and don't want to use it, you can manually switch to the Node version\n# stated in /.nvmrc and skip this step\nnvm use\n\n# Install the dependencies\nnpm install\n\n# Start the development server\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Processing - Basic Tokenization Example\nDESCRIPTION: Simple example demonstrating how to tokenize a string using the spaCy tokenizer.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tokenizer.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntokens = tokenizer(\"This is a sentence\")\nassert len(tokens) == 4\n```\n\n----------------------------------------\n\nTITLE: Generating Full Config from Base Config in spaCy\nDESCRIPTION: This command uses spaCy's CLI to generate a complete configuration file from a base config. It fills in remaining defaults to ensure reproducibility of experiments.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init fill-config base_config.cfg config.cfg\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy from Source using Git and Python\nDESCRIPTION: Steps to clone spaCy repository, set up a virtual environment, and install the package with its dependencies. The process includes creating and activating a virtual environment, updating core Python packages, and installing spaCy in editable mode.\nSOURCE: https://github.com/explosion/spacy/blob/master/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/explosion/spaCy\ncd spaCy\n\npython -m venv .env\nsource .env/bin/activate\n\n# make sure you are using the latest pip\npython -m pip install -U pip setuptools wheel\n\npip install -r requirements.txt\npip install --no-build-isolation --editable .\n```\n\n----------------------------------------\n\nTITLE: Clearing All Patterns from SpanRuler\nDESCRIPTION: Demonstrates how to remove all patterns from a SpanRuler using the clear method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanruler.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npatterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"}]\nruler = nlp.add_pipe(\"span_ruler\")\nruler.add_patterns(patterns)\nruler.clear()\n```\n\n----------------------------------------\n\nTITLE: Removing Doc Extensions in spaCy\nDESCRIPTION: Demonstrates how to remove a previously registered extension from the Doc class and verify its removal.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/doc.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Doc\nDoc.set_extension(\"has_city\", default=False)\nremoved = Doc.remove_extension(\"has_city\")\nassert not Doc.has_extension(\"has_city\")\n```\n\n----------------------------------------\n\nTITLE: Raw Task Configuration with Few-Shot Reader\nDESCRIPTION: Configuration for Raw.v1 task with few-shot learning enabled using external YAML file of examples.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_15\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.Raw.v1\"\nfield = \"llm_reply\"\n[components.llm.task.examples]\n@misc = \"spacy.FewShotReader.v1\"\npath = \"raw_examples.yml\"\n```\n\n----------------------------------------\n\nTITLE: Creating Accordions\nDESCRIPTION: Example of creating collapsible accordion sections using JSX syntax.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_12\n\nLANGUAGE: jsx\nCODE:\n```\n<Accordion title=\"This is an accordion\">\n  Accordion content goes here.\n</Accordion>\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy from Source\nDESCRIPTION: Commands for building and installing spaCy from source code.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m pip install -U pip setuptools wheel\n$ git clone https://github.com/explosion/spaCy\n$ cd spaCy\n$ python -m venv .env\n$ source .env/bin/activate\n$ pip install -r requirements.txt\n$ pip install --no-build-isolation --editable .\n```\n\n----------------------------------------\n\nTITLE: REL Few-Shot Examples with Entity Relations\nDESCRIPTION: JSONL examples showing relation extraction between named entities including location and person relationships.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_43\n\nLANGUAGE: json\nCODE:\n```\n{\"text\": \"Laura bought a house in Boston with her husband Mark.\", \"ents\": [{\"start_char\": 0, \"end_char\": 5, \"label\": \"PERSON\"}, {\"start_char\": 24, \"end_char\": 30, \"label\": \"GPE\"}, {\"start_char\": 48, \"end_char\": 52, \"label\": \"PERSON\"}], \"relations\": [{\"dep\": 0, \"dest\": 1, \"relation\": \"LivesIn\"}, {\"dep\": 2, \"dest\": 1, \"relation\": \"LivesIn\"}]}\n{\"text\": \"Michael travelled through South America by bike.\", \"ents\": [{\"start_char\": 0, \"end_char\": 7, \"label\": \"PERSON\"}, {\"start_char\": 26, \"end_char\": 39, \"label\": \"LOC\"}], \"relations\": [{\"dep\": 0, \"dest\": 1, \"relation\": \"Visits\"}]}\n```\n\n----------------------------------------\n\nTITLE: Filtering warnings in tests with pytest in Python\nDESCRIPTION: Demonstrates how to filter out specific expected warnings using the pytest.mark.filterwarnings decorator when they're not relevant to what's being tested.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.filterwarnings(\"ignore:\\\\[W036\")\ndef test_matcher_empty(en_vocab):\n    matcher = Matcher(en_vocab)\n    matcher(Doc(en_vocab, words=[\"test\"]))\n```\n\n----------------------------------------\n\nTITLE: Initializing SpanResolver Component\nDESCRIPTION: Different ways to initialize the SpanResolver component, including adding it to the pipeline with default or custom models, or constructing it directly from the class.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Construction via add_pipe with default model\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\n\n# Construction via add_pipe with custom model\nconfig = {\"model\": {\"@architectures\": \"my_span_resolver.v1\"}}\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\", config=config)\n\n# Construction from class\nfrom spacy_experimental.coref.span_resolver_component import SpanResolver\nspan_resolver = SpanResolver(nlp.vocab, model)\n```\n\n----------------------------------------\n\nTITLE: Getting Length of MorphAnalysis in Python\nDESCRIPTION: Demonstrates how to get the number of features in a MorphAnalysis object using the __len__ method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/morphology.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfeats = \"Feat1=Val1,Val2|Feat2=Val2\"\nmorph = MorphAnalysis(nlp.vocab, feats)\nassert len(morph) == 3\n```\n\n----------------------------------------\n\nTITLE: Configuring VS Code for Black Formatting in Python\nDESCRIPTION: JSON configuration for Visual Studio Code settings to use black as the Python formatter and automatically format Python files on save.\nSOURCE: https://github.com/explosion/spacy/blob/master/CONTRIBUTING.md#2025-04-21_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"python.formatting.provider\": \"black\",\n  \"[python]\": {\n    \"editor.formatOnSave\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Data Files to spaCy Format using Command Line\nDESCRIPTION: This command uses spaCy's CLI tool to convert training data from a standard format (.conll) to spaCy's binary format. It takes an input file path and an output directory as arguments.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy convert ./train.gold.conll ./corpus\n```\n\n----------------------------------------\n\nTITLE: Configuring NoOp Task for Testing\nDESCRIPTION: Example configuration for the NoOp task that is used for testing purposes. This task tells the LLM to do nothing and sets no fields on the docs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_52\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.NoOp.v1\"\n```\n\n----------------------------------------\n\nTITLE: Using spaCy Basic Example\nDESCRIPTION: Basic example of using spaCy to process text and analyze tokens.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a sentence.\")\nfor token in doc:\n    print(token.text, token.pos_)\n```\n\n----------------------------------------\n\nTITLE: Accessing Table Names in Lookups\nDESCRIPTION: Shows how to get a list of all table names in the lookups object using the tables property.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlookups = Lookups()\nlookups.add_table(\"some_table\")\nassert lookups.tables == [\"some_table\"]\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy via conda\nDESCRIPTION: Command for installing spaCy using conda package manager.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ conda install -c conda-forge spacy\n```\n\n----------------------------------------\n\nTITLE: Configuring Warning Filters in spaCy v2.3\nDESCRIPTION: Example showing how to migrate from spaCy's custom warnings to native Python warnings configuration.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-3.mdx#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", message=r\"\\[W007\\]\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Type Hinting Callbacks in Python\nDESCRIPTION: Shows how to properly type hint callback functions using Callable with specific argument and return types.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef create_callback(some_arg: bool) -> Callable[[str, int], List[str]]:\n    def callback(arg1: str, arg2: int) -> List[str]:\n        ...\n\n    return callback\n```\n\n----------------------------------------\n\nTITLE: Configuring a Sourced NER Component in spaCy\nDESCRIPTION: This snippet shows how to source a named entity recognition component from an existing model in spaCy's configuration. The 'source' parameter specifies the model to copy the component from.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Language.md#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[components.ner]\nsource = \"en_core_web_sm\"\n```\n\n----------------------------------------\n\nTITLE: Error Message Definition in Python\nDESCRIPTION: Shows how to define error messages with placeholders in spaCy's error system.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass Errors:\n    E123 = \"Something went wrong\"\n    E456 = \"Unexpected value: {value}\"\n```\n\n----------------------------------------\n\nTITLE: Running spaCy Test Suite\nDESCRIPTION: Commands to execute the spaCy test suite using pytest, including installation of test utilities from requirements.txt.\nSOURCE: https://github.com/explosion/spacy/blob/master/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython -m pytest --pyargs spacy\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields During Serialization\nDESCRIPTION: Shows how to exclude specific fields when serializing tokenizer data using the exclude parameter.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tokenizer.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndata = tokenizer.to_bytes(exclude=[\"vocab\", \"exceptions\"])\ntokenizer.from_disk(\"./data\", exclude=[\"token_match\"])\n```\n\n----------------------------------------\n\nTITLE: Adding Label to TrainablePipe in Python\nDESCRIPTION: Add a new label to be predicted by the model. This method needs to be overwritten with a custom implementation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\npipe = nlp.add_pipe(\"your_custom_pipe\")\npipe.add_label(\"MY_LABEL\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Translation Task in INI\nDESCRIPTION: This snippet shows how to configure the Translation task in spacy-llm using an INI configuration file. It sets up the task with a target language of Spanish.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_10\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.Translation.v1\"\nexamples = null\ntarget_lang = \"Spanish\"\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Commands to install LangChain integration dependencies for spaCy\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_61\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install \"langchain==0.0.191\"\n# Or install with spacy-llm directly\npython -m pip install \"spacy-llm[extras]\"\n```\n\n----------------------------------------\n\nTITLE: Internal Error Message Definition\nDESCRIPTION: Shows how to define a detailed error message for internal inconsistencies with guidance for users.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nE161 = (\"Found an internal inconsistency when predicting entity links. \"\n        \"This is likely a bug in spaCy, so feel free to open an issue: \"\n        \"https://github.com/explosion/spaCy/issues\")\n```\n\n----------------------------------------\n\nTITLE: Packaging and Visualizing spaCy Model\nDESCRIPTION: Commands to package the trained model and visualize its output using Streamlit.\nSOURCE: https://github.com/explosion/spacy/blob/master/examples/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy project run package\n```\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy project run visualize-model\n```\n\n----------------------------------------\n\nTITLE: Updating spaCy Version Requirements in meta.json\nDESCRIPTION: Example of updating the spaCy version requirements in the meta.json file for custom pipelines.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-2.mdx#2025-04-21_snippet_5\n\nLANGUAGE: diff\nCODE:\n```\n- \"spacy_version\": \">=3.1.0,<3.2.0\",\n+ \"spacy_version\": \">=3.2.0,<3.3.0\",\n```\n\n----------------------------------------\n\nTITLE: Creating Asides in Documentation\nDESCRIPTION: Examples showing how to create aside sections using Markdown and JSX syntax.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n> #### Aside title\n>\n> This is aside text.\n```\n\nLANGUAGE: jsx\nCODE:\n```\n<Aside title=\"Aside title\">This is aside text.</Aside>\n```\n\n----------------------------------------\n\nTITLE: Setting Lexeme Flag Values in spaCy\nDESCRIPTION: Demonstrates how to set binary flag attributes on a LexemeC struct using the c_set_flag static method. This example sets the IS_STOP flag to 0 (false).\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-structs.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs cimport IS_STOP\nfrom spacy.lexeme cimport Lexeme\n\nlexeme = doc.c[3].lex\nLexeme.c_set_flag(lexeme, IS_STOP, 0)\n```\n\n----------------------------------------\n\nTITLE: Setting Annotations with SpanFinder\nDESCRIPTION: Shows how to modify documents using pre-computed scores from the SpanFinder model's predictions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nspan_finder = nlp.add_pipe(\"span_finder\")\nscores = span_finder.predict(docs)\nspan_finder.set_annotations(docs, scores)\n```\n\n----------------------------------------\n\nTITLE: Configuring spaCy Dependencies with Version Constraints\nDESCRIPTION: A comprehensive list of dependencies required for the spaCy natural language processing library. The dependencies are categorized into spaCy's own libraries, third-party dependencies, Python utilities, and development dependencies, each with specific version constraints to ensure compatibility.\nSOURCE: https://github.com/explosion/spacy/blob/master/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n# Our libraries\nspacy-legacy>=3.0.11,<3.1.0\nspacy-loggers>=1.0.0,<2.0.0\ncymem>=2.0.2,<2.1.0\npreshed>=3.0.2,<3.1.0\nthinc>=8.3.4,<8.4.0\nml_datasets>=0.2.0,<0.3.0\nmurmurhash>=0.28.0,<1.1.0\nwasabi>=0.9.1,<1.2.0\nsrsly>=2.4.3,<3.0.0\ncatalogue>=2.0.6,<2.1.0\ntyper>=0.3.0,<1.0.0\nweasel>=0.1.0,<0.5.0\n# Third party dependencies\nnumpy>=2.0.0,<3.0.0\nrequests>=2.13.0,<3.0.0\ntqdm>=4.38.0,<5.0.0\npydantic>=1.7.4,!=1.8,!=1.8.1,<3.0.0\njinja2\nlangcodes>=3.2.0,<4.0.0\n# Official Python utilities\nsetuptools\npackaging>=20.0\n# Development dependencies\npre-commit>=2.13.0\ncython>=0.25,<3.0\npytest>=5.2.0,!=7.1.0\npytest-timeout>=1.3.0,<2.0.0\nmock>=2.0.0,<3.0.0\nflake8>=3.8.0,<6.0.0\nhypothesis>=3.27.0,<7.0.0\nmypy>=1.5.0,<1.6.0; platform_machine != \"aarch64\" and python_version >= \"3.8\"\ntypes-mock>=0.1.1\ntypes-setuptools>=57.0.0\ntypes-requests\ntypes-setuptools>=57.0.0\nblack==22.3.0\ncython-lint>=0.15.0\nisort>=5.0,<6.0\n```\n\n----------------------------------------\n\nTITLE: Formatting Headlines in JSX\nDESCRIPTION: JSX component syntax for creating headlines with different attributes and tags.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<H2>Headline 2</H2>\n<H2 id=\"some_id\">Headline 2</H2>\n<H2 id=\"some_id\" tag=\"method\">Headline 2</H2>\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom LLM Task in INI\nDESCRIPTION: INI configuration snippet for setting up a custom LLM task in spaCy. It specifies the task name, labels, and other configuration values.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/large-language-models.mdx#2025-04-21_snippet_7\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"my_namespace.MyTask.v1\"\nlabels = LABEL1,LABEL2,LABEL3\nmy_other_config_val = 0.3\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy with Apple M1 Optimization\nDESCRIPTION: Command to install spaCy with AppleOps for improved performance on Apple M1 Macs.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-2.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install spacy[apple]\n```\n\n----------------------------------------\n\nTITLE: Vocabulary Data First Line Format - Python\nDESCRIPTION: Defines the format for the first line of a vocabulary data file, specifying language and settings for vocabulary initialization\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n{\"lang\": \"en\", \"settings\": {\"oov_prob\": -20.502029418945312}}\n```\n\n----------------------------------------\n\nTITLE: Retokenizer for Merging Tokens in Python\nDESCRIPTION: Example of using the new Doc.retokenize context manager to merge spans of tokens efficiently.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-1.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like David Bowie\")\nwith doc.retokenize() as retokenizer:\n    attrs = {\"LEMMA\": \"David Bowie\"}\n    retokenizer.merge(doc[2:4], attrs=attrs)\n```\n\n----------------------------------------\n\nTITLE: Forward References in Type Hints\nDESCRIPTION: Demonstrates using string literals for forward references in type hints when referring to the containing class.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass SomeClass:\n    def from_bytes(self, data: bytes) -> \"SomeClass\":\n        ...\n```\n\n----------------------------------------\n\nTITLE: Defining Standalone Tok2VecTransformer Model Chain Architecture\nDESCRIPTION: Code showing the architecture of a standalone Tok2VecTransformer model, which differs from the listener version by including a split_trf_batch component between the transformer model and trfs2arrays.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Listeners.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = chain(\n    TransformerModel(name, get_spans, tokenizer_config),\n    split_trf_batch(),\n    trfs2arrays(pooling, grad_factor),\n)\n```\n\n----------------------------------------\n\nTITLE: GPU Testing with Custom Thinc Branch\nDESCRIPTION: Example command for running slow GPU tests with a specified Thinc branch. The branch can be either a named branch or an unmerged PR reference.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/ExplosionBot.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n@explosion-bot please test_slow_gpu --thinc-branch <branch_name>\n```\n\n----------------------------------------\n\nTITLE: Error Handling Pattern with Custom Messages\nDESCRIPTION: Demonstrates proper error raising with predefined error codes and message formatting.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nif something_went_wrong:\n    raise ValueError(Errors.E123)\n\nif not isinstance(value, int):\n    raise ValueError(Errors.E456.format(value=value))\n```\n\n----------------------------------------\n\nTITLE: Initializing PlainTextCorpus in Python\nDESCRIPTION: Example of how to initialize a PlainTextCorpus object with a file path. This creates a corpus reader for plain text data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/corpus.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import PlainTextCorpus\n\ncorpus = PlainTextCorpus(\"./data/docs.txt\")\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy with Optional Features\nDESCRIPTION: Command to install spaCy with additional optional features such as lookups and CUDA support for GPU acceleration.\nSOURCE: https://github.com/explosion/spacy/blob/master/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install --no-build-isolation --editable .[lookups,cuda102]\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for SpanResolver in Python\nDESCRIPTION: Create an optimizer specifically for the SpanResolver pipeline component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\noptimizer = span_resolver.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Using Frozen Collections for Default Arguments\nDESCRIPTION: Shows how to use SimpleFrozenList to avoid mutable default argument issues.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_14\n\nLANGUAGE: diff\nCODE:\n```\n- def to_bytes(self, *, exclude: List[str] = []):\n+ def to_bytes(self, *, exclude: List[str] = SimpleFrozenList()):\n    ...\n```\n\n----------------------------------------\n\nTITLE: JSON training data structure for spaCy (deprecated)\nDESCRIPTION: Example structure of the deprecated JSON format for training spaCy models, including document, paragraph, sentence, and token level annotations.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n[{\n    \"id\": int,                      # ID of the document within the corpus\n    \"paragraphs\": [{                # list of paragraphs in the corpus\n        \"raw\": string,              # raw text of the paragraph\n        \"sentences\": [{             # list of sentences in the paragraph\n            \"tokens\": [{            # list of tokens in the sentence\n                \"id\": int,          # index of the token in the document\n                \"dep\": string,      # dependency label\n                \"head\": int,        # offset of token head relative to token index\n                \"tag\": string,      # part-of-speech tag\n                \"orth\": string,     # verbatim text of the token\n                \"ner\": string       # BILUO label, e.g. \"O\" or \"B-ORG\"\n            }],\n            \"brackets\": [{          # phrase structure (NOT USED by current models)\n                \"first\": int,       # index of first token\n                \"last\": int,        # index of last token\n                \"label\": string     # phrase label\n            }]\n        }],\n        \"cats\": [{                  # new in v2.2: categories for text classifier\n            \"label\": string,        # text category label\n            \"value\": float / bool   # label applies (1.0/true) or not (0.0/false)\n        }]\n    }]\n}]\n```\n\n----------------------------------------\n\nTITLE: Converting IOB to spaCy v2 JSON Format\nDESCRIPTION: This command converts an IOB file to JSON format for spaCy v2. It uses the spacy convert utility with parameters for IOB conversion format, sentence segmentation, batch size, and a base language.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/example_data/ner_example_data/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy convert -c iob -s -n 10 -b en file.iob\n```\n\n----------------------------------------\n\nTITLE: Keyword-Only Arguments\nDESCRIPTION: Demonstrates converting optional arguments to keyword-only arguments for better code clarity and maintainability.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_13\n\nLANGUAGE: diff\nCODE:\n```\n- def do_something(name: str, validate: bool = False):\n+ def do_something(name: str, *, validate: bool = False):\n    ...\n\n- do_something(\"some_name\", True)\n+ do_something(\"some_name\", validate=True)\n```\n\n----------------------------------------\n\nTITLE: Displaying Custom spaCy Badge in Markdown\nDESCRIPTION: Demonstrates how to include a custom 'Made with love and spaCy' badge in Markdown format, which can be used to show support for spaCy in project documentation or README files.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/spacy-101.mdx#2025-04-21_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n[![Made with love and spaCy](https://img.shields.io/badge/made%20with%20❤%20and-spaCy-09a3d5.svg)](https://spacy.io)\n```\n\n----------------------------------------\n\nTITLE: Error: spaCy Module Import Failure - Python\nDESCRIPTION: Error message indicating that the spaCy module cannot be found in the Python environment.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nImport Error: No module named spacy\n```\n\n----------------------------------------\n\nTITLE: Tok2Vec Component Configuration\nDESCRIPTION: Configuration example for a tok2vec component with embed and encode sublayers.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[components.tok2vec]\nfactory = \"tok2vec\"\n\n[components.tok2vec.model]\n@architectures = \"spacy.Tok2Vec.v2\"\n\n[components.tok2vec.model.embed]\n@architectures = \"spacy.MultiHashEmbed.v2\"\n\n[components.tok2vec.model.encode]\n@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n```\n\n----------------------------------------\n\nTITLE: Explicit Variable Type Annotation\nDESCRIPTION: Shows the preferred format for variable type annotations using explicit syntax instead of inline comments.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_9\n\nLANGUAGE: diff\nCODE:\n```\n- var = value    # type: Type\n+ var: Type = value\n```\n\n----------------------------------------\n\nTITLE: Configuring TorchBiLSTMEncoder Architecture in spaCy\nDESCRIPTION: Configuration for TorchBiLSTMEncoder.v1 that implements bidirectional LSTM layers using PyTorch. Defines width, depth and dropout parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/architectures.mdx#2025-04-21_snippet_7\n\nLANGUAGE: ini\nCODE:\n```\n[model]\n@architectures = \"spacy.TorchBiLSTMEncoder.v1\"\nwidth = 64\ndepth = 2\ndropout = 0.0\n```\n\n----------------------------------------\n\nTITLE: Updating Matcher Patterns in spaCy v2.0\nDESCRIPTION: Shows how to use the simplified matcher API with one-step pattern addition and string IDs instead of separate add_entity and add_pattern calls.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_18\n\nLANGUAGE: diff\nCODE:\n```\n- matcher.add_entity(\"GoogleNow\", on_match=merge_phrases)\n- matcher.add_pattern(\"GoogleNow\", [{ORTH: \"Google\"}, {ORTH: \"Now\"}])\n\n+ matcher.add(\"GoogleNow\", merge_phrases, [{\"ORTH\": \"Google\"}, {\"ORTH\": \"Now\"}])\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy with conda\nDESCRIPTION: This command installs spaCy using conda package manager from the conda-forge channel.\nSOURCE: https://github.com/explosion/spacy/blob/master/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge spacy\n```\n\n----------------------------------------\n\nTITLE: Configuring Callback with Arguments in INI\nDESCRIPTION: Example of configuring a callback function with additional arguments in the spaCy config file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_23\n\nLANGUAGE: ini\nCODE:\n```\n[nlp.before_creation]\n@callbacks = \"customize_language_data\"\nextra_stop_words = [\"ooh\", \"aah\"]\ndebug = true\n```\n\n----------------------------------------\n\nTITLE: Converting JSONL to spaCy JSON training format for text categorization\nDESCRIPTION: Command to convert text categorization data from JSONL format to spaCy's JSON training format. It uses a Python script with parameters for model language and file paths.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/example_data/textcat_example_data/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython textcatjsonl_to_trainjson.py -m en file.jsonl .\n```\n\n----------------------------------------\n\nTITLE: Displaying Lexeme Attributes Table in Markdown\nDESCRIPTION: This code snippet defines a markdown table that lists and describes various attributes of Lexeme objects in spaCy. It includes attribute names, their descriptions, and data types.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lexeme.mdx#2025-04-21_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n| Name             | Description                                                                                                                                                                                                                                                          |\n| ---------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `vocab`          | The lexeme's vocabulary. ~~Vocab~~                                                                                                                                                                                                                                   |\n| `text`           | Verbatim text content. ~~str~~                                                                                                                                                                                                                                       |\n| `orth`           | ID of the verbatim text content. ~~int~~                                                                                                                                                                                                                             |\n| `orth_`          | Verbatim text content (identical to `Lexeme.text`). Exists mostly for consistency with the other attributes. ~~str~~                                                                                                                                                 |\n| `rank`           | Sequential ID of the lexeme's lexical type, used to index into tables, e.g. for word vectors. ~~int~~                                                                                                                                                                |\n| `flags`          | Container of the lexeme's binary flags. ~~int~~                                                                                                                                                                                                                      |\n| `norm`           | The lexeme's norm, i.e. a normalized form of the lexeme text. ~~int~~                                                                                                                                                                                                |\n| `norm_`          | The lexeme's norm, i.e. a normalized form of the lexeme text. ~~str~~                                                                                                                                                                                                |\n| `lower`          | Lowercase form of the word. ~~int~~                                                                                                                                                                                                                                  |\n| `lower_`         | Lowercase form of the word. ~~str~~                                                                                                                                                                                                                                  |\n| `shape`          | Transform of the word's string, to show orthographic features. Alphabetic characters are replaced by `x` or `X`, and numeric characters are replaced by `d`, and sequences of the same character are truncated after length 4. For example,`\"Xxxx\"`or`\"dd\"`. ~~int~~ |\n| `shape_`         | Transform of the word's string, to show orthographic features. Alphabetic characters are replaced by `x` or `X`, and numeric characters are replaced by `d`, and sequences of the same character are truncated after length 4. For example,`\"Xxxx\"`or`\"dd\"`. ~~str~~ |\n| `prefix`         | Length-N substring from the start of the word. Defaults to `N=1`. ~~int~~                                                                                                                                                                                            |\n| `prefix_`        | Length-N substring from the start of the word. Defaults to `N=1`. ~~str~~                                                                                                                                                                                            |\n| `suffix`         | Length-N substring from the end of the word. Defaults to `N=3`. ~~int~~                                                                                                                                                                                              |\n| `suffix_`        | Length-N substring from the end of the word. Defaults to `N=3`. ~~str~~                                                                                                                                                                                            |\n| `is_alpha`       | Does the lexeme consist of alphabetic characters? Equivalent to `lexeme.text.isalpha()`. ~~bool~~                                                                                                                                                                    |\n| `is_ascii`       | Does the lexeme consist of ASCII characters? Equivalent to `[any(ord(c) >= 128 for c in lexeme.text)]`. ~~bool~~                                                                                                                                                     |\n| `is_digit`       | Does the lexeme consist of digits? Equivalent to `lexeme.text.isdigit()`. ~~bool~~                                                                                                                                                                                   |\n| `is_lower`       | Is the lexeme in lowercase? Equivalent to `lexeme.text.islower()`. ~~bool~~                                                                                                                                                                                          |\n| `is_upper`       | Is the lexeme in uppercase? Equivalent to `lexeme.text.isupper()`. ~~bool~~                                                                                                                                                                                          |\n| `is_title`       | Is the lexeme in titlecase? Equivalent to `lexeme.text.istitle()`. ~~bool~~                                                                                                                                                                                          |\n| `is_punct`       | Is the lexeme punctuation? ~~bool~~                                                                                                                                                                                                                                  |\n| `is_left_punct`  | Is the lexeme a left punctuation mark, e.g. `(`? ~~bool~~                                                                                                                                                                                                            |\n| `is_right_punct` | Is the lexeme a right punctuation mark, e.g. `)`? ~~bool~~                                                                                                                                                                                                           |\n| `is_space`       | Does the lexeme consist of whitespace characters? Equivalent to `lexeme.text.isspace()`. ~~bool~~                                                                                                                                                                    |\n| `is_bracket`     | Is the lexeme a bracket? ~~bool~~                                                                                                                                                                                                                                    |\n| `is_quote`       | Is the lexeme a quotation mark? ~~bool~~                                                                                                                                                                                                                             |\n| `is_currency`    | Is the lexeme a currency symbol? ~~bool~~                                                                                                                                                                                                                            |\n| `like_url`       | Does the lexeme resemble a URL? ~~bool~~                                                                                                                                                                                                                             |\n| `like_num`       | Does the lexeme represent a number? e.g. \"10.9\", \"10\", \"ten\", etc. ~~bool~~                                                                                                                                                                                          |\n| `like_email`     | Does the lexeme resemble an email address? ~~bool~~                                                                                                                                                                                                                  |\n| `is_oov`         | Is the lexeme out-of-vocabulary (i.e. does it not have a word vector)? ~~bool~~                                                                                                                                                                                      |\n```\n\n----------------------------------------\n\nTITLE: Configuring Tok2Vec as a Shared Component in spaCy\nDESCRIPTION: Configuration for setting up a standalone Tok2Vec component that can be shared across multiple pipeline components. This defines a reusable embedding layer that other components can listen to.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Listeners.md#2025-04-21_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[components.tok2vec]\nfactory = \"tok2vec\"\n\n[components.tok2vec.model]\n@architectures = \"spacy.Tok2Vec.v2\"\n```\n\n----------------------------------------\n\nTITLE: Adding a pytest mark for issue references in Python\nDESCRIPTION: This snippet shows how to add a pytest mark to reference an issue number when fixing a bug. The test function should be placed in the relevant file in the spacy/tests folder.\nSOURCE: https://github.com/explosion/spacy/blob/master/CONTRIBUTING.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.issue(1234)\ndef test_issue1234():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Adding Key to Vector Table in Python\nDESCRIPTION: Method to add a key to the vector table. Returns the row number or -1 if not supported.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/basevectors.mdx#2025-04-21_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef add(self, key: Union[str, int]) -> int:\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Lexeme from the Vocabulary by String in Cython\nDESCRIPTION: Example demonstrating how to retrieve a LexemeC pointer from the vocabulary using a string. This shows how to access lexical entries directly from the vocabulary.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-classes.mdx#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nlexeme = vocab.get(vocab.mem, \"hello\")\n```\n\n----------------------------------------\n\nTITLE: Running Basic and Slow Tests with pytest in Bash\nDESCRIPTION: Commands for running different types of tests in the spaCy library using pytest. Shows how to run basic tests, include slow tests, or target specific test files and functions.\nSOURCE: https://github.com/explosion/spacy/blob/master/spacy/tests/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npy.test spacy                        # run basic tests\npy.test spacy --slow                 # run basic and slow tests\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parameters with EntityLinker in spaCy (Python)\nDESCRIPTION: Illustrates how to temporarily modify the EntityLinker's model parameters using a context manager. This is useful for tasks like serializing the best model during training.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nentity_linker = nlp.add_pipe(\"entity_linker\")\nwith entity_linker.use_params(optimizer.averages):\n    entity_linker.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Adding Inline Comments in Python Code\nDESCRIPTION: Examples of adding helpful inline comments to explain complex logic or provide context for certain code decisions in the spaCy project.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_5\n\nLANGUAGE: diff\nCODE:\n```\ntoken_index = indices[value]\n+ # Index describes Token.i of last token but Span indices are inclusive\nspan = doc[prev_token_index:token_index + 1]\n```\n\nLANGUAGE: diff\nCODE:\n```\n+ # To create the components we need to use the final interpolated config\n+ # so all values are available (if component configs use variables).\n+ # Later we replace the component config with the raw config again.\ninterpolated = filled.interpolate() if not filled.is_interpolated else filled\n```\n\nLANGUAGE: diff\nCODE:\n```\n+ # Ensure object is a Span, not a Doc (#1234)\nif isinstance(obj, Doc):\n    obj = obj[obj.start:obj.end]\n```\n\n----------------------------------------\n\nTITLE: Exception Chaining in Python\nDESCRIPTION: Example of re-raising exceptions with custom messages while preserving the original error chain.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    run_third_party_code_that_might_fail()\nexcept ValueError as e:\n    raise ValueError(Errors.E123) from e\n```\n\n----------------------------------------\n\nTITLE: Predicting Dependencies with Parser\nDESCRIPTION: Demonstrates how to apply the parser model to documents without modifying them.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\nscores = parser.predict([doc1, doc2])\n```\n\n----------------------------------------\n\nTITLE: Checking Key Existence in Vector Store\nDESCRIPTION: Example showing how to check if a key exists in the vector store.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncat_id = nlp.vocab.strings[\"cat\"]\nnlp.vocab.vectors.add(cat_id, numpy.random.uniform(-1, 1, (300,)))\nassert cat_id in vectors\n```\n\n----------------------------------------\n\nTITLE: Validating spaCy Installation and Models\nDESCRIPTION: These commands update spaCy and validate the installed models for compatibility after an update.\nSOURCE: https://github.com/explosion/spacy/blob/master/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -U spacy\npython -m spacy validate\n```\n\n----------------------------------------\n\nTITLE: Retrieving Entity Vectors\nDESCRIPTION: Shows how to retrieve vector representations for single and multiple entities.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/inmemorylookupkb.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nvector = kb.get_vector(\"Q42\")\nvectors = kb.get_vectors((\"Q42\", \"Q3107329\"))\n```\n\n----------------------------------------\n\nTITLE: Markdown Link and Image Reference\nDESCRIPTION: HTML and Markdown markup combining the Explosion AI logo with a link to the main examples documentation\nSOURCE: https://github.com/explosion/spacy/blob/master/examples/training/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<a href=\"https://explosion.ai\"><img src=\"https://explosion.ai/assets/img/logo.svg\" width=\"125\" height=\"125\" align=\"right\" /></a>\n\n# spaCy examples\n\nSee [examples/README.md](../README.md)\n```\n\n----------------------------------------\n\nTITLE: spaCy Website Project Structure\nDESCRIPTION: A YAML representation of the project structure, explaining the purpose of each directory and key configuration files in the spaCy website repository.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n├── docs                 # the actual markdown content\n├── meta                 # JSON-formatted site metadata\n|   ├── dynamicMeta.js   # At build time generated meta data\n|   ├── languages.json   # supported languages and statistical models\n|   ├── sidebars.json    # sidebar navigations for different sections\n|   ├── site.json        # general site metadata\n|   ├── type-annotations.json # Type annotations\n|   └── universe.json    # data for the spaCy universe section\n├── pages                # Next router pages\n├── public               # static images and other assets\n├── setup                # Jinja setup\n├── src                  # source\n|   ├── components       # React components\n|   ├── fonts            # webfonts\n|   ├── images           # images used in the layout\n|   ├── plugins          # custom plugins to transform Markdown\n|   ├── styles           # CSS modules and global styles\n|   ├── templates        # page layouts\n|   |   ├── docs.js      # layout template for documentation pages\n|   |   ├── index.js     # global layout template\n|   |   ├── models.js    # layout template for model pages\n|   |   └── universe.js  # layout templates for universe\n|   └── widgets          # non-reusable components with content, e.g. changelog\n├── .eslintrc.json       # ESLint config file\n├── .nvmrc               # NVM config file\n|                        # (to support \"nvm use\" to switch to correct Node version)\n|\n├── .prettierrc          # Prettier config file\n├── next.config.mjs      # Next config file\n├── package.json         # package settings and dependencies\n└── tsconfig.json        # TypeScript config file\n```\n\n----------------------------------------\n\nTITLE: Disabling Black Auto-formatting for Specific Code Blocks\nDESCRIPTION: Demonstrates how to disable Black auto-formatting for specific code blocks using '# fmt: off' and '# fmt: on' comments. This is useful for preserving intentional formatting in certain cases, such as in tests.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n+ # fmt: off\ntext = \"I look forward to using Thingamajig.  I've been told it will make my life easier...\"\ndeps = [\"nsubj\", \"ROOT\", \"advmod\", \"prep\", \"pcomp\", \"dobj\", \"punct\", \"\",\n        \"nsubjpass\", \"aux\", \"auxpass\", \"ROOT\", \"nsubj\", \"aux\", \"ccomp\",\n        \"poss\", \"nsubj\", \"ccomp\", \"punct\"]\n+ # fmt: on\n```\n\n----------------------------------------\n\nTITLE: Basic Bot Command Syntax\nDESCRIPTION: Shows the basic syntax for invoking the Explosion-bot with a test command.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/ExplosionBot.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@explosion-bot please test_gpu\n```\n\n----------------------------------------\n\nTITLE: Loading SpanRuler Patterns from File\nDESCRIPTION: Demonstrates loading patterns from a JSONL file for use with SpanRuler.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nimport srsly\n\npatterns = srsly.read_jsonl(\"patterns.jsonl\")\nruler = nlp.add_pipe(\"span_ruler\")\nruler.add_patterns(patterns)\n```\n\n----------------------------------------\n\nTITLE: Including TODO Comments in Python Code\nDESCRIPTION: Example of adding a TODO comment in the spaCy codebase to indicate potential future improvements or areas that need attention.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Code Conventions.md#2025-04-21_snippet_6\n\nLANGUAGE: diff\nCODE:\n```\n+ # TODO: this is currently pretty slow\ndir_checksum = hashlib.md5()\nfor sub_file in sorted(fp for fp in path.rglob(\"*\") if fp.is_file()):\n    dir_checksum.update(sub_file.read_bytes())\n```\n\n----------------------------------------\n\nTITLE: Validating spaCy Installation\nDESCRIPTION: Commands to upgrade spaCy and validate the installation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U %%SPACY_PKG_NAME%%SPACY_PKG_FLAGS\n$ python -m spacy validate\n```\n\n----------------------------------------\n\nTITLE: Checking Token Extensions - Python\nDESCRIPTION: Demonstrates how to check if an extension is registered on the Token class.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/token.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.tokens import Token\nToken.set_extension(\"is_fruit\", default=False)\nassert Token.has_extension(\"is_fruit\")\n```\n\n----------------------------------------\n\nTITLE: Command Help Request\nDESCRIPTION: Command syntax for requesting help documentation for a specific bot command.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/ExplosionBot.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n@explosion-bot please <command> --help\n```\n\n----------------------------------------\n\nTITLE: Error: Unhashable List Type - Python\nDESCRIPTION: Error related to Git line ending conversion issues when loading models on Windows systems.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nTypeError: unhashable type: 'list'\n```\n\n----------------------------------------\n\nTITLE: Initializing Lookups in Python\nDESCRIPTION: Creates a new Lookups object that serves as a container for large lookup tables and dictionaries.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.lookups import Lookups\nlookups = Lookups()\n```\n\n----------------------------------------\n\nTITLE: Defining NumPy Version Constraints for Spacy Builds\nDESCRIPTION: Specifies the acceptable version range for NumPy dependency, requiring version 2.x.x but less than 3.0.0. This is used by wheelwright during the build process to ensure compatibility.\nSOURCE: https://github.com/explosion/spacy/blob/master/build-constraints.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpy>=2.0.0,<3.0.0\n```\n\n----------------------------------------\n\nTITLE: Defining Lexeme Attributes in Markdown Table Format\nDESCRIPTION: This snippet presents a markdown table that defines several lexeme attributes used in spaCy. It includes the attribute names, their descriptions, and their corresponding data types.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lexeme.mdx#2025-04-21_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n| `is_stop`        | Is the lexeme part of a \"stop list\"? ~~bool~~                                                                                                                                                                                                                        |\n| `lang`           | Language of the parent vocabulary. ~~int~~                                                                                                                                                                                                                           |\n| `lang_`          | Language of the parent vocabulary. ~~str~~                                                                                                                                                                                                                           |\n| `prob`           | Smoothed log probability estimate of the lexeme's word type (context-independent entry in the vocabulary). ~~float~~                                                                                                                                                 |\n| `cluster`        | Brown cluster ID. ~~int~~                                                                                                                                                                                                                                            |\n| `sentiment`      | A scalar value indicating the positivity or negativity of the lexeme. ~~float~~                                                                                                                                                                                      |\n```\n\n----------------------------------------\n\nTITLE: Deserializing Tok2Vec Component from Bytes in Python\nDESCRIPTION: Shows how to load a Tok2Vec component from a bytestring. This is the counterpart to the to_bytes() method, used to restore a component from its serialized form.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tok2vec.mdx#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntok2vec_bytes = tok2vec.to_bytes()\ntok2vec = nlp.add_pipe(\"tok2vec\")\ntok2vec.from_bytes(tok2vec_bytes)\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss for SpanCategorizer in Python\nDESCRIPTION: Example of calculating the loss and gradient of loss for a batch of documents and their predicted scores using the SpanCategorizer component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spancategorizer.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nspancat = nlp.add_pipe(\"spancat\")\nscores = spancat.predict([eg.predicted for eg in examples])\nloss, d_loss = spancat.get_loss(examples, scores)\n```\n\n----------------------------------------\n\nTITLE: Using Matcher.add with List of Patterns in Python\nDESCRIPTION: In spaCy v3.0, Matcher.add and PhraseMatcher.add now only accept a list of patterns as the second argument, with the on_match callback as an optional keyword argument.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmatcher.add(\"pattern_name\", [pattern1, pattern2], on_match=callback_function)\n```\n\n----------------------------------------\n\nTITLE: Referencing a Model in spaCy Config\nDESCRIPTION: Shows how to reference a model from a components block in a spaCy configuration file. This syntax uses Thinc's configuration system to reference a previously defined model component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_33\n\nLANGUAGE: toml\nCODE:\n```\n# This references the [components.textcat.model] block above\nmodel = ${components.textcat.model}\nlabels = []\n```\n\n----------------------------------------\n\nTITLE: Manually Constructing a Doc Object for Testing\nDESCRIPTION: A test example showing how to construct a Doc object with predefined annotations like words, POS tags, dependency heads and labels without loading a full spaCy model, which is useful for lightweight testing.\nSOURCE: https://github.com/explosion/spacy/blob/master/spacy/tests/README.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef test_doc_token_api_strings(en_vocab):\n    words = [\"Give\", \"it\", \"back\", \"!\", \"He\", \"pleaded\", \".\"]\n    pos = ['VERB', 'PRON', 'PART', 'PUNCT', 'PRON', 'VERB', 'PUNCT']\n    heads = [0, 0, 0, 0, 5, 5, 5]\n    deps = ['ROOT', 'dobj', 'prt', 'punct', 'nsubj', 'ROOT', 'punct']\n\n    doc = Doc(en_vocab, words=words, pos=pos, heads=heads, deps=deps)\n    assert doc[0].text == 'Give'\n    assert doc[0].lower_ == 'give'\n    assert doc[0].pos_ == 'VERB'\n    assert doc[0].dep_ == 'ROOT'\n```\n\n----------------------------------------\n\nTITLE: Checking Lookups Length in Python\nDESCRIPTION: Gets the current number of tables in the lookups object, which is initially zero for a new instance.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlookups = Lookups()\nassert len(lookups) == 0\n```\n\n----------------------------------------\n\nTITLE: Applying a TrainablePipe Component to a Document in Python\nDESCRIPTION: Demonstrates how to apply a TrainablePipe component to a single document, which typically happens automatically in the spaCy pipeline.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/pipe.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"This is a sentence.\")\npipe = nlp.add_pipe(\"your_custom_pipe\")\n# This usually happens under the hood\nprocessed = pipe(doc)\n```\n\n----------------------------------------\n\nTITLE: Manually Rendering displaCy HTML in Jupyter\nDESCRIPTION: This snippet demonstrates how to manually render displaCy visualizations in Jupyter notebooks using IPython's display and HTML functions, which is what displaCy does internally when it detects a Jupyter environment.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/visualizers.mdx#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.core.display import display, HTML\n\nhtml = displacy.render(doc, style=\"dep\")\ndisplay(HTML(html))\n```\n\n----------------------------------------\n\nTITLE: Iterating Over C Arrays with Slice Notation in Cython\nDESCRIPTION: Shows the preferred pattern for iterating over C arrays by using slice notation with explicit length. This approach allows the compiler to generate efficient code while maintaining readable syntax.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython.mdx#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncdef int c_total(const int* int_array, int length) nogil:\n    total = 0\n    for item in int_array[:length]:\n        total += item\n    return total\n```\n\n----------------------------------------\n\nTITLE: Using Token Attribute IDs in spaCy\nDESCRIPTION: This example demonstrates how to work with token attribute IDs in spaCy. It shows that attribute IDs like DEP have consistent internal values, automatic conversion between string and integer representations, and how to look up IDs in the spacy.attrs.IDS dictionary.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributes.mdx#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nfrom spacy.attrs import DEP\n\nnlp = spacy.blank(\"en\")\ndoc = nlp(\"There are many attributes.\")\n\n# DEP always has the same internal value\nassert DEP == 76\n\n# \"DEP\" is automatically converted to DEP\nassert DEP == nlp.vocab.strings[\"DEP\"]\nassert doc.has_annotation(DEP) == doc.has_annotation(\"DEP\")\n\n# look up IDs in spacy.attrs.IDS\nfrom spacy.attrs import IDS\nassert IDS[\"DEP\"] == DEP\n```\n\n----------------------------------------\n\nTITLE: Serializing AttributeRuler to Bytes - Python\nDESCRIPTION: Example showing how to serialize an AttributeRuler pipe to a bytestring using the to_bytes() method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/attributeruler.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nruler = nlp.add_pipe(\"attribute_ruler\")\nruler = ruler.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Using Shape Inference in Thinc Model\nDESCRIPTION: Demonstrates how to use Thinc's shape inference capabilities to simplify model definition and initialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nwith Model.define_operators({\">>\":chain}):\n    layers = (\n        Relu(hidden_width)\n        >> Dropout(dropout)\n        >> Relu(hidden_width)\n        >> Dropout(dropout)\n        >> Softmax()\n    )\n    model = char_embed >> with_array(layers)\n    model.initialize(X=input_sample, Y=output_sample)\n```\n\n----------------------------------------\n\nTITLE: Setting Sentence Annotations with SentenceRecognizer in spaCy\nDESCRIPTION: Example of how to modify a batch of documents using pre-computed scores from the SentenceRecognizer's predict method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencerecognizer.mdx#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsenter = nlp.add_pipe(\"senter\")\nscores = senter.predict([doc1, doc2])\nsenter.set_annotations([doc1, doc2], scores)\n```\n\n----------------------------------------\n\nTITLE: Loading EntityLinker from bytes in Python\nDESCRIPTION: This snippet demonstrates how to load an EntityLinker pipe from a bytestring. It shows creating a bytestring from an existing entity_linker, adding a new entity_linker pipe to the NLP pipeline, and then loading the saved data into the new pipe.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entitylinker.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nentity_linker_bytes = entity_linker.to_bytes()\nentity_linker = nlp.add_pipe(\"entity_linker\")\nentity_linker.from_bytes(entity_linker_bytes)\n```\n\n----------------------------------------\n\nTITLE: Accessing Vocab Attributes (Python)\nDESCRIPTION: Demonstrates how to access string IDs and entity labels from the Vocab object's StringStore.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vocab.mdx#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\napple_id = nlp.vocab.strings[\"apple\"]\nassert type(apple_id) == int\nPERSON = nlp.vocab.strings[\"PERSON\"]\nassert type(PERSON) == int\n```\n\n----------------------------------------\n\nTITLE: Getting Lexeme Attribute Values in spaCy\nDESCRIPTION: Shows how to retrieve attribute values from a LexemeC struct using the get_struct_attr static method. This method takes a lexeme pointer and attribute ID to fetch the corresponding value.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/cython-structs.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.attrs cimport IS_ALPHA\nfrom spacy.lexeme cimport Lexeme\n\nlexeme = doc.c[3].lex\nis_alpha = Lexeme.get_struct_attr(lexeme, IS_ALPHA)\n```\n\n----------------------------------------\n\nTITLE: Matching Regular Expressions on Full Text in spaCy\nDESCRIPTION: Shows how to match regular expressions on the entire document text and create spans from the matches using spaCy's Doc.char_span method.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\nimport re\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"The United States of America (USA) are commonly known as the United States (U.S. or US) or America.\")\n\nexpression = r\"[Uu](nited|\\.) ?[Ss](tates|\\?)\"\nfor match in re.finditer(expression, doc.text):\n    start, end = match.span()\n    span = doc.char_span(start, end)\n    # This is a Span object or None if match doesn't map to valid token sequence\n    if span is not None:\n        print(\"Found match:\", span.text)\n```\n\n----------------------------------------\n\nTITLE: Using Memory Zone for Resource Management in spaCy (Python)\nDESCRIPTION: Example demonstrating the memory_zone context manager to create a block where resources are automatically freed at the end. This is useful for processing large volumes of text with a defined memory budget.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/language.mdx#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n### Example\ncounts = Counter()\nwith nlp.memory_zone():\n    for doc in nlp.pipe(texts):\n        for token in doc:\n            counts[token.text] += 1\n```\n\n----------------------------------------\n\nTITLE: Calculating Vector Norm in spaCy\nDESCRIPTION: Shows how to calculate the L2 norm of a span's vector representation.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span.mdx#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndoc = nlp(\"I like apples\")\ndoc[1:].vector_norm # 4.800883928527915\ndoc[2:].vector_norm # 6.895897646384268\nassert doc[1:].vector_norm != doc[2:].vector_norm\n```\n\n----------------------------------------\n\nTITLE: Adding Direct Object Dependency to Pattern\nDESCRIPTION: Further extends the pattern to include the direct object of 'founded' with the dependency relation 'dobj', representing the company that was founded.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_35\n\nLANGUAGE: python\nCODE:\n```\npattern = [\n    #...\n    {\n        \"LEFT_ID\": \"anchor_founded\",\n        \"REL_OP\": \">\",\n        \"RIGHT_ID\": \"founded_object\",\n        \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"},\n    }\n    # ...\n]\n```\n\n----------------------------------------\n\nTITLE: Using noqa Comments to Disable Flake8 Linting\nDESCRIPTION: Examples of using noqa comments to suppress specific flake8 warnings for justified exceptions to the code style rules, such as unused imports or bare except clauses.\nSOURCE: https://github.com/explosion/spacy/blob/master/CONTRIBUTING.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# The imported class isn't used in this file, but imported here, so it can be\n# imported *from* here by another module.\nfrom .submodule import SomeClass  # noqa: F401\n\ntry:\n    do_something()\nexcept:  # noqa: E722\n    # This bare except is justified, for some specific reason\n    do_something_else()\n```\n\n----------------------------------------\n\nTITLE: Custom Doc Attributes in spaCy v2.0\nDESCRIPTION: Shows how to use the new custom extension system to attach metadata to Doc objects instead of converting them to arrays and creating custom containers.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2.mdx#2025-04-21_snippet_15\n\nLANGUAGE: diff\nCODE:\n```\n- doc = nlp(\"This is a regular doc\")\n- doc_array = doc.to_array([\"ORTH\", \"POS\"])\n- doc_with_meta = {\"doc_array\": doc_array, \"meta\": get_doc_meta(doc_array)}\n\n+ Doc.set_extension(\"meta\", getter=get_doc_meta)\n+ doc_with_meta = nlp(u'This is a doc with meta data')\n+ meta = doc._.meta\n```\n\n----------------------------------------\n\nTITLE: Adding Tables to Lookups\nDESCRIPTION: Demonstrates how to add a new table with optional data to the lookups object. An error is raised if a table with the same name already exists.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlookups = Lookups()\nlookups.add_table(\"some_table\", {\"foo\": \"bar\"})\n```\n\n----------------------------------------\n\nTITLE: Creating Optimizer for spaCy Tagger\nDESCRIPTION: Create an optimizer instance for the tagger component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/tagger.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntagger = nlp.add_pipe(\"tagger\")\noptimizer = tagger.create_optimizer()\n```\n\n----------------------------------------\n\nTITLE: Setting Tok2VecListener Width from Upstream Component\nDESCRIPTION: Configuration that ensures the output dimension (width) of a Tok2VecListener matches the width of the upstream Tok2Vec component. This maintains dimension compatibility between the embedding provider and consumer.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/DEVELOPER_DOCS/Listeners.md#2025-04-21_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[components.ner.model.tok2vec]\n@architectures = \"spacy.Tok2VecListener.v1\"\nwidth = ${components.tok2vec.model.encode.width}\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch with CUDA Support\nDESCRIPTION: Command to install PyTorch 1.11.0 with CUDA 11.3 support using pip. This is a prerequisite for using transformer models in spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/embeddings-transformers.mdx#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n```\n\n----------------------------------------\n\nTITLE: Link Formatting Examples\nDESCRIPTION: Examples showing how to create links in both Markdown and JSX formats.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n[I am a link](https://spacy.io)\n```\n\nLANGUAGE: jsx\nCODE:\n```\n<Link to=\"https://spacy.io\">I am a link</Link>\n```\n\n----------------------------------------\n\nTITLE: Compiling Prefix Regular Expressions for Tokenization\nDESCRIPTION: Compiles a sequence of prefix rules into a regex object for use in the spaCy tokenizer. This allows customization of token prefix matching during tokenization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nprefixes = (\"§\", \"%\", \"=\", r\"\\+\")\nprefix_regex = util.compile_prefix_regex(prefixes)\nnlp.tokenizer.prefix_search = prefix_regex.search\n```\n\n----------------------------------------\n\nTITLE: Extended Component Registration with Annotations\nDESCRIPTION: Enhanced component registration with additional metadata about required annotations, assignments, and scoring.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.language import Language\n\n@Language.factory(\n    \"relation_extractor\",\n    requires=[\"doc.ents\", \"token.ent_iob\", \"token.ent_type\"],\n    assigns=[\"doc._.rel\"],\n    default_score_weights={\n        \"rel_micro_p\": None,\n        \"rel_micro_r\": None,\n        \"rel_micro_f\": None,\n    },\n)\ndef make_relation_extractor(nlp, name, model):\n    return RelationExtractor(nlp.vocab, model, name)\n```\n\n----------------------------------------\n\nTITLE: Modifying Vector Key Iteration for Floret Pipelines\nDESCRIPTION: Demonstrates how to modify code to use external word lists instead of vector keys when working with floret pipelines.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-3.mdx#2025-04-21_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n- lexemes = [nlp.vocab[orth] for orth in nlp.vocab.vectors]\n+ lexemes = [nlp.vocab[word] for word in external_word_list]\n```\n\n----------------------------------------\n\nTITLE: Configuring TextCat v2 Component in spaCy\nDESCRIPTION: Configuration example for TextCat v2 showing basic setup with label definitions for text classification. Uses an improved prompt template compared to v1.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_37\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.TextCat.v2\"\nlabels = [\"COMPLIMENT\", \"INSULT\"]\nexamples = null\n```\n\n----------------------------------------\n\nTITLE: Implementing Property Extensions in spaCy\nDESCRIPTION: Shows how to define property extensions with getter and optional setter functions. Property extensions are computed on-demand and are useful for derived values or Span objects.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nDoc.set_extension(\"hello\", getter=get_hello_value, setter=set_hello_value)\nassert doc._.hello\ndoc._.hello = \"Hi!\"\n```\n\n----------------------------------------\n\nTITLE: Installing New Language Models in spaCy v2.3\nDESCRIPTION: Commands to download and install the new language models for Danish, Japanese, Polish, Romanian and Chinese using spaCy's CLI.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-3.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy download da_core_news_sm\npython -m spacy download ja_core_news_sm\npython -m spacy download pl_core_news_sm\npython -m spacy download ro_core_news_sm\npython -m spacy download zh_core_web_sm\n```\n\n----------------------------------------\n\nTITLE: Excluding Fields in spaCy Serialization (Python)\nDESCRIPTION: This snippet demonstrates how to exclude specific fields when serializing a spaCy object to disk. In this example, the 'vocab' field is excluded from serialization.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/spanfinder.mdx#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndata = span_finder.to_disk(\"/path\", exclude=[\"vocab\"])\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy from Source with Extras\nDESCRIPTION: Command for installing spaCy from source with additional dependencies.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install --no-build-isolation --editable .[lookups,cuda102]\n```\n\n----------------------------------------\n\nTITLE: Configuring Variables in spaCy project.yml\nDESCRIPTION: YAML configuration that defines reusable variables and a command that uses them to run a custom evaluation script with the specified parameters.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/projects.mdx#2025-04-21_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\nvars:\n  batch_size: 128\n\ncommands:\n  - name: evaluate\n    script:\n      - 'python scripts/custom_evaluation.py ${vars.batch_size} ./training/model-best ./corpus/eval.json'\n    deps:\n      - 'training/model-best'\n      - 'corpus/eval.json'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Prior Probability from Knowledge Base in Python\nDESCRIPTION: Demonstrates how to get the prior probability of an entity mention using the get_prior_prob method. Takes an entity ID and alias as input and returns a float probability.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/inmemorylookupkb.mdx#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprobability = kb.get_prior_prob(\"Q42\", \"Douglas\")\n```\n\n----------------------------------------\n\nTITLE: Filling in New Config Settings for spaCy v3.1\nDESCRIPTION: Command to update a spaCy v3.0 configuration file to include new settings required for v3.1. This ensures compatibility with newer spaCy versions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3-1.mdx#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy init fill-config config-v3.0.cfg config-v3.1.cfg\n```\n\n----------------------------------------\n\nTITLE: Building spaCy with Python setup.py for Developers in Bash\nDESCRIPTION: Alternative commands for building spaCy using setup.py instead of pip. This method is no longer recommended but may be useful in certain scenarios.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -r requirements.txt\n$ python setup.py build_ext --inplace -j 4\n$ python setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Converting spaCy v2 JSON to spaCy v3 Format\nDESCRIPTION: This command converts an existing spaCy v2 JSON training file to .spacy format compatible with spaCy v3. It uses the spacy convert utility to perform the conversion.\nSOURCE: https://github.com/explosion/spacy/blob/master/extra/example_data/ner_example_data/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy convert file.json .\n```\n\n----------------------------------------\n\nTITLE: Loading DependencyParser from Disk in Python\nDESCRIPTION: This snippet illustrates how to load a DependencyParser component from disk. It adds a parser to the NLP pipeline and then loads it from a specified file path.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/dependencyparser.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nparser = nlp.add_pipe(\"parser\")\nparser.from_disk(\"/path/to/parser\")\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parameters with SpanResolver in Python\nDESCRIPTION: Temporarily modify the SpanResolver's model to use given parameter values within a context manager.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/span-resolver.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nspan_resolver = nlp.add_pipe(\"experimental_span_resolver\")\nwith span_resolver.use_params(optimizer.averages):\n    span_resolver.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: SpanRuler Basic Usage\nDESCRIPTION: Example of using SpanRuler to add spans to doc.spans based on pattern matching.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nimport spacy\n\nnlp = spacy.blank(\"en\")\nruler = nlp.add_pipe(\"span_ruler\")\npatterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\nruler.add_patterns(patterns)\n\ndoc = nlp(\"Apple is opening its first big office in San Francisco.\")\nprint([(span.text, span.label_) for span in doc.spans[\"ruler\"]])\n```\n\n----------------------------------------\n\nTITLE: Migrating from begin_training to initialize\nDESCRIPTION: Comparison showing the API change from begin_training to initialize method, which now takes a function returning Example objects.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v3.mdx#2025-04-21_snippet_29\n\nLANGUAGE: diff\nCODE:\n```\n- nlp.begin_training()\n+ nlp.initialize(lambda: examples)\n```\n\n----------------------------------------\n\nTITLE: Formatting Headlines in Markdown\nDESCRIPTION: Examples of how to format different headline styles with optional attributes in Markdown syntax.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## Headline 2\n\n## Headline 2 {id=\"some_id\"}\n\n## Headline 2 {id=\"some_id\" tag=\"method\"}\n```\n\n----------------------------------------\n\nTITLE: Summarization Task Configuration with Few-Shot Reader\nDESCRIPTION: Configuration for Summarization.v1 task with few-shot learning and word limit settings.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_18\n\nLANGUAGE: ini\nCODE:\n```\n[components.llm.task]\n@llm_tasks = \"spacy.Summarization.v1\"\nmax_n_words = 20\n[components.llm.task.examples]\n@misc = \"spacy.FewShotReader.v1\"\npath = \"summarization_examples.yml\"\n```\n\n----------------------------------------\n\nTITLE: Building spaCy Executable PEX File in Bash\nDESCRIPTION: Commands for cloning the spaCy repository and building an executable .pex file using the provided Makefile. This creates a standalone executable that includes spaCy and all its dependencies.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/explosion/spaCy\n$ cd spaCy\n$ make\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy Model and Basic Usage\nDESCRIPTION: Demonstrates how to download and load a spaCy language model using the command line and Python.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy download en_core_web_sm\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> import spacy\n>>> nlp = spacy.load(\"en_core_web_sm\")\n```\n\n----------------------------------------\n\nTITLE: Creating Code Blocks with Titles\nDESCRIPTION: Examples of code blocks with titles in Markdown and JSX, including syntax highlighting.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/styleguide.mdx#2025-04-21_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n```python\n### This is a title\nimport spacy\n```\n```\n\nLANGUAGE: jsx\nCODE:\n```\n<CodeBlock title=\"This is a title\" lang=\"python\">\n  import spacy\n</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Configuring ConsoleLogger.v2 in spaCy Training\nDESCRIPTION: Example configuration for setting up the ConsoleLogger.v2 component in spaCy training. This logger writes training results to the console in a tabular format and saves them to a jsonl file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_21\n\nLANGUAGE: ini\nCODE:\n```\n[training.logger]\n@loggers = \"spacy.ConsoleLogger.v2\"\nprogress_bar = true\nconsole_output = true\noutput_file = \"training_log.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Using Custom Parameters in NER Model\nDESCRIPTION: Demonstrates using a context manager to temporarily modify the NER model's parameters. Useful for saving specific parameter states.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/entityrecognizer.mdx#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nner = EntityRecognizer(nlp.vocab)\nwith ner.use_params(optimizer.averages):\n    ner.to_disk(\"/best_model\")\n```\n\n----------------------------------------\n\nTITLE: Installing Accelerate Package\nDESCRIPTION: Command to install accelerate package for CPU-based model distribution\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/large-language-models.mdx#2025-04-21_snippet_60\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install \"accelerate>=0.16.0,<1.0\"\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy Pipeline\nDESCRIPTION: Basic example of loading a spaCy pipeline using the spacy.load() function\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/processing-pipelines.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnlp = spacy.load(\"en_core_web_sm\")\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Probability Tables in SpaCy\nDESCRIPTION: Shows how to load probability tables into an existing model by first removing the empty placeholder table and then accessing the probability feature to load the full table from spacy-lookups-data.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/v2-3.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# prerequisite: pip install spacy-lookups-data\nimport spacy\n\nnlp = spacy.load(\"en_core_web_md\")\n\n# remove the empty placeholder prob table\nif nlp.vocab.lookups_extra.has_table(\"lexeme_prob\"):\n    nlp.vocab.lookups_extra.remove_table(\"lexeme_prob\")\n\n# access any `.prob` to load the full table into the model\nassert nlp.vocab[\"a\"].prob == -3.9297883511\n\n# if desired, save this model with the probability table included\nnlp.to_disk(\"/path/to/model\")\n```\n\n----------------------------------------\n\nTITLE: TokenC Struct Reference in spaCy\nDESCRIPTION: Reference to internal TokenC struct implementation that explains why certain token attributes are not available in the matcher. This is related to Cython optimization in spaCy.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/rule-based-matching.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nTokenC struct\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Sentencizer Settings\nDESCRIPTION: Examples showing how to save and load Sentencizer settings to/from disk.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/sentencizer.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconfig = {\"punct_chars\": [\".\", \"?\", \"!\", \"。\"]}\nsentencizer = nlp.add_pipe(\"sentencizer\", config=config)\nsentencizer.to_disk(\"/path/to/sentencizer.json\")\n\nsentencizer = nlp.add_pipe(\"sentencizer\")\nsentencizer.from_disk(\"/path/to/sentencizer.json\")\n```\n\n----------------------------------------\n\nTITLE: Using CLI Command to Initialize and Prune Word Vectors in spaCy\nDESCRIPTION: This command demonstrates how to use the spaCy CLI to initialize vectors from a file and prune them in one step. The --prune option limits the number of vectors to 10,000, with remaining words mapped to their closest vector.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/linguistic-features.mdx#2025-04-21_snippet_47\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy init vectors en la.300d.vec.tgz /tmp/la_vectors_web_md --prune 10000\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Version Requirement\nDESCRIPTION: Declares Python 3.8 as the required version for the spaCy project. This simple version specification informs users and build systems about the minimum Python version needed.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/runtime.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n3.8\n```\n\n----------------------------------------\n\nTITLE: Pipeline Package Directory Structure in YAML\nDESCRIPTION: Illustrates the directory structure of a packaged spaCy pipeline, showing the layout of files including setup.py, meta.json, pipeline data, and the installable package.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/saving-loading.mdx#2025-04-21_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\n└── /\n    ├── MANIFEST.in                           # to include meta.json\n    ├── meta.json                             # pipeline meta data\n    ├── setup.py                              # setup file for pip installation\n    ├── en_example_pipeline                   # pipeline directory\n    │    ├── __init__.py                      # init for pip installation\n    │    └── en_example_pipeline-1.0.0        # pipeline data\n    │        ├── config.cfg                   # pipeline config\n    │        ├── meta.json                    # pipeline meta\n    │        └── ...                          # directories with component data\n    └── dist\n        └── en_example_pipeline-1.0.0.tar.gz  # installable package\n```\n\n----------------------------------------\n\nTITLE: Classification Layer Configuration\nDESCRIPTION: Config snippet for the classification layer, specifying the architecture and input/output dimensions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/layers-architectures.mdx#2025-04-21_snippet_20\n\nLANGUAGE: ini\nCODE:\n```\n[model.classification_layer]\n@architectures = \"rel_classification_layer.v1\"\nnI = null\nnO = null\n```\n\n----------------------------------------\n\nTITLE: Configuring Single Function Corpora in spaCy Project Configuration\nDESCRIPTION: Example of configuring corpora using a single reader function that divides the data into train and dev partitions. Includes parameters for train and dev paths and a shuffle option.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/data-formats.mdx#2025-04-21_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[corpora]\n@readers = \"my_custom_reader.v1\"\ntrain_path = ${paths:train}\ndev_path = ${paths:dev}\nshuffle = true\n```\n\n----------------------------------------\n\nTITLE: Configuring orth_variants Data Augmenter in spaCy\nDESCRIPTION: Configuration for orth_variants augmenter that performs orthographic variant replacement. It supports level and lower parameters to control augmentation percentage, and loads variant definitions from a JSON file.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_31\n\nLANGUAGE: ini\nCODE:\n```\n[corpora.train.augmenter]\n@augmenters = \"spacy.orth_variants.v1\"\nlevel = 0.1\nlower = 0.5\n\n[corpora.train.augmenter.orth_variants]\n@readers = \"srsly.read_json.v1\"\npath = \"corpus/en_orth_variants.json\"\n```\n\n----------------------------------------\n\nTITLE: Loading Transformer Model from Bytes\nDESCRIPTION: Example demonstrating how to load a serialized transformer model from bytes back into a pipeline component.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/transformer.mdx#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntrf_bytes = trf.to_bytes()\ntrf = nlp.add_pipe(\"transformer\")\ntrf.from_bytes(trf_bytes)\n```\n\n----------------------------------------\n\nTITLE: Serializing spaCy Vectors to Binary Format\nDESCRIPTION: Demonstrates how to serialize a spaCy Vectors object to a binary string using the to_bytes() method. This is useful for saving vector data to disk or transmitting it over a network.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/vectors.mdx#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nvectors_bytes = vectors.to_bytes()\n```\n\n----------------------------------------\n\nTITLE: Training spaCy Model with Custom Tokenizer\nDESCRIPTION: Bash command to train a spaCy model using a custom configuration and additional code file containing custom tokenizer functions.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/training.mdx#2025-04-21_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n$ python -m spacy train config.cfg --code ./functions.py\n```\n\n----------------------------------------\n\nTITLE: Converting BILUO Tags to Span Objects\nDESCRIPTION: Example demonstrating how to convert BILUO tag annotations to spaCy Span objects for entity recognition.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/top-level.mdx#2025-04-21_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy.training import biluo_tags_to_spans\n\ndoc = nlp(\"I like London.\")\ntags = [\"O\", \"O\", \"U-LOC\", \"O\"]\ndoc.ents = biluo_tags_to_spans(doc, tags)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Tables from Lookups\nDESCRIPTION: Shows how to retrieve a table from the lookups object by name. This raises an error if the table doesn't exist.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/api/lookups.mdx#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlookups = Lookups()\nlookups.add_table(\"some_table\", {\"foo\": \"bar\"})\ntable = lookups.get_table(\"some_table\")\nassert table[\"foo\"] == \"bar\"\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy with Extra Dependencies\nDESCRIPTION: Example of installing spaCy with additional dependencies using pip.\nSOURCE: https://github.com/explosion/spacy/blob/master/website/docs/usage/index.mdx#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install %%SPACY_PKG_NAME[lookups,transformers]%%SPACY_PKG_FLAGS\n```"
  }
]
[
  {
    "owner": "mindverse",
    "repo": "second-me",
    "content": "TITLE: Chat Request Body Example JSON\nDESCRIPTION: This JSON payload represents the request body for the chat API endpoint. It includes messages defining the conversation history, metadata for request processing, stream setting for streaming responses, model identifier, temperature for randomness control and max_tokens for maximum number of tokens to generate.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Local Chat API.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n    {\"role\": \"assistant\", \"content\": \"I am a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What can you do for me?\"}  \n  ],\n  \"metadata\": {\n    \"enable_l0_retrieval\": true,\n    \"role_id\": \"uuid-string\"\n  },\n  \"stream\": true,\n  \"model\": \"gpt-3.5-turbo\",\n  \"temperature\": 0.1,\n  \"max_tokens\": 2000\n}\n```\n\n----------------------------------------\n\nTITLE: Making Chat Completion Request with Python\nDESCRIPTION: This Python code snippet shows how to use the `http.client` library to send a POST request to the chat completion API. It constructs the JSON payload with the `messages`, `metadata`, `temperature`, `max_tokens`, and `stream` parameters, sends the request, and processes the streaming response.  It requires the `json` and `http.client` libraries. Replace `{instance_id}` with the correct value.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Public Chat API.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport http.client\nimport json\n\nurl = \"app.secondme.io\"\npath = \"/api/chat/{instance_id}\"           \n\nheaders = {\"Content-Type\": \"application/json\"}\ndata = {\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello, please introduce yourself.\"}\n    ],\n    \"metadata\": {\n        \"enable_l0_retrieval\": False,\n        \"role_id\": \"default_role\"\n    },\n    \"temperature\": 0.7,\n    \"max_tokens\": 2000,\n    \"stream\": True\n}\n\n# Prepare the connection\nconn = http.client.HTTPSConnection(url)\n\n# Send the POST request\nconn.request(\"POST\", path, body=json.dumps(data), headers=headers)\n\n# Get the response\nresponse = conn.getresponse()\n\n\n# Read the body line by line\nfor line in response:\n    if line:\n        decoded_line = line.decode('utf-8').strip()\n        if decoded_line == 'data: [DONE]':\n            break\n        if decoded_line.startswith('data: '):\n            try:\n                json_str = decoded_line[6:]\n                chunk = json.loads(json_str)\n                content = chunk['choices'][0]['delta'].get('content', '')\n                if content:\n                    print(content, end='', flush=True)\n            except json.JSONDecodeError:\n                pass\n\n# Close the connection when done\nconn.close()\n\n```\n\n----------------------------------------\n\nTITLE: Python Chat Request Example\nDESCRIPTION: This Python code sends a POST request to the chat API and processes the streaming response. It uses the `http.client` and `json` libraries to send the request and parse the response chunks.  The code extracts and prints the content from each chunk until a '[DONE]' signal is received.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Local Chat API.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport http.client\n\nurl = \"localhost:8002\"\npath = \"/api/kernel2/chat\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Accept\": \"text/event-stream\"\n}\ndata = {\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me about artificial intelligence.\"}\n    ],\n    \"stream\": True\n}\n\nconn = http.client.HTTPConnection(url)\n\nconn.request(\"POST\", path, body=json.dumps(data), headers=headers)\n\nresponse = conn.getresponse()\n\nfor line in response:\n    if line:\n        decoded_line = line.decode('utf-8').strip()\n        if decoded_line == 'data: [DONE]':\n            break\n        if decoded_line.startswith('data: '):\n            try:\n                json_str = decoded_line[6:]\n                chunk = json.loads(json_str)\n                content = chunk['choices'][0]['delta'].get('content', '')\n                if content:\n                    print(content, end='', flush=True)\n            except json.JSONDecodeError:\n                pass\n\nconn.close()\n```\n\n----------------------------------------\n\nTITLE: Deploy SFT Model with llama.cpp\nDESCRIPTION: This command deploys the Supervised Fine-Tuning (SFT) model using llama.cpp, making it accessible via an API endpoint on port 8080. Before deployment, the model must be converted to gguf format. The `--model` parameter specifies the path to the SFT model.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/dpo/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./llama.cpp/build/llama_server --model /path/to/sft_model --port 8080\n```\n\n----------------------------------------\n\nTITLE: Train Model with DPO\nDESCRIPTION: This command initiates the training process for the model using the synthesized DPO data. It allows for specifying hyperparameters such as the number of training epochs (`--num_train_epochs`), learning rate (`--learning_rate`), LoRA rank (`--lora_r`), LoRA alpha (`--lora_alpha`), and batch size (`--batch_size`).\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/dpo/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython lpm_kernel/L2/dpo/dpo_train.py \\\n    --num_train_epochs 2 \\\n    --learning_rate 5e-6 \\\n    --lora_r 32 \\\n    --lora_alpha 64 \\\n    --batch_size 4\n```\n\n----------------------------------------\n\nTITLE: Ollama Model Details Command\nDESCRIPTION: This command shows the details of a specified Ollama model. In this example, it retrieves information about the 'snowflake-arctic-embed:110m' model, including architecture, parameters, context length, embedding length, and quantization details. This command is used to determine the `context length` which is crucial for configuring the `EMBEDDING_MAX_TEXT_LENGTH` in the Second Me application.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Custom Model Config(Ollama).md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Example: ollama show snowflake-arctic-embed:110m\n$ ollama show snowflake-arctic-embed:110m\n\nModel\n  architecture        bert       \n  parameters          108.89M    \n  context length      512        \n  embedding length    768        \n  quantization        F16        \n\nLicense\n  Apache License               \n  Version 2.0, January 2004\n```\n\n----------------------------------------\n\nTITLE: Ollama Chat Request using OpenAI-Compatible API\nDESCRIPTION: This command sends a chat request to the Ollama API using the OpenAI-compatible endpoint. It specifies the model to use ('qwen2.5:0.5b') and a user message ('Why is the sky blue?'). The request is sent as a JSON payload with the 'model' and 'messages' fields.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Custom Model Config(Ollama).md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://127.0.0.1:11434/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n  \"model\": \"qwen2.5:0.5b\",\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Why is the sky blue?\"}\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Execute MLX Training Script\nDESCRIPTION: This command runs the bash script to initiate the MLX training process for fine-tuning the language model. The script allows for configuration via a .yaml file or command-line arguments.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/mlx_training/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./lpm_kernel/L2/mlx_training/train_by_mlx.sh\n```\n\n----------------------------------------\n\nTITLE: Ollama Embedding Request using OpenAI-Compatible API\nDESCRIPTION: This command sends an embedding request to the Ollama API using the OpenAI-compatible endpoint. It specifies the model to use ('snowflake-arctic-embed:110m') and the input text ('Why is the sky blue?'). The request is sent as a JSON payload with the 'model' and 'input' fields.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Custom Model Config(Ollama).md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://127.0.0.1:11434/v1/embeddings -d '{\n  \"model\": \"snowflake-arctic-embed:110m\",\n  \"input\": \"Why is the sky blue?\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Run DPO Pipeline Script\nDESCRIPTION: This command executes the `dpo_pipeline.sh` script, which encapsulates all the necessary steps for the Direct Preference Optimization (DPO) workflow. It streamlines the process for users by automating the data synthesis and model training steps.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/dpo/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash lpm_kernel/L2/dpo/dpo_pipeline.sh\n```\n\n----------------------------------------\n\nTITLE: get_response Function using mindverse tool\nDESCRIPTION: This Python code defines an asynchronous function `get_response` using the `@mindverse.tool()` decorator. It sends a POST request to a specified mindverse model to get a response to a user query. The function takes the query and instance ID as input, constructs the request body with messages and metadata, and returns the response from the server. The function relies on the 'http.client' and 'json' modules, and assumes a variable named 'messages' exists in the scope.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_frontend/public/docs/mcp_content.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@mindverse.tool()\nasync def get_response(query:str, instance_id:str) -> HTTPResponse:\n    \"\"\"\n    Received a response based on public mindverse model.\n\n    Args:\n        query (str): Questions raised by users regarding the mindverse model.\n        instance_id (str): ID used to identify the mindverse model, or url used to identify the mindverse model.\n\n    \"\"\"\n    id = instance_id.split('/')[-1]\n    path = f\"/api/chat/{id}\"\n    headers = {\"Content-Type\": \"application/json\"}\n    messages.append({\"role\": \"user\", \"content\": query})\n\n    data = {\n        \"messages\": messages,\n        \"metadata\": {\n        \"enable_l0_retrieval\": False,\n        \"role_id\": \"default_role\"\n    },\n    \"temperature\": 0.7,\n    \"max_tokens\": 2000,\n    \"stream\": True\n    }\n\n    conn = http.client.HTTPSConnection(url)\n\n    # Send the POST request\n    conn.request(\"POST\", path, body=json.dumps(data), headers=headers)\n\n    # Get the response\n    response = conn.getresponse()\n    return response\n```\n\n----------------------------------------\n\nTITLE: Synthesize DPO Data\nDESCRIPTION: This command executes the `dpo_data.py` script, which synthesizes the data required for Direct Preference Optimization (DPO) training. It leverages the deployed SFT model to generate training data tailored for DPO.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/dpo/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython lpm_kernel/L2/dpo/dpo_data.py\n```\n\n----------------------------------------\n\nTITLE: Environment Variables Configuration for DeepSeek R1\nDESCRIPTION: Configures the environment variables required for utilizing the DeepSeek R1 model. It specifies the model name, API key, and base URL. This is necessary for the application to authenticate and communicate with the DeepSeek R1 API.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nDEEPSEEK_MODEL_NAME=deepseek-*\nDEEPSEEK_API_KEY=your_api_key\nDEEPSEEK_BASE_URL=your_base_url\n```\n\n----------------------------------------\n\nTITLE: Convert and Serve MLX Model\nDESCRIPTION: This command executes the bash script that merges the adapter weights with the base model and starts the model server for inference.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/mlx_training/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./lpm_kernel/L2/mlx_training/convert_and_serve.sh\n```\n\n----------------------------------------\n\nTITLE: cURL Chat Request Example\nDESCRIPTION: This cURL command demonstrates how to send a POST request to the chat API endpoint. It specifies the content type as application/json and accepts text/event-stream for streaming responses, and includes a JSON payload containing the message to send.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Local Chat API.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \\\n  'http://localhost:8002/api/kernel2/chat' \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: text/event-stream' \\\n  -d '{\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Tell me about artificial intelligence.\"}\n    ],\n    \"stream\": true\n  }'\n```\n\n----------------------------------------\n\nTITLE: MCP Server Configuration Example\nDESCRIPTION: This JSON snippet shows the structure of the `config.json` file used to configure MCP servers. It defines a server named 'mindverse', specifying the command to execute (Python interpreter) and the arguments, which include the path to the MCP Python script.  The user needs to replace `{replace-with-your-path}` with the actual path to the script.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_frontend/public/docs/mcp_content.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"mcpServers\": {\n    \"mindverse\": {\n      \"command\": \"python\",\n      \"args\": [\"{replace-with-your-path}/Second-Me/mcp/mcp_public.py\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Merge LoRA Weights\nDESCRIPTION: This command merges the LoRA adapter weights with the base model. It takes the paths to the base model (`--base_model_path`), the LoRA adapter (`--lora_adapter_path`), and the desired output path for the merged model (`--output_model_path`) as arguments. This step is only necessary if LoRA was used during DPO training.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/dpo/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython lpm_kernel/L2/merge_lora_weights.py \\\n--base_model_path \"resources/model/output/merged_model\" \\\n--lora_adapter_path \"resources/model/output/dpo_model/adapter\" \\\n--output_model_path \"resources/model/output/dpo_model/merged_model\"\n```\n\n----------------------------------------\n\nTITLE: Install MLX LM Python Package\nDESCRIPTION: This command installs the mlx-lm package, which provides the necessary components for training and using language models with MLX on Apple Silicon.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/mlx_training/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlx-lm\n```\n\n----------------------------------------\n\nTITLE: Accessing Second Me in the Browser\nDESCRIPTION: Provides the URL to access the Second Me application in a web browser after the services are running, either via local setup or Docker. It listens on port 3000 on localhost.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhttp://localhost:3000\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment from File\nDESCRIPTION: This command creates a new conda environment named 'second-me' using the environment.yml file.  It ensures all required Python packages are installed for the project and activates the environment for use.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README_ja.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda env create -f environment.yml   # これにより、'second-me'という名前の環境が作成されます\nconda activate second-me\n```\n\n----------------------------------------\n\nTITLE: Starting the Services\nDESCRIPTION: This command starts the services required for the Second Me application. This likely involves starting backend servers, frontend development servers, and any other necessary processes.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README_ja.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmake start\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Containers - Bash\nDESCRIPTION: Starts the Docker containers defined in the project using `docker-compose`. Assumes Docker and Docker Compose are installed and configured correctly. This command is part of the Docker setup option.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-up\n```\n\n----------------------------------------\n\nTITLE: Running Integrated Setup - Bash\nDESCRIPTION: Executes the integrated setup process, installing necessary dependencies and configuring the environment. Requires Python 3.10+, Node.js 18+, npm, and basic build tools.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake setup\n```\n\n----------------------------------------\n\nTITLE: Displaying Help Information\nDESCRIPTION: This command displays the available `make` commands and their descriptions. It provides information on various tasks that can be performed using the `make` utility.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README_ja.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake help\n```\n\n----------------------------------------\n\nTITLE: Restarting All Services - Bash\nDESCRIPTION: Restarts all necessary services after the integrated setup. Ensures that all components are running correctly and that changes are applied. Should be run after the setup.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake restart\n```\n\n----------------------------------------\n\nTITLE: Installing the gguf Package\nDESCRIPTION: This command installs the gguf Python package using pip. It fetches the package from PyPI and installs it into the current Python environment.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/gguf-py/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install gguf\n```\n\n----------------------------------------\n\nTITLE: Running the WeChat Bot in Python\nDESCRIPTION: This snippet executes the `wechat_bot.py` script using the Python interpreter, which initiates the WeChat bot. It assumes that Python is installed and the script is present in the current directory.\nSOURCE: https://github.com/mindverse/second-me/blob/master/integrate/Readme.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npython wechat_bot.py\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing Dependencies using Git and Pip\nDESCRIPTION: This snippet clones the Second-Me project repository from GitHub and installs the necessary Python dependencies using pip. It requires Git and Python to be installed. The `requirements.txt` file lists all the dependencies.\nSOURCE: https://github.com/mindverse/second-me/blob/master/integrate/Readme.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/mindverse/second-me.git\ncd second-me/integrate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Rebasing a Feature Branch\nDESCRIPTION: This snippet demonstrates how to update your feature branch with the latest changes from the master branch using rebase. It fetches the latest changes from the origin or upstream repository, then rebases the feature branch onto the master branch.\nSOURCE: https://github.com/mindverse/second-me/blob/master/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Fetch latest changes\ngit fetch origin  # or upstream if you're working with a fork\n\n# Rebase your feature branch\ngit checkout feature/your-feature-name\ngit rebase origin/master  # or upstream/master for forked repos\n```\n\n----------------------------------------\n\nTITLE: Modify Prompt in test_mlx.py (Python)\nDESCRIPTION: This code snippet demonstrates how to modify the prompt within the test_mlx.py script to align with specific training objectives and expected prompt formats. This includes adjusting the 'system' and 'user' roles with corresponding content.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/mlx_training/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"Your custom system prompt here...\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Your test question here\"\n        }\n    ],\n    \"temperature\": 0.7\n}\n```\n\n----------------------------------------\n\nTITLE: Chat Response Chunk Example JSON\nDESCRIPTION: This JSON represents the structure of each chunk in the streaming response from the chat API, delivered in Server-Sent Events (SSE) format. It includes a unique ID, object type, creation timestamp, model identifier, system fingerprint, and the incremental content.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Local Chat API.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion.chunk\",\n  \"created\": 1677652288,\n  \"model\": \"gpt-3.5-turbo\",\n  \"system_fingerprint\": \"fp_44709d6fcb\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"delta\": {\"content\": \"Hello\"},\n      \"finish_reason\": null\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Upstream Remote\nDESCRIPTION: This snippet shows how to add the original Second Me repository as an upstream remote to your local repository, allowing you to fetch updates from the main repository. It also verifies the remotes.\nSOURCE: https://github.com/mindverse/second-me/blob/master/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Add the upstream repository\ngit remote add upstream git@github.com:Mindverse/Second-Me.git\n\n# Verify your remotes\ngit remote -v\n```\n\n----------------------------------------\n\nTITLE: Installing Build and Twine\nDESCRIPTION: This command installs the `build` and `twine` packages, which are necessary for building and publishing the gguf package to PyPI. `build` is used to create distribution packages, and `twine` is used to upload them.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/gguf-py/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install build twine\n```\n\n----------------------------------------\n\nTITLE: Pushing to a Fork\nDESCRIPTION: This snippet shows how to push your feature branch to your forked repository on GitHub, necessary for external contributors after rebasing.\nSOURCE: https://github.com/mindverse/second-me/blob/master/CONTRIBUTING.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit push origin feature/your-feature-name\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedding Max Text Length in .env\nDESCRIPTION: This code snippet shows how to modify the `EMBEDDING_MAX_TEXT_LENGTH` variable in the `Second_Me/.env` file. This configuration is crucial for preventing chunk length overflow, which leads to server-side errors. The value should be set to match the context window of the embedding model being used.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Custom Model Config(Ollama).md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Embedding configurations\n\nEMBEDDING_MAX_TEXT_LENGTH=embedding_model_context_length\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: This command discovers and runs all unit tests in the `gguf-py` directory using the `unittest` module. The `-v` flag enables verbose output.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/gguf-py/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m unittest discover ./gguf-py -v\n```\n\n----------------------------------------\n\nTITLE: Committing Changes\nDESCRIPTION: This snippet illustrates how to stage and commit changes to your local repository using Git.  The git add command stages the changes, and the git commit command creates a new commit with a descriptive message.\nSOURCE: https://github.com/mindverse/second-me/blob/master/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Add your changes\ngit add <filename>\n# Or git add -A for all changes\n\n# Commit with a clear message\ngit commit -m \"feat: add new feature X\"\n```\n\n----------------------------------------\n\nTITLE: Building the Python Package\nDESCRIPTION: This command builds the Python package using the `build` module. It generates distribution archives (e.g., `.tar.gz` and `.whl` files) in the `dist/` directory.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/gguf-py/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m build\n```\n\n----------------------------------------\n\nTITLE: Uploading Package Archives\nDESCRIPTION: This command uploads the generated distribution archives to PyPI using `twine`. It requires proper authentication to upload the package.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/gguf-py/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m twine upload dist/*\n```\n\n----------------------------------------\n\nTITLE: Installing gguf in Editable Mode\nDESCRIPTION: This command installs the gguf package in editable mode, allowing for modifications to the source code to be immediately reflected in the installed package. This is useful for developers working on the package.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/gguf-py/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/llama.cpp/gguf-py\n\npip install --editable .\n```\n\n----------------------------------------\n\nTITLE: Creating a Feature Branch\nDESCRIPTION: This snippet demonstrates how to create a new feature branch from the master branch.  It first fetches the latest changes from the origin or upstream repository, then checks out the master branch, pulls any updates and creates a new branch named 'feature/your-feature-name'.\nSOURCE: https://github.com/mindverse/second-me/blob/master/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# First, ensure you have the latest changes\ngit fetch origin  # or upstream if you're working with a fork\n\n# Checkout the master branch\ngit checkout master\n\ngit pull\n\n# Create your feature branch from master\ngit checkout -b feature/your-feature-name\n```\n\n----------------------------------------\n\nTITLE: Creating a Git Tag\nDESCRIPTION: This command creates an annotated git tag for a new release.  The tag follows the `gguf-vX.X.X` convention and includes a message describing the release.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/gguf-py/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit tag -a gguf-v1.0.0 -m \"Version 1.0 release\"\n```\n\n----------------------------------------\n\nTITLE: Running the Setup Script\nDESCRIPTION: This command executes the setup script using `make`.  The script handles various tasks such as installing system dependencies, creating a Python environment, building llama.cpp, and setting up the frontend environment.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README_ja.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake setup\n```\n\n----------------------------------------\n\nTITLE: ChromaDB Reinitialization Log Messages\nDESCRIPTION: These log messages indicate the system has automatically detected and resolved an embedding dimension mismatch by reinitializing the ChromaDB collections. It provides information on the old and new dimensions.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Embedding Model Switching.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nWarning: Existing 'documents' collection has dimension X, but current model requires Y\nAutomatically reinitializing ChromaDB collections with the new dimension...\nSuccessfully reinitialized ChromaDB collections with the new dimension\n```\n\n----------------------------------------\n\nTITLE: Displaying Available Make Commands - Bash\nDESCRIPTION: Displays the available commands defined in the `Makefile`. Helpful for understanding the project's build and deployment process. Provides insight into available utilities.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake help\n```\n\n----------------------------------------\n\nTITLE: Upgrading Pip\nDESCRIPTION: This command upgrades the pip package installer to the latest version. This can resolve issues related to editable installations or other package management tasks.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/gguf-py/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade pip\n```\n\n----------------------------------------\n\nTITLE: Cloning the Second Me Repository\nDESCRIPTION: This snippet demonstrates how to clone a forked Second Me repository from GitHub to your local machine using Git. Replace USERNAME with your GitHub username.\nSOURCE: https://github.com/mindverse/second-me/blob/master/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd working_dir\n# Replace USERNAME with your GitHub username\ngit clone git@github.com:USERNAME/Second-Me.git\ncd Second-Me\n```\n\n----------------------------------------\n\nTITLE: Pushing Git Tags\nDESCRIPTION: This command pushes the created git tags to the remote repository. This makes the tags visible to other collaborators and triggers the CI/CD pipeline for releases.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/gguf-py/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit push origin --tags\n```\n\n----------------------------------------\n\nTITLE: CoT Prompt Structure\nDESCRIPTION: Defines the structure of the prompt used in the Long CoT implementation. The prompt consists of two main components: `<think>` for reasoning content and `<answer>` for the final content.  The model uses this structure to generate synthetic data with reasoning steps.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/README.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n<think>reasoning_content</think>\n<answer>final_content</answer>\n```\n\n----------------------------------------\n\nTITLE: Cloning the Second-Me Repository\nDESCRIPTION: This command clones the Second-Me repository from GitHub to your local machine. This is the first step in setting up the project and is required to access the source code and related files.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README_ja.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:Mindverse/Second-Me.git\ncd Second-Me\n```\n\n----------------------------------------\n\nTITLE: Cloning the Second Me Repository - Bash\nDESCRIPTION: Clones the Second Me repository from GitHub and navigates into the project directory. This is the first step for both Docker and Integrated setup methods.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:Mindverse/Second-Me.git\ncd Second-Me\n```\n\n----------------------------------------\n\nTITLE: Installing Xcode Command Line Tools\nDESCRIPTION: This command installs the Xcode command line tools, which are required for building and running the Second Me project. It's a prerequisite for macOS users and needs to be executed before proceeding with other installation steps.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README_ja.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nxcode-select --install\n```\n\n----------------------------------------\n\nTITLE: Error Response Example JSON\nDESCRIPTION: This JSON payload represents the error response format for the chat API endpoint. It includes a success flag, an error message, and an error code, providing details about the failure.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Local Chat API.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"success\": false,\n  \"message\": \"Error message\",\n  \"code\": 400\n}\n```\n\n----------------------------------------\n\nTITLE: Accepting Xcode License Agreement\nDESCRIPTION: This command accepts the Xcode license agreement after the command line tools are installed. This is a necessary step to use the tools effectively.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README_ja.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo xcodebuild -license accept\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Conda Mode\nDESCRIPTION: This command sets a custom conda mode in the `.env` file. This is likely a configuration setting used internally by the project's setup scripts to handle conda environments differently.\nSOURCE: https://github.com/mindverse/second-me/blob/master/README_ja.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nCUSTOM_CONDA_MODE=true\n```\n\n----------------------------------------\n\nTITLE: Dependency List\nDESCRIPTION: This lists the Python package dependencies and their minimum required versions for the Second-Me project. These packages are required for the project to function correctly.\nSOURCE: https://github.com/mindverse/second-me/blob/master/integrate/requirements.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nwxpy==0.3.9.8\npython-dotenv==0.19.0\ntorch>=1.8.0\ntransformers>=4.5.0\nnumpy>=1.19.0\n```\n\n----------------------------------------\n\nTITLE: Making Chat Completion Request with cURL\nDESCRIPTION: This cURL command demonstrates how to send a POST request to the chat completion API endpoint, including required headers and a JSON payload containing the message, metadata, temperature, max_tokens, and stream parameters. The `{instance_id}` placeholder needs to be replaced with the actual instance ID.\nSOURCE: https://github.com/mindverse/second-me/blob/master/docs/Public Chat API.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"https://app.secondme.io/api/chat/{instance_id}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Hello, please introduce yourself.\"}\n    ],\n    \"metadata\": {\n      \"enable_l0_retrieval\": false,\n      \"role_id\": \"default_role\"\n    },\n    \"temperature\": 0.7,\n    \"max_tokens\": 2000,\n    \"stream\": true\n  }'\n```\n\n----------------------------------------\n\nTITLE: Installing MCP Dependencies\nDESCRIPTION: This command installs the 'mcp' package using pip, the Python package installer. This is a necessary step to use the MCP functionality within the Second-Me project.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_frontend/public/docs/mcp_content.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mcp\n```\n\n----------------------------------------\n\nTITLE: Test MLX Model Endpoint\nDESCRIPTION: This command executes the Python script to send a test request to the model server and verify its response.  It's essential to adapt the prompt in the test script according to the specific fine-tuning objectives.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/mlx_training/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython lpm_kernel/L2/mlx_training/test_mlx.py\n```\n\n----------------------------------------\n\nTITLE: Run Data Conversion Script (MLX)\nDESCRIPTION: This command executes the data conversion script to transform the training data into a format compatible with MLX. It's crucial to ensure the data paths and configurations are correctly set before running.\nSOURCE: https://github.com/mindverse/second-me/blob/master/lpm_kernel/L2/mlx_training/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython lpm_kernel/L2/mlx_training/data_transform.py\n```"
  }
]
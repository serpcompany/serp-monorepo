[
  {
    "owner": "unclecode",
    "repo": "crawl4ai",
    "content": "TITLE: Extracting Structured Data with LLM using Crawl4AI in Python\nDESCRIPTION: This snippet demonstrates how to use the `Crawl4AI` library to extract structured data from a webpage (OpenAI pricing page) using an LLM. It defines a Pydantic schema (`OpenAIModelFee`) for the desired output structure, configures the `LLMExtractionStrategy` with the schema, instructions, and LLM provider details (handling API tokens for providers like OpenAI or using local models like Ollama), and then runs the `AsyncWebCrawler` to fetch and process the page. The extracted data, formatted according to the schema, is printed.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig, BrowserConfig, CacheMode\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom typing import Dict\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(\n        ..., description=\"Fee for output token for the OpenAI model.\"\n    )\n\nasync def extract_structured_data_using_llm(\n    provider: str, api_token: str = None, extra_headers: Dict[str, str] = None\n):\n    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n\n    if api_token is None and provider != \"ollama\" and not provider.startswith(\"ollama/\") and provider != \"no_token\": # Adjusted check for ollama variations\n        print(f\"API token is required for {provider} unless it's 'ollama' or 'no_token'. Skipping this example.\")\n        return\n\n    browser_config = BrowserConfig(headless=True)\n\n    extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000}\n    if extra_headers:\n        extra_args[\"extra_headers\"] = extra_headers\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        word_count_threshold=1,\n        page_timeout=80000,\n        extraction_strategy=LLMExtractionStrategy(\n            llm_config = LLMConfig(provider=provider,api_token=api_token),\n            schema=OpenAIModelFee.model_json_schema(),\n            extraction_type=\"schema\",\n            instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n            Do not miss any models in the entire content.\"\"\",\n            extra_args=extra_args,\n        ),\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://openai.com/api/pricing/\", config=crawler_config\n        )\n        print(result.extracted_content)\n\nif __name__ == \"__main__\":\n\n    asyncio.run(\n        extract_structured_data_using_llm(\n            provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\")\n        )\n    )\n    # Example for Ollama (assuming Ollama server is running)\n    # asyncio.run(\n    #     extract_structured_data_using_llm(\n    #         provider=\"ollama/llama3\" \n    #     )\n    # )\n```\n\n----------------------------------------\n\nTITLE: Extracting Data from Dynamic Pages with CSS and JS using Crawl4AI in Python\nDESCRIPTION: This example demonstrates handling dynamic web pages and extracting structured data using CSS selectors with `Crawl4AI`. It defines a schema for `JsonCssExtractionStrategy` to specify the data fields and their corresponding CSS selectors for course information on a specific webpage. JavaScript code (`js_click_tabs`) is defined to simulate user interaction (clicking tabs) on the target page. The `AsyncWebCrawler` is configured with JavaScript enabled (`BrowserConfig`) and the JS code to execute (`CrawlerRunConfig`), allowing extraction from content loaded dynamically after tab clicks. The extracted data is parsed from JSON and printed.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    schema = {\n        \"name\": \"KidoCode Courses\",\n        \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n        \"fields\": [\n            {\n                \"name\": \"section_title\",\n                \"selector\": \"h3.heading-50\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"section_description\",\n                \"selector\": \".charge-content\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_name\",\n                \"selector\": \".text-block-93\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_description\",\n                \"selector\": \".course-content-text\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_icon\",\n                \"selector\": \".image-92\",\n                \"type\": \"attribute\",\n                \"attribute\": \"src\",\n            },\n        ],\n    }\n\n    browser_config = BrowserConfig(headless=True, java_script_enabled=True)\n\n    js_click_tabs = \"\"\"\n    (async () => {\n        const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");\n        for(let tab of tabs) {\n            tab.scrollIntoView();\n            tab.click();\n            await new Promise(r => setTimeout(r, 500)); // Wait for content to potentially load\n        }\n    })();\n    \"\"\"\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        js_code=[js_click_tabs],\n        page_timeout=30000 # Increased timeout for JS execution\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config\n        )\n\n        if result.success and result.extracted_content:\n            try:\n              companies = json.loads(result.extracted_content)\n              print(f\"Successfully extracted {len(companies)} items\")\n              if companies:\n                 print(json.dumps(companies[0], indent=2))\n              else:\n                 print(\"No items extracted according to the schema.\")\n            except json.JSONDecodeError:\n                print(\"Error decoding extracted JSON content.\")\n                print(f\"Raw extracted content: {result.extracted_content}\")\n        elif result.success:\n             print(\"Extraction successful, but no content was extracted.\")\n        else:\n             print(f\"Crawling failed: {result.error_message}\")\n\nasync def main():\n    await extract_structured_data_using_css_extractor()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data with LLMs using Crawl4AI in Python\nDESCRIPTION: This Python script demonstrates using `AsyncWebCrawler` along with an LLM for structured data extraction. It defines a Pydantic schema (`OpenAIModelFee`) for the desired output structure, configures `LLMExtractionStrategy` specifying the LLM provider (e.g., OpenAI), API token, the schema, and instructions for extraction. It then crawls a URL (OpenAI pricing page), extracts data matching the schema using the LLM, and prints the extracted content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def main():\n    browser_config = BrowserConfig(verbose=True)\n    run_config = CrawlerRunConfig(\n        word_count_threshold=1,\n        extraction_strategy=LLMExtractionStrategy(\n            # Here you can use any provider that Litellm library supports, for instance: ollama/qwen2\n            # provider=\"ollama/qwen2\", api_token=\"no-token\", \n            llm_config = LLMConfig(provider=\"openai/gpt-4o\", api_token=os.getenv('OPENAI_API_KEY')), \n            schema=OpenAIModelFee.schema(),\n            extraction_type=\"schema\",\n            instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n            Do not miss any models in the entire content. One extracted model JSON format should look like this: \n            {\\\"model_name\\\": \\\"GPT-4\\\", \\\"input_fee\\\": \\\"US$10.00 / 1M tokens\\\", \\\"output_fee\\\": \\\"US$30.00 / 1M tokens\\\"}.\"\"\"\n        ),            \n        cache_mode=CacheMode.BYPASS,\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url='https://openai.com/api/pricing/',\n            config=run_config\n        )\n        print(result.extracted_content)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Building a Knowledge Graph with LLMExtractionStrategy in Python\nDESCRIPTION: A complete example demonstrating how to extract structured knowledge graph data from a webpage using the LLMExtractionStrategy with Pydantic models. The code shows how to define entity and relationship schemas, configure the LLM extraction strategy, and process a webpage to build a knowledge graph.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nimport asyncio\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Entity(BaseModel):\n    name: str\n    description: str\n\nclass Relationship(BaseModel):\n    entity1: Entity\n    entity2: Entity\n    description: str\n    relation_type: str\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[Entity]\n    relationships: List[Relationship]\n\nasync def main():\n    # LLM extraction strategy\n    llm_strat = LLMExtractionStrategy(\n        llmConfig = LlmConfig(provider=\"openai/gpt-4\", api_token=os.getenv('OPENAI_API_KEY')),\n        schema=KnowledgeGraph.schema_json(),\n        extraction_type=\"schema\",\n        instruction=\"Extract entities and relationships from the content. Return valid JSON.\",\n        chunk_token_threshold=1400,\n        apply_chunking=True,\n        input_format=\"html\",\n        extra_args={\"temperature\": 0.1, \"max_tokens\": 1500}\n    )\n\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strat,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:\n        # Example page\n        url = \"https://www.nbcnews.com/business\"\n        result = await crawler.arun(url=url, config=crawl_config)\n\n        if result.success:\n            with open(\"kb_result.json\", \"w\", encoding=\"utf-8\") as f:\n                f.write(result.extracted_content)\n            llm_strat.show_usage()\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Extracting HTML Tables to Pandas DataFrame with Crawl4AI in Python\nDESCRIPTION: This Python script demonstrates extracting HTML tables from a webpage using `AsyncWebCrawler` and converting the first detected table into a pandas DataFrame. It initializes the crawler, sets a `table_score_threshold` in `CrawlerRunConfig` for stricter table detection, crawls a URL (CoinMarketCap), iterates through the results, checks for successfully extracted tables (`result.media[\"tables\"]`), and populates a pandas DataFrame with the headers and rows of the first table found.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ncrawler = AsyncWebCrawler(config=browser_config)\nawait crawler.start()\n\ntry:\n    # Set up scraping parameters\n    crawl_config = CrawlerRunConfig(\n        table_score_threshold=8,  # Strict table detection\n    )\n\n    # Execute market data extraction\n    results: List[CrawlResult] = await crawler.arun(\n        url=\"https://coinmarketcap.com/?page=1\", config=crawl_config\n    )\n\n    # Process results\n    raw_df = pd.DataFrame()\n    for result in results:\n        if result.success and result.media[\"tables\"]:\n            raw_df = pd.DataFrame(\n                result.media[\"tables\"][0][\"rows\"],\n                columns=result.media[\"tables\"][0][\"headers\"],\n            )\n            break\n    print(raw_df.head())\n\nfinally:\n    await crawler.stop()\n```\n\n----------------------------------------\n\nTITLE: LLM-Powered Schema Generation for Data Extraction\nDESCRIPTION: Demonstrates how to use LLM capabilities to automatically generate extraction schemas for web scraping. The example uses JsonCssExtractionStrategy with Gemini LLM to create a schema for extracting product information from HTML.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai import LLMConfig\n\nllm_config = LLMConfig(provider=\"gemini/gemini-1.5-pro\", api_token=\"env:GEMINI_API_KEY\")\n\nschema = JsonCssExtractionStrategy.generate_schema(\n    html=\"<div class='product'><h2>Product Name</h2><span class='price'>$99</span></div>\",\n    llm_config = llm_config,\n    query=\"Extract product name and price\"\n)\nprint(schema)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-based Extraction for OpenAI Pricing Data\nDESCRIPTION: Demonstrates how to use language model-based extraction strategy to retrieve structured pricing data from OpenAI's website. Uses Pydantic for data modeling and supports multiple LLM providers with configurable API tokens and headers.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\nimport os, json\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(\n        ..., description=\"Fee for output token for the OpenAI model.\"\n    )\n\nasync def extract_structured_data_using_llm(provider: str, api_token: str = None, extra_headers: dict = None):\n    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n    \n    # Skip if API token is missing (for providers that require it)\n    if api_token is None and provider != \"ollama\":\n        print(f\"API token is required for {provider}. Skipping this example.\")\n        return\n\n    extra_args = {\"extra_headers\": extra_headers} if extra_headers else {}\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://openai.com/api/pricing/\",\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=provider,\n                api_token=api_token,\n                schema=OpenAIModelFee.schema(),\n                extraction_type=\"schema\",\n                instruction=\"\"\"Extract all model names along with fees for input and output tokens.\"\n                \"{model_name: 'GPT-4', input_fee: 'US$10.00 / 1M tokens', output_fee: 'US$30.00 / 1M tokens'}.\"\"\",\n                **extra_args\n            ),\n            bypass_cache=True,\n        )\n        print(json.loads(result.extracted_content)[:5])\n\n# Usage:\nawait extract_structured_data_using_llm(\"openai/gpt-4o-mini\", os.getenv(\"OPENAI_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Complete AsyncWebCrawler Usage Example in Python\nDESCRIPTION: A comprehensive example demonstrating the usage of AsyncWebCrawler with BrowserConfig, CrawlerRunConfig, and JsonCssExtractionStrategy for web scraping.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nimport json\n\nasync def main():\n    # 1. Browser config\n    browser_cfg = BrowserConfig(\n        browser_type=\"firefox\",\n        headless=False,\n        verbose=True\n    )\n\n    # 2. Run config\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"article.post\",\n        \"fields\": [\n            {\n                \"name\": \"title\", \n                \"selector\": \"h2\", \n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"url\", \n                \"selector\": \"a\", \n                \"type\": \"attribute\", \n                \"attribute\": \"href\"\n            }\n        ]\n    }\n\n    run_cfg = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        word_count_threshold=15,\n        remove_overlay_elements=True,\n        wait_for=\"css:.post\"  # Wait for posts to appear\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/blog\",\n            config=run_cfg\n        )\n\n        if result.success:\n            print(\"Cleaned HTML length:\", len(result.cleaned_html))\n            if result.extracted_content:\n                articles = json.loads(result.extracted_content)\n                print(\"Extracted articles:\", articles[:2])\n        else:\n            print(\"Error:\", result.error_message)\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: New CacheMode-based Caching in Crawl4AI (Python)\nDESCRIPTION: This code snippet showcases the new recommended way of controlling caching in Crawl4AI using the CacheMode enum. It uses CrawlerRunConfig to set the cache mode to BYPASS, which is equivalent to the old 'bypass_cache=True' flag.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cache-modes.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CacheMode\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def use_proxy():\n    # Use CacheMode in CrawlerRunConfig\n    config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)  \n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            config=config  # Pass the configuration object\n        )\n        print(len(result.markdown))\n\nasync def main():\n    await use_proxy()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Generating CSS Extraction Schemas using LLMs with Crawl4AI in Python\nDESCRIPTION: This snippet illustrates how to automatically generate a JSON schema for CSS-based extraction using an LLM via `JsonCssExtractionStrategy.generate_schema`. It shows examples using both OpenAI (requiring an API token via `LLMConfig`) and Ollama (which doesn't require a token). The function takes sample HTML and an `LLMConfig` specifying the provider (e.g., 'openai/gpt-4o' or 'ollama/llama3.3') to generate a reusable schema. This schema can then be used with `JsonCssExtractionStrategy` for efficient, LLM-free data extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Generate a schema (one-time cost)\nhtml = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\"\n\n# Using OpenAI (requires API token)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")  # Required for OpenAI\n)\n\n# Or using Ollama (open source, no token needed)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(schema)\n```\n\n----------------------------------------\n\nTITLE: Building an Advanced Asynchronous Crawler with Multiple Filters (Python)\nDESCRIPTION: Defines an end-to-end async crawling workflow using Crawl4AI, showcasing filter chains for domains, URL patterns, and content types, as well as a keyword relevance scorer to prioritize pages. Utilizes BestFirstCrawlingStrategy and LXMLWebScrapingStrategy for efficient crawling and scraping, processes crawling results with score and depth analysis, and prints statistics by crawl depth. Requires Crawl4AI and asyncio, with all imports explicitly managed.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\\nfrom crawl4ai.deep_crawling.filters import (\\n    FilterChain,\\n    DomainFilter,\\n    URLPatternFilter,\\n    ContentTypeFilter\\n)\\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\\n\\nasync def run_advanced_crawler():\\n    # Create a sophisticated filter chain\\n    filter_chain = FilterChain([\\n        # Domain boundaries\\n        DomainFilter(\\n            allowed_domains=[\\\"docs.example.com\\\"],\\n            blocked_domains=[\\\"old.docs.example.com\\\"]\\n        ),\\n        \\n        # URL patterns to include\\n        URLPatternFilter(patterns=[\\\"*guide*\\\", \\\"*tutorial*\\\", \\\"*blog*\\\"]),\\n        \\n        # Content type filtering\\n        ContentTypeFilter(allowed_types=[\\\"text/html\\\"])\\n    ])\\n\\n    # Create a relevance scorer\\n    keyword_scorer = KeywordRelevanceScorer(\\n        keywords=[\\\"crawl\\\", \\\"example\\\", \\\"async\\\", \\\"configuration\\\"],\\n        weight=0.7\\n    )\\n\\n    # Set up the configuration\\n    config = CrawlerRunConfig(\\n        deep_crawl_strategy=BestFirstCrawlingStrategy(\\n            max_depth=2,\\n            include_external=False,\\n            filter_chain=filter_chain,\\n            url_scorer=keyword_scorer\\n        ),\\n        scraping_strategy=LXMLWebScrapingStrategy(),\\n        stream=True,\\n        verbose=True\\n    )\\n\\n    # Execute the crawl\\n    results = []\\n    async with AsyncWebCrawler() as crawler:\\n        async for result in await crawler.arun(\\\"https://docs.example.com\\\", config=config):\\n            results.append(result)\\n            score = result.metadata.get(\\\"score\\\", 0)\\n            depth = result.metadata.get(\\\"depth\\\", 0)\\n            print(f\\\"Depth: {depth} | Score: {score:.2f} | {result.url}\\\")\\n\\n    # Analyze the results\\n    print(f\\\"Crawled {len(results)} high-value pages\\\")\\n    print(f\\\"Average score: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}\\\")\\n\\n    # Group by depth\\n    depth_counts = {}\\n    for result in results:\\n        depth = result.metadata.get(\\\"depth\\\", 0)\\n        depth_counts[depth] = depth_counts.get(depth, 0) + 1\\n\\n    print(\\\"Pages crawled by depth:\\\")\\n    for depth, count in sorted(depth_counts.items()):\\n        print(f\\\"  Depth {depth}: {count} pages\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    asyncio.run(run_advanced_crawler())\\n\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM Extraction Strategy in Python\nDESCRIPTION: Configuration class for LLM-based extraction strategy with parameters for provider settings, extraction configuration, chunking parameters, and API configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nLLMExtractionStrategy(\n    # Required Parameters\n    provider: str = DEFAULT_PROVIDER,     # LLM provider (e.g., \"ollama/llama2\")\n    api_token: Optional[str] = None,      # API token\n    \n    # Extraction Configuration\n    instruction: str = None,              # Custom extraction instruction\n    schema: Dict = None,                  # Pydantic model schema for structured data\n    extraction_type: str = \"block\",       # \"block\" or \"schema\"\n    \n    # Chunking Parameters\n    chunk_token_threshold: int = 4000,    # Maximum tokens per chunk\n    overlap_rate: float = 0.1,           # Overlap between chunks\n    word_token_rate: float = 0.75,       # Word to token conversion rate\n    apply_chunking: bool = True,         # Enable/disable chunking\n    \n    # API Configuration\n    base_url: str = None,                # Base URL for API\n    extra_args: Dict = {},               # Additional provider arguments\n    verbose: bool = False                # Enable verbose logging\n)\n```\n\n----------------------------------------\n\nTITLE: Complete Example of LLM-based Product Extraction in Python\nDESCRIPTION: A full working example showing how to define a Pydantic model, create an LLM extraction strategy, configure the crawler, and process the results. This includes error handling and usage tracking.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: str\n\nasync def main():\n    # 1. Define the LLM extraction strategy\n    llm_strategy = LLMExtractionStrategy(\n        llm_config = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv('OPENAI_API_KEY')),\n        schema=Product.schema_json(), # Or use model_json_schema()\n        extraction_type=\"schema\",\n        instruction=\"Extract all product objects with 'name' and 'price' from the content.\",\n        chunk_token_threshold=1000,\n        overlap_rate=0.0,\n        apply_chunking=True,\n        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n        extra_args={\"temperature\": 0.0, \"max_tokens\": 800}\n    )\n\n    # 2. Build the crawler config\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strategy,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    # 3. Create a browser config if needed\n    browser_cfg = BrowserConfig(headless=True)\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        # 4. Let's say we want to crawl a single page\n        result = await crawler.arun(\n            url=\"https://example.com/products\",\n            config=crawl_config\n        )\n\n        if result.success:\n            # 5. The extracted content is presumably JSON\n            data = json.loads(result.extracted_content)\n            print(\"Extracted items:\", data)\n            \n            # 6. Show usage stats\n            llm_strategy.show_usage()  # prints token usage\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawl4AI Browser and Run Settings in Python\nDESCRIPTION: This snippet shows how to configure the Crawl4AI crawler using `BrowserConfig` and `CrawlerRunConfig`. `BrowserConfig` is used to set browser options like running in headless mode (`headless=True`). `CrawlerRunConfig` controls run-specific settings, such as bypassing the cache (`cache_mode=CacheMode.BYPASS`) to ensure fresh content is fetched. The configurations are passed to the `AsyncWebCrawler` constructor and the `arun` method respectively.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    browser_conf = BrowserConfig(headless=True)  # or False to see the browser\n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler(config=browser_conf) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_conf\n        )\n        print(result.markdown)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: LLM-Based Extraction in Crawl4AI\nDESCRIPTION: This example demonstrates how to use Crawl4AI's LLM-based extraction strategy to extract structured data from crawled content. It uses a Pydantic model to define the extraction schema and leverages an LLM (like GPT-4) to parse content into headline and summary fields.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleData(BaseModel):\n    headline: str\n    summary: str\n\nasync def main():\n    llm_strategy = LLMExtractionStrategy(\n        llm_config = LLMConfig(provider=\"openai/gpt-4\",api_token=\"sk-YOUR_API_KEY\")\n        schema=ArticleData.schema(),\n        extraction_type=\"schema\",\n        instruction=\"Extract 'headline' and a short 'summary' from the content.\"\n    )\n\n    config = CrawlerRunConfig(\n        exclude_external_links=True,\n        word_count_threshold=20,\n        extraction_strategy=llm_strategy\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n        article = json.loads(result.extracted_content)\n        print(article)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Crawling Raw HTML Content with Crawl4AI in Python\nDESCRIPTION: This example demonstrates how to crawl raw HTML content by prefixing the HTML string with 'raw:'. The AsyncWebCrawler processes the HTML string directly and converts it to markdown format.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/local-files.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def crawl_raw_html():\n    raw_html = \"<html><body><h1>Hello, World!</h1></body></html>\"\n    raw_html_url = f\"raw:{raw_html}\"\n    config = CrawlerRunConfig(bypass_cache=True)\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=raw_html_url, config=config)\n        if result.success:\n            print(\"Markdown Content from Raw HTML:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl raw HTML: {result.error_message}\")\n\nasyncio.run(crawl_raw_html())\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Extraction Sensitivity with Crawl4AI in Python\nDESCRIPTION: Shows how to configure the table detection sensitivity in Crawl4AI using `CrawlerRunConfig`. By setting the `table_score_threshold` parameter (default is 7), users can adjust how readily HTML tables are identified and extracted as data tables based on criteria like headers, consistency, and text density. A lower value increases sensitivity, potentially capturing more tables.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n```python\ncrawler_cfg = CrawlerRunConfig(\n    table_score_threshold=5  # Lower value = more tables detected (default: 7)\n)\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Element Extraction Strategy Class\nDESCRIPTION: Abstract base class that defines a framework for extracting structured JSON data from HTML content using schemas. Supports hierarchical data extraction, field transformations, and computed fields. The class provides comprehensive extraction capabilities through multiple specialized methods for handling different field types and extraction scenarios.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_74\n\nLANGUAGE: python\nCODE:\n```\nclass JsonElementExtractionStrategy(ExtractionStrategy):\n    DEL = \"\\n\"\n\n    def __init__(self, schema: Dict[str, Any], **kwargs):\n        super().__init__(**kwargs)\n        self.schema = schema\n        self.verbose = kwargs.get(\"verbose\", False)\n\n    def extract(self, url: str, html_content: str, *q, **kwargs) -> List[Dict[str, Any]]:\n        parsed_html = self._parse_html(html_content)\n        base_elements = self._get_base_elements(parsed_html, self.schema[\"baseSelector\"])\n\n        results = []\n        for element in base_elements:\n            item = {}\n            if \"baseFields\" in self.schema:\n                for field in self.schema[\"baseFields\"]:\n                    value = self._extract_single_field(element, field)\n                    if value is not None:\n                        item[field[\"name\"]] = value\n\n            field_data = self._extract_item(element, self.schema[\"fields\"])\n            item.update(field_data)\n\n            if item:\n                results.append(item)\n\n        return results\n\n    @abstractmethod\n    def _parse_html(self, html_content: str):\n        pass\n\n    @abstractmethod\n    def _get_base_elements(self, parsed_html, selector: str):\n        pass\n\n    @abstractmethod\n    def _get_elements(self, element, selector: str):\n        pass\n\n    def _extract_field(self, element, field):\n        try:\n            if field[\"type\"] == \"nested\":\n                nested_elements = self._get_elements(element, field[\"selector\"])\n                nested_element = nested_elements[0] if nested_elements else None\n                return self._extract_item(nested_element, field[\"fields\"]) if nested_element else {}\n\n            if field[\"type\"] == \"list\":\n                elements = self._get_elements(element, field[\"selector\"])\n                return [self._extract_list_item(el, field[\"fields\"]) for el in elements]\n\n            if field[\"type\"] == \"nested_list\":\n                elements = self._get_elements(element, field[\"selector\"])\n                return [self._extract_item(el, field[\"fields\"]) for el in elements]\n\n            return self._extract_single_field(element, field)\n        except Exception as e:\n            if self.verbose:\n                print(f\"Error extracting field {field['name']}: {str(e)}\")\n            return field.get(\"default\")\n\n    def _extract_single_field(self, element, field):\n        if \"selector\" in field:\n            selected = self._get_elements(element, field[\"selector\"])\n            if not selected:\n                return field.get(\"default\")\n            selected = selected[0]\n        else:\n            selected = element\n\n        value = None\n        if field[\"type\"] == \"text\":\n            value = self._get_element_text(selected)\n        elif field[\"type\"] == \"attribute\":\n            value = self._get_element_attribute(selected, field[\"attribute\"])\n        elif field[\"type\"] == \"html\":\n            value = self._get_element_html(selected)\n        elif field[\"type\"] == \"regex\":\n            text = self._get_element_text(selected)\n            match = re.search(field[\"pattern\"], text)\n            value = match.group(1) if match else None\n\n        if \"transform\" in field:\n            value = self._apply_transform(value, field[\"transform\"])\n\n        return value if value is not None else field.get(\"default\")\n\n    def _extract_list_item(self, element, fields):\n        item = {}\n        for field in fields:\n            value = self._extract_single_field(element, field)\n            if value is not None:\n                item[field[\"name\"]] = value\n        return item\n\n    def _extract_item(self, element, fields):\n        item = {}\n        for field in fields:\n            if field[\"type\"] == \"computed\":\n                value = self._compute_field(item, field)\n            else:\n                value = self._extract_field(element, field)\n            if value is not None:\n                item[field[\"name\"]] = value\n        return item\n\n    def _apply_transform(self, value, transform):\n        if transform == \"lowercase\":\n            return value.lower()\n        elif transform == \"uppercase\":\n            return value.upper()\n        elif transform == \"strip\":\n            return value.strip()\n        return value\n\n    def _compute_field(self, item, field):\n        try:\n            if \"expression\" in field:\n                return eval(field[\"expression\"], {}, item)\n            elif \"function\" in field:\n                return field[\"function\"](item)\n        except Exception as e:\n            if self.verbose:\n                print(f\"Error computing field {field['name']}: {str(e)}\")\n            return field.get(\"default\")\n\n    def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]:\n        combined_html = self.DEL.join(sections)\n        return self.extract(url, combined_html, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Extracting Data with XPath and Raw HTML in Python\nDESCRIPTION: An example showing how to use JsonXPathExtractionStrategy with raw HTML input. This demonstrates XPath selectors instead of CSS and uses the raw:// scheme to process HTML without making a network request, which is useful for testing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonXPathExtractionStrategy\n\nasync def extract_crypto_prices_xpath():\n    # 1. Minimal dummy HTML with some repeating rows\n    dummy_html = \"\"\"\n    <html>\n      <body>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Bitcoin</h2>\n          <span class='coin-price'>$28,000</span>\n        </div>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Ethereum</h2>\n          <span class='coin-price'>$1,800</span>\n        </div>\n      </body>\n    </html>\n    \"\"\"\n\n    # 2. Define the JSON schema (XPath version)\n    schema = {\n        \"name\": \"Crypto Prices via XPath\",\n        \"baseSelector\": \"//div[@class='crypto-row']\",\n        \"fields\": [\n            {\n                \"name\": \"coin_name\",\n                \"selector\": \".//h2[@class='coin-name']\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \".//span[@class='coin-price']\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n\n    # 3. Place the strategy in the CrawlerRunConfig\n    config = CrawlerRunConfig(\n        extraction_strategy=JsonXPathExtractionStrategy(schema, verbose=True)\n    )\n\n    # 4. Use raw:// scheme to pass dummy_html directly\n    raw_url = f\"raw://{dummy_html}\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=raw_url,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin rows\")\n        if data:\n            print(\"First item:\", data[0])\n\nasyncio.run(extract_crypto_prices_xpath())\n```\n\n----------------------------------------\n\nTITLE: Basic Crawl4AI Usage Example\nDESCRIPTION: Minimal Python script demonstrating a basic web crawl using AsyncWebCrawler to verify installation.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_81\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.example.com\",\n        )\n        print(result.markdown[:300])  # Show the first 300 characters of extracted text\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Defining arun_many() Function for Concurrent URL Crawling in Python\nDESCRIPTION: This code snippet defines the arun_many() function, which is used to crawl multiple URLs concurrently or in batches. It takes a list of URLs, an optional configuration, and an optional dispatcher for concurrency control.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nasync def arun_many(\n    urls: Union[List[str], List[Any]],\n    config: Optional[CrawlerRunConfig] = None,\n    dispatcher: Optional[BaseDispatcher] = None,\n    ...\n) -> Union[List[CrawlResult], AsyncGenerator[CrawlResult, None]]:\n    \"\"\"\n    Crawl multiple URLs concurrently or in batches.\n\n    :param urls: A list of URLs (or tasks) to crawl.\n    :param config: (Optional) A default `CrawlerRunConfig` applying to each crawl.\n    :param dispatcher: (Optional) A concurrency controller (e.g. MemoryAdaptiveDispatcher).\n    ...\n    :return: Either a list of `CrawlResult` objects, or an async generator if streaming is enabled.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Hooks in AsyncWebCrawler with Python\nDESCRIPTION: A comprehensive example demonstrating how to configure and implement all available hooks in Crawl4AI's AsyncWebCrawler. The code shows how to set up browser configuration, define hook functions for different stages of the crawling process, attach hooks to the crawler, and execute a crawl operation.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/hooks-auth.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom playwright.async_api import Page, BrowserContext\n\nasync def main():\n    print(\"ðŸ”— Hooks Example: Demonstrating recommended usage\")\n\n    # 1) Configure the browser\n    browser_config = BrowserConfig(\n        headless=True,\n        verbose=True\n    )\n\n    # 2) Configure the crawler run\n    crawler_run_config = CrawlerRunConfig(\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n        wait_for=\"body\",\n        cache_mode=CacheMode.BYPASS\n    )\n\n    # 3) Create the crawler instance\n    crawler = AsyncWebCrawler(config=browser_config)\n\n    #\n    # Define Hook Functions\n    #\n\n    async def on_browser_created(browser, **kwargs):\n        # Called once the browser instance is created (but no pages or contexts yet)\n        print(\"[HOOK] on_browser_created - Browser created successfully!\")\n        # Typically, do minimal setup here if needed\n        return browser\n\n    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):\n        # Called right after a new page + context are created (ideal for auth or route config).\n        print(\"[HOOK] on_page_context_created - Setting up page & context.\")\n        \n        # Example 1: Route filtering (e.g., block images)\n        async def route_filter(route):\n            if route.request.resource_type == \"image\":\n                print(f\"[HOOK] Blocking image request: {route.request.url}\")\n                await route.abort()\n            else:\n                await route.continue_()\n\n        await context.route(\"**\", route_filter)\n\n        # Example 2: (Optional) Simulate a login scenario\n        # (We do NOT create or close pages here, just do quick steps if needed)\n        # e.g., await page.goto(\"https://example.com/login\")\n        # e.g., await page.fill(\"input[name='username']\", \"testuser\")\n        # e.g., await page.fill(\"input[name='password']\", \"password123\")\n        # e.g., await page.click(\"button[type='submit']\")\n        # e.g., await page.wait_for_selector(\"#welcome\")\n        # e.g., await context.add_cookies([...])\n        # Then continue\n\n        # Example 3: Adjust the viewport\n        await page.set_viewport_size({\"width\": 1080, \"height\": 600})\n        return page\n\n    async def before_goto(\n        page: Page, context: BrowserContext, url: str, **kwargs\n    ):\n        # Called before navigating to each URL.\n        print(f\"[HOOK] before_goto - About to navigate: {url}\")\n        # e.g., inject custom headers\n        await page.set_extra_http_headers({\n            \"Custom-Header\": \"my-value\"\n        })\n        return page\n\n    async def after_goto(\n        page: Page, context: BrowserContext, \n        url: str, response, **kwargs\n    ):\n        # Called after navigation completes.\n        print(f\"[HOOK] after_goto - Successfully loaded: {url}\")\n        # e.g., wait for a certain element if we want to verify\n        try:\n            await page.wait_for_selector('.content', timeout=1000)\n            print(\"[HOOK] Found .content element!\")\n        except:\n            print(\"[HOOK] .content not found, continuing anyway.\")\n        return page\n\n    async def on_user_agent_updated(\n        page: Page, context: BrowserContext, \n        user_agent: str, **kwargs\n    ):\n        # Called whenever the user agent updates.\n        print(f\"[HOOK] on_user_agent_updated - New user agent: {user_agent}\")\n        return page\n\n    async def on_execution_started(page: Page, context: BrowserContext, **kwargs):\n        # Called after custom JavaScript execution begins.\n        print(\"[HOOK] on_execution_started - JS code is running!\")\n        return page\n\n    async def before_retrieve_html(page: Page, context: BrowserContext, **kwargs):\n        # Called before final HTML retrieval.\n        print(\"[HOOK] before_retrieve_html - We can do final actions\")\n        # Example: Scroll again\n        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n        return page\n\n    async def before_return_html(\n        page: Page, context: BrowserContext, html: str, **kwargs\n    ):\n        # Called just before returning the HTML in the result.\n        print(f\"[HOOK] before_return_html - HTML length: {len(html)}\")\n        return page\n\n    #\n    # Attach Hooks\n    #\n\n    crawler.crawler_strategy.set_hook(\"on_browser_created\", on_browser_created)\n    crawler.crawler_strategy.set_hook(\n        \"on_page_context_created\", on_page_context_created\n    )\n    crawler.crawler_strategy.set_hook(\"before_goto\", before_goto)\n    crawler.crawler_strategy.set_hook(\"after_goto\", after_goto)\n    crawler.crawler_strategy.set_hook(\n        \"on_user_agent_updated\", on_user_agent_updated\n    )\n    crawler.crawler_strategy.set_hook(\n        \"on_execution_started\", on_execution_started\n    )\n    crawler.crawler_strategy.set_hook(\n        \"before_retrieve_html\", before_retrieve_html\n    )\n    crawler.crawler_strategy.set_hook(\n        \"before_return_html\", before_return_html\n    )\n\n    await crawler.start()\n\n    # 4) Run the crawler on an example page\n    url = \"https://example.com\"\n    result = await crawler.arun(url, config=crawler_run_config)\n    \n    if result.success:\n        print(\"\\nCrawled URL:\", result.url)\n        print(\"HTML length:\", len(result.html))\n    else:\n        print(\"Error:\", result.error_message)\n\n    await crawler.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing PruningContentFilter in Crawl4AI\nDESCRIPTION: Example showing how to set up and use PruningContentFilter to remove low-value content based on text density, link density, and tag importance. Includes configuration of threshold parameters and markdown generation.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # Step 1: Create a pruning filter\n    prune_filter = PruningContentFilter(\n        # Lower â†’ more content retained, higher â†’ more content pruned\n        threshold=0.45,           \n        # \"fixed\" or \"dynamic\"\n        threshold_type=\"dynamic\",  \n        # Ignore nodes with <5 words\n        min_word_threshold=5      \n    )\n\n    # Step 2: Insert it into a Markdown Generator\n    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\n    \n    # Step 3: Pass it to CrawlerRunConfig\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\", \n            config=config\n        )\n        \n        if result.success:\n            # 'fit_markdown' is your pruned content, focusing on \"denser\" text\n            print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n            print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Combining Page Interaction with Extraction Strategy in Crawl4AI\nDESCRIPTION: This code demonstrates how to set up a JsonCssExtractionStrategy with CrawlerRunConfig to extract content after dynamic page interaction. It defines a schema for extracting commit information from a list and configures the crawler with JavaScript code for pagination, wait conditions, and the extraction strategy.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Commits\",\n    \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\"}\n    ]\n}\nconfig = CrawlerRunConfig(\n    session_id=\"ts_commits_session\",\n    js_code=js_next_page,\n    wait_for=wait_for_more,\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n```\n\n----------------------------------------\n\nTITLE: Complete Network and Console Capture Example in Python with Crawl4AI\nDESCRIPTION: A comprehensive example that demonstrates how to capture and analyze network requests and console messages during web crawling. The code processes captured data to identify API calls, categorize message types, and export the results to a JSON file for detailed analysis.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_153\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Enable both network request capture and console message capture\n    config = CrawlerRunConfig(\n        capture_network_requests=True,\n        capture_console_messages=True\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=config\n        )\n        \n        if result.success:\n            # Analyze network requests\n            if result.network_requests:\n                print(f\"Captured {len(result.network_requests)} network events\")\n                \n                # Count request types\n                request_count = len([r for r in result.network_requests if r.get(\"event_type\") == \"request\"])\n                response_count = len([r for r in result.network_requests if r.get(\"event_type\") == \"response\"])\n                failed_count = len([r for r in result.network_requests if r.get(\"event_type\") == \"request_failed\"])\n                \n                print(f\"Requests: {request_count}, Responses: {response_count}, Failed: {failed_count}\")\n                \n                # Find API calls\n                api_calls = [r for r in result.network_requests \n                            if r.get(\"event_type\") == \"request\" and \"api\" in r.get(\"url\", \"\")]\n                if api_calls:\n                    print(f\"Detected {len(api_calls)} API calls:\")\n                    for call in api_calls[:3]:  # Show first 3\n                        print(f\"  - {call.get('method')} {call.get('url')}\")\n            \n            # Analyze console messages\n            if result.console_messages:\n                print(f\"Captured {len(result.console_messages)} console messages\")\n                \n                # Group by type\n                message_types = {}\n                for msg in result.console_messages:\n                    msg_type = msg.get(\"type\", \"unknown\")\n                    message_types[msg_type] = message_types.get(msg_type, 0) + 1\n                \n                print(\"Message types:\", message_types)\n                \n                # Show errors (often the most important)\n                errors = [msg for msg in result.console_messages if msg.get(\"type\") == \"error\"]\n                if errors:\n                    print(f\"Found {len(errors)} console errors:\")\n                    for err in errors[:2]:  # Show first 2\n                        print(f\"  - {err.get('text', '')[:100]}\")\n            \n            # Export all captured data to a file for detailed analysis\n            with open(\"network_capture.json\", \"w\") as f:\n                json.dump({\n                    \"url\": result.url,\n                    \"network_requests\": result.network_requests or [],\n                    \"console_messages\": result.console_messages or []\n                }, f, indent=2)\n            \n            print(\"Exported detailed capture data to network_capture.json\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Crawling a Local HTML File with Crawl4AI in Python\nDESCRIPTION: This snippet shows how to use Crawl4AI to process a local HTML file by prefixing the file path with 'file://'. The AsyncWebCrawler processes the local file and converts it to markdown format.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/local-files.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def crawl_local_file():\n    local_file_path = \"/path/to/apple.html\"  # Replace with your file path\n    file_url = f\"file://{local_file_path}\"\n    config = CrawlerRunConfig(bypass_cache=True)\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=file_url, config=config)\n        if result.success:\n            print(\"Markdown Content from Local File:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl local file: {result.error_message}\")\n\nasyncio.run(crawl_local_file())\n```\n\n----------------------------------------\n\nTITLE: Minimal Example of Using BrowserConfig in Python for Crawl4AI\nDESCRIPTION: This example demonstrates how to create a BrowserConfig instance and use it with AsyncWebCrawler in Crawl4AI. It sets up a Firefox browser in visible mode with text-only crawling enabled.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_conf = BrowserConfig(\n    browser_type=\"firefox\",\n    headless=False,\n    text_mode=True\n)\n\nasync with AsyncWebCrawler(config=browser_conf) as crawler:\n    result = await crawler.arun(\"https://example.com\")\n    print(result.markdown[:300])\n```\n\n----------------------------------------\n\nTITLE: Dynamic Content Handling with Crawl4AI\nDESCRIPTION: This snippet shows how to handle dynamic content using Crawl4AI. It includes options for waiting for specific conditions and executing JavaScript code to interact with the page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl_dynamic_content():\n    # You can use wait_for to wait for a condition to be met before returning the result\n    # wait_for = \"\"\"() => {\n    #     return Array.from(document.querySelectorAll('article.tease-card')).length > 10;\n    # }\"\"\"\n\n    # wait_for can be also just a css selector\n    # wait_for = \"article.tease-card:nth-child(10)\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        js_code = [\n            \"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\"\n        ]\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            # wait_for=wait_for,\n            bypass_cache=True,\n        )\n        print(result.markdown.raw_markdown[:500])  # Print first 500 characters\n\nasyncio.run(crawl_dynamic_content())\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running AsyncWebCrawler with Extraction and LLM Filtering in Python\nDESCRIPTION: This snippet demonstrates the complete process of setting up and running an AsyncWebCrawler. It includes configuring the browser, defining an extraction strategy, setting up LLM-based content filtering, and executing the crawl. The code showcases the use of BrowserConfig, CrawlerRunConfig, and LLMConfig to customize the crawling behavior.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    # 1) Browser config: headless, bigger viewport, no proxy\n    browser_conf = BrowserConfig(\n        headless=True,\n        viewport_width=1280,\n        viewport_height=720\n    )\n\n    # 2) Example extraction strategy\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"div.article\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n    extraction = JsonCssExtractionStrategy(schema)\n\n    # 3) Example LLM content filtering\n\n    gemini_config = LLMConfig(\n        provider=\"gemini/gemini-1.5-pro\" \n        api_token = \"env:GEMINI_API_TOKEN\"\n    )\n\n    # Initialize LLM filter with specific instruction\n    filter = LLMContentFilter(\n        llm_config=gemini_config,  # or your preferred provider\n        instruction=\"\"\"\n        Focus on extracting the core educational content.\n        Include:\n        - Key concepts and explanations\n        - Important code examples\n        - Essential technical details\n        Exclude:\n        - Navigation elements\n        - Sidebars\n        - Footer content\n        Format the output as clean markdown with proper code blocks and headers.\n        \"\"\",\n        chunk_token_threshold=500,  # Adjust based on your needs\n        verbose=True\n    )\n\n    md_generator = DefaultMarkdownGenerator(\n    content_filter=filter,\n    options={\"ignore_links\": True}\n\n    # 4) Crawler run config: skip cache, use extraction\n    run_conf = CrawlerRunConfig(\n        markdown_generator=md_generator,\n        extraction_strategy=extraction,\n        cache_mode=CacheMode.BYPASS,\n    )\n\n    async with AsyncWebCrawler(config=browser_conf) as crawler:\n        # 4) Execute the crawl\n        result = await crawler.arun(url=\"https://example.com/news\", config=run_conf)\n\n        if result.success:\n            print(\"Extracted content:\", result.extracted_content)\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Configuring BestFirstCrawlingStrategy with KeywordRelevanceScorer in Python\nDESCRIPTION: Explains how to configure `BestFirstCrawlingStrategy` for intelligent, prioritized crawling using a scorer. It initializes a `KeywordRelevanceScorer` with specific keywords and a weight, then assigns this scorer to the `url_scorer` parameter of the strategy. This approach evaluates discovered URLs and visits higher-scoring ones first, focusing the crawl on relevant content. Optional `max_pages` parameter limits the total pages crawled. Depends on `BestFirstCrawlingStrategy` and `KeywordRelevanceScorer` from `crawl4ai.deep_crawling`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n\n# Create a scorer\nscorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n    weight=0.7\n)\n\n# Configure the strategy\nstrategy = BestFirstCrawlingStrategy(\n    max_depth=2,\n    include_external=False,\n    url_scorer=scorer,\n    max_pages=25,              # Maximum number of pages to crawl (optional)\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Markdown Generation with DefaultMarkdownGenerator in Python\nDESCRIPTION: Demonstrates minimal setup for generating markdown from a webpage using DefaultMarkdownGenerator without additional filtering. Uses AsyncWebCrawler to fetch content and convert it to markdown.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator()\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        \n        if result.success:\n            print(\"Raw Markdown Output:\\n\")\n            print(result.markdown)  # The unfiltered markdown from the page\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Hook Functions with AsyncWebCrawler in Python\nDESCRIPTION: A comprehensive example demonstrating how to use hook functions with the AsyncWebCrawler class. The code shows the implementation of various hooks that intercept different stages of the crawling process, from browser creation to HTML retrieval. It includes custom handling for page context creation, user agent updates, and navigation events.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_162\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom playwright.async_api import Page, BrowserContext\n\n\nasync def main():\n    print(\"ðŸ”— Hooks Example: Demonstrating different hook use cases\")\n\n    # Configure browser settings\n    browser_config = BrowserConfig(headless=True)\n\n    # Configure crawler settings\n    crawler_run_config = CrawlerRunConfig(\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n        wait_for=\"body\",\n        cache_mode=CacheMode.BYPASS,\n    )\n\n    # Create crawler instance\n    crawler = AsyncWebCrawler(config=browser_config)\n\n    # Define and set hook functions\n    async def on_browser_created(browser, context: BrowserContext, **kwargs):\n        \"\"\"Hook called after the browser is created\"\"\"\n        print(\"[HOOK] on_browser_created - Browser is ready!\")\n        # Example: Set a cookie that will be used for all requests\n        return browser\n\n    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):\n        \"\"\"Hook called after a new page and context are created\"\"\"\n        print(\"[HOOK] on_page_context_created - New page created!\")\n        # Example: Set default viewport size\n        await context.add_cookies(\n            [\n                {\n                    \"name\": \"session_id\",\n                    \"value\": \"example_session\",\n                    \"domain\": \".example.com\",\n                    \"path\": \"/\",\n                }\n            ]\n        )\n        await page.set_viewport_size({\"width\": 1080, \"height\": 800})\n        return page\n\n    async def on_user_agent_updated(\n        page: Page, context: BrowserContext, user_agent: str, **kwargs\n    ):\n        \"\"\"Hook called when the user agent is updated\"\"\"\n        print(f\"[HOOK] on_user_agent_updated - New user agent: {user_agent}\")\n        return page\n\n    async def on_execution_started(page: Page, context: BrowserContext, **kwargs):\n        \"\"\"Hook called after custom JavaScript execution\"\"\"\n        print(\"[HOOK] on_execution_started - Custom JS executed!\")\n        return page\n\n    async def before_goto(page: Page, context: BrowserContext, url: str, **kwargs):\n        \"\"\"Hook called before navigating to each URL\"\"\"\n        print(f\"[HOOK] before_goto - About to visit: {url}\")\n        # Example: Add custom headers for the request\n        await page.set_extra_http_headers({\"Custom-Header\": \"my-value\"})\n        return page\n\n    async def after_goto(\n        page: Page, context: BrowserContext, url: str, response: dict, **kwargs\n    ):\n        \"\"\"Hook called after navigating to each URL\"\"\"\n        print(f\"[HOOK] after_goto - Successfully loaded: {url}\")\n        # Example: Wait for a specific element to be loaded\n        try:\n            await page.wait_for_selector(\".content\", timeout=1000)\n            print(\"Content element found!\")\n        except:\n            print(\"Content element not found, continuing anyway\")\n        return page\n\n    async def before_retrieve_html(page: Page, context: BrowserContext, **kwargs):\n        \"\"\"Hook called before retrieving the HTML content\"\"\"\n        print(\"[HOOK] before_retrieve_html - About to get HTML content\")\n        # Example: Scroll to bottom to trigger lazy loading\n        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n        return page\n\n    async def before_return_html(\n        page: Page, context: BrowserContext, html: str, **kwargs\n    ):\n        \"\"\"Hook called before returning the HTML content\"\"\"\n        print(f\"[HOOK] before_return_html - Got HTML content (length: {len(html)})\")\n        # Example: You could modify the HTML content here if needed\n        return page\n\n    # Set all the hooks\n    crawler.crawler_strategy.set_hook(\"on_browser_created\", on_browser_created)\n    crawler.crawler_strategy.set_hook(\n        \"on_page_context_created\", on_page_context_created\n    )\n    crawler.crawler_strategy.set_hook(\"on_user_agent_updated\", on_user_agent_updated)\n    crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n    crawler.crawler_strategy.set_hook(\"before_goto\", before_goto)\n    crawler.crawler_strategy.set_hook(\"after_goto\", after_goto)\n    crawler.crawler_strategy.set_hook(\"before_retrieve_html\", before_retrieve_html)\n    crawler.crawler_strategy.set_hook(\"before_return_html\", before_return_html)\n\n    await crawler.start()\n\n    # Example usage: crawl a simple website\n    url = \"https://example.com\"\n    result = await crawler.arun(url, config=crawler_run_config)\n    print(f\"\\nCrawled URL: {result.url}\")\n    print(f\"HTML length: {len(result.html)}\")\n\n    await crawler.close()\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Web Crawling Implementation with Caching and Error Handling\nDESCRIPTION: This code snippet is part of a web crawler implementation that handles fetching web content, processes HTML, and manages caching. It includes proxy rotation, robots.txt checking, and comprehensive error handling. The code also logs various steps of the crawling process and returns standardized result objects.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nif config.pdf and not pdf_data:\n    cached_result = None\n\nself.logger.url_status(\n    url=cache_context.display_url,\n    success=bool(html),\n    timing=time.perf_counter() - start_time,\n    tag=\"FETCH\",\n)\n\n# Update proxy configuration from rotation strategy if available\nif config and config.proxy_rotation_strategy:\n    next_proxy: ProxyConfig = await config.proxy_rotation_strategy.get_next_proxy()\n    if next_proxy:\n        self.logger.info(\n            message=\"Switch proxy: {proxy}\",\n            tag=\"PROXY\",\n            params={\"proxy\": next_proxy.server}\n        )\n        config.proxy_config = next_proxy\n        # config = config.clone(proxy_config=next_proxy)\n\n# Fetch fresh content if needed\nif not cached_result or not html:\n    t1 = time.perf_counter()\n\n    if config.user_agent:\n        self.crawler_strategy.update_user_agent(\n            config.user_agent)\n\n    # Check robots.txt if enabled\n    if config and config.check_robots_txt:\n        if not await self.robots_parser.can_fetch(\n            url, self.browser_config.user_agent\n        ):\n            return CrawlResult(\n                url=url,\n                html=\"\",\n                success=False,\n                status_code=403,\n                error_message=\"Access denied by robots.txt\",\n                response_headers={\n                    \"X-Robots-Status\": \"Blocked by robots.txt\"\n                },\n            )\n\n    ##############################\n    # Call CrawlerStrategy.crawl #\n    ##############################\n    async_response = await self.crawler_strategy.crawl(\n        url,\n        config=config,  # Pass the entire config object\n    )\n\n    html = sanitize_input_encode(async_response.html)\n    screenshot_data = async_response.screenshot\n    pdf_data = async_response.pdf_data\n    js_execution_result = async_response.js_execution_result\n\n    t2 = time.perf_counter()\n    self.logger.url_status(\n        url=cache_context.display_url,\n        success=bool(html),\n        timing=t2 - t1,\n        tag=\"FETCH\",\n    )\n\n    ###############################################################\n    # Process the HTML content, Call CrawlerStrategy.process_html #\n    ###############################################################\n    crawl_result: CrawlResult = await self.aprocess_html(\n        url=url,\n        html=html,\n        extracted_content=extracted_content,\n        config=config,  # Pass the config object instead of individual parameters\n        screenshot=screenshot_data,\n        pdf_data=pdf_data,\n        verbose=config.verbose,\n        is_raw_html=True if url.startswith(\"raw:\") else False,\n        **kwargs,\n    )\n\n    crawl_result.status_code = async_response.status_code\n    crawl_result.redirected_url = async_response.redirected_url or url\n    crawl_result.response_headers = async_response.response_headers\n    crawl_result.downloaded_files = async_response.downloaded_files\n    crawl_result.js_execution_result = js_execution_result\n    crawl_result.mhtml = async_response.mhtml_data\n    crawl_result.ssl_certificate = async_response.ssl_certificate\n    # Add captured network and console data if available\n    crawl_result.network_requests = async_response.network_requests\n    crawl_result.console_messages = async_response.console_messages\n\n    crawl_result.success = bool(html)\n    crawl_result.session_id = getattr(\n        config, \"session_id\", None)\n\n    self.logger.success(\n        message=\"{url:.50}... | Status: {status} | Total: {timing}\",\n        tag=\"COMPLETE\",\n        params={\n            \"url\": cache_context.display_url,\n            \"status\": crawl_result.success,\n            \"timing\": f\"{time.perf_counter() - start_time:.2f}s\",\n        },\n        colors={\n            \"status\": Fore.GREEN if crawl_result.success else Fore.RED,\n            \"timing\": Fore.YELLOW,\n        },\n    )\n\n    # Update cache if appropriate\n    if cache_context.should_write() and not bool(cached_result):\n        await async_db_manager.acache_url(crawl_result)\n\n    return CrawlResultContainer(crawl_result)\n\nelse:\n    self.logger.success(\n        message=\"{url:.50}... | Status: {status} | Total: {timing}\",\n        tag=\"COMPLETE\",\n        params={\n            \"url\": cache_context.display_url,\n            \"status\": True,\n            \"timing\": f\"{time.perf_counter() - start_time:.2f}s\",\n        },\n        colors={\"status\": Fore.GREEN, \"timing\": Fore.YELLOW},\n    )\n\n    cached_result.success = bool(html)\n    cached_result.session_id = getattr(\n        config, \"session_id\", None)\n    cached_result.redirected_url = cached_result.redirected_url or url\n    return CrawlResultContainer(cached_result)\n```\n\n----------------------------------------\n\nTITLE: Integrated JavaScript Execution and Waiting for Dynamic Content in Python\nDESCRIPTION: Combines JavaScript execution and waiting logic into a single approach for handling dynamic content. This technique uses an IIFE with asynchronous JavaScript to click the pagination button and wait for new content to load before continuing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync def integrated_js_and_wait_crawl():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"integrated_session\"\n        url = \"https://github.com/example/repo/commits/main\"\n\n        js_next_page_and_wait = \"\"\"\n        (async () => {\n            const getCurrentCommit = () => document.querySelector('li.commit-item h4').textContent.trim();\n            const initialCommit = getCurrentCommit();\n            document.querySelector('a.pagination-next').click();\n            while (getCurrentCommit() === initialCommit) {\n                await new Promise(resolve => setTimeout(resolve, 100));\n            }\n        })();\n        \"\"\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=js_next_page_and_wait if page > 0 else None,\n                css_selector=\"li.commit-item\",\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {len(result.extracted_content)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(integrated_js_and_wait_crawl())\n```\n\n----------------------------------------\n\nTITLE: Multi-Step GitHub Commits Crawling with Crawl4AI\nDESCRIPTION: Implements a complete multi-step interaction to crawl GitHub commits across multiple pages. Uses session management, JavaScript execution, and custom wait conditions to navigate through paginated content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def multi_page_commits():\n    browser_cfg = BrowserConfig(\n        headless=False,  # Visible for demonstration\n        verbose=True\n    )\n    session_id = \"github_ts_commits\"\n    \n    base_wait = \"\"\"js:() => {\n        const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n        return commits.length > 0;\n    }\"\"\"\n\n    # Step 1: Load initial commits\n    config1 = CrawlerRunConfig(\n        wait_for=base_wait,\n        session_id=session_id,\n        cache_mode=CacheMode.BYPASS,\n        # Not using js_only yet since it's our first load\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://github.com/microsoft/TypeScript/commits/main\",\n            config=config1\n        )\n        print(\"Initial commits loaded. Count:\", result.cleaned_html.count(\"commit\"))\n\n        # Step 2: For subsequent pages, we run JS to click 'Next Page' if it exists\n        js_next_page = \"\"\"\n        const selector = 'a[data-testid=\"pagination-next-button\"]';\n        const button = document.querySelector(selector);\n        if (button) button.click();\n        \"\"\"\n        \n        # Wait until new commits appear\n        wait_for_more = \"\"\"js:() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (!window.firstCommit && commits.length>0) {\n                window.firstCommit = commits[0].textContent;\n                return false;\n            }\n            // If top commit changes, we have new commits\n            const topNow = commits[0]?.textContent.trim();\n            return topNow && topNow !== window.firstCommit;\n        }\"\"\"\n\n        for page in range(2):  # let's do 2 more \"Next\" pages\n            config_next = CrawlerRunConfig(\n                session_id=session_id,\n                js_code=js_next_page,\n                wait_for=wait_for_more,\n                js_only=True,       # We're continuing from the open tab\n                cache_mode=CacheMode.BYPASS\n            )\n            result2 = await crawler.arun(\n                url=\"https://github.com/microsoft/TypeScript/commits/main\",\n                config=config_next\n            )\n            print(f\"Page {page+2} commits count:\", result2.cleaned_html.count(\"commit\"))\n\n        # Optionally kill session\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasync def main():\n    await multi_page_commits()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: AsyncWebCrawler Class Definition and Initialization\nDESCRIPTION: Core crawler class implementation with initialization logic, configuration handling, and context management. Supports both context manager and explicit lifecycle patterns.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass AsyncWebCrawler:\n    _domain_last_hit = {}\n\n    def __init__(\n        self,\n        crawler_strategy: AsyncCrawlerStrategy = None,\n        config: BrowserConfig = None,\n        base_directory: str = str(\n            os.getenv(\"CRAWL4_AI_BASE_DIRECTORY\", Path.home())),\n        thread_safe: bool = False,\n        logger: AsyncLoggerBase = None,\n        **kwargs,\n    ):\n        browser_config = config or BrowserConfig()\n        self.browser_config = browser_config\n        self.logger = logger or AsyncLogger(\n            log_file=os.path.join(base_directory, \".crawl4ai\", \"crawler.log\"),\n            verbose=self.browser_config.verbose,\n            tag_width=10,\n        )\n        params = {k: v for k, v in kwargs.items() if k in [\n            \"browser_config\", \"logger\"]}\n        self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy(\n            browser_config=browser_config,\n            logger=self.logger,\n            **params\n        )\n        self._lock = asyncio.Lock() if thread_safe else None\n        self.crawl4ai_folder = os.path.join(base_directory, \".crawl4ai\")\n        os.makedirs(self.crawl4ai_folder, exist_ok=True)\n        os.makedirs(f\"{self.crawl4ai_folder}/cache\", exist_ok=True)\n        self.robots_parser = RobotsParser()\n        self.ready = False\n        self._deep_handler = DeepCrawlDecorator(self)\n        self.arun = self._deep_handler(self.arun)\n```\n\n----------------------------------------\n\nTITLE: Running a Simple Asynchronous Web Crawl with Crawl4AI in Python\nDESCRIPTION: This Python snippet demonstrates how to perform an asynchronous web crawl using the AsyncWebCrawler class from Crawl4AI. It manages an async browser session context, fetches the specified URL, and prints the Markdown-formatted result. Dependencies include Crawl4AI and asyncio. The main input is the target URL; output is sent to the standard output. Running requires installing Crawl4AI and Playwright with all browser dependencies. The example is suitable for new users seeking a minimal programmable workflow for web extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\\nfrom crawl4ai import *\\n\\nasync def main():\\n    async with AsyncWebCrawler() as crawler:\\n        result = await crawler.arun(\\n            url=\\\"https://www.nbcnews.com/business\\\",\\n        )\\n        print(result.markdown)\\n\\nif __name__ == \\\"__main__\\\":\\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Using arun() Method with CrawlerRunConfig in Python\nDESCRIPTION: Example of using the arun() method with a CrawlerRunConfig object to set up crawl parameters like caching, content filtering, and screenshot capture.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import CrawlerRunConfig, CacheMode\n\nrun_cfg = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    css_selector=\"main.article\",\n    word_count_threshold=10,\n    screenshot=True\n)\n\nasync with AsyncWebCrawler(config=browser_cfg) as crawler:\n    result = await crawler.arun(\"https://example.com/news\", config=run_cfg)\n    print(\"Crawled HTML length:\", len(result.cleaned_html))\n    if result.screenshot:\n        print(\"Screenshot base64 length:\", len(result.screenshot))\n```\n\n----------------------------------------\n\nTITLE: Structured Content Extraction in Python\nDESCRIPTION: This snippet performs structured content extraction based on the configured extraction strategy. It selects the appropriate content format, applies chunking strategies, and extracts content using the specified extraction method.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nif (\n    not bool(extracted_content)\n    and config.extraction_strategy\n    and not isinstance(config.extraction_strategy, NoExtractionStrategy)\n):\n    t1 = time.perf_counter()\n    # Choose content based on input_format\n    content_format = config.extraction_strategy.input_format\n    if content_format == \"fit_markdown\" and not markdown_result.fit_markdown:\n        self.logger.warning(\n            message=\"Fit markdown requested but not available. Falling back to raw markdown.\",\n            tag=\"EXTRACT\",\n            params={\"url\": _url},\n        )\n        content_format = \"markdown\"\n\n    content = {\n        \"markdown\": markdown_result.raw_markdown,\n        \"html\": html,\n        \"cleaned_html\": cleaned_html,\n        \"fit_markdown\": markdown_result.fit_markdown,\n    }.get(content_format, markdown_result.raw_markdown)\n\n    # Use IdentityChunking for HTML input, otherwise use provided chunking strategy\n    chunking = (\n        IdentityChunking()\n        if content_format in [\"html\", \"cleaned_html\"]\n        else config.chunking_strategy\n    )\n    sections = chunking.chunk(content)\n    extracted_content = config.extraction_strategy.run(url, sections)\n    extracted_content = json.dumps(\n        extracted_content, indent=4, default=str, ensure_ascii=False\n    )\n\n    # Log extraction completion\n    self.logger.info(\n        message=\"Completed for {url:.50}... | Time: {timing}s\",\n        tag=\"EXTRACT\",\n        params={\"url\": _url, \"timing\": time.perf_counter() - t1},\n    )\n```\n\n----------------------------------------\n\nTITLE: Streaming Mode Execution for BFS Web Crawler in Python\nDESCRIPTION: Implements streaming mode execution for the BFS crawler. This method processes URLs level by level but yields results immediately as they arrive, enabling real-time processing of crawl results while maintaining BFS traversal order.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_180\n\nLANGUAGE: python\nCODE:\n```\nasync def _arun_stream(\n    self,\n    start_url: str,\n    crawler: AsyncWebCrawler,\n    config: CrawlerRunConfig,\n) -> AsyncGenerator[CrawlResult, None]:\n    \"\"\"\n    Streaming mode:\n    Processes one BFS level at a time and yields results immediately as they arrive.\n    \"\"\"\n    visited: Set[str] = set()\n    current_level: List[Tuple[str, Optional[str]]] = [(start_url, None)]\n    depths: Dict[str, int] = {start_url: 0}\n\n    while current_level and not self._cancel_event.is_set():\n        next_level: List[Tuple[str, Optional[str]]] = []\n        urls = [url for url, _ in current_level]\n        visited.update(urls)\n\n        stream_config = config.clone(deep_crawl_strategy=None, stream=True)\n        stream_gen = await crawler.arun_many(urls=urls, config=stream_config)\n        \n        # Keep track of processed results for this batch\n        results_count = 0\n        async for result in stream_gen:\n            url = result.url\n            depth = depths.get(url, 0)\n            result.metadata = result.metadata or {}\n            result.metadata[\"depth\"] = depth\n            parent_url = next((parent for (u, parent) in current_level if u == url), None)\n            result.metadata[\"parent_url\"] = parent_url\n            \n            # Count only successful crawls\n            if result.success:\n                self._pages_crawled += 1\n            \n            results_count += 1\n            yield result\n            \n            # Only discover links from successful crawls\n            if result.success:\n                # Link discovery will handle the max pages limit internally\n                await self.link_discovery(result, url, depth, visited, next_level, depths)\n        \n        # If we didn't get results back (e.g. due to errors), avoid getting stuck in an infinite loop\n        # by considering these URLs as visited but not counting them toward the max_pages limit\n        if results_count == 0 and urls:\n            self.logger.warning(f\"No results returned for {len(urls)} URLs, marking as visited\")\n            \n        current_level = next_level\n```\n\n----------------------------------------\n\nTITLE: Initializing AsyncWebCrawler in Python\nDESCRIPTION: Constructor for the AsyncWebCrawler class, including parameters for crawler strategy, browser configuration, caching, and thread safety.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass AsyncWebCrawler:\n    def __init__(\n        self,\n        crawler_strategy: Optional[AsyncCrawlerStrategy] = None,\n        config: Optional[BrowserConfig] = None,\n        always_bypass_cache: bool = False,           # deprecated\n        always_by_pass_cache: Optional[bool] = None, # also deprecated\n        base_directory: str = ...,\n        thread_safe: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Create an AsyncWebCrawler instance.\n\n        Args:\n            crawler_strategy: \n                (Advanced) Provide a custom crawler strategy if needed.\n            config: \n                A BrowserConfig object specifying how the browser is set up.\n            always_bypass_cache: \n                (Deprecated) Use CrawlerRunConfig.cache_mode instead.\n            base_directory:     \n                Folder for storing caches/logs (if relevant).\n            thread_safe: \n                If True, attempts some concurrency safeguards. Usually False.\n            **kwargs: \n                Additional legacy or debugging parameters.\n        \"\"\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Content Filtering with BM25 in Python\nDESCRIPTION: Implements BM25 content filtering for markdown generation, focusing on specific search queries and relevance thresholds. Demonstrates configuration of BM25ContentFilter with DefaultMarkdownGenerator.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai import CrawlerRunConfig\n\nbm25_filter = BM25ContentFilter(\n    user_query=\"machine learning\",\n    bm25_threshold=1.2,\n    use_stemming=True\n)\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=bm25_filter,\n    options={\"ignore_links\": True}\n)\n\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Filters (URL, Domain, ContentType) in Crawl4AI using Python\nDESCRIPTION: Illustrates creating a sophisticated `FilterChain` by combining multiple filter types: `URLPatternFilter` (for URL patterns like '*guide*' or '*tutorial*'), `DomainFilter` (to specify allowed and blocked domains), and `ContentTypeFilter` (to restrict crawling to specific content types like 'text/html'). This chain is then passed to the `BFSDeepCrawlStrategy` within `CrawlerRunConfig` for fine-grained control over the deep crawl process. Depends on `FilterChain`, `URLPatternFilter`, `DomainFilter`, `ContentTypeFilter` from `crawl4ai.deep_crawling.filters`, `CrawlerRunConfig`, and `BFSDeepCrawlStrategy`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling.filters import (\n    FilterChain,\n    URLPatternFilter,\n    DomainFilter,\n    ContentTypeFilter\n)\n\n# Create a chain of filters\nfilter_chain = FilterChain([\n    # Only follow URLs with specific patterns\n    URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\"]),\n    \n    # Only crawl specific domains\n    DomainFilter(\n        allowed_domains=[\"docs.example.com\"],\n        blocked_domains=[\"old.docs.example.com\"]\n    ),\n    \n    # Only include specific content types\n    ContentTypeFilter(allowed_types=[\"text/html\"])\n])\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=2,\n        filter_chain=filter_chain\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Executing JavaScript and Clicking Elements in Python with Crawl4AI\nDESCRIPTION: This function demonstrates how to execute JavaScript code to locate and click a 'Load More' button on a webpage. It uses a browser instance with JavaScript enabled to interact with an NBC News business page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_130\n\nLANGUAGE: python\nCODE:\n```\nasync def simple_example_with_running_js_code():\n    print(\"\\n--- Executing JavaScript and Using CSS Selectors ---\")\n\n    browser_config = BrowserConfig(headless=True, java_script_enabled=True)\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        js_code=\"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\",\n        # wait_for=\"() => { return Array.from(document.querySelectorAll('article.tease-card')).length > 10; }\"\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\", config=crawler_config\n        )\n        print(result.markdown[:500])\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Crawl4AI Example with Multiple Input Sources in Python\nDESCRIPTION: This complete example demonstrates all three crawling methods in Crawl4AI. It crawls a Wikipedia page, saves the HTML to a file, crawls the local file, extracts raw HTML, and crawls the raw content. It also verifies consistency between the different methods by checking markdown length.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/local-files.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport sys\nimport asyncio\nfrom pathlib import Path\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def main():\n    wikipedia_url = \"https://en.wikipedia.org/wiki/apple\"\n    script_dir = Path(__file__).parent\n    html_file_path = script_dir / \"apple.html\"\n\n    async with AsyncWebCrawler() as crawler:\n        # Step 1: Crawl the Web URL\n        print(\"\\n=== Step 1: Crawling the Wikipedia URL ===\")\n        web_config = CrawlerRunConfig(bypass_cache=True)\n        result = await crawler.arun(url=wikipedia_url, config=web_config)\n\n        if not result.success:\n            print(f\"Failed to crawl {wikipedia_url}: {result.error_message}\")\n            return\n\n        with open(html_file_path, 'w', encoding='utf-8') as f:\n            f.write(result.html)\n        web_crawl_length = len(result.markdown)\n        print(f\"Length of markdown from web crawl: {web_crawl_length}\\n\")\n\n        # Step 2: Crawl from the Local HTML File\n        print(\"=== Step 2: Crawling from the Local HTML File ===\")\n        file_url = f\"file://{html_file_path.resolve()}\"\n        file_config = CrawlerRunConfig(bypass_cache=True)\n        local_result = await crawler.arun(url=file_url, config=file_config)\n\n        if not local_result.success:\n            print(f\"Failed to crawl local file {file_url}: {local_result.error_message}\")\n            return\n\n        local_crawl_length = len(local_result.markdown)\n        assert web_crawl_length == local_crawl_length, \"Markdown length mismatch\"\n        print(\"âœ… Markdown length matches between web and local file crawl.\\n\")\n\n        # Step 3: Crawl Using Raw HTML Content\n        print(\"=== Step 3: Crawling Using Raw HTML Content ===\")\n        with open(html_file_path, 'r', encoding='utf-8') as f:\n            raw_html_content = f.read()\n        raw_html_url = f\"raw:{raw_html_content}\"\n        raw_config = CrawlerRunConfig(bypass_cache=True)\n        raw_result = await crawler.arun(url=raw_html_url, config=raw_config)\n\n        if not raw_result.success:\n            print(f\"Failed to crawl raw HTML content: {raw_result.error_message}\")\n            return\n\n        raw_crawl_length = len(raw_result.markdown)\n        assert web_crawl_length == raw_crawl_length, \"Markdown length mismatch\"\n        print(\"âœ… Markdown length matches between web and raw HTML crawl.\\n\")\n\n        print(\"All tests passed successfully!\")\n    if html_file_path.exists():\n        os.remove(html_file_path)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI and Running Setup Tasks Using Bash\nDESCRIPTION: These Bash code snippets explain how to install the Crawl4AI package, optionally target a pre-release version, complete a post-installation setup step, and verify that the installation was successful. Required dependencies include Python and Bash; 'pip' must be available and a suitable virtual environment is recommended. Commands are designed for terminal execution and do not require prior setup beyond a functional Python environment. Outputs are printed to the console or system shell.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the package\\npip install -U crawl4ai\\n\\n# For pre release versions\\npip install crawl4ai --pre\\n\\n# Run post-installation setup\\ncrawl4ai-setup\\n\\n# Verify your installation\\ncrawl4ai-doctor\n```\n\n----------------------------------------\n\nTITLE: Complete Example of Crawl4AI with LLM Extraction\nDESCRIPTION: Full implementation example showing how to set up a product extractor using LLM-based extraction with Crawl4AI, including Pydantic schema definition, crawler configuration, and result processing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_183\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: str\n\nasync def main():\n    # 1. Define the LLM extraction strategy\n    llm_strategy = LLMExtractionStrategy(\n        llm_config = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv('OPENAI_API_KEY')),\n        schema=Product.schema_json(), # Or use model_json_schema()\n        extraction_type=\"schema\",\n        instruction=\"Extract all product objects with 'name' and 'price' from the content.\",\n        chunk_token_threshold=1000,\n        overlap_rate=0.0,\n        apply_chunking=True,\n        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n        extra_args={\"temperature\": 0.0, \"max_tokens\": 800}\n    )\n\n    # 2. Build the crawler config\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strategy,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    # 3. Create a browser config if needed\n    browser_cfg = BrowserConfig(headless=True)\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        # 4. Let's say we want to crawl a single page\n        result = await crawler.arun(\n            url=\"https://example.com/products\",\n            config=crawl_config\n        )\n\n        if result.success:\n            # 5. The extracted content is presumably JSON\n            data = json.loads(result.extracted_content)\n            print(\"Extracted items:\", data)\n            \n            # 6. Show usage stats\n            llm_strategy.show_usage()  # prints token usage\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: LLM-Based Extraction in Crawl4AI\nDESCRIPTION: This example demonstrates how to use LLM-based extraction in Crawl4AI. It configures an LLMExtractionStrategy with OpenAI's GPT-4 model to extract article data based on a defined schema.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleData(BaseModel):\n    headline: str\n    summary: str\n\nasync def main():\n    llm_strategy = LLMExtractionStrategy(\n        llm_config = LLMConfig(provider=\"openai/gpt-4\",api_token=\"sk-YOUR_API_KEY\")\n        schema=ArticleData.schema(),\n        extraction_type=\"schema\",\n        instruction=\"Extract 'headline' and a short 'summary' from the content.\"\n    )\n\n    config = CrawlerRunConfig(\n        exclude_external_links=True,\n        word_count_threshold=20,\n        extraction_strategy=llm_strategy\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n        article = json.loads(result.extracted_content)\n        print(article)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Extraction Example\nDESCRIPTION: Complete example of using LLM extraction strategy with Pydantic schema and crawler integration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Define schema\nclass Article(BaseModel):\n    title: str\n    content: str\n    author: str\n\n# Create strategy\nstrategy = LLMExtractionStrategy(\n    llm_config = LLMConfig(provider=\"ollama/llama2\"),\n    schema=Article.schema(),\n    instruction=\"Extract article details\"\n)\n\n# Use with crawler\nresult = await crawler.arun(\n    url=\"https://example.com/article\",\n    extraction_strategy=strategy\n)\n\n# Access extracted data\ndata = json.loads(result.extracted_content)\n```\n\n----------------------------------------\n\nTITLE: Running Web Crawls via the Crawl4AI Command Line Interface in Bash\nDESCRIPTION: These Bash commands demonstrate the usage of the Crawl4AI CLI tool 'crwl', supporting various crawling strategies, output formats, and prompting LLM capabilities for extraction. No coding is necessary; only the CLI and its dependencies need to be installed. Parameters such as URL, output mode, crawl strategy, maximum page limit, and LLM queries can be customized as shown. Output is saved or displayed per the chosen options, with constraints determined by the specific CLI subcommands.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Basic crawl with markdown output\\ncrwl https://www.nbcnews.com/business -o markdown\\n\\n# Deep crawl with BFS strategy, max 10 pages\\ncrwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10\\n\\n# Use LLM extraction with a specific question\\ncrwl https://www.example.com/products -q \\\"Extract all product prices\\\"\n```\n\n----------------------------------------\n\nTITLE: Creating BrowserConfig from Keyword Arguments in Python\nDESCRIPTION: This static method creates a BrowserConfig instance from a dictionary of keyword arguments, allowing for flexible configuration creation.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n@staticmethod\ndef from_kwargs(kwargs: dict) -> \"BrowserConfig\":\n    return BrowserConfig(\n        browser_type=kwargs.get(\"browser_type\", \"chromium\"),\n        headless=kwargs.get(\"headless\", True),\n        browser_mode=kwargs.get(\"browser_mode\", \"dedicated\"),\n        use_managed_browser=kwargs.get(\"use_managed_browser\", False),\n        cdp_url=kwargs.get(\"cdp_url\"),\n        use_persistent_context=kwargs.get(\"use_persistent_context\", False),\n        user_data_dir=kwargs.get(\"user_data_dir\"),\n        chrome_channel=kwargs.get(\"chrome_channel\", \"chromium\"),\n        channel=kwargs.get(\"channel\", \"chromium\"),\n        proxy=kwargs.get(\"proxy\"),\n        proxy_config=kwargs.get(\"proxy_config\", None),\n        viewport_width=kwargs.get(\"viewport_width\", 1080),\n        viewport_height=kwargs.get(\"viewport_height\", 600),\n        accept_downloads=kwargs.get(\"accept_downloads\", False),\n        downloads_path=kwargs.get(\"downloads_path\"),\n        storage_state=kwargs.get(\"storage_state\"),\n        ignore_https_errors=kwargs.get(\"ignore_https_errors\", True),\n        java_script_enabled=kwargs.get(\"java_script_enabled\", True),\n        cookies=kwargs.get(\"cookies\", []),\n        headers=kwargs.get(\"headers\", {}),\n        user_agent=kwargs.get(\n            \"user_agent\",\n            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n            \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\",\n        ),\n        user_agent_mode=kwargs.get(\"user_agent_mode\"),\n        user_agent_generator_config=kwargs.get(\"user_agent_generator_config\"),\n        text_mode=kwargs.get(\"text_mode\", False),\n        light_mode=kwargs.get(\"light_mode\", False),\n        extra_args=kwargs.get(\"extra_args\", []),\n        debugging_port=kwargs.get(\"debugging_port\", 9222),\n        host=kwargs.get(\"host\", \"localhost\"),\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Extraction Algorithm in Python\nDESCRIPTION: Provides methods for extracting text chunks from HTML documents. This optimized implementation uses a queue-based approach to efficiently process HTML elements, categorizing them as headers or content while maintaining proper order.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_91\n\nLANGUAGE: python\nCODE:\n```\n    def extract_text_chunks(\n        self, body: Tag, min_word_threshold: int = None\n    ) -> List[Tuple[str, str]]:\n        \"\"\"\n        Extracts text chunks from a BeautifulSoup body element while preserving order.\n        Returns list of tuples (text, tag_name) for classification.\n\n        Args:\n            body: BeautifulSoup Tag object representing the body element\n\n        Returns:\n            List of (text, tag_name) tuples\n        \"\"\"\n        # Tags to ignore - inline elements that shouldn't break text flow\n        INLINE_TAGS = {\n            \"a\",\n            \"abbr\",\n            \"acronym\",\n            \"b\",\n            \"bdo\",\n            \"big\",\n            \"br\",\n            \"button\",\n            \"cite\",\n            \"code\",\n            \"dfn\",\n            \"em\",\n            \"i\",\n            \"img\",\n            \"input\",\n            \"kbd\",\n            \"label\",\n            \"map\",\n            \"object\",\n            \"q\",\n            \"samp\",\n            \"script\",\n            \"select\",\n            \"small\",\n            \"span\",\n            \"strong\",\n            \"sub\",\n            \"sup\",\n            \"textarea\",\n            \"time\",\n            \"tt\",\n            \"var\",\n        }\n\n        # Tags that typically contain meaningful headers\n        HEADER_TAGS = {\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"header\"}\n\n        chunks = []\n        current_text = []\n        chunk_index = 0\n\n        def should_break_chunk(tag: Tag) -> bool:\n            \"\"\"Determine if a tag should cause a break in the current text chunk\"\"\"\n            return tag.name not in INLINE_TAGS and not (\n                tag.name == \"p\" and len(current_text) == 0\n            )\n\n        # Use deque for efficient push/pop operations\n        stack = deque([(body, False)])\n\n        while stack:\n            element, visited = stack.pop()\n\n            if visited:\n                # End of block element - flush accumulated text\n                if current_text and should_break_chunk(element):\n                    text = \" \".join(\"\".join(current_text).split())\n                    if text:\n                        tag_type = (\n                            \"header\" if element.name in HEADER_TAGS else \"content\"\n                        )\n                        chunks.append((chunk_index, text, tag_type, element))\n                        chunk_index += 1\n                    current_text = []\n                continue\n\n            if isinstance(element, NavigableString):\n                if str(element).strip():\n                    current_text.append(str(element).strip())\n                continue\n\n            # Pre-allocate children to avoid multiple list operations\n            children = list(element.children)\n            if not children:\n                continue\n\n            # Mark block for revisit after processing children\n            stack.append((element, True))\n\n            # Add children in reverse order for correct processing\n            for child in reversed(children):\n                if isinstance(child, (Tag, NavigableString)):\n                    stack.append((child, False))\n\n        # Handle any remaining text\n        if current_text:\n            text = \" \".join(\"\".join(current_text).split())\n            if text:\n                chunks.append((chunk_index, text, \"content\", body))\n\n        if min_word_threshold:\n            chunks = [\n                chunk for chunk in chunks if len(chunk[1].split()) >= min_word_threshold\n            ]\n\n        return chunks\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Mode in Python with AsyncWebCrawler\nDESCRIPTION: This code demonstrates how to use AsyncWebCrawler in streaming mode, processing results as they become available, with MemoryAdaptiveDispatcher and CrawlerMonitor for efficient crawling and real-time monitoring.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_147\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl_streaming():\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=True  # Enable streaming mode\n    )\n    \n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=10,\n        monitor=CrawlerMonitor(\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        # Process results as they become available\n        async for result in await crawler.arun_many(\n            urls=urls,\n            config=run_config,\n            dispatcher=dispatcher\n        ):\n            if result.success:\n                # Process each result immediately\n                await process_result(result)\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing URLPatternFilter Class for Pattern-based URL Filtering in Python\nDESCRIPTION: This class provides advanced pattern-based URL filtering. It supports various pattern types including suffixes, prefixes, domains, and complex path patterns. It uses optimized data structures and algorithms for efficient matching.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_185\n\nLANGUAGE: Python\nCODE:\n```\nclass URLPatternFilter(URLFilter):\n    \"\"\"Pattern filter balancing speed and completeness\"\"\"\n\n    __slots__ = (\n        \"_simple_suffixes\",\n        \"_simple_prefixes\",\n        \"_domain_patterns\",\n        \"_path_patterns\",\n        \"_reverse\",\n    )\n\n    PATTERN_TYPES = {\n        \"SUFFIX\": 1,  # *.html\n        \"PREFIX\": 2,  # /foo/*\n        \"DOMAIN\": 3,  # *.example.com\n        \"PATH\": 4,  # Everything else\n        \"REGEX\": 5,\n    }\n\n    def __init__(\n        self,\n        patterns: Union[str, Pattern, List[Union[str, Pattern]]],\n        use_glob: bool = True,\n        reverse: bool = False,\n    ):\n        super().__init__()\n        self._reverse = reverse\n        patterns = [patterns] if isinstance(patterns, (str, Pattern)) else patterns\n\n        self._simple_suffixes = set()\n        self._simple_prefixes = set()\n        self._domain_patterns = []\n        self._path_patterns = []\n\n        for pattern in patterns:\n            pattern_type = self._categorize_pattern(pattern)\n            self._add_pattern(pattern, pattern_type)\n\n    # ... (methods _categorize_pattern and _add_pattern omitted for brevity)\n\n    @lru_cache(maxsize=10000)\n    def apply(self, url: str) -> bool:\n        # Quick suffix check (*.html)\n        if self._simple_suffixes:\n            path = url.split(\"?\")[0]\n            if path.split(\"/\")[-1].split(\".\")[-1] in self._simple_suffixes:\n                result = True\n                self._update_stats(result)\n                return not result if self._reverse else result\n\n        # Domain check\n        if self._domain_patterns:\n            for pattern in self._domain_patterns:\n                if pattern.match(url):\n                    result = True\n                    self._update_stats(result)\n                    return not result if self._reverse else result\n\n        # Prefix check (/foo/*)\n        if self._simple_prefixes:\n            path = url.split(\"?\")[0]\n            if any(path.startswith(p) for p in self._simple_prefixes):\n                result = True\n                self._update_stats(result)\n                return not result if self._reverse else result\n\n        # Complex patterns\n        if self._path_patterns:\n            if any(p.search(url) for p in self._path_patterns):\n                result = True\n                self._update_stats(result)\n                return not result if self._reverse else result\n\n        result = False\n        self._update_stats(result)\n        return not result if self._reverse else result\n```\n\n----------------------------------------\n\nTITLE: JavaScript Execution Configuration\nDESCRIPTION: Settings for executing JavaScript code during crawling sessions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    js_code=[\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        \"document.querySelector('.load-more')?.click();\"\n    ],\n    js_only=False\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Web Crawler Configuration Parameters\nDESCRIPTION: Class initialization method that sets up various configuration parameters for web crawling, including content processing, SSL, caching, navigation, and media handling settings. The method validates extraction and chunking strategies and sets default values.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self,\n        url: str,\n        # Content Processing Parameters\n        word_count_threshold: int = 200,\n        extraction_strategy: Optional[ExtractionStrategy] = None,\n        chunking_strategy: Optional[ChunkingStrategy] = None,\n        # ... parameter declarations ...\n):\n        self.url = url\n        # Content Processing Parameters\n        self.word_count_threshold = word_count_threshold\n        self.extraction_strategy = extraction_strategy\n        # ... parameter assignments ...\n\n        # Validate strategies\n        if self.extraction_strategy is not None and not isinstance(\n            self.extraction_strategy, ExtractionStrategy\n        ):\n            raise ValueError(\n                \"extraction_strategy must be an instance of ExtractionStrategy\"\n            )\n        if self.chunking_strategy is None:\n            self.chunking_strategy = RegexChunking()\n```\n\n----------------------------------------\n\nTITLE: Crawling a Web URL with Crawl4AI\nDESCRIPTION: Demonstrates how to crawl a live website (Wikipedia) using Crawl4AI. The code initializes an AsyncWebCrawler with a configuration to bypass cache, then extracts and prints the markdown content of the page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_100\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def crawl_web():\n    config = CrawlerRunConfig(bypass_cache=True)\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://en.wikipedia.org/wiki/apple\", \n            config=config\n        )\n        if result.success:\n            print(\"Markdown Content:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl: {result.error_message}\")\n\nasyncio.run(crawl_web())\n```\n\n----------------------------------------\n\nTITLE: Configuring Performance Monitoring Example\nDESCRIPTION: Example showing how to configure CosineStrategy with performance monitoring options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    verbose=True,  # Enable logging\n    word_count_threshold=20,  # Filter short content\n    top_k=5  # Limit results\n)\n```\n\n----------------------------------------\n\nTITLE: Executing JavaScript in Web Crawling with Crawl4AI in Python\nDESCRIPTION: Demonstrates how to execute JavaScript commands in web pages using Crawl4AI. The example shows both single and multiple JS commands for scrolling to the bottom of a page and clicking a 'More' link on Hacker News.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Single JS command\n    config = CrawlerRunConfig(\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # Example site\n            config=config\n        )\n        print(\"Crawled length:\", len(result.cleaned_html))\n\n    # Multiple commands\n    js_commands = [\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # 'More' link on Hacker News\n        \"document.querySelector('a.morelink')?.click();\",  \n    ]\n    config = CrawlerRunConfig(js_code=js_commands)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # Another pass\n            config=config\n        )\n        print(\"After scroll+click, length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Keyword Relevance Scoring in Web Crawler\nDESCRIPTION: Implementation of keyword-based relevance scoring for prioritized crawling using KeywordRelevanceScorer.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\n\n# Create a keyword relevance scorer\nkeyword_scorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n    weight=0.7  # Importance of this scorer (0.0 to 1.0)\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BestFirstCrawlingStrategy(\n        max_depth=2,\n        url_scorer=keyword_scorer\n    ),\n    stream=True  # Recommended with BestFirstCrawling\n)\n\n# Results will come in order of relevance score\nasync with AsyncWebCrawler() as crawler:\n    async for result in await crawler.arun(\"https://example.com\", config=config):\n        score = result.metadata.get(\"score\", 0)\n        print(f\"Score: {score:.2f} | {result.url}\")\n```\n\n----------------------------------------\n\nTITLE: Session Persistence and Local Storage in Crawl4AI\nDESCRIPTION: Shows how to use storage_state in Crawl4AI to preserve cookies and localStorage, allowing for continued sessions and skipping repeated authentication flows. It includes an example of providing pre-set storage state.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    storage_dict = {\n        \"cookies\": [\n            {\n                \"name\": \"session\",\n                \"value\": \"abcd1234\",\n                \"domain\": \"example.com\",\n                \"path\": \"/\",\n                \"expires\": 1699999999.0,\n                \"httpOnly\": False,\n                \"secure\": False,\n                \"sameSite\": \"None\"\n            }\n        ],\n        \"origins\": [\n            {\n                \"origin\": \"https://example.com\",\n                \"localStorage\": [\n                    {\"name\": \"token\", \"value\": \"my_auth_token\"}\n                ]\n            }\n        ]\n    }\n\n    # Provide the storage state as a dictionary to start \"already logged in\"\n    async with AsyncWebCrawler(\n        headless=True,\n        storage_state=storage_dict\n    ) as crawler:\n        result = await crawler.arun(\"https://example.com/protected\")\n        if result.success:\n            print(\"Protected page content length:\", len(result.html))\n        else:\n            print(\"Failed to crawl protected page\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of arun_many() for Batch Crawling in Python\nDESCRIPTION: This example demonstrates the basic usage of arun_many() for batch crawling multiple URLs. It uses the default dispatcher and processes the results after all crawls are completed.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Minimal usage: The default dispatcher will be used\nresults = await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\"],\n    config=CrawlerRunConfig(stream=False)  # Default behavior\n)\n\nfor res in results:\n    if res.success:\n        print(res.url, \"crawled OK!\")\n    else:\n        print(\"Failed:\", res.url, \"-\", res.error_message)\n```\n\n----------------------------------------\n\nTITLE: Best-First Crawling Core Logic\nDESCRIPTION: Core crawling implementation using priority queue for URL ordering. Processes URLs in batches and yields CrawlResults asynchronously.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_171\n\nLANGUAGE: python\nCODE:\n```\nasync def _arun_best_first(\n        self,\n        start_url: str,\n        crawler: AsyncWebCrawler,\n        config: CrawlerRunConfig,\n    ) -> AsyncGenerator[CrawlResult, None]:\n        queue: asyncio.PriorityQueue = asyncio.PriorityQueue()\n        await queue.put((0, 0, start_url, None))\n        visited: Set[str] = set()\n        depths: Dict[str, int] = {start_url: 0}\n\n        while not queue.empty() and not self._cancel_event.is_set():\n            if self._pages_crawled >= self.max_pages:\n                self.logger.info(f\"Max pages limit ({self.max_pages}) reached, stopping crawl\")\n                break\n                \n            batch: List[Tuple[float, int, str, Optional[str]]] = []\n            for _ in range(BATCH_SIZE):\n                if queue.empty():\n                    break\n                item = await queue.get()\n                score, depth, url, parent_url = item\n                if url in visited:\n                    continue\n                visited.add(url)\n                batch.append(item)\n\n            if not batch:\n                continue\n\n            urls = [item[2] for item in batch]\n            batch_config = config.clone(deep_crawl_strategy=None, stream=True)\n            stream_gen = await crawler.arun_many(urls=urls, config=batch_config)\n            async for result in stream_gen:\n                result_url = result.url\n                corresponding = next((item for item in batch if item[2] == result_url), None)\n                if not corresponding:\n                    continue\n                score, depth, url, parent_url = corresponding\n                result.metadata = result.metadata or {}\n                result.metadata[\"depth\"] = depth\n                result.metadata[\"parent_url\"] = parent_url\n                result.metadata[\"score\"] = score\n                \n                if result.success:\n                    self._pages_crawled += 1\n                \n                yield result\n                \n                if result.success:\n                    new_links: List[Tuple[str, Optional[str]]] = []\n                    await self.link_discovery(result, result_url, depth, visited, new_links, depths)\n                    \n                    for new_url, new_parent in new_links:\n                        new_depth = depths.get(new_url, depth + 1)\n                        new_score = self.url_scorer.score(new_url) if self.url_scorer else 0\n                        await queue.put((new_score, new_depth, new_url, new_parent))\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI using pip\nDESCRIPTION: Installs the core Crawl4AI library with essential dependencies using pip. This basic installation doesn't include advanced features like transformers or PyTorch.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai\n```\n\n----------------------------------------\n\nTITLE: BrowserManager Class Definition in Python\nDESCRIPTION: A class that manages browser instances and contexts using Playwright. It handles browser configuration, session management, and provides methods for starting browsers. It supports both managed browsers and direct Playwright instances.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_120\n\nLANGUAGE: python\nCODE:\n```\nclass BrowserManager:\n    \"\"\"\n    Manages the browser instance and context.\n\n    Attributes:\n        config (BrowserConfig): Configuration object containing all browser settings\n        logger: Logger instance for recording events and errors\n        browser (Browser): The browser instance\n        default_context (BrowserContext): The default browser context\n        managed_browser (ManagedBrowser): The managed browser instance\n        playwright (Playwright): The Playwright instance\n        sessions (dict): Dictionary to store session information\n        session_ttl (int): Session timeout in seconds\n    \"\"\"\n\n    _playwright_instance = None\n    \n    @classmethod\n    async def get_playwright(cls):\n        from playwright.async_api import async_playwright\n        cls._playwright_instance = await async_playwright().start()\n        return cls._playwright_instance    \n\n    def __init__(self, browser_config: BrowserConfig, logger=None):\n        \"\"\"\n        Initialize the BrowserManager with a browser configuration.\n\n        Args:\n            browser_config (BrowserConfig): Configuration object containing all browser settings\n            logger: Logger instance for recording events and errors\n        \"\"\"\n        self.config: BrowserConfig = browser_config\n        self.logger = logger\n\n        # Browser state\n        self.browser = None\n        self.default_context = None\n        self.managed_browser = None\n        self.playwright = None\n\n        # Session management\n        self.sessions = {}\n        self.session_ttl = 1800  # 30 minutes\n\n        # Keep track of contexts by a \"config signature,\" so each unique config reuses a single context\n        self.contexts_by_config = {}\n        self._contexts_lock = asyncio.Lock() \n\n        # Initialize ManagedBrowser if needed\n        if self.config.use_managed_browser:\n            self.managed_browser = ManagedBrowser(\n                browser_type=self.config.browser_type,\n                user_data_dir=self.config.user_data_dir,\n                headless=self.config.headless,\n                logger=self.logger,\n                debugging_port=self.config.debugging_port,\n                cdp_url=self.config.cdp_url,\n                browser_config=self.config,\n            )\n\n    async def start(self):\n        \"\"\"\n        Start the browser instance and set up the default context.\n\n        How it works:\n        1. Check if Playwright is already initialized.\n        2. If not, initialize Playwright.\n        3. If managed browser is used, start it and connect to the CDP endpoint.\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuration Factory Method from Dictionary\nDESCRIPTION: Static factory method that creates a CrawlerRunConfig instance from a dictionary of keyword arguments, setting default values for various configuration parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef from_kwargs(kwargs: dict) -> \"CrawlerRunConfig\":\n    return CrawlerRunConfig(\n        word_count_threshold=kwargs.get(\"word_count_threshold\", 200),\n        extraction_strategy=kwargs.get(\"extraction_strategy\"),\n        chunking_strategy=kwargs.get(\"chunking_strategy\", RegexChunking()),\n        # ... additional parameter assignments ...\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Customized Browser Context in Playwright\nDESCRIPTION: Asynchronous method to create a new browser context with configured settings. It supports various configurations including viewport dimensions, proxy settings, and content blocking for optimized performance in different browsing modes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_125\n\nLANGUAGE: python\nCODE:\n```\nasync def create_browser_context(self, crawlerRunConfig: CrawlerRunConfig = None):\n    \"\"\"\n    Creates and returns a new browser context with configured settings.\n    Applies text-only mode settings if text_mode is enabled in config.\n\n    Returns:\n        Context: Browser context object with the specified configurations\n    \"\"\"\n    # Base settings\n    user_agent = self.config.headers.get(\"User-Agent\", self.config.user_agent) \n    viewport_settings = {\n        \"width\": self.config.viewport_width,\n        \"height\": self.config.viewport_height,\n    }\n    proxy_settings = {\"server\": self.config.proxy} if self.config.proxy else None\n\n    blocked_extensions = [\n        # Images\n        \"jpg\",\n        \"jpeg\",\n        \"png\",\n        \"gif\",\n        \"webp\",\n        \"svg\",\n        \"ico\",\n        \"bmp\",\n        \"tiff\",\n        \"psd\",\n        # Fonts\n        \"woff\",\n        \"woff2\",\n        \"ttf\",\n        \"otf\",\n        \"eot\",\n        # Styles\n        # 'css', 'less', 'scss', 'sass',\n        # Media\n        \"mp4\",\n        \"webm\",\n        \"ogg\",\n        \"avi\",\n        \"mov\",\n        \"wmv\",\n        \"flv\",\n        \"m4v\",\n        \"mp3\",\n        \"wav\",\n        \"aac\",\n        \"m4a\",\n        \"opus\",\n        \"flac\",\n        # Documents\n        \"pdf\",\n        \"doc\",\n        \"docx\",\n        \"xls\",\n        \"xlsx\",\n        \"ppt\",\n        \"pptx\",\n        # Archives\n        \"zip\",\n        \"rar\",\n        \"7z\",\n        \"tar\",\n        \"gz\",\n        # Scripts and data\n        \"xml\",\n        \"swf\",\n        \"wasm\",\n    ]\n\n    # Common context settings\n    context_settings = {\n        \"user_agent\": user_agent,\n        \"viewport\": viewport_settings,\n        \"proxy\": proxy_settings,\n        \"accept_downloads\": self.config.accept_downloads,\n        \"storage_state\": self.config.storage_state,\n        \"ignore_https_errors\": self.config.ignore_https_errors,\n        \"device_scale_factor\": 1.0,\n        \"java_script_enabled\": self.config.java_script_enabled,\n    }\n    \n    if crawlerRunConfig:\n        # Check if there is value for crawlerRunConfig.proxy_config set add that to context\n        if crawlerRunConfig.proxy_config:\n            proxy_settings = {\n                \"server\": crawlerRunConfig.proxy_config.server,\n            }\n            if crawlerRunConfig.proxy_config.username:\n```\n\n----------------------------------------\n\nTITLE: Executing Web Crawler in Python\nDESCRIPTION: Runs the AsyncWebCrawler with the provided configurations. Handles verbose output and error handling. Returns the crawling result.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nasync def run_crawler(url: str, browser_cfg: BrowserConfig, crawler_cfg: CrawlerRunConfig, verbose: bool):\n    if verbose:\n        click.echo(\"Starting crawler with configurations:\")\n        click.echo(f\"Browser config: {browser_cfg.dump()}\")\n        click.echo(f\"Crawler config: {crawler_cfg.dump()}\")\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        try:\n            result = await crawler.arun(url=url, config=crawler_cfg)\n            return result\n        except Exception as e:\n            raise click.ClickException(f\"Crawling failed: {str(e)}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing CrawlerRunConfig with Keyword Arguments in Python\nDESCRIPTION: This snippet shows the initialization of CrawlerRunConfig using keyword arguments. It sets various parameters for web crawling, including content extraction, image handling, link processing, and debugging options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nexclude_all_images=kwargs.get(\"exclude_all_images\", False),\nexclude_external_images=kwargs.get(\"exclude_external_images\", False),\n# Link and Domain Handling Parameters\nexclude_social_media_domains=kwargs.get(\n    \"exclude_social_media_domains\", SOCIAL_MEDIA_DOMAINS\n),\nexclude_external_links=kwargs.get(\"exclude_external_links\", False),\nexclude_social_media_links=kwargs.get(\"exclude_social_media_links\", False),\nexclude_domains=kwargs.get(\"exclude_domains\", []),\nexclude_internal_links=kwargs.get(\"exclude_internal_links\", False),\n# Debugging and Logging Parameters\nverbose=kwargs.get(\"verbose\", True),\nlog_console=kwargs.get(\"log_console\", False),\n# Network and Console Capturing Parameters\ncapture_network_requests=kwargs.get(\"capture_network_requests\", False),\ncapture_console_messages=kwargs.get(\"capture_console_messages\", False),\n# Connection Parameters\nmethod=kwargs.get(\"method\", \"GET\"),\nstream=kwargs.get(\"stream\", False),\ncheck_robots_txt=kwargs.get(\"check_robots_txt\", False),\nuser_agent=kwargs.get(\"user_agent\"),\nuser_agent_mode=kwargs.get(\"user_agent_mode\"),\nuser_agent_generator_config=kwargs.get(\"user_agent_generator_config\", {}),\n# Deep Crawl Parameters\ndeep_crawl_strategy=kwargs.get(\"deep_crawl_strategy\"),\nurl=kwargs.get(\"url\"),\n# Experimental Parameters \nexperimental=kwargs.get(\"experimental\"),\n```\n\n----------------------------------------\n\nTITLE: Handling SSL Certificates in Crawl4AI\nDESCRIPTION: Illustrates how to fetch, verify, and export SSL certificates during crawling using Crawl4AI. It includes exporting the certificate in multiple formats (JSON, PEM, DER).\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio, os\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    tmp_dir = os.path.join(os.getcwd(), \"tmp\")\n    os.makedirs(tmp_dir, exist_ok=True)\n    \n    config = CrawlerRunConfig(\n        fetch_ssl_certificate=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=config)\n        \n        if result.success and result.ssl_certificate:\n            cert = result.ssl_certificate\n            print(\"\\nCertificate Information:\")\n            print(f\"Issuer (CN): {cert.issuer.get('CN', '')}\")\n            print(f\"Valid until: {cert.valid_until}\")\n            print(f\"Fingerprint: {cert.fingerprint}\")\n\n            # Export in multiple formats:\n            cert.to_json(os.path.join(tmp_dir, \"certificate.json\"))\n            cert.to_pem(os.path.join(tmp_dir, \"certificate.pem\"))\n            cert.to_der(os.path.join(tmp_dir, \"certificate.der\"))\n            \n            print(\"\\nCertificate exported to JSON/PEM/DER in 'tmp' folder.\")\n        else:\n            print(\"[ERROR] No certificate or crawl failed.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Full Example of Network and Console Capture in Crawl4AI (Python)\nDESCRIPTION: This comprehensive example shows how to use Crawl4AI to capture network requests and console messages, analyze the captured data, and export it to a JSON file.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/network-console-capture.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Enable both network request capture and console message capture\n    config = CrawlerRunConfig(\n        capture_network_requests=True,\n        capture_console_messages=True\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=config\n        )\n        \n        if result.success:\n            # Analyze network requests\n            if result.network_requests:\n                print(f\"Captured {len(result.network_requests)} network events\")\n                \n                # Count request types\n                request_count = len([r for r in result.network_requests if r.get(\"event_type\") == \"request\"])\n                response_count = len([r for r in result.network_requests if r.get(\"event_type\") == \"response\"])\n                failed_count = len([r for r in result.network_requests if r.get(\"event_type\") == \"request_failed\"])\n                \n                print(f\"Requests: {request_count}, Responses: {response_count}, Failed: {failed_count}\")\n                \n                # Find API calls\n                api_calls = [r for r in result.network_requests \n                            if r.get(\"event_type\") == \"request\" and \"api\" in r.get(\"url\", \"\")]\n                if api_calls:\n                    print(f\"Detected {len(api_calls)} API calls:\")\n                    for call in api_calls[:3]:  # Show first 3\n                        print(f\"  - {call.get('method')} {call.get('url')}\")\n            \n            # Analyze console messages\n            if result.console_messages:\n                print(f\"Captured {len(result.console_messages)} console messages\")\n                \n                # Group by type\n                message_types = {}\n                for msg in result.console_messages:\n                    msg_type = msg.get(\"type\", \"unknown\")\n                    message_types[msg_type] = message_types.get(msg_type, 0) + 1\n                \n                print(\"Message types:\", message_types)\n                \n                # Show errors (often the most important)\n                errors = [msg for msg in result.console_messages if msg.get(\"type\") == \"error\"]\n                if errors:\n                    print(f\"Found {len(errors)} console errors:\")\n                    for err in errors[:2]:  # Show first 2\n                        print(f\"  - {err.get('text', '')[:100]}\")\n            \n            # Export all captured data to a file for detailed analysis\n            with open(\"network_capture.json\", \"w\") as f:\n                json.dump({\n                    \"url\": result.url,\n                    \"network_requests\": result.network_requests or [],\n                    \"console_messages\": result.console_messages or []\n                }, f, indent=2)\n            \n            print(\"Exported detailed capture data to network_capture.json\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Using BM25ContentFilter for Query-Based Content Filtering\nDESCRIPTION: This snippet demonstrates how to use BM25ContentFilter to selectively filter content based on a search query. It configures a BM25ContentFilter with a specific user query, threshold, and stemming option, then integrates it with a markdown generator and crawler configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_106\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai import CrawlerRunConfig\n\nbm25_filter = BM25ContentFilter(\n    user_query=\"machine learning\",\n    bm25_threshold=1.2,\n    use_stemming=True\n)\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=bm25_filter,\n    options={\"ignore_links\": True}\n)\n\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\n```\n\n----------------------------------------\n\nTITLE: Implementing Magic Mode Anti-Bot Detection\nDESCRIPTION: Example of using Magic Mode to bypass anti-bot detection mechanisms on protected websites.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def magic_mode_demo():\n    async with AsyncWebCrawler() as crawler:  # Enables anti-bot detection bypass\n        result = await crawler.arun(\n            url=\"https://www.reuters.com/markets/us/global-markets-view-usa-pix-2024-08-29/\",\n            magic=True  # Enables magic mode\n        )\n        print(result.markdown)  # Shows the full content in Markdown format\n\n# Run the demo\nawait magic_mode_demo()\n```\n\n----------------------------------------\n\nTITLE: Interactive Q&A Complete Example for Crawl4AI\nDESCRIPTION: Shows a two-step workflow for interactive Q&A: first view content, then ask specific questions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n# First crawl and view\ncrwl https://example.com -o markdown\n\n# Then ask questions\ncrwl https://example.com -q \"What are the main points?\"\ncrwl https://example.com -q \"Summarize the conclusions\"\n```\n\n----------------------------------------\n\nTITLE: Multi-Step Page Interaction Example for GitHub Commits in Python\nDESCRIPTION: This comprehensive example demonstrates a multi-step interaction that loads multiple pages of GitHub commits by clicking 'Next Page' buttons. It maintains session state between requests and uses JavaScript to interact with the page elements.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_119\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def multi_page_commits():\n    browser_cfg = BrowserConfig(\n        headless=False,  # Visible for demonstration\n        verbose=True\n    )\n    session_id = \"github_ts_commits\"\n    \n    base_wait = \"\"\"js:() => {\n        const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n        return commits.length > 0;\n    }\"\"\"\n\n    # Step 1: Load initial commits\n    config1 = CrawlerRunConfig(\n        wait_for=base_wait,\n        session_id=session_id,\n        cache_mode=CacheMode.BYPASS,\n        # Not using js_only yet since it's our first load\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://github.com/microsoft/TypeScript/commits/main\",\n            config=config1\n        )\n        print(\"Initial commits loaded. Count:\", result.cleaned_html.count(\"commit\"))\n\n        # Step 2: For subsequent pages, we run JS to click 'Next Page' if it exists\n        js_next_page = \"\"\"\n        const selector = 'a[data-testid=\"pagination-next-button\"]';\n        const button = document.querySelector(selector);\n        if (button) button.click();\n        \"\"\"\n        \n        # Wait until new commits appear\n        wait_for_more = \"\"\"js:() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (!window.firstCommit && commits.length>0) {\n                window.firstCommit = commits[0].textContent;\n                return false;\n            }\n            // If top commit changes, we have new commits\n            const topNow = commits[0]?.textContent.trim();\n            return topNow && topNow !== window.firstCommit;\n        }\"\"\"\n\n        for page in range(2):  # let's do 2 more \"Next\" pages\n            config_next = CrawlerRunConfig(\n                session_id=session_id,\n                js_code=js_next_page,\n                wait_for=wait_for_more,\n                js_only=True,       # We're continuing from the open tab\n                cache_mode=CacheMode.BYPASS\n            )\n            result2 = await crawler.arun(\n                url=\"https://github.com/microsoft/TypeScript/commits/main\",\n                config=config_next\n            )\n            print(f\"Page {page+2} commits count:\", result2.cleaned_html.count(\"commit\"))\n\n        # Optionally kill session\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasync def main():\n    await multi_page_commits()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: CSS-based Structured Data Extraction from Web Pages in Python\nDESCRIPTION: This function demonstrates the use of JsonCssExtractionStrategy to extract structured data from a web page (The Hacker News) using CSS selectors. It generates a schema using LLM and then uses it for fast extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_151\n\nLANGUAGE: python\nCODE:\n```\nasync def demo_css_structured_extraction_no_schema():\n    \"\"\"Extract structured data using CSS selectors\"\"\"\n    print(\"\\n=== 5. CSS-Based Structured Extraction ===\")\n    # Sample HTML for schema generation (one-time cost)\n    sample_html = \"\"\"\n<div class=\"body-post clear\">\n    <a class=\"story-link\" href=\"https://thehackernews.com/2025/04/malicious-python-packages-on-pypi.html\">\n        <div class=\"clear home-post-box cf\">\n            <div class=\"home-img clear\">\n                <div class=\"img-ratio\">\n                    <img alt=\"...\" src=\"...\">\n                </div>\n            </div>\n            <div class=\"clear home-right\">\n                <h2 class=\"home-title\">Malicious Python Packages on PyPI Downloaded 39,000+ Times, Steal Sensitive Data</h2>\n                <div class=\"item-label\">\n                    <span class=\"h-datetime\"><i class=\"icon-font icon-calendar\"></i>Apr 05, 2025</span>\n                    <span class=\"h-tags\">Malware / Supply Chain Attack</span>\n                </div>\n                <div class=\"home-desc\"> Cybersecurity researchers have...</div>\n            </div>\n        </div>\n    </a>\n</div>\n    \"\"\"\n\n    # Check if schema file exists\n    schema_file_path = f\"{__cur_dir__}/tmp/schema.json\"\n    if os.path.exists(schema_file_path):\n        with open(schema_file_path, \"r\") as f:\n            schema = json.load(f)\n    else:\n        # Generate schema using LLM (one-time setup)\n        schema = JsonCssExtractionStrategy.generate_schema(\n            html=sample_html,\n            llm_config=LLMConfig(\n                provider=\"groq/qwen-2.5-32b\",\n                api_token=\"env:GROQ_API_KEY\",\n            ),\n            query=\"From https://thehackernews.com/, I have shared a sample of one news div with a title, date, and description. Please generate a schema for this news div.\",\n        )\n\n    print(f\"Generated schema: {json.dumps(schema, indent=2)}\")\n    # Save the schema to a file , and use it for future extractions, in result for such extraction you will call LLM once\n    with open(f\"{__cur_dir__}/tmp/schema.json\", \"w\") as f:\n        json.dump(schema, f, indent=2)\n\n    # Create no-LLM extraction strategy with the generated schema\n    extraction_strategy = JsonCssExtractionStrategy(schema)\n    config = CrawlerRunConfig(extraction_strategy=extraction_strategy)\n\n    # Use the fast CSS extraction (no LLM calls during extraction)\n    async with AsyncWebCrawler() as crawler:\n        results: List[CrawlResult] = await crawler.arun(\n            \"https://thehackernews.com\", config=config\n        )\n\n        for result in results:\n            print(f\"URL: {result.url}\")\n            print(f\"Success: {result.success}\")\n            if result.success:\n                data = json.loads(result.extracted_content)\n                print(json.dumps(data, indent=2))\n            else:\n                print(\"Failed to extract structured data\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data from Dynamic Content using Crawl4AI in Python\nDESCRIPTION: This snippet shows how to extract structured data from a dynamic webpage using Crawl4AI. It uses JsonCssExtractionStrategy for data extraction, and executes JavaScript to interact with the page before extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_128\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    schema = {\n        \"name\": \"KidoCode Courses\",\n        \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n        \"fields\": [\n            {\n                \"name\": \"section_title\",\n                \"selector\": \"h3.heading-50\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"section_description\",\n                \"selector\": \".charge-content\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_name\",\n                \"selector\": \".text-block-93\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_description\",\n                \"selector\": \".course-content-text\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_icon\",\n                \"selector\": \".image-92\",\n                \"type\": \"attribute\",\n                \"attribute\": \"src\",\n            },\n        ],\n    }\n\n    browser_config = BrowserConfig(headless=True, java_script_enabled=True)\n\n    js_click_tabs = \"\"\"\n    (async () => {\n        const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");\n        for(let tab of tabs) {\n            tab.scrollIntoView();\n            tab.click();\n            await new Promise(r => setTimeout(r, 500));\n        }\n    })();\n    \"\"\"\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        js_code=[js_click_tabs],\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config\n        )\n\n        companies = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(companies)} companies\")\n        print(json.dumps(companies[0], indent=2))\n\nasync def main():\n    await extract_structured_data_using_css_extractor()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Deep Crawl with BFS Strategy in Python\nDESCRIPTION: Demonstrates a minimal asynchronous deep crawl using `AsyncWebCrawler` and `BFSDeepCrawlStrategy`. It configures a 2-level deep crawl limited to the starting domain ('https://example.com'), uses `LXMLWebScrapingStrategy` for content extraction, enables verbose output, and prints the total count and details (URL, depth) of the first few crawled pages. Requires `asyncio`, `AsyncWebCrawler`, `CrawlerRunConfig`, `BFSDeepCrawlStrategy`, and `LXMLWebScrapingStrategy`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\n\nasync def main():\n    # Configure a 2-level deep crawl\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BFSDeepCrawlStrategy(\n            max_depth=2, \n            include_external=False\n        ),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        verbose=True\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun(\"https://example.com\", config=config)\n        \n        print(f\"Crawled {len(results)} pages in total\")\n        \n        # Access individual results\n        for result in results[:3]:  # Show first 3 results\n            print(f\"URL: {result.url}\")\n            print(f\"Depth: {result.metadata.get('depth', 0)}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: SSL Certificate Handling in Python Web Crawling\nDESCRIPTION: This function demonstrates how to fetch and handle SSL certificates during web crawling. It retrieves the SSL certificate of a website, accesses its properties, and exports it in various formats (JSON, PEM, DER) for further analysis or use.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_144\n\nLANGUAGE: python\nCODE:\n```\nasync def ssl_certification():\n    # Configure crawler to fetch SSL certificate\n    config = CrawlerRunConfig(\n        fetch_ssl_certificate=True,\n        cache_mode=CacheMode.BYPASS,  # Bypass cache to always get fresh certificates\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=config)\n\n        if result.success and result.ssl_certificate:\n            cert = result.ssl_certificate\n\n            tmp_dir = os.path.join(__location__, \"tmp\")\n            os.makedirs(tmp_dir, exist_ok=True)\n\n            # 1. Access certificate properties directly\n            print(\"\\nCertificate Information:\")\n            print(f\"Issuer: {cert.issuer.get('CN', '')}\")\n            print(f\"Valid until: {cert.valid_until}\")\n            print(f\"Fingerprint: {cert.fingerprint}\")\n\n            # 2. Export certificate in different formats\n            cert.to_json(os.path.join(tmp_dir, \"certificate.json\"))  # For analysis\n            print(\"\\nCertificate exported to:\")\n            print(f\"- JSON: {os.path.join(tmp_dir, 'certificate.json')}\")\n\n            pem_data = cert.to_pem(\n                os.path.join(tmp_dir, \"certificate.pem\")\n            )  # For web servers\n            print(f\"- PEM: {os.path.join(tmp_dir, 'certificate.pem')}\")\n\n            der_data = cert.to_der(\n                os.path.join(tmp_dir, \"certificate.der\")\n            )  # For Java apps\n            print(f\"- DER: {os.path.join(tmp_dir, 'certificate.der')}\")\n```\n\n----------------------------------------\n\nTITLE: Advanced Session Crawling with Custom Execution Hooks in Python\nDESCRIPTION: Uses custom execution hooks to handle complex dynamic content scenarios. This example demonstrates tracking the first commit on each page and ensuring new content loads before proceeding with the next action, with proper error handling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\").strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear: {e}\")\n\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"commit_session\"\n        url = \"https://github.com/example/repo/commits/main\"\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        js_next_page = \"\"\"document.querySelector('a.pagination-next').click();\"\"\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=js_next_page if page > 0 else None,\n                css_selector=\"li.commit-item\",\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {len(result.extracted_content)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(advanced_session_crawl_with_hooks())\n```\n\n----------------------------------------\n\nTITLE: Using BM25 Content Filter with Crawl4AI\nDESCRIPTION: Example demonstrating how to use the BM25ContentFilter with AsyncWebCrawler to extract content relevant to a specific query. The code shows the complete setup from filter creation to retrieving the results.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_72\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # 1) A BM25 filter with a user query\n    bm25_filter = BM25ContentFilter(\n        user_query=\"startup fundraising tips\",\n        # Adjust for stricter or looser results\n        bm25_threshold=1.2  \n    )\n\n    # 2) Insert into a Markdown Generator\n    md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\n    \n    # 3) Pass to crawler config\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\", \n            config=config\n        )\n        if result.success:\n            print(\"Fit Markdown (BM25 query-based):\")\n            print(result.markdown.fit_markdown)\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Accessing Raw HTML Content in Python\nDESCRIPTION: Demonstrates how to access the original, unmodified HTML content from a crawled page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Possibly large\nprint(len(result.html))\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Web Crawler in Python\nDESCRIPTION: Implementation of a streaming web crawler that processes results immediately as they become available. Uses async iterator pattern for real-time processing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),\n    stream=True  # Enable streaming\n)\n\nasync with AsyncWebCrawler() as crawler:\n    # Returns an async iterator\n    async for result in await crawler.arun(\"https://example.com\", config=config):\n        # Process each result as it becomes available\n        process_result(result)\n```\n\n----------------------------------------\n\nTITLE: Generating XPath Schema using Ollama in Crawl4AI\nDESCRIPTION: This snippet demonstrates how to generate an XPath schema using the Ollama LLM provider in Crawl4AI. It uses the JsonXPathExtractionStrategy to create a schema from HTML content, which can then be used for fast, repeated extractions without further LLM calls.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nxpath_schema = JsonXPathExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"xpath\",\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the generated schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(css_schema)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sliding Window Chunking\nDESCRIPTION: Implementation of sliding window approach for text chunking with configurable window and step sizes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nSlidingWindowChunking(\n    window_size: int = 100,    # Window size in words\n    step: int = 50             # Step size between windows\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing CrawlResultContainer with Generic Type Support\nDESCRIPTION: Defines a container class for crawl results that supports generic typing and provides list-like behavior. It implements common container methods for iteration, indexing, and attribute access, delegating attribute access to the first element when appropriate.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_86\n\nLANGUAGE: python\nCODE:\n```\nCrawlResultT = TypeVar('CrawlResultT', bound=CrawlResult)\n\nclass CrawlResultContainer(Generic[CrawlResultT]):\n    def __init__(self, results: Union[CrawlResultT, List[CrawlResultT]]):\n        # Normalize to a list\n        if isinstance(results, list):\n            self._results = results\n        else:\n            self._results = [results]\n\n    def __iter__(self):\n        return iter(self._results)\n\n    def __getitem__(self, index):\n        return self._results[index]\n\n    def __len__(self):\n        return len(self._results)\n\n    def __getattr__(self, attr):\n        # Delegate attribute access to the first element.\n        if self._results:\n            return getattr(self._results[0], attr)\n        raise AttributeError(f\"{self.__class__.__name__} object has no attribute '{attr}'\")\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self._results!r})\"\n\nRunManyReturn = Union[\n    CrawlResultContainer[CrawlResultT],\n    AsyncGenerator[CrawlResultT, None]\n]\n```\n\n----------------------------------------\n\nTITLE: Customizing Crawl Options with CrawlerRunConfig\nDESCRIPTION: Demonstrates how to customize crawl behavior using CrawlerRunConfig parameters such as content thresholds, link handling, and element processing options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    config=run_config\n)\n```\n\n----------------------------------------\n\nTITLE: Cache Key Generation and Text Chunking in LLMContentFilter\nDESCRIPTION: Helper methods for generating cache keys and merging text chunks. The cache key helps avoid redundant LLM calls, while the chunking strategy divides large text into manageable pieces with configurable overlap.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_102\n\nLANGUAGE: python\nCODE:\n```\ndef _get_cache_key(self, html: str, instruction: str) -> str:\n    \"\"\"Generate a unique cache key based on HTML and instruction\"\"\"\n    content = f\"{html}{instruction}\"\n    return hashlib.md5(content.encode()).hexdigest()\n\ndef _merge_chunks(self, text: str) -> List[str]:\n    \"\"\"Split text into chunks with overlap using char or word mode.\"\"\"\n    ov = int(self.chunk_token_threshold * self.overlap_rate)\n    sections = merge_chunks(\n        docs=[text],\n        target_size=self.chunk_token_threshold,\n        overlap=ov,\n```\n\n----------------------------------------\n\nTITLE: Media Handling with Crawl4AI\nDESCRIPTION: This snippet demonstrates media handling capabilities of Crawl4AI, including options to include external images and take screenshots. It prints details of the first 5 images found on the page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nasync def media_handling():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\", \n            bypass_cache=True,\n            exclude_external_images=False,\n            screenshot=True\n        )\n        for img in result.media['images'][:5]:\n            print(f\"Image URL: {img['src']}, Alt: {img['alt']}, Score: {img['score']}\")\n        \nasyncio.run(media_handling())\n```\n\n----------------------------------------\n\nTITLE: LLM-based Data Extraction\nDESCRIPTION: Advanced example of using LLM-based extraction with both open-source and closed-source providers for complex data structures.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_126\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def extract_structured_data_using_llm(\n    provider: str, api_token: str = None, extra_headers: Dict[str, str] = None\n):\n    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n\n    if api_token is None and provider != \"ollama\":\n        print(f\"API token is required for {provider}. Skipping this example.\")\n        return\n\n    browser_config = BrowserConfig(headless=True)\n\n    extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000}\n    if extra_headers:\n        extra_args[\"extra_headers\"] = extra_headers\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        word_count_threshold=1,\n        page_timeout=80000,\n        extraction_strategy=LLMExtractionStrategy(\n            llm_config = LLMConfig(provider=provider,api_token=api_token),\n            schema=OpenAIModelFee.model_json_schema(),\n            extraction_type=\"schema\",\n            instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n            Do not miss any models in the entire content.\"\"\",\n            extra_args=extra_args,\n        ),\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://openai.com/api/pricing/\", config=crawler_config\n        )\n        print(result.extracted_content)\n\nif __name__ == \"__main__\":\n\n    asyncio.run(\n        extract_structured_data_using_llm(\n            provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\")\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Browser Restart Command in Python\nDESCRIPTION: Implementation of the 'crwl browser restart' command that stops the current browser instance and starts a new one. It can use the same configuration as the current browser or accept new parameters for browser type, port, and headless mode.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_49\n\nLANGUAGE: python\nCODE:\n```\n@browser_cmd.command(\"restart\")\n@click.option(\"--browser-type\", \"-b\", type=click.Choice([\"chromium\", \"firefox\"]), default=None, \n              help=\"Browser type (defaults to same as current)\")\n@click.option(\"--port\", \"-p\", type=int, default=None, help=\"Debugging port (defaults to same as current)\")\n@click.option(\"--headless/--no-headless\", default=None, help=\"Run browser in headless mode\")\ndef browser_restart_cmd(browser_type: Optional[str], port: Optional[int], headless: Optional[bool]):\n    \"\"\"Restart the builtin browser\n    \n    Stops the current builtin browser if running and starts a new one.\n    By default, uses the same configuration as the current browser.\n    \"\"\"\n    profiler = BrowserProfiler()\n    \n    try:\n        # First check if browser is running and get its config\n        status = anyio.run(profiler.get_builtin_browser_status)\n        current_config = {}\n        \n        if status[\"running\"]:\n            info = status[\"info\"]\n            current_config = {\n                \"browser_type\": info[\"browser_type\"],\n                \"port\": info[\"debugging_port\"],\n                \"headless\": True  # Default assumption\n            }\n            \n            # Stop the browser\n            console.print(Panel(\n                \"[cyan]Stopping current builtin browser...[/cyan]\",\n                title=\"Builtin Browser Restart\", \n                border_style=\"cyan\"\n            ))\n            \n            success = anyio.run(profiler.kill_builtin_browser)\n            if not success:\n                console.print(Panel(\n                    \"[red]Failed to stop current browser[/red]\",\n                    title=\"Builtin Browser Restart\",\n                    border_style=\"red\"\n                ))\n                sys.exit(1)\n        \n        # Use provided options or defaults from current config\n        browser_type = browser_type or current_config.get(\"browser_type\", \"chromium\")\n        port = port or current_config.get(\"port\", 9222)\n        headless = headless if headless is not None else current_config.get(\"headless\", True)\n        \n        # Start a new browser\n        console.print(Panel(\n```\n\n----------------------------------------\n\nTITLE: Integrating Chunking with Cosine Similarity for Content Extraction\nDESCRIPTION: Combines text chunking with TF-IDF and cosine similarity to extract relevant content chunks based on a query. Uses scikit-learn for vector operations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass CosineSimilarityExtractor:\n    def __init__(self, query):\n        self.query = query\n        self.vectorizer = TfidfVectorizer()\n\n    def find_relevant_chunks(self, chunks):\n        vectors = self.vectorizer.fit_transform([self.query] + chunks)\n        similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n        return [(chunks[i], similarities[i]) for i in range(len(chunks))]\n\n# Example Workflow\ntext = \"\"\"This is a sample document. It has multiple sentences. \nWe are testing chunking and similarity.\"\"\"\n\nchunker = SlidingWindowChunking(window_size=5, step=3)\nchunks = chunker.chunk(text)\nquery = \"testing chunking\"\nextractor = CosineSimilarityExtractor(query)\nrelevant_chunks = extractor.find_relevant_chunks(chunks)\n\nprint(relevant_chunks)\n```\n\n----------------------------------------\n\nTITLE: Using Fit Markdown with PruningContentFilter in Python\nDESCRIPTION: This snippet shows how to use the PruningContentFilter to generate both raw and filtered markdown from a web crawl. It demonstrates accessing the raw_markdown and fit_markdown properties of the result.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator(\n            content_filter=PruningContentFilter(threshold=0.6),\n            options={\"ignore_links\": True}\n        )\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://news.example.com/tech\", config=config)\n        if result.success:\n            print(\"Raw markdown:\\n\", result.markdown)\n            \n            # If a filter is used, we also have .fit_markdown:\n            md_object = result.markdown  # or your equivalent\n            print(\"Filtered markdown:\\n\", md_object.fit_markdown)\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Initializing SSL Certificate from Binary Data in Python\nDESCRIPTION: Demonstrates how to initialize an SSLCertificate instance from raw binary data. This method is useful when certificate data is obtained from a socket or another source.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncert = SSLCertificate.from_binary(raw_bytes)\n```\n\n----------------------------------------\n\nTITLE: Using Managed Browsers in Crawl4AI (Python)\nDESCRIPTION: Complete Python script showcasing how to use managed browsers in Crawl4AI, including setting up the browser configuration and running a crawl with a persistent profile.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    # 1) Reference your persistent data directory\n    browser_config = BrowserConfig(\n        headless=True,             # 'True' for automated runs\n        verbose=True,\n        use_managed_browser=True,  # Enables persistent browser strategy\n        browser_type=\"chromium\",\n        user_data_dir=\"/path/to/my-chrome-profile\"\n    )\n\n    # 2) Standard crawl config\n    crawl_config = CrawlerRunConfig(\n        wait_for=\"css:.logged-in-content\"\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url=\"https://example.com/private\", config=crawl_config)\n        if result.success:\n            print(\"Successfully accessed private data with your identity!\")\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Excluding Specific Domains with CrawlerRunConfig (Python)\nDESCRIPTION: This snippet shows how to configure `CrawlerRunConfig` to exclude links pointing to a specific list of custom domains. The `exclude_domains` parameter accepts a list of domain strings (e.g., `[\"suspiciousads.com\"]`) that should be filtered out during the crawl.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_domains=[\"suspiciousads.com\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Reusing Browser Sessions for Efficient Crawling\nDESCRIPTION: This code shows how to create and reuse browser sessions across multiple crawl operations, which improves efficiency by avoiding the creation of new browser tabs for each crawled page, reducing memory usage and setup time.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsession_id = await crawler.create_session()\n\n# Use the same session for multiple crawls\nawait crawler.crawl(\n    url=\"https://example.com/page1\",\n    session_id=session_id  # Reuse the session\n)\nawait crawler.crawl(\n    url=\"https://example.com/page2\",\n    session_id=session_id\n)\n```\n\n----------------------------------------\n\nTITLE: Optimizing CosineStrategy Performance in Python\nDESCRIPTION: Demonstrates how to configure CosineStrategy for optimal performance by adjusting thresholds and enabling verbose mode.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    word_count_threshold=10,  # Filter early\n    top_k=5,                 # Limit results\n    verbose=True             # Monitor performance\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring High-Traffic Settings for Crawl4AI in YAML\nDESCRIPTION: This YAML snippet shows recommended settings for high-traffic scenarios in Crawl4AI, including memory thresholds and rate limiting.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_69\n\nLANGUAGE: yaml\nCODE:\n```\ncrawler:\n  memory_threshold_percent: 85.0  # More conservative memory limit\n  rate_limiter:\n    base_delay: [2.0, 4.0]       # More aggressive rate limiting\n```\n\n----------------------------------------\n\nTITLE: Reusing Saved Storage State\nDESCRIPTION: Second-run implementation that reuses previously saved storage state to skip login\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CacheMode\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    async with AsyncWebCrawler(\n        headless=True,\n        verbose=True,\n        use_persistent_context=True,\n        user_data_dir=\"./my_user_data\",\n        storage_state=\"my_storage_state.json\"  # Reuse previously exported state\n    ) as crawler:\n        \n        result = await crawler.arun(\n            url='https://example.com/protected-page',\n            cache_mode=CacheMode.BYPASS,\n            markdown_generator=DefaultMarkdownGenerator(options={\"ignore_links\": True}),\n        )\n        print(\"Second run result success:\", result.success)\n        if result.success:\n            print(\"Protected page HTML length:\", len(result.html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Web Crawling Command in Python\nDESCRIPTION: Click command implementation for web crawling with configurable browser settings, content filtering, and extraction strategies. Supports multiple output formats and profile management.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_51\n\nLANGUAGE: python\nCODE:\n```\n@cli.command(\"crawl\")\n@click.argument(\"url\", required=True)\n@click.option(\"--browser-config\", \"-B\", type=click.Path(exists=True), help=\"Browser config file (YAML/JSON)\")\n@click.option(\"--crawler-config\", \"-C\", type=click.Path(exists=True), help=\"Crawler config file (YAML/JSON)\")\n@click.option(\"--filter-config\", \"-f\", type=click.Path(exists=True), help=\"Content filter config file\")\n@click.option(\"--extraction-config\", \"-e\", type=click.Path(exists=True), help=\"Extraction strategy config file\")\n@click.option(\"--json-extract\", \"-j\", is_flag=False, flag_value=\"\", default=None, help=\"Extract structured data using LLM with optional description\")\n@click.option(\"--schema\", \"-s\", type=click.Path(exists=True), help=\"JSON schema for extraction\")\n@click.option(\"--browser\", \"-b\", type=str, callback=parse_key_values, help=\"Browser parameters as key1=value1,key2=value2\")\n@click.option(\"--crawler\", \"-c\", type=str, callback=parse_key_values, help=\"Crawler parameters as key1=value1,key2=value2\")\n@click.option(\"--output\", \"-o\", type=click.Choice([\"all\", \"json\", \"markdown\", \"md\", \"markdown-fit\", \"md-fit\"]), default=\"all\")\n@click.option(\"--output-file\", \"-O\", type=click.Path(), help=\"Output file path (default: stdout)\")\n@click.option(\"--bypass-cache\", \"-b\", is_flag=True, default=True, help=\"Bypass cache when crawling\")\n@click.option(\"--question\", \"-q\", help=\"Ask a question about the crawled content\")\n@click.option(\"--verbose\", \"-v\", is_flag=True)\n@click.option(\"--profile\", \"-p\", help=\"Use a specific browser profile (by name)\")\ndef crawl_cmd(url: str, browser_config: str, crawler_config: str, filter_config: str, \n           extraction_config: str, json_extract: str, schema: str, browser: Dict, crawler: Dict,\n           output: str, output_file: str, bypass_cache: bool, question: str, verbose: bool, profile: str):\n```\n\n----------------------------------------\n\nTITLE: Combining PruningContentFilter and BM25ContentFilter in Two Passes for Web Crawling in Python\nDESCRIPTION: This snippet demonstrates how to combine PruningContentFilter and BM25ContentFilter in two passes without re-crawling. It first applies PruningContentFilter to the raw HTML, then uses BM25ContentFilter on the pruned content with a user query.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter\nfrom bs4 import BeautifulSoup\n\nasync def main():\n    # 1. Crawl with minimal or no markdown generator, just get raw HTML\n    config = CrawlerRunConfig(\n        # If you only want raw HTML, you can skip passing a markdown_generator\n        # or provide one but focus on .html in this example\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/tech-article\", config=config)\n\n        if not result.success or not result.html:\n            print(\"Crawl failed or no HTML content.\")\n            return\n        \n        raw_html = result.html\n        \n        # 2. First pass: PruningContentFilter on raw HTML\n        pruning_filter = PruningContentFilter(threshold=0.5, min_word_threshold=50)\n        \n        # filter_content returns a list of \"text chunks\" or cleaned HTML sections\n        pruned_chunks = pruning_filter.filter_content(raw_html)\n        # This list is basically pruned content blocks, presumably in HTML or text form\n        \n        # For demonstration, let's combine these chunks back into a single HTML-like string\n        # or you could do further processing. It's up to your pipeline design.\n        pruned_html = \"\\n\".join(pruned_chunks)\n        \n        # 3. Second pass: BM25ContentFilter with a user query\n        bm25_filter = BM25ContentFilter(\n            user_query=\"machine learning\",\n            bm25_threshold=1.2,\n            language=\"english\"\n        )\n        \n        # returns a list of text chunks\n        bm25_chunks = bm25_filter.filter_content(pruned_html)  \n        \n        if not bm25_chunks:\n            print(\"Nothing matched the BM25 query after pruning.\")\n            return\n        \n        # 4. Combine or display final results\n        final_text = \"\\n---\\n\".join(bm25_chunks)\n        \n        print(\"==== PRUNED OUTPUT (first pass) ====\")\n        print(pruned_html[:500], \"... (truncated)\")  # preview\n\n        print(\"\\n==== BM25 OUTPUT (second pass) ====\")\n        print(final_text[:500], \"... (truncated)\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Configuring Geolocation with AsyncWebCrawler in Python\nDESCRIPTION: This snippet demonstrates how to control the GPS coordinates reported by the browser's geolocation API. It allows precise control over the perceived location of the browser, affecting map services and location-aware websites.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, GeolocationConfig\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://maps.google.com\",  # Or any location-aware site\n        config=CrawlerRunConfig(\n            # Configure precise GPS coordinates\n            geolocation=GeolocationConfig(\n                latitude=48.8566,   # Paris coordinates\n                longitude=2.3522,\n                accuracy=100        # Accuracy in meters (optional)\n            ),\n            \n            # This site will see you as being in Paris\n            page_timeout=60000\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Element Fallback and Extraction Methods\nDESCRIPTION: This snippet contains methods for handling selector caching, element retrieval, and extraction of text, HTML, and attributes from elements. It includes a fallback mechanism for invalid selectors.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_80\n\nLANGUAGE: python\nCODE:\n```\nself._selector_cache[selector_str] = select_func\n            except Exception as e:\n                if self.verbose:\n                    print(f\"Error compiling selector '{selector_str}': {e}\")\n                \n                # Fallback function for invalid selectors\n                def fallback_func(element):\n                    return []\n                \n                self._selector_cache[selector_str] = fallback_func\n                \n        return self._selector_cache[selector_str]\n    \n    def _get_base_elements(self, parsed_html, selector: str):\n        selector_func = self._get_selector(selector)\n        return selector_func(parsed_html)\n    \n    def _get_elements(self, element, selector: str):\n        selector_func = self._get_selector(selector)\n        return selector_func(element)\n    \n    def _get_element_text(self, element) -> str:\n        return \"\".join(element.xpath(\".//text()\")).strip()\n    \n    def _get_element_html(self, element) -> str:\n        from lxml import etree\n        return etree.tostring(element, encoding='unicode')\n    \n    def _get_element_attribute(self, element, attribute: str):\n        return element.get(attribute)\n```\n\n----------------------------------------\n\nTITLE: Handling Mixed Content Pages with CosineStrategy in Python\nDESCRIPTION: Shows configuration for extracting content from pages with diverse content types. Uses a more flexible similarity threshold, larger clusters via max_dist, and retrieves multiple relevant sections.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_180\n\nLANGUAGE: python\nCODE:\n```\n# For mixed content pages\nstrategy = CosineStrategy(\n    semantic_filter=\"product features\",\n    sim_threshold=0.4,      # More flexible matching\n    max_dist=0.3,          # Larger clusters\n    top_k=3                # Multiple relevant sections\n)\n```\n\n----------------------------------------\n\nTITLE: Using arun_many() with Custom Dispatcher for Controlled Concurrency in Python\nDESCRIPTION: This example demonstrates how to use arun_many() with a custom dispatcher (MemoryAdaptiveDispatcher) to control concurrency based on system memory usage and set maximum concurrent sessions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=70.0,\n    max_session_permit=10\n)\nresults = await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\", \"https://site3.com\"],\n    config=my_run_config,\n    dispatcher=dispatcher\n)\n```\n\n----------------------------------------\n\nTITLE: Saving PDF and MHTML Content from Crawl4AI Results\nDESCRIPTION: This snippet shows how to save PDF and MHTML content captured during crawling. The code writes binary PDF data to a file and saves the MHTML snapshot which contains the complete webpage with all resources for offline viewing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# Save the PDF\nwith open(\"page.pdf\", \"wb\") as f:\n    f.write(result.pdf)\n\n# Save the MHTML\nif result.mhtml:\n    with open(\"page.mhtml\", \"w\", encoding=\"utf-8\") as f:\n        f.write(result.mhtml)\n```\n\n----------------------------------------\n\nTITLE: Configuring LLMExtractionStrategy with Schema-Based Extraction\nDESCRIPTION: Demonstrates how to set up an LLM extraction strategy with OpenAI GPT-4, defining schema parameters, chunking settings, and additional configuration options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_182\n\nLANGUAGE: python\nCODE:\n```\nextraction_strategy = LLMExtractionStrategy(\n    llm_config = LLMConfig(provider=\"openai/gpt-4\", api_token=\"YOUR_OPENAI_KEY\"),\n    schema=MyModel.model_json_schema(),\n    extraction_type=\"schema\",\n    instruction=\"Extract a list of items from the text with 'name' and 'price' fields.\",\n    chunk_token_threshold=1200,\n    overlap_rate=0.1,\n    apply_chunking=True,\n    input_format=\"html\",\n    extra_args={\"temperature\": 0.1, \"max_tokens\": 1000},\n    verbose=True\n)\n```\n\n----------------------------------------\n\nTITLE: Executing JavaScript and Extracting Structured Data without LLMs using Crawl4AI in Python\nDESCRIPTION: This Python script showcases using `AsyncWebCrawler` to execute custom JavaScript on a target page and extract structured data based on a predefined JSON schema using CSS selectors. It defines a schema for course details, configures `JsonCssExtractionStrategy`, injects JavaScript to interact with page tabs, runs the browser non-headlessly, bypasses the cache, crawls the URL, parses the extracted JSON data, and prints the results.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nimport json\n\nasync def main():\n    schema = {\n    \"name\": \"KidoCode Courses\",\n    \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n    \"fields\": [\n        {\n            \"name\": \"section_title\",\n            \"selector\": \"h3.heading-50\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"section_description\",\n            \"selector\": \".charge-content\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"course_name\",\n            \"selector\": \".text-block-93\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"course_description\",\n            \"selector\": \".course-content-text\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"course_icon\",\n            \"selector\": \".image-92\",\n            \"type\": \"attribute\",\n            \"attribute\": \"src\"\n        }\n    ]\n}\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    browser_config = BrowserConfig(\n        headless=False,\n        verbose=True\n    )\n    run_config = CrawlerRunConfig(\n        extraction_strategy=extraction_strategy,\n        js_code=[\"\"\"(async () => {const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");for(let tab of tabs) {tab.scrollIntoView();tab.click();await new Promise(r => setTimeout(r, 500));}})();\"\"\"],\n        cache_mode=CacheMode.BYPASS\n    )\n        \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        \n        result = await crawler.arun(\n            url=\"https://www.kidocode.com/degrees/technology\",\n            config=run_config\n        )\n\n        companies = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(companies)} companies\")\n        print(json.dumps(companies[0], indent=2))\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Streaming Crawl Results via API using Python\nDESCRIPTION: Defines an asynchronous Python function `test_stream_crawl` that sends a POST request to the `/crawl/stream` endpoint (running locally on port 11235). It uses the `httpx` library for asynchronous streaming. The payload includes target URLs, browser configuration (headless, viewport), and crawler configuration (stream enabled, cache bypassed). It reads the NDJSON response line by line, prints each result, and checks for a completion marker. Error handling for HTTP errors and JSON decoding is included. Requires `httpx` and `json` libraries. An optional `token` parameter is present but commented out for JWT authentication.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport httpx # Use httpx for async streaming example\n\nasync def test_stream_crawl(token: str = None): # Made token optional\n    \"\"\"Test the /crawl/stream endpoint with multiple URLs.\"\"\"\n    url = \"http://localhost:11235/crawl/stream\" # Updated port\n    payload = {\n        \"urls\": [\n            \"https://httpbin.org/html\",\n            \"https://httpbin.org/links/5/0\",\n        ],\n        \"browser_config\": {\n            \"type\": \"BrowserConfig\",\n            \"params\": {\"headless\": True, \"viewport\": {\"type\": \"dict\", \"value\": {\"width\": 1200, \"height\": 800}}} # Viewport needs type:dict\n        },\n        \"crawler_config\": {\n            \"type\": \"CrawlerRunConfig\",\n            \"params\": {\"stream\": True, \"cache_mode\": \"bypass\"}\n        }\n    }\n\n    headers = {}\n    # if token:\n    #    headers = {\"Authorization\": f\"Bearer {token}\"} # If JWT is enabled\n\n    try:\n        async with httpx.AsyncClient() as client:\n            async with client.stream(\"POST\", url, json=payload, headers=headers, timeout=120.0) as response:\n                print(f\"Status: {response.status_code} (Expected: 200)\")\n                response.raise_for_status() # Raise exception for bad status codes\n\n                # Read streaming response line-by-line (NDJSON)\n                async for line in response.aiter_lines():\n                    if line:\n                        try:\n                            data = json.loads(line)\n                            # Check for completion marker\n                            if data.get(\"status\") == \"completed\":\n                                print(\"Stream completed.\")\n                                break\n                            print(f\"Streamed Result: {json.dumps(data, indent=2)}\")\n                        except json.JSONDecodeError:\n                            print(f\"Warning: Could not decode JSON line: {line}\")\n\n    except httpx.HTTPStatusError as e:\n         print(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n    except Exception as e:\n        print(f\"Error in streaming crawl test: {str(e)}\")\n\n# To run this example:\n# import asyncio\n# asyncio.run(test_stream_crawl())\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data using CSS Selectors with Crawl4AI in Python\nDESCRIPTION: This snippet demonstrates how to extract structured data (JSON) from HTML using a predefined CSS schema with `JsonCssExtractionStrategy`. A schema dictionary defines the base selector for items and specifies fields to extract (like 'title' and 'link') using CSS selectors. Raw HTML is passed directly to the crawler using the `raw://` prefix. The `JsonCssExtractionStrategy`, initialized with the schema, is provided in the `CrawlerRunConfig`. The extracted data is available as a JSON string in `result.extracted_content`, which is then parsed and printed.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    schema = {\n        \"name\": \"Example Items\",\n        \"baseSelector\": \"div.item\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n\n    raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"raw://\" + raw_html,\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                extraction_strategy=JsonCssExtractionStrategy(schema)\n            )\n        )\n        # The JSON output is stored in 'extracted_content'\n        data = json.loads(result.extracted_content)\n        print(data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Crawling Multiple URLs Concurrently with Crawl4AI in Python\nDESCRIPTION: This snippet illustrates how to crawl multiple URLs in parallel using the `arun_many` method of `Crawl4AI`. It sets up a list of target URLs and a `CrawlerRunConfig`. The example shows two approaches: streaming results as they complete using `async for` (`stream=True`) and collecting all results after all crawls are finished (`stream=False`). It prints success or error messages for each URL, including the length of the extracted markdown content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def quick_parallel_example():\n    urls = [\n        \"https://example.com/page1\",\n        \"https://example.com/page2\",\n        \"https://example.com/page3\"\n    ]\n    \n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=True  # Enable streaming mode\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        # Stream results as they complete\n        print(\"--- Streaming Results ---\")\n        async for result in await crawler.arun_many(urls, config=run_conf):\n            if result.success:\n                print(f\"[OK] {result.url}, length: {len(result.markdown.raw_markdown) if result.markdown else 'N/A'}\")\n            else:\n                print(f\"[ERROR] {result.url} => {result.error_message}\")\n\n        # Or get all results at once (default behavior)\n        print(\"\\n--- Batch Results ---\")\n        run_conf = run_conf.clone(stream=False)\n        results = await crawler.arun_many(urls, config=run_conf)\n        for res in results:\n            if res.success:\n                print(f\"[OK] {res.url}, length: {len(res.markdown.raw_markdown) if res.markdown else 'N/A'}\")\n            else:\n                print(f\"[ERROR] {res.url} => {res.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(quick_parallel_example())\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data Using CSS Selectors with Crawl4AI\nDESCRIPTION: This function shows how to extract structured data using CSS selectors and a predefined schema. It uses JsonCssExtractionStrategy to extract course information from a website, executes JavaScript to click through tabs for complete data collection, and prints the structured results.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_138\n\nLANGUAGE: python\nCODE:\n```\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    schema = {\n        \"name\": \"KidoCode Courses\",\n        \"baseSelector\": \"section.charge-methodology .framework-collection-item.w-dyn-item\",\n        \"fields\": [\n            {\n                \"name\": \"section_title\",\n                \"selector\": \"h3.heading-50\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"section_description\",\n                \"selector\": \".charge-content\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_name\",\n                \"selector\": \".text-block-93\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_description\",\n                \"selector\": \".course-content-text\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_icon\",\n                \"selector\": \".image-92\",\n                \"type\": \"attribute\",\n                \"attribute\": \"src\",\n            },\n        ],\n    }\n\n    browser_config = BrowserConfig(headless=True, java_script_enabled=True)\n\n    js_click_tabs = \"\"\"\n    (async () => {\n        const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");\n        for(let tab of tabs) {\n            tab.scrollIntoView();\n            tab.click();\n            await new Promise(r => setTimeout(r, 500));\n        }\n    })();\n    \"\"\"\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        js_code=[js_click_tabs],\n        delay_before_return_html=1\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config\n        )\n\n        companies = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(companies)} companies\")\n        print(json.dumps(companies[0], indent=2))\n```\n\n----------------------------------------\n\nTITLE: Multiple File Download Implementation in Crawl4AI\nDESCRIPTION: Complete example demonstrating how to download multiple files using Crawl4AI with custom configuration and error handling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\nimport os\nfrom pathlib import Path\n\nasync def download_multiple_files(url: str, download_path: str):\n    config = BrowserConfig(accept_downloads=True, downloads_path=download_path)\n    async with AsyncWebCrawler(config=config) as crawler:\n        run_config = CrawlerRunConfig(\n            js_code=\"\"\"\n                const downloadLinks = document.querySelectorAll('a[download]');\n                for (const link of downloadLinks) {\n                    link.click();\n                    // Delay between clicks\n                    await new Promise(r => setTimeout(r, 2000));  \n                }\n            \"\"\",\n            wait_for=10  # Wait for all downloads to start\n        )\n        result = await crawler.arun(url=url, config=run_config)\n\n        if result.downloaded_files:\n            print(\"Downloaded files:\")\n            for file in result.downloaded_files:\n                print(f\"- {file}\")\n        else:\n            print(\"No files downloaded.\")\n\n# Usage\ndownload_path = os.path.join(Path.home(), \".crawl4ai\", \"downloads\")\nos.makedirs(download_path, exist_ok=True)\n\nasyncio.run(download_multiple_files(\"https://www.python.org/downloads/windows/\", download_path))\n```\n\n----------------------------------------\n\nTITLE: Executing Deep Crawl and Processing Results in Python\nDESCRIPTION: This snippet shows how to execute a deep crawl using the AsyncWebCrawler, process the results, and group them by depth to visualize the crawl tree structure. It also includes performance measurement.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_206\n\nLANGUAGE: python\nCODE:\n```\nasync with AsyncWebCrawler() as crawler:\n    start_time = time.perf_counter()\n    results = await crawler.arun(url=\"https://docs.crawl4ai.com\", config=config)\n\n    # Group results by depth to visualize the crawl tree\n    pages_by_depth = {}\n    for result in results:\n        depth = result.metadata.get(\"depth\", 0)\n        if depth not in pages_by_depth:\n            pages_by_depth[depth] = []\n        pages_by_depth[depth].append(result.url)\n\n    print(f\"âœ… Crawled {len(results)} pages total\")\n\n    # Display crawl structure by depth\n    for depth, urls in sorted(pages_by_depth.items()):\n        print(f\"\\nDepth {depth}: {len(urls)} pages\")\n        # Show first 3 URLs for each depth as examples\n        for url in urls[:3]:\n            print(f\"  â†’ {url}\")\n        if len(urls) > 3:\n            print(f\"  ... and {len(urls) - 3} more\")\n\n    print(\n        f\"\\nâœ… Performance: {len(results)} pages in {time.perf_counter() - start_time:.2f} seconds\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Accessing Links from Crawl4AI Results\nDESCRIPTION: This code snippet shows how to access and print the first three internal links found in a crawl result. The links field is automatically populated during crawling unless link extraction is disabled.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nprint(result.links[\"internal\"][:3])  # Show first 3 internal links\n```\n\n----------------------------------------\n\nTITLE: Configuring Iframe Handling in Crawl4AI\nDESCRIPTION: This snippet demonstrates how to configure the Crawl4AI crawler to process iframes and remove overlay elements. It sets up a CrawlerRunConfig object with specific options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    # Merge iframe content into the final output\n    process_iframes=True,    \n    remove_overlay_elements=True\n)\n```\n\n----------------------------------------\n\nTITLE: Combining CSS Selection Methods in Crawl4AI\nDESCRIPTION: This example demonstrates how to combine target_elements and other configuration parameters in Crawl4AI to focus extraction on specific page elements while preserving the full page context for links and media analysis.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # Target specific content but preserve page context\n    config = CrawlerRunConfig(\n        # Focus markdown on main content and sidebar\n        target_elements=[\"#main-content\", \".sidebar\"],\n        \n        # Global filters applied to entire page\n        excluded_tags=[\"nav\", \"footer\", \"header\"],\n        exclude_external_links=True,\n        \n        # Use basic content thresholds\n        word_count_threshold=15,\n        \n        cache_mode=CacheMode.BYPASS\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/article\",\n            config=config\n        )\n        \n        print(f\"Content focuses on specific elements, but all links still analyzed\")\n        print(f\"Internal links: {len(result.links.get('internal', []))}\")\n        print(f\"External links: {len(result.links.get('external', []))}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Extracting HTML Table Data into Pandas DataFrame (Python)\nDESCRIPTION: Illustrates how to extract data from an HTML table captured during a crawl and load it into a Pandas DataFrame. The snippet accesses the table data stored in `result.media[\"tables\"]` and uses the headers and rows to construct the DataFrame. Requires the `pandas` library and a `result` object from a Crawl4AI execution.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.6.0.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nraw_df = pd.DataFrame(\n    result.media[\"tables\"][0][\"rows\"],\n    columns=result.media[\"tables\"][0][\"headers\"]\n)\n```\n```\n\n----------------------------------------\n\nTITLE: Using AsyncWebCrawler with Context Manager in Python\nDESCRIPTION: Recommended usage of AsyncWebCrawler within an async context manager, which automatically handles resource cleanup.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync with AsyncWebCrawler(config=browser_cfg) as crawler:\n    result = await crawler.arun(\"https://example.com\")\n    # The crawler automatically starts/closes resources\n```\n\n----------------------------------------\n\nTITLE: Filtering Links and Media in Crawl4AI\nDESCRIPTION: Example showing how to configure Crawl4AI to filter links and media. It demonstrates excluding external links, specific domains, social media links, and external images while ensuring images are properly loaded before crawling completes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_99\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    # Suppose we want to keep only internal links, remove certain domains, \n    # and discard external images from the final crawl data.\n    crawler_cfg = CrawlerRunConfig(\n        exclude_external_links=True,\n        exclude_domains=[\"spammyads.com\"],\n        exclude_social_media_links=True,   # skip Twitter, Facebook, etc.\n        exclude_external_images=True,      # keep only images from main domain\n        wait_for_images=True,             # ensure images are loaded\n        verbose=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://www.example.com\", config=crawler_cfg)\n\n        if result.success:\n            print(\"[OK] Crawled:\", result.url)\n            \n            # 1. Links\n            in_links = result.links.get(\"internal\", [])\n            ext_links = result.links.get(\"external\", [])\n            print(\"Internal link count:\", len(in_links))\n            print(\"External link count:\", len(ext_links))  # should be zero with exclude_external_links=True\n            \n            # 2. Images\n            images = result.media.get(\"images\", [])\n            print(\"Images found:\", len(images))\n            \n            # Let's see a snippet of these images\n            for i, img in enumerate(images[:3]):\n                print(f\"  - {img['src']} (alt={img.get('alt','')}, score={img.get('score','N/A')})\") \n        else:\n            print(\"[ERROR] Failed to crawl. Reason:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Setting Locale and Timezone with AsyncWebCrawler in Python\nDESCRIPTION: This code shows how to set the browser's locale and timezone using CrawlerRunConfig. It affects language preferences, date formats, and time-related functionality throughout the browsing session.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        config=CrawlerRunConfig(\n            # Set browser locale (language and region formatting)\n            locale=\"fr-FR\",  # French (France)\n            \n            # Set browser timezone\n            timezone_id=\"Europe/Paris\",\n            \n            # Other normal options...\n            magic=True,\n            page_timeout=60000\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Checking Crawl Success Status in Python\nDESCRIPTION: Shows how to check if a crawl operation was successful and access the error message if it failed.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring LLMContentFilter for Exact Content Preservation in Python\nDESCRIPTION: This snippet shows how to configure LLMContentFilter for exact content preservation, maintaining original wording and substance while removing irrelevant elements.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfilter = LLMContentFilter(\n    instruction=\"\"\"\n    Extract the main educational content while preserving its original wording and substance completely.\n    1. Maintain the exact language and terminology\n    2. Keep all technical explanations and examples intact\n    3. Preserve the original flow and structure\n    4. Remove only clearly irrelevant elements like navigation menus and ads\n    \"\"\",\n    chunk_token_threshold=4096\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Full-Page Screenshots and PDF Capture with Crawl4AI in Python\nDESCRIPTION: This code demonstrates how to use Crawl4AI to capture both PDF and screenshot of a large webpage. It shows the complete process of initializing the crawler, configuring the capture request with cache bypass, and saving both the PDF and screenshot to disk.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/full_page_screenshot_and_pdf_export.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport sys\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CacheMode, CrawlerRunConfig\n\n# Adjust paths as needed\nparent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.append(parent_dir)\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        # Request both PDF and screenshot\n        result = await crawler.arun(\n            url='https://en.wikipedia.org/wiki/List_of_common_misconceptions',\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                pdf=True,\n                screenshot=True\n            )\n        )\n        \n        if result.success:\n            # Save screenshot\n            if result.screenshot:\n                from base64 import b64decode\n                with open(os.path.join(__location__, \"screenshot.png\"), \"wb\") as f:\n                    f.write(b64decode(result.screenshot))\n            \n            # Save PDF\n            if result.pdf:\n                with open(os.path.join(__location__, \"page.pdf\"), \"wb\") as f:\n                    f.write(result.pdf)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Using Session Persistence with AsyncWebCrawler\nDESCRIPTION: This example demonstrates how to maintain session state across requests by providing storage_state to AsyncWebCrawler. It shows how to pass cookies and localStorage data to access protected pages without needing to authenticate again.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_134\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    storage_dict = {\n        \"cookies\": [\n            {\n                \"name\": \"session\",\n                \"value\": \"abcd1234\",\n                \"domain\": \"example.com\",\n                \"path\": \"/\",\n                \"expires\": 1699999999.0,\n                \"httpOnly\": False,\n                \"secure\": False,\n                \"sameSite\": \"None\"\n            }\n        ],\n        \"origins\": [\n            {\n                \"origin\": \"https://example.com\",\n                \"localStorage\": [\n                    {\"name\": \"token\", \"value\": \"my_auth_token\"}\n                ]\n            }\n        ]\n    }\n\n    # Provide the storage state as a dictionary to start \"already logged in\"\n    async with AsyncWebCrawler(\n        headless=True,\n        storage_state=storage_dict\n    ) as crawler:\n        result = await crawler.arun(\"https://example.com/protected\")\n        if result.success:\n            print(\"Protected page content length:\", len(result.html))\n        else:\n            print(\"Failed to crawl protected page\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Crawl4AI Configuration\nDESCRIPTION: Basic setup of Crawl4AI crawler with browser and run configurations. Shows how to initialize and run a basic crawl with cache bypass settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_122\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    browser_conf = BrowserConfig(headless=True)  # or False to see the browser\n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler(config=browser_conf) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_conf\n        )\n        print(result.markdown)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Configuring Iframe Handling in Crawl4AI\nDESCRIPTION: This snippet shows how to configure CrawlerRunConfig to process iframe content and merge it into the final output. The remove_overlay_elements parameter is also enabled to clean up common overlay elements that might interfere with content extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    # Merge iframe content into the final output\n    process_iframes=True,    \n    remove_overlay_elements=True\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Tutorial Runner in Python with Crawl4AI\nDESCRIPTION: This function executes all tutorial sections in sequence, demonstrating various aspects of deep crawling with Crawl4AI. It includes sections on basic deep crawl, stream vs non-stream, filters and scorers, max pages and thresholds, advanced filters, and a wrap-up.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_212\n\nLANGUAGE: python\nCODE:\n```\nasync def run_tutorial():\n    \"\"\"\n    Executes all tutorial sections in sequence.\n    \"\"\"\n    print(\"\\nðŸš€ CRAWL4AI DEEP CRAWLING TUTORIAL ðŸš€\")\n    print(\"======================================\")\n    print(\"This tutorial will walk you through deep crawling techniques,\")\n    print(\"from basic to advanced, using the Crawl4AI library.\")\n\n    # Define sections - uncomment to run specific parts during development\n    tutorial_sections = [\n        basic_deep_crawl,\n        stream_vs_nonstream,\n        filters_and_scorers,\n        max_pages_and_thresholds, \n        advanced_filters,\n        wrap_up,\n    ]\n\n    for section in tutorial_sections:\n        await section()\n\n    print(\"\\nðŸŽ‰ TUTORIAL COMPLETE! ðŸŽ‰\")\n    print(\"You now have a comprehensive understanding of deep crawling with Crawl4AI.\")\n    print(\"For more information, check out https://docs.crawl4ai.com\")\n\n# Execute the tutorial when run directly\nif __name__ == \"__main__\":\n    asyncio.run(run_tutorial())\n```\n\n----------------------------------------\n\nTITLE: Implementing Sliding Window Text Chunking in Python\nDESCRIPTION: Creates overlapping text chunks using a sliding window approach. Provides better context preservation through chunk overlap.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SlidingWindowChunking:\n    def __init__(self, window_size=100, step=50):\n        self.window_size = window_size\n        self.step = step\n\n    def chunk(self, text):\n        words = text.split()\n        chunks = []\n        for i in range(0, len(words) - self.window_size + 1, self.step):\n            chunks.append(' '.join(words[i:i + self.window_size]))\n        return chunks\n\n# Example Usage\ntext = \"This is a long text to demonstrate sliding window chunking.\"\nchunker = SlidingWindowChunking(window_size=5, step=2)\nprint(chunker.chunk(text))\n```\n\n----------------------------------------\n\nTITLE: Extracting HTML Element Attributes using crawl4ai\nDESCRIPTION: Shows how to extract attributes like href, src, or data-xxx from HTML elements using crawl4ai's extraction schema. This JSON snippet demonstrates the structure for defining attribute extraction in the schema.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_189\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"href\",\n  \"type\": \"attribute\",\n  \"attribute\": \"href\",\n  \"default\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Parallel Web Crawling of Multiple URLs in Python\nDESCRIPTION: This function demonstrates parallel crawling of multiple URLs using AsyncWebCrawler's arun_many method. It crawls three different websites simultaneously.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_148\n\nLANGUAGE: python\nCODE:\n```\nasync def demo_parallel_crawl():\n    \"\"\"Crawl multiple URLs in parallel\"\"\"\n    print(\"\\n=== 2. Parallel Crawling ===\")\n\n    urls = [\n        \"https://news.ycombinator.com/\",\n        \"https://example.com/\",\n        \"https://httpbin.org/html\",\n    ]\n\n    async with AsyncWebCrawler() as crawler:\n        results: List[CrawlResult] = await crawler.arun_many(\n            urls=urls,\n        )\n\n        print(f\"Crawled {len(results)} URLs in parallel:\")\n        for i, result in enumerate(results):\n            print(\n                f\"  {i + 1}. {result.url} - {'Success' if result.success else 'Failed'}\"\n            )\n```\n\n----------------------------------------\n\nTITLE: Converting Absolute XPath to Context-Sensitive XPath in Python\nDESCRIPTION: This function converts an absolute XPath to a context-sensitive XPath. It handles cases where the XPath is already context-sensitive, removes leading slashes, and provides a fallback for simpler descendant searches.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_76\n\nLANGUAGE: Python\nCODE:\n```\ndef _make_context_sensitive_xpath(self, xpath, element):\n    \"\"\"Convert absolute XPath to context-sensitive XPath\"\"\"\n    try:\n        # If starts with descendant-or-self, it's already context-sensitive\n        if xpath.startswith('descendant-or-self::'):\n            return xpath\n            \n        # Remove leading slash if present\n        if xpath.startswith('/'):\n            context_xpath = f\".{xpath}\"\n        else:\n            context_xpath = f\".//{xpath}\"\n            \n        # Validate the XPath by trying it\n        try:\n            element.xpath(context_xpath)\n            return context_xpath\n        except:\n            # If that fails, try a simpler descendant search\n            return f\".//{xpath.split('/')[-1]}\"\n    except:\n        return None\n```\n\n----------------------------------------\n\nTITLE: Accessing Error Messages in Python\nDESCRIPTION: Demonstrates how to access and display any error messages resulting from a failed crawl operation.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif not result.success:\n    print(\"Error:\", result.error_message)\n```\n\n----------------------------------------\n\nTITLE: CSS-based JSON Extraction from Raw HTML in Crawl4AI\nDESCRIPTION: Demonstrates structured data extraction using CSS selectors with Crawl4AI. This example processes raw HTML directly (without network requests) and extracts items according to a defined schema.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    schema = {\n        \"name\": \"Example Items\",\n        \"baseSelector\": \"div.item\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n    raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"raw://\" + raw_html,\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                extraction_strategy=JsonCssExtractionStrategy(schema)\n            )\n        )\n        data = json.loads(result.extracted_content)\n        print(data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Configuration Options for CosineStrategy in Python\nDESCRIPTION: Shows all available configuration parameters for the CosineStrategy class. This includes content filtering options, clustering parameters, model configuration, and verbosity settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_170\n\nLANGUAGE: python\nCODE:\n```\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring LXML-based Scraping Strategy in Crawl4AI\nDESCRIPTION: This snippet shows how to use the LXMLWebScrapingStrategy in Crawl4AI for improved performance when processing large HTML documents. It demonstrates the configuration of the scraping strategy in the CrawlerRunConfig.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LXMLWebScrapingStrategy\n\nasync def main():\n    config = CrawlerRunConfig(\n        scraping_strategy=LXMLWebScrapingStrategy()  # Faster alternative to default BeautifulSoup\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\", \n            config=config\n        )\n```\n\n----------------------------------------\n\nTITLE: Implementing PathDepthScorer Class for URL Path Analysis in Python\nDESCRIPTION: A scorer that evaluates URLs based on path depth, with higher scores for paths closer to an optimal depth. Uses efficient caching and path parsing without expensive regex operations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_203\n\nLANGUAGE: python\nCODE:\n```\nclass PathDepthScorer(URLScorer):\n    __slots__ = ('_weight', '_stats', '_optimal_depth')  # Remove _url_cache\n    \n    def __init__(self, optimal_depth: int = 3, weight: float = 1.0):\n        super().__init__(weight=weight)\n        self._optimal_depth = optimal_depth\n\n    @staticmethod\n    @lru_cache(maxsize=10000)\n    def _quick_depth(path: str) -> int:\n        \"\"\"Ultra fast path depth calculation.\n        \n        Examples:\n            - \"http://example.com\" -> 0  # No path segments\n            - \"http://example.com/\" -> 0  # Empty path\n            - \"http://example.com/a\" -> 1\n            - \"http://example.com/a/b\" -> 2\n        \"\"\"\n        if not path or path == '/':\n            return 0\n            \n        if '/' not in path:\n            return 0\n            \n        depth = 0\n        last_was_slash = True\n        \n        for c in path:\n            if c == '/':\n                if not last_was_slash:\n                    depth += 1\n                last_was_slash = True\n            else:\n                last_was_slash = False\n                \n        if not last_was_slash:\n            depth += 1\n            \n        return depth\n\n    @lru_cache(maxsize=10000)  # Cache the whole calculation\n    def _calculate_score(self, url: str) -> float:\n        pos = url.find('/', url.find('://') + 3)\n        if pos == -1:\n            depth = 0\n        else:\n            depth = self._quick_depth(url[pos:])\n            \n        # Use lookup table for common distances\n        distance = depth - self._optimal_depth\n        distance = distance if distance >= 0 else -distance  # Faster than abs()\n        \n        if distance < 4:\n            return _SCORE_LOOKUP[distance]\n            \n        return 1.0 / (1.0 + distance)                                             \n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI using pip (Bash)\nDESCRIPTION: This command uses pip, the Python package installer, to download and install the Crawl4AI library and its required dependencies. This installation is a prerequisite for running any of the provided examples.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/examples.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai\n```\n\n----------------------------------------\n\nTITLE: Implementing Cosine Similarity Strategy for Content Extraction\nDESCRIPTION: Shows how to implement semantic clustering-based content extraction using cosine similarity. This strategy helps extract topically related content sections with configurable parameters for clustering and similarity thresholds.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nasync def cosine_similarity_extraction():\n    async with AsyncWebCrawler() as crawler:\n        strategy = CosineStrategy(\n            word_count_threshold=10,\n            max_dist=0.2, # Maximum distance between two words\n            linkage_method=\"ward\", # Linkage method for hierarchical clustering (ward, complete, average, single)\n            top_k=3, # Number of top keywords to extract\n            sim_threshold=0.3, # Similarity threshold for clustering\n            semantic_filter=\"McDonald's economic impact, American consumer trends\", # Keywords to filter the content semantically using embeddings\n            verbose=True\n        )\n        \n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business/consumer/how-mcdonalds-e-coli-crisis-inflation-politics-reflect-american-story-rcna177156\",\n            extraction_strategy=strategy\n        )\n        print(json.loads(result.extracted_content)[:5])\n\nasyncio.run(cosine_similarity_extraction())\n```\n\n----------------------------------------\n\nTITLE: Combining CSS Selection Methods for Fine-Grained Content Control in Python\nDESCRIPTION: This example shows how to combine css_selector and target_elements in CrawlerRunConfig to achieve precise control over content extraction. It demonstrates focusing on specific elements while preserving page context and applying global filters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # Target specific content but preserve page context\n    config = CrawlerRunConfig(\n        # Focus markdown on main content and sidebar\n        target_elements=[\"#main-content\", \".sidebar\"],\n        \n        # Global filters applied to entire page\n        excluded_tags=[\"nav\", \"footer\", \"header\"],\n        exclude_external_links=True,\n        \n        # Use basic content thresholds\n        word_count_threshold=15,\n        \n        cache_mode=CacheMode.BYPASS\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/article\",\n            config=config\n        )\n        \n        print(f\"Content focuses on specific elements, but all links still analyzed\")\n        print(f\"Internal links: {len(result.links.get('internal', []))}\")\n        print(f\"External links: {len(result.links.get('external', []))}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Accessing and Processing Table Data in Crawl4AI\nDESCRIPTION: Example demonstrating how to extract and display structured data from HTML tables captured during crawling, including headers, rows, and metadata.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_96\n\nLANGUAGE: python\nCODE:\n```\nif result.success:\n    tables = result.media.get(\"tables\", [])\n    print(f\"Found {len(tables)} data tables on the page\")\n    \n    if tables:\n        # Access the first table\n        first_table = tables[0]\n        print(f\"Table caption: {first_table.get('caption', 'No caption')}\")\n        print(f\"Headers: {first_table.get('headers', [])}\")\n        \n        # Print the first 3 rows\n        for i, row in enumerate(first_table.get('rows', [])[:3]):\n            print(f\"Row {i+1}: {row}\")\n```\n\n----------------------------------------\n\nTITLE: Crawling a Web URL with AsyncWebCrawler in Python\nDESCRIPTION: This code demonstrates how to crawl a live website (Wikipedia) using Crawl4AI's AsyncWebCrawler. It creates a configuration object, initializes the crawler, and asynchronously retrieves and processes the web content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/local-files.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def crawl_web():\n    config = CrawlerRunConfig(bypass_cache=True)\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://en.wikipedia.org/wiki/apple\", \n            config=config\n        )\n        if result.success:\n            print(\"Markdown Content:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl: {result.error_message}\")\n\nasyncio.run(crawl_web())\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Crawl4AI\nDESCRIPTION: This snippet imports necessary modules and classes from the Crawl4AI library and other dependencies for web crawling and data extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_126\n\nLANGUAGE: python\nCODE:\n```\nimport os, sys\n\nfrom crawl4ai import LLMConfig\n\nsys.path.append(\n    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n)\n\nimport asyncio\nimport time\nimport json\nimport re\nfrom typing import Dict\nfrom bs4 import BeautifulSoup\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CacheMode, BrowserConfig, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.extraction_strategy import (\n    JsonCssExtractionStrategy,\n    LLMExtractionStrategy,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawler Settings and Extraction Strategy\nDESCRIPTION: Sets up crawler configuration including LLM extraction strategy, JSON CSS/XPath extraction, and cache settings. Handles different extraction configurations and validates required parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nllm_config=LLMConfig(provider=provider, api_token=token),\ninstruction=instruction,\nschema=load_schema_file(schema),\nextraction_type=\"schema\",\napply_chunking=False,\nforce_json_response=True,\nverbose=verbose\n```\n\n----------------------------------------\n\nTITLE: Implementing DomainFilter Class in Python\nDESCRIPTION: This class provides an optimized domain filter with fast lookups and caching. It includes methods for domain normalization, subdomain checking, and domain extraction using regex.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_189\n\nLANGUAGE: python\nCODE:\n```\nclass DomainFilter(URLFilter):\n    \"\"\"Optimized domain filter with fast lookups and caching\"\"\"\n\n    __slots__ = (\"_allowed_domains\", \"_blocked_domains\", \"_domain_cache\")\n\n    # Regex for fast domain extraction\n    _DOMAIN_REGEX = re.compile(r\"://([^/]+)\")\n\n    def __init__(\n        self,\n        allowed_domains: Union[str, List[str]] = None,\n        blocked_domains: Union[str, List[str]] = None,\n    ):\n        super().__init__()\n\n        # Convert inputs to frozensets for immutable, fast lookups\n        self._allowed_domains = (\n            frozenset(self._normalize_domains(allowed_domains))\n            if allowed_domains\n            else None\n        )\n        self._blocked_domains = (\n            frozenset(self._normalize_domains(blocked_domains))\n            if blocked_domains\n            else frozenset()\n        )\n\n    @staticmethod\n    def _normalize_domains(domains: Union[str, List[str]]) -> Set[str]:\n        \"\"\"Fast domain normalization\"\"\"\n        if isinstance(domains, str):\n            return {domains.lower()}\n        return {d.lower() for d in domains}\n    \n    @staticmethod\n    def _is_subdomain(domain: str, parent_domain: str) -> bool:\n        \"\"\"Check if domain is a subdomain of parent_domain\"\"\"\n        return domain == parent_domain or domain.endswith(f\".{parent_domain}\")\n\n    @staticmethod\n    @lru_cache(maxsize=10000)\n    def _extract_domain(url: str) -> str:\n        \"\"\"Ultra-fast domain extraction with regex and caching\"\"\"\n        match = DomainFilter._DOMAIN_REGEX.search(url)\n        return match.group(1).lower() if match else \"\"\n\n    def apply(self, url: str) -> bool:\n        \"\"\"Optimized domain checking with early returns\"\"\"\n        # Skip processing if no filters\n        if not self._blocked_domains and self._allowed_domains is None:\n            self._update_stats(True)\n            return True\n\n        domain = self._extract_domain(url)\n\n        # Check for blocked domains, including subdomains\n        for blocked in self._blocked_domains:\n            if self._is_subdomain(domain, blocked):\n                self._update_stats(False)\n                return False\n\n        # If no allowed domains specified, accept all non-blocked\n        if self._allowed_domains is None:\n            self._update_stats(True)\n            return True\n\n        # Check if domain matches any allowed domain (including subdomains)\n        for allowed in self._allowed_domains:\n            if self._is_subdomain(domain, allowed):\n                self._update_stats(True)\n                return True\n\n        # No matches found\n        self._update_stats(False)\n        return False\n```\n\n----------------------------------------\n\nTITLE: Implementing Advanced Filters in Python with AsyncWebCrawler\nDESCRIPTION: This function demonstrates advanced filtering techniques for specialized crawling. It includes examples of SEO filters, text relevancy filtering, and combining advanced filters for more precise crawling results.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_209\n\nLANGUAGE: python\nCODE:\n```\nasync def advanced_filters():\n    \"\"\"\n    PART 4: Demonstrates advanced filtering techniques for specialized crawling.\n\n    This function covers:\n    - SEO filters\n    - Text relevancy filtering\n    - Combining advanced filters\n    \"\"\"\n    print(\"\\n===== ADVANCED FILTERS =====\")\n\n    async with AsyncWebCrawler() as crawler:\n        # SEO FILTER EXAMPLE\n        print(\"\\nðŸ“Š EXAMPLE 1: SEO FILTERS\")\n        print(\n            \"Quantitative SEO quality assessment filter based searching keywords in the head section\"\n        )\n\n        seo_filter = SEOFilter(\n            threshold=0.5, keywords=[\"dynamic\", \"interaction\", \"javascript\"]\n        )\n\n        config = CrawlerRunConfig(\n            deep_crawl_strategy=BFSDeepCrawlStrategy(\n                max_depth=1, filter_chain=FilterChain([seo_filter])\n            ),\n            scraping_strategy=LXMLWebScrapingStrategy(),\n            verbose=True,\n            cache_mode=CacheMode.BYPASS,\n        )\n\n        results = await crawler.arun(url=\"https://docs.crawl4ai.com\", config=config)\n\n        print(f\"  âœ… Found {len(results)} pages with relevant keywords\")\n        for result in results:\n            print(f\"  â†’ {result.url}\")\n\n        # ADVANCED TEXT RELEVANCY FILTER\n        print(\"\\nðŸ“Š EXAMPLE 2: ADVANCED TEXT RELEVANCY FILTER\")\n\n        # More sophisticated content relevance filter\n        relevance_filter = ContentRelevanceFilter(\n            query=\"Interact with the web using your authentic digital identity\",\n            threshold=0.7,\n        )\n\n        config = CrawlerRunConfig(\n            deep_crawl_strategy=BFSDeepCrawlStrategy(\n                max_depth=1, filter_chain=FilterChain([relevance_filter])\n            ),\n            scraping_strategy=LXMLWebScrapingStrategy(),\n            verbose=True,\n            cache_mode=CacheMode.BYPASS,\n        )\n\n        results = await crawler.arun(url=\"https://docs.crawl4ai.com\", config=config)\n\n        print(f\"  âœ… Found {len(results)} pages\")\n        for result in results:\n            relevance_score = result.metadata.get(\"relevance_score\", 0)\n            print(f\"  â†’ Score: {relevance_score:.2f} | {result.url}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Headers in Crawl4AI\nDESCRIPTION: Demonstrates two methods of setting custom headers in Crawl4AI: at the crawler strategy level and directly in the 'arun()' method. It includes examples of setting user agents and language preferences.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Option 1: Set headers at the crawler strategy level\n    crawler1 = AsyncWebCrawler(\n        # The underlying strategy can accept headers in its constructor\n        crawler_strategy=None  # We'll override below for clarity\n    )\n    crawler1.crawler_strategy.update_user_agent(\"MyCustomUA/1.0\")\n    crawler1.crawler_strategy.set_custom_headers({\n        \"Accept-Language\": \"fr-FR,fr;q=0.9\"\n    })\n    result1 = await crawler1.arun(\"https://www.example.com\")\n    print(\"Example 1 result success:\", result1.success)\n\n    # Option 2: Pass headers directly to `arun()`\n    crawler2 = AsyncWebCrawler()\n    result2 = await crawler2.arun(\n        url=\"https://www.example.com\",\n        headers={\"Accept-Language\": \"es-ES,es;q=0.9\"}\n    )\n    print(\"Example 2 result success:\", result2.success)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Web Crawling with Crawl4AI in Python\nDESCRIPTION: This snippet demonstrates how to perform an asynchronous web crawl using the AsyncWebCrawler class from the crawl4ai Python package. It initializes an AsyncWebCrawler context, runs the crawler asynchronously on a given URL, and prints the Markdown-formatted extraction result. To run this code, install the crawl4ai package (e.g., via pip), ensure Python 3.7+ with asyncio support, and have network access to the target website. It accepts a target URL and outputs the extracted content; limitations may arise if the URL is unreachable or dependencies are unmet.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/index.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler() as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://crawl4ai.com\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Crawl4AI Example with CSS Selection and Pattern-Based Extraction\nDESCRIPTION: This comprehensive example demonstrates how to combine CSS selection, exclusion logic, and pattern-based extraction in Crawl4AI. It defines a function to extract main articles from a given URL with detailed configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_main_articles(url: str):\n    schema = {\n        \"name\": \"ArticleBlock\",\n        \"baseSelector\": \"div.article-block\",\n        \"fields\": [\n            {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n            {\n                \"name\": \"metadata\",\n                \"type\": \"nested\",\n                \"fields\": [\n                    {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                    {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n                ]\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        # Keep only #main-content\n        css_selector=\"#main-content\",\n        \n        # Filtering\n        word_count_threshold=10,\n        excluded_tags=[\"nav\", \"footer\"],  \n        exclude_external_links=True,\n        exclude_domains=[\"somebadsite.com\"],\n        exclude_external_images=True,\n\n        # Extraction\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        \n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url, config=config)\n        if not result.success:\n            print(f\"Error: {result.error_message}\")\n            return None\n        return json.loads(result.extracted_content)\n\nasync def main():\n    articles = await extract_main_articles(\"https://news.ycombinator.com/newest\")\n    if articles:\n        print(\"Extracted Articles:\", articles[:2])  # Show first 2\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Accessing Extracted Table Data with Crawl4AI in Python\nDESCRIPTION: Demonstrates how to access structured table data extracted by Crawl4AI. It checks if the crawl was successful (`result.success`), retrieves the list of detected data tables from `result.media.get(\"tables\", [])`, prints the total count, and then accesses properties like caption, headers, and rows for the first detected table. Extracted tables are those scoring above a defined threshold.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n```python\nif result.success:\n    tables = result.media.get(\"tables\", [])\n    print(f\"Found {len(tables)} data tables on the page\")\n    \n    if tables:\n        # Access the first table\n        first_table = tables[0]\n        print(f\"Table caption: {first_table.get('caption', 'No caption')}\")\n        print(f\"Headers: {first_table.get('headers', [])}\")\n        \n        # Print the first 3 rows\n        for i, row in enumerate(first_table.get('rows', [])[:3]):\n            print(f\"Row {i+1}: {row}\")\n```\n```\n\n----------------------------------------\n\nTITLE: Session Management with Storage State in Crawl4AI\nDESCRIPTION: Shows how to use storage state for authenticated crawling by importing a saved JSON state file. This enables reuse of authentication credentials across multiple crawling sessions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.2.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresult = await crawler.arun(\n    url=\"https://example.com/protected\",\n    storage_state=\"my_storage_state.json\"\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing RelevantContentFilter Abstract Base Class in Python\nDESCRIPTION: Defines an abstract base class for web content filtering with methods to extract relevant text from HTML. It includes configuration for allowed/excluded HTML tags and initialization parameters for user queries and logging.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_89\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\nimport re\nimport time\nfrom bs4 import BeautifulSoup, Tag\nfrom typing import List, Tuple, Dict, Optional\nfrom rank_bm25 import BM25Okapi\nfrom collections import deque\nfrom bs4 import NavigableString, Comment\n\nfrom .utils import (\n    clean_tokens,\n    perform_completion_with_backoff,\n    escape_json_string,\n    sanitize_html,\n    get_home_folder,\n    extract_xml_data,\n    merge_chunks,\n)\nfrom .types import LLMConfig\nfrom .config import DEFAULT_PROVIDER, OVERLAP_RATE, WORD_TOKEN_RATE\nfrom abc import ABC, abstractmethod\nimport math\nfrom snowballstemmer import stemmer\nfrom .models import TokenUsage\nfrom .prompts import PROMPT_FILTER_CONTENT\nimport json\nimport hashlib\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor\nfrom .async_logger import AsyncLogger, LogLevel\nfrom colorama import Fore, Style\n\n\nclass RelevantContentFilter(ABC):\n    \"\"\"Abstract base class for content filtering strategies\"\"\"\n\n    def __init__(\n        self,\n        user_query: str = None,\n        verbose: bool = False,\n        logger: Optional[AsyncLogger] = None,\n    ):\n        \"\"\"\n        Initializes the RelevantContentFilter class with optional user query.\n\n        Args:\n            user_query (str): User query for filtering (optional).\n            verbose (bool): Enable verbose logging (default: False).\n        \"\"\"\n        self.user_query = user_query\n        self.included_tags = {\n            # Primary structure\n            \"article\",\n            \"main\",\n            \"section\",\n            \"div\",\n            # List structures\n            \"ul\",\n            \"ol\",\n            \"li\",\n            \"dl\",\n            \"dt\",\n            \"dd\",\n            # Text content\n            \"p\",\n            \"span\",\n            \"blockquote\",\n            \"pre\",\n            \"code\",\n            # Headers\n            \"h1\",\n            \"h2\",\n            \"h3\",\n            \"h4\",\n            \"h5\",\n            \"h6\",\n            # Tables\n            \"table\",\n            \"thead\",\n            \"tbody\",\n            \"tr\",\n            \"td\",\n            \"th\",\n            # Other semantic elements\n            \"figure\",\n            \"figcaption\",\n            \"details\",\n            \"summary\",\n            # Text formatting\n            \"em\",\n            \"strong\",\n            \"b\",\n            \"i\",\n            \"mark\",\n            \"small\",\n            # Rich content\n            \"time\",\n            \"address\",\n            \"cite\",\n            \"q\",\n        }\n        self.excluded_tags = {\n            \"nav\",\n            \"footer\",\n            \"header\",\n            \"aside\",\n            \"script\",\n            \"style\",\n            \"form\",\n            \"iframe\",\n            \"noscript\",\n        }\n        self.header_tags = {\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"}\n        self.negative_patterns = re.compile(\n            r\"nav|footer|header|sidebar|ads|comment|promo|advert|social|share\", re.I\n        )\n        self.min_word_count = 2\n        self.verbose = False\n        self.logger = logger\n```\n\n----------------------------------------\n\nTITLE: Handling HTTP Status Codes in Python\nDESCRIPTION: Example of how to check the HTTP status code of a crawled page and respond accordingly, such as detecting 404 'Page Not Found' errors.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif result.status_code == 404:\n    print(\"Page not found!\")\n```\n\n----------------------------------------\n\nTITLE: Basic Crawl4AI CLI Usage Commands\nDESCRIPTION: Demonstrates the basic usage patterns for the Crawl4AI CLI tool, including simple crawling, output format selection, and additional options like verbose mode and cache bypass.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Basic crawling\ncrwl https://example.com\n\n# Get markdown output\ncrwl https://example.com -o markdown\n\n# Verbose JSON output with cache bypass\ncrwl https://example.com -o json -v --bypass-cache\n\n# See usage examples\ncrwl --example\n```\n\n----------------------------------------\n\nTITLE: Defining the JavaScript Execution API Endpoint\nDESCRIPTION: Specifies the HTTP method (POST) and path (`/execute_js`) for the endpoint that allows executing custom JavaScript code on a target web page and receiving the full crawl result.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_25\n\nLANGUAGE: plaintext\nCODE:\n```\nPOST /execute_js\n```\n\n----------------------------------------\n\nTITLE: Configuring BFS Deep Crawl Strategy in Crawl4AI\nDESCRIPTION: This snippet shows how to configure a Breadth-First Search (BFS) deep crawling strategy with parameters for depth control, domain boundaries, and page limits. It demonstrates the key parameters available for customizing BFS crawling behavior.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n\n# Basic configuration\nstrategy = BFSDeepCrawlStrategy(\n    max_depth=2,               # Crawl initial page + 2 levels deep\n    include_external=False,    # Stay within the same domain\n    max_pages=50,              # Maximum number of pages to crawl (optional)\n    score_threshold=0.3,       # Minimum score for URLs to be crawled (optional)\n)\n```\n\n----------------------------------------\n\nTITLE: CLI Entry Point for Crawl4AI\nDESCRIPTION: Main entry point function for the Crawl4AI CLI that handles command-line arguments. If no command is specified, it inserts 'crawl' as the default command.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_56\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    import sys\n    if len(sys.argv) < 2 or sys.argv[1] not in cli.commands:\n        sys.argv.insert(1, \"crawl\")\n    cli()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Accessing Markdown Content in Python\nDESCRIPTION: Shows how to access different types of markdown content from a CrawlResult object, including raw, fit, and HTML versions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(result.markdown.raw_markdown[:200])\nprint(result.markdown.fit_markdown)\nprint(result.markdown.fit_html)\n```\n\n----------------------------------------\n\nTITLE: Content Filtering Configuration in YAML for Crawl4AI\nDESCRIPTION: Defines two content filtering approaches: BM25 for relevance filtering based on a query and threshold, and pruning for focusing on specific topics.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\n# filter_bm25.yml\ntype: \"bm25\"\nquery: \"target content\"\nthreshold: 1.0\n\n# filter_pruning.yml\ntype: \"pruning\"\nquery: \"focus topic\"\nthreshold: 0.48\n```\n\n----------------------------------------\n\nTITLE: PDF Processing with Crawl4AI\nDESCRIPTION: Demonstrates how to extract text, metadata, and images from PDF files using Crawl4AI's PDF processing capabilities. This example uses PDFCrawlerStrategy and PDFContentScrapingStrategy to process a PDF from a URL.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.processors.pdf import PDFCrawlerStrategy, PDFContentScrapingStrategy\nimport asyncio\n\nasync def main():\n    async with AsyncWebCrawler(crawler_strategy=PDFCrawlerStrategy()) as crawler:\n        result = await crawler.arun(\n            \"https://arxiv.org/pdf/2310.06825.pdf\",\n            config=CrawlerRunConfig(\n                scraping_strategy=PDFContentScrapingStrategy()\n            )\n        )\n        print(result.markdown)  # Access extracted text\n        print(result.metadata)  # Access PDF metadata (title, author, etc.)\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Stream vs. Non-Stream Execution in Python with AsyncWebCrawler\nDESCRIPTION: This function compares stream and non-stream execution modes of the AsyncWebCrawler. It demonstrates how non-streaming mode waits for all results before processing, while streaming mode processes results as they become available.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_207\n\nLANGUAGE: python\nCODE:\n```\nasync def stream_vs_nonstream():\n    \"\"\"\n    PART 2: Demonstrates the difference between stream and non-stream execution.\n\n    Non-stream: Waits for all results before processing\n    Stream: Processes results as they become available\n    \"\"\"\n    print(\"\\n===== STREAM VS. NON-STREAM EXECUTION =====\")\n\n    # Common configuration for both examples\n    base_config = CrawlerRunConfig(\n        deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1, include_external=False),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        verbose=False,\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        # NON-STREAMING MODE\n        print(\"\\nðŸ“Š NON-STREAMING MODE:\")\n        print(\"  In this mode, all results are collected before being returned.\")\n\n        non_stream_config = base_config.clone()\n        non_stream_config.stream = False\n\n        start_time = time.perf_counter()\n        results = await crawler.arun(\n            url=\"https://docs.crawl4ai.com\", config=non_stream_config\n        )\n\n        print(f\"  âœ… Received all {len(results)} results at once\")\n        print(f\"  âœ… Total duration: {time.perf_counter() - start_time:.2f} seconds\")\n\n        # STREAMING MODE\n        print(\"\\nðŸ“Š STREAMING MODE:\")\n        print(\"  In this mode, results are processed as they become available.\")\n\n        stream_config = base_config.clone()\n        stream_config.stream = True\n\n        start_time = time.perf_counter()\n        result_count = 0\n        first_result_time = None\n\n        async for result in await crawler.arun(\n            url=\"https://docs.crawl4ai.com\", config=stream_config\n        ):\n            result_count += 1\n            if result_count == 1:\n                first_result_time = time.perf_counter() - start_time\n                print(\n                    f\"  âœ… First result received after {first_result_time:.2f} seconds: {result.url}\"\n                )\n            elif result_count % 5 == 0:  # Show every 5th result for brevity\n                print(f\"  â†’ Result #{result_count}: {result.url}\")\n\n        print(f\"  âœ… Total: {result_count} results\")\n        print(f\"  âœ… First result: {first_result_time:.2f} seconds\")\n        print(f\"  âœ… All results: {time.perf_counter() - start_time:.2f} seconds\")\n        print(\"\\nðŸ” Key Takeaway: Streaming allows processing results immediately\")\n```\n\n----------------------------------------\n\nTITLE: Defining the PDF Export API Endpoint\nDESCRIPTION: Specifies the HTTP method (POST) and path (`/pdf`) for the endpoint designed to generate a PDF document from the content of a specified URL.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_23\n\nLANGUAGE: plaintext\nCODE:\n```\nPOST /pdf\n```\n\n----------------------------------------\n\nTITLE: Downloading Crawl4AI Models using CLI\nDESCRIPTION: Command to download required models for Crawl4AI using the CLI, recommended after installation with advanced options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncrawl4ai-download-models\n```\n\n----------------------------------------\n\nTITLE: Defining LLMConfig Class for Language Model Integration in Python\nDESCRIPTION: This snippet defines the LLMConfig class, which manages configuration for language model providers. It handles API tokens, base URLs, and various model parameters, with support for environment variable-based configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nclass LLMConfig:\n    def __init__(\n        self,\n        provider: str = DEFAULT_PROVIDER,\n        api_token: Optional[str] = None,\n        base_url: Optional[str] = None,\n        temprature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        top_p: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        stop: Optional[List[str]] = None,\n        n: Optional[int] = None,    \n    ):\n        \"\"\"Configuaration class for LLM provider and API token.\"\"\"\n        self.provider = provider\n        if api_token and not api_token.startswith(\"env:\"):\n            self.api_token = api_token\n        elif api_token and api_token.startswith(\"env:\"):\n            self.api_token = os.getenv(api_token[4:])\n        else:\n            # Check if given provider starts with any of key in PROVIDER_MODELS_PREFIXES\n            # If not, check if it is in PROVIDER_MODELS\n            prefixes = PROVIDER_MODELS_PREFIXES.keys()\n            if any(provider.startswith(prefix) for prefix in prefixes):\n                selected_prefix = next(\n                    (prefix for prefix in prefixes if provider.startswith(prefix)),\n                    None,\n                )\n                self.api_token = PROVIDER_MODELS_PREFIXES.get(selected_prefix)                    \n            else:\n                self.provider = DEFAULT_PROVIDER\n                self.api_token = os.getenv(DEFAULT_PROVIDER_API_KEY)\n        self.base_url = base_url\n        self.temprature = temprature\n        self.max_tokens = max_tokens\n        self.top_p = top_p\n        self.frequency_penalty = frequency_penalty\n        self.presence_penalty = presence_penalty\n        self.stop = stop\n        self.n = n\n```\n\n----------------------------------------\n\nTITLE: Basic Installation of Crawl4AI using pip (Bash)\nDESCRIPTION: This command installs the basic asynchronous version of Crawl4AI using pip and runs the setup command to configure the necessary browser components (Playwright by default).\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai\ncrawl4ai-setup # Setup the browser\n```\n\n----------------------------------------\n\nTITLE: Setting Up LLM Configuration in Python\nDESCRIPTION: Configures the default LLM provider and API token, prompting the user if not already configured. Supports various LLM providers including OpenAI, Anthropic, and Ollama.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef setup_llm_config() -> tuple[str, str]:\n    config = get_global_config()\n    provider = config.get(\"DEFAULT_LLM_PROVIDER\")\n    token = config.get(\"DEFAULT_LLM_PROVIDER_TOKEN\")\n    \n    if not provider:\n        click.echo(\"\\nNo default LLM provider configured.\")\n        click.echo(\"Provider format: 'company/model' (e.g., 'openai/gpt-4o', 'anthropic/claude-3-sonnet')\")\n        click.echo(\"See available providers at: https://docs.litellm.ai/docs/providers\")\n        provider = click.prompt(\"Enter provider\")\n        \n    if not provider.startswith(\"ollama/\"):\n        if not token:\n            token = click.prompt(\"Enter API token for \" + provider, hide_input=True)\n    else:\n        token = \"no-token\"\n    \n    if not config.get(\"DEFAULT_LLM_PROVIDER\") or not config.get(\"DEFAULT_LLM_PROVIDER_TOKEN\"):\n        config[\"DEFAULT_LLM_PROVIDER\"] = provider\n        config[\"DEFAULT_LLM_PROVIDER_TOKEN\"] = token\n        save_global_config(config)\n        click.echo(\"\\nConfiguration saved to ~/.crawl4ai/global.yml\")\n    \n    return provider, token\n```\n\n----------------------------------------\n\nTITLE: Using Magic Mode in Crawl4AI (Python)\nDESCRIPTION: Python code example demonstrating how to use Magic Mode in Crawl4AI for simplified automation without persistent profiles.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        config=CrawlerRunConfig(\n            magic=True,  # Simplifies a lot of interaction\n            remove_overlay_elements=True,\n            page_timeout=60000\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: LLMContentFilter Class Definition for AI-Powered Content Filtering\nDESCRIPTION: Definition of the LLMContentFilter class that extends RelevantContentFilter. This class uses language models to generate markdown from HTML content and provides advanced filtering capabilities based on LLM-generated content scores.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_100\n\nLANGUAGE: python\nCODE:\n```\nclass LLMContentFilter(RelevantContentFilter):\n    \"\"\"Content filtering using LLMs to generate relevant markdown.\n\n    How it works:\n    1. Extracts page metadata with fallbacks.\n    2. Extracts text chunks from the body element.\n    3. Applies LLMs to generate markdown for each chunk.\n    4. Filters out chunks below the threshold.\n    5. Sorts chunks by score in descending order.\n    6. Returns the top N chunks.\n\n    Attributes:\n        llm_config (LLMConfig): LLM configuration object.\n        instruction (str): Instruction for LLM markdown generation\n        chunk_token_threshold (int): Chunk token threshold for splitting (default: 1e9).\n        overlap_rate (float): Overlap rate for chunking (default: 0.5).\n        word_token_rate (float): Word token rate for chunking (default: 0.2).\n        verbose (bool): Enable verbose logging (default: False).\n        logger (AsyncLogger): Custom logger for LLM operations (optional).\n    \"\"\"\n    _UNWANTED_PROPS = {\n        'provider' : 'Instead, use llm_config=LLMConfig(provider=\"...\")',\n        'api_token' : 'Instead, use llm_config=LlMConfig(api_token=\"...\")',\n        'base_url' : 'Instead, use llm_config=LLMConfig(base_url=\"...\")',\n        'api_base' : 'Instead, use llm_config=LLMConfig(base_url=\"...\")',\n    }\n\n    def __init__(\n        self,\n        llm_config: \"LLMConfig\" = None,\n        instruction: str = None,\n        chunk_token_threshold: int = int(1e9),\n        overlap_rate: float = OVERLAP_RATE,\n        word_token_rate: float = WORD_TOKEN_RATE,\n        # char_token_rate: float = WORD_TOKEN_RATE * 5,\n        # chunk_mode: str = \"char\",\n        verbose: bool = False,\n        logger: Optional[AsyncLogger] = None,\n        ignore_cache: bool = True,\n        # Deprecated properties\n        provider: str = DEFAULT_PROVIDER,\n        api_token: Optional[str] = None,\n        base_url: Optional[str] = None,\n        api_base: Optional[str] = None,\n        extra_args: Dict = None,\n    ):\n        super().__init__(None)\n        self.provider = provider\n        self.api_token = api_token\n        self.base_url = base_url or api_base\n        self.llm_config = llm_config\n        self.instruction = instruction\n        self.chunk_token_threshold = chunk_token_threshold\n        self.overlap_rate = overlap_rate\n        self.word_token_rate = word_token_rate or WORD_TOKEN_RATE\n        # self.chunk_mode: str = chunk_mode\n        # self.char_token_rate = char_token_rate or word_token_rate / 5\n        # self.token_rate = word_token_rate if chunk_mode == \"word\" else self.char_token_rate\n        self.token_rate = word_token_rate or WORD_TOKEN_RATE\n        self.extra_args = extra_args or {}\n        self.ignore_cache = ignore_cache\n        self.verbose = verbose\n\n        # Setup logger with custom styling for LLM operations\n        if logger:\n            self.logger = logger\n        elif verbose:\n            self.logger = AsyncLogger(\n                verbose=verbose,\n                icons={\n                    **AsyncLogger.DEFAULT_ICONS,\n                    \"LLM\": \"â˜…\",  # Star for LLM operations\n                    \"CHUNK\": \"â—ˆ\",  # Diamond for chunks\n                    \"CACHE\": \"âš¡\",  # Lightning for cache operations\n                },\n                colors={\n                    **AsyncLogger.DEFAULT_COLORS,\n                    LogLevel.INFO: Fore.MAGENTA\n                    + Style.DIM,  # Dimmed purple for LLM ops\n                },\n            )\n        else:\n            self.logger = None\n\n        self.usages = []\n        self.total_usage = TokenUsage()\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Download Directory in Crawl4AI\nDESCRIPTION: Shows how to specify a custom download directory for downloaded files using the downloads_path parameter in BrowserConfig.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import BrowserConfig\nimport os\n\ndownloads_path = os.path.join(os.getcwd(), \"my_downloads\")  # Custom download path\nos.makedirs(downloads_path, exist_ok=True)\n\nconfig = BrowserConfig(accept_downloads=True, downloads_path=downloads_path)\n\nasync def main():\n    async with AsyncWebCrawler(config=config) as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy Support and Rotation\nDESCRIPTION: Sets up proxy configuration for web crawling with authentication. Enables proxy rotation and verification for distributed crawling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    proxy_config={\n        \"server\": \"http://proxy:8080\",\n        \"username\": \"user\",\n        \"password\": \"pass\"\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Rotating Proxies in Crawl4AI\nDESCRIPTION: Demonstrates implementing a dynamic proxy rotation system using async functions. Shows how to create new browser contexts with different proxies for each URL crawl.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/proxy-security.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync def main():\n    browser_config = BrowserConfig()\n    run_config = CrawlerRunConfig()\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        # For each URL, create a new run config with different proxy\n        for url in urls:\n            proxy = await get_next_proxy()\n            # Clone the config and update proxy - this creates a new browser context\n            current_config = run_config.clone(proxy_config=proxy)\n            result = await crawler.arun(url=url, config=current_config)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Handling Downloaded Files in Python\nDESCRIPTION: Shows how to access and process files that were downloaded during a crawl operation when downloads are enabled in the crawler configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nif result.downloaded_files:\n    for file_path in result.downloaded_files:\n        print(\"Downloaded:\", file_path)\n```\n\n----------------------------------------\n\nTITLE: Configuring LLMContentFilter for Exact Content Preservation\nDESCRIPTION: This code example shows how to set up LLMContentFilter with instructions focused on preserving original content while removing irrelevant elements. It maintains the exact language and structure of the educational content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_109\n\nLANGUAGE: python\nCODE:\n```\nfilter = LLMContentFilter(\n    instruction=\"\"\"\n    Extract the main educational content while preserving its original wording and substance completely.\n    1. Maintain the exact language and terminology\n    2. Keep all technical explanations and examples intact\n    3. Preserve the original flow and structure\n    4. Remove only clearly irrelevant elements like navigation menus and ads\n    \"\"\",\n    chunk_token_threshold=4096\n)\n```\n\n----------------------------------------\n\nTITLE: Using MemoryAdaptiveDispatcher in Batch and Stream Modes with Python\nDESCRIPTION: This code snippet illustrates how to use the new MemoryAdaptiveDispatcher in both batch and stream modes. It demonstrates configuring the dispatcher and using it with AsyncWebCrawler for efficient memory management during crawling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, MemoryAdaptiveDispatcher\nimport asyncio\n\n# Configure the dispatcher (optional, defaults are used if not provided)\ndispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=80.0,  # Pause if memory usage exceeds 80%\n    check_interval=0.5,  # Check memory every 0.5 seconds\n)\n\nasync def batch_mode():\n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun_many(\n            urls=[\"https://docs.crawl4ai.com\", \"https://github.com/unclecode/crawl4ai\"],\n            config=CrawlerRunConfig(stream=False),  # Batch mode\n            dispatcher=dispatcher,\n        )\n        for result in results:\n            print(f\"Crawled: {result.url} with status code: {result.status_code}\")\n\nasync def stream_mode():\n    async with AsyncWebCrawler() as crawler:\n        # OR, for streaming:\n        async for result in await crawler.arun_many(\n            urls=[\"https://docs.crawl4ai.com\", \"https://github.com/unclecode/crawl4ai\"],\n            config=CrawlerRunConfig(stream=True),\n            dispatcher=dispatcher,\n        ):\n            print(f\"Crawled: {result.url} with status code: {result.status_code}\")\n\nprint(\"Dispatcher in batch mode:\")\nasyncio.run(batch_mode())\nprint(\"-\" * 50)\nprint(\"Dispatcher in stream mode:\")\nasyncio.run(stream_mode())\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI with PyTorch Integration\nDESCRIPTION: Command to install Crawl4AI with PyTorch-based features for advanced text processing capabilities.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_82\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai[torch]\ncrawl4ai-setup\n```\n\n----------------------------------------\n\nTITLE: Configuring Development Settings for Crawl4AI in YAML\nDESCRIPTION: This YAML snippet provides recommended development settings for Crawl4AI, including hot reloading and verbose logging.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_68\n\nLANGUAGE: yaml\nCODE:\n```\napp:\n  reload: True               # Enable hot reloading\n  timeout_keep_alive: 300    # Longer timeout for debugging\n\nlogging:\n  level: \"DEBUG\"            # More verbose logging\n```\n\n----------------------------------------\n\nTITLE: Implementing ProxyConfig Class in Python\nDESCRIPTION: Configuration class for handling proxy settings. Includes initialization, IP extraction, and various factory methods for creating proxy configurations from different sources.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass ProxyConfig:\n    def __init__(\n        self,\n        server: str,\n        username: Optional[str] = None,\n        password: Optional[str] = None,\n        ip: Optional[str] = None,\n    ):\n        self.server = server\n        self.username = username\n        self.password = password\n        self.ip = ip or self._extract_ip_from_server()\n    \n    def _extract_ip_from_server(self) -> Optional[str]:\n        try:\n            if \"://\" in self.server:\n                parts = self.server.split(\"://\")[1].split(\":\")\n                return parts[0]\n            else:\n                parts = self.server.split(\":\")\n                return parts[0]\n        except Exception:\n            return None\n    \n    @staticmethod\n    def from_string(proxy_str: str) -> \"ProxyConfig\":\n        parts = proxy_str.split(\":\")\n        if len(parts) == 4:\n            ip, port, username, password = parts\n            return ProxyConfig(\n                server=f\"http://{ip}:{port}\",\n                username=username,\n                password=password,\n                ip=ip\n            )\n        elif len(parts) == 2:\n            ip, port = parts\n            return ProxyConfig(\n                server=f\"http://{ip}:{port}\",\n                ip=ip\n            )\n        else:\n            raise ValueError(f\"Invalid proxy string format: {proxy_str}\")\n    \n    @staticmethod\n    def from_dict(proxy_dict: Dict) -> \"ProxyConfig\":\n        return ProxyConfig(\n            server=proxy_dict.get(\"server\"),\n            username=proxy_dict.get(\"username\"),\n            password=proxy_dict.get(\"password\"),\n            ip=proxy_dict.get(\"ip\")\n        )\n    \n    @staticmethod\n    def from_env(env_var: str = \"PROXIES\") -> List[\"ProxyConfig\"]:\n        proxies = []\n        try:\n            proxy_list = os.getenv(env_var, \"\").split(\",\")\n            for proxy in proxy_list:\n                if not proxy:\n                    continue\n                proxies.append(ProxyConfig.from_string(proxy))\n        except Exception as e:\n            print(f\"Error loading proxies from environment: {e}\")\n        return proxies\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming URL Processing in Python\nDESCRIPTION: Demonstrates real-time processing of crawled URLs using streaming configuration. Enables immediate processing of results as they become available instead of waiting for all crawls to complete.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(stream=True)\n\nasync with AsyncWebCrawler() as crawler:\n    async for result in await crawler.arun_many(urls, config=config):\n        print(f\"Got result for {result.url}\")\n        # Process each result immediately\n```\n\n----------------------------------------\n\nTITLE: HTML Tree Pruning Algorithm for Content Extraction\nDESCRIPTION: A method that recursively prunes an HTML tree starting from a given node. It calculates a score for each node based on metrics like text density and tag importance, and removes nodes with scores below a threshold.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_97\n\nLANGUAGE: python\nCODE:\n```\ndef _prune_tree(self, node):\n    \"\"\"\n    Prunes the tree starting from the given node.\n\n    Args:\n        node (Tag): The node from which the pruning starts.\n    \"\"\"\n    if not node or not hasattr(node, \"name\") or node.name is None:\n        return\n\n    text_len = len(node.get_text(strip=True))\n    tag_len = len(node.encode_contents().decode(\"utf-8\"))\n    link_text_len = sum(\n        len(s.strip())\n        for s in (a.string for a in node.find_all(\"a\", recursive=False))\n        if s\n    )\n\n    metrics = {\n        \"node\": node,\n        \"tag_name\": node.name,\n        \"text_len\": text_len,\n        \"tag_len\": tag_len,\n        \"link_text_len\": link_text_len,\n    }\n\n    score = self._compute_composite_score(metrics, text_len, tag_len, link_text_len)\n\n    if self.threshold_type == \"fixed\":\n        should_remove = score < self.threshold\n    else:  # dynamic\n        tag_importance = self.tag_importance.get(node.name, 0.7)\n        text_ratio = text_len / tag_len if tag_len > 0 else 0\n        link_ratio = link_text_len / text_len if text_len > 0 else 1\n\n        threshold = self.threshold  # base threshold\n        if tag_importance > 1:\n            threshold *= 0.8\n        if text_ratio > 0.4:\n            threshold *= 0.9\n        if link_ratio > 0.6:\n            threshold *= 1.2\n\n        should_remove = score < threshold\n\n    if should_remove:\n        node.decompose()\n    else:\n        children = [child for child in node.children if hasattr(child, \"name\")]\n        for child in children:\n            self._prune_tree(child)\n```\n\n----------------------------------------\n\nTITLE: NoExtractionStrategy Implementation\nDESCRIPTION: Implementation of a basic extraction strategy that doesn't process HTML content but simply returns it as-is. Used as a fallback or for cases where no content extraction is needed.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nclass NoExtractionStrategy(ExtractionStrategy):\n    \"\"\"\n    A strategy that does not extract any meaningful content from the HTML. It simply returns the entire HTML as a single block.\n    \"\"\"\n\n    def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extract meaningful blocks or chunks from the given HTML.\n        \"\"\"\n        return [{\"index\": 0, \"content\": html}]\n\n    def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]:\n        return [\n            {\"index\": i, \"tags\": [], \"content\": section}\n            for i, section in enumerate(sections)\n        ]\n```\n\n----------------------------------------\n\nTITLE: Generating Focused Markdown with LLM Content Filter in Python\nDESCRIPTION: This function demonstrates the use of a PruningContentFilter to generate focused markdown content from a crawled page (Wikipedia article on Python programming language).\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_149\n\nLANGUAGE: python\nCODE:\n```\nasync def demo_fit_markdown():\n    \"\"\"Generate focused markdown with LLM content filter\"\"\"\n    print(\"\\n=== 3. Fit Markdown with LLM Content Filter ===\")\n\n    async with AsyncWebCrawler() as crawler:\n        result: CrawlResult = await crawler.arun(\n            url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\",\n            config=CrawlerRunConfig(\n                markdown_generator=DefaultMarkdownGenerator(\n                    content_filter=PruningContentFilter()\n                )\n            ),\n        )\n\n        # Print stats and save the fit markdown\n        print(f\"Raw: {len(result.markdown.raw_markdown)} chars\")\n        print(f\"Fit: {len(result.markdown.fit_markdown)} chars\")\n```\n\n----------------------------------------\n\nTITLE: Schema-Based Crypto Price Extraction in Python\nDESCRIPTION: Shows how to extract cryptocurrency prices using JsonCssExtractionStrategy without LLMs. Defines a schema with CSS selectors to extract structured data from web pages efficiently.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_185\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_crypto_prices():\n    # 1. Define a simple extraction schema\n    schema = {\n        \"name\": \"Crypto Prices\",\n        \"baseSelector\": \"div.crypto-row\",    # Repeated elements\n        \"fields\": [\n            {\n                \"name\": \"coin_name\",\n                \"selector\": \"h2.coin-name\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"span.coin-price\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n\n    # 2. Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # 3. Set up your crawler config (if needed)\n    config = CrawlerRunConfig(\n        # e.g., pass js_code or wait_for if the page is dynamic\n        # wait_for=\"css:.crypto-row:nth-child(20)\"\n        cache_mode = CacheMode.BYPASS,\n        extraction_strategy=extraction_strategy,\n    )\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # 4. Run the crawl and extraction\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            \n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        # 5. Parse the extracted JSON\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin entries\")\n        print(json.dumps(data[0], indent=2) if data else \"No data found\")\n\nasyncio.run(extract_crypto_prices())\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI with Transformer features\nDESCRIPTION: Installs Crawl4AI with Hugging Face-based summarization or generation strategies, followed by running the setup command.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai[transformer]\ncrawl4ai-setup\n```\n\n----------------------------------------\n\nTITLE: Content Filtering Configuration in Crawl4AI\nDESCRIPTION: This snippet shows how to configure various content filtering options in CrawlerRunConfig including word count thresholds, tag exclusions, link filtering, and media filtering parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n\n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n\n    # Link filtering\n    exclude_external_links=True,    \n    exclude_social_media_links=True,\n    # Block entire domains\n    exclude_domains=[\"adtrackers.com\", \"spammynews.org\"],    \n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],\n\n    # Media filtering\n    exclude_external_images=True\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Topic-Based Segmentation with NLTK TextTiling in Python\nDESCRIPTION: Creates topic-coherent text chunks using the TextTilingTokenizer from NLTK. The class maintains a tokenizer instance and provides a chunk method that divides text based on topic transitions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_165\n\nLANGUAGE: python\nCODE:\n```\nfrom nltk.tokenize import TextTilingTokenizer\n\nclass TopicSegmentationChunking:\n    def __init__(self):\n        self.tokenizer = TextTilingTokenizer()\n\n    def chunk(self, text):\n        return self.tokenizer.tokenize(text)\n\n# Example Usage\ntext = \"\"\"This is an introduction.\nThis is a detailed discussion on the topic.\"\"\"\nchunker = TopicSegmentationChunking()\nprint(chunker.chunk(text))\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container with Custom Config Mount using Bash\nDESCRIPTION: Shows a `docker run` command in Bash to start the `unclecode/crawl4ai` container in detached mode (`-d`). It maps port 11235, names the container, loads environment variables from `.llm.env`, allocates shared memory (`--shm-size`), and crucially, mounts a local `my-custom-config.yml` file to `/app/config.yml` inside the container using the `-v` flag. This overrides the default configuration at runtime. Requires Docker installed and the custom config file present.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\n# Assumes my-custom-config.yml is in the current directory\ndocker run -d -p 11235:11235 \\\n  --name crawl4ai-custom-config \\\n  --env-file .llm.env \\\n  --shm-size=1g \\\n  -v $(pwd)/my-custom-config.yml:/app/config.yml \\\n  unclecode/crawl4ai:latest # Or your specific tag\n```\n\n----------------------------------------\n\nTITLE: Defining AsyncCrawlResponse Model\nDESCRIPTION: Defines a Pydantic model for asynchronous crawl responses that stores HTML content, response headers, and other crawl-related data. It includes support for various response types like screenshots, PDF data, and network requests.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_87\n\nLANGUAGE: python\nCODE:\n```\nclass AsyncCrawlResponse(BaseModel):\n    html: str\n    response_headers: Dict[str, str]\n    js_execution_result: Optional[Dict[str, Any]] = None\n    status_code: int\n    screenshot: Optional[str] = None\n    pdf_data: Optional[bytes] = None\n    mhtml_data: Optional[str] = None\n    get_delayed_content: Optional[Callable[[Optional[float]], Awaitable[str]]] = None\n    downloaded_files: Optional[List[str]] = None\n    ssl_certificate: Optional[SSLCertificate] = None\n    redirected_url: Optional[str] = None\n    network_requests: Optional[List[Dict[str, Any]]] = None\n    console_messages: Optional[List[Dict[str, Any]]] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n```\n\n----------------------------------------\n\nTITLE: Browser Flow Configuration\nDESCRIPTION: Configuration for controlling page navigation, timing, and wait conditions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    wait_for=\"css:.dynamic-content\", # Wait for .dynamic-content\n    delay_before_return_html=2.0,    # Wait 2s before capturing final HTML\n    page_timeout=60000,             # Navigation & script timeout (ms)\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI Basic Package in Python\nDESCRIPTION: Commands for installing the basic Crawl4AI package using pip and setting up Playwright dependencies.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai\nplaywright install # Install Playwright dependencies\n```\n\n----------------------------------------\n\nTITLE: Configuring BrowserConfig for Managed Browser in Crawl4AI (Python)\nDESCRIPTION: Python code snippet demonstrating how to configure BrowserConfig to use a managed browser with a custom user data directory in Crawl4AI.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nbrowser_config = BrowserConfig(\n    headless=True,\n    use_managed_browser=True,\n    user_data_dir=\"/home/<you>/my_chrome_profile\",\n    browser_type=\"chromium\"\n)\n```\n\n----------------------------------------\n\nTITLE: Development and GPU-Enabled Builds for Crawl4AI\nDESCRIPTION: Docker build commands for development environments with all features enabled and GPU acceleration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_49\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t crawl4ai\n  --build-arg INSTALL_TYPE=all \\\n  --build-arg PYTHON_VERSION=3.10 \\\n  --build-arg ENABLE_GPU=true \\\n  .\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t crawl4ai\n  --build-arg ENABLE_GPU=true \\\n  deploy/docker/\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Custom Configuration for Crawl4AI\nDESCRIPTION: This bash command demonstrates how to build a Docker image for Crawl4AI with a custom configuration file.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_70\n\nLANGUAGE: bash\nCODE:\n```\ndocker build --platform=linux/amd64 --no-cache \\\n  --build-arg CONFIG_PATH=/path/to/custom-config.yml \\ \n  -t crawl4ai:latest .\n```\n\n----------------------------------------\n\nTITLE: Building Knowledge Graph with LLMExtractionStrategy in Python\nDESCRIPTION: Demonstrates how to create a knowledge graph by combining LLMExtractionStrategy with Pydantic models. Uses OpenAI's GPT-4 to extract entities and relationships from web content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_184\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nimport asyncio\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Entity(BaseModel):\n    name: str\n    description: str\n\nclass Relationship(BaseModel):\n    entity1: Entity\n    entity2: Entity\n    description: str\n    relation_type: str\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[Entity]\n    relationships: List[Relationship]\n\nasync def main():\n    # LLM extraction strategy\n    llm_strat = LLMExtractionStrategy(\n        provider=\"openai/gpt-4\",\n        api_token=os.getenv('OPENAI_API_KEY'),\n        schema=KnowledgeGraph.schema_json(),\n        extraction_type=\"schema\",\n        instruction=\"Extract entities and relationships from the content. Return valid JSON.\",\n        chunk_token_threshold=1400,\n        apply_chunking=True,\n        input_format=\"html\",\n        extra_args={\"temperature\": 0.1, \"max_tokens\": 1500}\n    )\n\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strat,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:\n        # Example page\n        url = \"https://www.nbcnews.com/business\"\n        result = await crawler.arun(url=url, config=crawl_config)\n\n        if result.success:\n            with open(\"kb_result.json\", \"w\", encoding=\"utf-8\") as f:\n                f.write(result.extracted_content)\n            llm_strat.show_usage()\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM Provider for Crawl4AI Extraction Tasks\nDESCRIPTION: Example of setting up LLM provider configuration for extraction strategies and content filtering. It demonstrates how to specify the LLM provider and API token for use with various extraction capabilities.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nllm_config = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv(\"OPENAI_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Basic Installation of Crawl4AI\nDESCRIPTION: Command to install the core Crawl4AI library with essential dependencies using pip.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_78\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai\n```\n\n----------------------------------------\n\nTITLE: Using KeywordRelevanceScorer for Prioritized Crawling in Crawl4AI (Python)\nDESCRIPTION: Demonstrates using `KeywordRelevanceScorer` to prioritize URLs during a deep crawl with `BestFirstCrawlingStrategy`. A scorer is created with relevant keywords and a weight (0.0 to 1.0). This scorer is assigned to the `url_scorer` parameter of the `BestFirstCrawlingStrategy`. The example configures the crawl to use this strategy, enables streaming (`stream=True`), and iterates through results, printing the relevance score and URL. Higher-scoring URLs are crawled first. Depends on `KeywordRelevanceScorer`, `BestFirstCrawlingStrategy`, `AsyncWebCrawler`, and `CrawlerRunConfig`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\n\n# Create a keyword relevance scorer\nkeyword_scorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n    weight=0.7  # Importance of this scorer (0.0 to 1.0)\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BestFirstCrawlingStrategy(\n        max_depth=2,\n        url_scorer=keyword_scorer\n    ),\n    stream=True  # Recommended with BestFirstCrawling\n)\n\n# Results will come in order of relevance score\nasync with AsyncWebCrawler() as crawler:\n    async for result in await crawler.arun(\"https://example.com\", config=config):\n        score = result.metadata.get(\"score\", 0)\n        print(f\"Score: {score:.2f} | {result.url}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing LLMContentFilter for Intelligent Content Processing\nDESCRIPTION: This snippet demonstrates how to use the LLMContentFilter with a language model to generate high-quality markdown from web content. It shows the setup with custom instructions and configuration parameters for chunk processing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_108\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.content_filter_strategy import LLMContentFilter\n\nasync def main():\n    # Initialize LLM filter with specific instruction\n    filter = LLMContentFilter(\n        llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-api-token\"), #or use environment variable\n        instruction=\"\"\"\n        Focus on extracting the core educational content.\n        Include:\n        - Key concepts and explanations\n        - Important code examples\n        - Essential technical details\n        Exclude:\n        - Navigation elements\n        - Sidebars\n        - Footer content\n        Format the output as clean markdown with proper code blocks and headers.\n        \"\"\",\n        chunk_token_threshold=4096,  # Adjust based on your needs\n        verbose=True\n    )\n\n    config = CrawlerRunConfig(\n        content_filter=filter\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        print(result.markdown.fit_markdown)  # Filtered markdown content\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM Provider with LlmConfig in Python\nDESCRIPTION: Sets up an LLM configuration using OpenAI's GPT-4o-mini model with an API key from environment variables. This is the first step in creating an LLM-based extraction strategy.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nllmConfig = LlmConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv(\"OPENAI_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Defining Data Schema for LLM Extraction with Pydantic\nDESCRIPTION: This code defines a Pydantic model for structured data extraction of OpenAI model fees. It specifies fields for model name, input token fee, and output token fee with appropriate descriptions for the LLM to understand what to extract.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_136\n\nLANGUAGE: python\nCODE:\n```\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(\n        ..., description=\"Fee for output token for the OpenAI model.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Screenshot and PDF Configuration\nDESCRIPTION: Settings for capturing screenshots and generating PDFs of crawled pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    screenshot=True,             # Grab a screenshot as base64\n    screenshot_wait_for=1.0,     # Wait 1s before capturing\n    pdf=True,                    # Also produce a PDF\n    image_description_min_word_threshold=5,  # If analyzing alt text\n    image_score_threshold=3,                # Filter out low-score images\n)\n```\n\n----------------------------------------\n\nTITLE: Attribute Access Handler Implementation\nDESCRIPTION: Implementation of custom attribute access methods to handle deprecated properties and provide proper attribute error handling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef __getattr__(self, name):\n    \"\"\"Handle attribute access.\"\"\"\n    if name in self._UNWANTED_PROPS:\n        raise AttributeError(f\"Getting '{name}' is deprecated. {self._UNWANTED_PROPS[name]}\")\n    raise AttributeError(f\"'{self.__class__.__name__}' has no attribute '{name}'\")\n\ndef __setattr__(self, name, value):\n    \"\"\"Handle attribute setting.\"\"\"\n    sig = inspect.signature(self.__init__)\n    all_params = sig.parameters\n    if name in self._UNWANTED_PROPS and value is not all_params[name].default:\n        raise AttributeError(f\"Setting '{name}' is deprecated. {self._UNWANTED_PROPS[name]}\")\n    super().__setattr__(name, value)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Automatic Schema Generation with LLM\nDESCRIPTION: Generates extraction schemas automatically using LLM instead of manual CSS/XPath writing. Extracts specific data points from HTML content based on natural language queries.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nschema = JsonCssExtractionStrategy.generate_schema(\n    html_content,\n    schema_type=\"CSS\",\n    query=\"Extract product name, price, and description\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Regex Chunking Strategy\nDESCRIPTION: Configuration for regex-based text chunking strategy that splits text based on specified regex patterns.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nRegexChunking(\n    patterns: List[str] = None  # Regex patterns for splitting\n                               # Default: [r'\\n\\n']\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Screenshot Images in Python\nDESCRIPTION: Demonstrates how to decode and save a Base64-encoded screenshot image that was captured during a crawl operation.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nif result.screenshot:\n    with open(\"page.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\n```\n\n----------------------------------------\n\nTITLE: Implementing Abstract URLScorer Base Class in Python\nDESCRIPTION: Defines the abstract base class for all URL scoring strategies with memory-efficient implementation using slots. Provides common functionality for score weighting and statistics tracking, requiring subclasses to implement the core scoring logic.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_200\n\nLANGUAGE: python\nCODE:\n```\nclass URLScorer(ABC):\n    __slots__ = ('_weight', '_stats')\n    \n    def __init__(self, weight: float = 1.0):\n        # Store weight directly as float32 for memory efficiency\n        self._weight = ctypes.c_float(weight).value\n        self._stats = ScoringStats()\n    \n    @abstractmethod\n    def _calculate_score(self, url: str) -> float:\n        \"\"\"Calculate raw score for URL.\"\"\"\n        pass\n    \n    def score(self, url: str) -> float:\n        \"\"\"Calculate weighted score with minimal overhead.\"\"\"\n        score = self._calculate_score(url) * self._weight\n        self._stats.update(score)\n        return score\n    \n    @property\n    def stats(self):\n        \"\"\"Access to scoring statistics.\"\"\"\n        return self._stats\n    \n    @property\n    def weight(self):\n        return self._weight\n```\n\n----------------------------------------\n\nTITLE: Updating run_all.sh Wrapper Script for Benchmarking in Bash\nDESCRIPTION: Modified the run_all.sh script to correctly invoke run_benchmark.py with flexible arguments for stress testing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/JOURNAL.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n# Updated run_all.sh to call run_benchmark.py with correct arguments\n# Exact implementation details not provided in the original text\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI with Transformers Support\nDESCRIPTION: Command to install Crawl4AI with Hugging Face transformers for advanced NLP capabilities.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_83\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai[transformer]\ncrawl4ai-setup\n```\n\n----------------------------------------\n\nTITLE: Implementing Pruning Content Filter in Crawl4AI\nDESCRIPTION: Example of using the PruningContentFilter to extract the most relevant content from a page based on a threshold approach rather than a specific query.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_74\n\nLANGUAGE: python\nCODE:\n```\nprune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",\n    min_word_threshold=10\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\n```\n\n----------------------------------------\n\nTITLE: Manual Start and Close of AsyncWebCrawler in Python\nDESCRIPTION: Example of manually starting and closing an AsyncWebCrawler instance, useful for long-running applications or when full lifecycle control is needed.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncrawler = AsyncWebCrawler(config=browser_cfg)\nawait crawler.start()\n\nresult1 = await crawler.arun(\"https://example.com\")\nresult2 = await crawler.arun(\"https://another.com\")\n\nawait crawler.close()\n```\n\n----------------------------------------\n\nTITLE: Link Handling Configuration\nDESCRIPTION: Configuration options for handling different types of links including external and social media links.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    exclude_external_links=True,         # Remove external links from final content\n    exclude_social_media_links=True,     # Remove links to known social sites\n    exclude_domains=[\"ads.example.com\"], # Exclude links to these domains\n    exclude_social_media_domains=[\"facebook.com\",\"twitter.com\"], # Extend the default list\n)\n```\n\n----------------------------------------\n\nTITLE: Cache Control Configuration\nDESCRIPTION: Example showing how to configure cache control settings using CacheMode enum in CrawlerRunConfig.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring CosineStrategy for Product Review Analysis in Python\nDESCRIPTION: Shows optimal settings for extracting multiple customer reviews. Uses moderate word count threshold, higher top_k value to capture multiple reviews, and a lower similarity threshold to allow variety in review content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_175\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Crawl4AI Usage with CrawlerRunConfig\nDESCRIPTION: Basic example showing how to initialize and use AsyncWebCrawler with CrawlerRunConfig. Demonstrates core parameters like verbose logging and cache mode settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    run_config = CrawlerRunConfig(\n        verbose=True,            # Detailed logging\n        cache_mode=CacheMode.ENABLED,  # Use normal read/write cache\n        check_robots_txt=True,   # Respect robots.txt rules\n        # ... other parameters\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n        \n        # Check if blocked by robots.txt\n        if not result.success and result.status_code == 403:\n            print(f\"Error: {result.error_message}\")\n```\n\n----------------------------------------\n\nTITLE: Crawling Dynamic Content with Session Management in Python\nDESCRIPTION: Demonstrates how to crawl GitHub commits across multiple pages while preserving session state. It uses JsonCssExtractionStrategy for data extraction, JavaScript execution for pagination, and wait conditions to ensure content is loaded properly.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai.cache_context import CacheMode\n\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"github_commits_session\"\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        all_commits = []\n\n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [{\n                \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\"\n            }],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # JavaScript and wait configurations\n        js_next_page = \"\"\"document.querySelector('a[data-testid=\"pagination-next-button\"]').click();\"\"\"\n        wait_for = \"\"\"() => document.querySelectorAll('li.Box-sc-g0xbh4-0').length > 0\"\"\"\n\n        # Crawl multiple pages\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```\n\n----------------------------------------\n\nTITLE: Implementing Network and Console Capture in AsyncPlaywrightCrawlerStrategy\nDESCRIPTION: Placeholder for the implementation details of network request and console message capturing within the AsyncPlaywrightCrawlerStrategy class. This would include event listener setup for the Playwright page object and data collection logic.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/prompts/prompt_net_requests.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# ==== File: crawl4ai/async_crawler_strategy.py ====\n```\n\n----------------------------------------\n\nTITLE: CrawlerRunConfig Helper Methods Example in Python\nDESCRIPTION: Example demonstrating how to use CrawlerRunConfig's clone() method to create variations of crawler configurations. Shows creating a base configuration with caching, word count threshold, and wait-until settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create a base configuration\nbase_config = CrawlerRunConfig(\n    cache_mode=CacheMode.ENABLED,\n    word_count_threshold=200,\n    wait_until=\"networkidle\"\n)\n```\n\n----------------------------------------\n\nTITLE: Link Discovery and Validation\nDESCRIPTION: Extracts and validates links from crawl results, respecting depth limits and max pages configuration. Handles both internal and optional external links.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_170\n\nLANGUAGE: python\nCODE:\n```\nasync def link_discovery(\n        self,\n        result: CrawlResult,\n        source_url: str,\n        current_depth: int,\n        visited: Set[str],\n        next_links: List[Tuple[str, Optional[str]]],\n        depths: Dict[str, int],\n    ) -> None:\n        new_depth = current_depth + 1\n        if new_depth > self.max_depth:\n            return\n            \n        remaining_capacity = self.max_pages - self._pages_crawled\n        if remaining_capacity <= 0:\n            self.logger.info(f\"Max pages limit ({self.max_pages}) reached, stopping link discovery\")\n            return\n\n        links = result.links.get(\"internal\", [])\n        if self.include_external:\n            links += result.links.get(\"external\", [])\n\n        valid_links = []\n        for link in links:\n            url = link.get(\"href\")\n            if url in visited:\n                continue\n            if not await self.can_process_url(url, new_depth):\n                self.stats.urls_skipped += 1\n                continue\n                \n            valid_links.append(url)\n            \n        if len(valid_links) > remaining_capacity:\n            valid_links = valid_links[:remaining_capacity]\n            self.logger.info(f\"Limiting to {remaining_capacity} URLs due to max_pages limit\")\n            \n        for url in valid_links:\n            depths[url] = new_depth\n            next_links.append((url, source_url))\n```\n\n----------------------------------------\n\nTITLE: Implementing Wrap-Up and Key Takeaways in Python with Crawl4AI\nDESCRIPTION: This function demonstrates a complete crawler example combining filters, scorers, and streaming for an optimized crawl. It creates a sophisticated filter chain, a composite scorer, and executes the crawl using the BestFirstCrawlingStrategy.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_211\n\nLANGUAGE: python\nCODE:\n```\nasync def wrap_up():\n    \"\"\"\n    PART 6: Wrap-Up and Key Takeaways\n\n    Summarize the key concepts learned in this tutorial.\n    \"\"\"\n    print(\"\\n===== COMPLETE CRAWLER EXAMPLE =====\")\n    print(\"Combining filters, scorers, and streaming for an optimized crawl\")\n\n    # Create a sophisticated filter chain\n    filter_chain = FilterChain(\n        [\n            DomainFilter(\n                allowed_domains=[\"docs.crawl4ai.com\"],\n                blocked_domains=[\"old.docs.crawl4ai.com\"],\n            ),\n            URLPatternFilter(patterns=[\"*core*\", \"*advanced*\", \"*blog*\"]),\n            ContentTypeFilter(allowed_types=[\"text/html\"]),\n        ]\n    )\n\n    # Create a composite scorer that combines multiple scoring strategies\n    keyword_scorer = KeywordRelevanceScorer(\n        keywords=[\"crawl\", \"example\", \"async\", \"configuration\"], weight=0.7\n    )\n    # Set up the configuration\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BestFirstCrawlingStrategy(\n            max_depth=1,\n            include_external=False,\n            filter_chain=filter_chain,\n            url_scorer=keyword_scorer,\n        ),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        stream=True,\n        verbose=True,\n    )\n\n    # Execute the crawl\n    results = []\n    start_time = time.perf_counter()\n\n    async with AsyncWebCrawler() as crawler:\n        async for result in await crawler.arun(\n            url=\"https://docs.crawl4ai.com\", config=config\n        ):\n            results.append(result)\n            score = result.metadata.get(\"score\", 0)\n            depth = result.metadata.get(\"depth\", 0)\n            print(f\"â†’ Depth: {depth} | Score: {score:.2f} | {result.url}\")\n\n    duration = time.perf_counter() - start_time\n\n    # Summarize the results\n    print(f\"\\nâœ… Crawled {len(results)} high-value pages in {duration:.2f} seconds\")\n    print(\n        f\"âœ… Average score: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}\"\n    )\n\n    # Group by depth\n    depth_counts = {}\n    for result in results:\n        depth = result.metadata.get(\"depth\", 0)\n        depth_counts[depth] = depth_counts.get(depth, 0) + 1\n\n    print(\"\\nðŸ“Š Pages crawled by depth:\")\n    for depth, count in sorted(depth_counts.items()):\n        print(f\"  Depth {depth}: {count} pages\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Sentence-Based Text Chunking using NLTK\nDESCRIPTION: Uses NLTK's sentence tokenizer to split text into individual sentences. Provides clean, meaningful statement-level segmentation.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom nltk.tokenize import sent_tokenize\n\nclass NlpSentenceChunking:\n    def chunk(self, text):\n        sentences = sent_tokenize(text)\n        return [sentence.strip() for sentence in sentences]\n\n# Example Usage\ntext = \"This is sentence one. This is sentence two.\"\nchunker = NlpSentenceChunking()\nprint(chunker.chunk(text))\n```\n\n----------------------------------------\n\nTITLE: Basic Hello World Examples with Crawl4AI\nDESCRIPTION: This example demonstrates basic usage of the Crawl4AI library, including connecting to a Chrome DevTools Protocol endpoint and performing a simple crawl with content filtering. It shows how to execute JavaScript on a page and how to configure the AsyncWebCrawler with different options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_161\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    BrowserConfig,\n    CrawlerRunConfig,\n    CacheMode,\n    DefaultMarkdownGenerator,\n    PruningContentFilter,\n    CrawlResult\n)\n\nasync def example_cdp():\n    browser_conf = BrowserConfig(\n        headless=False,\n        cdp_url=\"http://localhost:9223\"\n    )\n    crawler_config = CrawlerRunConfig(\n        session_id=\"test\",\n        js_code = \"\"\"(() => { return {\"result\": \"Hello World!\"} })()\"\"\",\n        js_only=True\n    )\n    async with AsyncWebCrawler(\n        config=browser_conf,\n        verbose=True,\n    ) as crawler:\n        result : CrawlResult = await crawler.arun(\n            url=\"https://www.helloworld.org\",\n            config=crawler_config,\n        )\n        print(result.js_execution_result)\n                   \n\nasync def main():\n    browser_config = BrowserConfig(headless=True, verbose=True)\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        crawler_config = CrawlerRunConfig(\n            cache_mode=CacheMode.BYPASS,\n            markdown_generator=DefaultMarkdownGenerator(\n                content_filter=PruningContentFilter(\n                     threshold=0.48, threshold_type=\"fixed\", min_word_threshold=0\n                )\n            ),\n        )\n        result : CrawlResult = await crawler.arun(\n            url=\"https://www.helloworld.org\", config=crawler_config\n        )\n        print(result.markdown.raw_markdown[:500])\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Initializing Rich Console in Python\nDESCRIPTION: Creates a new Rich console instance for formatted terminal output.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n# Initialize rich console\nconsole = Console()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running AsyncWebCrawler with Content Processing\nDESCRIPTION: This example demonstrates how to configure and use AsyncWebCrawler with various settings including content filtering, iframe processing, overlay removal, and cache control. It also shows how to process the crawled content like text, images, and links.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_129\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    browser_config = BrowserConfig(verbose=True)\n    run_config = CrawlerRunConfig(\n        # Content filtering\n        word_count_threshold=10,\n        excluded_tags=['form', 'header'],\n        exclude_external_links=True,\n        \n        # Content processing\n        process_iframes=True,\n        remove_overlay_elements=True,\n        \n        # Cache control\n        cache_mode=CacheMode.ENABLED  # Use cache if available\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Setting up JsonCss Extraction Strategy\nDESCRIPTION: Configuration for CSS selector-based structured data extraction with schema definition for targeting specific HTML elements.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nJsonCssExtractionStrategy(\n    schema: Dict[str, Any],    # Extraction schema\n    verbose: bool = False      # Enable verbose logging\n)\n\n# Schema Structure\nschema = {\n    \"name\": str,              # Schema name\n    \"baseSelector\": str,      # Base CSS selector\n    \"fields\": [               # List of fields to extract\n        {\n            \"name\": str,      # Field name\n            \"selector\": str,  # CSS selector\n            \"type\": str,     # Field type: \"text\", \"attribute\", \"html\", \"regex\"\n            \"attribute\": str, # For type=\"attribute\"\n            \"pattern\": str,  # For type=\"regex\"\n            \"transform\": str, # Optional: \"lowercase\", \"uppercase\", \"strip\"\n            \"default\": Any    # Default value if extraction fails\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Manually Built Crawl4AI Container with LLM Support (Bash)\nDESCRIPTION: Illustrates running a container from a locally built image (`crawl4ai-local:latest`) while providing LLM API keys via an environment file (`--env-file .llm.env`). Assumes the `.llm.env` file is in the project root.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n# Make sure .llm.env is in the current directory (project root)\ndocker run -d \\\n  -p 11235:11235 \\\n  --name crawl4ai-standalone \\\n  --env-file .llm.env \\\n  --shm-size=1g \\\n  crawl4ai-local:latest\n```\n\n----------------------------------------\n\nTITLE: Initializing SEO Filter for Quality Assessment in Crawl4AI (Python)\nDESCRIPTION: Shows the basic import and instantiation setup for using an `SEOFilter` within a `FilterChain`. The `SEOFilter` is intended to help identify and potentially prioritize pages based on SEO characteristics (like meta tags, headers) during a crawl, although its specific usage within the chain is not shown in this snippet. Depends on `FilterChain` and `SEOFilter` from `crawl4ai.deep_crawling.filters`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling.filters import FilterChain, SEOFilter\n\n```\n\n----------------------------------------\n\nTITLE: Controlling BFS/DFS Crawl by Minimum Page Score Threshold (Python)\nDESCRIPTION: Sets a score_threshold parameter in DFSDeepCrawlStrategy to only follow links above a certain scored relevance, focusing the crawl on high-quality pages. Demonstrates combining a KeywordRelevanceScorer with DFS crawling to dynamically skip less relevant paths. Dependencies include Crawl4AI and proper scorer/filter setup.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Only follow links with scores above 0.4\\nstrategy = DFSDeepCrawlStrategy(\\n    max_depth=2,\\n    url_scorer=KeywordRelevanceScorer(keywords=[\\\"api\\\", \\\"guide\\\", \\\"reference\\\"]),\\n    score_threshold=0.4  # Skip URLs with scores below this value\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Capturing PDFs and Screenshots with AsyncWebCrawler\nDESCRIPTION: This example shows how to capture both PDF exports and screenshots of web pages using Crawl4AI. It demonstrates how to save the base64-encoded screenshot and PDF data to files after a successful crawl, with options to bypass the cache.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_131\n\nLANGUAGE: python\nCODE:\n```\nimport os, asyncio\nfrom base64 import b64decode\nfrom crawl4ai import AsyncWebCrawler, CacheMode\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://en.wikipedia.org/wiki/List_of_common_misconceptions\",\n            cache_mode=CacheMode.BYPASS,\n            pdf=True,\n            screenshot=True\n        )\n        \n        if result.success:\n            # Save screenshot\n            if result.screenshot:\n                with open(\"wikipedia_screenshot.png\", \"wb\") as f:\n                    f.write(b64decode(result.screenshot))\n            \n            # Save PDF\n            if result.pdf:\n                with open(\"wikipedia_page.pdf\", \"wb\") as f:\n                    f.write(result.pdf)\n            \n            print(\"[OK] PDF & screenshot captured.\")\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Attribute Setting Handler Method\nDESCRIPTION: Special method to handle attribute setting with validation for deprecated properties using reflection via inspect module.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_69\n\nLANGUAGE: python\nCODE:\n```\ndef __setattr__(self, name, value):\n        \"\"\"Handle attribute setting.\"\"\"\n        # TODO: Planning to set properties dynamically based on the __init__ signature\n        sig = inspect.signature(self.__init__)\n        all_params = sig.parameters  # Dictionary of parameter names and their details\n\n        if name in self._UNWANTED_PROPS and value is not all_params[name].default:\n            raise AttributeError(f\"Setting '{name}' is deprecated. {self._UNWANTED_PROPS[name]}\")\n        \n        super().__setattr__(name, value)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Deep Crawler Strategy in Crawl4AI\nDESCRIPTION: Illustrates the configuration of a deep crawler strategy in Crawl4AI, including depth settings, filter chain, and URL scoring mechanisms.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_61\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"crawler_config\": {\n    \"type\": \"CrawlerRunConfig\",\n    \"params\": {\n      \"deep_crawl_strategy\": {\n        \"type\": \"BFSDeepCrawlStrategy\",\n        \"params\": {\n          \"max_depth\": 3,\n          \"filter_chain\": {\n            \"type\": \"FilterChain\",\n            \"params\": {\n              \"filters\": [\n                {\n                  \"type\": \"ContentTypeFilter\",\n                  \"params\": {\n                    \"allowed_types\": [\"text/html\", \"application/xhtml+xml\"]\n                  }\n                },\n                {\n                  \"type\": \"DomainFilter\",\n                  \"params\": {\n                    \"allowed_domains\": [\"blog.*\", \"docs.*\"],\n                  }\n                }\n              ]\n            }\n          },\n          \"url_scorer\": {\n            \"type\": \"CompositeScorer\",\n            \"params\": {\n              \"scorers\": [\n                {\n                  \"type\": \"KeywordRelevanceScorer\",\n                  \"params\": {\n                    \"keywords\": [\"tutorial\", \"guide\", \"documentation\"],\n                  }\n                },\n                {\n                  \"type\": \"PathDepthScorer\",\n                  \"params\": {\n                    \"weight\": 0.5,\n                    \"optimal_depth\": 3  \n                  }\n                }\n              ]\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing or Upgrading to Crawl4AI v0.4.1\nDESCRIPTION: This command shows how to install or upgrade to version 0.4.1 of Crawl4AI using pip, the Python package manager.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai --upgrade\n```\n\n----------------------------------------\n\nTITLE: Performing a Basic Web Crawl with Crawl4AI in Python\nDESCRIPTION: This snippet demonstrates the minimal setup required to perform a web crawl using Crawl4AI. It imports the necessary `AsyncWebCrawler` class and `asyncio` library. An asynchronous `main` function initializes the crawler within an `async with` block, executes a crawl on 'https://example.com' using `crawler.arun()`, and prints the first 300 characters of the resulting Markdown generated automatically from the page's HTML.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\")\n        print(result.markdown[:300])  # Print first 300 chars\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Adding Crawl4AI as MCP Provider in Claude Code (Bash)\nDESCRIPTION: Shows the `claude mcp add` command to register the running Crawl4AI server as a Model Context Protocol (MCP) provider within Claude Code, using the SSE transport. Also includes the command `claude mcp list` to verify the provider was added.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n# Add the Crawl4AI server as an MCP provider\nclaude mcp add --transport sse c4ai-sse http://localhost:11235/mcp/sse\n\n# List all MCP providers to verify it was added\nclaude mcp list\n```\n\n----------------------------------------\n\nTITLE: Typical Initialization of AsyncWebCrawler in Python\nDESCRIPTION: Example of initializing an AsyncWebCrawler instance with a BrowserConfig object specifying browser type, headless mode, and verbosity.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_cfg = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    verbose=True\n)\n\ncrawler = AsyncWebCrawler(config=browser_cfg)\n```\n\n----------------------------------------\n\nTITLE: LLM Extraction with Filtering Complete Example for Crawl4AI\nDESCRIPTION: Comprehensive example showing LLM-based extraction with content filtering.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncrwl https://example.com \\\n    -B browser.yml \\\n    -e extract_llm.yml \\\n    -s llm_schema.json \\\n    -f filter_bm25.yml \\\n    -o json\n```\n\n----------------------------------------\n\nTITLE: JavaScript Interaction for Dynamic Content Loading in Python\nDESCRIPTION: This function demonstrates how to interact with JavaScript on a web page to load more content. It crawls Hacker News, extracts initial news items, clicks the 'More' link, and extracts additional items.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_153\n\nLANGUAGE: python\nCODE:\n```\nasync def demo_js_interaction():\n    \"\"\"Execute JavaScript to load more content\"\"\"\n    print(\"\\n=== 7. JavaScript Interaction ===\")\n\n    # A simple page that needs JS to reveal content\n    async with AsyncWebCrawler(config=BrowserConfig(headless=False)) as crawler:\n        # Initial load\n\n        news_schema = {\n            \"name\": \"news\",\n            \"baseSelector\": \"tr.athing\",\n            \"fields\": [\n                {\n                    \"name\": \"title\",\n                    \"selector\": \"span.titleline\",\n                    \"type\": \"text\",\n                }\n            ],\n        }\n        results: List[CrawlResult] = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=CrawlerRunConfig(\n                session_id=\"hn_session\",  # Keep session\n                extraction_strategy=JsonCssExtractionStrategy(schema=news_schema),\n            ),\n        )\n\n        news = []\n        for result in results:\n            if result.success:\n                data = json.loads(result.extracted_content)\n                news.extend(data)\n                print(json.dumps(data, indent=2))\n            else:\n                print(\"Failed to extract structured data\")\n\n        print(f\"Initial items: {len(news)}\")\n\n        # Click \"More\" link\n        more_config = CrawlerRunConfig(\n            js_code=\"document.querySelector('a.morelink').click();\",\n            js_only=True,  # Continue in same page\n            session_id=\"hn_session\",  # Keep session\n            extraction_strategy=JsonCssExtractionStrategy(\n                schema=news_schema,\n            ),\n        )\n\n        result: List[CrawlResult] = await crawler.arun(\n            url=\"https://news.ycombinator.com\", config=more_config\n        )\n\n        # Extract new items\n        for result in results:\n            if result.success:\n                data = json.loads(result.extracted_content)\n                news.extend(data)\n                print(json.dumps(data, indent=2))\n            else:\n                print(\"Failed to extract structured data\")\n        print(f\"Total items: {len(news)}\")\n```\n\n----------------------------------------\n\nTITLE: Authenticated Proxy Configuration in Python\nDESCRIPTION: Shows how to configure an authenticated proxy with username and password using BrowserConfig.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_159\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import BrowserConfig\n\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nbrowser_config = BrowserConfig(proxy_config=proxy_config)\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy Settings with AsyncWebCrawler\nDESCRIPTION: This code demonstrates how to configure and use proxy settings with AsyncWebCrawler. It includes setting up a proxy server with authentication credentials and testing the connection by visiting a website that displays IP information.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_130\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    browser_cfg = BrowserConfig(\n        proxy_config={\n            \"server\": \"http://proxy.example.com:8080\",\n            \"username\": \"myuser\",\n            \"password\": \"mypass\",\n        },\n        headless=True\n    )\n    crawler_cfg = CrawlerRunConfig(\n        verbose=True\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.whatismyip.com/\",\n            config=crawler_cfg\n        )\n        if result.success:\n            print(\"[OK] Page fetched via proxy.\")\n            print(\"Page HTML snippet:\", result.html[:200])\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Configuring DFS Deep Crawl Strategy in Crawl4AI\nDESCRIPTION: This snippet demonstrates the configuration of a Depth-First Search (DFS) deep crawling strategy. It explores branches fully before backtracking and allows customization of depth, domain boundaries, and page limits similar to BFS strategy.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling import DFSDeepCrawlStrategy\n\n# Basic configuration\nstrategy = DFSDeepCrawlStrategy(\n    max_depth=2,               # Crawl initial page + 2 levels deep\n    include_external=False,    # Stay within the same domain\n    max_pages=30,              # Maximum number of pages to crawl (optional)\n    score_threshold=0.5,       # Minimum score for URLs to be crawled (optional)\n)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Chunking Configuration Example\nDESCRIPTION: Example showing optimal chunking configuration for long documents using LLMExtractionStrategy.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# For long documents\nstrategy = LLMExtractionStrategy(\n    chunk_token_threshold=2000,  # Smaller chunks\n    overlap_rate=0.1           # 10% overlap\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Content Filtering and Exclusions in Crawl4AI\nDESCRIPTION: This snippet demonstrates the basic configuration options for filtering content, including word count thresholds, tag exclusions, link filtering, domain exclusions, and media filtering. These parameters help remove unwanted content from crawl results.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n\n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n\n    # Link filtering\n    exclude_external_links=True,    \n    exclude_social_media_links=True,\n    # Block entire domains\n    exclude_domains=[\"adtrackers.com\", \"spammynews.org\"],    \n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],\n\n    # Media filtering\n    exclude_external_images=True\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing AsyncWebCrawler Hooks with Authentication\nDESCRIPTION: Complete example demonstrating the implementation of all available hooks in AsyncWebCrawler, including browser configuration, crawler setup, and hook definitions for authentication and customization. Shows proper hook usage for route filtering, viewport adjustment, and page navigation monitoring.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_135\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom playwright.async_api import Page, BrowserContext\n\nasync def main():\n    print(\"ðŸ”— Hooks Example: Demonstrating recommended usage\")\n\n    # 1) Configure the browser\n    browser_config = BrowserConfig(\n        headless=True,\n        verbose=True\n    )\n\n    # 2) Configure the crawler run\n    crawler_run_config = CrawlerRunConfig(\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n        wait_for=\"body\",\n        cache_mode=CacheMode.BYPASS\n    )\n\n    # 3) Create the crawler instance\n    crawler = AsyncWebCrawler(config=browser_config)\n\n    #\n    # Define Hook Functions\n    #\n\n    async def on_browser_created(browser, **kwargs):\n        # Called once the browser instance is created (but no pages or contexts yet)\n        print(\"[HOOK] on_browser_created - Browser created successfully!\")\n        # Typically, do minimal setup here if needed\n        return browser\n\n    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):\n        # Called right after a new page + context are created (ideal for auth or route config).\n        print(\"[HOOK] on_page_context_created - Setting up page & context.\")\n        \n        # Example 1: Route filtering (e.g., block images)\n        async def route_filter(route):\n            if route.request.resource_type == \"image\":\n                print(f\"[HOOK] Blocking image request: {route.request.url}\")\n                await route.abort()\n            else:\n                await route.continue_()\n\n        await context.route(\"**\", route_filter)\n\n        # Example 2: (Optional) Simulate a login scenario\n        # Example 3: Adjust the viewport\n        await page.set_viewport_size({\"width\": 1080, \"height\": 600})\n        return page\n\n    async def before_goto(\n        page: Page, context: BrowserContext, url: str, **kwargs\n    ):\n        print(f\"[HOOK] before_goto - About to navigate: {url}\")\n        await page.set_extra_http_headers({\n            \"Custom-Header\": \"my-value\"\n        })\n        return page\n\n    async def after_goto(\n        page: Page, context: BrowserContext, \n        url: str, response, **kwargs\n    ):\n        print(f\"[HOOK] after_goto - Successfully loaded: {url}\")\n        try:\n            await page.wait_for_selector('.content', timeout=1000)\n            print(\"[HOOK] Found .content element!\")\n        except:\n            print(\"[HOOK] .content not found, continuing anyway.\")\n        return page\n\n    async def on_user_agent_updated(\n        page: Page, context: BrowserContext, \n        user_agent: str, **kwargs\n    ):\n        print(f\"[HOOK] on_user_agent_updated - New user agent: {user_agent}\")\n        return page\n\n    async def on_execution_started(page: Page, context: BrowserContext, **kwargs):\n        print(\"[HOOK] on_execution_started - JS code is running!\")\n        return page\n\n    async def before_retrieve_html(page: Page, context: BrowserContext, **kwargs):\n        print(\"[HOOK] before_retrieve_html - We can do final actions\")\n        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n        return page\n\n    async def before_return_html(\n        page: Page, context: BrowserContext, html: str, **kwargs\n    ):\n        print(f\"[HOOK] before_return_html - HTML length: {len(html)}\")\n        return page\n\n    crawler.crawler_strategy.set_hook(\"on_browser_created\", on_browser_created)\n    crawler.crawler_strategy.set_hook(\n        \"on_page_context_created\", on_page_context_created\n    )\n    crawler.crawler_strategy.set_hook(\"before_goto\", before_goto)\n    crawler.crawler_strategy.set_hook(\"after_goto\", after_goto)\n    crawler.crawler_strategy.set_hook(\n        \"on_user_agent_updated\", on_user_agent_updated\n    )\n    crawler.crawler_strategy.set_hook(\n        \"on_execution_started\", on_execution_started\n    )\n    crawler.crawler_strategy.set_hook(\n        \"before_retrieve_html\", before_retrieve_html\n    )\n    crawler.crawler_strategy.set_hook(\n        \"before_return_html\", before_return_html\n    )\n\n    await crawler.start()\n\n    # 4) Run the crawler on an example page\n    url = \"https://example.com\"\n    result = await crawler.arun(url, config=crawler_run_config)\n    \n    if result.success:\n        print(\"\\nCrawled URL:\", result.url)\n        print(\"HTML length:\", len(result.html))\n    else:\n        print(\"Error:\", result.error_message)\n\n    await crawler.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI Package\nDESCRIPTION: Command to install or upgrade the Crawl4AI package using pip package manager.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install -U crawl4ai\n```\n\n----------------------------------------\n\nTITLE: Implementing Load More Functionality with Hacker News Example in Python\nDESCRIPTION: This example demonstrates how to load the initial Hacker News page and then click the 'More' link to load additional content. It uses session management to maintain state between requests and custom JavaScript execution to trigger the load more action.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_116\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Step 1: Load initial Hacker News page\n    config = CrawlerRunConfig(\n        wait_for=\"css:.athing:nth-child(30)\"  # Wait for 30 items\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n        print(\"Initial items loaded.\")\n\n        # Step 2: Let's scroll and click the \"More\" link\n        load_more_js = [\n            \"window.scrollTo(0, document.body.scrollHeight);\",\n            # The \"More\" link at page bottom\n            \"document.querySelector('a.morelink')?.click();\"  \n        ]\n        \n        next_page_conf = CrawlerRunConfig(\n            js_code=load_more_js,\n            wait_for=\"\"\"js:() => {\n                return document.querySelectorAll('.athing').length > 30;\n            }\"\"\",\n            # Mark that we do not re-navigate, but run JS in the same session:\n            js_only=True,\n            session_id=\"hn_session\"\n        )\n\n        # Re-use the same crawler session\n        result2 = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # same URL but continuing session\n            config=next_page_conf\n        )\n        total_items = result2.cleaned_html.count(\"athing\")\n        print(\"Items after load-more:\", total_items)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Accessing Filtered HTML Content in Python\nDESCRIPTION: Example of how to check for and access the 'fit' HTML content that has been processed by a content filter or heuristic like Pruning or BM25.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nif result.markdown.fit_html:\n    print(\"High-value HTML content:\", result.markdown.fit_html[:300])\n```\n\n----------------------------------------\n\nTITLE: Processing HTML Content with Scraping Strategy in Python Web Crawler\nDESCRIPTION: This method processes HTML content using a configured scraping strategy. It handles different input formats, creates appropriate parameters, and executes the scraping strategy to extract content from the HTML. It also includes error handling for invalid CSS selectors and other extraction failures.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nasync def aprocess_html(\n    self,\n    url: str,\n    html: str,\n    extracted_content: str,\n    config: CrawlerRunConfig,\n    screenshot: str,\n    pdf_data: str,\n    verbose: bool,\n    **kwargs,\n) -> CrawlResult:\n    \"\"\"\n    Process HTML content using the provided configuration.\n\n    Args:\n        url: The URL being processed\n        html: Raw HTML content\n        extracted_content: Previously extracted content (if any)\n        config: Configuration object controlling processing behavior\n        screenshot: Screenshot data (if any)\n        pdf_data: PDF data (if any)\n        verbose: Whether to enable verbose logging\n        **kwargs: Additional parameters for backwards compatibility\n\n    Returns:\n        CrawlResult: Processed result containing extracted and formatted content\n    \"\"\"\n    cleaned_html = \"\"\n    try:\n        _url = url if not kwargs.get(\"is_raw_html\", False) else \"Raw HTML\"\n        t1 = time.perf_counter()\n\n        # Get scraping strategy and ensure it has a logger\n        scraping_strategy = config.scraping_strategy\n        if not scraping_strategy.logger:\n            scraping_strategy.logger = self.logger\n\n        # Process HTML content\n        params = config.__dict__.copy()\n        params.pop(\"url\", None)\n        # add keys from kwargs to params that doesn't exist in params\n        params.update({k: v for k, v in kwargs.items()\n                      if k not in params.keys()})\n\n        ################################\n        # Scraping Strategy Execution  #\n        ################################\n        result: ScrapingResult = scraping_strategy.scrap(\n            url, html, **params)\n\n        if result is None:\n            raise ValueError(\n                f\"Process HTML, Failed to extract content from the website: {url}\"\n            )\n\n    except InvalidCSSSelectorError as e:\n        raise ValueError(str(e))\n    except Exception as e:\n        raise ValueError(\n            f\"Process HTML, Failed to extract content from the website: {url}, error: {str(e)}\"\n        )\n\n    # Extract results - handle both dict and ScrapingResult\n    if isinstance(result, dict):\n        cleaned_html = sanitize_input_encode(\n            result.get(\"cleaned_html\", \"\"))\n        media = result.get(\"media\", {})\n        links = result.get(\"links\", {})\n        metadata = result.get(\"metadata\", {})\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI with PyTorch Support in Python\nDESCRIPTION: Command for installing Crawl4AI with PyTorch support, enabling advanced text clustering features.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai[torch]\n```\n\n----------------------------------------\n\nTITLE: JavaScript-Based Wait Conditions in Crawl4AI\nDESCRIPTION: Demonstrates how to use JavaScript-based wait conditions for more complex scenarios. This code waits until the page contains more than 50 items using a custom JavaScript function.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nwait_condition = \"\"\"() => {\n    const items = document.querySelectorAll('.athing');\n    return items.length > 50;  // Wait for at least 51 items\n}\"\"\"\n\nconfig = CrawlerRunConfig(wait_for=f\"js:{wait_condition}\")\n```\n\n----------------------------------------\n\nTITLE: Excluding External Images in Crawl4AI\nDESCRIPTION: Configuration to exclude third-party images from crawl results, useful for skipping advertisements or reducing page weight by focusing only on images from the primary domain.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_95\n\nLANGUAGE: python\nCODE:\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_external_images=True\n)\n```\n\n----------------------------------------\n\nTITLE: Loading SSL Certificate from File in Python\nDESCRIPTION: Shows how to load an SSL certificate from a local file containing certificate data in ASN.1 or DER format. This method is useful when working with stored certificate files.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncert = SSLCertificate.from_file(\"/path/to/cert.der\")\n```\n\n----------------------------------------\n\nTITLE: Configuring LLMContentFilter for Focused Content Extraction in Python\nDESCRIPTION: This snippet demonstrates how to set up LLMContentFilter for focused content extraction, specifically targeting technical documentation, code examples, and API references.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfilter = LLMContentFilter(\n    instruction=\"\"\"\n    Focus on extracting specific types of content:\n    - Technical documentation\n    - Code examples\n    - API references\n    Reformat the content into clear, well-structured markdown\n    \"\"\",\n    chunk_token_threshold=4096\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawler Monitor in Python\nDESCRIPTION: Initializes a CrawlerMonitor for real-time visibility of crawling operations, supporting detailed or aggregated view modes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import CrawlerMonitor, DisplayMode\nmonitor = CrawlerMonitor(\n    # Maximum rows in live display\n    max_visible_rows=15,          \n\n    # DETAILED or AGGREGATED view\n    display_mode=DisplayMode.DETAILED  \n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Asyncio Support\nDESCRIPTION: Initialization of nest_asyncio to enable asynchronous operations in Colab environment.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Import nest_asyncio and apply it to allow asyncio in Colab\nimport nest_asyncio\nnest_asyncio.apply()\n\nprint('Setup complete!')\n```\n\n----------------------------------------\n\nTITLE: Initializing LLMContentFilter for Web Crawling in Python\nDESCRIPTION: This snippet demonstrates how to initialize and use LLMContentFilter with AsyncWebCrawler for intelligent content filtering. It includes setting up the filter with custom instructions and configuring the crawler.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.content_filter_strategy import LLMContentFilter\n\nasync def main():\n    # Initialize LLM filter with specific instruction\n    filter = LLMContentFilter(\n        llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-api-token\"), #or use environment variable\n        instruction=\"\"\"\n        Focus on extracting the core educational content.\n        Include:\n        - Key concepts and explanations\n        - Important code examples\n        - Essential technical details\n        Exclude:\n        - Navigation elements\n        - Sidebars\n        - Footer content\n        Format the output as clean markdown with proper code blocks and headers.\n        \"\"\",\n        chunk_token_threshold=4096,  # Adjust based on your needs\n        verbose=True\n    )\n\n    config = CrawlerRunConfig(\n        content_filter=filter\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        print(result.markdown.fit_markdown)  # Filtered markdown content\n```\n\n----------------------------------------\n\nTITLE: Crawler Configuration Command Line Examples for Crawl4AI\nDESCRIPTION: Shows how to apply crawler configuration settings using either a YAML file or direct command line parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Using config file\ncrwl https://example.com -C crawler.yml\n\n# Using direct parameters\ncrwl https://example.com -c \"css_selector=#main,delay_before_return_html=2,scan_full_page=true\"\n```\n\n----------------------------------------\n\nTITLE: Loading Crawler4ai Configuration from JSON File\nDESCRIPTION: Code example showing how to load a configuration from a JSON file and initialize the Crawler4ai with those settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom crawler4ai import Crawler4ai\nimport json\n\n# Load configuration from file\nwith open(\"config.json\", \"r\") as f:\n    config = json.load(f)\n\n# Initialize crawler with config\ncrawler = Crawler4ai(**config)\n\n# Start crawling\nresults = crawler.crawl()\n```\n\n----------------------------------------\n\nTITLE: Browser Configuration Command Line Examples for Crawl4AI\nDESCRIPTION: Demonstrates how to apply browser configuration settings using either a YAML file or direct command line parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Using config file\ncrwl https://example.com -B browser.yml\n\n# Using direct parameters\ncrwl https://example.com -b \"headless=true,viewport_width=1280,user_agent_mode=random\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Hooks in Crawl4AI\nDESCRIPTION: This function demonstrates how to use custom hooks with the crawler to execute specific functions at different stages of the crawling process. It sets a 'before_goto' hook that runs custom code just before navigating to a URL.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_133\n\nLANGUAGE: python\nCODE:\n```\nasync def custom_hook_workflow(verbose=True):\n    async with AsyncWebCrawler() as crawler:\n        # Set a 'before_goto' hook to run custom code just before navigation\n        crawler.crawler_strategy.set_hook(\n            \"before_goto\",\n            lambda page, context: print(\"[Hook] Preparing to navigate...\"),\n        )\n\n        # Perform the crawl operation\n        result = await crawler.arun(url=\"https://crawl4ai.com\")\n        print(result.markdown.raw_markdown[:500].replace(\"\\n\", \" -- \"))\n```\n\n----------------------------------------\n\nTITLE: Browser Configuration JSON Structure Example\nDESCRIPTION: Example of the JSON structure representing a BrowserConfig object, demonstrating the type-params pattern used by the Crawl4AI API.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_52\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"BrowserConfig\",\n    \"params\": {\n        \"headless\": true\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Schema Files in Python\nDESCRIPTION: Loads schema files for data extraction. Returns None if no path is provided, otherwise delegates to the config file loader.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef load_schema_file(path: Optional[str]) -> dict:\n    if not path:\n        return None\n    return load_config_file(path)\n```\n\n----------------------------------------\n\nTITLE: Combining Managed Browsers with Location Settings in Python\nDESCRIPTION: This code demonstrates how to combine managed browsers with location settings for a complete identity solution. It includes setting up a browser config with a managed browser and a crawl config with locale, timezone, and geolocation settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import (\n    AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, \n    GeolocationConfig\n)\n\nbrowser_config = BrowserConfig(\n    use_managed_browser=True,\n    user_data_dir=\"/path/to/my-profile\",\n    browser_type=\"chromium\"\n)\n\ncrawl_config = CrawlerRunConfig(\n    # Location settings\n    locale=\"es-MX\",                  # Spanish (Mexico)\n    timezone_id=\"America/Mexico_City\",\n    geolocation=GeolocationConfig(\n        latitude=19.4326,            # Mexico City\n        longitude=-99.1332\n    )\n)\n\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\", config=crawl_config)\n```\n\n----------------------------------------\n\nTITLE: Directory Size Calculator Function\nDESCRIPTION: Python function to calculate the total size of a directory in bytes by recursively traversing its contents\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndef get_directory_size(path: str) -> int:\n    \"\"\"Calculate the total size of a directory in bytes\"\"\"\n    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            if not os.path.islink(fp):\n                total_size += os.path.getsize(fp)\n    return total_size\n```\n\n----------------------------------------\n\nTITLE: Excluding All Images in Crawl4AI\nDESCRIPTION: Configuration to completely remove all images early in the processing pipeline for improved memory efficiency and processing speed, useful when image data isn't needed in crawl results.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_88\n\nLANGUAGE: python\nCODE:\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_all_images=True\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced Crawler4ai Configuration with JSON\nDESCRIPTION: Example of a JSON configuration file that can be used with Crawler4ai to set various crawling parameters and options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"urls\": [\"https://example.com\"],\n    \"max_pages\": 100,\n    \"max_depth\": 3,\n    \"respect_robots_txt\": true,\n    \"crawl_delay\": 1.0,\n    \"timeout\": 30,\n    \"user_agent\": \"Crawler4ai/1.0\",\n    \"output_dir\": \"crawl_results\",\n    \"extract\": {\n        \"title\": true,\n        \"text\": true,\n        \"links\": true,\n        \"images\": false,\n        \"metadata\": true\n    },\n    \"exclude_patterns\": [\n        \".*\\\\.pdf$\",\n        \".*\\\\.jpg$\",\n        \"/login/\",\n        \"/logout/\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Merging Documents - Python Implementation\nDESCRIPTION: Merges documents into sections based on token threshold and overlap parameters using the merge_chunks function. Takes into account word-to-token ratio for accurate chunking.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_71\n\nLANGUAGE: python\nCODE:\n```\ndef _merge(self, documents, chunk_token_threshold, overlap) -> List[str]:\n    \"\"\"\n    Merge documents into sections based on chunk_token_threshold and overlap.\n    \"\"\"\n    sections =  merge_chunks(\n        docs = documents,\n        target_size= chunk_token_threshold,\n        overlap=overlap,\n        word_token_ratio=self.word_token_rate\n    )\n    return sections\n```\n\n----------------------------------------\n\nTITLE: Generating Markdown from HTML in Python\nDESCRIPTION: This snippet selects the appropriate HTML source for markdown generation based on configuration settings. It handles different HTML sources and potential errors, defaulting to cleaned HTML if issues occur.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\n# --- SELECT HTML SOURCE BASED ON CONTENT_SOURCE ---\n# Get the desired source from the generator config, default to 'cleaned_html'\nselected_html_source = getattr(markdown_generator, 'content_source', 'cleaned_html')\n\n# Define the source selection logic using dict dispatch\nhtml_source_selector = {\n    \"raw_html\": lambda: html,  # The original raw HTML\n    \"cleaned_html\": lambda: cleaned_html,  # The HTML after scraping strategy\n    \"fit_html\": lambda: preprocess_html_for_schema(html_content=html),  # Preprocessed raw HTML\n}\n\nmarkdown_input_html = cleaned_html  # Default to cleaned_html\n\ntry:\n    # Get the appropriate lambda function, default to returning cleaned_html if key not found\n    source_lambda = html_source_selector.get(selected_html_source, lambda: cleaned_html)\n    # Execute the lambda to get the selected HTML\n    markdown_input_html = source_lambda()\n\nexcept Exception as e:\n    # Handle potential errors, especially from preprocess_html_for_schema\n    if self.logger:\n        self.logger.warning(\n            f\"Error getting/processing '{selected_html_source}' for markdown source: {e}. Falling back to cleaned_html.\",\n            tag=\"MARKDOWN_SRC\"\n        )\n    # Ensure markdown_input_html is still the default cleaned_html in case of error\n    markdown_input_html = cleaned_html\n# --- END: HTML SOURCE SELECTION ---\n```\n\n----------------------------------------\n\nTITLE: Accessing Link Data in Crawl4AI Results\nDESCRIPTION: Example of how to access and process internal and external links from a crawl result, demonstrating how to extract link counts and examine link properties.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_89\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://www.example.com\")\n    if result.success:\n        internal_links = result.links.get(\"internal\", [])\n        external_links = result.links.get(\"external\", [])\n        print(f\"Found {len(internal_links)} internal links.\")\n        print(f\"Found {len(internal_links)} external links.\")\n        print(f\"Found {len(result.media)} media items.\")\n\n        # Each link is typically a dictionary with fields like:\n        # { \"href\": \"...\", \"text\": \"...\", \"title\": \"...\", \"base_domain\": \"...\" }\n        if internal_links:\n            print(\"Sample Internal Link:\", internal_links[0])\n    else:\n        print(\"Crawl failed:\", result.error_message)\n```\n\n----------------------------------------\n\nTITLE: Running a Builtin Browser Example in Crawl4AI\nDESCRIPTION: This snippet shows how to run a provided example script that demonstrates the builtin browser functionality.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython builtin_browser_example.py\n```\n\n----------------------------------------\n\nTITLE: LLM-based Structured Data Extraction from Web Pages in Python\nDESCRIPTION: This function demonstrates the use of LLMExtractionStrategy to extract structured data from a web page (Hacker News) using a language model. It extracts news titles, source URLs, and comment counts.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_150\n\nLANGUAGE: python\nCODE:\n```\nasync def demo_llm_structured_extraction_no_schema():\n    # Create a simple LLM extraction strategy (no schema required)\n    extraction_strategy = LLMExtractionStrategy(\n        llm_config=LLMConfig(\n            provider=\"groq/qwen-2.5-32b\",\n            api_token=\"env:GROQ_API_KEY\",\n        ),\n        instruction=\"This is news.ycombinator.com, extract all news, and for each, I want title, source url, number of comments.\",\n        extract_type=\"schema\",\n        schema=\"{title: string, url: string, comments: int}\",\n        extra_args={\n            \"temperature\": 0.0,\n            \"max_tokens\": 4096,\n        },\n        verbose=True,\n    )\n\n    config = CrawlerRunConfig(extraction_strategy=extraction_strategy)\n\n    async with AsyncWebCrawler() as crawler:\n        results: List[CrawlResult] = await crawler.arun(\n            \"https://news.ycombinator.com/\", config=config\n        )\n\n        for result in results:\n            print(f\"URL: {result.url}\")\n            print(f\"Success: {result.success}\")\n            if result.success:\n                data = json.loads(result.extracted_content)\n                print(json.dumps(data, indent=2))\n            else:\n                print(\"Failed to extract structured data\")\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Feature Integration\nDESCRIPTION: Combined implementation of all features including knowledge graph extraction, anti-bot detection bypass, and content extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel\nimport json, os\nfrom typing import List\n\n# Define classes for the knowledge graph structure\nclass Landmark(BaseModel):\n    name: str\n    description: str\n    activities: list[str]  # E.g., visiting, sightseeing, relaxing\n\nclass City(BaseModel):\n    name: str\n    description: str\n    landmarks: list[Landmark]\n    cultural_highlights: list[str]  # E.g., food, music, traditional crafts\n\nclass TravelKnowledgeGraph(BaseModel):\n    cities: list[City]  # Central Mexican cities to visit\n\nasync def combined_demo():\n    # Define the knowledge graph extraction strategy\n    strategy = LLMExtractionStrategy(\n        # provider=\"ollama/nemotron\",\n        provider='openai/gpt-4o-mini', # Or any other provider, including Ollama and open source models\n        pi_token=os.getenv('OPENAI_API_KEY'), # In case of Ollama just pass \"no-token\"\n        schema=TravelKnowledgeGraph.schema(),\n        instruction=(\n            \"Extract cities, landmarks, and cultural highlights for places to visit in Central Mexico. \"\n            \"For each city, list main landmarks with descriptions and activities, as well as cultural highlights.\"\n        )\n    )\n\n    # Set up the AsyncWebCrawler with multi-browser support, Magic Mode, and Fit Markdown\n    async with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n        result = await crawler.arun(\n            url=\"https://janineintheworld.com/places-to-visit-in-central-mexico\",\n            extraction_strategy=strategy,\n            bypass_cache=True,\n            magic=True\n        )\n        \n        # Display main article content in Fit Markdown format\n        print(\"Extracted Main Content:\\n\", result.fit_markdown)\n        \n        # Display extracted knowledge graph of cities, landmarks, and cultural highlights\n        if result.extracted_content:\n            travel_graph = json.loads(result.extracted_content)\n            print(\"\\nExtracted Knowledge Graph:\\n\", json.dumps(travel_graph, indent=2))\n\n# Run the combined demo\nawait combined_demo()\n```\n\n----------------------------------------\n\nTITLE: Configuring Robots.txt Compliance in Crawl4AI\nDESCRIPTION: Example of configuring crawler to respect robots.txt rules. It shows how to enable robots.txt checking and set a custom user agent for ethical web crawling practices.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    check_robots_txt=True,  # Enable robots.txt compliance\n    user_agent=\"MyBot/1.0\"  # Identify your crawler\n)\n```\n\n----------------------------------------\n\nTITLE: Combining Advanced Features in crawl4ai\nDESCRIPTION: This code demonstrates a comprehensive implementation of crawl4ai with advanced features including proxy configuration, PDF/screenshot capture, SSL certificate verification, custom headers, and session state management. It creates a configured crawler, processes a secure webpage, and handles the resulting data by saving PDFs and screenshots and checking SSL certificate information.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os, asyncio\nfrom base64 import b64decode\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # 1. Browser config with proxy + headless\n    browser_cfg = BrowserConfig(\n        proxy_config={\n            \"server\": \"http://proxy.example.com:8080\",\n            \"username\": \"myuser\",\n            \"password\": \"mypass\",\n        },\n        headless=True,\n    )\n\n    # 2. Crawler config with PDF, screenshot, SSL, custom headers, and ignoring caches\n    crawler_cfg = CrawlerRunConfig(\n        pdf=True,\n        screenshot=True,\n        fetch_ssl_certificate=True,\n        cache_mode=CacheMode.BYPASS,\n        headers={\"Accept-Language\": \"en-US,en;q=0.8\"},\n        storage_state=\"my_storage.json\",  # Reuse session from a previous sign-in\n        verbose=True,\n    )\n\n    # 3. Crawl\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url = \"https://secure.example.com/protected\", \n            config=crawler_cfg\n        )\n        \n        if result.success:\n            print(\"[OK] Crawled the secure page. Links found:\", len(result.links.get(\"internal\", [])))\n            \n            # Save PDF & screenshot\n            if result.pdf:\n                with open(\"result.pdf\", \"wb\") as f:\n                    f.write(b64decode(result.pdf))\n            if result.screenshot:\n                with open(\"result.png\", \"wb\") as f:\n                    f.write(b64decode(result.screenshot))\n            \n            # Check SSL cert\n            if result.ssl_certificate:\n                print(\"SSL Issuer CN:\", result.ssl_certificate.issuer.get(\"CN\", \"\"))\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Filtering Links and Media during Crawling with Crawl4AI in Python\nDESCRIPTION: Illustrates configuring `CrawlerRunConfig` for advanced filtering during a crawl with `AsyncWebCrawler`. It demonstrates excluding external links (`exclude_external_links=True`), specific domains (`exclude_domains`), social media links (`exclude_social_media_links=True`), and external images (`exclude_external_images=True`), while ensuring images are fully loaded (`wait_for_images=True`). The example then processes the results to show the counts of internal/external links and lists the first few found images with their source, alt text, and score.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    # Suppose we want to keep only internal links, remove certain domains, \n    # and discard external images from the final crawl data.\n    crawler_cfg = CrawlerRunConfig(\n        exclude_external_links=True,\n        exclude_domains=[\"spammyads.com\"],\n        exclude_social_media_links=True,   # skip Twitter, Facebook, etc.\n        exclude_external_images=True,      # keep only images from main domain\n        wait_for_images=True,             # ensure images are loaded\n        verbose=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://www.example.com\", config=crawler_cfg)\n\n        if result.success:\n            print(\"[OK] Crawled:\", result.url)\n            \n            # 1. Links\n            in_links = result.links.get(\"internal\", [])\n            ext_links = result.links.get(\"external\", [])\n            print(\"Internal link count:\", len(in_links))\n            print(\"External link count:\", len(ext_links))  # should be zero with exclude_external_links=True\n            \n            # 2. Images\n            images = result.media.get(\"images\", [])\n            print(\"Images found:\", len(images))\n            \n            # Let's see a snippet of these images\n            for i, img in enumerate(images[:3]):\n                print(f\"  - {img['src']} (alt={img.get('alt','')}, score={img.get('score','N/A')})\")\n        else:\n            print(\"[ERROR] Failed to crawl. Reason:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running AsyncWebCrawler with Custom Configurations in Python\nDESCRIPTION: Complete example of configuring and running an asynchronous web crawler with custom browser settings and run parameters. It demonstrates setting up browser configuration, run configuration with cache mode and selectors, and handling the crawl results including screenshots.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # Configure the browser\n    browser_cfg = BrowserConfig(\n        headless=False,\n        viewport_width=1280,\n        viewport_height=720,\n        proxy=\"http://user:pass@myproxy:8080\",\n        text_mode=True\n    )\n\n    # Configure the run\n    run_cfg = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        session_id=\"my_session\",\n        css_selector=\"main.article\",\n        excluded_tags=[\"script\", \"style\"],\n        exclude_external_links=True,\n        wait_for=\"css:.article-loaded\",\n        screenshot=True,\n        stream=True\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/news\",\n            config=run_cfg\n        )\n        if result.success:\n            print(\"Final cleaned_html length:\", len(result.cleaned_html))\n            if result.screenshot:\n                print(\"Screenshot captured (base64, length):\", len(result.screenshot))\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Basic Web Crawling with Crawl4AI\nDESCRIPTION: This function demonstrates a simple web crawl using Crawl4AI. It configures the browser and crawler, then crawls a news website and prints the first 500 characters of the extracted markdown content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_127\n\nLANGUAGE: python\nCODE:\n```\nasync def simple_crawl():\n    print(\"\\n--- Basic Usage ---\")\n    browser_config = BrowserConfig(headless=True)\n    crawler_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\", config=crawler_config\n        )\n        print(result.markdown[:500])\n```\n\n----------------------------------------\n\nTITLE: Working with Dispatcher Results in Python\nDESCRIPTION: Demonstrates how to access dispatcher information from parallel crawling operations, including task ID, memory usage, and execution duration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Example usage:\nfor result in results:\n    if result.success and result.dispatch_result:\n        dr = result.dispatch_result\n        print(f\"URL: {result.url}, Task ID: {dr.task_id}\")\n        print(f\"Memory: {dr.memory_usage:.1f} MB (Peak: {dr.peak_memory:.1f} MB)\")\n        print(f\"Duration: {dr.end_time - dr.start_time}\")\n```\n\n----------------------------------------\n\nTITLE: Using target_elements for Flexible Content Selection in Crawl4AI\nDESCRIPTION: This snippet shows how to use the target_elements parameter in CrawlerRunConfig to target multiple elements for content extraction while preserving entire page context. This provides more flexibility than css_selector.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Target article body and sidebar, but not other content\n        target_elements=[\"article.main-content\", \"aside.sidebar\"]\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/blog-post\", \n            config=config\n        )\n        print(\"Markdown focused on target elements\")\n        print(\"Links from entire page still available:\", len(result.links.get(\"internal\", [])))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Displaying CLI Examples in Python\nDESCRIPTION: Outputs a comprehensive list of examples for using the Crawl4AI CLI, including basic usage, configuration, profile management, CDP mode, and advanced scenarios.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndef show_examples():\n    examples = \"\"\"\nðŸš€ Crawl4AI CLI Examples\n\n1ï¸âƒ£  Basic Usage:\n    # Simple crawl with default settings\n    crwl https://example.com\n\n    # Get markdown output\n    crwl https://example.com -o markdown\n\n    # Verbose JSON output with cache bypass\n    crwl https://example.com -o json -v --bypass-cache\n\n2ï¸âƒ£  Using Config Files:\n    # Using browser and crawler configs\n    crwl https://example.com -B browser.yml -C crawler.yml\n\n    # CSS-based extraction\n    crwl https://example.com -e extract_css.yml -s css_schema.json -o json\n\n    # LLM-based extraction with config file\n    crwl https://example.com -e extract_llm.yml -s llm_schema.json -o json\n    \n    # Quick LLM-based JSON extraction (prompts for LLM provider first time)\n    crwl https://example.com -j  # Auto-extracts structured data\n    crwl https://example.com -j \\\"Extract product details including name, price, and features\\\"  # With specific instructions\n\n3ï¸âƒ£  Direct Parameters:\n    # Browser settings\n    crwl https://example.com -b \\\"headless=true,viewport_width=1280,user_agent_mode=random\\\"\n\n    # Crawler settings\n    crwl https://example.com -c \\\"css_selector=#main,delay_before_return_html=2,scan_full_page=true\\\"\n\n4ï¸âƒ£  Profile Management for Identity-Based Crawling:\n    # Launch interactive profile manager\n    crwl profiles\n\n    # Create, list, and delete browser profiles for identity-based crawling\n    # Use a profile for crawling (keeps you logged in)\n    crwl https://example.com -p my-profile-name\n\n    # Example: Crawl a site that requires login\n    # 1. First create a profile and log in:\n    crwl profiles\n    # 2. Then use that profile to crawl the authenticated site:\n    crwl https://site-requiring-login.com/dashboard -p my-profile-name\n\n5ï¸âƒ£  CDP Mode for Browser Automation:\n    # Launch browser with CDP debugging on default port 9222\n    crwl cdp\n\n    # Use a specific profile and custom port\n    crwl cdp -p my-profile -P 9223\n\n    # Launch headless browser with CDP enabled\n    crwl cdp --headless\n\n    # Launch in incognito mode (ignores profile)\n    crwl cdp --incognito\n\n    # Use the CDP URL with other tools (Puppeteer, Playwright, etc.)\n    # The URL will be displayed in the terminal when the browser starts\n\n    \n6ï¸âƒ£  Sample Config Files:\n\nbrowser.yml:\n    headless: true\n    viewport_width: 1280\n    user_agent_mode: \\\"random\\\"\n    verbose: true\n    ignore_https_errors: true\n\nextract_css.yml:\n    type: \\\"json-css\\\"\n    params:\n        verbose: true\n\ncss_schema.json:\n    {\n      \\\"name\\\": \\\"ArticleExtractor\\\",\n      \\\"baseSelector\\\": \\\".article\\\",\n      \\\"fields\\\": [\n        {\n          \\\"name\\\": \\\"title\\\",\n          \\\"selector\\\": \\\"h1.title\\\",\n          \\\"type\\\": \\\"text\\\"\n        },\n        {\n          \\\"name\\\": \\\"link\\\",\n          \\\"selector\\\": \\\"a.read-more\\\",\n          \\\"type\\\": \\\"attribute\\\",\n          \\\"attribute\\\": \\\"href\\\"\n        }\n      ]\n    }\n\nextract_llm.yml:\n    type: \\\"llm\\\"\n    provider: \\\"openai/gpt-4\\\"\n    instruction: \\\"Extract all articles with their titles and links\\\"\n    api_token: \\\"your-token\\\"\n    params:\n        temperature: 0.3\n        max_tokens: 1000\n\nllm_schema.json:\n    {\n      \\\"title\\\": \\\"Article\\\",\n      \\\"type\\\": \\\"object\\\",\n      \\\"properties\\\": {\n        \\\"title\\\": {\n          \\\"type\\\": \\\"string\\\",\n          \\\"description\\\": \\\"The title of the article\\\"\n        },\n        \\\"link\\\": {\n          \\\"type\\\": \\\"string\\\",\n          \\\"description\\\": \\\"URL to the full article\\\"\n        }\n      }\n    }\n\n7ï¸âƒ£  Advanced Usage:\n    # Combine configs with direct parameters\n    crwl https://example.com -B browser.yml -b \\\"headless=false,viewport_width=1920\\\"\n\n    # Full extraction pipeline with config files\n    crwl https://example.com \\\\\n        -B browser.yml \\\\\n        -C crawler.yml \\\\\n        -e extract_llm.yml \\\\\n        -s llm_schema.json \\\\\n        -o json \\\\\n        -v\n        \n    # Quick LLM-based extraction with specific instructions\n    crwl https://amazon.com/dp/B01DFKC2SO \\\\\n        -j \\\"Extract product title, current price, original price, rating, and all product specifications\\\" \\\\\n        -b \\\"headless=true,viewport_width=1280\\\" \\\\\n        -v\n\n    # Content filtering with BM25\n    crwl https://example.com \\\\\n        -f filter_bm25.yml \\\\\n        -o markdown-fit\n\n    # Authenticated crawling with profile\n    crwl https://login-required-site.com \\\\\n        -p my-authenticated-profile \\\\\n        -c \\\"css_selector=.dashboard-content\\\" \\\\\n        -o markdown\n\nFor more documentation visit: https://github.com/unclecode/crawl4ai\n\n8ï¸âƒ£  Q&A with LLM:\n    # Ask a question about the content\n    crwl https://example.com -q \\\"What is the main topic discussed?\\\"\n\n    # First view content, then ask questions\n    crwl https://example.com -o markdown  # See the crawled content first\n    crwl https://example.com -q \\\"Summarize the key points\\\"\n    crwl https://example.com -q \\\"What are the conclusions?\\\"\n\n    # Advanced crawling with Q&A\n    crwl https://example.com \\\\\n        -B browser.yml \\\\\n        -c \\\"css_selector=article,scan_full_page=true\\\" \\\\\n        -q \\\"What are the pros and cons mentioned?\\\"\n\n    Note: First time using -q will prompt for LLM provider and API token.\n    These will be saved in ~/.crawl4ai/global.yml for future use.\n    \n    Supported provider format: 'company/model'\n    Examples:\n      - ollama/llama3.3\n      - openai/gpt-4\n      - anthropic/claude-3-sonnet\n      - cohere/command\n      - google/gemini-pro\n    \n    See full list of providers: https://docs.litellm.ai/docs/providers\n    \n    # Set default LLM provider and token in advance\n    crwl config set DEFAULT_LLM_PROVIDER \\\"anthropic/claude-3-sonnet\\\"\n    crwl config set DEFAULT_LLM_PROVIDER_TOKEN \\\"your-api-token-here\\\"\n    \n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Pattern-Based Extraction with JsonCssExtractionStrategy in Crawl4AI\nDESCRIPTION: This example demonstrates how to use JsonCssExtractionStrategy for structured data extraction based on CSS patterns. It extracts news items from Hacker News with a defined schema that captures title and link information from repeated elements.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    # Minimal schema for repeated items\n    schema = {\n        \"name\": \"News Items\",\n        \"baseSelector\": \"tr.athing\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"span.titleline a\", \"type\": \"text\"},\n            {\n                \"name\": \"link\", \n                \"selector\": \"span.titleline a\", \n                \"type\": \"attribute\", \n                \"attribute\": \"href\"\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        # Content filtering\n        excluded_tags=[\"form\", \"header\"],\n        exclude_domains=[\"adsite.com\"],\n        \n        # CSS selection or entire page\n        css_selector=\"table.itemlist\",\n\n        # No caching for demonstration\n        cache_mode=CacheMode.BYPASS,\n\n        # Extraction strategy\n        extraction_strategy=JsonCssExtractionStrategy(schema)\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com/newest\", \n            config=config\n        )\n        data = json.loads(result.extracted_content)\n        print(\"Sample extracted item:\", data[:1])  # Show first item\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Batch Mode Execution for BFS Web Crawler in Python\nDESCRIPTION: Implements batch mode execution for the BFS crawler. This method processes one BFS level at a time, collecting all results before returning them as a complete list. It handles parent-child relationships between URLs and tracks crawl depth.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_179\n\nLANGUAGE: python\nCODE:\n```\nasync def _arun_batch(\n    self,\n    start_url: str,\n    crawler: AsyncWebCrawler,\n    config: CrawlerRunConfig,\n) -> List[CrawlResult]:\n    \"\"\"\n    Batch (non-streaming) mode:\n    Processes one BFS level at a time, then yields all the results.\n    \"\"\"\n    visited: Set[str] = set()\n    # current_level holds tuples: (url, parent_url)\n    current_level: List[Tuple[str, Optional[str]]] = [(start_url, None)]\n    depths: Dict[str, int] = {start_url: 0}\n\n    results: List[CrawlResult] = []\n\n    while current_level and not self._cancel_event.is_set():\n        next_level: List[Tuple[str, Optional[str]]] = []\n        urls = [url for url, _ in current_level]\n        visited.update(urls)\n\n        # Clone the config to disable deep crawling recursion and enforce batch mode.\n        batch_config = config.clone(deep_crawl_strategy=None, stream=False)\n        batch_results = await crawler.arun_many(urls=urls, config=batch_config)\n        \n        # Update pages crawled counter - count only successful crawls\n        successful_results = [r for r in batch_results if r.success]\n        self._pages_crawled += len(successful_results)\n        \n        for result in batch_results:\n            url = result.url\n            depth = depths.get(url, 0)\n            result.metadata = result.metadata or {}\n            result.metadata[\"depth\"] = depth\n            parent_url = next((parent for (u, parent) in current_level if u == url), None)\n            result.metadata[\"parent_url\"] = parent_url\n            results.append(result)\n            \n            # Only discover links from successful crawls\n            if result.success:\n                # Link discovery will handle the max pages limit internally\n                await self.link_discovery(result, url, depth, visited, next_level, depths)\n\n        current_level = next_level\n\n    return results\n```\n\n----------------------------------------\n\nTITLE: Excluding External and Social Media Links in Crawl4AI (Python)\nDESCRIPTION: This snippet demonstrates configuring `CrawlerRunConfig` to exclude both external links (links pointing outside the primary domain) and links to known social media platforms. `exclude_external_links=True` handles the former, and `exclude_social_media_links=True` handles the latter. The example shows passing this configuration to the `arun` method.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    crawler_cfg = CrawlerRunConfig(\n        exclude_external_links=True,          # No links outside primary domain\n        exclude_social_media_links=True       # Skip recognized social media domains\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            \"https://www.example.com\",\n            config=crawler_cfg\n        )\n        if result.success:\n            print(\"[OK] Crawled:\", result.url)\n            print(\"Internal links count:\", len(result.links.get(\"internal\", [])))\n            print(\"External links count:\", len(result.links.get(\"external\", [])))  \n            # Likely zero external links in this scenario\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Utility Methods for LLMConfig in Python\nDESCRIPTION: This snippet defines utility methods for the LLMConfig class, including from_kwargs() for creating an instance from a dictionary, to_dict() for serializing the object, and clone() for creating a copy with updated values.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n@staticmethod\ndef from_kwargs(kwargs: dict) -> \"LLMConfig\":\n    return LLMConfig(\n        provider=kwargs.get(\"provider\", DEFAULT_PROVIDER),\n        api_token=kwargs.get(\"api_token\"),\n        base_url=kwargs.get(\"base_url\"),\n        temprature=kwargs.get(\"temprature\"),\n        max_tokens=kwargs.get(\"max_tokens\"),\n        top_p=kwargs.get(\"top_p\"),\n        frequency_penalty=kwargs.get(\"frequency_penalty\"),\n        presence_penalty=kwargs.get(\"presence_penalty\"),\n        stop=kwargs.get(\"stop\"),\n        n=kwargs.get(\"n\")\n    )\n\ndef to_dict(self):\n    return {\n        \"provider\": self.provider,\n        \"api_token\": self.api_token,\n        \"base_url\": self.base_url,\n        \"temprature\": self.temprature,\n        \"max_tokens\": self.max_tokens,\n        \"top_p\": self.top_p,\n        \"frequency_penalty\": self.frequency_penalty,\n        \"presence_penalty\": self.presence_penalty,\n        \"stop\": self.stop,\n        \"n\": self.n\n    }\n\ndef clone(self, **kwargs):\n    \"\"\"Create a copy of this configuration with updated values.\n\n    Args:\n        **kwargs: Key-value pairs of configuration options to update\n\n    Returns:\n        llm_config: A new instance with the specified updates\n    \"\"\"\n    config_dict = self.to_dict()\n```\n\n----------------------------------------\n\nTITLE: Extracting Crypto Prices with XPath and raw:// HTML Scheme in Python\nDESCRIPTION: Demonstrates how to use JsonXPathExtractionStrategy with crawl4ai to extract structured data from HTML without making a network request. The example uses the raw:// scheme to pass dummy HTML directly for local testing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_186\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonXPathExtractionStrategy\n\nasync def extract_crypto_prices_xpath():\n    # 1. Minimal dummy HTML with some repeating rows\n    dummy_html = \"\"\"\n    <html>\n      <body>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Bitcoin</h2>\n          <span class='coin-price'>$28,000</span>\n        </div>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Ethereum</h2>\n          <span class='coin-price'>$1,800</span>\n        </div>\n      </body>\n    </html>\n    \"\"\"\n\n    # 2. Define the JSON schema (XPath version)\n    schema = {\n        \"name\": \"Crypto Prices via XPath\",\n        \"baseSelector\": \"//div[@class='crypto-row']\",\n        \"fields\": [\n            {\n                \"name\": \"coin_name\",\n                \"selector\": \".//h2[@class='coin-name']\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \".//span[@class='coin-price']\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n\n    # 3. Place the strategy in the CrawlerRunConfig\n    config = CrawlerRunConfig(\n        extraction_strategy=JsonXPathExtractionStrategy(schema, verbose=True)\n    )\n\n    # 4. Use raw:// scheme to pass dummy_html directly\n    raw_url = f\"raw://{dummy_html}\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=raw_url,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin rows\")\n        if data:\n            print(\"First item:\", data[0])\n\nasyncio.run(extract_crypto_prices_xpath())\n```\n\n----------------------------------------\n\nTITLE: Configuring DefaultMarkdownGenerator Options\nDESCRIPTION: This snippet demonstrates how to customize the DefaultMarkdownGenerator by passing an options dictionary. It shows how to ignore links, disable HTML escaping, and set text wrapping width.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_104\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Example: ignore all links, don't escape HTML, and wrap text at 80 characters\n    md_generator = DefaultMarkdownGenerator(\n        options={\n            \"ignore_links\": True,\n            \"escape_html\": False,\n            \"body_width\": 80\n        }\n    )\n\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/docs\", config=config)\n        if result.success:\n            print(\"Markdown:\\n\", result.markdown[:500])  # Just a snippet\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Executing Multiple Button Clicks in a Single Crawl4AI Call with Python and JavaScript\nDESCRIPTION: This snippet shows how to use Crawl4AI to execute a complex JavaScript snippet that clicks multiple modules on a page, waits for content updates, and returns control to Crawl4AI for extraction, all in a single arun() call.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/tutorial_dynamic_clicks.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CacheMode\n\njs_code = [\n    # Example JS that clicks multiple modules:\n    \"\"\"\n    (async () => {\n      const modules = document.querySelectorAll('.module-item');\n      for (let i = 0; i < modules.length; i++) {\n        modules[i].scrollIntoView();\n        modules[i].click();\n        // Wait for each module's content to load, adjust 100ms as needed\n        await new Promise(r => setTimeout(r, 100));\n      }\n    })();\n    \"\"\"\n]\n\nasync with AsyncWebCrawler(headless=True, verbose=True) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        js_code=js_code,\n        wait_for=\"css:.final-loaded-content-class\",\n        cache_mode=CacheMode.BYPASS\n    )\n\n# `result` now contains all content after all modules have been clicked in one go.\n```\n\n----------------------------------------\n\nTITLE: Complete Content Filtering Example in Crawl4AI\nDESCRIPTION: This snippet demonstrates a complete example of content filtering with Crawl4AI, combining CSS selection with various filtering options. It targets a specific content area while excluding navigation, footers, external links, and more.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    config = CrawlerRunConfig(\n        css_selector=\"main.content\", \n        word_count_threshold=10,\n        excluded_tags=[\"nav\", \"footer\"],\n        exclude_external_links=True,\n        exclude_social_media_links=True,\n        exclude_domains=[\"ads.com\", \"spammytrackers.net\"],\n        exclude_external_images=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n        print(\"Cleaned HTML length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Capturing PDFs and Screenshots with Crawl4AI\nDESCRIPTION: Shows how to capture both PDF and screenshot of a webpage using Crawl4AI. It demonstrates saving the captured PDF and screenshot to files.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os, asyncio\nfrom base64 import b64decode\nfrom crawl4ai import AsyncWebCrawler, CacheMode\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://en.wikipedia.org/wiki/List_of_common_misconceptions\",\n            cache_mode=CacheMode.BYPASS,\n            pdf=True,\n            screenshot=True\n        )\n        \n        if result.success:\n            # Save screenshot\n            if result.screenshot:\n                with open(\"wikipedia_screenshot.png\", \"wb\") as f:\n                    f.write(b64decode(result.screenshot))\n            \n            # Save PDF\n            if result.pdf:\n                with open(\"wikipedia_page.pdf\", \"wb\") as f:\n                    f.write(result.pdf)\n            \n            print(\"[OK] PDF & screenshot captured.\")\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Scraping Strategy in Crawl4AI\nDESCRIPTION: This example demonstrates how to create a custom scraping strategy by inheriting from ContentScrapingStrategy. It shows the structure of the ScrapingResult object and how to implement both synchronous and asynchronous scraping methods.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import ContentScrapingStrategy, ScrapingResult, MediaItem, Media, Link, Links\n\nclass CustomScrapingStrategy(ContentScrapingStrategy):\n    def scrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n        # Implement your custom scraping logic here\n        return ScrapingResult(\n            cleaned_html=\"<html>...</html>\",  # Cleaned HTML content\n            success=True,                     # Whether scraping was successful\n            media=Media(\n                images=[                      # List of images found\n                    MediaItem(\n                        src=\"https://example.com/image.jpg\",\n                        alt=\"Image description\",\n                        desc=\"Surrounding text\",\n                        score=1,\n                        type=\"image\",\n                        group_id=1,\n                        format=\"jpg\",\n                        width=800\n                    )\n                ],\n                videos=[],                    # List of videos (same structure as images)\n                audios=[]                     # List of audio files (same structure as images)\n            ),\n            links=Links(\n                internal=[                    # List of internal links\n                    Link(\n                        href=\"https://example.com/page\",\n                        text=\"Link text\",\n                        title=\"Link title\",\n                        base_domain=\"example.com\"\n                    )\n                ],\n                external=[]                   # List of external links (same structure)\n            ),\n            metadata={                        # Additional metadata\n                \"title\": \"Page Title\",\n                \"description\": \"Page description\"\n            }\n        )\n\n    async def ascrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n        # For simple cases, you can use the sync version\n        return await asyncio.to_thread(self.scrap, url, html, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Running a Crawl4AI Example Script (Bash)\nDESCRIPTION: This command executes a specific Crawl4AI example script, 'hello_world', located within the 'docs.examples' module. It uses the Python interpreter with the '-m' flag to run the module as a script. Assumes Crawl4AI is installed and the project structure is accessible.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/examples.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m docs.examples.hello_world\n```\n\n----------------------------------------\n\nTITLE: Basic Deep Crawling with BFS Strategy in Crawl4AI\nDESCRIPTION: This code implements a basic deep crawl using the BFSDeepCrawlStrategy to crawl up to 2 levels deep while staying within the same domain. It demonstrates how to configure and execute a deep crawl and access individual results with their metadata.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\n\nasync def main():\n    # Configure a 2-level deep crawl\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BFSDeepCrawlStrategy(\n            max_depth=2, \n            include_external=False\n        ),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        verbose=True\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun(\"https://example.com\", config=config)\n        \n        print(f\"Crawled {len(results)} pages in total\")\n        \n        # Access individual results\n        for result in results[:3]:  # Show first 3 results\n            print(f\"URL: {result.url}\")\n            print(f\"Depth: {result.metadata.get('depth', 0)}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Extracting Cryptocurrency Prices with CSS Selectors in Python\nDESCRIPTION: A complete example demonstrating how to extract cryptocurrency prices from a website using Crawl4AI's JsonCssExtractionStrategy. The code defines a schema with base selectors and fields, configures the crawler, and processes the extracted JSON data.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_crypto_prices():\n    # 1. Define a simple extraction schema\n    schema = {\n        \"name\": \"Crypto Prices\",\n        \"baseSelector\": \"div.crypto-row\",    # Repeated elements\n        \"fields\": [\n            {\n                \"name\": \"coin_name\",\n                \"selector\": \"h2.coin-name\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"span.coin-price\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n\n    # 2. Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # 3. Set up your crawler config (if needed)\n    config = CrawlerRunConfig(\n        # e.g., pass js_code or wait_for if the page is dynamic\n        # wait_for=\"css:.crypto-row:nth-child(20)\"\n        cache_mode = CacheMode.BYPASS,\n        extraction_strategy=extraction_strategy,\n    )\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # 4. Run the crawl and extraction\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            \n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        # 5. Parse the extracted JSON\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin entries\")\n        print(json.dumps(data[0], indent=2) if data else \"No data found\")\n\nasyncio.run(extract_crypto_prices())\n```\n\n----------------------------------------\n\nTITLE: Accessing Media Data in Crawl4AI Results\nDESCRIPTION: Example showing how to access and process image and table data from crawl results, including extracting metadata and displaying basic information about the found media.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_93\n\nLANGUAGE: python\nCODE:\n```\nif result.success:\n    # Get images\n    images_info = result.media.get(\"images\", [])\n    print(f\"Found {len(images_info)} images in total.\")\n    for i, img in enumerate(images_info[:3]):  # Inspect just the first 3\n        print(f\"[Image {i}] URL: {img['src']}\")\n        print(f\"           Alt text: {img.get('alt', '')}\")\n        print(f\"           Score: {img.get('score')}\")\n        print(f\"           Description: {img.get('desc', '')}\\n\")\n    \n    # Get tables\n    tables = result.media.get(\"tables\", [])\n    print(f\"Found {len(tables)} data tables in total.\")\n    for i, table in enumerate(tables):\n        print(f\"[Table {i}] Caption: {table.get('caption', 'No caption')}\")\n        print(f\"           Columns: {len(table.get('headers', []))}\")\n        print(f\"           Rows: {len(table.get('rows', []))}\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Media and Links from Web Pages in Python\nDESCRIPTION: This function demonstrates how to extract media and links from a web page using the AsyncWebCrawler. The implementation details are not provided in the given code snippet.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_154\n\nLANGUAGE: python\nCODE:\n```\nasync def demo_media_and_links():\n    \"\"\"Extract media and links from a page\"\"\"\n    print(\"\\n=== 8. Media and Links Extraction ===\")\n\n    async with AsyncWebCrawler() as crawler:\n        # Implementation details not provided in the snippet\n```\n\n----------------------------------------\n\nTITLE: Initializing LLMConfig in Python for Crawl4AI\nDESCRIPTION: This snippet demonstrates how to create an LLMConfig instance for Crawl4AI. It shows how to specify the LLM provider and API token, with an example of using an environment variable for the API key.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nllm_config = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv(\"OPENAI_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: LLM-based Extraction Strategy Class Definition\nDESCRIPTION: Defines a class for extracting content from HTML using LLMs with support for different extraction types (block/schema), chunking, and usage tracking. Includes configuration management and deprecated parameter handling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_67\n\nLANGUAGE: python\nCODE:\n```\nclass LLMExtractionStrategy(ExtractionStrategy):\n    \"\"\"\n    A strategy that uses an LLM to extract meaningful content from the HTML.\n\n    Attributes:\n        llm_config: The LLM configuration object.\n        instruction: The instruction to use for the LLM model.\n        schema: Pydantic model schema for structured data.\n        extraction_type: \"block\" or \"schema\".\n        chunk_token_threshold: Maximum tokens per chunk.\n        overlap_rate: Overlap between chunks.\n        word_token_rate: Word to token conversion rate.\n        apply_chunking: Whether to apply chunking.\n        verbose: Whether to print verbose output.\n        usages: List of individual token usages.\n        total_usage: Accumulated token usage.\n    \"\"\"\n    _UNWANTED_PROPS = {\n            'provider' : 'Instead, use llm_config=LLMConfig(provider=\"...\")',\n            'api_token' : 'Instead, use llm_config=LlMConfig(api_token=\"...\")',\n            'base_url' : 'Instead, use llm_config=LLMConfig(base_url=\"...\")',\n            'api_base' : 'Instead, use llm_config=LLMConfig(base_url=\"...\")',\n        }\n```\n\n----------------------------------------\n\nTITLE: Agentic Crawler Setup and Execution\nDESCRIPTION: Implements an autonomous crawling system capable of understanding complex goals and executing multi-step operations. Features automatic planning, visual recognition, and error recovery.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.agents import CrawlerAgent\n\nasync with AsyncWebCrawler() as crawler:\n    agent = CrawlerAgent(crawler)\n    \n    # Automatic planning and execution\n    result = await agent.arun(\n        goal=\"Find research papers about quantum computing published in 2023 with more than 50 citations\",\n        auto_retry=True\n    )\n    print(\"Generated Plan:\", result.executed_steps)\n    print(\"Extracted Data:\", result.data)\n    \n    # Using custom steps with automatic execution\n    result = await agent.arun(\n        goal=\"Extract conference deadlines from ML conferences\",\n        custom_plan=[\n            \"Navigate to conference page\",\n            \"Find important dates section\",\n            \"Extract submission deadlines\",\n            \"Verify dates are for 2024\"\n        ]\n    )\n    \n    # Monitoring execution\n    print(\"Step Completion:\", result.step_status)\n    print(\"Execution Time:\", result.execution_time)\n    print(\"Success Rate:\", result.success_rate)\n```\n\n----------------------------------------\n\nTITLE: Running Crawl4AI Docker Container with Custom Configuration\nDESCRIPTION: This bash command shows how to run the Crawl4AI Docker container with a custom configuration file mounted at runtime.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_71\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 8000:8000 \\\n  -v $(pwd)/custom-config.yml:/app/config.yml \\\n  crawl4ai-server:prod\n```\n\n----------------------------------------\n\nTITLE: Enabling SSL Certificate Fetching in Crawl4AI\nDESCRIPTION: Shows how to enable SSL certificate fetching in a Crawl4AI crawler configuration. This is the first step to utilize the SSLCertificate functionality within a crawler session.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCrawlerRunConfig(fetch_ssl_certificate=True, ...)\n```\n\n----------------------------------------\n\nTITLE: Listing Browser Profiles Method (Deprecated) in Python\nDESCRIPTION: A static method for listing available browser profiles that has been moved to the BrowserProfiler class. It provides documentation on how to use the new method and delegates to the BrowserProfiler implementation when called.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_118\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef list_profiles():\n    \"\"\"\n    This method has been moved to the BrowserProfiler class.\n    \n    Lists all available browser profiles in the Crawl4AI profiles directory.\n    \n    Please use BrowserProfiler.list_profiles() instead.\n    \n    Example:\n        ```python\n        from crawl4ai.browser_profiler import BrowserProfiler\n        \n        profiler = BrowserProfiler()\n        profiles = profiler.list_profiles()\n        ```\n    \"\"\"\n    from .browser_profiler import BrowserProfiler\n    \n    # Create a BrowserProfiler instance and delegate to it\n    profiler = BrowserProfiler()\n    return profiler.list_profiles()\n```\n\n----------------------------------------\n\nTITLE: Creating and Modifying BrowserConfig in Python for Crawl4AI\nDESCRIPTION: This code snippet shows how to create a base BrowserConfig and then clone it to create a modified version for debugging purposes in Crawl4AI. It demonstrates the use of the clone() method to create variations of configurations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create a base browser config\nbase_browser = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    text_mode=True\n)\n\n# Create a visible browser config for debugging\ndebug_browser = base_browser.clone(\n    headless=False,\n    verbose=True\n)\n```\n\n----------------------------------------\n\nTITLE: Capturing Screenshots and PDFs with AsyncWebCrawler in Python\nDESCRIPTION: An async function that demonstrates how to capture screenshots and PDFs of web pages using the AsyncWebCrawler. It configures the crawler to take a screenshot and generate a PDF of a Wikipedia page, then saves both to files. Base64 decoding is used for the screenshot data.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_156\n\nLANGUAGE: python\nCODE:\n```\nasync def demo_screenshot_and_pdf():\n    \"\"\"Capture screenshot and PDF of a page\"\"\"\n    print(\"\\n=== 9. Screenshot and PDF Capture ===\")\n\n    async with AsyncWebCrawler() as crawler:\n        result: List[CrawlResult] = await crawler.arun(\n            # url=\"https://example.com\",\n            url=\"https://en.wikipedia.org/wiki/Giant_anteater\",\n            config=CrawlerRunConfig(screenshot=True, pdf=True),\n        )\n\n        for i, result in enumerate(result):\n            # if result.screenshot_data:\n            if result.screenshot:\n                # Save screenshot\n                screenshot_path = f\"{__cur_dir__}/tmp/example_screenshot.png\"\n                with open(screenshot_path, \"wb\") as f:\n                    f.write(base64.b64decode(result.screenshot))\n                print(f\"Screenshot saved to {screenshot_path}\")\n\n            # if result.pdf_data:\n            if result.pdf:\n                # Save PDF\n                pdf_path = f\"{__cur_dir__}/tmp/example.pdf\"\n                with open(pdf_path, \"wb\") as f:\n                    f.write(result.pdf)\n                print(f\"PDF saved to {pdf_path}\")\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Web Crawling Example with Crawl4AI\nDESCRIPTION: A complete example showcasing common usage patterns including configuration, content filtering, processing, cache control, and handling the results with content, images, and links.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    browser_config = BrowserConfig(verbose=True)\n    run_config = CrawlerRunConfig(\n        # Content filtering\n        word_count_threshold=10,\n        excluded_tags=['form', 'header'],\n        exclude_external_links=True,\n        \n        # Content processing\n        process_iframes=True,\n        remove_overlay_elements=True,\n        \n        # Cache control\n        cache_mode=CacheMode.ENABLED  # Use cache if available\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing BFS Deep Crawl Strategy Class in Python\nDESCRIPTION: Defines the BFSDeepCrawlStrategy class that inherits from DeepCrawlStrategy. The constructor initializes crawling parameters including maximum depth, filter chain, URL scorer, and various control options like page limits and external link inclusion.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_176\n\nLANGUAGE: python\nCODE:\n```\nclass BFSDeepCrawlStrategy(DeepCrawlStrategy):\n    \"\"\"\n    Breadth-First Search deep crawling strategy.\n    \n    Core functions:\n      - arun: Main entry point; splits execution into batch or stream modes.\n      - link_discovery: Extracts, filters, and (if needed) scores the outgoing URLs.\n      - can_process_url: Validates URL format and applies the filter chain.\n    \"\"\"\n    def __init__(\n        self,\n        max_depth: int,\n        filter_chain: FilterChain = FilterChain(),\n        url_scorer: Optional[URLScorer] = None,        \n        include_external: bool = False,\n        score_threshold: float = -infinity,\n        max_pages: int = infinity,\n        logger: Optional[logging.Logger] = None,\n    ):\n        self.max_depth = max_depth\n        self.filter_chain = filter_chain\n        self.url_scorer = url_scorer\n        self.include_external = include_external\n        self.score_threshold = score_threshold\n        self.max_pages = max_pages\n        self.logger = logger or logging.getLogger(__name__)\n        self.stats = TraversalStats(start_time=datetime.now())\n        self._cancel_event = asyncio.Event()\n        self._pages_crawled = 0\n```\n\n----------------------------------------\n\nTITLE: Building and Running Crawl4AI Locally with Docker Compose (Bash)\nDESCRIPTION: Demonstrates building the Docker image locally using the `Dockerfile` in the project root and then running the container using `docker compose up --build -d`. This automatically uses the correct architecture.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Builds the image locally using Dockerfile and runs it\n# Automatically uses the correct architecture for your machine\ndocker compose up --build -d\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-Powered Markdown Generation\nDESCRIPTION: Configures LLM-based content filtering and organization system. Uses GPT-4 to extract technical documentation and code examples from crawled content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=LLMContentFilter(\n            provider=\"openai/gpt-4o\",\n            instruction=\"Extract technical documentation and code examples\"\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Using CSS Selector for Content Selection with Crawl4AI\nDESCRIPTION: This example demonstrates how to limit crawl results to a specific region of the page using the css_selector parameter in CrawlerRunConfig. The selector targets the first 30 items from Hacker News.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # e.g., first 30 items from Hacker News\n        css_selector=\".athing:nth-child(-n+30)\"  \n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com/newest\", \n            config=config\n        )\n        print(\"Partial HTML length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Configuring Geolocation and Locale for Crawl4AI in Python\nDESCRIPTION: This Python code snippet shows how to configure the `CrawlerRunConfig` in `crawl4ai` to simulate specific user location and locale settings. It sets the `locale` (for Accept-Language header and UI), `timezone_id` (for JavaScript Date/Intl objects), and `geolocation` (overriding GPS coordinates with latitude, longitude, and accuracy) to fetch locale-specific content from a target URL.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ncrun_cfg = CrawlerRunConfig(\n    url=\"https://browserleaks.com/geo\",          # test page that shows your location\n    locale=\"en-US\",                              # Accept-Language & UI locale\n    timezone_id=\"America/Los_Angeles\",           # JS Date()/Intl timezone\n    geolocation=GeolocationConfig(                 # override GPS coords\n        latitude=34.0522,\n        longitude=-118.2437,\n        accuracy=10.0,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Running Crawl4AI setup command\nDESCRIPTION: Executes the setup command after installation. This installs or updates required Playwright browsers, performs OS-level checks, and confirms the environment is ready for crawling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncrawl4ai-setup\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM Extraction Strategy for Crawl4AI\nDESCRIPTION: Shows how to set up an LLM (Language Model) extraction strategy in Crawl4AI, including instructions, provider details, and schema definition for extracted content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_60\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"crawler_config\": {\n    \"type\": \"CrawlerRunConfig\",\n    \"params\": {\n      \"extraction_strategy\": {\n        \"type\": \"LLMExtractionStrategy\",\n        \"params\": {\n          \"instruction\": \"Extract article title, author, publication date and main content\",\n          \"provider\": \"openai/gpt-4\",\n          \"api_token\": \"your-api-token\",\n          \"schema\": {\n            \"type\": \"dict\",\n            \"value\": {\n              \"title\": \"Article Schema\",\n              \"type\": \"object\",\n              \"properties\": {\n                \"title\": {\n                  \"type\": \"string\",\n                  \"description\": \"The article's headline\"\n                },\n                \"author\": {\n                  \"type\": \"string\",\n                  \"description\": \"The author's name\"\n                },\n                \"published_date\": {\n                  \"type\": \"string\",\n                  \"format\": \"date-time\",\n                  \"description\": \"Publication date and time\"\n                },\n                \"content\": {\n                  \"type\": \"string\",\n                  \"description\": \"The main article content\"\n                }\n              },\n              \"required\": [\"title\", \"content\"]\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Non-Streaming Deep Crawl with Crawl4AI in Python\nDESCRIPTION: Shows how to run a deep crawl in non-streaming mode using `AsyncWebCrawler`. The `CrawlerRunConfig` is configured with a `BFSDeepCrawlStrategy` and explicitly sets `stream=False` (which is the default). In this mode, the `crawler.arun` method blocks until the entire crawl is complete and returns all results as a single list. This is suitable when the full dataset is needed before processing. Requires `AsyncWebCrawler`, `CrawlerRunConfig`, and a deep crawl strategy (e.g., `BFSDeepCrawlStrategy`). Assumes `process_result` function is defined elsewhere.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),\n    stream=False  # Default behavior\n)\n\nasync with AsyncWebCrawler() as crawler:\n    # Wait for ALL results to be collected before returning\n    results = await crawler.arun(\"https://example.com\", config=config)\n    \n    for result in results:\n        process_result(result)\n```\n\n----------------------------------------\n\nTITLE: Response Capture Handler Implementation - Python\nDESCRIPTION: Implements response capture functionality for tracking HTTP responses, including status codes, headers, and timing information. Includes error handling and security considerations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/prompts/prompt_net_requests.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync def handle_response_capture(response):\n    try:\n        captured_requests.append({\n            \"event_type\": \"response\",\n            \"url\": response.url,\n            \"status\": response.status,\n            \"status_text\": response.status_text,\n            \"headers\": dict(response.headers),\n            \"from_service_worker\": response.from_service_worker,\n            \"request_timing\": response.request.timing,\n            \"timestamp\": time.time()\n        })\n    except Exception as e:\n        self.logger.warning(f\"Error capturing response details for {response.url}: {e}\", tag=\"CAPTURE\")\n        captured_requests.append({\"event_type\": \"response_capture_error\", \"url\": response.url, \"error\": str(e), \"timestamp\": time.time()})\n```\n\n----------------------------------------\n\nTITLE: Running Crawl4AI Docker Container with Various Configurations\nDESCRIPTION: Different methods for running the Crawl4AI Docker container, including basic run, with LLM support, and with host environment variables.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_47\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 8000:8000 --name crawl4ai crawl4ai\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 8000:8000 \\\n  --env-file .llm.env \\\n  --name crawl4ai \\\n  crawl4ai\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d -p 8000:8000 \\\n  --env-file .llm.env \\\n  --env \"$(env)\" \\\n  --name crawl4ai \\\n  crawl4ai\n```\n\n----------------------------------------\n\nTITLE: Configuring HTML Cleanup with CrawlerRunConfig\nDESCRIPTION: This example shows how to use CrawlerRunConfig to specify cleanup parameters that result in a sanitized HTML output. It demonstrates setting excluded tags and disabling data attributes preservation.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    excluded_tags=[\"form\", \"header\", \"footer\"],\n    keep_data_attributes=False\n)\nresult = await crawler.arun(\"https://example.com\", config=config)\nprint(result.cleaned_html)  # Freed of forms, header, footer, data-* attributes\n```\n\n----------------------------------------\n\nTITLE: Configuring CrawlerRunConfig for Content Processing\nDESCRIPTION: This example shows how to set up a CrawlerRunConfig object to control crawl behavior, including waiting for specific page elements, setting word count thresholds for content filtering, excluding specific HTML tags, and configuring link handling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nrun_cfg = CrawlerRunConfig(\n    wait_for=\"css:.main-content\",\n    word_count_threshold=15,\n    excluded_tags=[\"nav\", \"footer\"],\n    exclude_external_links=True,\n    stream=True,  # Enable streaming for arun_many()\n)\n```\n\n----------------------------------------\n\nTITLE: Performing a simple crawl with Crawl4AI in Python\nDESCRIPTION: A minimal Python script demonstrating a basic web crawl using Crawl4AI. It utilizes BrowserConfig and CrawlerRunConfig to crawl example.com and print the first 300 characters of extracted text in markdown format.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.example.com\",\n        )\n        print(result.markdown[:300])  # Show the first 300 characters of extracted text\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Basic Browser Configuration Commands\nDESCRIPTION: CLI commands to configure default browser behavior including headless mode and user agent settings\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_40\n\nLANGUAGE: bash\nCODE:\n```\ncrwl config set BROWSER_HEADLESS false\ncrwl config set USER_AGENT_MODE random\n```\n\n----------------------------------------\n\nTITLE: Running Crawl4AI in Docker\nDESCRIPTION: Commands to pull and run the experimental Docker image for Crawl4AI.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_86\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull unclecode/crawl4ai:basic\ndocker run -p 11235:11235 unclecode/crawl4ai:basic\n```\n\n----------------------------------------\n\nTITLE: Importing and Configuring Asyncio and Nest_asyncio\nDESCRIPTION: This snippet imports the necessary modules and applies nest_asyncio to allow for nested event loops, which is useful in environments like Jupyter notebooks.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport nest_asyncio\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Running Crawl4AI Diagnostics\nDESCRIPTION: Command to run diagnostics that verify the environment is properly configured for Crawl4AI.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_80\n\nLANGUAGE: bash\nCODE:\n```\ncrawl4ai-doctor\n```\n\n----------------------------------------\n\nTITLE: Token Usage Reporting - Python Implementation\nDESCRIPTION: Generates and displays a detailed token usage report showing total and per-request usage statistics. Formats output in a tabular format with completion, prompt, and total token counts.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_73\n\nLANGUAGE: python\nCODE:\n```\ndef show_usage(self) -> None:\n    \"\"\"Print a detailed token usage report showing total and per-request usage.\"\"\"\n    print(\"\\n=== Token Usage Summary ===\")\n    print(f\"{'Type':<15} {'Count':>12}\")\n    print(\"-\" * 30)\n    print(f\"{'Completion':<15} {self.total_usage.completion_tokens:>12,}\")\n    print(f\"{'Prompt':<15} {self.total_usage.prompt_tokens:>12,}\")\n    print(f\"{'Total':<15} {self.total_usage.total_tokens:>12,}\")\n\n    print(\"\\n=== Usage History ===\")\n    print(f\"{'Request #':<10} {'Completion':>12} {'Prompt':>12} {'Total':>12}\")\n    print(\"-\" * 48)\n    for i, usage in enumerate(self.usages, 1):\n        print(\n            f\"{i:<10} {usage.completion_tokens:>12,} {usage.prompt_tokens:>12,} {usage.total_tokens:>12,}\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Embedding AI Assistant Iframe in HTML\nDESCRIPTION: Creates a container div and embeds an iframe for the Crawl4AI Assistant. The iframe source is set to a relative path and styled for full width display.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/ask-ai.md#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"ask-ai-container\">\n<iframe id=\"ask-ai-frame\" src=\"../../ask_ai/index.html\" width=\"100%\" style=\"border:none; display: block;\" title=\"Crawl4AI Assistant\"></iframe>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Session-Based Crawling with Crawl4AI\nDESCRIPTION: This snippet demonstrates session-based crawling using Crawl4AI. It shows how to navigate through multiple pages while maintaining the same session context, which is useful for scenarios involving sequential actions or multi-page content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync def multi_page_session_crawl():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"page_navigation_session\"\n        url = \"https://example.com/paged-content\"\n\n        for page_number in range(1, 4):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.next-page-button').click();\" if page_number > 1 else None,\n                css_selector=\".content-section\",\n                bypass_cache=True\n            )\n            print(f\"Page {page_number} Content:\")\n            print(result.markdown.raw_markdown[:500])  # Print first 500 characters\n\n# asyncio.run(multi_page_session_crawl())\n```\n\n----------------------------------------\n\nTITLE: Selecting HTML Source for Markdown Generation\nDESCRIPTION: This example shows how to control which HTML content is used as input for markdown generation using the content_source parameter. It demonstrates three options: raw HTML, cleaned HTML, and preprocessed HTML optimized for schema extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_105\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Option 1: Use the raw HTML directly from the webpage (before any processing)\n    raw_md_generator = DefaultMarkdownGenerator(\n        content_source=\"raw_html\",\n        options={\"ignore_links\": True}\n    )\n    \n    # Option 2: Use the cleaned HTML (after scraping strategy processing - default)\n    cleaned_md_generator = DefaultMarkdownGenerator(\n        content_source=\"cleaned_html\",  # This is the default\n        options={\"ignore_links\": True}\n    )\n    \n    # Option 3: Use preprocessed HTML optimized for schema extraction\n    fit_md_generator = DefaultMarkdownGenerator(\n        content_source=\"fit_html\",\n        options={\"ignore_links\": True}\n    )\n    \n    # Use one of the generators in your crawler config\n    config = CrawlerRunConfig(\n        markdown_generator=raw_md_generator  # Try each of the generators\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        if result.success:\n            print(\"Markdown:\\n\", result.markdown.raw_markdown[:500])\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Initializing BrowserConfig in crawl4ai\nDESCRIPTION: This code demonstrates how to create a BrowserConfig object to control browser behavior including browser type, headless mode, viewport dimensions, proxy settings, and user agent specification.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_cfg = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    viewport_width=1280,\n    viewport_height=720,\n    proxy=\"http://user:pass@proxy:8080\",\n    user_agent=\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running Predefined Benchmark Configurations with Python\nDESCRIPTION: Examples of running various predefined test configurations using the run_benchmark.py script, from quick tests to extreme stress tests with different parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/tests/memory/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Quick test (50 URLs, 4 max sessions)\npython run_benchmark.py quick\n\n# Medium test (500 URLs, 16 max sessions)\npython run_benchmark.py medium\n\n# Large test (1000 URLs, 32 max sessions)\npython run_benchmark.py large\n\n# Extreme test (2000 URLs, 64 max sessions)\npython run_benchmark.py extreme\n\n# Custom configuration\npython run_benchmark.py custom --urls 300 --max-sessions 24 --chunk-size 50\n\n# Run 'small' test in streaming mode\npython run_benchmark.py small --stream\n\n# Override max_sessions for the 'medium' config\npython run_benchmark.py medium --max-sessions 20\n\n# Skip benchmark report generation after the test\npython run_benchmark.py small --no-report\n\n# Clean up reports and site files before running\npython run_benchmark.py medium --clean\n```\n\n----------------------------------------\n\nTITLE: Structured Data Extraction with LLM Command for Crawl4AI\nDESCRIPTION: Shows how to extract structured data using LLM-based extraction with configuration and schema files.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncrwl https://example.com \\\n    -e extract_llm.yml \\\n    -s llm_schema.json \\\n    -o json\n```\n\n----------------------------------------\n\nTITLE: Determining Browser Executable Path by Operating System in Python\nDESCRIPTION: A function that returns the browser executable path based on the operating system (macOS, Windows, Linux) and browser type (Chromium, Firefox, WebKit). It maps each browser to its standard installation path on different platforms.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_113\n\nLANGUAGE: python\nCODE:\n```\ndef _get_browser_path_WIP(self) -> str:\n    \"\"\"Returns the browser executable path based on OS and browser type\"\"\"\n    if sys.platform == \"darwin\":  # macOS\n        paths = {\n            \"chromium\": \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\",\n            \"firefox\": \"/Applications/Firefox.app/Contents/MacOS/firefox\",\n            \"webkit\": \"/Applications/Safari.app/Contents/MacOS/Safari\",\n        }\n    elif sys.platform == \"win32\":  # Windows\n        paths = {\n            \"chromium\": \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\",\n            \"firefox\": \"C:\\\\Program Files\\\\Mozilla Firefox\\\\firefox.exe\",\n            \"webkit\": None,  # WebKit not supported on Windows\n        }\n    else:  # Linux\n        paths = {\n            \"chromium\": \"google-chrome\",\n            \"firefox\": \"firefox\",\n            \"webkit\": None,  # WebKit not supported on Linux\n        }\n\n    return paths.get(self.browser_type)\n```\n\n----------------------------------------\n\nTITLE: Crawling Raw HTML Content with Crawl4AI\nDESCRIPTION: Illustrates how to use Crawl4AI to process raw HTML strings directly by prefixing the HTML content with 'raw:'. This is useful when you already have HTML content in memory and don't need to fetch it from a URL or file.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_102\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def crawl_raw_html():\n    raw_html = \"<html><body><h1>Hello, World!</h1></body></html>\"\n    raw_html_url = f\"raw:{raw_html}\"\n    config = CrawlerRunConfig(bypass_cache=True)\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=raw_html_url, config=config)\n        if result.success:\n            print(\"Markdown Content from Raw HTML:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl raw HTML: {result.error_message}\")\n\nasyncio.run(crawl_raw_html())\n```\n\n----------------------------------------\n\nTITLE: Running Crawler and Accessing CrawlResult Fields in Python\nDESCRIPTION: This snippet demonstrates how to run the crawler asynchronously and access various fields of the CrawlResult object, including status code, response headers, links, markdown content, and extracted structured content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresult = await crawler.arun(url=\"https://example.com\", config=some_config)\n\nif result.success:\n    print(result.status_code, result.response_headers)\n    print(\"Links found:\", len(result.links.get(\"internal\", [])))\n    if result.markdown:\n        print(\"Markdown snippet:\", result.markdown.raw_markdown[:200])\n    if result.extracted_content:\n        print(\"Structured JSON:\", result.extracted_content)\nelse:\n    print(\"Error:\", result.error_message)\n```\n\n----------------------------------------\n\nTITLE: Implementing Robots.txt Compliance in Python with Crawl4AI\nDESCRIPTION: Shows how to configure a web crawler to respect robots.txt rules for ethical and legal web crawling. The code sets up an AsyncWebCrawler with robots.txt checking enabled and processes the results to handle cases where crawling is blocked by robots.txt rules.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_149\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    urls = [\n        \"https://example1.com\",\n        \"https://example2.com\",\n        \"https://example3.com\"\n    ]\n    \n    config = CrawlerRunConfig(\n        cache_mode=CacheMode.ENABLED,\n        check_robots_txt=True,  # Will respect robots.txt for each URL\n        semaphore_count=3      # Max concurrent requests\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        async for result in crawler.arun_many(urls, config=config):\n            if result.success:\n                print(f\"Successfully crawled {result.url}\")\n            elif result.status_code == 403 and \"robots.txt\" in result.error_message:\n                print(f\"Skipped {result.url} - blocked by robots.txt\")\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Filtering Documents Using Embeddings and Cosine Similarity in Python\nDESCRIPTION: Implements a method to filter and sort documents based on their cosine similarity to a semantic filter. It uses embeddings to calculate similarity and returns a filtered list of documents.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_60\n\nLANGUAGE: python\nCODE:\n```\ndef filter_documents_embeddings(\n        self, documents: List[str], semantic_filter: str, at_least_k: int = 20\n    ) -> List[str]:\n        if not semantic_filter:\n            return documents\n\n        if len(documents) < at_least_k:\n            at_least_k = len(documents) // 2\n\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        query_embedding = self.get_embeddings([semantic_filter])[0]\n        document_embeddings = self.get_embeddings(documents)\n        similarities = cosine_similarity(\n            [query_embedding], document_embeddings\n        ).flatten()\n\n        filtered_docs = [\n            (doc, sim)\n            for doc, sim in zip(documents, similarities)\n            if sim >= self.sim_threshold\n        ]\n\n        if len(filtered_docs) < at_least_k:\n            remaining_docs = [\n                (doc, sim)\n                for doc, sim in zip(documents, similarities)\n                if sim < self.sim_threshold\n            ]\n            remaining_docs.sort(key=lambda x: x[1], reverse=True)\n            filtered_docs.extend(remaining_docs[: at_least_k - len(filtered_docs)])\n\n        filtered_docs = [doc for doc, _ in filtered_docs]\n\n        return filtered_docs[:at_least_k]\n```\n\n----------------------------------------\n\nTITLE: Implementing Robots.txt Compliant Crawler in Python\nDESCRIPTION: Demonstrates how to implement a crawler that respects robots.txt rules with error handling and status reporting.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    urls = [\n        \"https://example1.com\",\n        \"https://example2.com\",\n        \"https://example3.com\"\n    ]\n    \n    config = CrawlerRunConfig(\n        cache_mode=CacheMode.ENABLED,\n        check_robots_txt=True,  # Will respect robots.txt for each URL\n        semaphore_count=3      # Max concurrent requests\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        async for result in crawler.arun_many(urls, config=config):\n            if result.success:\n                print(f\"Successfully crawled {result.url}\")\n            elif result.status_code == 403 and \"robots.txt\" in result.error_message:\n                print(f\"Skipped {result.url} - blocked by robots.txt\")\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Advanced Web Crawler Implementation\nDESCRIPTION: Complete implementation of an advanced web crawler combining multiple techniques including filtering, scoring, and sophisticated configuration options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.filters import (\n    FilterChain,\n    DomainFilter,\n    URLPatternFilter,\n    ContentTypeFilter\n)\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n\nasync def run_advanced_crawler():\n    # Create a sophisticated filter chain\n    filter_chain = FilterChain([\n        # Domain boundaries\n        DomainFilter(\n            allowed_domains=[\"docs.example.com\"],\n            blocked_domains=[\"old.docs.example.com\"]\n        ),\n        \n        # URL patterns to include\n        URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\", \"*blog*\"]),\n        \n        # Content type filtering\n        ContentTypeFilter(allowed_types=[\"text/html\"])\n    ])\n\n    # Create a relevance scorer\n    keyword_scorer = KeywordRelevanceScorer(\n        keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n        weight=0.7\n    )\n\n    # Set up the configuration\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BestFirstCrawlingStrategy(\n            max_depth=2,\n            include_external=False,\n            filter_chain=filter_chain,\n            url_scorer=keyword_scorer\n        ),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        stream=True,\n        verbose=True\n    )\n\n    # Execute the crawl\n    results = []\n    async with AsyncWebCrawler() as crawler:\n        async for result in await crawler.arun(\"https://docs.example.com\", config=config):\n            results.append(result)\n            score = result.metadata.get(\"score\", 0)\n            depth = result.metadata.get(\"depth\", 0)\n            print(f\"Depth: {depth} | Score: {score:.2f} | {result.url}\")\n\n    # Analyze the results\n    print(f\"Crawled {len(results)} high-value pages\")\n    print(f\"Average score: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}\")\n\n    # Group by depth\n    depth_counts = {}\n    for result in results:\n        depth = result.metadata.get(\"depth\", 0)\n        depth_counts[depth] = depth_counts.get(depth, 0) + 1\n\n    print(\"Pages crawled by depth:\")\n    for depth, count in sorted(depth_counts.items()):\n        print(f\"  Depth {depth}: {count} pages\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_advanced_crawler())\n```\n\n----------------------------------------\n\nTITLE: Implementing Best-First Crawling Strategy with Keyword Scoring in Crawl4AI\nDESCRIPTION: This snippet shows how to use the BestFirstCrawlingStrategy with a KeywordRelevanceScorer to prioritize crawling pages that are most relevant to specified keywords. This intelligent approach focuses crawl resources on the most valuable content first.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n\n# Create a scorer\nscorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n    weight=0.7\n)\n\n# Configure the strategy\nstrategy = BestFirstCrawlingStrategy(\n    max_depth=2,\n    include_external=False,\n    url_scorer=scorer,\n    max_pages=25,              # Maximum number of pages to crawl (optional)\n)\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Crawl4AI Example\nDESCRIPTION: Complete example demonstrating multiple configuration options and extraction strategies in Crawl4AI.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    # Example schema\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"article.post\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\",  \"selector\": \"a\",  \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n\n    run_config = CrawlerRunConfig(\n        # Core\n        verbose=True,\n        cache_mode=CacheMode.ENABLED,\n        check_robots_txt=True,   # Respect robots.txt rules\n        \n        # Content\n        word_count_threshold=10,\n        css_selector=\"main.content\",\n        excluded_tags=[\"nav\", \"footer\"],\n        exclude_external_links=True,\n        \n        # Page & JS\n        js_code=\"document.querySelector('.show-more')?.click();\",\n        wait_for=\"css:.loaded-block\",\n        page_timeout=30000,\n        \n        # Extraction\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        \n        # Session\n        session_id=\"persistent_session\",\n        \n        # Media\n        screenshot=True,\n        pdf=True,\n        \n        # Anti-bot\n        simulate_user=True,\n        magic=True,\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/posts\", config=run_config)\n        if result.success:\n            print(\"HTML length:\", len(result.cleaned_html))\n            print(\"Extraction JSON:\", result.extracted_content)\n            if result.screenshot:\n                print(\"Screenshot length:\", len(result.screenshot))\n            if result.pdf:\n                print(\"PDF bytes length:\", len(result.pdf))\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Two-Pass Content Filtering with Crawl4AI\nDESCRIPTION: Demonstrates a two-pass approach to content filtering using PruningContentFilter and BM25ContentFilter. First crawls a webpage to get raw HTML, then applies pruning filter to remove noise, and finally uses BM25 to filter content based on relevance to a query.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_113\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter\nfrom bs4 import BeautifulSoup\n\nasync def main():\n    # 1. Crawl with minimal or no markdown generator, just get raw HTML\n    config = CrawlerRunConfig(\n        # If you only want raw HTML, you can skip passing a markdown_generator\n        # or provide one but focus on .html in this example\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/tech-article\", config=config)\n\n        if not result.success or not result.html:\n            print(\"Crawl failed or no HTML content.\")\n            return\n        \n        raw_html = result.html\n        \n        # 2. First pass: PruningContentFilter on raw HTML\n        pruning_filter = PruningContentFilter(threshold=0.5, min_word_threshold=50)\n        \n        # filter_content returns a list of \"text chunks\" or cleaned HTML sections\n        pruned_chunks = pruning_filter.filter_content(raw_html)\n        # This list is basically pruned content blocks, presumably in HTML or text form\n        \n        # For demonstration, let's combine these chunks back into a single HTML-like string\n        # or you could do further processing. It's up to your pipeline design.\n        pruned_html = \"\\n\".join(pruned_chunks)\n        \n        # 3. Second pass: BM25ContentFilter with a user query\n        bm25_filter = BM25ContentFilter(\n            user_query=\"machine learning\",\n            bm25_threshold=1.2,\n            language=\"english\"\n        )\n        \n        # returns a list of text chunks\n        bm25_chunks = bm25_filter.filter_content(pruned_html)  \n        \n        if not bm25_chunks:\n            print(\"Nothing matched the BM25 query after pruning.\")\n            return\n        \n        # 4. Combine or display final results\n        final_text = \"\\n---\\n\".join(bm25_chunks)\n        \n        print(\"==== PRUNED OUTPUT (first pass) ====\")\n        print(pruned_html[:500], \"... (truncated)\")  # preview\n\n        print(\"\\n==== BM25 OUTPUT (second pass) ====\")\n        print(final_text[:500], \"... (truncated)\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Managing Browser Profiles with BrowserProfiler\nDESCRIPTION: Example of using the BrowserProfiler class to create, list, and manage browser profiles.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_140\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import BrowserProfiler\n\nasync def manage_profiles():\n    # Create a profiler instance\n    profiler = BrowserProfiler()\n    \n    # Create a profile interactively - opens a browser window\n    profile_path = await profiler.create_profile(\n        profile_name=\"my-login-profile\"  # Optional: name your profile\n    )\n    \n    print(f\"Profile saved at: {profile_path}\")\n    \n    # List all available profiles\n    profiles = profiler.list_profiles()\n    \n    for profile in profiles:\n        print(f\"Profile: {profile['name']}\")\n        print(f\"  Path: {profile['path']}\")\n        print(f\"  Created: {profile['created']}\")\n        print(f\"  Browser type: {profile['type']}\")\n    \n    # Get a specific profile path by name\n    specific_profile = profiler.get_profile_path(\"my-login-profile\")\n    \n    # Delete a profile when no longer needed\n    success = profiler.delete_profile(\"old-profile-name\")\n    \nasyncio.run(manage_profiles())\n```\n\n----------------------------------------\n\nTITLE: Executing Step-by-Step Button Clicks with Crawl4AI in Python\nDESCRIPTION: This snippet demonstrates how to use Crawl4AI to load a page, click a 'Next' button, and wait for new content to load. It uses a session ID to maintain state across multiple arun() calls.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/tutorial_dynamic_clicks.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CacheMode\n\njs_code = [\n    # This JS finds the \"Next\" button and clicks it\n    \"const nextButton = document.querySelector('button.next'); nextButton && nextButton.click();\"\n]\n\nwait_for_condition = \"css:.new-content-class\"\n\nasync with AsyncWebCrawler(headless=True, verbose=True) as crawler:\n    # 1. Load the initial page\n    result_initial = await crawler.arun(\n        url=\"https://example.com\",\n        cache_mode=CacheMode.BYPASS,\n        session_id=\"my_session\"\n    )\n\n    # 2. Click the 'Next' button and wait for new content\n    result_next = await crawler.arun(\n        url=\"https://example.com\",\n        session_id=\"my_session\",\n        js_code=js_code,\n        wait_for=wait_for_condition,\n        js_only=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n# `result_next` now contains the updated HTML after clicking 'Next'\n```\n\n----------------------------------------\n\nTITLE: Implementing Real-Time Monitoring for Web Crawling in Python\nDESCRIPTION: Demonstrates how to initialize and configure the CrawlMonitor component to track system resources, active crawls, and performance metrics. Shows both the setup code and an example of the CLI output format.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.monitor import CrawlMonitor\n\n# Initialize monitoring\nmonitor = CrawlMonitor()\n\n# Start monitoring with CLI interface\nawait monitor.start(\n    mode=\"cli\",  # or \"gui\"\n    refresh_rate=\"1s\",\n    metrics={\n        \"resources\": [\"cpu\", \"memory\", \"network\"],\n        \"crawls\": [\"active\", \"queued\", \"completed\"],\n        \"performance\": [\"success_rate\", \"response_times\"]\n    }\n)\n\n# Example CLI output:\n\"\"\"\nCrawl4AI Monitor (Live) - Press Q to exit\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSystem Usage:\n â”œâ”€ CPU: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 70%\n â””â”€ Memory: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 2.1GB/8GB\n\nActive Crawls:\nID    URL                   Status    Progress\n001   docs.example.com     ðŸŸ¢ Active   75%\n002   api.service.com      ðŸŸ¡ Queue    -\n\nMetrics (Last 5min):\n â”œâ”€ Success Rate: 98%\n â”œâ”€ Avg Response: 0.6s\n â””â”€ Pages/sec: 8.5\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Processing in Python with AsyncWebCrawler\nDESCRIPTION: This snippet shows how to use AsyncWebCrawler for batch processing of URLs, utilizing MemoryAdaptiveDispatcher and CrawlerMonitor for efficient crawling and monitoring.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_146\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl_batch():\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=False  # Default: get all results at once\n    )\n    \n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=10,\n        monitor=CrawlerMonitor(\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        # Get all results at once\n        results = await crawler.arun_many(\n            urls=urls,\n            config=run_config,\n            dispatcher=dispatcher\n        )\n        \n        # Process all results after completion\n        for result in results:\n            if result.success:\n                await process_result(result)\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Ask-AI Integration with Iframe and DOM Manipulation\nDESCRIPTION: HTML, JavaScript, and CSS setup for embedding an AI assistant iframe with dynamic resizing and DOM manipulation. Handles responsive iframe height adjustment, footer removal, and styling for seamless integration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<div class=\"ask-ai-container\">\n<iframe id=\"ask-ai-frame\" src=\"../../ask_ai/index.html\" width=\"100%\" style=\"border:none; display: block;\" title=\"Crawl4AI Assistant\"></iframe>\n</div>\n\n<script>\n// Iframe height adjustment\nfunction resizeAskAiIframe() {\n  const iframe = document.getElementById('ask-ai-frame');\n  if (iframe) {\n    const headerHeight = parseFloat(getComputedStyle(document.documentElement).getPropertyValue('--header-height') || '55');\n    // Footer is removed by JS below, so calculate height based on header + small buffer\n    const topOffset = headerHeight + 20; // Header + buffer/margin\n\n    const availableHeight = window.innerHeight - topOffset;\n    iframe.style.height = Math.max(600, availableHeight) + 'px'; // Min height 600px\n  }\n}\n\n// Run immediately and on resize/load\nresizeAskAiIframe(); // Initial call\nlet resizeTimer;\nwindow.addEventListener('load', resizeAskAiIframe);\nwindow.addEventListener('resize', () => {\n    clearTimeout(resizeTimer);\n    resizeTimer = setTimeout(resizeAskAiIframe, 150);\n});\n\n// Remove Footer & HR from parent page (DOM Ready might be safer)\ndocument.addEventListener('DOMContentLoaded', () => {\n    setTimeout(() => { // Add slight delay just in case elements render slowly\n        const footer = window.parent.document.querySelector('footer'); // Target parent document\n        if (footer) {\n            const hrBeforeFooter = footer.previousElementSibling;\n            if (hrBeforeFooter && hrBeforeFooter.tagName === 'HR') {\n                hrBeforeFooter.remove();\n            }\n            footer.remove();\n            // Trigger resize again after removing footer\n            resizeAskAiIframe();\n        } else {\n             console.warn(\"Ask AI Page: Could not find footer in parent document to remove.\");\n        }\n    }, 100); // Shorter delay\n});\n</script>\n\n<style>\n#terminal-mkdocs-main-content {\n    padding: 0 !important;\n    margin: 0;\n    width: 100%;\n    height: 100%;\n    overflow: hidden; /* Prevent body scrollbars, panels handle scroll */\n}\n\n/* Ensure iframe container takes full space */\n#terminal-mkdocs-main-content .ask-ai-container {\n    /* Remove negative margins if footer removal handles space */\n     margin: 0;\n    padding: 0;\n    max-width: none;\n    /* Let the JS set the height */\n    /* height: 600px; Initial fallback height */\n    overflow: hidden; /* Hide potential overflow before JS resize */\n}\n\n/* Hide title/paragraph if they were part of the markdown */\n/* Alternatively, just remove them from the .md file directly */\n/* #terminal-mkdocs-main-content > h1,\n#terminal-mkdocs-main-content > p:first-of-type {\n    display: none;\n} */\n\n</style>\n```\n\n----------------------------------------\n\nTITLE: Authenticating and Using JWT Tokens with Crawl4AI in Python\nDESCRIPTION: This Python code demonstrates how to authenticate and use JWT tokens with the Crawl4AI Docker client, including making authenticated requests.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_66\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.docker_client import Crawl4aiDockerClient\n\nasync with Crawl4aiDockerClient() as client:\n    # Authenticate first\n    await client.authenticate(\"user@example.com\")\n    \n    # Now all requests will include the token automatically\n    result = await client.crawl(urls=[\"https://example.com\"])\n```\n\n----------------------------------------\n\nTITLE: Extracting Article Data with CSS Selectors and JSON Extraction Strategy in Python\nDESCRIPTION: This function demonstrates how to use CSS selection, exclusion logic, and a JSON-based extraction strategy to extract article data from a webpage. It uses AsyncWebCrawler with custom configuration to filter content and extract structured data.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_main_articles(url: str):\n    schema = {\n        \"name\": \"ArticleBlock\",\n        \"baseSelector\": \"div.article-block\",\n        \"fields\": [\n            {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n            {\n                \"name\": \"metadata\",\n                \"type\": \"nested\",\n                \"fields\": [\n                    {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                    {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n                ]\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        # Keep only #main-content\n        css_selector=\"#main-content\",\n        \n        # Filtering\n        word_count_threshold=10,\n        excluded_tags=[\"nav\", \"footer\"],  \n        exclude_external_links=True,\n        exclude_domains=[\"somebadsite.com\"],\n        exclude_external_images=True,\n\n        # Extraction\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        \n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url, config=config)\n        if not result.success:\n            print(f\"Error: {result.error_message}\")\n            return None\n        return json.loads(result.extracted_content)\n\nasync def main():\n    articles = await extract_main_articles(\"https://news.ycombinator.com/newest\")\n    if articles:\n        print(\"Extracted Articles:\", articles[:2])  # Show first 2\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Console Message Capture Implementation - Python\nDESCRIPTION: Implements console message capture functionality for tracking browser console output, including message type, arguments, and source location. Handles complex JavaScript objects and includes error handling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/prompts/prompt_net_requests.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef handle_console_capture(msg):\n    try:\n        location = msg.location()\n        resolved_args = []\n        try:\n            for arg in msg.args:\n                resolved_args.append(arg.json_value())\n        except Exception:\n            resolved_args.append(\"[Could not resolve JSHandle args]\")\n\n        captured_console.append({\n            \"type\": msg.type(),\n            \"text\": msg.text(),\n            \"args\": resolved_args,\n            \"location\": f\"{location['url']}:{location['lineNumber']}:{location['columnNumber']}\" if location else \"N/A\",\n            \"timestamp\": time.time()\n        })\n    except Exception as e:\n        self.logger.warning(f\"Error capturing console message: {e}\", tag=\"CAPTURE\")\n        captured_console.append({\"type\": \"console_capture_error\", \"error\": str(e), \"timestamp\": time.time()})\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Headers with AsyncWebCrawler\nDESCRIPTION: This example shows two different methods for setting custom headers when using AsyncWebCrawler. The first approach updates headers at the crawler strategy level, while the second passes headers directly to the arun() method.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_133\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Option 1: Set headers at the crawler strategy level\n    crawler1 = AsyncWebCrawler(\n        # The underlying strategy can accept headers in its constructor\n        crawler_strategy=None  # We'll override below for clarity\n    )\n    crawler1.crawler_strategy.update_user_agent(\"MyCustomUA/1.0\")\n    crawler1.crawler_strategy.set_custom_headers({\n        \"Accept-Language\": \"fr-FR,fr;q=0.9\"\n    })\n    result1 = await crawler1.arun(\"https://www.example.com\")\n    print(\"Example 1 result success:\", result1.success)\n\n    # Option 2: Pass headers directly to `arun()`\n    crawler2 = AsyncWebCrawler()\n    result2 = await crawler2.arun(\n        url=\"https://www.example.com\",\n        headers={\"Accept-Language\": \"es-ES,es;q=0.9\"}\n    )\n    print(\"Example 2 result success:\", result2.success)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Defining System Message for Schema Generation\nDESCRIPTION: This code snippet defines the system message that provides instructions for generating structured documentation from code snippets. It includes requirements for file-level analysis, snippet analysis, title guidelines, description requirements, and code formatting.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_75\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"role\": \"system\",\n  \"content\": \"Generate structured documentation for each code snippet in the provided text. \\n          Analyze both the overall file context and individual snippets.\\n          If there are no code snippets, return an empty array. Do not return any other text. \\n          The response should be always a valid JSON object with properly escaped strings.\\n\\n          If the content is not English, then throw an error.\\n\\n          Do not skip or miss any code snippet, all code snippets must be included.\\n\\n1. File-Level Analysis:\\n - Create a concise title that describes the overall purpose of the file\\n - Write a summary of what the file contains and its intended use\\n - Identify common patterns or relationships between snippets\\n\\n2. Snippet Analysis Requirements:\\n - Create separate entries for each unique combination of functionality and programming language\\n - Include all necessary context from surrounding text and comments\\n - Capture the purpose, dependencies, and key functionality of each snippet\\n\\n3. Title Guidelines:\\n - Start with the core action/operation (e.g., \\\"Initializing\\\", \\\"Querying\\\", \\\"Inserting\\\")\\n - Include the main technology/framework\\n - Specify the programming language\\n - Keep titles concise but descriptive (max 100 characters)\\n\\n4. Description Requirements:\\n - Explain the purpose and functionality in detail\\n - List any required dependencies or prerequisites\\n - Mention key parameters and their purposes\\n - Describe expected inputs and outputs\\n - Include any relevant limitations or constraints\\n - Target length: 2-3 sentences\\n - Focus on implementation details and technical context\\n\\n5. Code Formatting:\\n - Preserve all original code formatting and comments\\n - Properly escape special characters (especially quotes and backslashes)\\n - Include language identifier in code blocks\\n - Maintain original indentation\\n - Ensure all code is wrapped in double quotes and properly escaped\\n\\nReturn the result as a JSON object following this exact schema (note: all property names must be in double quotes):\\n{\\n  \\\"page_title\\\": \\\"string\\\",      \\n  \\\"page_description\\\": \\\"string\\\",    \\n  \\\"page_summary\\\": \\\"string\\\",    \\n  \\\"languages\\\": [\\\"string\\\"],\\n  \\\"codeSnippets\\\": [\\n    {\\n      \\\"title\\\": \\\"string\\\",         \\n      \\\"description\\\": \\\"string\\\",   \\n      \\\"language\\\": \\\"string\\\",   \\n      \\\"codeList\\\": [{\\n          \\\"language\\\": \\\"string\\\",\\n          \\\"code\\\": \\\"string\\\"\\n      }]\\n    }\\n  ]\\n}\\n\\nImportant: \\n- All property names must be in double quotes\\n- All string values must be in double quotes and properly escaped\\n- Use double backslashes for escaping special characters (\\\\n, \\\\\\\", \\\\\\\\)\\n- Ensure the response is valid JSON that can be parsed by JSON.parse()\\n- If the dominant language in the content is not English, then return:\\n{\\n  \\\"error\\\": \\\"non-english-content\\\"\\n}\\n\\nJust return the JSON object, do not include any other text.\\n\\nInput text to process: \\nProject: /unclecode/crawl4ai\\nContent:\\n# What are the instructions and details for this schema generation?\\n{prompt_template}\\\"\\\"\\\"\\n        \"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CrawlerMonitor in Python for Real-time Crawling Visibility\nDESCRIPTION: This code snippet shows how to set up a CrawlerMonitor for real-time visibility into crawling operations, including options for display mode and maximum visible rows.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_143\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import CrawlerMonitor, DisplayMode\nmonitor = CrawlerMonitor(\n    # Maximum rows in live display\n    max_visible_rows=15,          \n\n    # DETAILED or AGGREGATED view\n    display_mode=DisplayMode.DETAILED  \n)\n```\n\n----------------------------------------\n\nTITLE: Working with SSL Certificate Information in Python\nDESCRIPTION: Shows how to access SSL certificate information when the fetch_ssl_certificate option is enabled in the crawler configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nif result.ssl_certificate:\n    print(\"Issuer:\", result.ssl_certificate.issuer)\n```\n\n----------------------------------------\n\nTITLE: Excluding External Images with CrawlerRunConfig (Python)\nDESCRIPTION: This snippet demonstrates how to configure `CrawlerRunConfig` to exclude images originating from external domains. Setting `exclude_external_images=True` instructs the crawler to attempt discarding images whose source URLs do not belong to the primary domain being crawled, useful for ignoring third-party content like advertisements.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_external_images=True\n)\n```\n\n----------------------------------------\n\nTITLE: Install Crawl4AI with Cosine Similarity Features (Development Mode, Bash)\nDESCRIPTION: Installs Crawl4AI in editable mode along with optional dependencies required for cosine similarity calculations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[cosine]\"          # With cosine similarity features\n```\n\n----------------------------------------\n\nTITLE: Install Crawl4AI with Transformer Features (Development Mode, Bash)\nDESCRIPTION: Installs Crawl4AI in editable mode along with optional dependencies required for Transformer model features.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[transformer]\"     # With Transformer features\n```\n\n----------------------------------------\n\nTITLE: DispatchResult Class Definition in Python for Crawl4AI\nDESCRIPTION: Defines the DispatchResult dataclass that stores information about each crawling task, including memory usage, timing, and error details. This data structure is used to track and analyze the performance of individual crawling operations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_150\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass DispatchResult:\n    task_id: str\n    memory_usage: float\n    peak_memory: float\n    start_time: datetime\n    end_time: datetime\n    error_message: str = \"\"\n```\n\n----------------------------------------\n\nTITLE: Generating LLM-based Extraction Schema\nDESCRIPTION: Demonstrates how to generate extraction schemas using LLM providers (OpenAI or Ollama) for structured data extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_124\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Generate a schema (one-time cost)\nhtml = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\"\n\n# Using OpenAI (requires API token)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")  # Required for OpenAI\n)\n\n# Or using Ollama (open source, no token needed)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(schema)\n```\n\n----------------------------------------\n\nTITLE: Optimizing CosineStrategy Performance for Large Websites\nDESCRIPTION: Shows how to configure CosineStrategy for optimal performance when processing large websites. Sets appropriate word count threshold to filter early, limits results with top_k, and enables verbose mode for monitoring.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_179\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    word_count_threshold=10,  # Filter early\n    top_k=5,                 # Limit results\n    verbose=True             # Monitor performance\n)\n```\n\n----------------------------------------\n\nTITLE: Pulling Crawl4AI Docker Images from Docker Hub (Bash)\nDESCRIPTION: Demonstrates how to pull specific release candidate or the latest stable version of the Crawl4AI Docker image from Docker Hub using the `docker pull` command. The images are multi-architecture.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Pull the release candidate (recommended for latest features)\ndocker pull unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number\n\n# Or pull the latest stable version\ndocker pull unclecode/crawl4ai:latest\n```\n\n----------------------------------------\n\nTITLE: Connecting to Browser and Setting Up Default Context in Playwright\nDESCRIPTION: An asynchronous method that initializes the Playwright instance, connects to a browser (either managed or standalone), and sets up the default context. It handles different browser types and custom configuration options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_122\n\nLANGUAGE: python\nCODE:\n```\nif self.playwright is not None:\n    await self.close()\n    \nfrom playwright.async_api import async_playwright\n\nself.playwright = await async_playwright().start()\n\nif self.config.cdp_url or self.config.use_managed_browser:\n    self.config.use_managed_browser = True\n    cdp_url = await self.managed_browser.start() if not self.config.cdp_url else self.config.cdp_url\n    self.browser = await self.playwright.chromium.connect_over_cdp(cdp_url)\n    contexts = self.browser.contexts\n    if contexts:\n        self.default_context = contexts[0]\n    else:\n        self.default_context = await self.create_browser_context()\n    await self.setup_context(self.default_context)\nelse:\n    browser_args = self._build_browser_args()\n\n    # Launch appropriate browser type\n    if self.config.browser_type == \"firefox\":\n        self.browser = await self.playwright.firefox.launch(**browser_args)\n    elif self.config.browser_type == \"webkit\":\n        self.browser = await self.playwright.webkit.launch(**browser_args)\n    else:\n        self.browser = await self.playwright.chromium.launch(**browser_args)\n\n    self.default_context = self.browser\n```\n\n----------------------------------------\n\nTITLE: Practical Example of Content Filtering in Crawl4AI\nDESCRIPTION: This example demonstrates how to combine multiple content filtering techniques, including CSS selection, word count thresholds, tag exclusions, and link/domain filtering. The crawler is configured to bypass cache for demonstration purposes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    config = CrawlerRunConfig(\n        css_selector=\"main.content\", \n        word_count_threshold=10,\n        excluded_tags=[\"nav\", \"footer\"],\n        exclude_external_links=True,\n        exclude_social_media_links=True,\n        exclude_domains=[\"ads.com\", \"spammytrackers.net\"],\n        exclude_external_images=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n        print(\"Cleaned HTML length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Creating Content Relevance Filter for Page Scoring (Python)\nDESCRIPTION: Instantiates a ContentRelevanceFilter with a semantic query and similarity threshold, then applies it to content analysis using a BM25-based metric. Incorporates this filter into a BFSDeepCrawlStrategy within a CrawlerRunConfig to include only topically relevant pages. Relies on imports from crawl4ai.deep_crawling.filters and proper setup of the Crawl4AI package.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling.filters import FilterChain, ContentRelevanceFilter\\n\\n# Create a content relevance filter\\nrelevance_filter = ContentRelevanceFilter(\\n    query=\\\"Web crawling and data extraction with Python\\\",\\n    threshold=0.7  # Minimum similarity score (0.0 to 1.0)\\n)\\n\\nconfig = CrawlerRunConfig(\\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\\n        max_depth=1,\\n        filter_chain=FilterChain([relevance_filter])\\n    )\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Link Analysis and Smart Filtering with Crawl4AI\nDESCRIPTION: This snippet shows how to perform link analysis using Crawl4AI, including options to exclude external links, social media links, and specific domains. It prints the number of internal and external links found.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def link_analysis():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            bypass_cache=True,\n            exclude_external_links=True,\n            exclude_social_media_links=True,\n            # exclude_domains=[\"facebook.com\", \"twitter.com\"]\n        )\n        print(f\"Found {len(result.links['internal'])} internal links\")\n        print(f\"Found {len(result.links['external'])} external links\")\n\n        for link in result.links['internal'][:5]:\n            print(f\"Href: {link['href']}\\nText: {link['text']}\\n\")\n                \n\nasyncio.run(link_analysis())\n```\n\n----------------------------------------\n\nTITLE: Generating Benchmark Reports from Existing Test Results\nDESCRIPTION: Commands to generate benchmark reports from previously collected test results, with options to limit the number of results or specify custom directories.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/tests/memory/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Generate a report from existing test results in ./reports/\npython benchmark_report.py\n\n# Limit to the most recent 5 test results\npython benchmark_report.py --limit 5\n\n# Specify a custom source directory for test results\npython benchmark_report.py --reports-dir alternate_results\n```\n\n----------------------------------------\n\nTITLE: Builtin Browser Management Commands\nDESCRIPTION: Commands for controlling the builtin browser instance including starting, stopping, and checking status\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_42\n\nLANGUAGE: bash\nCODE:\n```\ncrwl browser start\ncrwl browser status\ncrwl browser view --url https://example.com\ncrwl browser stop\ncrwl browser restart --browser-type chromium --port 9223 --no-headless\n```\n\n----------------------------------------\n\nTITLE: Browser Configuration in Python\nDESCRIPTION: Python code showing how to configure and use the builtin browser programmatically\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nbrowser_config = BrowserConfig(\n    browser_mode=\"builtin\", \n    headless=True\n)\n```\n\n----------------------------------------\n\nTITLE: Managing Builtin Browser via CLI in Crawl4AI\nDESCRIPTION: This snippet shows the CLI commands available for managing the builtin browser in Crawl4AI. It includes commands for starting, checking status, viewing, stopping, and restarting the browser with different settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Start the builtin browser\ncrwl browser start\n\n# Check its status\ncrwl browser status\n\n# Open a visible window to see what the browser is doing\ncrwl browser view --url https://example.com\n\n# Stop it when no longer needed\ncrwl browser stop\n\n# Restart with different settings\ncrwl browser restart --no-headless\n```\n\n----------------------------------------\n\nTITLE: Advanced Crawl4AI Usage Example with JSON-CSS Extraction\nDESCRIPTION: Shows how to crawl a webpage and extract content according to a JSON-CSS schema, outputting the results in JSON format.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncrwl \"https://www.infoq.com/ai-ml-data-eng/\" -e docs/examples/cli/extract_css.yml -s docs/examples/cli/css_schema.json -o json;\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Content Filter for Crawl4AI\nDESCRIPTION: Template for implementing a custom content filter by inheriting from the RelevantContentFilter base class. This allows for specialized filtering logic beyond the built-in options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_77\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.content_filter_strategy import RelevantContentFilter\n\nclass MyCustomFilter(RelevantContentFilter):\n    def filter_content(self, html, min_word_threshold=None):\n        # parse HTML, implement custom logic\n        return [block for block in ... if ... some condition...]\n```\n\n----------------------------------------\n\nTITLE: Accessing Page Metadata in Python\nDESCRIPTION: Shows how to access and use page-level metadata such as title, description, and Open Graph data that was discovered during crawling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nif result.metadata:\n    print(\"Title:\", result.metadata.get(\"title\"))\n    print(\"Author:\", result.metadata.get(\"author\"))\n```\n\n----------------------------------------\n\nTITLE: Using Hooks for Custom Workflow in Crawl4AI\nDESCRIPTION: This snippet shows how to use hooks in Crawl4AI for custom workflows. It demonstrates setting a 'before_goto' hook and provides examples of other available hooks for different stages of the crawling process.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nasync def custom_hook_workflow():\n    async with AsyncWebCrawler() as crawler:\n        # Set a 'before_goto' hook to run custom code just before navigation\n        crawler.crawler_strategy.set_hook(\"before_goto\", lambda page: print(\"[Hook] Preparing to navigate...\"))\n        \n        # Perform the crawl operation\n        result = await crawler.arun(\n            url=\"https://crawl4ai.com\",\n            bypass_cache=True\n        )\n        print(result.markdown.raw_markdown[:500])  # Display the first 500 characters\n\nasyncio.run(custom_hook_workflow())\n```\n\n----------------------------------------\n\nTITLE: Constructing Browser Command Line Arguments in Python\nDESCRIPTION: An asynchronous method that builds browser-specific command line arguments for launching browsers. It handles different argument formats for Chromium and Firefox, including remote debugging port, user data directory, and headless mode.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_115\n\nLANGUAGE: python\nCODE:\n```\nasync def _get_browser_args(self) -> List[str]:\n    \"\"\"Returns browser-specific command line arguments\"\"\"\n    base_args = [await self._get_browser_path()]\n\n    if self.browser_type == \"chromium\":\n        args = [\n            f\"--remote-debugging-port={self.debugging_port}\",\n            f\"--user-data-dir={self.user_data_dir}\",\n        ]\n        if self.headless:\n            args.append(\"--headless=new\")\n    elif self.browser_type == \"firefox\":\n        args = [\n            \"--remote-debugging-port\",\n            str(self.debugging_port),\n            \"--profile\",\n            self.user_data_dir,\n        ]\n        if self.headless:\n            args.append(\"--headless\")\n    else:\n        raise NotImplementedError(f\"Browser type {self.browser_type} not supported\")\n\n    return base_args + args\n```\n\n----------------------------------------\n\nTITLE: Implementing Browser Status Command in Python\nDESCRIPTION: Implementation of the 'crwl browser status' command that displays the current status of the builtin browser. It shows whether the browser is running, along with details like CDP URL, process ID, browser type, and start time.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_45\n\nLANGUAGE: python\nCODE:\n```\n@browser_cmd.command(\"status\")\ndef browser_status_cmd():\n    \"\"\"Show status of the builtin browser\"\"\"\n    profiler = BrowserProfiler()\n    \n    try:\n        status = anyio.run(profiler.get_builtin_browser_status)\n        \n        if status[\"running\"]:\n            info = status[\"info\"]\n            console.print(Panel(\n                f\"[green]Builtin browser is running[/green]\\n\\n\"\n                f\"CDP URL: [cyan]{info['cdp_url']}[/cyan]\\n\"\n                f\"Process ID: [yellow]{info['pid']}[/yellow]\\n\"\n                f\"Browser type: [blue]{info['browser_type']}[/blue]\\n\"\n                f\"User data directory: [magenta]{info['user_data_dir']}[/magenta]\\n\"\n                f\"Started: [cyan]{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(info['start_time']))}[/cyan]\",\n                title=\"Builtin Browser Status\",\n                border_style=\"green\"\n            ))\n        else:\n            console.print(Panel(\n                \"[yellow]Builtin browser is not running[/yellow]\\n\\n\"\n                \"Use 'crwl browser start' to start a builtin browser\",\n                title=\"Builtin Browser Status\",\n                border_style=\"yellow\"\n            ))\n            \n    except Exception as e:\n        console.print(f\"[red]Error checking browser status: {str(e)}[/red]\")\n        sys.exit(1)\n```\n\n----------------------------------------\n\nTITLE: Main Crawling Entry Point\nDESCRIPTION: Main method that initiates crawling in either batch or streaming mode based on configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_174\n\nLANGUAGE: python\nCODE:\n```\nasync def arun(\n        self,\n        start_url: str,\n        crawler: AsyncWebCrawler,\n        config: Optional[CrawlerRunConfig] = None,\n    ) -> \"RunManyReturn\":\n        if config is None:\n            raise ValueError(\"CrawlerRunConfig must be provided\")\n        if config.stream:\n            return self._arun_stream(start_url, crawler, config)\n        else:\n            return await self._arun_batch(start_url, crawler, config)\n```\n\n----------------------------------------\n\nTITLE: Finding Playwright Chromium Binary Location (Bash)\nDESCRIPTION: Command to locate the Playwright-managed Chromium binary on the system. This helps in identifying the correct path for launching the browser with a custom user data directory.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m playwright install --dry-run\n```\n\nLANGUAGE: bash\nCODE:\n```\nplaywright install --dry-run\n```\n\n----------------------------------------\n\nTITLE: Extracting HTML Element Attributes in JSON Schema\nDESCRIPTION: A JSON snippet that shows how to define a field for extracting attributes from HTML elements, such as href, src, or data attributes. This pattern can be used within the larger schema definition.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"href\",\n  \"type\": \"attribute\",\n  \"attribute\": \"href\",\n  \"default\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Attribute Handling in LLMContentFilter for Backwards Compatibility\nDESCRIPTION: Custom attribute setter for LLMContentFilter that handles deprecated attributes and provides informative error messages. This helps maintain backwards compatibility while guiding users toward the preferred API.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_101\n\nLANGUAGE: python\nCODE:\n```\ndef __setattr__(self, name, value):\n    \"\"\"Handle attribute setting.\"\"\"\n    # TODO: Planning to set properties dynamically based on the __init__ signature\n    sig = inspect.signature(self.__init__)\n    all_params = sig.parameters  # Dictionary of parameter names and their details\n\n    if name in self._UNWANTED_PROPS and value is not all_params[name].default:\n        raise AttributeError(f\"Setting '{name}' is deprecated. {self._UNWANTED_PROPS[name]}\")\n    \n    super().__setattr__(name, value)\n```\n\n----------------------------------------\n\nTITLE: Implementing URL Extension Extraction in Python\nDESCRIPTION: This method extracts the file extension from a given URL. It handles various URL formats, removes scheme and domain if present, and returns the lowercase extension.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_188\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\n@lru_cache(maxsize=1000)\ndef _extract_extension(url: str) -> str:\n    \"\"\"Extracts file extension from a URL.\"\"\"\n    # Remove scheme (http://, https://) if present\n    if \"://\" in url:\n        url = url.split(\"://\", 1)[-1]  # Get everything after '://'\n\n    # Remove domain (everything up to the first '/')\n    path_start = url.find(\"/\")\n    path = url[path_start:] if path_start != -1 else \"\"\n\n    # Extract last filename in path\n    filename = path.rsplit(\"/\", 1)[-1] if \"/\" in path else \"\"\n\n    # Extract and validate extension\n    if \".\" not in filename:\n        return \"\"\n\n    return filename.rpartition(\".\")[-1].lower()\n```\n\n----------------------------------------\n\nTITLE: Defining Abstract Base Class URLFilter for URL Filtering in Python\nDESCRIPTION: This abstract base class provides a foundation for implementing URL filters. It includes methods for logging, statistics tracking, and an abstract method for applying the filter to a URL.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_183\n\nLANGUAGE: Python\nCODE:\n```\nclass URLFilter(ABC):\n    \"\"\"Optimized base filter class\"\"\"\n\n    __slots__ = (\"name\", \"stats\", \"_logger_ref\")\n\n    def __init__(self, name: str = None):\n        self.name = name or self.__class__.__name__\n        self.stats = FilterStats()\n        # Lazy logger initialization using weakref\n        self._logger_ref = None\n\n    @property\n    def logger(self):\n        if self._logger_ref is None or self._logger_ref() is None:\n            logger = logging.getLogger(f\"urlfilter.{self.name}\")\n            self._logger_ref = weakref.ref(logger)\n        return self._logger_ref()\n\n    @abstractmethod\n    def apply(self, url: str) -> bool:\n        pass\n\n    def _update_stats(self, passed: bool):\n        # Use direct array index for speed\n        self.stats._counters[0] += 1  # total\n        self.stats._counters[1] += passed  # passed\n        self.stats._counters[2] += not passed  # rejected\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI with All Features in Python\nDESCRIPTION: Command for installing Crawl4AI with all available features and dependencies.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai[all]\n```\n\n----------------------------------------\n\nTITLE: Profile Management Commands\nDESCRIPTION: CLI commands for creating and using browser profiles for authenticated crawling\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_41\n\nLANGUAGE: bash\nCODE:\n```\ncrwl profiles\ncrwl https://example.com -p my-profile-name\n```\n\n----------------------------------------\n\nTITLE: CSS Schema JSON for Structured Data Extraction in Crawl4AI\nDESCRIPTION: Defines a JSON schema for extracting article information using CSS selectors, specifying the base selector and fields to extract.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n// css_schema.json\n{\n  \"name\": \"ArticleExtractor\",\n  \"baseSelector\": \".article\",\n  \"fields\": [\n    {\n      \"name\": \"title\",\n      \"selector\": \"h1.title\",\n      \"type\": \"text\"\n    },\n    {\n      \"name\": \"link\",\n      \"selector\": \"a.read-more\",\n      \"type\": \"attribute\",\n      \"attribute\": \"href\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Score Threshold Implementation in Web Crawler\nDESCRIPTION: Implementation of score-based filtering using threshold values to only crawl high-quality pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n# Only follow links with scores above 0.4\nstrategy = DFSDeepCrawlStrategy(\n    max_depth=2,\n    url_scorer=KeywordRelevanceScorer(keywords=[\"api\", \"guide\", \"reference\"]),\n    score_threshold=0.4  # Skip URLs with scores below this value\n)\n```\n\n----------------------------------------\n\nTITLE: LLM-based Extraction Configuration in YAML for Crawl4AI\nDESCRIPTION: Defines extraction configuration for using a Language Learning Model to extract structured data from web pages, specifying provider, instruction, and parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n# extract_llm.yml\ntype: \"llm\"\nprovider: \"openai/gpt-4\"\ninstruction: \"Extract all articles with their titles and links\"\napi_token: \"your-token\"\nparams:\n  temperature: 0.3\n  max_tokens: 1000\n```\n\n----------------------------------------\n\nTITLE: Implementing a Content Filtering Pipeline with CosineStrategy in Python\nDESCRIPTION: Shows how to create a complete content filtering pipeline using CosineStrategy to extract pricing features from a web page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```\n\n----------------------------------------\n\nTITLE: Implementing FilterChain Class for Chaining URL Filters in Python\nDESCRIPTION: This class allows chaining multiple URL filters together. It provides methods for adding filters and applying them concurrently to a URL, with support for both synchronous and asynchronous filter operations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_184\n\nLANGUAGE: Python\nCODE:\n```\nclass FilterChain:\n    \"\"\"Optimized filter chain\"\"\"\n\n    __slots__ = (\"filters\", \"stats\", \"_logger_ref\")\n\n    def __init__(self, filters: List[URLFilter] = None):\n        self.filters = tuple(filters or [])  # Immutable tuple for speed\n        self.stats = FilterStats()\n        self._logger_ref = None\n\n    @property\n    def logger(self):\n        if self._logger_ref is None or self._logger_ref() is None:\n            logger = logging.getLogger(\"urlfilter.chain\")\n            self._logger_ref = weakref.ref(logger)\n        return self._logger_ref()\n\n    def add_filter(self, filter_: URLFilter) -> \"FilterChain\":\n        \"\"\"Add a filter to the chain\"\"\"\n        self.filters.append(filter_)\n        return self  # Enable method chaining\n\n    async def apply(self, url: str) -> bool:\n        \"\"\"Apply all filters concurrently when possible\"\"\"\n        self.stats._counters[0] += 1  # Total processed URLs\n\n        tasks = []\n        for f in self.filters:\n            result = f.apply(url)\n\n            if inspect.isawaitable(result):\n                tasks.append(result)  # Collect async tasks\n            elif not result:  # Sync rejection\n                self.stats._counters[2] += 1  # Sync rejected\n                return False\n\n        if tasks:\n            results = await asyncio.gather(*tasks)\n\n            # Count how many filters rejected\n            rejections = results.count(False)\n            self.stats._counters[2] += rejections\n\n            if not all(results):\n                return False  # Stop early if any filter rejected\n\n        self.stats._counters[1] += 1  # Passed\n        return True\n```\n\n----------------------------------------\n\nTITLE: CLI Configuration Management Commands\nDESCRIPTION: Implements CLI commands for managing global configuration settings including listing, getting, and setting configuration values. Handles different value types and secret configurations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_53\n\nLANGUAGE: python\nCODE:\n```\n@cli.group(\"config\")\ndef config_cmd():\n    \"\"\"Manage global configuration settings\n    \n    Commands to view and update global configuration settings:\n    - list: Display all current configuration settings\n    - get: Get the value of a specific setting\n    - set: Set the value of a specific setting\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Processing Media (Images) from CrawlResult in Python\nDESCRIPTION: Example of accessing and iterating through extracted image information from the CrawlResult's media field. Each image entry contains properties like source URL and alt text.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimages = result.media.get(\"images\", [])\nfor img in images:\n    print(\"Image URL:\", img[\"src\"], \"Alt:\", img.get(\"alt\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Proxy Rotation with RoundRobinProxyStrategy in Crawl4AI\nDESCRIPTION: Demonstrates how to set up proxy rotation using RoundRobinProxyStrategy. The code loads proxies from environment variables, configures the crawler with proxy rotation, and makes multiple requests to verify that different proxies are being used for each request.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    BrowserConfig,\n    CrawlerRunConfig,\n    CacheMode,\n    RoundRobinProxyStrategy,\n)\nimport asyncio\nfrom crawl4ai import ProxyConfig\nasync def main():\n    # Load proxies and create rotation strategy\n    proxies = ProxyConfig.from_env()\n    #eg: export PROXIES=\"ip1:port1:username1:password1,ip2:port2:username2:password2\"\n    if not proxies:\n        print(\"No proxies found in environment. Set PROXIES env variable!\")\n        return\n        \n    proxy_strategy = RoundRobinProxyStrategy(proxies)\n    \n    # Create configs\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        proxy_rotation_strategy=proxy_strategy\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        urls = [\"https://httpbin.org/ip\"] * (len(proxies) * 2)  # Test each proxy twice\n\n        print(\"\\nðŸ“ˆ Initializing crawler with proxy rotation...\")\n        async with AsyncWebCrawler(config=browser_config) as crawler:\n            print(\"\\nðŸš€ Starting batch crawl with proxy rotation...\")\n            results = await crawler.arun_many(\n                urls=urls,\n                config=run_config\n            )\n            for result in results:\n                if result.success:\n                    ip_match = re.search(r'(?:[0-9]{1,3}\\.){3}[0-9]{1,3}', result.html)\n                    current_proxy = run_config.proxy_config if run_config.proxy_config else None\n                    \n                    if current_proxy and ip_match:\n                        print(f\"URL {result.url}\")\n                        print(f\"Proxy {current_proxy.server} -> Response IP: {ip_match.group(0)}\")\n                        verified = ip_match.group(0) == current_proxy.ip\n                        if verified:\n                            print(f\"âœ… Proxy working! IP matches: {current_proxy.ip}\")\n                        else:\n                            print(\"âŒ Proxy failed or IP mismatch!\")\n                    print(\"---\")\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: HTML Cleanup Configuration Example in Crawl4AI\nDESCRIPTION: Example of using the CrawlerRunConfig to clean HTML by excluding specific tags and attributes. The resulting cleaned HTML is available in the cleaned_html field of the CrawlResult.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    excluded_tags=[\"form\", \"header\", \"footer\"],\n    keep_data_attributes=False\n)\nresult = await crawler.arun(\"https://example.com\", config=config)\nprint(result.cleaned_html)  # Freed of forms, header, footer, data-* attributes\n```\n\n----------------------------------------\n\nTITLE: Creating and Modifying CrawlerRunConfig in Python for Crawl4AI\nDESCRIPTION: This code snippet demonstrates how to create a base CrawlerRunConfig and then clone it to create variations for different use cases in Crawl4AI. It shows how to enable streaming mode and create a debug configuration with longer timeout.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create a base configuration\nbase_config = CrawlerRunConfig(\n    cache_mode=CacheMode.ENABLED,\n    word_count_threshold=200,\n    wait_until=\"networkidle\"\n)\n\n# Create variations for different use cases\nstream_config = base_config.clone(\n    stream=True,  # Enable streaming mode\n    cache_mode=CacheMode.BYPASS\n)\n\ndebug_config = base_config.clone(\n    page_timeout=120000,  # Longer timeout for debugging\n    verbose=True\n)\n```\n\n----------------------------------------\n\nTITLE: Filtering HTML Content with LLM and Caching\nDESCRIPTION: Method to filter HTML content using LLM with caching support. It splits content into chunks, processes them in parallel using ThreadPoolExecutor, and tracks token usage. Includes error handling and logging.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_103\n\nLANGUAGE: python\nCODE:\n```\ndef filter_content(self, html: str, ignore_cache: bool = True) -> List[str]:\n        if not html or not isinstance(html, str):\n            return []\n\n        if self.logger:\n            self.logger.info(\n                \"Starting LLM markdown content filtering process\",\n                tag=\"LLM\",\n                params={\"provider\": self.llm_config.provider},\n                colors={\"provider\": Fore.CYAN},\n            )\n\n        # Cache handling\n        cache_dir = Path(get_home_folder()) / \"llm_cache\" / \"content_filter\"\n        cache_dir.mkdir(parents=True, exist_ok=True)\n        cache_key = self._get_cache_key(html, self.instruction or \"\")\n        cache_file = cache_dir / f\"{cache_key}.json\"\n\n        # Process chunks and handle results\n        # [...rest of implementation...]\n```\n\n----------------------------------------\n\nTITLE: Handling Load More Functionality with Crawl4AI\nDESCRIPTION: Implements a two-step process to load initial content and then click a 'More' link to load additional content. Uses session management to maintain state between requests.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Step 1: Load initial Hacker News page\n    config = CrawlerRunConfig(\n        wait_for=\"css:.athing:nth-child(30)\"  # Wait for 30 items\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n        print(\"Initial items loaded.\")\n\n        # Step 2: Let's scroll and click the \"More\" link\n        load_more_js = [\n            \"window.scrollTo(0, document.body.scrollHeight);\",\n            # The \"More\" link at page bottom\n            \"document.querySelector('a.morelink')?.click();\"  \n        ]\n        \n        next_page_conf = CrawlerRunConfig(\n            js_code=load_more_js,\n            wait_for=\"\"\"js:() => {\n                return document.querySelectorAll('.athing').length > 30;\n            }\"\"\",\n            # Mark that we do not re-navigate, but run JS in the same session:\n            js_only=True,\n            session_id=\"hn_session\"\n        )\n\n        # Re-use the same crawler session\n        result2 = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # same URL but continuing session\n            config=next_page_conf\n        )\n        total_items = result2.cleaned_html.count(\"athing\")\n        print(\"Items after load-more:\", total_items)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Interacting with crawl4ai Server using Python SDK\nDESCRIPTION: Provides a Python example demonstrating how to use the `Crawl4aiDockerClient` from the `crawl4ai` SDK. It shows initializing the client, performing both non-streaming and streaming crawl operations by calling `client.crawl` with different configurations, handling results, and fetching the server's schema using `client.get_schema`. Requires the `crawl4ai` library and `asyncio`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai.docker_client import Crawl4aiDockerClient\nfrom crawl4ai import BrowserConfig, CrawlerRunConfig, CacheMode # Assuming you have crawl4ai installed\n\nasync def main():\n    # Point to the correct server port\n    async with Crawl4aiDockerClient(base_url=\"http://localhost:11235\", verbose=True) as client:\n        # If JWT is enabled on the server, authenticate first:\n        # await client.authenticate(\"user@example.com\") # See Server Configuration section\n\n        # Example Non-streaming crawl\n        print(\"--- Running Non-Streaming Crawl ---\")\n        results = await client.crawl(\n            [\"https://httpbin.org/html\"],\n            browser_config=BrowserConfig(headless=True), # Use library classes for config aid\n            crawler_config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n        )\n        if results: # client.crawl returns None on failure\n          print(f\"Non-streaming results success: {results.success}\")\n          if results.success:\n              for result in results: # Iterate through the CrawlResultContainer\n                  print(f\"URL: {result.url}, Success: {result.success}\")\n        else:\n            print(\"Non-streaming crawl failed.\")\n\n\n        # Example Streaming crawl\n        print(\"\\n--- Running Streaming Crawl ---\")\n        stream_config = CrawlerRunConfig(stream=True, cache_mode=CacheMode.BYPASS)\n        try:\n            async for result in await client.crawl( # client.crawl returns an async generator for streaming\n                [\"https://httpbin.org/html\", \"https://httpbin.org/links/5/0\"],\n                browser_config=BrowserConfig(headless=True),\n                crawler_config=stream_config\n            ):\n                print(f\"Streamed result: URL: {result.url}, Success: {result.success}\")\n        except Exception as e:\n            print(f\"Streaming crawl failed: {e}\")\n\n\n        # Example Get schema\n        print(\"\\n--- Getting Schema ---\")\n        schema = await client.get_schema()\n        print(f\"Schema received: {bool(schema)}\") # Print whether schema was received\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Deep Web Crawling with BFS Strategy in Python\nDESCRIPTION: This function demonstrates deep crawling using a Breadth-First Search (BFS) strategy. It crawls the crawl4ai.com domain up to a depth of 1 and a maximum of 5 pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_152\n\nLANGUAGE: python\nCODE:\n```\nasync def demo_deep_crawl():\n    \"\"\"Deep crawling with BFS strategy\"\"\"\n    print(\"\\n=== 6. Deep Crawling ===\")\n\n    filter_chain = FilterChain([DomainFilter(allowed_domains=[\"crawl4ai.com\"])])\n\n    deep_crawl_strategy = BFSDeepCrawlStrategy(\n        max_depth=1, max_pages=5, filter_chain=filter_chain\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        results: List[CrawlResult] = await crawler.arun(\n            url=\"https://docs.crawl4ai.com\",\n            config=CrawlerRunConfig(deep_crawl_strategy=deep_crawl_strategy),\n        )\n\n        print(f\"Deep crawl returned {len(results)} pages:\")\n        for i, result in enumerate(results):\n            depth = result.metadata.get(\"depth\", \"unknown\")\n            print(f\"  {i + 1}. {result.url} (Depth: {depth})\")\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Processing of CrawlResult Data\nDESCRIPTION: A complete example showing how to access all possible fields from a CrawlResult object. This includes basic info, HTML content, markdown output, media and links, extracted content, binary outputs (screenshot, PDF, MHTML), and network/console data.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nasync def handle_result(result: CrawlResult):\n    if not result.success:\n        print(\"Crawl error:\", result.error_message)\n        return\n    \n    # Basic info\n    print(\"Crawled URL:\", result.url)\n    print(\"Status code:\", result.status_code)\n    \n    # HTML\n    print(\"Original HTML size:\", len(result.html))\n    print(\"Cleaned HTML size:\", len(result.cleaned_html or \"\"))\n\n    # Markdown output\n    if result.markdown:\n        print(\"Raw Markdown:\", result.markdown.raw_markdown[:300])\n        print(\"Citations Markdown:\", result.markdown.markdown_with_citations[:300])\n        if result.markdown.fit_markdown:\n            print(\"Fit Markdown:\", result.markdown.fit_markdown[:200])\n\n    # Media & Links\n    if \"images\" in result.media:\n        print(\"Image count:\", len(result.media[\"images\"]))\n    if \"internal\" in result.links:\n        print(\"Internal link count:\", len(result.links[\"internal\"]))\n\n    # Extraction strategy result\n    if result.extracted_content:\n        print(\"Structured data:\", result.extracted_content)\n    \n    # Screenshot/PDF/MHTML\n    if result.screenshot:\n        print(\"Screenshot length:\", len(result.screenshot))\n    if result.pdf:\n        print(\"PDF bytes length:\", len(result.pdf))\n    if result.mhtml:\n        print(\"MHTML length:\", len(result.mhtml))\n        \n    # Network and console capturing\n    if result.network_requests:\n        print(f\"Network requests captured: {len(result.network_requests)}\")\n        # Analyze request types\n        req_types = {}\n        for req in result.network_requests:\n            if \"resource_type\" in req:\n                req_types[req[\"resource_type\"]] = req_types.get(req[\"resource_type\"], 0) + 1\n        print(f\"Resource types: {req_types}\")\n        \n    if result.console_messages:\n        print(f\"Console messages captured: {len(result.console_messages)}\")\n        # Count by message type\n        msg_types = {}\n        for msg in result.console_messages:\n            msg_types[msg.get(\"type\", \"unknown\")] = msg_types.get(msg.get(\"type\", \"unknown\"), 0) + 1\n        print(f\"Message types: {msg_types}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Max Pages and Score Thresholds in Python with Crawl4AI\nDESCRIPTION: This function demonstrates how to use max_pages and score_threshold parameters with different crawling strategies (BFS, DFS, and Best-First) in Crawl4AI. It shows how to limit the number of pages crawled and set score thresholds for more targeted crawling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_210\n\nLANGUAGE: python\nCODE:\n```\nasync def max_pages_and_thresholds():\n    \"\"\"\n    PART 5: Demonstrates using max_pages and score_threshold parameters with different strategies.\n    \n    This function shows:\n    - How to limit the number of pages crawled\n    - How to set score thresholds for more targeted crawling\n    - Comparing BFS, DFS, and Best-First strategies with these parameters\n    \"\"\"\n    print(\"\\n===== MAX PAGES AND SCORE THRESHOLDS =====\")\n    \n    from crawl4ai.deep_crawling import DFSDeepCrawlStrategy\n    \n    async with AsyncWebCrawler() as crawler:\n        # Define a common keyword scorer for all examples\n        keyword_scorer = KeywordRelevanceScorer(\n            keywords=[\"browser\", \"crawler\", \"web\", \"automation\"], \n            weight=1.0\n        )\n        \n        # EXAMPLE 1: BFS WITH MAX PAGES\n        print(\"\\nðŸ“Š EXAMPLE 1: BFS STRATEGY WITH MAX PAGES LIMIT\")\n        print(\"  Limit the crawler to a maximum of 5 pages\")\n        \n        bfs_config = CrawlerRunConfig(\n            deep_crawl_strategy=BFSDeepCrawlStrategy(\n                max_depth=2, \n                include_external=False,\n                url_scorer=keyword_scorer,\n                max_pages=5  # Only crawl 5 pages\n            ),\n            scraping_strategy=LXMLWebScrapingStrategy(),\n            verbose=True,\n            cache_mode=CacheMode.BYPASS,\n        )\n        \n        results = await crawler.arun(url=\"https://docs.crawl4ai.com\", config=bfs_config)\n        \n        print(f\"  âœ… Crawled exactly {len(results)} pages as specified by max_pages\")\n        for result in results:\n            depth = result.metadata.get(\"depth\", 0)\n            print(f\"  â†’ Depth: {depth} | {result.url}\")\n            \n        # EXAMPLE 2: DFS WITH SCORE THRESHOLD\n        print(\"\\nðŸ“Š EXAMPLE 2: DFS STRATEGY WITH SCORE THRESHOLD\")\n        print(\"  Only crawl pages with a relevance score above 0.5\")\n        \n        dfs_config = CrawlerRunConfig(\n            deep_crawl_strategy=DFSDeepCrawlStrategy(\n                max_depth=2,\n                include_external=False, \n                url_scorer=keyword_scorer,\n                score_threshold=0.7,  # Only process URLs with scores above 0.5\n                max_pages=10\n            ),\n            scraping_strategy=LXMLWebScrapingStrategy(),\n            verbose=True,\n            cache_mode=CacheMode.BYPASS,\n        )\n        \n        results = await crawler.arun(url=\"https://docs.crawl4ai.com\", config=dfs_config)\n        \n        print(f\"  âœ… Crawled {len(results)} pages with scores above threshold\")\n        for result in results:\n            score = result.metadata.get(\"score\", 0)\n            depth = result.metadata.get(\"depth\", 0)\n            print(f\"  â†’ Depth: {depth} | Score: {score:.2f} | {result.url}\")\n            \n        # EXAMPLE 3: BEST-FIRST WITH BOTH CONSTRAINTS\n        print(\"\\nðŸ“Š EXAMPLE 3: BEST-FIRST STRATEGY WITH BOTH CONSTRAINTS\")\n        print(\"  Limit to 7 pages with scores above 0.3, prioritizing highest scores\")\n        \n        bf_config = CrawlerRunConfig(\n            deep_crawl_strategy=BestFirstCrawlingStrategy(\n                max_depth=2,\n                include_external=False,\n                url_scorer=keyword_scorer,\n                max_pages=7,          # Limit to 7 pages total\n            ),\n            scraping_strategy=LXMLWebScrapingStrategy(),\n            verbose=True,\n            cache_mode=CacheMode.BYPASS,\n            stream=True,\n        )\n        \n        results = []\n        async for result in await crawler.arun(url=\"https://docs.crawl4ai.com\", config=bf_config):\n            results.append(result)\n            score = result.metadata.get(\"score\", 0)\n            depth = result.metadata.get(\"depth\", 0)\n            print(f\"  â†’ Depth: {depth} | Score: {score:.2f} | {result.url}\")\n            \n        print(f\"  âœ… Crawled {len(results)} high-value pages with scores above 0.3\")\n        if results:\n            avg_score = sum(r.metadata.get('score', 0) for r in results) / len(results)\n            print(f\"  âœ… Average score: {avg_score:.2f}\")\n            print(\"  ðŸ” Note: BestFirstCrawlingStrategy visited highest-scoring pages first\")\n```\n\n----------------------------------------\n\nTITLE: Cloning Crawl4AI Repository using Git (Bash)\nDESCRIPTION: Demonstrates how to clone the Crawl4AI project repository from GitHub using `git clone` and then change the current directory into the newly cloned repository folder.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/unclecode/crawl4ai.git\ncd crawl4ai\n```\n\n----------------------------------------\n\nTITLE: Link Discovery and Processing for BFS Web Crawler in Python\nDESCRIPTION: Handles extracting, validating, and scoring links from crawl results. This method respects depth and page limits, applies URL filtering, and scores URLs if a scorer is provided. It prepares the next batch of URLs to crawl based on the discovery process.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_178\n\nLANGUAGE: python\nCODE:\n```\nasync def link_discovery(\n    self,\n    result: CrawlResult,\n    source_url: str,\n    current_depth: int,\n    visited: Set[str],\n    next_level: List[Tuple[str, Optional[str]]],\n    depths: Dict[str, int],\n) -> None:\n    \"\"\"\n    Extracts links from the crawl result, validates and scores them, and\n    prepares the next level of URLs.\n    Each valid URL is appended to next_level as a tuple (url, parent_url)\n    and its depth is tracked.\n    \"\"\"            \n    next_depth = current_depth + 1\n    if next_depth > self.max_depth:\n        return\n\n    # If we've reached the max pages limit, don't discover new links\n    remaining_capacity = self.max_pages - self._pages_crawled\n    if remaining_capacity <= 0:\n        self.logger.info(f\"Max pages limit ({self.max_pages}) reached, stopping link discovery\")\n        return\n\n    # Get internal links and, if enabled, external links.\n    links = result.links.get(\"internal\", [])\n    if self.include_external:\n        links += result.links.get(\"external\", [])\n\n    valid_links = []\n    \n    # First collect all valid links\n    for link in links:\n        url = link.get(\"href\")\n        # Strip URL fragments to avoid duplicate crawling\n        # base_url = url.split('#')[0] if url else url\n        base_url = normalize_url_for_deep_crawl(url, source_url)\n        if base_url in visited:\n            continue\n        if not await self.can_process_url(url, next_depth):\n            self.stats.urls_skipped += 1\n            continue\n\n        # Score the URL if a scorer is provided\n        score = self.url_scorer.score(base_url) if self.url_scorer else 0\n        \n        # Skip URLs with scores below the threshold\n        if score < self.score_threshold:\n            self.logger.debug(f\"URL {url} skipped: score {score} below threshold {self.score_threshold}\")\n            self.stats.urls_skipped += 1\n            continue\n        \n        valid_links.append((base_url, score))\n    \n    # If we have more valid links than capacity, sort by score and take the top ones\n    if len(valid_links) > remaining_capacity:\n        if self.url_scorer:\n            # Sort by score in descending order\n            valid_links.sort(key=lambda x: x[1], reverse=True)\n        # Take only as many as we have capacity for\n        valid_links = valid_links[:remaining_capacity]\n        self.logger.info(f\"Limiting to {remaining_capacity} URLs due to max_pages limit\")\n        \n    # Process the final selected links\n    for url, score in valid_links:\n        # attach the score to metadata if needed\n        if score:\n            result.metadata = result.metadata or {}\n            result.metadata[\"score\"] = score\n        next_level.append((url, source_url))\n        depths[url] = next_depth\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy Usage in Crawl4AI\nDESCRIPTION: Demonstrates how to set up a proxy for routing crawl traffic in Crawl4AI. It includes proxy server configuration with optional authentication credentials.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    browser_cfg = BrowserConfig(\n        proxy_config={\n            \"server\": \"http://proxy.example.com:8080\",\n            \"username\": \"myuser\",\n            \"password\": \"mypass\",\n        },\n        headless=True\n    )\n    crawler_cfg = CrawlerRunConfig(\n        verbose=True\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.whatismyip.com/\",\n            config=crawler_cfg\n        )\n        if result.success:\n            print(\"[OK] Page fetched via proxy.\")\n            print(\"Page HTML snippet:\", result.html[:200])\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Pre-fetching models for Crawl4AI\nDESCRIPTION: Downloads and caches large models locally for Crawl4AI. This optional step is only necessary if your workflow requires these models.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncrawl4ai-download-models\n```\n\n----------------------------------------\n\nTITLE: CSS/XPath-based Extraction Configuration in YAML for Crawl4AI\nDESCRIPTION: Defines extraction configuration for using CSS/XPath selectors to extract structured data from web pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n# extract_css.yml\ntype: \"json-css\"\nparams:\n  verbose: true\n```\n\n----------------------------------------\n\nTITLE: Saving MHTML Files in Python\nDESCRIPTION: Demonstrates how to save an MHTML snapshot of a webpage, which preserves the entire page with all its resources in a single file.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nif result.mhtml:\n    with open(\"page.mhtml\", \"w\", encoding=\"utf-8\") as f:\n        f.write(result.mhtml)\n```\n\n----------------------------------------\n\nTITLE: Dictionary-based Storage State Implementation\nDESCRIPTION: Example showing how to pass storage_state as a Python dictionary to AsyncWebCrawler\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    storage_dict = {\n        \"cookies\": [\n            {\n                \"name\": \"session\",\n                \"value\": \"abcd1234\",\n                \"domain\": \"example.com\",\n                \"path\": \"/\",\n                \"expires\": 1675363572.037711,\n                \"httpOnly\": False,\n                \"secure\": False,\n                \"sameSite\": \"None\"\n            }\n        ],\n        \"origins\": [\n            {\n                \"origin\": \"https://example.com\",\n                \"localStorage\": [\n                    {\"name\": \"token\", \"value\": \"my_auth_token\"},\n                    {\"name\": \"refreshToken\", \"value\": \"my_refresh_token\"}\n                ]\n            }\n        ]\n    }\n\n    async with AsyncWebCrawler(\n        headless=True,\n        storage_state=storage_dict\n    ) as crawler:\n        result = await crawler.arun(url='https://example.com/protected')\n        if result.success:\n            print(\"Crawl succeeded with pre-loaded session data!\")\n            print(\"Page HTML length:\", len(result.html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI via pip (Python, Bash)\nDESCRIPTION: This group of bash code snippets demonstrates common pip commands for installing the Crawl4AI Python package. Instructions cover installing the latest stable version, opting into pre-release versions with the --pre flag, and targeting a specific version number. These commands require Python and pip to be installed, and may require the user to use a virtual environment for dependency management. Outputs include downloading and installing Crawl4AI with the appropriate version based on the command.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\npip install -U crawl4ai\n\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai --pre\n\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai==0.4.3b1\n\n```\n\n----------------------------------------\n\nTITLE: Cloning and Modifying CrawlerRunConfig in Crawl4AI\nDESCRIPTION: Example of creating a modified copy of a run configuration using the clone() method. It demonstrates how to efficiently create variations of configurations for different crawling scenarios.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create a modified copy with the clone() method\nstream_cfg = run_cfg.clone(\n    stream=True,\n    cache_mode=CacheMode.BYPASS\n)\n```\n\n----------------------------------------\n\nTITLE: Structured Data Extraction Complete Example for Crawl4AI\nDESCRIPTION: Comprehensive example showing structured data extraction with CSS-based extraction and schema.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncrwl https://example.com \\\n    -e extract_css.yml \\\n    -s css_schema.json \\\n    -o json \\\n    -v\n```\n\n----------------------------------------\n\nTITLE: Enhancing Data Models with Network and Console Capture Fields\nDESCRIPTION: Updates to the AsyncCrawlResponse and CrawlResult models to store captured network requests and console messages. The new optional fields allow for storing lists of dictionaries containing the captured data during web crawling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/prompts/prompt_net_requests.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# ==== File: crawl4ai/models.py ====\n# ... (imports) ...\n\n# ... (Existing dataclasses/models) ...\n\nclass AsyncCrawlResponse(BaseModel):\n    html: str\n    response_headers: Dict[str, str]\n    js_execution_result: Optional[Dict[str, Any]] = None\n    status_code: int\n    screenshot: Optional[str] = None\n    pdf_data: Optional[bytes] = None\n    get_delayed_content: Optional[Callable[[Optional[float]], Awaitable[str]]] = None\n    downloaded_files: Optional[List[str]] = None\n    ssl_certificate: Optional[SSLCertificate] = None\n    redirected_url: Optional[str] = None\n    # NEW: Fields for captured data\n    network_requests: Optional[List[Dict[str, Any]]] = None\n    console_messages: Optional[List[Dict[str, Any]]] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n\n# ... (Existing models like MediaItem, Link, etc.) ...\n\nclass CrawlResult(BaseModel):\n    url: str\n    html: str\n    success: bool\n    cleaned_html: Optional[str] = None\n    media: Dict[str, List[Dict]] = {}\n    links: Dict[str, List[Dict]] = {}\n    downloaded_files: Optional[List[str]] = None\n    js_execution_result: Optional[Dict[str, Any]] = None\n    screenshot: Optional[str] = None\n    pdf: Optional[bytes] = None\n    mhtml: Optional[str] = None # Added mhtml based on the provided models.py\n    _markdown: Optional[MarkdownGenerationResult] = PrivateAttr(default=None)\n    extracted_content: Optional[str] = None\n    metadata: Optional[dict] = None\n    error_message: Optional[str] = None\n    session_id: Optional[str] = None\n    response_headers: Optional[dict] = None\n    status_code: Optional[int] = None\n    ssl_certificate: Optional[SSLCertificate] = None\n    dispatch_result: Optional[DispatchResult] = None\n    redirected_url: Optional[str] = None\n    # NEW: Fields for captured data\n    network_requests: Optional[List[Dict[str, Any]]] = None\n    console_messages: Optional[List[Dict[str, Any]]] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    # ... (Existing __init__, properties, model_dump for markdown compatibility) ...\n\n# ... (Rest of the models) ...\n```\n\n----------------------------------------\n\nTITLE: Cloning CrawlerRunConfig in Python\nDESCRIPTION: Demonstrates how to create and modify crawler configurations using the clone() method. Shows how to create a base configuration and derive variations for different use cases like streaming and cache bypass.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create a base configuration\nbase_config = CrawlerRunConfig(\n    cache_mode=CacheMode.ENABLED,\n    word_count_threshold=200\n)\n\n# Create variations using clone()\nstream_config = base_config.clone(stream=True)\nno_cache_config = base_config.clone(\n    cache_mode=CacheMode.BYPASS,\n    stream=True\n)\n```\n\n----------------------------------------\n\nTITLE: Timing Control Configuration in Crawl4AI\nDESCRIPTION: Demonstrates how to configure timing controls in Crawl4AI, including page timeout and delay before returning HTML. These parameters help manage the crawler's behavior with dynamic content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    page_timeout=60000,  # 60s limit\n    delay_before_return_html=2.5\n)\n```\n\n----------------------------------------\n\nTITLE: Abstract Extraction Strategy Base Class\nDESCRIPTION: Definition of the abstract base class ExtractionStrategy which serves as the foundation for all content extraction strategies in Crawl4AI. It defines the interface for initializing and extracting content from web pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nclass ExtractionStrategy(ABC):\n    \"\"\"\n    Abstract base class for all extraction strategies.\n    \"\"\"\n\n    def __init__(self, input_format: str = \"markdown\", **kwargs):\n        \"\"\"\n        Initialize the extraction strategy.\n\n        Args:\n            input_format: Content format to use for extraction.\n                         Options: \"markdown\" (default), \"html\", \"fit_markdown\"\n            **kwargs: Additional keyword arguments\n        \"\"\"\n        self.input_format = input_format\n        self.DEL = \"<|DEL|>\"\n        self.name = self.__class__.__name__\n        self.verbose = kwargs.get(\"verbose\", False)\n\n    @abstractmethod\n    def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extract meaningful blocks or chunks from the given HTML.\n\n        :param url: The URL of the webpage.\n        :param html: The HTML content of the webpage.\n        :return: A list of extracted blocks or chunks.\n        \"\"\"\n        pass\n\n    def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]:\n        \"\"\"\n        Process sections of text in parallel by default.\n\n        :param url: The URL of the webpage.\n        :param sections: List of sections (strings) to process.\n        :return: A list of processed JSON blocks.\n        \"\"\"\n        extracted_content = []\n        with ThreadPoolExecutor() as executor:\n            futures = [\n                executor.submit(self.extract, url, section, **kwargs)\n                for section in sections\n            ]\n            for future in as_completed(futures):\n                extracted_content.extend(future.result())\n        return extracted_content\n```\n\n----------------------------------------\n\nTITLE: Combining Interaction with CSS-Based Extraction in Crawl4AI\nDESCRIPTION: This example demonstrates how to combine page interaction with structured data extraction using JsonCssExtractionStrategy. It defines a schema to extract commit information from GitHub's commit list after interacting with the page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_120\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Commits\",\n    \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\"}\n    ]\n}\nconfig = CrawlerRunConfig(\n    session_id=\"ts_commits_session\",\n    js_code=js_next_page,\n    wait_for=wait_for_more,\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Content Source for Markdown Generation in Python\nDESCRIPTION: This snippet demonstrates the addition of a new 'content_source' parameter to the MarkdownGenerationStrategy class, allowing users to specify which HTML input to use for markdown generation. Options include 'cleaned_html' (default), 'raw_html', and 'fit_html'.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/JOURNAL.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncontent_source: str = \"cleaned_html\"\n```\n\n----------------------------------------\n\nTITLE: Illustrating Client-Server-Crawler Communication Flow with Mermaid\nDESCRIPTION: A sequence diagram showing the interaction between Client, Server, and Crawler components in the proposed event-driven crawling system. It demonstrates how a crawl is initiated with a Process ID, how events are streamed to the client, and how the client can send instructions back to influence the ongoing crawl.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/articles/dockerize_hooks.md#2025-04-23_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant Client\n    participant Server\n    participant Crawler\n\n    Client->>Server: Start crawl request\n    Server->>Crawler: Initiate crawl with Process ID\n    Crawler-->>Server: Event: Page hit\n    Server-->>Client: Stream: Page hit event\n    Client->>Server: Instruction for Process ID\n    Server->>Crawler: Update crawl with new instructions\n    Crawler-->>Server: Event: Crawl completed\n    Server-->>Client: Stream: Crawl completed\n```\n\n----------------------------------------\n\nTITLE: Basic JavaScript Execution in Crawl4AI\nDESCRIPTION: Shows how to execute JavaScript commands during web crawling, including scrolling and clicking elements. Demonstrates both single and multiple JavaScript command execution with session management.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_114\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Single JS command\n    config = CrawlerRunConfig(\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # Example site\n            config=config\n        )\n        print(\"Crawled length:\", len(result.cleaned_html))\n\n    # Multiple commands\n    js_commands = [\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # 'More' link on Hacker News\n        \"document.querySelector('a.morelink')?.click();\",  \n    ]\n    config = CrawlerRunConfig(js_code=js_commands)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # Another pass\n            config=config\n        )\n        print(\"After scroll+click, length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing CSS Extraction Example\nDESCRIPTION: Example of using JsonCssExtractionStrategy for extracting structured data from HTML using CSS selectors.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\n# Define schema\nschema = {\n    \"name\": \"Product List\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\n            \"name\": \"title\",\n            \"selector\": \"h2.title\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \".price\",\n            \"type\": \"text\",\n            \"transform\": \"strip\"\n        },\n        {\n            \"name\": \"image\",\n            \"selector\": \"img\",\n            \"type\": \"attribute\",\n            \"attribute\": \"src\"\n        }\n    ]\n}\n\n# Create and use strategy\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming Usage of arun_many() for Concurrent Crawling in Python\nDESCRIPTION: This example shows how to use arun_many() with streaming enabled, allowing processing of results as they become available. It's ideal for handling large numbers of URLs efficiently.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    stream=True,  # Enable streaming mode\n    cache_mode=CacheMode.BYPASS\n)\n\n# Process results as they complete\nasync for result in await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\", \"https://site3.com\"],\n    config=config\n):\n    if result.success:\n        print(f\"Just completed: {result.url}\")\n        # Process each result immediately\n        process_result(result)\n```\n\n----------------------------------------\n\nTITLE: CSS-based Data Extraction\nDESCRIPTION: Implementation of CSS-based data extraction using JsonCssExtractionStrategy with a defined schema structure.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_125\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    schema = {\n        \"name\": \"Example Items\",\n        \"baseSelector\": \"div.item\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n\n    raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"raw://\" + raw_html,\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                extraction_strategy=JsonCssExtractionStrategy(schema)\n            )\n        )\n        # The JSON output is stored in 'extracted_content'\n        data = json.loads(result.extracted_content)\n        print(data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Semaphore-based Crawler in Python\nDESCRIPTION: Shows implementation of a crawler using SemaphoreDispatcher for controlled concurrent requests with rate limiting and monitoring features.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl_with_semaphore(urls):\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n    \n    dispatcher = SemaphoreDispatcher(\n        semaphore_count=5,\n        rate_limiter=RateLimiter(\n            base_delay=(0.5, 1.0),\n            max_delay=10.0\n        ),\n        monitor=CrawlerMonitor(\n            max_visible_rows=15,\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        results = await crawler.arun_many(\n            urls, \n            config=run_config,\n            dispatcher=dispatcher\n        )\n        return results\n```\n\n----------------------------------------\n\nTITLE: Initializing Rate Limiter in Python\nDESCRIPTION: Creates a RateLimiter class to manage request pacing and retries. Handles random delays between requests and exponential backoff for rate limit responses.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass RateLimiter:\n    def __init__(\n        # Random delay range between requests\n        base_delay: Tuple[float, float] = (1.0, 3.0),  \n        \n        # Maximum backoff delay\n        max_delay: float = 60.0,                        \n        \n        # Retries before giving up\n        max_retries: int = 3,                          \n        \n        # Status codes triggering backoff\n        rate_limit_codes: List[int] = [429, 503]        \n    )\n```\n\n----------------------------------------\n\nTITLE: Specific Playwright Chromium Installation (Bash)\nDESCRIPTION: An alternative command to install the Chromium browser specifically for Playwright using the Python module invocation. This method might be more reliable in certain environments.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m playwright install chromium\n```\n\n----------------------------------------\n\nTITLE: Installing or Updating Crawl4AI via Pip (Bash)\nDESCRIPTION: Provides the shell command to install the latest version of the Crawl4AI library or upgrade an existing installation using the Python package manager, pip. The `-U` flag ensures that the package is upgraded if it's already installed. Requires pip to be available in the shell environment.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.6.0.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n```bash\npip install -U crawl4ai\n```\n```\n\n----------------------------------------\n\nTITLE: SEO Filter Implementation in Web Crawler\nDESCRIPTION: Implementation of SEO-based filtering to identify pages with strong SEO characteristics using metadata analysis.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling.filters import FilterChain, SEOFilter\n\n# Create an SEO filter that looks for specific keywords in page metadata\nseo_filter = SEOFilter(\n    threshold=0.5,  # Minimum score (0.0 to 1.0)\n    keywords=[\"tutorial\", \"guide\", \"documentation\"]\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([seo_filter])\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Mode Crawler in Python\nDESCRIPTION: Demonstrates how to set up a streaming crawler using MemoryAdaptiveDispatcher for real-time result processing. Includes memory management and monitoring configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl_streaming():\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=True  # Enable streaming mode\n    )\n    \n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=10,\n        monitor=CrawlerMonitor(\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        # Process results as they become available\n        async for result in await crawler.arun_many(\n            urls=urls,\n            config=run_config,\n            dispatcher=dispatcher\n        ):\n            if result.success:\n                # Process each result immediately\n                await process_result(result)\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")\n```\n\n----------------------------------------\n\nTITLE: Token Usage Statistics Display\nDESCRIPTION: Method to display token usage statistics including completion, prompt, and total tokens. Shows both summary and detailed history of token usage per request.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_104\n\nLANGUAGE: python\nCODE:\n```\ndef show_usage(self) -> None:\n        \"\"\"Print usage statistics\"\"\"\n        print(\"\\n=== Token Usage Summary ===\")\n        print(f\"{'Type':<15} {'Count':>12}\")\n        print(\"-\" * 30)\n        print(f\"{'Completion':<15} {self.total_usage.completion_tokens:>12,}\")\n        print(f\"{'Prompt':<15} {self.total_usage.prompt_tokens:>12,}\")\n        print(f\"{'Total':<15} {self.total_usage.total_tokens:>12,}\")\n\n        if self.usages:\n            print(\"\\n=== Usage History ===\")\n            print(f\"{'Request #':<10} {'Completion':>12} {'Prompt':>12} {'Total':>12}\")\n            print(\"-\" * 48)\n            for i, usage in enumerate(self.usages, 1):\n                print(\n                    f\"{i:<10} {usage.completion_tokens:>12,} \"\n                    f\"{usage.prompt_tokens:>12,} {usage.total_tokens:>12,}\"\n                )\n```\n\n----------------------------------------\n\nTITLE: Dynamic Content Crawling with Session Management\nDESCRIPTION: Complex example showing how to crawl GitHub commits across multiple pages while maintaining session state.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_162\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai.cache_context import CacheMode\n\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"github_commits_session\"\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        all_commits = []\n\n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [{\n                \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\"\n            }],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # JavaScript and wait configurations\n        js_next_page = \"\"\"document.querySelector('a[data-testid=\"pagination-next-button\"]').click();\"\"\"\n        wait_for = \"\"\"() => document.querySelectorAll('li.Box-sc-g0xbh4-0').length > 0\"\"\"\n\n        # Crawl multiple pages\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits\n```\n\n----------------------------------------\n\nTITLE: Browser Disable Options Configuration in Python\nDESCRIPTION: Defines a list of Chrome command-line arguments to disable various browser features and optimizations for automated crawling purposes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_111\n\nLANGUAGE: python\nCODE:\n```\nBROWSER_DISABLE_OPTIONS = [\n    \"--disable-background-networking\",\n    \"--disable-background-timer-throttling\",\n    \"--disable-backgrounding-occluded-windows\",\n    \"--disable-breakpad\",\n    \"--disable-client-side-phishing-detection\",\n    \"--disable-component-extensions-with-background-pages\",\n    \"--disable-default-apps\",\n    \"--disable-extensions\",\n    \"--disable-features=TranslateUI\",\n    \"--disable-hang-monitor\",\n    \"--disable-ipc-flooding-protection\",\n    \"--disable-popup-blocking\",\n    \"--disable-prompt-on-repost\",\n    \"--disable-sync\",\n    \"--force-color-profile=srgb\",\n    \"--metrics-recording-only\",\n    \"--no-first-run\",\n    \"--password-store=basic\",\n    \"--use-mock-keychain\"\n]\n```\n\n----------------------------------------\n\nTITLE: Crawling Dynamic Multi-Page Content with Crawl4AI\nDESCRIPTION: This function demonstrates advanced multi-page crawling of dynamic content with JavaScript execution. It implements a custom execution hook to detect new content, navigates through multiple pages of GitHub commits, and aggregates the results across pages while maintaining session state.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_139\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl_dynamic_content_pages_method_1():\n    print(\"\\n--- Advanced Multi-Page Crawling with JavaScript Execution ---\")\n    first_commit = \"\"\n\n    async def on_execution_started(page, **kwargs):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.Box-sc-g0xbh4-0 h4\")\n                commit = await page.query_selector(\"li.Box-sc-g0xbh4-0 h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\")\n                commit = re.sub(r\"\\s+\", \"\", commit)\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear after JavaScript execution: {e}\")\n\n    browser_config = BrowserConfig(headless=False, java_script_enabled=True)\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        js_next_page = \"\"\"\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n        \"\"\"\n\n        for page in range(3):\n            crawler_config = CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                css_selector=\"li.Box-sc-g0xbh4-0\",\n                js_code=js_next_page if page > 0 else None,\n                js_only=page > 0,\n                session_id=session_id,\n            )\n\n            result = await crawler.arun(url=url, config=crawler_config)\n            assert result.success, f\"Failed to crawl page {page + 1}\"\n\n            soup = BeautifulSoup(result.cleaned_html, \"html.parser\")\n            commits = soup.select(\"li\")\n            all_commits.extend(commits)\n\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n```\n\n----------------------------------------\n\nTITLE: LLM Q&A Command Line Examples for Crawl4AI\nDESCRIPTION: Demonstrates how to use the LLM Q&A feature to ask questions about crawled content, including simple questions, multi-step workflows, and combined approaches.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# Simple question\ncrwl https://example.com -q \"What is the main topic discussed?\"\n\n# View content then ask questions\ncrwl https://example.com -o markdown  # See content first\ncrwl https://example.com -q \"Summarize the key points\"\ncrwl https://example.com -q \"What are the conclusions?\"\n\n# Combined with advanced crawling\ncrwl https://example.com \\\n    -B browser.yml \\\n    -c \"css_selector=article,scan_full_page=true\" \\\n    -q \"What are the pros and cons mentioned?\"\n```\n\n----------------------------------------\n\nTITLE: Implementing CDP Browser Launch Command in Python\nDESCRIPTION: Click command implementation for launching a standalone browser with CDP debugging capabilities. Handles browser configuration, user data directory management, and provides status feedback through console panels.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_50\n\nLANGUAGE: python\nCODE:\n```\n@cli.command(\"cdp\")\n@click.option(\"--user-data-dir\", \"-d\", help=\"Directory to use for browser data (will be created if it doesn't exist)\")\n@click.option(\"--port\", \"-P\", type=int, default=9222, help=\"Debugging port (default: 9222)\")\n@click.option(\"--browser-type\", \"-b\", type=click.Choice([\"chromium\", \"firefox\"]), default=\"chromium\", \n              help=\"Browser type (default: chromium)\")\n@click.option(\"--headless\", is_flag=True, help=\"Run browser in headless mode\")\n@click.option(\"--incognito\", is_flag=True, help=\"Run in incognito/private mode (ignores user-data-dir)\")\ndef cdp_cmd(user_data_dir: Optional[str], port: int, browser_type: str, headless: bool, incognito: bool):\n```\n\n----------------------------------------\n\nTITLE: Browser Process Cleanup in Python\nDESCRIPTION: An asynchronous method that safely terminates browser processes and cleans up temporary directories. It handles different termination strategies depending on the platform, with special handling for Windows processes, and includes graceful shutdown with fallback to force kill.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_116\n\nLANGUAGE: python\nCODE:\n```\nasync def cleanup(self):\n    \"\"\"Cleanup browser process and temporary directory\"\"\"\n    # Set shutting_down flag BEFORE any termination actions\n    self.shutting_down = True\n\n    if self.browser_process:\n        try:\n            # For builtin browsers that should persist, we should check if it's a detached process\n            # Only terminate if we have proper control over the process\n            if not self.browser_process.poll():\n                # Process is still running\n                self.browser_process.terminate()\n                # Wait for process to end gracefully\n                for _ in range(10):  # 10 attempts, 100ms each\n                    if self.browser_process.poll() is not None:\n                        break\n                    await asyncio.sleep(0.1)\n\n                # Force kill if still running\n                if self.browser_process.poll() is None:\n                    if sys.platform == \"win32\":\n                        # On Windows we might need taskkill for detached processes\n                        try:\n                            subprocess.run([\"taskkill\", \"/F\", \"/PID\", str(self.browser_process.pid)])\n                        except Exception:\n                            self.browser_process.kill()\n                    else:\n                        self.browser_process.kill()\n                    await asyncio.sleep(0.1)  # Brief wait for kill to take effect\n\n        except Exception as e:\n            self.logger.error(\n                message=\"Error terminating browser: {error}\",\n                tag=\"ERROR\", \n                params={\"error\": str(e)},\n            )\n\n    if self.temp_dir and os.path.exists(self.temp_dir):\n        try:\n            shutil.rmtree(self.temp_dir)\n        except Exception as e:\n            self.logger.error(\n                message=\"Error removing temporary directory: {error}\",\n                tag=\"ERROR\",\n                params={\"error\": str(e)},\n            )\n```\n\n----------------------------------------\n\nTITLE: Defining MIME Type Mappings in Python\nDESCRIPTION: This snippet defines a dictionary mapping file extensions to their corresponding MIME types. It covers various document formats, archives, and other common file types.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_187\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"docx\": \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n\"xlsx\": \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n\"xls\": \"application/vnd.ms-excel\",\n\"ppt\": \"application/vnd.ms-powerpoint\",\n\"pptx\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n# OpenDocument Formats\n\"odt\": \"application/vnd.oasis.opendocument.text\",\n\"ods\": \"application/vnd.oasis.opendocument.spreadsheet\",\n\"odp\": \"application/vnd.oasis.opendocument.presentation\",\n# Archives\n\"tar.gz\": \"application/gzip\",\n\"tgz\": \"application/gzip\",\n\"bz2\": \"application/x-bzip2\",\n# Others\n\"rtf\": \"application/rtf\",\n\"apk\": \"application/vnd.android.package-archive\",\n\"epub\": \"application/epub+zip\",\n\"jar\": \"application/java-archive\",\n\"swf\": \"application/x-shockwave-flash\",\n\"midi\": \"audio/midi\",\n\"mid\": \"audio/midi\",\n\"ps\": \"application/postscript\",\n\"ai\": \"application/postscript\",\n\"eps\": \"application/postscript\",\n# Custom or less common\n\"bin\": \"application/octet-stream\",\n\"dmg\": \"application/x-apple-diskimage\",\n\"iso\": \"application/x-iso9660-image\",\n\"deb\": \"application/x-debian-package\",\n\"rpm\": \"application/x-rpm\",\n\"sqlite\": \"application/vnd.sqlite3\",\n# Placeholder\n\"unknown\": \"application/octet-stream\",  # Fallback for unknown file types\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Processing Raw Data from Crawl4AI Stress Tests in Python\nDESCRIPTION: This Python code demonstrates how to load and analyze raw data from Crawl4AI stress test results. It reads JSON summary files and CSV memory sample files, calculates memory growth, and computes average URLs processed per second.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/tests/memory/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport pandas as pd\n\n# Load test summary\ntest_id = \"20250418_103015\" # Example ID\nwith open(f'reports/test_summary_{test_id}.json', 'r') as f:\n    results = json.load(f)\n\n# Load memory samples\nmemory_df = pd.read_csv(f'reports/memory_samples_{test_id}.csv')\n\n# Analyze memory_df (e.g., calculate growth, plot)\nif not memory_df['memory_info_mb'].isnull().all():\n    growth = memory_df['memory_info_mb'].iloc[-1] - memory_df['memory_info_mb'].iloc[0]\n    print(f\"Total Memory Growth: {growth:.1f} MB\")\nelse:\n    print(\"No valid memory samples found.\")\n\nprint(f\"Avg URLs/sec: {results['urls_processed'] / results['total_time_seconds']:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Running Basic Crawl4AI Container via Docker Run (Bash)\nDESCRIPTION: Illustrates the basic command to run the Crawl4AI Docker container in detached mode (`-d`), mapping the host port 11235 to the container port 11235 (`-p`), naming the container `crawl4ai`, and setting the shared memory size (`--shm-size`). Uses a specific pre-built image tag.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d \\\n  -p 11235:11235 \\\n  --name crawl4ai \\\n  --shm-size=1g \\\n  unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number\n```\n\n----------------------------------------\n\nTITLE: Basic Content Filter Pattern Examples\nDESCRIPTION: Simplified code patterns showing the basic setup for both Pruning and BM25 content filters with their essential configuration parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",\n    min_word_threshold=10\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\n```\n\nLANGUAGE: python\nCODE:\n```\nbm25_filter = BM25ContentFilter(\n    user_query=\"health benefits fruit\",\n    bm25_threshold=1.2\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\n```\n\n----------------------------------------\n\nTITLE: Building Browser Launch Arguments for Playwright\nDESCRIPTION: Method that constructs browser launch arguments based on configuration settings. It handles various browser options including headless mode, viewport settings, performance optimizations, and special browsing modes like light mode and text-only mode.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_123\n\nLANGUAGE: python\nCODE:\n```\ndef _build_browser_args(self) -> dict:\n    \"\"\"Build browser launch arguments from config.\"\"\"\n    args = [\n        \"--disable-gpu\",\n        \"--disable-gpu-compositing\",\n        \"--disable-software-rasterizer\",\n        \"--no-sandbox\",\n        \"--disable-dev-shm-usage\",\n        \"--no-first-run\",\n        \"--no-default-browser-check\",\n        \"--disable-infobars\",\n        \"--window-position=0,0\",\n        \"--ignore-certificate-errors\",\n        \"--ignore-certificate-errors-spki-list\",\n        \"--disable-blink-features=AutomationControlled\",\n        \"--window-position=400,0\",\n        \"--disable-renderer-backgrounding\",\n        \"--disable-ipc-flooding-protection\",\n        \"--force-color-profile=srgb\",\n        \"--mute-audio\",\n        \"--disable-background-timer-throttling\",\n        # \"--single-process\",\n        f\"--window-size={self.config.viewport_width},{self.config.viewport_height}\",\n    ]\n\n    if self.config.light_mode:\n        args.extend(BROWSER_DISABLE_OPTIONS)\n\n    if self.config.text_mode:\n        args.extend(\n            [\n                \"--blink-settings=imagesEnabled=false\",\n                \"--disable-remote-fonts\",\n                \"--disable-images\",\n                \"--disable-javascript\",\n                \"--disable-software-rasterizer\",\n                \"--disable-dev-shm-usage\",\n            ]\n        )\n\n    if self.config.extra_args:\n        args.extend(self.config.extra_args)\n\n    # Deduplicate args\n    args = list(dict.fromkeys(args))\n    \n    browser_args = {\"headless\": self.config.headless, \"args\": args}\n\n    if self.config.chrome_channel:\n        browser_args[\"channel\"] = self.config.chrome_channel\n\n    if self.config.accept_downloads:\n        browser_args[\"downloads_path\"] = self.config.downloads_path or os.path.join(\n            os.getcwd(), \"downloads\"\n        )\n        os.makedirs(browser_args[\"downloads_path\"], exist_ok=True)\n\n    if self.config.proxy or self.config.proxy_config:\n        from playwright.async_api import ProxySettings\n\n        proxy_settings = (\n            ProxySettings(server=self.config.proxy)\n            if self.config.proxy\n            else ProxySettings(\n                server=self.config.proxy_config.server,\n                username=self.config.proxy_config.username,\n                password=self.config.proxy_config.password,\n            )\n        )\n        browser_args[\"proxy\"] = proxy_settings\n\n    return browser_args\n```\n\n----------------------------------------\n\nTITLE: Using Schema Generation Utility with Crawl4AI\nDESCRIPTION: Code demonstrating how to automatically generate extraction schemas using LLM integrations. Shows two options: using OpenAI's GPT-4 or the open-source Ollama model, and how to apply the generated schema for extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_191\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, JsonXPathExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Sample HTML with product information\nhtml = \"\"\"\n<div class=\"product-card\">\n    <h2 class=\"title\">Gaming Laptop</h2>\n    <div class=\"price\">$999.99</div>\n    <div class=\"specs\">\n        <ul>\n            <li>16GB RAM</li>\n            <li>1TB SSD</li>\n        </ul>\n    </div>\n</div>\n\"\"\"\n\n# Option 1: Using OpenAI (requires API token)\ncss_schema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"css\", \n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")\n)\n\n# Option 2: Using Ollama (open source, no token needed)\nxpath_schema = JsonXPathExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"xpath\",\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the generated schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(css_schema)\n```\n\n----------------------------------------\n\nTITLE: Displaying Batch Log Output in Python for Crawl4AI Stress Testing\nDESCRIPTION: This code snippet shows the format of the batch log output when running test_stress_sdk.py without the --stream flag. It displays per-batch summary lines including progress, memory usage, processing speed, and status.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/tests/memory/README.md#2025-04-23_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n Batch | Progress | Start Mem | End Mem   | URLs/sec | Success/Fail | Time (s) | Status\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n 1     |  10.0%   |  50.1 MB  |  55.3 MB  |    23.8    |    10/0      |     0.42   | Success\n 2     |  20.0%   |  55.3 MB  |  60.1 MB  |    24.1    |    10/0      |     0.41   | Success\n ...\n```\n\n----------------------------------------\n\nTITLE: Storage State JSON Structure Example\nDESCRIPTION: Example structure showing how to format the storage_state JSON with cookies and localStorage data\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"cookies\": [\n    {\n      \"name\": \"session\",\n      \"value\": \"abcd1234\",\n      \"domain\": \"example.com\",\n      \"path\": \"/\",\n      \"expires\": 1675363572.037711,\n      \"httpOnly\": false,\n      \"secure\": false,\n      \"sameSite\": \"None\"\n    }\n  ],\n  \"origins\": [\n    {\n      \"origin\": \"https://example.com\",\n      \"localStorage\": [\n        { \"name\": \"token\", \"value\": \"my_auth_token\" },\n        { \"name\": \"refreshToken\", \"value\": \"my_refresh_token\" }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Stopping and Removing Crawl4AI Docker Container (Bash)\nDESCRIPTION: Provides the command to first stop the running container named `crawl4ai` and then remove it using `docker stop` and `docker rm`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker stop crawl4ai && docker rm crawl4ai\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Crawler4ai Pipeline\nDESCRIPTION: Advanced example showing how to create a custom processing pipeline with Crawler4ai that loads pages, processes them, and exports the results in a specific format.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom crawler4ai import Crawler4ai, Pipeline, processors\n\n# Create a pipeline for processing\npipeline = Pipeline([\n    processors.HtmlCleaner(),\n    processors.TextExtractor(),\n    processors.LanguageDetector(),\n    processors.Summarizer(max_length=200),\n    processors.JsonExporter(output_file=\"summaries.json\")\n])\n\n# Initialize crawler with the pipeline\ncrawler = Crawler4ai(\n    urls=[\"https://example.com/blog\"],\n    max_pages=25,\n    pipeline=pipeline\n)\n\n# Run the crawl and processing\ncrawler.crawl()\n```\n\n----------------------------------------\n\nTITLE: CrawlResult and MarkdownGenerationResult Model Definition in Python\nDESCRIPTION: The core schema definition for the CrawlResult and MarkdownGenerationResult classes in Crawl4AI. This model captures all aspects of a crawl operation including HTML, screenshot, PDF, markdown output, links, media, and more.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MarkdownGenerationResult(BaseModel):\n    raw_markdown: str\n    markdown_with_citations: str\n    references_markdown: str\n    fit_markdown: Optional[str] = None\n    fit_html: Optional[str] = None\n\nclass CrawlResult(BaseModel):\n    url: str\n    html: str\n    success: bool\n    cleaned_html: Optional[str] = None\n    media: Dict[str, List[Dict]] = {}\n    links: Dict[str, List[Dict]] = {}\n    downloaded_files: Optional[List[str]] = None\n    screenshot: Optional[str] = None\n    pdf : Optional[bytes] = None\n    mhtml: Optional[str] = None\n    markdown: Optional[Union[str, MarkdownGenerationResult]] = None\n    extracted_content: Optional[str] = None\n    metadata: Optional[dict] = None\n    error_message: Optional[str] = None\n    session_id: Optional[str] = None\n    response_headers: Optional[dict] = None\n    status_code: Optional[int] = None\n    ssl_certificate: Optional[SSLCertificate] = None\n    class Config:\n        arbitrary_types_allowed = True\n```\n\n----------------------------------------\n\nTITLE: Crawling Dynamic Content with JavaScript Execution in Python\nDESCRIPTION: This function demonstrates advanced multi-page crawling with JavaScript execution using Crawl4AI. It extracts commit information from GitHub's TypeScript repository across multiple pages by simulating next page clicks.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_140\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl_dynamic_content_pages_method_2():\n    print(\"\\n--- Advanced Multi-Page Crawling with JavaScript Execution ---\")\n\n    browser_config = BrowserConfig(headless=False, java_script_enabled=True)\n\n    js_next_page_and_wait = \"\"\"\n    (async () => {\n        const getCurrentCommit = () => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            return commits.length > 0 ? commits[0].textContent.trim() : null;\n        };\n\n        const initialCommit = getCurrentCommit();\n        const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n        if (button) button.click();\n\n        while (true) {\n            await new Promise(resolve => setTimeout(resolve, 100));\n            const newCommit = getCurrentCommit();\n            if (newCommit && newCommit !== initialCommit) {\n                break;\n            }\n        }\n    })();\n    \"\"\"\n\n    schema = {\n        \"name\": \"Commit Extractor\",\n        \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n        \"fields\": [\n            {\n                \"name\": \"title\",\n                \"selector\": \"h4.markdown-title\",\n                \"type\": \"text\",\n                \"transform\": \"strip\",\n            },\n        ],\n    }\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        session_id = \"typescript_commits_session\"\n        all_commits = []\n\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        for page in range(3):\n            crawler_config = CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                css_selector=\"li.Box-sc-g0xbh4-0\",\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page_and_wait if page > 0 else None,\n                js_only=page > 0,\n                session_id=session_id,\n            )\n\n            result = await crawler.arun(url=url, config=crawler_config)\n            assert result.success, f\"Failed to crawl page {page + 1}\"\n\n            commits = json.loads(result.extracted_content)\n            all_commits.extend(commits)\n            print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Crawling with Iframe Processing in Crawl4AI\nDESCRIPTION: This example shows how to use the AsyncWebCrawler with a configuration that processes iframes. It demonstrates the full usage pattern including the asynchronous context manager and error handling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        process_iframes=True,\n        remove_overlay_elements=True\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.org/iframe-demo\", \n            config=config\n        )\n        print(\"Iframe-merged length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Configuring Network and Console Capture in Crawl4AI with Python\nDESCRIPTION: This Python code snippet illustrates how to configure `CrawlerRunConfig` in the `crawl4ai` library to enable debugging features. Setting `capture_network=True` logs network traffic, `capture_console=True` captures browser console output, and `mhtml=True` saves MHTML snapshots of the crawled pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ncrawler_config = CrawlerRunConfig(\n    capture_network=True,\n    capture_console=True,\n    mhtml=True\n)\n```\n\n----------------------------------------\n\nTITLE: Defining E-commerce Product Catalog Schema with CSS Selectors in Python\nDESCRIPTION: A comprehensive schema definition for extracting e-commerce product data including categories, products, details, features, reviews, and related products using CSS selectors. The schema demonstrates nested structures for complex product hierarchies.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    # (1) We can define optional baseFields if we want to extract attributes \n    # from the category container\n    \"baseFields\": [\n        {\"name\": \"data_cat_id\", \"type\": \"attribute\", \"attribute\": \"data-cat-id\"}, \n    ],\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",    # repeated sub-objects\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",  # single sub-object\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\"name\": \"feature\", \"type\": \"text\"} \n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\", \n                            \"selector\": \"span.reviewer\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\", \n                            \"selector\": \"span.rating\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\", \n                            \"selector\": \"p.review-text\", \n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\", \n                            \"selector\": \"span.related-name\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\", \n                            \"selector\": \"span.related-price\", \n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Filtered Markdown with Crawl4AI in Python\nDESCRIPTION: This snippet demonstrates how to customize Markdown generation using a content filter. It initializes a `DefaultMarkdownGenerator` with a `PruningContentFilter`, which removes less relevant content based on a threshold. This generator is then passed to `CrawlerRunConfig`. The example crawls 'https://news.ycombinator.com' and prints the lengths of both the raw Markdown (`result.markdown.raw_markdown`) and the filtered Markdown (`result.markdown.fit_markdown`) to show the effect of the filter.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\")\n)\n\nconfig = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    markdown_generator=md_generator\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://news.ycombinator.com\", config=config)\n    print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n    print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\n```\n\n----------------------------------------\n\nTITLE: Configuring AsyncWebCrawler for Lazy-Loaded Images in Python\nDESCRIPTION: This snippet demonstrates how to configure the AsyncWebCrawler to handle lazy-loaded images. It uses wait_for_images, scan_full_page, and scroll_delay options to ensure all images are captured during the crawl.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/lazy-loading.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig\nfrom crawl4ai.async_configs import CacheMode\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Force the crawler to wait until images are fully loaded\n        wait_for_images=True,\n\n        # Option 1: If you want to automatically scroll the page to load images\n        scan_full_page=True,  # Tells the crawler to try scrolling the entire page\n        scroll_delay=0.5,     # Delay (seconds) between scroll steps\n\n        # Option 2: If the site uses a 'Load More' or JS triggers for images,\n        # you can also specify js_code or wait_for logic here.\n\n        cache_mode=CacheMode.BYPASS,\n        verbose=True\n    )\n\n    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:\n        result = await crawler.arun(\"https://www.example.com/gallery\", config=config)\n        \n        if result.success:\n            images = result.media.get(\"images\", [])\n            print(\"Images found:\", len(images))\n            for i, img in enumerate(images[:5]):\n                print(f\"[Image {i}] URL: {img['src']}, Score: {img.get('score','N/A')}\")\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Initializing Web Crawler Class with Configuration Parameters in Python\nDESCRIPTION: The __init__ method of a web crawler class with comprehensive parameter definitions for controlling crawling behavior. Parameters cover content processing, page navigation, caching, media handling, link processing, and debugging options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n    self,\n    # Content Processing Parameters\n    word_count_threshold: int = MIN_WORD_THRESHOLD,\n    extraction_strategy: ExtractionStrategy = None,\n    chunking_strategy: ChunkingStrategy = RegexChunking(),\n    markdown_generator: MarkdownGenerationStrategy = DefaultMarkdownGenerator(),\n    only_text: bool = False,\n    css_selector: str = None,\n    target_elements: List[str] = None,\n    excluded_tags: list = None,\n    excluded_selector: str = None,\n    keep_data_attributes: bool = False,\n    keep_attrs: list = None,\n    remove_forms: bool = False,\n    prettiify: bool = False,\n    parser_type: str = \"lxml\",\n    scraping_strategy: ContentScrapingStrategy = None,\n    proxy_config: Union[ProxyConfig, dict, None] = None,\n    proxy_rotation_strategy: Optional[ProxyRotationStrategy] = None,\n    # SSL Parameters\n    fetch_ssl_certificate: bool = False,\n    # Caching Parameters\n    cache_mode: CacheMode = CacheMode.BYPASS,\n    session_id: str = None,\n    bypass_cache: bool = False,\n    disable_cache: bool = False,\n    no_cache_read: bool = False,\n    no_cache_write: bool = False,\n    shared_data: dict = None,\n    # Page Navigation and Timing Parameters\n    wait_until: str = \"domcontentloaded\",\n    page_timeout: int = PAGE_TIMEOUT,\n    wait_for: str = None,\n    wait_for_images: bool = False,\n    delay_before_return_html: float = 0.1,\n    mean_delay: float = 0.1,\n    max_range: float = 0.3,\n    semaphore_count: int = 5,\n    # Page Interaction Parameters\n    js_code: Union[str, List[str]] = None,\n    js_only: bool = False,\n    ignore_body_visibility: bool = True,\n    scan_full_page: bool = False,\n    scroll_delay: float = 0.2,\n    process_iframes: bool = False,\n    remove_overlay_elements: bool = False,\n    simulate_user: bool = False,\n    override_navigator: bool = False,\n    magic: bool = False,\n    adjust_viewport_to_content: bool = False,\n    # Media Handling Parameters\n    screenshot: bool = False,\n    screenshot_wait_for: float = None,\n    screenshot_height_threshold: int = SCREENSHOT_HEIGHT_TRESHOLD,\n    pdf: bool = False,\n    capture_mhtml: bool = False,\n    image_description_min_word_threshold: int = IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD,\n    image_score_threshold: int = IMAGE_SCORE_THRESHOLD,\n    table_score_threshold: int = 7,\n    exclude_external_images: bool = False,\n    exclude_all_images: bool = False,\n    # Link and Domain Handling Parameters\n    exclude_social_media_domains: list = None,\n    exclude_external_links: bool = False,\n    exclude_social_media_links: bool = False,\n    exclude_domains: list = None,\n    exclude_internal_links: bool = False,\n    # Debugging and Logging Parameters\n    verbose: bool = True,\n    log_console: bool = False,\n    # Network and Console Capturing Parameters\n    capture_network_requests: bool = False,\n    capture_console_messages: bool = False,\n    # Connection Parameters\n    method: str = \"GET\",\n    stream: bool = False,\n    url: str = None,\n```\n\n----------------------------------------\n\nTITLE: Implementing Browser Stop Command in Python\nDESCRIPTION: Implementation of the 'crwl browser stop' command that terminates a running builtin browser instance. It first checks if a browser is running and then calls the BrowserProfiler to kill the process.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n@browser_cmd.command(\"stop\")\ndef browser_stop_cmd():\n    \"\"\"Stop the running builtin browser\"\"\"\n    profiler = BrowserProfiler()\n    \n    try:\n        # First check if browser is running\n        status = anyio.run(profiler.get_builtin_browser_status)\n        if not status[\"running\"]:\n            console.print(Panel(\n                \"[yellow]No builtin browser is currently running[/yellow]\",\n                title=\"Builtin Browser Stop\",\n                border_style=\"yellow\"\n            ))\n            return\n            \n        console.print(Panel(\n            \"[cyan]Stopping builtin browser...[/cyan]\",\n            title=\"Builtin Browser Stop\", \n            border_style=\"cyan\"\n        ))\n        \n        success = anyio.run(profiler.kill_builtin_browser)\n        \n        if success:\n            console.print(Panel(\n                \"[green]Builtin browser stopped successfully[/green]\",\n                title=\"Builtin Browser Stop\",\n                border_style=\"green\"\n            ))\n        else:\n            console.print(Panel(\n                \"[red]Failed to stop builtin browser[/red]\",\n                title=\"Builtin Browser Stop\",\n                border_style=\"red\"\n            ))\n            sys.exit(1)\n            \n    except Exception as e:\n        console.print(f\"[red]Error stopping builtin browser: {str(e)}[/red]\")\n        sys.exit(1)\n```\n\n----------------------------------------\n\nTITLE: Managing Browser Profiles with BrowserProfiler (Python)\nDESCRIPTION: Python script showcasing the usage of the BrowserProfiler class for creating, listing, and deleting browser profiles in Crawl4AI.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import BrowserProfiler\n\nasync def manage_profiles():\n    # Create a profiler instance\n    profiler = BrowserProfiler()\n    \n    # Create a profile interactively - opens a browser window\n    profile_path = await profiler.create_profile(\n        profile_name=\"my-login-profile\"  # Optional: name your profile\n    )\n    \n    print(f\"Profile saved at: {profile_path}\")\n    \n    # List all available profiles\n    profiles = profiler.list_profiles()\n    \n    for profile in profiles:\n        print(f\"Profile: {profile['name']}\")\n        print(f\"  Path: {profile['path']}\")\n        print(f\"  Created: {profile['created']}\")\n        print(f\"  Browser type: {profile['type']}\")\n    \n    # Get a specific profile path by name\n    specific_profile = profiler.get_profile_path(\"my-login-profile\")\n    \n    # Delete a profile when no longer needed\n    success = profiler.delete_profile(\"old-profile-name\")\n    \nasyncio.run(manage_profiles())\n```\n\n----------------------------------------\n\nTITLE: Implementing Non-Streaming Web Crawler in Python\nDESCRIPTION: Basic implementation of a web crawler in non-streaming mode using AsyncWebCrawler. Uses BFS strategy with a maximum depth of 1 to collect all results before processing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),\n    stream=False  # Default behavior\n)\n\nasync with AsyncWebCrawler() as crawler:\n    # Wait for ALL results to be collected before returning\n    results = await crawler.arun(\"https://example.com\", config=config)\n    \n    for result in results:\n        process_result(result)\n```\n\n----------------------------------------\n\nTITLE: Configuring CosineStrategy for Mixed Content Pages in Python\nDESCRIPTION: Shows how to set up CosineStrategy to handle pages with mixed content types, using more flexible matching and clustering parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# For mixed content pages\nstrategy = CosineStrategy(\n    semantic_filter=\"product features\",\n    sim_threshold=0.4,      # More flexible matching\n    max_dist=0.3,          # Larger clusters\n    top_k=3                # Multiple relevant sections\n)\n```\n\n----------------------------------------\n\nTITLE: Docker Deployment Commands for Crawl4AI\nDESCRIPTION: These bash commands demonstrate how to build and run a Docker container for Crawl4AI. The Docker image includes a FastAPI server with both streaming and non-streaming endpoints for crawling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Build the image (from the project root)\ndocker build -t crawl4ai .\n\n# Run the container\ndocker run -d -p 8000:8000 --name crawl4ai crawl4ai\n```\n\n----------------------------------------\n\nTITLE: Creating SEO Metadata Filter with Crawl4AI (Python)\nDESCRIPTION: Creates an SEOFilter instance configured with a minimum score threshold and a list of target keywords to evaluate web page metadata. Integrates this filter into a BFSDeepCrawlStrategy within a CrawlerRunConfig to limit crawling to pages meeting SEO criteria. Requires the Crawl4AI Python library with relevant filter modules.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Create an SEO filter that looks for specific keywords in page metadata\\nseo_filter = SEOFilter(\\n    threshold=0.5,  # Minimum score (0.0 to 1.0)\\n    keywords=[\\\"tutorial\\\", \\\"guide\\\", \\\"documentation\\\"]\\n)\\n\\nconfig = CrawlerRunConfig(\\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\\n        max_depth=1,\\n        filter_chain=FilterChain([seo_filter])\\n    )\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Error Handling Example\nDESCRIPTION: Example demonstrating proper error handling for extraction operations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    if result.success:\n        content = json.loads(result.extracted_content)\nexcept Exception as e:\n    print(f\"Extraction failed: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Scraping Strategy in Python\nDESCRIPTION: This code demonstrates how to create a custom scraping strategy by inheriting from ContentScrapingStrategy. It includes the structure for implementing custom scraping logic and returning a ScrapingResult object with cleaned HTML, media items, links, and metadata.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import ContentScrapingStrategy, ScrapingResult, MediaItem, Media, Link, Links\n\nclass CustomScrapingStrategy(ContentScrapingStrategy):\n    def scrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n        # Implement your custom scraping logic here\n        return ScrapingResult(\n            cleaned_html=\"<html>...</html>\",  # Cleaned HTML content\n            success=True,                     # Whether scraping was successful\n            media=Media(\n                images=[                      # List of images found\n                    MediaItem(\n                        src=\"https://example.com/image.jpg\",\n                        alt=\"Image description\",\n                        desc=\"Surrounding text\",\n                        score=1,\n                        type=\"image\",\n                        group_id=1,\n                        format=\"jpg\",\n                        width=800\n                    )\n                ],\n                videos=[],                    # List of videos (same structure as images)\n                audios=[]                     # List of audio files (same structure as images)\n            ),\n            links=Links(\n                internal=[                    # List of internal links\n                    Link(\n                        href=\"https://example.com/page\",\n                        text=\"Link text\",\n                        title=\"Link title\",\n                        base_domain=\"example.com\"\n                    )\n                ],\n                external=[]                   # List of external links (same structure)\n            ),\n            metadata={                        # Additional metadata\n                \"title\": \"Page Title\",\n                \"description\": \"Page description\"\n            }\n        )\n\n    async def ascrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n        # For simple cases, you can use the sync version\n        return await asyncio.to_thread(self.scrap, url, html, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Launching Playwright Chromium with Custom User Data Directory (Bash/PowerShell)\nDESCRIPTION: Commands to launch the Playwright-managed Chromium binary with a custom user data directory. This setup allows for creating and maintaining persistent browser profiles.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n~/.cache/ms-playwright/chromium-1234/chrome-linux/chrome \\\n    --user-data-dir=/home/<you>/my_chrome_profile\n```\n\nLANGUAGE: bash\nCODE:\n```\n~/Library/Caches/ms-playwright/chromium-1234/chrome-mac/Chromium.app/Contents/MacOS/Chromium \\\n    --user-data-dir=/Users/<you>/my_chrome_profile\n```\n\nLANGUAGE: powershell\nCODE:\n```\n\"C:\\Users\\<you>\\AppData\\Local\\ms-playwright\\chromium-1234\\chrome-win\\chrome.exe\" ^\n    --user-data-dir=\"C:\\Users\\<you>\\my_chrome_profile\"\n```\n\n----------------------------------------\n\nTITLE: Running Basic Manually Built Crawl4AI Container (Bash)\nDESCRIPTION: Shows the `docker run` command to start a container from a locally built image (`crawl4ai-local:latest`) without LLM support. Uses standard options like detached mode (`-d`), port mapping (`-p`), container naming (`--name`), and shared memory size (`--shm-size`).\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d \\\n  -p 11235:11235 \\\n  --name crawl4ai-standalone \\\n  --shm-size=1g \\\n  crawl4ai-local:latest\n```\n\n----------------------------------------\n\nTITLE: Streaming Crawl Results with Crawl4AI REST API\nDESCRIPTION: Shows how to stream crawl results asynchronously using the Crawl4AI REST API, processing multiple URLs and handling the streaming response.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_63\n\nLANGUAGE: python\nCODE:\n```\nasync def test_stream_crawl(session, token: str):\n    \"\"\"Test the /crawl/stream endpoint with multiple URLs.\"\"\"\n    url = \"http://localhost:8000/crawl/stream\"\n    payload = {\n        \"urls\": [\n            \"https://example.com\",\n            \"https://example.com/page1\",  \n            \"https://example.com/page2\",  \n            \"https://example.com/page3\",  \n        ],\n        \"browser_config\": {\"headless\": True, \"viewport\": {\"width\": 1200}},\n        \"crawler_config\": {\"stream\": True, \"cache_mode\": \"bypass\"}\n    }\n\n    # headers = {\"Authorization\": f\"Bearer {token}\"} # If JWT is enabled, more on this later\n    \n    try:\n        async with session.post(url, json=payload, headers=headers) as response:\n            status = response.status\n            print(f\"Status: {status} (Expected: 200)\")\n            assert status == 200, f\"Expected 200, got {status}\"\n            \n            # Read streaming response line-by-line (NDJSON)\n            async for line in response.content:\n                if line:\n                    data = json.loads(line.decode('utf-8').strip())\n                    print(f\"Streamed Result: {json.dumps(data, indent=2)}\")\n    except Exception as e:\n        print(f\"Error in streaming crawl test: {str(e)}\")\n```\n\n----------------------------------------\n\nTITLE: Running Crawl4AI Setup Command\nDESCRIPTION: Command to run the setup process after installation, which installs required browsers and performs environment checks.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_79\n\nLANGUAGE: bash\nCODE:\n```\ncrawl4ai-setup\n```\n\n----------------------------------------\n\nTITLE: Limiting Crawl Size by Page Count in BFS Strategy (Python)\nDESCRIPTION: Shows how to set a hard upper bound on the number of pages crawled regardless of depth using max_pages in the BFS strategy. Configures BFSDeepCrawlStrategy with both max_depth and max_pages, enabling cost control and test runs. Requires Crawl4AI environment and BFSDeepCrawlStrategy definition.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Limit to exactly 20 pages regardless of depth\\nstrategy = BFSDeepCrawlStrategy(\\n    max_depth=3,\\n    max_pages=20\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: CrawlResult Class Definition in Python\nDESCRIPTION: Definition of the CrawlResult class that encapsulates everything returned after a crawl operation, including the raw HTML, processed content, media information, links, and optional metadata.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass CrawlResult(BaseModel):\n    url: str\n    html: str\n    success: bool\n    cleaned_html: Optional[str] = None\n    media: Dict[str, List[Dict]] = {}\n    links: Dict[str, List[Dict]] = {}\n    downloaded_files: Optional[List[str]] = None\n    screenshot: Optional[str] = None\n    pdf : Optional[bytes] = None\n    mhtml: Optional[str] = None\n    markdown: Optional[Union[str, MarkdownGenerationResult]] = None\n    extracted_content: Optional[str] = None\n    metadata: Optional[dict] = None\n    error_message: Optional[str] = None\n    session_id: Optional[str] = None\n    response_headers: Optional[dict] = None\n    status_code: Optional[int] = None\n    ssl_certificate: Optional[SSLCertificate] = None\n    dispatch_result: Optional[DispatchResult] = None\n    ...\n```\n\n----------------------------------------\n\nTITLE: Cosine Similarity Extraction for Web Content in Python\nDESCRIPTION: This function demonstrates the use of cosine similarity strategy for content extraction. It uses various parameters like word count threshold, similarity threshold, and semantic filtering to extract relevant content from a web page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_141\n\nLANGUAGE: python\nCODE:\n```\nasync def cosine_similarity_extraction():\n    from crawl4ai.extraction_strategy import CosineStrategy\n    crawl_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=CosineStrategy(\n            word_count_threshold=10,\n            max_dist=0.2,  # Maximum distance between two words\n            linkage_method=\"ward\",  # Linkage method for hierarchical clustering (ward, complete, average, single)\n            top_k=3,  # Number of top keywords to extract\n            sim_threshold=0.3,  # Similarity threshold for clustering\n            semantic_filter=\"McDonald's economic impact, American consumer trends\",  # Keywords to filter the content semantically using embeddings\n            verbose=True,\n        ),\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business/consumer/how-mcdonalds-e-coli-crisis-inflation-politics-reflect-american-story-rcna177156\",\n            config=crawl_config,\n        )\n        print(json.loads(result.extracted_content)[:5])\n```\n\n----------------------------------------\n\nTITLE: Implementing Proxy Rotation with AsyncWebCrawler in Python\nDESCRIPTION: An async function that demonstrates how to configure and use proxy rotation with AsyncWebCrawler. It sets up a RoundRobinProxyStrategy with a list of proxy servers and configures the crawler to use this strategy. This example uses placeholder proxies that would need to be replaced with real ones.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_157\n\nLANGUAGE: python\nCODE:\n```\nasync def demo_proxy_rotation():\n    \"\"\"Proxy rotation for multiple requests\"\"\"\n    print(\"\\n=== 10. Proxy Rotation ===\")\n\n    # Example proxies (replace with real ones)\n    proxies = [\n        ProxyConfig(server=\"http://proxy1.example.com:8080\"),\n        ProxyConfig(server=\"http://proxy2.example.com:8080\"),\n    ]\n\n    proxy_strategy = RoundRobinProxyStrategy(proxies)\n\n    print(f\"Using {len(proxies)} proxies in rotation\")\n    print(\n        \"Note: This example uses placeholder proxies - replace with real ones to test\"\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        config = CrawlerRunConfig(\n            proxy_rotation_strategy=proxy_strategy\n        )\n\n        # In a real scenario, these would be run and the proxies would rotate\n        print(\"In a real scenario, requests would rotate through the available proxies\")\n```\n\n----------------------------------------\n\nTITLE: Mounting Custom Config in Docker Compose using YAML\nDESCRIPTION: Provides a YAML snippet for a `docker-compose.yml` file. It shows how to configure a service (e.g., `crawl4ai-hub-amd64`) to use a custom configuration file by adding a `volumes` section. The line `- ./my-custom-config.yml:/app/config.yml` mounts the local configuration file over the default one within the container. It also notes the need to keep existing volume mounts (like shared memory). Requires Docker Compose and the custom config file in the same directory as the compose file.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_35\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  crawl4ai-hub-amd64: # Or your chosen service\n    image: unclecode/crawl4ai:latest\n    profiles: [\"hub-amd64\"]\n    <<: *base-config\n    volumes:\n      # Mount local custom config over the default one in the container\n      - ./my-custom-config.yml:/app/config.yml\n      # Keep the shared memory volume from base-config\n      - /dev/shm:/dev/shm\n```\n\n----------------------------------------\n\nTITLE: Content Cleaning and Filtering with Crawl4AI\nDESCRIPTION: This function showcases content cleaning and filtering capabilities of Crawl4AI. It configures the crawler to exclude certain HTML tags, remove overlay elements, and apply content filtering. It then crawls a Wikipedia page and compares the lengths of full and filtered markdown content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_128\n\nLANGUAGE: python\nCODE:\n```\nasync def clean_content():\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        excluded_tags=[\"nav\", \"footer\", \"aside\"],\n        remove_overlay_elements=True,\n        markdown_generator=DefaultMarkdownGenerator(\n            content_filter=PruningContentFilter(\n                threshold=0.48, threshold_type=\"fixed\", min_word_threshold=0\n            ),\n            options={\"ignore_links\": True},\n        ),\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://en.wikipedia.org/wiki/Apple\",\n            config=crawler_config,\n        )\n        full_markdown_length = len(result.markdown.raw_markdown)\n        fit_markdown_length = len(result.markdown.fit_markdown)\n        print(f\"Full Markdown Length: {full_markdown_length}\")\n        print(f\"Fit Markdown Length: {fit_markdown_length}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing CrawlerRunConfig Class in Python\nDESCRIPTION: A comprehensive configuration class for controlling crawler runtime behavior including content extraction, page manipulation, caching, and timing parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass CrawlerRunConfig():\n    _UNWANTED_PROPS = {\n        'disable_cache' : 'Instead, use cache_mode=CacheMode.DISABLED',\n        'bypass_cache' : 'Instead, use cache_mode=CacheMode.BYPASS',\n        'no_cache_read' : 'Instead, use cache_mode=CacheMode.WRITE_ONLY',\n        'no_cache_write' : 'Instead, use cache_mode=CacheMode.READ_ONLY',\n    }\n\n    \"\"\"\n    Configuration class for controlling how the crawler runs each crawl operation.\n    This includes parameters for content extraction, page manipulation, waiting conditions,\n    caching, and other runtime behaviors.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Topic-Based Text Segmentation using TextTiling\nDESCRIPTION: Implements text chunking using NLTK's TextTilingTokenizer to create topic-coherent segments. Useful for content-aware text division.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom nltk.tokenize import TextTilingTokenizer\n\nclass TopicSegmentationChunking:\n    def __init__(self):\n        self.tokenizer = TextTilingTokenizer()\n\n    def chunk(self, text):\n        return self.tokenizer.tokenize(text)\n\n# Example Usage\ntext = \"\"\"This is an introduction.\nThis is a detailed discussion on the topic.\"\"\"\nchunker = TopicSegmentationChunking()\nprint(chunker.chunk(text))\n```\n\n----------------------------------------\n\nTITLE: Implementing Semaphore Dispatcher in Python\nDESCRIPTION: Sets up a SemaphoreDispatcher for simple concurrency control with fixed limits, including optional rate limiting and monitoring.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_dispatcher import SemaphoreDispatcher\n\ndispatcher = SemaphoreDispatcher(\n    max_session_permit=20,         # Maximum concurrent tasks\n    rate_limiter=RateLimiter(      # Optional rate limiting\n        base_delay=(0.5, 1.0),\n        max_delay=10.0\n    ),\n    monitor=CrawlerMonitor(        # Optional monitoring\n        max_visible_rows=15,\n        display_mode=DisplayMode.DETAILED\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing KeywordRelevanceScorer Class for URL Keyword Matching in Python\nDESCRIPTION: A scorer that evaluates URLs based on the presence of specific keywords, with case sensitivity options and performance optimization through caching. Scores are calculated based on the proportion of matched keywords.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_202\n\nLANGUAGE: python\nCODE:\n```\nclass KeywordRelevanceScorer(URLScorer):\n    __slots__ = ('_weight', '_stats', '_keywords', '_case_sensitive')\n    \n    def __init__(self, keywords: List[str], weight: float = 1.0, case_sensitive: bool = False):\n        super().__init__(weight=weight)\n        self._case_sensitive = case_sensitive\n        # Pre-process keywords once\n        self._keywords = [k if case_sensitive else k.lower() for k in keywords]\n    \n    @lru_cache(maxsize=10000)\n    def _url_bytes(self, url: str) -> bytes:\n        \"\"\"Cache decoded URL bytes\"\"\"\n        return url.encode('utf-8') if self._case_sensitive else url.lower().encode('utf-8')\n    \n    \n    def _calculate_score(self, url: str) -> float:\n        \"\"\"Fast string matching without regex or byte conversion\"\"\"\n        if not self._case_sensitive:\n            url = url.lower()\n            \n        matches = sum(1 for k in self._keywords if k in url)\n        \n        # Fast return paths\n        if not matches:\n            return 0.0\n        if matches == len(self._keywords):\n            return 1.0\n            \n        return matches / len(self._keywords)\n```\n\n----------------------------------------\n\nTITLE: Stopping Crawl4AI Service with Docker Compose (Bash)\nDESCRIPTION: Provides the command `docker compose down` to stop and remove the containers, networks, and volumes defined in the `docker-compose.yml` file.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# Stop the service\ndocker compose down\n```\n\n----------------------------------------\n\nTITLE: Deploying Crawl4AI to Cloud Environments in Python\nDESCRIPTION: Shows how to use the CloudDeployer component to set up and deploy a crawler service to AWS with auto-scaling and monitoring configurations. Includes deployment status and endpoint information retrieval.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.deploy import CloudDeployer\n\n# Initialize deployer\ndeployer = CloudDeployer()\n\n# Deploy crawler service\ndeployment = await deployer.deploy(\n    service_name=\"crawler-cluster\",\n    platform=\"aws\",  # or \"gcp\", \"azure\"\n    config={\n        \"instance_type\": \"compute-optimized\",\n        \"auto_scaling\": {\n            \"min_instances\": 2,\n            \"max_instances\": 10,\n            \"scale_based_on\": \"cpu_usage\"\n        },\n        \"region\": \"us-east-1\",\n        \"monitoring\": True\n    }\n)\n\n# Get deployment status and endpoints\nprint(f\"Service Status: {deployment.status}\")\nprint(f\"API Endpoint: {deployment.endpoint}\")\nprint(f\"Monitor URL: {deployment.monitor_url}\")\n```\n\n----------------------------------------\n\nTITLE: Install Crawl4AI with All Optional Features (Development Mode, Bash)\nDESCRIPTION: Installs Crawl4AI in editable mode along with all available optional dependencies for features like PyTorch, Transformers, Cosine Similarity, and Synchronous Crawling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[all]\"             # Install all optional features\n```\n\n----------------------------------------\n\nTITLE: Displaying Directory Structure for Crawl4AI Benchmarking Tools\nDESCRIPTION: This code snippet shows the directory structure for the Crawl4AI benchmarking tools, including locations for reports, test content, and script files.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/tests/memory/README.md#2025-04-23_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nbenchmarking/          # Or your top-level directory name\nâ”œâ”€â”€ benchmark_reports/ # Generated HTML reports (by benchmark_report.py)\nâ”œâ”€â”€ reports/           # Raw test result data (from test_stress_sdk.py)\nâ”œâ”€â”€ test_site/         # Generated test content (temporary)\nâ”œâ”€â”€ benchmark_report.py# Report generator\nâ”œâ”€â”€ run_benchmark.py   # Test runner with predefined configs\nâ”œâ”€â”€ test_stress_sdk.py # Main stress test implementation using arun_many\nâ””â”€â”€ run_all.sh         # Simple wrapper script (may need updates)\n#â””â”€â”€ requirements.txt   # Optional: Visualization dependencies for benchmark_report.py\n```\n\n----------------------------------------\n\nTITLE: User Simulation for Anti-Bot Evasion in Python Web Crawling\nDESCRIPTION: This function demonstrates how to use user simulation techniques to evade anti-bot measures. It configures the crawler with random user agents, simulates user behavior, and overrides navigator properties to mimic real user interactions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_143\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl_with_user_simulation():\n    browser_config = BrowserConfig(\n        headless=True,\n        user_agent_mode=\"random\",\n        user_agent_generator_config={\"device_type\": \"mobile\", \"os_type\": \"android\"},\n    )\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        magic=True,\n        simulate_user=True,\n        override_navigator=True,\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url=\"YOUR-URL-HERE\", config=crawler_config)\n        print(result.markdown)\n```\n\n----------------------------------------\n\nTITLE: Deprecated Caching Method in Crawl4AI (Python)\nDESCRIPTION: This code snippet demonstrates the old way of controlling caching in Crawl4AI using boolean flags. It uses the 'bypass_cache' parameter to skip the cache entirely when crawling a web page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cache-modes.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def use_proxy():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            bypass_cache=True  # Old way\n        )\n        print(len(result.markdown))\n\nasync def main():\n    await use_proxy()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Markdown Generation with Content Filtering\nDESCRIPTION: Example showing how to generate markdown output with content filtering using PruningContentFilter and DefaultMarkdownGenerator.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_123\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\")\n)\n\nconfig = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    markdown_generator=md_generator\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://news.ycombinator.com\", config=config)\n    print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n    print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Asynchronous Web Crawler in Python\nDESCRIPTION: This code snippet imports necessary modules and libraries for setting up an asynchronous web crawler. It includes version information, system modules, time tracking, color formatting, file path handling, typing, JSON parsing, and asynchronous operations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom .__version__ import __version__ as crawl4ai_version\nimport os\nimport sys\nimport time\nfrom colorama import Fore\nfrom pathlib import Path\nfrom typing import Optional, List\nimport json\nimport asyncio\n```\n\n----------------------------------------\n\nTITLE: Interactive Profile Management with BrowserProfiler (Python)\nDESCRIPTION: Python script demonstrating the use of BrowserProfiler's interactive management console for profile creation, listing, and deletion, along with a custom crawl function.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import BrowserProfiler, AsyncWebCrawler, BrowserConfig\n\n# Define a function to use a profile for crawling\nasync def crawl_with_profile(profile_path, url):\n    browser_config = BrowserConfig(\n        headless=True,\n        use_managed_browser=True,\n        user_data_dir=profile_path\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url)\n        return result\n\nasync def main():\n    # Create a profiler instance\n    profiler = BrowserProfiler()\n    \n    # Launch the interactive profile manager\n    # Passing the crawl function as a callback adds a \"crawl with profile\" option\n    await profiler.interactive_manager(crawl_callback=crawl_with_profile)\n    \nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Clusters from HTML Using Hierarchical Clustering in Python\nDESCRIPTION: Extracts and processes text clusters from HTML content using hierarchical clustering. It splits the HTML into chunks, filters them using embeddings, performs clustering, organizes texts by cluster labels, filters by word count, and assigns tags using NLP.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ndef extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]:\n    \"\"\"\n    Extract clusters from HTML content using hierarchical clustering.\n\n    Args:\n        url (str): The URL of the webpage.\n        html (str): The HTML content of the webpage.\n\n    Returns:\n        List[Dict[str, Any]]: A list of processed JSON blocks.\n    \"\"\"\n    # Assume `html` is a list of text chunks for this strategy\n    t = time.time()\n    text_chunks = html.split(self.DEL)  # Split by lines or paragraphs as needed\n\n    # Pre-filter documents using embeddings and semantic_filter\n    text_chunks = self.filter_documents_embeddings(\n        text_chunks, self.semantic_filter\n    )\n\n    if not text_chunks:\n        return []\n\n    # Perform clustering\n    labels = self.hierarchical_clustering(text_chunks)\n    # print(f\"[LOG] ðŸš€ Clustering done in {time.time() - t:.2f} seconds\")\n\n    # Organize texts by their cluster labels, retaining order\n    t = time.time()\n    clusters = {}\n    for index, label in enumerate(labels):\n        clusters.setdefault(label, []).append(text_chunks[index])\n\n    # Filter clusters by word count\n    filtered_clusters = self.filter_clusters_by_word_count(clusters)\n\n    # Convert filtered clusters to a sorted list of dictionaries\n    cluster_list = [\n        {\"index\": int(idx), \"tags\": [], \"content\": \" \".join(filtered_clusters[idx])}\n        for idx in sorted(filtered_clusters)\n    ]\n\n    if self.verbose:\n        print(f\"[LOG] ðŸš€ Assign tags using {self.device}\")\n\n    if self.device.type in [\"gpu\", \"cuda\", \"mps\", \"cpu\"]:\n        labels = self.nlp([cluster[\"content\"] for cluster in cluster_list])\n\n        for cluster, label in zip(cluster_list, labels):\n            cluster[\"tags\"] = label\n    # elif self.device.type == \"cpu\":\n    #     # Process the text with the loaded model\n    #     texts = [cluster['content'] for cluster in cluster_list]\n    #     # Batch process texts\n    #     docs = self.nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\n\n    #     for doc, cluster in zip(docs, cluster_list):\n    #         tok_k = self.top_k\n    #         top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k]\n    #         cluster['tags'] = [cat for cat, _ in top_categories]\n\n    # for cluster in  cluster_list:\n    #     doc = self.nlp(cluster['content'])\n    #     tok_k = self.top_k\n    #     top_categories = sorted(doc.cats.items(), key=lambda x: x[1], reverse=True)[:tok_k]\n    #     cluster['tags'] = [cat for cat, _ in top_categories]\n\n    if self.verbose:\n        print(f\"[LOG] ðŸš€ Categorization done in {time.time() - t:.2f} seconds\")\n\n    return cluster_list\n```\n\n----------------------------------------\n\nTITLE: Parallel Crawling of Multiple URLs in Python using Crawl4AI\nDESCRIPTION: This snippet demonstrates how to crawl multiple URLs concurrently using Crawl4AI's AsyncWebCrawler. It shows both streaming and batch modes for processing results, and includes error handling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_127\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def quick_parallel_example():\n    urls = [\n        \"https://example.com/page1\",\n        \"https://example.com/page2\",\n        \"https://example.com/page3\"\n    ]\n    \n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=True  # Enable streaming mode\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        # Stream results as they complete\n        async for result in await crawler.arun_many(urls, config=run_conf):\n            if result.success:\n                print(f\"[OK] {result.url}, length: {len(result.markdown.raw_markdown)}\")\n            else:\n                print(f\"[ERROR] {result.url} => {result.error_message}\")\n\n        # Or get all results at once (default behavior)\n        run_conf = run_conf.clone(stream=False)\n        results = await crawler.arun_many(urls, config=run_conf)\n        for res in results:\n            if res.success:\n                print(f\"[OK] {res.url}, length: {len(res.markdown.raw_markdown)}\")\n            else:\n                print(f\"[ERROR] {res.url} => {res.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(quick_parallel_example())\n```\n\n----------------------------------------\n\nTITLE: Processing Web Crawler Results to Extract Media and Links in Python\nDESCRIPTION: Code that processes web crawler results from Wikipedia to extract images and links (both internal and external). The extracted data is printed and saved to JSON files for further analysis. The code demonstrates how to access and manipulate the media and links properties of a CrawlResult object.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_155\n\nLANGUAGE: python\nCODE:\n```\nresult: List[CrawlResult] = await crawler.arun(\"https://en.wikipedia.org/wiki/Main_Page\")\n\nfor i, result in enumerate(result):\n    # Extract and save all images\n    images = result.media.get(\"images\", [])\n    print(f\"Found {len(images)} images\")\n\n    # Extract and save all links (internal and external)\n    internal_links = result.links.get(\"internal\", [])\n    external_links = result.links.get(\"external\", [])\n    print(f\"Found {len(internal_links)} internal links\")\n    print(f\"Found {len(external_links)} external links\")\n\n    # Print some of the images and links\n    for image in images[:3]:\n        print(f\"Image: {image['src']}\")\n    for link in internal_links[:3]:\n        print(f\"Internal link: {link['href']}\")\n    for link in external_links[:3]:\n        print(f\"External link: {link['href']}\")\n\n    # # Save everything to files\n    with open(f\"{__cur_dir__}/tmp/images.json\", \"w\") as f:\n        json.dump(images, f, indent=2)\n\n    with open(f\"{__cur_dir__}/tmp/links.json\", \"w\") as f:\n        json.dump(\n            {\"internal\": internal_links, \"external\": external_links},\n            f,\n            indent=2,\n        )\n```\n\n----------------------------------------\n\nTITLE: Implementing ContentRelevanceFilter Class in Python\nDESCRIPTION: This class implements a BM25-based relevance filter using the head section content of web pages. It includes methods for tokenization, document construction, and BM25 scoring.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_190\n\nLANGUAGE: python\nCODE:\n```\nclass ContentRelevanceFilter(URLFilter):\n    \"\"\"BM25-based relevance filter using head section content\"\"\"\n\n    __slots__ = (\"query_terms\", \"threshold\", \"k1\", \"b\", \"avgdl\")\n\n    def __init__(\n        self,\n        query: str,\n        threshold: float,\n        k1: float = 1.2,\n        b: float = 0.75,\n        avgdl: int = 1000,\n    ):\n        super().__init__(name=\"BM25RelevanceFilter\")\n        self.query_terms = self._tokenize(query)\n        self.threshold = threshold\n        self.k1 = k1  # TF saturation parameter\n        self.b = b  # Length normalization parameter\n        self.avgdl = avgdl  # Average document length (empirical value)\n\n    async def apply(self, url: str) -> bool:\n        head_content = await HeadPeekr.peek_html(url)\n        if not head_content:\n            self._update_stats(False)\n            return False\n\n        # Field extraction with weighting\n        fields = {\n            \"title\": HeadPeekr.get_title(head_content) or \"\",\n            \"meta\": HeadPeekr.extract_meta_tags(head_content),\n        }\n        doc_text = self._build_document(fields)\n\n        score = self._bm25(doc_text)\n        decision = score >= self.threshold\n        self._update_stats(decision)\n        return decision\n\n    def _build_document(self, fields: Dict) -> str:\n        \"\"\"Weighted document construction\"\"\"\n        return \" \".join(\n            [\n                fields[\"title\"] * 3,  # Title weight\n                fields[\"meta\"].get(\"description\", \"\") * 2,\n                fields[\"meta\"].get(\"keywords\", \"\"),\n                \" \".join(fields[\"meta\"].values()),\n            ]\n        )\n\n    def _tokenize(self, text: str) -> List[str]:\n        \"\"\"Fast case-insensitive tokenization\"\"\"\n        return text.lower().split()\n\n    def _bm25(self, document: str) -> float:\n        \"\"\"Optimized BM25 implementation for head sections\"\"\"\n        doc_terms = self._tokenize(document)\n        doc_len = len(doc_terms)\n        tf = defaultdict(int)\n\n        for term in doc_terms:\n            tf[term] += 1\n\n        score = 0.0\n        for term in set(self.query_terms):\n            term_freq = tf[term]\n            idf = math.log((1 + 1) / (term_freq + 0.5) + 1)  # Simplified IDF\n            numerator = term_freq * (self.k1 + 1)\n            denominator = term_freq + self.k1 * (\n                1 - self.b + self.b * (doc_len / self.avgdl)\n            )\n            score += idf * (numerator / denominator)\n\n        return score\n```\n\n----------------------------------------\n\nTITLE: Setting Up JSON CSS Extraction Strategy in Crawl4AI\nDESCRIPTION: Demonstrates how to configure a JSON CSS extraction strategy for Crawl4AI, specifying selectors for different fields to be extracted from web pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_59\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"crawler_config\": {\n        \"type\": \"CrawlerRunConfig\",\n        \"params\": {\n            \"extraction_strategy\": {\n                \"type\": \"JsonCssExtractionStrategy\",\n                \"params\": {\n                    \"schema\": {\n                        \"baseSelector\": \"article.post\",\n                        \"fields\": [\n                            {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n                            {\"name\": \"content\", \"selector\": \".content\", \"type\": \"html\"}\n                        ]\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Form Interaction Example with JavaScript in Crawl4AI\nDESCRIPTION: This snippet demonstrates how to interact with a search form on a website by filling in fields and submitting the form using JavaScript. It configures the crawler to wait for specific CSS elements after form submission.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_117\n\nLANGUAGE: python\nCODE:\n```\njs_form_interaction = \"\"\"\ndocument.querySelector('#your-search').value = 'TypeScript commits';\ndocument.querySelector('form').submit();\n\"\"\"\n\nconfig = CrawlerRunConfig(\n    js_code=js_form_interaction,\n    wait_for=\"css:.commit\"\n)\nresult = await crawler.arun(url=\"https://github.com/search\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Basic Web Crawler with Crawl4AI\nDESCRIPTION: Demonstrates how to initialize and use AsyncWebCrawler with default configurations to crawl a website and retrieve markdown content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    browser_config = BrowserConfig()  # Default browser configuration\n    run_config = CrawlerRunConfig()   # Default crawl run configuration\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: CrawlResult and MarkdownGenerationResult Model Definitions\nDESCRIPTION: This code snippet defines the core schema for CrawlResult and MarkdownGenerationResult models in Crawl4AI. It shows all available fields that capture different aspects of a crawl's result, including HTML variants, markdown generation, and structured extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nclass MarkdownGenerationResult(BaseModel):\n    raw_markdown: str\n    markdown_with_citations: str\n    references_markdown: str\n    fit_markdown: Optional[str] = None\n    fit_html: Optional[str] = None\n\nclass CrawlResult(BaseModel):\n    url: str\n    html: str\n    success: bool\n    cleaned_html: Optional[str] = None\n    media: Dict[str, List[Dict]] = {}\n    links: Dict[str, List[Dict]] = {}\n    downloaded_files: Optional[List[str]] = None\n    screenshot: Optional[str] = None\n    pdf : Optional[bytes] = None\n    mhtml: Optional[str] = None\n    markdown: Optional[Union[str, MarkdownGenerationResult]] = None\n    extracted_content: Optional[str] = None\n    metadata: Optional[dict] = None\n    error_message: Optional[str] = None\n    session_id: Optional[str] = None\n    response_headers: Optional[dict] = None\n    status_code: Optional[int] = None\n    ssl_certificate: Optional[SSLCertificate] = None\n    class Config:\n        arbitrary_types_allowed = True\n```\n\n----------------------------------------\n\nTITLE: Defining Deep Crawl Strategy Abstract Base Class in Python\nDESCRIPTION: This abstract base class defines the interface for deep crawling strategies. It includes methods for batch and streaming crawl modes, URL validation, link discovery, and resource cleanup. Concrete implementations must override abstract methods.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_165\n\nLANGUAGE: python\nCODE:\n```\nclass DeepCrawlStrategy(ABC):\n    \"\"\"\n    Abstract base class for deep crawling strategies.\n    \n    Core functions:\n      - arun: Main entry point that returns an async generator of CrawlResults.\n      - shutdown: Clean up resources.\n      - can_process_url: Validate a URL and decide whether to process it.\n      - _process_links: Extract and process links from a CrawlResult.\n    \"\"\"\n\n    @abstractmethod\n    async def _arun_batch(\n        self,\n        start_url: str,\n        crawler: AsyncWebCrawler,\n        config: CrawlerRunConfig,\n    ) -> List[CrawlResult]:\n        \"\"\"\n        Batch (non-streaming) mode:\n        Processes one BFS level at a time, then yields all the results.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def _arun_stream(\n        self,\n        start_url: str,\n        crawler: AsyncWebCrawler,\n        config: CrawlerRunConfig,\n    ) -> AsyncGenerator[CrawlResult, None]:\n        \"\"\"\n        Streaming mode:\n        Processes one BFS level at a time and yields results immediately as they arrive.\n        \"\"\"\n        pass\n    \n    async def arun(\n        self,\n        start_url: str,\n        crawler: AsyncWebCrawler,\n        config: Optional[CrawlerRunConfig] = None,\n    ) -> RunManyReturn:\n        \"\"\"\n        Traverse the given URL using the specified crawler.\n        \n        Args:\n            start_url (str): The URL from which to start crawling.\n            crawler (AsyncWebCrawler): The crawler instance to use.\n            crawler_run_config (Optional[CrawlerRunConfig]): Crawler configuration.\n        \n        Returns:\n            Union[CrawlResultT, List[CrawlResultT], AsyncGenerator[CrawlResultT, None]]\n        \"\"\"\n        if config is None:\n            raise ValueError(\"CrawlerRunConfig must be provided\")\n\n        if config.stream:\n            return self._arun_stream(start_url, crawler, config)\n        else:\n            return await self._arun_batch(start_url, crawler, config)\n\n    def __call__(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig):\n        return self.arun(start_url, crawler, config)\n\n    @abstractmethod\n    async def shutdown(self) -> None:\n        \"\"\"\n        Clean up resources used by the deep crawl strategy.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def can_process_url(self, url: str, depth: int) -> bool:\n        \"\"\"\n        Validate the URL format and apply custom filtering logic.\n        \n        Args:\n            url (str): The URL to validate.\n            depth (int): The current depth in the crawl.\n        \n        Returns:\n            bool: True if the URL should be processed, False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def link_discovery(\n        self,\n        result: CrawlResult,\n        source_url: str,\n        current_depth: int,\n        visited: Set[str],\n        next_level: List[tuple],\n        depths: Dict[str, int],\n    ) -> None:\n        \"\"\"\n        Extract and process links from the given crawl result.\n        \n        This method should:\n          - Validate each extracted URL using can_process_url.\n          - Optionally score URLs.\n          - Append valid URLs (and their parent references) to the next_level list.\n          - Update the depths dictionary with the new depth for each URL.\n        \n        Args:\n            result (CrawlResult): The result from a crawl operation.\n            source_url (str): The URL from which this result was obtained.\n            current_depth (int): The depth at which the source URL was processed.\n            visited (Set[str]): Set of already visited URLs.\n            next_level (List[tuple]): List of tuples (url, parent_url) for the next BFS level.\n            depths (Dict[str, int]): Mapping of URLs to their current depth.\n        \"\"\"\n        pass\n```\n\n----------------------------------------\n\nTITLE: Content Selection Configuration\nDESCRIPTION: Settings for selecting specific content regions and excluding unwanted elements using CSS selectors.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    css_selector=\".main-content\",  # Focus on .main-content region only\n    excluded_tags=[\"form\", \"nav\"], # Remove entire tag blocks\n    remove_forms=True,             # Specifically strip <form> elements\n    remove_overlay_elements=True,  # Attempt to remove modals/popups\n)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up After Crawl4AI Benchmarking Tests in Bash\nDESCRIPTION: This Bash script demonstrates how to clean up files and directories after running Crawl4AI benchmarking tests, including removing test site content and generated reports.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/tests/memory/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Remove the test site content (if not using --keep-site)\nrm -rf test_site\n\n# Remove all raw reports and generated benchmark reports\nrm -rf reports benchmark_reports\n\n# Or use the --clean flag with run_benchmark.py\npython run_benchmark.py medium --clean\n```\n\n----------------------------------------\n\nTITLE: Tracking URL Redirections\nDESCRIPTION: Monitors and tracks URL redirections during crawling. Provides ability to compare initial and final URLs after redirection chain.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresult = await crawler.arun(url)\nprint(f\"Initial URL: {url}\")\nprint(f\"Final URL: {result.redirected_url}\")\n```\n\n----------------------------------------\n\nTITLE: Using Filtered Markdown from Crawl4AI Results\nDESCRIPTION: This example demonstrates how to use the filtered markdown output from a crawling operation. It shows how to access both the raw and filtered markdown after applying a content filter to the crawled content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_111\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator(\n            content_filter=PruningContentFilter(threshold=0.6),\n            options={\"ignore_links\": True}\n        )\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://news.example.com/tech\", config=config)\n        if result.success:\n            print(\"Raw markdown:\\n\", result.markdown)\n            \n            # If a filter is used, we also have .fit_markdown:\n            md_object = result.markdown  # or your equivalent\n            print(\"Filtered markdown:\\n\", md_object.fit_markdown)\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Complete Example of SSL Certificate Handling with Crawl4AI\nDESCRIPTION: A comprehensive example demonstrating how to integrate SSL certificate fetching and processing within a Crawl4AI crawler workflow. This shows the entire process from configuration to accessing and exporting certificate data.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport os\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    tmp_dir = \"tmp\"\n    os.makedirs(tmp_dir, exist_ok=True)\n\n    config = CrawlerRunConfig(\n        fetch_ssl_certificate=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        if result.success and result.ssl_certificate:\n            cert = result.ssl_certificate\n            # 1. Basic Info\n            print(\"Issuer CN:\", cert.issuer.get(\"CN\", \"\"))\n            print(\"Valid until:\", cert.valid_until)\n            print(\"Fingerprint:\", cert.fingerprint)\n            \n            # 2. Export\n            cert.to_json(os.path.join(tmp_dir, \"certificate.json\"))\n            cert.to_pem(os.path.join(tmp_dir, \"certificate.pem\"))\n            cert.to_der(os.path.join(tmp_dir, \"certificate.der\"))\n    \nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Crawl4AI CLI Usage Examples\nDESCRIPTION: These bash commands illustrate various ways to use the new Crawl4AI command-line interface (CLI). Examples include basic crawling, output formatting, configuration file usage, LLM-based extraction, and querying crawled content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Basic crawl\ncrwl https://example.com\n\n# Get markdown output\ncrwl https://example.com -o markdown\n\n# Use a configuration file\ncrwl https://example.com -B browser.yml -C crawler.yml\n\n# Use LLM-based extraction\ncrwl https://example.com -e extract.yml -s schema.json\n\n# Ask a question about the crawled content\ncrwl https://example.com -q \"What is the main topic?\"\n\n# See usage examples\ncrwl --example\n```\n\n----------------------------------------\n\nTITLE: Implementing E-commerce Data Extraction with crawl4ai\nDESCRIPTION: Complete example showing how to use the advanced e-commerce schema with crawl4ai to extract hierarchical product data. The code initializes a crawler, applies the schema using JsonCssExtractionStrategy, and processes the extracted JSON results.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_188\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\necommerce_schema = {\n    # ... the advanced schema from above ...\n}\n\nasync def extract_ecommerce_data():\n    strategy = JsonCssExtractionStrategy(ecommerce_schema, verbose=True)\n    \n    config = CrawlerRunConfig()\n    \n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=strategy,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n        \n        # Parse the JSON output\n        data = json.loads(result.extracted_content)\n        print(json.dumps(data, indent=2) if data else \"No data found.\")\n\nasyncio.run(extract_ecommerce_data())\n```\n\n----------------------------------------\n\nTITLE: Composite Score Calculation for HTML Content Relevance\nDESCRIPTION: Method to compute a composite score for an HTML node based on various metrics like text density, link density, tag weight, and more. Used to determine which nodes to keep or remove during pruning.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_98\n\nLANGUAGE: python\nCODE:\n```\ndef _compute_composite_score(self, metrics, text_len, tag_len, link_text_len):\n    \"\"\"Computes the composite score\"\"\"\n    if self.min_word_threshold:\n        # Get raw text from metrics node - avoid extra processing\n        text = metrics[\"node\"].get_text(strip=True)\n        word_count = text.count(\" \") + 1\n        if word_count < self.min_word_threshold:\n            return -1.0  # Guaranteed removal\n    score = 0.0\n    total_weight = 0.0\n\n    if self.metric_config[\"text_density\"]:\n        density = text_len / tag_len if tag_len > 0 else 0\n        score += self.metric_weights[\"text_density\"] * density\n        total_weight += self.metric_weights[\"text_density\"]\n\n    if self.metric_config[\"link_density\"]:\n        density = 1 - (link_text_len / text_len if text_len > 0 else 0)\n        score += self.metric_weights[\"link_density\"] * density\n        total_weight += self.metric_weights[\"link_density\"]\n\n    if self.metric_config[\"tag_weight\"]:\n        tag_score = self.tag_weights.get(metrics[\"tag_name\"], 0.5)\n        score += self.metric_weights[\"tag_weight\"] * tag_score\n        total_weight += self.metric_weights[\"tag_weight\"]\n\n    if self.metric_config[\"class_id_weight\"]:\n        class_score = self._compute_class_id_weight(metrics[\"node\"])\n        score += self.metric_weights[\"class_id_weight\"] * max(0, class_score)\n        total_weight += self.metric_weights[\"class_id_weight\"]\n\n    if self.metric_config[\"text_length\"]:\n        score += self.metric_weights[\"text_length\"] * math.log(text_len + 1)\n        total_weight += self.metric_weights[\"text_length\"]\n\n    return score / total_weight if total_weight > 0 else 0\n```\n\n----------------------------------------\n\nTITLE: Creating XPath Extraction Strategy for JSON Elements\nDESCRIPTION: This class implements a JsonElementExtractionStrategy using XPath selectors. It parses HTML content into an lxml tree, selects elements using XPath expressions, and includes methods for converting CSS selectors to XPath when needed.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_81\n\nLANGUAGE: python\nCODE:\n```\nclass JsonXPathExtractionStrategy(JsonElementExtractionStrategy):\n    \"\"\"\n    Concrete implementation of `JsonElementExtractionStrategy` using XPath selectors.\n\n    How it works:\n    1. Parses HTML content into an lxml tree.\n    2. Selects elements using XPath expressions.\n    3. Converts CSS selectors to XPath when needed.\n\n    Attributes:\n        schema (Dict[str, Any]): The schema defining the extraction rules.\n        verbose (bool): Enables verbose logging for debugging purposes.\n\n    Methods:\n        _parse_html(html_content): Parses HTML content into an lxml tree.\n        _get_base_elements(parsed_html, selector): Selects base elements using an XPath selector.\n        _css_to_xpath(css_selector): Converts a CSS selector to an XPath expression.\n        _get_elements(element, selector): Selects child elements using an XPath selector.\n        _get_element_text(element): Extracts text content from an lxml element.\n        _get_element_html(element): Extracts the raw HTML content of an lxml element.\n        _get_element_attribute(element, attribute): Retrieves an attribute value from an lxml element.\n    \"\"\"\n\n    def __init__(self, schema: Dict[str, Any], **kwargs):\n        kwargs[\"input_format\"] = \"html\"  # Force HTML input\n        super().__init__(schema, **kwargs)\n\n    def _parse_html(self, html_content: str):\n        return html.fromstring(html_content)\n\n    def _get_base_elements(self, parsed_html, selector: str):\n        return parsed_html.xpath(selector)\n\n    def _css_to_xpath(self, css_selector: str) -> str:\n        \"\"\"Convert CSS selector to XPath if needed\"\"\"\n        if \"/\" in css_selector:  # Already an XPath\n            return css_selector\n        return self._basic_css_to_xpath(css_selector)\n\n    def _basic_css_to_xpath(self, css_selector: str) -> str:\n        \"\"\"Basic CSS to XPath conversion for common cases\"\"\"\n        if \" > \" in css_selector:\n            parts = css_selector.split(\" > \")\n            return \"//\" + \"/\".join(parts)\n        if \" \" in css_selector:\n            parts = css_selector.split(\" \")\n            return \"//\" + \"//\".join(parts)\n        return \"//\" + css_selector\n\n    def _get_elements(self, element, selector: str):\n        xpath = self._css_to_xpath(selector)\n        if not xpath.startswith(\".\"):\n            xpath = \".\" + xpath\n        return element.xpath(xpath)\n\n    def _get_element_text(self, element) -> str:\n        return \"\".join(element.xpath(\".//text()\")).strip()\n\n    def _get_element_html(self, element) -> str:\n        return etree.tostring(element, encoding=\"unicode\")\n\n    def _get_element_attribute(self, element, attribute: str):\n        return element.get(attribute)\n```\n\n----------------------------------------\n\nTITLE: Executing Benchmark Tests with Bash Script\nDESCRIPTION: This snippet shows the creation of a bash script 'run_all.sh' that serves as a wrapper for the 'run_benchmark.py' Python script. It's used to easily execute predefined benchmark configurations for stress testing the SDK.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/JOURNAL.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nrun_all.sh\n```\n\n----------------------------------------\n\nTITLE: Processing Network Requests from CrawlResult\nDESCRIPTION: Shows how to analyze network requests, responses, and failures captured during a crawl. The code demonstrates filtering by event type, counting different types of events, analyzing API calls, and identifying failed resources.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nif result.network_requests:\n    # Count different types of events\n    requests = [r for r in result.network_requests if r.get(\"event_type\") == \"request\"]\n    responses = [r for r in result.network_requests if r.get(\"event_type\") == \"response\"]\n    failures = [r for r in result.network_requests if r.get(\"event_type\") == \"request_failed\"]\n    \n    print(f\"Captured {len(requests)} requests, {len(responses)} responses, and {len(failures)} failures\")\n    \n    # Analyze API calls\n    api_calls = [r for r in requests if \"api\" in r.get(\"url\", \"\")]\n    \n    # Identify failed resources\n    for failure in failures:\n        print(f\"Failed to load: {failure.get('url')} - {failure.get('failure_text')}\")\n```\n\n----------------------------------------\n\nTITLE: Running E-commerce Data Extraction with AsyncWebCrawler in Python\nDESCRIPTION: Implementation of asynchronous web crawling with the crawl4ai library to extract structured e-commerce data using the defined schema. The example shows how to configure the crawler, apply the extraction strategy, and handle the results.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\necommerce_schema = {\n    # ... the advanced schema from above ...\n}\n\nasync def extract_ecommerce_data():\n    strategy = JsonCssExtractionStrategy(ecommerce_schema, verbose=True)\n    \n    config = CrawlerRunConfig()\n    \n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=strategy,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n        \n        # Parse the JSON output\n        data = json.loads(result.extracted_content)\n        print(json.dumps(data, indent=2) if data else \"No data found.\")\n\nasyncio.run(extract_ecommerce_data())\n```\n\n----------------------------------------\n\nTITLE: Implementing Robots.txt Compliance in Crawl4AI\nDESCRIPTION: Demonstrates how to enable and use robots.txt compliance in Crawl4AI. It shows how to configure the crawler to check and respect robots.txt rules, with efficient caching.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Enable robots.txt checking in config\n    config = CrawlerRunConfig(\n        check_robots_txt=True  # Will check and respect robots.txt rules\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            \"https://example.com\",\n            config=config\n        )\n        \n        if not result.success and result.status_code == 403:\n            print(\"Access denied by robots.txt\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: BM25ContentFilter Class Implementation in Python\nDESCRIPTION: A class that filters HTML content using the BM25 algorithm with priority tag handling. It extracts and ranks text chunks based on relevance to a query, adjusts scores based on HTML tag importance, and returns the most relevant content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_94\n\nLANGUAGE: python\nCODE:\n```\nclass BM25ContentFilter(RelevantContentFilter):\n    \"\"\"\n    Content filtering using BM25 algorithm with priority tag handling.\n\n    How it works:\n    1. Extracts page metadata with fallbacks.\n    2. Extracts text chunks from the body element.\n    3. Tokenizes the corpus and query.\n    4. Applies BM25 algorithm to calculate scores for each chunk.\n    5. Filters out chunks below the threshold.\n    6. Sorts chunks by score in descending order.\n    7. Returns the top N chunks.\n\n    Attributes:\n        user_query (str): User query for filtering (optional).\n        bm25_threshold (float): BM25 threshold for filtering (default: 1.0).\n        language (str): Language for stemming (default: 'english').\n\n        Methods:\n            filter_content(self, html: str, min_word_threshold: int = None)\n    \"\"\"\n\n    def __init__(\n        self,\n        user_query: str = None,\n        bm25_threshold: float = 1.0,\n        language: str = \"english\",\n    ):\n        \"\"\"\n        Initializes the BM25ContentFilter class, if not provided, falls back to page metadata.\n\n        Note:\n        If no query is given and no page metadata is available, then it tries to pick up the first significant paragraph.\n\n        Args:\n            user_query (str): User query for filtering (optional).\n            bm25_threshold (float): BM25 threshold for filtering (default: 1.0).\n            language (str): Language for stemming (default: 'english').\n        \"\"\"\n        super().__init__(user_query=user_query)\n        self.bm25_threshold = bm25_threshold\n        self.priority_tags = {\n            \"h1\": 5.0,\n            \"h2\": 4.0,\n            \"h3\": 3.0,\n            \"title\": 4.0,\n            \"strong\": 2.0,\n            \"b\": 1.5,\n            \"em\": 1.5,\n            \"blockquote\": 2.0,\n            \"code\": 2.0,\n            \"pre\": 1.5,\n            \"th\": 1.5,  # Table headers\n        }\n        self.stemmer = stemmer(language)\n\n    def filter_content(self, html: str, min_word_threshold: int = None) -> List[str]:\n        \"\"\"\n        Implements content filtering using BM25 algorithm with priority tag handling.\n\n            Note:\n        This method implements the filtering logic for the BM25ContentFilter class.\n        It takes HTML content as input and returns a list of filtered text chunks.\n\n        Args:\n            html (str): HTML content to be filtered.\n            min_word_threshold (int): Minimum word threshold for filtering (optional).\n\n        Returns:\n            List[str]: List of filtered text chunks.\n        \"\"\"\n        if not html or not isinstance(html, str):\n            return []\n\n        soup = BeautifulSoup(html, \"lxml\")\n\n        # Check if body is present\n        if not soup.body:\n            # Wrap in body tag if missing\n            soup = BeautifulSoup(f\"<body>{html}</body>\", \"lxml\")\n        body = soup.find(\"body\")\n\n        query = self.extract_page_query(soup, body)\n\n        if not query:\n            return []\n            # return [self.clean_element(soup)]\n\n        candidates = self.extract_text_chunks(body, min_word_threshold)\n\n        if not candidates:\n            return []\n\n        # Tokenize corpus\n        # tokenized_corpus = [chunk.lower().split() for _, chunk, _, _ in candidates]\n        # tokenized_query = query.lower().split()\n\n        # tokenized_corpus = [[ps.stem(word) for word in chunk.lower().split()]\n        #                 for _, chunk, _, _ in candidates]\n        # tokenized_query = [ps.stem(word) for word in query.lower().split()]\n\n        tokenized_corpus = [\n            [self.stemmer.stemWord(word) for word in chunk.lower().split()]\n            for _, chunk, _, _ in candidates\n        ]\n        tokenized_query = [\n            self.stemmer.stemWord(word) for word in query.lower().split()\n        ]\n\n        # tokenized_corpus = [[self.stemmer.stemWord(word) for word in tokenize_text(chunk.lower())]\n        #            for _, chunk, _, _ in candidates]\n        # tokenized_query = [self.stemmer.stemWord(word) for word in tokenize_text(query.lower())]\n\n        # Clean from stop words and noise\n        tokenized_corpus = [clean_tokens(tokens) for tokens in tokenized_corpus]\n        tokenized_query = clean_tokens(tokenized_query)\n\n        bm25 = BM25Okapi(tokenized_corpus)\n        scores = bm25.get_scores(tokenized_query)\n\n        # Adjust scores with tag weights\n        adjusted_candidates = []\n        for score, (index, chunk, tag_type, tag) in zip(scores, candidates):\n            tag_weight = self.priority_tags.get(tag.name, 1.0)\n            adjusted_score = score * tag_weight\n            adjusted_candidates.append((adjusted_score, index, chunk, tag))\n\n        # Filter candidates by threshold\n        selected_candidates = [\n            (index, chunk, tag)\n            for adjusted_score, index, chunk, tag in adjusted_candidates\n            if adjusted_score >= self.bm25_threshold\n        ]\n\n        if not selected_candidates:\n            return []\n\n        # Sort selected candidates by original document order\n        selected_candidates.sort(key=lambda x: x[0])\n\n        return [self.clean_element(tag) for _, _, tag in selected_candidates]\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Crawler Settings in JSON\nDESCRIPTION: Provides an example of an advanced crawler configuration in JSON format, including cache mode, markdown generator, and content filter settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_58\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"urls\": [\"https://example.com\"],\n    \"crawler_config\": {\n        \"type\": \"CrawlerRunConfig\",\n        \"params\": {\n            \"cache_mode\": \"bypass\",\n            \"markdown_generator\": {\n                \"type\": \"DefaultMarkdownGenerator\",\n                \"params\": {\n                    \"content_filter\": {\n                        \"type\": \"PruningContentFilter\",\n                        \"params\": {\n                            \"threshold\": 0.48,\n                            \"threshold_type\": \"fixed\",\n                            \"min_word_threshold\": 0\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Error Handling for Browser Process Monitoring in Python\nDESCRIPTION: Code that handles exceptions when monitoring browser processes. It logs errors with appropriate context unless the browser is in the process of shutting down.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_121\n\nLANGUAGE: python\nCODE:\n```\n            except Exception as e:\n                if not self.shutting_down:\n                    self.logger.error(\n                        message=\"Error monitoring browser process: {error}\",\n                        tag=\"ERROR\",\n                        params={\"error\": str(e)},\n                    )\n```\n\n----------------------------------------\n\nTITLE: Overriding model_dump Method for CrawlResult Serialization\nDESCRIPTION: Overrides the model_dump method to include the private _markdown attribute in serialization. This maintains backward compatibility by ensuring that the markdown content is properly serialized despite being stored in a private attribute.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_84\n\nLANGUAGE: python\nCODE:\n```\ndef model_dump(self, *args, **kwargs):\n    \"\"\"\n    Override model_dump to include the _markdown private attribute in serialization.\n    \n    This override is necessary because:\n    1. PrivateAttr fields are excluded from serialization by default\n    2. We need to maintain backward compatibility by including the 'markdown' field\n       in the serialized output\n    3. We're transitioning from 'markdown_v2' to enhancing 'markdown' to hold\n       the same type of data\n    \n    Future developers: This method ensures that the markdown content is properly\n    serialized despite being stored in a private attribute. If the serialization\n    requirements change, this is where you would update the logic.\n    \"\"\"\n    result = super().model_dump(*args, **kwargs)\n    if self._markdown is not None:\n        result[\"markdown\"] = self._markdown.model_dump() \n    return result\n```\n\n----------------------------------------\n\nTITLE: Accessing Dispatch Results in Python with Crawl4AI\nDESCRIPTION: Demonstrates how to access and analyze dispatch result information from crawling operations. The code iterates through crawling results and extracts memory usage and duration data from the dispatch_result attribute of each successful result.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_151\n\nLANGUAGE: python\nCODE:\n```\nfor result in results:\n    if result.success:\n        dr = result.dispatch_result\n        print(f\"URL: {result.url}\")\n        print(f\"Memory: {dr.memory_usage:.1f}MB\")\n        print(f\"Duration: {dr.end_time - dr.start_time}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using CosineStrategy for Web Content Extraction in Python\nDESCRIPTION: Demonstrates how to instantiate and configure the CosineStrategy class for web content extraction based on semantic similarity. The example shows basic usage with an AsyncWebCrawler to extract content from a URL.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_169\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```\n\n----------------------------------------\n\nTITLE: Initial Login and Storage State Export\nDESCRIPTION: First-run implementation that performs login and exports the storage state for future use\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CacheMode\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def on_browser_created_hook(browser):\n    context = browser.contexts[0]\n    page = await context.new_page()\n    \n    await page.goto(\"https://example.com/login\", wait_until=\"domcontentloaded\")\n    \n    await page.fill(\"input[name='username']\", \"myuser\")\n    await page.fill(\"input[name='password']\", \"mypassword\")\n    await page.click(\"button[type='submit']\")\n    await page.wait_for_load_state(\"networkidle\")\n    \n    await context.storage_state(path=\"my_storage_state.json\")\n    await page.close()\n\nasync def main():\n    async with AsyncWebCrawler(\n        headless=True,\n        verbose=True,\n        hooks={\"on_browser_created\": on_browser_created_hook},\n        use_persistent_context=True,\n        user_data_dir=\"./my_user_data\"\n    ) as crawler:\n        \n        result = await crawler.arun(\n            url='https://example.com/protected-page',\n            cache_mode=CacheMode.BYPASS,\n            markdown_generator=DefaultMarkdownGenerator(options={\"ignore_links\": True}),\n        )\n        print(\"First run result success:\", result.success)\n        if result.success:\n            print(\"Protected page HTML length:\", len(result.html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Accessing Various Result Fields in Crawl4AI\nDESCRIPTION: This code snippet demonstrates how to access different fields from a crawl result, including checking success status, status code, links count, markdown content, and extracted content. It also shows error handling when crawling fails.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nif result.success:\n    print(result.status_code, result.response_headers)\n    print(\"Links found:\", len(result.links.get(\"internal\", [])))\n    if result.markdown:\n        print(\"Markdown snippet:\", result.markdown.raw_markdown[:200])\n    if result.extracted_content:\n        print(\"Structured JSON:\", result.extracted_content)\nelse:\n    print(\"Error:\", result.error_message)\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Data Using LLM with Crawl4AI\nDESCRIPTION: This function demonstrates how to extract structured data from webpages using LLM-based extraction. It configures an extraction strategy with a defined schema, custom instructions, and provider-specific parameters to extract OpenAI model fee information from their pricing page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_137\n\nLANGUAGE: python\nCODE:\n```\nasync def extract_structured_data_using_llm(\n    provider: str, api_token: str = None, extra_headers: Dict[str, str] = None\n):\n    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n\n    if api_token is None and provider != \"ollama\":\n        print(f\"API token is required for {provider}. Skipping this example.\")\n        return\n\n    browser_config = BrowserConfig(headless=True)\n\n    extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000}\n    if extra_headers:\n        extra_args[\"extra_headers\"] = extra_headers\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        word_count_threshold=1,\n        page_timeout=80000,\n        extraction_strategy=LLMExtractionStrategy(\n            llm_config=LLMConfig(provider=provider,api_token=api_token),\n            schema=OpenAIModelFee.model_json_schema(),\n            extraction_type=\"schema\",\n            instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n            Do not miss any models in the entire content.\"\"\",\n            extra_args=extra_args,\n        ),\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://openai.com/api/pricing/\", config=crawler_config\n        )\n        print(result.extracted_content)\n```\n\n----------------------------------------\n\nTITLE: Removing HTML Comments and Unwanted Tags in BeautifulSoup\nDESCRIPTION: Methods to remove HTML comments and unwanted tags from a BeautifulSoup object. These helper functions are used during the HTML pruning process to clean the document before content extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_96\n\nLANGUAGE: python\nCODE:\n```\ndef _remove_comments(self, soup):\n    \"\"\"Removes HTML comments\"\"\"\n    for element in soup(text=lambda text: isinstance(text, Comment)):\n        element.extract()\n\ndef _remove_unwanted_tags(self, soup):\n    \"\"\"Removes unwanted tags\"\"\"\n    for tag in self.excluded_tags:\n        for element in soup.find_all(tag):\n            element.decompose()\n```\n\n----------------------------------------\n\nTITLE: Running Crawler4ai from Command Line\nDESCRIPTION: Command line example showing how to use the Crawler4ai CLI tool to start a crawl operation with a configuration file.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncrawler4ai --config config.json\n```\n\n----------------------------------------\n\nTITLE: Text Processing Configuration\nDESCRIPTION: Configuration for text processing settings including word count thresholds and content filtering options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    word_count_threshold=10,   # Ignore text blocks <10 words\n    only_text=False,           # If True, tries to remove non-text elements\n    keep_data_attributes=False # Keep or discard data-* attributes\n)\n```\n\n----------------------------------------\n\nTITLE: Verifying Crawl4AI Installation in Python\nDESCRIPTION: Python script to verify the correct installation and functionality of Crawl4AI by crawling a sample website and printing the extracted content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.example.com\")\n        print(result.markdown[:500])  # Print first 500 characters\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Basic Session Usage with AsyncWebCrawler in Python\nDESCRIPTION: Demonstrates the fundamental pattern for using session management in Crawl4AI. This example shows how to create a session, use it across multiple requests, and properly clean up when finished. It uses session_id to maintain state between different page requests.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n\n    # Define configurations\n    config1 = CrawlerRunConfig(\n        url=\"https://example.com/page1\", session_id=session_id\n    )\n    config2 = CrawlerRunConfig(\n        url=\"https://example.com/page2\", session_id=session_id\n    )\n\n    # First request\n    result1 = await crawler.arun(config=config1)\n\n    # Subsequent request using the same session\n    result2 = await crawler.arun(config=config2)\n\n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n----------------------------------------\n\nTITLE: Content Cleaning and Fit Markdown with Crawl4AI\nDESCRIPTION: This snippet demonstrates content cleaning features of Crawl4AI, including excluding specific tags, removing overlay elements, and applying word count thresholds. It also shows the difference between raw and fit markdown.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync def clean_content():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://janineintheworld.com/places-to-visit-in-central-mexico\",\n            excluded_tags=['nav', 'footer', 'aside'],\n            remove_overlay_elements=True,\n            word_count_threshold=10,\n            bypass_cache=True\n        )\n        full_markdown_length = len(result.markdown.raw_markdown)\n        fit_markdown_length = len(result.markdown.fit_markdown)\n        print(f\"Full Markdown Length: {full_markdown_length}\")\n        print(f\"Fit Markdown Length: {fit_markdown_length}\")\n        print(result.markdown.fit_markdown[:1000])\n        \n\nasyncio.run(clean_content())\n```\n\n----------------------------------------\n\nTITLE: URL Validation and Filtering for Web Crawling in Python\nDESCRIPTION: Implements URL validation and filtering functionality for the crawler. This method checks if a URL has a valid format and applies the filter chain, with an exception for the starting URL which bypasses filtering.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_177\n\nLANGUAGE: python\nCODE:\n```\nasync def can_process_url(self, url: str, depth: int) -> bool:\n    \"\"\"\n    Validates the URL and applies the filter chain.\n    For the start URL (depth 0) filtering is bypassed.\n    \"\"\"\n    try:\n        parsed = urlparse(url)\n        if not parsed.scheme or not parsed.netloc:\n            raise ValueError(\"Missing scheme or netloc\")\n        if parsed.scheme not in (\"http\", \"https\"):\n            raise ValueError(\"Invalid scheme\")\n        if \".\" not in parsed.netloc:\n            raise ValueError(\"Invalid domain\")\n    except Exception as e:\n        self.logger.warning(f\"Invalid URL: {url}, error: {e}\")\n        return False\n\n    if depth != 0 and not await self.filter_chain.apply(url):\n        return False\n\n    return True\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI with Transformers Support in Python\nDESCRIPTION: Command for installing Crawl4AI with Transformers support, enabling text summarization and Hugging Face models.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai[transformer]\n```\n\n----------------------------------------\n\nTITLE: Implementing Question-Based Web Crawler in Python\nDESCRIPTION: Demonstrates the usage of Question-Based Discovery system for intelligent web content extraction based on natural language questions. Uses AsyncWebCrawler with SerpiAPI integration and relevancy scoring.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.discovery import QuestionBasedDiscovery\n\nasync with AsyncWebCrawler() as crawler:\n    discovery = QuestionBasedDiscovery(crawler)\n    results = await discovery.arun(\n        question=\"What are the system requirements for major cloud providers' GPU instances?\",\n        max_urls=5,\n        relevance_threshold=0.7\n    )\n    \n    for result in results:\n        print(f\"Source: {result.url} (Relevance: {result.relevance_score})\") \n        print(f\"Content: {result.markdown}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Form Interaction with Crawl4AI\nDESCRIPTION: Shows how to interact with web forms by filling fields and submitting them using JavaScript execution in Crawl4AI. This example demonstrates a hypothetical search on GitHub.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\njs_form_interaction = \"\"\"\ndocument.querySelector('#your-search').value = 'TypeScript commits';\ndocument.querySelector('form').submit();\n\"\"\"\n\nconfig = CrawlerRunConfig(\n    js_code=js_form_interaction,\n    wait_for=\"css:.commit\"\n)\nresult = await crawler.arun(url=\"https://github.com/search\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Configuring API Keys for AI Services in Python\nDESCRIPTION: Sets up API key constants for accessing different AI services including Groq, OpenAI, and Anthropic. The keys are stored as string variables that would be replaced with actual API keys when deploying the application. The comment indicates that additional API keys can be added to this configuration file.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/.env.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nGROQ_API_KEY = \"YOUR_GROQ_API\"\nOPENAI_API_KEY = \"YOUR_OPENAI_API\"\nANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_API\"\n# You can add more API keys here\n```\n\n----------------------------------------\n\nTITLE: Configuring DFSDeepCrawlStrategy for Depth-First Crawling in Python\nDESCRIPTION: Demonstrates setting up `DFSDeepCrawlStrategy` for depth-first web exploration. Key parameters shown include `max_depth` (maximum crawl depth from start), `include_external` (whether to follow external links), `max_pages` (optional cap on pages crawled), and `score_threshold` (optional minimum score for URL inclusion). This strategy explores as far down one path as possible before backtracking. Depends on `DFSDeepCrawlStrategy` from `crawl4ai.deep_crawling`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling import DFSDeepCrawlStrategy\n\n# Basic configuration\nstrategy = DFSDeepCrawlStrategy(\n    max_depth=2,               # Crawl initial page + 2 levels deep\n    include_external=False,    # Stay within the same domain\n    max_pages=30,              # Maximum number of pages to crawl (optional)\n    score_threshold=0.5,       # Minimum score for URLs to be crawled (optional)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser and Crawler Settings in Python using Crawl4AI\nDESCRIPTION: Demonstrates how to use the new BrowserConfig and CrawlerRunConfig objects to configure browser and crawler behavior. Shows initialization of an AsyncWebCrawler with custom viewport settings and cache mode.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.2.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import BrowserConfig, CrawlerRunConfig, AsyncWebCrawler\n\nbrowser_config = BrowserConfig(headless=True, viewport_width=1920, viewport_height=1080)\ncrawler_config = CrawlerRunConfig(cache_mode=\"BYPASS\")\n\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\", config=crawler_config)\n    print(result.markdown[:500])\n```\n\n----------------------------------------\n\nTITLE: LLM Strategy Initialization Method\nDESCRIPTION: Constructor method for LLMExtractionStrategy that initializes configuration parameters, handles deprecated arguments, and sets up token usage tracking.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_68\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n        self,\n        llm_config: 'LLMConfig' = None,\n        instruction: str = None,\n        schema: Dict = None,\n        extraction_type=\"block\",\n        chunk_token_threshold=CHUNK_TOKEN_THRESHOLD,\n        overlap_rate=OVERLAP_RATE,\n        word_token_rate=WORD_TOKEN_RATE,\n        apply_chunking=True,\n        input_format: str = \"markdown\",\n        force_json_response=False,\n        verbose=False,\n        # Deprecated arguments\n        provider: str = DEFAULT_PROVIDER,\n        api_token: Optional[str] = None,\n        base_url: str = None,\n        api_base: str = None,\n        **kwargs,\n    ):\n```\n\n----------------------------------------\n\nTITLE: Generating Crawl4AI Configuration JSON from Python Objects\nDESCRIPTION: Demonstrates how to create Crawl4AI configuration objects in Python and convert them to JSON for API calls using the dump() method.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import CrawlerRunConfig, PruningContentFilter\n\nconfig = CrawlerRunConfig(\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=PruningContentFilter(threshold=0.48, threshold_type=\"fixed\")\n    ),\n    cache_mode= CacheMode.BYPASS\n)\nprint(config.dump())  # Use this JSON in your API calls\n```\n\n----------------------------------------\n\nTITLE: Implementing Browser Start Command in Python\nDESCRIPTION: Implementation of the 'crwl browser start' command that starts a persistent browser instance for Crawl4AI. It accepts options for browser type, debugging port, and headless mode, then launches the browser and displays the CDP URL for connection.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n@browser_cmd.command(\"start\")\n@click.option(\"--browser-type\", \"-b\", type=click.Choice([\"chromium\", \"firefox\"]), default=\"chromium\", \n              help=\"Browser type (default: chromium)\")\n@click.option(\"--port\", \"-p\", type=int, default=9222, help=\"Debugging port (default: 9222)\")\n@click.option(\"--headless/--no-headless\", default=True, help=\"Run browser in headless mode\")\ndef browser_start_cmd(browser_type: str, port: int, headless: bool):\n    \"\"\"Start a builtin browser instance\n    \n    This will start a persistent browser instance that can be used by Crawl4AI\n    by setting browser_mode=\"builtin\" in BrowserConfig.\n    \"\"\"\n    profiler = BrowserProfiler()\n    \n    # First check if browser is already running\n    status = anyio.run(profiler.get_builtin_browser_status)\n    if status[\"running\"]:\n        console.print(Panel(\n            \"[yellow]Builtin browser is already running[/yellow]\\n\\n\"\n            f\"CDP URL: [cyan]{status['cdp_url']}[/cyan]\\n\\n\"\n            \"Use 'crwl browser restart' to restart the browser\",\n            title=\"Builtin Browser Start\",\n            border_style=\"yellow\"\n        ))\n        return\n    \n    try:\n        console.print(Panel(\n            f\"[cyan]Starting builtin browser[/cyan]\\n\\n\"\n            f\"Browser type: [green]{browser_type}[/green]\\n\"\n            f\"Debugging port: [yellow]{port}[/yellow]\\n\"\n            f\"Headless: [cyan]{'Yes' if headless else 'No'}[/cyan]\",\n            title=\"Builtin Browser Start\",\n            border_style=\"cyan\"\n        ))\n        \n        cdp_url = anyio.run(\n            profiler.launch_builtin_browser,\n            browser_type,\n            port,\n            headless\n        )\n        \n        if cdp_url:\n            console.print(Panel(\n                f\"[green]Builtin browser started successfully[/green]\\n\\n\"\n                f\"CDP URL: [cyan]{cdp_url}[/cyan]\\n\\n\"\n                \"This browser will be used automatically when setting browser_mode='builtin'\",\n                title=\"Builtin Browser Start\",\n                border_style=\"green\"\n            ))\n        else:\n            console.print(Panel(\n                \"[red]Failed to start builtin browser[/red]\",\n                title=\"Builtin Browser Start\",\n                border_style=\"red\"\n            ))\n            sys.exit(1)\n            \n    except Exception as e:\n        console.print(f\"[red]Error starting builtin browser: {str(e)}[/red]\")\n        sys.exit(1)\n```\n\n----------------------------------------\n\nTITLE: Running Hierarchical Clustering on Text Sections in Python\nDESCRIPTION: Processes a list of text sections using hierarchical clustering. It joins all sections with a delimiter and passes them to the extract method for further processing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_66\n\nLANGUAGE: python\nCODE:\n```\ndef run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]:\n    \"\"\"\n    Process sections using hierarchical clustering.\n\n    Args:\n        url (str): The URL of the webpage.\n        sections (List[str]): List of sections (strings) to process.\n\n    Returns:\n    \"\"\"\n    # This strategy processes all sections together\n\n    return self.extract(url, self.DEL.join(sections), **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Configuring Stealth Settings for Browser Automation in Python\nDESCRIPTION: Defines stealth configuration settings to make automated browser behavior appear more human-like. Includes various Chrome-specific settings and navigator properties.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_110\n\nLANGUAGE: python\nCODE:\n```\nstealth_config = StealthConfig(\n    webdriver=True,\n    chrome_app=True,\n    chrome_csi=True,\n    chrome_load_times=True,\n    chrome_runtime=True,\n    navigator_languages=True,\n    navigator_plugins=True,\n    navigator_permissions=True,\n    webgl_vendor=True,\n    outerdimensions=True,\n    navigator_hardware_concurrency=True,\n    media_codecs=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Console Message JSON Structure in Crawl4AI\nDESCRIPTION: Example JSON structure for a console message captured by Crawl4AI. This shows the format of browser console output collected during crawling, including the message type, text content, and source location.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_157\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"error\",\n  \"text\": \"Uncaught TypeError: Cannot read property 'length' of undefined\",\n  \"location\": \"https://example.com/script.js:123:45\",\n  \"timestamp\": 1633456790.123\n}\n```\n\n----------------------------------------\n\nTITLE: Capturing Web Pages as MHTML with Crawl4AI\nDESCRIPTION: Demonstrates how to capture a web page as an MHTML snapshot using Crawl4AI. The code configures the crawler to save the entire page with all its resources in a single MHTML file, which is useful for archiving or offline viewing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_98\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    crawler_cfg = CrawlerRunConfig(\n        capture_mhtml=True  # Enable MHTML capture\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=crawler_cfg)\n        \n        if result.success and result.mhtml:\n            # Save the MHTML snapshot to a file\n            with open(\"example.mhtml\", \"w\", encoding=\"utf-8\") as f:\n                f.write(result.mhtml)\n            print(\"MHTML snapshot saved to example.mhtml\")\n        else:\n            print(\"Failed to capture MHTML:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Initializing CosineStrategy Class for Text Extraction in Python\nDESCRIPTION: Defines the CosineStrategy class, which inherits from ExtractionStrategy. It sets up parameters for semantic filtering, clustering, and embedding generation using a specified model.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nclass CosineStrategy(ExtractionStrategy):\n    def __init__(\n        self,\n        semantic_filter=None,\n        word_count_threshold=10,\n        max_dist=0.2,\n        linkage_method=\"ward\",\n        top_k=3,\n        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n        sim_threshold=0.3,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        import numpy as np\n\n        self.semantic_filter = semantic_filter\n        self.word_count_threshold = word_count_threshold\n        self.max_dist = max_dist\n        self.linkage_method = linkage_method\n        self.top_k = top_k\n        self.sim_threshold = sim_threshold\n        self.timer = time.time()\n        self.verbose = kwargs.get(\"verbose\", False)\n\n        self.buffer_embeddings = np.array([])\n        self.get_embedding_method = \"direct\"\n\n        self.device = get_device()\n        self.default_batch_size = calculate_batch_size(self.device)\n\n        if self.verbose:\n            print(f\"[LOG] Loading Extraction Model for {self.device.type} device.\")\n\n        self.tokenizer, self.model = load_HF_embedding_model(model_name)\n        self.model.to(self.device)\n        self.model.eval()\n\n        self.get_embedding_method = \"batch\"\n\n        self.buffer_embeddings = np.array([])\n\n        if self.verbose:\n            print(f\"[LOG] Loading Multilabel Classifier for {self.device.type} device.\")\n\n        self.nlp, _ = load_text_multilabel_classifier()\n\n        if self.verbose:\n            print(\n                f\"[LOG] Model loaded {model_name}, models/reuters, took \"\n                + str(time.time() - self.timer)\n                + \" seconds\"\n            )\n```\n\n----------------------------------------\n\nTITLE: Scoring Keyword Presence in Text for SEO\nDESCRIPTION: A method that calculates a score based on the presence of keywords in the text. It uses regex patterns to find matches and returns a score up to 1.0, with each match worth 0.3 points (max of 3 matches).\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_193\n\nLANGUAGE: python\nCODE:\n```\ndef _score_keyword_presence(self, text: str) -> float:\n    if not self._kw_patterns:\n        return 0.0\n    matches = len(self._kw_patterns.findall(text))\n    return min(matches * 0.3, 1.0)  # Max 3 matches\n```\n\n----------------------------------------\n\nTITLE: CSS-Based Wait Conditions in Crawl4AI\nDESCRIPTION: Demonstrates how to use wait conditions based on CSS selectors to ensure specific elements are loaded before proceeding with crawling. Includes example of waiting for multiple items to appear on a page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_115\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Wait for at least 30 items on Hacker News\n        wait_for=\"css:.athing:nth-child(30)\"  \n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n        print(\"We have at least 30 items loaded!\")\n        # Rough check\n        print(\"Total items in HTML:\", result.cleaned_html.count(\"athing\"))  \n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Basic Web Crawling with AsyncWebCrawler in Python\nDESCRIPTION: A minimal example demonstrating how to use Crawl4AI's AsyncWebCrawler to fetch a webpage and convert it to Markdown. This basic script fetches example.com and prints the first 300 characters of the resulting Markdown content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_121\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\")\n        print(result.markdown[:300])  # Print first 300 chars\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Session Management Configuration\nDESCRIPTION: Configuration for managing crawler sessions using session IDs.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    session_id=\"my_session123\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Deep Crawl with BFS Strategy in Python\nDESCRIPTION: This snippet demonstrates how to set up a basic deep crawl using the BFSDeepCrawlStrategy with a depth of 2 levels. It configures the crawler to only follow links within the same domain and uses the LXMLWebScrapingStrategy for content extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_205\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=2, include_external=False),\n    scraping_strategy=LXMLWebScrapingStrategy(),\n    verbose=True,  # Show progress during crawling\n)\n```\n\n----------------------------------------\n\nTITLE: Simulating Full-Page Scrolling for Dynamic Content\nDESCRIPTION: This snippet demonstrates how to enable full-page scanning that simulates scrolling to capture dynamically loaded content. It scrolls incrementally with configurable delay, making it effective for infinite scroll pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait crawler.crawl(\n    url=\"https://example.com\",\n    scan_full_page=True,   # Enables scrolling\n    scroll_delay=0.2       # Waits 200ms between scrolls (optional)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Production Settings for Crawl4AI in YAML\nDESCRIPTION: This YAML snippet shows recommended production settings for Crawl4AI, including app configuration, rate limiting, and security settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_67\n\nLANGUAGE: yaml\nCODE:\n```\napp:\n  reload: False              # Disable reload in production\n  timeout_keep_alive: 120    # Lower timeout for better resource management\n\nrate_limiting:\n  storage_uri: \"redis://redis:6379\"  # Use Redis for distributed rate limiting\n  default_limit: \"50/minute\"         # More conservative rate limit\n\nsecurity:\n  enabled: true                      # Enable all security features\n  trusted_hosts: [\"your-domain.com\"] # Restrict to your domain\n```\n\n----------------------------------------\n\nTITLE: Network Request Event Structure in Crawl4AI (JSON)\nDESCRIPTION: This JSON structure represents a typical network request event captured by Crawl4AI, including URL, method, headers, and other relevant information.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/network-console-capture.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"event_type\": \"request\",\n  \"url\": \"https://example.com/api/data.json\",\n  \"method\": \"GET\",\n  \"headers\": {\"User-Agent\": \"...\", \"Accept\": \"...\"},\n  \"post_data\": \"key=value&otherkey=value\",\n  \"resource_type\": \"fetch\",\n  \"is_navigation_request\": false,\n  \"timestamp\": 1633456789.123\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing Key-Value Parameters in Python for CLI\nDESCRIPTION: Parses a string of comma-separated key-value pairs into a dictionary. Handles various data types including booleans, numbers, lists, and JSON objects.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef parse_key_values(ctx, param, value) -> Dict[str, Any]:\n    if not value:\n        return {}\n    result = {}\n    pairs = value.split(',')\n    for pair in pairs:\n        try:\n            k, v = pair.split('=', 1)\n            # Handle common value types \n            if v.lower() == 'true': v = True\n            elif v.lower() == 'false': v = False\n            elif v.isdigit(): v = int(v)\n            elif v.replace('.','',1).isdigit(): v = float(v)\n            elif v.startswith('[') and v.endswith(']'):\n                v = [x.strip() for x in v[1:-1].split(',') if x.strip()]\n            elif v.startswith('{') and v.endswith('}'):\n                try:\n                    v = json.loads(v)\n                except json.JSONDecodeError:\n                    raise click.BadParameter(f'Invalid JSON object: {v}')\n            result[k.strip()] = v\n        except ValueError:\n            raise click.BadParameter(f'Invalid key=value pair: {pair}')\n    return result\n```\n\n----------------------------------------\n\nTITLE: Implementing Robots.txt Compliance\nDESCRIPTION: Enables robots.txt checking with SQLite caching support. Verifies crawling permissions and handles blocked access appropriately.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(check_robots_txt=True)\nresult = await crawler.arun(url, config=config)\nif result.status_code == 403:\n    print(\"Access blocked by robots.txt\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Authenticated Proxy in Crawl4AI\nDESCRIPTION: Shows how to configure an authenticated proxy using a dictionary configuration with server, username, and password credentials.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/proxy-security.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import BrowserConfig\n\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nbrowser_config = BrowserConfig(proxy_config=proxy_config)\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```\n\n----------------------------------------\n\nTITLE: Pre-Fetching Models for Crawl4AI\nDESCRIPTION: Command to download and cache large models locally for use with advanced Crawl4AI features.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_85\n\nLANGUAGE: bash\nCODE:\n```\ncrawl4ai-download-models\n```\n\n----------------------------------------\n\nTITLE: Accessing Different Markdown Formats from MarkdownGenerationResult\nDESCRIPTION: This snippet shows how to access the various markdown formats available in the MarkdownGenerationResult object, including raw markdown, markdown with citations, references, and filtered markdown.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_112\n\nLANGUAGE: python\nCODE:\n```\nmd_obj = result.markdown  # your library's naming may vary\nprint(\"RAW:\\n\", md_obj.raw_markdown)\nprint(\"CITED:\\n\", md_obj.markdown_with_citations)\nprint(\"REFERENCES:\\n\", md_obj.references_markdown)\nprint(\"FIT:\\n\", md_obj.fit_markdown)\n```\n\n----------------------------------------\n\nTITLE: Implementing Sliding Window Chunking in Python\nDESCRIPTION: Generates overlapping text chunks for better contextual coherence. The class uses configurable window size and step parameters to create chunks that overlap, preserving context between adjacent segments.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_167\n\nLANGUAGE: python\nCODE:\n```\nclass SlidingWindowChunking:\n    def __init__(self, window_size=100, step=50):\n        self.window_size = window_size\n        self.step = step\n\n    def chunk(self, text):\n        words = text.split()\n        chunks = []\n        for i in range(0, len(words) - self.window_size + 1, self.step):\n            chunks.append(' '.join(words[i:i + self.window_size]))\n        return chunks\n\n# Example Usage\ntext = \"This is a long text to demonstrate sliding window chunking.\"\nchunker = SlidingWindowChunking(window_size=5, step=2)\nprint(chunker.chunk(text))\n```\n\n----------------------------------------\n\nTITLE: Basic Link Extraction with AsyncWebCrawler (Python)\nDESCRIPTION: This snippet demonstrates the basic usage of `AsyncWebCrawler` to crawl a single URL. After a successful crawl (`result.success`), it accesses the extracted links stored in `result.links`, separating them into internal and external lists. It also prints the counts and a sample internal link dictionary.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://www.example.com\")\n    if result.success:\n        internal_links = result.links.get(\"internal\", [])\n        external_links = result.links.get(\"external\", [])\n        print(f\"Found {len(internal_links)} internal links.\")\n        print(f\"Found {len(internal_links)} external links.\")\n        print(f\"Found {len(result.media)} media items.\")\n\n        # Each link is typically a dictionary with fields like:\n        # { \"href\": \"...\", \"text\": \"...\", \"title\": \"...\", \"base_domain\": \"...\" }\n        if internal_links:\n            print(\"Sample Internal Link:\", internal_links[0])\n    else:\n        print(\"Crawl failed:\", result.error_message)\n```\n\n----------------------------------------\n\nTITLE: Generating Heuristic Markdown with Crawl4AI in Python\nDESCRIPTION: This Python script demonstrates how to use the `AsyncWebCrawler` from the `crawl4ai` library to crawl a website and generate cleaned Markdown content. It configures the browser to run headlessly, enables caching, and uses a `DefaultMarkdownGenerator` with a `PruningContentFilter` to refine the output based on a fixed threshold. The script then prints the length of the raw and fitted Markdown.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    browser_config = BrowserConfig(\n        headless=True,  \n        verbose=True,\n    )\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.ENABLED,\n        markdown_generator=DefaultMarkdownGenerator(\n            content_filter=PruningContentFilter(threshold=0.48, threshold_type=\"fixed\", min_word_threshold=0)\n        ),\n        # markdown_generator=DefaultMarkdownGenerator(\n        #     content_filter=BM25ContentFilter(user_query=\"WHEN_WE_FOCUS_BASED_ON_A_USER_QUERY\", bm25_threshold=1.0)\n        # ),\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://docs.micronaut.io/4.7.6/guide/\",\n            config=run_config\n        )\n        print(len(result.markdown.raw_markdown))\n        print(len(result.markdown.fit_markdown))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Manually Installing Browsers for Playwright in Bash\nDESCRIPTION: This Bash snippet details a troubleshooting step for manually installing Chromium browser dependencies used by Playwright if browser-related errors prevent normal operation. The prerequisite is an environment where Python and Playwright are installed. The command modifies system software by downloading browser binaries needed to enable Crawl4AIâ€™s browser automation features. Output is displayed via the terminal.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m playwright install --with-deps chromium\n```\n\n----------------------------------------\n\nTITLE: Exporting SSL Certificate to DER Format in Python\nDESCRIPTION: Shows how to export an SSL certificate to DER format (binary ASN.1). The method can return the binary data or write it directly to a specified file path.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nder_bytes = cert.to_der()\ncert.to_der(\"certificate.der\")\n```\n\n----------------------------------------\n\nTITLE: Domain-Specific Scrapers Implementation\nDESCRIPTION: Shows implementation of specialized extraction strategies for common website types including academic and e-commerce platforms. Features pre-configured extractors and site-specific optimizations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extractors import AcademicExtractor, EcommerceExtractor\n\nasync with AsyncWebCrawler() as crawler:\n    # Academic paper extraction\n    papers = await crawler.arun(\n        url=\"https://arxiv.org/list/cs.AI/recent\",\n        extractor=\"academic\",  # Built-in extractor type\n        site_type=\"arxiv\",     # Specific site optimization\n        extract_fields=[\n            \"title\", \n            \"authors\", \n            \"abstract\", \n            \"citations\"\n        ]\n    )\n    \n    # E-commerce product data\n    products = await crawler.arun(\n        url=\"https://store.example.com/products\",\n        extractor=\"ecommerce\",\n        extract_fields=[\n            \"name\",\n            \"price\",\n            \"availability\",\n            \"reviews\"\n        ]\n    )\n```\n\n----------------------------------------\n\nTITLE: Streaming Mode Implementation\nDESCRIPTION: Implements streaming mode crawling by yielding results as they become available.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_173\n\nLANGUAGE: python\nCODE:\n```\nasync def _arun_stream(\n        self,\n        start_url: str,\n        crawler: AsyncWebCrawler,\n        config: CrawlerRunConfig,\n    ) -> AsyncGenerator[CrawlResult, None]:\n        async for result in self._arun_best_first(start_url, crawler, config):\n            yield result\n```\n\n----------------------------------------\n\nTITLE: Initializing RateLimiter in Python for Web Crawling\nDESCRIPTION: This snippet demonstrates how to create a RateLimiter instance with custom settings for managing request pacing and handling rate-limiting errors in a web crawling project.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_142\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import RateLimiter\n\n# Create a RateLimiter with custom settings\nrate_limiter = RateLimiter(\n    base_delay=(2.0, 4.0),  # Random delay between 2-4 seconds\n    max_delay=30.0,         # Cap delay at 30 seconds\n    max_retries=5,          # Retry up to 5 times on rate-limiting errors\n    rate_limit_codes=[429, 503]  # Handle these HTTP status codes\n)\n\n# RateLimiter will handle delays and retries internally\n# No additional setup is required for its operation\n```\n\n----------------------------------------\n\nTITLE: Configuring PruningContentFilter for General Content Cleaning\nDESCRIPTION: This snippet shows how to initialize and configure a PruningContentFilter for removing junk content without requiring a specific query. It sets up the filter with threshold settings and minimum word count requirements.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_107\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nprune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",  # or \"dynamic\"\n    min_word_threshold=50\n)\n```\n\n----------------------------------------\n\nTITLE: Blog Post Extraction Schema with BaseFields in Python\nDESCRIPTION: A schema definition for extracting blog post information, demonstrating the use of baseFields to capture the URL from anchor tags, along with standard fields like title, date, summary, and author from each post card.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nschema = {\n  \"name\": \"Blog Posts\",\n  \"baseSelector\": \"a.blog-post-card\",\n  \"baseFields\": [\n    {\"name\": \"post_url\", \"type\": \"attribute\", \"attribute\": \"href\"}\n  ],\n  \"fields\": [\n    {\"name\": \"title\", \"selector\": \"h2.post-title\", \"type\": \"text\", \"default\": \"No Title\"},\n    {\"name\": \"date\", \"selector\": \"time.post-date\", \"type\": \"text\", \"default\": \"\"},\n    {\"name\": \"summary\", \"selector\": \"p.post-summary\", \"type\": \"text\", \"default\": \"\"},\n    {\"name\": \"author\", \"selector\": \"span.post-author\", \"type\": \"text\", \"default\": \"\"}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Knowledge-Optimal Crawler Implementation\nDESCRIPTION: Shows implementation of an intelligent crawling system that optimizes data extraction while maximizing knowledge acquisition. Features smart content prioritization and objective-driven crawling paths.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.optimization import KnowledgeOptimizer\n\nasync with AsyncWebCrawler() as crawler:\n    optimizer = KnowledgeOptimizer(\n        objective=\"Understand GPU instance pricing and limitations across cloud providers\",\n        required_knowledge=[\n            \"pricing structure\",\n            \"GPU specifications\",\n            \"usage limits\",\n            \"availability zones\"\n        ],\n        confidence_threshold=0.85\n    )\n    \n    result = await crawler.arun(\n        urls=[\n            \"https://aws.amazon.com/ec2/pricing/\",\n            \"https://cloud.google.com/gpu\",\n            \"https://azure.microsoft.com/pricing/\"\n        ],\n        optimizer=optimizer,\n        optimization_mode=\"minimal_extraction\"\n    )\n    \n    print(f\"Knowledge Coverage: {result.knowledge_coverage}\")\n    print(f\"Data Efficiency: {result.efficiency_ratio}\")\n    print(f\"Extracted Content: {result.optimal_content}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Filters and Scorers in Python with AsyncWebCrawler\nDESCRIPTION: This function demonstrates the use of filters and scorers for targeted crawling. It shows examples of using a single URL pattern filter, multiple filters in a chain, and a keyword relevance scorer for prioritizing pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_208\n\nLANGUAGE: python\nCODE:\n```\nasync def filters_and_scorers():\n    \"\"\"\n    PART 3: Demonstrates the use of filters and scorers for more targeted crawling.\n\n    This function progressively adds:\n    1. A single URL pattern filter\n    2. Multiple filters in a chain\n    3. Scorers for prioritizing pages\n    \"\"\"\n    print(\"\\n===== FILTERS AND SCORERS =====\")\n\n    async with AsyncWebCrawler() as crawler:\n        # SINGLE FILTER EXAMPLE\n        print(\"\\nðŸ“Š EXAMPLE 1: SINGLE URL PATTERN FILTER\")\n        print(\"  Only crawl pages containing 'core' in the URL\")\n\n        # Create a filter that only allows URLs with 'guide' in them\n        url_filter = URLPatternFilter(patterns=[\"*core*\"])\n\n        config = CrawlerRunConfig(\n            deep_crawl_strategy=BFSDeepCrawlStrategy(\n                max_depth=1,\n                include_external=False,\n                filter_chain=FilterChain([url_filter]),  # Single filter\n            ),\n            scraping_strategy=LXMLWebScrapingStrategy(),\n            cache_mode=CacheMode.BYPASS,\n            verbose=True,\n        )\n\n        results = await crawler.arun(url=\"https://docs.crawl4ai.com\", config=config)\n\n        print(f\"  âœ… Crawled {len(results)} pages matching '*core*'\")\n        for result in results[:3]:  # Show first 3 results\n            print(f\"  â†’ {result.url}\")\n        if len(results) > 3:\n            print(f\"  ... and {len(results) - 3} more\")\n\n        # MULTIPLE FILTERS EXAMPLE\n        print(\"\\nðŸ“Š EXAMPLE 2: MULTIPLE FILTERS IN A CHAIN\")\n        print(\"  Only crawl pages that:\")\n        print(\"  1. Contain '2024' in the URL\")\n        print(\"  2. Are from 'techcrunch.com'\")\n        print(\"  3. Are of text/html or application/javascript content type\")\n\n        # Create a chain of filters\n        filter_chain = FilterChain(\n            [\n                URLPatternFilter(patterns=[\"*2024*\"]),\n                DomainFilter(\n                    allowed_domains=[\"techcrunch.com\"],\n                    blocked_domains=[\"guce.techcrunch.com\", \"oidc.techcrunch.com\"],\n                ),\n                ContentTypeFilter(\n                    allowed_types=[\"text/html\", \"application/javascript\"]\n                ),\n            ]\n        )\n\n        config = CrawlerRunConfig(\n            deep_crawl_strategy=BFSDeepCrawlStrategy(\n                max_depth=1, include_external=False, filter_chain=filter_chain\n            ),\n            scraping_strategy=LXMLWebScrapingStrategy(),\n            verbose=True,\n        )\n\n        results = await crawler.arun(url=\"https://techcrunch.com\", config=config)\n\n        print(f\"  âœ… Crawled {len(results)} pages after applying all filters\")\n        for result in results[:3]:\n            print(f\"  â†’ {result.url}\")\n        if len(results) > 3:\n            print(f\"  ... and {len(results) - 3} more\")\n\n        # SCORERS EXAMPLE\n        print(\"\\nðŸ“Š EXAMPLE 3: USING A KEYWORD RELEVANCE SCORER\")\n        print(\n            \"Score pages based on relevance to keywords: 'crawl', 'example', 'async', 'configuration','javascript','css'\"\n        )\n\n        # Create a keyword relevance scorer\n        keyword_scorer = KeywordRelevanceScorer(\n            keywords=[\"crawl\", \"example\", \"async\", \"configuration\",\"javascript\",\"css\"], weight=1\n        )\n\n        config = CrawlerRunConfig(\n            deep_crawl_strategy=BestFirstCrawlingStrategy(  \n                max_depth=1, include_external=False, url_scorer=keyword_scorer\n            ),\n            scraping_strategy=LXMLWebScrapingStrategy(),\n            cache_mode=CacheMode.BYPASS,\n            verbose=True,\n            stream=True,\n        )\n\n        results = []\n        async for result in await crawler.arun(\n            url=\"https://docs.crawl4ai.com\", config=config\n        ):\n            results.append(result)\n            score = result.metadata.get(\"score\")\n            print(f\"  â†’ Score: {score:.2f} | {result.url}\")\n\n        print(f\"  âœ… Crawler prioritized {len(results)} pages by relevance score\")\n        print(\"  ðŸ” Note: BestFirstCrawlingStrategy visits highest-scoring pages first\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Content Filtering Pipeline with CosineStrategy in Python\nDESCRIPTION: Demonstrates how to create a complete extraction pipeline that uses CosineStrategy to extract pricing features from a webpage. The function returns structured data including content, cluster count, and similarity scores.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_178\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\n```\n\n----------------------------------------\n\nTITLE: Executing Streaming Deep Crawl with Crawl4AI in Python\nDESCRIPTION: Illustrates how to enable and use streaming mode for deep crawls with `AsyncWebCrawler`. By setting `stream=True` in `CrawlerRunConfig`, the `crawler.arun` method returns an asynchronous iterator. This allows processing each `result` as soon as it becomes available, which is useful for real-time applications or handling large numbers of pages efficiently. Requires `AsyncWebCrawler`, `CrawlerRunConfig`, and a deep crawl strategy. Assumes `process_result` function is defined elsewhere.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),\n    stream=True  # Enable streaming\n)\n\nasync with AsyncWebCrawler() as crawler:\n    # Returns an async iterator\n    async for result in await crawler.arun(\"https://example.com\", config=config):\n        # Process each result as it becomes available\n        process_result(result)\n```\n\n----------------------------------------\n\nTITLE: Implementing Page Limits in Web Crawler\nDESCRIPTION: Implementation of crawl size control using max_pages parameter to limit the total number of crawled pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_43\n\nLANGUAGE: python\nCODE:\n```\n# Limit to exactly 20 pages regardless of depth\nstrategy = BFSDeepCrawlStrategy(\n    max_depth=3,\n    max_pages=20\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Global Configuration in Python\nDESCRIPTION: Retrieves the global configuration for Crawl4AI from the user's home directory. Creates the configuration directory if it doesn't exist.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef get_global_config() -> dict:\n    config_dir = Path.home() / \".crawl4ai\"\n    config_file = config_dir / \"global.yml\"\n    \n    if not config_file.exists():\n        config_dir.mkdir(parents=True, exist_ok=True)\n        return {}\n        \n    with open(config_file) as f:\n        return yaml.safe_load(f) or {}\n```\n\n----------------------------------------\n\nTITLE: Basic Setup and Simple Crawl with Crawl4AI\nDESCRIPTION: This snippet demonstrates a basic crawl operation using Crawl4AI. It crawls a specific URL and prints the first 500 characters of the raw markdown content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def simple_crawl():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            bypass_cache=True # By default this is False, meaning the cache will be used\n        )\n        print(result.markdown.raw_markdown[:500])  # Print the first 500 characters\n        \nasyncio.run(simple_crawl())\n```\n\n----------------------------------------\n\nTITLE: Failed Network Request Event JSON Structure in Crawl4AI\nDESCRIPTION: Example JSON structure for a failed network request event captured by Crawl4AI. This shows the format of data collected when a request fails, including the error message and resource type.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_156\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"event_type\": \"request_failed\",\n  \"url\": \"https://example.com/missing.png\",\n  \"method\": \"GET\",\n  \"resource_type\": \"image\",\n  \"failure_text\": \"net::ERR_ABORTED 404\",\n  \"timestamp\": 1633456789.789\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Overlapping Window Chunking\nDESCRIPTION: Configuration for creating text chunks with specified overlap between consecutive chunks.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOverlappingWindowChunking(\n    window_size: int = 1000,   # Chunk size in words\n    overlap: int = 100         # Overlap size in words\n)\n```\n\n----------------------------------------\n\nTITLE: Text Attribution Line for Crawl4AI (Markdown/Plain Text)\nDESCRIPTION: This snippet provides the required text attribution statement for users integrating Crawl4AI into their documentation or deliverables. It states the dependency on Crawl4AI and cites the project's official URL. This line can be copied into README files, documentation, or project reports. Expected usage is as a single, unmodified line to ensure proper attribution per license requirements.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_26\n\nLANGUAGE: markdown\nCODE:\n```\nThis project uses Crawl4AI (https://github.com/unclecode/crawl4ai) for web data extraction.\n\n```\n\n----------------------------------------\n\nTITLE: Crawler Run Method Implementation\nDESCRIPTION: Main crawling method that handles URL processing, caching, and content extraction with extensive configuration options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n    async def arun(\n        self,\n        url: str,\n        config: CrawlerRunConfig = None,\n        **kwargs,\n    ) -> RunManyReturn:\n        if not self.ready:\n            await self.start()\n\n        config = config or CrawlerRunConfig()\n        if not isinstance(url, str) or not url:\n            raise ValueError(\n                \"Invalid URL, make sure the URL is a non-empty string\")\n\n        async with self._lock or self.nullcontext():\n            try:\n                self.logger.verbose = config.verbose\n                if config.cache_mode is None:\n                    config.cache_mode = CacheMode.ENABLED\n                cache_context = CacheContext(url, config.cache_mode, False)\n                async_response: AsyncCrawlResponse = None\n                cached_result: CrawlResult = None\n                screenshot_data = None\n                pdf_data = None\n                extracted_content = None\n                start_time = time.perf_counter()\n\n                if cache_context.should_read():\n                    cached_result = await async_db_manager.aget_cached_url(url)\n\n                if cached_result:\n                    html = sanitize_input_encode(cached_result.html)\n                    extracted_content = sanitize_input_encode(\n                        cached_result.extracted_content or \"\"\n                    )\n                    extracted_content = (\n                        None\n                        if not extracted_content or extracted_content == \"[]\"\n                        else extracted_content\n                    )\n                    screenshot_data = cached_result.screenshot\n                    pdf_data = cached_result.pdf\n                    if config.screenshot and not screenshot_data:\n                        cached_result = None\n```\n\n----------------------------------------\n\nTITLE: Custom Content Filter Implementation\nDESCRIPTION: Example showing how to create a custom content filter by inheriting from RelevantContentFilter base class and implementing the filter_content method.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.content_filter_strategy import RelevantContentFilter\n\nclass MyCustomFilter(RelevantContentFilter):\n    def filter_content(self, html, min_word_threshold=None):\n        # parse HTML, implement custom logic\n        return [block for block in ... if ... some condition...]\n```\n\n----------------------------------------\n\nTITLE: Configuring DefaultMarkdownGenerator Options in Python\nDESCRIPTION: Shows how to customize markdown generation by configuring options like link handling, HTML escaping, and text wrapping through the DefaultMarkdownGenerator.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Example: ignore all links, don't escape HTML, and wrap text at 80 characters\n    md_generator = DefaultMarkdownGenerator(\n        options={\n            \"ignore_links\": True,\n            \"escape_html\": False,\n            \"body_width\": 80\n        }\n    )\n\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/docs\", config=config)\n        if result.success:\n            print(\"Markdown:\\n\", result.markdown[:500])  # Just a snippet\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Initializing RateLimiter for Crawl4AI\nDESCRIPTION: Constructor for the RateLimiter class, which handles rate limiting and backoff for multiple URL crawling. It defines parameters for delay between requests, maximum backoff, retry attempts, and status codes that trigger rate limiting.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_141\n\nLANGUAGE: python\nCODE:\n```\nclass RateLimiter:\n    def __init__(\n        # Random delay range between requests\n        base_delay: Tuple[float, float] = (1.0, 3.0),  \n        \n        # Maximum backoff delay\n        max_delay: float = 60.0,                        \n        \n        # Retries before giving up\n        max_retries: int = 3,                          \n        \n        # Status codes triggering backoff\n        rate_limit_codes: List[int] = [429, 503]        \n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring SemaphoreDispatcher in Python for Concurrent Crawling\nDESCRIPTION: This code example demonstrates how to set up a SemaphoreDispatcher for simple concurrency control with a fixed limit, including optional rate limiting and monitoring features.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_145\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_dispatcher import SemaphoreDispatcher\n\ndispatcher = SemaphoreDispatcher(\n    max_session_permit=20,         # Maximum concurrent tasks\n    rate_limiter=RateLimiter(      # Optional rate limiting\n        base_delay=(0.5, 1.0),\n        max_delay=10.0\n    ),\n    monitor=CrawlerMonitor(        # Optional monitoring\n        max_visible_rows=15,\n        display_mode=DisplayMode.DETAILED\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Title Length for SEO in Python\nDESCRIPTION: A method that scores the quality of a page title based on its length. Optimal lengths (50-60 characters) receive a perfect score of 1.0, good lengths (40-50 or 60-70) receive 0.7, and other lengths receive 0.3.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_192\n\nLANGUAGE: python\nCODE:\n```\ndef _score_title_length(self, title: str) -> float:\n    length = len(title)\n    if 50 <= length <= 60:\n        return 1.0\n    if 40 <= length < 50 or 60 < length <= 70:\n        return 0.7\n    return 0.3  # Poor length\n```\n\n----------------------------------------\n\nTITLE: Using DefaultMarkdownGenerator with Crawl4AI\nDESCRIPTION: This snippet demonstrates how to configure and use the DefaultMarkdownGenerator in Crawl4AI. It shows how to enable citations and set options like body width for the markdown output.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nconfig = CrawlerRunConfig(\n    markdown_generator=DefaultMarkdownGenerator(\n        options={\"citations\": True, \"body_width\": 80}  # e.g. pass html2text style options\n    )\n)\nresult = await crawler.arun(url=\"https://example.com\", config=config)\n\nmd_res = result.markdown  # or eventually 'result.markdown'\nprint(md_res.raw_markdown[:500])\nprint(md_res.markdown_with_citations)\nprint(md_res.references_markdown)\n```\n\n----------------------------------------\n\nTITLE: Fallback Class and ID Search in HTML Elements using Python and lxml\nDESCRIPTION: This function provides a fallback mechanism to search for elements by class or ID when other selectors fail. It uses regular expressions to extract class and ID selectors from the given selector string.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_78\n\nLANGUAGE: Python\nCODE:\n```\ndef _fallback_class_id_search(self, element, selector_str):\n    \"\"\"Fallback to search by class or ID\"\"\"\n    results = []\n    \n    try:\n        # Extract class selectors (.classname)\n        import re\n        class_matches = re.findall(r'\\.([a-zA-Z0-9_-]+)', selector_str)\n        \n        # Extract ID selectors (#idname)\n        id_matches = re.findall(r'#([a-zA-Z0-9_-]+)', selector_str)\n        \n        # Try each class\n        for class_name in class_matches:\n            class_results = element.xpath(f\".//*[contains(@class, '{class_name}')]\")\n            results.extend(class_results)\n            \n        # Try each ID (usually more specific)\n        for id_name in id_matches:\n            id_results = element.xpath(f\".//*[@id='{id_name}']\")\n            results.extend(id_results)\n    except Exception as e:\n        if self.verbose:\n            print(f\"Error in fallback class/id search: {e}\")\n            \n    return results\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of Crawl4AI CLI\nDESCRIPTION: Demonstrates basic commands for using the Crawl4AI CLI, including crawling a website, getting markdown output, and using verbose JSON output with cache bypass.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Basic crawling\ncrwl https://example.com\n\n# Get markdown output\ncrwl https://example.com -o markdown\n\n# Verbose JSON output with cache bypass\ncrwl https://example.com -o json -v --bypass-cache\n\n# See usage examples\ncrwl --example\n```\n\n----------------------------------------\n\nTITLE: Basic Usage Example of Crawler4ai\nDESCRIPTION: Simple example demonstrating how to import and use Crawler4ai to crawl a website, with basic configuration options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawler4ai import Crawler4ai\n\ncrawler = Crawler4ai(\n    urls=[\"https://example.com\"],\n    max_pages=100,\n    output_dir=\"crawl_results\"\n)\n\nresults = crawler.crawl()\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI and Dependencies\nDESCRIPTION: This snippet shows how to install Crawl4AI and its dependencies using pip. It also includes the installation of nest_asyncio and Playwright.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# %%capture\n!pip install crawl4ai\n!pip install nest_asyncio\n!playwright install\n```\n\n----------------------------------------\n\nTITLE: Initializing MemoryAdaptiveDispatcher in Python for Adaptive Crawling\nDESCRIPTION: This snippet illustrates the configuration of a MemoryAdaptiveDispatcher, which automatically manages concurrency based on system memory usage, with options for rate limiting and monitoring.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_144\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher\n\ndispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=90.0,  # Pause if memory exceeds this\n    check_interval=1.0,             # How often to check memory\n    max_session_permit=10,          # Maximum concurrent tasks\n    rate_limiter=RateLimiter(       # Optional rate limiting\n        base_delay=(1.0, 2.0),\n        max_delay=30.0,\n        max_retries=2\n    ),\n    monitor=CrawlerMonitor(         # Optional monitoring\n        max_visible_rows=15,\n        display_mode=DisplayMode.DETAILED\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Session-Based Crawling Example in Python\nDESCRIPTION: A simple implementation of session-based crawling that loads more content dynamically using JavaScript execution. This example demonstrates how to reuse the same session across multiple page loads, bypassing the cache for fresh content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\nfrom crawl4ai.cache_context import CacheMode\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"dynamic_content_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                cache_mode=CacheMode.BYPASS\n            )\n            \n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\n```\n\n----------------------------------------\n\nTITLE: Implementing Sentence-Based Chunking with NLTK in Python\nDESCRIPTION: This code snippet shows an NlpSentenceChunking class that uses NLTK's sent_tokenize function to divide text into sentences. It's ideal for extracting meaningful statements from a larger text.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_164\n\nLANGUAGE: python\nCODE:\n```\nfrom nltk.tokenize import sent_tokenize\n\nclass NlpSentenceChunking:\n    def chunk(self, text):\n        sentences = sent_tokenize(text)\n        return [sentence.strip() for sentence in sentences]\n\n# Example Usage\ntext = \"This is sentence one. This is sentence two.\"\nchunker = NlpSentenceChunking()\nprint(chunker.chunk(text))\n```\n\n----------------------------------------\n\nTITLE: Using LLMConfig for Extraction and Filtering Tasks\nDESCRIPTION: Shows how to use the new LLMConfig parameter for streamlining LLM integration across different Crawl4AI components. This approach simplifies configuration and enables reuse of LLM settings across extraction, filtering, and schema generation tasks.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\n# Example of using LLMConfig with LLMExtractionStrategy\nllm_config = LLMConfig(provider=\"openai/gpt-4o\", api_token=\"YOUR_API_KEY\")\nstrategy = LLMExtractionStrategy(llm_config=llm_config, schema=...)\n\n# Example usage within a crawler\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        config=CrawlerRunConfig(extraction_strategy=strategy)\n    )\n```\n\n----------------------------------------\n\nTITLE: Concurrent URL Processing in Python\nDESCRIPTION: This method implements concurrent processing of multiple URLs using a configurable dispatcher strategy. It supports both batch processing and streaming of results, with options for caching and various content processing configurations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nasync def arun_many(\n    self,\n    urls: List[str],\n    config: Optional[CrawlerRunConfig] = None,\n    dispatcher: Optional[BaseDispatcher] = None,\n    **kwargs,\n) -> RunManyReturn:\n    config = config or CrawlerRunConfig()\n\n    if dispatcher is None:\n        dispatcher = MemoryAdaptiveDispatcher(\n            rate_limiter=RateLimiter(\n                base_delay=(1.0, 3.0), max_delay=60.0, max_retries=3\n            ),\n        )\n\n    def transform_result(task_result):\n        return (\n            setattr(\n                task_result.result,\n                \"dispatch_result\",\n                DispatchResult(\n                    task_id=task_result.task_id,\n                    memory_usage=task_result.memory_usage,\n                    peak_memory=task_result.peak_memory,\n                    start_time=task_result.start_time,\n                    end_time=task_result.end_time,\n                    error_message=task_result.error_message,\n                ),\n            )\n            or task_result.result\n        )\n\n    stream = config.stream\n\n    if stream:\n        async def result_transformer():\n            async for task_result in dispatcher.run_urls_stream(\n                crawler=self, urls=urls, config=config\n            ):\n                yield transform_result(task_result)\n\n        return result_transformer()\n    else:\n        _results = await dispatcher.run_urls(crawler=self, urls=urls, config=config)\n        return [transform_result(res) for res in _results]\n```\n\n----------------------------------------\n\nTITLE: Accessing CrawlResult Properties in Crawl4AI\nDESCRIPTION: Shows how to access and utilize different properties of the CrawlResult object returned by the crawler, including content formats, status information, and extracted media and links.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    config=CrawlerRunConfig(fit_markdown=True)\n)\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown.raw_markdown) # Raw markdown from cleaned html\nprint(result.markdown.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\n```\n\n----------------------------------------\n\nTITLE: Content Extraction Method\nDESCRIPTION: Core method for extracting meaningful content from HTML using LLM. Handles prompt construction, LLM interaction, response parsing, and error handling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_70\n\nLANGUAGE: python\nCODE:\n```\ndef extract(self, url: str, ix: int, html: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract meaningful blocks or chunks from the given HTML using an LLM.\"\"\"\n        if self.verbose:\n            print(f\"[LOG] Call LLM for {url} - block index: {ix}\")\n\n        variable_values = {\n            \"URL\": url,\n            \"HTML\": escape_json_string(sanitize_html(html)),\n        }\n\n        prompt_with_variables = PROMPT_EXTRACT_BLOCKS\n```\n\n----------------------------------------\n\nTITLE: Using a Custom Browser Profile with Crawl4AI in Python\nDESCRIPTION: This Python script demonstrates configuring `AsyncWebCrawler` to use a persistent browser user data directory. It creates a directory if it doesn't exist, sets up `BrowserConfig` to use this directory (`user_data_dir`) and enable persistent context (`use_persistent_context=True`). This allows maintaining browser state (like cookies or sessions) across runs, useful for sites requiring logins or having anti-bot measures. The script then crawls a specified URL with `magic=True` and prints the content length.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport os, sys\nfrom pathlib import Path\nimport asyncio, time\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def test_news_crawl():\n    # Create a persistent user data directory\n    user_data_dir = os.path.join(Path.home(), \".crawl4ai\", \"browser_profile\")\n    os.makedirs(user_data_dir, exist_ok=True)\n\n    browser_config = BrowserConfig(\n        verbose=True,\n        headless=True,\n        user_data_dir=user_data_dir,\n        use_persistent_context=True,\n    )\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        url = \"ADDRESS_OF_A_CHALLENGING_WEBSITE\"\n        \n        result = await crawler.arun(\n            url,\n            config=run_config,\n            magic=True,\n        )\n        \n        print(f\"Successfully crawled {url}\")\n        print(f\"Content length: {len(result.markdown)}\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Fit Content from Crawl4AI Results\nDESCRIPTION: Code snippet showing how to access the filtered (fit) content from the crawl results. The fit content is available in both markdown and HTML formats.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_73\n\nLANGUAGE: python\nCODE:\n```\nfit_md = result.markdown.fit_markdown\nfit_html = result.markdown.fit_html\n```\n\n----------------------------------------\n\nTITLE: Updating CrawlerRunConfig with MHTML Capture Parameter in Python\nDESCRIPTION: Added a boolean parameter 'capture_mhtml' to the CrawlerRunConfig class to enable MHTML snapshot capture of crawled pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/JOURNAL.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass CrawlerRunConfig:\n    capture_mhtml: bool = False\n```\n\n----------------------------------------\n\nTITLE: Automated Schema Generator Implementation\nDESCRIPTION: Demonstrates the implementation of automatic JsonCssExtractionStrategy schema generation from natural language descriptions. Includes example of generated schema structure.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.schema import SchemaGenerator\n\n# Generate schema from natural language description\ngenerator = SchemaGenerator()\nschema = await generator.generate(\n    url=\"https://news-website.com\",\n    description=\"For each news article on the page, I need the headline, publication date, and main image\"\n)\n\n# Use generated schema with crawler\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://news-website.com\",\n        extraction_strategy=schema\n    )\n```\n\n----------------------------------------\n\nTITLE: Network Response Event JSON Structure in Crawl4AI\nDESCRIPTION: Example JSON structure for a network response event captured by Crawl4AI. This shows the format of data collected when a response is received, including status code, headers, and timing information.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_155\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"event_type\": \"response\",\n  \"url\": \"https://example.com/api/data.json\",\n  \"status\": 200,\n  \"status_text\": \"OK\",\n  \"headers\": {\"Content-Type\": \"application/json\", \"Cache-Control\": \"...\"},\n  \"from_service_worker\": false,\n  \"request_timing\": {\"requestTime\": 1234.56, \"receiveHeadersEnd\": 1234.78},\n  \"timestamp\": 1633456789.456\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Lazy-Load Logic with Link and Media Filters in Python\nDESCRIPTION: This snippet shows how to combine lazy-load handling with other link and media filters in the CrawlerRunConfig. It includes options for excluding external images and specific domains.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/lazy-loading.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    wait_for_images=True,\n    scan_full_page=True,\n    scroll_delay=0.5,\n\n    # Filter out external images if you only want local ones\n    exclude_external_images=True,\n\n    # Exclude certain domains for links\n    exclude_domains=[\"spammycdn.com\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Managed Browser Configuration in Crawl4AI\nDESCRIPTION: Example of configuring and using a managed browser with persistent identity in Crawl4AI.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_138\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    # 1) Reference your persistent data directory\n    browser_config = BrowserConfig(\n        headless=True,             # 'True' for automated runs\n        verbose=True,\n        use_managed_browser=True,  # Enables persistent browser strategy\n        browser_type=\"chromium\",\n        user_data_dir=\"/path/to/my-chrome-profile\"\n    )\n\n    # 2) Standard crawl config\n    crawl_config = CrawlerRunConfig(\n        wait_for=\"css:.logged-in-content\"\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url=\"https://example.com/private\", config=crawl_config)\n        if result.success:\n            print(\"Successfully accessed private data with your identity!\")\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Enhancing Chunking with Cosine Similarity for Relevance Extraction in Python\nDESCRIPTION: Combines text chunking with cosine similarity to extract relevant content based on a query. This class uses TF-IDF vectorization and cosine similarity metrics to rank chunks based on their relevance to a specified query.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_168\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass CosineSimilarityExtractor:\n    def __init__(self, query):\n        self.query = query\n        self.vectorizer = TfidfVectorizer()\n\n    def find_relevant_chunks(self, chunks):\n        vectors = self.vectorizer.fit_transform([self.query] + chunks)\n        similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n        return [(chunks[i], similarities[i]) for i in range(len(chunks))]\n\n# Example Workflow\ntext = \"\"\"This is a sample document. It has multiple sentences. \nWe are testing chunking and similarity.\"\"\"\n\nchunker = SlidingWindowChunking(window_size=5, step=3)\nchunks = chunker.chunk(text)\nquery = \"testing chunking\"\nextractor = CosineSimilarityExtractor(query)\nrelevant_chunks = extractor.find_relevant_chunks(chunks)\n\nprint(relevant_chunks)\n```\n\n----------------------------------------\n\nTITLE: Using Crawl4AI Python SDK with Docker\nDESCRIPTION: Python code example demonstrating how to use the Crawl4AI Docker client SDK for both streaming and non-streaming crawls.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.docker_client import Crawl4aiDockerClient\nfrom crawl4ai import BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    async with Crawl4aiDockerClient(base_url=\"http://localhost:8000\", verbose=True) as client:\n      # If JWT is enabled, you can authenticate like this: (more on this later)\n        # await client.authenticate(\"test@example.com\")\n        \n        # Non-streaming crawl\n        results = await client.crawl(\n            [\"https://example.com\", \"https://python.org\"],\n            browser_config=BrowserConfig(headless=True),\n            crawler_config=CrawlerRunConfig()\n        )\n        print(f\"Non-streaming results: {results}\")\n        \n        # Streaming crawl\n        crawler_config = CrawlerRunConfig(stream=True)\n        async for result in await client.crawl(\n            [\"https://example.com\", \"https://python.org\"],\n            browser_config=BrowserConfig(headless=True),\n            crawler_config=crawler_config\n        ):\n            print(f\"Streamed result: {result}\")\n        \n        # Get schema\n        schema = await client.get_schema()\n        print(f\"Schema: {schema}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Using CSS Selectors for Content Extraction with Crawl4AI\nDESCRIPTION: This function demonstrates how to use CSS selectors to target specific elements on a webpage for extraction. It targets description elements on the NBC News business page and returns the first 500 characters of extracted content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_131\n\nLANGUAGE: python\nCODE:\n```\nasync def simple_example_with_css_selector():\n    print(\"\\n--- Using CSS Selectors ---\")\n    browser_config = BrowserConfig(headless=True)\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS, css_selector=\".wide-tease-item__description\"\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\", config=crawler_config\n        )\n        print(result.markdown[:500])\n```\n\n----------------------------------------\n\nTITLE: Crawling a Local HTML File with Crawl4AI\nDESCRIPTION: Shows how to use Crawl4AI to process a local HTML file by prefixing the file path with 'file://'. This approach allows crawling HTML content stored on the local filesystem rather than from the web.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_101\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def crawl_local_file():\n    local_file_path = \"/path/to/apple.html\"  # Replace with your file path\n    file_url = f\"file://{local_file_path}\"\n    config = CrawlerRunConfig(bypass_cache=True)\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=file_url, config=config)\n        if result.success:\n            print(\"Markdown Content from Local File:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl local file: {result.error_message}\")\n\nasyncio.run(crawl_local_file())\n```\n\n----------------------------------------\n\nTITLE: Building Crawl4AI Image with Options using Buildx (Bash)\nDESCRIPTION: Demonstrates passing build arguments (`--build-arg`) like `INSTALL_TYPE` and `ENABLE_GPU` to the `docker buildx build` command for customized local builds.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n# Build with additional options\ndocker buildx build \\\n  --build-arg INSTALL_TYPE=all \\\n  --build-arg ENABLE_GPU=false \\\n  -t crawl4ai-local:latest --load .\n```\n\n----------------------------------------\n\nTITLE: Adding MHTML Data Field to AsyncCrawlResponse Class in Python\nDESCRIPTION: Modified the AsyncCrawlResponse class to include an optional 'mhtml_data' field for storing the captured MHTML content before mapping to CrawlResult.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/JOURNAL.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass AsyncCrawlResponse:\n    mhtml_data: Optional[str] = None\n```\n\n----------------------------------------\n\nTITLE: Failed Network Request Event Structure in Crawl4AI (JSON)\nDESCRIPTION: This JSON structure represents a failed network request event captured by Crawl4AI, including the failure reason and resource type.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/network-console-capture.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"event_type\": \"request_failed\",\n  \"url\": \"https://example.com/missing.png\",\n  \"method\": \"GET\",\n  \"resource_type\": \"image\",\n  \"failure_text\": \"net::ERR_ABORTED 404\",\n  \"timestamp\": 1633456789.789\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Robots.txt Compliance in Crawl4AI\nDESCRIPTION: Shows how to configure Crawl4AI to respect robots.txt rules when crawling websites. This simple configuration option helps maintain ethical scraping practices.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(check_robots_txt=True)\n```\n\n----------------------------------------\n\nTITLE: Working with Extracted Content in Python\nDESCRIPTION: Demonstrates how to access and parse structured JSON content that has been extracted from a webpage using extraction strategies.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nif result.extracted_content:\n    data = json.loads(result.extracted_content)\n    print(data)\n```\n\n----------------------------------------\n\nTITLE: Default Markdown Generator Implementation\nDESCRIPTION: Concrete implementation of markdown generation strategy that handles HTML to markdown conversion with citations and content filtering.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_109\n\nLANGUAGE: python\nCODE:\n```\nclass DefaultMarkdownGenerator(MarkdownGenerationStrategy):\n    def __init__(\n        self,\n        content_filter: Optional[RelevantContentFilter] = None,\n        options: Optional[Dict[str, Any]] = None,\n        content_source: str = \"cleaned_html\",\n    ):\n        super().__init__(content_filter, options, verbose=False, content_source=content_source)\n\n    def convert_links_to_citations(\n        self, markdown: str, base_url: str = \"\"\n    ) -> Tuple[str, str]:\n        link_map = {}\n        url_cache = {}\n        parts = []\n        last_end = 0\n        counter = 1\n\n        for match in LINK_PATTERN.finditer(markdown):\n            parts.append(markdown[last_end : match.start()])\n            text, url, title = match.groups()\n\n            if base_url and not url.startswith((\"http://\", \"https://\", \"mailto:\")):\n                if url not in url_cache:\n                    url_cache[url] = fast_urljoin(base_url, url)\n                url = url_cache[url]\n\n            if url not in link_map:\n                desc = []\n                if title:\n                    desc.append(title)\n                if text and text != title:\n                    desc.append(text)\n                link_map[url] = (counter, \": \" + \" - \".join(desc) if desc else \"\")\n                counter += 1\n\n            num = link_map[url][0]\n            parts.append(\n                f\"{text}âŸ¨{num}âŸ©\"\n                if not match.group(0).startswith(\"!\")\n                else f\"![{text}âŸ¨{num}âŸ©]\"\n            )\n            last_end = match.end()\n\n        parts.append(markdown[last_end:])\n        converted_text = \"\".join(parts)\n\n        references = [\"\\n\\n## References\\n\\n\"]\n        references.extend(\n            f\"âŸ¨{num}âŸ© {url}{desc}\\n\"\n            for url, (num, desc) in sorted(link_map.items(), key=lambda x: x[1][0])\n        )\n\n        return converted_text, \"\".join(references)\n\n    def generate_markdown(\n        self,\n        input_html: str,\n        base_url: str = \"\",\n        html2text_options: Optional[Dict[str, Any]] = None,\n        options: Optional[Dict[str, Any]] = None,\n        content_filter: Optional[RelevantContentFilter] = None,\n        citations: bool = True,\n        **kwargs,\n    ) -> MarkdownGenerationResult:\n        try:\n            h = CustomHTML2Text(baseurl=base_url)\n            default_options = {\n                \"body_width\": 0,\n                \"ignore_emphasis\": False,\n                \"ignore_links\": False,\n                \"ignore_images\": False,\n                \"protect_links\": False,\n                \"single_line_break\": True,\n                \"mark_code\": True,\n                \"escape_snob\": False,\n            }\n\n            if html2text_options:\n                default_options.update(html2text_options)\n            elif options:\n                default_options.update(options)\n            elif self.options:\n                default_options.update(self.options)\n\n            h.update_params(**default_options)\n\n            if not input_html:\n                input_html = \"\"\n            elif not isinstance(input_html, str):\n                input_html = str(input_html)\n\n            try:\n                raw_markdown = h.handle(input_html)\n            except Exception as e:\n                raw_markdown = f\"Error converting HTML to markdown: {str(e)}\"\n\n            raw_markdown = raw_markdown.replace(\"    ```\", \"```\")\n\n            markdown_with_citations: str = raw_markdown\n            references_markdown: str = \"\"\n            if citations:\n                try:\n                    (\n                        markdown_with_citations,\n                        references_markdown,\n                    ) = self.convert_links_to_citations(raw_markdown, base_url)\n                except Exception as e:\n                    markdown_with_citations = raw_markdown\n                    references_markdown = f\"Error generating citations: {str(e)}\"\n\n            fit_markdown: Optional[str] = \"\"\n            filtered_html: Optional[str] = \"\"\n            if content_filter or self.content_filter:\n                try:\n                    content_filter = content_filter or self.content_filter\n                    filtered_html = content_filter.filter_content(input_html)\n                    filtered_html = \"\\n\".join(\n                        \"<div>{}</div>\".format(s) for s in filtered_html\n                    )\n                    fit_markdown = h.handle(filtered_html)\n                except Exception as e:\n                    fit_markdown = f\"Error generating fit markdown: {str(e)}\"\n                    filtered_html = \"\"\n\n            return MarkdownGenerationResult(\n                raw_markdown=raw_markdown or \"\",\n                markdown_with_citations=markdown_with_citations or \"\",\n                references_markdown=references_markdown or \"\",\n                fit_markdown=fit_markdown or \"\",\n                fit_html=filtered_html or \"\",\n            )\n        except Exception as e:\n            error_msg = f\"Error in markdown generation: {str(e)}\"\n            return MarkdownGenerationResult(\n                raw_markdown=error_msg,\n                markdown_with_citations=error_msg,\n                references_markdown=\"\",\n                fit_markdown=\"\",\n                fit_html=\"\",\n            )\n```\n\n----------------------------------------\n\nTITLE: Implementing Deep Crawling with BestFirstCrawlingStrategy in Python\nDESCRIPTION: This snippet demonstrates how to set up and use deep crawling in Crawl4AI v0.5.0, including configuring filters, scorers, and the crawling strategy. It uses BestFirstCrawlingStrategy to prioritize URLs based on relevance.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeepCrawlStrategy\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\nfrom crawl4ai.deep_crawling import DomainFilter, ContentTypeFilter, FilterChain, URLPatternFilter, KeywordRelevanceScorer, BestFirstCrawlingStrategy\nimport asyncio\n\n# Create a filter chain to filter urls based on patterns, domains and content type\nfilter_chain = FilterChain(\n    [\n        DomainFilter(\n            allowed_domains=[\"docs.crawl4ai.com\"],\n            blocked_domains=[\"old.docs.crawl4ai.com\"],\n        ),\n        URLPatternFilter(patterns=[\"*core*\", \"*advanced*\"],),\n        ContentTypeFilter(allowed_types=[\"text/html\"]),\n    ]\n)\n\n# Create a keyword scorer that prioritises the pages with certain keywords first\nkeyword_scorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"], weight=0.7\n)\n\n# Set up the configuration\ndeep_crawl_config = CrawlerRunConfig(\n    deep_crawl_strategy=BestFirstCrawlingStrategy(\n        max_depth=2,\n        include_external=False,\n        filter_chain=filter_chain,\n        url_scorer=keyword_scorer,\n    ),\n    scraping_strategy=LXMLWebScrapingStrategy(),\n    stream=True,\n    verbose=True,\n)\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        start_time = time.perf_counter()\n        results = []\n        async for result in await crawler.arun(url=\"https://docs.crawl4ai.com\", config=deep_crawl_config):\n            print(f\"Crawled: {result.url} (Depth: {result.metadata['depth']}), score: {result.metadata['score']:.2f}\")\n            results.append(result)\n        duration = time.perf_counter() - start_time\n        print(f\"\\nâœ… Crawled {len(results)} high-value pages in {duration:.2f} seconds\")\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Web Embedding Index Implementation\nDESCRIPTION: Implements semantic search infrastructure for crawled content using vector embeddings. Features automatic embedding generation, content chunking, and efficient vector storage.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.indexing import WebIndex\n\n# Initialize and build index\nindex = WebIndex(model=\"efficient-mini\")\n\nasync with AsyncWebCrawler() as crawler:\n    # Crawl and index content\n    await index.build(\n        urls=[\"https://docs.example.com\"],\n        crawler=crawler,\n        options={\n            \"chunk_method\": \"semantic\",\n            \"update_policy\": \"incremental\",\n            \"embedding_batch_size\": 100\n        }\n    )\n\n    # Search through indexed content\n    results = await index.search(\n        query=\"How to implement OAuth authentication?\",\n        filters={\n            \"content_type\": \"technical\",\n            \"recency\": \"6months\"\n        },\n        top_k=5\n    )\n\n    # Get similar content\n    similar = await index.find_similar(\n        url=\"https://docs.example.com/auth/oauth\",\n        threshold=0.85\n    )\n```\n\n----------------------------------------\n\nTITLE: Processing Raw HTML and Local Files with AsyncWebCrawler in Python\nDESCRIPTION: An async function that demonstrates how to process both raw HTML strings and local HTML files using AsyncWebCrawler. It creates a sample HTML file, then processes both the raw HTML string and the file URL format. The function shows how to bypass cache and access the markdown output from the crawler.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_158\n\nLANGUAGE: python\nCODE:\n```\nasync def demo_raw_html_and_file():\n    \"\"\"Process raw HTML and local files\"\"\"\n    print(\"\\n=== 11. Raw HTML and Local Files ===\")\n\n    raw_html = \"\"\"\n    <html><body>\n        <h1>Sample Article</h1>\n        <p>This is sample content for testing Crawl4AI's raw HTML processing.</p>\n    </body></html>\n    \"\"\"\n\n    # Save to file\n    file_path = Path(\"docs/examples/tmp/sample.html\").absolute()\n    with open(file_path, \"w\") as f:\n        f.write(raw_html)\n\n    async with AsyncWebCrawler() as crawler:\n        # Crawl raw HTML\n        raw_result = await crawler.arun(\n            url=\"raw:\" + raw_html, config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n        )\n        print(\"Raw HTML processing:\")\n        print(f\"  Markdown: {raw_result.markdown.raw_markdown[:50]}...\")\n\n        # Crawl local file\n        file_result = await crawler.arun(\n            url=f\"file://{file_path}\",\n            config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS),\n        )\n        print(\"\\nLocal file processing:\")\n        print(f\"  Markdown: {file_result.markdown.raw_markdown[:50]}...\")\n\n    # Clean up\n    os.remove(file_path)\n    print(f\"Processed both raw HTML and local file ({file_path})\")\n```\n\n----------------------------------------\n\nTITLE: Basic Extraction Complete Example for Crawl4AI\nDESCRIPTION: Comprehensive example showing basic extraction with browser and crawler configurations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncrwl https://example.com \\\n    -B browser.yml \\\n    -C crawler.yml \\\n    -o json\n```\n\n----------------------------------------\n\nTITLE: Implementing AsyncHTTPCrawlerStrategy in Python\nDESCRIPTION: This example shows how to use the new AsyncHTTPCrawlerStrategy, a lightweight and fast HTTP-only crawler. It demonstrates configuring the HTTP crawler and using it with AsyncWebCrawler for simple scraping tasks.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, HTTPCrawlerConfig\nfrom crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy\nimport asyncio\n\n# Use the HTTP crawler strategy\nhttp_crawler_config = HTTPCrawlerConfig(\n        method=\"GET\",\n        headers={\"User-Agent\": \"MyCustomBot/1.0\"},\n        follow_redirects=True,\n        verify_ssl=True\n)\n\nasync def main():\n    async with AsyncWebCrawler(crawler_strategy=AsyncHTTPCrawlerStrategy(browser_config =http_crawler_config)) as crawler:\n        result = await crawler.arun(\"https://example.com\")\n        print(f\"Status code: {result.status_code}\")\n        print(f\"Content length: {len(result.html)}\")\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Requesting HTML Extraction via API\nDESCRIPTION: Provides the JSON payload structure for making a request to the `/html` endpoint. It requires a single key, `url`, specifying the target web page from which to extract optimized HTML.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"url\": \"https://example.com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Saving PDF Documents in Python\nDESCRIPTION: Shows how to save a PDF document that was generated during a crawl operation when the pdf option is enabled.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nif result.pdf:\n    with open(\"page.pdf\", \"wb\") as f:\n        f.write(result.pdf)\n```\n\n----------------------------------------\n\nTITLE: Accessing MarkdownGenerationResult Properties in Python\nDESCRIPTION: This snippet demonstrates how to access various properties of the MarkdownGenerationResult object, including raw_markdown, markdown_with_citations, references_markdown, and fit_markdown.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmd_obj = result.markdown  # your library's naming may vary\nprint(\"RAW:\\n\", md_obj.raw_markdown)\nprint(\"CITED:\\n\", md_obj.markdown_with_citations)\nprint(\"REFERENCES:\\n\", md_obj.references_markdown)\nprint(\"FIT:\\n\", md_obj.fit_markdown)\n```\n\n----------------------------------------\n\nTITLE: Accessing Extracted Media (Images, Tables) in Crawl4AI (Python)\nDESCRIPTION: This snippet demonstrates how to access media elements extracted by Crawl4AI, which are stored in the `result.media` dictionary. It shows retrieving lists of images (`images_info`) and tables (`tables`) and iterating through them to print details like image `src`, `alt` text, `score`, table `caption`, number of columns, and number of rows.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif result.success:\n    # Get images\n    images_info = result.media.get(\"images\", [])\n    print(f\"Found {len(images_info)} images in total.\")\n    for i, img in enumerate(images_info[:3]):  # Inspect just the first 3\n        print(f\"[Image {i}] URL: {img['src']}\")\n        print(f\"           Alt text: {img.get('alt', '')}\")\n        print(f\"           Score: {img.get('score')}\")\n        print(f\"           Description: {img.get('desc', '')}\\n\")\n    \n    # Get tables\n    tables = result.media.get(\"tables\", [])\n    print(f\"Found {len(tables)} data tables in total.\")\n    for i, table in enumerate(tables):\n        print(f\"[Table {i}] Caption: {table.get('caption', 'No caption')}\")\n        print(f\"           Columns: {len(table.get('headers', []))}\")\n        print(f\"           Rows: {len(table.get('rows', []))}\")\n```\n\n----------------------------------------\n\nTITLE: Network Request Capture Handler Implementation - Python\nDESCRIPTION: Implements request capture functionality for tracking network requests, including headers, post data, and resource types. Handles binary data safely and includes error handling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/prompts/prompt_net_requests.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def handle_request_capture(request):\n    try:\n        post_data_str = None\n        try:\n            post_data = request.post_data_buffer\n            if post_data:\n                try:\n                    post_data_str = post_data.decode('utf-8', errors='replace')\n                except UnicodeDecodeError:\n                    post_data_str = f\"[Binary data: {len(post_data)} bytes]\"\n        except Exception:\n            post_data_str = \"[Error retrieving post data]\"\n\n        captured_requests.append({\n            \"event_type\": \"request\",\n            \"url\": request.url,\n            \"method\": request.method,\n            \"headers\": dict(request.headers),\n            \"post_data\": post_data_str,\n            \"resource_type\": request.resource_type,\n            \"is_navigation_request\": request.is_navigation_request(),\n            \"timestamp\": time.time()\n        })\n    except Exception as e:\n        self.logger.warning(f\"Error capturing request details for {request.url}: {e}\", tag=\"CAPTURE\")\n        captured_requests.append({\"event_type\": \"request_capture_error\", \"url\": request.url, \"error\": str(e), \"timestamp\": time.time()})\n```\n\n----------------------------------------\n\nTITLE: Applying Basic URL Pattern Filter in Crawl4AI Deep Crawl using Python\nDESCRIPTION: Demonstrates configuring a `BFSDeepCrawlStrategy` to use a `FilterChain` containing a `URLPatternFilter`. This filter restricts the crawler to only follow URLs that match specific wildcard patterns (e.g., containing 'blog' or 'docs'). The filter is passed within a `FilterChain` list to the `filter_chain` argument of the strategy. Depends on `FilterChain`, `URLPatternFilter` from `crawl4ai.deep_crawling.filters`, `CrawlerRunConfig`, and `BFSDeepCrawlStrategy`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter\n\n# Only follow URLs containing \"blog\" or \"docs\"\nurl_filter = URLPatternFilter(patterns=[\"*blog*\", \"*docs*\"])\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([url_filter])\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Performing a Simple Crawl via Direct API Call\nDESCRIPTION: Illustrates how to make a direct HTTP POST request to the crawl4ai server's `/crawl` endpoint using Python's `requests` library. It constructs the JSON payload, including nested `BrowserConfig` and `CrawlerRunConfig` objects following the required `{\"type\": \"ClassName\", \"params\": {...}}` structure, sends the request, and prints the status code and response.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\n# Configuration objects converted to the required JSON structure\nbrowser_config_payload = {\n    \"type\": \"BrowserConfig\",\n    \"params\": {\"headless\": True}\n}\ncrawler_config_payload = {\n    \"type\": \"CrawlerRunConfig\",\n    \"params\": {\"stream\": False, \"cache_mode\": \"bypass\"} # Use string value of enum\n}\n\ncrawl_payload = {\n    \"urls\": [\"https://httpbin.org/html\"],\n    \"browser_config\": browser_config_payload,\n    \"crawler_config\": crawler_config_payload\n}\nresponse = requests.post(\n    \"http://localhost:11235/crawl\", # Updated port\n    # headers={\"Authorization\": f\"Bearer {token}\"},  # If JWT is enabled\n    json=crawl_payload\n)\nprint(f\"Status Code: {response.status_code}\")\nif response.ok:\n    print(response.json())\nelse:\n    print(f\"Error: {response.text}\")\n\n```\n\n----------------------------------------\n\nTITLE: Requesting a Screenshot via API\nDESCRIPTION: Shows the JSON payload structure for the `/screenshot` endpoint. It includes the mandatory `url`, an optional `screenshot_wait_for` delay (in seconds, default 2), and an optional `output_path` to save the PNG file.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"url\": \"https://example.com\",\n  \"screenshot_wait_for\": 2,\n  \"output_path\": \"/path/to/save/screenshot.png\"\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing CrawlerRunConfig in Python for Crawl4AI\nDESCRIPTION: This snippet shows the structure of the CrawlerRunConfig class, which controls how each crawl operates in Crawl4AI. It includes parameters for content extraction, caching, JavaScript execution, resource management, and various other crawl-specific options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass CrawlerRunConfig:\n    def __init__(\n        word_count_threshold=200,\n        extraction_strategy=None,\n        markdown_generator=None,\n        cache_mode=None,\n        js_code=None,\n        wait_for=None,\n        screenshot=False,\n        pdf=False,\n        capture_mhtml=False,\n        # Location and Identity Parameters\n        locale=None,            # e.g. \"en-US\", \"fr-FR\"\n        timezone_id=None,       # e.g. \"America/New_York\"\n        geolocation=None,       # GeolocationConfig object\n        # Resource Management\n        enable_rate_limiting=False,\n        rate_limit_config=None,\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=20,\n        display_mode=None,\n        verbose=True,\n        stream=False,  # Enable streaming for arun_many()\n        # ... other advanced parameters omitted\n    ):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Exporting SSL Certificate to JSON in Python\nDESCRIPTION: Shows how to export an SSL certificate to JSON format, either as a returned string or written to a file. This method converts the certificate fields into a structured JSON format.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\njson_data = cert.to_json()  # returns JSON string\ncert.to_json(\"certificate.json\")  # writes file, returns None\n```\n\n----------------------------------------\n\nTITLE: Installing Crawler4ai with pip\nDESCRIPTION: Command to install the Crawler4ai package from PyPI using pip.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install crawler4ai\n```\n\n----------------------------------------\n\nTITLE: Adjusting Viewport Dynamically to Match Page Content\nDESCRIPTION: This code shows how to enable dynamic viewport adjustment that matches the page's content dimensions. The crawler calculates page width and height after loading and adjusts the viewport accordingly, ensuring all content is visible.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nawait crawler.crawl(\n    url=\"https://example.com\",\n    adjust_viewport_to_content=True  # Dynamically adjusts the viewport\n)\n```\n\n----------------------------------------\n\nTITLE: Crawler Lifecycle Management Methods\nDESCRIPTION: Implements async context management and explicit lifecycle control methods for starting and closing the crawler instance.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n    async def start(self):\n        await self.crawler_strategy.__aenter__()\n        self.logger.info(f\"Crawl4AI {crawl4ai_version}\", tag=\"INIT\")\n        self.ready = True\n        return self\n\n    async def close(self):\n        await self.crawler_strategy.__aexit__(None, None, None)\n\n    async def __aenter__(self):\n        return await self.start()\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close()\n\n    @asynccontextmanager\n    async def nullcontext(self):\n        yield\n```\n\n----------------------------------------\n\nTITLE: ManagedBrowser Class Implementation in Python\nDESCRIPTION: Implements a browser management class that handles browser process lifecycle, CDP connectivity, and browser configuration. Includes methods for starting, monitoring, and cleaning up browser processes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_112\n\nLANGUAGE: python\nCODE:\n```\nclass ManagedBrowser:\n    def __init__(\n        self,\n        browser_type: str = \"chromium\",\n        user_data_dir: Optional[str] = None,\n        headless: bool = False,\n        logger=None,\n        host: str = \"localhost\",\n        debugging_port: int = 9222,\n        cdp_url: Optional[str] = None, \n        browser_config: Optional[BrowserConfig] = None,\n    ):\n        self.browser_type = browser_config.browser_type\n        self.user_data_dir = browser_config.user_data_dir\n        self.headless = browser_config.headless\n        self.browser_process = None\n        self.temp_dir = None\n        self.debugging_port = browser_config.debugging_port\n        self.host = browser_config.host\n        self.logger = logger\n        self.shutting_down = False\n        self.cdp_url = browser_config.cdp_url\n        self.browser_config = browser_config\n```\n\n----------------------------------------\n\nTITLE: Implementing ContentTypeFilter Class for MIME Type Filtering in Python\nDESCRIPTION: This class provides content type filtering based on MIME types. It includes a fast extension to MIME type mapping for efficient lookups. The class is designed for optimized performance in URL filtering based on content types.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_186\n\nLANGUAGE: Python\nCODE:\n```\nclass ContentTypeFilter(URLFilter):\n    \"\"\"Optimized content type filter using fast lookups\"\"\"\n\n    __slots__ = (\"allowed_types\", \"_ext_map\", \"_check_extension\")\n\n    # Fast extension to mime type mapping\n    _MIME_MAP = {\n        # Text Formats\n        \"txt\": \"text/plain\",\n        \"html\": \"text/html\",\n        \"htm\": \"text/html\",\n        \"xhtml\": \"application/xhtml+xml\",\n        \"css\": \"text/css\",\n        \"csv\": \"text/csv\",\n        \"ics\": \"text/calendar\",\n        \"js\": \"application/javascript\",\n        # Images\n        \"bmp\": \"image/bmp\",\n        \"gif\": \"image/gif\",\n        \"jpeg\": \"image/jpeg\",\n        \"jpg\": \"image/jpeg\",\n        \"png\": \"image/png\",\n        \"svg\": \"image/svg+xml\",\n        \"tiff\": \"image/tiff\",\n        \"ico\": \"image/x-icon\",\n        \"webp\": \"image/webp\",\n        # Audio\n        \"mp3\": \"audio/mpeg\",\n        \"wav\": \"audio/wav\",\n        \"ogg\": \"audio/ogg\",\n        \"m4a\": \"audio/mp4\",\n        \"aac\": \"audio/aac\",\n        # Video\n        \"mp4\": \"video/mp4\",\n        \"mpeg\": \"video/mpeg\",\n        \"webm\": \"video/webm\",\n        \"avi\": \"video/x-msvideo\",\n        \"mov\": \"video/quicktime\",\n        \"flv\": \"video/x-flv\",\n        \"wmv\": \"video/x-ms-wmv\",\n        \"mkv\": \"video/x-matroska\",\n        # Applications\n        \"json\": \"application/json\",\n        \"xml\": \"application/xml\",\n        \"pdf\": \"application/pdf\",\n        \"zip\": \"application/zip\",\n        \"gz\": \"application/gzip\",\n        \"tar\": \"application/x-tar\",\n        \"rar\": \"application/vnd.rar\",\n        \"7z\": \"application/x-7z-compressed\",\n        \"exe\": \"application/vnd.microsoft.portable-executable\",\n        \"msi\": \"application/x-msdownload\",\n        # Fonts\n        \"woff\": \"font/woff\",\n        \"woff2\": \"font/woff2\",\n        \"ttf\": \"font/ttf\",\n        \"otf\": \"font/otf\",\n        # Microsoft Office\n        \"doc\": \"application/msword\",\n        \"dot\": \"application/msword\",\n        # ... (additional mappings)\n    }\n```\n\n----------------------------------------\n\nTITLE: Using LXML Web Scraping Strategy for Improved Performance in Python\nDESCRIPTION: This snippet demonstrates how to use the LXMLWebScrapingStrategy for faster HTML content processing. It configures AsyncWebCrawler with the LXML-based strategy, which is especially beneficial for large HTML documents.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LXMLWebScrapingStrategy\n\nasync def main():\n    config = CrawlerRunConfig(\n        scraping_strategy=LXMLWebScrapingStrategy()  # Faster alternative to default BeautifulSoup\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\", \n            config=config\n        )\n```\n\n----------------------------------------\n\nTITLE: Multiple Filter Implementation in Web Crawler\nDESCRIPTION: Advanced filtering implementation combining multiple filter types including URL patterns, domains, and content types.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling.filters import (\n    FilterChain,\n    URLPatternFilter,\n    DomainFilter,\n    ContentTypeFilter\n)\n\n# Create a chain of filters\nfilter_chain = FilterChain([\n    # Only follow URLs with specific patterns\n    URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\"]),\n    \n    # Only crawl specific domains\n    DomainFilter(\n        allowed_domains=[\"docs.example.com\"],\n        blocked_domains=[\"old.docs.example.com\"]\n    ),\n    \n    # Only include specific content types\n    ContentTypeFilter(allowed_types=[\"text/html\"])\n])\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=2,\n        filter_chain=filter_chain\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Handling Crawl Errors in Crawl4AI\nDESCRIPTION: Shows how to check for crawl success and handle error cases by examining the status code and error messages in the CrawlResult.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig()\nresult = await crawler.arun(url=\"https://example.com\", config=run_config)\n\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\n```\n\n----------------------------------------\n\nTITLE: Automatic Schema Generation with LLM in Python\nDESCRIPTION: Code for generating extraction schemas automatically using LLM models through the crawl4ai library. This example shows how to use OpenAI's GPT-4 to create a CSS selector-based schema from sample HTML content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, JsonXPathExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Sample HTML with product information\nhtml = \"\"\"\n<div class=\"product-card\">\n    <h2 class=\"title\">Gaming Laptop</h2>\n    <div class=\"price\">$999.99</div>\n    <div class=\"specs\">\n        <ul>\n            <li>16GB RAM</li>\n            <li>1TB SSD</li>\n        </ul>\n    </div>\n</div>\n\"\"\"\n\n# Option 1: Using OpenAI (requires API token)\ncss_schema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"css\", \n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Simple Crawl with Crawl4AI REST API\nDESCRIPTION: Demonstrates how to perform a simple crawl operation using the Crawl4AI REST API with Python requests library.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\ncrawl_payload = {\n    \"urls\": [\"https://example.com\"],\n    \"browser_config\": {\"headless\": True},\n    \"crawler_config\": {\"stream\": False}\n}\nresponse = requests.post(\n    \"http://localhost:8000/crawl\",\n    # headers={\"Authorization\": f\"Bearer {token}\"},  # If JWT is enabled, more on this later\n    json=crawl_payload\n)\nprint(response.json())  # Print the response for debugging\n```\n\n----------------------------------------\n\nTITLE: Implementing CompositeScorer Class for Combining Multiple Scoring Strategies in Python\nDESCRIPTION: A scorer that combines multiple scoring strategies with vectorized operations and memory optimization. Uses pre-allocated arrays for scores and weights, with optional normalization and caching for performance.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_201\n\nLANGUAGE: python\nCODE:\n```\nclass CompositeScorer(URLScorer):\n    __slots__ = ('_scorers', '_normalize', '_weights_array', '_score_array')\n    \n    def __init__(self, scorers: List[URLScorer], normalize: bool = True):\n        \"\"\"Initialize composite scorer combining multiple scoring strategies.\n        \n        Optimized for:\n        - Fast parallel scoring\n        - Memory efficient score aggregation\n        - Quick short-circuit conditions\n        - Pre-allocated arrays\n        \n        Args:\n            scorers: List of scoring strategies to combine\n            normalize: Whether to normalize final score by scorer count\n        \"\"\"\n        super().__init__(weight=1.0)\n        self._scorers = scorers\n        self._normalize = normalize\n        \n        # Pre-allocate arrays for scores and weights\n        self._weights_array = array('f', [s.weight for s in scorers])\n        self._score_array = array('f', [0.0] * len(scorers))\n\n    @lru_cache(maxsize=10000)\n    def _calculate_score(self, url: str) -> float:\n        \"\"\"Calculate combined score from all scoring strategies.\n        \n        Uses:\n        1. Pre-allocated arrays for scores\n        2. Short-circuit on zero scores\n        3. Optimized normalization\n        4. Vectorized operations where possible\n        \n        Args:\n            url: URL to score\n            \n        Returns:\n            Combined and optionally normalized score\n        \"\"\"\n        total_score = 0.0\n        scores = self._score_array\n        \n        # Get scores from all scorers\n        for i, scorer in enumerate(self._scorers):\n            # Use public score() method which applies weight\n            scores[i] = scorer.score(url)\n            total_score += scores[i]\n            \n        # Normalize if requested\n        if self._normalize and self._scorers:\n            count = len(self._scorers)\n            return total_score / count\n            \n        return total_score\n\n    def score(self, url: str) -> float:\n        \"\"\"Public scoring interface with stats tracking.\n        \n        Args:\n            url: URL to score\n            \n        Returns:\n            Final combined score\n        \"\"\"\n        score = self._calculate_score(url)\n        self.stats.update(score)\n        return score\n```\n\n----------------------------------------\n\nTITLE: Saving PDF and MHTML from CrawlResult in Python\nDESCRIPTION: Code example showing how to save the PDF and MHTML content captured during crawling to local files. These formats are useful for archiving or offline viewing of web pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Save the PDF\nwith open(\"page.pdf\", \"wb\") as f:\n    f.write(result.pdf)\n\n# Save the MHTML\nif result.mhtml:\n    with open(\"page.mhtml\", \"w\", encoding=\"utf-8\") as f:\n        f.write(result.mhtml)\n```\n\n----------------------------------------\n\nTITLE: Initializing BrowserConfig in Python for Crawl4AI\nDESCRIPTION: This snippet shows the structure of the BrowserConfig class, which controls how the browser is launched and behaves in Crawl4AI. It includes parameters for browser type, headless mode, proxy settings, viewport dimensions, and various other browser-related options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BrowserConfig:\n    def __init__(\n        browser_type=\"chromium\",\n        headless=True,\n        proxy_config=None,\n        viewport_width=1080,\n        viewport_height=600,\n        verbose=True,\n        use_persistent_context=False,\n        user_data_dir=None,\n        cookies=None,\n        headers=None,\n        user_agent=None,\n        text_mode=False,\n        light_mode=False,\n        extra_args=None,\n        # ... other advanced parameters omitted here\n    ):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Configuring CosineStrategy for Article Content Extraction in Python\nDESCRIPTION: Shows optimal settings for extracting the main content from articles. Uses higher word count threshold for longer text blocks and top_k=1 to focus on the primary content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_174\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n----------------------------------------\n\nTITLE: Crawler Configuration with Regex Chunking Strategy\nDESCRIPTION: JSON example demonstrating the strategy pattern implementation for text chunking in Crawl4AI crawler configurations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_54\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"crawler_config\": {\n        \"type\": \"CrawlerRunConfig\",\n        \"params\": {\n            \"chunking_strategy\": {\n                \"type\": \"RegexChunking\",      // Strategy implementation\n                \"params\": {\n                    \"patterns\": [\"\\n\\n\", \"\\\\.\\\\s+\"]\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Cosine Strategy for Content Similarity\nDESCRIPTION: Configuration for content similarity-based extraction and clustering with parameters for content filtering, clustering settings, and model configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,        # Topic/keyword filter\n    word_count_threshold: int = 10,     # Minimum words per cluster\n    sim_threshold: float = 0.3,         # Similarity threshold\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,             # Maximum cluster distance\n    linkage_method: str = 'ward',       # Clustering method\n    top_k: int = 3,                    # Top clusters to return\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False              # Enable verbose logging\n)\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Filtering Options in Crawl4AI\nDESCRIPTION: Example showing how to combine content filtering with additional exclusion parameters for a more refined extraction process.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_76\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    word_count_threshold=10,\n    excluded_tags=[\"nav\", \"footer\", \"header\"],\n    exclude_external_links=True,\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=PruningContentFilter(threshold=0.5)\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Using css_selector for Content Selection in Crawl4AI\nDESCRIPTION: This snippet demonstrates how to use the css_selector parameter in CrawlerRunConfig to limit crawl results to a specific region of a webpage. The example targets the first 30 items from Hacker News.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # e.g., first 30 items from Hacker News\n        css_selector=\".athing:nth-child(-n+30)\"  \n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com/newest\", \n            config=config\n        )\n        print(\"Partial HTML length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Working with Media Information in Python\nDESCRIPTION: Demonstrates how to access and filter media elements (like images) discovered during crawling, including using relevance scores to identify high-value content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimages = result.media.get(\"images\", [])\nfor img in images:\n    if img.get(\"score\", 0) > 5:\n        print(\"High-value image:\", img[\"src\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring CosineStrategy with Core Parameters in Python\nDESCRIPTION: Shows the core configuration parameters for CosineStrategy, including content filtering, clustering parameters, and model configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a CSS Extraction Schema for Blog Posts\nDESCRIPTION: A JSON schema that extracts blog post information including URLs, titles, dates, summaries, and authors using CSS selectors. The schema defines base selectors for post cards and fields for individual elements.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_190\n\nLANGUAGE: python\nCODE:\n```\nschema = {\n  \"name\": \"Blog Posts\",\n  \"baseSelector\": \"a.blog-post-card\",\n  \"baseFields\": [\n    {\"name\": \"post_url\", \"type\": \"attribute\", \"attribute\": \"href\"}\n  ],\n  \"fields\": [\n    {\"name\": \"title\", \"selector\": \"h2.post-title\", \"type\": \"text\", \"default\": \"No Title\"},\n    {\"name\": \"date\", \"selector\": \"time.post-date\", \"type\": \"text\", \"default\": \"\"},\n    {\"name\": \"summary\", \"selector\": \"p.post-summary\", \"type\": \"text\", \"default\": \"\"},\n    {\"name\": \"author\", \"selector\": \"span.post-author\", \"type\": \"text\", \"default\": \"\"}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LXML-Based Scraping Strategy\nDESCRIPTION: Implements high-performance web scraping using LXML strategy with caching enabled. Offers up to 20x faster parsing compared to standard methods.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    scraping_strategy=LXMLWebScrapingStrategy(),\n    cache_mode=CacheMode.ENABLED\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Regex-Based Chunking in Python\nDESCRIPTION: This code snippet demonstrates a RegexChunking class that splits text based on regular expression patterns. It's useful for coarse segmentation of text into paragraphs or other defined sections.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_163\n\nLANGUAGE: python\nCODE:\n```\nclass RegexChunking:\n    def __init__(self, patterns=None):\n        self.patterns = patterns or [r'\\n\\n']  # Default pattern for paragraphs\n\n    def chunk(self, text):\n        paragraphs = [text]\n        for pattern in self.patterns:\n            paragraphs = [seg for p in paragraphs for seg in re.split(pattern, p)]\n        return paragraphs\n\n# Example Usage\ntext = \"\"\"This is the first paragraph.\n\nThis is the second paragraph.\"\"\"\nchunker = RegexChunking()\nprint(chunker.chunk(text))\n```\n\n----------------------------------------\n\nTITLE: Excluding All Images with CrawlerRunConfig (Python)\nDESCRIPTION: This snippet shows how to configure `CrawlerRunConfig` to completely exclude all images during the crawl process. Setting `exclude_all_images=True` removes images early in the pipeline, which significantly improves performance and reduces memory usage, especially for image-heavy pages or when only text content is required.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_all_images=True\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Content Chunking Example\nDESCRIPTION: Example of using OverlappingWindowChunking strategy with LLM extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.chunking_strategy import OverlappingWindowChunking\nfrom crawl4ai import LLMConfig\n\n# Create chunking strategy\nchunker = OverlappingWindowChunking(\n    window_size=500,  # 500 words per chunk\n    overlap=50        # 50 words overlap\n)\n\n# Use with extraction strategy\nstrategy = LLMExtractionStrategy(\n    llm_config = LLMConfig(provider=\"ollama/llama2\"),\n    chunking_strategy=chunker\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/long-article\",\n    extraction_strategy=strategy\n)\n```\n\n----------------------------------------\n\nTITLE: Crawler Configuration in YAML for Crawl4AI\nDESCRIPTION: Defines crawler behavior settings including cache mode, page timeout, content handling, and performance optimizations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# crawler.yml\ncache_mode: \"bypass\"\nwait_until: \"networkidle\"\npage_timeout: 30000\ndelay_before_return_html: 0.5\nword_count_threshold: 100\nscan_full_page: true\nscroll_delay: 0.3\nprocess_iframes: false\nremove_overlay_elements: true\nmagic: true\nverbose: true\n```\n\n----------------------------------------\n\nTITLE: Content Processing with Rate Limiting - Python Implementation\nDESCRIPTION: Processes webpage sections with rate limiting support, particularly for LLMExtractionStrategy. Implements both sequential processing for Groq provider and parallel processing using ThreadPoolExecutor for others. Includes error handling and sanitization.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_72\n\nLANGUAGE: python\nCODE:\n```\ndef run(self, url: str, sections: List[str]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Process sections sequentially with a delay for rate limiting issues, specifically for LLMExtractionStrategy.\n\n    Args:\n        url: The URL of the webpage.\n        sections: List of sections (strings) to process.\n\n    Returns:\n        A list of extracted blocks or chunks.\n    \"\"\"\n\n    merged_sections = self._merge(\n        sections,\n        self.chunk_token_threshold,\n        overlap=int(self.chunk_token_threshold * self.overlap_rate),\n    )\n    extracted_content = []\n    if self.llm_config.provider.startswith(\"groq/\"):\n        # Sequential processing with a delay\n        for ix, section in enumerate(merged_sections):\n            extract_func = partial(self.extract, url)\n            extracted_content.extend(\n                extract_func(ix, sanitize_input_encode(section))\n            )\n            time.sleep(0.5)  # 500 ms delay between each processing\n    else:\n        with ThreadPoolExecutor(max_workers=4) as executor:\n            extract_func = partial(self.extract, url)\n            futures = [\n                executor.submit(extract_func, ix, sanitize_input_encode(section))\n                for ix, section in enumerate(merged_sections)\n            ]\n\n            for future in as_completed(futures):\n                try:\n                    extracted_content.extend(future.result())\n                except Exception as e:\n                    if self.verbose:\n                        print(f\"Error in thread execution: {e}\")\n                    # Add error information to extracted_content\n                    extracted_content.append(\n                        {\n                            \"index\": 0,\n                            \"error\": True,\n                            \"tags\": [\"error\"],\n                            \"content\": str(e),\n                        }\n                    )\n\n    return extracted_content\n```\n\n----------------------------------------\n\nTITLE: Media Structure Example in Crawl4AI Results\nDESCRIPTION: Sample data structure showing how different media types (images, videos, audio, tables) are organized in the CrawlResult object with their respective metadata.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_94\n\nLANGUAGE: python\nCODE:\n```\nresult.media = {\n  \"images\": [\n    {\n      \"src\": \"https://cdn.prod.website-files.com/.../Group%2089.svg\",\n      \"alt\": \"coding school for kids\",\n      \"desc\": \"Trial Class Degrees degrees All Degrees AI Degree Technology ...\",\n      \"score\": 3,\n      \"type\": \"image\",\n      \"group_id\": 0,\n      \"format\": None,\n      \"width\": None,\n      \"height\": None\n    },\n    # ...\n  ],\n  \"videos\": [\n    # Similar structure but with video-specific fields\n  ],\n  \"audio\": [\n    # Similar structure but with audio-specific fields\n  ],\n  \"tables\": [\n    {\n      \"headers\": [\"Name\", \"Age\", \"Location\"],\n      \"rows\": [\n        [\"John Doe\", \"34\", \"New York\"],\n        [\"Jane Smith\", \"28\", \"San Francisco\"],\n        [\"Alex Johnson\", \"42\", \"Chicago\"]\n      ],\n      \"caption\": \"Employee Directory\",\n      \"summary\": \"Directory of company employees\"\n    },\n    # More tables if present\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory-Adaptive Dispatcher in Python\nDESCRIPTION: Sets up an intelligent resource management system with memory monitoring and auto-throttling capabilities. Configures concurrent session management and real-time monitoring of crawler operations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DisplayMode\nfrom crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher, CrawlerMonitor\n\nasync def main():\n    urls = [\"https://example1.com\", \"https://example2.com\"] * 50\n    \n    # Configure memory-aware dispatch\n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=80.0,  # Auto-throttle at 80% memory\n        check_interval=0.5,             # Check every 0.5 seconds\n        max_session_permit=20,          # Max concurrent sessions\n        monitor=CrawlerMonitor(         # Real-time monitoring\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        results = await dispatcher.run_urls(\n            urls=urls,\n            crawler=crawler,\n            config=CrawlerRunConfig()\n        )\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Crawl4AI\nDESCRIPTION: Commands for cloning the repository and building the Docker image for either AMD64 or ARM64 architecture.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_45\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngit clone https://github.com/unclecode/crawl4ai.git\ncd crawl4ai/deploy\n\n# Build the Docker image\ndocker build --platform=linux/amd64 --no-cache -t crawl4ai .\n\n# Or build for arm64\ndocker build --platform=linux/arm64 --no-cache -t crawl4ai .\n```\n\n----------------------------------------\n\nTITLE: Integrating Crawl4AI with Claude Code via MCP using Bash\nDESCRIPTION: This Bash command demonstrates how to add Crawl4AI as a tool to Claude Code using the Model Context Protocol (MCP). It uses the `claude mcp add` command, specifying Server-Sent Events (`--transport sse`) as the communication method, assigning a name (`c4ai-sse`), and providing the URL endpoint (`http://localhost:11235/mcp/sse`) where the Crawl4AI MCP service is listening.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n# Add Crawl4AI to Claude Code\nclaude mcp add --transport sse c4ai-sse http://localhost:11235/mcp/sse\n```\n\n----------------------------------------\n\nTITLE: Configuring CosineStrategy for Technical Documentation Extraction in Python\nDESCRIPTION: Shows optimal settings for extracting technical specifications. Uses moderate word count threshold, higher similarity threshold for stricter matching of technical content, and adjusted maximum distance to allow related technical sections.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_176\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```\n\n----------------------------------------\n\nTITLE: Pattern-Based Extraction using JsonCssExtractionStrategy in Crawl4AI\nDESCRIPTION: This snippet demonstrates how to use the JsonCssExtractionStrategy for structured data extraction. It defines a schema for news items and configures the crawler with content filtering and CSS selection options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    # Minimal schema for repeated items\n    schema = {\n        \"name\": \"News Items\",\n        \"baseSelector\": \"tr.athing\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"span.titleline a\", \"type\": \"text\"},\n            {\n                \"name\": \"link\", \n                \"selector\": \"span.titleline a\", \n                \"type\": \"attribute\", \n                \"attribute\": \"href\"\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        # Content filtering\n        excluded_tags=[\"form\", \"header\"],\n        exclude_domains=[\"adsite.com\"],\n        \n        # CSS selection or entire page\n        css_selector=\"table.itemlist\",\n\n        # No caching for demonstration\n        cache_mode=CacheMode.BYPASS,\n\n        # Extraction strategy\n        extraction_strategy=JsonCssExtractionStrategy(schema)\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com/newest\", \n            config=config\n        )\n        data = json.loads(result.extracted_content)\n        print(\"Sample extracted item:\", data[:1])  # Show first item\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Content Filtering with Pruning in Python\nDESCRIPTION: Implements pruning-based content filtering for removing unwanted content without specific search queries. Shows configuration of PruningContentFilter with customizable thresholds.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nprune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",  # or \"dynamic\"\n    min_word_threshold=50\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Data with JSON CSS Strategy in Crawl4AI\nDESCRIPTION: This snippet demonstrates how to use the JsonCssExtractionStrategy with raw HTML content in Crawl4AI. It creates a schema to extract titles and links from HTML elements with specific CSS selectors, and processes the HTML without making actual network requests.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    schema = {\n        \"name\": \"Example Items\",\n        \"baseSelector\": \"div.item\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n    raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"raw://\" + raw_html,\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                extraction_strategy=JsonCssExtractionStrategy(schema)\n            )\n        )\n        data = json.loads(result.extracted_content)\n        print(data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Crawling with Memory Adaptive Dispatcher in Python\nDESCRIPTION: Demonstrates batch processing implementation using AsyncWebCrawler with MemoryAdaptiveDispatcher for controlled concurrent crawling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl_batch():\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=False  # Default: get all results at once\n    )\n    \n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=10,\n        monitor=CrawlerMonitor(\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        # Get all results at once\n        results = await crawler.arun_many(\n            urls=urls,\n            config=run_config,\n            dispatcher=dispatcher\n        )\n        \n        # Process all results after completion\n        for result in results:\n            if result.success:\n                await process_result(result)\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring BFSDeepCrawlStrategy for Breadth-First Crawling in Python\nDESCRIPTION: Illustrates the initialization of `BFSDeepCrawlStrategy` for breadth-first web crawling. It shows key configuration parameters: `max_depth` (limits crawl levels), `include_external` (controls following off-domain links), `max_pages` (optional limit on total pages), and `score_threshold` (optional minimum score for a URL to be crawled). This strategy explores all links at the current depth before moving to the next level. Depends on `BFSDeepCrawlStrategy` from `crawl4ai.deep_crawling`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n\n# Basic configuration\nstrategy = BFSDeepCrawlStrategy(\n    max_depth=2,               # Crawl initial page + 2 levels deep\n    include_external=False,    # Stay within the same domain\n    max_pages=50,              # Maximum number of pages to crawl (optional)\n    score_threshold=0.3,       # Minimum score for URLs to be crawled (optional)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Network and Console Capture in Python with Crawl4AI\nDESCRIPTION: Shows how to enable network request and browser console message capturing in a Crawl4AI configuration. This setup is useful for debugging, security analysis, or understanding page behavior during crawling operations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_152\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\n# Enable both network request capture and console message capture\nconfig = CrawlerRunConfig(\n    capture_network_requests=True,  # Capture all network requests and responses\n    capture_console_messages=True   # Capture all browser console output\n)\n```\n\n----------------------------------------\n\nTITLE: HTML Source Selection for Markdown Generation in Python\nDESCRIPTION: Demonstrates different HTML source options for markdown generation, including raw HTML, cleaned HTML, and fit HTML variants. Shows how to configure the content source for different use cases.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Option 1: Use the raw HTML directly from the webpage (before any processing)\n    raw_md_generator = DefaultMarkdownGenerator(\n        content_source=\"raw_html\",\n        options={\"ignore_links\": True}\n    )\n    \n    # Option 2: Use the cleaned HTML (after scraping strategy processing - default)\n    cleaned_md_generator = DefaultMarkdownGenerator(\n        content_source=\"cleaned_html\",  # This is the default\n        options={\"ignore_links\": True}\n    )\n    \n    # Option 3: Use preprocessed HTML optimized for schema extraction\n    fit_md_generator = DefaultMarkdownGenerator(\n        content_source=\"fit_html\",\n        options={\"ignore_links\": True}\n    )\n    \n    # Use one of the generators in your crawler config\n    config = CrawlerRunConfig(\n        markdown_generator=raw_md_generator  # Try each of the generators\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        if result.success:\n            print(\"Markdown:\\n\", result.markdown.raw_markdown[:500])\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Default Server Configuration Structure in YAML\nDESCRIPTION: Provides a detailed example of the `config.yml` file structure used to configure the Crawl4AI server. It covers sections like `app` (basic info, host, port), `llm` (provider, API key), `redis` (connection details), `rate_limiting`, `security` (JWT, HTTPS, trusted hosts, headers), `crawler` (memory limits, delays, timeouts), `logging`, and `observability` (Prometheus, health checks). Comments explain specific settings, like the distinction between the `app.port` and the port used by Gunicorn. This file dictates the server's runtime behavior.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_33\n\nLANGUAGE: yaml\nCODE:\n```\n# Application Configuration\napp:\n  title: \"Crawl4AI API\"\n  version: \"1.0.0\" # Consider setting this to match library version, e.g., \"0.5.1\"\n  host: \"0.0.0.0\"\n  port: 8020 # NOTE: This port is used ONLY when running server.py directly. Gunicorn overrides this (see supervisord.conf).\n  reload: False # Default set to False - suitable for production\n  timeout_keep_alive: 300\n\n# Default LLM Configuration\nllm:\n  provider: \"openai/gpt-4o-mini\"\n  api_key_env: \"OPENAI_API_KEY\"\n  # api_key: sk-...  # If you pass the API key directly then api_key_env will be ignored\n\n# Redis Configuration (Used by internal Redis server managed by supervisord)\nredis:\n  host: \"localhost\"\n  port: 6379\n  db: 0\n  password: \"\"\n  # ... other redis options ...\n\n# Rate Limiting Configuration\nrate_limiting:\n  enabled: True\n  default_limit: \"1000/minute\"\n  trusted_proxies: []\n  storage_uri: \"memory://\"  # Use \"redis://localhost:6379\" if you need persistent/shared limits\n\n# Security Configuration\nsecurity:\n  enabled: false # Master toggle for security features\n  jwt_enabled: false # Enable JWT authentication (requires security.enabled=true)\n  https_redirect: false # Force HTTPS (requires security.enabled=true)\n  trusted_hosts: [\"*\"] # Allowed hosts (use specific domains in production)\n  headers: # Security headers (applied if security.enabled=true)\n    x_content_type_options: \"nosniff\"\n    x_frame_options: \"DENY\"\n    content_security_policy: \"default-src 'self'\"\n    strict_transport_security: \"max-age=63072000; includeSubDomains\"\n\n# Crawler Configuration\ncrawler:\n  memory_threshold_percent: 95.0\n  rate_limiter:\n    base_delay: [1.0, 2.0] # Min/max delay between requests in seconds for dispatcher\n  timeouts:\n    stream_init: 30.0  # Timeout for stream initialization\n    batch_process: 300.0 # Timeout for non-streaming /crawl processing\n\n# Logging Configuration\nlogging:\n  level: \"INFO\"\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\n# Observability Configuration\nobservability:\n  prometheus:\n    enabled: True\n    endpoint: \"/metrics\"\n  health_check:\n    endpoint: \"/health\"\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI and Dependencies\nDESCRIPTION: Initial setup commands to install Crawl4AI, Playwright, and nest_asyncio for compatibility with Colab's environment.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install Crawl4AI and dependencies\n!pip install crawl4ai\n!playwright install\n!pip install nest_asyncio\n```\n\n----------------------------------------\n\nTITLE: Capturing and Saving Screenshots with Crawl4AI\nDESCRIPTION: This function demonstrates how to capture screenshots of webpages and save them to a file. It configures the crawler to take a screenshot during crawling, then decodes the base64 screenshot data and writes it to the specified output file.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_135\n\nLANGUAGE: python\nCODE:\n```\nasync def capture_and_save_screenshot(url: str, output_path: str):\n    browser_config = BrowserConfig(headless=True)\n    crawler_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, screenshot=True)\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url=url, config=crawler_config)\n\n        if result.success and result.screenshot:\n            import base64\n\n            screenshot_data = base64.b64decode(result.screenshot)\n            with open(output_path, \"wb\") as f:\n                f.write(screenshot_data)\n            print(f\"Screenshot saved successfully to {output_path}\")\n        else:\n            print(\"Failed to capture screenshot\")\n```\n\n----------------------------------------\n\nTITLE: Requesting PDF Export via API\nDESCRIPTION: Illustrates the JSON payload for the `/pdf` endpoint. It requires the `url` of the target page and allows an optional `output_path` for saving the generated PDF document.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"url\": \"https://example.com\",\n  \"output_path\": \"/path/to/save/document.pdf\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Network and Console Capture Configuration to CrawlerRunConfig\nDESCRIPTION: Updates to the CrawlerRunConfig class to include new boolean flags for enabling network request and console message capturing. Modifications include adding new attributes, updating initialization parameters, and ensuring the flags are properly handled in serialization/deserialization methods.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/prompts/prompt_net_requests.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# ==== File: crawl4ai/async_configs.py ====\n# ... (imports) ...\n\nclass CrawlerRunConfig():\n    # ... (existing attributes) ...\n\n    # NEW: Network and Console Capturing Parameters\n    capture_network_requests: bool = False\n    capture_console_messages: bool = False\n\n    # Experimental Parameters\n    experimental: Dict[str, Any] = None,\n\n    def __init__(\n        self,\n        # ... (existing parameters) ...\n\n        # NEW: Network and Console Capturing Parameters\n        capture_network_requests: bool = False,\n        capture_console_messages: bool = False,\n\n        # Experimental Parameters\n        experimental: Dict[str, Any] = None,\n    ):\n        # ... (existing assignments) ...\n\n        # NEW: Assign new parameters\n        self.capture_network_requests = capture_network_requests\n        self.capture_console_messages = capture_console_messages\n\n        # Experimental Parameters\n        self.experimental = experimental or {}\n\n        # ... (rest of __init__) ...\n\n    @staticmethod\n    def from_kwargs(kwargs: dict) -> \"CrawlerRunConfig\":\n        return CrawlerRunConfig(\n            # ... (existing kwargs gets) ...\n\n            # NEW: Get new parameters\n            capture_network_requests=kwargs.get(\"capture_network_requests\", False),\n            capture_console_messages=kwargs.get(\"capture_console_messages\", False),\n\n            # Experimental Parameters\n            experimental=kwargs.get(\"experimental\"),\n        )\n\n    def to_dict(self):\n        return {\n            # ... (existing dict entries) ...\n\n            # NEW: Add new parameters to dict\n            \"capture_network_requests\": self.capture_network_requests,\n            \"capture_console_messages\": self.capture_console_messages,\n\n            \"experimental\": self.experimental,\n        }\n\n    # clone(), dump(), load() should work automatically if they rely on to_dict() and from_kwargs()\n    # or the serialization logic correctly handles all attributes.\n```\n\n----------------------------------------\n\nTITLE: Accessing Downloaded Files in Crawl4AI\nDESCRIPTION: Shows how to access and process downloaded files from the CrawlResult object.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif result.downloaded_files:\n    print(\"Downloaded files:\")\n    for file_path in result.downloaded_files:\n        print(f\"- {file_path}\")\n        file_size = os.path.getsize(file_path)\n        print(f\"- File size: {file_size} bytes\")\nelse:\n    print(\"No files downloaded.\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory Adaptive Dispatcher in Python\nDESCRIPTION: Configures a MemoryAdaptiveDispatcher that manages concurrency based on system memory usage with integrated rate limiting and monitoring.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher\n\ndispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=90.0,  # Pause if memory exceeds this\n    check_interval=1.0,             # How often to check memory\n    max_session_permit=10,          # Maximum concurrent tasks\n    rate_limiter=RateLimiter(       # Optional rate limiting\n        base_delay=(1.0, 2.0),\n        max_delay=30.0,\n        max_retries=2\n    ),\n    monitor=CrawlerMonitor(         # Optional monitoring\n        max_visible_rows=15,\n        display_mode=DisplayMode.DETAILED\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Proxy Configuration in Python using BrowserConfig\nDESCRIPTION: Demonstrates basic proxy setup using BrowserConfig class with both HTTP and SOCKS proxy examples.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_158\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import BrowserConfig\n\n# Using proxy URL\nbrowser_config = BrowserConfig(proxy=\"http://proxy.example.com:8080\")\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nbrowser_config = BrowserConfig(proxy=\"socks5://proxy.example.com:1080\")\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```\n\n----------------------------------------\n\nTITLE: Combined Filtering Configuration Example\nDESCRIPTION: Example showing how to combine multiple filtering options including word count threshold, tag exclusions, and content filtering in a single configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    word_count_threshold=10,\n    excluded_tags=[\"nav\", \"footer\", \"header\"],\n    exclude_external_links=True,\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=PruningContentFilter(threshold=0.5)\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Content Relevance Filter Implementation\nDESCRIPTION: Implementation of content-based filtering using semantic similarity analysis between query and page content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling.filters import FilterChain, ContentRelevanceFilter\n\n# Create a content relevance filter\nrelevance_filter = ContentRelevanceFilter(\n    query=\"Web crawling and data extraction with Python\",\n    threshold=0.7  # Minimum similarity score (0.0 to 1.0)\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([relevance_filter])\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Building Multi-Platform Docker Image with Build Arguments\nDESCRIPTION: Demonstrates using `docker buildx build` to create a multi-platform (linux/amd64, linux/arm64) Docker image for crawl4ai. It passes the `--build-arg INSTALL_TYPE=all` to include all optional features during the build process and tags the resulting image.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\n# Example: Build with 'all' features using buildx\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64 \\\n  --build-arg INSTALL_TYPE=all \\\n  -t yourname/crawl4ai-all:latest \\\n  --load \\\n  . # Build from root context\n```\n\n----------------------------------------\n\nTITLE: Excluding Specific Domains in Crawl4AI\nDESCRIPTION: Configuration to exclude specific domains from crawling while still allowing other external links, useful for blocking spammy or unwanted sites.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_92\n\nLANGUAGE: python\nCODE:\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_domains=[\"suspiciousads.com\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Triggering File Downloads in Crawl4AI\nDESCRIPTION: Illustrates how to trigger downloads using JavaScript code injection and wait timing configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nconfig = CrawlerRunConfig(\n    js_code=\"\"\"\n        const downloadLink = document.querySelector('a[href$=\".exe\"]');\n        if (downloadLink) {\n            downloadLink.click();\n        }\n    \"\"\",\n    wait_for=5  # Wait 5 seconds for the download to start\n)\n\nresult = await crawler.arun(url=\"https://www.python.org/downloads/\", config=config)\n```\n\n----------------------------------------\n\nTITLE: Requesting JavaScript Execution via API\nDESCRIPTION: Shows the JSON payload for the `/execute_js` endpoint. It requires the `url` of the target page and a `scripts` array containing strings of JavaScript code to be executed sequentially in the page's context.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"url\": \"https://example.com\",\n  \"scripts\": [\n    \"return document.title\",\n    \"return Array.from(document.querySelectorAll('a')).map(a => a.href)\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Building Multi-Architecture Crawl4AI Docker Image using Buildx (Bash)\nDESCRIPTION: Shows how to use `docker buildx build` to create multi-architecture images (`linux/amd64` and `linux/arm64`) simultaneously. This is useful for publishing images that work across different platforms.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# Or build for multiple architectures (useful for publishing)\ndocker buildx build --platform linux/amd64,linux/arm64 -t crawl4ai-local:latest --load .\n```\n\n----------------------------------------\n\nTITLE: Error Handling with CosineStrategy and AsyncWebCrawler in Python\nDESCRIPTION: Demonstrates proper error handling when using CosineStrategy with AsyncWebCrawler. The example includes checking for successful extraction, handling empty content, and catching exceptions during the extraction process.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_181\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\n----------------------------------------\n\nTITLE: Integrating Crawl4AI Stress Tests in CI/CD Pipeline\nDESCRIPTION: This Bash script shows how to integrate Crawl4AI stress tests into a CI/CD pipeline. It runs a benchmark test, checks the exit code, and optionally runs a report generator and metric checker.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/tests/memory/README.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Example CI script\npython run_benchmark.py medium --no-report # Run test without interactive report gen\n# Check exit code\nif [ $? -ne 0 ]; then echo \"Stress test failed!\"; exit 1; fi\n# Optionally, run report generator and check its output/metrics\n# python benchmark_report.py\n# check_report_metrics.py reports/test_summary_*.json || exit 1\nexit 0\n```\n\n----------------------------------------\n\nTITLE: Setting Up Browser Context with Customized Options in Playwright\nDESCRIPTION: Asynchronous method to configure a browser context with various options like headers, cookies, storage state, and download settings. It supports customization for user agents, browser hints, and navigator overrides for simulating user behavior.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_124\n\nLANGUAGE: python\nCODE:\n```\nasync def setup_context(\n    self,\n    context: BrowserContext,\n    crawlerRunConfig: CrawlerRunConfig = None,\n    is_default=False,\n):\n    \"\"\"\n    Set up a browser context with the configured options.\n\n    How it works:\n    1. Set extra HTTP headers if provided.\n    2. Add cookies if provided.\n    3. Load storage state if provided.\n    4. Accept downloads if enabled.\n    5. Set default timeouts for navigation and download.\n    6. Set user agent if provided.\n    7. Set browser hints if provided.\n    8. Set proxy if provided.\n    9. Set downloads path if provided.\n    10. Set storage state if provided.\n    11. Set cache if provided.\n    12. Set extra HTTP headers if provided.\n    13. Add cookies if provided.\n    14. Set default timeouts for navigation and download if enabled.\n    15. Set user agent if provided.\n    16. Set browser hints if provided.\n\n    Args:\n        context (BrowserContext): The browser context to set up\n        crawlerRunConfig (CrawlerRunConfig): Configuration object containing all browser settings\n        is_default (bool): Flag indicating if this is the default context\n    Returns:\n        None\n    \"\"\"\n    if self.config.headers:\n        await context.set_extra_http_headers(self.config.headers)\n\n    if self.config.cookies:\n        await context.add_cookies(self.config.cookies)\n\n    if self.config.storage_state:\n        await context.storage_state(path=None)\n\n    if self.config.accept_downloads:\n        context.set_default_timeout(DOWNLOAD_PAGE_TIMEOUT)\n        context.set_default_navigation_timeout(DOWNLOAD_PAGE_TIMEOUT)\n        if self.config.downloads_path:\n            context._impl_obj._options[\"accept_downloads\"] = True\n            context._impl_obj._options[\n                \"downloads_path\"\n            ] = self.config.downloads_path\n\n    # Handle user agent and browser hints\n    if self.config.user_agent:\n        combined_headers = {\n            \"User-Agent\": self.config.user_agent,\n            \"sec-ch-ua\": self.config.browser_hint,\n        }\n        combined_headers.update(self.config.headers)\n        await context.set_extra_http_headers(combined_headers)\n\n    # Add default cookie\n    await context.add_cookies(\n        [\n            {\n                \"name\": \"cookiesEnabled\",\n                \"value\": \"true\",\n                \"url\": crawlerRunConfig.url\n                if crawlerRunConfig and crawlerRunConfig.url\n                else \"https://crawl4ai.com/\",\n            }\n        ]\n    )\n\n    # Handle navigator overrides\n    if crawlerRunConfig:\n        if (\n            crawlerRunConfig.override_navigator\n            or crawlerRunConfig.simulate_user\n            or crawlerRunConfig.magic\n        ):\n            await context.add_init_script(load_js_script(\"navigator_overrider\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Browser Support\nDESCRIPTION: Example showing how to use different browser engines (Firefox) for web crawling operations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync def multi_browser_demo():\n    async with AsyncWebCrawler(browser_type=\"firefox\") as crawler:  # Using Firefox instead of default Chromium\n        result = await crawler.arun(url=\"https://crawl4i.com\")\n        print(result.markdown)  # Shows content extracted using Firefox\n\n# Run the demo\nawait multi_browser_demo()\n```\n\n----------------------------------------\n\nTITLE: Implementing BM25ContentFilter in Crawl4AI\nDESCRIPTION: Example demonstrating the setup and usage of BM25ContentFilter for query-based content filtering. Shows how to configure the filter with user queries and threshold settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # 1) A BM25 filter with a user query\n    bm25_filter = BM25ContentFilter(\n        user_query=\"startup fundraising tips\",\n        # Adjust for stricter or looser results\n        bm25_threshold=1.2  \n    )\n\n    # 2) Insert into a Markdown Generator\n    md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\n    \n    # 3) Pass to crawler config\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\", \n            config=config\n        )\n        if result.success:\n            print(\"Fit Markdown (BM25 query-based):\")\n            print(result.markdown.fit_markdown)\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Excluding External and Social Media Links in Crawl4AI\nDESCRIPTION: Complete example demonstrating how to configure a crawler to exclude external links and social media platforms, with output reporting on the resulting link counts.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_91\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    crawler_cfg = CrawlerRunConfig(\n        exclude_external_links=True,          # No links outside primary domain\n        exclude_social_media_links=True       # Skip recognized social media domains\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            \"https://www.example.com\",\n            config=crawler_cfg\n        )\n        if result.success:\n            print(\"[OK] Crawled:\", result.url)\n            print(\"Internal links count:\", len(result.links.get(\"internal\", [])))\n            print(\"External links count:\", len(result.links.get(\"external\", [])))  \n            # Likely zero external links in this scenario\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Defining the Screenshot API Endpoint\nDESCRIPTION: Specifies the HTTP method (POST) and path (`/screenshot`) for the endpoint used to capture a full-page PNG screenshot of a given URL. This is one of the specialized utility endpoints.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_21\n\nLANGUAGE: plaintext\nCODE:\n```\nPOST /screenshot\n```\n\n----------------------------------------\n\nTITLE: Advanced Schema with Nested Structures for E-commerce Data Extraction\nDESCRIPTION: Shows how to define a complex schema with nested and list-type fields for extracting hierarchical data from e-commerce websites. The schema captures categories, products, features, reviews, and related items using a structured approach.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_187\n\nLANGUAGE: python\nCODE:\n```\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    # (1) We can define optional baseFields if we want to extract attributes \n    # from the category container\n    \"baseFields\": [\n        {\"name\": \"data_cat_id\", \"type\": \"attribute\", \"attribute\": \"data-cat-id\"}, \n    ],\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",    # repeated sub-objects\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",  # single sub-object\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\"name\": \"feature\", \"type\": \"text\"} \n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\", \n                            \"selector\": \"span.reviewer\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\", \n                            \"selector\": \"span.rating\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\", \n                            \"selector\": \"p.review-text\", \n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\", \n                            \"selector\": \"span.related-name\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\", \n                            \"selector\": \"span.related-price\", \n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: LLM Schema JSON for Structured Data Extraction in Crawl4AI\nDESCRIPTION: Defines a JSON schema for LLM-based extraction of article information, specifying the structure and properties to be extracted.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n// llm_schema.json\n{\n  \"title\": \"Article\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"title\": {\n      \"type\": \"string\",\n      \"description\": \"The title of the article\"\n    },\n    \"link\": {\n      \"type\": \"string\",\n      \"description\": \"URL to the full article\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Download Settings in Crawl4AI\nDESCRIPTION: Demonstrates how to enable downloads globally by configuring the BrowserConfig object with accept_downloads parameter.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import BrowserConfig, AsyncWebCrawler\n\nasync def main():\n    config = BrowserConfig(accept_downloads=True)  # Enable downloads globally\n    async with AsyncWebCrawler(config=config) as crawler:\n        # ... your crawling logic ...\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Defining CLI Command with Click for Crawl4AI\nDESCRIPTION: Definition of the 'default' CLI command for Crawl4AI using Click library. The command takes various parameters for configuring web crawling and content extraction, with options for browser profiles, configurations, and output format.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_55\n\nLANGUAGE: python\nCODE:\n```\n@click.option(\"--profile\", \"-p\", help=\"Use a specific browser profile (by name)\")\ndef default(url: str, example: bool, browser_config: str, crawler_config: str, filter_config: str, \n        extraction_config: str, json_extract: str, schema: str, browser: Dict, crawler: Dict,\n        output: str, bypass_cache: bool, question: str, verbose: bool, profile: str):\n    \"\"\"Crawl4AI CLI - Web content extraction tool\n\n    Simple Usage:\n        crwl https://example.com\n    \n    Run with --example to see detailed usage examples.\n    \n    Other commands:\n        crwl profiles   - Manage browser profiles for identity-based crawling\n        crwl crawl      - Crawl a website with advanced options\n        crwl cdp        - Launch browser with CDP debugging enabled\n        crwl browser    - Manage builtin browser (start, stop, status, restart)\n        crwl config     - Manage global configuration settings\n        crwl examples   - Show more usage examples\n        \n    Configuration Examples:\n        crwl config list                         - List all configuration settings\n        crwl config get DEFAULT_LLM_PROVIDER     - Show current LLM provider\n        crwl config set VERBOSE true             - Enable verbose mode globally\n        crwl config set BROWSER_HEADLESS false   - Default to visible browser\n    \"\"\"\n\n    if example:\n        show_examples()\n        return\n        \n    if not url:\n        # Show help without error message\n        ctx = click.get_current_context()\n        click.echo(ctx.get_help())\n        return\n        \n    # Forward to crawl command\n    ctx = click.get_current_context()\n    ctx.invoke(\n        crawl_cmd, \n        url=url, \n        browser_config=browser_config,\n        crawler_config=crawler_config,\n        filter_config=filter_config,\n        extraction_config=extraction_config,\n        json_extract=json_extract,\n        schema=schema,\n        browser=browser,\n        crawler=crawler,\n        output=output,\n        bypass_cache=bypass_cache,\n        question=question,\n        verbose=verbose,\n        profile=profile\n    )\n```\n\n----------------------------------------\n\nTITLE: CrawlerRunConfig Class Definition in Python\nDESCRIPTION: Python class definition for CrawlerRunConfig which controls crawl operation parameters. Includes settings for content extraction, caching, JavaScript execution, media capture, and system resource management during crawls.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass CrawlerRunConfig:\n    def __init__(\n        word_count_threshold=200,\n        extraction_strategy=None,\n        markdown_generator=None,\n        cache_mode=None,\n        js_code=None,\n        wait_for=None,\n        screenshot=False,\n        pdf=False,\n        capture_mhtml=False,\n        enable_rate_limiting=False,\n        rate_limit_config=None,\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=20,\n        display_mode=None,\n        verbose=True,\n        stream=False,  # Enable streaming for arun_many()\n        # ... other advanced parameters omitted\n    ):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Building Multi-Platform Docker Images for Crawl4AI\nDESCRIPTION: Commands for setting up Docker buildx and building images for multiple platforms (AMD64 and ARM64) simultaneously.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_48\n\nLANGUAGE: bash\nCODE:\n```\n# Set up buildx builder\ndocker buildx create --use\n\n# Build for multiple platforms\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64 \\\n  -t crawl4ai \\\n  --push \\\n  .\n```\n\n----------------------------------------\n\nTITLE: Implementing Fit Markdown Feature\nDESCRIPTION: Demonstration of the Fit Markdown feature that extracts main content from articles while removing unnecessary elements.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def fit_markdown_demo():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://janineintheworld.com/places-to-visit-in-central-mexico\")\n        print(result.fit_markdown)  # Shows main content in Markdown format\n\n# Run the demo\nawait fit_markdown_demo()\n```\n\n----------------------------------------\n\nTITLE: Setting Up BM25 Content Filter for Query-Based Extraction\nDESCRIPTION: Code pattern for implementing the BM25ContentFilter with a specific user query to extract content related to a topic.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_75\n\nLANGUAGE: python\nCODE:\n```\nbm25_filter = BM25ContentFilter(\n    user_query=\"health benefits fruit\",\n    bm25_threshold=1.2\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\n```\n\n----------------------------------------\n\nTITLE: Handling Media Content in Crawl4AI\nDESCRIPTION: This function shows how to handle and extract image data from a webpage. It configures the crawler to exclude external images, capture screenshots, and then displays information about the first five images found on the page, including their URLs, alt text, and scores.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_132\n\nLANGUAGE: python\nCODE:\n```\nasync def media_handling():\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS, exclude_external_images=True, screenshot=True\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\", config=crawler_config\n        )\n        for img in result.media[\"images\"][:5]:\n            print(f\"Image URL: {img['src']}, Alt: {img['alt']}, Score: {img['score']}\")\n```\n\n----------------------------------------\n\nTITLE: Processing Crawler4ai Results\nDESCRIPTION: Example demonstrating how to process and access the results obtained from a crawling operation, including how to iterate through pages and access their content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# After crawling\nfor page in results.pages:\n    print(f\"URL: {page.url}\")\n    print(f\"Title: {page.title}\")\n    print(f\"Content Length: {len(page.text)}\")\n    \n    # Access extracted links\n    for link in page.links:\n        print(f\"  - {link}\")\n    \n    # Access metadata\n    for key, value in page.metadata.items():\n        print(f\"  {key}: {value}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON CSS Extraction Strategy via API\nDESCRIPTION: Provides a JSON example demonstrating how to configure an extraction strategy directly via the API. It shows nesting a `JsonCssExtractionStrategy` within the `crawler_config`. Note the required `{\"type\": \"ClassName\", \"params\": {...}}` structure for complex objects and the `{\"type\": \"dict\", \"value\": {...}}` wrapper for the dictionary-based schema.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"crawler_config\": {\n        \"type\": \"CrawlerRunConfig\",\n        \"params\": {\n            \"extraction_strategy\": {\n                \"type\": \"JsonCssExtractionStrategy\",\n                \"params\": {\n                    \"schema\": {\n                        \"type\": \"dict\",\n                        \"value\": {\n                           \"baseSelector\": \"article.post\",\n                           \"fields\": [\n                               {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n                               {\"name\": \"content\", \"selector\": \".content\", \"type\": \"html\"}\n                           ]\n                         }\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: URL Pattern Filtering in Web Crawler\nDESCRIPTION: Implementation of basic URL pattern filtering using FilterChain to narrow down crawled pages based on URL patterns.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter\n\n# Only follow URLs containing \"blog\" or \"docs\"\nurl_filter = URLPatternFilter(patterns=[\"*blog*\", \"*docs*\"])\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([url_filter])\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Version Requirements\nDESCRIPTION: Specifies the minimum required versions of essential Python packages for data analysis, visualization, and console output formatting. The file uses the standard pip requirements format with the >= operator to indicate minimum version constraints.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/tests/memory/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npandas>=1.5.0\nmatplotlib>=3.5.0\nseaborn>=0.12.0\nrich>=12.0.0\n```\n\n----------------------------------------\n\nTITLE: Defining DispatchResult Class in Python\nDESCRIPTION: Defines a dataclass for storing crawl dispatch results including task ID, memory usage, and timing information.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass DispatchResult:\n    task_id: str\n    memory_usage: float\n    peak_memory: float\n    start_time: datetime\n    end_time: datetime\n    error_message: str = \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Network and Console Capture in Crawl4AI (Python)\nDESCRIPTION: This snippet demonstrates how to enable network request and console message capturing in Crawl4AI using the CrawlerRunConfig.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/network-console-capture.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\n# Enable both network request capture and console message capture\nconfig = CrawlerRunConfig(\n    capture_network_requests=True,  # Capture all network requests and responses\n    capture_console_messages=True   # Capture all browser console output\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Basic Crawl Information in Python\nDESCRIPTION: Demonstrates how to access basic crawl information like URL, success status, HTTP status code, and error messages from a CrawlResult object.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(result.url)  # e.g., \"https://example.com/\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Crawl4AI Server with YAML\nDESCRIPTION: Provides a detailed YAML configuration for the Crawl4AI server, including application settings, rate limiting, security options, crawler parameters, and logging configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_64\n\nLANGUAGE: yaml\nCODE:\n```\n# Application Configuration\napp:\n  title: \"Crawl4AI API\"           # Server title in OpenAPI docs\n  version: \"1.0.0\"               # API version\n  host: \"0.0.0.0\"               # Listen on all interfaces\n  port: 8000                    # Server port\n  reload: True                  # Enable hot reloading (development only)\n  timeout_keep_alive: 300       # Keep-alive timeout in seconds\n\n# Rate Limiting Configuration\nrate_limiting:\n  enabled: True                 # Enable/disable rate limiting\n  default_limit: \"100/minute\"   # Rate limit format: \"number/timeunit\"\n  trusted_proxies: []          # List of trusted proxy IPs\n  storage_uri: \"memory://\"     # Use \"redis://localhost:6379\" for production\n\n# Security Configuration\nsecurity:\n  enabled: false               # Master toggle for security features\n  jwt_enabled: true            # Enable JWT authentication\n  https_redirect: True         # Force HTTPS\n  trusted_hosts: [\"*\"]         # Allowed hosts (use specific domains in production)\n  headers:                     # Security headers\n    x_content_type_options: \"nosniff\"\n    x_frame_options: \"DENY\"\n    content_security_policy: \"default-src 'self'\"\n    strict_transport_security: \"max-age=63072000; includeSubDomains\"\n\n# Crawler Configuration\ncrawler:\n  memory_threshold_percent: 95.0  # Memory usage threshold\n  rate_limiter:\n    base_delay: [1.0, 2.0]      # Min and max delay between requests\n  timeouts:\n    stream_init: 30.0           # Stream initialization timeout\n    batch_process: 300.0        # Batch processing timeout\n\n# Logging Configuration\nlogging:\n  level: \"INFO\"                 # Log level (DEBUG, INFO, WARNING, ERROR)\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n```\n\n----------------------------------------\n\nTITLE: Working with Markdown Generation Results in Python\nDESCRIPTION: Demonstrates how to access various markdown formats generated from HTML, including raw markdown, markdown with citations, and references.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nif result.markdown:\n    md_res = result.markdown\n    print(\"Raw MD:\", md_res.raw_markdown[:300])\n    print(\"Citations MD:\", md_res.markdown_with_citations[:300])\n    print(\"References:\", md_res.references_markdown)\n    if md_res.fit_markdown:\n        print(\"Pruned text:\", md_res.fit_markdown[:300])\n```\n\n----------------------------------------\n\nTITLE: Implementing Builtin Browser in Python with Crawl4AI\nDESCRIPTION: This snippet demonstrates how to configure and use the builtin browser feature in Python code. It shows how to create a browser configuration with the builtin mode and use it with AsyncWebCrawler without explicitly starting or closing the browser.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\n# Create browser config with builtin mode\nbrowser_config = BrowserConfig(\n    browser_mode=\"builtin\",  # This is the key setting!\n    headless=True            # Can be headless or not\n)\n\n# Create the crawler\ncrawler = AsyncWebCrawler(config=browser_config)\n\n# Use it - no need to explicitly start()\nresult = await crawler.arun(\"https://example.com\")\n```\n\n----------------------------------------\n\nTITLE: Basic Web Crawling with Markdown Generation in Python\nDESCRIPTION: This function demonstrates basic web crawling using AsyncWebCrawler. It crawls a single URL (Hacker News) and generates markdown content from the crawled page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_147\n\nLANGUAGE: python\nCODE:\n```\nasync def demo_basic_crawl():\n    \"\"\"Basic web crawling with markdown generation\"\"\"\n    print(\"\\n=== 1. Basic Web Crawling ===\")\n    async with AsyncWebCrawler(config = BrowserConfig(\n        viewport_height=800,\n        viewport_width=1200,\n        headless=True,\n        verbose=True,\n    )) as crawler:\n        results: List[CrawlResult] = await crawler.arun(\n            url=\"https://news.ycombinator.com/\"\n        )\n\n        for i, result in enumerate(results):\n            print(f\"Result {i + 1}:\")\n            print(f\"Success: {result.success}\")\n            if result.success:\n                print(f\"Markdown length: {len(result.markdown.raw_markdown)} chars\")\n                print(f\"First 100 chars: {result.markdown.raw_markdown[:100]}...\")\n            else:\n                print(\"Failed to crawl the URL\")\n```\n\n----------------------------------------\n\nTITLE: Using Proxy Configurations with Crawl4AI\nDESCRIPTION: This function shows how to configure and use a proxy with the Crawl4AI crawler. It sets up a browser configuration with proxy settings including server URL, username, and password for authenticated proxy access.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_134\n\nLANGUAGE: python\nCODE:\n```\nasync def use_proxy():\n    print(\"\\n--- Using a Proxy ---\")\n    browser_config = BrowserConfig(\n        headless=True,\n        proxy_config={\n            \"server\": \"http://proxy.example.com:8080\",\n            \"username\": \"username\",\n            \"password\": \"password\",\n        },\n    )\n    crawler_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\", config=crawler_config\n        )\n        if result.success:\n            print(result.markdown[:500])\n```\n\n----------------------------------------\n\nTITLE: Structured Data Extraction with CSS Selectors Command for Crawl4AI\nDESCRIPTION: Shows how to extract structured data using CSS selectors with configuration and schema files.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncrwl https://example.com \\\n    -e extract_css.yml \\\n    -s css_schema.json \\\n    -o json\n```\n\n----------------------------------------\n\nTITLE: Finding Playwright Binary Location\nDESCRIPTION: Commands to locate the Playwright Chromium binary installation on your system.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_136\n\nLANGUAGE: bash\nCODE:\n```\npython -m playwright install --dry-run\n```\n\nLANGUAGE: bash\nCODE:\n```\nplaywright install --dry-run\n```\n\n----------------------------------------\n\nTITLE: Working with Cleaned HTML in Python\nDESCRIPTION: Shows how to access the sanitized HTML version of a crawled page, where scripts, styles, or excluded tags have been removed based on the crawler configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(result.cleaned_html[:500])  # Show a snippet\n```\n\n----------------------------------------\n\nTITLE: Timing Control Configuration for Web Crawling in Python\nDESCRIPTION: This example shows how to configure timing parameters for web crawling using CrawlerRunConfig. It sets a page timeout limit and adds a delay before capturing the final HTML content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_118\n\nLANGUAGE: python\nCODE:\n```\nconfig = CrawlerRunConfig(\n    page_timeout=60000,  # 60s limit\n    delay_before_return_html=2.5\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced Usage Example for Crawl4AI CLI\nDESCRIPTION: Shows how to use the Crawl4AI CLI with a JSON-CSS schema for structured data extraction from a specific webpage.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncrwl \"https://www.infoq.com/ai-ml-data-eng/\" -e docs/examples/cli/extract_css.yml -s docs/examples/cli/css_schema.json -o json;\n```\n\n----------------------------------------\n\nTITLE: Crawler Configuration for Crawl4AI CLI\nDESCRIPTION: Shows how to control crawling behavior using a YAML configuration file for the Crawl4AI CLI.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n# crawler.yml\ncache_mode: \"bypass\"\nwait_until: \"networkidle\"\npage_timeout: 30000\ndelay_before_return_html: 0.5\nword_count_threshold: 100\nscan_full_page: true\nscroll_delay: 0.3\nprocess_iframes: false\nremove_overlay_elements: true\nmagic: true\nverbose: true\n```\n\n----------------------------------------\n\nTITLE: Markdown Generation with Citations in Crawl4AI\nDESCRIPTION: Example of configuring the DefaultMarkdownGenerator to generate markdown with citations from an HTML page. The result contains raw markdown, markdown with citations, and references.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nconfig = CrawlerRunConfig(\n    markdown_generator=DefaultMarkdownGenerator(\n        options={\"citations\": True, \"body_width\": 80}  # e.g. pass html2text style options\n    )\n)\nresult = await crawler.arun(url=\"https://example.com\", config=config)\n\nmd_res = result.markdown  # or eventually 'result.markdown'\nprint(md_res.raw_markdown[:500])\nprint(md_res.markdown_with_citations)\nprint(md_res.references_markdown)\n```\n\n----------------------------------------\n\nTITLE: Link Analysis with Crawl4AI\nDESCRIPTION: This function demonstrates link analysis capabilities of Crawl4AI. It configures the crawler to exclude external and social media links, then crawls a news website and analyzes the internal and external links found on the page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_129\n\nLANGUAGE: python\nCODE:\n```\nasync def link_analysis():\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.ENABLED,\n        exclude_external_links=True,\n        exclude_social_media_links=True,\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            config=crawler_config,\n        )\n        print(f\"Found {len(result.links['internal'])} internal links\")\n        print(f\"Found {len(result.links['external'])} external links\")\n\n        for link in result.links[\"internal\"][:5]:\n            print(f\"Href: {link['href']}\\nText: {link['text']}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Deep Crawling Decorator for Web Crawlers in Python\nDESCRIPTION: This class is a decorator that adds deep crawling capabilities to a web crawler's arun method. It uses a context variable to prevent recursive calls and delegates to the provided deep crawl strategy when needed. It handles both streaming and non-streaming modes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_164\n\nLANGUAGE: python\nCODE:\n```\nclass DeepCrawlDecorator:\n    \"\"\"Decorator that adds deep crawling capability to arun method.\"\"\"\n    deep_crawl_active = ContextVar(\"deep_crawl_active\", default=False)\n    \n    def __init__(self, crawler: AsyncWebCrawler): \n        self.crawler = crawler\n\n    def __call__(self, original_arun):\n        @wraps(original_arun)\n        async def wrapped_arun(url: str, config: CrawlerRunConfig = None, **kwargs):\n            # If deep crawling is already active, call the original method to avoid recursion.\n            if config and config.deep_crawl_strategy and not self.deep_crawl_active.get():\n                token = self.deep_crawl_active.set(True)\n                # Await the arun call to get the actual result object.\n                result_obj = await config.deep_crawl_strategy.arun(\n                    crawler=self.crawler,\n                    start_url=url,\n                    config=config\n                )\n                if config.stream:\n                    async def result_wrapper():\n                        try:\n                            async for result in result_obj:\n                                yield result\n                        finally:\n                            self.deep_crawl_active.reset(token)\n                    return result_wrapper()\n                else:\n                    try:\n                        return result_obj\n                    finally:\n                        self.deep_crawl_active.reset(token)\n            return await original_arun(url, config=config, **kwargs)\n        return wrapped_arun\n```\n\n----------------------------------------\n\nTITLE: Accessing HTTP Response Headers in Python\nDESCRIPTION: Demonstrates how to access and use HTTP response headers from a crawl operation, such as retrieving the server information.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif result.response_headers:\n    print(\"Server:\", result.response_headers.get(\"Server\", \"Unknown\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Proxy Setup in Crawl4AI\nDESCRIPTION: Demonstrates how to configure both HTTP and SOCKS proxies using BrowserConfig. Shows basic proxy URL configuration for AsyncWebCrawler initialization.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/proxy-security.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import BrowserConfig\n\n# Using proxy URL\nbrowser_config = BrowserConfig(proxy=\"http://proxy.example.com:8080\")\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nbrowser_config = BrowserConfig(proxy=\"socks5://proxy.example.com:1080\")\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n```\n\n----------------------------------------\n\nTITLE: Implementing SEOFilter Class in Python\nDESCRIPTION: This class provides a quantitative SEO quality assessment filter using head section analysis. It includes methods for scoring various SEO factors such as title length, keyword presence, and meta tags.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_191\n\nLANGUAGE: python\nCODE:\n```\nclass SEOFilter(URLFilter):\n    \"\"\"Quantitative SEO quality assessment filter using head section analysis\"\"\"\n\n    __slots__ = (\"threshold\", \"_weights\", \"_kw_patterns\")\n\n    # Based on SEMrush/Google ranking factors research\n    DEFAULT_WEIGHTS = {\n        \"title_length\": 0.15,\n        \"title_kw\": 0.18,\n        \"meta_description\": 0.12,\n        \"canonical\": 0.10,\n        \"robot_ok\": 0.20,  # Most critical factor\n        \"schema_org\": 0.10,\n        \"url_quality\": 0.15,\n    }\n\n    def __init__(\n        self,\n        threshold: float = 0.65,\n        keywords: List[str] = None,\n        weights: Dict[str, float] = None,\n    ):\n        super().__init__(name=\"SEOFilter\")\n        self.threshold = threshold\n        self._weights = weights or self.DEFAULT_WEIGHTS\n        self._kw_patterns = (\n            re.compile(\n                r\"\\b({})\\b\".format(\"|\".join(map(re.escape, keywords or []))), re.I\n            )\n            if keywords\n            else None\n        )\n\n    async def apply(self, url: str) -> bool:\n        head_content = await HeadPeekr.peek_html(url)\n        if not head_content:\n            self._update_stats(False)\n            return False\n\n        meta = HeadPeekr.extract_meta_tags(head_content)\n        title = HeadPeekr.get_title(head_content) or \"\"\n        parsed_url = urlparse(url)\n\n        scores = {\n            \"title_length\": self._score_title_length(title),\n            \"title_kw\": self._score_keyword_presence(title),\n```\n\n----------------------------------------\n\nTITLE: Configuring LLMContentFilter for Focused Content Extraction\nDESCRIPTION: This code example shows how to configure LLMContentFilter with instructions for extracting specific types of content from a webpage, such as technical documentation and code examples.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_110\n\nLANGUAGE: python\nCODE:\n```\nfilter = LLMContentFilter(\n    instruction=\"\"\"\n    Focus on extracting specific types of content:\n    - Technical documentation\n    - Code examples\n    - API references\n    Reformat the content into clear, well-structured markdown\n    \"\"\",\n    chunk_token_threshold=4096\n)\n```\n\n----------------------------------------\n\nTITLE: Browser Configuration in YAML for Crawl4AI\nDESCRIPTION: Defines browser settings for the crawler including headless mode, viewport size, user agent configuration, and error handling options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# browser.yml\nheadless: true\nviewport_width: 1280\nuser_agent_mode: \"random\"\nverbose: true\nignore_https_errors: true\n```\n\n----------------------------------------\n\nTITLE: Defining the HTML Extraction API Endpoint\nDESCRIPTION: Specifies the HTTP method (POST) and path (`/html`) for the endpoint designed to crawl a URL and return preprocessed HTML optimized for schema extraction. This is part of the server's specialized API offerings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_19\n\nLANGUAGE: plaintext\nCODE:\n```\nPOST /html\n```\n\n----------------------------------------\n\nTITLE: Minimal AsyncWebCrawler Usage Example with BrowserConfig in Python\nDESCRIPTION: Simple example showing how to initialize and use AsyncWebCrawler with a custom BrowserConfig. Demonstrates creating a Firefox browser in visible mode with text-only processing and basic page crawling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_conf = BrowserConfig(\n    browser_type=\"firefox\",\n    headless=False,\n    text_mode=True\n)\n\nasync with AsyncWebCrawler(config=browser_conf) as crawler:\n    result = await crawler.arun(\"https://example.com\")\n    print(result.markdown[:300])\n```\n\n----------------------------------------\n\nTITLE: Configuring Observability for Crawl4AI in YAML\nDESCRIPTION: This YAML snippet configures observability settings for Crawl4AI, including Prometheus metrics and health check endpoints.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_65\n\nLANGUAGE: yaml\nCODE:\n```\nobservability:\n  prometheus:\n    enabled: True              # Enable Prometheus metrics\n    endpoint: \"/metrics\"       # Metrics endpoint\n  health_check:\n    endpoint: \"/health\"        # Health check endpoint\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy Settings in JSON for Crawl4AI\nDESCRIPTION: This JSON snippet demonstrates how to structure proxy configuration settings for the BrowserConfig in Crawl4AI. It includes fields for the proxy server, username, and password.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"server\": \"http://proxy.example.com:8080\", \n    \"username\": \"...\", \n    \"password\": \"...\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example Structure of result.links (Python)\nDESCRIPTION: This snippet illustrates the typical dictionary structure of the `result.links` attribute in a `CrawlResult` object. It shows how internal and external links are grouped, with each link represented as a dictionary containing keys like `href`, `text`, `title`, and `base_domain`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresult.links = {\n  \"internal\": [\n    {\n      \"href\": \"https://kidocode.com/\",\n      \"text\": \"\",\n      \"title\": \"\",\n      \"base_domain\": \"kidocode.com\"\n    },\n    {\n      \"href\": \"https://kidocode.com/degrees/technology\",\n      \"text\": \"Technology Degree\",\n      \"title\": \"KidoCode Tech Program\",\n      \"base_domain\": \"kidocode.com\"\n    },\n    # ...\n  ],\n  \"external\": [\n    # possibly other links leading to third-party sites\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Evaluating Canonical URL for SEO\nDESCRIPTION: A method that scores whether a canonical URL is properly set. It returns 1.0 if the canonical URL matches the original URL, 0.2 if there's a mismatch, and 0.5 if no canonical URL is specified.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_195\n\nLANGUAGE: python\nCODE:\n```\ndef _score_canonical(self, canonical: str, original: str) -> float:\n    if not canonical:\n        return 0.5  # Neutral score\n    return 1.0 if canonical == original else 0.2\n```\n\n----------------------------------------\n\nTITLE: Handling nth-child Selectors in Tables using Python and lxml\nDESCRIPTION: This function provides special handling for nth-child selectors in tables. It extracts the column number from the selector, handles remaining selectors after the nth-child part, and returns the appropriate elements.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_77\n\nLANGUAGE: Python\nCODE:\n```\ndef _handle_nth_child_selector(self, element, selector_str):\n    \"\"\"Special handling for nth-child selectors in tables\"\"\"\n    import re\n    results = []\n    \n    try:\n        # Extract the column number from td:nth-child(N)\n        match = re.search(r'td:nth-child\\((\\d+)\\)', selector_str)\n        if match:\n            col_num = match.group(1)\n            \n            # Check if there's content after the nth-child part\n            remaining_selector = selector_str.split(f\"td:nth-child({col_num})\", 1)[-1].strip()\n            \n            if remaining_selector:\n                # If there's a specific element we're looking for after the column\n                # Extract any tag names from the remaining selector\n                tag_match = re.search(r'(\\w+)', remaining_selector)\n                tag_name = tag_match.group(1) if tag_match else '*'\n                results = element.xpath(f\".//td[{col_num}]//{tag_name}\")\n            else:\n                # Just get the column cell\n                results = element.xpath(f\".//td[{col_num}]\")\n    except Exception as e:\n        if self.verbose:\n            print(f\"Error handling nth-child selector: {e}\")\n            \n    return results\n```\n\n----------------------------------------\n\nTITLE: Configuring CosineStrategy for Technical Documentation Extraction in Python\nDESCRIPTION: Illustrates the configuration of CosineStrategy for extracting technical specifications and documentation with stricter matching criteria.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\n```\n\n----------------------------------------\n\nTITLE: Anti-Bot Configuration\nDESCRIPTION: Configuration options for anti-bot detection measures.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    magic=True,\n    simulate_user=True,\n    override_navigator=True\n)\n```\n\n----------------------------------------\n\nTITLE: Example Structure of result.media (Python)\nDESCRIPTION: This snippet illustrates the potential structure of the `result.media` attribute in a `CrawlResult` object. It's a dictionary keyed by media type (e.g., `images`, `videos`, `audio`, `tables`). The example shows the detailed structure for image entries (with fields like `src`, `alt`, `desc`, `score`) and table entries (with `headers`, `rows`, `caption`).\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresult.media = {\n  \"images\": [\n    {\n      \"src\": \"https://cdn.prod.website-files.com/.../Group%2089.svg\",\n      \"alt\": \"coding school for kids\",\n      \"desc\": \"Trial Class Degrees degrees All Degrees AI Degree Technology ...\",\n      \"score\": 3,\n      \"type\": \"image\",\n      \"group_id\": 0,\n      \"format\": None,\n      \"width\": None,\n      \"height\": None\n    },\n    # ...\n  ],\n  \"videos\": [\n    # Similar structure but with video-specific fields\n  ],\n  \"audio\": [\n    # Similar structure but with audio-specific fields\n  ],\n  \"tables\": [\n    {\n      \"headers\": [\"Name\", \"Age\", \"Location\"],\n      \"rows\": [\n        [\"John Doe\", \"34\", \"New York\"],\n        [\"Jane Smith\", \"28\", \"San Francisco\"],\n        [\"Alex Johnson\", \"42\", \"Chicago\"]\n      ],\n      \"caption\": \"Employee Directory\",\n      \"summary\": \"Directory of company employees\"\n    },\n    # More tables if present\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Scoring Meta Description Length for SEO\nDESCRIPTION: A method that evaluates the quality of a meta description based on its length. Optimal lengths (140-160 characters) receive a perfect score of 1.0, acceptable lengths (120-200) receive 0.5, and others receive 0.2.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_194\n\nLANGUAGE: python\nCODE:\n```\ndef _score_meta_description(self, desc: str) -> float:\n    length = len(desc)\n    if 140 <= length <= 160:\n        return 1.0\n    return 0.5 if 120 <= length <= 200 else 0.2\n```\n\n----------------------------------------\n\nTITLE: Customizing Clustering Parameters in CosineStrategy for Advanced Use Cases\nDESCRIPTION: Shows how to customize the clustering algorithm by setting linkage method, maximum distance, and specifying a different embedding model with multilingual support.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_177\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring CosineStrategy for Product Review Analysis in Python\nDESCRIPTION: Shows how to set up CosineStrategy for extracting multiple customer reviews and ratings from a product page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Web Scraping Data Models\nDESCRIPTION: Defines Pydantic models for web scraping results, including MediaItem, Link, Media, Links, and ScrapingResult. These models structure data extracted from web pages, such as images, videos, tables, and hyperlinks.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_88\n\nLANGUAGE: python\nCODE:\n```\nclass MediaItem(BaseModel):\n    src: Optional[str] = \"\"\n    data: Optional[str] = \"\"\n    alt: Optional[str] = \"\"\n    desc: Optional[str] = \"\"\n    score: Optional[int] = 0\n    type: str = \"image\"\n    group_id: Optional[int] = 0\n    format: Optional[str] = None\n    width: Optional[int] = None\n\n\nclass Link(BaseModel):\n    href: Optional[str] = \"\"\n    text: Optional[str] = \"\"\n    title: Optional[str] = \"\"\n    base_domain: Optional[str] = \"\"\n\n\nclass Media(BaseModel):\n    images: List[MediaItem] = []\n    videos: List[\n        MediaItem\n    ] = []  # Using MediaItem model for now, can be extended with Video model if needed\n    audios: List[\n        MediaItem\n    ] = []  # Using MediaItem model for now, can be extended with Audio model if needed\n    tables: List[Dict] = []  # Table data extracted from HTML tables\n\n\nclass Links(BaseModel):\n    internal: List[Link] = []\n    external: List[Link] = []\n\n\nclass ScrapingResult(BaseModel):\n    cleaned_html: str\n    success: bool\n    media: Media = Media()\n    links: Links = Links()\n    metadata: Dict[str, Any] = {}\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using CosineStrategy in Python\nDESCRIPTION: Demonstrates how to import, configure, and use the CosineStrategy with AsyncWebCrawler for content extraction.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content\n```\n\n----------------------------------------\n\nTITLE: Example of Processing Iframes with Crawl4AI\nDESCRIPTION: This example demonstrates how to process and merge iframe content into the main page output. When process_iframes is enabled, the crawler will extract content from iframes and include it in the final cleaned HTML.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        process_iframes=True,\n        remove_overlay_elements=True\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.org/iframe-demo\", \n            config=config\n        )\n        print(\"Iframe-merged length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: PruningContentFilter Class Implementation in Python\nDESCRIPTION: A class that filters HTML content using a pruning algorithm with dynamic thresholds. It calculates weighted scores for HTML elements based on various metrics like text density, link density, and tag importance, then returns the most relevant content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_95\n\nLANGUAGE: python\nCODE:\n```\nclass PruningContentFilter(RelevantContentFilter):\n    \"\"\"\n    Content filtering using pruning algorithm with dynamic threshold.\n\n    How it works:\n    1. Extracts page metadata with fallbacks.\n    2. Extracts text chunks from the body element.\n    3. Applies pruning algorithm to calculate scores for each chunk.\n    4. Filters out chunks below the threshold.\n    5. Sorts chunks by score in descending order.\n    6. Returns the top N chunks.\n\n    Attributes:\n        user_query (str): User query for filtering (optional), if not provided, falls back to page metadata.\n        min_word_threshold (int): Minimum word threshold for filtering (optional).\n        threshold_type (str): Threshold type for dynamic threshold (default: 'fixed').\n        threshold (float): Fixed threshold value (default: 0.48).\n\n        Methods:\n            filter_content(self, html: str, min_word_threshold: int = None):\n    \"\"\"\n\n    def __init__(\n        self,\n        user_query: str = None,\n        min_word_threshold: int = None,\n        threshold_type: str = \"fixed\",\n        threshold: float = 0.48,\n    ):\n        \"\"\"\n        Initializes the PruningContentFilter class, if not provided, falls back to page metadata.\n\n        Note:\n        If no query is given and no page metadata is available, then it tries to pick up the first significant paragraph.\n\n        Args:\n            user_query (str): User query for filtering (optional).\n            min_word_threshold (int): Minimum word threshold for filtering (optional).\n            threshold_type (str): Threshold type for dynamic threshold (default: 'fixed').\n            threshold (float): Fixed threshold value (default: 0.48).\n        \"\"\"\n        super().__init__(None)\n        self.min_word_threshold = min_word_threshold\n        self.threshold_type = threshold_type\n        self.threshold = threshold\n\n        # Add tag importance for dynamic threshold\n        self.tag_importance = {\n            \"article\": 1.5,\n            \"main\": 1.4,\n            \"section\": 1.3,\n            \"p\": 1.2,\n            \"h1\": 1.4,\n            \"h2\": 1.3,\n            \"h3\": 1.2,\n            \"div\": 0.7,\n            \"span\": 0.6,\n        }\n\n        # Metric configuration\n        self.metric_config = {\n            \"text_density\": True,\n            \"link_density\": True,\n            \"tag_weight\": True,\n            \"class_id_weight\": True,\n            \"text_length\": True,\n        }\n\n        self.metric_weights = {\n            \"text_density\": 0.4,\n            \"link_density\": 0.2,\n            \"tag_weight\": 0.2,\n            \"class_id_weight\": 0.1,\n            \"text_length\": 0.1,\n        }\n\n        self.tag_weights = {\n            \"div\": 0.5,\n            \"p\": 1.0,\n            \"article\": 1.5,\n            \"section\": 1.0,\n            \"span\": 0.3,\n            \"li\": 0.5,\n            \"ul\": 0.5,\n            \"ol\": 0.5,\n            \"h1\": 1.2,\n            \"h2\": 1.1,\n            \"h3\": 1.0,\n            \"h4\": 0.9,\n            \"h5\": 0.8,\n            \"h6\": 0.7,\n        }\n\n    def filter_content(self, html: str, min_word_threshold: int = None) -> List[str]:\n        \"\"\"\n        Implements content filtering using pruning algorithm with dynamic threshold.\n\n        Note:\n        This method implements the filtering logic for the PruningContentFilter class.\n        It takes HTML content as input and returns a list of filtered text chunks.\n\n        Args:\n            html (str): HTML content to be filtered.\n```\n\n----------------------------------------\n\nTITLE: Basic Markdown Generation with Crawl4AI\nDESCRIPTION: A minimal example demonstrating how to use DefaultMarkdownGenerator with AsyncWebCrawler to generate markdown from a web page. It shows how to configure CrawlerRunConfig and access the generated markdown.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_103\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator()\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        \n        if result.success:\n            print(\"Raw Markdown Output:\\n\")\n            print(result.markdown)  # The unfiltered markdown from the page\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Enabling Text-Only Mode for Faster Crawling\nDESCRIPTION: This snippet shows how to enable text-only mode, which disables images, JavaScript and other resource-heavy elements to make crawling 3-4 times faster. Useful when only extracting text data from websites.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncrawler = AsyncPlaywrightCrawlerStrategy(\n    text_mode=True  # Set this to True to enable text-only crawling\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Media Content from Crawl4AI Results\nDESCRIPTION: This snippet demonstrates how to iterate through images found during crawling and print their URLs and alt text. The media dictionary contains categorized media elements including images, audio, and video.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimages = result.media.get(\"images\", [])\nfor img in images:\n    print(\"Image URL:\", img[\"src\"], \"Alt:\", img.get(\"alt\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Object Serialization in Python\nDESCRIPTION: Converts objects to serializable dictionaries using a {type, params} structure. Handles basic types, enums, datetime objects, collections, and class instances. Includes reverse deserialization functionality.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef to_serializable_dict(obj: Any, ignore_default_value : bool = False) -> Dict:\n    if obj is None:\n        return None\n\n    if isinstance(obj, (str, int, float, bool)):\n        return obj\n\n    if isinstance(obj, Enum):\n        return {\"type\": obj.__class__.__name__, \"params\": obj.value}\n\n    if hasattr(obj, \"isoformat\"):\n        return obj.isoformat()\n\n    if isinstance(obj, (list, tuple, set)) or hasattr(obj, '__iter__') and not isinstance(obj, dict):\n        return [to_serializable_dict(item) for item in obj]\n\n    if isinstance(obj, frozenset):\n        return [to_serializable_dict(item) for item in list(obj)]\n\n    if isinstance(obj, dict):\n        return {\n            \"type\": \"dict\",\n            \"value\": {str(k): to_serializable_dict(v) for k, v in obj.items()},\n        }\n\n    if hasattr(obj, \"__class__\"):\n        sig = inspect.signature(obj.__class__.__init__)\n        params = sig.parameters\n        current_values = {}\n        for name, param in params.items():\n            if name == \"self\":\n                continue\n            value = getattr(obj, name, param.default)\n            if not (is_empty_value(value) and is_empty_value(param.default)):\n                if value != param.default and not ignore_default_value:\n                    current_values[name] = to_serializable_dict(value)\n        \n        if hasattr(obj, '__slots__'):\n            for slot in obj.__slots__:\n                if slot.startswith('_'):\n                    attr_name = slot[1:]\n                    value = getattr(obj, slot, None)\n                    if value is not None:\n                        current_values[attr_name] = to_serializable_dict(value)\n        \n        return {\n            \"type\": obj.__class__.__name__,\n            \"params\": current_values\n        }\n        \n    return str(obj)\n```\n\n----------------------------------------\n\nTITLE: Excluding External Images in Crawl4AI\nDESCRIPTION: Configuration to exclude external images from crawled content, keeping only images from the primary domain.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_87\n\nLANGUAGE: python\nCODE:\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_external_images=True\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring CosineStrategy with Advanced Clustering Options in Python\nDESCRIPTION: Demonstrates advanced configuration of CosineStrategy, including custom clustering methods and multilingual support.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\n```\n\n----------------------------------------\n\nTITLE: Expected JSON Output from Schema Generation\nDESCRIPTION: Shows the expected output format from the LLM-powered schema generation utility. The JSON schema includes a name, base selector, and fields for extracting product information.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"ProductExtractor\",\n  \"baseSelector\": \"div.product\",\n  \"fields\": [\n      {\"name\": \"name\", \"selector\": \"h2\", \"type\": \"text\"},\n      {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n    ]\n }\n```\n\n----------------------------------------\n\nTITLE: Processing Link Information in Python\nDESCRIPTION: Shows how to iterate through and display internal links discovered during a crawl operation, including their target URLs and anchor text.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfor link in result.links[\"internal\"]:\n    print(f\"Internal link to {link['href']} with text {link['text']}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring CosineStrategy for Article Content Extraction in Python\nDESCRIPTION: Demonstrates how to configure CosineStrategy for extracting main content from articles, using specific thresholds and parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Error Handling for CosineStrategy in Python\nDESCRIPTION: Demonstrates proper error handling when using CosineStrategy, including checking for successful extraction and handling exceptions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\n```\n\n----------------------------------------\n\nTITLE: Error Handling for Web Crawler Exception Handling\nDESCRIPTION: This code implements error handling for the web crawler, capturing detailed information about exceptions including line numbers, function names, and code context. It formats error messages into a structured format and returns appropriate error results through the CrawlResultContainer.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nexcept Exception as e:\n    error_context = get_error_context(sys.exc_info())\n\n    error_message = (\n        f\"Unexpected error in _crawl_web at line {error_context['line_no']} \"\n        f\"in {error_context['function']} ({error_context['filename']}):\\n\"\n        f\"Error: {str(e)}\\n\\n\"\n        f\"Code context:\\n{error_context['code_context']}\"\n    )\n\n    self.logger.error_status(\n        url=url,\n        error=create_box_message(error_message, type=\"error\"),\n        tag=\"ERROR\",\n    )\n\n    return CrawlResultContainer(\n        CrawlResult(\n            url=url, html=\"\", success=False, error_message=error_message\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Geo-Aware Crawling in Crawl4AI (Python)\nDESCRIPTION: Demonstrates configuring a Crawl4AI run to simulate specific GPS coordinates, browser locale, and timezone. This uses the `CrawlerRunConfig` and `GeolocationConfig` classes to enable crawling as if from a different location, useful for accessing region-specific content. Dependencies include the `crawl4ai` library.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.6.0.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nCrawlerRunConfig(\n    url=\"https://browserleaks.com/geo\",\n    locale=\"en-US\",\n    timezone_id=\"America/Los_Angeles\",\n    geolocation=GeolocationConfig(\n        latitude=34.0522,\n        longitude=-118.2437,\n        accuracy=10.0\n    )\n)\n```\n```\n\n----------------------------------------\n\nTITLE: Using Builtin Browser in Crawl4AI CLI Command\nDESCRIPTION: This snippet demonstrates how to use the builtin browser mode when performing a crawl operation via the command-line interface.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncrwl https://example.com -b \"browser_mode=builtin\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Deserialization in Python\nDESCRIPTION: Converts serialized dictionaries back into Python objects. Handles basic types, enums, class instances, and collections. Uses crawl4ai module for class instantiation.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef from_serializable_dict(data: Any) -> Any:\n    if data is None:\n        return None\n\n    if isinstance(data, (str, int, float, bool)):\n        return data\n\n    if isinstance(data, dict) and \"type\" in data:\n        if data[\"type\"] == \"dict\" and \"value\" in data:\n            return {k: from_serializable_dict(v) for k, v in data[\"value\"].items()}\n\n        import crawl4ai\n\n        if hasattr(crawl4ai, data[\"type\"]):\n            cls = getattr(crawl4ai, data[\"type\"])\n\n            if issubclass(cls, Enum):\n                return cls(data[\"params\"])\n\n            if \"params\" in data:\n                constructor_args = {\n                    k: from_serializable_dict(v) for k, v in data[\"params\"].items()\n                }\n                return cls(**constructor_args)\n\n    if isinstance(data, list):\n        return [from_serializable_dict(item) for item in data]\n\n    if isinstance(data, dict):\n        return {k: from_serializable_dict(v) for k, v in data.items()}\n\n    return data\n```\n\n----------------------------------------\n\nTITLE: Initializing Best-First Crawler\nDESCRIPTION: Constructor for BestFirstCrawlingStrategy class that sets up crawling parameters including depth limits, filters, URL scoring, and logging configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_168\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n        self,\n        max_depth: int,\n        filter_chain: FilterChain = FilterChain(),\n        url_scorer: Optional[URLScorer] = None,\n        include_external: bool = False,\n        max_pages: int = infinity,\n        logger: Optional[logging.Logger] = None,\n    ):\n        self.max_depth = max_depth\n        self.filter_chain = filter_chain\n        self.url_scorer = url_scorer\n        self.include_external = include_external\n        self.max_pages = max_pages\n        self.logger = logger or logging.getLogger(__name__)\n        self.stats = TraversalStats(start_time=datetime.now())\n        self._cancel_event = asyncio.Event()\n        self._pages_crawled = 0\n```\n\n----------------------------------------\n\nTITLE: Checking Schema.org Markup Presence for SEO\nDESCRIPTION: A method that detects the presence of schema.org markup in HTML head content. It returns 1.0 if JSON-LD schema markup is found, otherwise 0.0.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_196\n\nLANGUAGE: python\nCODE:\n```\ndef _score_schema_org(self, html: str) -> float:\n    # Detect any schema.org markup in head\n    return (\n        1.0\n        if re.search(r'<script[^>]+type=[\"\\']application/ld\\+json', html)\n        else 0.0\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Table Extraction Sensitivity in Crawl4AI\nDESCRIPTION: Configuration to adjust the sensitivity of the table detection algorithm by modifying the score threshold, allowing more or fewer tables to be detected in crawled pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_97\n\nLANGUAGE: python\nCODE:\n```\ncrawler_cfg = CrawlerRunConfig(\n    table_score_threshold=5  # Lower value = more tables detected (default: 7)\n)\n```\n\n----------------------------------------\n\nTITLE: Working with Session IDs in Python\nDESCRIPTION: Shows how to access the session ID from a CrawlResult, which can be used for maintaining browser context across multiple crawl operations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# If you used session_id=\"login_session\" in CrawlerRunConfig, see it here:\nprint(\"Session:\", result.session_id)\n```\n\n----------------------------------------\n\nTITLE: Implementing Different Dispatcher Strategies with Crawl4AI\nDESCRIPTION: This example demonstrates multiple dispatcher implementations for the AsyncWebCrawler, including memory-adaptive and semaphore-based approaches with and without rate limiting. It also includes performance comparison functionality to evaluate the different strategies.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_160\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport time\nfrom rich import print\nfrom rich.table import Table\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    BrowserConfig,\n    CrawlerRunConfig,\n    MemoryAdaptiveDispatcher,\n    SemaphoreDispatcher,\n    RateLimiter,\n    CrawlerMonitor,\n    DisplayMode,\n    CacheMode,\n    LXMLWebScrapingStrategy,\n)\n\n\nasync def memory_adaptive(urls, browser_config, run_config):\n    \"\"\"Memory adaptive crawler with monitoring\"\"\"\n    start = time.perf_counter()\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        dispatcher = MemoryAdaptiveDispatcher(\n            memory_threshold_percent=70.0,\n            max_session_permit=10,\n            monitor=CrawlerMonitor(\n                max_visible_rows=15, display_mode=DisplayMode.DETAILED\n            ),\n        )\n        results = await crawler.arun_many(\n            urls, config=run_config, dispatcher=dispatcher\n        )\n    duration = time.perf_counter() - start\n    return len(results), duration\n\n\nasync def memory_adaptive_with_rate_limit(urls, browser_config, run_config):\n    \"\"\"Memory adaptive crawler with rate limiting\"\"\"\n    start = time.perf_counter()\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        dispatcher = MemoryAdaptiveDispatcher(\n            memory_threshold_percent=95.0,\n            max_session_permit=10,\n            rate_limiter=RateLimiter(\n                base_delay=(1.0, 2.0), max_delay=30.0, max_retries=2\n            ),\n            monitor=CrawlerMonitor(\n                max_visible_rows=15, display_mode=DisplayMode.DETAILED\n            ),\n        )\n        results = await crawler.arun_many(\n            urls, config=run_config, dispatcher=dispatcher\n        )\n    duration = time.perf_counter() - start\n    return len(results), duration\n\n\nasync def semaphore(urls, browser_config, run_config):\n    \"\"\"Basic semaphore crawler\"\"\"\n    start = time.perf_counter()\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        dispatcher = SemaphoreDispatcher(\n            semaphore_count=5,\n            monitor=CrawlerMonitor(\n                max_visible_rows=15, display_mode=DisplayMode.DETAILED\n            ),\n        )\n        results = await crawler.arun_many(\n            urls, config=run_config, dispatcher=dispatcher\n        )\n    duration = time.perf_counter() - start\n    return len(results), duration\n\n\nasync def semaphore_with_rate_limit(urls, browser_config, run_config):\n    \"\"\"Semaphore crawler with rate limiting\"\"\"\n    start = time.perf_counter()\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        dispatcher = SemaphoreDispatcher(\n            semaphore_count=5,\n            rate_limiter=RateLimiter(\n                base_delay=(1.0, 2.0), max_delay=30.0, max_retries=2\n            ),\n            monitor=CrawlerMonitor(\n                max_visible_rows=15, display_mode=DisplayMode.DETAILED\n            ),\n        )\n        results = await crawler.arun_many(\n            urls, config=run_config, dispatcher=dispatcher\n        )\n    duration = time.perf_counter() - start\n    return len(results), duration\n\n\ndef create_performance_table(results):\n    \"\"\"Creates a rich table showing performance results\"\"\"\n    table = Table(title=\"Crawler Strategy Performance Comparison\")\n    table.add_column(\"Strategy\", style=\"cyan\")\n    table.add_column(\"URLs Crawled\", justify=\"right\", style=\"green\")\n    table.add_column(\"Time (seconds)\", justify=\"right\", style=\"yellow\")\n    table.add_column(\"URLs/second\", justify=\"right\", style=\"magenta\")\n\n    sorted_results = sorted(results.items(), key=lambda x: x[1][1])\n\n    for strategy, (urls_crawled, duration) in sorted_results:\n        urls_per_second = urls_crawled / duration\n        table.add_row(\n            strategy, str(urls_crawled), f\"{duration:.2f}\", f\"{urls_per_second:.2f}\"\n        )\n\n    return table\n\n\nasync def main():\n    urls = [f\"https://example.com/page{i}\" for i in range(1, 40)]\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, scraping_strategy=LXMLWebScrapingStrategy())\n\n    results = {\n        \"Memory Adaptive\": await memory_adaptive(urls, browser_config, run_config),\n        # \"Memory Adaptive + Rate Limit\": await memory_adaptive_with_rate_limit(\n        #     urls, browser_config, run_config\n        # ),\n        # \"Semaphore\": await semaphore(urls, browser_config, run_config),\n        # \"Semaphore + Rate Limit\": await semaphore_with_rate_limit(\n        #     urls, browser_config, run_config\n        # ),\n    }\n\n    table = create_performance_table(results)\n    print(\"\\nPerformance Summary:\")\n    print(table)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Accessing Dispatch Results in Python\nDESCRIPTION: Shows how to access and process dispatch results from crawler output, including memory usage and duration information.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfor result in results:\n    if result.success:\n        dr = result.dispatch_result\n        print(f\"URL: {result.url}\")\n        print(f\"Memory: {dr.memory_usage:.1f}MB\")\n        print(f\"Duration: {dr.end_time - dr.start_time}\")\n```\n\n----------------------------------------\n\nTITLE: Crawler Shutdown Implementation in Python\nDESCRIPTION: Implements the shutdown method for the BFS crawler. This method signals cancellation of the crawling process and records the end time of the crawl operation for statistics tracking.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_181\n\nLANGUAGE: python\nCODE:\n```\nasync def shutdown(self) -> None:\n    \"\"\"\n    Clean up resources and signal cancellation of the crawl.\n    \"\"\"\n    self._cancel_event.set()\n    self.stats.end_time = datetime.now()\n```\n\n----------------------------------------\n\nTITLE: Content Filtering Command Example for Crawl4AI\nDESCRIPTION: Shows how to apply content filtering to crawled results and output filtered markdown.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncrwl https://example.com -f filter_bm25.yml -o markdown-fit\n```\n\n----------------------------------------\n\nTITLE: Configuring Strict vs. Loose Matching with CosineStrategy in Python\nDESCRIPTION: Shows how to configure the similarity threshold parameter for either strict or loose matching between content segments. Higher thresholds provide stricter matching while lower thresholds allow more variation.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_171\n\nLANGUAGE: python\nCODE:\n```\n# Strict matching\nstrategy = CosineStrategy(sim_threshold=0.8)\n\n# Loose matching\nstrategy = CosineStrategy(sim_threshold=0.3)\n```\n\n----------------------------------------\n\nTITLE: Proxy Configuration Format for BrowserConfig in JSON\nDESCRIPTION: JSON structure for configuring a proxy with BrowserConfig. Specifies server URL, username and password for authenticated proxies.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"server\": \"http://proxy.example.com:8080\", \n    \"username\": \"...\", \n    \"password\": \"...\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Crawl4AI Container with LLM Support via Docker Run (Bash)\nDESCRIPTION: Shows how to run the Crawl4AI container while providing LLM API keys using an environment file. Assumes the `.llm.env` file exists in the current directory. It uses the `--env-file` flag along with other standard `docker run` options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Make sure .llm.env is in the current directory\ndocker run -d \\\n  -p 11235:11235 \\\n  --name crawl4ai \\\n  --env-file .llm.env \\\n  --shm-size=1g \\\n  unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number\n```\n\n----------------------------------------\n\nTITLE: Handling SSL Certificates with AsyncWebCrawler\nDESCRIPTION: This code demonstrates how to fetch and process SSL certificates during web crawling. It shows how to retrieve certificate information such as issuer, validity, and fingerprint, and how to export the certificate in various formats (JSON, PEM, DER).\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_132\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio, os\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    tmp_dir = os.path.join(os.getcwd(), \"tmp\")\n    os.makedirs(tmp_dir, exist_ok=True)\n    \n    config = CrawlerRunConfig(\n        fetch_ssl_certificate=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=config)\n        \n        if result.success and result.ssl_certificate:\n            cert = result.ssl_certificate\n            print(\"\\nCertificate Information:\")\n            print(f\"Issuer (CN): {cert.issuer.get('CN', '')}\")\n            print(f\"Valid until: {cert.valid_until}\")\n            print(f\"Fingerprint: {cert.fingerprint}\")\n\n            # Export in multiple formats:\n            cert.to_json(os.path.join(tmp_dir, \"certificate.json\"))\n            cert.to_pem(os.path.join(tmp_dir, \"certificate.pem\"))\n            cert.to_der(os.path.join(tmp_dir, \"certificate.der\"))\n            \n            print(\"\\nCertificate exported to JSON/PEM/DER in 'tmp' folder.\")\n        else:\n            print(\"[ERROR] No certificate or crawl failed.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Browser Configuration for Crawl4AI CLI\nDESCRIPTION: Demonstrates how to configure browser settings using a YAML file and command-line parameters for the Crawl4AI CLI.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n# browser.yml\nheadless: true\nviewport_width: 1280\nuser_agent_mode: \"random\"\nverbose: true\nignore_https_errors: true\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Using config file\ncrwl https://example.com -B browser.yml\n\n# Using direct parameters\ncrwl https://example.com -b \"headless=true,viewport_width=1280,user_agent_mode=random\"\n```\n\n----------------------------------------\n\nTITLE: Configuring the Number of Content Clusters to Return in CosineStrategy\nDESCRIPTION: Shows how to set the top_k parameter which determines the number of top content clusters to return. Higher values will return more diverse content from the extracted webpage.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_173\n\nLANGUAGE: python\nCODE:\n```\n# Get top 5 most relevant content clusters\nstrategy = CosineStrategy(top_k=5)\n```\n\n----------------------------------------\n\nTITLE: Quick Test for Crawl4AI Docker Deployment (Python)\nDESCRIPTION: A Python script using the 'requests' library to test the running Crawl4AI Docker service. It sends a POST request to the '/crawl' endpoint to submit a job for 'https://example.com', retrieves the task ID, and then performs a GET request to poll the task status.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\n# Submit a crawl job\nresponse = requests.post(\n    \"http://localhost:11235/crawl\",\n    json={\"urls\": \"https://example.com\", \"priority\": 10}\n)\ntask_id = response.json()[\"task_id\"]\n\n# Continue polling until the task is complete (status=\"completed\")\nresult = requests.get(f\"http://localhost:11235/task/{task_id}\")\n```\n\n----------------------------------------\n\nTITLE: Creating `.llm.env` File for API Keys (Bash)\nDESCRIPTION: Shows how to create a `.llm.env` file in the current working directory using the `cat` command with a Here Document (EOL). This file is used to store API keys for various LLM providers (OpenAI, Anthropic, etc.) which are then passed to the Docker container.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Create a .llm.env file with your API keys\ncat > .llm.env << EOL\n# OpenAI\nOPENAI_API_KEY=sk-your-key\n\n# Anthropic\nANTHROPIC_API_KEY=your-anthropic-key\n\n# Other providers as needed\n# DEEPSEEK_API_KEY=your-deepseek-key\n# GROQ_API_KEY=your-groq-key\n# TOGETHER_API_KEY=your-together-key\n# MISTRAL_API_KEY=your-mistral-key\n# GEMINI_API_TOKEN=your-gemini-token\nEOL\n```\n\n----------------------------------------\n\nTITLE: Fetching SSL Certificate from URL in Python\nDESCRIPTION: Demonstrates how to manually load an SSL certificate from a URL using the from_url static method. This method connects to the specified URL (port 443) and retrieves the SSL certificate.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncert = SSLCertificate.from_url(\"https://example.com\")\nif cert:\n    print(\"Fingerprint:\", cert.fingerprint)\n```\n\n----------------------------------------\n\nTITLE: Enabling Verbose Logging for Debugging in Crawl4AI\nDESCRIPTION: Demonstrates how to enable verbose logging in BrowserConfig for debugging purposes during web crawling operations.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nbrowser_config = BrowserConfig(verbose=True)\n\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    run_config = CrawlerRunConfig()\n    result = await crawler.arun(url=\"https://example.com\", config=run_config)\n```\n\n----------------------------------------\n\nTITLE: Link Structure Example in Crawl4AI Results\nDESCRIPTION: Sample data structure showing how links are organized in the CrawlResult object, including internal and external link categorization with their properties.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_90\n\nLANGUAGE: python\nCODE:\n```\nresult.links = {\n  \"internal\": [\n    {\n      \"href\": \"https://kidocode.com/\",\n      \"text\": \"\",\n      \"title\": \"\",\n      \"base_domain\": \"kidocode.com\"\n    },\n    {\n      \"href\": \"https://kidocode.com/degrees/technology\",\n      \"text\": \"Technology Degree\",\n      \"title\": \"KidoCode Tech Program\",\n      \"base_domain\": \"kidocode.com\"\n    },\n    # ...\n  ],\n  \"external\": [\n    # possibly other links leading to third-party sites\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Fixed-Length Word Chunking in Python\nDESCRIPTION: Segments text into chunks with a fixed number of words. The class accepts a configurable chunk size parameter and splits the text into segments of the specified word count.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_166\n\nLANGUAGE: python\nCODE:\n```\nclass FixedLengthWordChunking:\n    def __init__(self, chunk_size=100):\n        self.chunk_size = chunk_size\n\n    def chunk(self, text):\n        words = text.split()\n        return [' '.join(words[i:i + self.chunk_size]) for i in range(0, len(words), self.chunk_size)]\n\n# Example Usage\ntext = \"This is a long text with many words to be chunked into fixed sizes.\"\nchunker = FixedLengthWordChunking(chunk_size=5)\nprint(chunker.chunk(text))\n```\n\n----------------------------------------\n\nTITLE: Crawler Shutdown Handler\nDESCRIPTION: Method to handle graceful shutdown of the crawler by setting cancellation event and recording end time.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_175\n\nLANGUAGE: python\nCODE:\n```\nasync def shutdown(self) -> None:\n        self._cancel_event.set()\n        self.stats.end_time = datetime.now()\n```\n\n----------------------------------------\n\nTITLE: Manual Playwright Installation Command (Bash)\nDESCRIPTION: This command manually triggers the installation of Playwright browsers. It should be used if the automatic setup via `crawl4ai-setup` fails.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nplaywright install\n```\n\n----------------------------------------\n\nTITLE: Pull and Run Crawl4AI Docker Container (Bash)\nDESCRIPTION: Pulls a specific revision of the Crawl4AI Docker image from Docker Hub and runs it as a detached container named 'crawl4ai'. It maps the container's port 11235 to the host's port 11235 and allocates 1GB of shared memory.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n# Pull and run the latest release candidate\ndocker pull unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number\ndocker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number\n\n# Visit the playground at http://localhost:11235/playground\n```\n\n----------------------------------------\n\nTITLE: Setting Up LLM Environment Variables for Crawl4AI\nDESCRIPTION: Example of an environment file for storing API keys for various Language Model providers that can be used with Crawl4AI.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_46\n\nLANGUAGE: env\nCODE:\n```\n# OpenAI\nOPENAI_API_KEY=sk-your-key\n\n# Anthropic\nANTHROPIC_API_KEY=your-anthropic-key\n\n# DeepSeek\nDEEPSEEK_API_KEY=your-deepseek-key\n\n# Check out https://docs.litellm.ai/docs/providers for more providers!\n```\n\n----------------------------------------\n\nTITLE: Capturing Web Pages as MHTML using Crawl4AI in Python\nDESCRIPTION: Provides an asynchronous Python example using `AsyncWebCrawler` and `CrawlerRunConfig` to capture a web page as an MHTML file. It enables the `capture_mhtml=True` option in the configuration, runs the crawler asynchronously for a given URL, checks for success and the presence of MHTML data (`result.mhtml`), and saves the MHTML content to a local file ('example.mhtml'). MHTML is useful for archiving complete web pages offline.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    crawler_cfg = CrawlerRunConfig(\n        capture_mhtml=True  # Enable MHTML capture\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=crawler_cfg)\n        \n        if result.success and result.mhtml:\n            # Save the MHTML snapshot to a file\n            with open(\"example.mhtml\", \"w\", encoding=\"utf-8\") as f:\n                f.write(result.mhtml)\n            print(\"MHTML snapshot saved to example.mhtml\")\n        else:\n            print(\"Failed to capture MHTML:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n```\n\n----------------------------------------\n\nTITLE: Configuration Output Handling\nDESCRIPTION: Handles various output formats including JSON, Markdown, and file output. Processes crawler results and formats them according to specified output type.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nif not output_file:\n    if output == \"all\":\n        click.echo(json.dumps(result.model_dump(), indent=2))\n    elif output == \"json\":\n        print(result.extracted_content)\n        extracted_items = json.loads(result.extracted_content)\n        click.echo(json.dumps(extracted_items, indent=2))\n    elif output in [\"markdown\", \"md\"]:\n        click.echo(result.markdown.raw_markdown)\n    elif output in [\"markdown-fit\", \"md-fit\"]:\n        click.echo(result.markdown.fit_markdown)\n```\n\n----------------------------------------\n\nTITLE: Using LLMContentFilter for Intelligent Markdown Generation\nDESCRIPTION: Shows how to integrate LLM-based content filtering with Crawl4AI for generating more focused markdown output. The code configures a DefaultMarkdownGenerator with LLMContentFilter using Gemini API and applies specific content extraction instructions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import LLMContentFilter\nfrom crawl4ai import LLMConfig\nimport asyncio\n\nllm_config = LLMConfig(provider=\"gemini/gemini-1.5-pro\", api_token=\"env:GEMINI_API_KEY\")\n\nmarkdown_generator = DefaultMarkdownGenerator(\n    content_filter=LLMContentFilter(llm_config=llm_config, instruction=\"Extract key concepts and summaries\")\n)\n\nconfig = CrawlerRunConfig(markdown_generator=markdown_generator)\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://docs.crawl4ai.com\", config=config)\n        print(result.markdown.fit_markdown)\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Text Chunks in Python\nDESCRIPTION: Defines a method to generate embeddings for a list of sentences using a pre-trained model. It supports batch processing and handles different device types (CPU, GPU, CUDA, MPS).\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_61\n\nLANGUAGE: python\nCODE:\n```\ndef get_embeddings(\n        self, sentences: List[str], batch_size=None, bypass_buffer=False\n    ):\n        if self.device.type in [\"cpu\", \"gpu\", \"cuda\", \"mps\"]:\n            import torch\n\n            if batch_size is None:\n                batch_size = self.default_batch_size\n\n            all_embeddings = []\n            for i in range(0, len(sentences), batch_size):\n                batch_sentences = sentences[i : i + batch_size]\n                encoded_input = self.tokenizer(\n                    batch_sentences, padding=True, truncation=True, return_tensors=\"pt\"\n                )\n                encoded_input = {\n                    key: tensor.to(self.device) for key, tensor in encoded_input.items()\n                }\n\n                with torch.no_grad():\n                    model_output = self.model(**encoded_input)\n\n                embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy()\n                all_embeddings.append(embeddings)\n\n            self.buffer_embeddings = np.vstack(all_embeddings)\n        elif self.device.type == \"cpu\":\n            if batch_size is None:\n                batch_size = self.default_batch_size\n\n            all_embeddings = []\n            for i in range(0, len(sentences), batch_size):\n                batch_sentences = sentences[i : i + batch_size]\n                embeddings = self.model(batch_sentences)\n                all_embeddings.append(embeddings)\n\n            self.buffer_embeddings = np.vstack(all_embeddings)\n        return self.buffer_embeddings\n```\n\n----------------------------------------\n\nTITLE: Using Target Elements for Flexible Content Selection in Crawl4AI\nDESCRIPTION: This example demonstrates using the target_elements parameter to select multiple specific elements while preserving the entire page context. This approach allows focused markdown generation while still extracting links and other metadata from the entire page.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Target article body and sidebar, but not other content\n        target_elements=[\"article.main-content\", \"aside.sidebar\"]\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/blog-post\", \n            config=config\n        )\n        print(\"Markdown focused on target elements\")\n        print(\"Links from entire page still available:\", len(result.links.get(\"internal\", [])))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Semaphore-based Crawling in Python with Crawl4AI\nDESCRIPTION: Demonstrates how to implement a web crawler with semaphore-based concurrency control using Crawl4AI. The code configures a crawler with a SemaphoreDispatcher that limits concurrency to 5 parallel requests and includes a rate limiter to prevent overwhelming servers.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_148\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl_with_semaphore(urls):\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n    \n    dispatcher = SemaphoreDispatcher(\n        semaphore_count=5,\n        rate_limiter=RateLimiter(\n            base_delay=(0.5, 1.0),\n            max_delay=10.0\n        ),\n        monitor=CrawlerMonitor(\n            max_visible_rows=15,\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        results = await crawler.arun_many(\n            urls, \n            config=run_config,\n            dispatcher=dispatcher\n        )\n        return results\n```\n\n----------------------------------------\n\nTITLE: Saving Global Configuration in Python\nDESCRIPTION: Writes the updated configuration dictionary to the global configuration file in YAML format.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef save_global_config(config: dict):\n    config_file = Path.home() / \".crawl4ai\" / \"global.yml\"\n    with open(config_file, \"w\") as f:\n        yaml.dump(config, f)\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI with all extras using pip\nDESCRIPTION: This shell command uses pip, the Python package installer, to install the 'crawl4ai' library. The '[all]' specifier ensures that all optional dependencies (like those for different LLM providers, deep crawling features, etc.) are installed along with the core package, providing access to the full range of features described in the release notes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/index.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install \\\"crawl4ai[all]\\\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Magic Mode in Crawl4AI\nDESCRIPTION: Example of using Magic Mode for simplified automation without persistent identity.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_139\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        config=CrawlerRunConfig(\n            magic=True,  # Simplifies a lot of interaction\n            remove_overlay_elements=True,\n            page_timeout=60000\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing Fixed-Length Word Chunking in Python\nDESCRIPTION: Splits text into chunks containing a fixed number of words. Allows for consistent chunk sizes based on word count.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass FixedLengthWordChunking:\n    def __init__(self, chunk_size=100):\n        self.chunk_size = chunk_size\n\n    def chunk(self, text):\n        words = text.split()\n        return [' '.join(words[i:i + self.chunk_size]) for i in range(0, len(words), self.chunk_size)]\n\n# Example Usage\ntext = \"This is a long text with many words to be chunked into fixed sizes.\"\nchunker = FixedLengthWordChunking(chunk_size=5)\nprint(chunker.chunk(text))\n```\n\n----------------------------------------\n\nTITLE: Setting Word Count Threshold for Content Filtering in CosineStrategy\nDESCRIPTION: Demonstrates setting the word count threshold to filter out short content blocks. This helps eliminate noise and irrelevant content by only considering substantial paragraphs.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_172\n\nLANGUAGE: python\nCODE:\n```\n# Only consider substantial paragraphs\nstrategy = CosineStrategy(word_count_threshold=50)\n```\n\n----------------------------------------\n\nTITLE: Cleaning HTML Elements with Minimal Overhead in Python\nDESCRIPTION: A common method for cleaning HTML tags by removing unwanted elements and attributes. It uses a string builder pattern for better performance and recursively processes nested tags, returning a cleaned HTML string.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_93\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Common method for cleaning HTML elements with minimal overhead\"\"\"\nif not tag or not isinstance(tag, Tag):\n    return \"\"\n\nunwanted_tags = {\"script\", \"style\", \"aside\", \"form\", \"iframe\", \"noscript\"}\nunwanted_attrs = {\n    \"style\",\n    \"onclick\",\n    \"onmouseover\",\n    \"align\",\n    \"bgcolor\",\n    \"class\",\n    \"id\",\n}\n\n# Use string builder pattern for better performance\nbuilder = []\n\ndef render_tag(elem):\n    if not isinstance(elem, Tag):\n        if isinstance(elem, str):\n            builder.append(elem.strip())\n        return\n\n    if elem.name in unwanted_tags:\n        return\n\n    # Start tag\n    builder.append(f\"<{elem.name}\")\n\n    # Add cleaned attributes\n    attrs = {k: v for k, v in elem.attrs.items() if k not in unwanted_attrs}\n    for key, value in attrs.items():\n        builder.append(f' {key}=\"{value}\"')\n\n    builder.append(\">\")\n\n    # Process children\n    for child in elem.children:\n        render_tag(child)\n\n    # Close tag\n    builder.append(f\"</{elem.name}>\")\n\ntry:\n    render_tag(tag)\n    return \"\".join(builder)\nexcept Exception:\n    return str(tag)  # Fallback to original if anything fails\n```\n\n----------------------------------------\n\nTITLE: Configuring LLMExtractionStrategy with Full Parameters in Python\nDESCRIPTION: Creates a complete LLM extraction strategy configuration with settings for the model, schema, chunking parameters, and input format. This example shows all the main parameters that can be customized.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nextraction_strategy = LLMExtractionStrategy(\n    llm_config = LLMConfig(provider=\"openai/gpt-4\", api_token=\"YOUR_OPENAI_KEY\"),\n    schema=MyModel.model_json_schema(),\n    extraction_type=\"schema\",\n    instruction=\"Extract a list of items from the text with 'name' and 'price' fields.\",\n    chunk_token_threshold=1200,\n    overlap_rate=0.1,\n    apply_chunking=True,\n    input_format=\"html\",\n    extra_args={\"temperature\": 0.1, \"max_tokens\": 1000},\n    verbose=True\n)\n```\n\n----------------------------------------\n\nTITLE: Media Filtering Configuration\nDESCRIPTION: Settings for filtering media content in crawled pages.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrun_config = CrawlerRunConfig(\n    exclude_external_images=True  # Strip images from other domains\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Regex-Based Text Chunking in Python\nDESCRIPTION: A class that implements text chunking using regular expressions. Splits text into paragraphs based on specified pattern(s), defaulting to double newlines.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass RegexChunking:\n    def __init__(self, patterns=None):\n        self.patterns = patterns or [r'\\n\\n']  # Default pattern for paragraphs\n\n    def chunk(self, text):\n        paragraphs = [text]\n        for pattern in self.patterns:\n            paragraphs = [seg for p in paragraphs for seg in re.split(pattern, p)]\n        return paragraphs\n\n# Example Usage\ntext = \"\"\"This is the first paragraph.\n\nThis is the second paragraph.\"\"\"\nchunker = RegexChunking()\nprint(chunker.chunk(text))\n```\n\n----------------------------------------\n\nTITLE: CSS-Based Wait Conditions in Crawl4AI\nDESCRIPTION: Shows how to wait for specific CSS elements to appear before proceeding with web scraping using Crawl4AI. This example waits for at least 30 items to load on Hacker News before continuing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Wait for at least 30 items on Hacker News\n        wait_for=\"css:.athing:nth-child(30)\"  \n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n        print(\"We have at least 30 items loaded!\")\n        # Rough check\n        print(\"Total items in HTML:\", result.cleaned_html.count(\"athing\"))  \n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Streaming LLM Responses in Python with LiteLLM\nDESCRIPTION: Asynchronously streams responses from an LLM using the configured provider and token. Uses the crawled URL content as context for answering user queries.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nasync def stream_llm_response(url: str, markdown: str, query: str, provider: str, token: str):\n    response = completion(\n        model=provider,\n        api_key=token,\n        messages=[\n            {\n                \"content\": f\"You are Crawl4ai assistant, answering user question based on the provided context which is crawled from {url}.\",\n                \"role\": \"system\"\n            },\n            {\n                \"content\": f\"<|start of context|>\\n{markdown}\\n<|end of context|>\\n\\n{query}\",\n                \"role\": \"user\"\n            },\n        ],\n        stream=True,\n    )\n    \n    for chunk in response:\n        if content := chunk[\"choices\"][0][\"delta\"].get(\"content\"):\n            print(content, end=\"\", flush=True)\n    print()  # New line at end\n```\n\n----------------------------------------\n\nTITLE: Install Crawl4AI with PyTorch Features (Development Mode, Bash)\nDESCRIPTION: Installs Crawl4AI in editable mode along with optional dependencies required for PyTorch-related features.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[torch]\"           # With PyTorch features\n```\n\n----------------------------------------\n\nTITLE: Complex Browser Configuration with Viewport Settings\nDESCRIPTION: JSON example showing how to structure more complex configurations with dictionaries using the type-params pattern in Crawl4AI.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_53\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"browser_config\": {\n        \"type\": \"BrowserConfig\",\n        \"params\": {\n            \"headless\": true,           // Simple boolean - direct value\n            \"viewport\": {               // Complex dictionary - needs type-params\n                \"type\": \"dict\",\n                \"value\": {\n                    \"width\": 1200,\n                    \"height\": 800\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Image Lazy Loading Support in Crawl4AI\nDESCRIPTION: This code demonstrates how to enable the wait_for_images feature that ensures all lazy-loaded images are fully loaded before proceeding with the crawl. This improves content capture for modern websites that use lazy-loading techniques.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nawait crawler.crawl(\n    url=\"https://example.com\",\n    wait_for_images=True  # Add this argument to ensure images are fully loaded\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Crawler4ai with Callbacks\nDESCRIPTION: Example showing how to define and use custom callback functions to process pages during crawling, allowing for custom extraction and processing logic.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom crawler4ai import Crawler4ai\nfrom bs4 import BeautifulSoup\n\ndef my_page_processor(html, url):\n    soup = BeautifulSoup(html, 'html.parser')\n    \n    # Custom extraction logic\n    custom_data = {\n        'headings': [h.text for h in soup.find_all(['h1', 'h2', 'h3'])],\n        'paragraphs': [p.text for p in soup.find_all('p')],\n        'custom_field': soup.find(id='specific-element').text if soup.find(id='specific-element') else None\n    }\n    \n    return custom_data\n\ncrawler = Crawler4ai(\n    urls=[\"https://example.com\"],\n    max_pages=50,\n    page_callback=my_page_processor\n)\n\nresults = crawler.crawl()\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI for Development in Python\nDESCRIPTION: Commands for cloning the Crawl4AI repository, installing it in editable mode with all features, and setting up Playwright dependencies for development purposes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/unclecode/crawl4ai.git\ncd crawl4ai\npip install -e \".[all]\"\nplaywright install # Install Playwright dependencies\n```\n\n----------------------------------------\n\nTITLE: Network Request Event JSON Structure in Crawl4AI\nDESCRIPTION: Example JSON structure for a network request event captured by Crawl4AI. This shows the format of data collected when a web page makes HTTP requests, including method, headers, and other metadata.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_154\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"event_type\": \"request\",\n  \"url\": \"https://example.com/api/data.json\",\n  \"method\": \"GET\",\n  \"headers\": {\"User-Agent\": \"...\", \"Accept\": \"...\"},\n  \"post_data\": \"key=value&otherkey=value\",\n  \"resource_type\": \"fetch\",\n  \"is_navigation_request\": false,\n  \"timestamp\": 1633456789.123\n}\n```\n\n----------------------------------------\n\nTITLE: Install Crawl4AI with Synchronous Crawling (Development Mode, Bash)\nDESCRIPTION: Installs Crawl4AI in editable mode along with optional dependencies required for synchronous crawling using Selenium.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[sync]\"            # With synchronous crawling (Selenium)\n```\n\n----------------------------------------\n\nTITLE: Implementing Browser View Command in Python\nDESCRIPTION: Implementation of the 'crwl browser view' command that opens a visible window connected to the running builtin browser. It detects the operating system to determine the appropriate browser command and connects to the running instance using the CDP URL.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_48\n\nLANGUAGE: python\nCODE:\n```\n@browser_cmd.command(\"view\")\n@click.option(\"--url\", \"-u\", help=\"URL to navigate to (defaults to about:blank)\")\ndef browser_view_cmd(url: Optional[str]):\n    \"\"\"\n    Open a visible window of the builtin browser\n    \n    This command connects to the running builtin browser and opens a visible window,\n    allowing you to see what the browser is currently viewing or navigate to a URL.\n    \"\"\"\n    profiler = BrowserProfiler()\n    \n    try:\n        # First check if browser is running\n        status = anyio.run(profiler.get_builtin_browser_status)\n        if not status[\"running\"]:\n            console.print(Panel(\n                \"[yellow]No builtin browser is currently running[/yellow]\\n\\n\"\n                \"Use 'crwl browser start' to start a builtin browser first\",\n                title=\"Builtin Browser View\",\n                border_style=\"yellow\"\n            ))\n            return\n        \n        info = status[\"info\"]\n        cdp_url = info[\"cdp_url\"]\n        \n        console.print(Panel(\n            f\"[cyan]Opening visible window connected to builtin browser[/cyan]\\n\\n\"\n            f\"CDP URL: [green]{cdp_url}[/green]\\n\"\n            f\"URL to load: [yellow]{url or 'about:blank'}[/yellow]\",\n            title=\"Builtin Browser View\",\n            border_style=\"cyan\"\n        ))\n        \n        # Use the CDP URL to launch a new visible window\n        import subprocess\n        import os\n        \n        # Determine the browser command based on platform\n        if sys.platform == \"darwin\":  # macOS\n            browser_cmd = [\"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\"]\n        elif sys.platform == \"win32\":  # Windows\n            browser_cmd = [\"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"]\n        else:  # Linux\n            browser_cmd = [\"google-chrome\"]\n        \n        # Add arguments\n        browser_args = [\n            f\"--remote-debugging-port={info['debugging_port']}\",\n            \"--remote-debugging-address=localhost\",\n            \"--no-first-run\",\n            \"--no-default-browser-check\"\n        ]\n        \n        # Add URL if provided\n        if url:\n            browser_args.append(url)\n        \n        # Launch browser\n        try:\n            subprocess.Popen(browser_cmd + browser_args)\n            console.print(\"[green]Browser window opened. Close it when finished viewing.[/green]\")\n        except Exception as e:\n            console.print(f\"[red]Error launching browser: {str(e)}[/red]\")\n            console.print(f\"[yellow]Try connecting manually to {cdp_url} in Chrome or using the '--remote-debugging-port' flag.[/yellow]\")\n    \n    except Exception as e:\n        console.print(f\"[red]Error viewing builtin browser: {str(e)}[/red]\")\n        sys.exit(1)\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI with PyTorch features\nDESCRIPTION: Installs Crawl4AI with PyTorch-based features such as cosine similarity or advanced semantic chunking, followed by running the setup command.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai[torch]\ncrawl4ai-setup\n```\n\n----------------------------------------\n\nTITLE: BrowserConfig Helper Methods Example in Python\nDESCRIPTION: Example demonstrating how to use BrowserConfig's clone() method to create modified configurations. Shows creating a base configuration and then a specialized debug configuration with modified parameters.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create a base browser config\nbase_browser = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    text_mode=True\n)\n\n# Create a visible browser config for debugging\ndebug_browser = base_browser.clone(\n    headless=False,\n    verbose=True\n)\n```\n\n----------------------------------------\n\nTITLE: Complex Nested Content Filter Configuration\nDESCRIPTION: Advanced JSON configuration example showing deeply nested configurations for content filtering in the Crawl4AI markdown generator.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_55\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"crawler_config\": {\n        \"type\": \"CrawlerRunConfig\",\n        \"params\": {\n            \"markdown_generator\": {\n                \"type\": \"DefaultMarkdownGenerator\",\n                \"params\": {\n                    \"content_filter\": {\n                        \"type\": \"PruningContentFilter\",\n                        \"params\": {\n                            \"threshold\": 0.48,\n                            \"threshold_type\": \"fixed\"\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Building Crawl4AI Docker Image for Current Architecture using Buildx (Bash)\nDESCRIPTION: Illustrates using `docker buildx build` to build the Crawl4AI image locally for the host machine's architecture. The `-t` flag tags the image as `crawl4ai-local:latest`, and `--load` loads the built image into the Docker daemon.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# Make sure you are in the 'crawl4ai' root directory\n# Build for the current architecture and load it into Docker\ndocker buildx build -t crawl4ai-local:latest --load .\n```\n\n----------------------------------------\n\nTITLE: BrowserConfig Class Definition in Python\nDESCRIPTION: Python class definition for BrowserConfig which controls browser launch parameters and behavior. Includes parameters for browser type, headless mode, proxy configuration, viewport dimensions, and other browser behavior settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass BrowserConfig:\n    def __init__(\n        browser_type=\"chromium\",\n        headless=True,\n        proxy_config=None,\n        viewport_width=1080,\n        viewport_height=600,\n        verbose=True,\n        use_persistent_context=False,\n        user_data_dir=None,\n        cookies=None,\n        headers=None,\n        user_agent=None,\n        text_mode=False,\n        light_mode=False,\n        extra_args=None,\n        # ... other advanced parameters omitted here\n    ):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Implementing HTTPCrawlerConfig Class in Python\nDESCRIPTION: A configuration class for HTTP-specific crawler settings including method, headers, data handling, and SSL verification. Provides methods for serialization and deserialization of config data.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass HTTPCrawlerConfig:\n    \"\"\"HTTP-specific crawler configuration\"\"\"\n\n    method: str = \"GET\"\n    headers: Optional[Dict[str, str]] = None\n    data: Optional[Dict[str, Any]] = None\n    json: Optional[Dict[str, Any]] = None\n    follow_redirects: bool = True\n    verify_ssl: bool = True\n\n    def __init__(\n        self,\n        method: str = \"GET\",\n        headers: Optional[Dict[str, str]] = None,\n        data: Optional[Dict[str, Any]] = None,\n        json: Optional[Dict[str, Any]] = None,\n        follow_redirects: bool = True,\n        verify_ssl: bool = True,\n    ):\n        self.method = method\n        self.headers = headers\n        self.data = data\n        self.json = json\n        self.follow_redirects = follow_redirects\n        self.verify_ssl = verify_ssl\n```\n\n----------------------------------------\n\nTITLE: Customizing Crawl4AI Build with Docker Compose (Bash)\nDESCRIPTION: Shows how to customize the local build process using Docker Compose by setting environment variables like `INSTALL_TYPE=all` (for all features including torch) or `ENABLE_GPU=true` (for GPU support on AMD64).\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Build with all features (includes torch and transformers)\nINSTALL_TYPE=all docker compose up --build -d\n\n# Build with GPU support (for AMD64 platforms)\nENABLE_GPU=true docker compose up --build -d\n```\n\n----------------------------------------\n\nTITLE: Rotating Proxy Implementation in Python\nDESCRIPTION: Example implementation of dynamic proxy rotation using async functions and crawler configuration.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_160\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync def main():\n    browser_config = BrowserConfig()\n    run_config = CrawlerRunConfig()\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        # For each URL, create a new run config with different proxy\n        for url in urls:\n            proxy = await get_next_proxy()\n            # Clone the config and update proxy - this creates a new browser context\n            current_config = run_config.clone(proxy_config=proxy)\n            result = await crawler.arun(url=url, config=current_config)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing ScoringStats Class for URL Score Tracking in Python\nDESCRIPTION: A memory-optimized class for tracking URL scoring statistics, using slots for memory efficiency and lazy initialization for min/max values. Provides methods for updating scores and retrieving aggregate statistics.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_199\n\nLANGUAGE: python\nCODE:\n```\nclass ScoringStats:\n    __slots__ = ('_urls_scored', '_total_score', '_min_score', '_max_score')\n    \n    def __init__(self):\n        self._urls_scored = 0\n        self._total_score = 0.0\n        self._min_score = None  # Lazy initialization\n        self._max_score = None\n    \n    def update(self, score: float) -> None:\n        \"\"\"Optimized update with minimal operations\"\"\"\n        self._urls_scored += 1\n        self._total_score += score\n        \n        # Lazy min/max tracking - only if actually accessed\n        if self._min_score is not None:\n            if score < self._min_score:\n                self._min_score = score\n        if self._max_score is not None:\n            if score > self._max_score:\n                self._max_score = score\n                \n    def get_average(self) -> float:\n        \"\"\"Direct calculation instead of property\"\"\"\n        return self._total_score / self._urls_scored if self._urls_scored else 0.0\n    \n    def get_min(self) -> float:\n        \"\"\"Lazy min calculation\"\"\"\n        if self._min_score is None:\n            self._min_score = self._total_score / self._urls_scored if self._urls_scored else 0.0\n        return self._min_score\n        \n    def get_max(self) -> float:\n        \"\"\"Lazy max calculation\"\"\"\n        if self._max_score is None:\n            self._max_score = self._total_score / self._urls_scored if self._urls_scored else 0.0\n        return self._max_score\n```\n\n----------------------------------------\n\nTITLE: Accessing and Processing Links from CrawlResult in Python\nDESCRIPTION: Shows how to access the extracted internal links from a CrawlResult object. The links field contains dictionaries of internal and external links, each with href, text, and other attributes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(result.links[\"internal\"][:3])  # Show first 3 internal links\n```\n\n----------------------------------------\n\nTITLE: Main Execution Function for Web Crawling Examples in Python\nDESCRIPTION: This function serves as the main entry point for executing various web crawling examples. It calls different crawling functions to demonstrate basic and advanced features of the Crawl4AI library, including dynamic content extraction, browser comparisons, and screenshot capture.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_145\n\nLANGUAGE: python\nCODE:\n```\nasync def main():\n    # Basic examples\n    await simple_crawl()\n    await simple_example_with_running_js_code()\n    await simple_example_with_css_selector()\n\n    # Advanced examples\n    await extract_structured_data_using_css_extractor()\n    await extract_structured_data_using_llm(\n        \"openai/gpt-4o\", os.getenv(\"OPENAI_API_KEY\")\n    )\n    await crawl_dynamic_content_pages_method_1()\n    await crawl_dynamic_content_pages_method_2()\n\n    # Browser comparisons\n    await crawl_custom_browser_type()\n\n    # Screenshot example\n    await capture_and_save_screenshot(\n        \"https://www.example.com\",\n        os.path.join(__location__, \"tmp/example_screenshot.jpg\")\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: File-based Storage State Implementation\nDESCRIPTION: Example demonstrating how to use storage_state with a JSON file path\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(\n        headless=True,\n        storage_state=\"mystate.json\"  # Uses a JSON file instead of a dictionary\n    ) as crawler:\n        result = await crawler.arun(url='https://example.com/protected')\n        if result.success:\n            print(\"Crawl succeeded with pre-loaded session data!\")\n            print(\"Page HTML length:\", len(result.html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Performing Hierarchical Clustering on Sentences in Python\nDESCRIPTION: Implements hierarchical clustering on a list of sentences using their embeddings. It uses scipy for clustering and returns cluster labels for the input sentences.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_62\n\nLANGUAGE: python\nCODE:\n```\ndef hierarchical_clustering(self, sentences: List[str], embeddings=None):\n        from scipy.cluster.hierarchy import linkage, fcluster\n        from scipy.spatial.distance import pdist\n\n        self.timer = time.time()\n        embeddings = self.get_embeddings(sentences, bypass_buffer=True)\n        distance_matrix = pdist(embeddings, \"cosine\")\n        linked = linkage(distance_matrix, method=self.linkage_method)\n        labels = fcluster(linked, self.max_dist, criterion=\"distance\")\n        return labels\n```\n\n----------------------------------------\n\nTITLE: Filtering Clusters by Word Count in Python\nDESCRIPTION: Filters text clusters based on a minimum word count threshold. It iterates through cluster IDs and their associated text lists, counts the total words in each cluster, and retains only those clusters meeting the minimum threshold.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_64\n\nLANGUAGE: python\nCODE:\n```\n# Concatenate texts for analysis\nfull_text = \" \".join(texts)\n# Count words\nword_count = len(full_text.split())\n\n# Keep clusters with word count above the threshold\nif word_count >= self.word_count_threshold:\n    filtered_clusters[cluster_id] = texts\n```\n\n----------------------------------------\n\nTITLE: Running Pre-built Crawl4AI Image with Docker Compose (Bash)\nDESCRIPTION: Uses `docker compose up -d` to pull and run a specific pre-built Crawl4AI image from Docker Hub. The `IMAGE` environment variable specifies the image tag. Docker Compose handles architecture selection automatically.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Pulls and runs the release candidate from Docker Hub\n# Automatically selects the correct architecture\nIMAGE=unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number docker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Browser Comparison for Web Crawling in Python\nDESCRIPTION: This function compares the performance of different browser types (Firefox, WebKit, and Chromium) for web crawling. It measures the time taken to crawl a sample website using each browser type and prints the results.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_142\n\nLANGUAGE: python\nCODE:\n```\nasync def crawl_custom_browser_type():\n    print(\"\\n--- Browser Comparison ---\")\n\n    # Firefox\n    browser_config_firefox = BrowserConfig(browser_type=\"firefox\", headless=True)\n    start = time.time()\n    async with AsyncWebCrawler(config=browser_config_firefox) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.example.com\",\n            config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS),\n        )\n        print(\"Firefox:\", time.time() - start)\n        print(result.markdown[:500])\n\n    # WebKit\n    browser_config_webkit = BrowserConfig(browser_type=\"webkit\", headless=True)\n    start = time.time()\n    async with AsyncWebCrawler(config=browser_config_webkit) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.example.com\",\n            config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS),\n        )\n        print(\"WebKit:\", time.time() - start)\n        print(result.markdown[:500])\n\n    # Chromium (default)\n    browser_config_chromium = BrowserConfig(browser_type=\"chromium\", headless=True)\n    start = time.time()\n    async with AsyncWebCrawler(config=browser_config_chromium) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.example.com\",\n            config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS),\n        )\n        print(\"Chromium:\", time.time() - start)\n        print(result.markdown[:500])\n```\n\n----------------------------------------\n\nTITLE: Displaying Summary Output for Crawl4AI Stress Testing in Python\nDESCRIPTION: This code snippet demonstrates the format of the final summary output after completing a Crawl4AI stress test. It includes test configuration, results, performance metrics, and memory usage statistics.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/tests/memory/README.md#2025-04-23_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n================================================================================\nTest Completed\n================================================================================\nTest ID: 20250418_103015\nConfiguration: 100 URLs, 16 max sessions, Chunk: 10, Stream: False, Monitor: DETAILED\nResults: 100 successful, 0 failed (100 processed, 100.0% success)\nPerformance: 5.85 seconds total, 17.09 URLs/second avg\nMemory Usage: Start: 50.1 MB, End: 75.3 MB, Max: 78.1 MB, Growth: 25.2 MB\nResults summary saved to reports/test_summary_20250418_103015.json\n```\n\n----------------------------------------\n\nTITLE: Implementing Serialization Methods for CrawlerRunConfig in Python\nDESCRIPTION: This snippet defines methods for serializing and deserializing CrawlerRunConfig objects. It includes a dump() method to convert the object to a dictionary, a load() method to create an object from a dictionary, and a to_dict() method that returns a dictionary representation of the object.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ndef dump(self) -> dict:\n    # Serialize the object to a dictionary\n    return to_serializable_dict(self)\n\n@staticmethod\ndef load(data: dict) -> \"CrawlerRunConfig\":\n    # Deserialize the object from a dictionary\n    config = from_serializable_dict(data)\n    if isinstance(config, CrawlerRunConfig):\n        return config\n    return CrawlerRunConfig.from_kwargs(config)\n\ndef to_dict(self):\n    return {\n        \"word_count_threshold\": self.word_count_threshold,\n        \"extraction_strategy\": self.extraction_strategy,\n        \"chunking_strategy\": self.chunking_strategy,\n        \"markdown_generator\": self.markdown_generator,\n        \"only_text\": self.only_text,\n        \"css_selector\": self.css_selector,\n        \"target_elements\": self.target_elements,\n        \"excluded_tags\": self.excluded_tags,\n        \"excluded_selector\": self.excluded_selector,\n        \"keep_data_attributes\": self.keep_data_attributes,\n        \"keep_attrs\": self.keep_attrs,\n        \"remove_forms\": self.remove_forms,\n        \"prettiify\": self.prettiify,\n        \"parser_type\": self.parser_type,\n        \"scraping_strategy\": self.scraping_strategy,\n        \"proxy_config\": self.proxy_config,\n        \"proxy_rotation_strategy\": self.proxy_rotation_strategy,\n        \"fetch_ssl_certificate\": self.fetch_ssl_certificate,\n        \"cache_mode\": self.cache_mode,\n        \"session_id\": self.session_id,\n        \"bypass_cache\": self.bypass_cache,\n        \"disable_cache\": self.disable_cache,\n        \"no_cache_read\": self.no_cache_read,\n        \"no_cache_write\": self.no_cache_write,\n        \"shared_data\": self.shared_data,\n        \"wait_until\": self.wait_until,\n        \"page_timeout\": self.page_timeout,\n        \"wait_for\": self.wait_for,\n        \"wait_for_images\": self.wait_for_images,\n        \"delay_before_return_html\": self.delay_before_return_html,\n        \"mean_delay\": self.mean_delay,\n        \"max_range\": self.max_range,\n        \"semaphore_count\": self.semaphore_count,\n        \"js_code\": self.js_code,\n        \"js_only\": self.js_only,\n        \"ignore_body_visibility\": self.ignore_body_visibility,\n        \"scan_full_page\": self.scan_full_page,\n        \"scroll_delay\": self.scroll_delay,\n        \"process_iframes\": self.process_iframes,\n        \"remove_overlay_elements\": self.remove_overlay_elements,\n        \"simulate_user\": self.simulate_user,\n        \"override_navigator\": self.override_navigator,\n        \"magic\": self.magic,\n        \"adjust_viewport_to_content\": self.adjust_viewport_to_content,\n        \"screenshot\": self.screenshot,\n        \"screenshot_wait_for\": self.screenshot_wait_for,\n        \"screenshot_height_threshold\": self.screenshot_height_threshold,\n        \"pdf\": self.pdf,\n        \"capture_mhtml\": self.capture_mhtml,\n        \"image_description_min_word_threshold\": self.image_description_min_word_threshold,\n        \"image_score_threshold\": self.image_score_threshold,\n        \"table_score_threshold\": self.table_score_threshold,\n        \"exclude_all_images\": self.exclude_all_images,\n        \"exclude_external_images\": self.exclude_external_images,\n        \"exclude_social_media_domains\": self.exclude_social_media_domains,\n        \"exclude_external_links\": self.exclude_external_links,\n        \"exclude_social_media_links\": self.exclude_social_media_links,\n        \"exclude_domains\": self.exclude_domains,\n        \"exclude_internal_links\": self.exclude_internal_links,\n        \"verbose\": self.verbose,\n        \"log_console\": self.log_console,\n        \"capture_network_requests\": self.capture_network_requests,\n        \"capture_console_messages\": self.capture_console_messages,\n        \"method\": self.method,\n        \"stream\": self.stream,\n        \"check_robots_txt\": self.check_robots_txt,\n        \"user_agent\": self.user_agent,\n        \"user_agent_mode\": self.user_agent_mode,\n        \"user_agent_generator_config\": self.user_agent_generator_config,\n        \"deep_crawl_strategy\": self.deep_crawl_strategy,\n        \"url\": self.url,\n        \"experimental\": self.experimental,\n    }\n```\n\n----------------------------------------\n\nTITLE: Running Crawl4AI diagnostics\nDESCRIPTION: Executes the diagnostics command to verify everything is functioning correctly. It checks Python version compatibility, verifies Playwright installation, and inspects environment variables or library conflicts.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncrawl4ai-doctor\n```\n\n----------------------------------------\n\nTITLE: Analyzing Console Messages from CrawlResult\nDESCRIPTION: Demonstrates how to process browser console messages captured during a crawl. The code shows counting messages by type and displaying error messages, which are typically the most important for debugging.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nif result.console_messages:\n    # Count messages by type\n    message_types = {}\n    for msg in result.console_messages:\n        msg_type = msg.get(\"type\", \"unknown\")\n        message_types[msg_type] = message_types.get(msg_type, 0) + 1\n    \n    print(f\"Message type counts: {message_types}\")\n    \n    # Display errors (which are usually most important)\n    for msg in result.console_messages:\n        if msg.get(\"type\") == \"error\":\n            print(f\"Error: {msg.get('text')}\")\n```\n\n----------------------------------------\n\nTITLE: Scoring URL Quality for SEO in Python\nDESCRIPTION: A method that evaluates URL quality based on several factors. It penalizes long paths, numbers in paths, URL parameters, and underscores, returning a composite score between 0.0 and 1.0.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_197\n\nLANGUAGE: python\nCODE:\n```\ndef _score_url_quality(self, parsed_url) -> float:\n    score = 1.0\n    path = parsed_url.path.lower()\n\n    # Penalty factors\n    if len(path) > 80:\n        score *= 0.7\n    if re.search(r\"\\d{4}\", path):\n        score *= 0.8  # Numbers in path\n    if parsed_url.query:\n        score *= 0.6  # URL parameters\n    if \"_\" in path:\n        score *= 0.9  # Underscores vs hyphens\n\n    return score\n```\n\n----------------------------------------\n\nTITLE: Network Response Event Structure in Crawl4AI (JSON)\nDESCRIPTION: This JSON structure represents a typical network response event captured by Crawl4AI, including status code, headers, and timing information.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/network-console-capture.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"event_type\": \"response\",\n  \"url\": \"https://example.com/api/data.json\",\n  \"status\": 200,\n  \"status_text\": \"OK\",\n  \"headers\": {\"Content-Type\": \"application/json\", \"Cache-Control\": \"...\"},\n  \"from_service_worker\": false,\n  \"request_timing\": {\"requestTime\": 1234.56, \"receiveHeadersEnd\": 1234.78},\n  \"timestamp\": 1633456789.456\n}\n```\n\n----------------------------------------\n\nTITLE: Main Runner Function for Crawl4AI Demos in Python\nDESCRIPTION: An async main function that sequentially runs all the demo functions for the Crawl4AI library. It demonstrates the complete set of capabilities from basic crawling to advanced features like proxy rotation and raw HTML processing. The function serves as the entry point for the demo script.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_159\n\nLANGUAGE: python\nCODE:\n```\nasync def main():\n    \"\"\"Run all demo functions sequentially\"\"\"\n    print(\"=== Comprehensive Crawl4AI Demo ===\")\n    print(\"Note: Some examples require API keys or other configurations\")\n\n    # Run all demos\n    await demo_basic_crawl()\n    await demo_parallel_crawl()\n    await demo_fit_markdown()\n    await demo_llm_structured_extraction_no_schema()\n    await demo_css_structured_extraction_no_schema()\n    await demo_deep_crawl()\n    await demo_js_interaction()\n    await demo_media_and_links()\n    await demo_screenshot_and_pdf()\n    # # await demo_proxy_rotation()\n    await demo_raw_html_and_file()\n\n    # Clean up any temp files that may have been created\n    print(\"\\n=== Demo Complete ===\")\n    print(\"Check for any generated files (screenshots, PDFs) in the current directory\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Builtin Browser in Crawl4AI via CLI\nDESCRIPTION: This snippet provides CLI commands for troubleshooting issues with the builtin browser, including checking status, restarting, and stopping the browser when problems persist.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Check the browser status:\ncrwl browser status\n\n# Try restarting it:\ncrwl browser restart\n\n# If problems persist, stop it and let Crawl4AI start a fresh one:\ncrwl browser stop\n```\n\n----------------------------------------\n\nTITLE: Abstract Markdown Generation Strategy Class\nDESCRIPTION: Abstract base class defining the interface for markdown generation strategies with configurable content filtering and options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_108\n\nLANGUAGE: python\nCODE:\n```\nclass MarkdownGenerationStrategy(ABC):\n    \"\"\"Abstract base class for markdown generation strategies.\"\"\"\n\n    def __init__(\n        self,\n        content_filter: Optional[RelevantContentFilter] = None,\n        options: Optional[Dict[str, Any]] = None,\n        verbose: bool = False,\n        content_source: str = \"cleaned_html\",\n    ):\n        self.content_filter = content_filter\n        self.options = options or {}\n        self.verbose = verbose\n        self.content_source = content_source\n\n    @abstractmethod\n    def generate_markdown(\n        self,\n        input_html: str,\n        base_url: str = \"\",\n        html2text_options: Optional[Dict[str, Any]] = None,\n        content_filter: Optional[RelevantContentFilter] = None,\n        citations: bool = True,\n        **kwargs,\n    ) -> MarkdownGenerationResult:\n        \"\"\"Generate markdown from the selected input HTML.\"\"\"\n        pass\n```\n\n----------------------------------------\n\nTITLE: Defining Crawl4AI Configuration Grammar in JSON\nDESCRIPTION: Specifies the grammar for Crawl4AI configuration objects, including type-params pattern, simple and complex values, and dictionary wrapping.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_56\n\nLANGUAGE: json\nCODE:\n```\nconfig := {\n    \"type\": string,\n    \"params\": {\n        key: simple_value | complex_value\n    }\n}\n\nsimple_value := string | number | boolean | [simple_value]\ncomplex_value := config | dict_value\n\ndict_value := {\n    \"type\": \"dict\",\n    \"value\": object\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Caching Selector Functions for HTML Parsing in Python\nDESCRIPTION: This class method creates and caches selector functions for efficient HTML parsing. It uses CSS selectors and XPath, with fallback mechanisms for special cases and error handling.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_79\n\nLANGUAGE: Python\nCODE:\n```\ndef _get_selector(self, selector_str):\n    \"\"\"Get a selector function that works within the context of an element\"\"\"\n    if selector_str not in self._selector_cache:\n        from lxml.cssselect import CSSSelector\n        try:\n            # Store both the compiled selector and its xpath translation\n            compiled = CSSSelector(selector_str)\n            \n            # Create a function that will apply this selector appropriately\n            def select_func(element):\n                try:\n                    # First attempt: direct CSS selector application\n                    results = compiled(element)\n                    if results:\n                        return results\n                    \n                    # Second attempt: contextual XPath selection\n                    # Convert the root-based XPath to a context-based XPath\n                    xpath = compiled.path\n                    \n                    # If the XPath already starts with descendant-or-self, handle it specially\n                    if xpath.startswith('descendant-or-self::'):\n                        context_xpath = xpath\n                    else:\n                        # For normal XPath expressions, make them relative to current context\n                        context_xpath = f\"./{xpath.lstrip('/')}\"\n                    \n                    results = element.xpath(context_xpath)\n                    if results:\n                        return results\n                    \n                    # Final fallback: simple descendant search for common patterns\n                    if 'nth-child' in selector_str:\n                        # Handle td:nth-child(N) pattern\n                        import re\n                        match = re.search(r'td:nth-child\\((\\d+)\\)', selector_str)\n                        if match:\n                            col_num = match.group(1)\n                            sub_selector = selector_str.split(')', 1)[-1].strip()\n                            if sub_selector:\n                                return element.xpath(f\".//td[{col_num}]//{sub_selector}\")\n                            else:\n                                return element.xpath(f\".//td[{col_num}]\")\n                    \n                    # Last resort: try each part of the selector separately\n                    parts = selector_str.split()\n                    if len(parts) > 1 and parts[-1]:\n                        return element.xpath(f\".//{parts[-1]}\")\n                        \n                    return []\n                except Exception as e:\n                    if self.verbose:\n                        print(f\"Error applying selector '{selector_str}': {e}\")\n                    return []\n```\n\n----------------------------------------\n\nTITLE: Implementing MHTML Capture Method in AsyncPlaywrightCrawlerStrategy in Python\nDESCRIPTION: Added a method to capture MHTML content using Chrome DevTools Protocol (CDP) via Playwright's CDP session API in the AsyncPlaywrightCrawlerStrategy class.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/JOURNAL.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass AsyncPlaywrightCrawlerStrategy:\n    async def capture_mhtml(self):\n        # Implementation details not provided in the original text\n```\n\n----------------------------------------\n\nTITLE: Displaying BrowserConfig Structure in Python\nDESCRIPTION: Simple Python example showing how to create a BrowserConfig object and display its JSON structure to understand the API configuration pattern.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai import BrowserConfig\n\n# Create a config and see its structure\nconfig = BrowserConfig(headless=True)\nprint(config.dump())\n```\n\n----------------------------------------\n\nTITLE: Removing Playwright Page Event Listeners in Python\nDESCRIPTION: Code that removes event listeners from a Playwright page instance after crawling. It handles cleanup for network request capture listeners (response and requestfailed events) and console message capture listeners (console and pageerror events).\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/prompts/prompt_net_requests.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npage.remove_listener(\"response\", handle_response_capture)\npage.remove_listener(\"requestfailed\", handle_request_failed_capture)\nif config.capture_console_messages:\n    page.remove_listener(\"console\", handle_console_capture)\n    page.remove_listener(\"pageerror\", handle_pageerror_capture)\n# Also remove logging listeners if they were attached\nif config.log_console:\n    # Need to figure out how to remove the lambdas if necessary,\n    # or ensure they don't cause issues on close. Often, it's fine.\n    pass\n\nawait page.close()\n```\n\n----------------------------------------\n\nTITLE: Defining the SSLCertificate Class Structure in Python\nDESCRIPTION: The core structure of the SSLCertificate class, showing its main methods and properties. This class represents an SSL certificate with functionality to fetch, parse, and export certificate data in various formats.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass SSLCertificate:\n    \"\"\"\n    Represents an SSL certificate with methods to export in various formats.\n\n    Main Methods:\n    - from_url(url, timeout=10)\n    - from_file(file_path)\n    - from_binary(binary_data)\n    - to_json(filepath=None)\n    - to_pem(filepath=None)\n    - to_der(filepath=None)\n    ...\n\n    Common Properties:\n    - issuer\n    - subject\n    - valid_from\n    - valid_until\n    - fingerprint\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Clone Method for CrawlerRunConfig in Python\nDESCRIPTION: This snippet defines a clone() method for the CrawlerRunConfig class. It allows creating a copy of the configuration with updated values, providing flexibility in modifying crawler settings.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ndef clone(self, **kwargs):\n    \"\"\"Create a copy of this configuration with updated values.\n\n    Args:\n        **kwargs: Key-value pairs of configuration options to update\n\n    Returns:\n        CrawlerRunConfig: A new instance with the specified updates\n\n    Example:\n        ```python\n        # Create a new config with streaming enabled\n        stream_config = config.clone(stream=True)\n\n        # Create a new config with multiple updates\n        new_config = config.clone(\n            stream=True,\n            cache_mode=CacheMode.BYPASS,\n            verbose=True\n        )\n        ```\n    \"\"\"\n    config_dict = self.to_dict()\n    config_dict.update(kwargs)\n    return CrawlerRunConfig.from_kwargs(config_dict)\n```\n\n----------------------------------------\n\nTITLE: Converting BrowserConfig to Dictionary in Python\nDESCRIPTION: This method converts the BrowserConfig instance to a dictionary, allowing for easy serialization and data transfer.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef to_dict(self):\n    result = {\n        \"browser_type\": self.browser_type,\n        \"headless\": self.headless,\n        \"browser_mode\": self.browser_mode,\n        \"use_managed_browser\": self.use_managed_browser,\n        \"cdp_url\": self.cdp_url,\n        # ... (other attributes)\n    }\n    return result\n```\n\n----------------------------------------\n\nTITLE: Filtering Clusters by Word Count in Python\nDESCRIPTION: Defines a method to filter clusters based on a word count threshold. It takes a dictionary of clusters and returns a filtered dictionary containing only clusters that meet the word count criteria.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_63\n\nLANGUAGE: python\nCODE:\n```\ndef filter_clusters_by_word_count(\n        self, clusters: Dict[int, List[str]]\n    ) -> Dict[int, List[str]]:\n        filtered_clusters = {}\n        for cluster_id, texts in clusters.items():\n```\n\n----------------------------------------\n\nTITLE: Testing MCP WebSocket Connection with Python Script (Bash)\nDESCRIPTION: Demonstrates how to run a Python test script (`tests/mcp/test_mcp_socket.py`) from the repository root directory to verify the MCP WebSocket connection to the Crawl4AI server.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n# From the repository root\npython tests/mcp/test_mcp_socket.py\n```\n\n----------------------------------------\n\nTITLE: Direct Usage of Stress Testing Script with Custom Parameters\nDESCRIPTION: Examples of directly using the test_stress_sdk.py script with various parameters for fine-grained control, including URL count, session limits, and cleanup options.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/tests/memory/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Test with 200 URLs and 32 max concurrent sessions\npython test_stress_sdk.py --urls 200 --max-sessions 32 --chunk-size 40\n\n# Clean up previous test data first\npython test_stress_sdk.py --clean-reports --clean-site --urls 100 --max-sessions 16 --chunk-size 20\n\n# Change the HTTP server port and use aggregated monitor\npython test_stress_sdk.py --port 8088 --urls 100 --max-sessions 16 --monitor-mode AGGREGATED\n\n# Enable streaming mode and use rate limiting\npython test_stress_sdk.py --urls 50 --max-sessions 8 --stream --use-rate-limiter\n\n# Change report output location\npython test_stress_sdk.py --report-path custom_reports --urls 100 --max-sessions 16\n```\n\n----------------------------------------\n\nTITLE: Launching Playwright Chromium with Custom Data Directory\nDESCRIPTION: Platform-specific commands for launching Playwright's Chromium browser with a custom user data directory.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_137\n\nLANGUAGE: bash\nCODE:\n```\n~/.cache/ms-playwright/chromium-1234/chrome-linux/chrome \\\n    --user-data-dir=/home/<you>/my_chrome_profile\n```\n\nLANGUAGE: bash\nCODE:\n```\n~/Library/Caches/ms-playwright/chromium-1234/chrome-mac/Chromium.app/Contents/MacOS/Chromium \\\n    --user-data-dir=/Users/<you>/my_chrome_profile\n```\n\nLANGUAGE: powershell\nCODE:\n```\n\"C:\\Users\\<you>\\AppData\\Local\\ms-playwright\\chromium-1234\\chrome-win\\chrome.exe\" ^\n    --user-data-dir=\"C:\\Users\\<you>\\my_chrome_profile\"\n```\n\n----------------------------------------\n\nTITLE: Loading Configuration Files in Python\nDESCRIPTION: Loads and parses configuration files in YAML or JSON format. Returns an empty dictionary if no path is provided.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef load_config_file(path: Optional[str]) -> dict:\n    if not path:\n        return {}\n    \n    try:\n        with open(path) as f:\n            if path.endswith((\".yaml\", \".yml\")):\n                return yaml.safe_load(f)\n            return json.load(f)\n    except Exception as e:\n        raise click.BadParameter(f'Error loading config file {path}: {str(e)}')\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for FastAPI Web Application\nDESCRIPTION: A comprehensive list of Python packages and their minimum version requirements needed for a web application. The dependencies include FastAPI for the web framework, uvicorn and gunicorn for serving the application, authentication libraries, validation tools, and various utilities for features like server-sent events and WebSockets.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nfastapi>=0.115.12\nuvicorn>=0.34.2\ngunicorn>=23.0.0\nslowapi==0.1.9\nprometheus-fastapi-instrumentator>=7.1.0\nredis>=5.2.1\njwt>=1.3.1\ndnspython>=2.7.0\nemail-validator==2.2.0\nsse-starlette==2.2.1\npydantic>=2.11\nrank-bm25==0.2.2\nanyio==4.9.0\nPyJWT==2.10.1\nmcp>=1.6.0\nwebsockets>=15.0.1\n```\n\n----------------------------------------\n\nTITLE: Compiling Regex Pattern for Markdown Links\nDESCRIPTION: Pre-compiles a regex pattern for matching markdown links including optional titles.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_106\n\nLANGUAGE: python\nCODE:\n```\nLINK_PATTERN = re.compile(r'!?\\[([^\\]]+)\\]\\(([^)]+?)(?:\\s+\"([^\"]*)\")?\\)')\n```\n\n----------------------------------------\n\nTITLE: Implementing BrowserConfig Class in Python\nDESCRIPTION: Configuration class for browser setup in AsyncPlaywrightCrawlerStrategy. Manages browser type, headless mode, debugging settings, and context persistence.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass BrowserConfig:\n    def __init__(self):\n        \"\"\"Configuration class for setting up a browser instance and its context in AsyncPlaywrightCrawlerStrategy.\n\n        This class centralizes all parameters that affect browser and context creation. Instead of passing\n        scattered keyword arguments, users can instantiate and modify this configuration object. The crawler\n        code will then reference these settings to initialize the browser in a consistent, documented manner.\n\n        Attributes:\n            browser_type (str): The type of browser to launch. Supported values: \"chromium\", \"firefox\", \"webkit\".\n                            Default: \"chromium\".\n            headless (bool): Whether to run the browser in headless mode (no visible GUI).\n                         Default: True.\n            browser_mode (str): Determines how the browser should be initialized:\n                           \"builtin\" - use the builtin CDP browser running in background\n                           \"dedicated\" - create a new dedicated browser instance each time\n                           \"cdp\" - use explicit CDP settings provided in cdp_url\n                           \"docker\" - run browser in Docker container with isolation\n                           Default: \"dedicated\"\n            use_managed_browser (bool): Launch the browser using a managed approach (e.g., via CDP), allowing\n                                    advanced manipulation. Default: False.\n            cdp_url (str): URL for the Chrome DevTools Protocol (CDP) endpoint. Default: \"ws://localhost:9222/devtools/browser/\".\n            debugging_port (int): Port for the browser debugging protocol. Default: 9222.\n            use_persistent_context (bool): Use a persistent browser context (like a persistent profile).\n                                       Automatically sets use_managed_browser=True. Default: False.\n            user_data_dir (str or None): Path to a user data directory for persistent sessions. If None, a\n                                     temporary directory may be used. Default: None.\n            chrome_channel (str): The Chrome channel to launch (e.g., \"chrome\", \"msedge\"). Only applies if browser_type\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running Crawl4AI in Docker (experimental)\nDESCRIPTION: Pulls and runs the experimental Crawl4AI Docker image. This approach allows making POST requests to the local server on port 11235 to perform crawls, but is not recommended for production use.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull unclecode/crawl4ai:basic\ndocker run -p 11235:11235 unclecode/crawl4ai:basic\n```\n\n----------------------------------------\n\nTITLE: Defining Content Filtering Methods in Python\nDESCRIPTION: Implements methods for filtering web content including an abstract filter_content method, extract_page_query for retrieving page metadata, and is_excluded for identifying non-relevant elements in HTML.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_90\n\nLANGUAGE: python\nCODE:\n```\n    @abstractmethod\n    def filter_content(self, html: str) -> List[str]:\n        \"\"\"Abstract method to be implemented by specific filtering strategies\"\"\"\n        pass\n\n    def extract_page_query(self, soup: BeautifulSoup, body: Tag) -> str:\n        \"\"\"Common method to extract page metadata with fallbacks\"\"\"\n        if self.user_query:\n            return self.user_query\n\n        query_parts = []\n\n        # Title\n        try:\n            title = soup.title.string\n            if title:\n                query_parts.append(title)\n        except Exception:\n            pass\n\n        if soup.find(\"h1\"):\n            query_parts.append(soup.find(\"h1\").get_text())\n\n        # Meta tags\n        temp = \"\"\n        for meta_name in [\"keywords\", \"description\"]:\n            meta = soup.find(\"meta\", attrs={\"name\": meta_name})\n            if meta and meta.get(\"content\"):\n                query_parts.append(meta[\"content\"])\n                temp += meta[\"content\"]\n\n        # If still empty, grab first significant paragraph\n        if not temp:\n            # Find the first tag P thatits text contains more than 50 characters\n            for p in body.find_all(\"p\"):\n                if len(p.get_text()) > 150:\n                    query_parts.append(p.get_text()[:150])\n                    break\n\n        return \" \".join(filter(None, query_parts))\n\n    def is_excluded(self, tag: Tag) -> bool:\n        \"\"\"Common method for exclusion logic\"\"\"\n        if tag.name in self.excluded_tags:\n            return True\n        class_id = \" \".join(\n            filter(None, [\" \".join(tag.get(\"class\", [])), tag.get(\"id\", \"\")])\n        )\n        return bool(self.negative_patterns.search(class_id))\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Web Crawling in Python\nDESCRIPTION: This snippet imports necessary modules and classes from the crawl4ai library and other standard Python libraries for web crawling and data processing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_146\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport os\nimport json\nimport base64\nfrom pathlib import Path\nfrom typing import List\nfrom crawl4ai import ProxyConfig\n\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, CrawlResult\nfrom crawl4ai import RoundRobinProxyStrategy\nfrom crawl4ai import JsonCssExtractionStrategy, LLMExtractionStrategy\nfrom crawl4ai import LLMConfig\nfrom crawl4ai import PruningContentFilter, BM25ContentFilter\nfrom crawl4ai import DefaultMarkdownGenerator\nfrom crawl4ai import BFSDeepCrawlStrategy, DomainFilter, FilterChain\nfrom crawl4ai import BrowserConfig\n\n__cur_dir__ = Path(__file__).parent\n```\n\n----------------------------------------\n\nTITLE: Installing Crawl4AI with all optional features\nDESCRIPTION: Installs Crawl4AI with all optional features including PyTorch and Transformers, followed by running the setup command.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai[all]\ncrawl4ai-setup\n```\n\n----------------------------------------\n\nTITLE: Initializing BrowserConfig Class in Python\nDESCRIPTION: This snippet defines the __init__ method for the BrowserConfig class, setting up various browser configuration options with default values and handling special cases for different browser types and modes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef __init__(\n    self,\n    browser_type: str = \"chromium\",\n    headless: bool = True,\n    browser_mode: str = \"dedicated\",\n    use_managed_browser: bool = False,\n    cdp_url: str = None,\n    use_persistent_context: bool = False,\n    user_data_dir: str = None,\n    chrome_channel: str = \"chromium\",\n    channel: str = \"chromium\",\n    proxy: str = None,\n    proxy_config: Union[ProxyConfig, dict, None] = None,\n    viewport_width: int = 1080,\n    viewport_height: int = 600,\n    viewport: dict = None,\n    accept_downloads: bool = False,\n    downloads_path: str = None,\n    storage_state: Union[str, dict, None] = None,\n    ignore_https_errors: bool = True,\n    java_script_enabled: bool = True,\n    sleep_on_close: bool = False,\n    verbose: bool = True,\n    cookies: list = None,\n    headers: dict = None,\n    user_agent: str = (\n        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36\"\n    ),\n    user_agent_mode: str = \"\",\n    user_agent_generator_config: dict = {},\n    text_mode: bool = False,\n    light_mode: bool = False,\n    extra_args: list = None,\n    debugging_port: int = 9222,\n    host: str = \"localhost\",\n):\n    # ... (initialization of attributes)\n\n    fa_user_agenr_generator = ValidUAGenerator()\n    if self.user_agent_mode == \"random\":\n        self.user_agent = fa_user_agenr_generator.generate(\n            **(self.user_agent_generator_config or {})\n        )\n    else:\n        pass\n\n    self.browser_hint = UAGen.generate_client_hints(self.user_agent)\n    self.headers.setdefault(\"sec-ch-ua\", self.browser_hint)\n\n    # ... (setting browser management flags based on browser_mode)\n```\n\n----------------------------------------\n\nTITLE: Setting up `.llm.env` File from Example for Docker Compose (Bash)\nDESCRIPTION: Shows how to copy the example environment file (`deploy/docker/.llm.env.example`) to the project root directory as `.llm.env`. Users are then instructed to edit this file to add their actual API keys. This is part of the Docker Compose setup.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Make sure you are in the 'crawl4ai' root directory\ncp deploy/docker/.llm.env.example .llm.env\n\n# Now edit .llm.env and add your API keys\n```\n\n----------------------------------------\n\nTITLE: Implementing CrawlResult Initialization and Markdown Property Handling\nDESCRIPTION: Defines the initialization method for the CrawlResult class and properties for handling markdown content with backward compatibility. It transforms markdown data into a StringCompatibleMarkdown object that behaves like a string while providing access to additional attributes.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_82\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, **data):\n    markdown_result = data.pop('markdown', None)\n    super().__init__(**data)\n    if markdown_result is not None:\n        self._markdown = (\n            MarkdownGenerationResult(**markdown_result)\n            if isinstance(markdown_result, dict)\n            else markdown_result\n        )\n\n@property\ndef markdown(self):\n    \"\"\"\n    Property that returns a StringCompatibleMarkdown object that behaves like\n    a string but also provides access to MarkdownGenerationResult attributes.\n    \n    This approach allows backward compatibility with code that expects 'markdown'\n    to be a string, while providing access to the full MarkdownGenerationResult.\n    \"\"\"\n    if self._markdown is None:\n        return None\n    return StringCompatibleMarkdown(self._markdown)\n\n@markdown.setter\ndef markdown(self, value):\n    \"\"\"\n    Setter for the markdown property.\n    \"\"\"\n    self._markdown = value\n```\n\n----------------------------------------\n\nTITLE: Performing Health Check using cURL in Bash\nDESCRIPTION: A simple Bash command using `curl` to make an HTTP GET request to the `/health` endpoint of the service running on `localhost:11235`. This is used to quickly verify if the service is running and responsive. No specific dependencies beyond a standard `curl` installation are required.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:11235/health\n```\n\n----------------------------------------\n\nTITLE: Stopping and Removing Manually Run Crawl4AI Container (Bash)\nDESCRIPTION: Provides the command to stop and remove the container named `crawl4ai-standalone`, which was started manually using `docker run`.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ndocker stop crawl4ai-standalone && docker rm crawl4ai-standalone\n```\n\n----------------------------------------\n\nTITLE: Implementing ContentTypeScorer Class for URL File Type Analysis in Python\nDESCRIPTION: A scorer that evaluates URLs based on file extensions and content types, with customizable weights for different extensions. Uses a hybrid approach with exact matching for common extensions and regex patterns for complex cases.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_204\n\nLANGUAGE: python\nCODE:\n```\nclass ContentTypeScorer(URLScorer):\n    __slots__ = ('_weight', '_exact_types', '_regex_types')\n\n    def __init__(self, type_weights: Dict[str, float], weight: float = 1.0):\n        \"\"\"Initialize scorer with type weights map.\n        \n        Args:\n            type_weights: Dict mapping file extensions/patterns to scores (e.g. {'.html$': 1.0})\n            weight: Overall weight multiplier for this scorer\n        \"\"\"\n        super().__init__(weight=weight)\n        self._exact_types = {}  # Fast lookup for simple extensions\n        self._regex_types = []  # Fallback for complex patterns\n        \n        # Split into exact vs regex matchers for performance\n        for pattern, score in type_weights.items():\n            if pattern.startswith('.') and pattern.endswith('$'):\n                ext = pattern[1:-1]\n                self._exact_types[ext] = score\n            else:\n                self._regex_types.append((re.compile(pattern), score))\n                \n        # Sort complex patterns by score for early exit\n        self._regex_types.sort(key=lambda x: -x[1])\n\n    @staticmethod\n    @lru_cache(maxsize=10000)\n    def _quick_extension(url: str) -> str:\n        \"\"\"Extract file extension ultra-fast without regex/splits.\n        \n        Handles:\n        - Basic extensions: \"example.html\" -> \"html\"\n        - Query strings: \"page.php?id=1\" -> \"php\" \n        - Fragments: \"doc.pdf#page=1\" -> \"pdf\"\n        - Path params: \"file.jpg;width=100\" -> \"jpg\"\n        \n        Args:\n            url: URL to extract extension from\n            \n        Returns:\n            Extension without dot, or empty string if none found\n        \"\"\"\n        pos = url.rfind('.')\n        if pos == -1:\n            return ''\n        \n        # Find first non-alphanumeric char after extension\n        end = len(url)\n        for i in range(pos + 1, len(url)):\n            c = url[i]\n            # Stop at query string, fragment, path param or any non-alphanumeric\n            if c in '?#;' or not c.isalnum():\n                end = i\n                break\n                \n        return url[pos + 1:end].lower()\n\n    @lru_cache(maxsize=10000)\n    def _calculate_score(self, url: str) -> float:\n        \"\"\"Calculate content type score for URL.\n        \n        Uses staged approach:\n        1. Try exact extension match (fast path)\n        2. Fall back to regex patterns if needed\n        \n        Args:\n            url: URL to score\n            \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Adding MHTML Field to CrawlResult Model in Python\nDESCRIPTION: Updated the CrawlResult model to include an optional 'mhtml' field for storing the captured MHTML content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/JOURNAL.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass CrawlResult:\n    mhtml: Optional[str] = None\n```\n\n----------------------------------------\n\nTITLE: Fast URL Joining Implementation\nDESCRIPTION: Optimized URL joining function that handles common cases without using urljoin for better performance. Supports absolute URLs, paths, and relative URLs.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_107\n\nLANGUAGE: python\nCODE:\n```\ndef fast_urljoin(base: str, url: str) -> str:\n    \"\"\"Fast URL joining for common cases.\"\"\"\n    if url.startswith((\"http://\", \"https://\", \"mailto://\", \"//\")):\n        return url\n    if url.startswith(\"/\"):\n        # Handle absolute paths\n        if base.endswith(\"/\"):\n            return base[:-1] + url\n        return base + url\n    return urljoin(base, url)\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Mode Implementation\nDESCRIPTION: Implements batch mode crawling by collecting all results into a list before returning.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_172\n\nLANGUAGE: python\nCODE:\n```\nasync def _arun_batch(\n        self,\n        start_url: str,\n        crawler: AsyncWebCrawler,\n        config: CrawlerRunConfig,\n    ) -> List[CrawlResult]:\n        results: List[CrawlResult] = []\n        async for result in self._arun_best_first(start_url, crawler, config):\n            results.append(result)\n        return results\n```\n\n----------------------------------------\n\nTITLE: URL Validation and Processing\nDESCRIPTION: Method to validate URL format and apply filtering rules. Special handling for depth 0 (starting URL) where filtering is bypassed.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_169\n\nLANGUAGE: python\nCODE:\n```\nasync def can_process_url(self, url: str, depth: int) -> bool:\n        try:\n            parsed = urlparse(url)\n            if not parsed.scheme or not parsed.netloc:\n                raise ValueError(\"Missing scheme or netloc\")\n            if parsed.scheme not in (\"http\", \"https\"):\n                raise ValueError(\"Invalid scheme\")\n            if \".\" not in parsed.netloc:\n                raise ValueError(\"Invalid domain\")\n        except Exception as e:\n            self.logger.warning(f\"Invalid URL: {url}, error: {e}\")\n            return False\n\n        if depth != 0 and not await self.filter_chain.apply(url):\n            return False\n\n        return True\n```\n\n----------------------------------------\n\nTITLE: Installing Playwright Dependencies on Ubuntu\nDESCRIPTION: Commands for installing additional dependencies required for Playwright on Ubuntu systems.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install -y \\\n    libwoff1 \\\n    libopus0 \\\n    libwebp7 \\\n    libwebpdemux2 \\\n    libenchant-2-2 \\\n    libgudev-1.0-0 \\\n    libsecret-1-0 \\\n    libhyphen0 \\\n    libgdk-pixbuf2.0-0 \\\n    libegl1 \\\n    libnotify4 \\\n    libxslt1.1 \\\n    libevent-2.1-7 \\\n    libgles2 \\\n    libxcomposite1 \\\n    libatk1.0-0 \\\n    libatk-bridge2.0-0 \\\n    libepoxy0 \\\n    libgtk-3-0 \\\n    libharfbuzz-icu0 \\\n    libgstreamer-gl1.0-0 \\\n    libgstreamer-plugins-bad1.0-0 \\\n    gstreamer1.0-plugins-good \\\n    gstreamer1.0-plugins-bad \\\n    libxt6 \\\n    libxaw7 \\\n    xvfb \\\n    fonts-noto-color-emoji \\\n    libfontconfig \\\n    libfreetype6 \\\n    xfonts-cyrillic \\\n    xfonts-scalable \\\n    fonts-liberation \\\n    fonts-ipafont-gothic \\\n    fonts-wqy-zenhei \\\n    fonts-tlwg-loma-otf \\\n    fonts-freefont-ttf\n```\n\n----------------------------------------\n\nTITLE: Getting Browser Path Using Chromium Utility in Python\nDESCRIPTION: An asynchronous method that retrieves the browser executable path using a utility function called get_chromium_path. It passes the browser type to the utility function and returns the path.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_114\n\nLANGUAGE: python\nCODE:\n```\nasync def _get_browser_path(self) -> str:\n    browser_path = await get_chromium_path(self.browser_type)\n    return browser_path\n```\n\n----------------------------------------\n\nTITLE: Class and ID Weight Calculation for HTML Elements\nDESCRIPTION: Helper method to calculate a weight score based on element class and ID attributes. It checks for negative patterns in class names and IDs to penalize elements that likely contain irrelevant content.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_99\n\nLANGUAGE: python\nCODE:\n```\ndef _compute_class_id_weight(self, node):\n    \"\"\"Computes the class ID weight\"\"\"\n    class_id_score = 0\n    if \"class\" in node.attrs:\n        classes = \" \".join(node[\"class\"])\n        if self.negative_patterns.match(classes):\n            class_id_score -= 0.5\n    if \"id\" in node.attrs:\n        element_id = node[\"id\"]\n        if self.negative_patterns.match(element_id):\n            class_id_score -= 0.5\n    return class_id_score\n```\n\n----------------------------------------\n\nTITLE: Implementing FilterStats Class for URL Filtering Statistics in Python\nDESCRIPTION: This class provides a data structure for tracking statistics of URL filtering operations. It uses an array of unsigned integers for efficient atomic operations on counters for total, passed, and rejected URLs.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_182\n\nLANGUAGE: Python\nCODE:\n```\n@dataclass\nclass FilterStats:\n    __slots__ = (\"_counters\",)\n\n    def __init__(self):\n        # Use array of unsigned ints for atomic operations\n        self._counters = array(\"I\", [0, 0, 0])  # total, passed, rejected\n\n    @property\n    def total_urls(self):\n        return self._counters[0]\n\n    @property\n    def passed_urls(self):\n        return self._counters[1]\n\n    @property\n    def rejected_urls(self):\n        return self._counters[2]\n```\n\n----------------------------------------\n\nTITLE: Installing All Crawl4AI Extensions\nDESCRIPTION: Command to install Crawl4AI with all available extensions and features.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_84\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai[all]\ncrawl4ai-setup\n```\n\n----------------------------------------\n\nTITLE: Implementing StringCompatibleMarkdown Class\nDESCRIPTION: Defines a string subclass that provides access to MarkdownGenerationResult attributes while behaving like a regular string. This allows backward compatibility with code expecting a string while providing access to additional markdown properties.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_85\n\nLANGUAGE: python\nCODE:\n```\nclass StringCompatibleMarkdown(str):\n    \"\"\"A string subclass that also provides access to MarkdownGenerationResult attributes\"\"\"\n    def __new__(cls, markdown_result):\n        return super().__new__(cls, markdown_result.raw_markdown)\n    \n    def __init__(self, markdown_result):\n        self._markdown_result = markdown_result\n    \n    def __getattr__(self, name):\n        return getattr(self._markdown_result, name)\n```\n\n----------------------------------------\n\nTITLE: Resizing and Adjusting AI Assistant Iframe with JavaScript\nDESCRIPTION: Implements functions to dynamically resize the iframe based on window size, remove footer elements, and handle page load and resize events. It calculates available height and adjusts the iframe accordingly.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/ask-ai.md#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nfunction resizeAskAiIframe() {\n  const iframe = document.getElementById('ask-ai-frame');\n  if (iframe) {\n    const headerHeight = parseFloat(getComputedStyle(document.documentElement).getPropertyValue('--header-height') || '55');\n    const topOffset = headerHeight + 20;\n\n    const availableHeight = window.innerHeight - topOffset;\n    iframe.style.height = Math.max(600, availableHeight) + 'px';\n  }\n}\n\nresizeAskAiIframe();\nlet resizeTimer;\nwindow.addEventListener('load', resizeAskAiIframe);\nwindow.addEventListener('resize', () => {\n    clearTimeout(resizeTimer);\n    resizeTimer = setTimeout(resizeAskAiIframe, 150);\n});\n\ndocument.addEventListener('DOMContentLoaded', () => {\n    setTimeout(() => {\n        const footer = window.parent.document.querySelector('footer');\n        if (footer) {\n            const hrBeforeFooter = footer.previousElementSibling;\n            if (hrBeforeFooter && hrBeforeFooter.tagName === 'HR') {\n                hrBeforeFooter.remove();\n            }\n            footer.remove();\n            resizeAskAiIframe();\n        } else {\n             console.warn(\"Ask AI Page: Could not find footer in parent document to remove.\");\n        }\n    }, 100);\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Session Management in Python\nDESCRIPTION: Demonstrates basic session management using session_id to maintain state across multiple requests.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_161\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n\n    # Define configurations\n    config1 = CrawlerRunConfig(\n        url=\"https://example.com/page1\", session_id=session_id\n    )\n    config2 = CrawlerRunConfig(\n        url=\"https://example.com/page2\", session_id=session_id\n    )\n\n    # First request\n    result1 = await crawler.arun(config=config1)\n\n    # Subsequent request using the same session\n    result2 = await crawler.arun(config=config2)\n\n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\n```\n\n----------------------------------------\n\nTITLE: Styling AI Assistant Container and Page Layout with CSS\nDESCRIPTION: Applies CSS styles to ensure the AI assistant container and iframe fit properly within the page layout. It removes padding, sets full width and height, and handles overflow to prevent scrollbars.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/ask-ai.md#2025-04-23_snippet_2\n\nLANGUAGE: css\nCODE:\n```\n#terminal-mkdocs-main-content {\n    padding: 0 !important;\n    margin: 0;\n    width: 100%;\n    height: 100%;\n    overflow: hidden;\n}\n\n#terminal-mkdocs-main-content .ask-ai-container {\n     margin: 0;\n    padding: 0;\n    max-width: none;\n    overflow: hidden;\n}\n```\n\n----------------------------------------\n\nTITLE: Running Default Stress Test with Bash\nDESCRIPTION: Command to run a default stress test with a small configuration and generate a report using the run_all.sh script.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/tests/memory/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Run a default stress test (small config) and generate a report\n# (Assumes run_all.sh is updated to call run_benchmark.py)\n./run_all.sh\n```\n\n----------------------------------------\n\nTITLE: Using Legacy Methods with ManagedBrowser in Python\nDESCRIPTION: This snippet demonstrates how to use the legacy methods on ManagedBrowser for backward compatibility. These methods now delegate to the new BrowserProfiler class internally.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom crawl4ai.browser_manager import ManagedBrowser\n\n# These methods still work but use BrowserProfiler internally\nprofiles = ManagedBrowser.list_profiles()\n```\n\n----------------------------------------\n\nTITLE: Console Message Structure in Crawl4AI (JSON)\nDESCRIPTION: This JSON structure represents a console message captured by Crawl4AI, including the message type, text content, and source location.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/network-console-capture.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"error\",\n  \"text\": \"Uncaught TypeError: Cannot read property 'length' of undefined\",\n  \"location\": \"https://example.com/script.js:123:45\",\n  \"timestamp\": 1633456790.123\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Deprecated Markdown Properties with Error Messages\nDESCRIPTION: Defines deprecated properties for markdown_v2, fit_markdown, and fit_html that raise AttributeError when accessed, directing users to use the new property structure. These methods help with backward compatibility and guide users toward the updated API.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_83\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef markdown_v2(self):\n    \"\"\"\n    Deprecated property that raises an AttributeError when accessed.\n\n    This property exists to inform users that 'markdown_v2' has been\n    deprecated and they should use 'markdown' instead.\n    \"\"\"\n    raise AttributeError(\n        \"The 'markdown_v2' attribute is deprecated and has been removed. \"\n        \"\"\"Please use 'markdown' instead, which now returns a MarkdownGenerationResult, with\n        following properties:\n        - raw_markdown: The raw markdown string\n        - markdown_with_citations: The markdown string with citations\n        - references_markdown: The markdown string with references\n        - fit_markdown: The markdown string with fit text\n        \"\"\"\n    )\n\n@property\ndef fit_markdown(self):\n    \"\"\"\n    Deprecated property that raises an AttributeError when accessed.\n    \"\"\"\n    raise AttributeError(\n        \"The 'fit_markdown' attribute is deprecated and has been removed. \"\n        \"Please use 'markdown.fit_markdown' instead.\"\n    )\n\n@property\ndef fit_html(self):\n    \"\"\"\n    Deprecated property that raises an AttributeError when accessed.\n    \"\"\"\n    raise AttributeError(\n        \"The 'fit_html' attribute is deprecated and has been removed. \"\n        \"Please use 'markdown.fit_html' instead.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Development Installation Setup for Crawl4AI (Bash)\nDESCRIPTION: Clones the Crawl4AI repository from GitHub, navigates into the project directory, and installs the package in editable mode using pip. This allows developers to modify the source code directly.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/unclecode/crawl4ai.git\ncd crawl4ai\npip install -e .                    # Basic installation in editable mode\n```\n\n----------------------------------------\n\nTITLE: Exporting SSL Certificate to PEM Format in Python\nDESCRIPTION: Demonstrates exporting an SSL certificate to PEM format, which is commonly used for web servers. The method can return the PEM string or save it directly to a file.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npem_str = cert.to_pem()              # in-memory PEM string\ncert.to_pem(\"/path/to/cert.pem\")     # saved to file\n```\n\n----------------------------------------\n\nTITLE: Updating run_benchmark.py Script for Stress Testing in Python\nDESCRIPTION: Modified the run_benchmark.py script to use '--max-sessions' instead of '--workers', correctly pass custom parameters, and provide clickable file:// links to generated reports.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/JOURNAL.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Updated argument handling in run_benchmark.py\n# Exact implementation details not provided in the original text\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for crawl4ai CLI\nDESCRIPTION: This snippet imports all the necessary modules and classes needed for the crawl4ai command-line interface. It includes Click for CLI functionality, Rich for terminal output formatting, and various components from the crawl4ai package for web crawling and content processing.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport click\nimport os\nimport sys\nimport time\n\nimport humanize\nfrom typing import Dict, Any, Optional, List\nimport json\nimport yaml\nimport anyio\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.panel import Panel\nfrom rich.prompt import Prompt, Confirm\n\nfrom crawl4ai import (\n    CacheMode,\n    AsyncWebCrawler, \n    CrawlResult,\n    BrowserConfig, \n    CrawlerRunConfig,\n    LLMExtractionStrategy, \n    LXMLWebScrapingStrategy,\n    JsonCssExtractionStrategy,\n    JsonXPathExtractionStrategy,\n    BM25ContentFilter, \n    PruningContentFilter,\n    BrowserProfiler,\n    DefaultMarkdownGenerator,\n    LLMConfig\n)\nfrom crawl4ai.config import USER_SETTINGS\nfrom litellm import completion\nfrom pathlib import Path\n```\n\n----------------------------------------\n\nTITLE: Initializing Pre-computed Freshness Scores for Year Differences in Python\nDESCRIPTION: Defines a constant lookup table of freshness scores based on content age, with scores ranging from 1.0 (current year) decreasing by 0.1 for each year in the past, up to 5 years.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_198\n\nLANGUAGE: python\nCODE:\n```\n# Pre-computed scores for common year differences\n_FRESHNESS_SCORES = [\n   1.0,    # Current year\n   0.9,    # Last year\n   0.8,    # 2 years ago\n   0.7,    # 3 years ago\n   0.6,    # 4 years ago\n   0.5,    # 5 years ago\n]\n```\n\n----------------------------------------\n\nTITLE: Browser Profile Creation Method (Deprecated) in Python\nDESCRIPTION: A static asynchronous method for creating browser profiles that has been moved to the BrowserProfiler class. It provides documentation on how to use the new method and delegates to the BrowserProfiler implementation when called.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_117\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\nasync def create_profile(browser_config=None, profile_name=None, logger=None):\n    \"\"\"\n    This method has been moved to the BrowserProfiler class.\n    \n    Creates a browser profile by launching a browser for interactive user setup\n    and waits until the user closes it. The profile is stored in a directory that\n    can be used later with BrowserConfig.user_data_dir.\n    \n    Please use BrowserProfiler.create_profile() instead.\n    \n    Example:\n        ```python\n        from crawl4ai.browser_profiler import BrowserProfiler\n        \n        profiler = BrowserProfiler()\n        profile_path = await profiler.create_profile(profile_name=\"my-login-profile\")\n        ```\n    \"\"\"\n    from .browser_profiler import BrowserProfiler\n    \n    # Create a BrowserProfiler instance and delegate to it\n    profiler = BrowserProfiler(logger=logger)\n    return await profiler.create_profile(profile_name=profile_name, browser_config=browser_config)\n```\n\n----------------------------------------\n\nTITLE: Updating CrawlerRunConfig Class in Python\nDESCRIPTION: Added boolean parameters to enable network request and console message capturing in the CrawlerRunConfig class.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/JOURNAL.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncapture_network_requests: bool = False\ncapture_console_messages: bool = False\n```\n\n----------------------------------------\n\nTITLE: Importing Configuration Components for Crawl4AI in Python\nDESCRIPTION: This code imports various modules and classes needed for the Crawl4AI web crawler. It brings in default configuration settings, strategy classes for different aspects of web scraping, and utility classes for operations like user agent generation, content extraction, chunking, and caching.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom .config import (\n    DEFAULT_PROVIDER,\n    DEFAULT_PROVIDER_API_KEY,\n    MIN_WORD_THRESHOLD,\n    IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD,\n    PROVIDER_MODELS,\n    PROVIDER_MODELS_PREFIXES,\n    SCREENSHOT_HEIGHT_TRESHOLD,\n    PAGE_TIMEOUT,\n    IMAGE_SCORE_THRESHOLD,\n    SOCIAL_MEDIA_DOMAINS,\n)\n\nfrom .user_agent_generator import UAGen, ValidUAGenerator  # , OnlineUAGenerator\nfrom .extraction_strategy import ExtractionStrategy, LLMExtractionStrategy\nfrom .chunking_strategy import ChunkingStrategy, RegexChunking\n\nfrom .markdown_generation_strategy import MarkdownGenerationStrategy, DefaultMarkdownGenerator\nfrom .content_scraping_strategy import ContentScrapingStrategy, WebScrapingStrategy\nfrom .deep_crawling import DeepCrawlStrategy\n\nfrom .cache_context import CacheMode\nfrom .proxy_strategy import ProxyRotationStrategy\n\nfrom typing import Union, List\nimport inspect\nfrom typing import Any, Dict, Optional\nfrom enum import Enum\n```\n\n----------------------------------------\n\nTITLE: Default Social Media Domains Exclusion List in Crawl4AI\nDESCRIPTION: This code snippet shows the default list of social media domains that are excluded when exclude_social_media_links is set to True. These domains include major social media platforms like Facebook, Twitter, LinkedIn, and others.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[\n    'facebook.com',\n    'twitter.com',\n    'x.com',\n    'linkedin.com',\n    'instagram.com',\n    'pinterest.com',\n    'tiktok.com',\n    'snapchat.com',\n    'reddit.com',\n]\n```\n\n----------------------------------------\n\nTITLE: Best First First Crawling Strategy Import Header in Python\nDESCRIPTION: This file header contains imports for implementing a Best-First-First crawling strategy, including dependencies for filtering, scoring, and traversal tracking. It extends the DeepCrawlStrategy base class.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_166\n\nLANGUAGE: python\nCODE:\n```\n# best_first_crawling_strategy.py\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom typing import AsyncGenerator, Optional, Set, Dict, List, Tuple\nfrom urllib.parse import urlparse\n\nfrom ..models import TraversalStats\nfrom .filters import FilterChain\nfrom .scorers import URLScorer\nfrom . import DeepCrawlStrategy\n\nfrom ..types import AsyncWebCrawler, CrawlerRunConfig, CrawlResult, RunManyReturn\n\nfrom math import inf as infinity\n```\n\n----------------------------------------\n\nTITLE: Legacy Text Extraction Method in Python\nDESCRIPTION: A deprecated implementation of text chunk extraction using recursive depth-first search. This method identifies relevant HTML elements, filters them based on tag inclusion/exclusion rules, and returns a list of text chunks with their positions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_92\n\nLANGUAGE: python\nCODE:\n```\n    def _deprecated_extract_text_chunks(\n        self, soup: BeautifulSoup\n    ) -> List[Tuple[int, str, Tag]]:\n        \"\"\"Common method for extracting text chunks\"\"\"\n        _text_cache = {}\n\n        def fast_text(element: Tag) -> str:\n            elem_id = id(element)\n            if elem_id in _text_cache:\n                return _text_cache[elem_id]\n            texts = []\n            for content in element.contents:\n                if isinstance(content, str):\n                    text = content.strip()\n                    if text:\n                        texts.append(text)\n            result = \" \".join(texts)\n            _text_cache[elem_id] = result\n            return result\n\n        candidates = []\n        index = 0\n\n        def dfs(element):\n            nonlocal index\n            if isinstance(element, Tag):\n                if element.name in self.included_tags:\n                    if not self.is_excluded(element):\n                        text = fast_text(element)\n                        word_count = len(text.split())\n\n                        # Headers pass through with adjusted minimum\n                        if element.name in self.header_tags:\n                            if word_count >= 3:  # Minimal sanity check for headers\n                                candidates.append((index, text, element))\n                                index += 1\n                        # Regular content uses standard minimum\n                        elif word_count >= self.min_word_count:\n                            candidates.append((index, text, element))\n                            index += 1\n\n                for child in element.children:\n                    dfs(child)\n\n        dfs(soup.body if soup.body else soup)\n        return candidates\n```\n\n----------------------------------------\n\nTITLE: Embedding Attribution Badges with HTML (HTML)\nDESCRIPTION: This snippet collection provides HTML markup for integrating attribution badges for Crawl4AI in documentation or on websites. Badge themes include animated disco, night/dark, light/classic, and a simple shield badge, all linking to the project's GitHub page. Usage requires copying and pasting the desired snippet into HTML-compatible documentation or site templates. Output is a visually embedded badge, each displaying 'Powered by Crawl4AI' with different styling, and each badge links to the project's GitHub repository.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_25\n\nLANGUAGE: html\nCODE:\n```\n<!-- Disco Theme (Animated) -->\n<a href=\"https://github.com/unclecode/crawl4ai\">\n  <img src=\"https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-disco.svg\" alt=\"Powered by Crawl4AI\" width=\"200\"/>\n</a>\n\n```\n\nLANGUAGE: html\nCODE:\n```\n<!-- Night Theme (Dark with Neon) -->\n<a href=\"https://github.com/unclecode/crawl4ai\">\n  <img src=\"https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-night.svg\" alt=\"Powered by Crawl4AI\" width=\"200\"/>\n</a>\n\n```\n\nLANGUAGE: html\nCODE:\n```\n<!-- Dark Theme (Classic) -->\n<a href=\"https://github.com/unclecode/crawl4ai\">\n  <img src=\"https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-dark.svg\" alt=\"Powered by Crawl4AI\" width=\"200\"/>\n</a>\n\n```\n\nLANGUAGE: html\nCODE:\n```\n<!-- Light Theme (Classic) -->\n<a href=\"https://github.com/unclecode/crawl4ai\">\n  <img src=\"https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-light.svg\" alt=\"Powered by Crawl4AI\" width=\"200\"/>\n</a>\n\n```\n\nLANGUAGE: html\nCODE:\n```\n<!-- Simple Shield Badge -->\n<a href=\"https://github.com/unclecode/crawl4ai\">\n  <img src=\"https://img.shields.io/badge/Powered%20by-Crawl4AI-blue?style=flat-square\" alt=\"Powered by Crawl4AI\"/>\n</a>\n\n```\n\n----------------------------------------\n\nTITLE: Markdown Generation Strategy Imports\nDESCRIPTION: Import statements for markdown generation strategy implementation, including abstract base classes, typing, and utility modules.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_105\n\nLANGUAGE: python\nCODE:\n```\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Dict, Any, Tuple\nfrom .models import MarkdownGenerationResult\nfrom .html2text import CustomHTML2Text\nfrom .content_filter_strategy import RelevantContentFilter\nimport re\nfrom urllib.parse import urljoin\n```\n\n----------------------------------------\n\nTITLE: Defining Deep Crawling Module Structure in Python\nDESCRIPTION: Module initialization file for the deep_crawling package in Crawl4AI. It exports various strategies, filters, and scorers for deep web crawling. The file defines the public API for deep crawling operations, including BFS and DFS strategies, content and domain filters, and various URL scoring mechanisms for prioritization.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_163\n\nLANGUAGE: python\nCODE:\n```\n# deep_crawling/__init__.py\nfrom .base_strategy import DeepCrawlDecorator, DeepCrawlStrategy\nfrom .bfs_strategy import BFSDeepCrawlStrategy\nfrom .bff_strategy import BestFirstCrawlingStrategy\nfrom .dfs_strategy import DFSDeepCrawlStrategy\nfrom .filters import (\n    FilterChain,\n    ContentTypeFilter,\n    DomainFilter,\n    URLFilter,\n    URLPatternFilter,\n    FilterStats,\n    ContentRelevanceFilter,\n    SEOFilter\n)\nfrom .scorers import (\n    KeywordRelevanceScorer,\n    URLScorer,\n    CompositeScorer,\n    DomainAuthorityScorer,\n    FreshnessScorer,\n    PathDepthScorer,\n    ContentTypeScorer\n)\n\n__all__ = [\n    \"DeepCrawlDecorator\",\n    \"DeepCrawlStrategy\",\n    \"BFSDeepCrawlStrategy\",\n    \"BestFirstCrawlingStrategy\",\n    \"DFSDeepCrawlStrategy\",\n    \"FilterChain\",\n    \"ContentTypeFilter\",\n    \"DomainFilter\",\n    \"URLFilter\",\n    \"URLPatternFilter\",\n    \"FilterStats\",\n    \"ContentRelevanceFilter\",\n    \"SEOFilter\",\n    \"KeywordRelevanceScorer\",\n    \"URLScorer\",\n    \"CompositeScorer\",\n    \"DomainAuthorityScorer\",\n    \"FreshnessScorer\",\n    \"PathDepthScorer\",\n    \"ContentTypeScorer\",\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Size Constant\nDESCRIPTION: Defines the batch size constant for processing URLs from the priority queue in groups to improve efficiency.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_167\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE = 10\n```\n\n----------------------------------------\n\nTITLE: Textual Citation Format for Crawl4AI (Markdown/Plain Text)\nDESCRIPTION: This snippet provides a ready-to-use plain text citation for Crawl4AI according to academic conventions. Information includes the author, year, software title, medium, and repository URL. The snippet is intended for documentation, presentations, or publication in environments that do not use BibTeX. It is static except for the year, which should be updated if referencing a different version.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_28\n\nLANGUAGE: markdown\nCODE:\n```\nUncleCode. (2024). Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper [Computer software]. \nGitHub. https://github.com/unclecode/crawl4ai\n\n```\n\n----------------------------------------\n\nTITLE: Markdown Contributors Documentation\nDESCRIPTION: A structured markdown document listing all contributors to the Crawl4AI project, including their GitHub profiles and specific contributions.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/CONTRIBUTORS.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Contributors to Crawl4AI\\n\\nWe would like to thank the following people for their contributions to Crawl4AI:\\n\\n## Core Team\\n\\n- [Unclecode](https://github.com/unclecode) - Project Creator and Main Developer\\n- [Nasrin](https://github.com/ntohidi) - Project Manager and Developer\\n- [Aravind Karnam](https://github.com/aravindkarnam) - Head of Community and Product \\n\\n## Community Contributors\\n\\n- [aadityakanjolia4](https://github.com/aadityakanjolia4) - Fix for `CustomHTML2Text` is not defined.\\n- [FractalMind](https://github.com/FractalMind) - Created the first official Docker Hub image and fixed Dockerfile errors\\n- [ketonkss4](https://github.com/ketonkss4) - Identified Selenium's new capabilities, helping reduce dependencies\\n- [jonymusky](https://github.com/jonymusky) - Javascript execution documentation, and wait_for\\n- [datehoer](https://github.com/datehoer) - Add browser prxy support\n```\n\n----------------------------------------\n\nTITLE: Deleting Browser Profiles Method (Deprecated) in Python\nDESCRIPTION: A static method for deleting browser profiles that has been moved to the BrowserProfiler class. It provides documentation on how to use the new method and delegates to the BrowserProfiler implementation when called.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_119\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef delete_profile(profile_name_or_path):\n    \"\"\"\n    This method has been moved to the BrowserProfiler class.\n    \n    Delete a browser profile by name or path.\n    \n    Please use BrowserProfiler.delete_profile() instead.\n    \n    Example:\n        ```python\n        from crawl4ai.browser_profiler import BrowserProfiler\n        \n        profiler = BrowserProfiler()\n        success = profiler.delete_profile(\"my-profile\")\n        ```\n    \"\"\"\n    from .browser_profiler import BrowserProfiler\n    \n    # Create a BrowserProfiler instance and delegate to it\n    profiler = BrowserProfiler()\n    return profiler.delete_profile(profile_name_or_path)\n```\n\n----------------------------------------\n\nTITLE: Modifying AsyncCrawlResponse and CrawlResult Models in Python\nDESCRIPTION: Added fields to store captured network requests and console messages in both AsyncCrawlResponse and CrawlResult models.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/JOURNAL.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nnetwork_requests: Optional[List[Dict[str, Any]]] = None\nconsole_messages: Optional[List[Dict[str, Any]]] = None\n```\n\n----------------------------------------\n\nTITLE: Installation of Synchronous Crawl4AI Version (Bash)\nDESCRIPTION: Installs Crawl4AI with the synchronous crawling capabilities provided by Selenium. Note that this version is deprecated and planned for removal.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install crawl4ai[sync]\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation Entry for Crawl4AI (BibTeX)\nDESCRIPTION: The provided BibTeX snippet enables users to cite Crawl4AI in academic or research publications. It defines a @software entry including author, title, year, publisher, repository link, and a placeholder for the commit hash. This citation can be added directly to BiBTeX (.bib) files for LaTeX documents. Users must specify the actual commit hash to identify the exact software version used in their research.\nSOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_27\n\nLANGUAGE: bibtex\nCODE:\n```\n@software{crawl4ai2024,\n  author = {UncleCode},\n  title = {Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper},\n  year = {2024},\n  publisher = {GitHub},\n  journal = {GitHub Repository},\n  howpublished = {\\url{https://github.com/unclecode/crawl4ai}},\n  commit = {Please use the commit hash you\\'re working with}\n}\n\n```"
  }
]
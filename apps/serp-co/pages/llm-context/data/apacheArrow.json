[
  {
    "owner": "apache",
    "repo": "arrow",
    "content": "TITLE: Complete Arrow Compute Example Implementation (C++)\nDESCRIPTION: The full implementation of the compute example demonstrating how to use Arrow compute functions with various approaches. Includes all the necessary imports, function calls, and error handling.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\n// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\n#include <arrow/api.h>\n#include <arrow/compute/api.h>\n#include <arrow/compute/api_scalar.h>\n#include <arrow/io/api.h>\n#include <arrow/result.h>\n#include <arrow/status.h>\n#include <arrow/table.h>\n\n#include <cassert>\n#include <cstdlib>\n#include <iostream>\n\n// Demonstrate instruction on the Arrow C++ compute functions API\n//\n// Create arrow table with a numerical column and compute\n// min, max mean values.\n//\n// Writes \"compute_example demo completed successfully\\n\" to stdout when complete.\n\n#include \"arrow/util/logging.h\"\n\n#include <fmt/core.h>\n\narrow::Status RunMain() {\n  arrow::Int64Builder a_builder;\n  arrow::Int64Builder b_builder;\n\n  ARROW_RETURN_NOT_OK(a_builder.AppendValues({1, 2, 2223, 3}));\n  ARROW_RETURN_NOT_OK(b_builder.AppendValues({3, 5, 7, 11}));\n\n  std::shared_ptr<arrow::Array> a, b;\n  ARROW_ASSIGN_OR_RAISE(a, a_builder.Finish());\n  ARROW_ASSIGN_OR_RAISE(b, b_builder.Finish());\n\n  std::shared_ptr<arrow::Field> field_a, field_b;\n  field_a = arrow::field(\"A\", arrow::int64());\n  field_b = arrow::field(\"B\", arrow::int64());\n\n  std::shared_ptr<arrow::Schema> schema;\n  schema = arrow::schema({field_a, field_b});\n\n  std::shared_ptr<arrow::Table> table;\n  table = arrow::Table::Make(schema, {a, b});\n\n  fmt::print(\"Table size: {}\\n\", table->num_rows());\n  fmt::print(\"-- Function with convenience function --\\n\");\n\n  ARROW_ASSIGN_OR_RAISE(arrow::Datum mean_a, arrow::compute::Mean(table->column(0)));\n  fmt::print(\"Mean of column a: {}\\n\", mean_a.scalar_as<arrow::DoubleScalar>().value);\n\n  fmt::print(\"-- Function with direct call --\\n\");\n\n  ARROW_ASSIGN_OR_RAISE(\n      arrow::compute::Datum sum_b,\n      arrow::compute::CallFunction(\"sum\", {arrow::compute::Datum(table->column(1))}));\n  fmt::print(\"Sum of column b: {}\\n\", sum_b.scalar_as<arrow::Int64Scalar>().value);\n\n  fmt::print(\"-- Function needing options struct --\\n\");\n\n  arrow::compute::IndexOptions options;\n  options.value = arrow::compute::Datum(2223);\n\n  ARROW_ASSIGN_OR_RAISE(arrow::compute::Datum result,\n                         arrow::compute::CallFunction(\"index\", {table->column(0)}, &options));\n\n  ARROW_ASSIGN_OR_RAISE(auto scalar_result, result.scalar());\n  fmt::print(\"index result: {}\\n\", scalar_result->ToString());\n\n  fmt::print(\"compute_example demo completed successfully\\n\");\n  return arrow::Status::OK();\n}\n\nint main() {\n  arrow::Status st = RunMain();\n  if (!st.ok()) {\n    std::cerr << st << std::endl;\n    return EXIT_FAILURE;\n  }\n  return EXIT_SUCCESS;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Arrays and Tables in Apache Arrow with Python\nDESCRIPTION: Demonstrates how to create arrays of different data types and combine them into a table. This snippet shows the creation of arrays for days, months, and years, which are then combined into a 'birthdays' table.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/getstarted.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\ndays = pa.array([1, 12, 17, 23, 28], type=pa.int8())\n\nmonths = pa.array([1, 3, 5, 7, 1], type=pa.int8())\nyears = pa.array([1990, 2000, 1995, 2000, 1995], type=pa.int16())\n\nbirthdays_table = pa.table([days, months, years],\n                           names=[\"days\", \"months\", \"years\"])\n\nbirthdays_table\n```\n\n----------------------------------------\n\nTITLE: Performing Computations on Apache Arrow Tables in Python\nDESCRIPTION: Demonstrates how to use Arrow's compute functions to perform operations on table data. This example shows how to count unique values in a column of the table.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/getstarted.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.compute as pc\n\npc.value_counts(birthdays_table[\"years\"])\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Data Types in PyArrow\nDESCRIPTION: Shows how to create various Arrow data type objects using PyArrow's factory functions. Demonstrates creating primitive types like int32, string, binary, fixed-size binary, and timestamp.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nt1 = pa.int32()\nt2 = pa.string()\nt3 = pa.binary()\nt4 = pa.binary(10)\nt5 = pa.timestamp('ms')\n\nt1\nprint(t1)\nprint(t4)\nprint(t5)\n```\n\n----------------------------------------\n\nTITLE: Chaining Filters on PyArrow Dataset\nDESCRIPTION: Example demonstrating how to chain multiple filters on a PyArrow Dataset and retrieve the filtered data as a table. Filters are applied lazily until data is accessed.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/compute.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = ds.dataset(table)\n>>> filtered = dataset.filter(pc.field(\"nums\") < 5).filter(pc.field(\"nums\") > 2)\n>>> filtered.to_table()\npyarrow.Table\nnums: int64\nchars: string\n----\nnums: [[3,4]]\nchars: [[\"c\",\"d\"]]\n```\n\n----------------------------------------\n\nTITLE: Registering a User-Defined Scalar Function in PyArrow\nDESCRIPTION: Complete example of registering a custom scalar function (GCD calculator) using NumPy as the implementation. Includes function definition, metadata, input/output types specification, and registration.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/compute.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nimport pyarrow as pa\nimport pyarrow.compute as pc\n\nfunction_name = \"numpy_gcd\"\nfunction_docs = {\n      \"summary\": \"Calculates the greatest common divisor\",\n      \"description\":\n         \"Given 'x' and 'y' find the greatest number that divides\\n\"\n         \"evenly into both x and y.\"\n}\n\ninput_types = {\n   \"x\" : pa.int64(),\n   \"y\" : pa.int64()\n}\n\noutput_type = pa.int64()\n\ndef to_np(val):\n    if isinstance(val, pa.Scalar):\n       return val.as_py()\n    else:\n       return np.array(val)\n\ndef gcd_numpy(ctx, x, y):\n    np_x = to_np(x)\n    np_y = to_np(y)\n    return pa.array(np.gcd(np_x, np_y))\n\npc.register_scalar_function(gcd_numpy,\n                           function_name,\n                           function_docs,\n                           input_types,\n                           output_type)\n```\n\n----------------------------------------\n\nTITLE: Basic Parquet File Reading and Writing with PyArrow\nDESCRIPTION: Demonstrates creating a pandas DataFrame, converting it to a PyArrow Table, and writing/reading it to/from Parquet format.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n\ndf = pd.DataFrame({'one': [-1, np.nan, 2.5],\n                  'two': ['foo', 'bar', 'baz'],\n                  'three': [True, False, True]},\n                  index=list('abc'))\ntable = pa.Table.from_pandas(df)\n\nimport pyarrow.parquet as pq\npq.write_table(table, 'example.parquet')\n\ntable2 = pq.read_table('example.parquet')\ntable2.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Iterative Reading of Large Datasets in Arrow\nDESCRIPTION: This snippet demonstrates how to perform iterative (out of core or streaming) reads on large datasets, calculating the average of a column without loading the entire column into memory.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.compute as pc\n\ncol2_sum = 0\ncount = 0\nfor batch in dataset.to_batches(columns=[\"col2\"], filter=~ds.field(\"col2\").is_null()):\n    col2_sum += pc.sum(batch.column(\"col2\")).as_py()\n    count += batch.num_rows\nmean_a = col2_sum/count\n```\n\n----------------------------------------\n\nTITLE: Converting NumPy Array to Arrow Array in Python\nDESCRIPTION: This snippet demonstrates how to convert a NumPy array to an Arrow Array using the pyarrow.array() factory function. It creates a NumPy array with sequential integers of int16 type and converts it to an Arrow Int16Array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/numpy.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> import pyarrow as pa\n>>> data = np.arange(10, dtype='int16')\n>>> arr = pa.array(data)\n>>> arr\n<pyarrow.lib.Int16Array object at 0x7fb1d1e6ae58>\n[\n  0,\n  1,\n  2,\n  3,\n  4,\n  5,\n  6,\n  7,\n  8,\n  9\n]\n```\n\n----------------------------------------\n\nTITLE: Combining PyArrow Filters with AND Operator\nDESCRIPTION: Example showing how to combine multiple filters with the logical AND operator (&) to find all even numbers greater than 5 in a PyArrow table.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/compute.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> table.filter(even_filter & (pc.field(\"nums\") > 5))\npyarrow.Table\nnums: int64\nchars: string\n----\nnums: [[6,8,10]]\nchars: [[\"f\",\"h\",\"l\"]]\n```\n\n----------------------------------------\n\nTITLE: Writing Arrow Dataset with Format-Specific Parameters\nDESCRIPTION: Shows how to configure format-specific options when writing a dataset by creating custom file options. The example uses Parquet format with the allow_truncated_timestamps option enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nparquet_format = ds.ParquetFileFormat()\nwrite_options = parquet_format.make_write_options(allow_truncated_timestamps=True)\nds.write_dataset(table, \"sample_dataset2\", format=\"parquet\", partitioning=part,\n                 file_options=write_options)\n```\n\n----------------------------------------\n\nTITLE: Complete Dataset Reading and Writing Example in C++\nDESCRIPTION: This is the full implementation of reading from and writing to Arrow datasets, including creating scanners, converting to tables, and writing partitioned datasets with custom configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_24\n\nLANGUAGE: cpp\nCODE:\n```\n#include <arrow/api.h>\n#include <arrow/acero/api.h>\n#include <arrow/dataset/dataset.h>\n#include <arrow/dataset/discovery.h>\n#include <arrow/dataset/file_base.h>\n#include <arrow/dataset/file_parquet.h>\n#include <arrow/dataset/partition.h>\n#include <arrow/dataset/scanner.h>\n#include <arrow/filesystem/filesystem.h>\n#include <arrow/filesystem/path_util.h>\n#include <arrow/result.h>\n#include <arrow/status.h>\n\n#include <iostream>\n#include <vector>\n\nusing arrow::Status;\n\nusing arrow::acero::Declaration;\n\nusing arrow::field;\nusing arrow::int64;\nusing arrow::schema;\n\nusing arrow::compute::Expression;\n\nusing arrow::fs::FileInfo;\nusing arrow::fs::FileSelector;\nusing arrow::fs::FileSystem;\nusing arrow::fs::FileType;\nusing arrow::fs::LocalFileSystem;\n\nusing arrow::dataset::Dataset;\nusing arrow::dataset::ExistingDataBehavior;\nusing arrow::dataset::FileSystemDataset;\nusing arrow::dataset::FileSystemDatasetWriteOptions;\nusing arrow::dataset::FileSystemFactoryOptions;\nusing arrow::dataset::HivePartitioning;\nusing arrow::dataset::InMemoryDataset;\nusing arrow::dataset::ParquetFileFormat;\nusing arrow::dataset::Scanner;\nusing arrow::dataset::ScannerBuilder;\n\nnamespace {\n\nstatic void PrintValues(const arrow::Array& values, int64_t n) {\n  for (int64_t i = 0; i < n; ++i) {\n    std::string value;\n    if (values.IsNull(i)) {\n      value = \"null\";\n    } else {\n      value = values.ToString();\n    }\n    std::cout << value << \"\\n\";\n  }\n}\n\nstatic Status PrettyPrint(const arrow::Table& table) {\n  std::cout << \"Table has \" << table.column_names().size() << \" columns and \"\n            << table.num_rows() << \" rows.\\n\\n\";\n\n  for (int i = 0; i < table.column_names().size(); i++) {\n    std::string name = table.column_names()[i];\n    std::shared_ptr<arrow::Array> col = table.column(i)->chunk(0);\n\n    std::cout << name << \":\";\n    PrintValues(*col, std::min(5, static_cast<int>(table.num_rows())));\n    std::cout << \"\\n\";\n  }\n  return Status::OK();\n}\n\nStatus RunDatasetExample() {\n  /* Explore Datasets using Apache Arrow                             */\n  /* In this example, we'll use a local directory dataset.           */\n\n  // Set up a dataset creation helper for filesystem-based data source\n  std::shared_ptr<FileSystem> fs = std::make_shared<LocalFileSystem>();\n\n  // Find files from the root of this project\n  std::string base_dir = \"dataset_test\";\n  std::string partition_path = arrow::fs::internal::ConcatAbstractPath(\n      std::vector<std::string>{base_dir, \"partition_test\"});\n\n  /* Now let's look at the nested paths to see what we have in them. This is\n  useful if you're looking at data from a LocalFileSystem, or from something\n  more complicated like S3. By exploring like this, we can see if there are\n  partition directories, for example.*/\n\n  // Set options for finding and handling all files recursively\n  FileSelector selector;\n  selector.base_dir = base_dir;\n  selector.recursive = true;\n\n  // Get a list of all FileInfo, which we can use to check on the file structure\n  // itself\n  ARROW_ASSIGN_OR_RAISE(auto info, fs->GetFileInfo(selector));\n\n  /* A Dataset is a logical collection of files that can be read like a table. A\n  Dataset can be fragmented (partitioned by folder, like a Hive dataset, for\n  example) */\n\n  // For our illustration, let's use a partitioning pattern to parse the\n  // directory structure\n  auto factory_options = FileSystemFactoryOptions{}\n                             .WithPartitioning(\n                                 arrow::dataset::HivePartitioning::MakeFactory());\n\n  // Set up factory, which gets information from parsed files and partitions\n  auto factory_result = arrow::dataset::FileSystemDatasetFactory::Make(\n      fs, {partition_path}, std::make_shared<arrow::dataset::ParquetFileFormat>(),\n      factory_options);\n\n  ARROW_RETURN_NOT_OK(factory_result.status());\n  auto factory = factory_result.ValueOrDie();\n\n  // Get the schema, which registers schema fields from both those in the\n  // directory hierarchy (partition keys) as well as derived from Parquet column\n  // schemas\n  auto schema_result = factory->Inspect();\n  ARROW_RETURN_NOT_OK(schema_result.status());\n  auto schema = schema_result.ValueOrDie();\n\n  // Finish initializing our Scanner factory\n  auto finish_result = factory->Finish(schema);\n  ARROW_RETURN_NOT_OK(finish_result.status());\n  auto dataset = finish_result.ValueOrDie();\n\n  /* Okay, so we now have a Dataset, which is a generic data source, which can be\n  Acero-based (either the Acero engine, or a dummy exec node that provides\n  record batches from a Dataset), or Parquet, or ORC, or...you get the idea.\n\n  The Dataset is a general abstraction. Now let's scan a particular dataset, in\n  case we want to load the table. */\n\n  // Set up a scanner for passing through the data\n  auto builder = ScannerBuilder(dataset);\n\n  // Synchronously finish the ScannerBuilder\n  auto scanner_result = builder.Finish();\n  if (!scanner_result.ok()) {\n    return scanner_result.status();\n  }\n\n  // Get our Scanner\n  std::shared_ptr<Scanner> scanner = scanner_result.ValueOrDie();\n\n  /* Now that we have a tool to move through our Dataset, let's use it to get our\n  Table. */\n\n  // Read the entire dataset as a Table\n  auto table_result = scanner->ToTable();\n  if (!table_result.ok()) {\n    return table_result.status();\n  }\n\n  // Get our Table\n  std::shared_ptr<Table> table = table_result.ValueOrDie();\n\n  // Work with Table as in other examples\n  PrettyPrint(*table);\n\n  /* That leaves us with a normal Table. Again, if you want to work with a Dataset\n  without materializing a Table, you'll want to use Acero. */\n\n  /* Now let's write our own Dataset, to show how to use partitioning schemes when\n  writing a Dataset back to the filesystem. */\n\n  /* In this case, we're going to take our Table, and write it back out to the\n  filesystem, partitioning by the value in the column \"a\" for each row. */\n\n  // Create a TableBatchReader for our Result\n  TableBatchReader table_batch_reader(*table);\n\n  // Make our ScannerBuilder\n  auto write_scanner_builder_result = ScannerBuilder::FromRecordBatchReader(&table_batch_reader);\n  if (!write_scanner_builder_result.ok()) {\n    return write_scanner_builder_result.status();\n  }\n  std::shared_ptr<ScannerBuilder> write_scanner_builder = write_scanner_builder_result.ValueOrDie();\n\n  // Make our Scanner\n  auto write_scanner_result = write_scanner_builder->Finish();\n  if (!write_scanner_result.ok()) {\n    return write_scanner_result.status();\n  }\n  std::shared_ptr<Scanner> write_scanner = write_scanner_result.ValueOrDie();\n\n  // Create our partitioning schema\n  std::shared_ptr<Schema> partition_schema = arrow::schema({field(\"a\", int64())});\n\n  // Create our partitioning\n  auto partitioning_factory = std::make_shared<HivePartitioning>(partition_schema);\n\n  // Create our file format, in this case Parquet\n  auto format = std::make_shared<ParquetFileFormat>();\n\n  // Prepare write options\n  FileSystemDatasetWriteOptions write_options;\n\n  // Set our filesystem\n  write_options.filesystem = fs;\n\n  // Set base_dir\n  write_options.base_dir = \"write_dataset\";\n\n  // Set partitioning\n  write_options.partitioning = partitioning_factory;\n\n  // Finish write_options\n  write_options.basename_template = \"part{i}.parquet\";\n\n  // Set file behavior\n  write_options.existing_data_behavior =\n      ExistingDataBehavior::kDeleteMatchingPartitions;\n\n  // Write out\n  STATUS_AND_ASSIGN_OR_RAISE(auto,,\n                         FileSystemDataset::Write(write_options, write_scanner));\n\n  return Status::OK();\n}\n}  // anonymous namespace\n```\n\n----------------------------------------\n\nTITLE: Filtering PyArrow Table with Logical NOT Operator\nDESCRIPTION: Example showing how to use the logical NOT operator (~) with a filter to select all odd numbers from a PyArrow table.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/compute.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> table.filter(~even_filter)\npyarrow.Table\nnums: int64\nchars: string\n----\nnums: [[1,3,5,7,9]]\nchars: [[\"a\",\"c\",\"e\",\"g\",\"i\"]]\n```\n\n----------------------------------------\n\nTITLE: Converting between Arrow and pandas nullable integer types\nDESCRIPTION: Shows round-trip conversion between Arrow and pandas using pandas' experimental nullable integer types. Arrow preserves the metadata about pandas' Int64 dtype when converted to an Arrow table and back.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/pandas.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> df = pd.DataFrame({'a': pd.Series([1, 2, None], dtype=\"Int64\")})\n>>> df\n      a\n0     1\n1     2\n2  <NA>\n\n>>> table = pa.table(df)\n>>> table\nOut[32]:\npyarrow.Table\na: int64\n----\na: [[1,2,null]]\n\n>>> table.to_pandas()\n      a\n0     1\n1     2\n2  <NA>\n\n>>> table.to_pandas().dtypes\na    Int64\ndtype: object\n```\n\n----------------------------------------\n\nTITLE: Defining Partitioning Schemes in Arrow Dataset\nDESCRIPTION: This code shows how to explicitly define the schema of partition keys using the partitioning function, including examples for both Hive-style and directory partitioning.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npart = ds.partitioning(\n    pa.schema([(\"year\", pa.int16()), (\"month\", pa.int8()), (\"day\", pa.int32())]),\n    flavor=\"hive\"\n)\ndataset = ds.dataset(..., partitioning=part)\n```\n\nLANGUAGE: python\nCODE:\n```\npart = ds.partitioning(field_names=[\"year\", \"month\", \"day\"])\n```\n\n----------------------------------------\n\nTITLE: Slicing Arrays in PyArrow\nDESCRIPTION: Demonstrates how to slice Arrow arrays, which creates a view without copying the underlying data, maintaining memory efficiency.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\narr[1:3]\n```\n\n----------------------------------------\n\nTITLE: Complete Arrow Table Creation Example in C++\nDESCRIPTION: The complete example demonstrating how to create Arrow data structures including ChunkedArrays and Tables. This code shows the full process from initialization to creation of tabular data structures in Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/basic_arrow.rst#2025-04-16_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n(Doc section: Basic Example)\n```\n\n----------------------------------------\n\nTITLE: Creating Struct Types with Fields in PyArrow\nDESCRIPTION: Demonstrates creating a struct type by explicitly defining fields with names and types. A struct type is a collection of named fields, each with its own data type.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfields = [\n    pa.field('s0', t1),\n    pa.field('s1', t2),\n    pa.field('s2', t4),\n    pa.field('s3', t6),\n]\n\nt7 = pa.struct(fields)\nprint(t7)\n```\n\n----------------------------------------\n\nTITLE: Customized CSV Parsing in PyArrow\nDESCRIPTION: Demonstrates custom parsing options for CSV files with non-standard formats using ParseOptions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/csv.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nimport pyarrow.csv as csv\n\ntable = csv.read_csv('tips.csv.gz', parse_options=csv.ParseOptions(\n   delimiter=\";\",\n   invalid_row_handler=skip_handler\n))\n```\n\n----------------------------------------\n\nTITLE: Querying a Dataset in Python with Arrow\nDESCRIPTION: This snippet demonstrates how to query a dataset and convert it to a pandas DataFrame, including filtering on partition keys.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset.to_table().to_pandas().head(3)\n```\n\nLANGUAGE: python\nCODE:\n```\ndataset.to_table(filter=ds.field(\"part\") == \"b\").to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Working with Large Datasets using Apache Arrow Dataset API in Python\nDESCRIPTION: Illustrates how to use the Dataset API to handle large data efficiently. It shows how to write a partitioned dataset and then read and process it in chunks.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/getstarted.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.dataset as ds\n\nds.write_dataset(birthdays_table, \"savedir\", format=\"parquet\",\n                 partitioning=ds.partitioning(\n                    pa.schema([birthdays_table.schema.field(\"years\")])\n                ))\n\nbirthdays_dataset = ds.dataset(\"savedir\", format=\"parquet\", partitioning=[\"years\"])\n\nbirthdays_dataset.files\n\nimport datetime\n\ncurrent_year = datetime.datetime.utcnow().year\nfor table_chunk in birthdays_dataset.to_batches():\n    print(\"AGES\", pc.subtract(current_year, table_chunk[\"years\"]))\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet from Cloud Storage\nDESCRIPTION: Example of reading Parquet files from Amazon S3 using PyArrow's filesystem interface.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom pyarrow import fs\n\ns3  = fs.S3FileSystem(region=\"us-east-2\")\ntable = pq.read_table(\"bucket/object/key/prefix\", filesystem=s3)\n```\n\n----------------------------------------\n\nTITLE: Querying Parquet File Using Apache Arrow Dataset API in Java\nDESCRIPTION: This Java snippet demonstrates how to query a Parquet file using Apache Arrow's Dataset API. It involves setting scan options, creating a DatasetFactory, and using a Scanner to read data into ArrowRecordBatch objects. Dependencies include Apache Arrow Java library and specific classes like BufferAllocator and Dataset.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/dataset.rst#2025-04-16_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// read data from file /opt/example.parquet\nString uri = \"file:/opt/example.parquet\";\nScanOptions options = new ScanOptions(/*batchSize*/ 32768);\ntry (\n    BufferAllocator allocator = new RootAllocator();\n    DatasetFactory datasetFactory = new FileSystemDatasetFactory(\n            allocator, NativeMemoryPool.getDefault(),\n            FileFormat.PARQUET, uri);\n    Dataset dataset = datasetFactory.finish();\n    Scanner scanner = dataset.newScan(options);\n    ArrowReader reader = scanner.scanBatches()\n) {\n    List<ArrowRecordBatch> batches = new ArrayList<>();\n    while (reader.loadNextBatch()) {\n        try (VectorSchemaRoot root = reader.getVectorSchemaRoot()) {\n            final VectorUnloader unloader = new VectorUnloader(root);\n            batches.add(unloader.getRecordBatch());\n        }\n    }\n\n    // do something with read record batches, for example:\n    analyzeArrowData(batches);\n\n    // finished the analysis of the data, close all resources:\n    AutoCloseables.close(batches);\n} catch (Exception e) {\n    e.printStackTrace();\n}\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Array Properties and Elements in PyArrow\nDESCRIPTION: Shows how to access array properties like type, length, and null count, as well as how to access individual elements using indexing. Demonstrates how None values in Python become NA values in Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\narr.type\nlen(arr)\narr.null_count\narr[0]\narr[2]\n```\n\n----------------------------------------\n\nTITLE: Arrow Replace Functions Table\nDESCRIPTION: ASCII table showing replace functions including fill_null and replace_with_mask operations, detailing their arity, input types and output types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n+--------------------------+------------+-----------------------+--------------+--------------+--------------+-------+\n| Function name            | Arity      | Input type 1          | Input type 2 | Input type 3 | Output type  | Notes |\n+==========================+============+=======================+==============+==============+==============+=======+\n| fill_null_backward       | Unary      | Fixed-width or binary |              |              | Input type 1 | \\(1)  |\n+--------------------------+------------+-----------------------+--------------+--------------+--------------+-------+\n| fill_null_forward        | Unary      | Fixed-width or binary |              |              | Input type 1 | \\(1)  |\n+--------------------------+------------+-----------------------+--------------+--------------+--------------+-------+\n| replace_with_mask        | Ternary    | Fixed-width or binary | Boolean      | Input type 1 | Input type 1 | \\(2)  |\n+--------------------------+------------+-----------------------+--------------+--------------+--------------+-------+\n```\n\n----------------------------------------\n\nTITLE: Table Joins in PyArrow\nDESCRIPTION: Shows how to perform different types of joins (left outer, full outer) between PyArrow tables with single and multiple join keys.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/compute.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\ntable1 = pa.table({'id': [1, 2, 3],\n                     'year': [2020, 2022, 2019]})\n\ntable2 = pa.table({'id': [3, 4],\n                     'n_legs': [5, 100],\n                     'animal': [\"Brittle stars\", \"Centipede\"]})\n\njoined_table = table1.join(table2, keys=\"id\")\n```\n\n----------------------------------------\n\nTITLE: Handling Date Types between Python, Pandas and PyArrow\nDESCRIPTION: Shows how Python's datetime.date objects in pandas Series are handled when converting to and from Arrow arrays, with options for date32 or date64 types and controlling pandas representation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/pandas.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import date\ns = pd.Series([date(2018, 12, 31), None, date(2000, 1, 1)])\ns\n\narr = pa.array(s)\narr.type\narr[0]\n\narr = pa.array(s, type='date64')\narr.type\n\narr.to_pandas()\n\ns2 = pd.Series(arr.to_pandas(date_as_object=False))\ns2.dtype\n```\n\n----------------------------------------\n\nTITLE: Reading Feather Files in PyArrow\nDESCRIPTION: Shows how to read Feather files as either pandas DataFrames or Arrow Tables using the read_feather and read_table functions respectively.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/feather.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Result is pandas.DataFrame\nread_df = feather.read_feather('/path/to/file')\n\n# Result is pyarrow.Table\nread_arrow = feather.read_table('/path/to/file')\n```\n\n----------------------------------------\n\nTITLE: Creating Struct Types with Tuples in PyArrow\nDESCRIPTION: Shows a more concise way to create struct types using (name, type) tuples instead of Field instances. This is a convenience syntax that produces equivalent results.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nt8 = pa.struct([('s0', t1), ('s1', t2), ('s2', t4), ('s3', t6)])\nprint(t8)\nt8 == t7\n```\n\n----------------------------------------\n\nTITLE: Creating an Arrow Table from ChunkedArrays in C++\nDESCRIPTION: Demonstrates how to create an Arrow Table from ChunkedArrays using a Schema. Tables store tabular data made up of ChunkedArrays, which allows them to handle non-contiguous data and exceed the row limit of Arrays and RecordBatches.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/basic_arrow.rst#2025-04-16_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n(Doc section: Table)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Search Results from Arrow Compute Function (C++)\nDESCRIPTION: Extracts and outputs the result from a Datum returned by the 'index' compute function. The result is a Scalar containing a 64-bit integer representing the position of the found value.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\nARROW_ASSIGN_OR_RAISE(auto scalar_result, result.scalar());\nfmt::print(\"index result: {}\\n\", scalar_result->ToString());\n```\n\n----------------------------------------\n\nTITLE: Filtering PyArrow Tables with Expressions\nDESCRIPTION: Demonstrates how to filter PyArrow tables using boolean expressions and compute functions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/compute.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow.compute as pc\n>>> table = pa.table({'nums': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n...                   'chars': [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"l\"]})\n>>> even_filter = (pc.bit_wise_and(pc.field(\"nums\"), pc.scalar(1)) == pc.scalar(0))\n>>> table.filter(even_filter)\npyarrow.Table\nnums: int64\nchars: string\n----\nnums: [[2,4,6,8,10]]\nchars: [[\"b\",\"d\",\"f\",\"h\",\"l\"]]\n```\n\n----------------------------------------\n\nTITLE: Implementing Flight Server in C++\nDESCRIPTION: Example of creating a custom Flight server by subclassing FlightServerBase and implementing the ListFlights RPC method. Shows how to return flight listings using SimpleFlightListing.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/flight.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nclass MyFlightServer : public FlightServerBase {\n  Status ListFlights(const ServerCallContext& context, const Criteria* criteria,\n                     std::unique_ptr<FlightListing>* listings) override {\n    std::vector<FlightInfo> flights = ...;\n    *listings = std::unique_ptr<FlightListing>(new SimpleFlightListing(flights));\n    return Status::OK();\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Basic PyArrow Compute Operations\nDESCRIPTION: Demonstrates basic compute operations using PyArrow's compute module to perform sum and comparison operations on arrays and scalars.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/compute.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow as pa\n>>> import pyarrow.compute as pc\n>>> a = pa.array([1, 1, 2, 3])\n>>> pc.sum(a)\n<pyarrow.Int64Scalar: 7>\n>>> b = pa.array([4, 1, 2, 8])\n>>> pc.equal(a, b)\n<pyarrow.lib.BooleanArray object at 0x7f686e4eef30>\n[\n  false,\n  true,\n  true,\n  false\n]\n>>> x, y = pa.scalar(7.8), pa.scalar(9.3)\n>>> pc.multiply(x, y)\n<pyarrow.DoubleScalar: 72.54>\n```\n\n----------------------------------------\n\nTITLE: Reading from MinIO with Arrow Dataset\nDESCRIPTION: This snippet shows how to read from a MinIO object storage instance that emulates S3 APIs, which is useful for testing or benchmarking.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pyarrow import fs\n\n# By default, MinIO will listen for unencrypted HTTP traffic.\nminio = fs.S3FileSystem(scheme=\"http\", endpoint_override=\"localhost:9000\")\ndataset = ds.dataset(\"voltrondata-labs-datasets/nyc-taxi/\", filesystem=minio)\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Parquet Dataset\nDESCRIPTION: Creates a partitioned dataset using the write_to_dataset function. Demonstrates Hive-style partitioning with partition columns.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntable = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5,\n                  'part': ['a'] * 5 + ['b'] * 5})\npq.write_to_dataset(table, \"parquet_dataset_partitioned\",\n                    partition_cols=['part'])\n```\n\n----------------------------------------\n\nTITLE: Converting a Dataset Scanner to a Table in C++\nDESCRIPTION: This snippet shows how to convert a Scanner to a Table using the ToTable method, allowing all the data in a Dataset to be loaded into memory and manipulated as a single Table.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n// Read the entire dataset as a Table\nauto table_result = scanner->ToTable();\nif (!table_result.ok()) {\n  return table_result.status();\n}\n\n// Get our Table\nstd::shared_ptr<Table> table = table_result.ValueOrDie();\n\n// Work with Table as in other examples\nPrettyPrint(*table);\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow RecordBatch in C++\nDESCRIPTION: This code shows how to create an Arrow RecordBatch using a Schema and Arrays. It demonstrates combining multiple Arrays into a tabular structure with defined column types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/basic_arrow.rst#2025-04-16_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<arrow::RecordBatch> rbatch;\nARROW_ASSIGN_OR_RAISE(rbatch, arrow::RecordBatch::Make(schema, days_data.size(),\n                                       {day_array, month_array, year_array}));\n```\n\n----------------------------------------\n\nTITLE: Creating a ChunkedArray in Apache Arrow C++\nDESCRIPTION: This code demonstrates how to create a ChunkedArray by building multiple arrays and combining them into chunks. It includes error handling and shows how to access basic properties of the ChunkedArray.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/arrays.rst#2025-04-16_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nstd::vector<std::shared_ptr<arrow::Array>> chunks;\nstd::shared_ptr<arrow::Array> array;\n\narrow::Int64Builder builder;\nbuilder.Append(1);\nbuilder.Append(2);\nbuilder.Append(3);\nif (!builder.Finish(&array).ok()) {\n   // ... do something on array building failure\n}\nchunks.push_back(std::move(array));\n\nbuilder.Reset();\nbuilder.AppendNull();\nbuilder.Append(5);\nbuilder.Append(6);\nbuilder.Append(7);\nbuilder.Append(8);\nif (!builder.Finish(&array).ok()) {\n   // ... do something on array building failure\n}\nchunks.push_back(std::move(array));\n\nauto chunked_array = std::make_shared<arrow::ChunkedArray>(std::move(chunks));\n\nassert(chunked_array->num_chunks() == 2);\nassert(chunked_array->length() == 8);\nassert(chunked_array->null_count() == 1);\n```\n\n----------------------------------------\n\nTITLE: Creating a Parquet Dataset with Metadata in Arrow\nDESCRIPTION: This code demonstrates how to create a Dataset from a partitioned Parquet dataset with a _metadata file, which can be more efficient for certain processing frameworks.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndataset = ds.parquet_dataset(\"/path/to/dir/_metadata\")\n```\n\n----------------------------------------\n\nTITLE: Reading Arrow Random Access Files in Java\nDESCRIPTION: This snippet shows how to read data from an Arrow random access file using `ArrowFileReader` in Java. It creates an `ArrowFileReader` with a seekable input channel, retrieves the record blocks, loads a specific record batch, and accesses the data through the `VectorSchemaRoot`.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/ipc.rst#2025-04-16_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\ntry (ArrowFileReader reader = new ArrowFileReader(\n    new ByteArrayReadableSeekableByteChannel(out.toByteArray()), allocator)) {\n\n  // read the 4-th batch\n  ArrowBlock block = reader.getRecordBlocks().get(3);\n  reader.loadRecordBatch(block);\n  VectorSchemaRoot readBatch = reader.getVectorSchemaRoot();\n}\n```\n\n----------------------------------------\n\nTITLE: Table Sorting with PyArrow\nDESCRIPTION: Shows how to sort a PyArrow table using sort_indices compute function with custom sort keys.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/compute.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow as pa\n>>> import pyarrow.compute as pc\n>>> t = pa.table({'x':[1,2,3],'y':[3,2,1]})\n>>> i = pc.sort_indices(t, sort_keys=[('y', 'ascending')])\n>>> i\n<pyarrow.lib.UInt64Array object at 0x7fcee5df75e8>\n[\n  2,\n  1,\n  0\n]\n```\n\n----------------------------------------\n\nTITLE: Defining a Numba CUDA Kernel for Array Processing\nDESCRIPTION: Defines a simple Numba CUDA kernel that increments each element of an integer array by one. This kernel will be used to process data in GPU memory.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/cuda.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numba.cuda\n\n@numba.cuda.jit\ndef increment_by_one(an_array):\n    pos = numba.cuda.grid(1)\n    if pos < an_array.size:\n        an_array[pos] += 1\n```\n\n----------------------------------------\n\nTITLE: Writing an Arrow Table to a Parquet File\nDESCRIPTION: Demonstrates how to write an Arrow Table to a Parquet file in a single operation. The example specifies the Table to write, memory pool to use, output file path, and chunk size.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n  PARQUET_THROW_NOT_OK(\n      parquet::arrow::WriteTable(*table, pool, \"test_write.parquet\", 2048));\n```\n\n----------------------------------------\n\nTITLE: Defining a Partition Schema for Dataset Writing in C++\nDESCRIPTION: This code creates a Schema for partitioning a Dataset based on a single column 'a'. The Schema defines which fields will be used as partitioning keys when writing the Dataset.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n// Create our partitioning schema\nstd::shared_ptr<Schema> partition_schema = arrow::schema({field(\"a\", int64())});\n```\n\n----------------------------------------\n\nTITLE: Including Necessary Headers for Arrow Dataset Operations in C++\nDESCRIPTION: This snippet includes the required C++ headers for working with Arrow datasets, including iostream for output and various Arrow headers for dataset, filesystem, and I/O operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <iostream>\n\n#include \"arrow/api.h\"\n#include \"arrow/csv/api.h\"\n#include \"arrow/dataset/api.h\"\n#include \"arrow/filesystem/api.h\"\n#include \"arrow/io/api.h\"\n#include \"arrow/ipc/api.h\"\n#include \"arrow/result.h\"\n#include \"arrow/status.h\"\n#include \"arrow/util/vector.h\"\n\n#include \"parquet/arrow/reader.h\"\n#include \"parquet/arrow/writer.h\"\n```\n\n----------------------------------------\n\nTITLE: Writing Partitioned Data with Arrow in Python\nDESCRIPTION: Shows how to write a table to a partitioned dataset based on a column value using Hive-style partitioning. This creates separate directories for each unique value in the partition column.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npart = ds.partitioning(\n    pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n)\nds.write_dataset(table, \"partitioned_dataset\", format=\"parquet\", partitioning=part)\n```\n\n----------------------------------------\n\nTITLE: Creating ListView Arrays in PyArrow\nDESCRIPTION: Demonstrates creating a ListView array, which is an alternative to List arrays with a different memory layout and capabilities.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnested_arr = pa.array([[], None, [1, 2], [None, 1]], type=pa.list_view(pa.int64()))\nprint(nested_arr.type)\n```\n\n----------------------------------------\n\nTITLE: Reading Full Parquet File into Arrow Table\nDESCRIPTION: Example showing how to read an entire Parquet file into an Arrow Table using FileReader. This demonstrates the basic file reading workflow with error handling.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/parquet.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\narrow::Status ReadFullFile(\n  std::shared_ptr<arrow::io::RandomAccessFile> input_file) {\n  std::unique_ptr<parquet::arrow::FileReader> arrow_reader;\n  ARROW_RETURN_NOT_OK(\n      parquet::arrow::OpenFile(input_file, arrow::default_memory_pool(), &arrow_reader));\n  std::shared_ptr<arrow::Table> table;\n  ARROW_RETURN_NOT_OK(arrow_reader->ReadTable(&table));\n  return arrow::Status::OK();\n}\n```\n\n----------------------------------------\n\nTITLE: Writing Arrow Dataset with File Visitor\nDESCRIPTION: Demonstrates writing a partitioned dataset with a file visitor function that gets called for each file created. This allows tracking of all files in the dataset without directory scanning later.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nds.write_dataset(table, \"dataset_visited\", format=\"parquet\", partitioning=part,\n                 file_visitor=file_visitor)\n```\n\n----------------------------------------\n\nTITLE: Building Dataset from FileSystemDatasetFactory in Arrow C++\nDESCRIPTION: This snippet builds the Dataset object using the FileSystemDatasetFactory. It demonstrates how to create an in-memory representation of the dataset on disk.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nARROW_ASSIGN_OR_RAISE(auto dataset, factory->Finish());\n```\n\n----------------------------------------\n\nTITLE: Reading JSON File as Table using TableReader in C++\nDESCRIPTION: Demonstrates how to read an entire JSON file into an Arrow Table using TableReader. The code shows initialization of reader options, creating a TableReader instance, and reading the complete table while handling potential errors.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/json.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"arrow/json/api.h\"\n\n{\n   // ...\n   arrow::MemoryPool* pool = default_memory_pool();\n   std::shared_ptr<arrow::io::InputStream> input = ...;\n\n   auto read_options = arrow::json::ReadOptions::Defaults();\n   auto parse_options = arrow::json::ParseOptions::Defaults();\n\n   // Instantiate TableReader from input stream and options\n   auto maybe_reader = arrow::json::TableReader::Make(pool, input, read_options, parse_options);\n   if (!maybe_reader.ok()) {\n      // Handle TableReader instantiation error...\n   }\n   auto reader = *maybe_reader;\n\n   // Read table from JSON file\n   auto maybe_table = reader->Read();\n   if (!maybe_table.ok()) {\n      // Handle JSON read error\n      // (for example a JSON syntax error or failed type conversion)\n   }\n   auto table = *maybe_table;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Record Batch in Python Arrow\nDESCRIPTION: Creates a sample record batch with three arrays containing integers, strings, and booleans using PyArrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/ipc.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\ndata = [\n    pa.array([1, 2, 3, 4]),\n    pa.array(['foo', 'bar', 'baz', None]),\n    pa.array([True, None, False, True])\n]\n\nbatch = pa.record_batch(data, names=['f0', 'f1', 'f2'])\nbatch.num_rows\nbatch.num_columns\n```\n\n----------------------------------------\n\nTITLE: Reading Data from HDFS with Apache Arrow Dataset API in Java\nDESCRIPTION: This snippet describes how to read data from HDFS using Apache Arrow's Dataset API in Java. It involves passing an HDFS URI to the FileSystemDatasetFactory and requires the use of BufferAllocator and DatasetFactory classes from the Apache Arrow library. This method assumes HDFS support is available in the Arrow Java package.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/dataset.rst#2025-04-16_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nString uri = \"hdfs://{hdfs_host}:{port}/data/example.parquet\";\nBufferAllocator allocator = new RootAllocator(Long.MAX_VALUE);\nDatasetFactory factory = new FileSystemDatasetFactory(allocator,\n    NativeMemoryPool.getDefault(), FileFormat.PARQUET, uri);\n\n```\n\n----------------------------------------\n\nTITLE: Splitting Parquet Dataset Fragments by Row Group in Arrow\nDESCRIPTION: This snippet shows how to split fragments of a Parquet dataset by row group, which allows for more granular control over data loading.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfragments = list(dataset.get_fragments())\nfragments[0].split_by_row_group()\n```\n\n----------------------------------------\n\nTITLE: Connecting to Azure Blob Storage with PyArrow\nDESCRIPTION: Creates a connection to an Azure Blob Storage account using PyArrow's AzureFileSystem interface. This example shows basic initialization with an account name and demonstrates listing files and reading data from Azure storage.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from pyarrow import fs\n>>> azure_fs = fs.AzureFileSystem(account_name='myaccount')\n\n# List all contents in a container, recursively\n>>> azure_fs.get_file_info(fs.FileSelector('my-container', recursive=True))\n[<FileInfo for 'my-container/File1': type=FileType.File, size=10>,\n <FileInfo for 'my-container/File2': type=FileType.File, size=20>,\n <FileInfo for 'my-container/Dir1': type=FileType.Directory>,\n <FileInfo for 'my-container/Dir1/File3': type=FileType.File, size=30>]\n\n# Open a file for reading and download its contents\n>>> f = azure_fs.open_input_stream('my-container/File1')\n>>> f.readall()\nb'some data'\n```\n\n----------------------------------------\n\nTITLE: Basic CSV Reading in PyArrow\nDESCRIPTION: Demonstrates basic CSV file reading using PyArrow's csv module, including loading data and converting to pandas DataFrame.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/csv.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from pyarrow import csv\n>>> fn = 'tips.csv.gz'\n>>> table = csv.read_csv(fn)\n>>> table\npyarrow.Table\ntotal_bill: double\ntip: double\nsex: string\nsmoker: string\nday: string\ntime: string\nsize: int64\n>>> len(table)\n244\n>>> df = table.to_pandas()\n>>> df.head()\n   total_bill   tip     sex smoker  day    time  size\n0       16.99  1.01  Female     No  Sun  Dinner     2\n1       10.34  1.66    Male     No  Sun  Dinner     3\n2       21.01  3.50    Male     No  Sun  Dinner     3\n3       23.68  3.31    Male     No  Sun  Dinner     2\n4       24.59  3.61  Female     No  Sun  Dinner     4\n```\n\n----------------------------------------\n\nTITLE: Serializing and Deserializing Records with Extension Types in PyArrow\nDESCRIPTION: This code demonstrates writing a record batch with a custom extension type to a stream and reading it back while preserving the type information.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> with pa.RecordBatchStreamWriter(sink, batch.schema) as writer:\n...    writer.write_batch(batch)\n>>> buf = sink.getvalue()\n\n>>> with pa.ipc.open_stream(buf) as reader:\n...    result = reader.read_all()\n>>> result.column(\"ext\").type\nRationalType(StructType(struct<numer: int32, denom: int32>))\n```\n\n----------------------------------------\n\nTITLE: Defining a File Visitor Function for Arrow Dataset Writing\nDESCRIPTION: Creates a file visitor function that prints information about each file created during dataset writing, including the path, size, and metadata. This can be useful for tracking files in partitioned datasets.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef file_visitor(written_file):\n    print(f\"path={written_file.path}\")\n    print(f\"size={written_file.size} bytes\")\n    print(f\"metadata={written_file.metadata}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a KMS Client for Parquet Encryption in PyArrow\nDESCRIPTION: Provides a skeleton for implementing a Key Management System (KMS) client class required for Parquet encryption. The implementation must handle wrapping and unwrapping encryption keys with the KMS system.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.parquet.encryption as pe\n\nclass MyKmsClient(pe.KmsClient):\n\n   \"\"\"An example KmsClient implementation skeleton\"\"\"\n   def __init__(self, kms_connection_configuration):\n      pe.KmsClient.__init__(self)\n      # Any KMS-specific initialization based on\n      # kms_connection_configuration comes here\n\n   def wrap_key(self, key_bytes, master_key_identifier):\n      wrapped_key = ... # call KMS to wrap key_bytes with key specified by\n                        # master_key_identifier\n      return wrapped_key\n\n   def unwrap_key(self, wrapped_key, master_key_identifier):\n      key_bytes = ... # call KMS to unwrap wrapped_key with key specified by\n                      # master_key_identifier\n      return key_bytes\n```\n\n----------------------------------------\n\nTITLE: Converting PyArrow Array to NumPy Array Using DLPack\nDESCRIPTION: This example demonstrates how to convert a PyArrow array into a NumPy array using the DLPack protocol, which enables zero-copy data sharing between the two libraries. The array is first created in PyArrow and then converted using NumPy's from_dlpack function.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dlpack.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow as pa\n>>> array = pa.array([2, 0, 2, 4])\n<pyarrow.lib.Int64Array object at 0x121fd4880>\n[\n2,\n0,\n2,\n4\n]\n\n>>> import numpy as np\n>>> np.from_dlpack(array)\narray([2, 0, 2, 4])\n```\n\n----------------------------------------\n\nTITLE: CSV Reading with Custom Options in PyArrow\nDESCRIPTION: Demonstrates reading CSV files with custom ReadOptions for column names and row skipping.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/csv.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nimport pyarrow.csv as csv\n\ntable = csv.read_csv('tips.csv.gz', read_options=csv.ReadOptions(\n   column_names=[\"animals\", \"n_legs\", \"entry\"],\n   skip_rows=1\n))\n```\n\n----------------------------------------\n\nTITLE: Specifying Projections in Apache Arrow Dataset API in Java\nDESCRIPTION: This Java code snippet demonstrates how to specify a subset of columns to be projected using ScanOptions in Apache Arrow's Dataset API. Users can define which columns to project, and if none are needed, all columns will be emitted. This snippet assumes familiarity with Java and the Apache Arrow library.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/dataset.rst#2025-04-16_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nString[] projection = new String[] {\"id\", \"name\"};\nScanOptions options = new ScanOptions(32768, Optional.of(projection));\n\n```\n\n----------------------------------------\n\nTITLE: Creating CUDA Buffer from NumPy Array in PyArrow\nDESCRIPTION: Demonstrates how to create a CUDA buffer by copying data from a NumPy array (host memory) to GPU device memory using Context.buffer_from_data. It also shows how to check the buffer's properties.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/cuda.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> arr = np.arange(4, dtype=np.int32)\n>>> arr.nbytes\n16\n>>> cuda_buf = ctx.buffer_from_data(arr)\n>>> type(cuda_buf)\npyarrow._cuda.CudaBuffer\n>>> cuda_buf.size     # The buffer's size in bytes\n16\n>>> cuda_buf.address  # The buffer's address in device memory\n30088364544\n>>> cuda_buf.context.device_number\n0\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Writer Properties\nDESCRIPTION: Example showing how to configure Parquet writer properties including compression, version, page size and other options using WriterProperties::Builder.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/parquet.rst#2025-04-16_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<WriterProperties> props = WriterProperties::Builder()\n   .max_row_group_length(64 * 1024)\n   .created_by(\"My Application\")\n   .version(ParquetVersion::PARQUET_2_6)\n   .data_page_version(ParquetDataPageVersion::V2)\n   .compression(Compression::SNAPPY)\n   .build();\n```\n\n----------------------------------------\n\nTITLE: Writing Parquet File in Batches\nDESCRIPTION: Example demonstrating how to write record batches incrementally to a Parquet file using FileWriter for better memory efficiency.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/parquet.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\narrow::Status WriteInBatches(\n  const std::shared_ptr<arrow::Schema>& schema,\n  const std::vector<std::shared_ptr<arrow::RecordBatch>>& batches) {\n  std::unique_ptr<parquet::arrow::FileWriter> writer;\n  ARROW_ASSIGN_OR_THROW(\n      output,\n      arrow::io::FileOutputStream::Open(\"test.parquet\"));\n  ARROW_RETURN_NOT_OK(parquet::arrow::FileWriter::Open(\n      *schema, arrow::default_memory_pool(), output, &writer));\n  for (const auto& batch : batches) {\n    ARROW_RETURN_NOT_OK(writer->WriteRecordBatch(*batch));\n  }\n  ARROW_RETURN_NOT_OK(writer->Close());\n  return arrow::Status::OK();\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Encrypted Parquet Files in PyArrow\nDESCRIPTION: Shows how to read an encrypted Parquet file by creating decryption properties using a CryptoFactory with KMS connection configuration. These properties are then passed to the ParquetFile constructor.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndecryption_properties = crypto_factory.file_decryption_properties(\n                                                 kms_connection_config)\nparquet_file = pq.ParquetFile(filename,\n                              decryption_properties=decryption_properties)\n```\n\n----------------------------------------\n\nTITLE: Using the Custom Rational Type Extension in PyArrow\nDESCRIPTION: Example showing how to instantiate a RationalType extension, inspect its properties, create arrays with this type, and wrap storage arrays with the extension type. Also demonstrates creating ExtensionArray objects.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> rational_type = RationalType(pa.int32())\n>>> rational_type.extension_name\n'my_package.rational'\n>>> rational_type.storage_type\nStructType(struct<numer: int32, denom: int32>)\n\n>>> storage_array = pa.array(\n...     [\n...         {\"numer\": 10, \"denom\": 17},\n...         {\"numer\": 20, \"denom\": 13},\n...     ],\n...     type=rational_type.storage_type,\n... )\n>>> arr = rational_type.wrap_array(storage_array)\n>>> # or equivalently\n>>> arr = pa.ExtensionArray.from_storage(rational_type, storage_array)\n>>> arr\n<pyarrow.lib.ExtensionArray object at 0x1067f5420>\n-- is_valid: all not null\n-- child 0 type: int32\n  [\n    10,\n    20\n  ]\n-- child 1 type: int32\n  [\n    17,\n    13\n  ]\n```\n\n----------------------------------------\n\nTITLE: Using Red Gandiva with Arrow Tables and Expressions\nDESCRIPTION: Example demonstrating how to use Red Gandiva to create Arrow tables, build expressions, and evaluate them using a projector. Shows creation of simple arithmetic and conditional expressions on table columns.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-gandiva/README.md#2025-04-16_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"gandiva\"\n\ntable = Arrow::Table.new(:field1 => Arrow::Int32Array.new([1, 2, 3, 4]),\n                         :field2 => Arrow::Int32Array.new([11, 13, 15, 17]))\nschema = table.schema\n\nexpression1 = schema.build_expression do |record|\n  record.field1 + record.field2\nend\n\nexpression2 = schema.build_expression do |record, context|\n  context.if(record.field1 > record.field2)\n    .then(record.field1 / record.field2)\n    .else(record.field1)\nend\n\nprojector = Gandiva::Projector.new(schema, [expression1, expression2])\ntable.each_record_batch do |record_batch|\n  outputs = projector.evaluate(record_batch)\n  puts outputs.collect(&:values)\nend\n```\n\n----------------------------------------\n\nTITLE: Writing Partitioned Parquet Datasets\nDESCRIPTION: Examples of writing partitioned datasets to local and remote filesystems using PyArrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Local dataset write\npq.write_to_dataset(table, root_path='dataset_name',\n                    partition_cols=['one', 'two'])\n\n# Remote file-system example\nfrom pyarrow.fs import HadoopFileSystem\nfs = HadoopFileSystem(host, port, user=user, kerb_ticket=ticket_cache_path)\npq.write_to_dataset(table, root_path='dataset_name',\n                    partition_cols=['one', 'two'], filesystem=fs)\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet Files from Azure Blob Storage in PyArrow\nDESCRIPTION: Shows how to read a Parquet table from Azure Blob Storage using the adlfs package, which provides an fsspec-compatible implementation for Azure Blob Storage. This requires instantiating the filesystem object explicitly.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom adlfs import AzureBlobFileSystem\n\nabfs = AzureBlobFileSystem(account_name=\"XXXX\", account_key=\"XXXX\", container_name=\"XXXX\")\ntable = pq.read_table(\"file.parquet\", filesystem=abfs)\n```\n\n----------------------------------------\n\nTITLE: Creating Dictionary Arrays in PyArrow\nDESCRIPTION: Illustrates creation of dictionary arrays using indices and dictionary values, similar to R factors or pandas Categoricals.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nindices = pa.array([0, 1, 0, 1, 2, 0, None, 2])\ndictionary = pa.array(['foo', 'bar', 'baz'])\ndict_array = pa.DictionaryArray.from_arrays(indices, dictionary)\n```\n\n----------------------------------------\n\nTITLE: Reading JSON File with pyarrow.json in Python\nDESCRIPTION: This snippet demonstrates how to read a JSON file using the read_json function from pyarrow.json module. It shows the basic usage and the resulting table structure.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/json.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from pyarrow import json\n>>> fn = 'my_data.json'\n>>> table = json.read_json(fn)\n>>> table\npyarrow.Table\na: int64\nb: double\nc: string\nd: bool\n>>> table.to_pandas()\n   a    b     c      d\n0  1  2.0   foo  False\n1  4 -5.5  None   True\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Parquet Dataset in Python\nDESCRIPTION: Creates a sample dataset by writing an Arrow Table into two Parquet files. Uses temporary directory and demonstrates basic table creation with multiple columns.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\nimport pathlib\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport numpy as np\n\nbase = pathlib.Path(tempfile.mkdtemp(prefix=\"pyarrow-\"))\n(base / \"parquet_dataset\").mkdir(exist_ok=True)\n\n# creating an Arrow Table\ntable = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n\n# writing it into two parquet files\npq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\npq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Using Arrow Compute Functions with Options in C++\nDESCRIPTION: Example showing how to call an Arrow compute function with additional options structure to control function behavior. Uses min_max function with ScalarAggregateOptions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nScalarAggregateOptions scalar_aggregate_options;\nscalar_aggregate_options.skip_nulls = false;\n\nstd::shared_ptr<arrow::Array> array = ...;\narrow::Datum min_max;\n\nARROW_ASSIGN_OR_RAISE(min_max,\n                       arrow::compute::CallFunction(\"min_max\", {array},\n                                                    &scalar_aggregate_options));\n\n// Unpack struct scalar result (a two-field {\"min\", \"max\"} scalar)\nstd::shared_ptr<arrow::Scalar> min_value, max_value;\nmin_value = min_max.scalar_as<arrow::StructScalar>().value[0];\nmax_value = min_max.scalar_as<arrow::StructScalar>().value[1];\n```\n\n----------------------------------------\n\nTITLE: Creating Directory Partitioning in C++ with Apache Arrow\nDESCRIPTION: Shows how to create directory partitioning in Apache Arrow by specifying field names 'year', 'month', and 'day'. It illustrates constructing a directory partitioning scheme without inferring field names from file paths.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nauto part = ds::DirectoryPartitioning::MakeFactory({\"year\", \"month\", \"day\"});\n```\n\n----------------------------------------\n\nTITLE: Creating a Schema in PyArrow\nDESCRIPTION: Demonstrates creating a Schema object which defines column names and types for tabular data structures. Schemas are used to describe the structure of record batches and tables.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmy_schema = pa.schema([('field0', t1),\n                       ('field1', t2),\n                       ('field2', t4),\n                       ('field3', t6)])\nmy_schema\n```\n\n----------------------------------------\n\nTITLE: Converting Arrow Table to STL Tuple Range\nDESCRIPTION: Shows how to convert an Arrow Table back into a pre-allocated vector of std::tuple objects using TupleRangeFromTable. The table columns must match the tuple structure in order.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/tuple_range_conversion.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<Table> table = ..;\n\n// The range needs to be pre-allocated to the respective amount of rows.\n// This allows us to pass in an arbitrary range object, not only\n// `std::vector`.\nstd::vector<std::tuple<double, std::string>> rows(2);\nif (!arrow::stl::TupleRangeFromTable(*table, &rows).ok()) {\n  // Error handling code should go here.\n}\n```\n\n----------------------------------------\n\nTITLE: Resource Management for Apache Arrow Dataset API in Java\nDESCRIPTION: This Java snippet highlights the importance of manual resource management when using Apache Arrow's Dataset API due to JNI dependencies. It demonstrates using a try-with-resources block to ensure all native resources are properly closed, preventing memory leaks. This is crucial for efficient resource usage in Java applications using Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/dataset.rst#2025-04-16_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nString uri = \"file:/opt/example.parquet\";\nScanOptions options = new ScanOptions(/*batchSize*/ 32768);\ntry (\n    BufferAllocator allocator = new RootAllocator();\n    DatasetFactory factory = new FileSystemDatasetFactory(\n            allocator, NativeMemoryPool.getDefault(),\n            FileFormat.PARQUET, uri);\n    Dataset dataset = factory.finish();\n    Scanner scanner = dataset.newScan(options)\n) {\n\n    // do something\n\n}\n\n```\n\n----------------------------------------\n\nTITLE: Converting RecordBatch with Null Values to Tensor in C++\nDESCRIPTION: This snippet shows how to convert a RecordBatch containing null values to a Tensor. By setting 'null_to_nan' to true, all types are promoted to floating-point, allowing successful conversion of null values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/converting_recordbatch_to_tensor.rst#2025-04-16_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nstd::shared_ptr<RecordBatch> batch;\n\nASSERT_OK_AND_ASSIGN(auto tensor, batch->ToTensor(/*null_to_nan=*/true));\nASSERT_OK(tensor->Validate());\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet File in Batches\nDESCRIPTION: Example demonstrating how to read a Parquet file as a stream of record batches for better memory efficiency using GetRecordBatchReader.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/parquet.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\narrow::Status ReadInBatches(\n  std::shared_ptr<arrow::io::RandomAccessFile> input_file) {\n  ARROW_ASSIGN_OR_THROW(\n      batches_reader,\n      arrow_reader->GetRecordBatchReader());\n  return arrow::Status::OK();\n}\n```\n\n----------------------------------------\n\nTITLE: Joining Arrow Tables\nDESCRIPTION: Shows how to join two Arrow tables based on a common key column using the join method, similar to SQL JOIN operations between tables.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_11\n\nLANGUAGE: ruby\nCODE:\n```\namounts = Arrow::Table.new(\n  'name' => ['Tom', 'Max', 'Kate'],\n  'amount' => [10, 2, 3]\n)\nlevels = Arrow::Table.new(\n  'name' => ['Max', 'Kate', 'Tom'],\n  'level' => [1, 9, 5]\n)\namounts.join(levels, [:name])\n# => #<Arrow::Table:0x55d512ceb1b0 ptr=0x55d51262aa70>\n# \tname\tamount\tname\tlevel\n# 0\tTom \t    10\tTom \t    5\n# 1\tMax \t     2\tMax \t    1\n# 2\tKate\t     3\tKate\t    9\n```\n\n----------------------------------------\n\nTITLE: Grouped Aggregations in PyArrow\nDESCRIPTION: Demonstrates grouped aggregation operations on PyArrow tables using group_by method with different aggregation functions and options.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/compute.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow as pa\n>>> t = pa.table([\n...       pa.array([\"a\", \"a\", \"b\", \"b\", \"c\"]),\n...       pa.array([1, 2, 3, 4, 5]),\n... ], names=[\"keys\", \"values\"])\n>>> t.group_by(\"keys\").aggregate([(\"values\", \"sum\")])\npyarrow.Table\nvalues_sum: int64\nkeys: string\n----\nvalues_sum: [[3,7,5]]\nkeys: [[\"a\",\"b\",\"c\"]]\n```\n\n----------------------------------------\n\nTITLE: Creating a Schema in Apache Arrow Java\nDESCRIPTION: Shows how to create a Schema object with two columns: an int32 column 'A' and a utf8-encoded string column 'B', along with schema-wide metadata.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector_schema_root.rst#2025-04-16_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.apache.arrow.vector.types.pojo.ArrowType;\nimport org.apache.arrow.vector.types.pojo.Field;\nimport org.apache.arrow.vector.types.pojo.FieldType;\nimport org.apache.arrow.vector.types.pojo.Schema;\nimport static java.util.Arrays.asList;\n\nMap<String, String> metadata = new HashMap<>();\nmetadata.put(\"K1\", \"V1\");\nmetadata.put(\"K2\", \"V2\");\nField a = new Field(\"A\", FieldType.nullable(new ArrowType.Int(32, true)), null);\nField b = new Field(\"B\", FieldType.nullable(new ArrowType.Utf8()), null);\nSchema schema = new Schema(asList(a, b), metadata);\n```\n\n----------------------------------------\n\nTITLE: Working with Pandas Categorical Data in PyArrow\nDESCRIPTION: Shows how pandas categorical columns are converted to Arrow dictionary arrays, which are optimized for repeated values with a limited set of possibilities.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/pandas.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"cat\": pd.Categorical([\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"])})\ndf.cat.dtype.categories\ndf\n\ntable = pa.Table.from_pandas(df)\ntable\n\ncolumn = table[0]\nchunk = column.chunk(0)\nchunk.dictionary\nchunk.indices\n```\n\n----------------------------------------\n\nTITLE: Custom Type Conversion in PyArrow CSV\nDESCRIPTION: Shows how to specify custom data type conversions when reading CSV files using ConvertOptions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/csv.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nimport pyarrow.csv as csv\n\ntable = csv.read_csv('tips.csv.gz', convert_options=csv.ConvertOptions(\n    column_types={\n        'total_bill': pa.decimal128(precision=10, scale=2),\n        'tip': pa.decimal128(precision=10, scale=2),\n    }\n))\n```\n\n----------------------------------------\n\nTITLE: Resolving S3 Region in PyArrow\nDESCRIPTION: Examples of different methods to resolve the AWS region for S3FileSystem, including using the resolve_s3_region function and the from_uri method. This helps with properly configuring S3 connections.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from pyarrow import fs\n>>> s3 = fs.S3FileSystem(region=fs.resolve_s3_region('my-test-bucket'))\n\n# Or via URI:\n>>> s3, path = fs.S3FileSystem.from_uri('s3://[access_key:secret_key@]bucket/path]')\n```\n\n----------------------------------------\n\nTITLE: Writing Data with PyArrow Output Stream\nDESCRIPTION: Demonstrates writing data to disk using PyArrow's output_stream interface\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/memory.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith pa.output_stream('example1.dat') as stream:\n    stream.write(b'some data')\n\nf = open('example1.dat', 'rb')\nf.read()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Scalar Conversion for Extension Types\nDESCRIPTION: This example shows how to implement custom scalar conversion for extension types by creating a custom ExtensionScalar subclass.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import namedtuple\n\nPoint3D = namedtuple(\"Point3D\", [\"x\", \"y\", \"z\"])\n\nclass Point3DScalar(pa.ExtensionScalar):\n    def as_py(self) -> Point3D:\n        return Point3D(*self.value.as_py())\n\nclass Point3DType(pa.ExtensionType):\n    def __init__(self):\n        super().__init__(pa.list_(pa.float32(), 3), \"my_package.Point3DType\")\n\n    def __arrow_ext_serialize__(self):\n        return b\"\"\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        return Point3DType()\n\n    def __arrow_ext_scalar_class__(self):\n        return Point3DScalar\n```\n\n----------------------------------------\n\nTITLE: Writing CSV Files in C++\nDESCRIPTION: Demonstrates two methods for writing CSV files: one-shot table writing and incremental batch writing. Includes error handling for file operations and writer management.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/csv.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n#include <arrow/csv/api.h>\n{\n    // Oneshot write\n    // ...\n    std::shared_ptr<arrow::io::OutputStream> output = ...;\n    auto write_options = arrow::csv::WriteOptions::Defaults();\n    if (WriteCSV(table, write_options, output.get()).ok()) {\n        // Handle writer error...\n    }\n}\n{\n    // Write incrementally\n    // ...\n    std::shared_ptr<arrow::io::OutputStream> output = ...;\n    auto write_options = arrow::csv::WriteOptions::Defaults();\n    auto maybe_writer = arrow::csv::MakeCSVWriter(output, schema, write_options);\n    if (!maybe_writer.ok()) {\n        // Handle writer instantiation error...\n    }\n    std::shared_ptr<arrow::ipc::RecordBatchWriter> writer = *maybe_writer;\n\n    // Write batches...\n    if (!writer->WriteRecordBatch(*batch).ok()) {\n        // Handle write error...\n    }\n\n    if (!writer->Close().ok()) {\n        // Handle close error...\n    }\n    if (!output->Close().ok()) {\n        // Handle file close error...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet File Subsets\nDESCRIPTION: Shows how to read specific columns from a Parquet file and maintain index data when reading from pandas-sourced files.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npq.read_table('example.parquet', columns=['one', 'three'])\n\npq.read_pandas('example.parquet', columns=['two']).to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Using User-Defined Functions in PyArrow Dataset Projections\nDESCRIPTION: Example of using a user-defined function in a dataset projection to create a new column. Shows how to reference a UDF in an expression and apply it to dataset fields.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/compute.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow.dataset as ds\n>>> data_table = pa.table({'category': ['A', 'B', 'C', 'D'], 'value': [90, 630, 1827, 2709]})\n>>> dataset = ds.dataset(data_table)\n>>> func_args = [pc.scalar(30), ds.field(\"value\")]\n>>> dataset.to_table(\n...             columns={\n...                 'gcd_value': ds.field('')._call(\"numpy_gcd\", func_args),\n...                 'value': ds.field('value'),\n...                 'category': ds.field('category')\n...             })\npyarrow.Table\ngcd_value: int64\nvalue: int64\ncategory: string\n----\ngcd_value: [[30,30,3,3]]\nvalue: [[90,630,1827,2709]]\ncategory: [[\"A\",\"B\",\"C\",\"D\"]]\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Flight Server in Python\nDESCRIPTION: Example of creating a Flight server subclass with a list_flights method implementation. The server handles RPC requests and can yield Flight information to clients.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/flight.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.flight as flight\n\nclass MyFlightServer(flight.FlightServerBase):\n    def list_flights(self, context, criteria):\n        info = flight.FlightInfo(...)\n        yield info\n```\n\n----------------------------------------\n\nTITLE: Using LocalFileSystem in PyArrow\nDESCRIPTION: Example of writing to and reading from the local filesystem using PyArrow's LocalFileSystem class. This demonstrates the basic stream operations for file I/O.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from pyarrow import fs\n>>> local = fs.LocalFileSystem()\n>>> with local.open_output_stream('/tmp/pyarrowtest.dat') as stream:\n        stream.write(b'data')\n4\n>>> with local.open_input_stream('/tmp/pyarrowtest.dat') as stream:\n        print(stream.readall())\nb'data'\n```\n\n----------------------------------------\n\nTITLE: Converting PyArrow Array to PyTorch Tensor Using DLPack\nDESCRIPTION: This example shows how to convert a PyArrow array into a PyTorch tensor using the DLPack protocol. This allows for efficient data transfer between PyArrow and PyTorch without creating a copy of the data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dlpack.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> torch.from_dlpack(array)\ntensor([2, 0, 2, 4])\n```\n\n----------------------------------------\n\nTITLE: Using fsspec-compatible Google Cloud Storage with PyArrow\nDESCRIPTION: Demonstrates how to use an fsspec-compatible Google Cloud Storage filesystem with PyArrow. This example shows creating a GCSFileSystem object and using it to read a partitioned dataset with PyArrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# creating an fsspec-based filesystem object for Google Cloud Storage\nimport gcsfs\nfs = gcsfs.GCSFileSystem(project='my-google-project')\n\n# using this to read a partitioned dataset\nimport pyarrow.dataset as ds\nds.dataset(\"data/\", filesystem=fs)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Tables with Apache Arrow Parquet in Python\nDESCRIPTION: Shows how to save a table to a Parquet file and then load it back. This demonstrates the ease of persisting and retrieving data using Arrow's Parquet integration.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/getstarted.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.parquet as pq\n\npq.write_table(birthdays_table, 'birthdays.parquet')\n\nreloaded_birthdays = pq.read_table('birthdays.parquet')\n\nreloaded_birthdays\n```\n\n----------------------------------------\n\nTITLE: Basic Dataset Writing with Arrow in Python\nDESCRIPTION: Demonstrates how to write a table to a dataset using the ds.write_dataset function with the Parquet format. This creates a single file named part-0.parquet in the specified directory.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntable = pa.table({\"a\": range(10), \"b\": np.random.randn(10), \"c\": [1, 2] * 5})\nds.write_dataset(table, \"sample_dataset\", format=\"parquet\")\n```\n\n----------------------------------------\n\nTITLE: Using Red Parquet to Load and Save Parquet Files in Ruby\nDESCRIPTION: Example of loading a Parquet file into an Arrow Table, processing the data, and saving it back to a different file. This demonstrates the basic usage pattern for working with Parquet files in Ruby.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-parquet/README.md#2025-04-16_snippet_1\n\nLANGUAGE: Ruby\nCODE:\n```\nrequire \"parquet\"\n\ntable = Arrow::Table.load(\"/dev/shm/data.parquet\")\n# Process data in table\ntable.save(\"/dev/shm/data-processed.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Table for Computation in C++\nDESCRIPTION: Generates an Arrow Table with two columns of integer data for use in compute examples.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nstd::shared_ptr<arrow::Table> table;\nARROW_ASSIGN_OR_RAISE(\n    table,\n    CreateTable({\"A\", \"B\"},\n               {\n                   {1, 2, 3, 4, 5},        // Column A\n                   {75375, 645, 2284, 5667, 5087}  // Column B\n               },\n               arrow::int32()));\n```\n\n----------------------------------------\n\nTITLE: Inspecting Dataset Schema Using Apache Arrow in Java\nDESCRIPTION: This code snippet illustrates how to inspect the schema of a dataset before reading it using Apache Arrow's DatasetFactory in Java. The inspection is performed with or without a user-defined schema acting as a reference. It requires classes such as BufferAllocator and Schema from the Apache Arrow Java library.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/dataset.rst#2025-04-16_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// read data from local file /opt/example.parquet\nString uri = \"file:/opt/example.parquet\";\nBufferAllocator allocator = new RootAllocator(Long.MAX_VALUE);\nDatasetFactory factory = new FileSystemDatasetFactory(allocator,\n    NativeMemoryPool.getDefault(), FileFormat.PARQUET, uri);\n\n// inspect schema\nSchema schema = factory.inspect();\n\n```\n\n----------------------------------------\n\nTITLE: Cancelling a Flight Call Using StopToken in C++\nDESCRIPTION: Demonstrates how to cancel an in-progress Flight call using the StopSource and stop_token mechanism. This allows gracefully terminating calls that might be unresponsive or no longer needed.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/flight.rst#2025-04-16_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nStopSource stop_source;\nFlightCallOptions options;\noptions.stop_token = stop_source.token();\nstop_source.RequestStop(Status::Cancelled(\"StopSource\"));\nflight_client->DoAction(options, {});\n```\n\n----------------------------------------\n\nTITLE: Reading from Cloud Storage with Arrow Dataset\nDESCRIPTION: These snippets demonstrate how to read datasets from cloud storage, specifically Amazon S3, including how to customize connection parameters.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndataset = ds.dataset(\"s3://voltrondata-labs-datasets/nyc-taxi/\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pyarrow import fs\n\ns3  = fs.S3FileSystem(region=\"us-east-2\")\ndataset = ds.dataset(\"voltrondata-labs-datasets/nyc-taxi/\", filesystem=s3)\n```\n\n----------------------------------------\n\nTITLE: File Reading and Writing Operations in Arrow\nDESCRIPTION: Implementation of file I/O operations including opening files, creating readers/writers, and performing read/write operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<arrow::io::ReadableFile> infile;\\nARROW_ASSIGN_OR_RAISE(infile, arrow::io::ReadableFile::Open(\"test.arrow\"));\n```\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<arrow::ipc::RecordBatchFileReader> reader;\\nARROW_ASSIGN_OR_RAISE(reader, arrow::ipc::RecordBatchFileReader::Open(infile));\n```\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<arrow::io::FileOutputStream> outfile;\\nARROW_ASSIGN_OR_RAISE(outfile, arrow::io::FileOutputStream::Open(\"test_out.arrow\"));\n```\n\n----------------------------------------\n\nTITLE: Managing Memory Allocation with PyArrow MemoryPool\nDESCRIPTION: Shows how to allocate resizable buffers and track memory usage using PyArrow's memory pool system\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/memory.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbuf = pa.allocate_buffer(1024, resizable=True)\npa.total_allocated_bytes()\nbuf.resize(2048)\npa.total_allocated_bytes()\n```\n\n----------------------------------------\n\nTITLE: Converting Arrow Array to NumPy Array in Python\nDESCRIPTION: This snippet shows how to convert an Arrow Array back to a NumPy array using the to_numpy() method. This approach creates a view of the Arrow data, which is limited to primitive types with the same physical representation in both systems and without null values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/numpy.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> import pyarrow as pa\n>>> arr = pa.array([4, 5, 6], type=pa.int32())\n>>> view = arr.to_numpy()\n>>> view\narray([4, 5, 6], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Creating Struct Arrays from Component Arrays in PyArrow\nDESCRIPTION: Shows how to create a struct array by combining existing arrays for each field. This approach shares data storage with the component arrays, avoiding copies.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nxs = pa.array([5, 6, 7], type=pa.int16())\nys = pa.array([False, True, True])\narr = pa.StructArray.from_arrays((xs, ys), names=('x', 'y'))\narr.type\narr\n```\n\n----------------------------------------\n\nTITLE: Creating a PyArrow FileSystem from fsspec Handler\nDESCRIPTION: Shows how to manually wrap an fsspec filesystem into a PyArrow filesystem using PyFileSystem and FSSpecHandler. This gives access to all PyArrow FileSystem interface methods with an fsspec backend.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pyarrow.fs import PyFileSystem, FSSpecHandler\npa_fs = PyFileSystem(FSSpecHandler(fs))\n\n# write data\nwith pa_fs.open_output_stream('mycontainer/pyarrowtest.dat') as stream:\n   stream.write(b'data')\n\n# read data\nwith pa_fs.open_input_stream('mycontainer/pyarrowtest.dat') as stream:\n   print(stream.readall())\n#b'data'\n\n# read a partitioned dataset\nds.dataset(\"data/\", filesystem=pa_fs)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Fixed Shape Tensor Extension Types\nDESCRIPTION: This example shows how to create and use the fixed shape tensor extension type for representing arrays of tensors with equal shape.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> tensor_type = pa.fixed_shape_tensor(pa.int32(), (2, 2))\n\n>>> arr = [[1, 2, 3, 4], [10, 20, 30, 40], [100, 200, 300, 400]]\n>>> storage = pa.array(arr, pa.list_(pa.int32(), 4))\n>>> tensor_array = pa.ExtensionArray.from_storage(tensor_type, storage)\n\n>>> tensor_type_2 = pa.fixed_shape_tensor(pa.float32(), (2, 2))\n>>> storage_2 = pa.array(arr, pa.list_(pa.float32(), 4))\n>>> tensor_array_2 = pa.ExtensionArray.from_storage(tensor_type_2, storage_2)\n```\n\n----------------------------------------\n\nTITLE: Concatenating Arrow Arrays in R\nDESCRIPTION: Demonstrates different methods for concatenating Arrow arrays, chunked arrays, record batches and tables in R.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_9\n\nLANGUAGE: R\nCODE:\n```\nconcat_arrays()\nChunkedArray$create()\nc()\ncbind()\nrbind()\nconcat_tables()\n```\n\n----------------------------------------\n\nTITLE: In-Memory IO with PyArrow BufferOutputStream\nDESCRIPTION: Demonstrates reading and writing data in memory using PyArrow's buffer-based IO interfaces\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/memory.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwriter = pa.BufferOutputStream()\nwriter.write(b'hello, friends')\n\nbuf = writer.getvalue()\nbuf\nbuf.size\nreader = pa.BufferReader(buf)\nreader.seek(7)\nreader.read(7)\n```\n\n----------------------------------------\n\nTITLE: Converting Arrow Expressions to Substrait Expressions in Python\nDESCRIPTION: This code shows how to serialize Arrow compute expressions into Substrait format. It creates an Arrow schema with two integer fields, then serializes an expression that adds these fields together. The result is a Substrait extended expression that represents the addition operation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/substrait.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nimport pyarrow.compute as pa\nimport pyarrow.substrait as pa_substrait\n\narrow_schema = pa.schema([\n    pa.field(\"x\", pa.int32()),\n    pa.field(\"y\", pa.int32())\n])\n\nsubstrait_expr = pa_substrait.serialize_expressions(\n    exprs=[pc.field(\"x\") + pc.field(\"y\")],\n    names=[\"total\"],\n    schema=arrow_schema\n)\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Feather Format Dataset\nDESCRIPTION: Demonstrates writing table data to Feather format and reading it back using pyarrow dataset API. Shows format specification and basic data retrieval.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.feather as feather\n\nfeather.write_feather(table, base / \"data.feather\")\n\ndataset = ds.dataset(base / \"data.feather\", format=\"feather\")\ndataset.to_table().to_pandas().head()\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow ChunkedArray in C++\nDESCRIPTION: This snippet demonstrates how to create an Arrow ChunkedArray, which is composed of multiple Arrays. It shows the process of combining Arrays into an ArrayVector and then constructing the ChunkedArray.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/basic_arrow.rst#2025-04-16_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\narrow::ArrayVector day_vectors = {day_array, day_array2};\nstd::shared_ptr<arrow::ChunkedArray> day_chunked_array;\nARROW_ASSIGN_OR_RAISE(day_chunked_array, arrow::ChunkedArray::Make(day_vectors));\n```\n\nLANGUAGE: cpp\nCODE:\n```\narrow::ArrayVector month_vectors = {month_array, month_array2};\nstd::shared_ptr<arrow::ChunkedArray> month_chunked_array;\nARROW_ASSIGN_OR_RAISE(month_chunked_array, arrow::ChunkedArray::Make(month_vectors));\n\narrow::ArrayVector year_vectors = {year_array, year_array2};\nstd::shared_ptr<arrow::ChunkedArray> year_chunked_array;\nARROW_ASSIGN_OR_RAISE(year_chunked_array, arrow::ChunkedArray::Make(year_vectors));\n```\n\n----------------------------------------\n\nTITLE: Creating Struct Arrays from Dictionaries in PyArrow\nDESCRIPTION: Demonstrates how PyArrow can infer the schema of a struct type from a sequence of Python dictionaries, automatically determining the field types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npa.array([{'x': 1, 'y': True}, {'z': 3.4, 'x': 4}])\n```\n\n----------------------------------------\n\nTITLE: Setting Call Timeout for Flight Operations in Java\nDESCRIPTION: Sets a timeout for a Flight DoAction call in Java. The operation will be cancelled after 0.2 seconds if it hasn't completed, which prevents clients from hanging on unresponsive servers.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/flight.rst#2025-04-16_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nIterator<Result> results = client.doAction(new Action(\"hang\"), CallOptions.timeout(0.2, TimeUnit.SECONDS));\n```\n\n----------------------------------------\n\nTITLE: Example of Line-Delimited JSON Data\nDESCRIPTION: This snippet shows an example of a line-delimited JSON file containing two rows of data with four columns: 'a', 'b', 'c', and 'd'.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/json.rst#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"a\": 1, \"b\": 2.0, \"c\": \"foo\", \"d\": false}\n{\"a\": 4, \"b\": -5.5, \"c\": null, \"d\": true}\n```\n\n----------------------------------------\n\nTITLE: Custom CSV Writing Options in PyArrow\nDESCRIPTION: Shows how to customize CSV writing behavior using WriteOptions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/csv.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow as pa\n>>> import pyarrow.csv as csv\n>>> # Omit the header row (include_header=True is the default)\n>>> options = csv.WriteOptions(include_header=False)\n>>> csv.write_csv(table, \"data.csv\", options)\n```\n\n----------------------------------------\n\nTITLE: Executing Queries Using Substrait Extended Expressions with Arrow Dataset\nDESCRIPTION: This code demonstrates how to execute queries on an Arrow dataset using Substrait extended expressions. It creates a query that selects the 'project_name' field from a dataset and filters for rows where 'project_name' equals 'pyarrow'. The query is executed using the Arrow dataset scanner API with Substrait expressions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/substrait.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.dataset as ds\nimport pyarrow.substrait as pa_substrait\n\n# Use substrait-python to create the queries\nfrom substrait import proto\n\ndataset = ds.dataset(\"./data/index-0.parquet\")\nsubstrait_schema = pa_substrait.serialize_schema(dataset.schema).to_pysubstrait()\n\n# SELECT project_name FROM dataset WHERE project_name = 'pyarrow'\n\nprojection = proto.ExtendedExpression(referred_expr=[\n    {\"expression\": {\"selection\": {\"direct_reference\": {\"struct_field\": {\"field\": 0}}}},\n    \"output_names\": [\"project_name\"]}]\n)\nprojection.MergeFrom(substrait_schema)\n\nfiltering = proto.ExtendedExpression(\n        extension_uris=[{\"extension_uri_anchor\": 99, \"uri\": \"/functions_comparison.yaml\"}],\n        extensions=[{\"extension_function\": {\"extension_uri_reference\": 99, \"function_anchor\": 199, \"name\": \"equal:any1_any1\"}}],\n        referred_expr=[\n            {\"expression\": {\"scalar_function\": {\"function_reference\": 199, \"arguments\": [\n                {\"value\": {\"selection\": {\"direct_reference\": {\"struct_field\": {\"field\": 0}}}}},\n                {\"value\": {\"literal\": {\"string\": \"pyarrow\"}}}\n            ], \"output_type\": {\"bool\": {\"nullability\": False}}}}}\n        ]\n)\nfiltering.MergeFrom(substrait_schema)\n\nresults = dataset.scanner(\n    columns=pa.substrait.BoundExpressions.from_substrait(projection),\n    filter=pa.substrait.BoundExpressions.from_substrait(filtering)\n).head(5)\n```\n\n----------------------------------------\n\nTITLE: Converting Pandas DataFrame to PyArrow Table with from_dataframe()\nDESCRIPTION: Shows how to convert a pandas DataFrame to a PyArrow table using the dataframe interchange protocol's from_dataframe() function. The example creates a simple DataFrame with numeric and string columns.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/interchange_protocol.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow\n>>> from pyarrow.interchange import from_dataframe\n\n>>> import pandas as pd\n>>> df = pd.DataFrame({\n...         \"n_attendees\": [100, 10, 1],\n...         \"country\": [\"Italy\", \"Spain\", \"Slovenia\"],\n...     })\n>>> df\n   n_attendees   country\n0          100     Italy\n1           10     Spain\n2            1  Slovenia\n>>> from_dataframe(df)\npyarrow.Table\nn_attendees: int64\ncountry: large_string\n----\nn_attendees: [[100,10,1]]\ncountry: [[\"Italy\",\"Spain\",\"Slovenia\"]]\n```\n\n----------------------------------------\n\nTITLE: Reading Partitioned Parquet Datasets\nDESCRIPTION: Examples of reading partitioned Parquet datasets using ParquetDataset class and read_table function.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndataset = pq.ParquetDataset('dataset_name/')\ntable = dataset.read()\n\ntable = pq.read_table('dataset_name')\n```\n\n----------------------------------------\n\nTITLE: Creating Nested List Type in PyArrow\nDESCRIPTION: Shows how to create a nested list type using the list_ function, which requires specifying the value type of the list elements.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nt6 = pa.list_(t1)\nt6\n```\n\n----------------------------------------\n\nTITLE: Streaming JSON File Reading using StreamingReader in C++\nDESCRIPTION: Shows how to incrementally read JSON data as RecordBatches using StreamingReader. The code demonstrates setting up the reader and implementing a loop to process batches of data while handling potential errors.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/json.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"arrow/json/api.h\"\n\n{\n   // ...\n   auto read_options = arrow::json::ReadOptions::Defaults();\n   auto parse_options = arrow::json::ParseOptions::Defaults();\n\n   std::shared_ptr<arrow::io::InputStream> stream;\n   auto result = arrow::json::StreamingReader::Make(stream,\n                                                    read_options,\n                                                    parse_options);\n   if (!result.ok()) {\n      // Handle instantiation error\n   }\n   std::shared_ptr<arrow::json::StreamingReader> reader = *result;\n\n   for (arrow::Result<std::shared_ptr<arrow::RecordBatch>> maybe_batch : *reader) {\n      if (!maybe_batch.ok()) {\n         // Handle read/parse error\n      }\n      std::shared_ptr<arrow::RecordBatch> batch = *maybe_batch;\n      // Operate on each batch...\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet Files from S3 in PyArrow\nDESCRIPTION: Demonstrates how to read a Parquet table from an Amazon S3 bucket using PyArrow's built-in filesystem inference from URI paths. This allows direct access to S3 objects without explicitly creating a filesystem object.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntable = pq.read_table(\"s3://bucket/object/key/prefix\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Compression and Encoding\nDESCRIPTION: Shows various compression options and dictionary encoding settings for Parquet files including Snappy, Gzip, Brotli, ZSTD, and LZ4 compression types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npq.write_table(table, where, use_dictionary=False)\n\npq.write_table(table, where, compression='snappy')\npq.write_table(table, where, compression='gzip')\npq.write_table(table, where, compression='brotli')\npq.write_table(table, where, compression='zstd')\npq.write_table(table, where, compression='lz4')\npq.write_table(table, where, compression='none')\n\npq.write_table(table, where, compression={'foo': 'snappy', 'bar': 'gzip'},\n                 use_dictionary=['foo', 'bar'])\n```\n\n----------------------------------------\n\nTITLE: Creating Map Arrays from Component Arrays in PyArrow\nDESCRIPTION: Shows how to create a map array from offset, key, and item arrays. Demonstrates accessing the flattened keys and items, and reconstructing the logical structure using ListArray.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_18\n\nLANGUAGE: python\nCODE:\n```\narr = pa.MapArray.from_arrays([0, 2, 3], ['x', 'y', 'z'], [4, 5, 6])\narr.keys\narr.items\npa.ListArray.from_arrays(arr.offsets, arr.keys)\npa.ListArray.from_arrays(arr.offsets, arr.items)\n```\n\n----------------------------------------\n\nTITLE: Writing Encrypted Parquet File with Apache Arrow C++\nDESCRIPTION: This code snippet demonstrates how to create an Arrow Table and write it to an encrypted Parquet file. It uses different encryption keys for the metadata footer and columns, showcasing Parquet's column encryption feature.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/parquet_column_encryption.rst#2025-04-16_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n.. literalinclude:: ../../../../cpp/examples/arrow/parquet_column_encryption.cc\n```\n\n----------------------------------------\n\nTITLE: Creating a Schema with Fields in Apache Arrow C++\nDESCRIPTION: Demonstrates how to create a schema with two fields: an int32 column 'A' and a utf8-encoded string column 'B'. This example shows the use of arrow::field() and arrow::schema() factory functions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tables.rst#2025-04-16_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n// Create a schema describing datasets with two columns:\n// a int32 column \"A\" and a utf8-encoded string column \"B\"\nstd::shared_ptr<arrow::Field> field_a, field_b;\nstd::shared_ptr<arrow::Schema> schema;\n\nfield_a = arrow::field(\"A\", arrow::int32());\nfield_b = arrow::field(\"B\", arrow::utf8());\nschema = arrow::schema({field_a, field_b});\n```\n\n----------------------------------------\n\nTITLE: Copying CUDA Buffer to Host Memory in PyArrow\nDESCRIPTION: Shows how to copy data from a CUDA buffer back to CPU memory, converting it to a regular PyArrow Buffer, and then access the data using NumPy.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/cuda.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> buf = cuda_buf.copy_to_host()\n>>> type(buf)\npyarrow.lib.Buffer\n>>> np.frombuffer(buf, dtype=np.int32)\narray([0, 1, 2, 3], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Incremental CSV Writing in PyArrow\nDESCRIPTION: Demonstrates how to write CSV files incrementally using CSVWriter class.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/csv.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow as pa\n>>> import pyarrow.csv as csv\n>>> with csv.CSVWriter(\"data.csv\", table.schema) as writer:\n>>>     writer.write_table(table)\n```\n\n----------------------------------------\n\nTITLE: Creating Input Stream from Compressed File\nDESCRIPTION: Example of reading compressed data using PyArrow's input_stream functionality with gzip\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/memory.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport gzip\nwith gzip.open('example.gz', 'wb') as f:\n    f.write(b'some data\\n' * 3)\n\nstream = pa.input_stream('example.gz')\nstream.read()\n```\n\n----------------------------------------\n\nTITLE: Allocating a Buffer in C++\nDESCRIPTION: Demonstrates how to allocate a buffer using Arrow's AllocateBuffer function, ensuring 64-byte alignment and padding as per Arrow memory specification.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/memory.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\narrow::Result<std::unique_ptr<Buffer>> maybe_buffer = arrow::AllocateBuffer(4096);\nif (!maybe_buffer.ok()) {\n   // ... handle allocation error\n}\n\nstd::shared_ptr<arrow::Buffer> buffer = *std::move(maybe_buffer);\nuint8_t* buffer_data = buffer->mutable_data();\nmemcpy(buffer_data, \"hello world\", 11);\n```\n\n----------------------------------------\n\nTITLE: Performing Arrow Compute Functions\nDESCRIPTION: Shows how to access and use Arrow compute functions through the Arrow::Function interface, demonstrated with the 'add' function to sum column values.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_9\n\nLANGUAGE: ruby\nCODE:\n```\nadd = Arrow::Function.find('add')\nadd.execute([table['age'].data, table['age'].data]).value\n# => #<Arrow::ChunkedArray:0x7fa389b87250 ptr=0x7fa3a4bb5c40 [\n#   [\n#     44,\n#     46,\n#     38\n#   ]\n# ]>\n```\n\n----------------------------------------\n\nTITLE: Debugging Memory Leaks in Arrow Java\nDESCRIPTION: Shows how to enable debug mode for memory allocation and the resulting output when a memory leak occurs.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/memory.rst#2025-04-16_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.apache.arrow.memory.ArrowBuf;\nimport org.apache.arrow.memory.BufferAllocator;\nimport org.apache.arrow.memory.RootAllocator;\n\ntry (BufferAllocator bufferAllocator = new RootAllocator(8 * 1024)) {\n    ArrowBuf arrowBuf = bufferAllocator.buffer(4 * 1024);\n    System.out.println(arrowBuf);\n}\n```\n\n----------------------------------------\n\nTITLE: Memory Mapped Parquet Reading\nDESCRIPTION: Example showing memory-mapped reading of Parquet files and memory usage reporting.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npq_array = pa.parquet.read_table(\"area1.parquet\", memory_map=True)\nprint(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n\npq_array = pa.parquet.read_table(\"area1.parquet\", memory_map=False)\nprint(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n```\n\n----------------------------------------\n\nTITLE: Checking Datum Type for Sum Result in Arrow C++\nDESCRIPTION: Prints the type information of the Datum containing the sum result.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nstd::cout << \"Datum kind: \" << sum_result.kind() << std::endl;\nstd::cout << \"Datum type: \" << *sum_result.type() << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Converting ResultSet to Arrow with JdbcToArrow\nDESCRIPTION: This Java code snippet demonstrates using the Arrow JDBC Adapter to convert a ResultSet into batches of Arrow data using the JdbcToArrow API. It includes a try-with-resources block to ensure proper iterator management, and processes each batch of data iteratively.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/jdbc.rst#2025-04-16_snippet_0\n\nLANGUAGE: java\nCODE:\n```\ntry (ArrowVectorIterator it = JdbcToArrow.sqlToArrowVectorIterator(resultSet, allocator)) {\\n  while (it.hasNext()) {\\n    VectorSchemaRoot root = it.next();\\n    // Consume the root…\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Registering and Using Extension Types with PyArrow IPC\nDESCRIPTION: Example demonstrating how to register an extension type for deserialization and create a RecordBatch with extension arrays. This enables serialization/deserialization across processes using the IPC protocol.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> pa.register_extension_type(RationalType(pa.int32()))\n\n>>> batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n>>> sink = pa.BufferOutputStream()\n```\n\n----------------------------------------\n\nTITLE: Creating Struct Arrays with Null Values in PyArrow\nDESCRIPTION: Demonstrates creating struct arrays with null values at both the struct level and individual field level. Shows how missing dictionary keys are treated as null values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\npa.array([{'x': 1}, None, {'y': None}], type=ty)\n```\n\n----------------------------------------\n\nTITLE: Creating CUDA Context in PyArrow\nDESCRIPTION: Creates a CUDA context to access CUDA device number 0. This is the first step in working with GPU memory in PyArrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/cuda.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from pyarrow import cuda\n>>> ctx = cuda.Context(0)\n>>>\n```\n\n----------------------------------------\n\nTITLE: Using Google Cloud Storage FileSystem in PyArrow\nDESCRIPTION: Example of accessing Google Cloud Storage using PyArrow's GcsFileSystem. This demonstrates how to connect to public buckets and read file contents from GCS storage.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from datetime import timedelta\n>>> from pyarrow import fs\n>>> gcs = fs.GcsFileSystem(anonymous=True, retry_time_limit=timedelta(seconds=15))\n\n# List all contents in a bucket, recursively\n>>> uri = \"gcp-public-data-landsat/LC08/01/001/003/\"\n>>> file_list = gcs.get_file_info(fs.FileSelector(uri, recursive=True))\n\n# Open a file for reading and download its contents\n>>> f = gcs.open_input_stream(file_list[0].path)\n>>> f.read(64)\nb'GROUP = FILE_HEADER\\n  LANDSAT_SCENE_ID = \"LC80010032013082LGN03\"\\n  S'\n```\n\n----------------------------------------\n\nTITLE: Converting Pandas Timestamp to Arrow TimestampArray\nDESCRIPTION: Demonstrates how pandas Timestamps (datetime64[ns]) are converted to Arrow TimestampArray objects, preserving timezone information.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/pandas.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"datetime\": pd.date_range(\"2020-01-01T00:00:00Z\", freq=\"h\", periods=3)})\ndf.dtypes\ndf\n\ntable = pa.Table.from_pandas(df)\ntable\n```\n\n----------------------------------------\n\nTITLE: Calling 'index' Compute Function with Options in Arrow (C++)\nDESCRIPTION: Executes the 'index' compute function using CallFunction with the configured IndexOptions. The function searches for the specified target value in the input data and returns its position.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\nARROW_ASSIGN_OR_RAISE(compute::Datum result,\n                   compute::CallFunction(\"index\", {table->column(0)}, &options));\n```\n\n----------------------------------------\n\nTITLE: Dataset Discovery and Reading\nDESCRIPTION: Demonstrates discovering and reading a dataset using FileSystemDatasetFactory, including creating a scanner and reading into a table.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// Create a dataset by crawling files in a directory\nauto fs = std::make_shared<fs::LocalFileSystem>();\nauto format = std::make_shared<ds::ParquetFileFormat>();\n\nds::FileSystemFactoryOptions options;\nds::FileSelector selector;\nselector.base_dir = dir_path;\n\nauto factory = ds::FileSystemDatasetFactory::Make(fs, selector, format, options)\n                   .ValueOrDie();\nauto dataset = factory->Finish().ValueOrDie();\n\n// Read the entire dataset into a Table\nds::ScannerBuilder builder(dataset);\nauto scanner = builder.Finish().ValueOrDie();\nauto table = scanner->ToTable().ValueOrDie();\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow RecordBatch from MATLAB Table\nDESCRIPTION: Example showing how to convert a MATLAB table to an Arrow RecordBatch using the arrow.recordBatch function.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_7\n\nLANGUAGE: matlab\nCODE:\n```\n>> matlabTable = table([\"A\"; \"B\"; \"C\"], [1; 2; 3], [true; false; true])\n\nmatlabTable =\n\n  3x3 table\n\n    Var1    Var2    Var3\n    ____    ____    _____\n\n    \"A\"      1      true\n    \"B\"      2      false\n    \"C\"      3      true\n\n>> arrowRecordBatch = arrow.recordBatch(matlabTable)\n\narrowRecordBatch =\n\nVar1:   [\n    \"A\",\n    \"B\",\n    \"C\"\n  ]\nVar2:   [\n    1,\n    2,\n    3\n  ]\nVar3:   [\n    true,\n    false,\n    true\n  ]\n```\n\n----------------------------------------\n\nTITLE: Building BigIntVector with Writer in Java\nDESCRIPTION: This example shows how to create and populate a BigIntVector using a BigIntWriter.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\ntry (BigIntVector vector = new BigIntVector(\"vector\", allocator);\n  BigIntWriter writer = new BigIntWriterImpl(vector)) {\n  writer.setPosition(0);\n  writer.writeBigInt(1);\n  writer.setPosition(1);\n  writer.writeBigInt(2);\n  writer.setPosition(2);\n  writer.writeBigInt(3);\n  // writer.setPosition(3) is not called which means the fourth value is null.\n  writer.setPosition(4);\n  writer.writeBigInt(5);\n  writer.setPosition(5);\n  writer.writeBigInt(6);\n  writer.setPosition(6);\n  writer.writeBigInt(7);\n  writer.setPosition(7);\n  writer.writeBigInt(8);\n}\n```\n\n----------------------------------------\n\nTITLE: Manually Specifying a Dataset in Arrow\nDESCRIPTION: This code demonstrates how to manually create a Dataset without automatic discovery, specifying paths, schema, format, filesystem, and partition expressions explicitly.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# creating a dummy dataset: directory with two files\ntable = pa.table({'col1': range(3), 'col2': np.random.randn(3)})\n(base / \"parquet_dataset_manual\").mkdir(exist_ok=True)\npq.write_table(table, base / \"parquet_dataset_manual\" / \"data_2018.parquet\")\npq.write_table(table, base / \"parquet_dataset_manual\" / \"data_2019.parquet\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pyarrow import fs\n\nschema = pa.schema([(\"year\", pa.int64()), (\"col1\", pa.int64()), (\"col2\", pa.float64())])\n\ndataset = ds.FileSystemDataset.from_paths(\n    [\"data_2018.parquet\", \"data_2019.parquet\"], schema=schema, format=ds.ParquetFileFormat(),\n    filesystem=fs.SubTreeFileSystem(str(base / \"parquet_dataset_manual\"), fs.LocalFileSystem()),\n    partitions=[ds.field('year') == 2018, ds.field('year') == 2019])\n```\n\n----------------------------------------\n\nTITLE: Starting and Writing Batches with ArrowStreamWriter in Java\nDESCRIPTION: Demonstrates how to start the `ArrowStreamWriter`, write record batches, and end the stream. The VectorSchemaRoot is populated with new data before each batch is written.  It showcases writing multiple batches to the output stream.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/ipc.rst#2025-04-16_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nwriter.start();\n// write the first batch\nwriter.writeBatch();\n\n// write another four batches.\nfor (int i = 0; i < 4; i++) {\n  // populate VectorSchemaRoot data and write the second batch\n  BitVector childVector1 = (BitVector)root.getVector(0);\n  VarCharVector childVector2 = (VarCharVector)root.getVector(1);\n  childVector1.reset();\n  childVector2.reset();\n  // ... do some populate work here, could be different for each batch\n  writer.writeBatch();\n}\n\nwriter.end();\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet Files with PyArrow FileSystem\nDESCRIPTION: Examples showing how to read Parquet files using both a URI (where the filesystem is inferred) and with an explicit filesystem argument. This demonstrates the flexibility of PyArrow's IO functions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.parquet as pq\n\n# using a URI -> filesystem is inferred\npq.read_table(\"s3://my-bucket/data.parquet\")\n# using a path and filesystem\ns3 = fs.S3FileSystem(..)\npq.read_table(\"my-bucket/data.parquet\", filesystem=s3)\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Encryption C++\nDESCRIPTION: This C++ code snippet demonstrates configuring Parquet encryption settings using the CryptoFactory and EncryptionConfiguration classes. It shows how to set write options to enable uniform encryption or individual column encryption by specifying key IDs. The configuration is necessary for managing encrypted metadata and columns within a Parquet file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/parquet.rst#2025-04-16_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n// Set write options with encryption configuration\n```\n\nLANGUAGE: C++\nCODE:\n```\nencryption_config->column_keys\n```\n\nLANGUAGE: C++\nCODE:\n```\nauto parquet_encryption_config\n```\n\n----------------------------------------\n\nTITLE: CSV Writing in PyArrow\nDESCRIPTION: Shows how to write CSV files using PyArrow, including both uncompressed and compressed output options.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/csv.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow as pa\n>>> import pyarrow.csv as csv\n>>> csv.write_csv(table, \"tips.csv\")\n>>> with pa.CompressedOutputStream(\"tips.csv.gz\", \"gzip\") as out:\n...     csv.write_csv(table, out)\n```\n\n----------------------------------------\n\nTITLE: String Trimming Function Definitions in Apache Arrow\nDESCRIPTION: This code snippet defines string trimming functions in Apache Arrow, including their input types, output types, and associated options. These functions are used to trim characters from both sides, left side, or right side of strings.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+---------+\n| Function name            | Arity      | Input types             | Output type         | Options class                          | Notes   |\n+==========================+============+=========================+=====================+========================================+=========+\n| ascii_ltrim              | Unary      | String-like             | String-like         | :struct:`TrimOptions`                  | \\(1)    |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+---------+\n| ascii_ltrim_whitespace   | Unary      | String-like             | String-like         |                                        | \\(2)    |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+---------+\n| ascii_rtrim              | Unary      | String-like             | String-like         | :struct:`TrimOptions`                  | \\(1)    |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+---------+\n| ascii_rtrim_whitespace   | Unary      | String-like             | String-like         |                                        | \\(2)    |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+---------+\n| ascii_trim               | Unary      | String-like             | String-like         | :struct:`TrimOptions`                  | \\(1)    |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+---------+\n| ascii_trim_whitespace    | Unary      | String-like             | String-like         |                                        | \\(2)    |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+---------+\n| utf8_ltrim               | Unary      | String-like             | String-like         | :struct:`TrimOptions`                  | \\(3)    |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+---------+\n| utf8_ltrim_whitespace    | Unary      | String-like             | String-like         |                                        | \\(4)    |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+---------+\n| utf8_rtrim               | Unary      | String-like             | String-like         | :struct:`TrimOptions`                  | \\(3)    |\n```\n\n----------------------------------------\n\nTITLE: C Consumer Example for Using Arrow C Stream\nDESCRIPTION: The C consumer example illustrates how to execute a SQL query and process the result using the Arrow C stream interface. It demonstrates initializing the stream, querying the schema, iterating over the data chunks, handling errors, and releasing resources properly.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CStreamInterface.rst#2025-04-16_snippet_1\n\nLANGUAGE: c\nCODE:\n```\nstatic void handle_error(int errcode, struct ArrowArrayStream* stream) {\n   // Print stream error\n   const char* errdesc = stream->get_last_error(stream);\n   if (errdesc != NULL) {\n      fputs(errdesc, stderr);\n   } else {\n      fputs(strerror(errcode), stderr);\n   }\n   // Release stream and abort\n   stream->release(stream),\n   exit(1);\n}\n\nvoid run_query() {\n   struct ArrowArrayStream stream;\n   struct ArrowSchema schema;\n   struct ArrowArray chunk;\n   int errcode;\n\n   MyDB_Query(\"SELECT * FROM my_table\", &stream);\n\n   // Query result set schema\n   errcode = stream.get_schema(&stream, &schema);\n   if (errcode != 0) {\n      handle_error(errcode, &stream);\n   }\n\n   int64_t num_rows = 0;\n\n   // Iterate over results: loop until error or end of stream\n   while ((errcode = stream.get_next(&stream, &chunk) == 0) &&\n          chunk.release != NULL) {\n      // Do something with chunk...\n      fprintf(stderr, \"Result chunk: got %lld rows\\n\", chunk.length);\n      num_rows += chunk.length;\n\n      // Release chunk\n      chunk.release(&chunk);\n   }\n\n   // Was it an error?\n   if (errcode != 0) {\n      handle_error(errcode, &stream);\n   }\n\n   fprintf(stderr, \"Result stream ended: total %lld rows\\n\", num_rows);\n\n   // Release schema and stream\n   schema.release(&schema);\n   stream.release(&stream);\n}\n```\n\n----------------------------------------\n\nTITLE: Slicing a VectorSchemaRoot in Apache Arrow Java\nDESCRIPTION: Shows how to create a new VectorSchemaRoot by slicing an existing one without copying data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector_schema_root.rst#2025-04-16_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n// 0 indicates start index (inclusive) and 5 indicated length (exclusive).\nVectorSchemaRoot newRoot = vectorSchemaRoot.slice(0, 5);\n```\n\n----------------------------------------\n\nTITLE: Complete Arrow C++ File I/O Example\nDESCRIPTION: Complete example demonstrating file I/O with CSV and Parquet formats in Apache Arrow C++. Includes opening files, reading data into Arrow Tables, and writing data back to different file formats.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\nStatus RunMain() {\n  // For dealing with any needs to allocate memory\n  std::shared_ptr<arrow::MemoryPool> pool = arrow::default_memory_pool();\n\n  // 1. Define schema that represents the table structure we wish to read in\n  auto schema = arrow::schema({arrow::field(\"A\", arrow::int64()),\n                              arrow::field(\"B\", arrow::float64()),\n                              arrow::field(\"C\", arrow::utf8())});\n\n  // 2. Read in CSV into table using streaming reader\n  std::shared_ptr<arrow::io::ReadableFile> inp;\n  ARROW_ASSIGN_OR_THROW(inp, arrow::io::ReadableFile::Open(\"simple_table.csv\"));\n\n  auto read_options = arrow::csv::ReadOptions::Defaults();\n  auto parse_options = arrow::csv::ParseOptions::Defaults();\n  // Treating the first row as column names\n  parse_options.header_rows = 1;\n  auto convert_options = arrow::csv::ConvertOptions::Defaults();\n\n  // 3. Create the reader\n  std::shared_ptr<arrow::csv::StreamingReader> reader;\n  ARROW_ASSIGN_OR_THROW(\n      reader, arrow::csv::StreamingReader::Make(arrow::io::default_io_context(), inp,\n                                              read_options, parse_options,\n                                              convert_options));\n\n  // 4. Read the full table contained in our simple_table.csv file\n  std::shared_ptr<arrow::Table> table;\n  ARROW_ASSIGN_OR_THROW(table, reader->Read());\n\n  // 5. Write the contents of that table to a csv file using the WriteOptions defaults\n  std::shared_ptr<arrow::io::FileOutputStream> outfile;\n  PARQUET_ASSIGN_OR_THROW(outfile, arrow::io::FileOutputStream::Open(\"simple_table.csv\"));\n  PARQUET_THROW_NOT_OK(arrow::csv::WriteCSV(*table, arrow::csv::WriteOptions::Defaults(),\n                                            outfile.get()));\n\n  // 6. Set up for a read of a parquet file\n  std::shared_ptr<arrow::io::ReadableFile> infile;\n  PARQUET_ASSIGN_OR_THROW(infile, arrow::io::ReadableFile::Open(\"test.parquet\",\n                                                          pool));\n\n  // 7. Create the reader for the parquet file\n  std::unique_ptr<parquet::arrow::FileReader> parquet_reader;\n  PARQUET_THROW_NOT_OK(\n      parquet::arrow::OpenFile(infile, pool, &parquet_reader));\n\n  // 8. Read the parquet file into a table\n  std::shared_ptr<arrow::Table> parquet_table;\n  PARQUET_THROW_NOT_OK(parquet_reader->ReadTable(&parquet_table));\n\n  // 9. Write the parquet_table to a parquet file.\n  PARQUET_THROW_NOT_OK(\n      parquet::arrow::WriteTable(*parquet_table, pool, \"test_write.parquet\", 2048));\n\n  return arrow::Status::OK();\n}\n```\n\n----------------------------------------\n\nTITLE: Connecting to HDFS with PyArrow Filesystem Interface\nDESCRIPTION: Creates a connection to a Hadoop Distributed File System using PyArrow's filesystem interface. Requires host and port parameters, with optional user and Kerberos ticket cache path for authentication.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pyarrow import fs\nhdfs = fs.HadoopFileSystem(host, port, user=user, kerb_ticket=ticket_cache_path)\n```\n\n----------------------------------------\n\nTITLE: Building an Int64Array using Int64Builder in Apache Arrow C++\nDESCRIPTION: This snippet demonstrates how to create an Int64Array using the Int64Builder class. It shows how to append values, including null values, and finalize the array construction.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/arrays.rst#2025-04-16_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\narrow::Int64Builder builder;\nbuilder.Append(1);\nbuilder.Append(2);\nbuilder.Append(3);\nbuilder.AppendNull();\nbuilder.Append(5);\nbuilder.Append(6);\nbuilder.Append(7);\nbuilder.Append(8);\n\nauto maybe_array = builder.Finish();\nif (!maybe_array.ok()) {\n   // ... do something on array building failure\n}\nstd::shared_ptr<arrow::Array> array = *maybe_array;\n```\n\n----------------------------------------\n\nTITLE: Setting Call Timeout for Flight Operations in C++\nDESCRIPTION: Configures a timeout for a Flight operation to ensure it doesn't hang indefinitely. This example sets a 200ms timeout for a GetFlightInfo call, after which it will be automatically cancelled.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/flight.rst#2025-04-16_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nFlightCallOptions options;\noptions.timeout = TimeoutDuration{0.2};\nStatus status = client->GetFlightInfo(options, FlightDescriptor{}).status();\n```\n\n----------------------------------------\n\nTITLE: Getting CPU Thread Pool Capacity in Apache Arrow (C++)\nDESCRIPTION: This function retrieves the current capacity of the CPU thread pool in Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/thread.rst#2025-04-16_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\narrow::GetCpuThreadPoolCapacity\n```\n\n----------------------------------------\n\nTITLE: Reading Data from ListVector in Java Arrow\nDESCRIPTION: This code shows two methods for accessing data in a ListVector: using the get API which returns lists of values, and using the UnionListReader which allows for iterative reading of values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n// access via get API\nfor (int i = 0; i < listVector.getValueCount(); i++) {\n   if (!listVector.isNull(i)) {\n       ArrayList<Integer> elements = (ArrayList<Integer>) listVector.getObject(i);\n       for (Integer element : elements) {\n           System.out.println(element);\n       }\n   }\n}\n\n// access via reader\nUnionListReader reader = listVector.getReader();\nfor (int i = 0; i < listVector.getValueCount(); i++) {\n   reader.setPosition(i);\n   while (reader.next()) {\n       IntReader intReader = reader.reader();\n       if (intReader.isSet()) {\n           System.out.println(intReader.readInteger());\n       }\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Using S3FileSystem in PyArrow\nDESCRIPTION: Example of listing and reading contents from an S3 bucket using PyArrow's S3FileSystem. This demonstrates how to interact with S3-compatible storage systems through a unified interface.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from pyarrow import fs\n>>> s3 = fs.S3FileSystem(region='eu-west-3')\n\n# List all contents in a bucket, recursively\n>>> s3.get_file_info(fs.FileSelector('my-test-bucket', recursive=True))\n[<FileInfo for 'my-test-bucket/File1': type=FileType.File, size=10>,\n <FileInfo for 'my-test-bucket/File5': type=FileType.File, size=10>,\n <FileInfo for 'my-test-bucket/Dir1': type=FileType.Directory>,\n <FileInfo for 'my-test-bucket/Dir2': type=FileType.Directory>,\n <FileInfo for 'my-test-bucket/EmptyDir': type=FileType.Directory>,\n <FileInfo for 'my-test-bucket/Dir1/File2': type=FileType.File, size=11>,\n <FileInfo for 'my-test-bucket/Dir1/Subdir': type=FileType.Directory>,\n <FileInfo for 'my-test-bucket/Dir2/Subdir': type=FileType.Directory>,\n <FileInfo for 'my-test-bucket/Dir2/Subdir/File3': type=FileType.File, size=10>]\n\n# Open a file for reading and download its contents\n>>> f = s3.open_input_stream('my-test-bucket/Dir1/File2')\n>>> f.readall()\nb'some data'\n```\n\n----------------------------------------\n\nTITLE: Arrow Dataset File Format Configuration\nDESCRIPTION: Example showing the file format configuration for write_dataset() when using IPC or feather formats\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_6\n\nLANGUAGE: R\nCODE:\n```\nwrite_dataset(format = \"ipc\")\nwrite_dataset(format = \"feather\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Int64Array data in Apache Arrow C++\nDESCRIPTION: This code snippet shows how to access the contents of an Int64Array, including casting the array, accessing the null bitmap and raw values, and querying individual values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/arrays.rst#2025-04-16_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nauto int64_array = std::static_pointer_cast<arrow::Int64Array>(array);\n\nconst uint8_t* null_bitmap = int64_array->null_bitmap_data();\n\nconst int64_t* data = int64_array->raw_values();\n\nint64_t index = 2;\nif (!int64_array->IsNull(index)) {\n   int64_t value = int64_array->Value(index);\n}\n```\n\n----------------------------------------\n\nTITLE: Java Implementation of FillTen with C Data Interface Support\nDESCRIPTION: This Java class demonstrates how to implement methods that work with the C Data Interface. It includes a method to fill an Arrow BigIntVector and a method to receive C Data Interface pointers and convert them to a usable FieldVector.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_12\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.arrow.c.ArrowArray;\nimport org.apache.arrow.c.ArrowSchema;\nimport org.apache.arrow.c.Data;\nimport org.apache.arrow.memory.RootAllocator;\nimport org.apache.arrow.vector.FieldVector;\nimport org.apache.arrow.vector.BigIntVector;\n\n\npublic class FillTen {\n    static RootAllocator allocator = new RootAllocator();\n\n    public static void fillCArray(long c_array_ptr, long c_schema_ptr) {\n        ArrowArray arrow_array = ArrowArray.wrap(c_array_ptr);\n        ArrowSchema arrow_schema = ArrowSchema.wrap(c_schema_ptr);\n\n        FieldVector v = Data.importVector(allocator, arrow_array, arrow_schema, null);\n        FillTen.fillVector((BigIntVector)v);\n    }\n\n    private static void fillVector(BigIntVector iv) {\n        iv.setSafe(0, 1);\n        iv.setSafe(1, 2);\n        iv.setSafe(2, 3);\n        iv.setSafe(3, 4);\n        iv.setSafe(4, 5);\n        iv.setSafe(5, 6);\n        iv.setSafe(6, 7);\n        iv.setSafe(7, 8);\n        iv.setSafe(8, 9);\n        iv.setSafe(9, 10);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Numba Device Array from NumPy Array\nDESCRIPTION: Creates a new NumPy array with values and transfers it to a Numba device array on the GPU for further processing.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/cuda.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> arr = np.arange(10, 14, dtype=np.int32)\n>>> arr\narray([10, 11, 12, 13], dtype=int32)\n>>> device_arr = numba.cuda.to_device(arr)\n```\n\n----------------------------------------\n\nTITLE: Calling User-Defined Functions in PyArrow\nDESCRIPTION: Examples of calling a user-defined function with both scalar and array inputs using the call_function API. Shows both scalar and array results.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/compute.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> pc.call_function(\"numpy_gcd\", [pa.scalar(27), pa.scalar(63)])\n<pyarrow.Int64Scalar: 9>\n>>> pc.call_function(\"numpy_gcd\", [pa.scalar(27), pa.array([81, 12, 5])])\n<pyarrow.lib.Int64Array object at 0x7fcfa0e7b100>\n[\n  27,\n  3,\n  1\n]\n```\n\n----------------------------------------\n\nTITLE: Writing Parquet Files with Timestamp Version Control\nDESCRIPTION: Demonstrates writing Parquet files with version 2.6 support for nanosecond timestamps and INT96 timestamp format for backwards compatibility.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npq.write_table(table, where, version='2.6')\n\npq.write_table(table, where, use_deprecated_int96_timestamps=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Extension Types in R\nDESCRIPTION: Example of how to create and register custom Arrow extension types and arrays in R using the new_extension_type() function.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_8\n\nLANGUAGE: R\nCODE:\n```\n?new_extension_type\n```\n\n----------------------------------------\n\nTITLE: Creating Struct Arrays with Explicit Types in PyArrow\nDESCRIPTION: Shows how to create struct arrays with an explicitly defined type from sequences of dictionaries or tuples. This allows controlling the exact field types and handling type conversions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nty = pa.struct([('x', pa.int8()),\n               ('y', pa.bool_())])\npa.array([{'x': 1, 'y': True}, {'x': 2, 'y': False}], type=ty)\npa.array([(3, True), (4, False)], type=ty)\n```\n\n----------------------------------------\n\nTITLE: Creating PyArrow CUDA Buffer from Numba Device Array\nDESCRIPTION: Demonstrates how to create a PyArrow CUDA buffer from an existing Numba device array using CudaBuffer.from_numba, including how to inspect its properties.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/cuda.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> cuda_buf = cuda.CudaBuffer.from_numba(device_arr.gpu_data)\n>>> cuda_buf.size\n16\n>>> cuda_buf.address\n30088364032\n>>> cuda_buf.context.device_number\n0\n```\n\n----------------------------------------\n\nTITLE: Loading S3 Data with Authentication\nDESCRIPTION: Shows how to load data from S3 with authentication credentials properly encoded in the URI format.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_5\n\nLANGUAGE: ruby\nCODE:\n```\nrequire 'cgi/util'\n\ns3_uri = URI(\"s3://#{CGI.escape(access_key)}:#{CGI.escape(secret_key)}@bucket/private.parquet\")\nArrow::Table.load(s3_uri)\n```\n\n----------------------------------------\n\nTITLE: Optimizing memory usage when converting Arrow tables to pandas DataFrames\nDESCRIPTION: Demonstrates how to use split_blocks and self_destruct parameters to minimize memory overhead when converting large Arrow tables to pandas DataFrames, which can help avoid memory doubling issues.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/pandas.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = table.to_pandas(split_blocks=True, self_destruct=True)\ndel table  # not necessary, but a good practice\n```\n\n----------------------------------------\n\nTITLE: Building BigIntVector with Direct Setting in Java\nDESCRIPTION: This snippet demonstrates how to create, allocate, and populate a BigIntVector using direct set methods.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\ntry (BufferAllocator allocator = new RootAllocator(Long.MAX_VALUE);\n  BigIntVector vector = new BigIntVector(\"vector\", allocator)) {\n  vector.allocateNew(8);\n  vector.set(0, 1);\n  vector.set(1, 2);\n  vector.set(2, 3);\n  vector.setNull(3);\n  vector.set(4, 5);\n  vector.set(5, 6);\n  vector.set(6, 7);\n  vector.set(7, 8);\n  vector.setValueCount(8); // this will finalizes the vector by convention.\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Arrow Stream Format\nDESCRIPTION: Shows how to read record batches from a stream using PyArrow's RecordBatchStreamReader.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/ipc.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith pa.ipc.open_stream(buf) as reader:\n      schema = reader.schema\n      batches = [b for b in reader]\n```\n\n----------------------------------------\n\nTITLE: Installing PyArrow Core with Parquet Support via Conda\nDESCRIPTION: Command to install PyArrow core package with additional Parquet support using conda\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/install.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge pyarrow-core libparquet\n```\n\n----------------------------------------\n\nTITLE: Reading a Partitioned Dataset with Hive-like Scheme in C++\nDESCRIPTION: This snippet demonstrates how to read a partitioned dataset using a Hive-like partitioning scheme in Apache Arrow. It shows how partition fields not included in Parquet files are added back to the resulting table during data scanning.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n// Example of reading with Hive partitioning\n// Code demonstrating dataset reading\n```\n\n----------------------------------------\n\nTITLE: Running Memory Leak Analysis on Processed Perf Events\nDESCRIPTION: Shell command that demonstrates how to pipe the processed events JSONL file to the traceback counting script to identify memory leaks. This command analyzes the allocations and prints the results.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/memory.rst#2025-04-16_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncat processed_events.jsonl | python3 /arrow/count_tracebacks.py\n```\n\n----------------------------------------\n\nTITLE: Sorting Vector Elements in Java\nDESCRIPTION: Various sorting algorithms are offered by Apache Arrow's Java library, including in-place, out-of-place, and index sorters. The in-place sorter (FixedWidthInPlaceVectorSorter) sorts the original vector, while out-of-place sorters create sorted copies (FixedWidthOutOfPlaceVectorSorter, VariableWidthOutOfPlaceVectorSorter). The index sorter, IndexSorter, returns indices representing a sorted vector order, supportive of tasks like identifying the k-th smallest element.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/algorithm.rst#2025-04-16_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n\"\"\"\nSorting algorithms include:\n1. In-place sorter: FixedWidthInPlaceVectorSorter (O(nlog(n))) for fixed width vectors\n2. Out-of-place sorter: FixedWidthOutOfPlaceVectorSorter and VariableWidthOutOfPlaceVectorSorter (O(nlog(n)))\n3. Index sorter: IndexSorter (O(nlog(n))), returns indices for constructing sorted vector\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Numeric Array Summation Using Arrow Type Predicates\nDESCRIPTION: Template function that sums values in any numeric (integer or float) Arrow array using type predicates.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/datatypes.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename ArrayType, typename DataType = typename ArrayType::TypeClass,\n          typename CType = typename DataType::c_type>\narrow::enable_if_number<DataType, CType> SumArray(const ArrayType& array) {\n  CType sum = 0;\n  for (std::optional<CType> value : array) {\n    if (value.has_value()) {\n      sum += value.value();\n    }\n  }\n  return sum;\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Numba CUDA Kernel on Device Array\nDESCRIPTION: Shows how to execute the previously defined Numba CUDA kernel on the device array with a 16x16 grid size, performing the computation on the GPU.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/cuda.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> increment_by_one[16, 16](device_arr)\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Arrow Flight\nDESCRIPTION: This snippet defines the set of error codes utilized in Arrow Flight, detailing each error's meaning. This is crucial for developers to implement reliable error handling routines when using the protocol, noting that implementations may differ by programming language.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Flight.rst#2025-04-16_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n+----------------+-------------------------------------------+\n|Error Code      |Description                                |\n+================+===========================================+\n|UNKNOWN         |An unknown error. The default if no other  |\n|                |error applies.                             |\n+----------------+-------------------------------------------+\n|INTERNAL        |An error internal to the service           |\n|                |implementation occurred.                   |\n+----------------+-------------------------------------------+\n|INVALID_ARGUMENT|The client passed an invalid argument to   |\n|                |the RPC.                                   |\n+----------------+-------------------------------------------+\n|TIMED_OUT       |The operation exceeded a timeout or        |\n|                |deadline.                                  |\n+----------------+-------------------------------------------+\n|NOT_FOUND       |The requested resource (action, data       |\n|                |stream) was not found.                     |\n+----------------+-------------------------------------------+\n|ALREADY_EXISTS  |The resource already exists.               |\n+----------------+-------------------------------------------+\n|CANCELLED       |The operation was cancelled (either by the |\n|                |client or the server).                     |\n+----------------+-------------------------------------------+\n|UNAUTHENTICATED |The client is not authenticated.           |\n+----------------+-------------------------------------------+\n|UNAUTHORIZED    |The client is authenticated, but does not  |\n|                |have permissions for the requested         |\n|                |operation.                                 |\n+----------------+-------------------------------------------+\n|UNIMPLEMENTED   |The RPC is not implemented.                |\n+----------------+-------------------------------------------+\n|UNAVAILABLE     |The server is not available. May be emitted|\n|                |by the client for connectivity reasons.    |\n+----------------+-------------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Initializing Flight Server in C++\nDESCRIPTION: Demonstrates how to initialize and start a Flight server, including setting up the listening location, configuring shutdown behavior, and starting the server process.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/flight.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nstd::unique_ptr<arrow::flight::FlightServerBase> server;\n// Initialize server\narrow::flight::Location location;\n// Listen to all interfaces on a free port\nARROW_CHECK_OK(arrow::flight::Location::ForGrpcTcp(\"0.0.0.0\", 0, &location));\narrow::flight::FlightServerOptions options(location);\n\n// Start the server\nARROW_CHECK_OK(server->Init(options));\n// Exit with a clean error code (0) on SIGTERM\nARROW_CHECK_OK(server->SetShutdownOnSignals({SIGTERM}));\n\nstd::cout << \"Server listening on localhost:\" << server->port() << std::endl;\nARROW_CHECK_OK(server->Serve());\n```\n\n----------------------------------------\n\nTITLE: Using Extension Types with Custom Scalar Conversion\nDESCRIPTION: This code demonstrates how arrays with custom extension types provide scalars that convert to custom Python objects.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> storage = pa.array([[1, 2, 3], [4, 5, 6]], pa.list_(pa.float32(), 3))\n>>> arr = pa.ExtensionArray.from_storage(Point3DType(), storage)\n>>> arr[0].as_py()\nPoint3D(x=1.0, y=2.0, z=3.0)\n\n>>> arr.to_pylist()\n[Point3D(x=1.0, y=2.0, z=3.0), Point3D(x=4.0, y=5.0, z=6.0)]\n```\n\n----------------------------------------\n\nTITLE: Extended Expressions with Substrait in Java\nDESCRIPTION: Implementation of Substrait extended expressions for projections and filters. The example demonstrates filtering nations and creating computed columns using string concatenation and arithmetic operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/substrait.rst#2025-04-16_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nimport com.google.common.collect.ImmutableList;\nimport io.substrait.isthmus.SqlExpressionToSubstrait;\nimport io.substrait.proto.ExtendedExpression;\nimport org.apache.arrow.dataset.file.FileFormat;\nimport org.apache.arrow.dataset.file.FileSystemDatasetFactory;\nimport org.apache.arrow.dataset.jni.NativeMemoryPool;\nimport org.apache.arrow.dataset.scanner.ScanOptions;\nimport org.apache.arrow.dataset.scanner.Scanner;\nimport org.apache.arrow.dataset.source.Dataset;\nimport org.apache.arrow.dataset.source.DatasetFactory;\nimport org.apache.arrow.memory.BufferAllocator;\nimport org.apache.arrow.memory.RootAllocator;\nimport org.apache.arrow.vector.ipc.ArrowReader;\nimport org.apache.calcite.sql.parser.SqlParseException;\n\nimport java.nio.ByteBuffer;\nimport java.util.Base64;\nimport java.util.Optional;\n\npublic class ClientSubstraitExtendedExpressionsCookbook {\n\n  public static void main(String[] args) throws SqlParseException {\n    projectAndFilterDataset();\n  }\n\n  private static void projectAndFilterDataset() throws SqlParseException {\n    String uri = \"file:///Users/data/tpch_parquet/nation.parquet\";\n    ScanOptions options =\n        new ScanOptions.Builder(/*batchSize*/ 32768)\n            .columns(Optional.empty())\n            .substraitFilter(getByteBuffer(new String[]{\"N_NATIONKEY > 18\"}))\n            .substraitProjection(getByteBuffer(new String[]{\"N_REGIONKEY + 10\",\n                \"N_NAME || CAST(' - ' as VARCHAR) || N_COMMENT\"}))\n            .build();\n    try (BufferAllocator allocator = new RootAllocator();\n         DatasetFactory datasetFactory =\n             new FileSystemDatasetFactory(\n                 allocator, NativeMemoryPool.getDefault(), FileFormat.PARQUET, uri);\n         Dataset dataset = datasetFactory.finish();\n         Scanner scanner = dataset.newScan(options);\n         ArrowReader reader = scanner.scanBatches()) {\n      while (reader.loadNextBatch()) {\n        System.out.println(reader.getVectorSchemaRoot().contentToTSVString());\n      }\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  private static ByteBuffer getByteBuffer(String[] sqlExpression) throws SqlParseException {\n    String schema =\n        \"CREATE TABLE NATION (N_NATIONKEY INT NOT NULL, N_NAME VARCHAR, \"\n            + \"N_REGIONKEY INT NOT NULL, N_COMMENT VARCHAR)\";\n    SqlExpressionToSubstrait expressionToSubstrait = new SqlExpressionToSubstrait();\n    ExtendedExpression expression =\n        expressionToSubstrait.convert(sqlExpression, ImmutableList.of(schema));\n    byte[] expressionToByte =\n        Base64.getDecoder().decode(Base64.getEncoder().encodeToString(expression.toByteArray()));\n    ByteBuffer byteBuffer = ByteBuffer.allocateDirect(expressionToByte.length);\n    byteBuffer.put(expressionToByte);\n    return byteBuffer;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Arrow Data Aggregation Functions\nDESCRIPTION: List of supported aggregation functions in Arrow for dplyr operations including summarize(), count(), tally(), and distinct(). These functions work with Arrow Datasets, Tables, and RecordBatches.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_12\n\nLANGUAGE: R\nCODE:\n```\nn()\nn_distinct()\nmin()\nmax()\nsum()\nmean()\nvar()\nsd()\nany()\nall()\nmedian()\nquantile()\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Files with TableReader in C++\nDESCRIPTION: Demonstrates how to read a CSV file into an Arrow Table using TableReader. Shows initialization with default options and error handling for both reader creation and data reading operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/csv.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"arrow/csv/api.h\"\n\n{\n   // ...\n   arrow::io::IOContext io_context = arrow::io::default_io_context();\n   std::shared_ptr<arrow::io::InputStream> input = ...;\n\n   auto read_options = arrow::csv::ReadOptions::Defaults();\n   auto parse_options = arrow::csv::ParseOptions::Defaults();\n   auto convert_options = arrow::csv::ConvertOptions::Defaults();\n\n   // Instantiate TableReader from input stream and options\n   auto maybe_reader =\n     arrow::csv::TableReader::Make(io_context,\n                                   input,\n                                   read_options,\n                                   parse_options,\n                                   convert_options);\n   if (!maybe_reader.ok()) {\n     // Handle TableReader instantiation error...\n   }\n   std::shared_ptr<arrow::csv::TableReader> reader = *maybe_reader;\n\n   // Read table from CSV file\n   auto maybe_table = reader->Read();\n   if (!maybe_table.ok()) {\n     // Handle CSV read error\n     // (for example a CSV syntax error or failed type conversion)\n   }\n   std::shared_ptr<arrow::Table> table = *maybe_table;\n}\n```\n\n----------------------------------------\n\nTITLE: Reading ORC Files with ORCFileReader in C++\nDESCRIPTION: This snippet demonstrates how to use the ORCFileReader class to read an entire ORC file into an Arrow Table. It shows the process of opening the file reader and reading the data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/orc.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <arrow/adapters/orc/adapter.h>\n\n{\n    // ...\n    arrow::Status st;\n    arrow::MemoryPool* pool = default_memory_pool();\n    std::shared_ptr<arrow::io::RandomAccessFile> input = ...;\n\n    // Open ORC file reader\n    auto maybe_reader = arrow::adapters::orc::ORCFileReader::Open(input, pool);\n    if (!maybe_reader.ok()) {\n        // Handle error instantiating file reader...\n    }\n    std::unique_ptr<arrow::adapters::orc::ORCFileReader> reader = maybe_reader.ValueOrDie();\n\n    // Read entire file as a single Arrow table\n    auto maybe_table = reader->Read();\n    if (!maybe_table.ok()) {\n        // Handle error reading ORC data...\n    }\n    std::shared_ptr<arrow::Table> table = maybe_table.ValueOrDie();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for PyArrow Development\nDESCRIPTION: Sets up a Conda environment with all dependencies needed for Arrow C++ and PyArrow development, targeting Python 3.10 with pandas integration.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n$ conda create -y -n pyarrow-dev -c conda-forge \\\n       --file arrow/ci/conda_env_unix.txt \\\n       --file arrow/ci/conda_env_cpp.txt \\\n       --file arrow/ci/conda_env_python.txt \\\n       --file arrow/ci/conda_env_gandiva.txt \\\n       compilers \\\n       python=3.10 \\\n       pandas\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow-specific WriterProperties in C++\nDESCRIPTION: This snippet shows how to create an ArrowWriterProperties object to configure Arrow-specific settings for Parquet writing, such as enabling INT96 timestamps and storing the Arrow schema in the Parquet metadata.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/parquet.rst#2025-04-16_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"parquet/arrow/writer.h\"\n\nusing parquet::ArrowWriterProperties;\n\nstd::shared_ptr<ArrowWriterProperties> arrow_props = ArrowWriterProperties::Builder()\n   .enable_deprecated_int96_timestamps() // default False\n   ->store_schema() // default False\n   ->build();\n```\n\n----------------------------------------\n\nTITLE: Installing PyArrow with Pip\nDESCRIPTION: Command to install PyArrow using pip package manager from PyPI. This method works on Linux, macOS, and Windows using binary wheels.\nSOURCE: https://github.com/apache/arrow/blob/main/python/README.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install pyarrow\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet WriterProperties in C++\nDESCRIPTION: This snippet demonstrates how to create and configure a WriterProperties object for Parquet file writing. It shows how to set compression, encoding, and dictionary options both globally and for specific columns.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/parquet.rst#2025-04-16_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"parquet/arrow/writer.h\"\n#include \"arrow/util/type_fwd.h\"\n\nusing parquet::WriterProperties;\nusing arrow::Compression;\nusing parquet::Encoding;\n\nstd::shared_ptr<WriterProperties> props = WriterProperties::Builder()\n  .compression(Compression::SNAPPY)        // Fallback\n  ->compression(\"colA\", Compression::ZSTD) // Only applies to column \"colA\"\n  ->encoding(Encoding::BIT_PACKED)         // Fallback\n  ->encoding(\"colB\", Encoding::RLE)        // Only applies to column \"colB\"\n  ->disable_dictionary(\"colB\")             // Never dictionary-encode column \"colB\"\n  ->build();\n```\n\n----------------------------------------\n\nTITLE: Unsafe appending for optimized Int64Array construction in Apache Arrow C++\nDESCRIPTION: This snippet shows how to use unsafe appending methods for higher performance when building an Int64Array, assuming the working area has been correctly presized.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/arrays.rst#2025-04-16_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\narrow::Int64Builder builder;\nbuilder.Reserve(8);\nbuilder.UnsafeAppend(1);\nbuilder.UnsafeAppend(2);\nbuilder.UnsafeAppend(3);\nbuilder.UnsafeAppendNull();\nbuilder.UnsafeAppend(5);\nbuilder.UnsafeAppend(6);\nbuilder.UnsafeAppend(7);\nbuilder.UnsafeAppend(8);\n\nauto maybe_array = builder.Finish();\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow C Device Data Interface Constants and Structs in C\nDESCRIPTION: This C code snippet defines the Arrow C Device Data Interface, including device type definitions (ArrowDeviceType) and device-specific macros for various hardware like CPU, CUDA, OpenCL, and others. It also defines the ArrowDeviceArray structure, which embeds the ArrowArray structure and includes device ID, device type, and synchronization event information.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDeviceDataInterface.rst#2025-04-16_snippet_0\n\nLANGUAGE: c\nCODE:\n```\n\"#ifndef ARROW_C_DEVICE_DATA_INTERFACE\\n#define ARROW_C_DEVICE_DATA_INTERFACE\\n\\n// Device type for the allocated memory\\ntypedef int32_t ArrowDeviceType;\\n\\n// CPU device, same as using ArrowArray directly\\n#define ARROW_DEVICE_CPU 1\\n// CUDA GPU Device\\n#define ARROW_DEVICE_CUDA 2\\n// Pinned CUDA CPU memory by cudaMallocHost\\n#define ARROW_DEVICE_CUDA_HOST 3\\n// OpenCL Device\\n#define ARROW_DEVICE_OPENCL 4\\n// Vulkan buffer for next-gen graphics\\n#define ARROW_DEVICE_VULKAN 7\\n// Metal for Apple GPU\\n#define ARROW_DEVICE_METAL 8\\n// Verilog simulator buffer\\n#define ARROW_DEVICE_VPI 9\\n// ROCm GPUs for AMD GPUs\\n#define ARROW_DEVICE_ROCM 10\\n// Pinned ROCm CPU memory allocated by hipMallocHost\\n#define ARROW_DEVICE_ROCM_HOST 11\\n// Reserved for extension\\n//\\n// used to quickly test extension devices, semantics\\n// can differ based on implementation\\n#define ARROW_DEVICE_EXT_DEV 12\\n// CUDA managed/unified memory allocated by cudaMallocManaged\\n#define ARROW_DEVICE_CUDA_MANAGED 13\\n// Unified shared memory allocated on a oneAPI\\n// non-partitioned device.\\n//\\n// A call to the oneAPI runtime is required to determine the\\n// device type, the USM allocation type and the sycl context\\n// that it is bound to.\\n#define ARROW_DEVICE_ONEAPI 14\\n// GPU support for next-gen WebGPU standard\\n#define ARROW_DEVICE_WEBGPU 15\\n// Qualcomm Hexagon DSP\\n#define ARROW_DEVICE_HEXAGON 16\\n\\nstruct ArrowDeviceArray {\\n  struct ArrowArray array;\\n  int64_t device_id;\\n  ArrowDeviceType device_type;\\n  void* sync_event;\\n\\n  // reserved bytes for future expansion\\n  int64_t reserved[3];\\n};\\n\\n#endif  // ARROW_C_DEVICE_DATA_INTERFACE\"\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Field Objects\nDESCRIPTION: Demonstrates how to create Field objects, which combine a name with a type and optional metadata. Fields are used as building blocks for more complex data structures like structs and schemas.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nf0 = pa.field('int32_field', t1)\nf0\nf0.name\nf0.type\n```\n\n----------------------------------------\n\nTITLE: Reading Feather V1 File into MATLAB Table\nDESCRIPTION: This snippet demonstrates how to read a Feather V1 file into a MATLAB table. It uses the 'featherread' function to load the data from the Feather file, preserving the structure and data types of the original table.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_21\n\nLANGUAGE: matlab\nCODE:\n```\n>> filename = \"table.feather\";\n\n>> t = featherread(filename)\n\nt =\n\n  3×3 table\n\n    Var1    Var2    Var3\n    ____    ____    _____\n\n    \"A\"      1      true\n    \"B\"      2      false\n    \"C\"      3      true\n```\n\n----------------------------------------\n\nTITLE: Writing a Feather File using PyArrow\nDESCRIPTION: Demonstrates how to write a pandas DataFrame to a Feather file using the write_feather function from the pyarrow.feather module.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/feather.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.feather as feather\nfeather.write_feather(df, '/path/to/file')\n```\n\n----------------------------------------\n\nTITLE: Integrating PyArrow Extension Types with Pandas\nDESCRIPTION: This example shows how to make PyArrow extension types work with pandas by implementing the to_pandas_dtype method and handling the conversion between Arrow and pandas.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass PeriodType(pa.ExtensionType):\n    ...\n\n    def to_pandas_dtype(self):\n        import pandas as pd\n        return pd.PeriodDtype(freq=self.freq)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Parameterized Extension Type in PyArrow\nDESCRIPTION: This code shows how to create a parameterized extension type for representing time periods, with frequency as a parameter that is serialized and deserialized with the type.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass PeriodType(pa.ExtensionType):\n\n    def __init__(self, freq):\n        # attributes need to be set first before calling\n        # super init (as that calls serialize)\n        self._freq = freq\n        super().__init__(pa.int64(), \"my_package.period\")\n\n    @property\n    def freq(self):\n        return self._freq\n\n    def __arrow_ext_serialize__(self):\n        return \"freq={}\".format(self.freq).encode()\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        # Return an instance of this subclass given the serialized\n        # metadata.\n        serialized = serialized.decode()\n        assert serialized.startswith(\"freq=\")\n        freq = serialized.split(\"=\")[1]\n        return PeriodType(freq)\n```\n\n----------------------------------------\n\nTITLE: Creating Tables with Tensor Arrays in PyArrow\nDESCRIPTION: Shows how to create a PyArrow Table with tensor arrays and custom schema. The example includes creating arrays with different data types, defining a schema with tensor types, and viewing the resulting table structure.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> data = [\n...     pa.array([1, 2, 3]),\n...     pa.array([\"foo\", \"bar\", None]),\n...     pa.array([True, None, True]),\n...     tensor_array,\n...     tensor_array_2\n... ]\n>>> my_schema = pa.schema([(\"f0\", pa.int8()),\n...                        (\"f1\", pa.string()),\n...                        (\"f2\", pa.bool_()),\n...                        (\"tensors_int\", tensor_type),\n...                        (\"tensors_float\", tensor_type_2)])\n>>> table = pa.Table.from_arrays(data, schema=my_schema)\n>>> table\npyarrow.Table\nf0: int8\nf1: string\nf2: bool\ntensors_int: extension<arrow.fixed_shape_tensor[value_type=int32, shape=[2,2]]>\ntensors_float: extension<arrow.fixed_shape_tensor[value_type=float, shape=[2,2]]>\n----\nf0: [[1,2,3]]\nf1: [[\"foo\",\"bar\",null]]\nf2: [[true,null,true]]\ntensors_int: [[[1,2,3,4],[10,20,30,40],[100,200,300,400]]]\ntensors_float: [[[1,2,3,4],[10,20,30,40],[100,200,300,400]]]\n```\n\n----------------------------------------\n\nTITLE: Starting a Flight Server on a Network Interface\nDESCRIPTION: Code for starting a Flight server that listens on all network interfaces with a dynamically assigned port. The server is created and then enters a blocking serve mode.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/flight.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Listen to all interfaces on a free port\nserver = MyFlightServer(\"grpc://0.0.0.0:0\")\n\nprint(\"Server listening on port\", server.port)\nserver.serve()\n```\n\n----------------------------------------\n\nTITLE: Reading JSON with Nested Structures in Python\nDESCRIPTION: This snippet demonstrates reading a JSON file with nested structures (arrays and objects) and shows the resulting table structure with inferred types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/json.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> table = json.read_json(\"my_data.json\")\n>>> table\npyarrow.Table\na: list<item: int64>\n  child 0, item: int64\nb: struct<c: bool, d: timestamp[s]>\n  child 0, c: bool\n  child 1, d: timestamp[s]\n>>> table.to_pandas()\n          a                                       b\n0     [1, 2]   {'c': True, 'd': 1991-02-03 00:00:00}\n1  [3, 4, 5]  {'c': False, 'd': 2019-04-01 00:00:00}\n```\n\n----------------------------------------\n\nTITLE: Specifying Dimension Names in Fixed Shape Tensor Type\nDESCRIPTION: Demonstrates how to create a fixed shape tensor type with named dimensions. This example uses the \"C\", \"H\", \"W\" naming convention for the NCHW image format, providing semantic meaning to each dimension.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n>>> tensor_type = pa.fixed_shape_tensor(pa.bool_(), [2, 2, 3], dim_names=[\"C\", \"H\", \"W\"])\n```\n\n----------------------------------------\n\nTITLE: Creating a Partitioned Dataset with Apache Arrow C++\nDESCRIPTION: Demonstrates how to use Apache Arrow's dataset writing functionality to create a small partitioned dataset. The code creates directories ('part=a' and 'part=b') and writes Parquet files within these directories, excluding the 'part' column from the Parquet files.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n#include <arrow/dataset/api.h>\n// More includes and namespace usage\n\nint main() {\n    // Dataset writing code here\n}\n```\n\n----------------------------------------\n\nTITLE: Allocating Memory with BufferAllocator in Java\nDESCRIPTION: Demonstrates how to create a RootAllocator, allocate an ArrowBuf, and properly close resources using try-with-resources.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/memory.rst#2025-04-16_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nimport org.apache.arrow.memory.ArrowBuf;\nimport org.apache.arrow.memory.BufferAllocator;\nimport org.apache.arrow.memory.RootAllocator;\n\ntry(BufferAllocator bufferAllocator = new RootAllocator(8 * 1024)){\n    ArrowBuf arrowBuf = bufferAllocator.buffer(4 * 1024);\n    System.out.println(arrowBuf);\n    arrowBuf.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Encryption for Parquet Files in PyArrow\nDESCRIPTION: Example of creating an encryption configuration for Parquet files, specifying which columns to encrypt with which master keys. This configuration is used when writing encrypted Parquet files.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nencryption_config = pq.EncryptionConfiguration(\n   footer_key=\"footer_key_name\",\n   column_keys={\n      \"column_key_name\": [\"Column1\", \"Column2\"],\n   },\n)\n```\n\n----------------------------------------\n\nTITLE: Writing Arrow Files with PyArrow FileSystem\nDESCRIPTION: Demonstrates how to open an output stream on a filesystem and write a table in Arrow format. This shows the direct usage of the filesystem interface for file operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\nlocal = fs.LocalFileSystem()\n\nwith local.open_output_stream(\"test.arrow\") as file:\n   with pa.RecordBatchFileWriter(file, table.schema) as writer:\n      writer.write_table(table)\n```\n\n----------------------------------------\n\nTITLE: Filtering Partitioned Data in Apache Arrow C++\nDESCRIPTION: Illustrates filtering on partition keys in a partitioned dataset using Apache Arrow. The code shows how filtering avoids loading non-matching files, which can be a significant performance optimization.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n// Filtering dataset based on partition keys\n// Filtering code example\n```\n\n----------------------------------------\n\nTITLE: Searching Vector Elements in Java\nDESCRIPTION: Apache Arrow's Java library provides algorithms like linear, binary, parallel, and range search to find specific values within vectors. `VectorSearcher` offers linear and binary search implementations, while `ParallelSearcher` supports parallel search, and `VectorRangeSearcher` facilitates range search. Each algorithm has specific vector type requirements and time complexities.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/algorithm.rst#2025-04-16_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n\"\"\"\nSearch algorithms available:\n1. Linear search via VectorSearcher.linearSearch (O(n))\n2. Binary search via VectorSearcher.binarySearch (O(log(n))), requires sorted vectors\n3. Parallel search using ParallelSearcher for large vectors\n4. Range search using VectorRangeSearcher for sorted vectors\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Vectors in Java\nDESCRIPTION: This snippet demonstrates how to create `BitVector` and `VarCharVector` in Java, populate them with data, and combine them into a `VectorSchemaRoot`. The `VectorSchemaRoot` is used as a container for the vectors and schema.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/ipc.rst#2025-04-16_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nBitVector bitVector = new BitVector(\"boolean\", allocator);\nVarCharVector varCharVector = new VarCharVector(\"varchar\", allocator);\nfor (int i = 0; i < 10; i++) {\n  bitVector.setSafe(i, i % 2 == 0 ? 0 : 1);\n  varCharVector.setSafe(i, (\"test\" + i).getBytes(StandardCharsets.UTF_8));\n}\nbitVector.setValueCount(10);\nvarCharVector.setValueCount(10);\n\nList<Field> fields = Arrays.asList(bitVector.getField(), varCharVector.getField());\nList<FieldVector> vectors = Arrays.asList(bitVector, varCharVector);\nVectorSchemaRoot root = new VectorSchemaRoot(fields, vectors);\n```\n\n----------------------------------------\n\nTITLE: Converting RecordBatch to Tensor in C++\nDESCRIPTION: This snippet demonstrates how to convert a RecordBatch to a Tensor. It supports signed and unsigned integer types and float types. The resulting Tensor is validated after conversion.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/converting_recordbatch_to_tensor.rst#2025-04-16_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nstd::shared_ptr<RecordBatch> batch;\n\nASSERT_OK_AND_ASSIGN(auto tensor, batch->ToTensor());\nASSERT_OK(tensor->Validate());\n```\n\n----------------------------------------\n\nTITLE: Writing ORC Files with ORCFileWriter in C++\nDESCRIPTION: This snippet shows how to use the ORCFileWriter class to write an Arrow Table to an ORC file. It demonstrates the process of opening the file writer, writing the data, and closing the writer.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/orc.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <arrow/adapters/orc/adapter.h>\n{\n    // Oneshot write\n    // ...\n    std::shared_ptr<arrow::io::OutputStream> output = ...;\n    auto writer_options = WriterOptions();\n    auto maybe_writer = arrow::adapters::orc::ORCFileWriter::Open(output.get(), writer_options);\n    if (!maybe_writer.ok()) {\n       // Handle error instantiating file writer...\n    }\n    std::unique_ptr<arrow::adapters::orc::ORCFileWriter> writer = maybe_writer.ValueOrDie();\n    if (!(writer->Write(*input_table)).ok()) {\n        // Handle write error...\n    }\n    if (!(writer->Close()).ok()) {\n        // Handle close error...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing FlightProducer Interface in Java\nDESCRIPTION: Demonstrates creating a custom Flight service by implementing the FlightProducer interface. Provides a basic template for defining RPC methods with error handling and call context.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/flight.rst#2025-04-16_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic class TutorialFlightProducer implements FlightProducer {\n    @Override\n    // Override methods or use NoOpFlightProducer for only methods needed\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Complex Record Batch Schema and Data in Apache Arrow\nDESCRIPTION: This snippet defines a complex record batch schema with nested types and provides sample data. It includes a struct type with an integer, a list of integers, and a float field, along with a UTF-8 string field.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/StatisticsSchema.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nSchema::\n\n    col1: struct<a: int32, b: list<item: int64>, c: float64>\n    col2: utf8\n\nData::\n\n    col1: [\n            {a: 1, b: [20, 30, 40], c: 2.9},\n            {a: 2, b: null,         c: -2.9},\n            {a: 3, b: [99],         c: null},\n          ]\n    col2: [\"x\", null, \"z\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Up a File Format for Dataset Writing in C++\nDESCRIPTION: This code creates a ParquetFileFormat object to specify that the Dataset should be written in Parquet format, which is commonly used with Arrow for efficient storage of columnar data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\n// Create our file format, in this case Parquet\nauto format = std::make_shared<ParquetFileFormat>();\n```\n\n----------------------------------------\n\nTITLE: Using a Custom Extension Array Class with 3D Points\nDESCRIPTION: This code demonstrates how to create and use an array with the custom Point3D extension type and access its specialized methods.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> storage = pa.array([[1, 2, 3], [4, 5, 6]], pa.list_(pa.float32(), 3))\n>>> arr = pa.ExtensionArray.from_storage(Point3DType(), storage)\n>>> arr\n<__main__.Point3DArray object at 0x7f40dea80670>\n[\n    [\n        1,\n        2,\n        3\n    ],\n    [\n        4,\n        5,\n        6\n    ]\n]\n\n>>> arr.to_numpy_array()\narray([[1., 2., 3.],\n   [4., 5., 6.]], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Arrow Tables from Files\nDESCRIPTION: Creates Arrow tables by loading data from various file formats including Arrow, CSV, and Parquet. Each format is loaded using the same method with format specification.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire 'arrow'\nrequire 'parquet'\n\ntable = Arrow::Table.load('data.arrow')\ntable = Arrow::Table.load('data.csv', format: :csv)\ntable = Arrow::Table.load('data.parquet', format: :parquet)\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Int8Array with Null Values\nDESCRIPTION: Example demonstrating how to specify null values when constructing an Arrow Int8Array by providing a validity mask.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_6\n\nLANGUAGE: matlab\nCODE:\n```\n>> matlabArray = int8([122, -1, 456, -10, 789])\n\nmatlabArray =\n\n  1×5 int8 row vector\n\n    122     -1    127    -10    127\n\n% Treat all negative array elements as Null\n>> validElements = matlabArray > 0\n\nvalidElements =\n\n  1×5 logical array\n\n   1   0   1   0   1\n\n% Specify which values are Null/Valid by supplying a logical validity \"mask\"\n>> arrowArray = arrow.array(matlabArray, Valid=validElements)\n\narrowArray = \n\n[\n  122,\n  null,\n  127,\n  null,\n  127\n]\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Array\nDESCRIPTION: Example of using the convenience function arrow_array() to create Arrow Arrays.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_2\n\nLANGUAGE: R\nCODE:\n```\narrow_array(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Sink Node Consumer in C++\nDESCRIPTION: Example of creating a custom sink node consumer that tracks the number of batches processed. Demonstrates how to implement the SinkNodeConsumer interface and handle batch consumption.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/user_guide.rst#2025-04-16_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nstd::atomic<uint32_t> batches_seen{0};\narrow::Future<> finish = arrow::Future<>::Make();\nstruct CustomSinkNodeConsumer : public cp::SinkNodeConsumer {\n\n    CustomSinkNodeConsumer(std::atomic<uint32_t> *batches_seen, arrow::Future<>finish):\n    batches_seen(batches_seen), finish(std::move(finish)) {}\n    // Consumption logic can be written here\n    arrow::Status Consume(cp::ExecBatch batch) override {\n    // data can be consumed in the expected way\n    // transfer to another system or just do some work\n    // and write to disk\n    (*batches_seen)++;\n    return arrow::Status::OK();\n    }\n\n    arrow::Future<> Finish() override { return finish; }\n\n    std::atomic<uint32_t> *batches_seen;\n    arrow::Future<> finish;\n\n};\n\nstd::shared_ptr<CustomSinkNodeConsumer> consumer =\n        std::make_shared<CustomSinkNodeConsumer>(&batches_seen, finish);\n\narrow::acero::ExecNode *consuming_sink;\n\nARROW_ASSIGN_OR_RAISE(consuming_sink, MakeExecNode(\"consuming_sink\", plan.get(),\n    {source}, cp::ConsumingSinkNodeOptions(consumer)));\n```\n\n----------------------------------------\n\nTITLE: Instantiating a FileSystem in PyArrow\nDESCRIPTION: This code demonstrates how to create a FileSystem object using either a direct constructor or by inferring from a URI. The example shows creating a local filesystem directly and inferring an S3 filesystem from a URI.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from pyarrow import fs\n>>> local = fs.LocalFileSystem()\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> s3, path = fs.FileSystem.from_uri(\"s3://my-bucket\")\n>>> s3\n<pyarrow._s3fs.S3FileSystem at 0x7f6760cbf4f0>\n>>> path\n'my-bucket'\n```\n\n----------------------------------------\n\nTITLE: Writing Complete Arrow Table to Parquet\nDESCRIPTION: Example showing how to write an entire Arrow Table to a Parquet file using the WriteTable function with default settings.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/parquet.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\narrow::Status WriteFullFile(\n  const std::shared_ptr<arrow::Table>& table) {\n  ARROW_ASSIGN_OR_THROW(\n      output,\n      arrow::io::FileOutputStream::Open(\"test.parquet\"));\n  ARROW_RETURN_NOT_OK(\n      parquet::arrow::WriteTable(*table, arrow::default_memory_pool(), output));\n  return arrow::Status::OK();\n}\n```\n\n----------------------------------------\n\nTITLE: Using CommandGetExportedKeys for Foreign Key Relationships\nDESCRIPTION: CommandGetExportedKeys provides a mechanism to list foreign key columns that reference primary keys in a specified table. This information is crucial for understanding the integrity constraints between tables.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_3\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandGetExportedKeys\"\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Duck Array with __arrow_array__ Protocol in Python\nDESCRIPTION: Example of implementing the __arrow_array__ protocol for a custom duck array class to make it compatible with pyarrow.array() conversion. The method takes an optional type parameter and returns a pyarrow Array or ChunkedArray.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MyDuckArray:\n\n    ...\n\n    def __arrow_array__(self, type=None):\n        # convert the underlying array values to a pyarrow Array\n        import pyarrow\n        return pyarrow.array(..., type=type)\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Arrow IPC File - Python\nDESCRIPTION: Creates a temporary file to write Arrow RecordBatches using IPC format, then exports the stream to Java and reads it back. Demonstrates bidirectional data exchange between Python and Java using Arrow's IPC functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nwith tempfile.NamedTemporaryFile() as temp:\n    with pa.ipc.new_file(temp.name, schema) as sink:\n        for batch in batches:\n            sink.write_batch(batch)\n\n    demo.exportStream(temp.name, c_stream_ptr)\n    with pa.RecordBatchReader._import_from_c(c_stream_ptr) as source:\n        print(\"IPC file read by Java:\")\n        print(source.read_all())\n```\n\n----------------------------------------\n\nTITLE: Python-R Integration Using C Data Interface\nDESCRIPTION: This Python code demonstrates how to create a PyArrow array, export it to C Data Interface, pass it to an R function, and import the result back as a PyArrow array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_r.rst#2025-04-16_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Get a reference to the addthree_cdata R function\nimport rpy2.robjects as robjects\nr_source = robjects.r[\"source\"]\nr_source(\"addthree.R\")\naddthree_cdata = robjects.r[\"addthree_cdata\"]\n\n# Create the pyarrow array we want to pass to R\nimport pyarrow\narray = pyarrow.array((1, 2, 3))\n\n# Import the pyarrow module that provides access to the C Data interface\nfrom pyarrow.cffi import ffi as arrow_c\n\n# Allocate structures where we will export the Array data\n# and the Array schema. They will be released when we exit the with block.\nwith arrow_c.new(\"struct ArrowArray*\") as c_array, \\\n     arrow_c.new(\"struct ArrowSchema*\") as c_schema:\n    # Get the references to the C Data structures.\n    c_array_ptr = int(arrow_c.cast(\"uintptr_t\", c_array))\n    c_schema_ptr = int(arrow_c.cast(\"uintptr_t\", c_schema))\n\n    # Export the Array and its schema to the C Data structures.\n    array._export_to_c(c_array_ptr)\n    array.type._export_to_c(c_schema_ptr)\n\n    # Invoke the R addthree_cdata function passing the references\n    # to the array and schema C Data structures.\n    # Those references are passed as strings as R doesn't have\n    # native support for 64bit integers, so the integers are\n    # converted to their string representation for R to convert it back.\n    r_result_array = addthree_cdata(str(c_array_ptr), str(c_schema_ptr))\n\n    # r_result will be an Environment variable that contains the\n    # arrow Array built from R as the return value of addthree.\n    # To make it available as a Python pyarrow array we need to export\n    # it as a C Data structure invoking the Array$export_to_c R method\n    r_result_array[\"export_to_c\"](str(c_array_ptr), str(c_schema_ptr))\n\n    # Once the returned array is exported to a C Data infrastructure\n    # we can import it back into pyarrow using Array._import_from_c\n    py_array = pyarrow.Array._import_from_c(c_array_ptr, c_schema_ptr)\n\nprint(\"RESULT\", py_array)\n```\n\n----------------------------------------\n\nTITLE: Creating Nested List Arrays in PyArrow\nDESCRIPTION: Shows how to create a nested list array using type inference, where each element can be a list, None, or a list with None elements.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nnested_arr = pa.array([[], None, [1, 2], [None, 1]])\nprint(nested_arr.type)\n```\n\n----------------------------------------\n\nTITLE: Creating and Writing Table to ORC Format\nDESCRIPTION: Creates a sample PyArrow table with different data types and writes it to an ORC file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/orc.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport pyarrow as pa\n\ntable = pa.table(\n    {\n        'one': [-1, np.nan, 2.5],\n        'two': ['foo', 'bar', 'baz'],\n        'three': [True, False, True]\n    }\n)\n\nfrom pyarrow import orc\norc.write_table(table, 'example.orc')\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Scalar\nDESCRIPTION: Example of using the convenience function scalar() to create Arrow Scalars.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_3\n\nLANGUAGE: R\nCODE:\n```\nscalar(x)\n```\n\n----------------------------------------\n\nTITLE: Exporting ArrowArrayStream with PyCapsule in Python\nDESCRIPTION: Details the method for exporting tables, data frames, or streams as an ArrowArrayStream in a PyCapsule. Accepts an optional requested schema for flexible data representation. Ensures that data export complies with the expected format and safety standards.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_3\n\nLANGUAGE: py\nCODE:\n```\n\"\"\"\n    Export the object as an ArrowArrayStream.\n\n    :param requested_schema: A PyCapsule containing a C ArrowSchema representation\n        of a requested schema. Conversion to this schema is best-effort. See\n        `Schema Requests`_.\n    :type requested_schema: PyCapsule or None\n\n    :return: A PyCapsule containing a C ArrowArrayStream representation of the\n        object. The capsule must have a name of \\\"arrow_array_stream\\\".\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Compression Options for Feather V2 Files\nDESCRIPTION: Shows different compression options (LZ4, ZSTD, or no compression) when writing Feather V2 files.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/feather.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Uses LZ4 by default\nfeather.write_feather(df, file_path)\n\n# Use LZ4 explicitly\nfeather.write_feather(df, file_path, compression='lz4')\n\n# Use ZSTD\nfeather.write_feather(df, file_path, compression='zstd')\n\n# Do not compress\nfeather.write_feather(df, file_path, compression='uncompressed')\n```\n\n----------------------------------------\n\nTITLE: Listing Files with PyArrow FileSystem\nDESCRIPTION: Shows how to list files and directories on a filesystem using get_file_info method with FileSelector. The example demonstrates recursive directory listing and querying information about individual files.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> local.get_file_info(fs.FileSelector(\"dataset/\", recursive=True))\n[<FileInfo for 'dataset/part=B': type=FileType.Directory>,\n <FileInfo for 'dataset/part=B/data0.parquet': type=FileType.File, size=1564>,\n <FileInfo for 'dataset/part=A': type=FileType.Directory>,\n <FileInfo for 'dataset/part=A/data0.parquet': type=FileType.File, size=1564>]\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> local.get_file_info('test.arrow')\n<FileInfo for 'test.arrow': type=FileType.File, size=3250>\n\n>>> local.get_file_info('non_existent')\n<FileInfo for 'non_existent': type=FileType.NotFound>\n```\n\n----------------------------------------\n\nTITLE: Creating Record Batches in PyArrow\nDESCRIPTION: Shows how to create record batches from multiple arrays with column names.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndata = [\n    pa.array([1, 2, 3, 4]),\n    pa.array(['foo', 'bar', 'baz', None]),\n    pa.array([True, None, False, True])\n]\nbatch = pa.RecordBatch.from_arrays(data, ['f0', 'f1', 'f2'])\n```\n\n----------------------------------------\n\nTITLE: Converting between pandas DataFrame and Arrow Table\nDESCRIPTION: Demonstrates basic conversion between pandas DataFrame and PyArrow Table objects, and how to infer an Arrow schema from a pandas DataFrame.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/pandas.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nimport pandas as pd\n\ndf = pd.DataFrame({\"a\": [1, 2, 3]})\n# Convert from pandas to Arrow\ntable = pa.Table.from_pandas(df)\n# Convert back to pandas\ndf_new = table.to_pandas()\n\n# Infer Arrow schema from pandas\nschema = pa.Schema.from_pandas(df)\n```\n\n----------------------------------------\n\nTITLE: Defining the Arrow C Stream Interface Structure\nDESCRIPTION: This C snippet defines the `ArrowArrayStream` structure with mandatory and optional callbacks to manage and interact with data streams. The callbacks include `get_schema` for querying data schema, `get_next` for fetching data chunks, and `get_last_error` for error handling. It includes a guard to prevent duplicate definitions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CStreamInterface.rst#2025-04-16_snippet_0\n\nLANGUAGE: c\nCODE:\n```\n#ifndef ARROW_C_STREAM_INTERFACE\n#define ARROW_C_STREAM_INTERFACE\n\nstruct ArrowArrayStream {\n  // Callbacks providing stream functionality\n  int (*get_schema)(struct ArrowArrayStream*, struct ArrowSchema* out);\n  int (*get_next)(struct ArrowArrayStream*, struct ArrowArray* out);\n  const char* (*get_last_error)(struct ArrowArrayStream*);\n\n  // Release callback\n  void (*release)(struct ArrowArrayStream*);\n\n  // Opaque producer-specific data\n  void* private_data;\n};\n\n#endif  // ARROW_C_STREAM_INTERFACE\n```\n\n----------------------------------------\n\nTITLE: Creating Arrays with Type Inference in PyArrow\nDESCRIPTION: Shows how to create Arrow arrays using the pa.array function with automatic type inference. This example demonstrates creating an array with null values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\narr = pa.array([1, 2, None, 3])\narr\n```\n\n----------------------------------------\n\nTITLE: Wrapping PyArrow FileSystem with fsspec Interface\nDESCRIPTION: Demonstrates how to wrap a PyArrow FileSystem object with the fsspec interface using ArrowFSWrapper. This allows using PyArrow filesystem implementations with the richer fsspec API.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> from pyarrow import fs\n>>> local = fs.LocalFileSystem()\n>>> from fsspec.implementations.arrow import ArrowFSWrapper\n>>> local_fsspec = ArrowFSWrapper(local)\n\n>>> local_fsspec.mkdir(\"./test\")\n>>> local_fsspec.touch(\"./test/file.txt\")\n>>> local_fsspec.ls(\"./test/\")\n['./test/file.txt']\n```\n\n----------------------------------------\n\nTITLE: Using fsspec-compatible Azure Blob Storage with PyArrow\nDESCRIPTION: Shows how to use the adlfs package to create an fsspec-compatible Azure Blob Storage filesystem and use it with PyArrow. The example demonstrates dataset access with the filesystem.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport adlfs\n# ... load your credentials and configure the filesystem\nfs = adlfs.AzureBlobFileSystem(account_name=account_name, account_key=account_key)\n\nimport pyarrow.dataset as ds\nds.dataset(\"mycontainer/data/\", filesystem=fs)\n```\n\n----------------------------------------\n\nTITLE: Establishing Connection with Flight Client\nDESCRIPTION: Shows how to create a Flight client by specifying a location and allocator. Provides a template for connecting to a Flight service and performing operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/flight.rst#2025-04-16_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nLocation location = Location.forGrpcInsecure(\"0.0.0.0\", 58104);\n\ntry(BufferAllocator allocator = new RootAllocator();\n    FlightClient client = FlightClient.builder(allocator, location).build()){\n    // ... Consume operations exposed by Flight server\n} catch (Exception e) {\n    e.printStackTrace();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Data Types in C++\nDESCRIPTION: Demonstrates how to instantiate different Arrow data types using factory functions, including integer, timestamp, and list types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/datatypes.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<arrow::DataType> type;\n\n// A 16-bit integer type\ntype = arrow::int16();\n// A 64-bit timestamp type (with microsecond granularity)\ntype = arrow::timestamp(arrow::TimeUnit::MICRO);\n// A list type of single-precision floating-point values\ntype = arrow::list(arrow::float32());\n```\n\n----------------------------------------\n\nTITLE: FixedSizeList Byte Array Layout Example\nDESCRIPTION: Example showing the memory layout of a FixedSizeList<byte>[4] array with length 4, including validity bitmap and values array structure.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\n* Length: 4, Null count: 1\n* Validity bitmap buffer:\n  | Byte 0 (validity bitmap) | Bytes 1-63            |\n  |--------------------------|-----------------------|\n  | 00001101                 | 0 (padding)           |\n\n* Values array (byte array):\n  * Length: 16,  Null count: 0\n  * validity bitmap buffer: Not required\n    | Bytes 0-3       | Bytes 4-7   | Bytes 8-15                      |\n    |-----------------|-------------|---------------------------------|\n    | 192, 168, 0, 12 | unspecified | 192, 168, 0, 25, 192, 168, 0, 1 |\n```\n\n----------------------------------------\n\nTITLE: Comparing Vector Elements in Java\nDESCRIPTION: This snippet discusses the methods provided by Apache Arrow's Java library to compare vector elements using equality and ordering comparisons. Dependencies include the interfaces `VectorValueEqualizer` and `VectorValueComparator`. These methods are vital for sorting and searching algorithms and can be customized as needed.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/algorithm.rst#2025-04-16_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n\"\"\"\nVector elements can be compared using:\n1. VectorValueEqualizer interface for equality comparisons\n2. VectorValueComparator abstract class for ordering comparisons\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Writing a Dataset to Disk in C++\nDESCRIPTION: This snippet performs the actual writing of a Dataset to disk using the configured Scanner and write options. It takes data from the Scanner and writes it to files according to the partitioning scheme.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\n// Write out\nSTATUS_AND_ASSIGN_OR_RAISE(auto,,\n                       FileSystemDataset::Write(write_options, write_scanner));\n```\n\n----------------------------------------\n\nTITLE: Configuring JdbcToArrow Type Mapping\nDESCRIPTION: This code snippet configures the Arrow JDBC Adapter to customize batch size and type mapping for the conversion from JDBC to Arrow. It uses a JdbcToArrowConfigBuilder to set specific conversion rules and then processes the data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/jdbc.rst#2025-04-16_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nJdbcToArrowConfig config = new JdbcToArrowConfigBuilder(allocator, /*calendar=*/null)\\n    .setReuseVectorSchemaRoot(reuseVectorSchemaRoot)\\n    .setJdbcToArrowTypeConverter((jdbcFieldInfo -> {\\n      switch (jdbcFieldInfo.getJdbcType()) {\\n        case Types.BIGINT:\\n          // Assume actual value range is SMALLINT\\n          return new ArrowType.Int(16, true);\\n        default:\\n          return null;\\n      }\\n    }))\\n    .build();\\ntry (ArrowVectorIterator iter = JdbcToArrow.sqlToArrowVectorIterator(rs, config)) {\\n  while (iter.hasNext()) {\\n    VectorSchemaRoot root = iter.next();\\n    // Consume the root…\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow FieldPath Class in C++\nDESCRIPTION: This snippet documents the arrow::FieldPath class, which represents a path to a nested field in an Arrow schema.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenclass:: arrow::FieldPath\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Arrow Modules via Maven\nDESCRIPTION: This Maven configuration snippet outlines the setup necessary to integrate Apache Arrow modules, such as arrow-vector and arrow-memory-netty, within a project. Ensure your pom.xml file includes these settings, particularly specifying correct version numbers and any bill of materials (BOM) configurations to streamline dependency handling.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/install.rst#2025-04-16_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>org.example</groupId>\n    <artifactId>demo</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <properties>\n        <arrow.version>9.0.0</arrow.version>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.arrow</groupId>\n            <artifactId>arrow-vector</artifactId>\n            <version>${arrow.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.arrow</groupId>\n            <artifactId>arrow-memory-netty</artifactId>\n            <version>${arrow.version}</version>\n        </dependency>\n    </dependencies>\n</project>\n```\n\n----------------------------------------\n\nTITLE: Executing Substrait Query Plans in Java with Apache Arrow\nDESCRIPTION: Example showing how to execute a Substrait plan against a Parquet file using Java. The code demonstrates setting up a dataset scanner, converting SQL to Substrait, and executing the query using AceroSubstraitConsumer.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/substrait.rst#2025-04-16_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nimport com.google.common.collect.ImmutableList;\nimport io.substrait.isthmus.SqlToSubstrait;\nimport io.substrait.proto.Plan;\nimport org.apache.arrow.dataset.file.FileFormat;\nimport org.apache.arrow.dataset.file.FileSystemDatasetFactory;\nimport org.apache.arrow.dataset.jni.NativeMemoryPool;\nimport org.apache.arrow.dataset.scanner.ScanOptions;\nimport org.apache.arrow.dataset.scanner.Scanner;\nimport org.apache.arrow.dataset.source.Dataset;\nimport org.apache.arrow.dataset.source.DatasetFactory;\nimport org.apache.arrow.dataset.substrait.AceroSubstraitConsumer;\nimport org.apache.arrow.memory.BufferAllocator;\nimport org.apache.arrow.memory.RootAllocator;\nimport org.apache.arrow.vector.ipc.ArrowReader;\nimport org.apache.calcite.sql.parser.SqlParseException;\n\nimport java.nio.ByteBuffer;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class ClientSubstrait {\n    public static void main(String[] args) {\n        String uri = \"file:///data/tpch_parquet/nation.parquet\";\n        ScanOptions options = new ScanOptions(/*batchSize*/ 32768);\n        try (\n            BufferAllocator allocator = new RootAllocator();\n            DatasetFactory datasetFactory = new FileSystemDatasetFactory(allocator, NativeMemoryPool.getDefault(),\n                    FileFormat.PARQUET, uri);\n            Dataset dataset = datasetFactory.finish();\n            Scanner scanner = dataset.newScan(options);\n            ArrowReader reader = scanner.scanBatches()\n        ) {\n            // map table to reader\n            Map<String, ArrowReader> mapTableToArrowReader = new HashMap<>();\n            mapTableToArrowReader.put(\"NATION\", reader);\n            // get binary plan\n            Plan plan = getPlan();\n            ByteBuffer substraitPlan = ByteBuffer.allocateDirect(plan.toByteArray().length);\n            substraitPlan.put(plan.toByteArray());\n            // run query\n            try (ArrowReader arrowReader = new AceroSubstraitConsumer(allocator).runQuery(\n                    substraitPlan,\n                    mapTableToArrowReader\n            )) {\n                while (arrowReader.loadNextBatch()) {\n                    System.out.println(arrowReader.getVectorSchemaRoot().contentToTSVString());\n                }\n            }\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    static Plan getPlan() throws SqlParseException {\n        String sql = \"SELECT * from nation\";\n        String nation = \"CREATE TABLE NATION (N_NATIONKEY BIGINT NOT NULL, N_NAME CHAR(25), \" +\n                \"N_REGIONKEY BIGINT NOT NULL, N_COMMENT VARCHAR(152))\";\n        SqlToSubstrait sqlToSubstrait = new SqlToSubstrait();\n        Plan plan = sqlToSubstrait.execute(sql, ImmutableList.of(nation));\n        return plan;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Expressions and Conditions in Gandiva (C++)\nDESCRIPTION: Demonstrates how to create an Expression representing 'x + 3' and a Condition representing 'x < 3' using Gandiva's TreeExprBuilder.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gandiva/expr_projector_filter.rst#2025-04-16_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nauto field = field(\"x\", int32());\nauto schema = arrow::schema({field});\n\n// x + 3\nauto node_x = TreeExprBuilder::MakeField(field);\nauto node_3 = TreeExprBuilder::MakeLiteral(3);\nauto add = TreeExprBuilder::MakeFunction(\"add\", {node_x, node_3}, int32());\nauto expr = TreeExprBuilder::MakeExpression(add, field);\n\n// x < 3\nauto node_x_2 = TreeExprBuilder::MakeField(field);\nauto node_3_2 = TreeExprBuilder::MakeLiteral(3);\nauto less_than = TreeExprBuilder::MakeFunction(\"less_than\", {node_x_2, node_3_2}, boolean());\nauto condition = TreeExprBuilder::MakeCondition(less_than);\n```\n\n----------------------------------------\n\nTITLE: Converting datetime.time objects to Arrow time64 arrays in Python\nDESCRIPTION: Demonstrates how to convert a pandas Series containing datetime.time objects to an Arrow array. The conversion creates a time64 type Arrow array which can be converted back to pandas as datetime.time objects.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/pandas.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import time\ns = pd.Series([time(1, 1, 1), time(2, 2, 2)])\ns\n\narr = pa.array(s)\narr.type\narr\n\narr.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Java Implementation for RecordBatchReader Exchange\nDESCRIPTION: Java class that demonstrates how to exchange RecordBatchReader objects between Java and Python using the C Stream Interface. It includes methods to export a Java reader to Python and import a Python reader in Java.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_14\n\nLANGUAGE: java\nCODE:\n```\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\nimport org.apache.arrow.c.ArrowArrayStream;\nimport org.apache.arrow.c.Data;\nimport org.apache.arrow.memory.BufferAllocator;\nimport org.apache.arrow.memory.RootAllocator;\nimport org.apache.arrow.vector.ipc.ArrowFileReader;\nimport org.apache.arrow.vector.ipc.ArrowReader;\nimport org.apache.arrow.vector.ipc.JsonFileWriter;\n\npublic class PythonInteropDemo implements AutoCloseable {\n  private final BufferAllocator allocator;\n\n  public PythonInteropDemo() {\n    this.allocator = new RootAllocator();\n  }\n\n  public void exportStream(String path, long cStreamPointer) throws Exception {\n    try (final ArrowArrayStream stream = ArrowArrayStream.wrap(cStreamPointer)) {\n      ArrowFileReader reader = new ArrowFileReader(Files.newByteChannel(Paths.get(path)), allocator);\n      Data.exportArrayStream(allocator, reader, stream);\n    }\n  }\n\n  public void importStream(String path, long cStreamPointer) throws Exception {\n    try (final ArrowArrayStream stream = ArrowArrayStream.wrap(cStreamPointer);\n         final ArrowReader input = Data.importArrayStream(allocator, stream);\n         JsonFileWriter writer = new JsonFileWriter(new File(path))) {\n      writer.start(input.getVectorSchemaRoot().getSchema(), input);\n      while (input.loadNextBatch()) {\n        writer.write(input.getVectorSchemaRoot());\n      }\n    }\n  }\n\n  @Override\n  public void close() throws Exception {\n    allocator.close();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking Arrow Compute Function by Name in C++\nDESCRIPTION: Example showing how to call an Arrow compute function by name using CallFunction() to add values from an array and scalar input. The result is converted back to an array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<arrow::Array> numbers_array = ...;\nstd::shared_ptr<arrow::Scalar> increment = ...;\narrow::Datum incremented_datum;\n\nARROW_ASSIGN_OR_RAISE(incremented_datum,\n                       arrow::compute::CallFunction(\"add\", {numbers_array, increment}));\nstd::shared_ptr<Array> incremented_array = std::move(incremented_datum).make_array();\n```\n\n----------------------------------------\n\nTITLE: Displaying Dictionary Type with glimpse() in R Arrow\nDESCRIPTION: Shows how glimpse() outputs a Table with a dictionary type column. The function provides a reminder to call print() for full schema details when complex types like dictionaries are present.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_6\n\nLANGUAGE: r\nCODE:\n```\nglimpse(dictionary_but_no_metadata)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Extension Type for Rational Numbers in PyArrow\nDESCRIPTION: Implementation of a custom rational number type by subclassing pa.ExtensionType. The example shows how to define the extension type with storage, serialization, and deserialization methods for fractions represented as pairs of integers.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass RationalType(pa.ExtensionType):\n\n    def __init__(self, data_type: pa.DataType):\n        if not pa.types.is_integer(data_type):\n            raise TypeError(f\"data_type must be an integer type not {data_type}\")\n\n        super().__init__(\n            pa.struct(\n                [\n                    (\"numer\", data_type),\n                    (\"denom\", data_type),\n                ],\n            ),\n            \"my_package.rational\",\n        )\n\n    def __arrow_ext_serialize__(self) -> bytes:\n        # No parameters are necessary\n        return b\"\"\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        # Sanity checks, not required but illustrate the method signature.\n        assert pa.types.is_struct(storage_type)\n        assert pa.types.is_integer(storage_type[0].type)\n        assert storage_type[0].type == storage_type[1].type\n        assert serialized == b\"\"\n\n        # return an instance of this subclass\n        return RationalType(storage_type[0].type)\n```\n\n----------------------------------------\n\nTITLE: Handling nullable integer types conversion between Arrow and pandas\nDESCRIPTION: Shows how Arrow arrays with null values convert to pandas float types by default since standard pandas integer types don't support nulls. The example demonstrates how integer data with nulls gets converted to float in pandas.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/pandas.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> arr = pa.array([1, 2, None])\n>>> arr\n<pyarrow.lib.Int64Array object at 0x7f07d467c640>\n[\n  1,\n  2,\n  null\n]\n>>> arr.to_pandas()\n0    1.0\n1    2.0\n2    NaN\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Column Projection with Expressions\nDESCRIPTION: Shows how to project columns with transformations using expressions to derive new columns.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// Project columns with transformations\nds::ScannerBuilder builder(dataset);\n\n// Build a new table with some operations on the columns\nstd::vector<compute::Expression> exprs{\n    field_ref(\"a\"),\n    call(\"add\", {field_ref(\"a\"), literal(1)}),\n    call(\"multiply\", {field_ref(\"b\"), literal(2)})\n};\nstd::vector<std::string> names{\"a\", \"a_plus_one\", \"b_times_2\"};\nFLIGHT_THROW_IF_NOT_OK(builder.Project(exprs, names));\n\nauto scanner = builder.Finish().ValueOrDie();\nauto table = scanner->ToTable().ValueOrDie();\n```\n\n----------------------------------------\n\nTITLE: Converting NumPy ndarrays to Fixed Shape Tensor Arrays in PyArrow\nDESCRIPTION: Shows how to convert a NumPy ndarray to a PyArrow FixedShapeTensorArray. During conversion, the first dimension of the ndarray becomes the length of the resulting PyArrow extension array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> pa.FixedShapeTensorArray.from_numpy_ndarray(numpy_tensor)\n<pyarrow.lib.FixedShapeTensorArray object at ...>\n[\n  [\n    1,\n    2,\n    3,\n    4\n  ],\n  [\n    10,\n    20,\n    30,\n    40\n  ],\n  [\n    100,\n    200,\n    300,\n    400\n  ]\n]\n```\n\n----------------------------------------\n\nTITLE: Updating with Prepared Statements: CommandPreparedStatementUpdate\nDESCRIPTION: CommandPreparedStatementUpdate executes a prepared statement that does not return results. It returns the number of affected rows, aiding in operations that alter data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_12\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandPreparedStatementUpdate\"\n```\n\n----------------------------------------\n\nTITLE: Using rpy2-arrow to Pass PyArrow Array to R Function\nDESCRIPTION: This Python code demonstrates how to use rpy2-arrow to pass a PyArrow array to an R function and convert the result back to a PyArrow array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_r.rst#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport rpy2.robjects as robjects\nfrom rpy2_arrow.pyarrow_rarrow import (rarrow_to_py_array,\n                                           converter as arrowconverter)\nfrom rpy2.robjects.conversion import localconverter\n\nr_source = robjects.r[\"source\"]\nr_source(\"addthree.R\")\n\naddthree = robjects.r[\"addthree\"]\n\nimport pyarrow\n\narray = pyarrow.array((1, 2, 3))\n\n# Enable rpy2-arrow converter so that R can receive the array.\nwith localconverter(arrowconverter):\n    r_result = addthree(array)\n\n# The result of the R function will be an R Environment\n# we can convert the Environment back to a pyarrow Array\n# using the rarrow_to_py_array function\npy_result = rarrow_to_py_array(r_result)\nprint(\"RESULT\", type(py_result), py_result)\n```\n\n----------------------------------------\n\nTITLE: Row Batch Builder Implementation\nDESCRIPTION: Class that builds rows from Arrow arrays using type traits and visitor patterns. Handles primitive C types and complex Arrow types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/row_columnar_conversion.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nclass RowBatchBuilder {\n // ... implementation details\n};\n```\n\n----------------------------------------\n\nTITLE: Copying Numba-sourced CUDA Buffer to Host Memory\nDESCRIPTION: Shows how to copy a CUDA buffer created from a Numba device array back to host memory and verify its contents.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/cuda.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> np.frombuffer(cuda_buf.copy_to_host(), dtype=np.int32)\narray([10, 11, 12, 13], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Using DirectReservationListener for Memory Management in Apache Arrow Java\nDESCRIPTION: This snippet demonstrates how to use DirectReservationListener with NativeMemoryPool in Apache Arrow's Java API for memory management of Arrow buffer allocation. It ensures memory limits are respected and throws an OutOfMemoryError if direct buffer memory is exceeded, using a NativeMemoryPool configured with DirectReservationListener.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/dataset.rst#2025-04-16_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nNativeMemoryPool pool = NativeMemoryPool.createListenable(\n    DirectReservationListener.instance());\n\n```\n\n----------------------------------------\n\nTITLE: Basic Red Arrow Usage Example\nDESCRIPTION: Simple code example showing how to load an Arrow file into a table, process the data, and save it back to a new file using Red Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow/README.md#2025-04-16_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"arrow\"\n\ntable = Arrow::Table.load(\"/dev/shm/data.arrow\")\n# Process data in table\ntable.save(\"/dev/shm/data-processed.arrow\")\n```\n\n----------------------------------------\n\nTITLE: Creating Map Arrays with Explicit Type in PyArrow\nDESCRIPTION: Demonstrates creating map arrays from lists of key-item pairs, requiring explicit type specification. Map arrays represent key-value mappings with a specific key and item type.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndata = [[('x', 1), ('y', 0)], [('a', 2), ('b', 45)]]\nty = pa.map_(pa.string(), pa.int64())\npa.array(data, type=ty)\n```\n\n----------------------------------------\n\nTITLE: Using types_mapper to control pandas dtypes when converting from Arrow\nDESCRIPTION: Demonstrates how to use the types_mapper parameter to specify which pandas dtypes should be used when converting Arrow data to pandas. This example shows converting Arrow int64 with nulls to pandas' Int64Dtype instead of float64.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/pandas.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> table = pa.table({\"a\": [1, 2, None]})\n>>> table.to_pandas()\n     a\n0  1.0\n1  2.0\n2  NaN\n>>> table.to_pandas(types_mapper={pa.int64(): pd.Int64Dtype()}.get)\n      a\n0     1\n1     2\n2  <NA>\n```\n\n----------------------------------------\n\nTITLE: Setting Call Timeout for Flight Operations in Python\nDESCRIPTION: Configures a timeout for a Flight DoAction call in Python. This example sets a 200ms timeout, after which the client will cancel the operation if it hasn't completed.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/flight.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\noptions = pyarrow.flight.FlightCallOptions(timeout=0.2)\nresult = client.do_action(action, options=options)\n```\n\n----------------------------------------\n\nTITLE: Bit Endianness Example\nDESCRIPTION: This example demonstrates the least-significant bit (LSB) numbering (bit-endianness) used in Arrow. Within a group of 8 bits, we read right-to-left.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"values = [0, 1, null, 2, null, 3]\n\n    bitmap\n    j mod 8   7  6  5  4  3  2  1  0\n              0  0  1  0  1  0  1  1\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Pandas Integration for Custom Extension Types\nDESCRIPTION: This code demonstrates the required pandas side implementation for converting Arrow extension arrays to pandas extension arrays.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass MyExtensionDtype(pd.api.extensions.ExtensionDtype):\n    ...\n\n    def __from_arrow__(self, array: pyarrow.Array/ChunkedArray) -> pandas.ExtensionArray:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Opening a Parquet File with FileReader in Arrow C++\nDESCRIPTION: Initializes a Parquet FileReader with an already opened file handle. This step prepares the reader to access the contents of the Parquet file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n  PARQUET_THROW_NOT_OK(\n      parquet::arrow::OpenFile(infile, pool, &reader));\n```\n\n----------------------------------------\n\nTITLE: Using Python with C Stream Interface for RecordBatchReader Exchange\nDESCRIPTION: This Python script demonstrates how to create a RecordBatchReader in Python, export it via the C Stream Interface, and pass it to Java for processing. It shows how to use the C Data Interface to exchange record batch streams between languages.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\n\nimport jpype\nimport jpype.imports\nfrom jpype.types import *\n\n# Init the JVM and make demo class available to Python.\njpype.startJVM(classpath=[\"./dependencies/*\", \"./target/*\"])\nPythonInteropDemo = JClass(\"PythonInteropDemo\")\ndemo = PythonInteropDemo()\n\n# Create a Python record batch reader\nimport pyarrow as pa\nschema = pa.schema([\n    (\"ints\", pa.int64()),\n    (\"strs\", pa.string())\n])\nbatches = [\n    pa.record_batch([\n        [0, 2, 4, 8],\n        [\"a\", \"b\", \"c\", None],\n    ], schema=schema),\n    pa.record_batch([\n        [None, 32, 64, None],\n        [\"e\", None, None, \"h\"],\n    ], schema=schema),\n]\nreader = pa.RecordBatchReader.from_batches(schema, batches)\n\nfrom pyarrow.cffi import ffi as arrow_c\n\n# Export the Python reader through C Data\nc_stream = arrow_c.new(\"struct ArrowArrayStream*\")\nc_stream_ptr = int(arrow_c.cast(\"uintptr_t\", c_stream))\nreader._export_to_c(c_stream_ptr)\n\n# Send reader to the Java function that writes a JSON file\nwith tempfile.NamedTemporaryFile() as temp:\n    demo.importStream(temp.name, c_stream_ptr)\n\n    # Read the JSON file back\n    with open(temp.name) as source:\n        print(\"JSON file written by Java:\")\n        print(source.read())\n\n\n# Write an Arrow IPC file for Java to read\n```\n\n----------------------------------------\n\nTITLE: String Transformation Functions\nDESCRIPTION: Unary and binary functions for modifying string and binary data, including case changes, reversing, repeating, and substring replacements\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_5\n\nLANGUAGE: Apache Arrow\nCODE:\n```\nascii_capitalize\nascii_lower\nascii_reverse\nascii_swapcase\nascii_title\nascii_upper\nbinary_length\nbinary_repeat\nbinary_replace_slice\nbinary_reverse\nreplace_substring\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple Files from a Directory\nDESCRIPTION: Demonstrates how to load and combine multiple files from a local directory into a single Arrow table using the arrow-dataset functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_6\n\nLANGUAGE: ruby\nCODE:\n```\nrequire 'arrow-dataset'\n\nArrow::Table.load(URI(\"file:///your/folder/\"), format: :parquet)\n```\n\n----------------------------------------\n\nTITLE: Writing a CSV File from a Table in Arrow C++\nDESCRIPTION: Demonstrates how to write a CSV file from an Arrow Table object using an IPC RecordBatchWriter with the WriteTable method. The example shows setting up the Schema and writing the Table to the file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n  std::shared_ptr<io::FileOutputStream> outfile;\n  PARQUET_ASSIGN_OR_THROW(outfile, io::FileOutputStream::Open(\"simple_table.csv\"));\n  PARQUET_THROW_NOT_OK(arrow::csv::WriteCSV(*table, arrow::csv::WriteOptions::Defaults(),\n                                            outfile.get()));\n```\n\n----------------------------------------\n\nTITLE: Building PyArrow from Source on Linux\nDESCRIPTION: Commands for building PyArrow Python extension with parallel compilation enabled and installing in-place for development.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n$ pushd arrow/python\n$ export PYARROW_PARALLEL=4\n$ python setup.py build_ext --inplace\n$ popd\n```\n\n----------------------------------------\n\nTITLE: Creating FileSelector for Dataset Traversal in Arrow C++\nDESCRIPTION: This code creates and configures a FileSelector object to traverse the dataset directory. It sets the base directory and enables recursive traversal.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\narrow::fs::FileSelector selector;\n\nselector.base_dir = \"parquet_dataset\";\nselector.recursive = true;\n```\n\n----------------------------------------\n\nTITLE: Examining Shapes During NumPy to Fixed Shape Tensor Array Conversion\nDESCRIPTION: Demonstrates the shape transformation during conversion between NumPy ndarray and PyArrow FixedShapeTensorArray. An ndarray of shape (3, 2, 2) becomes an arrow array of length 3 with tensor elements of shape (2, 2).\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# ndarray of shape (3, 2, 2)\n>>> numpy_tensor.shape\n(3, 2, 2)\n\n# arrow array of length 3 with tensor elements of shape (2, 2)\n>>> pyarrow_tensor_array = pa.FixedShapeTensorArray.from_numpy_ndarray(numpy_tensor)\n>>> len(pyarrow_tensor_array)\n3\n>>> pyarrow_tensor_array.type.shape\n[2, 2]\n```\n\n----------------------------------------\n\nTITLE: Closing Session: CloseSession\nDESCRIPTION: CloseSession invalidates the current session context, ensuring that any resources or states associated with the session are properly released. This command helps maintain database integrity.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_18\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CloseSession\"\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Tables from Ruby Hash\nDESCRIPTION: Demonstrates how to create an Arrow table from a Ruby hash where column types are automatically detected based on the provided data.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\ntable = Arrow::Table.new('name' => ['Tom', 'Max'], 'age' => [22, 23])\n```\n\n----------------------------------------\n\nTITLE: Setting up Cython Extension for PyArrow\nDESCRIPTION: A setup.py file for building a Cython extension that interfaces with PyArrow's C++ API. It includes proper configuration for include directories, libraries, and compiler flags required to build C++ extensions that work with PyArrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/extending.rst#2025-04-16_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom setuptools import setup\nfrom Cython.Build import cythonize\n\nimport os\nimport numpy as np\nimport pyarrow as pa\n\n\next_modules = cythonize(\"example.pyx\")\n\nfor ext in ext_modules:\n    # The Numpy C headers are currently required\n    ext.include_dirs.append(np.get_include())\n    ext.include_dirs.append(pa.get_include())\n    ext.libraries.extend(pa.get_libraries())\n    ext.library_dirs.extend(pa.get_library_dirs())\n\n    if os.name == 'posix':\n        ext.extra_compile_args.append('-std=c++17')\n\nsetup(ext_modules=ext_modules)\n```\n\n----------------------------------------\n\nTITLE: Creating a Numba Device Array from CUDA Buffer\nDESCRIPTION: Demonstrates how to wrap a PyArrow CUDA buffer into a Numba device array with appropriate metadata (shape, strides, dtype) to make it usable in Numba CUDA kernels.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/cuda.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from numba.cuda.cudadrv.devicearray import DeviceNDArray\n>>> device_arr = DeviceNDArray(arr.shape, arr.strides, arr.dtype, gpu_data=cuda_buf.to_numba())\n```\n\n----------------------------------------\n\nTITLE: Creating a KMS Client Factory for Parquet Encryption in PyArrow\nDESCRIPTION: Shows how to implement a factory function for creating KMS client instances and using it to initialize a CryptoFactory. This factory will be used to generate encryption and decryption properties.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef kms_client_factory(kms_connection_configuration):\n   return MyKmsClient(kms_connection_configuration)\n\ncrypto_factory = CryptoFactory(kms_client_factory)\n```\n\n----------------------------------------\n\nTITLE: Installing PyArrow with Flight RPC Support via Conda\nDESCRIPTION: Command to install PyArrow with additional Flight RPC support using conda\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/install.rst#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge pyarrow libarrow-flight\n```\n\n----------------------------------------\n\nTITLE: Configuring gRPC Options in Flight Client\nDESCRIPTION: Examples showing how to configure gRPC options for Flight clients in both C++ and Python, specifically setting keepalive ping time.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/flight.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nauto options = FlightClientOptions::Defaults();\n// Set the period after which a keepalive ping is sent on transport.\noptions.generic_options.emplace_back(GRPC_ARG_KEEPALIVE_TIME_MS, 60000);\n```\n\nLANGUAGE: python\nCODE:\n```\n# Set the period after which a keepalive ping is sent on transport.\ngeneric_options = [(\"GRPC_ARG_KEEPALIVE_TIME_MS\", 60000)]\nclient = pyarrow.flight.FlightClient(server_uri, generic_options=generic_options)\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow Package from CRAN in R\nDESCRIPTION: Basic installation command to get the latest release of arrow package from CRAN repository.\nSOURCE: https://github.com/apache/arrow/blob/main/r/README.md#2025-04-16_snippet_0\n\nLANGUAGE: r\nCODE:\n```\ninstall.packages(\"arrow\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Filter Node Example in C++\nDESCRIPTION: Example demonstrating how to use filter node to select rows based on a boolean condition.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/user_guide.rst#2025-04-16_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nARROW_RETURN_NOT_OK(Declaration::Sequence({\n    {\"table_source\", TableSourceNodeOptions{table, max_batch_size}},\n    {\"filter\", FilterNodeOptions{greater(field_ref(\"b\"), literal(3))}},\n    {\"sink\", SinkNodeOptions{table->schema()}},\n}).AddToPlan(plan.get()));\n```\n\n----------------------------------------\n\nTITLE: Writing Arrow Random Access Files in Java\nDESCRIPTION: This snippet demonstrates writing Arrow data to a random access file using `ArrowFileWriter` in Java. It creates an `ArrowFileWriter`, starts the writing process, writes multiple batches of data, and then ends the writing process. The API is very similar to the stream writer, but produces a file with a specific format suitable for random access.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/ipc.rst#2025-04-16_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\ntry (\n  ByteArrayOutputStream out = new ByteArrayOutputStream();\n  ArrowFileWriter writer = new ArrowFileWriter(root, /*DictionaryProvider=*/null, Channels.newChannel(out));\n) {\n  writer.start();\n  // write the first batch\n  writer.writeBatch();\n  // write another four batches.\n  for (int i = 0; i < 4; i++) {\n    // ... do populate work\n    writer.writeBatch();\n  }\n  writer.end();\n}\n```\n\n----------------------------------------\n\nTITLE: Using PyArrow OSFile and MemoryMappedFile\nDESCRIPTION: Shows file operations using PyArrow's native file interfaces including memory mapping\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/memory.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith pa.OSFile('example3.dat', 'wb') as f:\n    f.write(b'some example data')\n\nfile_obj = pa.OSFile('example2.dat')\nmmap = pa.memory_map('example3.dat')\nfile_obj.read(4)\nmmap.read(4)\n```\n\n----------------------------------------\n\nTITLE: Creating and Allocating IntVector in Java\nDESCRIPTION: This snippet demonstrates how to create an IntVector and allocate memory for it with a specific capacity.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nRootAllocator allocator = new RootAllocator(Long.MAX_VALUE);\n...\nIntVector vector = new IntVector(\"int vector\", allocator);\nvector.allocateNew(10);\n```\n\n----------------------------------------\n\nTITLE: Writing Arrow Streaming Format in Java\nDESCRIPTION: This code snippet shows how to write a stream of record batches using `ArrowStreamWriter` in Java. It initializes an `ArrowStreamWriter` with a `VectorSchemaRoot` and writes batches of data to an output stream.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/ipc.rst#2025-04-16_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\ntry (\n  ByteArrayOutputStream out = new ByteArrayOutputStream();\n  ArrowStreamWriter writer = new ArrowStreamWriter(root, /*DictionaryProvider=*/null, Channels.newChannel(out));\n) {\n  // ... do write into the ArrowStreamWriter\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Arrow Data using PyCapsule Interface in Python\nDESCRIPTION: This Python code demonstrates how to import Arrow data using the PyCapsule interface, preferring it over the older `_export_to_c` method for backward compatibility. It checks if the input object has the `__arrow_c_array__` method, and if so, uses it to import the data.  If not, it falls back to the deprecated `_export_to_c` method if the input is a PyArrow array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# NEW METHOD\ndef from_arrow(arr)\n    # Newer versions of PyArrow as well as other libraries with Arrow data\n    # implement this method, so prefer it over _export_to_c.\n    if hasattr(arr, \"__arrow_c_array__\"):\n         schema_ptr, array_ptr = arr.__arrow_c_array__()\n         return import_c_capsule_data(schema_ptr, array_ptr)\n    elif isinstance(arr, pa.Array):\n         # Deprecated method, used for older versions of PyArrow\n         array_import_ptr = make_array_import_ptr()\n         schema_import_ptr = make_schema_import_ptr()\n         arr._export_to_c(array_import_ptr, schema_import_ptr)\n         return import_c_data(array_import_ptr, schema_import_ptr)\n    else:\n        raise TypeError(f\"Cannot import {type(arr)} as Arrow array data.\")\n```\n\n----------------------------------------\n\nTITLE: Representing List<List<Int8>> Array in Arrow Format\nDESCRIPTION: Demonstrates the memory layout for a nested List<List<Int8>> array [[[1, 2], [3, 4]], [[5, 6, 7], null, [8]], [[9, 10]]] in Arrow format, including offsets buffer and nested child arrays.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n* Length 3\n* Nulls count: 0\n* Validity bitmap buffer: Not required\n* Offsets buffer (int32)\n\n  | Bytes 0-3  | Bytes 4-7  | Bytes 8-11 | Bytes 12-15 | Bytes 16-63           |\n  |------------|------------|------------|-------------|-----------------------|\n  | 0          |  2         |  5         |  6          | unspecified (padding) |\n\n* Values array (`List<Int8>`)\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Filesystem Factory in C++\nDESCRIPTION: Example of registering a new filesystem implementation with Arrow's filesystem factory system. Shows how to create an automatic registration using FileSystemRegistrar that handles initialization and cleanup.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/io.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nauto kExampleFileSystemModule = ARROW_REGISTER_FILESYSTEM(\n    \"example\",\n    [](const Uri& uri, const io::IOContext& io_context,\n        std::string* out_path) -> Result<std::shared_ptr<arrow::fs::FileSystem>> {\n      EnsureExampleFileSystemInitialized();\n      return std::make_shared<ExampleFileSystem>();\n    },\n    &EnsureExampleFileSystemFinalized\n);\n```\n\n----------------------------------------\n\nTITLE: Invoking R Function from Python using rpy2\nDESCRIPTION: This Python code uses rpy2 to load and execute the 'addthree' R function defined in 'addthree.R'.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_r.rst#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport rpy2.robjects as robjects\n\n# Load the addthree.R file\nr_source = robjects.r[\"source\"]\nr_source(\"addthree.R\")\n\n# Get a reference to the addthree function\naddthree = robjects.r[\"addthree\"]\n\n# Invoke the function\nr = addthree(3)\n\n# Access the returned value\nvalue = r[0]\nprint(value)\n```\n\n----------------------------------------\n\nTITLE: Getting Table Types with CommandGetTableTypes\nDESCRIPTION: CommandGetTableTypes is used to list the different types of tables within the database. The list of table types may vary by vendor, providing insights into the database's structure.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_8\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandGetTableTypes\"\n```\n\n----------------------------------------\n\nTITLE: Grouping and Aggregating Arrow Table Data\nDESCRIPTION: Demonstrates grouping table data by a column and then applying an aggregation function (sum) to another column, similar to SQL GROUP BY operations.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_10\n\nLANGUAGE: ruby\nCODE:\n```\ntable = Arrow::Table.new(\n  'name' => ['Tom', 'Max', 'Kate', 'Tom'],\n  'amount' => [10, 2, 3, 5]\n)\ntable.group('name').sum('amount')\n# => #<Arrow::Table:0x7fa389894ae8 ptr=0x7fa364141a50>\n#   name\tamount\n# 0\tKate\t     3\n# 1\tMax \t     2\n# 2\tTom \t    15\n```\n\n----------------------------------------\n\nTITLE: Creating FileSystemDatasetFactory for Dataset Reading in Arrow C++\nDESCRIPTION: This code creates a FileSystemDatasetFactory using the configured FileSystem, FileSelector, options, and file format. It's used to build the Dataset object.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nARROW_ASSIGN_OR_RAISE(\n    auto factory,\n    arrow::dataset::FileSystemDatasetFactory::Make(\n        fs, selector, format, options));\n```\n\n----------------------------------------\n\nTITLE: Creating a Parquet FileReader in Arrow C++\nDESCRIPTION: Creates a Parquet FileReader object that will be used to read data from a Parquet file. The FileReader provides the interface to access Parquet file contents.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n  std::unique_ptr<parquet::arrow::FileReader> reader;\n```\n\n----------------------------------------\n\nTITLE: Creating Project Declaration Example in C++\nDESCRIPTION: Demonstrates how to create a source declaration and project declaration using input tables in Acero.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/user_guide.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n    auto input_table = GetInputTable();\n    auto source = Declaration::Sequence({\n        {\"table_source\",\n         TableSourceNodeOptions{input_table}}\n    });\n    auto project = Declaration::Sequence({\n        {\"project\",\n         source,\n         ProjectNodeOptions{/*expressions=*/{field_ref(\"a\")}}}\n    });\n```\n\n----------------------------------------\n\nTITLE: Creating Example Dataset with Parquet Files\nDESCRIPTION: Creates a sample dataset by writing two Parquet files containing tabular data with 'a' and 'b' columns.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nauto schema = arrow::schema({field(\"a\", int64()), field(\"b\", int64())});\n\nauto table1 = TableFromJSON(schema, {{\"a\", \"[1, 2, 3]\"}, {\"b\", \"[10, 20, 30]\"}});\nauto table2 = TableFromJSON(schema, {{\"a\", \"[4, 5, 6]\"}, {\"b\", \"[40, 50, 60]\"}});\n\nstd::string dir_path = \"/dataset\";\nstd::string file1 = dir_path + \"/data1.parquet\";\nstd::string file2 = dir_path + \"/data2.parquet\";\n\nFLIGHT_THROW_IF_NOT_OK(WriteTable(*table1, default_memory_pool(), \n                                 ParquetFileWriter::Open(file1)));\nFLIGHT_THROW_IF_NOT_OK(WriteTable(*table2, default_memory_pool(), \n                                 ParquetFileWriter::Open(file2)));\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Virtual Environment for PyArrow\nDESCRIPTION: Creates and activates a Python virtual environment for PyArrow development, then installs build dependencies from the requirements file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n$ python3 -m venv pyarrow-dev\n$ source ./pyarrow-dev/bin/activate\n$ pip install -r arrow/python/requirements-build.txt\n\n$ # This is the folder where we will install the Arrow libraries during\n$ # development\n$ mkdir dist\n```\n\n----------------------------------------\n\nTITLE: Creating a Scanner for Writing a Dataset in C++\nDESCRIPTION: This snippet builds a Scanner from a TableBatchReader using a ScannerBuilder. The Scanner will be used to read data from the Table in preparation for writing it to disk as a Dataset.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n// Make our ScannerBuilder\nauto write_scanner_builder_result = ScannerBuilder::FromRecordBatchReader(&table_batch_reader);\nif (!write_scanner_builder_result.ok()) {\n  return write_scanner_builder_result.status();\n}\nstd::shared_ptr<ScannerBuilder> write_scanner_builder = write_scanner_builder_result.ValueOrDie();\n\n// Make our Scanner\nauto write_scanner_result = write_scanner_builder->Finish();\nif (!write_scanner_result.ok()) {\n  return write_scanner_result.status();\n}\nstd::shared_ptr<Scanner> write_scanner = write_scanner_result.ValueOrDie();\n```\n\n----------------------------------------\n\nTITLE: Printing Addition Result from Arrow Datum in C++\nDESCRIPTION: Extracts and prints the ChunkedArray containing the element-wise addition results.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\nstd::cout << add_result.chunked_array()->ToString() << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Memory-Mapped Arrow File Reading\nDESCRIPTION: Demonstrates efficient reading of Arrow files using memory mapping to minimize memory consumption.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/ipc.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith pa.memory_map('bigfile.arrow', 'rb') as source:\n   loaded_array = pa.ipc.open_file(source).read_all()\nprint(\"LEN:\", len(loaded_array))\nprint(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n```\n\n----------------------------------------\n\nTITLE: Using C Data Interface to Modify Python Array from Java\nDESCRIPTION: This Python script demonstrates how to create a PyArrow array, export it through the C Data Interface, and pass the pointers to Java to modify the array contents without copying data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport jpype\nimport jpype.imports\nfrom jpype.types import *\n\n# Init the JVM and make FillTen class available to Python.\njpype.startJVM(classpath=[\"./dependencies/*\", \"./target/*\"])\nFillTen = JClass('FillTen')\n\n# Create a Python array of 10 elements\nimport pyarrow as pa\narray = pa.array([0]*10)\n\nfrom pyarrow.cffi import ffi as arrow_c\n\n# Export the Python array through C Data\nc_array = arrow_c.new(\"struct ArrowArray*\")\nc_array_ptr = int(arrow_c.cast(\"uintptr_t\", c_array))\narray._export_to_c(c_array_ptr)\n\n# Export the Schema of the Array through C Data\nc_schema = arrow_c.new(\"struct ArrowSchema*\")\nc_schema_ptr = int(arrow_c.cast(\"uintptr_t\", c_schema))\narray.type._export_to_c(c_schema_ptr)\n\n# Send Array and its Schema to the Java function\n# that will populate the array with numbers from 1 to 10\nFillTen.fillCArray(c_array_ptr, c_schema_ptr)\n\n# See how the content of our Python array was changed from Java\n# while it remained of the Python type.\nprint(\"ARRAY\", type(array), array)\n```\n\n----------------------------------------\n\nTITLE: Configuring Encryption for Nested Fields in Parquet Files with PyArrow\nDESCRIPTION: Demonstrates how to create an encryption configuration for columns with nested data types like lists, maps, and structs. This requires specifying the inner fields rather than the outer column names.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.parquet.encryption as pe\n\nschema = pa.schema([\n  (\"ListColumn\", pa.list_(pa.int32())),\n  (\"MapColumn\", pa.map_(pa.string(), pa.int32())),\n  (\"StructColumn\", pa.struct([(\"f1\", pa.int32()), (\"f2\", pa.string())])),\n])\n\nencryption_config = pe.EncryptionConfiguration(\n   footer_key=\"footer_key_name\",\n   column_keys={\n      \"column_key_id\": [\n        \"ListColumn.list.element\",\n        \"MapColumn.key_value.key\", \"MapColumn.key_value.value\",\n        \"StructColumn.f1\", \"StructColumn.f2\"\n      ],\n   },\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing ScannerBuilder for Dataset Reading in Arrow C++\nDESCRIPTION: This code creates a ScannerBuilder from the Dataset object. The ScannerBuilder is used to configure and create a Scanner for reading the dataset.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nARROW_ASSIGN_OR_RAISE(auto builder, dataset->NewScan());\n```\n\n----------------------------------------\n\nTITLE: Verifying Numba CUDA Kernel Results\nDESCRIPTION: Copies the modified GPU data back to host memory and verifies that the kernel correctly incremented each element by one.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/cuda.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> np.frombuffer(cuda_buf.copy_to_host(), dtype=np.int32)\narray([1, 2, 3, 4], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Wrapping and Unwrapping Arrow Objects in Cython Example\nDESCRIPTION: A sample Cython module demonstrating how to unwrap a Python PyArrow object to access the underlying C++ object's API, specifically retrieving an array's length. The example shows proper type checking and error handling.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/extending.rst#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# distutils: language=c++\n\nfrom pyarrow.lib cimport *\n\n\ndef get_array_length(obj):\n    # Just an example function accessing both the pyarrow Cython API\n    # and the Arrow C++ API\n    cdef shared_ptr[CArray] arr = pyarrow_unwrap_array(obj)\n    if arr.get() == NULL:\n        raise TypeError(\"not an array\")\n    return arr.get().length()\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Extension Array Class for 3D Points\nDESCRIPTION: This example defines a custom extension array class for 3D points with a method to convert the data to a NumPy array without copying.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Point3DArray(pa.ExtensionArray):\n    def to_numpy_array(self):\n        return self.storage.flatten().to_numpy().reshape((-1, 3))\n\n\nclass Point3DType(pa.ExtensionType):\n    def __init__(self):\n        super().__init__(pa.list_(pa.float32(), 3), \"my_package.Point3DType\")\n\n    def __arrow_ext_serialize__(self):\n        return b\"\"\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        return Point3DType()\n\n    def __arrow_ext_class__(self):\n        return Point3DArray\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow Package from R-universe\nDESCRIPTION: Alternative installation method using R-universe which provides pre-compiled binaries for common operating systems.\nSOURCE: https://github.com/apache/arrow/blob/main/r/README.md#2025-04-16_snippet_1\n\nLANGUAGE: r\nCODE:\n```\ninstall.packages(\"arrow\", repos = c(\"https://apache.r-universe.dev\", \"https://cloud.r-project.org\"))\n```\n\n----------------------------------------\n\nTITLE: Updating Ad-Hoc Queries: CommandStatementUpdate\nDESCRIPTION: CommandStatementUpdate manages the execution of an ad-hoc query that does not return results, and indicates the number of affected rows resulting from the operation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_14\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandStatementUpdate\"\n```\n\n----------------------------------------\n\nTITLE: Implementing a Global Allocator Pattern in Apache Arrow\nDESCRIPTION: This snippet demonstrates how to implement a global allocator pattern in Apache Arrow when explicitly passing allocators through application layers is difficult. It shows a singleton approach with child allocator creation, proper naming, and resource cleanup validation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/memory.rst#2025-04-16_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n//1\nprivate static final BufferAllocator allocator = new RootAllocator();\nprivate static final AtomicInteger childNumber = new AtomicInteger(0);\n...\n//2\npublic static BufferAllocator getChildAllocator() {\n    return allocator.newChildAllocator(nextChildName(), 0, Long.MAX_VALUE);\n}\n...\n//3\nprivate static String nextChildName() {\n    return \"Allocator-Child-\" + childNumber.incrementAndGet();\n}\n...\n//4: Business code\ntry (BufferAllocator allocator = GlobalAllocator.getChildAllocator()) {\n    ...\n}\n...\n//5\npublic static void checkGlobalCleanUpResources() {\n    ...\n    if (!allocator.getChildAllocators().isEmpty()) {\n      throw new IllegalStateException(...);\n    } else if (allocator.getAllocatedMemory() != 0) {\n      throw new IllegalStateException(...);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow Binary-like Data Types in C++\nDESCRIPTION: This snippet documents the binary-like data types in Arrow, such as BinaryType, StringType, FixedSizeBinaryType, etc.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygengroup:: binary-datatypes\n   :content-only:\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Reading a Parquet File to an Arrow Table\nDESCRIPTION: Reads the entire Parquet file content into an Arrow Table structure. The Table is passed by reference and populated with the file data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n  std::shared_ptr<Table> table;\n  PARQUET_THROW_NOT_OK(reader->ReadTable(&table));\n```\n\n----------------------------------------\n\nTITLE: Implementing ExecNode Lifecycle Methods in C++\nDESCRIPTION: Shows the implementation of key lifecycle methods for ExecNodes, including StartProducing, InputReceived, InputFinished, and StopProducing. Highlights the importance of thread-safety and proper resource management.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/developer_guide.rst#2025-04-16_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nclass CustomExecNode : public ExecNode {\n public:\n  Status StartProducing() override {\n    // Initialization when the plan starts\n    return Status::OK();\n  }\n\n  Status InputReceived(ExecNode* input, ExecBatch batch) override {\n    // Process incoming data\n    // Must be thread-safe\n    return Status::OK();\n  }\n\n  Status InputFinished(ExecNode* input, int total_batches) override {\n    // Handle completion of input\n    return Status::OK();\n  }\n\n  void StopProducing() override {\n    // Clean up resources, handle cancellation\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Reading Selective Columns from ORC File\nDESCRIPTION: Demonstrates how to read specific columns from an ORC file, which can improve performance by only loading required data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/orc.rst#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\norc.read_table('example.orc', columns=['one', 'three'])\n```\n\n----------------------------------------\n\nTITLE: Registering External IR Functions from Bitcode File in Gandiva\nDESCRIPTION: API for registering a set of external IR functions from a specified bitcode file into the Gandiva function registry, requiring function metadata and the path to the bitcode file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gandiva/external_func.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// Registers a set of functions from a specified bitcode file\narrow::Status Register(const std::vector<NativeFunction>& funcs,\n                     const std::string& bitcode_path);\n```\n\n----------------------------------------\n\nTITLE: Exporting ArrowSchema with PyCapsule in Python\nDESCRIPTION: Defines the method for exporting an object as an ArrowSchema encapsulated in a PyCapsule. When implemented, the method returns a PyCapsule named 'arrow_schema'. This standardizes the export of schema-related data structures, increasing compatibility across libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_0\n\nLANGUAGE: py\nCODE:\n```\n\"\"\"\n    Export the object as an ArrowSchema.\n\n    :return: A PyCapsule containing a C ArrowSchema representation of the\n        object. The capsule must have a name of \\\"arrow_schema\\\".\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Writing Encrypted Parquet Files in PyArrow\nDESCRIPTION: Demonstrates how to write a Parquet file with columnar encryption enabled. This requires creating encryption properties using a CryptoFactory with KMS connection configuration and applying them when creating a ParquetWriter.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nencryption_properties = crypto_factory.file_encryption_properties(\n                                 kms_connection_config, encryption_config)\nwith pq.ParquetWriter(filename, schema,\n                     encryption_properties=encryption_properties) as writer:\n   writer.write_table(table)\n```\n\n----------------------------------------\n\nTITLE: Illustrating Memory Perspective Hierarchy in Apache Arrow Java\nDESCRIPTION: This code block presents a hierarchical structure showing the relationship between AllocationManager, BufferLedgers, Allocators, and ArrowBufs from a memory-centric perspective. It demonstrates how multiple BufferLedgers and ArrowBufs can be associated with a single AllocationManager.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/memory.rst#2025-04-16_snippet_6\n\nLANGUAGE: none\nCODE:\n```\n+ AllocationManager\n|\n|-- UnsignedDirectLittleEndian (One per AllocationManager)\n|\n|-+ BufferLedger 1 ==> Allocator A (owning)\n| ` - ArrowBuf 1\n|-+ BufferLedger 2 ==> Allocator B (non-owning)\n| ` - ArrowBuf 2\n|-+ BufferLedger 3 ==> Allocator C (non-owning)\n  | - ArrowBuf 3\n  | - ArrowBuf 4\n  ` - ArrowBuf 5\n```\n\n----------------------------------------\n\nTITLE: Managing Dictionaries with DictionaryProvider in Java Arrow\nDESCRIPTION: This code shows how to use a DictionaryProvider to manage multiple dictionaries, which is particularly useful when working with VectorSchemaRoot containing dictionary-encoded vectors.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nDictionaryProvider.MapDictionaryProvider provider =\n    new DictionaryProvider.MapDictionaryProvider();\n\nprovider.put(dictionary);\n```\n\n----------------------------------------\n\nTITLE: Implementing device array export with error handling in Python\nDESCRIPTION: This Python code snippet demonstrates how to implement the `__arrow_c_device_array__` method, which is used to export Arrow data structures that might reside on a non-CPU device.  It includes error handling for unsupported keyword arguments. The method accepts a `requested_schema` and arbitrary keyword arguments.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef __arrow_c_device_array__(self, requested_schema=None, **kwargs):\n\n    non_default_kwargs = [\n        name for name, value in kwargs.items() if value is not None\n    ]\n    if non_default_kwargs:\n        raise NotImplementedError(\n            f\"Received unsupported keyword argument(s): {non_default_kwargs}\"\n        )\n\n    ...\n```\n\n----------------------------------------\n\nTITLE: Fixed Shape Tensor Specification - Apache Arrow\nDESCRIPTION: Defines the fixed shape tensor extension parameters and serialization format for Arrow. It specifies the extension name as 'arrow.fixed_shape_tensor' and outlines the storage type, parameters including value type and shape, and serialization requirements.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CanonicalExtensions.rst#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n* Extension name: ``arrow.fixed_shape_tensor``.\n\n* The storage type of the extension: ``FixedSizeList`` where:\n\n  * **value_type** is the data type of individual tensor elements.\n  * **list_size** is the product of all the elements in tensor shape.\n\n* Extension type parameters:\n\n  * **value_type** = the Arrow data type of individual tensor elements.\n  * **shape** = the physical shape of the contained tensors\n    as an array.\n\n  Optional parameters describing the logical layout:\n\n  * **dim_names** = explicit names to tensor dimensions\n    as an array. The length of it should be equal to the shape\n    length and equal to the number of dimensions.\n\n    ``dim_names`` can be used if the dimensions have well-known\n    names and they map to the physical layout (row-major).\n\n  * **permutation**  = indices of the desired ordering of the\n    original dimensions, defined as an array.\n\n    The indices contain a permutation of the values [0, 1, .., N-1] where\n    N is the number of dimensions. The permutation indicates which\n    dimension of the logical layout corresponds to which dimension of the\n    physical tensor.\n\n* Description of the serialization:\n\n  The metadata must be a valid JSON object including shape of\n  the contained tensors as an array with key **\"shape\"** plus optional\n  dimension names with keys **\"dim_names\"** and ordering of the\n  dimensions with key **\"permutation\"**.\n\n  - Example: ``{ \"shape\": [2, 5]}``\n  - Example with ``dim_names`` metadata for NCHW ordered data:\n\n    ``{ \"shape\": [100, 200, 500], \"dim_names\": [\"C\", \"H\", \"W\"]}``\n\n  - Example of permuted 3-dimensional tensor:\n\n    ``{ \"shape\": [100, 200, 500], \"permutation\": [2, 0, 1]}``\n```\n\n----------------------------------------\n\nTITLE: Converting JSON Documents to Arrow RecordBatch\nDESCRIPTION: Implementation of ConvertToRecordBatch function that transforms JSON documents into Arrow RecordBatch using RecordBatchBuilder. Includes schema validation and error handling.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/row_columnar_conversion.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\narrow::Result<std::shared_ptr<arrow::RecordBatch>> ConvertToRecordBatch(\n   const std::vector<rapidjson::Document>& docs, std::shared_ptr<arrow::Schema> schema) {\n  ARROW_ASSIGN_OR_RAISE(auto batch_builder,\n                        arrow::RecordBatchBuilder::Make(schema, arrow::default_memory_pool()));\n\n  JsonValueConverter converter{docs};\n  for (int i = 0; i < schema->num_fields(); ++i) {\n    ARROW_RETURN_NOT_OK(converter.Convert(schema->field(i), batch_builder->GetField(i)));\n  }\n\n  std::shared_ptr<arrow::RecordBatch> batch;\n  ARROW_ASSIGN_OR_RAISE(batch, batch_builder->Flush());\n  ARROW_RETURN_NOT_OK(batch->ValidateFull());\n  return batch;\n}  // ConvertToRecordBatch\n```\n\n----------------------------------------\n\nTITLE: Writing Parquet Metadata Files\nDESCRIPTION: Demonstrates writing _metadata and _common_metadata files for compatibility with processing frameworks like Spark and Dask.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmetadata_collector = []\npq.write_to_dataset(table, root_path, metadata_collector=metadata_collector)\n\npq.write_metadata(table.schema, root_path / '_common_metadata')\n\npq.write_metadata(\n    table.schema, root_path / '_metadata',\n    metadata_collector=metadata_collector\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow CSV module with CMake\nDESCRIPTION: This snippet shows how to enable the CSV reader module by passing a boolean flag to CMake.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_CSV=ON\"\n```\n\n----------------------------------------\n\nTITLE: Opening a Parquet File in Arrow C++\nDESCRIPTION: Shows how to open a Parquet file using a ReadableFile object. The example creates a file handle for subsequent Parquet reading operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n  std::shared_ptr<io::ReadableFile> infile;\n  PARQUET_ASSIGN_OR_THROW(infile, io::ReadableFile::Open(\"test.parquet\",\n                                                          pool));\n```\n\n----------------------------------------\n\nTITLE: Setting Session Options: SetSessionOptions\nDESCRIPTION: SetSessionOptions allows clients to set server session options by specifying name/value pairs. This command is critical for defining the context in which queries operate.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_16\n\nLANGUAGE: protobuf\nCODE:\n```\n\"SetSessionOptions\"\n```\n\n----------------------------------------\n\nTITLE: Arrow Data Type Coercion Example\nDESCRIPTION: Example showing how to filter an Arrow table using column comparison. Demonstrates array expression functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_21\n\nLANGUAGE: R\nCODE:\n```\narrow_table[arrow_table$var1 > 5, ]\n```\n\n----------------------------------------\n\nTITLE: Exchanging Data with DoExchange in C++\nDESCRIPTION: This snippet explains how to perform data exchange in Arrow Flight using the DoExchange method. It involves the same initialization as DoPut but allows simultaneous data streaming from both client and server in a single call, improving the efficiency of stateful applications.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Flight.rst#2025-04-16_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nConstruct or acquire a ``FlightDescriptor``, as before.\n#. Call ``DoExchange(FlightData)``.\n\nThe ``FlightDescriptor`` is included with the first message, as with ``DoPut``. At this point, both the client and the server may simultaneously stream data to the other side.\n```\n\n----------------------------------------\n\nTITLE: Configuring FileSystemFactoryOptions for Dataset Reading in Arrow C++\nDESCRIPTION: This snippet sets up the FileSystemFactoryOptions for reading a dataset. It configures partitioning and sets the file format to Parquet.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\narrow::dataset::FileSystemFactoryOptions options;\noptions.partitioning = arrow::dataset::HivePartitioning::MakeFactory();\n\nauto format = std::make_shared<arrow::dataset::ParquetFileFormat>();\n```\n\n----------------------------------------\n\nTITLE: Creating and Manipulating Arrow Arrays in MATLAB\nDESCRIPTION: Demonstrates how to create an Arrow array from a MATLAB double array, access its properties, slice it, and handle missing values. The example shows conversion of MATLAB NaN values to Arrow NULL values.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/doc/matlab_interface_for_apache_arrow_design.md#2025-04-16_snippet_0\n\nLANGUAGE: matlab\nCODE:\n```\n>> A = randi(100, 1, 5) \nA = \n    82 91 13 92 64 \n\n>> class(A) \nans = \n    'double' \n\n>> A(4) = NaN; % Set the fourth element to NaN. \n\n>> AA = arrow.array(A); % Create an arrow.Array from A. \n\n>> class(AA) \nans = \n    'arrow.Float64Array' \n\n>> AA(3:5) % Extract elements at indices 3 to 5 from AA. \nans = \n    13 <NULL> 64 \n\n>> clear AA; % Clear AA from workspace and release Arrow C++ memory.\n```\n\n----------------------------------------\n\nTITLE: Converting Polars DataFrame to PyArrow Table with from_dataframe()\nDESCRIPTION: Demonstrates converting a polars DataFrame with datetime values to a PyArrow table using the dataframe interchange protocol. The example shows how timestamp data is properly handled in the conversion.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/interchange_protocol.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import polars as pl\n>>> from datetime import datetime\n>>> arr = [datetime(2023, 5, 20, 10, 0),\n...        datetime(2023, 5, 20, 11, 0),\n...        datetime(2023, 5, 20, 13, 30)]\n>>> df = pl.DataFrame({\n...          'Talk': ['About Polars','Intro into PyArrow','Coding in Rust'],\n...          'Time': arr,\n...      })\n>>> df\nshape: (3, 2)\n┌────────────────────┬─────────────────────┐\n│ Talk               ┆ Time                │\n│ ---                ┆ ---                 │\n│ str                ┆ datetime[μs]        │\n╞════════════════════╪═════════════════════╡\n│ About Polars       ┆ 2023-05-20 10:00:00 │\n│ Intro into PyArrow ┆ 2023-05-20 11:00:00 │\n│ Coding in Rust     ┆ 2023-05-20 13:30:00 │\n└────────────────────┴─────────────────────┘\n>>> from_dataframe(df)\npyarrow.Table\nTalk: large_string\nTime: timestamp[us]\n----\nTalk: [[\"About Polars\",\"Intro into PyArrow\",\"Coding in Rust\"]]\nTime: [[2023-05-20 10:00:00.000000,2023-05-20 11:00:00.000000,2023-05-20 13:30:00.000000]]\n```\n\n----------------------------------------\n\nTITLE: Using nullable dtypes with pandas read_parquet\nDESCRIPTION: Shows how to use pandas' read_parquet function with the use_nullable_dtypes parameter to automatically convert Arrow data to pandas nullable dtypes when reading Parquet files.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/pandas.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_parquet(path, use_nullable_dtypes=True)\n```\n\n----------------------------------------\n\nTITLE: Writing Arrow Data to Feather File in MATLAB\nDESCRIPTION: Demonstrates the process of writing an Arrow table to a Feather V1 file. The example shows creating a RecordBatch from an Arrow table and using a Feather writer to serialize the data to disk.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/doc/matlab_interface_for_apache_arrow_design.md#2025-04-16_snippet_2\n\nLANGUAGE: matlab\nCODE:\n```\n% Make an `arrow.tabular.RecordBatch` from the `arrow.tabular.Table` created in the previous step\n>> recordBatch = arrow.recordBatch(AT);\n>> filename = \"data.feather\";\n% Write the `arrow.tabular.RecordBatch` to disk as a Feather V1 file named `data.feather`\n>> writer = arrow.internal.io.feather.Writer(filename);\n>> writer.write(recordBatch);\n```\n\n----------------------------------------\n\nTITLE: Custom Type Support Implementation for Arrow Conversion\nDESCRIPTION: Demonstrates how to implement custom type support for Arrow conversions by specializing CTypeTraits and ConversionTraits templates. This example shows implementation for boost::posix_time::ptime type.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/tuple_range_conversion.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace arrow {\n\ntemplate<>\nstruct CTypeTraits<boost::posix_time::ptime> {\n  using ArrowType = ::arrow::TimestampType;\n\n  static std::shared_ptr<::arrow::DataType> type_singleton() {\n    return ::arrow::timestamp(::arrow::TimeUnit::MICRO);\n  }\n};\n\n}\n\nnamespace arrow { namespace stl {\n\ntemplate <>\nstruct ConversionTraits<boost::posix_time::ptime> : public CTypeTraits<boost::posix_time::ptime> {\n  constexpr static bool nullable = false;\n\n  // This is the specialization to load a scalar value into an Arrow builder.\n  static Status AppendRow(\n        typename TypeTraits<TimestampType>::BuilderType& builder,\n        boost::posix_time::ptime cell) {\n    boost::posix_time::ptime const epoch({1970, 1, 1}, {0, 0, 0, 0});\n    return builder.Append((cell - epoch).total_microseconds());\n  }\n\n  // Specify how we can fill the tuple from the values stored in the Arrow\n  // array.\n  static boost::posix_time::ptime GetEntry(\n        const TimestampArray& array, size_t j) {\n    return psapp::arrow::internal::timestamp_epoch\n        + boost::posix_time::time_duration(0, 0, 0, array.Value(j));\n  }\n};\n\n}}\n```\n\n----------------------------------------\n\nTITLE: Defining Computations in PyArrow in Python\nDESCRIPTION: Defines a new tutorial function `tutorial_min_max` for the PyArrow library. This function computes modified minimum and maximum values of a numeric array, offset by 1. Key parameters include 'values' for the array input and 'skip_nulls' for handling null values in calculations. Returns a StructScalar object showing modified min and max values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef tutorial_min_max(values, skip_nulls=True):\n    \"\"\"\n    Compute the minimum-1 and maximum-1 values of a numeric array.\n\n    This is a made-up feature for the tutorial purposes.\n\n    Parameters\n    ----------\n    values : Array\n    skip_nulls : bool, default True\n        If True, ignore nulls in the input.\n\n    Returns\n    -------\n    result : StructScalar of min-1 and max+1\n\n    Examples\n    --------\n    >>> import pyarrow.compute as pc\n    >>> data = [4, 5, 6, None, 1]\n    >>> pc.tutorial_min_max(data)\n    <pyarrow.StructScalar: [('min-', 0), ('max+', 7)]>\n    \"\"\"\n\n    options = ScalarAggregateOptions(skip_nulls=skip_nulls)\n    min_max = call_function(\"min_max\", [values], options)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Prepared Statements: ActionCreatePreparedStatementRequest\nDESCRIPTION: The ActionCreatePreparedStatementRequest command initiates a new prepared statement for a specified SQL query. The response includes a handle to reference the prepared statement.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_10\n\nLANGUAGE: protobuf\nCODE:\n```\n\"ActionCreatePreparedStatementRequest\"\n```\n\n----------------------------------------\n\nTITLE: Parquet Dataset Directory Structure\nDESCRIPTION: Example of a partitioned Parquet dataset directory structure organized by year and month.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_7\n\nLANGUAGE: text\nCODE:\n```\ndataset_name/\n  year=2007/\n    month=01/\n       0.parq\n       1.parq\n       ...\n    month=02/\n       0.parq\n       1.parq\n       ...\n    month=03/\n    ...\n  year=2008/\n    month=01/\n    ...\n  ...\n```\n\n----------------------------------------\n\nTITLE: Creating and Starting a Flight Server in Java\nDESCRIPTION: Demonstrates how to create a Flight server by specifying a location, allocator, and producer. Shows server startup, port discovery, and blocking until server termination.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/flight.rst#2025-04-16_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nLocation location = Location.forGrpcInsecure(\"0.0.0.0\", 0);\ntry(\n    BufferAllocator allocator = new RootAllocator();\n    FlightServer server = FlightServer.builder(\n            allocator,\n            location,\n            new TutorialFlightProducer()\n    ).build();\n){\n    server.start();\n    System.out.println(\"Server listening on port \" + server.getPort());\n    server.awaitTermination();\n} catch (Exception e) {\n    e.printStackTrace();\n}\n```\n\n----------------------------------------\n\nTITLE: String Predicate Functions for ASCII and Unicode\nDESCRIPTION: Collection of unary functions to classify and validate string elements based on character content, with separate implementations for ASCII and Unicode character sets\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_3\n\nLANGUAGE: Apache Arrow\nCODE:\n```\nascii_is_alnum\nascii_is_alpha\nascii_is_decimal\nascii_is_lower\nascii_is_printable\nascii_is_space\nascii_is_upper\nutf8_is_alnum\nutf8_is_alpha\nutf8_is_decimal\nutf8_is_digit\nutf8_is_lower\nutf8_is_numeric\nutf8_is_printable\nutf8_is_space\nutf8_is_upper\n```\n\n----------------------------------------\n\nTITLE: Getting CPU View of Arbitrary Buffer in C++\nDESCRIPTION: Shows how to obtain a CPU-readable view or copy of an arbitrary buffer, which may be useful when dealing with device-agnostic memory operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/memory.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<arrow::Buffer> arbitrary_buffer = ... ;\nstd::shared_ptr<arrow::Buffer> cpu_buffer = arrow::Buffer::ViewOrCopy(\n   arbitrary_buffer, arrow::default_cpu_memory_manager());\n```\n\n----------------------------------------\n\nTITLE: Creating Dense Union Arrays in PyArrow\nDESCRIPTION: Shows creation of a dense union array using types and offsets arrays to select values from child arrays at specific positions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nxs = pa.array([5, 6, 7])\nys = pa.array([False, True])\ntypes = pa.array([0, 1, 1, 0, 0], type=pa.int8())\noffsets = pa.array([0, 0, 1, 1, 2], type=pa.int32())\nunion_arr = pa.UnionArray.from_dense(types, offsets, [xs, ys])\n```\n\n----------------------------------------\n\nTITLE: Building a Buffer Incrementally in C++\nDESCRIPTION: Shows how to allocate and build a Buffer incrementally using the BufferBuilder API, which allows appending data in chunks.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/memory.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nBufferBuilder builder;\nbuilder.Resize(11);  // reserve enough space for 11 bytes\nbuilder.Append(\"hello \", 6);\nbuilder.Append(\"world\", 5);\n\nauto maybe_buffer = builder.Finish();\nif (!maybe_buffer.ok()) {\n   // ... handle buffer allocation error\n}\nstd::shared_ptr<arrow::Buffer> buffer = *maybe_buffer;\n```\n\n----------------------------------------\n\nTITLE: Sparse Union Layout Example\nDESCRIPTION: Example layout for a SparseUnion type containing Int32, Float32, and VarBinary values, demonstrating buffer organization and child array structures.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_11\n\nLANGUAGE: data-format\nCODE:\n```\n[{i=5}, {f=1.2}, {s='joe'}, {f=3.4}, {i=4}, {s='mark'}]\n```\n\n----------------------------------------\n\nTITLE: Creating a Table in Apache Arrow Java\nDESCRIPTION: Demonstrates how to create an immutable Table object with two columns: a boolean column and a varchar column, each containing 10 elements.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector_schema_root.rst#2025-04-16_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nBitVector bitVector = new BitVector(\"boolean\", allocator);\nVarCharVector varCharVector = new VarCharVector(\"varchar\", allocator);\nbitVector.allocateNew();\nvarCharVector.allocateNew();\nfor (int i = 0; i < 10; i++) {\n  bitVector.setSafe(i, i % 2 == 0 ? 0 : 1);\n  varCharVector.setSafe(i, (\"test\" + i).getBytes(StandardCharsets.UTF_8));\n}\nbitVector.setValueCount(10);\nvarCharVector.setValueCount(10);\n\nList<FieldVector> vectors = Arrays.asList(bitVector, varCharVector);\nTable table = new Table(vectors);\n```\n\n----------------------------------------\n\nTITLE: Installing JPype for Java-Python Integration\nDESCRIPTION: Command to install JPype library using pip. JPype enables running a Java Virtual Machine within Python to interact with Java classes.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ pip install jpype1\n```\n\n----------------------------------------\n\nTITLE: Running Arrow IPC Integration Tests with C++ and Java\nDESCRIPTION: This set of commands first defines the version of the Arrow Java tools and sets the ARROW_JAVA_INTEGRATION_JAR environment variable to the path of the JAR file.  Then, it runs the Arrow IPC integration tests with both C++ and Java implementations enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nVERSION=14.0.0-SNAPSHOT\nexport ARROW_JAVA_INTEGRATION_JAR=$JAVA_DIR/tools/target/arrow-tools-$VERSION-jar-with-dependencies.jar\narchery integration --run-ipc --with-cpp=1 --with-java=1\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Files with StreamingReader in C++\nDESCRIPTION: Shows how to stream CSV data as RecordBatches using StreamingReader. Includes initialization, batch processing loop, and error handling for streaming operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/csv.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"arrow/csv/api.h\"\n\n{\n   // ...\n   arrow::io::IOContext io_context = arrow::io::default_io_context();\n   std::shared_ptr<arrow::io::InputStream> input = ...;\n\n   auto read_options = arrow::csv::ReadOptions::Defaults();\n   auto parse_options = arrow::csv::ParseOptions::Defaults();\n   auto convert_options = arrow::csv::ConvertOptions::Defaults();\n\n   // Instantiate StreamingReader from input stream and options\n   auto maybe_reader =\n     arrow::csv::StreamingReader::Make(io_context,\n                                       input,\n                                       read_options,\n                                       parse_options,\n                                       convert_options);\n   if (!maybe_reader.ok()) {\n     // Handle StreamingReader instantiation error...\n   }\n   std::shared_ptr<arrow::csv::StreamingReader> reader = *maybe_reader;\n\n   // Set aside a RecordBatch pointer for re-use while streaming\n   std::shared_ptr<RecordBatch> batch;\n\n   while (true) {\n       // Attempt to read the first RecordBatch\n       arrow::Status status = reader->ReadNext(&batch);\n\n       if (!status.ok()) {\n         // Handle read error\n       }\n\n       if (batch == NULL) {\n         // Handle end of file\n         break;\n       }\n\n       // Do something with the batch\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Large Array Writing with Batches\nDESCRIPTION: Shows how to efficiently write large arrays by breaking them into smaller batches.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/ipc.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE = 10000\nNUM_BATCHES = 1000\n\nschema = pa.schema([pa.field('nums', pa.int32())])\n\nwith pa.OSFile('bigfile.arrow', 'wb') as sink:\n   with pa.ipc.new_file(sink, schema) as writer:\n        for row in range(NUM_BATCHES):\n              batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n              writer.write(batch)\n```\n\n----------------------------------------\n\nTITLE: Accessing BigIntVector Values with Reader in Java\nDESCRIPTION: This example demonstrates how to access values from a BigIntVector using a BigIntReader.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n// access via reader\nBigIntReader reader = vector.getReader();\nfor (int i = 0; i < vector.getValueCount(); i++) {\n  reader.setPosition(i);\n  if (reader.isSet()) {\n    System.out.println(reader.readLong());\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting CPU Thread Pool Capacity in Apache Arrow (C++)\nDESCRIPTION: This function sets the capacity of the CPU thread pool in Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/thread.rst#2025-04-16_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\narrow::SetCpuThreadPoolCapacity\n```\n\n----------------------------------------\n\nTITLE: Detailing Column Struct Schema in Statistics\nDESCRIPTION: This snippet provides detailed notes on the struct representing each column's statistics, including its type, nullability, and the purpose of each field within the struct.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/StatisticsSchema.rst#2025-04-16_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nSchema::\n\n    vendor_id: int32\n    passenger_count: int64\n\nStatistics:\n\n.. flat-table::\n   :header-rows: 1\n\n   * - Target\n     - Name\n     - Value\n   * - Record batch\n     - The number of rows\n     - ``5``\n   * - :rspan:`3` ``vendor_id``\n     - The number of nulls\n     - ``0``\n   * - The number of distinct values\n     - ``2``\n   * - The max value\n     - ``5``\n   * - The min value\n     - ``1``\n   * - :rspan:`4` ``passenger_count``\n     - The number of nulls\n     - ``1``\n   * - The number of distinct values\n     - ``3``\n   * - The max value\n     - ``2``\n   * - The min value\n     - ``0``\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow CUDA integration with CMake\nDESCRIPTION: This snippet shows how to enable CUDA integration for GPU development, requiring the NVIDIA CUDA toolkit. The CUDA toolchain location can be customized using the $CUDA_HOME environment variable.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_CUDA=ON\"\n```\n\n----------------------------------------\n\nTITLE: Preparing FileSystem Object for Local Filesystem in Arrow C++\nDESCRIPTION: This snippet demonstrates how to create a FileSystem object for the local filesystem using Arrow C++. It declares the object and initializes it using FileSystemFromUriOrPath.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nstd::shared_ptr<arrow::fs::FileSystem> fs;\n\nARROW_ASSIGN_OR_RAISE(fs, arrow::fs::FileSystemFromUriOrPath(\"\"));\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow compute module with CMake\nDESCRIPTION: This snippet shows how to enable building all computational kernel functions by passing a boolean flag to CMake.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_COMPUTE=ON\"\n```\n\n----------------------------------------\n\nTITLE: Converting RecordBatch to Tensor in PyArrow\nDESCRIPTION: Demonstrates conversion of a record batch to a tensor format suitable for machine learning applications.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_23\n\nLANGUAGE: python\nCODE:\n```\narr1 = [1, 2, 3, 4, 5]\narr2 = [10, 20, 30, 40, 50]\nbatch = pa.RecordBatch.from_arrays([\n    pa.array(arr1, type=pa.uint16()),\n    pa.array(arr2, type=pa.int16()),\n], [\"a\", \"b\"])\nbatch.to_tensor()\n```\n\n----------------------------------------\n\nTITLE: Reading Feather File into MATLAB\nDESCRIPTION: Shows how to read a Feather file back into MATLAB using a Feather reader. The example demonstrates creating a reader, reading the first RecordBatch, and converting it to a MATLAB table.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/doc/matlab_interface_for_apache_arrow_design.md#2025-04-16_snippet_3\n\nLANGUAGE: matlab\nCODE:\n```\n>> reader = arrow.internal.io.feather.Reader(filename);\n\n% Read in the first RecordBatch\n>> newBatch = reader.read();\n\n% Create a MATLAB `table` from the `arrow.tabular.RecordBatch`\n>> AT = table(newBatch);\n```\n\n----------------------------------------\n\nTITLE: Using Arrow's Direct Compute API in C++\nDESCRIPTION: Example demonstrating how to use Arrow's concrete compute API directly by calling the Add() function with array and scalar inputs.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<arrow::Array> numbers_array = ...;\nstd::shared_ptr<arrow::Scalar> increment = ...;\narrow::Datum incremented_datum;\n\nARROW_ASSIGN_OR_RAISE(incremented_datum,\n                       arrow::compute::Add(numbers_array, increment));\nstd::shared_ptr<Array> incremented_array = std::move(incremented_datum).make_array();\n```\n\n----------------------------------------\n\nTITLE: Running Arrow IPC Integration Tests with C++\nDESCRIPTION: This command executes the Arrow IPC integration tests with the C++ implementation enabled. The --run-ipc flag specifies that the IPC tests should be run, and the --with-cpp=1 flag indicates that the C++ component should be included in the test run.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\narchery integration --run-ipc --with-cpp=1\n```\n\n----------------------------------------\n\nTITLE: Loading Arrow Data from HTTP Response\nDESCRIPTION: Shows how to fetch Arrow-formatted data from a ClickHouse database via HTTP request and convert the binary response into an Arrow table.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\nrequire 'net/http'\n\nparams = {\n  query: \"SELECT WatchID as watch FROM hits LIMIT 10 FORMAT Arrow\",\n  user: \"play\",\n  password: \"\",\n  database: \"default\"\n}\nuri = URI('https://play.clickhouse.com:443/')\nuri.query = URI.encode_www_form(params)\nresp = Net::HTTP.get(uri)\ntable = Arrow::Table.load(Arrow::Buffer.new(resp))\n```\n\n----------------------------------------\n\nTITLE: Creating a Partitioning Method for Dataset Writing in C++\nDESCRIPTION: This snippet initializes a Hive-style partitioning factory using a partition schema. The partitioning method determines how data will be organized in directories when writing a Dataset.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\n// Create our partitioning\nauto partitioning_factory = std::make_shared<HivePartitioning>(partition_schema);\n```\n\n----------------------------------------\n\nTITLE: Setting Target Directory for Dataset Writing in C++\nDESCRIPTION: This snippet sets the base directory path where the Dataset will be written. All data files and partition directories will be created under this path.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\n// Set base_dir\nwrite_options.base_dir = \"write_dataset\";\n```\n\n----------------------------------------\n\nTITLE: Filtering and Column Selection\nDESCRIPTION: Demonstrates filtering data and selecting specific columns using ScannerBuilder's Project and Filter methods.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// Read just the column 'a' where it is > 2\nds::ScannerBuilder builder(dataset);\n\n// Select only the columns we want\nFLIGHT_THROW_IF_NOT_OK(builder.Project({\"a\"}));\n\n// Filter rows\nFLIGHT_THROW_IF_NOT_OK(builder.Filter(greater(field_ref(\"a\"), literal(2))));\n\nauto scanner = builder.Finish().ValueOrDie();\nauto table = scanner->ToTable().ValueOrDie();\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Dictionary Encoded Vectors in Java\nDESCRIPTION: This code demonstrates how to create, write, and read dictionary-encoded vectors using Apache Arrow in Java.  It shows the creation of dictionary and data vectors, encoding the data vector, writing the data to a stream, and then reading and decoding it.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/ipc.rst#2025-04-16_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n// create provider\nDictionaryProvider.MapDictionaryProvider provider = new DictionaryProvider.MapDictionaryProvider();\n\ntry (\n  final VarCharVector dictVector = new VarCharVector(\"dict\", allocator);\n  final VarCharVector vector = new VarCharVector(\"vector\", allocator);\n) {\n  // create dictionary vector\n  dictVector.allocateNewSafe();\n  dictVector.setSafe(0, \"aa\".getBytes());\n  dictVector.setSafe(1, \"bb\".getBytes());\n  dictVector.setSafe(2, \"cc\".getBytes());\n  dictVector.setValueCount(3);\n\n  // create dictionary\n  Dictionary dictionary =\n      new Dictionary(dictVector, new DictionaryEncoding(1L, false, /*indexType=*/null));\n  provider.put(dictionary);\n\n  // create original data vector\n  vector.allocateNewSafe();\n  vector.setSafe(0, \"bb\".getBytes());\n  vector.setSafe(1, \"bb\".getBytes());\n  vector.setSafe(2, \"cc\".getBytes());\n  vector.setSafe(3, \"aa\".getBytes());\n  vector.setValueCount(4);\n\n  // get the encoded vector\n  IntVector encodedVector = (IntVector) DictionaryEncoder.encode(vector, dictionary);\n\n  ByteArrayOutputStream out = new ByteArrayOutputStream();\n\n  // create VectorSchemaRoot\n  List<Field> fields = Arrays.asList(encodedVector.getField());\n  List<FieldVector> vectors = Arrays.asList(encodedVector);\n  try (VectorSchemaRoot root = new VectorSchemaRoot(fields, vectors)) {\n\n      // write data\n      ArrowStreamWriter writer = new ArrowStreamWriter(root, provider, Channels.newChannel(out));\n      writer.start();\n      writer.writeBatch();\n      writer.end();\n  }\n\n  // read data\n  try (ArrowStreamReader reader = new ArrowStreamReader(new ByteArrayInputStream(out.toByteArray()), allocator)) {\n    reader.loadNextBatch();\n    VectorSchemaRoot readRoot = reader.getVectorSchemaRoot();\n    // get the encoded vector\n    IntVector intVector = (IntVector) readRoot.getVector(0);\n\n    // get dictionaries and decode the vector\n    Map<Long, Dictionary> dictionaryMap = reader.getDictionaryVectors();\n    long dictionaryId = intVector.getField().getDictionary().getId();\n    try (VarCharVector varCharVector =\n        (VarCharVector) DictionaryEncoder.decode(intVector, dictionaryMap.get(dictionaryId))) {\n      // ... use decoded vector\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Collecting Maven Dependencies for Python Integration\nDESCRIPTION: Maven command to gather all Java dependencies in a single directory for easier loading from Python. This shows how the required JAR files are copied to a 'dependencies' folder.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_7\n\nLANGUAGE: console\nCODE:\n```\n$ mvn org.apache.maven.plugins:maven-dependency-plugin:2.7:copy-dependencies -DoutputDirectory=dependencies\n[INFO] Scanning for projects...\n[INFO]\n[INFO] ------------------< org.apache.arrow.py2java:FillTen >------------------\n[INFO] Building FillTen 1\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- maven-dependency-plugin:2.7:copy-dependencies (default-cli) @ FillTen ---\n[INFO] Copying jsr305-3.0.2.jar to /experiments/java2py/dependencies/jsr305-3.0.2.jar\n[INFO] Copying netty-common-4.1.72.Final.jar to /experiments/java2py/dependencies/netty-common-4.1.72.Final.jar\n[INFO] Copying arrow-memory-core-8.0.0-SNAPSHOT.jar to /experiments/java2py/dependencies/arrow-memory-core-8.0.0-SNAPSHOT.jar\n[INFO] Copying arrow-vector-8.0.0-SNAPSHOT.jar to /experiments/java2py/dependencies/arrow-vector-8.0.0-SNAPSHOT.jar\n[INFO] Copying arrow-c-data-8.0.0-SNAPSHOT.jar to /experiments/java2py/dependencies/arrow-c-data-8.0.0-SNAPSHOT.jar\n[INFO] Copying arrow-vector-8.0.0-SNAPSHOT.pom to /experiments/java2py/dependencies/arrow-vector-8.0.0-SNAPSHOT.pom\n[INFO] Copying jackson-core-2.11.4.jar to /experiments/java2py/dependencies/jackson-core-2.11.4.jar\n[INFO] Copying jackson-annotations-2.11.4.jar to /experiments/java2py/dependencies/jackson-annotations-2.11.4.jar\n[INFO] Copying slf4j-api-1.7.25.jar to /experiments/java2py/dependencies/slf4j-api-1.7.25.jar\n[INFO] Copying arrow-memory-netty-8.0.0-SNAPSHOT.jar to /experiments/java2py/dependencies/arrow-memory-netty-8.0.0-SNAPSHOT.jar\n[INFO] Copying arrow-format-8.0.0-SNAPSHOT.jar to /experiments/java2py/dependencies/arrow-format-8.0.0-SNAPSHOT.jar\n[INFO] Copying flatbuffers-java-1.12.0.jar to /experiments/java2py/dependencies/flatbuffers-java-1.12.0.jar\n[INFO] Copying arrow-memory-8.0.0-SNAPSHOT.pom to /experiments/java2py/dependencies/arrow-memory-8.0.0-SNAPSHOT.pom\n[INFO] Copying netty-buffer-4.1.72.Final.jar to /experiments/java2py/dependencies/netty-buffer-4.1.72.Final.jar\n[INFO] Copying jackson-databind-2.11.4.jar to /experiments/java2py/dependencies/jackson-databind-2.11.4.jar\n[INFO] Copying commons-codec-1.10.jar to /experiments/java2py/dependencies/commons-codec-1.10.jar\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n```\n\n----------------------------------------\n\nTITLE: Converting Tensor Arrays to NumPy ndarrays in PyArrow\nDESCRIPTION: Demonstrates how to convert a PyArrow FixedShapeTensorArray to a multi-dimensional NumPy ndarray. When converted, the length of the arrow array becomes the first dimension of the resulting ndarray.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> numpy_tensor = tensor_array_2.to_numpy_ndarray()\n>>> numpy_tensor\narray([[[  1.,   2.],\n       [  3.,   4.]],\n      [[ 10.,  20.],\n       [ 30.,  40.]],\n      [[100., 200.],\n       [300., 400.]]])\n>>> numpy_tensor.shape\n(3, 2, 2)\n```\n\n----------------------------------------\n\nTITLE: Representing List<Int8> Array in Arrow Format\nDESCRIPTION: Shows the memory layout for a List<Int8> array [[12, -7, 25], null, [0, -127, 127, 50], []] in Arrow format, including validity bitmap, offsets buffer, and child array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n* Length: 4, Null count: 1\n* Validity bitmap buffer:\n\n  | Byte 0 (validity bitmap) | Bytes 1-63            |\n  |--------------------------|-----------------------|\n  | 00001101                 | 0 (padding)           |\n\n* Offsets buffer (int32)\n\n  | Bytes 0-3  | Bytes 4-7   | Bytes 8-11  | Bytes 12-15 | Bytes 16-19 | Bytes 20-63           |\n  |------------|-------------|-------------|-------------|-------------|-----------------------|\n  | 0          | 3           | 3           | 7           | 7           | unspecified (padding) |\n\n* Values array (Int8Array):\n  * Length: 7,  Null count: 0\n  * Validity bitmap buffer: Not required\n  * Values buffer (int8)\n\n    | Bytes 0-6                    | Bytes 7-63            |\n    |------------------------------|-----------------------|\n    | 12, -7, 25, 0, -127, 127, 50 | unspecified (padding) |\n```\n\n----------------------------------------\n\nTITLE: Mutating and Setting Value Count for IntVector in Java\nDESCRIPTION: This example shows how to set a value in an IntVector and then set its value count to finalize it.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nvector.set(/*index*/5, /*value*/25);\nvector.setValueCount(10);\n```\n\n----------------------------------------\n\nTITLE: Defining CommandGetCatalogs for Metadata Fetching in Protobuf\nDESCRIPTION: The CommandGetCatalogs command allows clients to list the available catalogs in the SQL database. The actual definition varies depending on the database vendor. This command is part of the SQL metadata commands in Arrow Flight SQL.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_0\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandGetCatalogs\"\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow Type Enum in C++\nDESCRIPTION: This snippet documents the arrow::Type::type enum, which represents the various data types supported by Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenenum:: arrow::Type::type\n```\n\n----------------------------------------\n\nTITLE: Converting STL Tuple Range to Arrow Table\nDESCRIPTION: Demonstrates how to convert a vector of std::tuple objects into an Arrow Table using TableFromTupleRange. The conversion is inferred at compile time based on the tuple types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/tuple_range_conversion.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstd::vector<std::tuple<double, std::string>> rows = ..;\nstd::shared_ptr<Table> table;\n\nif (!arrow::stl::TableFromTupleRange(\n      arrow::default_memory_pool(),\n      rows, names, &table).ok()\n) {\n  // Error handling code should go here.\n}\n```\n\n----------------------------------------\n\nTITLE: Evaluating Filter and Projection in Gandiva (C++)\nDESCRIPTION: Demonstrates how to apply both filtering and projection on an Arrow record batch using Gandiva.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gandiva/expr_projector_filter.rst#2025-04-16_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n// evaluate the filter\nauto selection_vector = std::make_shared<SelectionVector>(batch->num_rows(), pool_);\nGANDIVA_RETURN_NOT_OK(filter->Evaluate(*batch, selection_vector));\n\n// project on the selection vector\nstd::shared_ptr<Projector> projector;\nGANDIVA_RETURN_NOT_OK(Projector::Make(schema, {expr}, TestConfiguration(), &projector));\narrow::ArrayVector outputs;\nGANDIVA_RETURN_NOT_OK(\n    projector->Evaluate(*batch, selection_vector->GetNumSlots(), selection_vector->GetRawData(),\n                      selection_vector->GetMode(), pool_, &outputs));\n\n// create output batch\nstd::shared_ptr<arrow::RecordBatch> output_batch;\nGANDIVA_RETURN_NOT_OK(\n    MakeRecordBatch(schema, selection_vector->GetNumSlots(), outputs, &output_batch));\n```\n\n----------------------------------------\n\nTITLE: Loading Data from S3 Storage\nDESCRIPTION: Demonstrates loading Arrow data directly from Amazon S3 storage using the arrow-dataset library. Shows both public and authenticated access patterns.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_4\n\nLANGUAGE: ruby\nCODE:\n```\nrequire 'arrow-dataset'\n\ns3_uri = URI('s3://bucket/public.csv')\nArrow::Table.load(s3_uri)\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Schema in C++\nDESCRIPTION: This snippet demonstrates how to define an Arrow Schema, which describes the structure of a RecordBatch or Table. It shows the creation of Fields for each column and combining them into a Schema.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/basic_arrow.rst#2025-04-16_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nauto schema = arrow::schema({\n  arrow::field(\"day\", arrow::int8()),\n  arrow::field(\"month\", arrow::int8()),\n  arrow::field(\"year\", arrow::int16())\n});\n```\n\n----------------------------------------\n\nTITLE: Memory-Mapped IPC File Example\nDESCRIPTION: MATLAB code demonstrating how to share Arrow data between MATLAB and Python processes using memory-mapped IPC files.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/doc/matlab_interface_for_apache_arrow_design.md#2025-04-16_snippet_8\n\nLANGUAGE: matlab\nCODE:\n```\n% Create a MATLAB arrow.Table. \n>> Var1 = arrow.array([\"foo\", \"bar\", \"baz\"]); \n\n>> Var2 = arrow.array([today, today + 1, today + 2]); \n\n>> Var3 = arrow.array([10, 20, 30]); \n\n>> AT = arrow.Table(Var1, Var2, Var3); \n\n% Write the MATLAB arrow.Table to the Arrow IPC File Format on disk. \n>> recordBatch = arrow.recordBatch(AT);\n\n>> filename = \"data.arrow\"\n\n% Open `data.arrow` as an IPC file\n>> writer = arrow.io.ipc.RecordBatchFileWriter(filename, recordBatch.Schema);\n\n% Write the `RecordBatch` to `data.arrow`\n>> writer.writeRecordBatch(recordBatch);\n\n% Close the writer -- don't forget this step!\n>> writer.close()\n\n% Run Python in a separate process. \n>> pyenv(\"ExecutionMode\", \"OutOfProcess\");  \n\n% Memory map the Arrow IPC File. \n>> memoryMappedFile = py.pyarrow.memory_map(\"data.arrow\"); \n\n% Construct pyarrow.ipc.RecordBatchFileReader to read the Arrow IPC File. \n>> recordBatchFileReader = py.pyarrow.ipc.open_file(memoryMappedFile); \n\n% Read all record batches from the Arrow IPC File in one-shot and return a pyarrow.Table. \n>> PAT = recordBatchFileReader.read_all()\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow Numeric Data Types in C++\nDESCRIPTION: This snippet documents the numeric data types in Arrow, such as IntegerType, FloatingPointType, etc.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygengroup:: numeric-datatypes\n   :content-only:\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Initializing FileSystemDatasetWriteOptions in C++\nDESCRIPTION: This snippet initializes a FileSystemDatasetWriteOptions struct with default values. This struct will hold all the configuration needed for writing a Dataset to disk.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n// Prepare write options\nFileSystemDatasetWriteOptions write_options;\n```\n\n----------------------------------------\n\nTITLE: Creating an Arrow TimestampType with TimeUnit and TimeZone\nDESCRIPTION: Example demonstrating how to create an Arrow TimestampType with specific TimeUnit and TimeZone settings.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_12\n\nLANGUAGE: matlab\nCODE:\n```\n>> type = arrow.timestamp(TimeUnit=\"Second\", TimeZone=\"Asia/Kolkata\")\n\ntype =\n\n  TimestampType with properties:\n\n          ID: Timestamp\n    TimeUnit: Second\n    TimeZone: \"Asia/Kolkata\"\n```\n\n----------------------------------------\n\nTITLE: Defining Statistics Schema for Complex Record Batch in Apache Arrow\nDESCRIPTION: This snippet shows the statistics schema for a complex record batch. It uses a struct type with an int32 column field and a map for statistics, where the key is a dictionary and the items are a dense union of int64 and float64 types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/StatisticsSchema.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nStatistics schema::\n\n    struct<\n      column: int32,\n      statistics: map<\n        key: dictionary<values: utf8, indices: int32>,\n        items: dense_union<\n          # For the number of rows, the number of nulls and so on.\n          0: int64,\n          # For the max/min values of col1.c.\n          1: float64\n        >\n      >\n    >\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow mimalloc-based allocator with CMake\nDESCRIPTION: This snippet shows how to enable building the Arrow mimalloc-based allocator by passing a boolean flag to CMake.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_MIMALLOC=ON\"\n```\n\n----------------------------------------\n\nTITLE: Working with Different Integer Types in Extension Types\nDESCRIPTION: This example shows how extension types can work with different Arrow integer types while maintaining the same extension type name during serialization.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> big_rational_type = RationalType(pa.int64())\n>>> storage_array = pa.array(\n...     [\n...         {\"numer\": 10, \"denom\": 17},\n...         {\"numer\": 20, \"denom\": 13},\n...     ],\n...     type=big_rational_type.storage_type,\n... )\n>>> arr = big_rational_type.wrap_array(storage_array)\n>>> batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n>>> sink = pa.BufferOutputStream()\n>>> with pa.RecordBatchStreamWriter(sink, batch.schema) as writer:\n...    writer.write_batch(batch)\n>>> buf = sink.getvalue()\n>>> with pa.ipc.open_stream(buf) as reader:\n...    result = reader.read_all()\n>>> result.column(\"ext\").type\nRationalType(StructType(struct<numer: int64, denom: int64>))\n```\n\n----------------------------------------\n\nTITLE: Building Arrow with Emscripten Preset\nDESCRIPTION: Commands to build Arrow using the ninja-release-emscripten CMake preset and install it to the Emscripten sysroot cache.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/emscripten.rst#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nemcmake cmake --preset \"ninja-release-emscripten\"\nninja install\n```\n\n----------------------------------------\n\nTITLE: Configuring ScanOptions with BatchSize in Java\nDESCRIPTION: This code snippet demonstrates how to create a ScanOptions object with a specified batch size of 32768. This configuration is used when scanning Parquet files to control the maximum size of individual batches returned by the scanner.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/dataset.rst#2025-04-16_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nScanOptions options = new ScanOptions(/*batchSize*/ 32768);\n```\n\n----------------------------------------\n\nTITLE: Creating PyArrow Buffer from Python bytes\nDESCRIPTION: Demonstrates creating a zero-copy Buffer view from Python bytes object using py_buffer() function\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/memory.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\ndata = b'abcdefghijklmnopqrstuvwxyz'\nbuf = pa.py_buffer(data)\nbuf\nbuf.size\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project for Apache Arrow Examples\nDESCRIPTION: This CMake script sets up the build environment for Apache Arrow tutorial examples. It defines the project, sets C++ standards, configures compiler flags, and creates executable targets linked against Arrow libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/tutorial_examples/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.25)\n\nproject(ArrowTutorialExamples)\n\nfind_package(ArrowDataset)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror -Wall -Wextra\")\n\nset(CMAKE_BUILD_TYPE Release)\n\nmessage(STATUS \"Arrow version: ${ARROW_VERSION}\")\nmessage(STATUS \"Arrow SO version: ${ARROW_FULL_SO_VERSION}\")\n\nadd_executable(arrow_example arrow_example.cc)\ntarget_link_libraries(arrow_example PRIVATE Arrow::arrow_shared)\n\nadd_executable(file_access_example file_access_example.cc)\ntarget_link_libraries(file_access_example PRIVATE Arrow::arrow_shared\n                                                  Parquet::parquet_shared)\n\nadd_executable(compute_example compute_example.cc)\ntarget_link_libraries(compute_example PRIVATE Arrow::arrow_shared)\n\nadd_executable(dataset_example dataset_example.cc)\ntarget_link_libraries(dataset_example PRIVATE ArrowDataset::arrow_dataset_shared)\n```\n\n----------------------------------------\n\nTITLE: Implementing Source Node Example in C++\nDESCRIPTION: Example demonstrating how to use a source node in an execution plan, including data generation and configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/user_guide.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nARROW_RETURN_NOT_OK(Declaration::Sequence({\n    {\"source\", SourceNodeOptions{schema, gen}},\n    {\"sink\", SinkNodeOptions{schema}},\n}).AddToPlan(plan.get()));\n```\n\n----------------------------------------\n\nTITLE: Setting FileSystem for Dataset Writing in C++\nDESCRIPTION: This code assigns a FileSystem instance to the write options. The FileSystem determines where and how the Dataset files will be written, such as to a local filesystem or cloud storage.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\n// Set our filesystem\nwrite_options.filesystem = fs;\n```\n\n----------------------------------------\n\nTITLE: Opening S3 Dataset in R with Arrow\nDESCRIPTION: Example showing how to open a dataset from S3 using Arrow. Note that this requires a special C++ build with additional dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_20\n\nLANGUAGE: R\nCODE:\n```\nds <- open_dataset(\"s3://...\")\n```\n\n----------------------------------------\n\nTITLE: Performing Element-wise Addition with Arrow CallFunction in C++\nDESCRIPTION: Uses CallFunction to perform element-wise addition of two columns in the Arrow Table.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nARROW_ASSIGN_OR_RAISE(\n    add_result,\n    arrow::compute::CallFunction(\n        \"add\", {table->GetColumnByName(\"A\"), table->GetColumnByName(\"B\")}));\n```\n\n----------------------------------------\n\nTITLE: Building PyArrow Wheel with C++ Libraries\nDESCRIPTION: Commands to build a self-contained PyArrow wheel that includes Arrow and Parquet C++ libraries bundled together.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install wheel  # if not installed\n$ python setup.py build_ext --build-type=$ARROW_BUILD_TYPE \\\n         --bundle-arrow-cpp bdist_wheel\n```\n\n----------------------------------------\n\nTITLE: Exporting Arrow Data using PyCapsule Interface in Python\nDESCRIPTION: This Python code demonstrates how to export Arrow data using the PyCapsule interface. It checks if the PyArrow version supports constructing arrays from objects implementing the `__arrow_c_array__` protocol, and if so, uses the `pa.Array` constructor to create an Arrow array from the object.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\n# NEW METHOD\ndef to_arrow(self) -> pa.Array:\n    # PyArrow added support for constructing arrays from objects implementing\n```\n\n----------------------------------------\n\nTITLE: Dense Union Layout Example\nDESCRIPTION: Example layout for a DenseUnion type containing Float32 and Int32 values, showing buffer organization for types, offsets, and child arrays.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_10\n\nLANGUAGE: data-format\nCODE:\n```\n[{f=1.2}, null, {f=3.4}, {i=5}]\n```\n\n----------------------------------------\n\nTITLE: GPU Buffer Operations with Red Arrow CUDA\nDESCRIPTION: Example demonstrating how to initialize a CUDA device context, create and write to a GPU buffer, and copy data back to host memory. Shows basic GPU operations using the Red Arrow CUDA Ruby API.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-cuda/README.md#2025-04-16_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"arrow-cuda\"\n\nmanager = ArrowCUDA::DeviceManager.new\nif manager.n_devices.zero?\n  raise \"No GPU is found\"\nend\n\ncontext = manager[0]\nbuffer = ArrowCUDA::Buffer.new(context, 128)\nArrowCUDA::BufferOutputStream.open(buffer) do |stream|\n  stream.write(\"Hello World\")\nend\nputs buffer.copy_to_host(0, 11) # => \"Hello World\"\n```\n\n----------------------------------------\n\nTITLE: Bulk Ingestion with CommandStatementIngest\nDESCRIPTION: CommandStatementIngest facilitates the execution of bulk ingestion operations, allowing the loading of Arrow record batches into a specified target table while tracking the number of rows ingested.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_15\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandStatementIngest\"\n```\n\n----------------------------------------\n\nTITLE: Exporting ArrowDeviceArrayStream with PyCapsule in Python\nDESCRIPTION: Provides the method for exporting an object as an ArrowDeviceArrayStream in a PyCapsule. This method allows additional keyword arguments and requires a requested schema as input, facilitating device-specific data exporting protocols.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_4\n\nLANGUAGE: py\nCODE:\n```\n\"\"\"\n    Export the object as an ArrowDeviceArrayStream.\n\n    :param requested_schema: A PyCapsule containing a C ArrowSchema representation\n        of a requested schema. Conversion to this schema is best-effort. See\n        `Schema Requests`_.\n    :type requested_schema: PyCapsule or None\n    :param kwargs: Additional keyword arguments should only be accepted if they have\n        a default value of \\\"None\\\", to allow for future addition of new keywords.\n        See :ref:`arrow-pycapsule-interface-device-support` for more details.\n\n    :return: A PyCapsule containing a C ArrowDeviceArrayStream representation of the\n        object. The capsule must have a name of \\\"arrow_device_array_stream\\\".\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Basic CMake Configuration for Arrow Project\nDESCRIPTION: Minimal CMakeLists.txt configuration to compile a source file with Arrow C++ shared library dependencies. Demonstrates basic find_package usage and target linking.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/build_system.rst#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.25)\n\nproject(MyExample)\n\nfind_package(Arrow REQUIRED)\n\nadd_executable(my_example my_example.cc)\ntarget_link_libraries(my_example PRIVATE Arrow::arrow_shared)\n```\n\n----------------------------------------\n\nTITLE: Creating a VectorSchemaRoot in Apache Arrow Java\nDESCRIPTION: Illustrates the creation of a VectorSchemaRoot with two columns: a boolean column and a varchar column, each containing 10 elements.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector_schema_root.rst#2025-04-16_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nBitVector bitVector = new BitVector(\"boolean\", allocator);\nVarCharVector varCharVector = new VarCharVector(\"varchar\", allocator);\nbitVector.allocateNew();\nvarCharVector.allocateNew();\nfor (int i = 0; i < 10; i++) {\n  bitVector.setSafe(i, i % 2 == 0 ? 0 : 1);\n  varCharVector.setSafe(i, (\"test\" + i).getBytes(StandardCharsets.UTF_8));\n}\nbitVector.setValueCount(10);\nvarCharVector.setValueCount(10);\n\nList<Field> fields = Arrays.asList(bitVector.getField(), varCharVector.getField());\nList<FieldVector> vectors = Arrays.asList(bitVector, varCharVector);\nVectorSchemaRoot vectorSchemaRoot = new VectorSchemaRoot(fields, vectors);\n```\n\n----------------------------------------\n\nTITLE: Exporting ArrowArray with PyCapsule in Python\nDESCRIPTION: Defines the method for exporting an array or record batch as a pair of ArrowSchema and ArrowArray structures in PyCapsules. This method optionally accepts a requested schema to customize the export, enhancing flexibility and compatibility.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_1\n\nLANGUAGE: py\nCODE:\n```\n\"\"\"\n    Export the object as a pair of ArrowSchema and ArrowArray structures.\n\n    :param requested_schema: A PyCapsule containing a C ArrowSchema representation\n        of a requested schema. Conversion to this schema is best-effort. See\n        `Schema Requests`_.\n    :type requested_schema: PyCapsule or None\n\n    :return: A pair of PyCapsules containing a C ArrowSchema and ArrowArray,\n        respectively. The schema capsule should have the name \\\"arrow_schema\\\"\n        and the array capsule should have the name \\\"arrow_array\\\".\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Exporting ArrowDeviceArray with PyCapsule in Python\nDESCRIPTION: Describes the method for exporting an object as ArrowSchema and ArrowDeviceArray structures. The method accepts a requested schema and additional arguments, promoting compatibility and support for device-specific data transfers.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_2\n\nLANGUAGE: py\nCODE:\n```\n\"\"\"\n    Export the object as a pair of ArrowSchema and ArrowDeviceArray structures.\n\n    :param requested_schema: A PyCapsule containing a C ArrowSchema representation\n        of a requested schema. Conversion to this schema is best-effort. See\n        `Schema Requests`_.\n    :type requested_schema: PyCapsule or None\n    :param kwargs: Additional keyword arguments should only be accepted if they have\n        a default value of \\\"None\\\", to allow for future addition of new keywords.\n        See :ref:`arrow-pycapsule-interface-device-support` for more details.\n\n    :return: A pair of PyCapsules containing a C ArrowSchema and ArrowDeviceArray,\n        respectively. The schema capsule should have the name \\\"arrow_schema\\\"\n        and the array capsule should have the name \\\"arrow_device_array\\\".\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Demonstrating File Reading in R with Arrow\nDESCRIPTION: This snippet showcases various file reading functions in the Arrow R package, including support for Parquet, Feather, CSV, and raw vector input.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_22\n\nLANGUAGE: R\nCODE:\n```\nread_csv_arrow(file, col_names = TRUE, na = c(\"\", \"NA\"), quoted_na = TRUE, skip = 0)\nread_parquet(file)\nread_feather(file)\nread_parquet(raw_vector)\nread_feather(raw_vector)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Arrow Main Function in C++\nDESCRIPTION: This code sets up the main function for an Arrow program, including error handling using Arrow's Status object. It demonstrates the pattern for using Arrow's error-handling macros.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/basic_arrow.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nint main() {\n  arrow::Status st = RunMain();\n  if (!st.ok()) {\n    std::cerr << st << std::endl;\n    return 1;\n  }\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating a ListVector in Java Arrow\nDESCRIPTION: This code demonstrates how to build a ListVector of integers using UnionListWriter. The example builds a vector with 10 elements, where each element is a list of 5 values calculated as j*i.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\ntry (BufferAllocator allocator = new RootAllocator(Long.MAX_VALUE);\n  ListVector listVector = ListVector.empty(\"vector\", allocator)) {\n  UnionListWriter writer = listVector.getWriter();\n  for (int i = 0; i < 10; i++) {\n     writer.startList();\n     writer.setPosition(i);\n     for (int j = 0; j < 5; j++) {\n         writer.writeInt(j * i);\n     }\n     writer.setValueCount(5);\n     writer.endList();\n  }\n  listVector.setValueCount(10);\n}\n```\n\n----------------------------------------\n\nTITLE: Writing MATLAB Table to Feather V1 File\nDESCRIPTION: This code snippet shows how to create a MATLAB table and write it to a Feather V1 file. It uses the 'featherwrite' function to save the table data in the Feather format, which is compatible with Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_20\n\nLANGUAGE: matlab\nCODE:\n```\n>> t = table([\"A\"; \"B\"; \"C\"], [1; 2; 3], [true; false; true])\n\nt =\n\n  3×3 table\n\n    Var1    Var2    Var3\n    ____    ____    _____\n\n    \"A\"      1      true\n    \"B\"      2      false\n    \"C\"      3      true\n\n>> filename = \"table.feather\";\n\n>> featherwrite(filename, t)\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Filter Conditions\nDESCRIPTION: Demonstrates combining multiple filter conditions using logical operators (&, |, ^) within a slice operation on an Arrow table.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_8\n\nLANGUAGE: ruby\nCODE:\n```\ntable.slice { |slicer| (slicer['age'] > 19) & (slicer['age'] < 23) }\n# => #<Arrow::Table:0x7fa3882cc300 ptr=0x7fa3ad260b00>\n#   name\tage\n# 0\tTom \t 22\n```\n\n----------------------------------------\n\nTITLE: Defining Extension Type Field in Arrow Schema JSON\nDESCRIPTION: JSON representation of an extension type field (e.g., 'rational') in the Arrow schema. Includes underlying storage type and metadata for type reconstruction.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\" : \"name_of_the_field\",\n  \"nullable\" : /* boolean */,\n  \"type\" : {\n    \"name\" : \"struct\"\n  },\n  \"children\" : [\n    {\n      \"name\": \"numer\",\n      \"type\": {\n        \"name\": \"int\",\n        \"bitWidth\": 32,\n        \"isSigned\": true\n      }\n    },\n    {\n      \"name\": \"denom\",\n      \"type\": {\n        \"name\": \"int\",\n        \"bitWidth\": 32,\n        \"isSigned\": true\n      }\n    }\n  ],\n  \"metadata\" : [\n     {\"key\": \"ARROW:extension:name\", \"value\": \"rational\"},\n     {\"key\": \"ARROW:extension:metadata\", \"value\": \"rational-serialized\"}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Headers for Arrow File I/O in C++\nDESCRIPTION: Required header includes for using Arrow's I/O functionality across different file formats.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <arrow/api.h>\\n#include <arrow/csv/api.h>\\n#include <arrow/io/api.h>\\n#include <arrow/ipc/api.h>\\n#include <arrow/result.h>\\n#include <iostream>\n```\n\n----------------------------------------\n\nTITLE: Creating Dictionary-Encoded ChunkedArray from Factors in R\nDESCRIPTION: Creates a dictionary-encoded ChunkedArray from two factor objects. The resulting array has two chunks, each with its own dictionary. The dictionaries contain string values, and the indices are stored as int8 values.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/test-chunked-array.txt#2025-04-16_snippet_3\n\nLANGUAGE: r\nCODE:\n```\nchunked_array(factor(c(\"a\", \"b\")), factor(c(\"c\", \"d\")))\n```\n\n----------------------------------------\n\nTITLE: Adding External gRPC Services to Flight Server\nDESCRIPTION: Shows how to integrate additional gRPC services like Health Check service into a Flight server using transport hints and builder consumers.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/flight.rst#2025-04-16_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nfinal HealthStatusManager statusManager = new HealthStatusManager();\nfinal Consumer<NettyServerBuilder> consumer = (builder) -> {\n  builder.addService(statusManager.getHealthService());\n};\nfinal Location location = forGrpcInsecure(LOCALHOST, 5555);\ntry (\n    BufferAllocator a = new RootAllocator(Long.MAX_VALUE);\n    Producer producer = new Producer(a);\n    FlightServer s = FlightServer.builder(a, location, producer)\n        .transportHint(\"grpc.builderConsumer\", consumer).build().start();\n) {\n  Channel channel = NettyChannelBuilder.forAddress(location.toSocketAddress()).usePlaintext().build();\n  HealthCheckResponse response = HealthGrpc\n          .newBlockingStub(channel)\n          .check(HealthCheckRequest.getDefaultInstance());\n\n  System.out.println(response.getStatus());\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Parquet Modular Encryption with CMake\nDESCRIPTION: This snippet shows how to enable Parquet Modular Encryption by passing a boolean flag to CMake.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\n\"-DPARQUET_REQUIRE_ENCRYPTION=ON\"\n```\n\n----------------------------------------\n\nTITLE: Defining Tensor Structure in Arrow with Flatbuffers\nDESCRIPTION: This code defines how to encapsulate a multidimensional array of fixed-size values within a tensor message using the Flatbuffers protocol. The message encapsulation includes padding and aligns the tensor body for efficient IPC.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Other.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n    <metadata prefix and metadata>\n    <PADDING>\n    <tensor body>\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using JPype to Access Java Arrow Objects from Python\nDESCRIPTION: This Python script demonstrates how to use JPype to start a JVM, import the FillTen Java class, and invoke its createArray method to get an Arrow array. It then converts the Java Arrow object to a proper PyArrow array using pyarrow.jvm.array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport jpype\nimport jpype.imports\nfrom jpype.types import *\n\n# Start a JVM making available all dependencies we collected\n# and our class from target/FillTen-1.jar\njpype.startJVM(classpath=[\"./dependencies/*\", \"./target/*\"])\n\nFillTen = JClass('FillTen')\n\narray = FillTen.createArray()\nprint(\"ARRAY\", type(array), array)\n\n# Convert the proxied BigIntVector to an actual pyarrow array\nimport pyarrow.jvm\npyarray = pyarrow.jvm.array(array)\nprint(\"ARRAY\", type(pyarray), pyarray)\ndel pyarray\n```\n\n----------------------------------------\n\nTITLE: Creating a Field with Metadata in Apache Arrow Java\nDESCRIPTION: Demonstrates how to create a Field object representing a 'document' column of string type with associated metadata.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector_schema_root.rst#2025-04-16_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nimport org.apache.arrow.vector.types.pojo.ArrowType;\nimport org.apache.arrow.vector.types.pojo.Field;\nimport org.apache.arrow.vector.types.pojo.FieldType;\n\nMap<String, String> metadata = new HashMap<>();\nmetadata.put(\"A\", \"Id card\");\nmetadata.put(\"B\", \"Passport\");\nmetadata.put(\"C\", \"Visa\");\nField document = new Field(\"document\", new FieldType(true, new ArrowType.Utf8(), /*dictionary*/ null, metadata), /*children*/ null);\n```\n\n----------------------------------------\n\nTITLE: Writing Arrow File Format\nDESCRIPTION: Demonstrates writing record batches to a file format that supports random access.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/ipc.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsink = pa.BufferOutputStream()\n\nwith pa.ipc.new_file(sink, batch.schema) as writer:\n   for i in range(10):\n      writer.write_batch(batch)\n```\n\n----------------------------------------\n\nTITLE: Managing Native Memory with NativeMemoryPool in Apache Arrow Java\nDESCRIPTION: This code snippet shows how to manage native memory allocation with a NativeMemoryPool in Apache Arrow's Java implementation. It demonstrates creating a listenable memory pool with custom reservation listeners to track memory use. This is essential for managing resources across C++ and Java interactions in Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/dataset.rst#2025-04-16_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nAtomicLong reserved = new AtomicLong(0L);\nReservationListener listener = new ReservationListener() {\n  @Override\n  public void reserve(long size) {\n    reserved.getAndAdd(size);\n  }\n\n  @Override\n  public void unreserve(long size) {\n    reserved.getAndAdd(-size);\n  }\n};\nNativeMemoryPool pool = NativeMemoryPool.createListenable(listener);\nFileSystemDatasetFactory factory = new FileSystemDatasetFactory(allocator,\n    pool, FileFormat.PARQUET, uri);\n\n```\n\n----------------------------------------\n\nTITLE: Building Apache Parquet C++ Libraries with CMake\nDESCRIPTION: Demonstrates how to compile Apache Parquet C++ libraries using CMake, with optional encryption support. It requires CMake and possibly additional packages like bison and flex, depending on the build environment.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nmake parquet\n```\n\n----------------------------------------\n\nTITLE: Preparing Datum for Addition Output in Arrow C++\nDESCRIPTION: Declares a Datum object to store the result of the element-wise addition computation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\narrow::Datum add_result;\n```\n\n----------------------------------------\n\nTITLE: Specifying Permutation in Fixed Shape Tensor Type\nDESCRIPTION: Shows how to create a fixed shape tensor type with a specified permutation. The permutation parameter allows for logical reordering of dimensions compared to the physical layout of the data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/extending_types.rst#2025-04-16_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n>>> tensor_type = pa.fixed_shape_tensor(pa.float64(), [2, 2, 3], permutation=[0, 2, 1])\n```\n\n----------------------------------------\n\nTITLE: Working with File-like Objects in Feather\nDESCRIPTION: Demonstrates reading and writing Feather files using file-like objects instead of file paths.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/feather.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith open('/path/to/file', 'wb') as f:\n    feather.write_feather(df, f)\n\nwith open('/path/to/file', 'rb') as f:\n    read_df = feather.read_feather(f)\n```\n\n----------------------------------------\n\nTITLE: Importing CUDA Module in Python for Apache Arrow\nDESCRIPTION: This snippet shows how to import the CUDA module from pyarrow. It's used to access CUDA-related functionality in Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/cuda.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: pyarrow.cuda\n```\n\n----------------------------------------\n\nTITLE: Creating ListView Arrays from Component Arrays in PyArrow\nDESCRIPTION: Shows how to create a ListView array from explicit offsets, sizes, and values arrays, demonstrating the ability to specify out-of-order offsets.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nvalues = [1, 2, 3, 4, 5, 6]\noffsets = [4, 2, 0]\nsizes = [2, 2, 2]\narr = pa.ListViewArray.from_arrays(offsets, sizes, values)\narr\n```\n\n----------------------------------------\n\nTITLE: Representing Int32 Array in Arrow Format\nDESCRIPTION: Demonstrates the memory layout for an Int32 array [1, null, 2, 4, 8] in Arrow format, including validity bitmap and value buffer.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n* Length: 5, Null count: 1\n* Validity bitmap buffer:\n\n  | Byte 0 (validity bitmap) | Bytes 1-63            |\n  |--------------------------|-----------------------|\n  | 00011101                 | 0 (padding)           |\n\n* Value Buffer:\n\n  | Bytes 0-3   | Bytes 4-7   | Bytes 8-11  | Bytes 12-15 | Bytes 16-19 | Bytes 20-63           |\n  |-------------|-------------|-------------|-------------|-------------|-----------------------|\n  | 1           | unspecified | 2           | 4           | 8           | unspecified (padding) |\n```\n\n----------------------------------------\n\nTITLE: Defining FlightSQL Protocol Buffers Schema\nDESCRIPTION: Protocol Buffer definitions for Apache Arrow Flight SQL service that specify the interface and message types for database access and query execution.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_19\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage FlightSql {\n  // Protocol buffer content referenced but not shown in snippet\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Simple Array Schema and Data in Apache Arrow\nDESCRIPTION: This snippet defines a simple array schema with int64 type and provides sample data. It also includes statistics for the array such as row count, null count, distinct count, and max/min values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/StatisticsSchema.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nSchema::\n\n    int64\n\nData::\n\n    [1, 1, 2, 0, null]\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Minimal Example CMake Project\nDESCRIPTION: A complete CMake configuration file for building a minimal Arrow example. It sets up project requirements, configures C++ standards, finds the Arrow package dependency, and creates an executable that links against either the shared or static Arrow library based on user preference.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/minimal_build/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.16)\n\nproject(ArrowMinimalExample)\n\noption(ARROW_LINK_SHARED \"Link to the Arrow shared library\" ON)\n\nfind_package(Arrow REQUIRED)\n\nif(NOT DEFINED CMAKE_CXX_STANDARD)\n  set(CMAKE_CXX_STANDARD 17)\nendif()\n\n# We require a C++17 compliant compiler\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nif(NOT CMAKE_BUILD_TYPE)\n  set(CMAKE_BUILD_TYPE Release)\nendif()\n\nmessage(STATUS \"Arrow version: ${ARROW_VERSION}\")\nmessage(STATUS \"Arrow SO version: ${ARROW_FULL_SO_VERSION}\")\n\nadd_executable(arrow-example example.cc)\n\nif(ARROW_LINK_SHARED)\n  target_link_libraries(arrow-example PRIVATE Arrow::arrow_shared)\nelse()\n  target_link_libraries(arrow-example PRIVATE Arrow::arrow_static)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating ExecBatch Structure in Python\nDESCRIPTION: This code snippet illustrates the structure of an ExecBatch, which can contain both arrays and scalars. It shows four different ways to represent the same batch of data using combinations of arrays and scalars.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/overview.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. figure:: scalar_vs_array.svg\n\n   There are four different ways to represent the given batch of data using different combinations\n   of arrays and scalars.  All four exec batches should be considered semantically equivalent.\n```\n\n----------------------------------------\n\nTITLE: Listing Classes for Tables and Arrays in PyArrow\nDESCRIPTION: This code block enumerates the classes available in PyArrow for working with tables, record batches, and arrays.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/tables.rst#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   ChunkedArray\n   RecordBatch\n   Table\n   TableGroupBy\n   RecordBatchReader\n```\n\n----------------------------------------\n\nTITLE: Evaluating Projection in Gandiva (C++)\nDESCRIPTION: Demonstrates how to evaluate a projection on an Arrow record batch using the created Projector.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gandiva/expr_projector_filter.rst#2025-04-16_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n// evaluate the projection\narrow::ArrayVector outputs;\nGANDIVA_RETURN_NOT_OK(projector->Evaluate(*batch, pool_, &outputs));\n\n// create a new record batch with the projected output\nstd::shared_ptr<arrow::RecordBatch> output_batch;\nGANDIVA_RETURN_NOT_OK(MakeRecordBatch(schema, batch->num_rows(), outputs, &output_batch));\n```\n\n----------------------------------------\n\nTITLE: Installing PyArrow with Conda\nDESCRIPTION: Command to install PyArrow using the conda package manager from conda-forge channel. This is the recommended cross-platform installation method.\nSOURCE: https://github.com/apache/arrow/blob/main/python/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nconda install pyarrow -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Benchmarks in CMake\nDESCRIPTION: Configures performance benchmarks for various Arrow components using the add_arrow_benchmark helper function. These benchmarks can be used to measure and track performance of the library.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_25\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_benchmark(builder_benchmark)\nadd_arrow_benchmark(chunk_resolver_benchmark)\nadd_arrow_benchmark(compare_benchmark)\nadd_arrow_benchmark(memory_pool_benchmark)\nadd_arrow_benchmark(type_benchmark)\nadd_arrow_benchmark(tensor_benchmark)\n```\n\n----------------------------------------\n\nTITLE: Repartitioning a Dataset with Arrow in Python\nDESCRIPTION: Demonstrates how to read from an existing partitioned dataset and write it with a different partitioning scheme without loading all data into memory at once. It changes from Hive-style partitioning to directory partitioning.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nold_part = ds.partitioning(\n    pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n)\nnew_part = ds.partitioning(\n    pa.schema([(\"c\", pa.int16())]), flavor=None\n)\ninput_dataset = ds.dataset(\"partitioned_dataset\", partitioning=old_part)\n# A scanner can act as an iterator of record batches but you could also receive\n# data from the network (e.g. via flight), from your own scanning, or from any\n# other method that yields record batches.  In addition, you can pass a dataset\n# into write_dataset directly but this method is useful if you want to customize\n# the scanner (e.g. to filter the input dataset or set a maximum batch size)\nscanner = input_dataset.scanner()\n\nds.write_dataset(scanner, \"repartitioned_dataset\", format=\"parquet\", partitioning=new_part)\n```\n\n----------------------------------------\n\nTITLE: Creating Double ChunkedArray with Multiple Chunks in R\nDESCRIPTION: Creates a ChunkedArray of double type with two chunks using numeric vectors. The resulting array contains two chunks with values [1,2,3] and [4,5,6].\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/test-chunked-array.txt#2025-04-16_snippet_0\n\nLANGUAGE: r\nCODE:\n```\nchunked_array(c(1, 2, 3), c(4, 5, 6))\n```\n\n----------------------------------------\n\nTITLE: Configuring Trace Logging for Memory Leak Detection in Apache Arrow\nDESCRIPTION: This snippet demonstrates how to configure trace logging in Apache Arrow to get detailed stack traces when memory leaks occur. It sets the logging level to TRACE for the Arrow package and creates a situation where a buffer is leaked to demonstrate the logging output.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/memory.rst#2025-04-16_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// Assumes use of Logback; adjust for Log4j, etc. as appropriate\nimport ch.qos.logback.classic.Level;\nimport ch.qos.logback.classic.Logger;\nimport org.apache.arrow.memory.ArrowBuf;\nimport org.apache.arrow.memory.BufferAllocator;\nimport org.apache.arrow.memory.RootAllocator;\nimport org.slf4j.LoggerFactory;\n\n// Set log level to TRACE to get tracebacks\n((Logger) LoggerFactory.getLogger(\"org.apache.arrow\")).setLevel(Level.TRACE);\ntry (final BufferAllocator allocator = new RootAllocator()) {\n  // Leak buffer\n  allocator.buffer(1024);\n}\n```\n\n----------------------------------------\n\nTITLE: Debugging PyArrow with GDB\nDESCRIPTION: Commands for using GDB to debug Arrow C++ libraries while running Python unit tests, including setting breakpoints.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n$ gdb --args python -m pytest pyarrow/tests/test_to_run.py -k $TEST_TO_MATCH\n```\n\n----------------------------------------\n\nTITLE: Handling Flight Service Errors and Listener Notifications\nDESCRIPTION: Shows how to handle errors in Flight RPC methods by using CallContext and StreamListener. Demonstrates sending custom error messages and handling exceptions on the client side.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/flight.rst#2025-04-16_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// Server\n@Override\npublic void listFlights(CallContext context, Criteria criteria, StreamListener<FlightInfo> listener) {\n    // ...\n    listener.onError(\n        CallStatus.UNAUTHENTICATED.withDescription(\n            \"Custom UNAUTHENTICATED description message.\").toRuntimeException());\n    // ...\n}\n\n// Client\ntry{\n    Iterable<FlightInfo> flightInfosBefore = flightClient.listFlights(Criteria.ALL);\n    // ...\n} catch (FlightRuntimeException e){\n    // Catch UNAUTHENTICATED exception\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Address Sanitizer with CMake\nDESCRIPTION: This snippet shows how to enable the Address Sanitizer to check for memory leaks, buffer overflows, or other kinds of memory management issues.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_50\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_USE_ASAN=ON\"\n```\n\n----------------------------------------\n\nTITLE: Creating an Arrow Schema from Multiple Fields\nDESCRIPTION: Example demonstrating how to create an Arrow Schema from multiple Field objects using the arrow.schema function.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_18\n\nLANGUAGE: matlab\nCODE:\n```\n>> letter = arrow.field(\"Letter\", arrow.string())\n\nletter =\n\nLetter: string\n\n>> number = arrow.field(\"Number\", arrow.int8())\n\nnumber =\n\nNumber: int8\n\n>> schema = arrow.schema([letter, number])\n\nschema =\n\nLetter: string\nNumber: int8\n```\n\n----------------------------------------\n\nTITLE: Creating a Java Class with Arrow Vector for Data Exchange\nDESCRIPTION: Java class that creates and populates an Arrow BigIntVector with values 1-10. This demonstrates how to use Arrow's Java implementation to create data structures that can be exchanged with Python.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.arrow.memory.RootAllocator;\nimport org.apache.arrow.vector.BigIntVector;\n\n\npublic class FillTen {\n    static RootAllocator allocator = new RootAllocator();\n\n    public static BigIntVector createArray() {\n        BigIntVector intVector = new BigIntVector(\"ints\", allocator);\n        intVector.allocateNew(10);\n        intVector.setValueCount(10);\n        FillTen.fillVector(intVector);\n        return intVector;\n    }\n\n    private static void fillVector(BigIntVector iv) {\n        iv.setSafe(0, 1);\n        iv.setSafe(1, 2);\n        iv.setSafe(2, 3);\n        iv.setSafe(3, 4);\n        iv.setSafe(4, 5);\n        iv.setSafe(5, 6);\n        iv.setSafe(6, 7);\n        iv.setSafe(7, 8);\n        iv.setSafe(8, 9);\n        iv.setSafe(9, 10);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Dictionaries from VectorSchemaRoot in Java Arrow\nDESCRIPTION: This code demonstrates how to retrieve a dictionary for a specific encoded vector in a VectorSchemaRoot by using the field's dictionary encoding ID to look up the dictionary in a provider.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\n// create the encoded vector, the Dictionary and DictionaryProvider as discussed above\n\n// Create a VectorSchemaRoot with one encoded vector\nVectorSchemaRoot vsr = new VectorSchemaRoot(List.of(encoded));\n\n// now we want to decode our vector, so we retrieve its dictionary from the provider\nField f = vsr.getField(encoded.getName());\nDictionaryEncoding encoding = f.getDictionary();\nDictionary dictionary = provider.lookup(encoding.getId());\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow ORC integration with CMake\nDESCRIPTION: This snippet shows how to enable Arrow integration with Apache ORC.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_ORC=ON\"\n```\n\n----------------------------------------\n\nTITLE: Saving and Comparing Benchmark Results\nDESCRIPTION: Commands for saving benchmark results to JSON files and comparing them later.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/benchmarks.rst#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# Run the benchmarks on a given commit and save the result\narchery benchmark run --output=run-head-1.json HEAD~1\n# Compare the previous captured result with HEAD\narchery benchmark diff HEAD run-head-1.json\n```\n\n----------------------------------------\n\nTITLE: Creating Projector and Filter in Gandiva (C++)\nDESCRIPTION: Shows how to create a Projector and a Filter using the previously created expression and condition.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gandiva/expr_projector_filter.rst#2025-04-16_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n// Create a projector for x + 3\nstd::shared_ptr<Projector> projector;\nGANDIVA_RETURN_NOT_OK(Projector::Make(schema, {expr}, TestConfiguration(), &projector));\n\n// Create a filter for x < 3\nstd::shared_ptr<Filter> filter;\nGANDIVA_RETURN_NOT_OK(Filter::Make(schema, condition, TestConfiguration(), &filter));\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Morsel-Batch Execution Model in Python\nDESCRIPTION: Illustrates the morsel-batch execution model used in Acero. The outer loop processes morsels in parallel while the inner loop processes smaller batches sequentially to maximize cache efficiency and parallelism.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/developer_guide.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor morsel in dataset: # parallel\n  for batch in morsel:\n    run_pipeline(batch)\n```\n\n----------------------------------------\n\nTITLE: Defining RecordBatch in Arrow JSON\nDESCRIPTION: JSON structure for a RecordBatch in Arrow, containing the row count and an array of FieldData objects.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"count\": /* integer number of rows */,\n  \"columns\": [ /* FieldData */ ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using TypedBufferBuilder for Fixed-Width Types in C++\nDESCRIPTION: Demonstrates the use of TypedBufferBuilder for creating a buffer containing fixed-width type values, specifically int32_t in this example.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/memory.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nTypedBufferBuilder<int32_t> builder;\nbuilder.Reserve(2);  // reserve enough space for two int32_t values\nbuilder.Append(0x12345678);\nbuilder.Append(-0x765643210);\n\nauto maybe_buffer = builder.Finish();\nif (!maybe_buffer.ok()) {\n   // ... handle buffer allocation error\n}\nstd::shared_ptr<arrow::Buffer> buffer = *maybe_buffer;\n```\n\n----------------------------------------\n\nTITLE: Recompiling PyArrow after changes to Cython files\nDESCRIPTION: Command to recompile PyArrow when changes are made to .pyx or .pxd files. This is necessary to incorporate Cython code changes into the Python package.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/step_by_step/building.rst#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ python setup.py build_ext --inplace\n```\n\n----------------------------------------\n\nTITLE: Setting File Name Template for Dataset Writing in C++\nDESCRIPTION: This snippet defines the naming pattern for data files within each partition directory. The {i} placeholder will be replaced with a sequential number for each file fragment.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\n// Finish write_options\nwrite_options.basename_template = \"part{i}.parquet\";\n```\n\n----------------------------------------\n\nTITLE: Releasing Device Array Resources in C\nDESCRIPTION: This function cleans up resources associated with an ArrowDeviceArray. It destroys the CUDA event and frees the allocated device memory buffers, ensuring no memory leaks occur during the lifetime of the device array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDeviceDataInterface.rst#2025-04-16_snippet_1\n\nLANGUAGE: c\nCODE:\n```\nstatic void release_int32_device_array(struct ArrowArray* array) {\n    assert(array->n_buffers == 2);\n    // destroy the event\n    cudaEvent_t* ev_ptr = (cudaEvent_t*)(array->private_data);\n    cudaError_t status = cudaEventDestroy(*ev_ptr);\n    assert(status == cudaSuccess);\n    free(ev_ptr);\n\n    // free the buffers and the buffers array\n    status = cudaFree(array->buffers[1]);\n    assert(status == cudaSuccess);\n    free(array->buffers);\n\n    // mark released\n    array->release = NULL;\n}\n```\n\n----------------------------------------\n\nTITLE: ListView Int8 Array Layout Example\nDESCRIPTION: Example showing the memory layout of a ListView<Int8> array with length 4, demonstrating validity bitmap, offsets buffer, sizes buffer, and values array organization.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\n* Length: 4, Null count: 1\n* Validity bitmap buffer:\n  | Byte 0 (validity bitmap) | Bytes 1-63            |\n  |--------------------------|-----------------------|\n  | 00001101                 | 0 (padding)           |\n\n* Offsets buffer (int32)\n  | Bytes 0-3  | Bytes 4-7   | Bytes 8-11  | Bytes 12-15 | Bytes 16-63           |\n  |------------|-------------|-------------|-------------|-----------------------|\n  | 0          | 7           | 3           | 0           | unspecified (padding) |\n```\n\n----------------------------------------\n\nTITLE: Reading Arrow File Asynchronously in C#\nDESCRIPTION: This code snippet demonstrates how to read an Arrow file asynchronously using the Apache.Arrow library. It opens a file stream, creates an ArrowFileReader, and reads the next record batch.\nSOURCE: https://github.com/apache/arrow/blob/main/csharp/README.md#2025-04-16_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nusing System.Diagnostics;\nusing System.IO;\nusing System.Threading.Tasks;\nusing Apache.Arrow;\nusing Apache.Arrow.Ipc;\n\npublic static async Task<RecordBatch> ReadArrowAsync(string filename)\n{\n    using (var stream = File.OpenRead(filename))\n    using (var reader = new ArrowFileReader(stream))\n    {\n        var recordBatch = await reader.ReadNextRecordBatchAsync();\n        Debug.WriteLine(\"Read record batch with {0} column(s)\", recordBatch.ColumnCount);\n        return recordBatch;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Arrays with Explicit Type in PyArrow\nDESCRIPTION: Demonstrates creating an array with an explicitly specified data type, overriding the default type inference behavior.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npa.array([1, 2], type=pa.uint16())\n```\n\n----------------------------------------\n\nTITLE: Defining FieldData in Arrow JSON\nDESCRIPTION: JSON structure for FieldData in Arrow, including field name, count, buffer data, and children for nested types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"field_name\",\n  \"count\" \"field_length\",\n  \"$BUFFER_TYPE\": /* BufferData */\n  ...\n  \"$BUFFER_TYPE\": /* BufferData */\n  \"children\": [ /* FieldData */ ]\n}\n```\n\n----------------------------------------\n\nTITLE: Registering External C Functions in Gandiva\nDESCRIPTION: API for registering an external C function into the Gandiva function registry, including options for function metadata, implementation pointer, and optional function holder maker.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gandiva/external_func.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n/// \\brief register a C function into the function registry\n/// @param func the registered function's metadata\n/// @param c_function_ptr the function pointer to the\n/// registered function's implementation\n/// @param function_holder_maker this will be used as the function holder if the\n/// function requires a function holder\narrow::Status Register(\n    NativeFunction func, void* c_function_ptr,\n    std::optional<FunctionHolderMaker> function_holder_maker = std::nullopt);\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with Arrow in R\nDESCRIPTION: Shows how to use read_csv_arrow() function to read CSV files, mapping readr-style 'T' type to nanosecond-precision timestamps.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_10\n\nLANGUAGE: R\nCODE:\n```\nread_csv_arrow()\n```\n\n----------------------------------------\n\nTITLE: Configuring Call Timeouts and Cancellations\nDESCRIPTION: Demonstrates setting timeouts for client calls using CallOptions and handling call cancellations on both client and server sides.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/flight.rst#2025-04-16_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n// Client\nLocation location = Location.forGrpcInsecure(\"0.0.0.0\", 58609);\n\ntry(BufferAllocator allocator = new RootAllocator();\n    FlightClient tutorialFlightClient = FlightClient.builder(allocator, location).build()){\n\n    Iterator<Result> resultIterator = tutorialFlightClient.doAction(\n            new Action(\"test-timeout\"),\n            CallOptions.timeout(2, TimeUnit.SECONDS)\n    );\n} catch (Exception e) {\n    e.printStackTrace();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Sparse Union Arrays in PyArrow\nDESCRIPTION: Demonstrates how to create a sparse union array by combining multiple arrays with a types array that indicates which child array each value comes from.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/data.rst#2025-04-16_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nxs = pa.array([5, 6, 7])\nys = pa.array([False, False, True])\ntypes = pa.array([0, 1, 1], type=pa.int8())\nunion_arr = pa.UnionArray.from_sparse(types, [xs, ys])\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Arrow Ruby Gems\nDESCRIPTION: Shell commands to install the necessary Ruby gems for Apache Arrow functionality, including basic Arrow support, Parquet format support, and dataset functionality for S3 access.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngem install red-arrow\ngem install red-parquet # for parquet support\ngem install red-arrow-dataset # reading from s3 / folders\n```\n\n----------------------------------------\n\nTITLE: Displaying Dataset with glimpse() in R Arrow\nDESCRIPTION: Shows how glimpse() formats a Dataset object. The output includes file source information, row count, column details with types and value previews, and a note about viewing full schema details.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_10\n\nLANGUAGE: r\nCODE:\n```\nglimpse(ds)\n```\n\n----------------------------------------\n\nTITLE: Displaying Arrow C++ Dataset Overview in Markdown\nDESCRIPTION: This markdown snippet provides a brief introduction to the Arrow C++ Dataset subcomponent, explaining its purpose and current development status.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/dataset/README.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Arrow C++ Dataset\n\nThe `arrow::dataset` subcomponent provides an API to read and write\nsemantic datasets stored in different locations and formats. It\nfacilitates parallel processing of datasets spread across different\nphysical files and serialization formats. Other concerns such as\npartitioning, filtering (partition- and column-level), and schema\nnormalization are also addressed.\n\n## Development Status\n\nAlpha/beta stage as of April 2020. API subject to change, possibly\nwithout deprecation notices.\n```\n\n----------------------------------------\n\nTITLE: ArrowAsyncDeviceStreamHandler Structure Members in C\nDESCRIPTION: Core callback definitions and structure members for handling async device streams. Includes handlers for schema updates, new tasks, errors, and resource cleanup.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDeviceDataInterface.rst#2025-04-16_snippet_5\n\nLANGUAGE: c\nCODE:\n```\nint (*on_next_task)(struct ArrowAsyncDeviceStreamHandler*, struct ArrowAsyncTask*, const char*);\nvoid (*on_error)(struct ArrowAsyncDeviceStreamHandler, int, const char*, const char*);\nvoid (*release)(struct ArrowAsyncDeviceStreamHandler*);\nstruct ArrowAsyncProducer producer;\nvoid* private_data;\n```\n\n----------------------------------------\n\nTITLE: Loading GDB extension for Arrow C++\nDESCRIPTION: GDB command to manually load the Arrow C++ extension for improved debugging output.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gdb.rst#2025-04-16_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n(gdb) source path/to/gdb_arrow.py\n```\n\n----------------------------------------\n\nTITLE: Creating Tables from RecordBatches\nDESCRIPTION: Demonstrates the recommended approach for combining multiple RecordBatches using Table$create() since direct rbind is not supported.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/RecordBatch.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nUse `Table$create()` to combine RecordBatches into a Table\n```\n\n----------------------------------------\n\nTITLE: Creating an Arrow Float64Array from MATLAB Double Array\nDESCRIPTION: Example showing how to convert a MATLAB double array to an Arrow Float64Array using the arrow.array function.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_4\n\nLANGUAGE: matlab\nCODE:\n```\n>> matlabArray = double([1, 2, 3])\n\nmatlabArray =\n\n     1     2     3\n\n>> arrowArray = arrow.array(matlabArray)\n\narrowArray = \n\n[\n  1,\n  2,\n  3\n]\n```\n\n----------------------------------------\n\nTITLE: Reading Arrow Streaming Format in Java\nDESCRIPTION: This snippet shows how to read a stream of record batches using `ArrowStreamReader` in Java.  It initializes an `ArrowStreamReader` with an input stream and iterates through the batches using `loadNextBatch()`. The VectorSchemaRoot within the reader is re-populated with new values on every call to loadNextBatch().\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/ipc.rst#2025-04-16_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\ntry (ArrowStreamReader reader = new ArrowStreamReader(new ByteArrayInputStream(out.toByteArray()), allocator)) {\n  // This will be loaded with new values on every call to loadNextBatch\n  VectorSchemaRoot readRoot = reader.getVectorSchemaRoot();\n  Schema schema = readRoot.getSchema();\n  for (int i = 0; i < 5; i++) {\n    reader.loadNextBatch();\n    // ... do something with readRoot\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking Java Methods from Python using JPype\nDESCRIPTION: Python script that starts a JVM, loads a Java class, and calls a static method from that class. This demonstrates basic Python-Java interoperability.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport jpype\nfrom jpype.types import *\n\njpype.startJVM(classpath=[\"./\"])\n\nSimple = JClass('Simple')\n\nprint(Simple.getNumber())\n```\n\n----------------------------------------\n\nTITLE: Setting Up Protocol Buffer Generation for Arrow Flight in CMake\nDESCRIPTION: Configures the paths, command, and custom targets for generating Protocol Buffer files using protoc and gRPC plugin for the Flight protocol.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(FLIGHT_PROTO_PATH \"${ARROW_SOURCE_DIR}/../format\")\nset(FLIGHT_PROTO \"${ARROW_SOURCE_DIR}/../format/Flight.proto\")\n\nset(FLIGHT_GENERATED_PROTO_FILES\n    \"${CMAKE_CURRENT_BINARY_DIR}/Flight.pb.cc\" \"${CMAKE_CURRENT_BINARY_DIR}/Flight.pb.h\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/Flight.grpc.pb.cc\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/Flight.grpc.pb.h\")\n\nset(PROTO_DEPENDS ${FLIGHT_PROTO} ${ARROW_PROTOBUF_LIBPROTOBUF} gRPC::grpc_cpp_plugin)\n\nset(FLIGHT_PROTOC_COMMAND ${ARROW_PROTOBUF_PROTOC} \"-I${FLIGHT_PROTO_PATH}\")\nif(Protobuf_VERSION VERSION_LESS 3.15)\n  list(APPEND FLIGHT_PROTOC_COMMAND \"--experimental_allow_proto3_optional\")\nendif()\nadd_custom_command(OUTPUT ${FLIGHT_GENERATED_PROTO_FILES}\n                   DEPENDS ${PROTO_DEPENDS} ARGS\n                   COMMAND ${FLIGHT_PROTOC_COMMAND}\n                           \"--cpp_out=${CMAKE_CURRENT_BINARY_DIR}\" \"${FLIGHT_PROTO}\"\n                   COMMAND ${FLIGHT_PROTOC_COMMAND}\n                           \"--grpc_out=${CMAKE_CURRENT_BINARY_DIR}\"\n                           \"--plugin=protoc-gen-grpc=$<TARGET_FILE:gRPC::grpc_cpp_plugin>\"\n                           \"${FLIGHT_PROTO}\")\n\nset_source_files_properties(${FLIGHT_GENERATED_PROTO_FILES} PROPERTIES GENERATED TRUE)\n\nadd_custom_target(flight_grpc_gen ALL DEPENDS ${FLIGHT_GENERATED_PROTO_FILES})\n```\n\n----------------------------------------\n\nTITLE: Optimized Int64Array construction using bulk appending in Apache Arrow C++\nDESCRIPTION: This example demonstrates how to optimize array construction using bulk appending methods and preallocation. It uses the Reserve and AppendValues methods for improved performance.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/arrays.rst#2025-04-16_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\narrow::Int64Builder builder;\nbuilder.Reserve(8);\nstd::vector<bool> validity = {true, true, true, false, true, true, true, true};\nstd::vector<int64_t> values = {1, 2, 3, 0, 5, 6, 7, 8};\nbuilder.AppendValues(values, validity);\n\nauto maybe_array = builder.Finish();\n```\n\n----------------------------------------\n\nTITLE: Red Arrow Development Setup\nDESCRIPTION: Console commands for setting up a development environment for Red Arrow, including installing dependencies and running tests.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow/README.md#2025-04-16_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ cd ruby/red-arrow\n$ bundle install\n$ bundle exec rake test\n```\n\n----------------------------------------\n\nTITLE: Error Handling with Arrow Buffer Allocation\nDESCRIPTION: Demonstrates the standard pattern for handling errors when allocating an Arrow buffer using Result types. Shows how to check for successful allocation and handle potential errors.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/conventions.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nconst int64_t buffer_size = 4096;\n\nauto maybe_buffer = arrow::AllocateBuffer(buffer_size, &buffer);\nif (!maybe_buffer.ok()) {\n   // ... handle error\n} else {\n   std::shared_ptr<arrow::Buffer> buffer = *maybe_buffer;\n   // ... use allocated buffer\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Ad-Hoc Queries: CommandStatementQuery\nDESCRIPTION: CommandStatementQuery allows for the execution of ad-hoc SQL queries, returning results. It provides flexibility for dynamic SQL operations and notification of results through flight.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_13\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandStatementQuery\"\n```\n\n----------------------------------------\n\nTITLE: Creating an Arrow Field with Int8Type\nDESCRIPTION: Example showing how to create an Arrow Field with a specified name and Int8Type, and how to access its properties.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_14\n\nLANGUAGE: matlab\nCODE:\n```\n>> field = arrow.field(\"Number\", arrow.int8())\n\nfield =\n\nNumber: int8\n\n>> field.Name\n\nans =\n\n    \"Number\"\n\n>> field.Type\n\nans =\n\n  Int8Type with properties:\n\n    ID: Int8\n```\n\n----------------------------------------\n\nTITLE: Assigning Partitioning for Dataset Writing in C++\nDESCRIPTION: This code assigns the previously created partitioning factory to the write options. This determines how the data will be organized into directories based on column values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\n// Set partitioning\nwrite_options.partitioning = partitioning_factory;\n```\n\n----------------------------------------\n\nTITLE: Printing Sum Result from Arrow Datum in C++\nDESCRIPTION: Extracts and prints the sum value from the Datum object.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nstd::cout << sum_result.scalar_as<arrow::Int64Scalar>().value << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Run-End-Encoded Data Structure Example\nDESCRIPTION: Example showing how run-end encoded data is structured for a Float32 array with runs of values and nulls.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_13\n\nLANGUAGE: text\nCODE:\n```\ntype: Float32\n[1.0, 1.0, 1.0, 1.0, null, null, 2.0]\n```\n\n----------------------------------------\n\nTITLE: Using Explicit JDBC Type Mapping\nDESCRIPTION: This snippet demonstrates how to explicitly specify JDBC types in JdbcToArrow configurations, handling cases where JDBC drivers may provide inaccurate type information. The snippet illustrates setting scale and rounding mode configurations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/jdbc.rst#2025-04-16_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nMap<Integer, JdbcFieldInfo> mapping = new HashMap<>();\\nmapping.put(1, new JdbcFieldInfo(Types.DECIMAL, 20, 7));\\nJdbcToArrowConfig config = new JdbcToArrowConfigBuilder(allocator, /*calendar=*/null)\\n    .setBigDecimalRoundingMode(RoundingMode.UNNECESSARY)\\n    .setExplicitTypesByColumnIndex(mapping)\\n    .build();\\ntry (ArrowVectorIterator iter = JdbcToArrow.sqlToArrowVectorIterator(rs, config)) {\\n  while (iter.hasNext()) {\\n    VectorSchemaRoot root = iter.next();\\n    // Consume the root…\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing BigIntVector Values with Get API in Java\nDESCRIPTION: This snippet shows how to access values from a BigIntVector using the get API.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\n// access via get API\nfor (int i = 0; i < vector.getValueCount(); i++) {\n  if (!vector.isNull(i)) {\n    System.out.println(vector.get(i));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting PyArrow Array to JAX Array Using DLPack\nDESCRIPTION: This example demonstrates two methods to convert a PyArrow array into a JAX array using the DLPack protocol. Both jax.numpy.from_dlpack and jax.dlpack.from_dlpack can be used to create a JAX array that shares memory with the original PyArrow array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dlpack.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> jax.numpy.from_dlpack(array)\nArray([2, 0, 2, 4], dtype=int32)\n>>> jax.dlpack.from_dlpack(array)\nArray([2, 0, 2, 4], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Table Column Summation Using Arrow Visitor Pattern\nDESCRIPTION: Demonstrates implementing the visitor pattern to sum columns of different numeric types in an Arrow table.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/datatypes.rst#2025-04-16_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nclass TableSummation {\n  double partial = 0.0;\n public:\n\n  arrow::Result<double> Compute(std::shared_ptr<arrow::RecordBatch> batch) {\n    for (std::shared_ptr<arrow::Array> array : batch->columns()) {\n      ARROW_RETURN_NOT_OK(arrow::VisitArrayInline(*array, this));\n    }\n    return partial;\n  }\n\n  // Default implementation\n  arrow::Status Visit(const arrow::Array& array) {\n    return arrow::Status::NotImplemented(\"Cannot compute sum for array of type \",\n                                         array.type()->ToString());\n  }\n\n  template <typename ArrayType, typename T = typename ArrayType::TypeClass>\n  arrow::enable_if_number<T, arrow::Status> Visit(const ArrayType& array) {\n    for (std::optional<typename T::c_type> value : array) {\n      if (value.has_value()) {\n        partial += static_cast<double>(value.value());\n      }\n    }\n    return arrow::Status::OK();\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Threading Support\nDESCRIPTION: CMake configuration for adding pthread support to a target. Required for Linux and macOS when linking with Arrow static libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_54\n\nLANGUAGE: cmake\nCODE:\n```\nset(THREADS_PREFER_PTHREAD_FLAG ON)\nfind_package(Threads REQUIRED)\ntarget_link_libraries(my_target PRIVATE Threads::Threads)\n```\n\n----------------------------------------\n\nTITLE: Creating a Scanner for Reading a Dataset in C++\nDESCRIPTION: This snippet demonstrates creating a Scanner from a ScannerBuilder to read through a Dataset. The Scanner is a tool that allows iteration through Dataset fragments.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n// Synchronously finish the ScannerBuilder\nauto scanner_result = builder.Finish();\nif (!scanner_result.ok()) {\n  return scanner_result.status();\n}\n\n// Get our Scanner\nstd::shared_ptr<Scanner> scanner = scanner_result.ValueOrDie();\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow ZLIB compression with CMake\nDESCRIPTION: This snippet shows how to enable support for zlib (gzip) compression.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_40\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_WITH_ZLIB=ON\"\n```\n\n----------------------------------------\n\nTITLE: Explaining BatchSpan and KeyColumnArray Concepts\nDESCRIPTION: This note explains the lightweight versions of batches and arrays used in the compute module and Acero. It describes the purpose of these types and mentions potential future merging of concepts.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/overview.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. note::\n   Both Acero and the compute module have \"lightweight\" versions of batches and arrays.\n   In the compute module these are called ``BatchSpan``, ``ArraySpan``, and ``BufferSpan``.  In\n   Acero the concept is called ``KeyColumnArray``.  These types were developed concurrently\n   and serve the same purpose.  They aim to provide an array container that can be completely\n   stack allocated (provided the data type is non-nested) in order to avoid heap allocation\n   overhead.  Ideally these two concepts will be merged someday.\n```\n\n----------------------------------------\n\nTITLE: Displaying ChunkedArray with glimpse() in R Arrow\nDESCRIPTION: Demonstrates using glimpse() on a string column (ChunkedArray) from a Table object. The output shows the data type and all values in the array.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_1\n\nLANGUAGE: r\nCODE:\n```\nglimpse(tab$chr)\n```\n\n----------------------------------------\n\nTITLE: Multi-Row Group Parquet Writing\nDESCRIPTION: Demonstrates writing a Parquet file with multiple row groups using ParquetWriter.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith pq.ParquetWriter('example2.parquet', table.schema) as writer:\n   for i in range(3):\n      writer.write_table(table)\n\npf2 = pq.ParquetFile('example2.parquet')\npf2.num_row_groups\n```\n\n----------------------------------------\n\nTITLE: Extracting a Column from Arrow RecordBatch by Index\nDESCRIPTION: Example showing how to extract a specific column from an Arrow RecordBatch by its index position using the column method.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_10\n\nLANGUAGE: matlab\nCODE:\n```\n>> arrowRecordBatch = arrow.tabular.RecordBatch.fromArrays(stringArray, timestampArray, booleanArray)\n\narrowRecordBatch =\n\nColumn1:   [\n    \"A\",\n    \"B\",\n    \"C\"\n  ]\nColumn2:   [\n    1997-01-01 00:00:00.000000,\n    1998-01-01 00:00:00.000000,\n    1999-01-01 00:00:00.000000\n  ]\nColumn3:   [\n    true,\n    false,\n    true\n  ]\n\n>> timestampArray = arrowRecordBatch.column(2)\n\ntimestampArray =\n\n[\n  1997-01-01 00:00:00.000000,\n  1998-01-01 00:00:00.000000,\n  1999-01-01 00:00:00.000000\n]\n```\n\n----------------------------------------\n\nTITLE: Creating an ArrowSchema PyCapsule in C\nDESCRIPTION: This C code demonstrates how to create a PyCapsule for an `ArrowSchema`. It allocates memory for the schema, fills in the fields (indicated by comments), and then creates a PyCapsule with a destructor function (`ReleaseArrowSchemaPyCapsule`) to release the memory when the capsule is no longer needed.  The capsule name is \"arrow_schema\".\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_11\n\nLANGUAGE: c\nCODE:\n```\n#include <Python.h>\n\nvoid ReleaseArrowSchemaPyCapsule(PyObject* capsule) {\n    struct ArrowSchema* schema =\n        (struct ArrowSchema*)PyCapsule_GetPointer(capsule, \"arrow_schema\");\n    if (schema->release != NULL) {\n        schema->release(schema);\n    }\n    free(schema);\n}\n\nPyObject* ExportArrowSchemaPyCapsule() {\n    struct ArrowSchema* schema =\n        (struct ArrowSchema*)malloc(sizeof(struct ArrowSchema));\n    // Fill in ArrowSchema fields\n    // ...\n    return PyCapsule_New(schema, \"arrow_schema\", ReleaseArrowSchemaPyCapsule);\n}\n```\n\n----------------------------------------\n\nTITLE: Build Apache Arrow with MSVC\nDESCRIPTION: Command to compile the Apache Arrow project from the generated MSVC solution file in release mode. This ensures the build process uses the generated solution configurations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncmake --build . --config Release\n```\n\n----------------------------------------\n\nTITLE: Hash Table Second Pass Resize Algorithm\nDESCRIPTION: Algorithm for the second pass of hash table resizing that handles overflow entries. It reinserts overflow entries into the target table without performing key comparisons.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/acero/doc/key_map.md#2025-04-16_snippet_1\n\nLANGUAGE: pseudocode\nCODE:\n```\nfor each slot in source_table:\n    if is_overflow_entry(slot.hash):\n        target_block = calculate_start_block(slot.hash)\n        while block_is_full(target_block):\n            target_block = next_block(target_block)\n        insert_into_first_empty_slot(target_block, slot)\n```\n\n----------------------------------------\n\nTITLE: Autosummary for Python-Implemented FileSystem Handlers\nDESCRIPTION: This snippet generates an autosummary for classes used to define filesystems with behavior implemented in Python. It includes PyFileSystem, FileSystemHandler, and FSSpecHandler.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/filesystems.rst#2025-04-16_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   PyFileSystem\n   FileSystemHandler\n   FSSpecHandler\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building Arrow Flight RPC with Dependencies\nDESCRIPTION: Shows how to configure and build the Arrow Flight RPC, indicating required dependencies like gRPC and Protobuf. Provides commands for installing and linking these dependencies using environment variables.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ncmake .. -DARROW_FLIGHT=ON -DARROW_BUILD_TESTS=ON\nmake\n```\n\n----------------------------------------\n\nTITLE: Converting Arrow Schema to Substrait Schema in Python\nDESCRIPTION: This code demonstrates how to serialize an Arrow schema into a Substrait schema. It creates a simple Arrow schema with integer and string fields, then converts it to Substrait format using the serialize_schema function.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/substrait.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nimport pyarrow.substrait as pa_substrait\n\narrow_schema = pa.schema([\n    pa.field(\"x\", pa.int32()),\n    pa.field(\"y\", pa.string())\n])\nsubstrait_schema = pa_substrait.serialize_schema(arrow_schema)\n```\n\n----------------------------------------\n\nTITLE: Defining PyArrow Build Options in CMake\nDESCRIPTION: Defines a macro for creating PyArrow build options that can be controlled via environment variables or CMake settings. It then uses this macro to define options for various PyArrow components.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nmacro(define_option name description arrow_option)\n  set(\"PYARROW_${name}\"\n      \"AUTO\"\n      CACHE STRING ${description})\n\n  if(\"${PYARROW_${name}}\" STREQUAL \"AUTO\")\n    # by default, first check if env variable exists, otherwise use Arrow C++ config\n    set(env_variable \"PYARROW_WITH_${name}\")\n    if(DEFINED ENV{${env_variable}})\n      if($ENV{${env_variable}})\n        set(\"PYARROW_BUILD_${name}\" ON)\n      else()\n        set(\"PYARROW_BUILD_${name}\" OFF)\n      endif()\n    else()\n      if(${arrow_option})\n        set(\"PYARROW_BUILD_${name}\" ON)\n      else()\n        set(\"PYARROW_BUILD_${name}\" OFF)\n      endif()\n    endif()\n  else()\n    if(\"${PYARROW_${name}}\")\n      set(\"PYARROW_BUILD_${name}\" ON)\n    else()\n      set(\"PYARROW_BUILD_${name}\" OFF)\n    endif()\n  endif()\nendmacro()\n\ndefine_option(ACERO \"Build the PyArrow Acero integration\" ARROW_ACERO)\ndefine_option(CUDA \"Build the PyArrow CUDA support\" ARROW_CUDA)\ndefine_option(DATASET \"Build the PyArrow Dataset integration\" ARROW_DATASET)\ndefine_option(FLIGHT \"Build the PyArrow Flight integration\" ARROW_FLIGHT)\ndefine_option(GANDIVA \"Build the PyArrow Gandiva integration\" ARROW_GANDIVA)\ndefine_option(ORC \"Build the PyArrow ORC integration\" ARROW_ORC)\ndefine_option(PARQUET \"Build the PyArrow Parquet integration\" ARROW_PARQUET)\ndefine_option(PARQUET_ENCRYPTION \"Build the PyArrow Parquet encryption integration\"\n              PARQUET_REQUIRE_ENCRYPTION)\ndefine_option(SUBSTRAIT \"Build the PyArrow Substrait integration\" ARROW_SUBSTRAIT)\ndefine_option(AZURE \"Build the PyArrow Azure integration\" ARROW_AZURE)\ndefine_option(GCS \"Build the PyArrow GCS integration\" ARROW_GCS)\ndefine_option(S3 \"Build the PyArrow S3 integration\" ARROW_S3)\ndefine_option(HDFS \"Build the PyArrow HDFS integration\" ARROW_HDFS)\noption(PYARROW_BUNDLE_ARROW_CPP \"Bundle the Arrow C++ libraries\" OFF)\noption(PYARROW_BUNDLE_CYTHON_CPP \"Bundle the C++ files generated by Cython\" OFF)\noption(PYARROW_GENERATE_COVERAGE \"Build with Cython code coverage enabled\" OFF)\nset(PYARROW_CXXFLAGS\n    \"\"\n    CACHE STRING \"Compiler flags to append when compiling PyArrow C++\")\n```\n\n----------------------------------------\n\nTITLE: Setting Test Directory Macro and Dependencies in CMake\nDESCRIPTION: Configures test targets with the appropriate directory macro and dependencies. It iterates through targets that depend on test bitcode files, adds the extension-tests dependency, and defines the test directory location.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/tests/external_functions/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGETS_DEPENDING_ON_TEST_BITCODE_FILES gandiva-internals-test gandiva-projector-test\n                                            gandiva-projector-test-static)\nforeach(TARGET ${TARGETS_DEPENDING_ON_TEST_BITCODE_FILES})\n  if(TARGET ${TARGET})\n    add_dependencies(${TARGET} extension-tests)\n    target_compile_definitions(${TARGET}\n                               PRIVATE -DGANDIVA_EXTENSION_TEST_DIR=\"${CMAKE_CURRENT_BINARY_DIR}\"\n    )\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Experimental Build on Windows ARM64 with Ninja\nDESCRIPTION: Experimental setup using Ninja and Clang for building Apache Arrow on Windows/ARM64 architecture. Suitable for ARM64 platform but limited in testing due to infrastructure constraints.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_15\n\nLANGUAGE: batch\nCODE:\n```\ncd cpp\nmkdir build\ncd build\n\nset CC=clang-cl\nset CXX=clang-cl\n\ncmake -G \"Ninja\" ..\n\ncmake --build . --config Release\n```\n\n----------------------------------------\n\nTITLE: Evaluating Filter in Gandiva (C++)\nDESCRIPTION: Shows how to evaluate a filter on an Arrow record batch and materialize the filtered results.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gandiva/expr_projector_filter.rst#2025-04-16_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n// evaluate the filter\nauto selection_vector = std::make_shared<SelectionVector>(batch->num_rows(), pool_);\nGANDIVA_RETURN_NOT_OK(filter->Evaluate(*batch, selection_vector));\n\n// materialize the filtered batch\nstd::shared_ptr<arrow::Array> filtered_indices;\nGANDIVA_RETURN_NOT_OK(selection_vector->ToArray(&filtered_indices));\nstd::shared_ptr<arrow::RecordBatch> filtered_batch;\nGANDIVA_RETURN_NOT_OK(\n    arrow::compute::Take(batch, filtered_indices, arrow::compute::TakeOptions::Defaults(),\n                        &filtered_batch));\n```\n\n----------------------------------------\n\nTITLE: Creating an Arrow Field with StringType\nDESCRIPTION: Example showing how to create an Arrow Field with a specified name and StringType, and how to access its properties.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_15\n\nLANGUAGE: matlab\nCODE:\n```\n>> field = arrow.field(\"Letter\", arrow.string())\n\nfield =\n\nLetter: string\n\n>> field.Name\n\nans =\n\n    \"Letter\"\n\n>> field.Type\n\nans =\n\n  StringType with properties:\n\n    ID: String\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variables for Maven Execution\nDESCRIPTION: This snippet demonstrates setting an environment variable for JVM options affecting Maven execution, allowing correct configuration of module opens for your Java application without modifying the POM file directly.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/install.rst#2025-04-16_snippet_7\n\nLANGUAGE: [unspecified]\nCODE:\n```\nJDK_JAVA_OPTIONS=\"--add-opens=java.base/java.nio=ALL-UNNAMED\" mvn exec:java -Dexec.mainClass=\"YourMainCode\"\n```\n\n----------------------------------------\n\nTITLE: Arrow Type Traits Example for Boolean Type\nDESCRIPTION: Shows the implementation of type traits for the Boolean data type, defining associated array, builder, scalar, and C types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/datatypes.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <>\nstruct TypeTraits<BooleanType> {\n  using ArrayType = BooleanArray;\n  using BuilderType = BooleanBuilder;\n  using ScalarType = BooleanScalar;\n  using CType = bool;\n\n  static constexpr int64_t bytes_required(int64_t elements) {\n    return bit_util::BytesForBits(elements);\n  }\n  constexpr static bool is_parameter_free = true;\n  static inline std::shared_ptr<DataType> type_singleton() { return boolean(); }\n};\n```\n\n----------------------------------------\n\nTITLE: Converting PyArrow Table to Interchange Object with __dataframe__()\nDESCRIPTION: Demonstrates how to use the __dataframe__() method on a PyArrow table to create an interchange object that can be consumed by other libraries. This method returns a PyArrow-specific implementation of the dataframe protocol.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/interchange_protocol.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow as pa\n>>> table = pa.table({\"n_attendees\": [100, 10, 1]})\n>>> table.__dataframe__()\n<pyarrow.interchange.dataframe._PyArrowDataFrame object at ...>\n```\n\n----------------------------------------\n\nTITLE: Querying with Prepared Statements: CommandPreparedStatementQuery\nDESCRIPTION: CommandPreparedStatementQuery executes a previously created prepared statement, retrieving results. This command can bind parameter values to the prepared statement for flexible query execution.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_11\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandPreparedStatementQuery\"\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow RecordBatch from Multiple Arrow Arrays\nDESCRIPTION: Example demonstrating how to create an Arrow RecordBatch from multiple Arrow Arrays using the RecordBatch.fromArrays method.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_9\n\nLANGUAGE: matlab\nCODE:\n```\n>> stringArray = arrow.array([\"A\", \"B\", \"C\"])\n\nstringArray =\n\n[\n  \"A\",\n  \"B\",\n  \"C\"\n]\n\n>> timestampArray = arrow.array([datetime(1997, 01, 01), datetime(1998, 01, 01), datetime(1999, 01, 01)])\n\ntimestampArray =\n\n[\n  1997-01-01 00:00:00.000000,\n  1998-01-01 00:00:00.000000,\n  1999-01-01 00:00:00.000000\n]\n\n>> booleanArray = arrow.array([true, false, true])\n\nbooleanArray =\n\n[\n  true,\n  false,\n  true\n]\n\n>> arrowRecordBatch = arrow.tabular.RecordBatch.fromArrays(stringArray, timestampArray, booleanArray)\n\narrowRecordBatch =\n\nColumn1:   [\n    \"A\",\n    \"B\",\n    \"C\"\n  ]\nColumn2:   [\n    1997-01-01 00:00:00.000000,\n    1998-01-01 00:00:00.000000,\n    1999-01-01 00:00:00.000000\n  ]\nColumn3:   [\n    true,\n    false,\n    true\n  ]\n```\n\n----------------------------------------\n\nTITLE: Defining DictionaryBatch in Arrow JSON\nDESCRIPTION: JSON structure for a DictionaryBatch in Arrow, containing an ID and a RecordBatch.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": /* integer */,\n  \"data\": [ /* RecordBatch */ ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Project Node Example in C++\nDESCRIPTION: Example showing how to use project node for column transformation and manipulation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/user_guide.rst#2025-04-16_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nARROW_RETURN_NOT_OK(Declaration::Sequence({\n    {\"table_source\", TableSourceNodeOptions{table, max_batch_size}},\n    {\"project\",\n     ProjectNodeOptions{{add(field_ref(\"a\"), literal(1)), multiply(field_ref(\"b\"), literal(3))},\n                       {\"a_plus_1\", \"b_times_3\"}}},\n    {\"sink\", SinkNodeOptions{table->schema()}},\n}).AddToPlan(plan.get()));\n```\n\n----------------------------------------\n\nTITLE: Declaring IndexOptions Struct in Arrow Compute Functions (C++)\nDESCRIPTION: Creates an IndexOptions struct which is required for configuring index-based search operations in Arrow compute functions. This structure is used to set parameters for the 'index' function.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\ncompute::IndexOptions options;\n```\n\n----------------------------------------\n\nTITLE: Building Arrow C++ on Windows with CMake\nDESCRIPTION: CMake configuration and build commands for Windows environment using Ninja generator and various Arrow components.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_22\n\nLANGUAGE: batch\nCODE:\n```\n$ cmake -G \"Ninja\" ^\n      -DCMAKE_INSTALL_PREFIX=%ARROW_HOME% ^\n      -DCMAKE_UNITY_BUILD=ON ^\n      -DARROW_COMPUTE=ON ^\n      -DARROW_CSV=ON ^\n      -DARROW_CXXFLAGS=\"/WX /MP\" ^\n      -DARROW_DATASET=ON ^\n      -DARROW_FILESYSTEM=ON ^\n      -DARROW_HDFS=ON ^\n      -DARROW_JSON=ON ^\n      -DARROW_PARQUET=ON ^\n      -DARROW_WITH_LZ4=ON ^\n      -DARROW_WITH_SNAPPY=ON ^\n      -DARROW_WITH_ZLIB=ON ^\n      -DARROW_WITH_ZSTD=ON ^\n      ..\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow Nested Data Types in C++\nDESCRIPTION: This snippet documents the nested data types in Arrow, such as ListType, StructType, MapType, etc.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygengroup:: nested-datatypes\n   :content-only:\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Downloading Timezone Database for Arrow on Windows\nDESCRIPTION: Batch script to download and extract the IANA timezone database and Windows timezone mapping XML. This is a required runtime dependency for Arrow on Windows systems, as Arrow doesn't use the OS-provided timezone database on this platform.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/build_system.rst#2025-04-16_snippet_4\n\nLANGUAGE: batch\nCODE:\n```\n@rem (Doc section: Download timezone database)\nif not exist %USERPROFILE%\\Downloads\\tzdata (\n    mkdir %USERPROFILE%\\Downloads\\tzdata\n    pushd %USERPROFILE%\\Downloads\\tzdata\n\n    curl -f -O https://raw.githubusercontent.com/unicode-org/cldr/master/common/supplemental/windowsZones.xml\n    curl -f -O https://data.iana.org/time-zones/releases/tzdata2019c.tar.gz\n    7z x tzdata2019c.tar.gz\n    7z x tzdata2019c.tar\n    del tzdata2019c.tar\n    del tzdata2019c.tar.gz\n\n    popd\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Ninja Build System with CMake\nDESCRIPTION: This snippet shows how to enable the Ninja build system when using CMake. Ninja can provide faster builds, especially for incremental changes.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n\"-GNinja\"\n```\n\n----------------------------------------\n\nTITLE: String Title Case and ASCII Checks\nDESCRIPTION: Functions to validate title casing and ASCII composition of string elements, with specific rules for word boundaries and character classification\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_4\n\nLANGUAGE: Apache Arrow\nCODE:\n```\nascii_is_title\nutf8_is_title\nstring_is_ascii\n```\n\n----------------------------------------\n\nTITLE: Displaying arrow::Status with GDB extension\nDESCRIPTION: Example of how GDB displays an arrow::Status object with the custom extension loaded. Shows a more concise and informative representation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gdb.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n$5 = arrow::Status::OK()\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow Schema Class in C++\nDESCRIPTION: This snippet documents the arrow::Schema class, which represents the schema of an Arrow dataset or record batch.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenclass:: arrow::Schema\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Converting MATLAB Table to Arrow Table\nDESCRIPTION: Shows the process of creating a MATLAB table with numeric data and converting it to an Arrow table using the arrow.table function. This is the first step in serializing MATLAB data to file formats like Feather.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/doc/matlab_interface_for_apache_arrow_design.md#2025-04-16_snippet_1\n\nLANGUAGE: matlab\nCODE:\n```\n>> Weight = [10; 24; 10; 12; 18]; \n\n>> Radius = [80; 135; 65; 70; 150]; \n\n>> Density = [10.2; 20.5; 11.2; 13.7; 17.8]; \n\n% Create a MATLAB `table`\n>> T = table(Weight, Radius, Density);\n\n% Create an `arrow.tabular.Table` from the MATLAB `table`\n>> AT = arrow.table(T);\n```\n\n----------------------------------------\n\nTITLE: Adding Parquet Example Executables in CMake\nDESCRIPTION: Defines executable targets for various Parquet examples, including low-level API, Arrow integration, and stream API. It also sets include directories for some targets.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/parquet/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(parquet-low-level-example low_level_api/reader_writer.cc)\nadd_executable(parquet-low-level-example2 low_level_api/reader_writer2.cc)\nadd_executable(parquet-arrow-example parquet_arrow/reader_writer.cc)\nadd_executable(parquet-stream-api-example parquet_stream_api/stream_reader_writer.cc)\ntarget_include_directories(parquet-low-level-example PRIVATE low_level_api/)\ntarget_include_directories(parquet-low-level-example2 PRIVATE low_level_api/)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Timezone Database Path in Arrow C++\nDESCRIPTION: C++ code snippet demonstrating how to set a custom path for the timezone database at runtime using the ArrowGlobalOptions struct. This allows specifying a non-default location for the timezone data files.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/build_system.rst#2025-04-16_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\narrow::GlobalOptions options;\noptions.timezone_db_path = \"path/to/tzdata\";\nARROW_RETURN_NOT_OK(arrow::Initialize(options));\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow Gandiva expression compiler with CMake\nDESCRIPTION: This snippet shows how to enable the Gandiva expression compiler, depending on LLVM, Protocol Buffers, and re2.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_GANDIVA=ON\"\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Object Library Creation Function in CMake\nDESCRIPTION: Creates a function to generate object libraries for Arrow that optimize parallel builds by reducing dependencies on bundled libraries. Handles both shared and static builds with appropriate compile definitions and properties.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\n# This creates OBJECT libraries for arrow_shared/arrow_static. This is\n# not intended to use for other libraries such as\n# arrow_acero_shared/arrow_acero_static for now.\n#\n# arrow_shared/arrow_static depends on many external libraries such as\n# Zstandard and jemalloc. If we use bundled libraries, we can't start\n# building arrow_shared/arrow_static until all bundled libraries are\n# built. It prevent parallel build speedup.\n#\n# We can avoid the situation by creating small OBJECT libraries that\n# depend only needed external libraries. If an OBJECT library doesn't\n# depend on any bundled libraries, it can be built before bundled\n# libraries are built. If an OBJECT library depend on only a few\n# bundled libraries, it can be built after only they are built.\nfunction(arrow_add_object_library PREFIX)\n  set(SOURCES ${ARGN})\n  string(TOLOWER \"${PREFIX}\" prefix)\n  if(WIN32)\n    set(targets)\n    if(ARROW_BUILD_SHARED)\n      add_library(${prefix}_shared OBJECT ${SOURCES})\n      set_target_properties(${prefix}_shared PROPERTIES POSITION_INDEPENDENT_CODE ON)\n      target_compile_definitions(${prefix}_shared PRIVATE ARROW_EXPORTING)\n      target_compile_features(${prefix}_shared PRIVATE cxx_std_17)\n      set(${PREFIX}_TARGET_SHARED\n          ${prefix}_shared\n          PARENT_SCOPE)\n      list(APPEND targets ${prefix}_shared)\n    endif()\n    if(ARROW_BUILD_STATIC)\n      add_library(${prefix}_static OBJECT ${SOURCES})\n      set_target_properties(${prefix}_static PROPERTIES POSITION_INDEPENDENT_CODE ON)\n      target_compile_definitions(${prefix}_static PRIVATE ARROW_STATIC)\n      target_compile_features(${prefix}_static PRIVATE cxx_std_17)\n      set(${PREFIX}_TARGET_STATIC\n          ${prefix}_static\n          PARENT_SCOPE)\n      list(APPEND targets ${prefix}_static)\n    endif()\n    set(${PREFIX}_TARGETS\n        ${targets}\n        PARENT_SCOPE)\n  else()\n    add_library(${prefix} OBJECT ${SOURCES})\n    set_target_properties(${prefix} PROPERTIES POSITION_INDEPENDENT_CODE ON)\n    target_compile_features(${prefix} PRIVATE cxx_std_17)\n    set(${PREFIX}_TARGET_SHARED\n        ${prefix}\n        PARENT_SCOPE)\n    set(${PREFIX}_TARGET_STATIC\n        ${prefix}\n        PARENT_SCOPE)\n    set(${PREFIX}_TARGETS\n        ${prefix}\n        PARENT_SCOPE)\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Creating Zero-Copy Slices of Vectors in Java Arrow\nDESCRIPTION: This code demonstrates how to create a zero-copy slice of an IntVector using TransferPair, which allows referencing a logical sub-sequence of the data without copying the memory.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nIntVector vector = new IntVector(\"intVector\", allocator);\nfor (int i = 0; i < 10; i++) {\n  vector.setSafe(i, i);\n}\nvector.setValueCount(10);\n\nTransferPair tp = vector.getTransferPair(allocator);\ntp.splitAndTransfer(0, 5);\nIntVector sliced = (IntVector) tp.getTo();\n// In this case, the vector values are [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] and the sliceVector values are [0, 1, 2, 3, 4].\n```\n\n----------------------------------------\n\nTITLE: Defining BatchesWithSchema Structure in C++\nDESCRIPTION: Structure definition that holds record batches, schema and a generator function for streaming data execution.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/user_guide.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nstruct BatchesWithSchema {\n  std::vector<compute::ExecBatch> batches;\n  std::shared_ptr<Schema> schema;\n  std::function<Future<std::optional<compute::ExecBatch>>()> generator;\n```\n\n----------------------------------------\n\nTITLE: Analyzing Memory Leaks in Apache Arrow with Traceback Analysis\nDESCRIPTION: Python script that analyzes processed memory allocation events to find allocations without corresponding deallocations. It tracks memory addresses allocated and freed, then counts and displays the tracebacks of dangling allocations to help identify memory leaks.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/memory.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n'''Find tracebacks of allocations with no corresponding free'''\nimport sys\nimport json\nfrom collections import defaultdict\n\nallocated = dict()\n\nfor line in sys.stdin:\n    line = line.rstrip('\\n')\n    data = json.loads(line)\n\n    if data['event'] == \"probe_libarrow:je_arrow_mallocx__return\":\n        address = data['params']['arg1']\n        allocated[address] = data['traceback']\n    elif data['event'] == \"probe_libarrow:je_arrow_rallocx\":\n        address = data['params']['ptr']\n        del allocated[address]\n    elif data['event'] == \"probe_libarrow:je_arrow_rallocx__return\":\n        address = data['params']['arg1']\n        allocated[address] = data['traceback']\n    elif data['event'] == \"probe_libarrow:je_arrow_dallocx\":\n        address = data['params']['ptr']\n        if address in allocated:\n            del allocated[address]\n    elif data['event'] == \"probe_libarrow:mi_malloc_aligned__return\":\n        address = data['params']['arg1']\n        allocated[address] = data['traceback']\n    elif data['event'] == \"probe_libarrow:mi_realloc_aligned\":\n        address = data['params']['p']\n        del allocated[address]\n    elif data['event'] == \"probe_libarrow:mi_realloc_aligned__return\":\n        address = data['params']['arg1']\n        allocated[address] = data['traceback']\n    elif data['event'] == \"probe_libarrow:mi_free\":\n        address = data['params']['p']\n        if address in allocated:\n            del allocated[address]\n\ntraceback_counts = defaultdict(int)\n\nfor traceback in allocated.values():\n    traceback_counts[traceback] += 1\n\nfor traceback, count in sorted(traceback_counts.items(), key=lambda x: -x[1]):\n    print(\"Num of dangling allocations:\", count)\n    print(traceback)\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC Build Options in PyArrow CMake\nDESCRIPTION: Sets up ORC (Optimized Row Columnar) support in PyArrow, verifying Arrow C++ ORC support and configuring Cython extensions.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nif(PYARROW_BUILD_ORC)\n  message(STATUS \"Building PyArrow with ORC\")\n  if(NOT ARROW_ORC)\n    message(FATAL_ERROR \"You must build Arrow C++ with ARROW_ORC=ON\")\n  endif()\n  list(APPEND CYTHON_EXTENSIONS _orc)\n  if(PYARROW_BUILD_DATASET)\n    list(APPEND CYTHON_EXTENSIONS _dataset_orc)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating a comprehensive dtype mapping for Arrow to pandas nullable types\nDESCRIPTION: Shows how to create a complete mapping dictionary from Arrow data types to pandas nullable dtypes. This mapping can be used with the types_mapper parameter to ensure optimal type preservation when converting Arrow data to pandas.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/pandas.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndtype_mapping = {\n    pa.int8(): pd.Int8Dtype(),\n    pa.int16(): pd.Int16Dtype(),\n    pa.int32(): pd.Int32Dtype(),\n    pa.int64(): pd.Int64Dtype(),\n    pa.uint8(): pd.UInt8Dtype(),\n    pa.uint16(): pd.UInt16Dtype(),\n    pa.uint32(): pd.UInt32Dtype(),\n    pa.uint64(): pd.UInt64Dtype(),\n    pa.bool_(): pd.BooleanDtype(),\n    pa.float32(): pd.Float32Dtype(),\n    pa.float64(): pd.Float64Dtype(),\n    pa.string(): pd.StringDtype(),\n}\n\ndf = table.to_pandas(types_mapper=dtype_mapping.get)\n```\n\n----------------------------------------\n\nTITLE: Building the MATLAB Interface with CMake\nDESCRIPTION: Commands to build the MATLAB interface to Apache Arrow using CMake, generating build files and compiling the source code.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ cmake -S . -B build\n$ cmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Invoking Python Function from R using reticulate\nDESCRIPTION: This R code demonstrates how to use the reticulate library to invoke a Python function (pyarrow.compute.add) from R on an Arrow Array created in R.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_r.rst#2025-04-16_snippet_3\n\nLANGUAGE: R\nCODE:\n```\n# Load arrow and reticulate libraries\nlibrary(arrow)\nlibrary(reticulate)\n\n# Create a new array in R\na <- Array$create(c(1, 2, 3))\n\n# Make pyarrow.compute available to R\npc <- import(\"pyarrow.compute\")\n\n# Invoke pyarrow.compute.add with the array and 3\n# This will add 3 to all elements of the array and return a new Array\nresult <- pc$add(a, 3)\n\n# Print the result to confirm it's what we expect\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Representing Non-null Int32 Array in Arrow Format\nDESCRIPTION: Shows two possible memory layouts for a non-null Int32 array [1, 2, 3, 4, 8] in Arrow format, with and without a validity bitmap.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n* Length: 5, Null count: 0\n* Validity bitmap buffer:\n\n  | Byte 0 (validity bitmap) | Bytes 1-63            |\n  |--------------------------|-----------------------|\n  | 00011111                 | 0 (padding)           |\n\n* Value Buffer:\n\n  | Bytes 0-3   | Bytes 4-7   | Bytes 8-11  | Bytes 12-15 | Bytes 16-19 | Bytes 20-63           |\n  |-------------|-------------|-------------|-------------|-------------|-----------------------|\n  | 1           | 2           | 3           | 4           | 8           | unspecified (padding) |\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n* Length 5, Null count: 0\n* Validity bitmap buffer: Not required\n* Value Buffer:\n\n  | Bytes 0-3   | Bytes 4-7   | Bytes 8-11  | bytes 12-15 | bytes 16-19 | Bytes 20-63           |\n  |-------------|-------------|-------------|-------------|-------------|-----------------------|\n  | 1           | 2           | 3           | 4           | 8           | unspecified (padding) |\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Timezone Database Path in PyArrow\nDESCRIPTION: Python code to set a custom path for the timezone database, required for Windows systems\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/install.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow as pa\n>>> pa.set_timezone_db_path(\"custom_path\")\n```\n\n----------------------------------------\n\nTITLE: Defining ArrowDeviceArrayStream Structure in C\nDESCRIPTION: Defines the ArrowDeviceArrayStream structure for handling device-specific streaming data. It includes callbacks for schema retrieval, data iteration, error handling, and resource management.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDeviceDataInterface.rst#2025-04-16_snippet_3\n\nLANGUAGE: C\nCODE:\n```\n#ifndef ARROW_C_DEVICE_STREAM_INTERFACE\n#define ARROW_C_DEVICE_STREAM_INTERFACE\n\nstruct ArrowDeviceArrayStream {\n    // device type that all arrays will be accessible from\n    ArrowDeviceType device_type;\n    // callbacks\n    int (*get_schema)(struct ArrowDeviceArrayStream*, struct ArrowSchema*);\n    int (*get_next)(struct ArrowDeviceArrayStream*, struct ArrowDeviceArray*);\n    const char* (*get_last_error)(struct ArrowDeviceArrayStream*);\n\n    // release callback\n    void (*release)(struct ArrowDeviceArrayStream*);\n\n    // opaque producer-specific data\n    void* private_data;\n};\n\n#endif  // ARROW_C_DEVICE_STREAM_INTERFACE\n```\n\n----------------------------------------\n\nTITLE: Getting Primary Keys with CommandGetPrimaryKeys\nDESCRIPTION: The CommandGetPrimaryKeys command is used to fetch the primary keys of a specified table. This command is essential for obtaining unique identifiers within a table.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_5\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandGetPrimaryKeys\"\n```\n\n----------------------------------------\n\nTITLE: Installing the MATLAB Interface\nDESCRIPTION: Command to install the MATLAB interface to the default software installation location for the target machine, which also adds the installation directory to the MATLAB Search Path.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ cmake --build build --config Release --target install\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow FieldRef Class in C++\nDESCRIPTION: This snippet documents the arrow::FieldRef class, which is used for referencing fields in an Arrow schema.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_17\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenclass:: arrow::FieldRef\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Fibonacci Array Generator Using Arrow Type Traits\nDESCRIPTION: Template function that creates an array of Fibonacci numbers for any Arrow numeric type using type traits.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/datatypes.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename DataType,\n          typename BuilderType = typename arrow::TypeTraits<DataType>::BuilderType,\n          typename ArrayType = typename arrow::TypeTraits<DataType>::ArrayType,\n          typename CType = typename arrow::TypeTraits<DataType>::CType>\narrow::Result<std::shared_ptr<ArrayType>> MakeFibonacci(int32_t n) {\n  BuilderType builder;\n  CType val = 0;\n  CType next_val = 1;\n  for (int32_t i = 0; i < n; ++i) {\n    builder.Append(val);\n    CType temp = val + next_val;\n    val = next_val;\n    next_val = temp;\n  }\n  std::shared_ptr<ArrayType> out;\n  ARROW_RETURN_NOT_OK(builder.Finish(&out));\n  return out;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining List Field in Arrow Schema JSON\nDESCRIPTION: JSON representation of a nullable list of int32 values in the Arrow schema. Demonstrates the structure for nested types with a 'children' array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"list_nullable\",\n  \"type\": {\n    \"name\": \"list\"\n  },\n  \"nullable\": true,\n  \"children\": [\n    {\n      \"name\": \"item\",\n      \"type\": {\n        \"name\": \"int\",\n        \"isSigned\": true,\n        \"bitWidth\": 32\n      },\n      \"nullable\": true,\n      \"children\": []\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Unit Tests\nDESCRIPTION: Navigates to the Python directory in the Arrow repository and runs the PyArrow unit test suite using pytest.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ pushd arrow/python\n$ python -m pytest pyarrow\n$ popd\n```\n\n----------------------------------------\n\nTITLE: R Functions for C Data Interface Integration\nDESCRIPTION: This R code defines two functions: 'addthree_cdata' which imports an Arrow Array from C Data Interface and 'addthree' which adds 3 to all elements of an array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_r.rst#2025-04-16_snippet_4\n\nLANGUAGE: R\nCODE:\n```\nlibrary(arrow)\n\naddthree_cdata <- function(array_ptr_s, schema_ptr_s) {\n    a <- Array$import_from_c(array_ptr, schema_ptr)\n\n    return(addthree(a))\n}\n\naddthree <- function(arr) {\n    return(arr + 3L)\n}\n```\n\n----------------------------------------\n\nTITLE: Maven Build and Execute Commands - Console\nDESCRIPTION: Console commands for building the Java project with Maven, copying dependencies, and executing the Python demo script. Shows the data exchange output including JSON representation and Arrow table structure.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_17\n\nLANGUAGE: console\nCODE:\n```\n$ mvn package\n$ mvn org.apache.maven.plugins:maven-dependency-plugin:2.7:copy-dependencies -DoutputDirectory=dependencies\n$ python demo.py\nJSON file written by Java:\n{\"schema\":{\"fields\":[{\"name\":\"ints\",\"nullable\":true,\"type\":{\"name\":\"int\",\"bitWidth\":64,\"isSigned\":true},\"children\":[]},{\"name\":\"strs\",\"nullable\":true,\"type\":{\"name\":\"utf8\"},\"children\":[]}]},\"batches\":[{\"count\":4,\"columns\":[{\"name\":\"ints\",\"count\":4,\"VALIDITY\":[1,1,1,1],\"DATA\":[\"0\",\"2\",\"4\",\"8\"]},{\"name\":\"strs\",\"count\":4,\"VALIDITY\":[1,1,1,0],\"OFFSET\":[0,1,2,3,3],\"DATA\":[\"a\",\"b\",\"c\",\"\"]}]},{\"count\":4,\"columns\":[{\"name\":\"ints\",\"count\":4,\"VALIDITY\":[0,1,1,0],\"DATA\":[\"0\",\"32\",\"64\",\"0\"]},{\"name\":\"strs\",\"count\":4,\"VALIDITY\":[1,0,0,1],\"OFFSET\":[0,1,1,1,2],\"DATA\":[\"e\",\"\",\"\",\"h\"]}]}]}\nIPC file read by Java:\npyarrow.Table\nints: int64\nstrs: string\n----\nints: [[0,2,4,8],[null,32,64,null]]\nstrs: [[\"a\",\"b\",\"c\",null],[\"e\",null,null,\"h\"]]\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow Flight SQL with CMake\nDESCRIPTION: This snippet shows how to enable the Arrow Flight SQL by passing a boolean flag to CMake.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_FLIGHT_SQL=ON\"\n```\n\n----------------------------------------\n\nTITLE: Writing ORC Files with Compression\nDESCRIPTION: Demonstrates different compression options available when writing ORC files including uncompressed, zlib, zstd, and snappy.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/orc.rst#2025-04-16_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\norc.write_table(table, where, compression='uncompressed')\norc.write_table(table, where, compression='zlib')\norc.write_table(table, where, compression='zstd')\norc.write_table(table, where, compression='snappy')\n```\n\n----------------------------------------\n\nTITLE: Implementing InputFinished Method in C++ ExecNode\nDESCRIPTION: The InputFinished method is called once per input when a node has finished sending data. It's used to signal the end of data stream and may trigger final processing in some nodes.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/developer_guide.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nExecNode::InputFinished()\n```\n\n----------------------------------------\n\nTITLE: Trace Output from Apache Arrow Memory Leak Detection\nDESCRIPTION: This shows the detailed stack trace output when a memory leak is detected in Apache Arrow. The trace includes information about the allocator state, buffer references, and the complete stack trace where the leaked buffer was created.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/memory.rst#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n|  Exception java.lang.IllegalStateException: Allocator[ROOT] closed with outstanding buffers allocated (1).\nAllocator(ROOT) 0/1024/1024/9223372036854775807 (res/actual/peak/limit)\n  child allocators: 0\n  ledgers: 1\n    ledger[1] allocator: ROOT), isOwning: , size: , references: 1, life: 712040870231544..0, allocatorManager: [, life: ] holds 1 buffers.\n        ArrowBuf[2], address:139926571810832, length:1024\n     event log for: ArrowBuf[2]\n       712040888650134 create()\n              at org.apache.arrow.memory.util.StackTrace.<init>(StackTrace.java:34)\n              at org.apache.arrow.memory.util.HistoricalLog$Event.<init>(HistoricalLog.java:175)\n              at org.apache.arrow.memory.util.HistoricalLog.recordEvent(HistoricalLog.java:83)\n              at org.apache.arrow.memory.ArrowBuf.<init>(ArrowBuf.java:96)\n              at org.apache.arrow.memory.BufferLedger.newArrowBuf(BufferLedger.java:271)\n              at org.apache.arrow.memory.BaseAllocator.bufferWithoutReservation(BaseAllocator.java:300)\n              at org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:276)\n              at org.apache.arrow.memory.RootAllocator.buffer(RootAllocator.java:29)\n              at org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:240)\n              at org.apache.arrow.memory.RootAllocator.buffer(RootAllocator.java:29)\n              at REPL.$JShell$18.do_it$($JShell$18.java:13)\n              at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)\n              at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n              at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n              at java.lang.reflect.Method.invoke(Method.java:566)\n              at jdk.jshell.execution.DirectExecutionControl.invoke(DirectExecutionControl.java:209)\n              at jdk.jshell.execution.RemoteExecutionControl.invoke(RemoteExecutionControl.java:116)\n              at jdk.jshell.execution.DirectExecutionControl.invoke(DirectExecutionControl.java:119)\n              at jdk.jshell.execution.ExecutionControlForwarder.processCommand(ExecutionControlForwarder.java:144)\n              at jdk.jshell.execution.ExecutionControlForwarder.commandLoop(ExecutionControlForwarder.java:262)\n              at jdk.jshell.execution.Util.forwardExecutionControl(Util.java:76)\n              at jdk.jshell.execution.Util.forwardExecutionControlAndIO(Util.java:137)\n\n  reservations: 0\n\n|        at BaseAllocator.close (BaseAllocator.java:405)\n|        at RootAllocator.close (RootAllocator.java:29)\n|        at (#8:1)\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Arrow Core Components with CMake\nDESCRIPTION: This snippet adds core subdirectories to the Apache Arrow project build. It includes essential components like array, compute, io, and util.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_26\n\nLANGUAGE: CMake\nCODE:\n```\n# Unconditionally install testing headers that are also useful for Arrow consumers.\nadd_subdirectory(testing)\n\nadd_subdirectory(array)\nadd_subdirectory(c)\nadd_subdirectory(compute)\nadd_subdirectory(extension)\nadd_subdirectory(io)\nadd_subdirectory(tensor)\nadd_subdirectory(util)\nadd_subdirectory(vendored)\n```\n\n----------------------------------------\n\nTITLE: Cloning the Arrow Repository\nDESCRIPTION: Clones the Apache Arrow GitHub repository to get the source code needed for development.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n$ git clone https://github.com/apache/arrow.git\n```\n\n----------------------------------------\n\nTITLE: Accessing IntVector Values in Java\nDESCRIPTION: This snippet demonstrates how to access a value from an IntVector using the get method.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nint value = vector.get(5);  // value == 25\n```\n\n----------------------------------------\n\nTITLE: Updating Arrow format flatbuffers generated code\nDESCRIPTION: This shell script demonstrates how to update the Arrow format flatbuffers generated code. It includes steps for modifying flatbuffers schemas, generating TypeScript source, and cleaning up temporary files.\nSOURCE: https://github.com/apache/arrow/blob/main/js/DEVELOP.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd $ARROW_HOME\n\n# Create a tmpdir to store modified flatbuffers schemas\ntmp_format_dir=$(mktemp -d)\ncp ./format/*.fbs $tmp_format_dir\n\n# Remove namespaces from the flatbuffers schemas\nsed -i '+s+namespace org.apache.arrow.flatbuf;++ig' $tmp_format_dir/*.fbs\nsed -i '+s+org.apache.arrow.flatbuf.++ig' $tmp_format_dir/*.fbs\n\n# Generate TS source from the modified Arrow flatbuffers schemas\nflatc --ts -o ./js/src/fb $tmp_format_dir/{File,Schema,Message,Tensor,SparseTensor}.fbs\n\n# Remove the tmpdir\nrm -rf $tmp_format_dir\n```\n\n----------------------------------------\n\nTITLE: Arrow to Document Converter Class\nDESCRIPTION: Class that converts Arrow RecordBatches and Tables to JSON documents. Implements batch processing and iteration patterns for efficient conversion.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/row_columnar_conversion.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nclass ArrowToDocumentConverter {\n // ... implementation details\n};\n```\n\n----------------------------------------\n\nTITLE: Equivalent Docker Compose Commands for Building\nDESCRIPTION: The sequence of Docker Compose commands that Archery executes when running a conda-python build.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose pull --ignore-pull-failures conda-cpp\ndocker compose pull --ignore-pull-failures conda-python\ndocker compose build conda-cpp\ndocker compose build conda-python\ndocker compose run --rm conda-python\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow Flight SQL Library Target in CMake\nDESCRIPTION: Adds the arrow_flight_sql library target with appropriate configuration settings, dependencies, link flags, and install settings. This creates both shared and static versions of the library based on build configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/sql/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_lib(arrow_flight_sql\n              CMAKE_PACKAGE_NAME\n              ArrowFlightSql\n              PKG_CONFIG_NAME\n              arrow-flight-sql\n              OUTPUTS\n              ARROW_FLIGHT_SQL_LIBRARIES\n              SOURCES\n              ${ARROW_FLIGHT_SQL_SRCS}\n              DEPENDENCIES\n              flight_sql_protobuf_gen\n              SHARED_LINK_FLAGS\n              ${ARROW_VERSION_SCRIPT_FLAGS} # Defined in cpp/arrow/CMakeLists.txt\n              SHARED_LINK_LIBS\n              arrow_flight_shared\n              SHARED_INSTALL_INTERFACE_LIBS\n              ArrowFlight::arrow_flight_shared\n              STATIC_LINK_LIBS\n              arrow_flight_static\n              STATIC_INSTALL_INTERFACE_LIBS\n              ArrowFlight::arrow_flight_static\n              PRIVATE_INCLUDES\n              \"${Protobuf_INCLUDE_DIRS}\")\n```\n\n----------------------------------------\n\nTITLE: JSON Value Converter Implementation\nDESCRIPTION: Class that handles converting JSON field values to Arrow arrays using the visitor pattern. Processes both primitive and nested data types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/row_columnar_conversion.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nclass JsonValueConverter {\n // ... implementation details\n};\n```\n\n----------------------------------------\n\nTITLE: Defining Async Device Stream Interface Structures in C\nDESCRIPTION: Defines structures for the Async Device Stream Interface, including ArrowAsyncTask, ArrowAsyncProducer, and ArrowAsyncDeviceStreamHandler. These structures facilitate asynchronous communication between producers and consumers of device-specific streaming data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDeviceDataInterface.rst#2025-04-16_snippet_4\n\nLANGUAGE: C\nCODE:\n```\n#ifndef ARROW_C_ASYNC_STREAM_INTERFACE\n#define ARROW_C_ASYNC_STREAM_INTERFACE\n\nstruct ArrowAsyncTask {\n  int (*extract_data)(struct ArrowArrayTask* self, struct ArrowDeviceArray* out);\n\n  void* private_data;\n};\n\nstruct ArrowAsyncProducer {\n  ArrowDeviceType device_type;\n\n  void (*request)(struct ArrowAsyncProducer* self, int64_t n);\n  void (*cancel)(struct ArrowAsyncProducer* self);\n\n  void (*release)(struct ArrowAsyncProducer* self);\n  const char* additional_metadata;\n  void* private_data;\n};\n\nstruct ArrowAsyncDeviceStreamHandler {\n  // consumer-specific handlers\n  int (*on_schema)(struct ArrowAsyncDeviceStreamHandler* self,\n                   struct ArrowSchema* stream_schema);\n  int (*on_next_task)(struct ArrowAsyncDeviceStreamHandler* self,\n                      struct ArrowAsyncTask* task, const char* metadata);\n  void (*on_error)(struct ArrowAsyncDeviceStreamHandler* self,\n                   int code, const char* message, const char* metadata);\n\n  // release callback\n  void (*release)(struct ArrowAsyncDeviceStreamHandler* self);\n\n  // must be populated before calling any callbacks\n  struct ArrowAsyncProducer* producer;\n\n  // opaque handler-specific data\n  void* private_data;\n};\n\n#endif  // ARROW_C_ASYNC_STREAM_INTERFACE\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Memory with Shell Commands\nDESCRIPTION: This command variant is necessary when using the arrow-dataset module, specifying additional JDK internals that need to be exposed to prevent inaccessible object exceptions during execution.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/install.rst#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ java --add-opens=java.base/java.nio=org.apache.arrow.memory.core,ALL-UNNAMED -jar ...\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ env JDK_JAVA_OPTIONS=\"--add-opens=java.base/java.nio=org.apache.arrow.dataset,org.apache.arrow.memory.core,ALL-UNNAMED\" java -jar ...\n```\n\n----------------------------------------\n\nTITLE: File Generation Helper Function for Arrow Example\nDESCRIPTION: Helper function to generate initial files for demonstration purposes using Arrow's data structures.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\narrow::Status GenInitialFile() {\\n  arrow::StringBuilder builder;\\n  ARROW_RETURN_NOT_OK(builder.Append(\"1/1/2018\"));\\n  ARROW_RETURN_NOT_OK(builder.Append(\"1/2/2018\"));\\n  ARROW_RETURN_NOT_OK(builder.Append(\"1/3/2018\"));\\n  std::shared_ptr<arrow::Array> array;\\n  ARROW_RETURN_NOT_OK(builder.Finish(&array));\\n  // Additional implementation details omitted for brevity\\n  return arrow::Status::OK();\\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring gRPC Client Reconnection Backoff in C++\nDESCRIPTION: Sets up a Flight client with a minimum time between subsequent connection attempts using gRPC options. This helps prevent aggressive reconnection attempts when servers are under load.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/flight.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nauto options = FlightClientOptions::Defaults();\n// Set the minimum time between subsequent connection attempts.\noptions.generic_options.emplace_back(GRPC_ARG_MIN_RECONNECT_BACKOFF_MS, 2000);\n```\n\n----------------------------------------\n\nTITLE: Executing Prepared Statements: ActionClosePreparedStatementRequest\nDESCRIPTION: The ActionClosePreparedStatementRequest command is used to close a previously created prepared statement. This is essential for resource management in database operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_9\n\nLANGUAGE: protobuf\nCODE:\n```\n\"ActionClosePreparedStatementRequest\"\n```\n\n----------------------------------------\n\nTITLE: Getting Type Enumeration ID from Arrow Type Object\nDESCRIPTION: Examples showing how to retrieve the type enumeration ID from different Arrow Type objects.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_13\n\nLANGUAGE: matlab\nCODE:\n```\n>> type.ID\n\nans =\n\n  ID enumeration\n\n    Timestamp\n\n>> type = arrow.string()\n\ntype =\n\n  StringType with properties:\n\n    ID: String\n\n>> type.ID\n\nans =\n\n  ID enumeration\n\n    String\n```\n\n----------------------------------------\n\nTITLE: Illustrating Declaration and ExecPlan Relationship\nDESCRIPTION: This figure demonstrates the relationship between Declaration and ExecPlan. It shows that a Declaration serves as a blueprint for instantiating ExecPlan instances.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/overview.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. figure:: decl_vs_ep.svg\n\n   A declaration is a blueprint that is used to instantiate exec plan instances\n```\n\n----------------------------------------\n\nTITLE: Binding VectorSchemaRoot Data with JdbcParameterBinder\nDESCRIPTION: This code snippet illustrates binding Arrow data from a VectorSchemaRoot to JDBC PreparedStatement parameters using JdbcParameterBinder. It demonstrates iterating through data rows and executing SQL updates, handling null values appropriately.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/jdbc.rst#2025-04-16_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nfinal JdbcParameterBinder binder =\\n    JdbcParameterBinder.builder(statement, root).bindAll().build();\\nwhile (binder.next()) {\\n    statement.executeUpdate();\\n}\\n// Use a VectorLoader to update the root\\nbinder.reset();\\nwhile (binder.next()) {\\n    statement.executeUpdate();\\n}\n```\n\n----------------------------------------\n\nTITLE: Using ORCWriter Class\nDESCRIPTION: Shows how to write an ORC file using the ORCWriter class with context manager.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/orc.rst#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nwith orc.ORCWriter('example2.orc') as writer:\n    writer.write(table)\n```\n\n----------------------------------------\n\nTITLE: Running Docstring Tests in PyArrow Cython Files\nDESCRIPTION: Executes pytest with the doctest-cython flag to verify docstring examples in PyArrow's .pyx and .pxi files are correct, requiring the pytest-cython plugin.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n$ pushd arrow/python\n$ python -m pytest --doctest-cython\n$ python -m pytest --doctest-cython path/to/module.pyx # checking single file\n$ popd\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow Schema Factory Functions in C++\nDESCRIPTION: This snippet documents the schema factory functions used for creating Arrow schemas.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygengroup:: schema-factories\n   :content-only:\n```\n\n----------------------------------------\n\nTITLE: Implementing PauseProducing and ResumeProducing Methods in C++ ExecNode\nDESCRIPTION: These methods control backpressure in the data pipeline. Nodes call PauseProducing on their input when they need to slow down data flow, and ResumeProducing when they're ready for more data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/developer_guide.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nExecNode::PauseProducing()\nExecNode::ResumeProducing()\n```\n\n----------------------------------------\n\nTITLE: Build Apache Arrow on MSYS2\nDESCRIPTION: Instructions for building Apache Arrow using MSYS2 makefiles in a variety of shell environments including MSYS2 terminal, cmd.exe, or PowerShell terminal.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ncd cpp\nmkdir build\ncd build\ncmake -G \"MSYS Makefiles\" ..\nmake\n```\n\n----------------------------------------\n\nTITLE: Specifying Projections and Filters Using Substrait in Apache Arrow\nDESCRIPTION: This Java snippet shows how to specify new columns or filters using Substrait expressions in Apache Arrow's Dataset API. Users can serialize expressions into ByteBuffer objects and set them in ScanOptions. This requires the use of Substrait libraries and knowledge of Apache Arrow in Java.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/dataset.rst#2025-04-16_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nByteBuffer substraitExpressionFilter = getSubstraitExpressionFilter();\nByteBuffer substraitExpressionProject = getSubstraitExpressionProjection();\n// Use Substrait APIs to create an Expression and serialize to a ByteBuffer\nScanOptions options = new ScanOptions.Builder(/*batchSize*/ 32768)\n             .columns(Optional.empty())\n             .substraitExpressionFilter(substraitExpressionFilter)\n             .substraitExpressionProjection(getSubstraitExpressionProjection())\n             .build();\n\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Build with conda using Docker Compose on Ubuntu\nDESCRIPTION: Command to run the conda-based build process for PyArrow using Docker Compose with the Ubuntu container.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose run --rm minimal-ubuntu-conda\n```\n\n----------------------------------------\n\nTITLE: Parquet File Metadata Inspection\nDESCRIPTION: Shows how to access and inspect Parquet file metadata using ParquetFile and read_metadata.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/parquet.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nparquet_file = pq.ParquetFile('example.parquet')\nmetadata = parquet_file.metadata\n\nmetadata = pq.read_metadata('example.parquet')\nmetadata.row_group(0)\nmetadata.row_group(0).column(0)\n```\n\n----------------------------------------\n\nTITLE: Calculating Sum over Arrow Array in C++\nDESCRIPTION: Uses the Sum() convenience function to calculate the sum of a column in the Arrow Table.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nARROW_ASSIGN_OR_RAISE(sum_result,\n                     arrow::compute::Sum(table->GetColumnByName(\"A\")));\n```\n\n----------------------------------------\n\nTITLE: Implementing RunMain Function for Arrow Dataset Operations in C++\nDESCRIPTION: This function implements the main logic for the Arrow dataset example. It prepares the environment and performs dataset operations, returning an arrow::Status to indicate success or failure.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\narrow::Status RunMain(int argc, char** argv) {\n  arrow::Status st;\n\n  ARROW_RETURN_NOT_OK(PrepareEnv());\n\n  // Dataset operations will be implemented here\n\n  return arrow::Status::OK();\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Arrow Test Data\nDESCRIPTION: Updates Git submodules to get test data and sets environment variables needed for Arrow and Parquet tests.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n$ pushd arrow\n$ git submodule update --init\n$ export PARQUET_TEST_DATA=\"${PWD}/cpp/submodules/parquet-testing/data\"\n$ export ARROW_TEST_DATA=\"${PWD}/testing/data\"\n$ popd\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding Integration Tests in Apache Arrow CMake\nDESCRIPTION: This snippet conditionally adds the integration subdirectory to the build if either ARROW_BUILD_INTEGRATION or ARROW_BUILD_TESTS is enabled. It ensures JSON integration machinery is tested in various builds.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_27\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_INTEGRATION OR ARROW_BUILD_TESTS)\n  # We build tests for the JSON integration machinery even if integration\n  # is not enabled, to ensure it's exercised in more builds than just the\n  # integration build.\n  add_subdirectory(integration)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on macOS with Homebrew\nDESCRIPTION: Commands to clone Arrow repository and install dependencies using Homebrew package manager on macOS.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/apache/arrow.git\ncd arrow\nbrew update && brew bundle --file=cpp/Brewfile\n```\n\n----------------------------------------\n\nTITLE: Defining Complex Array Schema and Data in Apache Arrow\nDESCRIPTION: This snippet defines a complex array schema with nested types and provides sample data. It includes a struct type with an integer, a list of integers, and a float field.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/StatisticsSchema.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nSchema::\n\n    struct<a: int32, b: list<item: int64>, c: float64>\n\nData::\n\n    [\n      {a: 1, b: [20, 30, 40], c: 2.9},\n      {a: 2, b: null,         c: -2.9},\n      {a: 3, b: [99],         c: null},\n    ]\n```\n\n----------------------------------------\n\nTITLE: Dependency Version Requirements for Apache Arrow\nDESCRIPTION: Comprehensive list of dependencies with their version specifications required for building Apache Arrow. Includes cloud provider SDKs, testing frameworks, compression libraries, and development tools. Each dependency is listed with its minimum version requirement where applicable.\nSOURCE: https://github.com/apache/arrow/blob/main/ci/conda_env_cpp.txt#2025-04-16_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\naws-sdk-cpp=1.11.68\nazure-core-cpp>=1.10.3\nazure-identity-cpp>=1.6.0\nazure-storage-blobs-cpp>=12.10.0\nazure-storage-common-cpp>=12.5.0\nazure-storage-files-datalake-cpp>=12.9.0\nbenchmark>=1.6.0,!=1.8.4\nboost-cpp>=1.68.0\nbrotli\nbzip2\nc-ares\ncmake\ngflags\nglog\ngmock>=1.10.0\ngoogle-cloud-cpp>=1.34.0\ngrpc-cpp<=1.50.1\ngtest>=1.10.0\nlibprotobuf\nlibutf8proc\nlz4-c\nmake\nmeson\nninja\nnodejs\norc\npkg-config\npython\nrapidjson\nre2\nsnappy\nthrift-cpp>=0.11.0\nxsimd\nzlib\nzstd\n```\n\n----------------------------------------\n\nTITLE: Reading ORC Files from Cloud Storage\nDESCRIPTION: Shows how to read ORC files from cloud storage (S3) using PyArrow's filesystem interface.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/orc.rst#2025-04-16_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom pyarrow import fs\n\ns3 = fs.S3FileSystem(region=\"us-east-2\")\ntable = orc.read_table(\"bucket/object/key/prefix\", filesystem=s3)\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow Package using Conda\nDESCRIPTION: Command to install the R arrow package through conda-forge channel using conda package manager.\nSOURCE: https://github.com/apache/arrow/blob/main/r/README.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nconda install -c conda-forge --strict-channel-priority r-arrow\n```\n\n----------------------------------------\n\nTITLE: Implementing Dictionary Encoding in Java Arrow\nDESCRIPTION: This code demonstrates the process of dictionary encoding a vector for improved memory efficiency. It shows creating a dictionary, encoding an original vector, and decoding it back when needed.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\n// 1. create a vector for the un-encoded data and populate it\nVarCharVector unencoded = new VarCharVector(\"unencoded\", allocator);\n// now put some data in it before continuing\n\n// 2. create a vector to hold the dictionary and populate it\nVarCharVector dictionaryVector = new VarCharVector(\"dictionary\", allocator);\n\n// 3. create a dictionary object\nDictionary dictionary = new Dictionary(dictionaryVector, new DictionaryEncoding(1L, false, null));\n\n// 4. create a dictionary encoder\nDictionaryEncoder encoder = new DictionaryEncoder.encode(dictionary, allocator);\n\n// 5. encode the data\nIntVector encoded = (IntVector) encoder.encode(unencoded);\n\n// 6. re-create an un-encoded version from the encoded vector\nVarCharVector decoded = (VarCharVector) encoder.decode(encoded);\n```\n\n----------------------------------------\n\nTITLE: Initializing ExecNode in C++\nDESCRIPTION: Describes the initialization process for ExecNodes, including constructor, factory method, Init, and Validate methods. Emphasizes the preferred approaches for different initialization scenarios.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/developer_guide.rst#2025-04-16_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n// Simple initialization in constructor\nExecNode::ExecNode() {\n  // Simple, non-error-prone initialization\n}\n\n// Factory method for validation\nResult<std::unique_ptr<ExecNode>> ExecNode::Make() {\n  // Validation logic\n  return Status::OK();\n}\n\n// Init method for expensive initialization\nStatus ExecNode::Init() {\n  // Expensive allocation or resource consumption\n  return Status::OK();\n}\n\n// Custom validation\nStatus ExecNode::Validate() {\n  // Custom validation logic\n  return Status::OK();\n}\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Extension Metadata in Arrow\nDESCRIPTION: Demonstrates the metadata format for the Arrow JSON extension type. The metadata is either an empty string or a JSON string with an empty object.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CanonicalExtensions.rst#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{}\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Custom Targets in CMake\nDESCRIPTION: Creates main custom targets for the Arrow project including arrow-all (main target), arrow, arrow-benchmarks, arrow-tests, and arrow-integration. The arrow-all target depends on all other targets.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(arrow-all)\nadd_custom_target(arrow)\nadd_custom_target(arrow-benchmarks)\nadd_custom_target(arrow-tests)\nadd_custom_target(arrow-integration)\nadd_dependencies(arrow-all\n                 arrow\n                 arrow-tests\n                 arrow-benchmarks\n                 arrow-integration)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Async I/O Operations in Apache Arrow\nDESCRIPTION: A sequence diagram showing the interaction flow between Thread Pool, CPU, and I/O components during asynchronous operations. The diagram demonstrates task scheduling, I/O reads, future handling, and continuation processing.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/async.md#2025-04-16_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    %% This is the source diagram for async.svg\n    %% included here for future doc maintainers\n    Thread Pool->>+CPU: Start Task\n    CPU->>+IO: Read\n    IO->>+CPU: Future<Buffer>\n    activate IO\n    CPU->>+CPU: Add Continuation\n    CPU->>+Thread Pool: Finish Task\n    Note right of IO: Blocked on IO\n    Thread Pool->>+CPU: Other Task\n    CPU->>+Thread Pool:\n    Thread Pool->>+CPU: Other Task\n    CPU->>+Thread Pool:\n    Thread Pool->>+CPU: Other Task\n    CPU->>+Thread Pool:\n    deactivate IO\n    IO->>+IO: Read Finished\n    IO->>+IO: Run Continuation\n    IO->>+Thread Pool: Schedule Task\n    Thread Pool->>+CPU: Start Task\n    CPU->>+CPU: Process Read Result\n```\n\n----------------------------------------\n\nTITLE: Configuring Backtrace Support for Arrow Library\nDESCRIPTION: Adds backtrace support to Arrow libraries if enabled during configuration. Finds the Backtrace package and adds appropriate compile definitions to the Arrow targets.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_21\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_WITH_BACKTRACE)\n  find_package(Backtrace)\n\n  foreach(LIB_TARGET ${ARROW_LIBRARIES})\n    if(Backtrace_FOUND AND ARROW_WITH_BACKTRACE)\n      target_compile_definitions(${LIB_TARGET} PRIVATE ARROW_WITH_BACKTRACE)\n    endif()\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Listing Tables with CommandGetTables\nDESCRIPTION: The CommandGetTables command lists all the tables present in the database. This command assists in exploring the database structure and available data entities.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_7\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandGetTables\"\n```\n\n----------------------------------------\n\nTITLE: Run-End-Encoded Memory Layout\nDESCRIPTION: Detailed memory layout example showing run_ends and values arrays with their respective buffer structures.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n* Length: 7, Null count: 0\n* Child Arrays:\n\n  * run_ends (Int32):\n    * Length: 3, Null count: 0 (Run Ends cannot be null)\n    * Validity bitmap buffer: Not required (if it exists, it should be all 1s)\n    * Values buffer\n\n      | Bytes 0-3   | Bytes 4-7   | Bytes 8-11  | Bytes 12-63           |\n      |-------------|-------------|-------------|-----------------------|\n      | 4           | 6           | 7           | unspecified (padding) |\n\n  * values (Float32):\n    * Length: 3, Null count: 1\n    * Validity bitmap buffer:\n\n      | Byte 0 (validity bitmap) | Bytes 1-63            |\n      |--------------------------|-----------------------|\n      | 00000101                 | 0 (padding)           |\n\n    * Values buffer\n\n      | Bytes 0-3   | Bytes 4-7   | Bytes 8-11  | Bytes 12-63           |\n      |-------------|-------------|-------------|-----------------------|\n      | 1.0         | unspecified | 2.0         | unspecified (padding) |\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Statistics Array Structure in YAML\nDESCRIPTION: A YAML representation of an Arrow statistics array structure that shows how column statistics are stored and referenced. The structure includes column indices, offset positions, statistic keys, and actual statistic values stored in different data types (int64 and float64).\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/StatisticsSchema.rst#2025-04-16_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nStatistics array::\n\n    column: [\n      0, # array\n      1, # a\n      2, # b\n      3, # b.item\n      4, # c\n    ]\n    statistics:\n      offsets: [\n        0,\n        2,  # array: 2 values: [0, 1]\n        6,  # a: 4 values: [2, 3, 4, 5]\n        7,  # b: 1 value: [6]\n        9,  # b.item: 2 values: [7, 8]\n        12, # c: 3 values: [9, 10, 11]\n      ]\n      key:\n        values: [\n          \"ARROW:row_count:exact\",\n          \"ARROW:null_count:exact\",\n          \"ARROW:distinct_count:exact\",\n          \"ARROW:max_value:approximate\",\n          \"ARROW:min_value:approximate\",\n          \"ARROW:max_value:exact\",\n          \"ARROW:min_value:exact\",\n        ]\n        indices: [\n          0, # \"ARROW:row_count:exact\"\n          1, # \"ARROW:null_count:exact\"\n          1, # \"ARROW:null_count:exact\"\n          2, # \"ARROW:distinct_count:exact\"\n          3, # \"ARROW:max_value:approximate\"\n          4, # \"ARROW:min_value:approximate\"\n          1, # \"ARROW:null_count:exact\"\n          5, # \"ARROW:max_value:exact\"\n          6, # \"ARROW:min_value:exact\"\n          1, # \"ARROW:null_count:exact\"\n          3, # \"ARROW:max_value:approximate\"\n          4, # \"ARROW:min_value:approximate\"\n        ]\n      items:\n        children:\n          0: [ # int64\n            3,  # array: \"ARROW:row_count:exact\"\n            0,  # array: \"ARROW:null_count:exact\"\n            0,  # a: \"ARROW:null_count:exact\"\n            3,  # a: \"ARROW:distinct_count:exact\"\n            5,  # a: \"ARROW:max_value:approximate\"\n            0,  # a: \"ARROW:min_value:approximate\"\n            1,  # b: \"ARROW:null_count:exact\"\n            99, # b.item: \"ARROW:max_value:exact\"\n            20, # b.item: \"ARROW:min_value:exact\"\n            1,  # c: \"ARROW:null_count:exact\"\n          ]\n          1: [ # float64\n            3.0,  # c: \"ARROW:max_value:approximate\"\n            -3.0, # c: \"ARROW:min_value:approximate\"\n          ]\n        types: [\n          0, # int64: array: \"ARROW:row_count:exact\"\n          0, # int64: array: \"ARROW:null_count:exact\"\n          0, # int64: a: \"ARROW:null_count:exact\"\n          0, # int64: a: \"ARROW:distinct_count:exact\"\n          0, # int64: a: \"ARROW:max_value:approximate\"\n          0, # int64: a: \"ARROW:min_value:approximate\"\n          0, # int64: b: \"ARROW:null_count:exact\"\n          0, # int64: b.item: \"ARROW:max_value:exact\"\n          0, # int64: b.item: \"ARROW:min_value:exact\"\n          0, # int64: c: \"ARROW:null_count:exact\"\n          1, # float64: c: \"ARROW:max_value:approximate\"\n          1, # float64: c: \"ARROW:min_value:approximate\"\n        ]\n        offsets: [\n          0, # int64: array: \"ARROW:row_count:exact\"\n          1, # int64: array: \"ARROW:null_count:exact\"\n          2, # int64: a: \"ARROW:null_count:exact\"\n          3, # int64: a: \"ARROW:distinct_count:exact\"\n          4, # int64: a: \"ARROW:max_value:approximate\"\n          5, # int64: a: \"ARROW:min_value:approximate\"\n          6, # int64: b: \"ARROW:null_count:exact\"\n          7, # int64: b.item: \"ARROW:max_value:exact\"\n          8, # int64: b.item: \"ARROW:min_value:exact\"\n          9, # int64: c: \"ARROW:null_count:exact\"\n          0, # float64: c: \"ARROW:max_value:approximate\"\n          1, # float64: c: \"ARROW:min_value:approximate\"\n        ]\n```\n\n----------------------------------------\n\nTITLE: Converting Arrow RecordBatch to MATLAB Table\nDESCRIPTION: Example showing how to convert an Arrow RecordBatch back to a MATLAB table using the table function.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_8\n\nLANGUAGE: matlab\nCODE:\n```\n>> arrowRecordBatch\n\narrowRecordBatch =\n\nVar1:   [\n    \"A\",\n    \"B\",\n    \"C\"\n  ]\nVar2:   [\n    1,\n    2,\n    3\n  ]\nVar3:   [\n    true,\n    false,\n    true\n  ]\n\n>> matlabTable = table(arrowRecordBatch)\n\nmatlabTable =\n\n  3x3 table\n\n    Var1    Var2    Var3\n    ____    ____    _____\n\n    \"A\"      1      true\n    \"B\"      2      false\n    \"C\"      3      true\n```\n\n----------------------------------------\n\nTITLE: Registering External IR Functions from Bitcode Buffer in Gandiva\nDESCRIPTION: API for registering a set of external IR functions from a preloaded bitcode buffer into the Gandiva function registry, requiring function metadata and the bitcode buffer.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gandiva/external_func.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// Registers a set of functions from a bitcode buffer\narrow::Status Register(const std::vector<NativeFunction>& funcs,\n                     std::shared_ptr<arrow::Buffer> bitcode_buffer);\n```\n\n----------------------------------------\n\nTITLE: Formatting Python and C++ Code with Archery\nDESCRIPTION: Uses Archery to lint Python code and format C++ files in the PyArrow codebase using clang-format, with automatic fixing of style issues.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n$ archery lint --python --clang-format --fix\n```\n\n----------------------------------------\n\nTITLE: Building Arrow C++ with CMake and Ninja\nDESCRIPTION: Configures and builds the Arrow C++ libraries using CMake with the ninja-release-python preset, installing them to the specified directory.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n$ cmake -S arrow/cpp -B arrow/cpp/build \\\n         -DCMAKE_INSTALL_PREFIX=$ARROW_HOME \\\n         --preset ninja-release-python\n$ cmake --build arrow/cpp/build --target install\n```\n\n----------------------------------------\n\nTITLE: Batch Build Script for MSYS2\nDESCRIPTION: Batch file for setting up and building Apache Arrow on Windows using MSYS2 with specific build conditions for 64-bit systems. The script automates environment variable setting and build process.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_14\n\nLANGUAGE: batch\nCODE:\n```\nsetlocal\n\nREM For 64bit\nset MINGW_PACKAGE_PREFIX=mingw-w64-x86_64\nset MINGW_PREFIX=c:\\msys64\\mingw64\nset MSYSTEM=MINGW64\n\nset PATH=%MINGW_PREFIX%\\bin;c:\\msys64\\usr\\bin;%PATH%\n\nrmdir /S /Q cpp\\build\nmkdir cpp\\build\npushd cpp\\build\ncmake -G \"MSYS Makefiles\" .. || exit /B\nmake || exit /B\npopd\n```\n\n----------------------------------------\n\nTITLE: Importing pyarrow module in Python\nDESCRIPTION: This snippet sets the current module to pyarrow, which is the Python library for Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/files.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: pyarrow\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Library Dependencies and Compile Features\nDESCRIPTION: Sets up dependencies, compile definitions, and C++ standard requirements for all Arrow library targets. Enforces C++17 as the required standard and adds special handling for static builds on Windows.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\nadd_dependencies(arrow ${ARROW_LIBRARIES})\n\nif(ARROW_BUILD_STATIC AND WIN32)\n  target_compile_definitions(arrow_static PUBLIC ARROW_STATIC)\nendif()\n\nforeach(LIB_TARGET ${ARROW_LIBRARIES})\n  target_compile_definitions(${LIB_TARGET} PRIVATE ARROW_EXPORTING)\n  # C++17 is required to compile against Arrow C++ headers and libraries\n  target_compile_features(${LIB_TARGET} PUBLIC cxx_std_17)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: String Padding Function Definitions in Apache Arrow\nDESCRIPTION: This code snippet defines string padding functions in Apache Arrow, including their input types, output types, and associated options. These functions are used to center, left-align, or right-align strings by appending or prepending padding characters.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+\n| Function name            | Arity      | Input types             | Output type         | Options class                          |\n+==========================+============+=========================+=====================+========================================+\n| ascii_center             | Unary      | String-like             | String-like         | :struct:`PadOptions`                   |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+\n| ascii_lpad               | Unary      | String-like             | String-like         | :struct:`PadOptions`                   |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+\n| ascii_rpad               | Unary      | String-like             | String-like         | :struct:`PadOptions`                   |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+\n| utf8_center              | Unary      | String-like             | String-like         | :struct:`PadOptions`                   |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+\n| utf8_lpad                | Unary      | String-like             | String-like         | :struct:`PadOptions`                   |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+\n| utf8_rpad                | Unary      | String-like             | String-like         | :struct:`PadOptions`                   |\n+--------------------------+------------+-------------------------+---------------------+----------------------------------------+\n```\n\n----------------------------------------\n\nTITLE: Importing pyarrow.compute Module in Python\nDESCRIPTION: This code snippet shows how to import the pyarrow.compute module, which is necessary to use the compute functions documented in this file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/compute.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: pyarrow.compute\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding Optional Features in Apache Arrow CMake\nDESCRIPTION: This section conditionally adds various optional features to the Apache Arrow build based on configuration flags. It includes support for CSV, Acero, CUDA, datasets, filesystem operations, Flight RPC, IPC, JSON, ORC, Substrait, OpenTelemetry, and TensorFlow.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_28\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_CSV)\n  add_subdirectory(csv)\nendif()\n\nif(ARROW_ACERO)\n  add_subdirectory(acero)\nendif()\n\nif(ARROW_CUDA)\n  add_subdirectory(gpu)\nendif()\n\nif(ARROW_DATASET)\n  add_subdirectory(dataset)\nendif()\n\nif(ARROW_FILESYSTEM)\n  add_subdirectory(filesystem)\nendif()\n\nif(ARROW_FLIGHT)\n  add_subdirectory(flight)\nendif()\n\nif(ARROW_IPC)\n  add_subdirectory(ipc)\nendif()\n\nif(ARROW_JSON)\n  add_subdirectory(json)\nendif()\n\nif(ARROW_ORC)\n  add_subdirectory(adapters/orc)\nendif()\n\nif(ARROW_SUBSTRAIT)\n  add_subdirectory(engine)\nendif()\n\nif(ARROW_WITH_OPENTELEMETRY)\n  add_subdirectory(telemetry)\nendif()\n\nif(ARROW_TENSORFLOW)\n  add_subdirectory(adapters/tensorflow)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Bootstrap Arrow Build Environment with Conda\nDESCRIPTION: The command initializes and configures a conda environment for building Apache Arrow. The necessary packages and dependencies are specified in the file `ci\\conda_env_cpp.txt`. The `arrow-dev` environment is created to allow for easy management of dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nconda create -y -n arrow-dev --file=ci\\conda_env_cpp.txt\n```\n\n----------------------------------------\n\nTITLE: Defining Custom IPC Test Function in CMake\nDESCRIPTION: Defines a custom CMake function 'ADD_ARROW_IPC_TEST' to add IPC-specific tests with optional prefix configuration. This function wraps the 'add_arrow_test' function with default IPC-related settings.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/ipc/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ADD_ARROW_IPC_TEST REL_TEST_NAME)\n  set(options)\n  set(one_value_args PREFIX)\n  set(multi_value_args)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n\n  if(ARG_PREFIX)\n    set(PREFIX ${ARG_PREFIX})\n  else()\n    set(PREFIX \"arrow-ipc\")\n  endif()\n\n  add_arrow_test(${REL_TEST_NAME} PREFIX ${PREFIX} ${ARG_UNPARSED_ARGUMENTS})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Building Apache Arrow in Debug Mode with CMake (Shell)\nDESCRIPTION: This snippet shows the command sequence to create a build directory, configure the CMake for Debug mode, and build the Apache Arrow project using Visual Studio. It includes settings for linking with Boost libraries and defining build types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\ncd cpp\nmkdir build\ncd build\ncmake .. -G \"Visual Studio 15 2017\" -A x64 ^\n      -DARROW_BOOST_USE_SHARED=OFF ^\n      -DCMAKE_BUILD_TYPE=Debug ^\n      -DBOOST_ROOT=C:/local/boost_1_63_0  ^\n      -DBOOST_LIBRARYDIR=C:/local/boost_1_63_0/lib64-msvc-14.0\ncmake --build . --config Debug\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation Based on Arrow Version in C++\nDESCRIPTION: Demonstrates how to use the ARROW_VERSION macro to conditionally compile code based on the Arrow version. This example checks if the code is being compiled against Arrow version 7.0.1 or later.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/support.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#if ARROW_VERSION >= 7000001\n// Arrow 7.0.1 or later...\n#endif\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform-Specific Shared Library Patterns for Arrow Installation in CMake\nDESCRIPTION: This code configures platform-specific regular expression patterns to match Arrow shared libraries for installation. It handles different naming conventions for shared libraries on macOS, Linux, and Windows platforms, including version numbering in filenames.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nget_filename_component(ARROW_SHARED_LIB_DIR ${ARROW_SHARED_LIB} DIRECTORY)\nget_filename_component(ARROW_SHARED_LIB_FILENAME ${ARROW_SHARED_LIB} NAME_WE)\n\nif(NOT Arrow_FOUND)\n  if(APPLE)\n    # Install libarrow.dylib (symlink) and the real files it points to.\n    # on macOS, we need to match these files: libarrow.dylib\n    #                                         libarrow.1300.dylib\n    #                                         libarrow.1300.0.0.dylib\n    # where the version number might change.\n    set(SHARED_LIBRARY_VERSION_REGEX\n        \"${ARROW_SHARED_LIB_FILENAME}(([.][0-9]+)?([.][0-9]+)?([.][0-9]+)?)${CMAKE_SHARED_LIBRARY_SUFFIX}\"\n    )\n  elseif(UNIX AND NOT CYGWIN)\n    # Install libarrow.so (symlink) and the real files it points to.\n    # On Linux, we need to match these files: libarrow.so\n    #                                         libarrow.so.1200\n    #                                         libarrow.so.1200.0.0\n    # where the version number might change.\n    set(SHARED_LIBRARY_VERSION_REGEX\n        \"${ARROW_SHARED_LIB_FILENAME}${CMAKE_SHARED_LIBRARY_SUFFIX}(([.][0-9]+)?([.][0-9]+)?([.][0-9]+)?)\"\n    )\n  else()\n    set(SHARED_LIBRARY_VERSION_REGEX\n        ${ARROW_SHARED_LIB_FILENAME}${CMAKE_SHARED_LIBRARY_SUFFIX})\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Exporting Arrow Array to C Interface (Python)\nDESCRIPTION: Python script for exporting a PyArrow array to C-style memory addresses using the Arrow C Data Interface.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/doc/matlab_interface_for_apache_arrow_design.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nPA._export_to_c(arrayMemoryAddress, schemaMemoryAddress)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Version and Policies\nDESCRIPTION: Sets up minimum CMake version requirement and configures various CMake policies related to compiler behavior, RPATH settings, and package finding.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.25)\nmessage(STATUS \"Building using CMake version: ${CMAKE_VERSION}\")\n\ncmake_policy(SET CMP0025 NEW)\ncmake_policy(SET CMP0042 NEW)\ncmake_policy(SET CMP0054 NEW)\ncmake_policy(SET CMP0057 NEW)\ncmake_policy(SET CMP0063 NEW)\ncmake_policy(SET CMP0068 NEW)\ncmake_policy(SET CMP0074 NEW)\ncmake_policy(SET CMP0091 NEW)\n```\n\n----------------------------------------\n\nTITLE: Setting up Gandiva Custom Targets in CMake\nDESCRIPTION: Creates custom targets for Gandiva components including the main library, tests, and benchmarks. These targets help organize the build process by grouping related components.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(gandiva-all)\nadd_custom_target(gandiva)\nadd_custom_target(gandiva-tests)\nadd_custom_target(gandiva-benchmarks)\n\nadd_dependencies(gandiva-all gandiva gandiva-tests gandiva-benchmarks)\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow DictionaryType in C++\nDESCRIPTION: This snippet documents the arrow::DictionaryType class, which represents dictionary-encoded data in Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenclass:: arrow::DictionaryType\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Arrow Compute Tests in CMake\nDESCRIPTION: This snippet sets up the main configuration for Apache Arrow compute tests. It creates a custom target for tests, installs headers, and configures pkg-config support if ARROW_COMPUTE is enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(arrow-compute-tests)\n\narrow_install_all_headers(\"arrow/compute\")\n\nif(ARROW_COMPUTE)\n  # pkg-config support\n  arrow_add_pkg_config(\"arrow-compute\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: MATLAB to Python Array Transfer\nDESCRIPTION: MATLAB code demonstrating how to create an Arrow array and export it to Python using the C Data Interface, including memory address handling.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/doc/matlab_interface_for_apache_arrow_design.md#2025-04-16_snippet_5\n\nLANGUAGE: matlab\nCODE:\n```\n% Create a MATLAB arrow.Array. \n>> AA = arrow.array([1, 2, 3, 4, 5]); \n\n% Export C Data Interface C-style structs for `arrow.array.Array` values and schema\n>> cArray = arrow.c.Array();\n>> cSchema = arrow.c.Schema();\n\n% Export the MATLAB arrow.Array to the C Data Interface format, returning the \n% memory addresses of the required ArrowArray and ArrowSchema C-style structs. \n>> AA.export(cArray.Address, cSchema.Address); \n\n% Import the memory addresses of the C Data Interface format structs to create a pyarrow.Array. \n>> PA = pyrunfile(\"import_from_c.py\", \"array\", arrayMemoryAddress=cArray.Address, schemaMemoryAddress=cSchema.Address);\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Installation and Package Files in CMake\nDESCRIPTION: Sets up the installation of Arrow headers, generates CMake configuration files, and ensures backward compatibility. This makes Arrow discoverable via find_package() after installation.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_23\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow\")\n\nconfig_summary_cmake_setters(\"${CMAKE_CURRENT_BINARY_DIR}/ArrowOptions.cmake\")\ninstall(FILES ${CMAKE_CURRENT_BINARY_DIR}/ArrowOptions.cmake\n        DESTINATION \"${ARROW_CMAKE_DIR}/Arrow\")\n\n# For backward compatibility for find_package(arrow)\ninstall(FILES ${CMAKE_CURRENT_SOURCE_DIR}/arrow-config.cmake\n        DESTINATION \"${ARROW_CMAKE_DIR}/Arrow\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Java Class for Python Integration\nDESCRIPTION: A basic Java class with a static method that returns an integer value. This class will be used to demonstrate Python-Java interoperability using JPype.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic class Simple {\n    public static int getNumber() {\n        return 4;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Representing VarBinary Array in Arrow Format\nDESCRIPTION: Illustrates the memory layout for a VarBinary array ['joe', null, null, 'mark'] in Arrow format, including validity bitmap, offsets buffer, and value buffer.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n* Length: 4, Null count: 2\n* Validity bitmap buffer:\n\n  | Byte 0 (validity bitmap) | Bytes 1-63            |\n  |--------------------------|-----------------------|\n  | 00001001                 | 0 (padding)           |\n\n* Offsets buffer:\n\n  | Bytes 0-19     | Bytes 20-63           |\n  |----------------|-----------------------|\n  | 0, 3, 3, 3, 7  | unspecified (padding) |\n\n * Value buffer:\n\n  | Bytes 0-6      | Bytes 7-63            |\n  |----------------|-----------------------|\n  | joemark        | unspecified (padding) |\n```\n\n----------------------------------------\n\nTITLE: First Implementation of the tutorial_min_max Function\nDESCRIPTION: Initial implementation of the tutorial_min_max function that calls the underlying min_max function with appropriate options.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef tutorial_min_max(values, skip_nulls=True):\n    \"\"\"\n    Add docstrings\n\n    Parameters\n    ----------\n    values : Array\n\n    Returns\n    -------\n    result : TODO\n\n    Examples\n    --------\n    >>> import pyarrow.compute as pc\n    >>> data = [4, 5, 6, None, 1]\n    >>> pc.tutorial_min_max(data)\n    <pyarrow.StructScalar: [('min-', 0), ('max+', 7)]>\n    \"\"\"\n\n    options = ScalarAggregateOptions(skip_nulls=skip_nulls)\n    return call_function(\"min_max\", [values], options)\n```\n\n----------------------------------------\n\nTITLE: Column Projection with Schema\nDESCRIPTION: Demonstrates how to include derived columns alongside existing columns by building expressions from the dataset schema.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n// Project columns including existing ones from schema\nds::ScannerBuilder builder(dataset);\n\n// Get the schema from the dataset\nstd::vector<compute::Expression> exprs;\nstd::vector<std::string> names;\n\n// Add all fields in the schema\nfor (const auto& field : dataset->schema()->fields()) {\n    exprs.push_back(field_ref(field->name()));\n    names.push_back(field->name());\n}\n\n// Add the computed columns\nexprs.push_back(call(\"add\", {field_ref(\"a\"), literal(1)}));\nnames.push_back(\"a_plus_one\");\n\nFLIGHT_THROW_IF_NOT_OK(builder.Project(exprs, names));\n```\n\n----------------------------------------\n\nTITLE: Simplified Maven Configuration with Arrow BOM\nDESCRIPTION: This Maven setup uses the Arrow bill of materials to manage Apache Arrow module versions centrally, reducing complexity in dependency management. It's critical to include the BOM within the dependency management section to avoid manually updating each module version.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/install.rst#2025-04-16_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>org.example</groupId>\n    <artifactId>demo</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <properties>\n        <arrow.version>15.0.0</arrow.version>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.arrow</groupId>\n            <artifactId>arrow-vector</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.arrow</groupId>\n            <artifactId>arrow-memory-netty</artifactId>\n        </dependency>\n    </dependencies>\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.apache.arrow</groupId>\n                <artifactId>arrow-bom</artifactId>\n                <version>${arrow.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n</project>\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Build System\nDESCRIPTION: Sets environment variables to help Arrow's build system locate dependencies and libraries during development.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n$ export ARROW_HOME=$(pwd)/dist\n$ export LD_LIBRARY_PATH=$(pwd)/dist/lib:$LD_LIBRARY_PATH\n$ export CMAKE_PREFIX_PATH=$ARROW_HOME:$CMAKE_PREFIX_PATH\n```\n\n----------------------------------------\n\nTITLE: Autosummary for FileSystem Implementations in Python\nDESCRIPTION: This snippet generates an autosummary for various FileSystem implementations. It includes LocalFileSystem, S3FileSystem, GcsFileSystem, HadoopFileSystem, SubTreeFileSystem, and AzureFileSystem.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/filesystems.rst#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   LocalFileSystem\n   S3FileSystem\n   GcsFileSystem\n   HadoopFileSystem\n   SubTreeFileSystem\n   AzureFileSystem\n```\n\n----------------------------------------\n\nTITLE: Exporting Int32 Device Array in C\nDESCRIPTION: This function exports a non-nullable int32 device array by transferring ownership to the consumer. It initiates a CUDA event for synchronization and populates the ArrowDeviceArray structure's fields, including memory allocation for the event pointer.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDeviceDataInterface.rst#2025-04-16_snippet_2\n\nLANGUAGE: c\nCODE:\n```\nvoid export_int32_device_array(void* cudaAllocedPtr,\n                                   cudaStream_t stream,\n                                   int64_t length,\n                                   struct ArrowDeviceArray* array) {\n    // get device id\n    int device;\n    cudaError_t status;\n    status = cudaGetDevice(&device);\n    assert(status == cudaSuccess);\n\n    cudaEvent_t* ev_ptr = (cudaEvent_t*)malloc(sizeof(cudaEvent_t));\n    assert(ev_ptr != NULL);\n    status = cudaEventCreate(ev_ptr);\n    assert(status == cudaSuccess);\n\n    // record event on the stream, assuming that the passed in\n    // stream is where the work to produce the data will be processing.\n    status = cudaEventRecord(*ev_ptr, stream);\n    assert(status == cudaSuccess);\n\n    memset(array, 0, sizeof(struct ArrowDeviceArray));\n    // initialize fields\n    *array = (struct ArrowDeviceArray) {\n        .array = (struct ArrowArray) {\n            .length = length,\n            .null_count = 0,\n            .offset = 0,\n            .n_buffers = 2,\n            .n_children = 0,\n            .children = NULL,\n            .dictionary = NULL,\n            // bookkeeping\n            .release = &release_int32_device_array,\n            // store the event pointer as private data in the array\n            // so that we can access it in the release callback.\n            .private_data = (void*)(ev_ptr),\n        },\n    };\n```\n\n----------------------------------------\n\nTITLE: Building and Installing Skyhook\nDESCRIPTION: This snippet clones the Arrow repository, configures CMake with Skyhook options, builds the project, and installs the Skyhook library.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/dataset_skyhook_scan_example.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/apache/arrow\ncd arrow/\nmkdir -p cpp/release\ncd cpp/release\ncmake -DARROW_SKYHOOK=ON \\\n      -DARROW_PARQUET=ON \\\n      -DARROW_WITH_SNAPPY=ON \\\n      -DARROW_BUILD_EXAMPLES=ON \\\n      -DARROW_DATASET=ON \\\n      -DARROW_CSV=ON \\\n      -DARROW_WITH_LZ4=ON \\\n      ..\n\nmake -j install\ncp release/libcls_skyhook.so /usr/lib/x86_64-linux-gnu/rados-classes/\n```\n\n----------------------------------------\n\nTITLE: Implementing StartProducing Method in C++ ExecNode\nDESCRIPTION: The StartProducing method is called at the start of the plan execution. It's typically used by source nodes to schedule tasks for reading and providing data, initiating the push-based data flow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/developer_guide.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nExecNode::StartProducing()\n```\n\n----------------------------------------\n\nTITLE: Building JNI C Data Interface Library with Maven (Windows)\nDESCRIPTION: Maven commands to build only the JNI C Data Interface library on Windows platforms.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ cd arrow/java\n$ mvn generate-resources -Pgenerate-libs-cdata-all-os -N\n$ dir \"../java-dist/bin\"\n|__ arrow_cdata_jni/\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Testing Library Target in CMake\nDESCRIPTION: Sets up the Arrow testing library which provides common testing utilities for Arrow. Similar to the main library configuration, it handles both shared and static builds with their dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_22\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_TESTING)\n  add_arrow_lib(arrow_testing\n                CMAKE_PACKAGE_NAME\n                ArrowTesting\n                PKG_CONFIG_NAME\n                arrow-testing\n                SOURCES\n                ${ARROW_TESTING_SRCS}\n                OUTPUTS\n                ARROW_TESTING_LIBRARIES\n                PRECOMPILED_HEADERS\n                \"$<$<COMPILE_LANGUAGE:CXX>:arrow/pch.h>\"\n                SHARED_LINK_LIBS\n                ${ARROW_TESTING_SHARED_LINK_LIBS}\n                SHARED_PRIVATE_LINK_LIBS\n                ${ARROW_TESTING_SHARED_PRIVATE_LINK_LIBS}\n                SHARED_INSTALL_INTERFACE_LIBS\n                ${ARROW_TESTING_SHARED_INSTALL_INTERFACE_LIBS}\n                STATIC_LINK_LIBS\n                ${ARROW_TESTING_STATIC_LINK_LIBS}\n                STATIC_INSTALL_INTERFACE_LIBS\n                ${ARROW_TESTING_STATIC_INSTALL_INTERFACE_LIBS})\n\n  add_custom_target(arrow_testing)\n  add_dependencies(arrow_testing ${ARROW_TESTING_LIBRARIES})\n\n  if(ARROW_BUILD_STATIC AND WIN32)\n    target_compile_definitions(arrow_testing_static PUBLIC ARROW_TESTING_STATIC)\n  endif()\n\n  foreach(LIB_TARGET ${ARROW_TESTING_LIBRARIES})\n    target_compile_definitions(${LIB_TARGET} PRIVATE ARROW_TESTING_EXPORTING)\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Python to MATLAB Array Transfer\nDESCRIPTION: MATLAB code showing how to create a PyArrow array and import it into MATLAB using the C Data Interface.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/doc/matlab_interface_for_apache_arrow_design.md#2025-04-16_snippet_7\n\nLANGUAGE: matlab\nCODE:\n```\n% Make a pyarrow.Array. \n>> PA = py.pyarrow.array([1, 2, 3, 4, 5]); \n\n% Create ArrowArray and ArrowSchema C-style structs adhering to the Arrow C Data Interface format. \n>> cArray = arrow.c.Array();\n>> cSchema = arrow.c.Schema();\n\n% Export the pyarrow.Array to the C Data Interface format, populating the required ArrowArray and ArrowShema structs. \n>> pyrunfile(\"export_to_c.py\", PA=PA, arrayMemoryAddress=cArray.Address, schemaMemoryAddress=cSchema.Address);\n\n% Import the C Data Interface structs to create a MATLAB arrow.Array. \n>> AA = arrow.array.Array.import(cArray, cSchema);\n```\n\n----------------------------------------\n\nTITLE: Running Source Verification for Apache Arrow Releases on Linux/macOS\nDESCRIPTION: Commands for performing required source verification of Apache Arrow releases. The script creates a temporary verification environment and runs tests against the source code. Various TEST_* variables can be set to test specific implementations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release_verification.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# this will create and automatically clean up a temporary directory for the verification environment and will run the source verification\nTEST_DEFAULT=0 TEST_SOURCE=1 verify-release-candidate.sh $VERSION $RC_NUM\n\n# to verify only certain implementations use the TEST_DEFAULT=0 and TEST_* variables\n# here are a couple of examples, but see the source code for the available options\nTEST_DEFAULT=0 TEST_CPP=1 verify-release-candidate.sh $VERSION $RC_NUM  # only C++ tests\nTEST_DEFAULT=0 TEST_CPP=1 TEST_PYTHON=1 verify-release-candidate.sh $VERSION $RC_NUM  # C++ and Python tests\nTEST_DEFAULT=0 TEST_INTEGRATION_CPP=1 TEST_INTEGRATION_JAVA=1 verify-release-candidate.sh $VERSION $RC_NUM  # C++ and Java integration tests\n```\n\n----------------------------------------\n\nTITLE: Defining ARROW_HOME Environment Variable in RST\nDESCRIPTION: Specifies the base path to the PyArrow installation, overriding default library path computations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/env_vars.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. envvar:: ARROW_HOME\n\n   The base path to the PyArrow installation.  This variable overrides the\n   default computation of library paths in introspection functions such\n   as :func:`get_library_dirs`.\n```\n\n----------------------------------------\n\nTITLE: Set Arrow Build System with Conda\nDESCRIPTION: Set environment variables in the build system to use packages from the activated conda environment. These settings adjust the CMake configuration for resolving project dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n-DARROW_DEPENDENCY_SOURCE=SYSTEM ^\n-DARROW_PACKAGE_PREFIX=%CONDA_PREFIX%\\Library\n```\n\n----------------------------------------\n\nTITLE: Adding Parquet Tests and Benchmarks in CMake\nDESCRIPTION: CMake configuration that defines test and benchmark targets for the Parquet library. Includes tests for file deserialization and schema validation, along with benchmarks for bloom filters, column operations, encoding, metadata, and Arrow integration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/CMakeLists.txt#2025-04-16_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\n# symbols which are not exported by parquet.dll on Windows (PARQUET-1420).\nadd_parquet_test(file_deserialize_test SOURCES file_deserialize_test.cc)\nadd_parquet_test(schema_test)\n\nadd_parquet_benchmark(bloom_filter_benchmark SOURCES bloom_filter_benchmark.cc\n                      benchmark_util.cc)\nadd_parquet_benchmark(column_reader_benchmark)\nadd_parquet_benchmark(column_io_benchmark)\nadd_parquet_benchmark(encoding_benchmark)\nadd_parquet_benchmark(level_conversion_benchmark)\nadd_parquet_benchmark(metadata_benchmark)\nadd_parquet_benchmark(page_index_benchmark SOURCES page_index_benchmark.cc\n                      benchmark_util.cc)\nadd_parquet_benchmark(arrow/reader_writer_benchmark PREFIX \"parquet-arrow\")\nadd_parquet_benchmark(arrow/size_stats_benchmark PREFIX \"parquet-arrow\")\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Build with conda in Fedora Docker Container\nDESCRIPTION: Command to run the Fedora Docker container and execute the build script using conda. It mounts the current directory and Arrow source directory as volumes.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -t -i -v $PWD:/io -v $PWD/../../..:/arrow arrow_fedora_minimal /io/build_conda.sh\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow with GCS support with CMake\nDESCRIPTION: This snippet shows how to enable building Arrow with GCS support, requiring the GCloud SDK for C++.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_GCS=ON\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Tests and Benchmarks for Apache Arrow C Interface in CMake\nDESCRIPTION: This CMake snippet defines testing and benchmarking configurations for Apache Arrow's C interface. It adds tests for the bridge and DLPack functionalities, sets up benchmarking for the bridge component, and installs all headers from the arrow/c directory.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/c/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_test(bridge_test PREFIX \"arrow-c\")\nadd_arrow_test(dlpack_test)\n\nadd_arrow_benchmark(bridge_benchmark)\n\narrow_install_all_headers(\"arrow/c\")\n```\n\n----------------------------------------\n\nTITLE: Getting Schema of Arrow RecordBatch in MATLAB\nDESCRIPTION: This snippet demonstrates how to create a MATLAB table, convert it to an Arrow RecordBatch, and then retrieve its Schema. It shows the correspondence between MATLAB's table structure and Arrow's data representation.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_19\n\nLANGUAGE: matlab\nCODE:\n```\n>> matlabTable = table([\"A\"; \"B\"; \"C\"], [1; 2; 3], VariableNames=[\"Letter\", \"Number\"])\n\nmatlabTable =\n\n  3x2 table\n\n    Letter    Number\n    ______    ______\n\n     \"A\"        1\n     \"B\"        2\n     \"C\"        3\n\n>> arrowRecordBatch = arrow.recordBatch(matlabTable)\n\narrowRecordBatch =\n\nLetter:   [\n    \"A\",\n    \"B\",\n    \"C\"\n  ]\nNumber:   [\n    1,\n    2,\n    3\n  ]\n\n>> arrowSchema = arrowRecordBatch.Schema\n\narrowSchema =\n\nLetter: string\nNumber: double\n```\n\n----------------------------------------\n\nTITLE: Configuring Flight Server in ASP.NET Core Program.cs\nDESCRIPTION: This snippet shows how to add the Flight server to the gRPC services and map the endpoints using extension methods in the Program.cs file.\nSOURCE: https://github.com/apache/arrow/blob/main/csharp/examples/FlightAspServerExample/readme.md#2025-04-16_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\ngrpcBuilder.AddFlightServer<InMemoryFlightServer>();\n...\napp.MapFlightEndpoint();\n```\n\n----------------------------------------\n\nTITLE: Constructing Hive Partitioning in Apache Arrow C++\nDESCRIPTION: Creates a Hive partitioning with explicit schema definition using Apache Arrow. It defines fields such as 'year', 'month', and 'day' with specific types, demonstrating how partition keys can be constructed directly.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nauto part = std::make_shared<ds::HivePartitioning>(arrow::schema({\n    arrow::field(\"year\", arrow::int16()),\n    arrow::field(\"month\", arrow::int8()),\n    arrow::field(\"day\", arrow::int32())\n}));\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Compute Source Files in CMake\nDESCRIPTION: Sets up the list of source files for Arrow Compute functionality, including scalar casts and select kernels. Additional files are added if ARROW_COMPUTE is enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_COMPUTE_SRCS\n    compute/api_aggregate.cc\n    compute/api_scalar.cc\n    compute/api_vector.cc\n    compute/cast.cc\n    compute/exec.cc\n    compute/expression.cc\n    compute/function.cc\n    compute/function_internal.cc\n    compute/kernel.cc\n    compute/ordering.cc\n    compute/registry.cc\n    compute/kernels/chunked_internal.cc\n    compute/kernels/codegen_internal.cc\n    compute/kernels/ree_util_internal.cc\n    compute/kernels/scalar_cast_boolean.cc\n    compute/kernels/scalar_cast_dictionary.cc\n    compute/kernels/scalar_cast_extension.cc\n    compute/kernels/scalar_cast_internal.cc\n    compute/kernels/scalar_cast_nested.cc\n    compute/kernels/scalar_cast_numeric.cc\n    compute/kernels/scalar_cast_string.cc\n    compute/kernels/scalar_cast_temporal.cc\n    compute/kernels/util_internal.cc\n    compute/kernels/vector_hash.cc\n    compute/kernels/vector_selection.cc\n    compute/kernels/vector_selection_filter_internal.cc\n    compute/kernels/vector_selection_internal.cc\n    compute/kernels/vector_selection_take_internal.cc)\n\nif(ARROW_COMPUTE)\n  # Include the remaining kernels\n  list(APPEND\n       ARROW_COMPUTE_SRCS\n       compute/kernels/aggregate_basic.cc\n       compute/kernels/aggregate_mode.cc\n       compute/kernels/aggregate_pivot.cc\n       compute/kernels/aggregate_quantile.cc\n       compute/kernels/aggregate_tdigest.cc\n       compute/kernels/aggregate_var_std.cc\n       compute/kernels/hash_aggregate.cc\n       compute/kernels/hash_aggregate_numeric.cc\n       compute/kernels/hash_aggregate_pivot.cc\n       compute/kernels/pivot_internal.cc\n       compute/kernels/scalar_arithmetic.cc\n       compute/kernels/scalar_boolean.cc\n       compute/kernels/scalar_compare.cc\n       compute/kernels/scalar_if_else.cc\n       compute/kernels/scalar_nested.cc\n       compute/kernels/scalar_random.cc\n       compute/kernels/scalar_round.cc\n       compute/kernels/scalar_set_lookup.cc\n       compute/kernels/scalar_string_ascii.cc\n       compute/kernels/scalar_string_utf8.cc\n       compute/kernels/scalar_temporal_binary.cc\n       compute/kernels/scalar_temporal_unary.cc\n       compute/kernels/scalar_validity.cc\n       compute/kernels/vector_array_sort.cc\n       compute/kernels/vector_cumulative_ops.cc\n       compute/kernels/vector_nested.cc\n       compute/kernels/vector_pairwise.cc\n       compute/kernels/vector_rank.cc\n       compute/kernels/vector_replace.cc\n       compute/kernels/vector_run_end_encode.cc\n       compute/kernels/vector_select_k.cc\n       compute/kernels/vector_sort.cc\n       compute/kernels/vector_statistics.cc\n       compute/kernels/vector_swizzle.cc\n       compute/key_hash_internal.cc\n       compute/key_map_internal.cc\n       compute/light_array_internal.cc\n       compute/row/encode_internal.cc\n       compute/row/compare_internal.cc\n       compute/row/grouper.cc\n       compute/row/row_encoder_internal.cc\n       compute/row/row_internal.cc\n       compute/util.cc\n       compute/util_internal.cc)\n\n  append_runtime_avx2_src(ARROW_COMPUTE_SRCS compute/kernels/aggregate_basic_avx2.cc)\n  append_runtime_avx512_src(ARROW_COMPUTE_SRCS compute/kernels/aggregate_basic_avx512.cc)\n  append_runtime_avx2_src(ARROW_COMPUTE_SRCS compute/key_hash_internal_avx2.cc)\n  append_runtime_avx2_bmi2_src(ARROW_COMPUTE_SRCS compute/key_map_internal_avx2.cc)\n  append_runtime_avx2_src(ARROW_COMPUTE_SRCS compute/row/compare_internal_avx2.cc)\n  append_runtime_avx2_src(ARROW_COMPUTE_SRCS compute/row/encode_internal_avx2.cc)\n  append_runtime_avx2_bmi2_src(ARROW_COMPUTE_SRCS compute/util_avx2.cc)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Makefile Configuration with pkg-config\nDESCRIPTION: Example Makefile that compiles a source file using Arrow C++ shared library, utilizing pkg-config to obtain the necessary compiler and linker flags.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/build_system.rst#2025-04-16_snippet_3\n\nLANGUAGE: makefile\nCODE:\n```\nmy_example: my_example.cc\n    $(CXX) -o $@ $(CXXFLAGS) $< $$(pkg-config --cflags --libs arrow)\n```\n\n----------------------------------------\n\nTITLE: Defining autosummary for Arrow stream factory functions in Python\nDESCRIPTION: This snippet defines an autosummary for Arrow stream factory functions, including input_stream, output_stream, memory_map, and create_memory_map.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/files.rst#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   input_stream\n   output_stream\n   memory_map\n   create_memory_map\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Dataset Test Function\nDESCRIPTION: Creates a function to add Arrow Dataset tests, handling test name prefixes, extra link libraries, and test labels with sensible defaults.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/dataset/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ADD_ARROW_DATASET_TEST REL_TEST_NAME)\n  set(options)\n  set(one_value_args PREFIX)\n  set(multi_value_args EXTRA_LINK_LIBS LABELS)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n\n  if(ARG_PREFIX)\n    set(PREFIX ${ARG_PREFIX})\n  else()\n    set(PREFIX \"arrow-dataset\")\n  endif()\n\n  if(ARG_EXTRA_LINK_LIBS)\n    set(EXTRA_LINK_LIBS ${ARG_EXTRA_LINK_LIBS})\n  else()\n    set(EXTRA_LINK_LIBS ${ARROW_DATASET_TEST_LINK_LIBS})\n  endif()\n\n  if(ARG_LABELS)\n    set(LABELS ${ARG_LABELS})\n  else()\n    set(LABELS \"arrow_dataset\")\n  endif()\n\n  add_arrow_test(${REL_TEST_NAME}\n                 EXTRA_LINK_LIBS\n                 ${EXTRA_LINK_LIBS}\n                 PREFIX\n                 ${PREFIX}\n                 LABELS\n                 ${LABELS}\n                 ${ARG_UNPARSED_ARGUMENTS})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Importing Acero Module in Python\nDESCRIPTION: This snippet shows how to import the Acero module in Python. It sets the current module to pyarrow.acero for the documentation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/acero.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: pyarrow.acero\n```\n\n----------------------------------------\n\nTITLE: Configuring Gandiva Projector Tests in CMake\nDESCRIPTION: Sets up a Gandiva test target named 'projector-test' with multiple source files. This test covers various functionalities of the Gandiva projector, including binary operations, boolean expressions, date/time handling, decimal operations, filtering, and more.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/tests/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_gandiva_test(projector-test\n                 SOURCES\n                 binary_test.cc\n                 boolean_expr_test.cc\n                 date_time_test.cc\n                 decimal_single_test.cc\n                 decimal_test.cc\n                 filter_project_test.cc\n                 filter_test.cc\n                 hash_test.cc\n                 huge_table_test.cc\n                 if_expr_test.cc\n                 in_expr_test.cc\n                 literal_test.cc\n                 null_validity_test.cc\n                 projector_build_validation_test.cc\n                 projector_test.cc\n                 test_util.cc\n                 to_string_test.cc\n                 utf8_test.cc)\n```\n\n----------------------------------------\n\nTITLE: Illustrating Allocator Perspective Hierarchy in Apache Arrow Java\nDESCRIPTION: This code block presents a hierarchical structure showing the relationship between RootAllocator, ChildAllocators, BufferLedgers, AllocationManagers, and ArrowBufs from an allocator-centric perspective. It demonstrates how multiple allocators can share underlying memory through AllocationManagers.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/memory.rst#2025-04-16_snippet_7\n\nLANGUAGE: none\nCODE:\n```\n+ RootAllocator\n|-+ ChildAllocator 1\n| | - ChildAllocator 1.1\n| ` ...\n|\n|-+ ChildAllocator 2\n|-+ ChildAllocator 3\n| |\n| |-+ BufferLedger 1 ==> AllocationManager 1 (owning) ==> UDLE\n| | `- ArrowBuf 1\n| `-+ BufferLedger 2 ==> AllocationManager 2 (non-owning)==> UDLE\n|   `- ArrowBuf 2\n|\n|-+ BufferLedger 3 ==> AllocationManager 1 (non-owning)==> UDLE\n| ` - ArrowBuf 3\n|-+ BufferLedger 4 ==> AllocationManager 2 (owning) ==> UDLE\n  | - ArrowBuf 4\n  | - ArrowBuf 5\n  ` - ArrowBuf 6\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with CTest in Apache Arrow\nDESCRIPTION: Command to run unit tests with CTest, using parallel execution and displaying output on failure. The -j16 option runs up to 16 tests in parallel to leverage multiple CPU cores.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ ctest -j16 --output-on-failure\n```\n\n----------------------------------------\n\nTITLE: Displaying arrow::Decimal128Scalar with GDB extension\nDESCRIPTION: Example of how GDB displays an arrow::Decimal128Scalar object with the custom extension loaded. Shows a more readable and informative representation of the decimal value.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gdb.rst#2025-04-16_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n$6 = arrow::Decimal128Scalar of value 123.4567 [precision=10, scale=4]\n```\n\n----------------------------------------\n\nTITLE: Arrow Map and Struct Operations Table\nDESCRIPTION: ASCII table documenting map_lookup and struct_field operations, showing their arity, input types, output types and references to detailed options structs.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n+---------------------+------------+-------------------------------------+------------------+------------------------------+--------+\n| map_lookup          | Unary      | Map                                 | Computed         | :struct:`MapLookupOptions`   | \\(5)   |\n+---------------------+------------+-------------------------------------+------------------+------------------------------+--------+\n| struct_field        | Unary      | Struct or Union                     | Computed         | :struct:`StructFieldOptions` | \\(6)   |\n+---------------------+------------+-------------------------------------+------------------+------------------------------+--------+\n```\n\n----------------------------------------\n\nTITLE: Defining Test Dependencies for Apache Arrow\nDESCRIPTION: List of Python package dependencies required for testing Apache Arrow. Includes data processing libraries like numpy, testing frameworks like pytest, and cloud storage dependencies like s3fs and boto3. Specific version constraints are provided for certain packages.\nSOURCE: https://github.com/apache/arrow/blob/main/ci/conda_env_python.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nboto3\ncffi\ncython>=3\ncloudpickle\nfsspec\nhypothesis\nnumpy>=1.16.6\npytest\npytest-faulthandler\ns3fs>=2023.10.0\nsetuptools>=64\nsetuptools_scm>=8\n```\n\n----------------------------------------\n\nTITLE: Preparing Datum for Index Search Output in Arrow C++\nDESCRIPTION: Declares a Datum object to store the result of the index search computation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\narrow::Datum index_result;\n```\n\n----------------------------------------\n\nTITLE: Processing Perf Events into JSON Format for Memory Analysis\nDESCRIPTION: Python script that parses the output of 'perf script' and converts it into JSON-formatted data. The script processes memory allocation events and their associated tracebacks, making them easier to analyze programmatically.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/memory.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nimport re\nimport json\n\n# Example non-traceback line\n# arrow-array-tes 14344 [003]  7501.073802: probe_libarrow:je_arrow_mallocx: (7fbcd20bb640) size=0x80 flags=6\n\ncurrent = {}\ncurrent_traceback = ''\n\ndef new_row():\n    global current_traceback\n    current['traceback'] = current_traceback\n    print(json.dumps(current))\n    current_traceback = ''\n\nfor line in sys.stdin:\n    if line == '\\n':\n        continue\n    elif line[0] == '\\t':\n        # traceback line\n        current_traceback += line.strip(\"\\t\")\n    else:\n        line = line.rstrip('\\n')\n        if not len(current) == 0:\n            new_row()\n        parts = re.sub(' +', ' ', line).split(' ')\n\n        parts.reverse()\n        parts.pop() # file\n        parts.pop() # \"14344\"\n        parts.pop() # \"[003]\"\n\n        current['time'] = float(parts.pop().rstrip(\":\"))\n        current['event'] = parts.pop().rstrip(\":\")\n\n        parts.pop() # (7fbcd20bddf0)\n        if parts[-1] == \"<-\":\n            parts.pop()\n            parts.pop()\n\n        params = {}\n\n        for pair in parts:\n            key, value = pair.split(\"=\")\n            params[key] = value\n\n        current['params'] = params\n```\n\n----------------------------------------\n\nTITLE: Running Arrow Flight Unit Tests\nDESCRIPTION: Demonstrates how to run the Flight unit tests by either setting the current working directory to the executables directory or adding it to the PATH environment variable.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nPATH=debug:$PATH debug/flight-test\n```\n\n----------------------------------------\n\nTITLE: Configuring Flight SQL Protobuf Generation in CMake\nDESCRIPTION: Sets up the protobuf generation commands for Flight SQL. This snippet defines the paths, output files, and protoc command for generating C++ code from the FlightSql.proto definition file.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/sql/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(FLIGHT_SQL_PROTO_PATH \"${ARROW_SOURCE_DIR}/../format\")\nset(FLIGHT_SQL_PROTO ${ARROW_SOURCE_DIR}/../format/FlightSql.proto)\n\nset(FLIGHT_SQL_GENERATED_PROTO_FILES \"${CMAKE_CURRENT_BINARY_DIR}/FlightSql.pb.cc\"\n                                     \"${CMAKE_CURRENT_BINARY_DIR}/FlightSql.pb.h\")\n\nset(PROTO_DEPENDS ${FLIGHT_SQL_PROTO} ${ARROW_PROTOBUF_LIBPROTOBUF})\n\nset(FLIGHT_SQL_PROTOC_COMMAND\n    ${ARROW_PROTOBUF_PROTOC} \"-I${FLIGHT_SQL_PROTO_PATH}\"\n    \"--cpp_out=dllexport_decl=ARROW_FLIGHT_SQL_EXPORT:${CMAKE_CURRENT_BINARY_DIR}\")\nif(Protobuf_VERSION VERSION_LESS 3.15)\n  list(APPEND FLIGHT_SQL_PROTOC_COMMAND \"--experimental_allow_proto3_optional\")\nendif()\nlist(APPEND FLIGHT_SQL_PROTOC_COMMAND \"${FLIGHT_SQL_PROTO}\")\n\nadd_custom_command(OUTPUT ${FLIGHT_SQL_GENERATED_PROTO_FILES}\n                   COMMAND ${FLIGHT_SQL_PROTOC_COMMAND}\n                   DEPENDS ${PROTO_DEPENDS})\n\nset_source_files_properties(${FLIGHT_SQL_GENERATED_PROTO_FILES} PROPERTIES GENERATED TRUE)\nadd_custom_target(flight_sql_protobuf_gen ALL DEPENDS ${FLIGHT_SQL_GENERATED_PROTO_FILES})\n```\n\n----------------------------------------\n\nTITLE: Unit Test for the tutorial_min_max Function\nDESCRIPTION: Test function to verify that the tutorial_min_max function correctly adjusts min/max values and handles null values based on the skip_nulls parameter.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef test_tutorial_min_max():\n    arr = [4, 5, 6, None, 1]\n    l1 = {'min-': 0, 'max+': 7}\n    l2 = {'min-': None, 'max+': None}\n    assert pc.tutorial_min_max(arr).as_py() == l1\n    assert pc.tutorial_min_max(arr,\n                               skip_nulls=False).as_py() == l2\n```\n\n----------------------------------------\n\nTITLE: Running Maven Checkstyle Check in Apache Arrow Java\nDESCRIPTION: This command demonstrates how to validate code style with Maven's checkstyle plugin without building the project. It checks all source code under the current directory against Arrow's checkstyle rules.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/development.rst#2025-04-16_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ mvn checkstyle:check\n```\n\n----------------------------------------\n\nTITLE: Setting ARROW_PRE_0_15_IPC_FORMAT Environment Variable in RST\nDESCRIPTION: Controls the use of pre-0.15 Arrow IPC format in the PyArrow IPC writer when set to a non-zero integer value.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/env_vars.rst#2025-04-16_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. envvar:: ARROW_PRE_0_15_IPC_FORMAT\n\n   If this environment variable is set to a non-zero integer value, the PyArrow\n   IPC writer will default to the pre-0.15 Arrow IPC format.\n   This behavior can also be enabled using :attr:`IpcWriteOptions.use_legacy_format`.\n```\n\n----------------------------------------\n\nTITLE: Configure vcpkg Dependencies\nDESCRIPTION: Run a command to install required C++ packages using vcpkg. It installs dependencies from the manifest file for Arrow using specific triplet settings for Windows.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nvcpkg install ^\n  --triplet x64-windows ^\n  --x-manifest-root cpp  ^\n  --feature-flags=versions ^\n  --clean-after-build\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files for Gandiva Precompiled Bitcode\nDESCRIPTION: Sets up the list of source files that will be compiled to LLVM bitcode for Gandiva's precompiled operations. These files contain implementations for various operations like arithmetic, bitmap manipulation, decimal operations, and string processing.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/precompiled/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(PRECOMPILED_SRCS\n    arithmetic_ops.cc\n    bitmap.cc\n    decimal_ops.cc\n    decimal_wrapper.cc\n    extended_math_ops.cc\n    hash.cc\n    print.cc\n    string_ops.cc\n    time.cc\n    timestamp_arithmetic.cc\n    ../../arrow/util/basic_decimal.cc)\n```\n\n----------------------------------------\n\nTITLE: Variable Shape Tensor Specification - Apache Arrow\nDESCRIPTION: Details the variable shape tensor extension parameters and serialization format for Arrow. It establishes the extension name as 'arrow.variable_shape_tensor' and defines the storage type, parameters including value type, and serialization strategy.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CanonicalExtensions.rst#2025-04-16_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n* Extension name: ``arrow.variable_shape_tensor``.\n\n* The storage type of the extension is: ``StructArray`` where struct\n  is composed of **data** and **shape** fields describing a single\n  tensor per row:\n\n  * **data** is a ``List`` holding tensor elements.\n  * **shape** is a ``FixedSizeList<int32>[ndim]`` of the tensor shape where\n    the size of the list ``ndim`` is equal to the number of dimensions of the\n    tensor.\n\n* Extension type parameters:\n\n  * **value_type** = the Arrow data type of individual tensor elements.\n\n  Optional parameters describing the logical layout:\n\n  * **dim_names** = explicit names to tensor dimensions\n    as an array.\n  * **permutation**  = indices of the desired ordering of the\n    original dimensions, defined as an array.\n  * **uniform_shape** = sizes of individual tensor's dimensions.\n\n* Description of the serialization:\n\n  The metadata must be a valid JSON object that optionally includes\n  dimension names with keys **\"dim_names\"** and ordering of dimensions\n  with key **\"permutation\"** and shapes of tensors defined in some dimensions by providing\n  key **\"uniform_shape\"**.\n\n  - Example with ``dim_names`` metadata for NCHW ordered data:\n\n    ``{ \"dim_names\": [\"C\", \"H\", \"W\"] }``\n  - Example with ``uniform_shape`` metadata for a set of color images:\n\n    ``{ \"dim_names\": [\"H\", \"W\", \"C\"], \"uniform_shape\": [400, null, 3] }``\n  - Example of permuted 3-dimensional tensor:\n\n    ``{ \"permutation\": [2, 0, 1] }``\n```\n\n----------------------------------------\n\nTITLE: Hash Table First Pass Resize Algorithm\nDESCRIPTION: Algorithm for the first pass of hash table resizing that handles non-overflow entries. It distributes entries from source block L to target blocks (2*L+0) and (2*L+1) based on the highest bit of the stamp.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/acero/doc/key_map.md#2025-04-16_snippet_0\n\nLANGUAGE: pseudocode\nCODE:\n```\nfor each slot in source_table:\n    if !is_overflow_entry(slot.hash):\n        target_block = 2 * source_block + (slot.stamp >> 7)\n        move_entry_to_target(slot, target_block)\n```\n\n----------------------------------------\n\nTITLE: Configuring Clang-Tidy Targets in CMake\nDESCRIPTION: Creates clang-tidy and check-clang-tidy targets for static analysis. The clang-tidy target attempts to fix warnings automatically while check-clang-tidy reports errors.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(${CLANG_TIDY_FOUND})\n  add_custom_target(clang-tidy\n                    ${PYTHON_EXECUTABLE}\n                    ${BUILD_SUPPORT_DIR}/run_clang_tidy.py\n                    --clang_tidy_binary\n                    ${CLANG_TIDY_BIN}\n                    --compile_commands\n                    ${CMAKE_BINARY_DIR}/compile_commands.json\n                    ${COMMON_LINT_OPTIONS}\n                    --fix\n                    ${ARROW_LINT_QUIET})\n\n  add_custom_target(check-clang-tidy\n                    ${PYTHON_EXECUTABLE}\n                    ${BUILD_SUPPORT_DIR}/run_clang_tidy.py\n                    --clang_tidy_binary\n                    ${CLANG_TIDY_BIN}\n                    --compile_commands\n                    ${CMAKE_BINARY_DIR}/compile_commands.json\n                    ${COMMON_LINT_OPTIONS}\n                    ${ARROW_LINT_QUIET})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring IndexOptions for Search Target in Arrow (C++)\nDESCRIPTION: Sets up the IndexOptions struct with a target value of 2223 to search for in the data column. This demonstrates how to configure search parameters before executing a compute function.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\noptions.value = compute::Datum(2223);\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Flight Examples in CMake\nDESCRIPTION: Sets up Flight examples with appropriate linking libraries and protobuf generation. Handles different linking scenarios for macOS vs other platforms and shared vs static builds.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/arrow/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_FLIGHT)\n  # Static gRPC means we cannot linked to shared Arrow, since then\n  # we'll violate ODR for gRPC symbols\n  if(ARROW_BUILD_SHARED AND ARROW_GRPC_USE_SHARED)\n    set(FLIGHT_EXAMPLES_LINK_LIBS arrow_flight_shared)\n    if(APPLE)\n      set(GRPC_REFLECTION_LINK_LIBS gRPC::grpc++_reflection)\n    else()\n      # We don't directly use symbols from the reflection library, so\n      # ensure the linker still links to it\n      set(GRPC_REFLECTION_LINK_LIBS -Wl,--no-as-needed gRPC::grpc++_reflection\n                                    -Wl,--as-needed)\n    endif()\n  elseif(NOT ARROW_BUILD_STATIC)\n    message(FATAL_ERROR \"Statically built gRPC requires ARROW_BUILD_STATIC=ON\")\n  else()\n    set(FLIGHT_EXAMPLES_LINK_LIBS arrow_flight_static)\n    if(APPLE)\n      set(GRPC_REFLECTION_LINK_LIBS -Wl,-force_load gRPC::grpc++_reflection)\n    else()\n      set(GRPC_REFLECTION_LINK_LIBS -Wl,--whole-archive gRPC::grpc++_reflection\n                                    -Wl,--no-whole-archive)\n    endif()\n  endif()\n\n  set(FLIGHT_EXAMPLE_GENERATED_PROTO_FILES\n      \"${CMAKE_CURRENT_BINARY_DIR}/helloworld.pb.cc\"\n      \"${CMAKE_CURRENT_BINARY_DIR}/helloworld.pb.h\"\n      \"${CMAKE_CURRENT_BINARY_DIR}/helloworld.grpc.pb.cc\"\n      \"${CMAKE_CURRENT_BINARY_DIR}/helloworld.grpc.pb.h\")\n  set_source_files_properties(${FLIGHT_EXAMPLE_GENERATED_PROTO_FILES} PROPERTIES GENERATED\n                                                                                 TRUE)\n\n  set(FLIGHT_EXAMPLE_PROTO \"helloworld.proto\")\n  set(FLIGHT_EXAMPLE_PROTO_PATH \"${CMAKE_CURRENT_LIST_DIR}\")\n  set(FLIGHT_EXAMPLE_PROTO_DEPENDS ${FLIGHT_EXAMPLE_PROTO} gRPC::grpc_cpp_plugin)\n\n  add_custom_command(OUTPUT ${FLIGHT_EXAMPLE_GENERATED_PROTO_FILES}\n                     COMMAND ${ARROW_PROTOBUF_PROTOC} \"-I${FLIGHT_EXAMPLE_PROTO_PATH}\"\n                             \"--cpp_out=${CMAKE_CURRENT_BINARY_DIR}\"\n                             \"${FLIGHT_EXAMPLE_PROTO}\"\n                     COMMAND ${ARROW_PROTOBUF_PROTOC} \"-I${FLIGHT_EXAMPLE_PROTO_PATH}\"\n                             \"--grpc_out=${CMAKE_CURRENT_BINARY_DIR}\"\n                             \"--plugin=protoc-gen-grpc=$<TARGET_FILE:gRPC::grpc_cpp_plugin>\"\n                             \"${FLIGHT_EXAMPLE_PROTO}\"\n                     DEPENDS ${FLIGHT_EXAMPLE_PROTO_DEPENDS})\n\n  add_custom_target(flight_grpc_example_gen ALL\n                    DEPENDS ${FLIGHT_EXAMPLE_GENERATED_PROTO_FILES})\n\n  set(FLIGHT_GRPC_EXAMPLE_LINK_LIBS\n      ${FLIGHT_EXAMPLES_LINK_LIBS}\n      gRPC::grpc++\n      ${GRPC_REFLECTION_LINK_LIBS}\n      ${ARROW_PROTOBUF_LIBPROTOBUF}\n      ${GFLAGS_LIBRARIES})\n  if(TARGET absl::log_internal_check_op)\n    # Protobuf generated files may use ABSL_DCHECK*() and\n    # absl::log_internal_check_op is needed for them.\n    list(APPEND FLIGHT_GRPC_EXAMPLE_LINK_LIBS absl::log_internal_check_op)\n  endif()\n  add_arrow_example(flight_grpc_example\n                    DEPENDENCIES\n                    flight_grpc_example_gen\n                    # Not CMAKE_CURRENT_BINARY_DIR so we can #include\n                    # \"examples/arrow/helloworld.pb.h\" instead of\n                    # \"helloworld.pb.h\" (which fails lint)\n                    EXTRA_INCLUDES\n                    ${CMAKE_BINARY_DIR}\n                    EXTRA_LINK_LIBS\n                    ${FLIGHT_GRPC_EXAMPLE_LINK_LIBS}\n                    EXTRA_SOURCES\n                    \"${CMAKE_CURRENT_BINARY_DIR}/helloworld.pb.cc\"\n                    \"${CMAKE_CURRENT_BINARY_DIR}/helloworld.grpc.pb.cc\")\n\n  if(ARROW_FLIGHT_SQL)\n    if(ARROW_BUILD_SHARED AND ARROW_GRPC_USE_SHARED)\n      set(FLIGHT_SQL_EXAMPLES_LINK_LIBS arrow_flight_sql_shared)\n    else()\n      set(FLIGHT_SQL_EXAMPLES_LINK_LIBS arrow_flight_sql_static)\n    endif()\n\n    add_arrow_example(flight_sql_example\n                      DEPENDENCIES\n                      flight-sql-test-server\n                      EXTRA_LINK_LIBS\n                      ${FLIGHT_EXAMPLES_LINK_LIBS}\n                      ${FLIGHT_SQL_EXAMPLES_LINK_LIBS}\n                      gRPC::grpc++\n                      ${ARROW_PROTOBUF_LIBPROTOBUF}\n                      ${GFLAGS_LIBRARIES})\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Red Arrow with RubyGems\nDESCRIPTION: Command line instructions for installing Red Arrow using RubyGems with the rubygems-requirements-system plugin to handle dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow/README.md#2025-04-16_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ gem install rubygems-requirements-system red-arrow\n```\n\n----------------------------------------\n\nTITLE: Defining Map Field in Arrow Schema JSON\nDESCRIPTION: JSON representation of a Map field in the Arrow schema. Includes a 'keysSorted' boolean property and a single struct child with 'key' and 'value' fields.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"map\",\n  \"keysSorted\": /* boolean */\n}\n```\n\n----------------------------------------\n\nTITLE: Warning About Experimental Acero Structures\nDESCRIPTION: This warning advises against using ExecBatch and ExecPlan outside of Acero due to their experimental nature. It recommends using standard structures like RecordBatch and Declaration objects instead.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/overview.rst#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. warning::\n   The structures within Acero, including ``ExecBatch``, are still experimental.  The ``ExecBatch``\n   class should not be used outside of Acero.  Instead, an ``ExecBatch`` should be converted to\n   a more standard structure such as a ``RecordBatch``.\n\n   Similarly, an ExecPlan is an internal concept.  Users creating plans should be using Declaration\n   objects.  APIs for consuming and executing plans should abstract away the details of the underlying\n   plan and not expose the object itself.\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building Parquet Tools Executables with CMake\nDESCRIPTION: This CMake script sets up the build process for Parquet tools executables. It defines a list of tools, creates executable targets for each, links them with the appropriate Parquet library, and configures their installation. The script also handles RPATH settings and adds the tools as dependencies to the main Parquet target.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/tools/parquet/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(PARQUET_BUILD_EXECUTABLES)\n  set(PARQUET_TOOLS\n      parquet-dump-arrow-statistics\n      parquet-dump-footer\n      parquet-dump-schema\n      parquet-reader\n      parquet-scan)\n\n  foreach(TOOL ${PARQUET_TOOLS})\n    string(REGEX REPLACE \"-\" \"_\" TOOL_SOURCE ${TOOL})\n    add_executable(${TOOL} \"${TOOL_SOURCE}.cc\")\n    if(ARROW_BUILD_SHARED)\n      target_link_libraries(${TOOL} parquet_shared)\n    else()\n      target_link_libraries(${TOOL} parquet_static)\n    endif()\n    # Avoid unsetting RPATH when installing\n    set_target_properties(${TOOL} PROPERTIES INSTALL_RPATH_USE_LINK_PATH TRUE)\n    install(TARGETS ${TOOL} ${INSTALL_IS_OPTIONAL}\n            RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR})\n  endforeach(TOOL)\n\n  add_dependencies(parquet ${PARQUET_TOOLS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Backend for Jaeger Integration\nDESCRIPTION: Command to set the tracing backend for sending traces to a HTTP OTLP collector.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/opentelemetry.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ARROW_TRACING_BACKEND=otlp_http\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow Tests in CMake\nDESCRIPTION: Conditionally adds Arrow test targets for generator, gtest utility, and random tests if ARROW_BUILD_TESTS is enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/testing/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_TESTS)\n  add_arrow_test(generator_test)\n  add_arrow_test(gtest_util_test)\n  add_arrow_test(random_test)\n\n  if(ARROW_FILESYSTEM)\n    add_library(arrow_filesystem_example MODULE examplefs.cc)\n    target_link_libraries(arrow_filesystem_example ${ARROW_TEST_LINK_LIBS}\n                          ${ARROW_EXAMPLE_LINK_LIBS})\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Extracting an Arrow Field from Schema by Index\nDESCRIPTION: Example showing how to extract a specific Field from an Arrow Schema by its index position.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_16\n\nLANGUAGE: matlab\nCODE:\n```\n>> arrowSchema\n\narrowSchema =\n\nLetter: string\nNumber: double\n\n% Specify the field to extract by its index (i.e. 2)\n>> field = arrowSchema.field(2)\n\nfield =\n\nNumber: double\n```\n\n----------------------------------------\n\nTITLE: Running C++ Style Checks using Ninja in Apache Arrow\nDESCRIPTION: Commands to build and run style check targets using Ninja, including format (clang-format), lint_cpp_cli, lint (cpplint), and clang-tidy.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ cmake -GNinja ../cpp ...\n$ ninja format lint clang-tidy lint_cpp_cli\n```\n\n----------------------------------------\n\nTITLE: Dataset Implementations Documentation Directive\nDESCRIPTION: Sphinx/Doxygen directives for documenting concrete dataset implementations including filesystem and file formats.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/dataset.rst#2025-04-16_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. doxygengroup:: dataset-implementations\n   :content-only:\n   :members:\n\n.. doxygengroup:: dataset-filesystem\n   :content-only:\n   :members:\n\n.. doxygengroup:: dataset-file-formats\n   :content-only:\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Building All JNI Libraries with Maven (Windows)\nDESCRIPTION: Maven commands to build all JNI libraries except the C Data Interface library on Windows platforms.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n$ cd arrow/java\n$ mvn generate-resources -Pgenerate-libs-jni-windows -N\n$ dir \"../java-dist/bin\"\n|__ arrow_dataset_jni/\n```\n\n----------------------------------------\n\nTITLE: R Package Directory Structure Overview\nDESCRIPTION: Describes the main components and directories in the Apache Arrow R package, highlighting the roles of different file groups and their interactions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/architectural_overview.rst#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- r/R/dplyr-*: Define dplyr syntax verbs for Arrow objects\n- r/R/dplyr-funcs*: Define bindings to Arrow C++ functions\n- arrow/r/src: Contains C++ code connecting libarrow and R package\n- r/tools/cpp: Includes libarrow source package when synced\n- r/man: Generated R documentation\n- r/vignettes: Long-form package guides\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow Temporal Data Types in C++\nDESCRIPTION: This snippet documents the temporal data types in Arrow, such as Date32Type, Time32Type, TimestampType, etc.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygengroup:: temporal-datatypes\n   :content-only:\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Activate Conda Environment\nDESCRIPTION: Activates the `arrow-dev` conda environment, allowing for the use of its packaged dependencies during development. After activation, environment variables are adjusted, including `CONDA_PREFIX` for build dependency resolution.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nactivate arrow-dev\n```\n\n----------------------------------------\n\nTITLE: Creating an ArrowSchema PyCapsule in Cython\nDESCRIPTION: This Cython code demonstrates how to create a PyCapsule for an `ArrowSchema`. It allocates memory for the schema, ensures the `release` field is NULL to prevent issues if the schema is incompletely initialized, creates a PyCapsule with a destructor function (`release_arrow_schema_py_capsule`) to release the memory when the capsule is no longer needed.  The capsule name is \"arrow_schema\".\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_12\n\nLANGUAGE: cython\nCODE:\n```\ncimport cpython\nfrom libc.stdlib cimport malloc, free\n\ncdef void release_arrow_schema_py_capsule(object schema_capsule):\n    cdef ArrowSchema* schema = <ArrowSchema*>cpython.PyCapsule_GetPointer(\n        schema_capsule, 'arrow_schema'\n    )\n    if schema.release != NULL:\n        schema.release(schema)\n\n    free(schema)\n\ncdef object export_arrow_schema_py_capsule():\n    cdef ArrowSchema* schema = <ArrowSchema*>malloc(sizeof(ArrowSchema))\n    # It's recommended to immediately wrap the struct in a capsule, so\n    # if subsequent lines raise an exception memory will not be leaked.\n    schema.release = NULL\n    capsule = cpython.PyCapsule_New(\n        <void*>schema, 'arrow_schema', release_arrow_schema_py_capsule\n    )\n    # Fill in ArrowSchema fields:\n    # schema.format = ...\n    # ...\n    return capsule\n```\n\n----------------------------------------\n\nTITLE: Testing Invalid Argument for Arrow Format in R\nDESCRIPTION: This code tests the error handling when an invalid argument is used with the Arrow format in write_dataset. It expects an error message with a list of supported arguments.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dataset-write.md#2025-04-16_snippet_2\n\nLANGUAGE: R\nCODE:\n```\nwrite_dataset(df, dst_dir, format = \"arrow\", nonsensical_arg = \"blah-blah\")\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Flight Library Target in CMake\nDESCRIPTION: Defines the Arrow Flight library target with shared and static versions, specifying sources, dependencies, link flags, and interface libraries. Handles different linking requirements based on build configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_lib(arrow_flight\n              CMAKE_PACKAGE_NAME\n              ArrowFlight\n              PKG_CONFIG_NAME\n              arrow-flight\n              OUTPUTS\n              ARROW_FLIGHT_LIBRARIES\n              SOURCES\n              ${ARROW_FLIGHT_SRCS}\n              PRECOMPILED_HEADERS\n              \"$<$<COMPILE_LANGUAGE:CXX>:arrow/flight/pch.h>\"\n              DEPENDENCIES\n              flight_grpc_gen\n              SHARED_LINK_FLAGS\n              ${ARROW_VERSION_SCRIPT_FLAGS} # Defined in cpp/arrow/CMakeLists.txt\n              SHARED_LINK_LIBS\n              # We must use gRPC::grpc++ first. If gRPC::grpc++\n              # depends on bundled Abseil, bundled Abseil and system\n              # Abseil may be mixed.\n              #\n              # See also a comment for \"if(ARROW_GCS)\" in\n              # cpp/CMakeLists.txt.\n              ${ARROW_FLIGHT_LINK_LIBS}\n              arrow_shared\n              SHARED_INSTALL_INTERFACE_LIBS\n              Arrow::arrow_shared\n              STATIC_LINK_LIBS\n              ${ARROW_FLIGHT_LINK_LIBS}\n              arrow_static\n              STATIC_INSTALL_INTERFACE_LIBS\n              ${ARROW_FLIGHT_STATIC_INSTALL_INTERFACE_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Type Predicates Usage Example in C++\nDESCRIPTION: Demonstrates template usage with type predicates in Apache Arrow. Shows how to use enable_if_number with template functions to create type-safe implementations that work with numeric types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/utilities.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate<typename TypeClass>\narrow::enable_if_number<TypeClass, RETURN_TYPE> MyFunction(const TypeClass& type) {\n  ..\n}\n\ntemplate<typename ArrayType, typename TypeClass=ArrayType::TypeClass>\narrow::enable_if_number<TypeClass, RETURN_TYPE> MyFunction(const ArrayType& array) {\n  ..\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Unit Test with pytest\nDESCRIPTION: Command to run the specific unit test for the tutorial_min_max function using pytest with the -k parameter to filter for the test name.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_9\n\nLANGUAGE: console\nCODE:\n```\n$ cd python\n$ python -m pytest pyarrow/tests/test_compute.py -k test_tutorial_min_max\n======================== test session starts ==========================\nplatform darwin -- Python 3.9.7, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\n```\n\n----------------------------------------\n\nTITLE: Building Java Modules with Maven\nDESCRIPTION: Commands to build the default Arrow Java modules using Maven. Requires setting JAVA_HOME to the absolute path of your Java installation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ cd arrow/java\n$ export JAVA_HOME=<absolute path to your java home>\n$ java --version\n$ mvn clean install\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up the Arrow Repository\nDESCRIPTION: Commands for cloning the forked Apache Arrow repository and adding the upstream repository for future synchronization.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ git clone https://github.com/<your username>/arrow.git\n$ cd arrow\n$ git remote add upstream https://github.com/apache/arrow\n```\n\n----------------------------------------\n\nTITLE: Building Flight Integration Client Executable in CMake\nDESCRIPTION: Creates the flight-test-integration-client executable by compiling test_integration_client.cc and test_integration.cc source files, and links against the previously configured Flight libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/integration_tests/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(flight-test-integration-client test_integration_client.cc\n                                              test_integration.cc)\ntarget_link_libraries(flight-test-integration-client\n                      ${ARROW_FLIGHT_INTEGRATION_TEST_LINK_LIBS})\n```\n\n----------------------------------------\n\nTITLE: ArrowAsyncProducer Structure Members in C\nDESCRIPTION: Producer structure definition with device type specification and methods for requesting data and canceling operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDeviceDataInterface.rst#2025-04-16_snippet_7\n\nLANGUAGE: c\nCODE:\n```\nArrowDeviceType device_type;\nvoid (*request)(struct ArrowAsyncProducer*, uint64_t);\nvoid (*cancel)(struct ArrowAsyncProducer*);\n```\n\n----------------------------------------\n\nTITLE: Using os-maven-plugin for Arrow Flight Dependencies\nDESCRIPTION: This Maven snippet includes the os-maven-plugin for handling platform-dependent properties required for the Arrow Flight module. This plugin is essential for resolving transitive dependencies correctly for different operating system architectures.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/install.rst#2025-04-16_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>org.example</groupId>\n    <artifactId>demo</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <properties>\n        <arrow.version>9.0.0</arrow.version>\n    </properties>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.arrow</groupId>\n            <artifactId>flight-core</artifactId>\n            <version>${arrow.version}</version>\n        </dependency>\n    </dependencies>\n    <build>\n        <extensions>\n            <extension>\n                <groupId>kr.motd.maven</groupId>\n                <artifactId>os-maven-plugin</artifactId>\n                <version>1.7.0</version>\n            </extension>\n        </extensions>\n    </build>\n</project>\n```\n\n----------------------------------------\n\nTITLE: Compiling JNI Bindings for C Data Interface\nDESCRIPTION: Maven commands to compile the JNI bindings for the C Data Interface. This requires the previously built C Data Interface library.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n$ cd arrow/java\n$ mvn -Darrow.c.jni.dist.dir=<absolute path to your arrow folder>/java-dist/lib -Parrow-c-data clean install\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow KeyValueMetadata Class in C++\nDESCRIPTION: This snippet documents the arrow::KeyValueMetadata class, which represents key-value metadata associated with Arrow objects.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenclass:: arrow::KeyValueMetadata\n```\n\n----------------------------------------\n\nTITLE: Arrow Home Directory Configuration\nDESCRIPTION: Environment variable to specify Arrow libraries directory during source build.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nARROW_HOME=/path/to/arrow\n```\n\n----------------------------------------\n\nTITLE: Struct Layout Example with VarBinary and Int32\nDESCRIPTION: Example demonstrating the memory layout of a Struct<VarBinary, Int32> including child arrays and validity bitmaps for both the struct and its fields.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\n* Length: 4, Null count: 1\n* Validity bitmap buffer:\n  | Byte 0 (validity bitmap) | Bytes 1-63            |\n  |--------------------------|-----------------------|\n  | 00001011                 | 0 (padding)           |\n\n* Children arrays:\n  * field-0 array (`VarBinary`):\n    * Length: 4, Null count: 1\n    * Validity bitmap buffer:\n      | Byte 0 (validity bitmap) | Bytes 1-63            |\n      |--------------------------|-----------------------|\n      | 00001101                 | 0 (padding)           |\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Arrow IO Tests\nDESCRIPTION: Adds core IO-related test targets for buffered operations, compression, and file handling.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/io/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_test(buffered_test PREFIX \"arrow-io\")\nadd_arrow_test(compressed_test PREFIX \"arrow-io\")\nadd_arrow_test(file_test PREFIX \"arrow-io\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow Gandiva JNI bindings for Java with CMake\nDESCRIPTION: This snippet shows how to enable the Gandiva JNI bindings for Java by passing a boolean flag to CMake.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_GANDIVA_JAVA=ON\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Flight Test Server in CMake\nDESCRIPTION: Sets up the build for the Arrow Flight test server executable, used in unit tests and benchmarks. It's linked against the necessary libraries and added as a dependency to relevant targets.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_TESTS OR ARROW_BUILD_BENCHMARKS)\n  add_executable(flight-test-server test_server.cc)\n  target_link_libraries(flight-test-server ${ARROW_FLIGHT_TEST_LINK_LIBS}\n                        ${GFLAGS_LIBRARIES})\n\n  if(ARROW_BUILD_TESTS)\n    add_dependencies(arrow-flight-test flight-test-server)\n  endif()\n\n  add_dependencies(arrow_flight flight-test-server)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Test and Example Components for Arrow Flight SQL in CMake\nDESCRIPTION: Sets up test and example components for Arrow Flight SQL, including configuring test servers based on SQLite. This code is conditionally included when tests or examples are enabled in the build.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/sql/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# Build test server for unit tests\nif(ARROW_BUILD_TESTS OR ARROW_BUILD_EXAMPLES)\n  find_package(SQLite3Alt REQUIRED)\n\n  set(ARROW_FLIGHT_SQL_TEST_SERVER_SRCS\n      example/sqlite_sql_info.cc\n      example/sqlite_type_info.cc\n      example/sqlite_statement.cc\n      example/sqlite_statement_batch_reader.cc\n      example/sqlite_server.cc\n      example/sqlite_tables_schema_batch_reader.cc)\n\n  set(ARROW_FLIGHT_SQL_TEST_SRCS server_test.cc\n                                 server_session_middleware_internals_test.cc)\n\n  set(ARROW_FLIGHT_SQL_TEST_LIBS ${SQLite3_LIBRARIES})\n  set(ARROW_FLIGHT_SQL_ACERO_SRCS example/acero_server.cc)\n\n  if(ARROW_COMPUTE\n     AND ARROW_PARQUET\n     AND ARROW_SUBSTRAIT)\n    list(APPEND ARROW_FLIGHT_SQL_TEST_SRCS ${ARROW_FLIGHT_SQL_ACERO_SRCS} acero_test.cc)\n    if(ARROW_FLIGHT_TEST_LINKAGE STREQUAL \"static\")\n      list(APPEND ARROW_FLIGHT_SQL_TEST_LIBS arrow_substrait_static)\n    else()\n      list(APPEND ARROW_FLIGHT_SQL_TEST_LIBS arrow_substrait_shared)\n    endif()\n\n    if(ARROW_BUILD_EXAMPLES)\n      add_executable(acero-flight-sql-server ${ARROW_FLIGHT_SQL_ACERO_SRCS}\n                                             example/acero_main.cc)\n      target_link_libraries(acero-flight-sql-server\n                            PRIVATE ${ARROW_FLIGHT_SQL_TEST_LINK_LIBS}\n                                    ${ARROW_FLIGHT_SQL_TEST_LIBS} ${GFLAGS_LIBRARIES})\n    endif()\n  endif()\n\n  add_arrow_test(flight_sql_test\n                 SOURCES\n                 ${ARROW_FLIGHT_SQL_TEST_SRCS}\n                 ${ARROW_FLIGHT_SQL_TEST_SERVER_SRCS}\n                 STATIC_LINK_LIBS\n                 ${ARROW_FLIGHT_SQL_TEST_LINK_LIBS}\n                 ${ARROW_FLIGHT_SQL_TEST_LIBS}\n                 EXTRA_INCLUDES\n                 \"${CMAKE_CURRENT_BINARY_DIR}/../\"\n                 LABELS\n                 \"arrow_flight_sql\")\n\n  add_executable(flight-sql-test-server test_server_cli.cc\n                                        ${ARROW_FLIGHT_SQL_TEST_SERVER_SRCS})\n  target_link_libraries(flight-sql-test-server\n                        PRIVATE ${ARROW_FLIGHT_SQL_TEST_LINK_LIBS} ${GFLAGS_LIBRARIES}\n                                ${SQLite3_LIBRARIES})\n\n  add_executable(flight-sql-test-app test_app_cli.cc)\n  target_link_libraries(flight-sql-test-app PRIVATE ${ARROW_FLIGHT_SQL_TEST_LINK_LIBS}\n                                                    ${GFLAGS_LIBRARIES})\n\n  if(ARROW_FLIGHT_TEST_LINKAGE STREQUAL \"static\" AND ARROW_BUILD_STATIC)\n    foreach(TEST_TARGET arrow-flight-sql-test flight-sql-test-server flight-sql-test-app)\n      target_compile_definitions(${TEST_TARGET} PUBLIC ARROW_FLIGHT_STATIC\n                                                       ARROW_FLIGHT_SQL_STATIC)\n    endforeach()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow TypeVisitor Class in C++\nDESCRIPTION: This snippet documents the arrow::TypeVisitor class, which is a utility for implementing the visitor pattern for Arrow data types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenclass:: arrow::TypeVisitor\n   :project: arrow_cpp\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow Scalar Cast Test in CMake\nDESCRIPTION: Configures a test target for scalar cast operations in Arrow Compute, linking it with necessary testing libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/kernels/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_test(scalar_cast_test\n               ${ARROW_COMPUTE_TEST_ARGS}\n               SOURCES\n               scalar_cast_test.cc\n               EXTRA_LINK_LIBS\n               arrow_compute_kernels_testing\n               arrow_compute_testing)\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow Flight RPC system with CMake\nDESCRIPTION: This snippet shows how to enable the Arrow Flight RPC system, which depends at least on gRPC.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_FLIGHT=ON\"\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment for Appveyor Builds (Shell)\nDESCRIPTION: This snippet provides a detailed environment setup for replicating Appveyor builds for Apache Arrow on Windows. It includes commands for installing dependencies and configuring environment variables for a static build.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\ncd $EXTRACT_BOOST_DIRECTORY\n.\bootstrap.bat\n@rem This is for static libraries needed for static_crt_build in appveyor\n.\b2 link=static --with-filesystem --with-regex --with-system install\n@rem this should put libraries and headers in c:\\Boost\n\n@rem this might differ for miniconda\nC:\\Users\\User\\Anaconda3\\Scripts\\activate\n\n@rem Change the build type based on which appveyor job you want.\nSET JOB=Static_Crt_Build\nSET GENERATOR=Ninja\nSET APPVEYOR_BUILD_WORKER_IMAGE=Visual Studio 2017\nSET USE_CLCACHE=false\nSET ARROW_BUILD_GANDIVA=OFF\nSET ARROW_LLVM_VERSION=8.0.*\nSET PYTHON=3.9\nSET ARCH=64\nSET PATH=C:\\Users\\User\\Anaconda3;C:\\Users\\User\\Anaconda3\\Scripts;C:\\Users\\User\\Anaconda3\\Library\\bin;%PATH%\nSET BOOST_LIBRARYDIR=C:\\Boost\\lib\nSET BOOST_ROOT=C:\\Boost\n\nconda install -c conda-forge --file .\\ci\\conda_env_cpp.txt\n.\\ci\\appveyor-cpp-setup.bat\n@rem this might fail but at this point most unit tests should be buildable by their individual targets\n@rem see next line for example.\n.\\ci\\appveyor-cpp-build.bat\n@rem you can also just invoke cmake directly with the desired options\ncmake --build . --config Release --target arrow-compute-hash-test\n```\n\n----------------------------------------\n\nTITLE: Comparing ABI Compliance between Arrow Versions\nDESCRIPTION: Command to generate an ABI compliance report between two different versions of the Arrow library using abi-compliance-checker.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nabi-compliance-checker -l libarrow -d1 ABI-PY-9.dump -d2 ABI-PY-10.dump\n```\n\n----------------------------------------\n\nTITLE: Uploading Data with DoPut in C++\nDESCRIPTION: This snippet describes the process of uploading data using the DoPut method. It involves constructing a FlightDescriptor and calling DoPut with FlightData to upload record batches, allowing the server to respond with metadata regarding the upload status.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Flight.rst#2025-04-16_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nConstruct or acquire a ``FlightDescriptor``, as before.\n#. Call ``DoPut(FlightData)`` and upload a stream of Arrow record batches.\n\nThe ``FlightDescriptor`` is included with the first message so the server can identify the dataset.\n\n``DoPut`` allows the server to send response messages back to the client with custom metadata. This can be used to implement things like resumable writes (e.g. the server can periodically send a message indicating how many rows have been committed so far).\n```\n\n----------------------------------------\n\nTITLE: Installing Parquet Encryption Headers in CMake\nDESCRIPTION: CMake command to install all public API headers for the Parquet encryption module. This ensures that the encryption-related header files are properly installed in the system.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/encryption/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"parquet/encryption\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Toolchains for Testing\nDESCRIPTION: XML configuration for setting up Maven toolchains to use a specific JDK version for testing. This XML snippet should be added to the toolchains.xml file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_15\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF8\"?>\n<toolchains>\n\n  [...]\n\n  <toolchain>\n    <type>jdk</type>\n```\n\n----------------------------------------\n\nTITLE: Displaying Array with glimpse() in R Arrow\nDESCRIPTION: Demonstrates using glimpse() on an integer column (Array) from a RecordBatch. The output shows the data type and all values in the array.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_3\n\nLANGUAGE: r\nCODE:\n```\nglimpse(batch$int)\n```\n\n----------------------------------------\n\nTITLE: Running Python Benchmarks with Conbench\nDESCRIPTION: This snippet demonstrates how to run Python benchmarks using conbench.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd benchmarks\nconbench file-read ALL --iterations=3 --all=true --drop-caches=true\n```\n\n----------------------------------------\n\nTITLE: Building and Running Arrow Spark Integration Tests\nDESCRIPTION: These commands build and run Docker containers for testing Arrow's integration with Apache Spark, including C++, Python, and Java components.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/README.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose build conda-cpp\ndocker compose build conda-python\ndocker compose build conda-python-spark\ndocker compose run --rm conda-python-spark\n```\n\n----------------------------------------\n\nTITLE: CPU Instruction Set Support Macros in CMake\nDESCRIPTION: Collection of CMake macros for handling AVX2, BMI2, and AVX512 instruction set support in source files.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(append_runtime_avx2_src SRCS SRC)\n  if(ARROW_HAVE_RUNTIME_AVX2)\n    list(APPEND ${SRCS} ${SRC})\n    set_source_files_properties(${SRC} PROPERTIES SKIP_PRECOMPILE_HEADERS ON)\n    set_source_files_properties(${SRC} PROPERTIES COMPILE_FLAGS ${ARROW_AVX2_FLAG})\n  endif()\nendmacro()\n\nmacro(append_runtime_avx2_bmi2_src SRCS SRC)\n  if(ARROW_HAVE_RUNTIME_AVX2 AND ARROW_HAVE_RUNTIME_BMI2)\n    list(APPEND ${SRCS} ${SRC})\n    set_source_files_properties(${SRC} PROPERTIES SKIP_PRECOMPILE_HEADERS ON)\n    set_source_files_properties(${SRC} PROPERTIES COMPILE_FLAGS\n                                                  \"${ARROW_AVX2_FLAG} ${ARROW_BMI2_FLAG}\")\n  endif()\nendmacro()\n\nmacro(append_runtime_avx512_src SRCS SRC)\n  if(ARROW_HAVE_RUNTIME_AVX512)\n    list(APPEND ${SRCS} ${SRC})\n    set_source_files_properties(${SRC} PROPERTIES SKIP_PRECOMPILE_HEADERS ON)\n    set_source_files_properties(${SRC} PROPERTIES COMPILE_FLAGS ${ARROW_AVX512_FLAG})\n  endif()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Extracting an Arrow Field from Schema by Name\nDESCRIPTION: Example showing how to extract a specific Field from an Arrow Schema by its field name.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_17\n\nLANGUAGE: matlab\nCODE:\n```\n>> arrowSchema\n\narrowSchema =\n\nLetter: string\nNumber: double\n\n% Specify the field to extract by its name (i.e. \"Letter\")\n>> field = arrowSchema.field(\"Letter\")\n\nfield =\n\nLetter: string\n```\n\n----------------------------------------\n\nTITLE: Linking Parquet Examples with Libraries in CMake\nDESCRIPTION: Links the Parquet example executables with the appropriate Parquet libraries. Includes conditional linking for encryption examples if enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/parquet/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(parquet-arrow-example ${PARQUET_EXAMPLE_LINK_LIBS})\ntarget_link_libraries(parquet-low-level-example ${PARQUET_EXAMPLE_LINK_LIBS})\ntarget_link_libraries(parquet-low-level-example2 ${PARQUET_EXAMPLE_LINK_LIBS})\ntarget_link_libraries(parquet-stream-api-example ${PARQUET_EXAMPLE_LINK_LIBS})\n\nif(PARQUET_REQUIRE_ENCRYPTION)\n  target_link_libraries(parquet-encryption-example ${PARQUET_EXAMPLE_LINK_LIBS})\n  target_link_libraries(parquet-encryption-example-all-crypto-options\n                        ${PARQUET_EXAMPLE_LINK_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow UTF8PROC support with CMake\nDESCRIPTION: This snippet shows how to enable building with support for Unicode properties using the utf8proc library.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_34\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_WITH_UTF8PROC=ON\"\n```\n\n----------------------------------------\n\nTITLE: Installing gRPC on macOS using Homebrew\nDESCRIPTION: Explains the command to install gRPC using Homebrew on macOS, which is essential for building the Arrow Flight RPC. Ensure Homebrew is installed before executing the command.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nbrew install grpc\n```\n\n----------------------------------------\n\nTITLE: Configuring MATLAB Path Integration Installation in CMake\nDESCRIPTION: Sets up the necessary variables and scripts to be executed during installation if either MATLAB path integration option is enabled. Uses the UpdateMatlabSearchPath.cmake script to perform the actual MATLAB path modifications.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/CMakeLists.txt#2025-04-16_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(MATLAB_ADD_INSTALL_DIR_TO_SEARCH_PATH OR MATLAB_ADD_INSTALL_DIR_TO_STARTUP_FILE)\n  set(TOOLS_DIR \"${CMAKE_SOURCE_DIR}/tools\")\n\n  # Pass Matlab_MAIN_PROGRAM, TOOLS_DIR, and INSTALL_DIR to the install step\n  # code/script execution scope.\n  install(CODE \"set(Matlab_MAIN_PROGRAM \\\"${Matlab_MAIN_PROGRAM}\\\")\")\n  install(CODE \"set(TOOLS_DIR \\\"${TOOLS_DIR}\\\")\")\n  install(CODE \"set(INSTALL_DIR \\\"${CMAKE_INSTALL_DIR}\\\")\")\n  install(CODE \"set(MATLAB_ADD_INSTALL_DIR_TO_STARTUP_FILE \\\"${MATLAB_ADD_INSTALL_DIR_TO_STARTUP_FILE}\\\")\")\n  install(CODE \"set(MATLAB_ADD_INSTALL_DIR_TO_SEARCH_PATH \\\"${MATLAB_ADD_INSTALL_DIR_TO_SEARCH_PATH}\\\")\")\n\n  # Call the CMake script that runs the MATLAB function to add the install directory\n  # to the MATLAB Search Path or add a command to the MATLAB startup file to add the\n  # install directory to the MATLAB Search Path.\n  install(SCRIPT \"${TOOLS_DIR}/UpdateMatlabSearchPath.cmake\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Schema Exportable Protocol in Python\nDESCRIPTION: This Python code defines a Protocol for objects that can export an Arrow schema via the `__arrow_c_schema__` method. It uses typing hints to specify the expected return type of the method. This is part of the PyCapsule interface for Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple, Protocol\n\nclass ArrowSchemaExportable(Protocol):\n    def __arrow_c_schema__(self) -> object: ...\n```\n\n----------------------------------------\n\nTITLE: Multi-Threading Function References in PyArrow\nDESCRIPTION: Reference documentation for PyArrow's multi-threading related functions including cpu_count, set_cpu_count, io_thread_count, and set_io_thread_count.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/misc.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   cpu_count\n   set_cpu_count\n   io_thread_count\n   set_io_thread_count\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Filesystem Examples in CMake\nDESCRIPTION: Creates a filesystem definition module example and a filesystem usage example. Sets compile definitions to locate the module at runtime.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/arrow/CMakeLists.txt#2025-04-16_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_FILESYSTEM)\n  add_library(filesystem_definition_example MODULE filesystem_definition_example.cc)\n  target_link_libraries(filesystem_definition_example ${ARROW_EXAMPLE_LINK_LIBS})\n\n  add_arrow_example(filesystem_usage_example)\n  target_compile_definitions(filesystem-usage-example\n                             PUBLIC FILESYSTEM_EXAMPLE_LIBPATH=\"$<TARGET_FILE:filesystem_definition_example>\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Basic Benchmarks with Archery\nDESCRIPTION: Basic commands to run benchmarks in the current git workspace and store results in a JSON file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/benchmarks.rst#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Run benchmarks in the current git workspace\narchery benchmark run\n# Storing the results in a file\narchery benchmark run --output=run.json\n```\n\n----------------------------------------\n\nTITLE: Deploying a Ceph Cluster\nDESCRIPTION: This snippet deploys a Ceph cluster with a single in-memory OSD using a provided script.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/dataset_skyhook_scan_example.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./micro-osd.sh /tmp/skyhook\n```\n\n----------------------------------------\n\nTITLE: Building Java Arrow Project with Maven\nDESCRIPTION: Console commands to build the Java project with Maven. This output shows the successful compilation and packaging of the FillTen class with its Arrow dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n$ mvn package\n[INFO] Scanning for projects...\n[INFO]\n[INFO] ------------------< org.apache.arrow.py2java:FillTen >------------------\n[INFO] Building FillTen 1\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ FillTen ---\n[INFO] Changes detected - recompiling the module!\n[INFO] Compiling 1 source file to /experiments/java2py/target/classes\n[INFO]\n[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ FillTen ---\n[INFO] Building jar: /experiments/java2py/target/FillTen-1.jar\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n```\n\n----------------------------------------\n\nTITLE: Handling RecordBatchReader with glimpse() in R Arrow\nDESCRIPTION: Shows how glimpse() handles a RecordBatchReader. Since RecordBatchReaders can only be read once, glimpse() displays a warning message and shows only schema information instead of data.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_8\n\nLANGUAGE: r\nCODE:\n```\nexample_data %>% as_record_batch_reader() %>% glimpse()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Arrow Flight Dependencies in CMake\nDESCRIPTION: Configures the necessary dependencies for Arrow Flight, including gRPC, Protobuf, and platform-specific libraries. Handles special cases for macOS with OpenSSL and Windows.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_FLIGHT_LINK_LIBS gRPC::grpc++ ${ARROW_PROTOBUF_LIBPROTOBUF})\nif(ARROW_WITH_OPENTELEMETRY)\n  list(APPEND ARROW_FLIGHT_LINK_LIBS ${ARROW_OPENTELEMETRY_LIBS})\nendif()\nif(WIN32)\n  list(APPEND ARROW_FLIGHT_LINK_LIBS ws2_32.lib)\nendif()\n# Updating the MACOSX_DEPLOYMENT_TARGET to 12 required us to explicitly\n# link Flight with OpenSSL on macOS. Read this comment for more details:\n# https://github.com/apache/arrow/pull/43137#pullrequestreview-2267476893\nif(APPLE AND ARROW_USE_OPENSSL)\n  list(APPEND ARROW_FLIGHT_LINK_LIBS ${ARROW_OPENSSL_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project for Arrow Test Package\nDESCRIPTION: Sets up a CMake project for testing the Arrow library. The configuration finds the Arrow package, creates an executable from test_package.cpp, links it against the appropriate Arrow library (shared or static), and sets the C++ standard based on the Arrow version (C++11 for versions below 10.0.0, C++17 for newer versions).\nSOURCE: https://github.com/apache/arrow/blob/main/ci/conan/all/test_package/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.8)\nproject(test_package LANGUAGES CXX)\n\nfind_package(Arrow REQUIRED CONFIG)\n\nadd_executable(${PROJECT_NAME} test_package.cpp)\n\nif (TARGET Arrow::arrow_shared)\n    target_link_libraries(${PROJECT_NAME} PRIVATE Arrow::arrow_shared)\nelse()\n    target_link_libraries(${PROJECT_NAME} PRIVATE Arrow::arrow_static)\nendif()\n\nif (${Arrow_VERSION} VERSION_LESS \"10.0.0\")\n    target_compile_features(${PROJECT_NAME} PRIVATE cxx_std_11)\nelse()\n    target_compile_features(${PROJECT_NAME} PRIVATE cxx_std_17)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Statistics Schema Example with Column Indexes\nDESCRIPTION: This example demonstrates how to structure example statistics data within the schema, showing the relationship between column indexes and their corresponding statistics.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/StatisticsSchema.rst#2025-04-16_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nStatistics schema::\n\n    struct<\n      column: int32,\n      statistics: map<\n        key: dictionary<values: utf8, indices: int32>,\n        items: dense_union<0: int64>\n      >\n    >\n\nStatistics array::\n\n    column: [\n      null, # record batch\n      0,    # vendor_id\n      1,    # passenger_count\n    ]\n    statistics:\n      offsets: [\n        0,\n        1, # record batch: 1 value: [0]\n        5, # vendor_id: 4 values: [1, 2, 3, 4]\n        9, # passenger_count: 4 values: [5, 6, 7, 8]\n      ]\n      key:\n        values: [\n          \"ARROW:row_count:exact\",\n          \"ARROW:null_count:exact\",\n          \"ARROW:distinct_count:exact\",\n          \"ARROW:max_value:exact\",\n          \"ARROW:min_value:exact\",\n        ]\n        indices: [\n          0, # \"ARROW:row_count:exact\"\n          1, # \"ARROW:null_count:exact\"\n          2, # \"ARROW:distinct_count:exact\"\n          3, # \"ARROW:max_value:exact\"\n          4, # \"ARROW:min_value:exact\"\n          1, # \"ARROW:null_count:exact\"\n          2, # \"ARROW:distinct_count:exact\"\n          3, # \"ARROW:max_value:exact\"\n          4, # \"ARROW:min_value:exact\"\n        ]\n      items:\n        children:\n          0: [ # int64\n            5, # record batch: \"ARROW:row_count:exact\"\n```\n\n----------------------------------------\n\nTITLE: Installing Substrait Engine Headers in CMake for Apache Arrow\nDESCRIPTION: This command installs all header files from the 'arrow/engine/substrait' directory in the Apache Arrow project. It's part of the build system configuration for the Substrait integration within Arrow's engine component.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/engine/substrait/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/engine/substrait\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow LZ4 compression with CMake\nDESCRIPTION: This snippet shows how to enable support for lz4 compression.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_38\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_WITH_LZ4=ON\"\n```\n\n----------------------------------------\n\nTITLE: Use NMake for Arrow Build\nDESCRIPTION: Commands to set up and build Apache Arrow using NMake. These commands generate and compile a project suited for systems using NMake instead of other build systems.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ncd cpp\nmkdir build\ncd build\ncmake -G \"NMake Makefiles\" ..\nnmake\n```\n\n----------------------------------------\n\nTITLE: Listing Factory Functions for Tables and Arrays in PyArrow\nDESCRIPTION: This code block lists the factory functions available in PyArrow for creating and manipulating tables and arrays.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/tables.rst#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   chunked_array\n   concat_arrays\n   concat_tables\n   record_batch\n   concat_batches\n   table\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow NullType in C++\nDESCRIPTION: This snippet documents the arrow::NullType class, which represents the null data type in Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenclass:: arrow::NullType\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with vcpkg\nDESCRIPTION: Commands to clone Arrow repository and install dependencies using vcpkg package manager.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/apache/arrow.git\ncd arrow\nvcpkg install \\\n  --x-manifest-root cpp \\\n  --feature-flags=versions \\\n  --clean-after-build\n```\n\n----------------------------------------\n\nTITLE: Defining the Structure for Statistics Representation in Apache Arrow\nDESCRIPTION: This code snippet defines the canonical schema for representing statistics about Apache Arrow datasets. It includes columns and statistics as map types, allowing for efficient encoding and retrieval of statistical data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/StatisticsSchema.rst#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nstruct<\n  column: int32,\n  statistics: map<\n    key: dictionary<values: utf8, indices: int32>,\n    items: dense_union<...all needed types...>\n  >\n>\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow ExtensionType in C++\nDESCRIPTION: This snippet documents the arrow::ExtensionType class, which is the base class for custom extension types in Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenclass:: arrow::ExtensionType\n   :members:\n```\n\n----------------------------------------\n\nTITLE: R Testing Example with Altrep Roundtrip\nDESCRIPTION: Demonstration of a testthat expectation function for testing data transformation and preservation of values across different representations\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/step_by_step/testing.rst#2025-04-16_snippet_3\n\nLANGUAGE: R\nCODE:\n```\nx <- c(1, 2, 3, NA_real_)\nexpect_altrep_roundtrip(x, min, na.rm = TRUE)\n```\n\n----------------------------------------\n\nTITLE: Consuming an ArrowSchema PyCapsule in Cython\nDESCRIPTION: This Cython code demonstrates how to consume a PyCapsule containing an `ArrowSchema`. It uses `cpython.PyCapsule_GetPointer` to retrieve the pointer to the `ArrowSchema` struct from the capsule. The function is declared `except NULL` to indicate that it may return NULL if the capsule is invalid.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_14\n\nLANGUAGE: cython\nCODE:\n```\ncimport cpython\n\ncdef ArrowSchema* get_arrow_schema_py_capsule(object capsule) except NULL:\n    return <ArrowSchema*>cpython.PyCapsule_GetPointer(capsule, 'arrow_schema')\n```\n\n----------------------------------------\n\nTITLE: Running JavaScript Benchmarks with Conbench\nDESCRIPTION: This snippet installs JavaScript project dependencies and runs JavaScript benchmarks using conbench.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npushd arrow\nsource dev/conbench_envs/hooks.sh install_java_script_project_dependencies\npopd\ncd benchmarks\nconbench js-micro\n```\n\n----------------------------------------\n\nTITLE: Setting Up Arrow Flight Benchmarks in CMake\nDESCRIPTION: Configures the build process for Arrow Flight benchmarking tools, including a performance server and benchmark executable. It also sets up custom commands for generating protocol buffer files.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_BENCHMARKS)\n  set(PERF_PROTO_GENERATED_FILES \"${CMAKE_CURRENT_BINARY_DIR}/perf.pb.cc\"\n                                 \"${CMAKE_CURRENT_BINARY_DIR}/perf.pb.h\")\n\n  add_custom_command(OUTPUT ${PERF_PROTO_GENERATED_FILES}\n                     COMMAND ${ARROW_PROTOBUF_PROTOC} \"-I${CMAKE_CURRENT_SOURCE_DIR}\"\n                             \"--cpp_out=${CMAKE_CURRENT_BINARY_DIR}\" \"perf.proto\"\n                     DEPENDS ${PROTO_DEPENDS})\n\n  add_executable(arrow-flight-perf-server perf_server.cc perf.pb.cc)\n  target_link_libraries(arrow-flight-perf-server ${ARROW_FLIGHT_TEST_LINK_LIBS}\n                        ${GFLAGS_LIBRARIES})\n\n  add_executable(arrow-flight-benchmark flight_benchmark.cc perf.pb.cc)\n  target_link_libraries(arrow-flight-benchmark ${ARROW_FLIGHT_TEST_LINK_LIBS}\n                        ${GFLAGS_LIBRARIES})\n\n  add_dependencies(arrow-flight-benchmark arrow-flight-perf-server)\n\n  add_dependencies(arrow_flight arrow-flight-benchmark)\nendif(ARROW_BUILD_BENCHMARKS)\n```\n\n----------------------------------------\n\nTITLE: Implementing InputReceived Method in C++ ExecNode\nDESCRIPTION: The InputReceived method is called to pass data between nodes. It's where most of the node's work happens, such as processing input data and pushing results to the next node in the pipeline.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/developer_guide.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nExecNode::InputReceived()\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Filesystem Tests and Benchmarks\nDESCRIPTION: Complex configuration for Amazon S3 filesystem testing including narrative tests, benchmarks, and module tests with specific linking requirements.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/filesystem/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARROW_S3)\n  set(ARROW_S3_TEST_EXTRA_LINK_LIBS)\n  if(ARROW_TEST_LINKAGE STREQUAL \"shared\")\n    list(APPEND ARROW_S3_TEST_EXTRA_LINK_LIBS arrow_shared)\n  else()\n    list(APPEND ARROW_S3_TEST_EXTRA_LINK_LIBS arrow_static)\n  endif()\n  list(APPEND ARROW_S3_TEST_EXTRA_LINK_LIBS ${AWSSDK_LINK_LIBRARIES})\n  add_arrow_test(s3fs_test\n                 SOURCES\n                 s3fs_test.cc\n                 s3_test_util.cc\n                 EXTRA_LABELS\n                 filesystem\n                 EXTRA_LINK_LIBS\n                 ${ARROW_S3_TEST_EXTRA_LINK_LIBS})\n  if(TARGET arrow-s3fs-test)\n    set(ARROW_S3FS_TEST_COMPILE_DEFINITIONS)\n    get_target_property(AWS_CPP_SDK_S3_TYPE aws-cpp-sdk-s3 TYPE)\n    if(AWS_CPP_SDK_S3_TYPE STREQUAL \"STATIC_LIBRARY\"\n       AND CXX_LINKER_SUPPORTS_VERSION_SCRIPT)\n      list(APPEND ARROW_S3FS_TEST_COMPILE_DEFINITIONS \"AWS_CPP_SDK_S3_PRIVATE_STATIC\")\n    endif()\n    target_compile_definitions(arrow-s3fs-test\n                               PRIVATE ${ARROW_S3FS_TEST_COMPILE_DEFINITIONS})\n  endif()\n\n  if(ARROW_BUILD_TESTS)\n    add_executable(arrow-s3fs-narrative-test s3fs_narrative_test.cc)\n    target_link_libraries(arrow-s3fs-narrative-test ${ARROW_TEST_LINK_LIBS}\n                          ${GFLAGS_LIBRARIES})\n    add_dependencies(arrow-tests arrow-s3fs-narrative-test)\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Building JNI C Data Interface Library with Maven (macOS/Linux)\nDESCRIPTION: Maven commands to build only the JNI C Data Interface library on macOS or Linux platforms.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n$ cd arrow/java\n$ export JAVA_HOME=<absolute path to your java home>\n$ java --version\n$ mvn generate-resources -Pgenerate-libs-cdata-all-os -N\n$ ls -latr ../java-dist/lib\n|__ arrow_cdata_jni/\n```\n\n----------------------------------------\n\nTITLE: Maven Manual Installation Commands\nDESCRIPTION: Shell commands to manually install Arrow nightly build artifacts to local Maven repository\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\n$ mkdir nightly-packaging-2022-07-30-0-github-java-jars\n$ cd nightly-packaging-2022-07-30-0-github-java-jars\n$ wget https://github.com/ursacomputing/crossbow/releases/download/nightly-packaging-2022-07-30-0-github-java-jars/arrow-java-root-9.0.0.dev501.pom\n[...additional wget commands...]\n$ mvn install:install-file -Dfile=\"$(pwd)/arrow-java-root-9.0.0.dev501.pom\" -DgroupId=org.apache.arrow -DartifactId=arrow-java-root -Dversion=9.0.0.dev501 -Dpackaging=pom\n[...additional mvn install commands...]\n```\n\n----------------------------------------\n\nTITLE: Adding Parquet Example Dependencies in CMake\nDESCRIPTION: Adds dependencies to ensure that the Parquet examples are built after the main Parquet target. Includes conditional dependencies for encryption examples if enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/parquet/CMakeLists.txt#2025-04-16_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_dependencies(parquet\n                 parquet-low-level-example\n                 parquet-low-level-example2\n                 parquet-arrow-example\n                 parquet-stream-api-example)\n\nif(PARQUET_REQUIRE_ENCRYPTION)\n  add_dependencies(parquet parquet-encryption-example\n                   parquet-encryption-example-all-crypto-options)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Integer ChunkedArray from Range in R\nDESCRIPTION: Creates a ChunkedArray of integer type with a single chunk using a sequence from 1 to 30. This demonstrates creating a chunked array with just one chunk containing a sequence of integers.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/test-chunked-array.txt#2025-04-16_snippet_2\n\nLANGUAGE: r\nCODE:\n```\nchunked_array(1:30)\n```\n\n----------------------------------------\n\nTITLE: Setting Gandiva Export Definitions\nDESCRIPTION: Adds compiler definitions to properly handle symbol exporting when building the library. Sets GANDIVA_EXPORTING for all library targets to ensure symbols are correctly exported, and adds GANDIVA_STATIC for static builds on Windows.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/CMakeLists.txt#2025-04-16_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(LIB_TARGET ${GANDIVA_LIBRARIES})\n  target_compile_definitions(${LIB_TARGET} PRIVATE GANDIVA_EXPORTING)\nendforeach()\n\nif(ARROW_BUILD_STATIC AND WIN32)\n  target_compile_definitions(gandiva_static PUBLIC GANDIVA_STATIC)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Benchmarks with Custom CMake Flags\nDESCRIPTION: Example showing how to run benchmarks with custom compiler settings and CMake flags.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/benchmarks.rst#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport CC=clang-8 CXX=clang++8\narchery benchmark run --cmake-extras=\"-DARROW_SIMD_LEVEL=NONE\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Acero Examples in CMake\nDESCRIPTION: Sets up Arrow Acero example with appropriate linking libraries based on whether shared or static builds are enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/arrow/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_ACERO)\n  if(ARROW_BUILD_SHARED)\n    set(ENGINE_ACERO_EXAMPLE_LINK_LIBS arrow_acero_shared)\n  else()\n    set(ENGINE_ACERO_EXAMPLE_LINK_LIBS arrow_acero_static)\n  endif()\n  add_arrow_example(acero_register_example EXTRA_LINK_LIBS\n                    ${ENGINE_ACERO_EXAMPLE_LINK_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow Field Class in C++\nDESCRIPTION: This snippet documents the arrow::Field class, which represents a named field in an Arrow schema.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenclass:: arrow::Field\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Creating a New Branch for Feature Development\nDESCRIPTION: Git commands to update the main branch from upstream and create a new feature branch for development.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ git checkout main\n$ git fetch upstream\n$ git pull --ff-only upstream main\n$ git checkout -b ARROW-14977\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow examples with CMake\nDESCRIPTION: This snippet shows how to enable building examples of using the Arrow C++ API.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_44\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_BUILD_EXAMPLES=ON\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Libraries for Arrow Flight in CMake\nDESCRIPTION: Sets up the test interface libraries and link libraries for Arrow Flight based on whether static or shared linkage is used, and adds optional CUDA support if enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_FLIGHT_TEST_INTERFACE_LIBS)\nif(ARROW_BUILD_BENCHMARKS\n   OR ARROW_BUILD_INTEGRATION\n   OR ARROW_BUILD_TESTS)\n  if(ARROW_FLIGHT_TEST_LINKAGE STREQUAL \"static\")\n    if(NOT ARROW_BUILD_STATIC)\n      message(STATUS \"If static Protobuf or gRPC are used, Arrow must be built statically\"\n      )\n      message(STATUS \"(These libraries have global state, and linkage must be consistent)\"\n      )\n      message(FATAL_ERROR \"Must build Arrow statically to link Flight tests statically\")\n    endif()\n    set(ARROW_FLIGHT_TEST_LINK_LIBS arrow_flight_static arrow_flight_testing_static)\n    list(APPEND ARROW_FLIGHT_TEST_LINK_LIBS ${ARROW_TEST_STATIC_LINK_LIBS})\n    if(ARROW_CUDA)\n      list(APPEND ARROW_FLIGHT_TEST_INTERFACE_LIBS arrow_cuda_static)\n      list(APPEND ARROW_FLIGHT_TEST_LINK_LIBS arrow_cuda_static)\n    endif()\n  else()\n    set(ARROW_FLIGHT_TEST_LINK_LIBS arrow_flight_shared arrow_flight_testing_shared\n                                    ${ARROW_TEST_SHARED_LINK_LIBS})\n    if(ARROW_CUDA)\n      list(APPEND ARROW_FLIGHT_TEST_INTERFACE_LIBS arrow_cuda_shared)\n      list(APPEND ARROW_FLIGHT_TEST_LINK_LIBS arrow_cuda_shared)\n    endif()\n  endif()\nendif()\nlist(APPEND ARROW_FLIGHT_TEST_LINK_LIBS gRPC::grpc++)\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Encryption Support\nDESCRIPTION: Conditional build configuration for Parquet encryption support. Creates arrow_python_parquet_encryption library if encryption is enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nset(PYARROW_CPP_ENCRYPTION_SRCS ${PYARROW_CPP_SOURCE_DIR}/parquet_encryption.cc)\nif(NOT PYARROW_BUILD_PARQUET_ENCRYPTION)\n  message(STATUS \"Parquet Encryption is NOT Enabled\")\nelse()\n  if(PARQUET_REQUIRE_ENCRYPTION)\n    add_library(arrow_python_parquet_encryption SHARED ${PYARROW_CPP_ENCRYPTION_SRCS})\n    target_link_libraries(arrow_python_parquet_encryption PUBLIC arrow_python\n                                                                 ${PARQUET_LINK_LIBS})\n    target_compile_definitions(arrow_python_parquet_encryption\n                               PRIVATE ARROW_PYTHON_PARQUET_ENCRYPTION_EXPORTING)\n    set_target_properties(arrow_python_parquet_encryption\n                          PROPERTIES VERSION \"${PYARROW_FULL_SO_VERSION}\"\n                                     SOVERSION \"${PYARROW_SO_VERSION}\")\n    install(TARGETS arrow_python_parquet_encryption\n            ARCHIVE DESTINATION .\n            LIBRARY DESTINATION .\n            RUNTIME DESTINATION .)\n    message(STATUS \"Parquet Encryption Enabled\")\n  else()\n    message(FATAL_ERROR \"You must build Arrow C++ with PARQUET_REQUIRE_ENCRYPTION=ON\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Registering External Functions with NativeFunction Constructor in C++\nDESCRIPTION: The NativeFunction class constructor is used to register external functions in Gandiva. This snippet shows the constructor signature with parameters for defining function metadata including name, aliases, parameter types, return type, nullability behavior, and implementation details.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gandiva/external_func.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nNativeFunction(const std::string& base_name, const std::vector<std::string>& aliases,\n               const DataTypeVector& param_types, const DataTypePtr& ret_type,\n               the ResultNullableType& result_nullable_type, std::string pc_name,\n               int32_t flags = 0);\n```\n\n----------------------------------------\n\nTITLE: Converting Bitcode to C++ Static Data Variable\nDESCRIPTION: Creates a custom command to transform the combined bitcode file into a C++ source file containing a static data variable. This uses a Python script to encode the bitcode as C++ data that can be embedded directly into the Gandiva library.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/precompiled/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_command(OUTPUT ${GANDIVA_PRECOMPILED_CC_PATH}\n                   COMMAND ${PYTHON_EXECUTABLE}\n                           \"${CMAKE_CURRENT_SOURCE_DIR}/../make_precompiled_bitcode.py\"\n                           ${GANDIVA_PRECOMPILED_CC_IN_PATH}\n                           ${GANDIVA_PRECOMPILED_BC_PATH} ${GANDIVA_PRECOMPILED_CC_PATH}\n                   DEPENDS ${GANDIVA_PRECOMPILED_CC_IN_PATH}\n                           ${GANDIVA_PRECOMPILED_BC_PATH})\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow benchmarks with CMake\nDESCRIPTION: This snippet shows how to enable building executable benchmarks.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_43\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_BUILD_BENCHMARKS=ON\"\n```\n\n----------------------------------------\n\nTITLE: Installing Conda Dependencies for Fuzzing\nDESCRIPTION: Commands to install required Clang compiler and runtime libraries via Conda for building fuzz targets.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/fuzzing.rst#2025-04-16_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ conda install clang clangxx compiler-rt\n$ cmake .. --preset=fuzzing\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow integration tests with CMake\nDESCRIPTION: This snippet shows how to enable building additional executables that are used to exercise protocol interoperability between the different Arrow implementations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_45\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_BUILD_INTEGRATION=ON\"\n```\n\n----------------------------------------\n\nTITLE: String Manipulation Function Definitions in Apache Arrow\nDESCRIPTION: This code snippet defines various string manipulation functions in Apache Arrow, including their input types, output types, and associated options. Functions cover operations like substring replacement, case conversion, length calculation, and more.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n+-------------------------+--------+-----------------------------------------+------------------------+-----------------------------------+-------+\n| replace_substring_regex | Unary  | String-like                             | String-like            | :struct:`ReplaceSubstringOptions` | \\(8)  |\n+-------------------------+--------+-----------------------------------------+------------------------+-----------------------------------+-------+\n| utf8_capitalize         | Unary  | String-like                             | String-like            |                                   | \\(9)  |\n+-------------------------+--------+-----------------------------------------+------------------------+-----------------------------------+-------+\n| utf8_length             | Unary  | String-like                             | Int32 or Int64         |                                   | \\(10) |\n+-------------------------+--------+-----------------------------------------+------------------------+-----------------------------------+-------+\n| utf8_lower              | Unary  | String-like                             | String-like            |                                   | \\(9)  |\n+-------------------------+--------+-----------------------------------------+------------------------+-----------------------------------+-------+\n| utf8_replace_slice      | Unary  | String-like                             | String-like            | :struct:`ReplaceSliceOptions`     | \\(7)  |\n+-------------------------+--------+-----------------------------------------+------------------------+-----------------------------------+-------+\n| utf8_reverse            | Unary  | String-like                             | String-like            |                                   | \\(11) |\n+-------------------------+--------+-----------------------------------------+------------------------+-----------------------------------+-------+\n| utf8_swapcase           | Unary  | String-like                             | String-like            |                                   | \\(9)  |\n+-------------------------+--------+-----------------------------------------+------------------------+-----------------------------------+-------+\n| utf8_title              | Unary  | String-like                             | String-like            |                                   | \\(9)  |\n+-------------------------+--------+-----------------------------------------+------------------------+-----------------------------------+-------+\n| utf8_upper              | Unary  | String-like                             | String-like            |                                   | \\(9)  |\n+-------------------------+--------+-----------------------------------------+------------------------+-----------------------------------+-------+\n```\n\n----------------------------------------\n\nTITLE: Creating Release Candidate Branch and Tag\nDESCRIPTION: Process for creating a release candidate branch from the maintenance branch, including updating version numbers and creating git tags.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Start from the updated maintenance branch.\ngit checkout maint-X.Y.Z\n\n# The following script will create a branch for the Release Candidate,\n# place the necessary commits updating the version number and then create a git tag\n# on OSX use gnu-sed with homebrew: brew install gnu-sed (and export to $PATH)\n#\n# <rc-number> starts at 0 and increments every time the Release Candidate is created\n# so for the first RC this would be: dev/release/01-prepare.sh 4.0.0 5.0.0 0\ndev/release/01-prepare.sh <version> <next-version> <rc-number>\n\n# Push the release candidate tag\ngit push -u apache apache-arrow-<version>-rc<rc-number>\n# Push the release candidate branch in order to trigger verification jobs later\ngit push -u apache release-<version>-rc<rc-number>\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow fuzzing with CMake\nDESCRIPTION: This snippet shows how to enable building fuzz targets and related executables.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_49\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_FUZZING=ON\"\n```\n\n----------------------------------------\n\nTITLE: Importing PyArrow Module for Memory Management\nDESCRIPTION: This snippet shows how to import the pyarrow module, which is required to use the memory management features documented in this file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/memory.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: pyarrow\n```\n\n----------------------------------------\n\nTITLE: Displaying VctrsExtensionType with glimpse() in R Arrow\nDESCRIPTION: Shows how glimpse() handles a Table with haven_labelled extension types. The output includes a message about conversion issues with the cat_chr column and notes about viewing full schema details.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_4\n\nLANGUAGE: r\nCODE:\n```\nglimpse(haven)\n```\n\n----------------------------------------\n\nTITLE: Adding Upstream Remote\nDESCRIPTION: Commands to navigate to the arrow directory and add the official Apache Arrow repository as an upstream remote.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/step_by_step/set_up.rst#2025-04-16_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ cd arrow\n$ git remote add upstream https://github.com/apache/arrow\n```\n\n----------------------------------------\n\nTITLE: Setting Up Test Linkage for Arrow Flight in CMake\nDESCRIPTION: Configures the test linkage for Arrow Flight, with special handling for static Protobuf and gRPC. Ensures consistent linkage is used for libraries with global state.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_FLIGHT_TEST_LINKAGE \"${ARROW_TEST_LINKAGE}\")\nif(Protobuf_USE_STATIC_LIBS)\n  message(STATUS \"Linking Arrow Flight tests statically due to static Protobuf\")\n  set(ARROW_FLIGHT_TEST_LINKAGE \"static\")\nendif()\nif(NOT ARROW_GRPC_USE_SHARED)\n  message(STATUS \"Linking Arrow Flight tests statically due to static gRPC\")\n  set(ARROW_FLIGHT_TEST_LINKAGE \"static\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Foreign Keys with CommandGetImportedKeys\nDESCRIPTION: CommandGetImportedKeys allows clients to obtain a list of foreign key columns associated with a specified table. This command is useful for exploring database relationships.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_4\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandGetImportedKeys\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow Substrait support with CMake\nDESCRIPTION: This snippet shows how to enable building with support for Substrait.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_32\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_SUBSTRAIT=ON\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Substrait Library Build in CMake\nDESCRIPTION: Sets up the build target for the Arrow Substrait library, including source files, compilation flags, and linking options. It also configures the installation of headers and manages dependencies for both shared and static builds.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/engine/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(arrow_substrait)\n\narrow_install_all_headers(\"arrow/engine\")\n\nif(NOT ARROW_BUILD_SHARED AND ARROW_BUILD_STATIC)\n  string(APPEND ARROW_SUBSTRAIT_PC_CFLAGS \"${ARROW_SUBSTRAIT_PC_CFLAGS_PRIVATE}\")\n  set(ARROW_SUBSTRAIT_PC_CFLAGS_PRIVATE \"\")\nendif()\n\nset(ARROW_SUBSTRAIT_SRCS\n    substrait/expression_internal.cc\n    substrait/extended_expression_internal.cc\n    substrait/extension_set.cc\n    substrait/extension_types.cc\n    substrait/options.cc\n    substrait/plan_internal.cc\n    substrait/relation_internal.cc\n    substrait/serde.cc\n    substrait/test_plan_builder.cc\n    substrait/type_internal.cc\n    substrait/util_internal.cc\n    substrait/util.cc)\n\nadd_arrow_lib(arrow_substrait\n              CMAKE_PACKAGE_NAME\n              ArrowSubstrait\n              PKG_CONFIG_NAME\n              arrow-substrait\n              OUTPUTS\n              ARROW_SUBSTRAIT_LIBRARIES\n              SOURCES\n              ${ARROW_SUBSTRAIT_SRCS}\n              PRECOMPILED_HEADERS\n              \"$<$<COMPILE_LANGUAGE:CXX>:arrow/engine/pch.h>\"\n              SHARED_LINK_FLAGS\n              ${ARROW_VERSION_SCRIPT_FLAGS}\n              SHARED_LINK_LIBS\n              arrow_dataset_shared\n              substrait\n              SHARED_INSTALL_INTERFACE_LIBS\n              ArrowDataset::arrow_dataset_shared\n              STATIC_LINK_LIBS\n              arrow_dataset_static\n              substrait\n              STATIC_INSTALL_INTERFACE_LIBS\n              ArrowDataset::arrow_dataset_static\n              PRIVATE_INCLUDES\n              ${SUBSTRAIT_INCLUDES})\n\nforeach(LIB_TARGET ${ARROW_SUBSTRAIT_LIBRARIES})\n  target_compile_definitions(${LIB_TARGET} PRIVATE ARROW_ENGINE_EXPORTING)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Uploading Python Packages to PyPI\nDESCRIPTION: This script uploads Python wheels and sdist packages to PyPI using the twine tool.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_14\n\nLANGUAGE: Bash\nCODE:\n```\ndev/release/post-11-python.sh <version>\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on Ubuntu/Debian\nDESCRIPTION: Commands to install required build dependencies on Ubuntu/Debian Linux systems using apt-get package manager.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install \\\n     build-essential \\\n     ninja-build \\\n     cmake\n```\n\n----------------------------------------\n\nTITLE: Running Jaeger All-in-One Docker Container\nDESCRIPTION: Docker command to start Jaeger all-in-one container with OTLP collector enabled and necessary ports exposed for trace collection and UI access.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/opentelemetry.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n  -e COLLECTOR_OTLP_ENABLED=true \\\n  -p 16686:16686 \\\n  -p 4317:4317 \\\n  -p 4318:4318 \\\n  jaegertracing/all-in-one:1.35\n```\n\n----------------------------------------\n\nTITLE: Dictionary Encoding Example\nDESCRIPTION: Example of dictionary-encoded VarBinary data showing both the index array and dictionary values layout.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_12\n\nLANGUAGE: data-format\nCODE:\n```\n['foo', 'bar', 'foo', 'bar', null, 'baz']\n\ndata VarBinary (dictionary-encoded)\n   index_type: Int32\n   values: [0, 1, 0, 1, null, 2]\n\ndictionary\n   type: VarBinary\n   values: ['foo', 'bar', 'baz']\n```\n\n----------------------------------------\n\nTITLE: Building Skyhook CLS Library\nDESCRIPTION: Configures the build for the Skyhook CLS (Ceph Loadable Storage) library with shared and static versions and necessary dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/skyhook/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_lib(cls_skyhook\n              SOURCES\n              ${ARROW_SKYHOOK_CLS_SOURCES}\n              OUTPUTS\n              ARROW_SKYHOOK_CLS_LIBRARIES\n              SHARED_LINK_LIBS\n              ${ARROW_SKYHOOK_LINK_SHARED}\n              SHARED_PRIVATE_LINK_LIBS\n              arrow::flatbuffers\n              STATIC_LINK_LIBS\n              ${ARROW_SKYHOOK_LINK_STATIC})\n```\n\n----------------------------------------\n\nTITLE: Recording Memory Allocations using Linux Perf with Apache Arrow\nDESCRIPTION: Shell command that demonstrates how to run perf record with probes to capture memory allocations during Arrow's StructArray unit tests. This allows for detailed tracing of memory behavior during test execution.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/memory.rst#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nperf record -g --call-graph dwarf \\\n  $PROBE_ARGS \\\n  ./arrow-array-test --gtest_filter=StructArray*\n```\n\n----------------------------------------\n\nTITLE: Enabling Unity Builds with CMake\nDESCRIPTION: This snippet demonstrates how to enable unity builds in CMake, potentially leading to significantly faster full builds at the cost of increased memory consumption. The option is enabled using a CMake variable.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n\"-DCMAKE_UNITY_BUILD=ON\"\n```\n\n----------------------------------------\n\nTITLE: Cloning Arrow Repository\nDESCRIPTION: Command to clone the forked Apache Arrow repository to local machine.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/step_by_step/set_up.rst#2025-04-16_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ git clone https://github.com/<your username>/arrow.git\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows-Specific Definitions for Arrow Flight Static Library\nDESCRIPTION: Adds a Windows-specific compile definition for the Arrow Flight static library to properly handle symbol visibility.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_STATIC AND WIN32)\n  target_compile_definitions(arrow_flight_static PUBLIC ARROW_FLIGHT_STATIC)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow HDFS integration with CMake\nDESCRIPTION: This snippet shows how to enable Arrow integration with libhdfs for accessing the Hadoop Filesystem.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_HDFS=ON\"\n```\n\n----------------------------------------\n\nTITLE: Building HTML Documentation with Sphinx - Shell\nDESCRIPTION: This code snippet builds the complete documentation using Sphinx after ensuring that the Python bindings library 'pyarrow' is installed in the environment.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/documentation.rst#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npushd arrow/docs\nmake html\npopd\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow tests with CMake\nDESCRIPTION: This snippet shows how to enable building executable unit tests.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_47\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_BUILD_TESTS=ON\"\n```\n\n----------------------------------------\n\nTITLE: Column Binding Constraints\nDESCRIPTION: Explains the requirement for equal row counts when performing column binding operations with RecordBatch objects.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/RecordBatch.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nNon-scalar inputs must have an equal number of rows.\\ni ..1 has 10, ..2 has 2\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow pytest Commands\nDESCRIPTION: Command-line instructions for executing PyArrow tests using pytest, demonstrating ways to run specific tests, test files, or entire test suite\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/step_by_step/testing.rst#2025-04-16_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ pytest pyarrow/tests/test_file.py -k test_your_unit_test\n$ pytest pyarrow/tests/test_file.py\n$ pytest pyarrow\n```\n\n----------------------------------------\n\nTITLE: Automatically Fixing Python Style Issues with Archery\nDESCRIPTION: Runs the Archery linting tool with the --fix option to automatically correct Python style issues in the PyArrow codebase.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ archery lint --python --fix\n```\n\n----------------------------------------\n\nTITLE: Implementing Arrow Interchange Protocol in Python\nDESCRIPTION: This code snippet demonstrates how to implement the Arrow Interchange Protocol. It checks for the availability of the __arrow_c_array__ method and either uses it directly or falls back to a manual export method using C data pointers.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nif hasattr(pa.Array, \"__arrow_c_array__\"):\n    return pa.array(self)\nelse:\n    array_export_ptr = make_array_export_ptr()\n    schema_export_ptr = make_schema_export_ptr()\n    self.export_c_data(array_export_ptr, schema_export_ptr)\n    return pa.Array._import_from_c(array_export_ptr, schema_export_ptr)\n```\n\n----------------------------------------\n\nTITLE: Push Release Tag and Create GitHub Release\nDESCRIPTION: Script for pushing a release tag to GitHub, typically executed by a committer as part of the release process\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\ndev/release/post-01-tag.sh <version> <rc>\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Output Directories in CMake for PyArrow\nDESCRIPTION: Sets up the build output directories for PyArrow, including handling of in-source builds and creating symlinks for the latest build. It also configures output paths for archives, libraries, and executables.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(${CMAKE_SOURCE_DIR} STREQUAL ${CMAKE_CURRENT_BINARY_DIR})\n  set(BUILD_OUTPUT_ROOT_DIRECTORY\n      \"${CMAKE_CURRENT_BINARY_DIR}/build/${BUILD_SUBDIR_NAME}\")\n  # Link build/latest to the current build directory, to avoid developers\n  # accidentally running the latest debug build when in fact they're building\n  # release builds.\n  file(MAKE_DIRECTORY ${BUILD_OUTPUT_ROOT_DIRECTORY})\n  if(NOT APPLE)\n    set(MORE_ARGS \"-T\")\n  endif()\n  execute_process(COMMAND ln ${MORE_ARGS} -sf ${BUILD_OUTPUT_ROOT_DIRECTORY}\n                          ${CMAKE_CURRENT_BINARY_DIR}/build/latest)\nelse()\n  set(BUILD_OUTPUT_ROOT_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}/${BUILD_SUBDIR_NAME}\")\nendif()\n\nmessage(STATUS \"Generator: ${CMAKE_GENERATOR}\")\nmessage(STATUS \"Build output directory: ${BUILD_OUTPUT_ROOT_DIRECTORY}\")\n\n# where to put generated archives (.a files)\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY \"${BUILD_OUTPUT_ROOT_DIRECTORY}\")\nset(ARCHIVE_OUTPUT_DIRECTORY \"${BUILD_OUTPUT_ROOT_DIRECTORY}\")\n\n# where to put generated libraries (.so files)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY \"${BUILD_OUTPUT_ROOT_DIRECTORY}\")\nset(LIBRARY_OUTPUT_DIRECTORY \"${BUILD_OUTPUT_ROOT_DIRECTORY}\")\n\n# where to put generated binaries\nset(EXECUTABLE_OUTPUT_PATH \"${BUILD_OUTPUT_ROOT_DIRECTORY}\")\n```\n\n----------------------------------------\n\nTITLE: Generating MSVC Solution for Arrow\nDESCRIPTION: Commands for generating a Microsoft Visual Studio project solution for Apache Arrow using CMake. Enable tests within the generated solution for the 64-bit architecture using a specified MSVC generator.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncd cpp\nmkdir build\ncd build\ncmake .. -G \"Visual Studio 15 2017\" -A x64 ^\n      -DARROW_BUILD_TESTS=ON\n```\n\n----------------------------------------\n\nTITLE: Testing lubridate mday() Function in R\nDESCRIPTION: R code examples showing how lubridate's mday() function works with different date inputs, returning the day of the month as a numeric value.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_2\n\nLANGUAGE: R\nCODE:\n```\n> library(lubridate)\n> mday(as.Date(\"2000-12-31\"))\n[1] 31\n> mday(ymd(080306))\n[1] 6\n```\n\n----------------------------------------\n\nTITLE: Basic pkg-config Usage with Arrow\nDESCRIPTION: Shell commands demonstrating how to get build flags for Arrow using pkg-config for both shared and static linking.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/build_system.rst#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npkg-config --cflags --libs arrow\n```\n\n----------------------------------------\n\nTITLE: Configuring PyArrow C++ Source Files and Dependencies in CMake\nDESCRIPTION: Sets up the PyArrow C++ source files, including conditional compilation based on Arrow C++ options. It also configures linking with Arrow C++ libraries and handles shared vs static builds.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\n# PyArrow C++\nset(PYARROW_CPP_ROOT_DIR pyarrow/src)\nset(PYARROW_CPP_SOURCE_DIR ${PYARROW_CPP_ROOT_DIR}/arrow/python)\nset(PYARROW_CPP_SRCS\n    ${PYARROW_CPP_SOURCE_DIR}/arrow_to_pandas.cc\n    ${PYARROW_CPP_SOURCE_DIR}/benchmark.cc\n    ${PYARROW_CPP_SOURCE_DIR}/common.cc\n    ${PYARROW_CPP_SOURCE_DIR}/datetime.cc\n    ${PYARROW_CPP_SOURCE_DIR}/decimal.cc\n    ${PYARROW_CPP_SOURCE_DIR}/extension_type.cc\n    ${PYARROW_CPP_SOURCE_DIR}/gdb.cc\n    ${PYARROW_CPP_SOURCE_DIR}/helpers.cc\n    ${PYARROW_CPP_SOURCE_DIR}/inference.cc\n    ${PYARROW_CPP_SOURCE_DIR}/io.cc\n    ${PYARROW_CPP_SOURCE_DIR}/ipc.cc\n    ${PYARROW_CPP_SOURCE_DIR}/numpy_convert.cc\n    ${PYARROW_CPP_SOURCE_DIR}/numpy_init.cc\n    ${PYARROW_CPP_SOURCE_DIR}/numpy_to_arrow.cc\n    ${PYARROW_CPP_SOURCE_DIR}/python_test.cc\n    ${PYARROW_CPP_SOURCE_DIR}/python_to_arrow.cc\n    ${PYARROW_CPP_SOURCE_DIR}/pyarrow.cc\n    ${PYARROW_CPP_SOURCE_DIR}/udf.cc)\nset_source_files_properties(${PYARROW_CPP_SOURCE_DIR}/numpy_init.cc\n                            PROPERTIES SKIP_PRECOMPILE_HEADERS ON\n                                       SKIP_UNITY_BUILD_INCLUSION ON)\n\nset(PYARROW_CPP_LINK_LIBS \"\")\n\n# ... (Additional configuration for various components)\n\nadd_library(arrow_python SHARED ${PYARROW_CPP_SRCS})\ntarget_include_directories(arrow_python PUBLIC ${PYARROW_CPP_ROOT_DIR}\n                                               ${CMAKE_CURRENT_BINARY_DIR}/pyarrow/src)\nif(ARROW_USE_PRECOMPILED_HEADERS)\n  target_precompile_headers(arrow_python PUBLIC\n                            \"$<$<COMPILE_LANGUAGE:CXX>:arrow/python/pch.h>\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating Bitcode Files from Source Files\nDESCRIPTION: Iterates through each source file in the PRECOMPILED_SRCS list, generates bitcode using the gandiva_add_bitcode custom function, and collects the resulting .bc files into GANDIVA_PRECOMPILED_BC_FILES list for further processing.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/precompiled/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(GANDIVA_PRECOMPILED_BC_FILES)\nforeach(SOURCE ${PRECOMPILED_SRCS})\n  gandiva_add_bitcode(${SOURCE})\n  get_filename_component(SOURCE_BASE ${SOURCE} NAME_WE)\n  list(APPEND GANDIVA_PRECOMPILED_BC_FILES ${CMAKE_CURRENT_BINARY_DIR}/${SOURCE_BASE}.bc)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Stream Exportable Protocol in Python\nDESCRIPTION: This Python code defines a Protocol for objects that can export an Arrow stream via the `__arrow_c_stream__` method. It uses typing hints to specify the expected argument and return types of the method. The `requested_schema` argument is optional, and the method returns a single object.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple, Protocol\n\nclass ArrowStreamExportable(Protocol):\n    def __arrow_c_stream__(\n        self,\n        requested_schema: object | None = None\n    ) -> object:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Setting Arrow Test Libraries in CMake\nDESCRIPTION: Creates variables for linking with Arrow testing libraries, defining both static and shared library configurations with testing dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(ARROW_TEST_LINK_TOOLCHAIN ${ARROW_GTEST_GMOCK} ${ARROW_GTEST_GTEST_MAIN})\nset(ARROW_TEST_STATIC_LINK_LIBS arrow::flatbuffers arrow_testing_static arrow_static\n                                ${ARROW_TEST_LINK_TOOLCHAIN})\nset(ARROW_TEST_SHARED_LINK_LIBS arrow::flatbuffers arrow_testing_shared arrow_shared\n                                ${ARROW_TEST_LINK_TOOLCHAIN})\n```\n\n----------------------------------------\n\nTITLE: Building Arrow C++ Libraries Using ExternalProject_Add in CMake\nDESCRIPTION: This function builds the Arrow C++ libraries as an external project. It configures the build with appropriate options, creates imported library targets, and sets up the proper paths for headers and libraries across different platforms.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(build_arrow)\n  set(options)\n  set(one_value_args)\n  set(multi_value_args)\n\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n  if(ARG_UNPARSED_ARGUMENTS)\n    message(SEND_ERROR \"Error: unrecognized arguments: ${ARG_UNPARSED_ARGUMENTS}\")\n  endif()\n\n  set(ARROW_PREFIX \"${CMAKE_CURRENT_BINARY_DIR}/arrow_ep-prefix\")\n  set(ARROW_BINARY_DIR \"${CMAKE_CURRENT_BINARY_DIR}/arrow_ep-build\")\n\n  # Supply -DARROW_CXXFLAGS=-D_DISABLE_CONSTEXPR_MUTEX_CONSTRUCTOR to fix\n  # a segfault on windows. See https://github.com/apache/arrow/issues/42015.\n  set(ARROW_CMAKE_ARGS\n      \"-DCMAKE_INSTALL_PREFIX=${ARROW_PREFIX}\"\n      \"-DCMAKE_INSTALL_LIBDIR=lib\"\n      \"-DARROW_BUILD_STATIC=OFF\"\n      \"-DARROW_CSV=ON\"\n      \"-DARROW_CXXFLAGS=-D_DISABLE_CONSTEXPR_MUTEX_CONSTRUCTOR\")\n\n  add_library(arrow_shared SHARED IMPORTED)\n  set(ARROW_LIBRARY_TARGET arrow_shared)\n\n  # Set the runtime shared library (.dll, .so, or .dylib)\n  if(WIN32)\n    # The shared library (i.e. .dll) is located in the \"bin\" directory.\n    set(ARROW_SHARED_LIBRARY_DIR \"${ARROW_PREFIX}/bin\")\n  else()\n    # The shared library (i.e. .so or .dylib) is located in the \"lib\" directory.\n    set(ARROW_SHARED_LIBRARY_DIR \"${ARROW_PREFIX}/lib\")\n  endif()\n\n  set(ARROW_SHARED_LIB_FILENAME\n      \"${CMAKE_SHARED_LIBRARY_PREFIX}arrow${CMAKE_SHARED_LIBRARY_SUFFIX}\")\n  set(ARROW_SHARED_LIB \"${ARROW_SHARED_LIBRARY_DIR}/${ARROW_SHARED_LIB_FILENAME}\")\n\n  set_target_properties(arrow_shared PROPERTIES IMPORTED_LOCATION ${ARROW_SHARED_LIB})\n\n  # Set the link-time import library (.lib)\n  if(WIN32)\n    # The import library (i.e. .lib) is located in the \"lib\" directory.\n    set(ARROW_IMPORT_LIB_FILENAME\n        \"${CMAKE_IMPORT_LIBRARY_PREFIX}arrow${CMAKE_IMPORT_LIBRARY_SUFFIX}\")\n    set(ARROW_IMPORT_LIB \"${ARROW_PREFIX}/lib/${ARROW_IMPORT_LIB_FILENAME}\")\n\n    set_target_properties(arrow_shared PROPERTIES IMPORTED_IMPLIB ${ARROW_IMPORT_LIB})\n  endif()\n\n  # Set the include directories\n  set(ARROW_INCLUDE_DIR \"${ARROW_PREFIX}/include\")\n  file(MAKE_DIRECTORY \"${ARROW_INCLUDE_DIR}\")\n  set_target_properties(arrow_shared PROPERTIES INTERFACE_INCLUDE_DIRECTORIES\n                                                ${ARROW_INCLUDE_DIR})\n\n  # Set the build byproducts for the ExternalProject build\n  if(WIN32)\n    set(ARROW_BUILD_BYPRODUCTS \"${ARROW_IMPORT_LIB}\")\n  else()\n    set(ARROW_BUILD_BYPRODUCTS \"${ARROW_SHARED_LIB}\")\n  endif()\n\n  # Building the Arrow C++ libraries requires ExternalProject.\n  include(ExternalProject)\n\n  externalproject_add(arrow_ep\n                      SOURCE_DIR \"${CMAKE_SOURCE_DIR}/../cpp\"\n                      BINARY_DIR \"${ARROW_BINARY_DIR}\"\n                      CMAKE_ARGS \"${ARROW_CMAKE_ARGS}\"\n                      BUILD_BYPRODUCTS \"${ARROW_BUILD_BYPRODUCTS}\")\n\n  add_dependencies(${ARROW_LIBRARY_TARGET} arrow_ep)\n\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Defining FixedSizeList Field in Arrow Schema JSON\nDESCRIPTION: JSON representation of a FixedSizeList field in the Arrow schema. Includes a 'listSize' property to specify the fixed length of the list.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"fixedsizelist\",\n  \"listSize\": /* integer */\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow utilities with CMake\nDESCRIPTION: This snippet shows how to enable building executable utilities.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_46\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_BUILD_UTILITIES=ON\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow File System Source Files in CMake\nDESCRIPTION: Sets up the list of source files for Arrow File System functionality, including local, mock, and cloud storage implementations. Additional files are added based on enabled features like Azure, GCS, HDFS, and S3.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_FILESYSTEM)\n  set(ARROW_FILESYSTEM_SRCS\n      filesystem/filesystem.cc\n      filesystem/localfs.cc\n      filesystem/mockfs.cc\n      filesystem/path_util.cc\n      filesystem/util_internal.cc)\n\n  if(ARROW_AZURE)\n    list(APPEND ARROW_FILESYSTEM_SRCS filesystem/azurefs.cc)\n    set_source_files_properties(filesystem/azurefs.cc\n                                PROPERTIES SKIP_PRECOMPILE_HEADERS ON\n                                           SKIP_UNITY_BUILD_INCLUSION ON)\n  endif()\n  if(ARROW_GCS)\n    list(APPEND ARROW_FILESYSTEM_SRCS filesystem/gcsfs.cc filesystem/gcsfs_internal.cc)\n    set_source_files_properties(filesystem/gcsfs.cc filesystem/gcsfs_internal.cc\n                                PROPERTIES SKIP_PRECOMPILE_HEADERS ON\n                                           SKIP_UNITY_BUILD_INCLUSION ON)\n  endif()\n  if(ARROW_HDFS)\n    list(APPEND ARROW_FILESYSTEM_SRCS filesystem/hdfs.cc)\n  endif()\n  if(ARROW_S3)\n    list(APPEND ARROW_FILESYSTEM_SRCS filesystem/s3fs.cc)\n    set_source_files_properties(filesystem/s3fs.cc\n                                PROPERTIES SKIP_PRECOMPILE_HEADERS ON\n                                           SKIP_UNITY_BUILD_INCLUSION ON)\n  endif()\n\n  arrow_add_object_library(ARROW_FILESYSTEM ${ARROW_FILESYSTEM_SRCS})\n  # ... (additional configuration for cloud storage libraries)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring PyArrow Core Library\nDESCRIPTION: Sets up the main arrow_python shared library with appropriate linking, version properties and installation rules. Handles both shared and static build configurations.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARROW_BUILD_SHARED)\n  target_link_libraries(arrow_python PUBLIC ${PYARROW_CPP_LINK_LIBS})\nelse()\n  target_link_libraries(arrow_python PRIVATE ${PYARROW_CPP_LINK_LIBS})\nendif()\ntarget_link_libraries(arrow_python PUBLIC Python3::NumPy)\ntarget_compile_definitions(arrow_python PRIVATE ARROW_PYTHON_EXPORTING)\nset_target_properties(arrow_python PROPERTIES VERSION \"${PYARROW_FULL_SO_VERSION}\"\n                                              SOVERSION \"${PYARROW_SO_VERSION}\")\ninstall(TARGETS arrow_python\n        ARCHIVE DESTINATION .\n        LIBRARY DESTINATION .\n        RUNTIME DESTINATION .)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Arrow C++ Build Artifacts\nDESCRIPTION: Command to remove stale Arrow C++ build artifacts from the build directory\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n$ rm -rf arrow/cpp/build\n```\n\n----------------------------------------\n\nTITLE: Building All JNI Libraries with CMake (macOS/Linux)\nDESCRIPTION: CMake commands to build all JNI libraries except the C Data Interface library on macOS or Linux platforms. This involves building the C++ libraries first and then the JNI bindings.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n$ cd arrow\n$ brew bundle --file=cpp/Brewfile\n# Homebrew Bundle complete! 25 Brewfile dependencies now installed.\n$ brew uninstall aws-sdk-cpp\n#  (We can't use aws-sdk-cpp installed by Homebrew because it has\n#  an issue: https://github.com/aws/aws-sdk-cpp/issues/1809 )\n$ export JAVA_HOME=<absolute path to your java home>\n$ mkdir -p java-dist cpp-jni\n$ cmake \\\n    -S cpp \\\n    -B cpp-jni \\\n    -DARROW_BUILD_SHARED=OFF \\\n    -DARROW_CSV=ON \\\n    -DARROW_DATASET=ON \\\n    -DARROW_DEPENDENCY_SOURCE=BUNDLED \\\n    -DARROW_DEPENDENCY_USE_SHARED=OFF \\\n    -DARROW_FILESYSTEM=ON \\\n    -DARROW_GANDIVA=ON \\\n    -DARROW_GANDIVA_STATIC_LIBSTDCPP=ON \\\n    -DARROW_JSON=ON \\\n    -DARROW_ORC=ON \\\n    -DARROW_PARQUET=ON \\\n    -DARROW_S3=ON \\\n    -DARROW_SUBSTRAIT=ON \\\n    -DARROW_USE_CCACHE=ON \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=java-dist \\\n    -DCMAKE_UNITY_BUILD=ON\n$ cmake --build cpp-jni --target install --config Release\n$ cmake \\\n    -S java \\\n    -B java-jni \\\n    -DARROW_JAVA_JNI_ENABLE_C=OFF \\\n    -DARROW_JAVA_JNI_ENABLE_DEFAULT=ON \\\n    -DBUILD_TESTING=OFF \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=java-dist \\\n    -DCMAKE_PREFIX_PATH=$PWD/java-dist \\\n    -DProtobuf_ROOT=$PWD/../cpp-jni/protobuf_ep-install \\\n    -DProtobuf_USE_STATIC_LIBS=ON\n$ cmake --build java-jni --target install --config Release\n$ ls -latr java-dist/lib/\n|__ arrow_dataset_jni/\n|__ arrow_orc_jni/\n|__ gandiva_jni/\n```\n\n----------------------------------------\n\nTITLE: Creating Int16 Arrow Arrays in C++\nDESCRIPTION: This code shows how to create Arrow Arrays for 16-bit integers, demonstrating the use of a different ArrayBuilder type for larger integer values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/basic_arrow.rst#2025-04-16_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\narrow::Int16Builder year_builder;\nARROW_RETURN_NOT_OK(year_builder.AppendValues(years_data, sizeof(years_data)));\nstd::shared_ptr<arrow::Array> year_array;\nARROW_ASSIGN_OR_RAISE(year_array, year_builder.Finish());\n```\n\n----------------------------------------\n\nTITLE: Installing R Package from Local Tarball\nDESCRIPTION: R command to install the Arrow package from a local tarball to verify that hosted binaries are used correctly on Ubuntu.\nSOURCE: https://github.com/apache/arrow/blob/main/r/PACKAGING.md#2025-04-16_snippet_5\n\nLANGUAGE: r\nCODE:\n```\ninstall.packages(\"arrow_X.Y.Z.tar.gz\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform-Specific Libraries for Arrow in CMake\nDESCRIPTION: Sets up platform-specific system libraries for Arrow. Handles different requirements for Windows, Linux, and ARM platforms including special handling for Raspbian.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\n# ----------------------------------------------------------------------\n# Handle platform-related libraries like -pthread\n\nset(ARROW_SYSTEM_LINK_LIBS)\n\nif(CMAKE_THREAD_LIBS_INIT)\n  string(APPEND ARROW_PC_LIBS_PRIVATE \" ${CMAKE_THREAD_LIBS_INIT}\")\nendif()\n\nif(WIN32)\n  list(APPEND ARROW_SYSTEM_LINK_LIBS \"ws2_32\")\nendif()\n\nif(CMAKE_SYSTEM_NAME STREQUAL \"Linux\")\n  list(APPEND ARROW_SYSTEM_LINK_LIBS rt)\nendif()\n\nlist(APPEND ARROW_SHARED_INSTALL_INTERFACE_LIBS ${ARROW_SYSTEM_LINK_LIBS})\nlist(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS ${ARROW_SYSTEM_LINK_LIBS})\n\n# Need -latomic on Raspbian.\n# See also: https://issues.apache.org/jira/browse/ARROW-12860\nif(${CMAKE_SYSTEM_NAME} STREQUAL \"Linux\" AND ${CMAKE_SYSTEM_PROCESSOR} MATCHES \"armv7\")\n  string(APPEND ARROW_PC_LIBS_PRIVATE \" -latomic\")\n  list(APPEND ARROW_SHARED_INSTALL_INTERFACE_LIBS \"atomic\")\n  list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS \"atomic\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Tests in Terminal\nDESCRIPTION: Command to run tests for the Python implementation of Arrow (PyArrow) from the terminal. This verifies that the PyArrow installation and functionality are working correctly.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ pytest pyarrow\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure in reStructuredText\nDESCRIPTION: This snippet defines the structure of the documentation using reStructuredText directives. It creates a grid layout with cards for different sections of the documentation, including 'Getting started', 'User Guide', 'Examples', 'API Reference', and 'Cookbook'.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. grid:: 1 2 2 2\n   :gutter: 4\n   :padding: 2 2 0 0\n   :class-container: sd-text-center\n\n   .. grid-item-card:: Getting started\n      :class-card: contrib-card\n      :shadow: none\n\n      Start here to gain a basic understanding of Arrow with\n      an installation and linking guide, documentation of\n      conventions used in the codebase, tutorials etc.\n\n      +++\n\n      .. button-link:: getting_started.html\n         :click-parent:\n         :color: primary\n         :expand:\n\n         To Getting started\n\n   .. grid-item-card:: User Guide\n      :class-card: contrib-card\n      :shadow: none\n\n      Explore more specific topics and underlying concepts\n      of Arrow C++\n\n      +++\n\n      .. button-link:: user_guide.html\n         :click-parent:\n         :color: primary\n         :expand:\n\n         To the User Guide\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow jemalloc-based allocator with CMake\nDESCRIPTION: This snippet shows how to enable building the Arrow jemalloc-based allocator.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_JEMALLOC=ON\"\n```\n\n----------------------------------------\n\nTITLE: Arrow Environment Configuration\nDESCRIPTION: Environment variable settings for configuring Arrow package installation and build options on Linux systems.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_13\n\nLANGUAGE: R\nCODE:\n```\nLIBARROW_MINIMAL=true\nARROW_DEPENDENCY_SOURCE=AUTO\nARROW_JSON=OFF\noptions(arrow.use_altrep = FALSE)\noptions(arrow.summarise.sort = TRUE)\noptions(arrow.preserve_row_level_metadata = TRUE)\n```\n\n----------------------------------------\n\nTITLE: Installing CFFI for C Data Interface\nDESCRIPTION: Command to install the CFFI package which is required for using the C Data Interface with PyArrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install cffi\n```\n\n----------------------------------------\n\nTITLE: Testing Arrow's C++ day() Function from R\nDESCRIPTION: R code example demonstrating how to call the Arrow C++ day() function through R's call_function interface to verify its behavior.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_3\n\nLANGUAGE: R\nCODE:\n```\n> call_function(\"day\", Scalar$create(lubridate::ymd(\"2000-12-31\")))\nScalar\n31\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Script with R Integration\nDESCRIPTION: Terminal command showing the execution of a Python script named 'addthree.py' that adds 3 to each element of a PyArrow array. The output shows R's arrow package being attached and the resulting array with values [4, 5, 6].\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_r.rst#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ python addthree.py\nR[write to console]: Attaching package: 'arrow'\nRESULT [\n  4,\n  5,\n  6\n]\n```\n\n----------------------------------------\n\nTITLE: Error Handling for Non-existent Join Columns in Apache Arrow Tables in R\nDESCRIPTION: Demonstrates various error scenarios when attempting to join Arrow tables using column names that don't exist in the datasets. The examples show different patterns of invalid column specifications and the resulting error messages.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-join.md#2025-04-16_snippet_0\n\nLANGUAGE: r\nCODE:\n```\nleft_join(arrow_table(example_data), arrow_table(example_data), by = \"made_up_colname\")\n```\n\nLANGUAGE: r\nCODE:\n```\nleft_join(arrow_table(example_data), arrow_table(example_data), by = c(int = \"made_up_colname\"))\n```\n\nLANGUAGE: r\nCODE:\n```\nleft_join(arrow_table(example_data), arrow_table(example_data), by = c(\n  made_up_colname = \"int\"))\n```\n\nLANGUAGE: r\nCODE:\n```\nleft_join(arrow_table(example_data), arrow_table(example_data), by = c(\n  \"made_up_colname1\", \"made_up_colname2\"))\n```\n\nLANGUAGE: r\nCODE:\n```\nleft_join(arrow_table(example_data), arrow_table(example_data), by = c(\n  made_up_colname1 = \"made_up_colname2\"))\n```\n\n----------------------------------------\n\nTITLE: Update Website Release Notes\nDESCRIPTION: Process for preparing and generating release notes on the Apache Arrow website, including forking and creating a pull request\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone git@github.com:<YOUR_GITHUB_ID>/arrow-site.git ../\ncd ../arrow-site\ngit remote add apache git@github.com:apache/arrow-site.git\ncd -\ndev/release/post-04-website.sh OLD_X.OLD_Y.OLD_Z X.Y.Z\n```\n\n----------------------------------------\n\nTITLE: Using transmute() with stringr operations in Arrow\nDESCRIPTION: A test case showing how the transmute() function handles dots arguments when working with string operations. It demonstrates string concatenation with str_c() and cleaning whitespace with str_squish(), with the latter operation being performed in R after a warning that it's not supported in Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-mutate.md#2025-04-16_snippet_0\n\nLANGUAGE: r\nCODE:\n```\ntbl %>% Table$create() %>% transmute(a = stringr::str_c(padded_strings,\n        padded_strings), b = stringr::str_squish(a)) %>% collect()\n```\n\n----------------------------------------\n\nTITLE: Converting Character to Date using as.Date() in Arrow R\nDESCRIPTION: This code attempts to convert a character column to a date format using `as.Date()` within an Arrow data manipulation pipeline. It demonstrates that using multiple `tryFormats` is not supported in Arrow, resulting in an error.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-funcs-datetime.md#2025-04-16_snippet_0\n\nLANGUAGE: R\nCODE:\n```\ntest_df %>% InMemoryDataset$create() %>% transmute(date_char_ymd = as.Date(\n        character_ymd_var, tryFormats = c(\"%Y-%m-%d\", \"%Y/%m/%d\"))) %>% collect()\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Output for Docker Builds\nDESCRIPTION: Command to run Archery with increased debugging output for troubleshooting Docker build issues.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\narchery --debug docker run ubuntu-cpp\n```\n\n----------------------------------------\n\nTITLE: Setting Parquet Example Link Libraries in CMake\nDESCRIPTION: Determines whether to use shared or static Parquet libraries for linking the examples, based on the ARROW_BUILD_SHARED flag.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/parquet/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_SHARED)\n  set(PARQUET_EXAMPLE_LINK_LIBS parquet_shared)\nelse()\n  set(PARQUET_EXAMPLE_LINK_LIBS parquet_static)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Using ArrowBuf.print() for Debugging in Java\nDESCRIPTION: Demonstrates how to use ArrowBuf.print() to obtain detailed debug information about a buffer when debug mode is enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/memory.rst#2025-04-16_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nimport org.apache.arrow.memory.ArrowBuf;\nimport org.apache.arrow.memory.BufferAllocator;\nimport org.apache.arrow.memory.RootAllocator;\n\ntry (final BufferAllocator allocator = new RootAllocator()) {\n  try (final ArrowBuf buf = allocator.buffer(1024)) {\n    final StringBuilder sb = new StringBuilder();\n    buf.print(sb, /*indent*/ 0);\n    System.out.println(sb.toString());\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Tests for mday() with Date Input\nDESCRIPTION: R test code for verifying the mday() function works correctly with date inputs using the dplyr binding framework.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_6\n\nLANGUAGE: R\nCODE:\n```\ntest_that(\"extract mday from date\", {\n  compare_dplyr_binding(\n    .input %>%\n      mutate(x = mday(date)) %>%\n      collect(),\n    test_df\n  )\n})\n```\n\n----------------------------------------\n\nTITLE: Defining PKG_CONFIG Environment Variable in RST\nDESCRIPTION: Specifies the path to the pkg-config executable, which may be necessary for proper functioning of introspection functions if pkg-config is not available on the system PATH.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/env_vars.rst#2025-04-16_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. envvar:: PKG_CONFIG\n\n   The path to the ``pkg-config`` executable.  This may be required for\n   proper functioning of introspection functions such as\n   :func:`get_library_dirs` if ``pkg-config`` is not available on the system\n   ``PATH``.\n```\n\n----------------------------------------\n\nTITLE: Maven Configuration for Arrow Java Dependencies\nDESCRIPTION: Maven POM file defining dependencies required for the Arrow Java implementation. It includes core Arrow modules like arrow-memory, arrow-vector, and the critical arrow-c-data component necessary for C Data Interface support.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<project>\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>org.apache.arrow.py2java</groupId>\n    <artifactId>FillTen</artifactId>\n    <version>1</version>\n\n    <properties>\n        <maven.compiler.source>8</maven.compiler.source>\n        <maven.compiler.target>8</maven.compiler.target>\n    </properties>\n\n    <dependencies>\n        <dependency>\n        <groupId>org.apache.arrow</groupId>\n        <artifactId>arrow-memory</artifactId>\n        <version>8.0.0</version>\n        <type>pom</type>\n        </dependency>\n        <dependency>\n        <groupId>org.apache.arrow</groupId>\n        <artifactId>arrow-memory-netty</artifactId>\n        <version>8.0.0</version>\n        <type>jar</type>\n        </dependency>\n        <dependency>\n        <groupId>org.apache.arrow</groupId>\n        <artifactId>arrow-vector</artifactId>\n        <version>8.0.0</version>\n        <type>pom</type>\n        </dependency>\n        <dependency>\n        <groupId>org.apache.arrow</groupId>\n        <artifactId>arrow-c-data</artifactId>\n        <version>8.0.0</version>\n        <type>jar</type>\n        </dependency>\n    </dependencies>\n</project>\n```\n\n----------------------------------------\n\nTITLE: Enabling Thread Sanitizer with CMake\nDESCRIPTION: This snippet shows how to enable the Thread Sanitizer to check for races in multi-threaded code.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_51\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_USE_TSAN=ON\"\n```\n\n----------------------------------------\n\nTITLE: Building Subsection of Documentation - Shell\nDESCRIPTION: This snippet shows how to build just the specifications and protocol section of the documentation to speed up the development process.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/documentation.rst#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npushd arrow/docs\nmake format\npopd\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Source Files\nDESCRIPTION: Defines the core source files that make up the Parquet library implementation including readers, writers, encryption, and various utilities.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(PARQUET_SRCS\n    arrow/path_internal.cc\n    arrow/reader.cc\n    arrow/reader_internal.cc\n    arrow/schema.cc\n    arrow/schema_internal.cc\n    arrow/variant_internal.cc\n    arrow/writer.cc\n    bloom_filter.cc\n    bloom_filter_reader.cc\n    column_reader.cc\n    column_scanner.cc\n    column_writer.cc\n    decoder.cc\n    encoder.cc\n    encryption/encryption.cc\n    encryption/internal_file_decryptor.cc\n    encryption/internal_file_encryptor.cc\n    exception.cc\n    file_reader.cc\n    file_writer.cc\n    level_comparison.cc\n    level_conversion.cc\n    metadata.cc\n    xxhasher.cc\n    page_index.cc\n    \"${PARQUET_THRIFT_SOURCE_DIR}/parquet_types.cpp\"\n    platform.cc\n    printer.cc\n    properties.cc\n    schema.cc\n    size_statistics.cc\n    statistics.cc\n    stream_reader.cc\n    stream_writer.cc\n    types.cc)\n```\n\n----------------------------------------\n\nTITLE: Querying GDB auto-load settings\nDESCRIPTION: GDB commands to determine the auto-load directories for automatic loading of debug scripts.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gdb.rst#2025-04-16_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n(gdb) show auto-load scripts-directory\nList of directories from which to load auto-loaded scripts is $debugdir:$datadir/auto-load.\n(gdb) show data-directory\nGDB's data directory is \"/usr/share/gdb\".\n(gdb) show debug-file-directory\nThe directory where separate debug symbols are searched for is \"/usr/lib/debug\".\n```\n\n----------------------------------------\n\nTITLE: Defining RunEndEncoded Field in Arrow Schema JSON\nDESCRIPTION: JSON representation of a RunEndEncoded field in the Arrow schema. Requires two specific child fields: 'run_ends' and 'values'.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"runendencoded\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files for Arrow Flight in CMake\nDESCRIPTION: Lists all the source files that make up the Arrow Flight library, including generated Protocol Buffer files, client and server implementations, middleware, and transport layer components.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\n# Note, we do not compile the generated gRPC sources directly, instead\n# compiling them via protocol_internal.cc which contains some gRPC template\n# overrides to enable Flight-specific optimizations. See comments in\n# protocol_internal.cc\nset(ARROW_FLIGHT_SRCS\n    \"${CMAKE_CURRENT_BINARY_DIR}/Flight.pb.cc\"\n    client.cc\n    client_cookie_middleware.cc\n    client_tracing_middleware.cc\n    cookie_internal.cc\n    middleware.cc\n    serialization_internal.cc\n    server.cc\n    server_auth.cc\n    server_tracing_middleware.cc\n    transport.cc\n    transport_server.cc\n    # Bundle the gRPC impl with libarrow_flight\n    transport/grpc/grpc_client.cc\n    transport/grpc/grpc_server.cc\n    transport/grpc/serialization_internal.cc\n    transport/grpc/protocol_grpc_internal.cc\n    transport/grpc/util_internal.cc\n    types.cc)\n\nif(ARROW_WITH_OPENTELEMETRY)\n  list(APPEND ARROW_FLIGHT_SRCS otel_logging.cc)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Using System Libraries for Static Linking\nDESCRIPTION: Command for running the static build with Arrow's dependencies from the system libraries using Docker Compose.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/minimal_build/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose run --rm static-system-dependency\n```\n\n----------------------------------------\n\nTITLE: Configuring pkg-config Flags for Static-only Arrow Builds\nDESCRIPTION: Adjusts pkg-config flags when only the static Arrow library is built. Moves private flags to public flags for static linking since they need to be exposed to consumers in this configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\n# If libarrow.a is only built, \"pkg-config --cflags --libs arrow\"\n# outputs build flags for static linking not shared\n# linking. ARROW_PC_* except ARROW_PC_*_PRIVATE are for the static\n# linking case.\nif(NOT ARROW_BUILD_SHARED AND ARROW_BUILD_STATIC)\n  string(APPEND ARROW_PC_CFLAGS \"${ARROW_PC_CFLAGS_PRIVATE}\")\n  set(ARROW_PC_CFLAGS_PRIVATE \"\")\n  set(ARROW_PC_LIBS \"${ARROW_PC_LIBS_PRIVATE}\")\n  set(ARROW_PC_LIBS_PRIVATE \"\")\n  set(ARROW_PC_REQUIRES \"${ARROW_PC_REQUIRES_PRIVATE}\")\n  set(ARROW_PC_REQUIRES_PRIVATE \"\")\n\n  string(APPEND ARROW_TESTING_PC_CFLAGS \"${ARROW_TESTING_PC_CFLAGS_PRIVATE}\")\n  set(ARROW_TESTING_PC_CFLAGS_PRIVATE \"\")\nelse()\n  set(ARROW_PC_LIBS \"\")\n  set(ARROW_PC_REQUIRES \"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing PyArrow via Conda\nDESCRIPTION: Command to install PyArrow using the conda package manager from conda-forge channel\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/install.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge pyarrow\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on Alpine Linux\nDESCRIPTION: Commands to install required build dependencies on Alpine Linux using apk package manager.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\napk add autoconf \\\n        bash \\\n        cmake \\\n        g++ \\\n        gcc \\\n        ninja \\\n        make\n```\n\n----------------------------------------\n\nTITLE: Adding Source Files Based on Optional Features\nDESCRIPTION: Conditionally adds source files to the build based on enabled features such as CSV, JSON, ORC, and Parquet support.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/dataset/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_CSV)\n  list(APPEND ARROW_DATASET_SRCS file_csv.cc)\nendif()\n\nif(ARROW_JSON)\n  list(APPEND ARROW_DATASET_SRCS file_json.cc)\nendif()\n\nif(ARROW_ORC)\n  list(APPEND ARROW_DATASET_SRCS file_orc.cc)\nendif()\n\nif(ARROW_PARQUET)\n  list(APPEND ARROW_DATASET_STATIC_LINK_LIBS parquet_static)\n  list(APPEND ARROW_DATASET_SHARED_LINK_LIBS parquet_shared)\n  list(APPEND ARROW_DATASET_STATIC_INSTALL_INTERFACE_LIBS Parquet::parquet_static)\n  list(APPEND ARROW_DATASET_SHARED_INSTALL_INTERFACE_LIBS Parquet::parquet_shared)\n  list(APPEND ARROW_DATASET_SRCS file_parquet.cc)\n  list(APPEND ARROW_DATASET_PRIVATE_INCLUDES ${PROJECT_SOURCE_DIR}/src/parquet)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building and Running the C Data Interface Example\nDESCRIPTION: Command sequence to build the Java classes, update dependencies, and run the Python script that demonstrates the C Data Interface integration.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ mvn package\n$ mvn org.apache.maven.plugins:maven-dependency-plugin:2.7:copy-dependencies -DoutputDirectory=dependencies\n$ python fillten.py\nARRAY <class 'pyarrow.lib.Int64Array'> [\n    1,\n    2,\n    3,\n    4,\n    5,\n    6,\n    7,\n    8,\n    9,\n    10\n]\n```\n\n----------------------------------------\n\nTITLE: Testing Extraction of Year Day from Date - R\nDESCRIPTION: This snippet tests the extraction of the year day from a date using the yday() function from the lubridate package. It employs similar dplyr functions as the previous snippet to ensure data integrity and validity for year day extraction.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_12\n\nLANGUAGE: R\nCODE:\n```\ntest_that(\"extract yday from date\", {\n  compare_dplyr_binding(\n    .input %>%\n      mutate(x = yday(date)) %>%\n      collect(),\n    test_df\n  )\n})\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Table Structure with glimpse() in R Arrow\nDESCRIPTION: Shows how glimpse() formats a Table object and its columns. The function displays row count, column names, data types, and a preview of values for each column, with a message about calling print() for full schema details.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_0\n\nLANGUAGE: r\nCODE:\n```\nglimpse(tab)\n```\n\n----------------------------------------\n\nTITLE: Main Function Implementation for Arrow File I/O\nDESCRIPTION: Main function implementation using Arrow's Status pattern for error handling.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nint main() {\\n  arrow::Status st = RunMain();\\n  if (!st.ok()) {\\n    std::cerr << st << std::endl;\\n    return 1;\\n  }\\n  return 0;\\n}\n```\n\n----------------------------------------\n\nTITLE: Bumping Versions for Next Release\nDESCRIPTION: This script updates version numbers in the project for the next release cycle.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_17\n\nLANGUAGE: Bash\nCODE:\n```\ndev/release/post-12-bump-versions.sh X.Y.Z NEXT_X.NEXT_Y.NEXT_Z\n```\n\n----------------------------------------\n\nTITLE: Comparing Saved Benchmark Results\nDESCRIPTION: Commands for running and comparing benchmarks using saved JSON result files.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/benchmarks.rst#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\narchery benchmark run --output=baseline.json $HOME/arrow/cpp/release-build\ngit checkout some-feature\narchery benchmark run --output=contender.json $HOME/arrow/cpp/release-build\narchery benchmark diff contender.json baseline.json\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow Parquet integration with CMake\nDESCRIPTION: This snippet shows how to enable Apache Parquet libraries and Arrow integration.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_PARQUET=ON\"\n```\n\n----------------------------------------\n\nTITLE: Testing Invalid Argument for CSV Format in R\nDESCRIPTION: This code checks the error handling when an invalid argument is used with the CSV format in write_dataset. It expects an error message with a comprehensive list of supported arguments specific to CSV.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dataset-write.md#2025-04-16_snippet_4\n\nLANGUAGE: R\nCODE:\n```\nwrite_dataset(df, dst_dir, format = \"csv\", nonsensical_arg = \"blah-blah\")\n```\n\n----------------------------------------\n\nTITLE: Arrow Status Return Pattern with ARROW Macros\nDESCRIPTION: Shows how to use Arrow's convenience macros ARROW_ASSIGN_OR_RAISE for error propagation when working with Result types. Demonstrates proper error handling and status propagation patterns.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/conventions.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\narrow::Status DoSomething() {\n   const int64_t buffer_size = 4096;\n   std::shared_ptr<arrow::Buffer> buffer;\n   ARROW_ASSIGN_OR_RAISE(buffer, arrow::AllocateBuffer(buffer_size));\n   // ... allocation successful, do something with buffer below\n\n   // return success at the end\n   return Status::OK();\n}\n```\n\n----------------------------------------\n\nTITLE: Preserving Build Directory for Iterative Development\nDESCRIPTION: Commands for preserving build directories and rerunning benchmarks efficiently during development.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/benchmarks.rst#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# First invocation clone and checkouts in a temporary directory\narchery benchmark diff --preserve\n\n# Re-run benchmark in the previously created build directory\narchery benchmark diff /tmp/arrow-bench*/{WORKSPACE,master}/build\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow JSON Benchmark in CMake\nDESCRIPTION: Adds a benchmark target for Arrow JSON parser. It specifies the prefix and additional link libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/json/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_benchmark(parser_benchmark\n                    PREFIX\n                    \"arrow-json\"\n                    EXTRA_LINK_LIBS\n                    RapidJSON)\n```\n\n----------------------------------------\n\nTITLE: Using Declaration Sequence in C++\nDESCRIPTION: Shows a simplified way to create a linear sequence of declarations using Declaration::Sequence.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/user_guide.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n    auto input_table = GetInputTable();\n    auto project = Declaration::Sequence({\n        {\"table_source\", TableSourceNodeOptions{input_table}},\n        {\"project\", ProjectNodeOptions{/*expressions=*/{field_ref(\"a\")}}}\n    });\n```\n\n----------------------------------------\n\nTITLE: Basic Red Arrow Flight Usage Template\nDESCRIPTION: Basic template showing how to require and use the Arrow Flight library in Ruby. Currently contains a placeholder TODO comment.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-flight/README.md#2025-04-16_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"arrow-flight\"\n\n# TODO\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on Arch Linux\nDESCRIPTION: Commands to install required build dependencies on Arch Linux using pacman package manager.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsudo pacman -S --needed \\\n     base-devel \\\n     ninja \\\n     cmake\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Basic Examples in CMake\nDESCRIPTION: Adds basic Arrow example for row-wise conversion and conditionally adds RapidJSON example when Arrow has RapidJSON support enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/arrow/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_example(row_wise_conversion_example)\n\nif(ARROW_WITH_RAPIDJSON)\n  add_arrow_example(rapidjson_row_converter EXTRA_LINK_LIBS RapidJSON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven for Apache Arrow Staging Repository\nDESCRIPTION: Maven XML configuration for setting up Arrow staging repository access and dependencies with version properties.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_21\n\nLANGUAGE: xml\nCODE:\n```\n<properties>\n   <arrow.version>9.0.0</arrow.version>\n</properties>\n...\n<repositories>\n   <repository>\n         <id>arrow-apache-staging</id>\n         <url>https://repository.apache.org/content/repositories/staging</url>\n   </repository>\n</repositories>\n...\n<dependencies>\n   <dependency>\n         <groupId>org.apache.arrow</groupId>\n         <artifactId>arrow-vector</artifactId>\n         <version>${arrow.version}</version>\n   </dependency>\n</dependencies>\n...\n```\n\n----------------------------------------\n\nTITLE: Handling Dataset Query with glimpse() in R Arrow\nDESCRIPTION: Demonstrates glimpse() behavior with a Dataset aggregation query. The function warns about potential performance issues for full table scans and recommends computing the query first.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_11\n\nLANGUAGE: r\nCODE:\n```\nds %>% summarize(max(int)) %>% glimpse()\n```\n\n----------------------------------------\n\nTITLE: Checking Datum Type for Addition Result in Arrow C++\nDESCRIPTION: Prints the type information of the Datum containing the addition result.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nstd::cout << \"Datum kind: \" << add_result.kind()\n          << \" content type: \" << *add_result.type() << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Setting OpenTelemetry Tracing Backend Environment Variable\nDESCRIPTION: Command to configure the OpenTelemetry tracing backend to output JSON traces to stdout.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/opentelemetry.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport ARROW_TRACING_BACKEND=arrow_otlp_stdout\n```\n\n----------------------------------------\n\nTITLE: Defining Null Field in Arrow Schema JSON\nDESCRIPTION: JSON representation of a Null field in the Arrow schema. Simplest type definition with only a name property.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"null\"\n}\n```\n\n----------------------------------------\n\nTITLE: Error Message for write_feather Function with Invalid Input Type in R/Arrow\nDESCRIPTION: This error message is displayed when attempting to use the write_feather function with an object that cannot be converted to an Arrow Table. The error explains that the object must be coercible via the as_arrow_table() function, and it occurs specifically with objects of class Array, ArrowDatum, ArrowObject, or R6.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/feather.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# write_feather with invalid input type\n\n    Object must be coercible to an Arrow Table using `as_arrow_table()`\n    Caused by error in `as_arrow_table()`:\n    ! No method for `as_arrow_table()` for object of class Array / ArrowDatum / ArrowObject / R6\n```\n\n----------------------------------------\n\nTITLE: Writing Arrow Stream Format\nDESCRIPTION: Demonstrates writing multiple record batches to a stream using PyArrow's RecordBatchStreamWriter.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/ipc.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsink = pa.BufferOutputStream()\n\nwith pa.ipc.new_stream(sink, batch.schema) as writer:\n   for i in range(5):\n      writer.write_batch(batch)\n```\n\n----------------------------------------\n\nTITLE: Updating Checksums for Binary Distributions\nDESCRIPTION: R script to download and update checksums for pre-compiled binaries from the ASF artifactory into the tools directory.\nSOURCE: https://github.com/apache/arrow/blob/main/r/PACKAGING.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nRscript tools/update-checksums.R <libarrow version>\n```\n\n----------------------------------------\n\nTITLE: Running Static Build on Windows\nDESCRIPTION: Command for running the static build script on Windows using the command prompt with Visual Studio command line tools.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/minimal_build/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncall run_static.bat\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Dependency Linking for Arrow in CMake\nDESCRIPTION: Conditional logic to add optional dependencies to Arrow's linking libraries based on enabled features. Adds system libraries when using system-provided dependencies rather than bundled ones.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARROW_GCS)\n  if(google_cloud_cpp_storage_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS google-cloud-cpp::storage)\n  endif()\nendif()\n\nif(ARROW_USE_OPENSSL)\n  list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS ${ARROW_OPENSSL_LIBS})\nendif()\n\nif(ARROW_WITH_BROTLI)\n  if(Brotli_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS ${ARROW_BROTLI_LIBS})\n  endif()\nendif()\n\nif(ARROW_WITH_BZ2)\n  if(BZip2_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS BZip2::BZip2)\n  endif()\nendif()\n\nif(ARROW_WITH_LZ4)\n  if(lz4_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS LZ4::lz4)\n  endif()\nendif()\n\nif(ARROW_WITH_SNAPPY)\n  if(Snappy_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS ${Snappy_TARGET})\n  endif()\nendif()\n\nif(ARROW_WITH_ZLIB)\n  if(ZLIB_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS ZLIB::ZLIB)\n  endif()\nendif()\n\nif(ARROW_WITH_ZSTD)\n  if(zstd_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS ${ARROW_ZSTD_LIBZSTD})\n  endif()\nendif()\n\nif(ARROW_ORC)\n  if(ORC_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS orc::orc)\n  endif()\nendif()\n\nif(ARROW_USE_GLOG)\n  if(GLOG_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS glog::glog)\n  endif()\nendif()\n\nif(ARROW_S3)\n  if(AWSSDK_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND\n         ARROW_STATIC_INSTALL_INTERFACE_LIBS\n         aws-cpp-sdk-identity-management\n         aws-cpp-sdk-sts\n         aws-cpp-sdk-cognito-identity\n         aws-cpp-sdk-s3\n         aws-cpp-sdk-core)\n  elseif(AWSSDK_SOURCE STREQUAL \"BUNDLED\")\n    if(UNIX)\n      list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS CURL::libcurl)\n    endif()\n  endif()\nendif()\n\nif(ARROW_WITH_OPENTELEMETRY)\n  if(opentelemetry_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS ${ARROW_OPENTELEMETRY_LIBS})\n  endif()\n  list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS CURL::libcurl)\nendif()\n\nif(ARROW_WITH_UTF8PROC)\n  if(utf8proc_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS utf8proc::utf8proc)\n  endif()\nendif()\n\nif(ARROW_WITH_RE2)\n  if(re2_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS re2::re2)\n  endif()\nendif()\n\nif(ARROW_PROTOBUF_ARROW_CMAKE_PACKAGE_NAME STREQUAL \"Arrow\")\n  if(Protobuf_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS ${ARROW_PROTOBUF_LIBPROTOBUF})\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Docstring Tests in PyArrow Python Files\nDESCRIPTION: Executes pytest with the doctest-modules flag to verify that all docstring examples in PyArrow .py files are correct and up-to-date.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ pushd arrow/python\n$ python -m pytest --doctest-modules\n$ python -m pytest --doctest-modules path/to/module.py # checking single file\n$ popd\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow JSON Integration Test Build\nDESCRIPTION: CMake configuration that sets up the JSON integration test executable. It handles both test and non-test build scenarios, linking necessary dependencies like RapidJSON and GFlags. The configuration creates either a test executable or a standalone integration test executable based on build flags.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/integration/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\narrow_install_all_headers(\"arrow/integration\")\n\nif(ARROW_BUILD_TESTS)\n  add_arrow_test(json_integration_test EXTRA_LINK_LIBS RapidJSON ${GFLAGS_LIBRARIES})\n  add_dependencies(arrow-integration arrow-json-integration-test)\nelseif(ARROW_BUILD_INTEGRATION)\n  add_executable(arrow-json-integration-test json_integration_test.cc)\n  target_link_libraries(arrow-json-integration-test\n                        RapidJSON\n                        ${ARROW_TEST_LINK_LIBS}\n                        ${GFLAGS_LIBRARIES}\n                        ${ARROW_GTEST_GTEST})\n\n  add_dependencies(arrow-json-integration-test arrow arrow_testing)\n  add_dependencies(arrow-integration arrow-json-integration-test)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Checking Maven POM XML Style with Spotless in Apache Arrow Java\nDESCRIPTION: This command shows how to check Maven POM XML files for style conformance using the Spotless plugin without building the project. It verifies that POM files follow Apache Maven guidelines.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/development.rst#2025-04-16_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n$ mvn spotless:check\n```\n\n----------------------------------------\n\nTITLE: Loading Red Arrow Dataset in Ruby\nDESCRIPTION: Basic Ruby code showing how to require the Arrow Dataset library. This is a placeholder example with a TODO comment indicating that more specific usage examples will be added.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-dataset/README.md#2025-04-16_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"arrow-dataset\"\n\n# TODO\n```\n\n----------------------------------------\n\nTITLE: Linting with Archery in Console\nDESCRIPTION: Demonstrates using the Archery tool to check Python code against the PEP 8 style guide and automatically fix issues where possible. Archery runs the autopep8 formatter and flake8 linter. Not all issues can be automatically corrected; particularly, line length must be manually adjusted.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_10\n\nLANGUAGE: console\nCODE:\n```\n$ archery lint --python --fix\nINFO:archery:Running Python formatter (autopep8)\nINFO:archery:Running Python linter (flake8)\n/Users/alenkafrim/repos/arrow/python/pyarrow/tests/test_compute.py:2288:80: E501 line too long (88 > 79 characters)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on Fedora\nDESCRIPTION: Commands to install required build dependencies on Fedora Linux using dnf package manager.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsudo dnf install \\\n     cmake \\\n     gcc \\\n     gcc-c++ \\\n     ninja-build \\\n     make\n```\n\n----------------------------------------\n\nTITLE: Dataset Discovery Documentation Directive\nDESCRIPTION: Sphinx/Doxygen directive for documenting dataset discovery and factory implementations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/dataset.rst#2025-04-16_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. doxygengroup:: dataset-discovery\n   :content-only:\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Running Docker-based Arrow Minimal Build Example\nDESCRIPTION: Command for running the minimal Arrow C++ build example using Docker Compose, which sets up a basic Ubuntu image with C++ toolchain.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/minimal_build/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose run --rm minimal\n```\n\n----------------------------------------\n\nTITLE: Demonstrating CSV Reading Issue with Timestamp Columns in R\nDESCRIPTION: This code snippet shows a bug in arrow's read_csv_arrow function when reading CSV files with timestamp columns. It fails to properly handle millisecond precision when specifying column types as 'T' or 't'.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/bug_reports.rst#2025-04-16_snippet_1\n\nLANGUAGE: r\nCODE:\n```\nlibrary(arrow, warn.conflicts = FALSE)\ntf <- tempfile()\nwrite.csv(data.frame(x = '2018-10-07 19:04:05.005'), tf, row.names = FALSE)\n\n# successfully read in file\nread_csv_arrow(tf, as_data_frame = TRUE)\n#> # A tibble: 1 × 1\n#>   x\n#>   <dttm>\n#> 1 2018-10-07 20:04:05\n\n# the unit here is seconds - doesn't work\nread_csv_arrow(\n  tf,\n  col_names = \"x\",\n  col_types = \"T\",\n  skip = 1\n)\n#> Error in `handle_csv_read_error()`:\n#> ! Invalid: In CSV column #0: CSV conversion error to timestamp[s]: invalid value '2018-10-07 19:04:05.005'\n\n# the unit here is ms - doesn't work\nread_csv_arrow(\n  tf,\n  col_names = \"x\",\n  col_types = \"t\",\n  skip = 1\n)\n#> Error in `handle_csv_read_error()`:\n#> ! Invalid: In CSV column #0: CSV conversion error to time32[ms]: invalid value '2018-10-07 19:04:05.005'\n\n# the unit here is inferred as ns - does work!\nread_csv_arrow(\n  tf,\n  col_names = \"x\",\n  col_types = \"?\",\n  skip = 1,\n  as_data_frame = FALSE\n)\n#> Table\n#> 1 rows x 1 columns\n#> $x <timestamp[ns]>\n```\n\n----------------------------------------\n\nTITLE: Using across() within summarise() in Arrow\nDESCRIPTION: This snippet tests the use of across() within summarise() in Arrow. It demonstrates that certain expressions may not be valid aggregation expressions or may not be supported in Arrow, resulting in the data being pulled into R for processing.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-summarize.md#2025-04-16_snippet_2\n\nLANGUAGE: R\nCODE:\n```\ndata.frame(x = 1, y = 2) %>% arrow_table() %>% group_by(x) %>% summarise(across(everything())) %>% collect()\n```\n\n----------------------------------------\n\nTITLE: Error Message for Invalid Input in write_parquet() Function\nDESCRIPTION: This snippet shows the error message displayed when trying to use write_parquet() with an object that cannot be converted to an Arrow Table. The error indicates that the as_arrow_table() method is not available for the given object class.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/parquet.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# write_parquet() with invalid input type\n\n    Object must be coercible to an Arrow Table using `as_arrow_table()`\n    Caused by error in `as_arrow_table()`:\n    ! No method for `as_arrow_table()` for object of class Array / ArrowDatum / ArrowObject / R6\n```\n\n----------------------------------------\n\nTITLE: Adding pkg-config Support for Arrow TensorFlow Adapter\nDESCRIPTION: This command adds pkg-config support for the Arrow TensorFlow adapter. It generates the necessary pkg-config files, allowing other projects to easily find and link against the Arrow TensorFlow adapter library.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/adapters/tensorflow/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\narrow_add_pkg_config(\"arrow-tensorflow\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Static Linking Options for GCC\nDESCRIPTION: Configures static linking flags for libstdc++ and libgcc when using GNU compilers and when the static linking option is enabled, ensuring these libraries are statically linked into the final binaries.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_GANDIVA_STATIC_LIBSTDCPP AND (CMAKE_COMPILER_IS_GNUCC OR CMAKE_COMPILER_IS_GNUCXX\n                                      ))\n  list(APPEND GANDIVA_STATIC_LINK_LIBS -static-libstdc++ -static-libgcc)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Validity Bitmap Initialization\nDESCRIPTION: This code snippet shows how validity bitmaps are initialized to indicate nullness or non-nullness of each value slot in an array. The validity bitmap must be large enough to have at least 1 bit for each array slot.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"is_valid[j] -> bitmap[j / 8] & (1 << (j % 8))\"\n```\n\n----------------------------------------\n\nTITLE: Setting CMAKE_PREFIX_PATH for Arrow\nDESCRIPTION: Shell command to set the CMAKE_PREFIX_PATH environment variable to include Arrow installation directory while preserving existing paths.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/build_system.rst#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport CMAKE_PREFIX_PATH=${ARROW_ROOT}${CMAKE_PREFIX_PATH:+:${CMAKE_PREFIX_PATH}}\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Toolkit in CMake for Apache Arrow\nDESCRIPTION: Detects and configures the CUDA toolkit based on the CMake version. For older CMake versions, it uses find_package(CUDA), while newer versions use find_package(CUDAToolkit).\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/gpu/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(DEFINED ENV{CUDA_HOME})\n  set(CUDA_TOOLKIT_ROOT_DIR \"$ENV{CUDA_HOME}\")\nendif()\n\nset(ARROW_CUDA_LINK_LIBS arrow::flatbuffers)\nif(CMAKE_VERSION VERSION_LESS 3.17)\n  find_package(CUDA REQUIRED)\n  add_library(ArrowCUDA::cuda_driver SHARED IMPORTED)\n  set_target_properties(ArrowCUDA::cuda_driver\n                        PROPERTIES IMPORTED_LOCATION \"${CUDA_CUDA_LIBRARY}\"\n                                   INTERFACE_INCLUDE_DIRECTORIES \"${CUDA_INCLUDE_DIRS}\")\n  set(ARROW_CUDA_SHARED_LINK_LIBS ArrowCUDA::cuda_driver)\nelse()\n  find_package(CUDAToolkit REQUIRED)\n  set(ARROW_CUDA_SHARED_LINK_LIBS CUDA::cuda_driver)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Substrait Examples in CMake\nDESCRIPTION: Sets up Arrow Substrait examples with appropriate linking libraries based on whether shared or static builds are enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/arrow/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_SUBSTRAIT)\n  if(ARROW_BUILD_SHARED)\n    set(ENGINE_SUBSTRAIT_CONSUMPTION_LINK_LIBS arrow_substrait_shared)\n  else()\n    set(ENGINE_SUBSTRAIT_CONSUMPTION_LINK_LIBS arrow_substrait_static)\n  endif()\n  add_arrow_example(engine_substrait_consumption EXTRA_LINK_LIBS\n                    ${ENGINE_SUBSTRAIT_CONSUMPTION_LINK_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Java Modules with Archery\nDESCRIPTION: Commands to build Arrow Java modules using Archery, which runs the build process in a Docker container.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n$ cd arrow/java\n$ export JAVA_HOME=<absolute path to your java home>\n$ java --version\n$ archery docker run java\n```\n\n----------------------------------------\n\nTITLE: Testing Expressions on Aggregations in Arrow\nDESCRIPTION: This snippet tests nested aggregation expressions in Arrow using the any() function. It shows that aggregate within aggregate expressions are not supported in Arrow, resulting in the data being pulled into R for processing.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-summarize.md#2025-04-16_snippet_1\n\nLANGUAGE: R\nCODE:\n```\nrecord_batch(tbl) %>% summarise(any(any(lgl)))\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Flight Testing Library in CMake\nDESCRIPTION: Sets up build configurations for the Arrow Flight testing library, including shared and static linking options, and adds the library target with its dependencies and compile definitions.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_TESTING)\n  if(ARROW_BUILD_SHARED AND ARROW_BUILD_STATIC)\n    set(ARROW_FLIGHT_TESTING_SHARED_LINK_LIBS arrow_flight_shared)\n    set(ARROW_FLIGHT_TESTING_STATIC_LINK_LIBS arrow_flight_static)\n    set(ARROW_FLIGHT_TESTING_SHARED_INSTALL_INTERFACE_LIBS\n        ArrowFlight::arrow_flight_shared)\n    set(ARROW_FLIGHT_TESTING_STATIC_INSTALL_INTERFACE_LIBS\n        ArrowFlight::arrow_flight_static)\n  elseif(ARROW_BUILD_SHARED)\n    set(ARROW_FLIGHT_TESTING_SHARED_LINK_LIBS arrow_flight_shared)\n    set(ARROW_FLIGHT_TESTING_STATIC_LINK_LIBS arrow_flight_shared)\n    set(ARROW_FLIGHT_TESTING_SHARED_INSTALL_INTERFACE_LIBS\n        ArrowFlight::arrow_flight_shared)\n    set(ARROW_FLIGHT_TESTING_STATIC_INSTALL_INTERFACE_LIBS\n        ArrowFlight::arrow_flight_shared)\n  else()\n    set(ARROW_FLIGHT_TESTING_SHARED_LINK_LIBS arrow_flight_static)\n    set(ARROW_FLIGHT_TESTING_STATIC_LINK_LIBS arrow_flight_static)\n    set(ARROW_FLIGHT_TESTING_SHARED_INSTALL_INTERFACE_LIBS\n        ArrowFlight::arrow_flight_static)\n    set(ARROW_FLIGHT_TESTING_STATIC_INSTALL_INTERFACE_LIBS\n        ArrowFlight::arrow_flight_static)\n  endif()\n  # ... (additional configuration)\n  add_arrow_lib(arrow_flight_testing\n                CMAKE_PACKAGE_NAME\n                ArrowFlightTesting\n                PKG_CONFIG_NAME\n                arrow-flight-testing\n                OUTPUTS\n                ARROW_FLIGHT_TESTING_LIBRARIES\n                SOURCES\n                test_auth_handlers.cc\n                test_definitions.cc\n                test_flight_server.cc\n                test_util.cc\n                DEPENDENCIES\n                flight_grpc_gen\n                SHARED_LINK_LIBS\n                ${ARROW_FLIGHT_TESTING_SHARED_LINK_LIBS}\n                SHARED_INSTALL_INTERFACE_LIBS\n                ${ARROW_FLIGHT_TESTING_SHARED_INSTALL_INTERFACE_LIBS}\n                STATIC_LINK_LIBS\n                ${ARROW_FLIGHT_TESTING_STATIC_LINK_LIBS}\n                STATIC_INSTALL_INTERFACE_LIBS\n                ${ARROW_FLIGHT_TESTING_STATIC_INSTALL_INTERFACE_LIBS}\n                PRIVATE_INCLUDES\n                \"${Protobuf_INCLUDE_DIRS}\")\n\n  foreach(LIB_TARGET ${ARROW_FLIGHT_TESTING_LIBRARIES})\n    target_compile_definitions(${LIB_TARGET} PRIVATE ARROW_FLIGHT_EXPORTING)\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Checking Built R Package with Devtools\nDESCRIPTION: R command to run comprehensive checks on the built R package tarball using devtools, equivalent to R CMD check but with additional helpful output.\nSOURCE: https://github.com/apache/arrow/blob/main/r/PACKAGING.md#2025-04-16_snippet_1\n\nLANGUAGE: r\nCODE:\n```\ndevtools::check_built(\"arrow_X.Y.Z.tar.gz\")\n```\n\n----------------------------------------\n\nTITLE: Implementing StopProducing Method in C++ ExecNode\nDESCRIPTION: The StopProducing method is called when a plan needs to end early, either due to cancellation or an error. It's used to stop producing new data and clean up resources.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/developer_guide.rst#2025-04-16_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nExecNode::StopProducing()\n```\n\n----------------------------------------\n\nTITLE: Setting CLASSPATH Environment Variable for HDFS Connection\nDESCRIPTION: Sets the CLASSPATH environment variable required for HDFS connections. This command adds all Hadoop jars to the classpath using the hadoop classpath command with the --glob option.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/filesystems.rst#2025-04-16_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nexport CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath --glob`\n# or on Windows\n%HADOOP_HOME%/bin/hadoop classpath --glob > %CLASSPATH%\n```\n\n----------------------------------------\n\nTITLE: Running Linting in Docker for Apache Arrow\nDESCRIPTION: Command to run standardized linting in a Docker container, ensuring consistent linting environments across different developer machines.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ docker compose run ubuntu-lint\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Build with pip in Ubuntu Docker Container\nDESCRIPTION: Command to run the Ubuntu Docker container and execute the build script using pip with a virtual environment. It mounts the current directory and Arrow source directory as volumes.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -t -i -v $PWD:/io -v $PWD/../../..:/arrow arrow_ubuntu_minimal /io/build_venv.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring AVX2 and BMI2 Optimizations\nDESCRIPTION: Sets up compiler flags and source properties for AVX2 and BMI2 optimized code paths, including runtime dispatch capabilities.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/CMakeLists.txt#2025-04-16_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARROW_HAVE_RUNTIME_AVX2)\n  list(APPEND PARQUET_SRCS level_comparison_avx2.cc level_conversion_bmi2.cc)\n  set(AVX2_FLAGS \"${ARROW_AVX2_FLAG}\")\n  if(NOT MSVC)\n    string(APPEND AVX2_FLAGS \" ${CMAKE_CXX_FLAGS_RELEASE}\")\n  endif()\n  set_source_files_properties(level_comparison_avx2.cc\n                              PROPERTIES SKIP_PRECOMPILE_HEADERS ON COMPILE_FLAGS\n                                                                    \"${AVX2_FLAGS}\")\n  if(ARROW_HAVE_RUNTIME_BMI2)\n    set(BMI2_FLAGS \"${AVX2_FLAGS} ${ARROW_BMI2_FLAG} -DARROW_HAVE_BMI2\")\n    set_source_files_properties(level_conversion_bmi2.cc\n                                PROPERTIES SKIP_PRECOMPILE_HEADERS ON COMPILE_FLAGS\n                                                                      \"${BMI2_FLAGS}\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Archery Integration Component\nDESCRIPTION: This command installs the integration component of the Archery utility, which is required for running the Arrow integration tests. The -e flag installs in editable mode, allowing for modifications to the archery package.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ pip install -e \"dev/archery[integration]\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow S3 support with CMake\nDESCRIPTION: This snippet shows how to enable support for Amazon S3-compatible filesystems.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_31\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_S3=ON\"\n```\n\n----------------------------------------\n\nTITLE: Building Documentation from Temporary Index - Shell\nDESCRIPTION: This snippet describes how to build documentation in a single directory without all prerequisites, using Sphinx and a temporary index file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/documentation.rst#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npip install sphinx\ncd arrow/docs\necho $'.. toctree::\\n\\t:glob:\\n\\n\\t*' > ./source/developers/temp_index.rst\nsphinx-build ./source/developers ./source/developers/_build -c ./source -D master_doc=temp_index\nrm ./source/developers/temp_index.rst\n```\n\n----------------------------------------\n\nTITLE: Setting Up IWYU (Include-What-You-Use) for Apache Arrow\nDESCRIPTION: Commands for setting up Include-What-You-Use (IWYU) to clean unnecessary imports. Creates a build directory and configures CMake with the necessary options.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmkdir -p $ARROW_ROOT/cpp/iwyu\ncd $ARROW_ROOT/cpp/iwyu\ncmake -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \\\n  -DARROW_BUILD_BENCHMARKS=ON \\\n  -DARROW_BUILD_BENCHMARKS_REFERENCE=ON \\\n  -DARROW_BUILD_TESTS=ON \\\n  -DARROW_BUILD_UTILITIES=ON \\\n  -DARROW_COMPUTE=ON \\\n  -DARROW_CSV=ON \\\n  -DARROW_DATASET=ON \\\n  -DARROW_FILESYSTEM=ON \\\n  -DARROW_FLIGHT=ON \\\n  -DARROW_GANDIVA=ON \\\n  -DARROW_HDFS=ON \\\n  -DARROW_JSON=ON \\\n  -DARROW_PARQUET=ON \\\n  -DARROW_S3=ON \\\n  -DARROW_WITH_BROTLI=ON \\\n  -DARROW_WITH_BZ2=ON \\\n  -DARROW_WITH_LZ4=ON \\\n  -DARROW_WITH_SNAPPY=ON \\\n  -DARROW_WITH_ZLIB=ON \\\n  -DARROW_WITH_ZSTD=ON \\\n  ..\n```\n\n----------------------------------------\n\nTITLE: Committing Changes in Git\nDESCRIPTION: Instructions for finalizing changes with `git commit` after ensuring all modifications are correct. The command includes a message indicating the nature and purpose of the changes, such as adding new features.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_12\n\nLANGUAGE: console\nCODE:\n```\n$ git commit -am \"Adding a new compute feature for tutorial purposes\"\n[ARROW-14977 170ef85be] Adding a new compute feature for tutorial purposes\n 2 files changed, 51 insertions(+)\n```\n\n----------------------------------------\n\nTITLE: Basic Git Clone Instructions\nDESCRIPTION: Commands to clone the Arrow repository and navigate to the C++ directory.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n$ git clone https://github.com/apache/arrow.git\n$ cd arrow/cpp\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Programming Documentation Structure in RST\nDESCRIPTION: RST markup defining the documentation structure for asynchronous programming features, including Futures class documentation and utility functions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/async.rst#2025-04-16_snippet_1\n\nLANGUAGE: RST\nCODE:\n```\n========================\nAsynchronous programming\n========================\n\nFutures\n=======\n\n.. doxygenclass:: arrow::Future\n   :members:\n\n.. doxygengroup:: future-utilities\n   :content-only:\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents in reStructuredText\nDESCRIPTION: This snippet creates a hidden table of contents for the documentation using the 'toctree' directive in reStructuredText. It includes links to various sections of the documentation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/index.rst#2025-04-16_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n   :hidden:\n\n   getting_started\n   user_guide\n   Examples <examples/index>\n   api\n   C++ cookbook <https://arrow.apache.org/cookbook/cpp/>\n```\n\n----------------------------------------\n\nTITLE: Running R Code Style Checks\nDESCRIPTION: Shell command for running style checks on R code to ensure it conforms to the tidyverse style guide before submission.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_8\n\nLANGUAGE: console\nCODE:\n```\n$ make style\nR -s -e 'setwd(\"...\"); if (requireNamespace(\"styler\")) styler::style_file(setdiff(system(\"git diff --name-only | grep r/.*R$\", intern = TRUE), file.path(\"r\", source(\"r/.styler_excludes.R\")$value)))'\nLoading required namespace: styler\nStyling  2  files:\n r/R/expression.R                             ✔\n r/tests/testthat/test-dplyr-funcs-datetime.R ℹ\n────────────────────────────────────────────\nStatus   Count Legend\n✔  1  File unchanged.\nℹ  1  File changed.\n✖  0  Styling threw an error.\n────────────────────────────────────────────\nPlease review the changes carefully!\n```\n\n----------------------------------------\n\nTITLE: Dissociated IPC URI Query Parameters Specification\nDESCRIPTION: Defines required and optional URI query parameters for implementing the Dissociated IPC Protocol, including data request and memory management parameters\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/DissociatedIPC.rst#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* \"want_data\" - **REQUIRED** - uint64 integer value\n* \"free_data\" - **OPTIONAL** - uint64 integer value\n* \"remote_handle\" - **OPTIONAL** - base64-encoded string\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow C++ Build with CMake\nDESCRIPTION: CMake configuration flags for building Arrow C++ with various optional components and compression libraries enabled. Sets debug build type and enables testing.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_18\n\nLANGUAGE: cmake\nCODE:\n```\n-DCMAKE_BUILD_TYPE=Debug \\\n-DARROW_BUILD_TESTS=ON \\\n-DARROW_COMPUTE=ON \\\n-DARROW_CSV=ON \\\n-DARROW_DATASET=ON \\\n-DARROW_FILESYSTEM=ON \\\n-DARROW_HDFS=ON \\\n-DARROW_JSON=ON \\\n-DARROW_PARQUET=ON \\\n-DARROW_WITH_BROTLI=ON \\\n-DARROW_WITH_BZ2=ON \\\n-DARROW_WITH_LZ4=ON \\\n-DARROW_WITH_SNAPPY=ON \\\n-DARROW_WITH_ZLIB=ON \\\n-DARROW_WITH_ZSTD=ON \\\n-DPARQUET_REQUIRE_ENCRYPTION=ON\n```\n\n----------------------------------------\n\nTITLE: Error Message for Invalid Type Conversion to Arrow Table\nDESCRIPTION: Error message displayed when attempting to convert an object that cannot be coerced to an Arrow Table, specifically when trying to convert a character class object.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/util.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nObject must be coercible to an Arrow Table using `as_arrow_table()`\nCaused by error in `as_arrow_table()`:\n! No method for `as_arrow_table()` for object of class character\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Array Exportable Protocol in Python\nDESCRIPTION: This Python code defines a Protocol for objects that can export an Arrow array via the `__arrow_c_array__` method. It uses typing hints to specify the expected argument and return types of the method. The `requested_schema` argument is optional, and the method returns a tuple of two objects.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple, Protocol\n\nclass ArrowArrayExportable(Protocol):\n    def __arrow_c_array__(\n        self,\n        requested_schema: object | None = None\n    ) -> Tuple[object, object]:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Updating Maintenance Branch for Follow-up Release Candidates\nDESCRIPTION: Commands for updating an existing maintenance branch by cherry-picking specific commits for follow-up release candidates.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# First run in dry-mode to see which commits will be cherry-picked.\n# If there are commits that we don't want to get applied, ensure the\n# milestone on GitHub is set to the following release.\narchery release cherry-pick X.Y.Z --continue\n# Update the maintenance branch with the previous commits\narchery release cherry-pick X.Y.Z --continue --execute\n# Push the updated maintenance branch to the remote repository\ngit push -u apache maint-X.Y.Z\n```\n\n----------------------------------------\n\nTITLE: Configuring MSVC Compiler Options for Generated Protobuf Files in CMake\nDESCRIPTION: Sets source file properties for the generated Protocol Buffer files when building with MSVC, suppressing specific warnings and setting generation-related properties.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n  # Protobuf generated files trigger spurious warnings on MSVC.\n  foreach(GENERATED_SOURCE \"${CMAKE_CURRENT_BINARY_DIR}/Flight.pb.cc\"\n                           \"${CMAKE_CURRENT_BINARY_DIR}/Flight.pb.h\")\n    # Suppress missing dll-interface warning\n    set_source_files_properties(\"${GENERATED_SOURCE}\"\n                                PROPERTIES COMPILE_OPTIONS \"/wd4251\"\n                                           GENERATED TRUE\n                                           SKIP_UNITY_BUILD_INCLUSION TRUE)\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Arrow Pairwise Functions Table\nDESCRIPTION: ASCII table documenting pairwise computation functions for numeric and temporal data types, including options and behavior notes.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/compute.rst#2025-04-16_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n+------------------------+-------+----------------------+----------------------+--------------------------------+----------+\n| Function name          | Arity | Input types          | Output type          | Options class                  | Notes    |\n+========================+=======+======================+======================+================================+==========+\n| pairwise_diff          | Unary | Numeric/Temporal     | Numeric/Temporal     | :struct:`PairwiseOptions`      | \\(1)(2)  |\n+------------------------+-------+----------------------+----------------------+--------------------------------+----------+\n| pairwise_diff_checked  | Unary | Numeric/Temporal     | Numeric/Temporal     | :struct:`PairwiseOptions`      | \\(1)(3)  |\n+------------------------+-------+----------------------+----------------------+--------------------------------+----------+\n```\n\n----------------------------------------\n\nTITLE: Creating gRPC Service for Apache Arrow Flight in .NET\nDESCRIPTION: This command creates a new gRPC service project, which serves as the foundation for the Flight server.\nSOURCE: https://github.com/apache/arrow/blob/main/csharp/examples/FlightAspServerExample/readme.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndotnet new grpc\n```\n\n----------------------------------------\n\nTITLE: Configuring Thread Support for Arrow in CMake\nDESCRIPTION: Adds threading support libraries when threading is enabled. These are added to both private and interface linking libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARROW_ENABLE_THREADING)\n  list(APPEND ARROW_SHARED_PRIVATE_LINK_LIBS Threads::Threads)\n  list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS Threads::Threads)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Device Array Exportable Protocol in Python\nDESCRIPTION: This Python code defines a Protocol for objects that can export an Arrow device array via the `__arrow_c_device_array__` method. It uses typing hints to specify the expected argument and return types of the method. The `requested_schema` argument is optional, and the method accepts arbitrary keyword arguments. It returns a tuple of two objects.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple, Protocol\n\nclass ArrowDeviceArrayExportable(Protocol):\n    def __arrow_c_device_array__(\n        self,\n        requested_schema: object | None = None,\n        **kwargs,\n    ) -> Tuple[object, object]:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Maven Dependency Configuration for Apache Nightlies\nDESCRIPTION: Maven POM configuration to use Arrow nightly builds from Apache nightlies repository\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_17\n\nLANGUAGE: xml\nCODE:\n```\n<properties>\n   <arrow.version>9.0.0.dev501</arrow.version>\n</properties>\n...\n<repositories>\n   <repository>\n         <id>arrow-apache-nightlies</id>\n         <url>https://nightlies.apache.org/arrow/java</url>\n   </repository>\n</repositories>\n...\n<dependencies>\n   <dependency>\n         <groupId>org.apache.arrow</groupId>\n         <artifactId>arrow-vector</artifactId>\n         <version>${arrow.version}</version>\n   </dependency>\n</dependencies>\n```\n\n----------------------------------------\n\nTITLE: Comparing Integer Field with String Literal in Arrow\nDESCRIPTION: Example showing an unsupported expression in Arrow where an integer field is compared with a string literal '5'. This operation is not supported due to Arrow's strict type matching requirements.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-query.md#2025-04-16_snippet_0\n\nLANGUAGE: r\nCODE:\n```\nint == \"5\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Acero Test Targets in CMake\nDESCRIPTION: Adds individual test targets for Arrow Acero components. Each target corresponds to specific functionality in the Acero library, including plan execution, nodes, joins, and aggregations.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/acero/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_acero_test(plan_test SOURCES plan_test.cc test_nodes_test.cc)\nadd_arrow_acero_test(source_node_test SOURCES source_node_test.cc)\nadd_arrow_acero_test(fetch_node_test SOURCES fetch_node_test.cc)\nadd_arrow_acero_test(order_by_node_test SOURCES order_by_node_test.cc)\nadd_arrow_acero_test(hash_join_node_test SOURCES hash_join_node_test.cc\n                     bloom_filter_test.cc)\nadd_arrow_acero_test(pivot_longer_node_test SOURCES pivot_longer_node_test.cc)\n\nadd_arrow_acero_test(asof_join_node_test SOURCES asof_join_node_test.cc)\nadd_arrow_acero_test(sorted_merge_node_test SOURCES sorted_merge_node_test.cc)\n\nadd_arrow_acero_test(tpch_node_test SOURCES tpch_node_test.cc)\nadd_arrow_acero_test(union_node_test SOURCES union_node_test.cc)\nadd_arrow_acero_test(aggregate_node_test SOURCES aggregate_node_test.cc)\nadd_arrow_acero_test(util_test SOURCES util_test.cc task_util_test.cc)\nadd_arrow_acero_test(hash_aggregate_test SOURCES hash_aggregate_test.cc)\n```\n\n----------------------------------------\n\nTITLE: Query Results Output Format\nDESCRIPTION: Example output format showing the results of the Substrait query execution, displaying field paths and corresponding values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/substrait.rst#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n// Results example:\nFieldPath(0)    FieldPath(1)    FieldPath(2)    FieldPath(3)\n0               ALGERIA         0               haggle. carefully final deposits detect slyly agai\n1               ARGENTINA       1               al foxes promise slyly according to the regular accounts. bold requests alon\n```\n\n----------------------------------------\n\nTITLE: Running Maven Build with Skip Tests Option in Apache Arrow Java\nDESCRIPTION: This snippet demonstrates how to build the Apache Arrow Java project while skipping unit tests to speed up the build process. It uses Maven with specific Arrow JNI and C data parameters.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/development.rst#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ cd arrow/java\n$ mvn \\\n    -Darrow.cpp.build.dir=../java-dist/lib -Parrow-jni \\\n    -Darrow.c.jni.dist.dir=../java-dist/lib -Parrow-c-data \\\n    clean install\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow TensorFlow support with CMake\nDESCRIPTION: This snippet shows how to enable building Arrow with TensorFlow support enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_35\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_TENSORFLOW=ON\"\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow Type Factory Functions in C++\nDESCRIPTION: This snippet documents the type factory functions used for creating Arrow data types. These functions may return new objects or existing singletons.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygengroup:: type-factories\n   :content-only:\n```\n\n----------------------------------------\n\nTITLE: Building C++ API Documentation with Doxygen - Shell\nDESCRIPTION: This snippet describes how to process the C++ API documentation using Doxygen, which is essential for generating API reference documentation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/documentation.rst#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npushd arrow/cpp/apidoc\ndoxygen\npopd\n```\n\n----------------------------------------\n\nTITLE: Configuring Version Script for Shared Library Symbol Visibility\nDESCRIPTION: Sets up linker flags for controlling symbol visibility in shared libraries using a version script when the compiler supports it. This helps restrict which symbols are exported from the shared library.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(CXX_LINKER_SUPPORTS_VERSION_SCRIPT)\n  string(APPEND GANDIVA_SHARED_LINK_FLAGS\n         \" -Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/symbols.map\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Archery Lint Subpackage\nDESCRIPTION: Command to install the lint subpackage of Archery, which is used for linting and auto-formatting code in the Arrow repository.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/archery/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \"arrow/dev/archery[lint]\"\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for Apache Arrow Project\nDESCRIPTION: This code snippet lists all Python dependencies required for the Apache Arrow project, including core packages, conditional platform-specific dependencies, and version-specific numpy requirements that are selected based on the Python version being used.\nSOURCE: https://github.com/apache/arrow/blob/main/python/requirements-wheel-test.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncffi\ncython\nhypothesis\npackaging\npytest\npytz\npyuwsgi; sys.platform != 'win32' and python_version < '3.13'\nrequests; sys_platform == 'win32'\ntzdata; sys_platform == 'win32'\n\n# We generally test with the oldest numpy version that supports a given Python\n# version. However, there is no need to make this strictly the oldest version,\n# so it can be broadened to have a single version specification across platforms.\n# (`~=x.y.z` specifies a compatible release as `>=x.y.z, == x.y.*`)\nnumpy~=1.21.3; python_version < \"3.11\"\nnumpy~=1.23.2; python_version == \"3.11\"\nnumpy~=1.26.0; python_version == \"3.12\"\nnumpy~=2.1.0; python_version >= \"3.13\"\n\npandas\n```\n\n----------------------------------------\n\nTITLE: Generating ABI Dumps for Apache Arrow\nDESCRIPTION: Command to generate ABI (Application Binary Interface) dumps using abi-dumper, which are used for comparing different versions of the Arrow library for ABI stability.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nabi-dumper -lver 9 debug/libarrow.so -o ABI-9.dump\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake with Custom Clang Tools Path in Apache Arrow\nDESCRIPTION: Example of specifying a custom path for clang tools during CMake configuration using the ClangTools_PATH variable.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# We unpacked LLVM here:\n$ ~/tools/bin/clang-format --version\nclang-format version 12.0.0\n# Pass the directory containing the tools to CMake\n$ cmake ../cpp -DClangTools_PATH=~/tools/bin/\n...snip...\n-- clang-tidy found at /home/user/tools/bin/clang-tidy\n-- clang-format found at /home/user/tools/bin/clang-format\n...snip...\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up Arrow Repository with Git\nDESCRIPTION: Commands for cloning the forked Arrow repository and configuring the upstream remote to point to the main Apache Arrow repository.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ git clone https://github.com/<your username>/arrow.git\n$ cd arrow\n$ git remote add upstream https://github.com/apache/arrow\n```\n\n----------------------------------------\n\nTITLE: Update JavaScript Packages\nDESCRIPTION: Process for publishing Apache Arrow JavaScript packages to npm, requiring npm login and package upload\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\nnpm login --registry=https://registry.yarnpkg.com/\ndev/release/post-07-js.sh X.Y.Z\n```\n\n----------------------------------------\n\nTITLE: Cleanup Code for Arrow Dataset Examples\nDESCRIPTION: Suppressed code that handles cleanup after examples, removing temporary directories created during the tutorial. It changes back to the original working directory and removes temporary files.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/dataset.rst#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# clean-up custom working directory\nimport os\nimport shutil\n\nos.chdir(orig_working_dir)\nshutil.rmtree(temp_working_dir, ignore_errors=True)\n\n# also clean-up custom base directory used in some examples\nshutil.rmtree(str(base), ignore_errors=True)\n```\n\n----------------------------------------\n\nTITLE: Preparing Datum for Sum Output in Arrow C++\nDESCRIPTION: Declares a Datum object to store the result of the sum computation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\narrow::Datum sum_result;\n```\n\n----------------------------------------\n\nTITLE: Reviewing Code Changes with Git Diff\nDESCRIPTION: Git command for reviewing the actual code changes that have been made to implement the mday() binding and its tests.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_10\n\nLANGUAGE: console\nCODE:\n```\n$ git diff\ndiff --git a/r/R/expression.R b/r/R/expression.R\nindex 37fc21c25..0e71803ec 100644\n--- a/r/R/expression.R\n+++ b/r/R/expression.R\n@@ -70,6 +70,7 @@\n   \"quarter\" = \"quarter\",\n   # second is defined in dplyr-functions.R\n   # wday is defined in dplyr-functions.R\n+  \"mday\" = \"day\",\n   \"yday\" = \"day_of_year\",\n   \"year\" = \"year\",\n\ndiff --git a/r/tests/testthat/test-dplyr-funcs-datetime.R b/r/tests/testthat/test-dplyr-funcs-datetime.R\nindex 359a5403a..228eca56a 100644\n--- a/r/tests/testthat/test-dplyr-funcs-datetime.R\n+++ b/r/tests/testthat/test-dplyr-funcs-datetime.R\n@@ -444,6 +444,15 @@ test_that(\"extract wday from timestamp\", {\n   )\n })\n\n+test_that(\"extract mday from timestamp\", {\n+  compare_dplyr_binding(\n+    .input %>%\n+      mutate(x = mday(datetime)) %>%\n+      collect(),\n+    test_df\n+  )\n+})\n+\n test_that(\"extract yday from timestamp\", {\n   compare_dplyr_binding(\n     .input %>%\n```\n\n----------------------------------------\n\nTITLE: Displaying arrow::Status in GDB without extension\nDESCRIPTION: Example of how GDB displays an arrow::Status object without the custom extension. Shows the raw member variables which are not very informative.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gdb.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n$3 = {\n  <arrow::util::EqualityComparable<arrow::Status>> = {<No data fields>},\n  <arrow::util::ToStringOstreamable<arrow::Status>> = {<No data fields>},\n  members of arrow::Status:\n  state_ = 0x0\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing CommandGetDbSchemas for Metadata Fetching\nDESCRIPTION: The CommandGetDbSchemas command lists the schemas available in the database. The specific definition of schemas can differ by vendor. It is part of the SQL metadata commands in Arrow Flight SQL.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_2\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandGetDbSchemas\"\n```\n\n----------------------------------------\n\nTITLE: Defining ADD_ARROW_COMPUTE_TEST Function in CMake\nDESCRIPTION: This function defines a custom CMake function to add Arrow compute tests. It checks if ARROW_COMPUTE is enabled and sets up test prefixes and labels.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ADD_ARROW_COMPUTE_TEST REL_TEST_NAME)\n  if(NOT ARROW_COMPUTE)\n    return()\n  endif()\n\n  set(one_value_args PREFIX)\n  set(multi_value_args LABELS)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n\n  if(ARG_PREFIX)\n    set(PREFIX ${ARG_PREFIX})\n  else()\n    set(PREFIX ${ARROW_COMPUTE_TEST_PREFIX})\n  endif()\n\n  if(ARG_LABELS)\n    set(LABELS ${ARG_LABELS})\n  else()\n    set(LABELS ${ARROW_COMPUTE_TEST_LABELS})\n  endif()\n\n  add_arrow_test(${REL_TEST_NAME}\n                 PREFIX\n                 ${PREFIX}\n                 LABELS\n                 ${LABELS}\n                 ${ARG_UNPARSED_ARGUMENTS})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Adding pkg-config Support for Arrow JSON in CMake\nDESCRIPTION: Adds pkg-config support for Arrow JSON library.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/json/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\narrow_add_pkg_config(\"arrow-json\")\n```\n\n----------------------------------------\n\nTITLE: Adding Maven Dependency for Arrow Flight SQL JDBC Driver\nDESCRIPTION: This XML snippet shows how to add the Arrow Flight SQL JDBC driver as a dependency to a Maven project. It includes the necessary group ID, artifact ID, and version (using a property).\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/flight_sql_jdbc_driver.rst#2025-04-16_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<project xmlns=\\\"http://maven.apache.org/POM/4.0.0\\\"\\n         xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\"\\n         xsi:schemaLocation=\\\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\\\">\\n  <modelVersion>4.0.0</modelVersion>\\n  <groupId>org.example</groupId>\\n  <artifactId>demo</artifactId>\\n  <version>1.0-SNAPSHOT</version>\\n  <properties>\\n    <arrow.version>18.1.0</arrow.version>\\n  </properties>\\n  <dependencies>\\n    <dependency>\\n      <groupId>org.apache.arrow</groupId>\\n      <artifactId>flight-sql-jdbc-driver</artifactId>\\n      <version>${arrow.version}</version>\\n    </dependency>\\n  </dependencies>\\n</project>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow IPC, JSON, and ORC Components in CMake\nDESCRIPTION: Sets up source files and dependencies for Arrow's IPC, JSON, and ORC components. Each component is conditionally included based on build options.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_IPC)\n  list(APPEND\n       ARROW_IPC_SRCS\n       ipc/dictionary.cc\n       ipc/feather.cc\n       ipc/message.cc\n       ipc/metadata_internal.cc\n       ipc/options.cc\n       ipc/reader.cc\n       ipc/writer.cc)\n  if(ARROW_JSON)\n    list(APPEND ARROW_IPC_SRCS ipc/json_simple.cc)\n  endif()\n  arrow_add_object_library(ARROW_IPC ${ARROW_IPC_SRCS})\n  # ... (additional IPC configuration)\nendif()\n\nif(ARROW_JSON)\n  arrow_add_object_library(ARROW_JSON\n                           extension/fixed_shape_tensor.cc\n                           extension/opaque.cc\n                           json/options.cc\n                           json/chunked_builder.cc\n                           json/chunker.cc\n                           json/converter.cc\n                           json/object_parser.cc\n                           json/object_writer.cc\n                           json/parser.cc\n                           json/reader.cc)\n  # ... (additional JSON configuration)\nendif()\n\nif(ARROW_ORC)\n  arrow_add_object_library(ARROW_ORC adapters/orc/adapter.cc adapters/orc/options.cc\n                           adapters/orc/util.cc)\n  # ... (additional ORC configuration)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Applying Maven POM XML Style with Spotless in Apache Arrow Java\nDESCRIPTION: This command demonstrates how to automatically apply the correct style to all Maven POM XML files using the Spotless plugin. It formats the files according to Apache Maven guidelines.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/development.rst#2025-04-16_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n$ mvn spotless:apply\n```\n\n----------------------------------------\n\nTITLE: Adding Parquet Arrow Fuzzing Target\nDESCRIPTION: Creates a fuzzing target for the Parquet Arrow integration using a custom CMake function with the prefix 'parquet-arrow'.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/arrow/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_parquet_fuzz_target(fuzz PREFIX \"parquet-arrow\")\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests via Archery in Apache Arrow\nDESCRIPTION: This code snippet demonstrates how to run IPC integration tests for Apache Arrow Java using the Archery tool. It specifies to run only the IPC tests with Java support enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/development.rst#2025-04-16_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ archery integration --run-ipc --with-java 1\n```\n\n----------------------------------------\n\nTITLE: Testing Extraction of Month Day from Date - R\nDESCRIPTION: This snippet tests the extraction of the month day from a date using the mday() function from the lubridate package. It utilizes dplyr's mutate() to create a new column and collect() to gather results into a data frame. This test ensures that the implementation of mday() binds correctly to the provided input and expected output.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_11\n\nLANGUAGE: R\nCODE:\n```\ntest_that(\"extract mday from date\", {\n  compare_dplyr_binding(\n    .input %>%\n      mutate(x = mday(date)) %>%\n      collect(),\n    test_df\n  )\n})\n\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Unit Tests in CMake\nDESCRIPTION: Configures all the unit tests for the Arrow library using the add_arrow_test helper function. Tests are grouped by functionality area, with some tests having multiple source files.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_24\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# Unit tests\n#\nadd_arrow_test(array_test\n               SOURCES\n               array/array_test.cc\n               array/array_binary_test.cc\n               array/array_dict_test.cc\n               array/array_list_test.cc\n               array/array_list_view_test.cc\n               array/array_run_end_test.cc\n               array/array_struct_test.cc\n               array/array_union_test.cc\n               array/array_view_test.cc\n               array/statistics_test.cc\n               PRECOMPILED_HEADERS\n               \"$<$<COMPILE_LANGUAGE:CXX>:arrow/testing/pch.h>\")\n\nadd_arrow_test(buffer_test)\n\nif(ARROW_IPC)\n  # The extension type unit tests require IPC / Flatbuffers support\n  add_arrow_test(extension_type_test)\nendif()\n\nadd_arrow_test(misc_test\n               SOURCES\n               datum_test.cc\n               memory_pool_test.cc\n               result_test.cc\n               pretty_print_test.cc\n               status_test.cc)\n\nadd_arrow_test(public_api_test)\nset_source_files_properties(public_api_test.cc PROPERTIES SKIP_PRECOMPILE_HEADERS ON\n                                                          SKIP_UNITY_BUILD_INCLUSION ON)\n\nadd_arrow_test(scalar_test)\nadd_arrow_test(type_test SOURCES field_ref_test.cc type_test.cc)\n\nadd_arrow_test(table_test\n               SOURCES\n               chunked_array_test.cc\n               record_batch_test.cc\n               table_test.cc\n               table_builder_test.cc)\n\nadd_arrow_test(tensor_test)\nadd_arrow_test(sparse_tensor_test)\n\nadd_arrow_test(stl_test SOURCES stl_iterator_test.cc stl_test.cc)\n```\n\n----------------------------------------\n\nTITLE: Creating Initial Release Candidate by Creating Maintenance Branch\nDESCRIPTION: Commands for creating a new maintenance branch from the main branch for an initial release candidate and pushing it to the remote repository.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Execute the following from an up to date main branch.\n# This will create a branch locally called maint-X.Y.Z.\n# X.Y.Z corresponds with the Major, Minor and Patch version number\n# of the release respectively. As an example 9.0.0\narchery release cherry-pick X.Y.Z --execute\n# Push the maintenance branch to the remote repository\ngit push -u apache maint-X.Y.Z\n```\n\n----------------------------------------\n\nTITLE: Displaying RecordBatch Structure with glimpse() in R Arrow\nDESCRIPTION: Shows how glimpse() formats a RecordBatch object. Similar to Table output, it shows row count, columns, data types, and value previews, along with a note about using print() for schema details.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_2\n\nLANGUAGE: r\nCODE:\n```\nglimpse(batch)\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Documentation Index\nDESCRIPTION: Table of contents structure defining the navigation hierarchy for Apache Arrow C++ documentation, including configuration directives for C++ syntax highlighting and domain settings.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/user_guide.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. default-domain:: cpp\n.. highlight:: cpp\n\nUser Guide\n==========\n\n.. toctree::\n\n   overview\n   memory\n   arrays\n   datatypes\n   tables\n   compute\n   gandiva\n   streaming_execution\n   io\n   ipc\n   orc\n   parquet\n   csv\n   json\n   dataset\n   flight\n   gdb\n   threading\n   opentelemetry\n   env_vars\n```\n\n----------------------------------------\n\nTITLE: Configure sccache for Build Caching\nDESCRIPTION: Sets the `SCCACHE_DIR` environment variable for sccache's local caching mechanism, which aids in reducing build times by reusing cached compilations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nset SCCACHE_DIR=%LOCALAPPDATA%\\Mozilla\\sccache\ncmake -G \"Ninja\" ^\n...\n```\n\n----------------------------------------\n\nTITLE: Building Arrow CUDA Library in CMake\nDESCRIPTION: Configures and builds the Arrow CUDA library using the add_arrow_lib function. It sets up sources, link libraries, and installation options for both shared and static builds.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/gpu/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_CUDA_SRCS cuda_arrow_ipc.cc cuda_context.cc cuda_internal.cc cuda_memory.cc)\n\nset(ARROW_CUDA_PKG_CONFIG_NAME_ARGS)\nif(NOT WINDOWS)\n  list(APPEND ARROW_CUDA_PKG_CONFIG_NAME_ARGS PKG_CONFIG_NAME arrow-cuda)\nendif()\nadd_arrow_lib(arrow_cuda\n              CMAKE_PACKAGE_NAME\n              ArrowCUDA\n              ${ARROW_CUDA_PKG_CONFIG_NAME_ARGS}\n              SOURCES\n              ${ARROW_CUDA_SRCS}\n              OUTPUTS\n              ARROW_CUDA_LIBRARIES\n              SHARED_LINK_FLAGS\n              ${ARROW_VERSION_SCRIPT_FLAGS}\n              SHARED_LINK_LIBS\n              ${ARROW_CUDA_LINK_LIBS}\n              arrow_shared\n              ${ARROW_CUDA_SHARED_LINK_LIBS}\n              SHARED_INSTALL_INTERFACE_LIBS\n              Arrow::arrow_shared\n              ${ARROW_CUDA_SHARED_LINK_LIBS}\n              STATIC_LINK_LIBS\n              ${ARROW_CUDA_LINK_LIBS}\n              ${ARROW_CUDA_SHARED_LINK_LIBS}\n              STATIC_INSTALL_INTERFACE_LIBS\n              Arrow::arrow_static\n              ${ARROW_CUDA_SHARED_LINK_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Force Pushing with Lease\nDESCRIPTION: Command to push local changes to a remote repository with safety measures to prevent overwriting commits not in the local history.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/overview.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ git push --force-with-lease origin branch\n```\n\n----------------------------------------\n\nTITLE: Compiling JNI Bindings for ORC/Gandiva/Dataset\nDESCRIPTION: Maven commands to compile the JNI bindings for ORC, Gandiva, and Dataset components. This requires the previously built JNI libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n$ cd arrow/java\n$ mvn \\\n    -Darrow.cpp.build.dir=<absolute path to your arrow folder>/java-dist/lib/ \\\n    -Darrow.c.jni.dist.dir=<absolute path to your arrow folder>/java-dist/lib/ \\\n    -Parrow-jni clean install\n```\n\n----------------------------------------\n\nTITLE: Cleaning PyArrow Build Artifacts\nDESCRIPTION: Git command to clean PyArrow build artifacts from the python directory\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n$ git clean -Xfd python\n```\n\n----------------------------------------\n\nTITLE: Running IWYU on Apache Arrow Codebase\nDESCRIPTION: Commands for running Include-What-You-Use on the Apache Arrow codebase, either on the entire codebase or on specific components using pattern matching.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nIWYU_SH=$ARROW_ROOT/cpp/build-support/iwyu/iwyu.sh\n./$IWYU_SH\n```\n\n----------------------------------------\n\nTITLE: Building Live Documentation - Shell\nDESCRIPTION: This code enables live building of documentation, which automatically rebuilds the documentation when changes are saved.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/documentation.rst#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npushd arrow/docs\nmake html-live\npopd\n```\n\n----------------------------------------\n\nTITLE: Merge Release Branch to Maintenance Branch\nDESCRIPTION: Git commands to merge a release candidate branch into the maintenance branch for patch releases, ensuring version continuity\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\ngit checkout maint-X.Y.Z\ngit merge release-X.Y.Z-rcN\ngit push -u apache maint-X.Y.Z\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Dataset Library\nDESCRIPTION: Defines the Arrow Dataset library using the add_arrow_lib CMake function, configuring build outputs, source files, precompiled headers, and link dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/dataset/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_lib(arrow_dataset\n              CMAKE_PACKAGE_NAME\n              ArrowDataset\n              PKG_CONFIG_NAME\n              arrow-dataset\n              OUTPUTS\n              ARROW_DATASET_LIBRARIES\n              SOURCES\n              ${ARROW_DATASET_SRCS}\n              PRECOMPILED_HEADERS\n              \"$<$<COMPILE_LANGUAGE:CXX>:arrow/dataset/pch.h>\"\n              PRIVATE_INCLUDES\n              ${ARROW_DATASET_PRIVATE_INCLUDES}\n              SHARED_LINK_LIBS\n              ${ARROW_DATASET_SHARED_LINK_LIBS}\n              SHARED_PRIVATE_LINK_LIBS\n              ${ARROW_DATASET_SHARED_PRIVATE_LINK_LIBS}\n              SHARED_INSTALL_INTERFACE_LIBS\n              ${ARROW_DATASET_SHARED_INSTALL_INTERFACE_LIBS}\n              STATIC_LINK_LIBS\n              ${ARROW_DATASET_STATIC_LINK_LIBS}\n              STATIC_INSTALL_INTERFACE_LIBS\n              ${ARROW_DATASET_STATIC_INSTALL_INTERFACE_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Running Benchmarks in Apache Arrow\nDESCRIPTION: Example of running benchmarks in Apache Arrow. It's recommended to build in Release mode for meaningful benchmark numbers to enable compiler optimizations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ ./build/release/arrow-builder-benchmark\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes to GitHub - Console\nDESCRIPTION: This snippet outlines the command to push local changes to a forked repository on GitHub. It illustrates the process of synchronizing local changes with the remote repository.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_16\n\nLANGUAGE: console\nCODE:\n```\n$ git push origin ARROW-14816\n\n```\n\n----------------------------------------\n\nTITLE: Installing Skyhook Client Headers in Apache Arrow\nDESCRIPTION: This command installs all the header files from the 'skyhook/client' directory as part of the Apache Arrow build process. It uses a custom CMake function defined elsewhere in the project.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/skyhook/client/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"skyhook/client\")\n```\n\n----------------------------------------\n\nTITLE: Building JNI C Data Interface Library with CMake (macOS/Linux)\nDESCRIPTION: CMake commands to build only the JNI C Data Interface library on macOS or Linux platforms.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n$ cd arrow\n$ mkdir -p java-dist java-cdata\n$ cmake \\\n    -S java \\\n    -B java-cdata \\\n    -DARROW_JAVA_JNI_ENABLE_C=ON \\\n    -DARROW_JAVA_JNI_ENABLE_DEFAULT=OFF \\\n    -DBUILD_TESTING=OFF \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=java-dist\n$ cmake --build java-cdata --target install --config Release\n$ ls -latr java-dist/lib\n|__ arrow_cdata_jni/\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow DataType Class in C++\nDESCRIPTION: This snippet documents the arrow::DataType class, which is the base class for all Arrow data types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenclass:: arrow::DataType\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Autosummary of Acero Classes and Options\nDESCRIPTION: This snippet generates an automatic summary of the main Acero classes and options, including Declaration, ExecNodeOptions, and various node options for different operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/acero.rst#2025-04-16_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   Declaration\n   ExecNodeOptions\n   TableSourceNodeOptions\n   ScanNodeOptions\n   FilterNodeOptions\n   ProjectNodeOptions\n   AggregateNodeOptions\n   OrderByNodeOptions\n   HashJoinNodeOptions\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow Compute Vector Tests in CMake\nDESCRIPTION: Configures test targets for various vector operations in Arrow Compute, including cumulative ops, pairwise operations, hashing, nested structures, and more.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/kernels/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_compute_test(vector_test\n                       SOURCES\n                       vector_cumulative_ops_test.cc\n                       vector_pairwise_test.cc\n                       vector_hash_test.cc\n                       vector_nested_test.cc\n                       vector_replace_test.cc\n                       vector_run_end_encode_test.cc\n                       vector_statistics_test.cc\n                       select_k_test.cc\n                       EXTRA_LINK_LIBS\n                       arrow_compute_kernels_testing\n                       arrow_compute_testing)\n```\n\n----------------------------------------\n\nTITLE: Triggering Crossbow CI Jobs\nDESCRIPTION: GitHub comment to trigger the crossbow CI system to run all R-related tests against the CRAN-specific release branch.\nSOURCE: https://github.com/apache/arrow/blob/main/r/PACKAGING.md#2025-04-16_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n@github-actions crossbow submit --group r\n```\n\n----------------------------------------\n\nTITLE: Getting pyarrow Include Directory in Python\nDESCRIPTION: This snippet demonstrates how to obtain the absolute path to the pyarrow include directory, which contains the Arrow C++ and PyArrow C++ header files.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/extending.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\npa.get_include()\n```\n\n----------------------------------------\n\nTITLE: Running R Package Tests\nDESCRIPTION: Command for running tests filtered to datetime functions to verify the implemented mday() binding works as expected.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_7\n\nLANGUAGE: R\nCODE:\n```\ndevtools::test(filter=\"datetime\")\n\n> devtools::test(filter=\"datetime\")\nℹ Loading arrow\nSee arrow_info() for available features\nℹ Testing arrow\nSee arrow_info() for available features\n✔ | F W S  OK | Context\n✖ | 1     230 | dplyr-funcs-datetime [1.4s]\n────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\nFailure (test-dplyr-funcs-datetime.R:187:3): strftime\n``%>%`(...)` did not throw the expected error.\nBacktrace:\n 1. testthat::expect_error(...) test-dplyr-funcs-datetime.R:187:2\n 2. testthat:::expect_condition_matching(...)\n────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n══ Results ═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\nDuration: 1.4 s\n\n[ FAIL 1 | WARN 0 | SKIP 0 | PASS 230 ]\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow Compute Benchmark in CMake\nDESCRIPTION: This snippet adds a benchmark for Arrow compute functions with a specified prefix.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_benchmark(function_benchmark PREFIX \"arrow-compute\")\n```\n\n----------------------------------------\n\nTITLE: Running Perf Script and Processing Events with Python\nDESCRIPTION: Shell command that demonstrates how to pipe the output of 'perf script' to the Python processor and store the results in a JSONL file. It also includes an example of viewing the processed data with 'head' and 'cut'.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/memory.rst#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nperf script | python3 /arrow/process_perf_events.py > processed_events.jsonl\nhead processed_events.jsonl | cut -c -120\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark Comparisons\nDESCRIPTION: Example of comparing benchmark results between different versions using the diff command with filters.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/benchmarks.rst#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\narchery --quiet benchmark diff --benchmark-filter=FloatParsing\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow Snappy compression with CMake\nDESCRIPTION: This snippet shows how to enable support for Snappy compression.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_39\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_WITH_SNAPPY=ON\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Memory Pool Environment Variable\nDESCRIPTION: Environment variable setting for configuring Arrow's default memory pool allocator between jemalloc and mimalloc options.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nARROW_DEFAULT_MEMORY_POOL=mimalloc\n```\n\n----------------------------------------\n\nTITLE: Handling RecordBatchReader Query with glimpse() in R Arrow\nDESCRIPTION: Demonstrates glimpse() behavior with a filtered RecordBatchReader. The function shows a warning about single-use readers and displays schema information for the query result.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_9\n\nLANGUAGE: r\nCODE:\n```\nexample_data %>% as_record_batch_reader() %>% select(int) %>% glimpse()\n```\n\n----------------------------------------\n\nTITLE: Defining Source Reference Option in Meson\nDESCRIPTION: Defines a string option for specifying the source reference (revision/branch/tag) to use in source URLs for GI-DocGen generated documentation. Defaults to 'main'.\nSOURCE: https://github.com/apache/arrow/blob/main/c_glib/meson_options.txt#2025-04-16_snippet_4\n\nLANGUAGE: meson\nCODE:\n```\noption('source_reference',\n       type: 'string',\n       value: 'main',\n       description: 'Source reference (revision/branch/tag/...) to refer source URL in documents generated by GI-DocGen')\n```\n\n----------------------------------------\n\nTITLE: ArrowAsyncTask Structure Members in C\nDESCRIPTION: Definition of task structure used for efficient thread handling and data transfer. Contains callback for data extraction and private data storage.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDeviceDataInterface.rst#2025-04-16_snippet_6\n\nLANGUAGE: c\nCODE:\n```\nint (*extract_data)(struct ArrowArrayTask*, struct ArrowDeviceArray*);\nvoid* private_data;\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow IPC Utilities in CMake\nDESCRIPTION: Sets up executables for Arrow IPC utilities, including 'arrow-file-to-stream' and 'arrow-stream-to-file'. These are conditionally built based on ARROW_BUILD_UTILITIES or ARROW_BUILD_INTEGRATION flags.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/ipc/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_UTILITIES OR ARROW_BUILD_INTEGRATION)\n  add_executable(arrow-file-to-stream file_to_stream.cc)\n  target_link_libraries(arrow-file-to-stream ${ARROW_UTIL_LIB})\n  add_executable(arrow-stream-to-file stream_to_file.cc)\n  target_link_libraries(arrow-stream-to-file ${ARROW_UTIL_LIB})\n\n  if(ARROW_BUILD_UTILITIES)\n    install(TARGETS arrow-file-to-stream arrow-stream-to-file ${INSTALL_IS_OPTIONAL}\n            DESTINATION ${CMAKE_INSTALL_BINDIR})\n  endif()\n\n  if(ARROW_BUILD_INTEGRATION)\n    add_dependencies(arrow-integration arrow-file-to-stream)\n    add_dependencies(arrow-integration arrow-stream-to-file)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Tensor Conversion Benchmark\nDESCRIPTION: CMake directive to add a benchmark target for tensor conversion operations.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/tensor/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_benchmark(tensor_conversion_benchmark)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Arrow Benchmark\nDESCRIPTION: This snippet sets necessary environment variables for the Arrow benchmark build, including repository URL, commit hash, and conbench credentials.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport ARROW_REPO=https://github.com/apache/arrow.git\nexport BENCHMARKABLE=e6e9e6ea52b7a8f2682ffc4160168c936ca1d3e6\nexport BENCHMARKABLE_TYPE=arrow-commit\nexport PYTHON_VERSION=3.8\nexport CONBENCH_EMAIL=...\nexport CONBENCH_URL=\"https://conbench.ursa.dev\"\nexport CONBENCH_PASSWORD=...\nexport MACHINE=...\n```\n\n----------------------------------------\n\nTITLE: Initializing PyArrow CMake Project\nDESCRIPTION: Sets up the initial CMake configuration for PyArrow, including minimum CMake version, project name, and version information.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.25)\nproject(pyarrow)\n\nset(PYARROW_VERSION \"20.0.0-SNAPSHOT\")\nstring(REGEX MATCH \"^[0-9]+\\\\.[0-9]+\\\\.[0-9]+\" PYARROW_BASE_VERSION \"${PYARROW_VERSION}\")\n\nproject(pyarrow VERSION \"${PYARROW_BASE_VERSION}\")\nset(PYARROW_VERSION_MAJOR \"${pyarrow_VERSION_MAJOR}\")\nset(PYARROW_VERSION_MINOR \"${pyarrow_VERSION_MINOR}\")\nset(PYARROW_VERSION_PATCH \"${pyarrow_VERSION_PATCH}\")\nmath(EXPR PYARROW_SO_VERSION \"${PYARROW_VERSION_MAJOR} * 100 + ${PYARROW_VERSION_MINOR}\")\nset(PYARROW_FULL_SO_VERSION \"${PYARROW_SO_VERSION}.${PYARROW_VERSION_PATCH}.0\")\n```\n\n----------------------------------------\n\nTITLE: Defining an R Function to Add Three to Array Elements\nDESCRIPTION: This R code defines a function 'addthree' that adds 3 to all elements of an input Arrow Array.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_r.rst#2025-04-16_snippet_0\n\nLANGUAGE: R\nCODE:\n```\nlibrary(arrow)\n\naddthree <- function(arr) {\n    return(arr + 3L)\n}\n```\n\n----------------------------------------\n\nTITLE: Running C++ Unit Tests for Apache Parquet\nDESCRIPTION: Provides the command to run all unit tests for the Apache Parquet C++ libraries. The environment variable `PARQUET_TEST_DATA` must be set to the path of the parquet-testing data directory. This involves initializing a git submodule.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ngit submodule update --init\nexport PARQUET_TEST_DATA=$ARROW_ROOT/cpp/submodules/parquet-testing/data\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Docker - Shell\nDESCRIPTION: This snippet demonstrates how to build the documentation within a Docker container using Archery. It mounts the documentation directory and executes the build process.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/documentation.rst#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\narchery docker run -v \"${PWD}/docs:/build/docs\" debian-docs\n```\n\n----------------------------------------\n\nTITLE: Squashing Local Git Commits\nDESCRIPTION: Interactive rebase command to squash multiple local commits into a single commit before pushing to simplify conflict resolution.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/overview.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ git rebase --interactive ORIG_HEAD~n\n```\n\n----------------------------------------\n\nTITLE: Using lubridate Functions in Arrow dplyr Queries\nDESCRIPTION: Lists newly supported lubridate functions that can be used in Arrow dplyr queries for date and time operations.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_11\n\nLANGUAGE: R\nCODE:\n```\nlubridate::tz()\nlubridate::semester()\nlubridate::dst()\nlubridate::date()\nlubridate::epiyear()\nlubridate::month()\nlubridate::make_date()\nlubridate::make_datetime()\nbase::ISOdatetime()\nbase::ISOdate()\nlubridate::decimal_date()\nlubridate::date_decimal()\nlubridate::make_difftime()\nlubridate::dyears()\nlubridate::dhours()\nlubridate::dseconds()\nlubridate::leap_year()\nlubridate::as_date()\nlubridate::as_datetime()\n```\n\n----------------------------------------\n\nTITLE: Configuring ARROW_PRE_1_0_METADATA_VERSION Environment Variable in RST\nDESCRIPTION: Enables writing of V4 Arrow metadata (pre-1.0 Arrow with incompatible Union data layout) when set to a non-zero integer value.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/env_vars.rst#2025-04-16_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. envvar:: ARROW_PRE_1_0_METADATA_VERSION\n\n   If this environment variable is set to a non-zero integer value, the PyArrow\n   IPC writer will write V4 Arrow metadata (corresponding to pre-1.0 Arrow\n   with an incompatible Union data layout).\n   This behavior can also be enabled using :attr:`IpcWriteOptions.metadata_version`.\n```\n\n----------------------------------------\n\nTITLE: Building with No Leaf Cache and Custom Build Parameter\nDESCRIPTION: Command to force rebuilding only the leaf image in a dependency tree while setting a custom build parameter.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nPANDAS=upstream_devel archery docker run --no-leaf-cache conda-python-pandas\n```\n\n----------------------------------------\n\nTITLE: Configuring Substrait Integration in PyArrow CMake\nDESCRIPTION: Sets up Substrait query engine integration with proper library linking and import handling for both shared and static builds.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nif(PYARROW_BUILD_SUBSTRAIT)\n  message(STATUS \"Building PyArrow with Substrait\")\n\n  if(ARROW_BUILD_SHARED)\n    if(PYARROW_BUNDLE_ARROW_CPP)\n      bundle_arrow_lib(${ARROW_SUBSTRAIT_SHARED_LIB} SO_VERSION ${ARROW_SO_VERSION})\n      if(MSVC)\n        bundle_arrow_import_lib(${ARROW_SUBSTRAIT_IMPORT_LIB})\n      endif()\n    endif()\n    set(SUBSTRAIT_LINK_LIBS ArrowSubstrait::arrow_substrait_shared)\n  else()\n    set(SUBSTRAIT_LINK_LIBS)\n  endif()\n\n  list(APPEND CYTHON_EXTENSIONS _substrait)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Compute Testing Object Library in CMake\nDESCRIPTION: This snippet defines an object library for common test files used in Arrow compute tests. It includes the necessary dependencies and configures include paths.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_TESTING)\n  add_library(arrow_compute_testing OBJECT test_util_internal.cc)\n  # Even though this is still just an object library we still need to \"link\" our\n  # dependencies so that include paths are configured correctly\n  target_link_libraries(arrow_compute_testing PUBLIC ${ARROW_GTEST_GMOCK})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining RST Table of Contents for Arrow Examples\nDESCRIPTION: A reStructuredText directive that creates a table of contents listing various Apache Arrow examples with a maximum depth of 1 level. The list includes examples for cmake builds, compute operations, dataset handling, Parquet encryption, and data format conversions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   cmake_minimal_build\n   compute_and_write_example\n   dataset_documentation_example\n   dataset_skyhook_scan_example\n   parquet_column_encryption\n   row_columnar_conversion\n   std::tuple-like ranges to Arrow <tuple_range_conversion>\n   Converting RecordBatch to Tensor <converting_recordbatch_to_tensor>\n```\n\n----------------------------------------\n\nTITLE: Setting up Apache Arrow for MATLAB with Git Clone\nDESCRIPTION: Commands to clone the Apache Arrow repository and navigate to the MATLAB subdirectory to begin working with the MATLAB interface.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ git clone https://github.com/apache/arrow.git\n$ cd arrow/matlab\n```\n\n----------------------------------------\n\nTITLE: Installing Red Arrow with Bundler\nDESCRIPTION: Example Gemfile configuration for installing Red Arrow using Bundler with the rubygems-requirements-system plugin to automatically install Apache Arrow GLib dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow/README.md#2025-04-16_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nplugin \"rubygems-requirements-system\"\n\ngem \"red-arrow\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on MSYS2\nDESCRIPTION: Commands to install required build dependencies on MSYS2 using pacman package manager.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npacman --sync --refresh --noconfirm \\\n  ccache \\\n  git \\\n  mingw-w64-${MSYSTEM_CARCH}-boost \\\n  mingw-w64-${MSYSTEM_CARCH}-brotli \\\n  mingw-w64-${MSYSTEM_CARCH}-cmake \\\n  mingw-w64-${MSYSTEM_CARCH}-gcc \\\n  mingw-w64-${MSYSTEM_CARCH}-gflags \\\n  mingw-w64-${MSYSTEM_CARCH}-glog \\\n  mingw-w64-${MSYSTEM_CARCH}-gtest \\\n  mingw-w64-${MSYSTEM_CARCH}-lz4 \\\n  mingw-w64-${MSYSTEM_CARCH}-protobuf \\\n  mingw-w64-${MSYSTEM_CARCH}-python3-numpy \\\n  mingw-w64-${MSYSTEM_CARCH}-rapidjson \\\n  mingw-w64-${MSYSTEM_CARCH}-snappy \\\n  mingw-w64-${MSYSTEM_CARCH}-thrift \\\n  mingw-w64-${MSYSTEM_CARCH}-zlib \\\n  mingw-w64-${MSYSTEM_CARCH}-zstd\n```\n\n----------------------------------------\n\nTITLE: Importing PyArrow Module in Python\nDESCRIPTION: This snippet shows the import statement for the PyArrow module, which is necessary to use the Tables and Tensors functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/tables.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: pyarrow\n```\n\n----------------------------------------\n\nTITLE: Feather File Compression Setting\nDESCRIPTION: Example demonstrating how to disable compression when writing feather files\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_7\n\nLANGUAGE: R\nCODE:\n```\nwrite_feather(compression = FALSE)\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Build with conda using Docker Compose on Fedora\nDESCRIPTION: Command to run the conda-based build process for PyArrow using Docker Compose with the Fedora container.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose run --rm minimal-fedora-conda\n```\n\n----------------------------------------\n\nTITLE: Executing Python-to-Java Integration Example\nDESCRIPTION: Console output showing the result of running the Python script that calls the Java method. It demonstrates successful execution of Java code from Python.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ python simple.py\n4\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Benchmark Function in CMake\nDESCRIPTION: CMake function for adding benchmarks to the Arrow suite with configurable prefix and labels.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ADD_ARROW_BENCHMARK REL_TEST_NAME)\n  set(options)\n  set(one_value_args PREFIX)\n  set(multi_value_args)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n  if(ARG_PREFIX)\n    set(PREFIX ${ARG_PREFIX})\n  else()\n    set(PREFIX \"arrow\")\n  endif()\n  add_benchmark(${REL_TEST_NAME}\n                PREFIX\n                ${PREFIX}\n                LABELS\n                \"arrow-benchmarks\"\n                ${ARG_UNPARSED_ARGUMENTS})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Header in Markdown\nDESCRIPTION: Standard Apache License 2.0 header included as a comment in the markdown file. It outlines the licensing terms for the Apache Arrow project.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/xxhash/README.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!---\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Installing PyArrow Nightly Build\nDESCRIPTION: pip command to install the latest nightly version of PyArrow from scientific-python-nightly-wheels\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\npip install \\\n  -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple \\\n  pyarrow\n```\n\n----------------------------------------\n\nTITLE: Disabling Image Caching with Archery\nDESCRIPTION: Command to run a build with the --no-cache option to disable using cached images.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\narchery docker run --no-cache conda-python\n```\n\n----------------------------------------\n\nTITLE: Running R Package Tests in R Console\nDESCRIPTION: R command to run tests for the Arrow R package using the devtools package. This validates that the R implementation of Arrow is functioning properly.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/index.rst#2025-04-16_snippet_1\n\nLANGUAGE: R\nCODE:\n```\ndevtools::test()\n```\n\n----------------------------------------\n\nTITLE: Setting Warning Suppression Files for Parquet Examples in CMake\nDESCRIPTION: Defines a list of files for which certain compiler warnings should be suppressed. This is done for illustration purposes in the example files.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/parquet/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(PARQUET_EXAMPLES_WARNING_SUPPRESSIONS low_level_api/reader_writer.cc\n                                          low_level_api/reader_writer2.cc)\n```\n\n----------------------------------------\n\nTITLE: Running C++ Benchmarks with Conbench\nDESCRIPTION: This snippet installs archery and runs C++ benchmarks using conbench.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npushd arrow\nsource dev/conbench_envs/hooks.sh install_archery\npopd\ncd benchmarks\nconbench cpp-micro --iterations=1\n```\n\n----------------------------------------\n\nTITLE: PyArrow Test Project Structure\nDESCRIPTION: Standard directory layout for PyArrow tests following pytest conventions, organized with test files mirroring source code structure\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/step_by_step/testing.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npyarrow/\n    __init__.py\n    csv.py\n    dataset.py\n    ...\n    tests/\n        __init__.py\n        test_csv.py\n        test_dataset.py\n        ...\n```\n\n----------------------------------------\n\nTITLE: Autosummary for FileSystem Utilities in Python\nDESCRIPTION: This snippet generates an autosummary for utility functions related to filesystems. It includes copy_files, initialize_s3, finalize_s3, resolve_s3_region, and S3LogLevel.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/filesystems.rst#2025-04-16_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   copy_files\n   initialize_s3\n   finalize_s3\n   resolve_s3_region\n   S3LogLevel\n```\n\n----------------------------------------\n\nTITLE: Installing Archery Docker Subpackage\nDESCRIPTION: Command to install the docker subpackage of Archery, which facilitates running docker compose based tasks more easily.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/archery/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \"arrow/dev/archery[docker]\"\n```\n\n----------------------------------------\n\nTITLE: Generating Package with Make Build Command\nDESCRIPTION: Command to build the Arrow R package which copies Arrow C++ into tools/cpp, prunes unnecessary components, and runs R CMD build to generate the source tarball.\nSOURCE: https://github.com/apache/arrow/blob/main/r/PACKAGING.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Activating the PyArrow Conda Environment\nDESCRIPTION: Activates the previously created Conda environment for PyArrow development.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n$ conda activate pyarrow-dev\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Schema Types in R\nDESCRIPTION: This snippet demonstrates how to create Arrow schema types in R, including support for time types with human-friendly resolution strings.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_23\n\nLANGUAGE: R\nCODE:\n```\nschema(\n  field(\"a\", double()),\n  field(\"b\", time32(\"ms\")),\n  field(\"c\", time64(\"s\"))\n)\n```\n\n----------------------------------------\n\nTITLE: Removing Conda Environment\nDESCRIPTION: Commands to deactivate and remove the pyarrow-dev conda environment\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\n$ conda deactivate\n$ conda remove -n pyarrow-dev\n```\n\n----------------------------------------\n\nTITLE: Cloning the Arrow Repository with Git\nDESCRIPTION: Commands to clone the Apache Arrow git repository and initialize submodules. This is a prerequisite step for all building operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ git clone https://github.com/apache/arrow.git\n$ cd arrow\n$ git submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Build with pip using Docker Compose on Fedora\nDESCRIPTION: Command to run the pip-based build process for PyArrow using Docker Compose with the Fedora container.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose run --rm minimal-fedora-venv\n```\n\n----------------------------------------\n\nTITLE: Configuring Skyhook Test Link Libraries\nDESCRIPTION: Sets up the appropriate test link libraries based on the ARROW_TEST_LINKAGE setting (static or shared).\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/skyhook/CMakeLists.txt#2025-04-16_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_TEST_LINKAGE STREQUAL \"static\")\n  set(ARROW_SKYHOOK_TEST_LINK_LIBS arrow_skyhook_static arrow_dataset_static\n                                   ${ARROW_TEST_STATIC_LINK_LIBS})\nelse()\n  set(ARROW_SKYHOOK_TEST_LINK_LIBS arrow_skyhook_shared arrow_dataset_shared\n                                   ${ARROW_TEST_SHARED_LINK_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring gRPC Client Reconnection Backoff in Python\nDESCRIPTION: Sets up a Flight client with a minimum time between subsequent connection attempts using gRPC options. This configuration creates a client that will wait at least 2000ms between reconnection attempts.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/flight.rst#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Set the minimum time between subsequent connection attempts.\ngeneric_options = [(\"GRPC_ARG_MIN_RECONNECT_BACKOFF_MS\", 2000)]\nclient = pyarrow.flight.FlightClient(server_uri, generic_options=generic_options)\n```\n\n----------------------------------------\n\nTITLE: Running MATLAB Tests for Arrow Interface\nDESCRIPTION: MATLAB command to run all tests for the Arrow interface, including tests in subdirectories using the runtests function.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_3\n\nLANGUAGE: matlab\nCODE:\n```\n>> runtests(\"test\", IncludeSubFolders=true);\n```\n\n----------------------------------------\n\nTITLE: Adding --add-opens Flag for Maven Unit Testing\nDESCRIPTION: This XML snippet modifies the Maven Surefire plugin configuration to include the --add-opens argument. This ensures that JDK internals are exposed during unit tests, preventing access issues when testing Java applications using Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/install.rst#2025-04-16_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<build>\n    <plugins>\n        <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-surefire-plugin</artifactId>\n            <version>3.0.0-M6</version>\n            <configuration>\n                    <argLine>--add-opens=java.base/java.nio=ALL-UNNAMED</argLine>\n            </configuration>\n        </plugin>\n    </plugins>\n</build>\n```\n\n----------------------------------------\n\nTITLE: Testing Invalid Argument for Parquet Format in R\nDESCRIPTION: This snippet tests the error handling when an unsupported argument is provided for the Parquet format in write_dataset. It expects an error message listing the numerous supported arguments specific to Parquet.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dataset-write.md#2025-04-16_snippet_5\n\nLANGUAGE: R\nCODE:\n```\nwrite_dataset(df, dst_dir, format = \"parquet\", nonsensical_arg = \"blah-blah\")\n```\n\n----------------------------------------\n\nTITLE: Configuring MATLAB Startup File Integration in CMake\nDESCRIPTION: Defines a CMake option that controls whether the Arrow installation directory should be added to the MATLAB startup.m file. This provides a persistent way for MATLAB to find Arrow components.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\noption(MATLAB_ADD_INSTALL_DIR_TO_STARTUP_FILE\n       \"Sets whether the path to the install directory should be added to the startup.m file located at the MATLAB userpath\"\n       OFF)\n```\n\n----------------------------------------\n\nTITLE: Installing PyArrow via Pip\nDESCRIPTION: Command to install PyArrow using pip package manager from PyPI\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/install.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pyarrow\n```\n\n----------------------------------------\n\nTITLE: Enabling Undefined Behavior Sanitizer with CMake\nDESCRIPTION: This snippet shows how to enable the Undefined Behavior Sanitizer to check for situations which trigger C++ undefined behavior.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_52\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_USE_UBSAN=ON\"\n```\n\n----------------------------------------\n\nTITLE: Importing FileSystem Module in Python\nDESCRIPTION: This snippet shows how to import the FileSystem module from pyarrow. It's used to set the current module context for the rest of the documentation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/filesystems.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: pyarrow.fs\n```\n\n----------------------------------------\n\nTITLE: Navigating to Python Directory and Opening Python Console\nDESCRIPTION: Commands to navigate to the Python directory and start a Python interactive console for exploring PyArrow functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ cd python\n$ python\n\nPython 3.9.7 (default, Oct 22 2021, 13:24:00)\n[Clang 13.0.0 (clang-1300.0.29.3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n```\n\n----------------------------------------\n\nTITLE: Detecting Python, NumPy, and Cython for PyArrow Build in CMake\nDESCRIPTION: Finds and configures Python, NumPy, and Cython for the PyArrow build process. It also includes the Arrow C++ package and sets up default PyArrow build options.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n# Python and Numpy libraries\nfind_package(Python3Alt REQUIRED)\nmessage(STATUS \"Found NumPy version: ${Python3_NumPy_VERSION}\")\nmessage(STATUS \"NumPy include dir: ${NUMPY_INCLUDE_DIRS}\")\n\ninclude(UseCython)\nmessage(STATUS \"Found Cython version: ${CYTHON_VERSION}\")\n\n# Arrow C++ and set default PyArrow build options\ninclude(GNUInstallDirs)\nfind_package(Arrow REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow Dataset API with CMake\nDESCRIPTION: This snippet shows how to enable the Dataset API by passing a boolean flag to CMake. It implies the Filesystem API.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_DATASET=ON\"\n```\n\n----------------------------------------\n\nTITLE: Configuring MATLAB Search Path Integration in CMake\nDESCRIPTION: Defines a CMake option to control whether the Arrow installation directory should be directly added to the MATLAB search path. This provides immediate access to Arrow components within the current MATLAB session.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\noption(MATLAB_ADD_INSTALL_DIR_TO_SEARCH_PATH\n       \"Sets whether the path to the install directory should be directly added to the MATLAB Search Path\"\n       ON)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Requirements - Shell\nDESCRIPTION: This code snippet installs the Python-based requirements necessary for building the documentation using pip. It reads from a requirements.txt file provided in the documentation directory.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/documentation.rst#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install -r arrow/docs/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Flight Support\nDESCRIPTION: Sets up Arrow Flight integration if enabled. Requires shared Arrow libraries and configures proper linking with gRPC dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nset(PYARROW_CPP_FLIGHT_SRCS ${PYARROW_CPP_SOURCE_DIR}/flight.cc)\nif(PYARROW_BUILD_FLIGHT)\n  message(STATUS \"Building PyArrow with Flight\")\n  if(NOT ARROW_FLIGHT)\n    message(FATAL_ERROR \"You must build Arrow C++ with ARROW_FLIGHT=ON\")\n  endif()\n  if(NOT ARROW_BUILD_SHARED)\n    message(FATAL_ERROR \"You must build Arrow C++ with ARROW_BUILD_SHARED=ON\")\n  endif()\n  find_package(ArrowFlight REQUIRED)\n\n  add_library(arrow_python_flight SHARED ${PYARROW_CPP_FLIGHT_SRCS})\n  target_link_libraries(arrow_python_flight PUBLIC arrow_python\n                                                   ArrowFlight::arrow_flight_shared)\n  target_compile_definitions(arrow_python_flight PRIVATE ARROW_PYFLIGHT_EXPORTING)\n  set_target_properties(arrow_python_flight\n                        PROPERTIES VERSION \"${PYARROW_FULL_SO_VERSION}\"\n                                   SOVERSION \"${PYARROW_SO_VERSION}\")\n  install(TARGETS arrow_python_flight\n          ARCHIVE DESTINATION .\n          LIBRARY DESTINATION .\n          RUNTIME DESTINATION .)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Fedora Minimal Environment\nDESCRIPTION: Command to build a Docker image for Fedora 35 that will be used as the environment for building PyArrow.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t arrow_fedora_minimal -f Dockerfile.fedora .\n```\n\n----------------------------------------\n\nTITLE: Testing the Initial Implementation in Python Console\nDESCRIPTION: Code for testing the first version of the tutorial_min_max function in the Python console to verify its behavior.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow.compute as pc\n>>> data = [4, 5, 6, None, 1]\n>>> pc.tutorial_min_max(data)\n<pyarrow.StructScalar: [('min', 1), ('max', 6)]>\n```\n\n----------------------------------------\n\nTITLE: Defining Opaque Extension Metadata in Arrow\nDESCRIPTION: Shows the JSON object structure for the Opaque extension type metadata. It includes type_name and vendor_name parameters to describe the unknown type from an external system.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CanonicalExtensions.rst#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"type_name\": \"varray\", \"vendor_name\": \"Oracle\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"type_name\": \"geometry\", \"vendor_name\": \"PostGIS\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"type_name\": \"database_name.schema_name.complex\", \"vendor_name\": \"PostgreSQL\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"type_name\": \"OTHER\", \"vendor_name\": \"JDBC driver name\"}\n```\n\n----------------------------------------\n\nTITLE: Building Arrow Integration Base Docker Image\nDESCRIPTION: This command builds the base Docker image used for multiple integration tests in Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/README.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker build -t arrow_integration_xenial_base -f docker_common/Dockerfile.xenial.base .\n```\n\n----------------------------------------\n\nTITLE: Creating Integer ChunkedArray with Range and Vector in R\nDESCRIPTION: Creates a ChunkedArray of integer type with two chunks, the first using a sequence from 1 to 30, and the second using a small vector of integers. The resulting array has two chunks containing an integer sequence and three additional values.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/test-chunked-array.txt#2025-04-16_snippet_1\n\nLANGUAGE: r\nCODE:\n```\nchunked_array(1:30, c(4, 5, 6))\n```\n\n----------------------------------------\n\nTITLE: Creating Bundled Dependencies Static Library in CMake for Arrow\nDESCRIPTION: Merges multiple static libraries into a single bundled dependencies library and configures installation. This is used when ARROW_OPTIONAL_STATIC is enabled to create a bundled static library of dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\narrow_cdr(_OTHER_LIBS ${ARROW_BUNDLED_STATIC_LIBS})\n  arrow_create_merged_static_lib(arrow_bundled_dependencies\n                                 NAME\n                                 arrow_bundled_dependencies\n                                 ROOT\n                                 ${_FIRST_LIB}\n                                 TO_MERGE\n                                 ${_OTHER_LIBS})\n  # We can't use install(TARGETS) here because\n  # arrow_bundled_dependencies is an IMPORTED library.\n  get_target_property(arrow_bundled_dependencies_path arrow_bundled_dependencies\n                      IMPORTED_LOCATION)\n  install(FILES ${arrow_bundled_dependencies_path} ${INSTALL_IS_OPTIONAL}\n          DESTINATION ${CMAKE_INSTALL_LIBDIR})\n  string(PREPEND ARROW_PC_LIBS_PRIVATE \" -larrow_bundled_dependencies\")\n  list(INSERT ARROW_STATIC_INSTALL_INTERFACE_LIBS 0 \"Arrow::arrow_bundled_dependencies\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation of Arrow Compute Benchmark in CMake\nDESCRIPTION: This CMake code block conditionally adds a benchmark for the Arrow compute grouper functionality. It only compiles the benchmark if the ARROW_COMPUTE flag is set.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/row/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_COMPUTE)\n  add_arrow_benchmark(grouper_benchmark PREFIX \"arrow-compute\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Debugging .deb Package Build Process\nDESCRIPTION: These commands open a console for debugging the .deb package build process, allowing interactive exploration and manual build execution.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/tasks/linux-packages/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd dev/tasks/linux-packages/apache-arrow\nrake version:update\nrake apt:build:console APT_TARGETS=debian-bookworm\n```\n\n----------------------------------------\n\nTITLE: Linking Bitcode Files into a Single Bitcode File\nDESCRIPTION: Creates a custom command to link all the individual bitcode files into a single combined bitcode file using LLVM's link tool. The output is specified by GANDIVA_PRECOMPILED_BC_PATH.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/precompiled/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_command(OUTPUT ${GANDIVA_PRECOMPILED_BC_PATH}\n                   COMMAND ${LLVM_LINK_EXECUTABLE} -o ${GANDIVA_PRECOMPILED_BC_PATH}\n                           ${GANDIVA_PRECOMPILED_BC_FILES}\n                   DEPENDS ${GANDIVA_PRECOMPILED_BC_FILES})\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Version and SO Version\nDESCRIPTION: Sets up Arrow version numbers and calculates shared object version numbers based on major/minor version components.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(ARROW_VERSION \"20.0.0-SNAPSHOT\")\n\nstring(REGEX MATCH \"^[0-9]+\\\\.[0-9]+\\\\.[0-9]+\" ARROW_BASE_VERSION \"${ARROW_VERSION}\")\n\nif(ARROW_VERSION_MAJOR STREQUAL \"0\")\n  set(ARROW_SO_VERSION \"${ARROW_VERSION_MINOR}\")\n  set(ARROW_FULL_SO_VERSION \"${ARROW_SO_VERSION}.${ARROW_VERSION_PATCH}.0\")\nelse()\n  math(EXPR ARROW_SO_VERSION \"${ARROW_VERSION_MAJOR} * 100 + ${ARROW_VERSION_MINOR}\")\n  set(ARROW_FULL_SO_VERSION \"${ARROW_SO_VERSION}.${ARROW_VERSION_PATCH}.0\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow ORC Headers\nDESCRIPTION: Configures the installation of ORC adapter header files to the specified include directory.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/adapters/orc/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES adapter.h options.h\n        DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}/arrow/adapters/orc\")\n```\n\n----------------------------------------\n\nTITLE: Implementing FlatBuffer Offset Verification in C++\nDESCRIPTION: Implementation of a template specialization method VerifyOffset for uoffset64_t type in the FlatBuffer Verifier class. The method verifies offset integrity within a FlatBuffer data structure.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/thirdparty/flatbuffers/README.md#2025-04-16_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n// Helper class to verify the integrity of a FlatBuffer\n@@ -328,5 +335,6 @@ inline size_t Verifier::VerifyOffset<uoffset64_t>(const size_t start) const {\n}\n\n}  // namespace flatbuffers\n+}  // namespace arrow_vendored_private\n\n#endif  // FLATBUFFERS_VERIFIER_H_\n```\n\n----------------------------------------\n\nTITLE: Red Arrow Development Setup for macOS with Homebrew\nDESCRIPTION: Console commands for setting up a Red Arrow development environment specifically on macOS using Homebrew to install Arrow dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow/README.md#2025-04-16_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ cd ruby/red-arrow\n$ bundle install\n$ brew install apache-arrow --head\n$ brew install apache-arrow-glib --head\n$ bundle exec rake test\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud Storage Tests (GCS and Azure)\nDESCRIPTION: Sets up tests for Google Cloud Storage and Azure Blob Storage filesystem implementations when enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/filesystem/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARROW_GCS)\n  add_arrow_test(gcsfs_test\n                 EXTRA_LABELS\n                 filesystem\n                 EXTRA_LINK_LIBS\n                 google-cloud-cpp::storage)\nendif()\n\nif(ARROW_AZURE)\n  add_arrow_test(azurefs_test\n                 EXTRA_LABELS\n                 filesystem\n                 EXTRA_LINK_LIBS\n                 ${AZURE_SDK_LINK_LIBRARIES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Downloading Crossbow Build Artifacts in Console\nDESCRIPTION: Command to download the artifacts of a Crossbow build using its build ID or branch name.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/crossbow.rst#2025-04-16_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n$ archery crossbow artifacts <build id / branch name>\n```\n\n----------------------------------------\n\nTITLE: Updating Apache Arrow Vendored Datetime Library\nDESCRIPTION: Command line instructions for updating the vendored datetime library in the Apache Arrow codebase. This script updates the library to a specific version (3.0.3 in the example).\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/datetime/README.md#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ cd cpp/src/arrow/vendored/datetime\n$ ./update.sh 3.0.3\n```\n\n----------------------------------------\n\nTITLE: Setting Arrow Link Library Variables in CMake\nDESCRIPTION: Initializes variables for storing link libraries for Arrow. These include private libraries for shared builds and interface libraries for installation with both shared and static builds.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# Libraries to link with libarrow.so. They aren't exported.\nset(ARROW_SHARED_PRIVATE_LINK_LIBS)\n\n# Libraries to link with exported libarrow.{so,a}.\nset(ARROW_SHARED_INSTALL_INTERFACE_LIBS)\nset(ARROW_STATIC_INSTALL_INTERFACE_LIBS)\n```\n\n----------------------------------------\n\nTITLE: Building JNI C Data Interface Library with CMake (Windows)\nDESCRIPTION: CMake commands to build only the JNI C Data Interface library on Windows platforms.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n$ cd arrow\n$ mkdir java-dist, java-cdata\n$ cmake ^\n    -S java ^\n    -B java-cdata ^\n    -DARROW_JAVA_JNI_ENABLE_C=ON ^\n    -DARROW_JAVA_JNI_ENABLE_DEFAULT=OFF ^\n    -DBUILD_TESTING=OFF ^\n    -DCMAKE_BUILD_TYPE=Release ^\n    -DCMAKE_INSTALL_PREFIX=java-dist\n$ cmake --build java-cdata --target install --config Release\n$ dir \"java-dist/bin\"\n|__ arrow_cdata_jni/\n```\n\n----------------------------------------\n\nTITLE: Setting Arrow Build Environment Variables\nDESCRIPTION: Environment variables used to control Arrow's build configuration for Linux installations.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nLIBARROW_MINIMAL=false\nLIBARROW_DOWNLOAD=true\nNOT_CRAN=true\n```\n\n----------------------------------------\n\nTITLE: R Package Test Project Structure\nDESCRIPTION: Standard directory layout for testthat-based testing in the Arrow R package, showing the typical location of test files and configuration\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/step_by_step/testing.rst#2025-04-16_snippet_2\n\nLANGUAGE: R\nCODE:\n```\ntests\n ├── testthat      # test files live here\n └── testthat.R    # runs tests when R CMD check runs\n```\n\n----------------------------------------\n\nTITLE: IPC Message Format Structure\nDESCRIPTION: Binary format specification for encapsulated IPC messages including continuation indicator, metadata size, and body layout.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Columnar.rst#2025-04-16_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n<continuation: 0xFFFFFFFF>\n<metadata_size: int32>\n<metadata_flatbuffer: bytes>\n<padding>\n<message body>\n```\n\n----------------------------------------\n\nTITLE: Configuring LD_LIBRARY_PATH for gRPC Protobuf Plugin\nDESCRIPTION: Sets up the LD_LIBRARY_PATH environment variable to include the Protobuf library directory, which is required for the gRPC protobuf plugin to function correctly.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport LD_LIBRARY_PATH=$PROTOBUF_HOME/lib:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: PyArrow Import Example (Implied Code Pattern)\nDESCRIPTION: An implied code pattern showing how PyArrow exposes C++ functionalities through Python modules by importing from internal implementations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/getting_involved.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Typical pattern in *.py files\nfrom pyarrow.lib import SomeClass, some_function\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow ZSTD compression with CMake\nDESCRIPTION: This snippet shows how to enable support for ZSTD compression.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_41\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_WITH_ZSTD=ON\"\n```\n\n----------------------------------------\n\nTITLE: Installing Ceph and Skyhook Dependencies on Ubuntu\nDESCRIPTION: This snippet installs the necessary dependencies for Ceph and Skyhook on Ubuntu 20.04 or above using apt.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/dataset_skyhook_scan_example.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\napt update\napt install -y cmake \\\n                 libradospp-dev \\\n                 rados-objclass-dev \\\n                 ceph \\\n                 ceph-common \\\n                 ceph-osd \\\n                 ceph-mon \\\n                 ceph-mgr \\\n                 ceph-mds \\\n                 rbd-mirror \\\n                 ceph-fuse \\\n                 rapidjson-dev \\\n                 libboost-all-dev \\\n                 python3-pip\n```\n\n----------------------------------------\n\nTITLE: Viewing Git Log and Pushing Changes\nDESCRIPTION: Explains how to use `git log` to review commit history and `git push` to upload changes to the designated GitHub branch. This process assists in synchronizing local changes with the remote repository.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_13\n\nLANGUAGE: console\nCODE:\n```\n$ git log\ncommit 170ef85beb8ee629be651e3f93bcc4a69e29cfb8 (HEAD -> ARROW-14977)\nAuthor: Alenka Frim <frim.alenka@gmail.com>\nDate:   Tue Dec 7 13:45:06 2021 +0100\n\n    Adding a new compute feature for tutorial purposes\n\ncommit 8cebc4948ab5c5792c20a3f463e2043e01c49828 (main)\nAuthor: Sutou Kouhei <kou@clear-code.com>\nDate:   Sun Dec 5 15:19:46 2021 +0900\n\n    ARROW-14981: [CI][Docs] Upload built documents\n\n    We can use this in release process instead of building on release\n    manager's local environment.\n\n    Closes #11856 from kou/ci-docs-upload\n\n    Authored-by: Sutou Kouhei <kou@clear-code.com>\n    Signed-off-by: Sutou Kouhei <kou@clear-code.com>\n```\n\nLANGUAGE: console\nCODE:\n```\n$ git push origin ARROW-14977\nEnumerating objects: 13, done.\nCounting objects: 100% (13/13), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (7/7), done.\nWriting objects: 100% (7/7), 1.19 KiB | 1.19 MiB/s, done.\nTotal 7 (delta 6), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (6/6), completed with 6 local objects.\nremote:\nremote: Create a pull request for 'ARROW-14977' on GitHub by visiting:\nremote:      https://github.com/AlenkaF/arrow/pull/new/ARROW-14977\nremote:\nTo https://github.com/AlenkaF/arrow.git\n * [new branch]          ARROW-14977 -> ARROW-14977\n```\n\n----------------------------------------\n\nTITLE: Configuring TOC Tree for Arrow GLib Documentation\nDESCRIPTION: RestructuredText directive that sets up the table of contents tree for Apache Arrow GLib documentation sections, including components like CUDA, Dataset, Flight, Parquet and Gandiva.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/c_glib/index.md#2025-04-16_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n```{toctree}\n:maxdepth: 1\n\nApache Arrow GLib <arrow-glib/index>\nApache Arrow CUDA GLib <arrow-cuda-glib/index>\nApache Arrow Dataset <arrow-dataset-glib/index>\nApache Arrow Flight GLib <arrow-flight-glib/index>\nApache Arrow Flight SQL GLib <arrow-flight-sql-glib/index>\nApache Parquet GLib <parquet-glib/index>\nGandiva GLib <gandiva-glib/index>\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Main Library Target in CMake\nDESCRIPTION: Defines the core Arrow library target with all its sources, dependencies, link flags and installation settings. Uses the add_arrow_lib helper function to create both shared and static versions if configured.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_lib(arrow\n              CMAKE_PACKAGE_NAME\n              Arrow\n              PKG_CONFIG_NAME\n              arrow\n              SOURCES\n              ${ARROW_SRCS}\n              OUTPUTS\n              ARROW_LIBRARIES\n              PRECOMPILED_HEADERS\n              \"$<$<COMPILE_LANGUAGE:CXX>:arrow/pch.h>\"\n              SHARED_LINK_FLAGS\n              ${ARROW_SHARED_LINK_FLAGS}\n              SHARED_PRIVATE_LINK_LIBS\n              ${ARROW_ARRAY_TARGET_SHARED}\n              ${ARROW_COMPUTE_TARGET_SHARED}\n              ${ARROW_CSV_TARGET_SHARED}\n              ${ARROW_FILESYSTEM_TARGET_SHARED}\n              ${ARROW_INTEGRATION_TARGET_SHARED}\n              ${ARROW_IO_TARGET_SHARED}\n              ${ARROW_IPC_TARGET_SHARED}\n              ${ARROW_JSON_TARGET_SHARED}\n              ${ARROW_MEMORY_POOL_TARGET_SHARED}\n              ${ARROW_ORC_TARGET_SHARED}\n              ${ARROW_TELEMETRY_TARGET_SHARED}\n              ${ARROW_UTIL_TARGET_SHARED}\n              ${ARROW_VENDORED_TARGET_SHARED}\n              ${ARROW_SHARED_PRIVATE_LINK_LIBS}\n              ${ARROW_SYSTEM_LINK_LIBS}\n              STATIC_LINK_LIBS\n              ${ARROW_ARRAY_TARGET_STATIC}\n              ${ARROW_COMPUTE_TARGET_STATIC}\n              ${ARROW_CSV_TARGET_STATIC}\n              ${ARROW_FILESYSTEM_TARGET_STATIC}\n              ${ARROW_INTEGRATION_TARGET_STATIC}\n              ${ARROW_IO_TARGET_STATIC}\n              ${ARROW_IPC_TARGET_STATIC}\n              ${ARROW_JSON_TARGET_STATIC}\n              ${ARROW_MEMORY_POOL_TARGET_STATIC}\n              ${ARROW_ORC_TARGET_STATIC}\n              ${ARROW_TELEMETRY_TARGET_STATIC}\n              ${ARROW_UTIL_TARGET_STATIC}\n              ${ARROW_VENDORED_TARGET_STATIC}\n              ${ARROW_SYSTEM_LINK_LIBS}\n              STATIC_INSTALL_INTERFACE_LIBS\n              ${ARROW_STATIC_INSTALL_INTERFACE_LIBS}\n              SHARED_INSTALL_INTERFACE_LIBS\n              ${ARROW_SHARED_INSTALL_INTERFACE_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Displaying In-Memory Query Results with glimpse() in R Arrow\nDESCRIPTION: Shows how glimpse() handles in-memory query results from an Arrow table. For in-memory queries, even aggregations, glimpse() displays the actual results rather than just schema information.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_12\n\nLANGUAGE: r\nCODE:\n```\nexample_data %>% arrow_table() %>% summarize(sum(int, na.rm = TRUE)) %>%\n  glimpse()\n```\n\n----------------------------------------\n\nTITLE: Configuring pkg-config Support\nDESCRIPTION: Adds pkg-config support for arrow-orc component.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/adapters/orc/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\narrow_add_pkg_config(\"arrow-orc\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Simple Table with glimpse() in R Arrow\nDESCRIPTION: Demonstrates glimpse() with a simple Table containing just one numeric column. For simple schemas, no additional message about viewing schema details is displayed.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_7\n\nLANGUAGE: r\nCODE:\n```\nglimpse(Table$create(a = 1))\n```\n\n----------------------------------------\n\nTITLE: Installing JavaScript Dependencies for Arrow\nDESCRIPTION: This snippet installs Node.js, Yarn, and other dependencies required for building and running Arrow JavaScript benchmarks.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo apt -y upgrade\nsudo apt update\nsudo apt -y install curl dirmngr apt-transport-https lsb-release ca-certificates\ncurl -fsSL https://deb.nodesource.com/setup_14.x | sudo -E bash -\nsudo apt-get install -y nodejs\nsudo apt -y install yarn\nsudo apt -y install gcc g++ make\n```\n\n----------------------------------------\n\nTITLE: Extension Type Creation Error Message in R\nDESCRIPTION: Error message shown when attempting to create an extension type with an invalid input. The error indicates that the extension_array parameter must be a ChunkedArray or ExtensionArray, but received a character type instead.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/extension.md#2025-04-16_snippet_0\n\nLANGUAGE: r\nCODE:\n```\n`extension_array` must be a ChunkedArray or ExtensionArray\ni Got object of type character\n```\n\n----------------------------------------\n\nTITLE: Using Docker Compose to Build Fedora Environment\nDESCRIPTION: Command to build the Docker container using Docker Compose, which simplifies the build process by using predefined configurations in a compose file.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose build\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Parquet Examples in CMake\nDESCRIPTION: Adds Parquet read-write example when Arrow Parquet is enabled, using either shared or static libraries based on build configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/arrow/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_PARQUET)\n  if(ARROW_BUILD_SHARED)\n    add_arrow_example(parquet_read_write EXTRA_LINK_LIBS parquet_shared)\n  else()\n    add_arrow_example(parquet_read_write EXTRA_LINK_LIBS parquet_static)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Customizing Parameter Binding in JdbcParameterBinder\nDESCRIPTION: This snippet presents how to customize the mapping of Arrow data columns to JDBC statement parameters using specific bind configurations. It demonstrates binding parameters by index and using custom column binders.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/jdbc.rst#2025-04-16_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nfinal JdbcParameterBinder binder =\\n    JdbcParameterBinder.builder(statement, root)\\n        .bind(/*parameterIndex*/2, /*columnIndex*/0)\\n        .bind(/*parameterIndex*/1, customColumnBinderInstance)\\n        .build();\n```\n\n----------------------------------------\n\nTITLE: Table of Contents Configuration in RestructuredText for Apache Arrow Contribution Guide\nDESCRIPTION: A RestructuredText toctree directive that defines the structure and navigation for the guide on making a first PR to Apache Arrow. It includes links to pages covering setup, building, finding issues, understanding the codebase, testing, styling, and the PR lifecycle.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/step_by_step/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   set_up\n   building\n   finding_issues\n   arrow_codebase\n   testing\n   styling\n   pr_lifecycle\n```\n\n----------------------------------------\n\nTITLE: Namespace Refactoring for FlatBuffers Headers in C++\nDESCRIPTION: This snippet shows the common pattern applied across multiple FlatBuffers header files to move the flatbuffers namespace into a private namespace and create an alias. This change helps prevent conflicts with other versions of FlatBuffers that might be used in a project.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/thirdparty/flatbuffers/README.md#2025-04-16_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n// Move this vendored copy of flatbuffers to a private namespace,\n// but continue to access it through the \"flatbuffers\" alias.\nnamespace arrow_vendored_private::flatbuffers {\n}\nnamespace flatbuffers = arrow_vendored_private::flatbuffers;\n```\n\n----------------------------------------\n\nTITLE: Running Release Verification on Windows\nDESCRIPTION: Command for verifying Apache Arrow releases on Windows. This requires downloading the source tarball from the SVN dist system before running the verification script.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release_verification.rst#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndev\\release\\verify-release-candidate.bat %VERSION% %RC_NUM%\n```\n\n----------------------------------------\n\nTITLE: Linking to Crossbow Usage Guide in HTML\nDESCRIPTION: This HTML snippet provides a link to the Crossbow usage guide documentation page for Apache Arrow developers.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/tasks/README.md#2025-04-16_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<a href=\"../../docs/source/developers/continuous_integration/crossbow.rst\">documentation page</a>\n```\n\n----------------------------------------\n\nTITLE: Updating Namespace for FlatBuffers Implementation in C++\nDESCRIPTION: This snippet demonstrates how the flatbuffers namespace is wrapped within the arrow_vendored_private namespace in the implementation files. This change is consistent across multiple files to ensure all FlatBuffers code is properly isolated.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/thirdparty/flatbuffers/README.md#2025-04-16_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nnamespace arrow_vendored_private {\nnamespace flatbuffers {\n\n// Original flatbuffers code here\n\n}  // namespace flatbuffers\n}  // namespace arrow_vendored_private\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Flight Test Linkage in CMake\nDESCRIPTION: Sets up the linkage configuration for Arrow Flight integration tests, determining whether to use static or shared libraries based on the ARROW_FLIGHT_TEST_LINKAGE setting. Appends additional required libraries including GFLAGS.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/integration_tests/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_FLIGHT_TEST_LINKAGE STREQUAL \"static\" AND ARROW_BUILD_STATIC)\n  set(ARROW_FLIGHT_INTEGRATION_TEST_LINK_LIBS arrow_flight_sql_static)\nelse()\n  set(ARROW_FLIGHT_INTEGRATION_TEST_LINK_LIBS arrow_flight_sql_shared)\nendif()\nlist(APPEND ARROW_FLIGHT_INTEGRATION_TEST_LINK_LIBS ${ARROW_FLIGHT_TEST_LINK_LIBS}\n     ${GFLAGS_LIBRARIES})\n```\n\n----------------------------------------\n\nTITLE: Defining GTK Documentation Build Option in Meson\nDESCRIPTION: Defines a boolean option to enable or disable building GTK-specific documentation. Contains a commented-out deprecated attribute that would require Meson 0.63.0 or later.\nSOURCE: https://github.com/apache/arrow/blob/main/c_glib/meson_options.txt#2025-04-16_snippet_3\n\nLANGUAGE: meson\nCODE:\n```\noption('gtk_doc',\n       type: 'boolean',\n       value: false,\n       # This requires Meson 0.63.0 or later\n       # deprecated: 'doc',\n       description: 'Build document')\n```\n\n----------------------------------------\n\nTITLE: Installing Pyodide Build Tools\nDESCRIPTION: Command to install the Pyodide build tools via pip, specifying version compatibility requirements.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/emscripten.rst#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# install Pyodide build tools.\n# e.g., for version 0.26 of Pyodide, pyodide-build 0.26 and later work\npip install \"pyodide-build>=0.26\"\n```\n\n----------------------------------------\n\nTITLE: Viewing Git Commit History - Console\nDESCRIPTION: This snippet illustrates how to view the history of commits in a Git repository using the git log command. This is essential for tracking changes and understanding the evolution of the codebase.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_14\n\nLANGUAGE: console\nCODE:\n```\n$ git log\n\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies\nDESCRIPTION: Defines required Python packages and dependencies, including a conditional requirement for pyuwsgi that is platform and version specific. The list includes common data science and testing packages needed for Apache Arrow development.\nSOURCE: https://github.com/apache/arrow/blob/main/python/requirements-test.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncffi\nhypothesis\npackaging\npandas\npytest\npytz\npyuwsgi; sys.platform != 'win32' and python_version < '3.13'\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Tests and Benchmarks\nDESCRIPTION: Sets up memory-related test and benchmark targets, with SIMD-dependent configuration for memory benchmarks.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/io/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_test(memory_test PREFIX \"arrow-io\")\n\nadd_arrow_benchmark(file_benchmark PREFIX \"arrow-io\")\n\nif(NOT (${ARROW_SIMD_LEVEL} STREQUAL \"NONE\") AND NOT (${ARROW_SIMD_LEVEL} STREQUAL \"NEON\"\n                                                     ))\n  # This benchmark either requires SSE4.2 or ARMV8 SIMD to be enabled\n  add_arrow_benchmark(memory_benchmark PREFIX \"arrow-io\")\nendif()\n\nadd_arrow_benchmark(compressed_benchmark PREFIX \"arrow-io\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow Filesystem API with CMake\nDESCRIPTION: This snippet shows how to enable the Filesystem API for accessing local and remote filesystems by passing a boolean flag to CMake.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_FILESYSTEM=ON\"\n```\n\n----------------------------------------\n\nTITLE: Installing xxhash Headers in Apache Arrow\nDESCRIPTION: A CMake command that installs all header files from the 'arrow/vendored/xxhash' directory as part of the Arrow build process. This ensures that the vendored xxhash headers are properly installed alongside Arrow's own headers.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/xxhash/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/vendored/xxhash\")\n```\n\n----------------------------------------\n\nTITLE: Submitting Crossbow Tasks in Console\nDESCRIPTION: Examples of submitting various Crossbow tasks for building and testing Apache Arrow components.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/crossbow.rst#2025-04-16_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ archery crossbow submit conda-win conda-linux conda-osx\n```\n\nLANGUAGE: console\nCODE:\n```\n$ git checkout ARROW-<ticket number>\n$ archery crossbow submit --dry-run conda-linux conda-osx\n```\n\nLANGUAGE: console\nCODE:\n```\n$ archery crossbow submit debian-stretch conda-linux-gcc-py37-r40\n```\n\nLANGUAGE: console\nCODE:\n```\n$ archery crossbow submit --dry-run task_name\n```\n\nLANGUAGE: console\nCODE:\n```\n$ archery crossbow submit --group conda centos-7\n```\n\nLANGUAGE: console\nCODE:\n```\n$ archery crossbow submit --group wheel\n```\n\n----------------------------------------\n\nTITLE: Adding pkg-config Support for Arrow CSV in CMake\nDESCRIPTION: Adds pkg-config support for the Arrow CSV module. This allows other projects to easily find and link against the Arrow CSV library using pkg-config.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/csv/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\narrow_add_pkg_config(\"arrow-csv\")\n```\n\n----------------------------------------\n\nTITLE: Adding Dynamic Loading Libraries for Arrow in CMake\nDESCRIPTION: Adds dynamic loading libraries (dlopen API) if available on the platform to both static and shared linking configurations.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_DL_LIBS)\n  list(APPEND ARROW_SHARED_INSTALL_INTERFACE_LIBS ${CMAKE_DL_LIBS})\n  list(APPEND ARROW_STATIC_INSTALL_INTERFACE_LIBS ${CMAKE_DL_LIBS})\n  list(APPEND ARROW_TEST_SHARED_LINK_LIBS ${CMAKE_DL_LIBS})\n  list(APPEND ARROW_TEST_STATIC_LINK_LIBS ${CMAKE_DL_LIBS})\n  string(APPEND ARROW_PC_LIBS_PRIVATE \" -l${CMAKE_DL_LIBS}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Filtering Arrow Dataset with Unsupported Expression in R\nDESCRIPTION: This code snippet attempts to filter an Arrow dataset using dplyr's filter() function with an unsupported expression. It demonstrates the error message received when trying to use max() within the filter condition.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dataset-dplyr.md#2025-04-16_snippet_0\n\nLANGUAGE: R\nCODE:\n```\nds %>% filter(int > 6, dbl > max(dbl))\n```\n\n----------------------------------------\n\nTITLE: Adding Skyhook Protocol Test\nDESCRIPTION: Configures and adds the protocol test to the build with appropriate link libraries and prefixes.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/skyhook/CMakeLists.txt#2025-04-16_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_test(protocol_test\n               SOURCES\n               protocol/skyhook_protocol_test.cc\n               EXTRA_LINK_LIBS\n               ${ARROW_SKYHOOK_TEST_LINK_LIBS}\n               PREFIX\n               \"skyhook\")\n```\n\n----------------------------------------\n\nTITLE: Setting Protobuf Source to Bundled with CMake\nDESCRIPTION: This snippet shows how to override the default dependency resolution strategy for Protocol Buffers and build it from source using the BUNDLED method.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_53\n\nLANGUAGE: shell\nCODE:\n```\n\"-DProtobuf_SOURCE=BUNDLED\"\n```\n\n----------------------------------------\n\nTITLE: Building Specific .deb Packages for Supported Platforms\nDESCRIPTION: This command builds .deb packages for specific target platforms using the APT_TARGETS environment variable.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/tasks/linux-packages/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd dev/tasks/linux-packages/apache-arrow\nrake version:update\nrake apt:build APT_TARGETS=debian-bookworm,ubuntu-noble\n```\n\n----------------------------------------\n\nTITLE: Executing the Skyhook Example\nDESCRIPTION: This snippet runs the Skyhook example, specifying the library path and the dataset location.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/dataset_skyhook_scan_example.rst#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nLD_LIBRARY_PATH=/usr/local/lib release/dataset-skyhook-scan-example file:///mnt/cephfs/nyc\n```\n\n----------------------------------------\n\nTITLE: Building Vala Examples with Arrow Dependencies\nDESCRIPTION: Command line instruction for compiling Vala example files with Arrow GLib and POSIX dependencies. The XXX.vala placeholder should be replaced with the actual example filename.\nSOURCE: https://github.com/apache/arrow/blob/main/c_glib/example/vala/README.md#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nvalac --pkg arrow-glib --pkg posix XXX.vala\n```\n\n----------------------------------------\n\nTITLE: Building the Gandiva Library with Arrow's CMake Helpers\nDESCRIPTION: Uses Arrow's custom add_arrow_lib helper function to build the Gandiva library with all its dependencies, outputs, and compilation flags. Configures both shared and static library variants with their specific requirements.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/CMakeLists.txt#2025-04-16_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_lib(gandiva\n              CMAKE_PACKAGE_NAME\n              Gandiva\n              PKG_CONFIG_NAME\n              gandiva\n              SOURCES\n              ${SRC_FILES}\n              PRECOMPILED_HEADERS\n              \"$<$<COMPILE_LANGUAGE:CXX>:gandiva/pch.h>\"\n              OUTPUTS\n              GANDIVA_LIBRARIES\n              DEPENDENCIES\n              precompiled\n              SHARED_LINK_FLAGS\n              ${GANDIVA_SHARED_LINK_FLAGS}\n              SHARED_LINK_LIBS\n              ${GANDIVA_SHARED_LINK_LIBS}\n              SHARED_PRIVATE_LINK_LIBS\n              ${GANDIVA_SHARED_PRIVATE_LINK_LIBS}\n              SHARED_INSTALL_INTERFACE_LIBS\n              Arrow::arrow_shared\n              LLVM::LLVM_HEADERS\n              STATIC_LINK_LIBS\n              ${GANDIVA_STATIC_LINK_LIBS}\n              STATIC_INSTALL_INTERFACE_LIBS\n              Arrow::arrow_static\n              LLVM::LLVM_HEADERS\n              LLVM::LLVM_LIBS)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Target for Test Data in CMake\nDESCRIPTION: Creates a custom target to copy test data from the source directory to the build directory.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/tests/external_functions/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(extension-tests-data\n                  COMMAND ${CMAKE_COMMAND} -E copy_directory ${CMAKE_CURRENT_SOURCE_DIR}\n                          ${CMAKE_CURRENT_BINARY_DIR})\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Acero Library Build in CMake\nDESCRIPTION: Sets up the Arrow Acero library build with source files, dependencies, and build configurations. Handles both static and shared library builds, defines source files, and manages dependencies like OpenTelemetry.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/acero/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_lib(arrow_acero\n              CMAKE_PACKAGE_NAME\n              ArrowAcero\n              PKG_CONFIG_NAME\n              arrow-acero\n              OUTPUTS\n              ARROW_ACERO_LIBRARIES\n              SOURCES\n              ${ARROW_ACERO_SRCS}\n              PRECOMPILED_HEADERS\n              \"$<$<COMPILE_LANGUAGE:CXX>:arrow/acero/pch.h>\"\n              SHARED_LINK_LIBS\n              ${ARROW_ACERO_SHARED_LINK_LIBS}\n              SHARED_PRIVATE_LINK_LIBS\n              ${ARROW_ACERO_SHARED_PRIVATE_LINK_LIBS}\n              SHARED_INSTALL_INTERFACE_LIBS\n              ${ARROW_ACERO_SHARED_INSTALL_INTERFACE_LIBS}\n              STATIC_LINK_LIBS\n              ${ARROW_ACERO_STATIC_LINK_LIBS}\n              STATIC_INSTALL_INTERFACE_LIBS\n              ${ARROW_ACERO_STATIC_INSTALL_INTERFACE_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Installing Archery Toolset with Crossbow in Console\nDESCRIPTION: Command to install the Archery toolset containing Crossbow using pip.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/crossbow.rst#2025-04-16_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ pip install -e \"arrow/dev/archery[crossbow]\"\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow Adapter Headers in Apache Arrow\nDESCRIPTION: This command installs all headers for the Apache Arrow TensorFlow adapter. It ensures that the necessary header files are available for development and integration with TensorFlow.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/adapters/tensorflow/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/adapters/tensorflow\")\n```\n\n----------------------------------------\n\nTITLE: Setting Test File Properties\nDESCRIPTION: Configures specific build properties for the adapter test file, disabling precompiled headers and unity build inclusion.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/adapters/orc/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset_source_files_properties(adapter_test.cc PROPERTIES SKIP_PRECOMPILE_HEADERS ON\n                                                       SKIP_UNITY_BUILD_INCLUSION ON)\n```\n\n----------------------------------------\n\nTITLE: Installing LGI on Debian/Ubuntu for Arrow Lua Examples\nDESCRIPTION: These console commands install LGI (Lua-GObject Introspection) on Debian GNU/Linux and Ubuntu systems. LGI is required to use Arrow GLib based bindings in the Lua examples.\nSOURCE: https://github.com/apache/arrow/blob/main/c_glib/example/lua/README.md#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ sudo apt install -y luarocks\n$ sudo luarocks install lgi\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Library Linkage Flags for Gandiva\nDESCRIPTION: Configures compiler flags for static library linking in Gandiva. This code handles the case when building only the static library and not the shared library, by properly setting the pkg-config flags.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT ARROW_BUILD_SHARED AND ARROW_BUILD_STATIC)\n  string(APPEND GANDIVA_PC_CFLAGS \"${GANDIVA_PC_CFLAGS_PRIVATE}\")\n  set(GANDIVA_PC_CFLAGS_PRIVATE \"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Specific JDK\nDESCRIPTION: Commands to run Arrow tests using a specific JDK version using Maven\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n$ cd arrow/java\n$ mvn -Darrow.test.jdk-version=17 clean verify\n```\n\n----------------------------------------\n\nTITLE: Example of Updating Double-Conversion to Version 3.3.0\nDESCRIPTION: Demonstrates a specific example of updating the double-conversion library to version 3.3.0 using the update script.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/double-conversion/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./update.sh 3.3.0\n```\n\n----------------------------------------\n\nTITLE: Extended Expressions Query Results\nDESCRIPTION: Output showing the results of the extended expressions query with filtered and computed columns.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/substrait.rst#2025-04-16_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ncolumn-1  column-2\n13        ROMANIA - ular asymptotes are about the furious multipliers. express dependencies nag above the ironically ironic account\n14        SAUDI ARABIA - ts. silent requests haggle. closely express packages sleep across the blithely\n12        VIETNAM - hely enticingly express accounts. even, final\n13        RUSSIA -  requests against the platelets use never according to the quickly regular pint\n13        UNITED KINGDOM - eans boost carefully special requests. accounts are. carefull\n11        UNITED STATES - y final packages. slow foxes cajole quickly. quickly silent platelets breach ironic accounts. unusual pinto be\n```\n\n----------------------------------------\n\nTITLE: Utility Test Dependencies Configuration\nDESCRIPTION: Sets up link dependencies for utility tests, including optional components like XSIMD and OpenTelemetry.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/util/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(ARROW_UTILITY_TEST_LINK_LIBS Boost::headers)\nif(ARROW_USE_XSIMD)\n  list(APPEND ARROW_UTILITY_TEST_LINK_LIBS ${ARROW_XSIMD})\nendif()\nif(ARROW_WITH_OPENTELEMETRY)\n  list(APPEND ARROW_UTILITY_TEST_LINK_LIBS ${ARROW_OPENTELEMETRY_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Parquet Arrow Fuzzing Corpus Generator\nDESCRIPTION: Conditionally creates a fuzzing corpus generator executable when ARROW_FUZZING is enabled. Links against either static or shared libraries based on build configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/arrow/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_FUZZING)\n  add_executable(parquet-arrow-generate-fuzz-corpus generate_fuzz_corpus.cc)\n  if(ARROW_BUILD_STATIC)\n    target_link_libraries(parquet-arrow-generate-fuzz-corpus parquet_static\n                          arrow_testing_static)\n  else()\n    target_link_libraries(parquet-arrow-generate-fuzz-corpus parquet_shared\n                          arrow_testing_shared)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Updating Conan Port\nDESCRIPTION: This script prepares a pull request to update the Arrow port in Conan. It requires a fork of the conan-center-index repository.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_16\n\nLANGUAGE: Bash\nCODE:\n```\ndev/release/post-16-conan.sh X.Y.Z <YOUR_CONAN_CENTER_INDEX_FORK>\n```\n\n----------------------------------------\n\nTITLE: Installing Required Software with Conda - Shell\nDESCRIPTION: This code snippet installs the required software for building the documentation using Conda. It fetches dependencies from a specified Conda environment file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/documentation.rst#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nconda install -c conda-forge --file=arrow/ci/conda_env_sphinx.txt\n```\n\n----------------------------------------\n\nTITLE: Displaying Archery's Main Help Menu\nDESCRIPTION: Command to view Archery's main help menu, showing available options and subcommands for Arrow development utilities.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/archery.rst#2025-04-16_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ archery --help\n```\n\n----------------------------------------\n\nTITLE: Implementing group_by example\nDESCRIPTION: Example call showing the dplyr group_by syntax supported in Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_1\n\nLANGUAGE: R\nCODE:\n```\ngroup_by(my_table, .by = group_col)\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Acero Source Files in CMake\nDESCRIPTION: Specifies the source files that comprise the Arrow Acero library. This includes core functionality files and platform-specific optimized implementations using AVX2 instruction sets.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/acero/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_ACERO_SRCS\n    accumulation_queue.cc\n    scalar_aggregate_node.cc\n    groupby_aggregate_node.cc\n    aggregate_internal.cc\n    asof_join_node.cc\n    bloom_filter.cc\n    exec_plan.cc\n    fetch_node.cc\n    filter_node.cc\n    hash_join.cc\n    hash_join_dict.cc\n    hash_join_node.cc\n    map_node.cc\n    options.cc\n    order_by_node.cc\n    order_by_impl.cc\n    partition_util.cc\n    pivot_longer_node.cc\n    project_node.cc\n    query_context.cc\n    sink_node.cc\n    sorted_merge_node.cc\n    source_node.cc\n    swiss_join.cc\n    task_util.cc\n    time_series_util.cc\n    tpch_node.cc\n    union_node.cc\n    util.cc)\n\nappend_runtime_avx2_src(ARROW_ACERO_SRCS bloom_filter_avx2.cc)\nappend_runtime_avx2_src(ARROW_ACERO_SRCS swiss_join_avx2.cc)\n```\n\n----------------------------------------\n\nTITLE: Setting Arrow Pull Behavior\nDESCRIPTION: Code example showing how to configure the default behavior of dplyr::pull() to return a ChunkedArray instead of an R vector\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_5\n\nLANGUAGE: R\nCODE:\n```\npull(as_vector = FALSE)\noptions(arrow.pull_as_vector = FALSE)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Target for Precompiled Bitcode Generation\nDESCRIPTION: Defines a custom target named 'precompiled' that depends on the generated bitcode file and the C++ source file. This allows the precompiled bitcode generation to be included in the build process.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/precompiled/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(precompiled ALL DEPENDS ${GANDIVA_PRECOMPILED_BC_PATH}\n                                          ${GANDIVA_PRECOMPILED_CC_PATH})\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Modules with Shell Commands\nDESCRIPTION: These shell commands configure Java to expose specific JDK internals needed by Apache Arrow modules. The command includes flags to add necessary reads and opens for unnamed modules, crucial for avoiding access errors during runtime. Ensure Java version compatibility with JDK 11 or above as per the document guidelines.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/install.rst#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ java --add-opens=java.base/java.nio=org.apache.arrow.memory.core,ALL-UNNAMED -jar ...\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ env JDK_JAVA_OPTIONS=\"--add-opens=java.base/java.nio=org.apache.arrow.memory.core,ALL-UNNAMED\" java -jar ...\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Ubuntu Minimal Environment\nDESCRIPTION: Command to build a Docker image for Ubuntu 24.04 that will be used as the environment for building PyArrow.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t arrow_ubuntu_minimal -f Dockerfile.ubuntu .\n```\n\n----------------------------------------\n\nTITLE: Setting Up Gandiva Source Files List\nDESCRIPTION: Defines the list of source files that comprise the Gandiva library. This includes all the implementation files for different functionalities like expression handling, LLVM code generation, and function registries.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(SRC_FILES\n    annotator.cc\n    bitmap_accumulator.cc\n    cache.cc\n    cast_time.cc\n    configuration.cc\n    context_helper.cc\n    decimal_ir.cc\n    decimal_type_util.cc\n    decimal_xlarge.cc\n    engine.cc\n    date_utils.cc\n    encrypt_utils.cc\n    expr_decomposer.cc\n    expr_validator.cc\n    expression.cc\n    expression_registry.cc\n    exported_funcs_registry.cc\n    exported_funcs.cc\n    external_c_functions.cc\n    filter.cc\n    function_holder_maker_registry.cc\n    function_ir_builder.cc\n    function_registry.cc\n    function_registry_arithmetic.cc\n    function_registry_datetime.cc\n    function_registry_hash.cc\n    function_registry_math_ops.cc\n    function_registry_string.cc\n    function_registry_timestamp_arithmetic.cc\n    function_signature.cc\n    gandiva_object_cache.cc\n    gdv_function_stubs.cc\n    gdv_string_function_stubs.cc\n    gdv_hash_function_stubs.cc\n    hash_utils.cc\n    interval_holder.cc\n    llvm_generator.cc\n    llvm_types.cc\n    literal_holder.cc\n    projector.cc\n    regex_util.cc\n    regex_functions_holder.cc\n    selection_vector.cc\n    tree_expr_builder.cc\n    to_date_holder.cc\n    random_generator_holder.cc\n    ${GANDIVA_PRECOMPILED_CC_PATH})\n```\n\n----------------------------------------\n\nTITLE: Listing Supported Platforms for .rpm Packages\nDESCRIPTION: This command lists all supported platforms for .rpm packages by examining the directory structure.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/tasks/linux-packages/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nfor x in dev/tasks/linux-packages/apache-arrow/yum/{alma,amazon,centos}*; do basename $x; done\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow IPC extensions with CMake\nDESCRIPTION: This snippet shows how to enable building the IPC extensions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_42\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_IPC=ON\"\n```\n\n----------------------------------------\n\nTITLE: Passing Environment Variables to Docker Container\nDESCRIPTION: Command to pass environment variables to the Docker container when running a build.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\narchery docker run --env CMAKE_BUILD_TYPE=release ubuntu-cpp\n```\n\n----------------------------------------\n\nTITLE: Returning Status at Program End in Arrow C++ Example\nDESCRIPTION: Shows how to return Status::OK() at the end of an Arrow application to indicate successful completion. This is a common pattern in Arrow C++ code to propagate success status to the main function.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/basic_arrow.rst#2025-04-16_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n(Doc section: Ret)\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow Tensor Headers\nDESCRIPTION: CMake directive to install all headers from the arrow/tensor directory into the project.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/tensor/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\narrow_install_all_headers(\"arrow/tensor\")\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding Tests Based on ARROW_JSON Flag in CMake\nDESCRIPTION: Conditionally extends the test file list with additional tests when the ARROW_JSON feature is enabled. Adds fixed_shape_tensor_test.cc and opaque_test.cc to the test suite.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/extension/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_JSON)\n  list(APPEND CANONICAL_EXTENSION_TESTS fixed_shape_tensor_test.cc opaque_test.cc)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Getting IO Thread Pool Capacity in Apache Arrow (C++)\nDESCRIPTION: This function retrieves the current capacity of the IO thread pool in Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/thread.rst#2025-04-16_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\narrow::io::GetIOThreadPoolCapacity\n```\n\n----------------------------------------\n\nTITLE: Exporting GitHub Token in Bash\nDESCRIPTION: Command to export the GitHub Personal Access Token as an environment variable for Crossbow authentication.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/crossbow.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport CROSSBOW_GITHUB_TOKEN=<token>\n```\n\n----------------------------------------\n\nTITLE: Configuring Lint Targets in CMake\nDESCRIPTION: Sets up linting targets including cpplint and exclusion file configuration. Defines common lint options and creates a custom lint target that runs cpplint on source files.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT ARROW_VERBOSE_LINT)\n  set(ARROW_LINT_QUIET \"--quiet\")\nendif()\n\nif(NOT LINT_EXCLUSIONS_FILE)\n  set(LINT_EXCLUSIONS_FILE ${BUILD_SUPPORT_DIR}/lint_exclusions.txt)\nendif()\n\nfind_program(CPPLINT_BIN\n             NAMES cpplint cpplint.py\n             HINTS ${BUILD_SUPPORT_DIR})\n\nset(COMMON_LINT_OPTIONS\n    --exclude_globs\n    ${LINT_EXCLUSIONS_FILE}\n    --source_dir\n    ${CMAKE_CURRENT_SOURCE_DIR}/src\n    --source_dir\n    ${CMAKE_CURRENT_SOURCE_DIR}/examples\n    --source_dir\n    ${CMAKE_CURRENT_SOURCE_DIR}/tools)\n\nadd_custom_target(lint\n                  ${PYTHON_EXECUTABLE}\n                  ${BUILD_SUPPORT_DIR}/run_cpplint.py\n                  --cpplint_binary\n                  ${CPPLINT_BIN}\n                  ${COMMON_LINT_OPTIONS}\n                  ${ARROW_LINT_QUIET}\n                  WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/..)\n```\n\n----------------------------------------\n\nTITLE: Listing Supported Platforms for .deb Packages\nDESCRIPTION: This command lists all supported platforms for .deb packages by examining the directory structure.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/tasks/linux-packages/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nfor x in dev/tasks/linux-packages/apache-arrow/apt/{debian,ubuntu}*; do basename $x; done\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Requirements and Project Configuration for Parquet Arrow Example\nDESCRIPTION: Configures the CMake environment by setting the minimum required version to 3.25, defining the project name, and including necessary CMake modules. The configuration also sets the C++ standard to C++17 and makes it a hard requirement.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/parquet/parquet_arrow/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.25)\n\nproject(parquet_arrow_example)\n\ninclude(ExternalProject)\ninclude(FindPkgConfig)\ninclude(GNUInstallDirs)\n\noption(PARQUET_LINK_SHARED \"Link to the Parquet shared library\" ON)\n\n# This ensures that things like -std=gnu++... get passed correctly\nif(NOT DEFINED CMAKE_CXX_STANDARD)\n  set(CMAKE_CXX_STANDARD 17)\nendif()\n\n# We require a C++17 compliant compiler\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n```\n\n----------------------------------------\n\nTITLE: Running All Integration Tests (IPC, Flight, C Data)\nDESCRIPTION: This command runs all the integration tests, including IPC, Flight, and C Data Interface tests, with all available components enabled. The --with-all flag activates all components, and the --run-flight, --run-ipc, and --run-c-data flags specify which tests to run.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\narchery integration --with-all --run-flight --run-ipc --run-c-data\n```\n\n----------------------------------------\n\nTITLE: Running Statically-linked Arrow Build Example\nDESCRIPTION: Command for running the statically-linked Arrow build example using Docker Compose.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/minimal_build/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose run --rm static\n```\n\n----------------------------------------\n\nTITLE: Updating vcpkg Port\nDESCRIPTION: This script prepares a pull request to update the Arrow port in vcpkg. It requires a fork of the vcpkg repository.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_15\n\nLANGUAGE: Bash\nCODE:\n```\ndev/release/post-15-vcpkg.sh X.Y.Z <YOUR_VCPKG_FORK>\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Timestamp Array Printing Issue in Python\nDESCRIPTION: This code snippet illustrates a bug in pyarrow where the print method of a timestamp array with timezone doesn't display correctly and throws an error when accessing individual elements.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/bug_reports.rst#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\n\na = pa.array([0], pa.timestamp('s', tz='+02:00'))\n\nprint(a) # representation not correct?\n# <pyarrow.lib.TimestampArray object at 0x7f834c7cb9a8>\n# [\n#  1970-01-01 00:00:00\n# ]\n\nprint(a[0])\n#Traceback (most recent call last):\n#  File \"<stdin>\", line 1, in <module>\n#  File \"pyarrow/scalar.pxi\", line 80, in pyarrow.lib.Scalar.__repr__\n#  File \"pyarrow/scalar.pxi\", line 463, in pyarrow.lib.TimestampScalar.as_py\n#  File \"pyarrow/scalar.pxi\", line 393, in pyarrow.lib._datetime_from_int\n#ValueError: fromutc: dt.tzinfo is not self\n```\n\n----------------------------------------\n\nTITLE: Setting Export Definitions for Arrow Flight Libraries in CMake\nDESCRIPTION: Sets the ARROW_FLIGHT_EXPORTING compile definition for all Arrow Flight library targets to properly handle symbol visibility during compilation.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(LIB_TARGET ${ARROW_FLIGHT_LIBRARIES})\n  target_compile_definitions(${LIB_TARGET} PRIVATE ARROW_FLIGHT_EXPORTING)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Installing Red Arrow Flight Ruby Package\nDESCRIPTION: Command to install the Red Arrow Flight Ruby gem using the gem package manager.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-flight/README.md#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ gem install red-arrow-flight\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow Dependencies on Ubuntu\nDESCRIPTION: This snippet installs the necessary dependencies for building Arrow on Ubuntu, including development tools and libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo su\napt-get update -y -q && \\\n    apt-get install -y -q --no-install-recommends \\\n        autoconf \\\n        ca-certificates \\\n        ccache \\\n        cmake \\\n        g++ \\\n        gcc \\\n        gdb \\\n        git \\\n        libbenchmark-dev \\\n        libboost-filesystem-dev \\\n        libboost-regex-dev \\\n        libboost-system-dev \\\n        libbrotli-dev \\\n        libbz2-dev \\\n        libgflags-dev \\\n        libcurl4-openssl-dev \\\n        libgoogle-glog-dev \\\n        liblz4-dev \\\n        libprotobuf-dev \\\n        libprotoc-dev \\\n        libre2-dev \\\n        libsnappy-dev \\\n        libssl-dev \\\n        libthrift-dev \\\n        libutf8proc-dev \\\n        libzstd-dev \\\n        make \\\n        ninja-build \\\n        pkg-config \\\n        protobuf-compiler \\\n        rapidjson-dev \\\n        tzdata \\\n        wget && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists*\n\napt-get update -y -q && \\\n    apt-get install -y -q \\\n        python3 \\\n        python3-pip \\\n        python3-dev && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists*\n```\n\n----------------------------------------\n\nTITLE: Building .deb Packages for All Supported Platforms\nDESCRIPTION: These commands update the version and build .deb packages for all supported platforms using Rake tasks.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/tasks/linux-packages/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd dev/tasks/linux-packages/apache-arrow\nrake version:update\nrake apt:build\n```\n\n----------------------------------------\n\nTITLE: Initializing CUDA Targets in CMake for Apache Arrow\nDESCRIPTION: Sets up custom targets for building CUDA-related components of Apache Arrow. This includes targets for the main library, tests, and benchmarks.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/gpu/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(arrow_cuda-all)\nadd_custom_target(arrow_cuda)\nadd_custom_target(arrow_cuda-benchmarks)\nadd_custom_target(arrow_cuda-tests)\nadd_dependencies(arrow_cuda-all arrow_cuda arrow_cuda-tests arrow_cuda-benchmarks)\n```\n\n----------------------------------------\n\nTITLE: Running Java Benchmarks with Conbench\nDESCRIPTION: This snippet builds Arrow Java, installs archery, and runs Java benchmarks using conbench.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npushd arrow\nsource dev/conbench_envs/hooks.sh build_arrow_java\nsource dev/conbench_envs/hooks.sh install_archery\npopd\ncd benchmarks\nconbench java-micro --iterations=1\n```\n\n----------------------------------------\n\nTITLE: Configuring Conda with conda-forge\nDESCRIPTION: Command to add conda-forge channel to conda configuration which is necessary for managing build dependencies for the Arrow project on Windows using Miniconda.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nconda config --add channels conda-forge\n```\n\n----------------------------------------\n\nTITLE: Clearing IntVector in Java\nDESCRIPTION: This example shows how to clear an IntVector to release its memory.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector.rst#2025-04-16_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nvector.close();\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow Testing Headers in CMake\nDESCRIPTION: Installs all headers for the Arrow testing module using a custom CMake function.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/testing/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/testing\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Build Options in PyArrow CMake\nDESCRIPTION: Configures build settings for Parquet support in PyArrow, including shared library bundling, linking options and Cython extension setup. Handles encryption and dataset features.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nif(PYARROW_BUILD_PARQUET)\n  if(PYARROW_BUNDLE_ARROW_CPP)\n    get_filename_component(PARQUET_INCLUDE_PARQUET_DIR_REAL\n                           ${PARQUET_INCLUDE_DIR}/parquet REALPATH)\n    install(DIRECTORY ${PARQUET_INCLUDE_PARQUET_DIR_REAL}\n            DESTINATION ${CMAKE_INSTALL_INCLUDEDIR})\n  endif()\n\n  if(ARROW_BUILD_SHARED)\n    if(PYARROW_BUNDLE_ARROW_CPP)\n      bundle_arrow_lib(${PARQUET_SHARED_LIB} SO_VERSION ${PARQUET_SO_VERSION})\n      if(MSVC)\n        bundle_arrow_import_lib(${PARQUET_IMPORT_LIB})\n      endif()\n    endif()\n    set(PARQUET_LINK_LIBS Parquet::parquet_shared)\n  else()\n    set(PARQUET_LINK_LIBS)\n  endif()\n  list(APPEND CYTHON_EXTENSIONS _parquet)\n  if(PYARROW_BUILD_PARQUET_ENCRYPTION)\n    list(APPEND CYTHON_EXTENSIONS _parquet_encryption)\n  endif()\n  if(PYARROW_BUILD_DATASET)\n    list(APPEND CYTHON_EXTENSIONS _dataset_parquet)\n    if(PYARROW_BUILD_PARQUET_ENCRYPTION)\n      list(APPEND CYTHON_EXTENSIONS _dataset_parquet_encryption)\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Flight-Core with Shell Commands\nDESCRIPTION: These shell commands are tailored for when using the flight-core module in Apache Arrow, including adjustments to allow flight-core to read unnamed modules. This setup prevents IllegalAccessErrors by correctly defining the necessary module reads and opens.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/install.rst#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ java --add-opens=java.base/java.nio=org.apache.arrow.memory.core,ALL-UNNAMED -jar ...\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ env JDK_JAVA_OPTIONS=\"--add-reads=org.apache.arrow.flight.core=ALL-UNNAMED --add-opens=java.base/java.nio=org.apache.arrow.memory.core,ALL-UNNAMED\" java -jar ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Filesystem Tests\nDESCRIPTION: Configures core filesystem tests including local filesystem testing with example dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/filesystem/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_test(filesystem-test\n               SOURCES\n               filesystem_test.cc\n               localfs_test.cc\n               EXTRA_LABELS\n               filesystem\n               DEFINITIONS\n               ARROW_FILESYSTEM_EXAMPLE_LIBPATH=\"$<TARGET_FILE:arrow_filesystem_example>\"\n               EXTRA_DEPENDENCIES\n               arrow_filesystem_example)\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment\nDESCRIPTION: Standard Apache License 2.0 header comment used at the beginning of the file to specify licensing terms.\nSOURCE: https://github.com/apache/arrow/blob/main/README.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!---\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Verifying Git Remotes\nDESCRIPTION: Commands to verify the configured remotes and their URLs, showing both origin (personal fork) and upstream (official repository).\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/step_by_step/set_up.rst#2025-04-16_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ git remote -v\n```\n\nLANGUAGE: console\nCODE:\n```\norigin    https://github.com/<your username>/arrow.git (fetch)\norigin    https://github.com/<your username>/arrow.git (push)\nupstream  https://github.com/apache/arrow (fetch)\nupstream  https://github.com/apache/arrow (push)\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for Apache Arrow Documentation\nDESCRIPTION: This code snippet enumerates the Python packages required for building the Apache Arrow documentation. It includes Sphinx and various extensions for enhanced documentation features. The file should be kept in sync with 'conda_env_sphinx.txt'.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/requirements.txt#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n#\n# Note: keep this file in sync with conda_env_sphinx.txt !\n#\n\nbreathe\nipython\nlinuxdoc\nmyst-parser[linkify]\nnumpydoc\npydata-sphinx-theme~=0.14\nsphinx-autobuild\nsphinx-copybutton\nsphinx-design\nsphinx-lint\nsphinxcontrib-mermaid\nsphinx==6.2\npandas\n```\n\n----------------------------------------\n\nTITLE: Adding Skyhook CLS Test\nDESCRIPTION: Configures and adds the CLS test to the build with appropriate link libraries and prefixes.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/skyhook/CMakeLists.txt#2025-04-16_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_test(cls_test\n               SOURCES\n               cls/cls_skyhook_test.cc\n               EXTRA_LINK_LIBS\n               ${ARROW_SKYHOOK_TEST_LINK_LIBS}\n               PREFIX\n               \"skyhook\")\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow Flight Test Targets in CMake\nDESCRIPTION: Defines test targets for Arrow Flight, including flight_internals_test and flight_test, with their respective link libraries and labels.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_test(flight_internals_test\n               STATIC_LINK_LIBS\n               ${ARROW_FLIGHT_TEST_LINK_LIBS}\n               LABELS\n               \"arrow_flight\")\n\nadd_arrow_test(flight_test\n               STATIC_LINK_LIBS\n               ${ARROW_FLIGHT_TEST_LINK_LIBS}\n               LABELS\n               \"arrow_flight\")\n```\n\n----------------------------------------\n\nTITLE: Specifying Custom Build Directory for Benchmarks\nDESCRIPTION: Command to run benchmarks using a specific CMake build directory.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/benchmarks.rst#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\narchery benchmark run $HOME/arrow/cpp/release-build\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Vendored Libraries in CMake\nDESCRIPTION: These commands add subdirectories for various vendored libraries used in the Apache Arrow project. Each subdirectory likely contains its own CMakeLists.txt file for building the respective library.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(datetime)\nadd_subdirectory(double-conversion)\nadd_subdirectory(pcg)\nadd_subdirectory(portable-snippets)\nadd_subdirectory(xxhash)\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Caching with Sccache/Ccache\nDESCRIPTION: Sets up compiler caching using either sccache or ccache if enabled and available on the system.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARROW_USE_SCCACHE\n   AND NOT CMAKE_C_COMPILER_LAUNCHER\n   AND NOT CMAKE_CXX_COMPILER_LAUNCHER)\n\n  find_program(SCCACHE_FOUND sccache)\n\n  if(NOT SCCACHE_FOUND AND DEFINED ENV{SCCACHE_PATH})\n    set(SCCACHE_FOUND $ENV{SCCACHE_PATH})\n  endif()\n\n  if(SCCACHE_FOUND\n     AND (DEFINED ENV{SCCACHE_AZURE_BLOB_CONTAINER}\n          OR DEFINED ENV{SCCACHE_BUCKET}\n          OR DEFINED ENV{SCCACHE_DIR}\n          OR DEFINED ENV{SCCACHE_GCS_BUCKET}\n          OR DEFINED ENV{SCCACHE_MEMCACHED}\n          OR DEFINED ENV{SCCACHE_REDIS}\n         ))\n    message(STATUS \"Using sccache: ${SCCACHE_FOUND}\")\n    set(CMAKE_C_COMPILER_LAUNCHER ${SCCACHE_FOUND})\n    set(CMAKE_CXX_COMPILER_LAUNCHER ${SCCACHE_FOUND})\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Getting Current Session Options: GetSessionOptions\nDESCRIPTION: GetSessionOptions retrieves the current server session options, providing insights into both client-specified and server-default options for the session.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_17\n\nLANGUAGE: protobuf\nCODE:\n```\n\"GetSessionOptions\"\n```\n\n----------------------------------------\n\nTITLE: Adding Gandiva Internal Tests\nDESCRIPTION: Adds a comprehensive test suite for Gandiva internal components using the previously defined ADD_GANDIVA_TEST function. Includes tests for various functionalities like bitmap accumulation, caching, LLVM integration, and hash utilities.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/CMakeLists.txt#2025-04-16_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nadd_gandiva_test(internals-test\n                 SOURCES\n                 bitmap_accumulator_test.cc\n                 cache_test.cc\n                 engine_llvm_test.cc\n                 function_registry_test.cc\n                 function_signature_test.cc\n                 llvm_types_test.cc\n                 llvm_generator_test.cc\n                 annotator_test.cc\n                 tree_expr_test.cc\n                 encrypt_utils_test.cc\n                 expr_decomposer_test.cc\n                 exported_funcs_registry_test.cc\n                 expression_registry_test.cc\n                 selection_vector_test.cc\n                 lru_cache_test.cc\n                 to_date_holder_test.cc\n                 simple_arena_test.cc\n                 regex_functions_holder_test.cc\n                 decimal_type_util_test.cc\n                 random_generator_holder_test.cc\n                 hash_utils_test.cc\n                 gdv_function_stubs_test.cc\n                 interval_holder_test.cc\n                 tests/test_util.cc\n                 EXTRA_LINK_LIBS\n                 re2::re2\n                 ${GANDIVA_OPENSSL_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Defining CSV Test Sources in CMake\nDESCRIPTION: Sets up a list of source files for CSV-related tests in the Apache Arrow project. These files cover various aspects of CSV processing such as chunking, column building, decoding, conversion, parsing, reading, and writing.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/csv/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(CSV_TEST_SRCS\n    chunker_test.cc\n    column_builder_test.cc\n    column_decoder_test.cc\n    converter_test.cc\n    parser_test.cc\n    reader_test.cc\n    writer_test.cc)\n```\n\n----------------------------------------\n\nTITLE: Installing Archery Crossbow-Upload Subpackage\nDESCRIPTION: Command to install the crossbow-upload subpackage of Archery.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/archery/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \"arrow/dev/archery[crossbow-upload]\"\n```\n\n----------------------------------------\n\nTITLE: Data Frame Function Call Example in R\nDESCRIPTION: Example showing how to reference the write_dataset() function with different data types including Arrow Tables, RecordBatches, and R data frames.\nSOURCE: https://github.com/apache/arrow/blob/main/r/STYLE.md#2025-04-16_snippet_0\n\nLANGUAGE: r\nCODE:\n```\nwrite_dataset()\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow commandline utilities with CMake\nDESCRIPTION: This snippet shows how to enable building the Arrow commandline utilities by passing a boolean flag to CMake.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_BUILD_UTILITIES=ON\"\n```\n\n----------------------------------------\n\nTITLE: Checking GLIBCXX CXX11 ABI Configuration in CMake for GNU Compiler\nDESCRIPTION: Detects whether the GNU C++ compiler is using the new C++11 ABI for the STL. Creates a temporary compile test and sets appropriate flags if the new ABI is not being used.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n  set(GLIBCXX_USE_CXX11_ABI_SOURCE\n      ${CMAKE_CURRENT_BINARY_DIR}/try_compile_glibcxx_use_cxx_abi.cc)\n  file(WRITE ${GLIBCXX_USE_CXX11_ABI_SOURCE}\n       \"#include <string>\\n\"\n       \"#if !_GLIBCXX_USE_CXX11_ABI\\n\"\n       \"#error Not using CXX11 ABI\\n\"\n       \"#endif\\n\"\n       \"int main(void) {return 0;}\\n\")\n  try_compile(IS_GLIBCXX_USE_CXX11_ABI ${CMAKE_CURRENT_BINARY_DIR}/try_compile\n              SOURCES ${GLIBCXX_USE_CXX11_ABI_SOURCE})\n  if(NOT IS_GLIBCXX_USE_CXX11_ABI)\n    string(APPEND ARROW_PC_CFLAGS \" -D_GLIBCXX_USE_CXX11_ABI=0\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: System Setup for Ubuntu Verification Environment\nDESCRIPTION: Command to set up an Ubuntu system with the required packages for performing source verification of Apache Arrow releases.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release_verification.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# From the arrow clone\nsudo dev/release/setup-ubuntu.sh\n```\n\n----------------------------------------\n\nTITLE: Python Dependencies Requirements File\nDESCRIPTION: Lists required Python packages organized by component categories including CLI tools, bot/crossbow automation tools, benchmarking dependencies, Docker integration, and release management packages. Each section specifies the exact package names needed for that component.\nSOURCE: https://github.com/apache/arrow/blob/main/ci/conda_env_archery.txt#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n# cli\nclick\n\n# bot, crossbow\ngithub3.py\njinja2\njira\npygit2\npygithub\nruamel.yaml\nsetuptools_scm\ntoolz\n\n# benchmark\npandas\n\n# docker\npython-dotenv\n#ruamel.yaml\n\n# release\ngitpython\n#jinja2\n#jira\nsemver\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Interface Libraries for Arrow Flight in CMake\nDESCRIPTION: Sets up the static installation interface libraries for Arrow Flight, with special handling for Protobuf when it's sourced from the system.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_FLIGHT_STATIC_INSTALL_INTERFACE_LIBS Arrow::arrow_static)\nif(ARROW_PROTOBUF_ARROW_CMAKE_PACKAGE_NAME STREQUAL \"ArrowFlight\")\n  if(Protobuf_SOURCE STREQUAL \"SYSTEM\")\n    list(APPEND ARROW_FLIGHT_STATIC_INSTALL_INTERFACE_LIBS ${ARROW_PROTOBUF_LIBPROTOBUF})\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Red Arrow Flight SQL via RubyGems\nDESCRIPTION: Command to install the Red Arrow Flight SQL gem using the gem package manager after installing Apache Arrow Flight SQL GLib dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-flight-sql/README.md#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ gem install red-arrow-flight-sql\n```\n\n----------------------------------------\n\nTITLE: Setup and Build with Ninja and sccache\nDESCRIPTION: Ninja and sccache usage instructions for parallelized builds and caching compilation results. Configures the cmake command to use Ninja as a build tool and sets up caching with sccache.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ncd cpp\nmkdir build\ncd build\ncmake -G \"Ninja\" ^\n      -DARROW_BUILD_TESTS=ON ^\n      -DGTest_SOURCE=BUNDLED ..\n```\n\n----------------------------------------\n\nTITLE: Sequence Diagram Placeholders for Dissociated IPC Protocol\nDESCRIPTION: References sequence diagram templates illustrating two possible data transfer scenarios: separate connections and simultaneous connections for metadata and body data streams\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/DissociatedIPC.rst#2025-04-16_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nDissociatedIPC/SequenceDiagramSeparate.mmd\nDissociatedIPC/SequenceDiagramSame.mmd\n```\n\n----------------------------------------\n\nTITLE: Installing Portable Snippets Headers in Apache Arrow\nDESCRIPTION: CMake command to install all headers from the vendored portable-snippets directory into the Arrow installation. This is part of the build system configuration that ensures necessary header files are properly installed.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/portable-snippets/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/vendored/portable-snippets\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow BZ2 compression with CMake\nDESCRIPTION: This snippet shows how to enable support for BZ2 compression.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_37\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_WITH_BZ2=ON\"\n```\n\n----------------------------------------\n\nTITLE: Setting Python Configuration for PyArrow\nDESCRIPTION: Configures Python-related settings for the PyArrow build, including search strategy and deployment target.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(Python3_FIND_STRATEGY \"LOCATION\")\nset(Python3_FIND_REGISTRY \"LAST\")\nset(Python3_FIND_FRAMEWORK \"LAST\")\n\nset(CMAKE_MACOSX_RPATH 1)\nif(DEFINED ENV{MACOSX_DEPLOYMENT_TARGET})\n  set(CMAKE_OSX_DEPLOYMENT_TARGET $ENV{MACOSX_DEPLOYMENT_TARGET})\nelse()\n  set(CMAKE_OSX_DEPLOYMENT_TARGET 12.0)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Flight Integration Server Executable in CMake\nDESCRIPTION: Creates the flight-test-integration-server executable by compiling test_integration_server.cc and test_integration.cc source files, and links against the previously configured Flight libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/integration_tests/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(flight-test-integration-server test_integration_server.cc\n                                              test_integration.cc)\ntarget_link_libraries(flight-test-integration-server\n                      ${ARROW_FLIGHT_INTEGRATION_TEST_LINK_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Pinning LLVM Dependencies to Avoid Conda Packaging Bug\nDESCRIPTION: Specifies version constraints for clangdev and llvmdev packages to be below version 15. This configuration addresses ARROW-17830, which notes a bug in Conda's packaging of LLVM 15 that affects Apache Arrow builds on Appveyor.\nSOURCE: https://github.com/apache/arrow/blob/main/ci/conda_env_gandiva_win.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nclangdev<15\nllvmdev<15\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build with Custom Options\nDESCRIPTION: Demonstrates how to configure the Arrow C++ build with custom CMake options instead of using the predefined presets.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\n$ cmake -S arrow/cpp -B arrow/cpp/build \\\n         -DCMAKE_INSTALL_PREFIX=$ARROW_HOME \\\n\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow Compute Row Headers in CMake\nDESCRIPTION: This CMake command installs all headers related to Arrow's compute row functionality. It ensures that necessary header files are available for compilation and usage.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/row/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/compute/row\")\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Device Stream Exportable Protocol in Python\nDESCRIPTION: This Python code defines a Protocol for objects that can export an Arrow device stream via the `__arrow_c_device_stream__` method. It uses typing hints to specify the expected argument and return types of the method. The `requested_schema` argument is optional, and the method accepts arbitrary keyword arguments. It returns a single object.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple, Protocol\n\nclass ArrowDeviceStreamExportable(Protocol):\n    def __arrow_c_device_stream__(\n        self,\n        requested_schema: object | None = None,\n        **kwargs,\n    ) -> object:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Setting Telemetry Test Link Libraries\nDESCRIPTION: Defines link libraries for telemetry tests using OpenTelemetry dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/telemetry/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(ARROW_TELEMETRY_TEST_LINK_LIBS ${ARROW_OPENTELEMETRY_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Installing Double-Conversion Headers in Apache Arrow\nDESCRIPTION: Calls a custom CMake function to install all header files from the vendored double-conversion library into the Apache Arrow installation directory. This ensures that necessary third-party header files are available for Arrow components that depend on double-conversion functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/double-conversion/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/vendored/double-conversion\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Test Suite Function in CMake\nDESCRIPTION: CMake function for adding unit tests to the Arrow test suite. Handles prefix configuration, labels, and precompiled headers support.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ADD_ARROW_TEST REL_TEST_NAME)\n  set(options)\n  set(one_value_args PREFIX)\n  set(multi_value_args LABELS PRECOMPILED_HEADERS)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n\n  if(ARG_PREFIX)\n    set(PREFIX ${ARG_PREFIX})\n  else()\n    set(PREFIX \"arrow\")\n  endif()\n\n  if(ARG_LABELS)\n    set(LABELS ${ARG_LABELS})\n  else()\n    set(LABELS \"arrow-tests\")\n  endif()\n\n  if(ARG_PRECOMPILED_HEADERS)\n    set(PCH_ARGS PRECOMPILED_HEADERS ${ARG_PRECOMPILED_HEADERS})\n  else()\n    set(PCH_ARGS PRECOMPILED_HEADER_LIB \"arrow-array-test\")\n  endif()\n\n  add_test_case(${REL_TEST_NAME}\n                PREFIX\n                ${PREFIX}\n                LABELS\n                ${LABELS}\n                ${PCH_ARGS}\n                ${ARG_UNPARSED_ARGUMENTS})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Updating Double-Conversion Library Using the Update Script\nDESCRIPTION: Shows how to use the update.sh script to update the vendored double-conversion library to a specified version. The script takes a version number as a parameter.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/double-conversion/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./update.sh VERSION\n```\n\n----------------------------------------\n\nTITLE: Final Implementation of the tutorial_min_max Function\nDESCRIPTION: Complete implementation of the tutorial_min_max function that extends the min_max results by decreasing the minimum by 1 and increasing the maximum by 1.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef tutorial_min_max(values, skip_nulls=True):\n\n   \"\"\"\n   Compute the minimum-1 and maximum+1 values of a numeric array.\n\n   This is a made-up feature for the tutorial purposes.\n\n   Parameters\n   ----------\n   values : Array\n   skip_nulls : bool, default True\n       If True, ignore nulls in the input.\n\n   Returns\n   -------\n   result : StructScalar of min-1 and max+1\n\n   Examples\n   --------\n   >>> import pyarrow.compute as pc\n   >>> data = [4, 5, 6, None, 1]\n   >>> pc.tutorial_min_max(data)\n   <pyarrow.StructScalar: [('min-', 0), ('max+', 7)]>\n   \"\"\"\n\n   options = ScalarAggregateOptions(skip_nulls=skip_nulls)\n   min_max = call_function(\"min_max\", [values], options)\n\n   if min_max[0].as_py() is not None:\n     min_t = min_max[0].as_py()-1\n     max_t = min_max[1].as_py()+1\n   else:\n     min_t = min_max[0].as_py()\n     max_t = min_max[1].as_py()\n\n   ty = pa.struct([\n     pa.field('min-', pa.int64()),\n     pa.field('max+', pa.int64()),\n   ])\n   return pa.scalar([('min-', min_t), ('max+', max_t)], type=ty)\n```\n\n----------------------------------------\n\nTITLE: Fetching SQL Information with CommandGetSqlInfo\nDESCRIPTION: CommandGetSqlInfo retrieves metadata regarding the SQL database server, including supported features. It helps clients understand the capabilities of the database server.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_6\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandGetSqlInfo\"\n```\n\n----------------------------------------\n\nTITLE: Running yarn scripts for Apache Arrow project\nDESCRIPTION: This snippet demonstrates various yarn commands for cleaning, building, and testing the Apache Arrow project. It explains how to target specific ES versions and module formats.\nSOURCE: https://github.com/apache/arrow/blob/main/js/DEVELOP.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* `yarn clean` - cleans targets\n* `yarn build` - cleans and compiles all targets\n* `yarn test` - executes tests against built targets\n\nThese scripts accept argument lists of targets × modules:\n\n* Available `targets` are `es5`, `es2015`, `esnext`, `ts`, and `all` (default: `all`)\n* Available `modules` are `cjs`, `esm`, `umd`, and `all` (default: `all`)\n\nExamples:\n\n* `yarn build` -- builds all ES targets in all module formats\n* `yarn build -t es5 -m all` -- builds the ES5 target in all module formats\n* `yarn build -t all -m cjs` -- builds all ES targets in the CommonJS module format\n* `yarn build -t es5 -t es2015 -m all` -- builds the ES5 and ES2015 targets in all module formats\n* `yarn build -t es5 -m cjs -m esm` -- builds the ES5 target in CommonJS and ESModules module formats\n```\n\n----------------------------------------\n\nTITLE: Use vcpkg with CMake for Arrow\nDESCRIPTION: Configures the CMake command to use vcpkg for managing dependencies for Arrow development on Windows. Specifies vcpkg as the source for dependencies, which impacts the build system’s handling of the build process.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n-DARROW_DEPENDENCY_SOURCE=VCPKG\n```\n\n----------------------------------------\n\nTITLE: Defining Apache Arrow GLib Documentation Stub in HTML\nDESCRIPTION: This HTML snippet contains a license header for the Apache Software Foundation and a brief markdown-style content indicating that this is a stub page for Apache Arrow GLib documentation.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/c_glib/arrow-glib/index.md#2025-04-16_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\n(arrow-glib)=\n# Apache Arrow GLib\n\nStub page for the Apache Arrow GLib docs; actual source is located in c_glib/ sub-directory.\n```\n\n----------------------------------------\n\nTITLE: Installing Archery in Editable Mode for Apache Arrow Development\nDESCRIPTION: Command to install Archery in editable mode with all dependencies. This allows the installation to automatically update when pulling the Arrow repository.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/archery.rst#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ pip install -e \"dev/archery[all]\"\n```\n\n----------------------------------------\n\nTITLE: Consuming an ArrowSchema PyCapsule in C\nDESCRIPTION: This C code shows how to consume a PyCapsule containing an `ArrowSchema`. It uses `PyCapsule_GetPointer` to retrieve the pointer to the `ArrowSchema` struct from the capsule.  If the capsule does not contain an `ArrowSchema`, it will return NULL and set an exception.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDataInterface/PyCapsuleInterface.rst#2025-04-16_snippet_13\n\nLANGUAGE: c\nCODE:\n```\n#include <Python.h>\n\n// If the capsule is not an ArrowSchema, will return NULL and set an exception.\nstruct ArrowSchema* GetArrowSchemaPyCapsule(PyObject* capsule) {\n  return PyCapsule_GetPointer(capsule, \"arrow_schema\");\n}\n```\n\n----------------------------------------\n\nTITLE: System Setup for macOS ARM Verification Environment\nDESCRIPTION: Commands to set up a macOS ARM system with the required dependencies for performing verification of Apache Arrow releases, including installing necessary packages via Homebrew.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release_verification.rst#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# From the arrow clone\nbrew install gpg\nbrew bundle --file=cpp/Brewfile\nbrew bundle --file=c_glib/Brewfile\nbrew uninstall node\n# You might need to add node, ruby java and maven to the PATH, follow\n# instructions from brew after installing.\nbrew install node@20\nbrew install ruby\nbrew install openjdk\nbrew install maven\n```\n\n----------------------------------------\n\nTITLE: Testing Invalid Compression Argument for Feather Format in R\nDESCRIPTION: This snippet tests the error handling when an invalid 'compression' argument is used with the Feather format in write_dataset. It expects an error message suggesting the use of 'codec' instead.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dataset-write.md#2025-04-16_snippet_0\n\nLANGUAGE: R\nCODE:\n```\nwrite_dataset(df, dst_dir, format = \"feather\", compression = \"snappy\")\n```\n\n----------------------------------------\n\nTITLE: Defining ADD_PARQUET_TEST Function\nDESCRIPTION: CMake function for adding Parquet test cases. Handles both static and shared library linking scenarios based on ARROW_TEST_LINKAGE setting.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ADD_PARQUET_TEST REL_TEST_NAME)\n  set(one_value_args)\n  set(multi_value_args EXTRA_DEPENDENCIES LABELS)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n\n  set(TEST_ARGUMENTS PREFIX \"parquet\" LABELS \"parquet-tests\")\n\n  if(ARROW_TEST_LINKAGE STREQUAL \"static\")\n    add_test_case(${REL_TEST_NAME}\n                  STATIC_LINK_LIBS\n                  parquet_static\n                  ${PARQUET_TEST_LINK_LIBS}\n                  ${TEST_ARGUMENTS}\n                  ${ARG_UNPARSED_ARGUMENTS})\n  else()\n    add_test_case(${REL_TEST_NAME}\n                  STATIC_LINK_LIBS\n                  parquet_shared\n                  ${PARQUET_TEST_LINK_LIBS}\n                  ${TEST_ARGUMENTS}\n                  ${ARG_UNPARSED_ARGUMENTS})\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Installing Red Arrow Dataset with RubyGems\nDESCRIPTION: Command for installing the Red Arrow Dataset gem after Apache Arrow Dataset GLib has been installed on the system.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-dataset/README.md#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ gem install red-arrow-dataset\n```\n\n----------------------------------------\n\nTITLE: Creating Arrow Flight Integration Tests Target in CMake\nDESCRIPTION: Creates a custom target for Arrow Flight integration tests that will be used as a collection point for all test-related targets.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/integration_tests/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(arrow_flight_integration_tests)\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for Apache Arrow\nDESCRIPTION: This snippet defines an array named DEPENDENCIES that lists various libraries required for the Apache Arrow project along with their download URLs and version information. Each entry in the array corresponds to a specific library that must be built or linked during the project compilation.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/thirdparty/versions.txt#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nDEPENDENCIES=(\n  \"ARROW_ABSL_URL absl-${ARROW_ABSL_BUILD_VERSION}.tar.gz https://github.com/abseil/abseil-cpp/archive/${ARROW_ABSL_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_C_AUTH_URL aws-c-auth-${ARROW_AWS_C_AUTH_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-c-auth/archive/${ARROW_AWS_C_AUTH_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_C_CAL_URL aws-c-cal-${ARROW_AWS_C_CAL_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-c-cal/archive/${ARROW_AWS_C_CAL_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_C_COMMON_URL aws-c-common-${ARROW_AWS_C_COMMON_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-c-common/archive/${ARROW_AWS_C_COMMON_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_C_COMPRESSION_URL aws-c-compression-${ARROW_AWS_C_COMPRESSION_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-c-compression/archive/${ARROW_AWS_C_COMPRESSION_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_C_EVENT_STREAM_URL aws-c-event-stream-${ARROW_AWS_C_EVENT_STREAM_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-c-event-stream/archive/${ARROW_AWS_C_EVENT_STREAM_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_C_HTTP_URL aws-c-http-${ARROW_AWS_C_HTTP_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-c-http/archive/${ARROW_AWS_C_HTTP_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_C_IO_URL aws-c-io-${ARROW_AWS_C_IO_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-c-io/archive/${ARROW_AWS_C_IO_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_C_MQTT_URL aws-c-mqtt-${ARROW_AWS_C_MQTT_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-c-mqtt/archive/${ARROW_AWS_C_MQTT_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_C_S3_URL aws-c-s3-${ARROW_AWS_C_S3_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-c-s3/archive/${ARROW_AWS_C_S3_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_C_SDKUTILS_URL aws-c-sdkutils-${ARROW_AWS_C_SDKUTILS_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-c-sdkutils/archive/${ARROW_AWS_C_SDKUTILS_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_CHECKSUMS_URL aws-checksums-${ARROW_AWS_CHECKSUMS_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-checksums/archive/${ARROW_AWS_CHECKSUMS_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_CRT_CPP_URL aws-crt-cpp-${ARROW_AWS_CRT_CPP_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-crt-cpp/archive/${ARROW_AWS_CRT_CPP_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWS_LC_URL aws-lc-${ARROW_AWS_LC_BUILD_VERSION}.tar.gz https://github.com/awslabs/aws-lc/archive/${ARROW_AWS_LC_BUILD_VERSION}.tar.gz\"\n  \"ARROW_AWSSDK_URL aws-sdk-cpp-${ARROW_AWSSDK_BUILD_VERSION}.tar.gz https://github.com/aws/aws-sdk-cpp/archive/${ARROW_AWSSDK_BUILD_VERSION}.tar.gz\"\n  \"ARROW_BOOST_URL boost-${ARROW_BOOST_BUILD_VERSION}.tar.gz https://apache.jfrog.io/artifactory/arrow/thirdparty/7.0.0/boost_${ARROW_BOOST_BUILD_VERSION//./_}.tar.gz\"\n  \"ARROW_BROTLI_URL brotli-${ARROW_BROTLI_BUILD_VERSION}.tar.gz https://github.com/google/brotli/archive/${ARROW_BROTLI_BUILD_VERSION}.tar.gz\"\n  \"ARROW_BZIP2_URL bzip2-${ARROW_BZIP2_BUILD_VERSION}.tar.gz https://sourceware.org/pub/bzip2/bzip2-${ARROW_BZIP2_BUILD_VERSION}.tar.gz\"\n  \"ARROW_CARES_URL cares-${ARROW_CARES_BUILD_VERSION}.tar.gz https://github.com/c-ares/c-ares/releases/download/cares-${ARROW_CARES_BUILD_VERSION//./_}/c-ares-${ARROW_CARES_BUILD_VERSION}.tar.gz\"\n  \"ARROW_CRC32C_URL crc32c-${ARROW_CRC32C_BUILD_VERSION}.tar.gz https://github.com/google/crc32c/archive/refs/tags/${ARROW_CRC32C_BUILD_VERSION}.tar.gz\"\n  \"ARROW_GBENCHMARK_URL gbenchmark-${ARROW_GBENCHMARK_BUILD_VERSION}.tar.gz https://github.com/google/benchmark/archive/${ARROW_GBENCHMARK_BUILD_VERSION}.tar.gz\"\n  \"ARROW_GFLAGS_URL gflags-${ARROW_GFLAGS_BUILD_VERSION}.tar.gz https://github.com/gflags/gflags/archive/${ARROW_GFLAGS_BUILD_VERSION}.tar.gz\"\n  \"ARROW_GLOG_URL glog-${ARROW_GLOG_BUILD_VERSION}.tar.gz https://github.com/google/glog/archive/${ARROW_GLOG_BUILD_VERSION}.tar.gz\"\n  \"ARROW_GOOGLE_CLOUD_CPP_URL google-cloud-cpp-${ARROW_GOOGLE_CLOUD_CPP_BUILD_VERSION}.tar.gz https://github.com/googleapis/google-cloud-cpp/archive/${ARROW_GOOGLE_CLOUD_CPP_BUILD_VERSION}.tar.gz\"\n  \"ARROW_GRPC_URL grpc-${ARROW_GRPC_BUILD_VERSION}.tar.gz https://github.com/grpc/grpc/archive/${ARROW_GRPC_BUILD_VERSION}.tar.gz\"\n  \"ARROW_GTEST_URL gtest-${ARROW_GTEST_BUILD_VERSION}.tar.gz https://github.com/google/googletest/releases/download/v${ARROW_GTEST_BUILD_VERSION}/googletest-${ARROW_GTEST_BUILD_VERSION}.tar.gz\"\n  \"ARROW_JEMALLOC_URL jemalloc-${ARROW_JEMALLOC_BUILD_VERSION}.tar.bz2 https://github.com/jemalloc/jemalloc/releases/download/${ARROW_JEMALLOC_BUILD_VERSION}/jemalloc-${ARROW_JEMALLOC_BUILD_VERSION}.tar.bz2\"\n  \"ARROW_LZ4_URL lz4-${ARROW_LZ4_BUILD_VERSION}.tar.gz https://github.com/lz4/lz4/archive/${ARROW_LZ4_BUILD_VERSION}.tar.gz\"\n  \"ARROW_MIMALLOC_URL mimalloc-${ARROW_MIMALLOC_BUILD_VERSION}.tar.gz https://github.com/microsoft/mimalloc/archive/${ARROW_MIMALLOC_BUILD_VERSION}.tar.gz\"\n  \"ARROW_NLOHMANN_JSON_URL nlohmann-json-${ARROW_NLOHMANN_JSON_BUILD_VERSION}.tar.gz https://github.com/nlohmann/json/archive/refs/tags/${ARROW_NLOHMANN_JSON_BUILD_VERSION}.tar.gz\"\n  \"ARROW_OPENTELEMETRY_URL opentelemetry-cpp-${ARROW_OPENTELEMETRY_BUILD_VERSION}.tar.gz https://github.com/open-telemetry/opentelemetry-cpp/archive/refs/tags/${ARROW_OPENTELEMETRY_BUILD_VERSION}.tar.gz\"\n  \"ARROW_OPENTELEMETRY_PROTO_URL opentelemetry-proto-${ARROW_OPENTELEMETRY_PROTO_BUILD_VERSION}.tar.gz https://github.com/open-telemetry/opentelemetry-proto/archive/refs/tags/${ARROW_OPENTELEMETRY_PROTO_BUILD_VERSION}.tar.gz\"\n  \"ARROW_ORC_URL orc-${ARROW_ORC_BUILD_VERSION}.tar.gz https://www.apache.org/dyn/closer.lua/orc/orc-${ARROW_ORC_BUILD_VERSION}/orc-${ARROW_ORC_BUILD_VERSION}.tar.gz?action=download\"\n  \"ARROW_PROTOBUF_URL protobuf-${ARROW_PROTOBUF_BUILD_VERSION}.tar.gz https://github.com/google/protobuf/releases/download/${ARROW_PROTOBUF_BUILD_VERSION}/protobuf-all-${ARROW_PROTOBUF_BUILD_VERSION:1}.tar.gz\"\n  \"ARROW_RAPIDJSON_URL rapidjson-${ARROW_RAPIDJSON_BUILD_VERSION}.tar.gz https://github.com/miloyip/rapidjson/archive/${ARROW_RAPIDJSON_BUILD_VERSION}.tar.gz\"\n  \"ARROW_RE2_URL re2-${ARROW_RE2_BUILD_VERSION}.tar.gz https://github.com/google/re2/archive/${ARROW_RE2_BUILD_VERSION}.tar.gz\"\n  \"ARROW_S2N_TLS_URL s2n-${ARROW_S2N_TLS_BUILD_VERSION}.tar.gz https://github.com/aws/s2n-tls/archive/${ARROW_S2N_TLS_BUILD_VERSION}.tar.gz\"\n  \"ARROW_SNAPPY_URL snappy-${ARROW_SNAPPY_BUILD_VERSION}.tar.gz https://github.com/google/snappy/archive/${ARROW_SNAPPY_BUILD_VERSION}.tar.gz\"\n  \"ARROW_THRIFT_URL thrift-${ARROW_THRIFT_BUILD_VERSION}.tar.gz https://www.apache.org/dyn/closer.lua/thrift/${ARROW_THRIFT_BUILD_VERSION}/thrift-${ARROW_THRIFT_BUILD_VERSION}.tar.gz?action=download\"\n  \"ARROW_UTF8PROC_URL utf8proc-${ARROW_UTF8PROC_BUILD_VERSION}.tar.gz https://github.com/JuliaStrings/utf8proc/archive/${ARROW_UTF8PROC_BUILD_VERSION}.tar.gz\"\n  \"ARROW_XSIMD_URL xsimd-${ARROW_XSIMD_BUILD_VERSION}.tar.gz https://github.com/xtensor-stack/xsimd/archive/${ARROW_XSIMD_BUILD_VERSION}.tar.gz\"\n  \"ARROW_ZLIB_URL zlib-${ARROW_ZLIB_BUILD_VERSION}.tar.gz https://zlib.net/fossils/zlib-${ARROW_ZLIB_BUILD_VERSION}.tar.gz\"\n  \"ARROW_ZSTD_URL zstd-${ARROW_ZSTD_BUILD_VERSION}.tar.gz https://github.com/facebook/zstd/releases/download/v${ARROW_ZSTD_BUILD_VERSION}/zstd-${ARROW_ZSTD_BUILD_VERSION}.tar.gz\"\n)\n```\n\n----------------------------------------\n\nTITLE: Adding External Functions Subdirectory in CMake\nDESCRIPTION: Includes the 'external_functions' subdirectory in the CMake build process. This allows for the integration of additional external functions into the Gandiva library build.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/tests/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(external_functions)\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Canonical Extension Tests in CMake\nDESCRIPTION: Adds a test target using the add_arrow_test CMake function with the specified sources and a prefix to identify these tests as belonging to canonical extensions.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/extension/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_test(test\n               SOURCES\n               ${CANONICAL_EXTENSION_TESTS}\n               PREFIX\n               \"arrow-canonical-extensions\")\n```\n\n----------------------------------------\n\nTITLE: Importing Arrow Array from C Interface (Python)\nDESCRIPTION: Python script that imports an Arrow array from C-style memory addresses using the Arrow C Data Interface.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/doc/matlab_interface_for_apache_arrow_design.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\narray = pa.Array._import_from_c(arrayMemoryAddress, schemaMemoryAddress)\n```\n\n----------------------------------------\n\nTITLE: Updating Fast Float Vendored Library in Apache Arrow with Bash\nDESCRIPTION: This bash command shows how to update the vendored fast_float library to a new version. The VERSION parameter must be replaced with the desired version number (e.g., 3.10.1).\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/fast_float/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncpp/src/arrow/vendoered/fast_float/update.sh VERSION\ngit commit add cpp/src/arrow/vendoered/fast_float/\n```\n\n----------------------------------------\n\nTITLE: Linux Distribution List\nDESCRIPTION: List of Linux distributions related to Apache Arrow project compatibility or testing.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tools/nixlibs-allowlist.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nubuntu\ncentos\nredhat\nrhel\nrocky\nopensuse\n```\n\n----------------------------------------\n\nTITLE: Specifying Documentation Dependencies for Apache Arrow\nDESCRIPTION: This code snippet lists the Python packages required to build the documentation for Apache Arrow. It includes Sphinx and its extensions, tools for API documentation, and other formatting utilities. Some packages are commented out or have specific version requirements.\nSOURCE: https://github.com/apache/arrow/blob/main/ci/conda_env_sphinx.txt#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Requirements for building the documentation\nbreathe\ndoxygen\nipython\nlinkify-it-py\n# We can't install linuxdoc by conda. We install linuxdoc by pip in\n# ci/dockerfiles/conda-python-pandas.dockerfile.\n# linuxdoc\nmyst-parser\nnumpydoc\npydata-sphinx-theme=0.14\nsphinx-autobuild\nsphinx-design\nsphinx-copybutton\nsphinx-lint\nsphinxcontrib-jquery\nsphinxcontrib-mermaid\nsphinx==6.2\npytest-cython\npandas\n```\n\n----------------------------------------\n\nTITLE: Installing Parquet API Headers with CMake in Apache Arrow\nDESCRIPTION: Uses the arrow_install_all_headers CMake function to install all headers in the 'parquet/api' directory, making them part of the public API for the Parquet module in Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/api/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"parquet/api\")\n```\n\n----------------------------------------\n\nTITLE: C Extension Integration Function References in PyArrow\nDESCRIPTION: Reference documentation for PyArrow's C extension integration functions including get_include, get_libraries, and get_library_dirs for building C extensions against PyArrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/misc.rst#2025-04-16_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   get_include\n   get_libraries\n   get_library_dirs\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Fuzzing Target Function in CMake\nDESCRIPTION: CMake function for adding fuzzing targets with configurable prefix and library linking options.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ADD_ARROW_FUZZ_TARGET REL_FUZZING_NAME)\n  set(options)\n  set(one_value_args PREFIX)\n  set(multi_value_args)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n\n  if(ARG_PREFIX)\n    set(PREFIX ${ARG_PREFIX})\n  else()\n    set(PREFIX \"arrow\")\n  endif()\n\n  if(ARROW_BUILD_STATIC)\n    set(LINK_LIBS arrow_static)\n  else()\n    set(LINK_LIBS arrow_shared)\n  endif()\n  add_fuzz_target(${REL_FUZZING_NAME}\n                  PREFIX\n                  ${PREFIX}\n                  LINK_LIBS\n                  ${LINK_LIBS}\n                  ${ARG_UNPARSED_ARGUMENTS})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Installing Archery Benchmark Subpackage\nDESCRIPTION: Command to install the benchmark subpackage of Archery, which is used for running Arrow benchmarks.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/archery/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \"arrow/dev/archery[benchmark]\"\n```\n\n----------------------------------------\n\nTITLE: Attempting to Use across() with Arguments in Arrow Dataset\nDESCRIPTION: Example of an unsupported operation in Arrow where dplyr's across() function is used with multiple arguments. The code attempts to round two double columns with a digits parameter, which triggers an error because Arrow doesn't support the ... argument to across().\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-across.md#2025-04-16_snippet_0\n\nLANGUAGE: r\nCODE:\n```\nInMemoryDataset$create(example_data) %>% mutate(across(c(dbl, dbl2), round, digits = -1))\n```\n\n----------------------------------------\n\nTITLE: GMock UBSAN Suppression Rule\nDESCRIPTION: Suppression rule to handle null pointer dereference in GMock's spec builder implementation that causes runtime errors during testing.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/build-support/sanitizer-disallowed-entries.txt#2025-04-16_snippet_0\n\nLANGUAGE: config\nCODE:\n```\nfun:*testing*internal*InvokeWith*\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Substrait Test Build in CMake\nDESCRIPTION: Sets up the test configuration for the Arrow Substrait library, including test source files, linking libraries, and test labels. It handles different linkage options (static or shared) for the tests.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/engine/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_SUBSTRAIT_TEST_LINK_LIBS ${ARROW_SUBSTRAIT_LINK_lIBS} ${ARROW_TEST_LINK_LIBS})\nif(ARROW_TEST_LINKAGE STREQUAL \"static\")\n  list(APPEND ARROW_SUBSTRAIT_TEST_LINK_LIBS arrow_substrait_static\n       arrow_substrait_static)\nelse()\n  list(APPEND ARROW_SUBSTRAIT_TEST_LINK_LIBS arrow_substrait_shared)\nendif()\n\nadd_arrow_test(substrait_test\n               SOURCES\n               substrait/ext_test.cc\n               substrait/function_test.cc\n               substrait/serde_test.cc\n               substrait/protobuf_test_util.cc\n               substrait/test_util.cc\n               EXTRA_LINK_LIBS\n               ${ARROW_SUBSTRAIT_TEST_LINK_LIBS}\n               PREFIX\n               \"arrow-substrait\"\n               LABELS\n               \"arrow_substrait\")\n\nadd_subdirectory(substrait)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Policies for PyArrow\nDESCRIPTION: Sets various CMake policies to ensure consistent behavior across different CMake versions and platforms.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_policy(SET CMP0042 NEW)\ncmake_policy(SET CMP0054 NEW)\nif(POLICY CMP0068)\n  cmake_policy(SET CMP0068 NEW)\nendif()\nif(POLICY CMP0074)\n  cmake_policy(SET CMP0074 NEW)\nendif()\nif(POLICY CMP0095)\n  cmake_policy(SET CMP0095 NEW)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Querying Crossbow Build Status in Console\nDESCRIPTION: Command to check the status of a Crossbow build using its build ID or branch name.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/crossbow.rst#2025-04-16_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n$ archery crossbow status <build id / branch name>\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow Vendored Headers in CMake\nDESCRIPTION: This command installs all headers for the 'arrow/vendored' directory. It's part of the Apache Arrow project's build system to ensure all necessary headers are available.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/vendored\")\n```\n\n----------------------------------------\n\nTITLE: Exploring StructScalar Creation in Python Console\nDESCRIPTION: Code demonstrating how to create a custom StructScalar object which will be used to return the modified min-max values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow as pa\n>>> ty = pa.struct([\n...      pa.field('min-', pa.int64()),\n...      pa.field('max+', pa.int64()),\n...    ])\n>>> pa.scalar([('min-', 3), ('max+', 9)], type=ty)\n<pyarrow.StructScalar: [('min-', 3), ('max+', 9)]>\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Flight SQL Library Source Files in CMake\nDESCRIPTION: Lists the source files required to build the Arrow Flight SQL library. These files implement the core functionality for Flight SQL client and server components.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/sql/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_FLIGHT_SQL_SRCS\n    server.cc\n    sql_info_internal.cc\n    column_metadata.cc\n    client.cc\n    protocol_internal.cc\n    server_session_middleware.cc)\n```\n\n----------------------------------------\n\nTITLE: Checking URLs in the R Package\nDESCRIPTION: R command to check all URLs in the package for validity, ensuring there are no broken links before submission to CRAN.\nSOURCE: https://github.com/apache/arrow/blob/main/r/PACKAGING.md#2025-04-16_snippet_2\n\nLANGUAGE: r\nCODE:\n```\nurlchecker::url_check()\n```\n\n----------------------------------------\n\nTITLE: Testing Invalid Argument for IPC Format in R\nDESCRIPTION: This snippet verifies the error handling when an unsupported argument is provided for the IPC format in write_dataset. It expects an error message listing the valid arguments.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dataset-write.md#2025-04-16_snippet_3\n\nLANGUAGE: R\nCODE:\n```\nwrite_dataset(df, dst_dir, format = \"ipc\", nonsensical_arg = \"blah-blah\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Clang Format Targets in CMake\nDESCRIPTION: Creates format and check-format targets using clang-format. The format target updates files in place while check-format verifies formatting without making changes.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(${CLANG_FORMAT_FOUND})\n  add_custom_target(format\n                    ${PYTHON_EXECUTABLE}\n                    ${BUILD_SUPPORT_DIR}/run_clang_format.py\n                    --clang_format_binary\n                    ${CLANG_FORMAT_BIN}\n                    ${COMMON_LINT_OPTIONS}\n                    --fix\n                    ${ARROW_LINT_QUIET})\n\n  add_custom_target(check-format\n                    ${PYTHON_EXECUTABLE}\n                    ${BUILD_SUPPORT_DIR}/run_clang_format.py\n                    --clang_format_binary\n                    ${CLANG_FORMAT_BIN}\n                    ${COMMON_LINT_OPTIONS}\n                    ${ARROW_LINT_QUIET})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Testing n_distinct() with Single Argument in Arrow\nDESCRIPTION: This snippet tests the n_distinct() function with a single argument in an Arrow InMemoryDataset. It demonstrates that n_distinct() with 0 arguments is not supported in Arrow and suggests using collect() to pull data into R.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-summarize.md#2025-04-16_snippet_0\n\nLANGUAGE: R\nCODE:\n```\nInMemoryDataset$create(tbl) %>% summarize(distinct = n_distinct())\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow JSON Headers in CMake\nDESCRIPTION: Installs all headers for Arrow JSON functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/json/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/json\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Compiler Flags for gRPC Compatibility in Arrow Flight\nDESCRIPTION: Adjusts compiler flags to handle warnings and errors when working with gRPC, and adds conditional definition flags based on the detected gRPC version to ensure compatibility.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n# <KLUDGE> -Werror / /WX cause try_compile to fail because there seems to be no\n# way to pass -isystem $GRPC_INCLUDE_DIR instead of -I$GRPC_INCLUDE_DIR\nset(CMAKE_CXX_FLAGS_BACKUP \"${CMAKE_CXX_FLAGS}\")\nstring(REPLACE \"/WX\" \"\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\nstring(REPLACE \"-Werror \" \" \" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n\nif(ARROW_GRPC_VERSION VERSION_GREATER_EQUAL \"1.43\")\n  add_definitions(-DGRPC_USE_TLS_CHANNEL_CREDENTIALS_OPTIONS\n                  -DGRPC_NAMESPACE_FOR_TLS_CREDENTIALS_OPTIONS=grpc::experimental\n                  -DGRPC_USE_CERTIFICATE_VERIFIER)\nelseif(ARROW_GRPC_VERSION VERSION_GREATER_EQUAL \"1.36\")\n  add_definitions(-DGRPC_USE_TLS_CHANNEL_CREDENTIALS_OPTIONS\n                  -DGRPC_NAMESPACE_FOR_TLS_CREDENTIALS_OPTIONS=grpc::experimental)\nelseif(ARROW_GRPC_VERSION VERSION_GREATER_EQUAL \"1.34\")\n  add_definitions(-DGRPC_USE_TLS_CHANNEL_CREDENTIALS_OPTIONS\n                  -DGRPC_USE_TLS_CHANNEL_CREDENTIALS_OPTIONS_ROOT_CERTS\n                  -DGRPC_NAMESPACE_FOR_TLS_CREDENTIALS_OPTIONS=grpc::experimental)\nelseif(ARROW_GRPC_VERSION VERSION_GREATER_EQUAL \"1.32\")\n  add_definitions(-DGRPC_NAMESPACE_FOR_TLS_CREDENTIALS_OPTIONS=grpc::experimental)\nelse()\n  add_definitions(-DGRPC_NAMESPACE_FOR_TLS_CREDENTIALS_OPTIONS=grpc_impl::experimental)\nendif()\n\n# Was in a different namespace, or simply not supported, prior to this\nif(ARROW_GRPC_VERSION VERSION_GREATER_EQUAL \"1.40\")\n  add_definitions(-DGRPC_ENABLE_ASYNC)\nendif()\n\n# </KLUDGE> Restore the CXXFLAGS that were modified above\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS_BACKUP}\")\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Structure Definition\nDESCRIPTION: RST-formatted table of contents configuration for Apache Arrow API documentation. Defines the documentation hierarchy and organization using Sphinx toctree directive with a maximum depth of 3 levels.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 3\n\n   api/support\n   api/memory\n   api/thread\n   api/datatype\n   api/array\n   api/scalar\n   api/builder\n   api/table\n   api/c_abi\n   api/compute\n   api/acero\n   api/gandiva\n   api/tensor\n   api/utilities\n   api/async\n   api/io\n   api/ipc\n   api/formats\n   api/cuda\n   api/flight\n   api/flightsql\n   api/filesystem\n   api/dataset\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Encryption Examples in CMake\nDESCRIPTION: Conditionally adds encryption example executables and their include directories if PARQUET_REQUIRE_ENCRYPTION is enabled. It also updates the warning suppression file list.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/parquet/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(PARQUET_REQUIRE_ENCRYPTION)\n  add_executable(parquet-encryption-example low_level_api/encryption_reader_writer.cc)\n  add_executable(parquet-encryption-example-all-crypto-options\n                 low_level_api/encryption_reader_writer_all_crypto_options.cc)\n  target_include_directories(parquet-encryption-example PRIVATE low_level_api/)\n  target_include_directories(parquet-encryption-example-all-crypto-options\n                             PRIVATE low_level_api/)\n\n  set(PARQUET_EXAMPLES_WARNING_SUPPRESSIONS\n      ${PARQUET_EXAMPLES_WARNING_SUPPRESSIONS} low_level_api/encryption_reader_writer.cc\n      low_level_api/encryption_reader_writer_all_crypto_options.cc)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Sample Output from JPype Integration\nDESCRIPTION: This code block shows the output of running the Python script that uses JPype to access Java Arrow objects. It displays both the Java proxy object and the converted PyArrow array with their types and values.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/python_java.rst#2025-04-16_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nARRAY <java class 'org.apache.arrow.vector.BigIntVector'> [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nARRAY <class 'pyarrow.lib.Int64Array'> [\n    1,\n    2,\n    3,\n    4,\n    5,\n    6,\n    7,\n    8,\n    9,\n    10\n]\n```\n\n----------------------------------------\n\nTITLE: Force Bundled Build Configuration\nDESCRIPTION: Environment variable to force using bundled C++ build on macOS.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nFORCE_BUNDLED_BUILD=true\n```\n\n----------------------------------------\n\nTITLE: Installing Archery Linting Tools for Python Development\nDESCRIPTION: Installs the Archery linting tools required for checking code style issues in PyArrow development. Archery is Arrow's development tool for managing code quality.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install -e \"arrow/dev/archery[lint]\"\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow C++ Build Type Option in Meson\nDESCRIPTION: Defines a string option for specifying the CMake build type for Arrow C++. Defaults to 'release' configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/c_glib/meson_options.txt#2025-04-16_snippet_1\n\nLANGUAGE: meson\nCODE:\n```\noption('arrow_cpp_build_type',\n       type: 'string',\n       value: 'release',\n       description: '-DCMAKE_BUILD_TYPE option value for Arrow C++')\n```\n\n----------------------------------------\n\nTITLE: Configuring Gandiva Library Dependencies\nDESCRIPTION: Sets up shared and static library dependencies for Gandiva, including LLVM, OpenSSL, and various feature-dependent libraries like RE2, UTF8PROC, and XSIMD based on build configuration options.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(GANDIVA_OPENSSL_LIBS ${ARROW_OPENSSL_LIBS})\nif(WIN32 AND NOT CMAKE_VERSION VERSION_LESS 3.18)\n  list(APPEND GANDIVA_OPENSSL_LIBS OpenSSL::applink)\nendif()\n\nset(GANDIVA_SHARED_LINK_LIBS arrow_shared LLVM::LLVM_HEADERS)\nset(GANDIVA_SHARED_PRIVATE_LINK_LIBS LLVM::LLVM_LIBS ${GANDIVA_OPENSSL_LIBS}\n                                     Boost::headers)\nset(GANDIVA_STATIC_LINK_LIBS\n    arrow_static\n    LLVM::LLVM_HEADERS\n    LLVM::LLVM_LIBS\n    ${GANDIVA_OPENSSL_LIBS}\n    Boost::headers)\nif(ARROW_USE_XSIMD)\n  list(APPEND GANDIVA_SHARED_PRIVATE_LINK_LIBS ${ARROW_XSIMD})\n  list(APPEND GANDIVA_STATIC_LINK_LIBS ${ARROW_XSIMD})\nendif()\nif(ARROW_WITH_RE2)\n  list(APPEND GANDIVA_SHARED_PRIVATE_LINK_LIBS re2::re2)\n  list(APPEND GANDIVA_STATIC_LINK_LIBS re2::re2)\nendif()\nif(ARROW_WITH_UTF8PROC)\n  list(APPEND GANDIVA_SHARED_PRIVATE_LINK_LIBS utf8proc::utf8proc)\n  list(APPEND GANDIVA_STATIC_LINK_LIBS utf8proc::utf8proc)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Telemetry Headers in CMake\nDESCRIPTION: Installs all header files from the arrow/telemetry directory using Arrow's custom installation function.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/telemetry/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\narrow_install_all_headers(\"arrow/telemetry\")\n```\n\n----------------------------------------\n\nTITLE: Configuring GDB Support for Arrow Shared Library\nDESCRIPTION: Sets up GDB debugger support for Arrow shared library by configuring and installing a Python helper script. Handles platform-specific paths including special case for Windows/MSYS2 environments.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/CMakeLists.txt#2025-04-16_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\n# The following block includes a code for MSYS2 but GDB on MSYS2 doesn't\n# auto load our plugin. GDB's auto load may not be supported on MSYS2...\n# So it's disabled for now.\nif(ARROW_BUILD_SHARED AND NOT WIN32)\n  configure_file(libarrow_gdb.py.in \"${CMAKE_CURRENT_BINARY_DIR}/libarrow_gdb.py\" @ONLY)\n  if(NOT ARROW_GDB_INSTALL_DIR)\n    if(WIN32)\n      set(ARROW_GDB_INSTALL_DIR ${CMAKE_INSTALL_FULL_BINDIR})\n    else()\n      set(ARROW_GDB_INSTALL_DIR ${CMAKE_INSTALL_FULL_LIBDIR})\n    endif()\n  endif()\n  set(ARROW_GDB_AUTO_LOAD_LIBARROW_GDB_INSTALL FALSE)\n  if(WIN32)\n    find_program(cygpath \"cygpath\")\n    if(cygpath)\n      execute_process(COMMAND cygpath \"${ARROW_GDB_INSTALL_DIR}\"\n                      OUTPUT_VARIABLE MSYS2_ARROW_GDB_INSTALL_DIR\n                      OUTPUT_STRIP_TRAILING_WHITESPACE)\n      set(ARROW_GDB_AUTO_LOAD_LIBARROW_GDB_DIR\n          \"${ARROW_GDB_AUTO_LOAD_DIR}${MSYS2_ARROW_GDB_INSTALL_DIR}\")\n      set(ARROW_GDB_AUTO_LOAD_LIBARROW_GDB_INSTALL TRUE)\n    endif()\n  else()\n    set(ARROW_GDB_AUTO_LOAD_LIBARROW_GDB_DIR\n        \"${ARROW_GDB_AUTO_LOAD_DIR}/${ARROW_GDB_INSTALL_DIR}\")\n    set(ARROW_GDB_AUTO_LOAD_LIBARROW_GDB_INSTALL TRUE)\n  endif()\n  if(ARROW_GDB_AUTO_LOAD_LIBARROW_GDB_INSTALL)\n    install(FILES \"${CMAKE_CURRENT_BINARY_DIR}/libarrow_gdb.py\"\n            DESTINATION \"${ARROW_GDB_AUTO_LOAD_LIBARROW_GDB_DIR}\"\n            RENAME \"$<TARGET_FILE_NAME:arrow_shared>-gdb.py\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building All JNI Libraries with CMake (Windows)\nDESCRIPTION: CMake commands to build all JNI libraries except the C Data Interface library on Windows platforms. This involves building the C++ libraries first and then the JNI bindings.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n$ cd arrow\n$ mkdir java-dist, cpp-jni\n$ cmake ^\n    -S cpp ^\n    -B cpp-jni ^\n    -DARROW_BUILD_SHARED=OFF ^\n    -DARROW_CSV=ON ^\n    -DARROW_DATASET=ON ^\n    -DARROW_DEPENDENCY_USE_SHARED=OFF ^\n    -DARROW_FILESYSTEM=ON ^\n    -DARROW_GANDIVA=OFF ^\n    -DARROW_JSON=ON ^\n    -DARROW_ORC=ON ^\n    -DARROW_PARQUET=ON ^\n    -DARROW_S3=ON ^\n    -DARROW_SUBSTRAIT=ON ^\n    -DARROW_USE_CCACHE=ON ^\n    -DARROW_WITH_BROTLI=ON ^\n    -DARROW_WITH_LZ4=ON ^\n    -DARROW_WITH_SNAPPY=ON ^\n    -DARROW_WITH_ZLIB=ON ^\n    -DARROW_WITH_ZSTD=ON ^\n    -DCMAKE_BUILD_TYPE=Release ^\n    -DCMAKE_INSTALL_PREFIX=java-dist ^\n    -DCMAKE_UNITY_BUILD=ON ^\n    -GNinja\n$ cd cpp-jni\n$ ninja install\n$ cd ../\n$ cmake ^\n    -S java ^\n    -B java-jni ^\n    -DARROW_JAVA_JNI_ENABLE_C=OFF ^\n    -DARROW_JAVA_JNI_ENABLE_DATASET=ON ^\n    -DARROW_JAVA_JNI_ENABLE_DEFAULT=ON ^\n    -DARROW_JAVA_JNI_ENABLE_GANDIVA=OFF ^\n    -DARROW_JAVA_JNI_ENABLE_ORC=ON ^\n    -DBUILD_TESTING=OFF ^\n    -DCMAKE_BUILD_TYPE=Release ^\n    -DCMAKE_INSTALL_PREFIX=java-dist ^\n    -DCMAKE_PREFIX_PATH=$PWD/java-dist\n$ cmake --build java-jni --target install --config Release\n$ dir \"java-dist/bin\"\n|__ arrow_orc_jni/\n|__ arrow_dataset_jni/\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Header Template\nDESCRIPTION: Standard license header to be included at the top of source files in the Apache Arrow project. It references the Apache License 2.0 and includes important disclaimers about copyright and terms of use.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/build-support/ubsan-suppressions.txt#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n```\n\n----------------------------------------\n\nTITLE: Dataset Partitioning Documentation Directive\nDESCRIPTION: Sphinx/Doxygen directive for documenting dataset partitioning functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/dataset.rst#2025-04-16_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. doxygengroup:: dataset-partitioning\n   :content-only:\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet pkg-config Settings\nDESCRIPTION: Handles pkg-config flags configuration for static linking case when shared libraries are not being built but static libraries are.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT ARROW_BUILD_SHARED AND ARROW_BUILD_STATIC)\n  string(APPEND PARQUET_PC_CFLAGS \"${PARQUET_PC_CFLAGS_PRIVATE}\")\n  set(PARQUET_PC_CFLAGS_PRIVATE \"\")\n  string(APPEND PARQUET_PC_REQUIRES \"${PARQUET_PC_REQUIRES_PRIVATE}\")\n  set(PARQUET_PC_REQUIRES_PRIVATE \"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Python Style Linting with Archery\nDESCRIPTION: Uses the Archery tool to check Python code style issues in the PyArrow codebase, following PEP8-like guidelines similar to the pandas project.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ archery lint --python\n```\n\n----------------------------------------\n\nTITLE: Installing PCG Headers for Apache Arrow\nDESCRIPTION: This command installs all PCG (Permuted Congruential Generator) headers to the arrow/vendored/pcg directory. PCG is a random number generation library that Apache Arrow has vendored.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/pcg/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/vendored/pcg\")\n```\n\n----------------------------------------\n\nTITLE: Building Debug Version with Tests\nDESCRIPTION: Commands to create and build a debug version of Arrow C++ with unit tests, requiring at least 4GB RAM.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n$ git submodule update --init --recursive\n$ export ARROW_TEST_DATA=$PWD/../testing/data\n$ mkdir build-debug\n$ cd build-debug\n$ cmake -DCMAKE_BUILD_TYPE=Debug -DARROW_BUILD_TESTS=ON ..\n$ make -j8       # if you have 8 CPU cores, otherwise adjust\n$ make unittest  # to run the tests\n$ make install\n```\n\n----------------------------------------\n\nTITLE: Setting Arrow Default Memory Pool in R\nDESCRIPTION: Demonstrates how to set the ARROW_DEFAULT_MEMORY_POOL environment variable before loading the Arrow package to change memory allocators. This can be useful for debugging purposes.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_18\n\nLANGUAGE: R\nCODE:\n```\nSys.setenv(ARROW_DEFAULT_MEMORY_POOL = \"system\")\n```\n\n----------------------------------------\n\nTITLE: Installing Nightly Development Version of r-arrow with Conda\nDESCRIPTION: Shows the conda command to install the nightly development version of the r-arrow package from the arrow-nightlies channel.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c arrow-nightlies -c conda-forge --strict-channel-priority r-arrow\n```\n\n----------------------------------------\n\nTITLE: Displaying haven_labelled Column with glimpse() in R Arrow\nDESCRIPTION: Demonstrates glimpse() output for a specific haven_labelled column extracted from a Table. The function shows the extension type and values in the column.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-glimpse.md#2025-04-16_snippet_5\n\nLANGUAGE: r\nCODE:\n```\nglimpse(haven[[3]])\n```\n\n----------------------------------------\n\nTITLE: Defining Apache Parquet GLib Documentation Reference in Markdown\nDESCRIPTION: This snippet defines a reference label for the Apache Parquet GLib documentation section and provides a header for the content. It also includes a note about the actual source location.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/c_glib/parquet-glib/index.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n(parquet-glib)=\n# Apache Parquet GLib\n\nStub page for the Apache Parquet GLib docs; actual source is located in c_glib/ sub-directory.\n```\n\n----------------------------------------\n\nTITLE: Patching Flatbuffers Headers for Private Namespace in Apache Arrow\nDESCRIPTION: This diff shows changes made to multiple Flatbuffers header files. The primary modification is moving the Flatbuffers code into a private namespace (arrow_vendored_private::flatbuffers) while maintaining access through the original 'flatbuffers' namespace alias. This change helps prevent conflicts with other versions of Flatbuffers that might be used in a project.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/thirdparty/flatbuffers/README.md#2025-04-16_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\ndiff --git a/cpp/thirdparty/flatbuffers/include/flatbuffers/allocator.h b/cpp/thirdparty/flatbuffers/include/flatbuffers/allocator.h\nindex 30427190b..f0e91d0a3 100644\n--- a/cpp/thirdparty/flatbuffers/include/flatbuffers/allocator.h\n+++ b/cpp/thirdparty/flatbuffers/include/flatbuffers/allocator.h\n@@ -17,9 +17,15 @@\n #ifndef FLATBUFFERS_ALLOCATOR_H_\n #define FLATBUFFERS_ALLOCATOR_H_\n \n+// Move this vendored copy of flatbuffers to a private namespace,\n+// but continue to access it through the \"flatbuffers\" alias.\n+namespace arrow_vendored_private::flatbuffers {\n+}\n+namespace flatbuffers = arrow_vendored_private::flatbuffers;\n+\n #include \"flatbuffers/base.h\"\n \n-namespace flatbuffers {\n+namespace arrow_vendored_private::flatbuffers {\n \n // Allocator interface. This is flatbuffers-specific and meant only for\n // `vector_downward` usage.\n@@ -63,6 +69,6 @@ class Allocator {\n   }\n };\n \n-}  // namespace flatbuffers\n+}  // namespace arrow_vendored_private\n \n #endif  // FLATBUFFERS_ALLOCATOR_H_\ndiff --git a/cpp/thirdparty/flatbuffers/include/flatbuffers/array.h b/cpp/thirdparty/flatbuffers/include/flatbuffers/array.h\nindex f4bfbf054..f8e78243a 100644\n--- a/cpp/thirdparty/flatbuffers/include/flatbuffers/array.h\n+++ b/cpp/thirdparty/flatbuffers/include/flatbuffers/array.h\n@@ -17,6 +17,12 @@\n #ifndef FLATBUFFERS_ARRAY_H_\n #define FLATBUFFERS_ARRAY_H_\n \n+// Move this vendored copy of flatbuffers to a private namespace,\n+// but continue to access it through the \"flatbuffers\" alias.\n+namespace arrow_vendored_private::flatbuffers {\n+}\n+namespace flatbuffers = arrow_vendored_private::flatbuffers;\n+\n #include <cstdint>\n #include <memory>\n \n@@ -24,7 +30,7 @@\n #include \"flatbuffers/stl_emulation.h\"\n #include \"flatbuffers/vector.h\"\n \n-namespace flatbuffers {\n+namespace arrow_vendored_private::flatbuffers {\n \n // This is used as a helper type for accessing arrays.\n template<typename T, uint16_t length> class Array {\n@@ -251,6 +257,6 @@ bool operator==(const Array<T, length> &lhs,\n           std::memcmp(lhs.Data(), rhs.Data(), rhs.size() * sizeof(T)) == 0);\n }\n \n-}  // namespace flatbuffers\n+}  // namespace arrow_vendored_private\n \n #endif  // FLATBUFFERS_ARRAY_H_\ndiff --git a/cpp/thirdparty/flatbuffers/include/flatbuffers/base.h b/cpp/thirdparty/flatbuffers/include/flatbuffers/base.h\nindex 5c4cae791..1b0c7d987 100644\n--- a/cpp/thirdparty/flatbuffers/include/flatbuffers/base.h\n+++ b/cpp/thirdparty/flatbuffers/include/flatbuffers/base.h\n@@ -1,6 +1,12 @@\n #ifndef FLATBUFFERS_BASE_H_\n #define FLATBUFFERS_BASE_H_\n \n+// Move this vendored copy of flatbuffers to a private namespace,\n+// but continue to access it through the \"flatbuffers\" alias.\n+namespace arrow_vendored_private::flatbuffers {\n+}\n+namespace flatbuffers = arrow_vendored_private::flatbuffers;\n+\n // clang-format off\n \n // If activate should be declared and included first.\n@@ -144,10 +150,13 @@\n #define FLATBUFFERS_VERSION_REVISION 26\n #define FLATBUFFERS_STRING_EXPAND(X) #X\n #define FLATBUFFERS_STRING(X) FLATBUFFERS_STRING_EXPAND(X)\n+\n+namespace arrow_vendored_private {\n namespace flatbuffers {\n   // Returns version as string  \"MAJOR.MINOR.REVISION\".\n   const char* FLATBUFFERS_VERSION();\n }\n+}\n \n #if (!defined(_MSC_VER) || _MSC_VER > 1600) && \\\n     (!defined(__GNUC__) || (__GNUC__ * 100 + __GNUC_MINOR__ >= 407)) || \\\n@@ -222,14 +231,14 @@ namespace flatbuffers {\n     // Check for std::string_view (in c++17)\n     #if __has_include(<string_view>) && (__cplusplus >= 201606 || (defined(_HAS_CXX17) && _HAS_CXX17))\n       #include <string_view>\n-      namespace flatbuffers {\n+      namespace arrow_vendored_private::flatbuffers {\n         typedef std::string_view string_view;\n       }\n       #define FLATBUFFERS_HAS_STRING_VIEW 1\n     // Check for std::experimental::string_view (in c++14, compiler-dependent)\n     #elif __has_include(<experimental/string_view>) && (__cplusplus >= 201411)\n       #include <experimental/string_view>\n-      namespace flatbuffers {\n+      namespace arrow_vendored_private::flatbuffers {\n         typedef std::experimental::string_view string_view;\n       }\n       #define FLATBUFFERS_HAS_STRING_VIEW 1\n@@ -240,7 +249,7 @@ namespace flatbuffers {\n       #include \"absl/base/config.h\"\n       #if !defined(ABSL_USES_STD_STRING_VIEW)\n         #include \"absl/strings/string_view.h\"\n-        namespace flatbuffers {\n+        namespace arrow_vendored_private::flatbuffers {\n           typedef absl::string_view string_view;\n         }\n         #define FLATBUFFERS_HAS_STRING_VIEW 1\n@@ -317,6 +326,7 @@ template<typename T> FLATBUFFERS_CONSTEXPR inline bool IsConstTrue(T t) {\n /// @endcond\n \n /// @file\n+namespace arrow_vendored_private {\n namespace flatbuffers {\n \n /// @cond FLATBUFFERS_INTERNAL\n@@ -492,4 +502,6 @@ inline bool IsInRange(const T &v, const T &low, const T &high) {\n }\n \n }  // namespace flatbuffers\n+}  // namespace arrow_vendored_private\n+\n #endif  // FLATBUFFERS_BASE_H_\ndiff --git a/cpp/thirdparty/flatbuffers/include/flatbuffers/buffer.h b/cpp/thirdparty/flatbuffers/include/flatbuffers/buffer.h\nindex 94d4f7903..e791c3007 100644\n--- a/cpp/thirdparty/flatbuffers/include/flatbuffers/buffer.h\n+++ b/cpp/thirdparty/flatbuffers/include/flatbuffers/buffer.h\n@@ -17,11 +17,17 @@\n #ifndef FLATBUFFERS_BUFFER_H_\n #define FLATBUFFERS_BUFFER_H_\n \n+// Move this vendored copy of flatbuffers to a private namespace,\n+// but continue to access it through the \"flatbuffers\" alias.\n+namespace arrow_vendored_private::flatbuffers {\n+}\n+namespace flatbuffers = arrow_vendored_private::flatbuffers;\n+\n #include <algorithm>\n \n #include \"flatbuffers/base.h\"\n \n-namespace flatbuffers {\n+namespace arrow_vendored_private::flatbuffers {\n \n // Wrapper for uoffset_t to allow safe template specialization.\n // Value is allowed to be 0 to indicate a null object (see e.g. AddOffset).\n@@ -194,6 +200,6 @@ const T *GetSizePrefixedRoot(const void *buf) {\n   return GetRoot<T>(reinterpret_cast<const uint8_t *>(buf) + sizeof(SizeT));\n }\n \n-}  // namespace flatbuffers\n+}  // namespace arrow_vendored_private\n \n #endif  // FLATBUFFERS_BUFFER_H_\ndiff --git a/cpp/thirdparty/flatbuffers/include/flatbuffers/buffer_ref.h b/cpp/thirdparty/flatbuffers/include/flatbuffers/buffer_ref.h\nindex f70941fc6..8a4a43f81 100644\n--- a/cpp/thirdparty/flatbuffers/include/flatbuffers/buffer_ref.h\n+++ b/cpp/thirdparty/flatbuffers/include/flatbuffers/buffer_ref.h\n@@ -17,10 +17,16 @@\n #ifndef FLATBUFFERS_BUFFER_REF_H_\n #define FLATBUFFERS_BUFFER_REF_H_\n \n+// Move this vendored copy of flatbuffers to a private namespace,\n+// but continue to access it through the \"flatbuffers\" alias.\n+namespace arrow_vendored_private::flatbuffers {\n+}\n+namespace flatbuffers = arrow_vendored_private::flatbuffers;\n+\n #include \"flatbuffers/base.h\"\n #include \"flatbuffers/verifier.h\"\n \n-namespace flatbuffers {\n+namespace arrow_vendored_private::flatbuffers {\n \n // Convenient way to bundle a buffer and its length, to pass it around\n // typed by its root.\n@@ -48,6 +54,6 @@ template<typename T> struct BufferRef : BufferRefBase {\n   bool must_free;\n };\n \n-}  // namespace flatbuffers\n+}  // namespace arrow_vendored_private\n \n #endif  // FLATBUFFERS_BUFFER_REF_H_\ndiff --git a/cpp/thirdparty/flatbuffers/include/flatbuffers/default_allocator.h b/cpp/thirdparty/flatbuffers/include/flatbuffers/default_allocator.h\nindex d4724122c..fd2dc60a9 100644\n--- a/cpp/thirdparty/flatbuffers/include/flatbuffers/default_allocator.h\n+++ b/cpp/thirdparty/flatbuffers/include/flatbuffers/default_allocator.h\n@@ -17,10 +17,16 @@\n #ifndef FLATBUFFERS_DEFAULT_ALLOCATOR_H_\n #define FLATBUFFERS_DEFAULT_ALLOCATOR_H_\n \n+// Move this vendored copy of flatbuffers to a private namespace,\n+// but continue to access it through the \"flatbuffers\" alias.\n+namespace arrow_vendored_private::flatbuffers {\n+}\n+namespace flatbuffers = arrow_vendored_private::flatbuffers;\n+\n #include \"flatbuffers/allocator.h\"\n #include \"flatbuffers/base.h\"\n \n-namespace flatbuffers {\n+namespace arrow_vendored_private::flatbuffers {\n \n // DefaultAllocator uses new/delete to allocate memory regions\n class DefaultAllocator : public Allocator {\n@@ -59,6 +65,6 @@ inline uint8_t *ReallocateDownward(Allocator *allocator, uint8_t *old_p,\n                          old_p, old_size, new_size, in_use_back, in_use_front);\n }\n \n-}  // namespace flatbuffers\n+}  // namespace arrow_vendored_private\n \n #endif  // FLATBUFFERS_DEFAULT_ALLOCATOR_H_\ndiff --git a/cpp/thirdparty/flatbuffers/include/flatbuffers/detached_buffer.h b/cpp/thirdparty/flatbuffers/include/flatbuffers/detached_buffer.h\nindex 5e900baeb..4779545b3 100644\n--- a/cpp/thirdparty/flatbuffers/include/flatbuffers/detached_buffer.h\n+++ b/cpp/thirdparty/flatbuffers/include/flatbuffers/detached_buffer.h\n@@ -17,11 +17,17 @@\n #ifndef FLATBUFFERS_DETACHED_BUFFER_H_\n #define FLATBUFFERS_DETACHED_BUFFER_H_\n \n+// Move this vendored copy of flatbuffers to a private namespace,\n+// but continue to access it through the \"flatbuffers\" alias.\n+namespace arrow_vendored_private::flatbuffers {\n+}\n+namespace flatbuffers = arrow_vendored_private::flatbuffers;\n+\n #include \"flatbuffers/allocator.h\"\n\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow Filesystem Headers and Package Config\nDESCRIPTION: Sets up installation of filesystem headers and pkg-config support for Arrow filesystem component.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/filesystem/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\narrow_install_all_headers(\"arrow/filesystem\")\n\narrow_add_pkg_config(\"arrow-filesystem\")\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow Dataset Benchmarks\nDESCRIPTION: Configures benchmark targets for the Arrow Dataset module if ARROW_BUILD_BENCHMARKS is enabled, setting up appropriate link dependencies based on build configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/dataset/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_BENCHMARKS)\n  add_arrow_benchmark(file_benchmark PREFIX \"arrow-dataset\")\n  add_arrow_benchmark(scanner_benchmark PREFIX \"arrow-dataset\")\n\n  if(ARROW_BUILD_STATIC)\n    target_link_libraries(arrow-dataset-file-benchmark PUBLIC arrow_dataset_static)\n    target_link_libraries(arrow-dataset-scanner-benchmark PUBLIC arrow_dataset_static)\n  else()\n    target_link_libraries(arrow-dataset-file-benchmark PUBLIC arrow_dataset_shared)\n    target_link_libraries(arrow-dataset-scanner-benchmark PUBLIC arrow_dataset_shared)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Autosummary for FileSystem Interface in Python\nDESCRIPTION: This snippet generates an autosummary for the FileSystem interface classes. It includes FileInfo, FileSelector, and FileSystem classes.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/filesystems.rst#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   FileInfo\n   FileSelector\n   FileSystem\n```\n\n----------------------------------------\n\nTITLE: Configuring Gandiva Test Link Libraries\nDESCRIPTION: Sets up link libraries for Gandiva tests, configuring both static and shared linking options. Includes test-specific dependencies like Google Test and optional components based on the build configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/CMakeLists.txt#2025-04-16_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nset(GANDIVA_STATIC_TEST_LINK_LIBS gandiva_static arrow_testing_static)\nset(GANDIVA_SHARED_TEST_LINK_LIBS gandiva_shared arrow_testing_shared LLVM::LLVM_LIBS)\nif(ARROW_WITH_UTF8PROC)\n  list(APPEND GANDIVA_SHARED_TEST_LINK_LIBS utf8proc::utf8proc)\n  list(APPEND GANDIVA_STATIC_TEST_LINK_LIBS utf8proc::utf8proc)\nendif()\nif(WIN32)\n  list(APPEND GANDIVA_STATIC_TEST_LINK_LIBS ${GANDIVA_OPENSSL_LIBS})\n  list(APPEND GANDIVA_SHARED_TEST_LINK_LIBS ${GANDIVA_OPENSSL_LIBS})\nendif()\nlist(APPEND GANDIVA_STATIC_TEST_LINK_LIBS ${ARROW_GTEST_GMOCK} ${ARROW_GTEST_GTEST_MAIN})\nlist(APPEND GANDIVA_SHARED_TEST_LINK_LIBS ${ARROW_GTEST_GMOCK} ${ARROW_GTEST_GTEST_MAIN})\n```\n\n----------------------------------------\n\nTITLE: Configuring Library Dependencies\nDESCRIPTION: Sets up linking configuration for shared and static library dependencies including OpenSSL, Thrift, and OpenTelemetry.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/CMakeLists.txt#2025-04-16_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nset(PARQUET_SHARED_LINK_LIBS)\nset(PARQUET_SHARED_PRIVATE_LINK_LIBS)\n\nif(ARROW_USE_XSIMD)\n  list(APPEND PARQUET_SHARED_LINK_LIBS ${ARROW_XSIMD})\n  list(APPEND PARQUET_SHARED_PRIVATE_LINK_LIBS ${ARROW_XSIMD})\n  list(APPEND PARQUET_STATIC_LINK_LIBS ${ARROW_XSIMD})\nendif()\n\nif(NOT PARQUET_MINIMAL_DEPENDENCY)\n  list(APPEND PARQUET_SHARED_LINK_LIBS arrow_shared)\n  list(APPEND PARQUET_SHARED_PRIVATE_LINK_LIBS thrift::thrift)\n  list(APPEND PARQUET_STATIC_LINK_LIBS thrift::thrift)\n  if(NOT THRIFT_VENDORED)\n    list(APPEND PARQUET_STATIC_INSTALL_INTERFACE_LIBS thrift::thrift)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating Arrow Bindings at Runtime in Ruby\nDESCRIPTION: Example of using Apache Arrow from Ruby by leveraging Arrow GLib and the gobject-introspection gem to generate bindings at runtime. The code demonstrates how to access arrow::BooleanArray in Arrow C++ through the Ruby binding.\nSOURCE: https://github.com/apache/arrow/blob/main/c_glib/README.md#2025-04-16_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\n# Generate bindings at runtime\nrequire \"gi\"\nArrow = GI.load(\"Arrow\")\n\n# Now, you can access arrow::BooleanArray in Arrow C++ by\n# Arrow::BooleanArray\np Arrow::BooleanArray\n```\n\n----------------------------------------\n\nTITLE: Setting IO Thread Count\nDESCRIPTION: Example showing thread count setting with warning for values less than 2.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_4\n\nLANGUAGE: R\nCODE:\n```\nset_io_thread_count(num_threads)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure with Apache License\nDESCRIPTION: ReStructuredText markup file containing Apache License 2.0 header and a placeholder note for JavaScript documentation location.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/js/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n.. or more contributor license agreements.  See the NOTICE file\n.. distributed with this work for additional information\n.. regarding copyright ownership.  The ASF licenses this file\n.. to you under the Apache License, Version 2.0 (the\n.. \"License\"); you may not use this file except in compliance\n.. with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n.. software distributed under the License is distributed on an\n.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n.. KIND, either express or implied.  See the License for the\n.. specific language governing permissions and limitations\n.. under the License.\n\n.. _js:\n\nJavaScript docs\n===============\n\nStub page for the JavaScript docs; actual source is located in js/ sub-directory.\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment in Markdown\nDESCRIPTION: Standard Apache 2.0 license header included as an HTML comment in the markdown file, specifying the terms under which the code is licensed.\nSOURCE: https://github.com/apache/arrow/blob/main/csharp/examples/FlightClientExample/readme.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!---\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Conditional Setting of MATLAB Search Path Flag in CMake\nDESCRIPTION: Sets the MATLAB_ADD_INSTALL_DIR_TO_SEARCH_PATH option to OFF if the startup file integration is enabled but the search path option is not explicitly defined. This prevents redundant path additions.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(MATLAB_ADD_INSTALL_DIR_TO_STARTUP_FILE AND NOT DEFINED\n                                              MATLAB_ADD_INSTALL_DIR_TO_SEARCH_PATH)\n  set(MATLAB_ADD_INSTALL_DIR_TO_SEARCH_PATH OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Parquet Arrow Example Executable with Dependency Linking\nDESCRIPTION: Finds the required Parquet package, creates an executable target from reader_writer.cc, and links it against either the shared or static Parquet library based on the PARQUET_LINK_SHARED option.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/parquet/parquet_arrow/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# Look for installed packages the system\nfind_package(Parquet REQUIRED)\n\nadd_executable(parquet-arrow-example reader_writer.cc)\nif(PARQUET_LINK_SHARED)\n  target_link_libraries(parquet-arrow-example Parquet::parquet_shared)\nelse()\n  target_link_libraries(parquet-arrow-example Parquet::parquet_static)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Skyhook Dependencies and Link Libraries\nDESCRIPTION: Finds the required librados package and sets up link libraries for static and shared builds.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/skyhook/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(librados REQUIRED)\nset(ARROW_SKYHOOK_LINK_STATIC arrow_dataset_static librados::rados)\nset(ARROW_SKYHOOK_LINK_SHARED arrow_dataset_shared librados::rados)\n```\n\n----------------------------------------\n\nTITLE: Initializing Visual Studio Environment Shell 2019\nDESCRIPTION: The script configures a shell environment suitable for compiling with Visual Studio 2019. It should be executed at the start of each shell session to apply the appropriate environment variables for building and running applications.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\VsDevCmd.bat\" -arch=amd64\n```\n\n----------------------------------------\n\nTITLE: Linking to Apache Arrow Release Documentation in Markdown\nDESCRIPTION: This snippet provides a link to the Apache Arrow project documentation specifically for release management. It uses Markdown syntax to create a hyperlink.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/release/README.md#2025-04-16_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\nhttps://arrow.apache.org/docs/developers/release.html\n```\n\n----------------------------------------\n\nTITLE: Defining autosummary for Arrow stream classes in Python\nDESCRIPTION: This snippet defines an autosummary for Arrow stream classes, including various file and buffer types for input and output operations.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/files.rst#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   NativeFile\n   OSFile\n   PythonFile\n   BufferReader\n   BufferOutputStream\n   FixedSizeBufferWriter\n   MemoryMappedFile\n   CompressedInputStream\n   CompressedOutputStream\n```\n\n----------------------------------------\n\nTITLE: Generating Bitcode Files for Test Sources in CMake\nDESCRIPTION: Defines test source files and generates corresponding bitcode files for each source. It iterates through the source files and adds them to the bitcode generation process.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/tests/external_functions/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_PRECOMPILED_SOURCES multiply_by_two.cc)\nset(TEST_PRECOMPILED_BC_FILES)\nforeach(SOURCE ${TEST_PRECOMPILED_SOURCES})\n  gandiva_add_bitcode(${SOURCE})\n  get_filename_component(SOURCE_BASE ${SOURCE} NAME_WE)\n  list(APPEND TEST_PRECOMPILED_BC_FILES ${CMAKE_CURRENT_BINARY_DIR}/${SOURCE_BASE}.bc)\nendforeach()\nadd_custom_target(extension-tests ALL DEPENDS extension-tests-data\n                                              ${TEST_PRECOMPILED_BC_FILES})\n```\n\n----------------------------------------\n\nTITLE: Converting Arrow BooleanArray to MATLAB Logical Array\nDESCRIPTION: Example showing how to convert an Arrow BooleanArray back to a MATLAB logical array using the toMATLAB method.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_5\n\nLANGUAGE: matlab\nCODE:\n```\n>> arrowArray = arrow.array([true, false, true])\n\narrowArray = \n\n[\n  true,\n  false,\n  true\n]\n\n>> matlabArray = toMATLAB(arrowArray)\n\nmatlabArray =\n\n  3×1 logical array\n\n   1\n   0\n   1\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Targets and Headers for Arrow Flight in CMake\nDESCRIPTION: Creates a custom target for arrow_flight and configures header installation. Also sets up pkg-config configuration flags for static linking case.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(arrow_flight)\n\narrow_install_all_headers(\"arrow/flight\")\n\n# If libarrow_flight.a is only built, \"pkg-config --cflags --libs\n# arrow-flight\" outputs build flags for static linking not shared\n# linking. ARROW_FLIGHT_PC_* except ARROW_FLIGHT_PC_*_PRIVATE are for\n# the static linking case.\nif(NOT ARROW_BUILD_SHARED AND ARROW_BUILD_STATIC)\n  string(APPEND ARROW_FLIGHT_PC_CFLAGS \"${ARROW_FLIGHT_PC_CFLAGS_PRIVATE}\")\n  set(ARROW_FLIGHT_PC_CFLAGS_PRIVATE \"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for Apache Arrow\nDESCRIPTION: A list of Python package dependencies with minimum version requirements and conditional Python version constraints for the Apache Arrow project.\nSOURCE: https://github.com/apache/arrow/blob/main/python/requirements-build.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncython>=3\noldest-supported-numpy>=0.14; python_version<'3.9'\nnumpy>=1.25; python_version>='3.9'\nsetuptools_scm>=8\nsetuptools>=64\n```\n\n----------------------------------------\n\nTITLE: Windows IO Test Source Configuration\nDESCRIPTION: Configures source files for IO utility tests on Windows, including manifest file for long path support on Windows 10+.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/util/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n  if(MSVC)\n    set(IO_UTIL_TEST_SOURCES io_util_test.cc io_util_test.manifest)\n  else()\n    set(IO_UTIL_TEST_SOURCES io_util_test.cc io_util_test.rc)\n  endif()\nelse()\n  set(IO_UTIL_TEST_SOURCES io_util_test.cc)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Update Homebrew Packages\nDESCRIPTION: Script for updating Apache Arrow packages in the Homebrew package manager, involving forking and creating a pull request\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\ncd \"$(brew --repository homebrew/core)\"\ngit remote add <YOUR_GITHUB_ID> git@github.com:<YOUR_GITHUB_ID>/homebrew-core.git\ncd -\ndev/release/post-14-homebrew.sh X.Y.Z <YOUR_GITHUB_ID>\n```\n\n----------------------------------------\n\nTITLE: Adding Tests for mday() with Timestamp Input\nDESCRIPTION: R test code for verifying the mday() function works correctly with timestamp inputs using the dplyr binding framework.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_5\n\nLANGUAGE: R\nCODE:\n```\ntest_that(\"extract mday from timestamp\", {\n  compare_dplyr_binding(\n    .input %>%\n      mutate(x = mday(datetime)) %>%\n      collect(),\n    test_df\n  )\n})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating CUDA Buffer Segmentation Fault\nDESCRIPTION: Warning example showing what happens when incorrectly passing a CUDA buffer to a function that expects a CPU buffer, resulting in a segmentation fault.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/cuda.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> pa.py_buffer(b\"x\" * 16).equals(cuda_buf)\nSegmentation fault\n```\n\n----------------------------------------\n\nTITLE: Defining Skyhook Custom Targets\nDESCRIPTION: Defines the main custom targets for building Arrow Skyhook components.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/skyhook/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(arrow_skyhook)\nadd_custom_target(cls_skyhook)\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Dataset and Parquet Examples in CMake\nDESCRIPTION: Sets up various dataset examples with Parquet integration, including scan examples, documentation examples, and specialized features like column encryption when enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/arrow/CMakeLists.txt#2025-04-16_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_PARQUET AND ARROW_DATASET)\n  if(ARROW_BUILD_SHARED)\n    set(DATASET_EXAMPLES_LINK_LIBS arrow_dataset_shared)\n  else()\n    set(DATASET_EXAMPLES_LINK_LIBS arrow_dataset_static)\n  endif()\n\n  add_arrow_example(dataset_parquet_scan_example EXTRA_LINK_LIBS\n                    ${DATASET_EXAMPLES_LINK_LIBS})\n  add_dependencies(dataset-parquet-scan-example parquet)\n\n  add_arrow_example(dataset_documentation_example EXTRA_LINK_LIBS\n                    ${DATASET_EXAMPLES_LINK_LIBS})\n  add_dependencies(dataset-documentation-example parquet)\n\n  add_arrow_example(execution_plan_documentation_examples EXTRA_LINK_LIBS\n                    ${DATASET_EXAMPLES_LINK_LIBS})\n  add_dependencies(execution-plan-documentation-examples parquet)\n\n  if(PARQUET_REQUIRE_ENCRYPTION)\n    add_arrow_example(parquet_column_encryption\n                      EXTRA_SOURCES\n                      ${PROJECT_SOURCE_DIR}/src/parquet/encryption/test_in_memory_kms.cc\n                      EXTRA_LINK_LIBS\n                      ${DATASET_EXAMPLES_LINK_LIBS})\n    add_dependencies(parquet-column-encryption parquet)\n  endif()\n\n  if(ARROW_CSV)\n    add_arrow_example(join_example EXTRA_LINK_LIBS ${DATASET_EXAMPLES_LINK_LIBS})\n    add_dependencies(join-example parquet)\n  endif()\n\n  add_arrow_example(udf_example)\n\n  if(ARROW_SKYHOOK)\n    if(ARROW_BUILD_SHARED)\n      list(APPEND DATASET_EXAMPLES_LINK_LIBS arrow_skyhook_shared)\n    else()\n      list(APPEND DATASET_EXAMPLES_LINK_LIBS arrow_skyhook_static)\n    endif()\n\n    add_arrow_example(dataset_skyhook_scan_example EXTRA_LINK_LIBS\n                      ${DATASET_EXAMPLES_LINK_LIBS})\n    add_dependencies(dataset-skyhook-scan-example parquet)\n  endif()\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform-Specific Library Paths for Arrow in CMake\nDESCRIPTION: This snippet handles the configuration of Arrow library paths for different platforms (Windows, Linux, macOS). It determines the correct shared library locations and sets up the linking libraries appropriately based on whether Arrow is being built or found via find_package.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# ARROW_SHARED_LIB\n# On Windows, this will be ARROW_HOME/bin/arrow.dll and on Linux and macOS, it is the arrow.so/dylib in the newly built arrow_shared library.\nif(NOT Arrow_FOUND)\n  message(STATUS \"ARROW_SHARED_LIB will be set using IMPORTED_LOCATION value when building.\"\n  )\n  get_target_property(ARROW_SHARED_LIB arrow_shared IMPORTED_LOCATION)\nelse()\n  # If not building Arrow, ARROW_SHARED_LIB derived from ARROW_PREFIX set to the ARROW_HOME specified with cmake would be non-empty.\n  message(STATUS \"ARROW_SHARED_LIB: ${ARROW_SHARED_LIB}\")\nendif()\n\n# ARROW_LINK_LIB\n# On Windows, we use the arrow.lib for linking arrow_matlab against the Arrow C++ library.\n# The location of arrow.lib is previously saved in IMPORTED_IMPLIB.\nif(WIN32)\n  # If not building Arrow, IMPORTED_IMPLIB will be empty.\n  # Then set ARROW_LINK_LIB to ARROW_IMPORT_LIB which would have been derived from ARROW_PREFIX set to the ARROW_HOME specified with cmake. This will avoid the ARROW_LINK_LIB set to NOTFOUND error.\n  # The ARROW_IMPORT_LIB should be ARROW_HOME/lib/arrow.lib on Windows.\n  if(NOT Arrow_FOUND)\n    message(STATUS \"ARROW_LINK_LIB will be set using IMPORTED_IMPLIB value when building.\"\n    )\n    get_target_property(ARROW_LINK_LIB arrow_shared IMPORTED_IMPLIB)\n  else()\n    set(ARROW_LINK_LIB \"${ARROW_IMPORT_LIB}\")\n    message(STATUS \"Setting ARROW_LINK_LIB to ARROW_IMPORT_LIB: ${ARROW_IMPORT_LIB}, which is derived from the ARROW_HOME provided.\"\n    )\n  endif()\nelse()\n  # On Linux and macOS, it is the arrow.so/dylib in the newly built arrow_shared library used for linking.\n  # On Unix, this is the same as ARROW_SHARED_LIB.\n  message(STATUS \"Setting ARROW_LINK_LIB to ARROW_SHARED_LIB as they are same on Unix.\")\n  set(ARROW_LINK_LIB \"${ARROW_SHARED_LIB}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow Compute Benchmarks in CMake\nDESCRIPTION: Sets up various benchmark targets for Arrow Compute operations, including scalar arithmetic, boolean operations, casting, comparison, and vector operations.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/kernels/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_benchmark(scalar_arithmetic_benchmark PREFIX \"arrow-compute\")\nadd_arrow_benchmark(scalar_boolean_benchmark PREFIX \"arrow-compute\")\nadd_arrow_benchmark(scalar_cast_benchmark PREFIX \"arrow-compute\")\nadd_arrow_benchmark(scalar_compare_benchmark PREFIX \"arrow-compute\")\nadd_arrow_benchmark(scalar_if_else_benchmark PREFIX \"arrow-compute\")\nadd_arrow_benchmark(scalar_list_benchmark PREFIX \"arrow-compute\")\nadd_arrow_benchmark(scalar_random_benchmark PREFIX \"arrow-compute\")\nadd_arrow_benchmark(scalar_round_benchmark PREFIX \"arrow-compute\")\nadd_arrow_benchmark(scalar_set_lookup_benchmark PREFIX \"arrow-compute\")\nadd_arrow_benchmark(scalar_string_benchmark PREFIX \"arrow-compute\")\nadd_arrow_benchmark(scalar_temporal_benchmark PREFIX \"arrow-compute\")\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Arrow Flight\nDESCRIPTION: ReStructuredText documentation defining the structure and API reference for Apache Arrow Flight module. Includes section definitions for types, client/server APIs, authentication, and error handling.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/flight.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: pyarrow.flight\n\nArrow Flight\n============\n\n.. ifconfig:: not flight_enabled\n\n   .. error::\n      This documentation was built without Flight enabled.  The Flight\n      API docs are not available.\n\n.. NOTE We still generate those API docs (with empty docstrings)\n.. when Flight is disabled and `pyarrow.flight` mocked (see conf.py).\n.. Otherwise we'd get autodoc warnings, see https://github.com/sphinx-doc/sphinx/issues/4770\n\n.. warning:: Flight is currently unstable. APIs are subject to change,\n             though we don't expect drastic changes.\n```\n\n----------------------------------------\n\nTITLE: Including pyarrow API in C++\nDESCRIPTION: This code snippet shows how to include the pyarrow API in C++ code, assuming the include path is properly set.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/integration/extending.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <arrow/python/pyarrow.h>\n```\n\n----------------------------------------\n\nTITLE: Building PyArrow for Pyodide\nDESCRIPTION: Command to build PyArrow specifically for Pyodide integration after installing the main Arrow library.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/emscripten.rst#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npyodide build\n```\n\n----------------------------------------\n\nTITLE: Defining Vala API Build Option in Meson\nDESCRIPTION: Defines a boolean option to enable or disable building the Vala API. Defaults to false.\nSOURCE: https://github.com/apache/arrow/blob/main/c_glib/meson_options.txt#2025-04-16_snippet_5\n\nLANGUAGE: meson\nCODE:\n```\noption('vapi',\n       type: 'boolean',\n       value: false,\n       description: 'Build Vala API')\n```\n\n----------------------------------------\n\nTITLE: Building All JNI Libraries with Archery\nDESCRIPTION: Commands to build all JNI libraries using Archery, which runs the build process in a Docker container based on manylinux-2014.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n$ cd arrow\n$ archery docker run java-jni-manylinux-2014\n$ ls -latr java-dist\n|__ arrow_cdata_jni/\n|__ arrow_dataset_jni/\n|__ arrow_orc_jni/\n|__ gandiva_jni/\n```\n\n----------------------------------------\n\nTITLE: Including Apache License Header in HTML Comments\nDESCRIPTION: This code snippet contains the Apache License 2.0 header enclosed in HTML comments. It specifies the terms under which the file is licensed and provides a link to the full license text.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/c_glib/arrow-dataset-glib/index.md#2025-04-16_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Function for Adding Gandiva Tests\nDESCRIPTION: Creates a custom CMake function to add Gandiva test targets with appropriate configuration. Handles static vs. shared linking options, sets up test labels, and handles special requirements for LLVM symbol exporting.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/CMakeLists.txt#2025-04-16_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ADD_GANDIVA_TEST REL_TEST_NAME)\n  set(options USE_STATIC_LINKING)\n  set(one_value_args)\n  set(multi_value_args)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n\n  if(NO_TESTS)\n    return()\n  endif()\n\n  set(TEST_ARGUMENTS\n      ENABLED\n      PREFIX\n      \"gandiva\"\n      LABELS\n      \"gandiva-tests\"\n      ${ARG_UNPARSED_ARGUMENTS})\n\n  # and uses less disk space, but in some cases we need to force static\n  # linking (see rationale below).\n  if(ARG_USE_STATIC_LINKING OR ARROW_TEST_LINKAGE STREQUAL \"static\")\n    add_test_case(${REL_TEST_NAME}\n                  ${TEST_ARGUMENTS}\n                  STATIC_LINK_LIBS\n                  ${GANDIVA_STATIC_TEST_LINK_LIBS}\n                  ${ARG_UNPARSED_ARGUMENTS})\n  else()\n    add_test_case(${REL_TEST_NAME}\n                  ${TEST_ARGUMENTS}\n                  STATIC_LINK_LIBS\n                  ${GANDIVA_SHARED_TEST_LINK_LIBS}\n                  ${ARG_UNPARSED_ARGUMENTS})\n  endif()\n\n  set(TEST_NAME gandiva-${REL_TEST_NAME})\n  string(REPLACE \"_\" \"-\" TEST_NAME ${TEST_NAME})\n\n  if(ARG_USE_STATIC_LINKING OR ARROW_TEST_LINKAGE STREQUAL \"static\")\n    # LLVM 17 or later requires that an executable exports\n    # \"llvm_orc_registerEHFrameSectionWrapper()\" and\n    # \"llvm_orc_unregisterEHFrameSectionWrapper()\". We need to do\n    # nothing when we use libLLVM.so. But we need to export symbols\n    # explicitly when we use libLLVM*.a.\n    set_target_properties(${TEST_NAME} PROPERTIES ENABLE_EXPORTS TRUE)\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Installing Vendored Datetime Headers in Arrow Project\nDESCRIPTION: This CMake command installs all header files from the arrow/vendored/datetime directory. These are vendored (third-party) datetime-related headers that are packaged with the Arrow project.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/datetime/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\narrow_install_all_headers(\"arrow/vendored/datetime\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Test Execution in CMake\nDESCRIPTION: Checks if tests are disabled (NO_TESTS) and exits early if they are.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/tests/external_functions/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(NO_TESTS)\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Sparse Tensor Structure in Arrow with Flatbuffers\nDESCRIPTION: This code provides the format for encapsulating a sparse tensor message in Apache Arrow. It ensures proper padding and alignment of the starting offsets for both the sparse index and the tensor body to enhance performance in interprocess communication.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Other.rst#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n    <metadata prefix and metadata>\n    <PADDING>\n    <sparse index>\n    <PADDING>\n    <sparse tensor body>\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Skyhook Targets\nDESCRIPTION: Adds the built libraries as dependencies to the main Skyhook targets to ensure proper build order.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/skyhook/CMakeLists.txt#2025-04-16_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_dependencies(arrow_skyhook ${ARROW_SKYHOOK_CLIENT_LIBRARIES})\nadd_dependencies(cls_skyhook ${ARROW_SKYHOOK_CLS_LIBRARIES})\n```\n\n----------------------------------------\n\nTITLE: Adding Parquet Library Target\nDESCRIPTION: Configures the main Parquet library target with all necessary compile definitions, link flags and dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/CMakeLists.txt#2025-04-16_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_lib(parquet\n              CMAKE_PACKAGE_NAME\n              Parquet\n              PKG_CONFIG_NAME\n              parquet\n              SOURCES\n              ${PARQUET_SRCS}\n              PRECOMPILED_HEADERS\n              \"$<$<COMPILE_LANGUAGE:CXX>:parquet/pch.h>\"\n              OUTPUTS\n              PARQUET_LIBRARIES\n              SHARED_LINK_FLAGS\n              ${PARQUET_SHARED_LINK_FLAGS}\n              SHARED_LINK_LIBS\n              ${PARQUET_SHARED_LINK_LIBS}\n              SHARED_PRIVATE_LINK_LIBS\n              ${PARQUET_SHARED_PRIVATE_LINK_LIBS}\n              SHARED_INSTALL_INTERFACE_LIBS\n              Arrow::arrow_shared\n              STATIC_LINK_LIBS\n              ${PARQUET_STATIC_LINK_LIBS}\n              STATIC_INSTALL_INTERFACE_LIBS\n              ${PARQUET_STATIC_INSTALL_INTERFACE_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Creating a TableBatchReader from a Table in C++\nDESCRIPTION: This code creates a TableBatchReader from a Table, which allows the Table to be processed as a stream of RecordBatches. This is useful when preparing to write a Dataset from a Table.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n// Create a TableBatchReader for our Result\nTableBatchReader table_batch_reader(*table);\n```\n\n----------------------------------------\n\nTITLE: Running Archery Integration Tests - Help\nDESCRIPTION: This command displays the help message for the archery integration command, providing information on available options and usage. It allows users to understand the different parameters that can be used to configure the integration tests.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ archery integration --help\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Apache Arrow\nDESCRIPTION: This snippet specifies the required Python packages and their versions for the Apache Arrow project. It includes Cython 3 or higher, NumPy with version requirements based on Python version, setuptools_scm, setuptools 58 or higher, and wheel.\nSOURCE: https://github.com/apache/arrow/blob/main/python/requirements-wheel-build.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncython>=3\noldest-supported-numpy>=0.14; python_version<'3.9'\nnumpy>=2.0.0; python_version>='3.9'\nsetuptools_scm\nsetuptools>=58\nwheel\n```\n\n----------------------------------------\n\nTITLE: Defining Clang and LLVM Dependencies\nDESCRIPTION: Specifies minimum version requirements for Clang and LLVM development tools. Both components require version 11 or higher.\nSOURCE: https://github.com/apache/arrow/blob/main/ci/conda_env_gandiva.txt#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclang>=11\nllvmdev>=11\n```\n\n----------------------------------------\n\nTITLE: Defining Dependency Versions and Checksums for Apache Arrow\nDESCRIPTION: This code snippet defines version numbers and SHA256 checksums for various dependencies used in the Apache Arrow project. It includes libraries such as Abseil, AWS SDK, Boost, Brotli, and many others. These variables are used by build scripts to ensure consistent versions across the project.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/thirdparty/versions.txt#2025-04-16_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nARROW_ABSL_BUILD_VERSION=20211102.0\nARROW_ABSL_BUILD_SHA256_CHECKSUM=dcf71b9cba8dc0ca9940c4b316a0c796be8fab42b070bb6b7cab62b48f0e66c4\nARROW_AWS_C_AUTH_BUILD_VERSION=v0.6.22\nARROW_AWS_C_AUTH_BUILD_SHA256_CHECKSUM=691a6b4418afcd3dc141351b6ad33fccd8e3ff84df0e9e045b42295d284ee14c\nARROW_AWS_C_CAL_BUILD_VERSION=v0.5.20\nARROW_AWS_C_CAL_BUILD_SHA256_CHECKSUM=acc352359bd06f8597415c366cf4ec4f00d0b0da92d637039a73323dd55b6cd0\nARROW_AWS_C_COMMON_BUILD_VERSION=v0.8.9\nARROW_AWS_C_COMMON_BUILD_SHA256_CHECKSUM=2f3fbaf7c38eae5a00e2a816d09b81177f93529ae8ba1b82dc8f31407565327a\nARROW_AWS_C_COMPRESSION_BUILD_VERSION=v0.2.16\nARROW_AWS_C_COMPRESSION_BUILD_SHA256_CHECKSUM=044b1dbbca431a07bde8255ef9ec443c300fc60d4c9408d4b862f65e496687f4\nARROW_AWS_C_EVENT_STREAM_BUILD_VERSION=v0.2.18\nARROW_AWS_C_EVENT_STREAM_BUILD_SHA256_CHECKSUM=310ca617f713bf664e4c7485a3d42c1fb57813abd0107e49790d107def7cde4f\nARROW_AWS_C_HTTP_BUILD_VERSION=v0.7.3\nARROW_AWS_C_HTTP_BUILD_SHA256_CHECKSUM=07e16c6bf5eba6f0dea96b6f55eae312a7c95b736f4d2e4a210000f45d8265ae\nARROW_AWS_C_IO_BUILD_VERSION=v0.13.14\nARROW_AWS_C_IO_BUILD_SHA256_CHECKSUM=12b66510c3d9a4f7e9b714e9cfab2a5bf835f8b9ce2f909d20ae2a2128608c71\nARROW_AWS_C_MQTT_BUILD_VERSION=v0.8.4\nARROW_AWS_C_MQTT_BUILD_SHA256_CHECKSUM=232eeac63e72883d460c686a09b98cdd811d24579affac47c5c3f696f956773f\nARROW_AWS_C_S3_BUILD_VERSION=v0.2.3\nARROW_AWS_C_S3_BUILD_SHA256_CHECKSUM=a00b3c9f319cd1c9aa2c3fa15098864df94b066dcba0deaccbb3caa952d902fe\nARROW_AWS_C_SDKUTILS_BUILD_VERSION=v0.1.6\nARROW_AWS_C_SDKUTILS_BUILD_SHA256_CHECKSUM=8a2951344b2fb541eab1e9ca17c18a7fcbfd2aaff4cdd31d362d1fad96111b91\nARROW_AWS_CHECKSUMS_BUILD_VERSION=v0.1.13\nARROW_AWS_CHECKSUMS_BUILD_SHA256_CHECKSUM=0f897686f1963253c5069a0e495b85c31635ba146cd3ac38cc2ea31eaf54694d\nARROW_AWS_CRT_CPP_BUILD_VERSION=v0.18.16\nARROW_AWS_CRT_CPP_BUILD_SHA256_CHECKSUM=9e69bc1dc4b50871d1038aa9ff6ddeb4c9b28f7d6b5e5b1b69041ccf50a13483\nARROW_AWS_LC_BUILD_VERSION=v1.3.0\nARROW_AWS_LC_BUILD_SHA256_CHECKSUM=ae96a3567161552744fc0cae8b4d68ed88b1ec0f3d3c98700070115356da5a37\nARROW_AWSSDK_BUILD_VERSION=1.10.55\nARROW_AWSSDK_BUILD_SHA256_CHECKSUM=2d552fb1a84bef4a9b65e34aa7031851ed2aef5319e02cc6e4cb735c48aa30de\nARROW_AZURE_SDK_BUILD_VERSION=azure-identity_1.9.0\nARROW_AZURE_SDK_BUILD_SHA256_CHECKSUM=97065bfc971ac8df450853ce805f820f52b59457bd7556510186a1569502e4a1\nARROW_BOOST_BUILD_VERSION=1.81.0\nARROW_BOOST_BUILD_SHA256_CHECKSUM=9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574\nARROW_BROTLI_BUILD_VERSION=v1.0.9\nARROW_BROTLI_BUILD_SHA256_CHECKSUM=f9e8d81d0405ba66d181529af42a3354f838c939095ff99930da6aa9cdf6fe46\nARROW_BZIP2_BUILD_VERSION=1.0.8\nARROW_BZIP2_BUILD_SHA256_CHECKSUM=ab5a03176ee106d3f0fa90e381da478ddae405918153cca248e682cd0c4a2269\nARROW_CARES_BUILD_VERSION=1.17.2\nARROW_CARES_BUILD_SHA256_CHECKSUM=4803c844ce20ce510ef0eb83f8ea41fa24ecaae9d280c468c582d2bb25b3913d\nARROW_CRC32C_BUILD_VERSION=1.1.2\nARROW_CRC32C_BUILD_SHA256_CHECKSUM=ac07840513072b7fcebda6e821068aa04889018f24e10e46181068fb214d7e56\nARROW_GBENCHMARK_BUILD_VERSION=v1.8.3\nARROW_GBENCHMARK_BUILD_SHA256_CHECKSUM=6bc180a57d23d4d9515519f92b0c83d61b05b5bab188961f36ac7b06b0d9e9ce\nARROW_GFLAGS_BUILD_VERSION=v2.2.2\nARROW_GFLAGS_BUILD_SHA256_CHECKSUM=34af2f15cf7367513b352bdcd2493ab14ce43692d2dcd9dfc499492966c64dcf\nARROW_GLOG_BUILD_VERSION=v0.5.0\nARROW_GLOG_BUILD_SHA256_CHECKSUM=eede71f28371bf39aa69b45de23b329d37214016e2055269b3b5e7cfd40b59f5\nARROW_GOOGLE_CLOUD_CPP_BUILD_VERSION=v2.22.0\nARROW_GOOGLE_CLOUD_CPP_BUILD_SHA256_CHECKSUM=0c68782e57959c82e0c81def805c01460a042c1aae0c2feee905acaa2a2dc9bf\nARROW_GRPC_BUILD_VERSION=v1.46.3\nARROW_GRPC_BUILD_SHA256_CHECKSUM=d6cbf22cb5007af71b61c6be316a79397469c58c82a942552a62e708bce60964\nARROW_GTEST_BUILD_VERSION=1.16.0\nARROW_GTEST_BUILD_SHA256_CHECKSUM=78c676fc63881529bf97bf9d45948d905a66833fbfa5318ea2cd7478cb98f399\nARROW_JEMALLOC_BUILD_VERSION=5.3.0\nARROW_JEMALLOC_BUILD_SHA256_CHECKSUM=2db82d1e7119df3e71b7640219b6dfe84789bc0537983c3b7ac4f7189aecfeaa\nARROW_LZ4_BUILD_VERSION=v1.10.0\nARROW_LZ4_BUILD_SHA256_CHECKSUM=537512904744b35e232912055ccf8ec66d768639ff3abe5788d90d792ec5f48b\nARROW_MIMALLOC_BUILD_VERSION=v2.0.6\nARROW_MIMALLOC_BUILD_SHA256_CHECKSUM=9f05c94cc2b017ed13698834ac2a3567b6339a8bde27640df5a1581d49d05ce5\nARROW_NLOHMANN_JSON_BUILD_VERSION=v3.12.0\nARROW_NLOHMANN_JSON_BUILD_SHA256_CHECKSUM=4b92eb0c06d10683f7447ce9406cb97cd4b453be18d7279320f7b2f025c10187\nARROW_OPENTELEMETRY_BUILD_VERSION=v1.13.0\nARROW_OPENTELEMETRY_BUILD_SHA256_CHECKSUM=7735cc56507149686e6019e06f588317099d4522480be5f38a2a09ec69af1706\nARROW_OPENTELEMETRY_PROTO_BUILD_VERSION=v0.17.0\nARROW_OPENTELEMETRY_PROTO_BUILD_SHA256_CHECKSUM=f269fbcb30e17b03caa1decd231ce826e59d7651c0f71c3b28eb5140b4bb5412\nARROW_ORC_BUILD_VERSION=2.1.1\nARROW_ORC_BUILD_SHA256_CHECKSUM=15af8baeee322bab0298559a14a09cf8c14cf2008e35d8a78d3cc8a4c98d1e59\nARROW_PROTOBUF_BUILD_VERSION=v21.3\nARROW_PROTOBUF_BUILD_SHA256_CHECKSUM=2f723218f6cb709ae4cdc4fb5ed56a5951fc5d466f0128ce4c946b8c78c8c49f\nARROW_RAPIDJSON_BUILD_VERSION=232389d4f1012dddec4ef84861face2d2ba85709\nARROW_RAPIDJSON_BUILD_SHA256_CHECKSUM=b9290a9a6d444c8e049bd589ab804e0ccf2b05dc5984a19ed5ae75d090064806\nARROW_RE2_BUILD_VERSION=2022-06-01\nARROW_RE2_BUILD_SHA256_CHECKSUM=f89c61410a072e5cbcf8c27e3a778da7d6fd2f2b5b1445cd4f4508bee946ab0f\nARROW_SNAPPY_BUILD_VERSION=1.2.2\nARROW_SNAPPY_BUILD_SHA256_CHECKSUM=90f74bc1fbf78a6c56b3c4a082a05103b3a56bb17bca1a27e052ea11723292dc\nARROW_SUBSTRAIT_BUILD_VERSION=v0.44.0\nARROW_SUBSTRAIT_BUILD_SHA256_CHECKSUM=f989a862f694e7dbb695925ddb7c4ce06aa6c51aca945105c075139aed7e55a2\nARROW_S2N_TLS_BUILD_VERSION=v1.3.35\nARROW_S2N_TLS_BUILD_SHA256_CHECKSUM=9d32b26e6bfcc058d98248bf8fc231537e347395dd89cf62bb432b55c5da990d\nARROW_THRIFT_BUILD_VERSION=0.20.0\nARROW_THRIFT_BUILD_SHA256_CHECKSUM=b5d8311a779470e1502c027f428a1db542f5c051c8e1280ccd2163fa935ff2d6\nARROW_UTF8PROC_BUILD_VERSION=v2.10.0\nARROW_UTF8PROC_BUILD_SHA256_CHECKSUM=6f4f1b639daa6dca9f80bc5db1233e9cbaa31a67790887106160b33ef743f136\nARROW_XSIMD_BUILD_VERSION=13.0.0\nARROW_XSIMD_BUILD_SHA256_CHECKSUM=8bdbbad0c3e7afa38d88d0d484d70a1671a1d8aefff03f4223ab2eb6a41110a3\nARROW_ZLIB_BUILD_VERSION=1.3.1\nARROW_ZLIB_BUILD_SHA256_CHECKSUM=9a93b2b7dfdac77ceba5a558a580e74667dd6fede4585b91eefb60f03b72df23\nARROW_ZSTD_BUILD_VERSION=1.5.7\nARROW_ZSTD_BUILD_SHA256_CHECKSUM=eb33e51f49a15e023950cd7825ca74a4a2b43db8354825ac24fc1b7ee09e6fa3\n```\n\n----------------------------------------\n\nTITLE: Examining Parquet File Metadata using parquet-tools\nDESCRIPTION: This shell command uses the parquet-tools utility to display metadata of a Parquet file named 'data4_3rg_gzip.parquet'. It shows the file schema and details about three row groups, including their row counts and offsets.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/dataset.rst#2025-04-16_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n$ parquet-tools meta data4_3rg_gzip.parquet\nfile schema: schema\nage:         OPTIONAL INT64 R:0 D:1\nname:        OPTIONAL BINARY L:STRING R:0 D:1\nrow group 1: RC:4 TS:182 OFFSET:4\nrow group 2: RC:4 TS:190 OFFSET:420\nrow group 3: RC:3 TS:179 OFFSET:838\n```\n\n----------------------------------------\n\nTITLE: Deleting Local Tags and Setting Up Release Environment\nDESCRIPTION: Commands for removing local tags, setting up GPG agent for signing, and curating the release by generating a report that shows GitHub issues with incorrect version numbers.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Delete the local tag for RC1 or later\ngit tag -d apache-arrow-<version>\n\n# Setup gpg agent for signing artifacts\nsource dev/release/setup-gpg-agent.sh\n\n# Curate the release\n# The end of the generated report shows any GitHub issues with the wrong\n# version number assigned.\narchery release curate <version>\n```\n\n----------------------------------------\n\nTITLE: Rebasing Branch in Git - Console\nDESCRIPTION: This snippet shows how to rebase a branch in Git to match the upstream main branch. This command ensures that the local branch is up to date with the main branch, helping to avoid merge conflicts.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_15\n\nLANGUAGE: console\nCODE:\n```\n$ git pull upstream main --rebase\n\n```\n\n----------------------------------------\n\nTITLE: macOS Shared Library Loading Error Stack Trace\nDESCRIPTION: Detailed error stack trace showing the failure to load shared library 'libparquet-glib.400.dylib' due to @rpath dependency issues on macOS.\nSOURCE: https://github.com/apache/arrow/blob/main/c_glib/README.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n(NULL)-WARNING **: Failed to load shared library '/usr/local/lib/libparquet-glib.400.dylib' referenced by the typelib: dlopen(/usr/local/lib/libparquet-glib.400.dylib, 0x0009): dependent dylib '@rpath/libparquet.400.dylib' not found for '/usr/local/lib/libparquet-glib.400.dylib'. relative file paths not allowed '@rpath/libparquet.400.dylib'\n        from /Library/Ruby/Gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/loader.rb:215:in `load_object_info'\n        from /Library/Ruby/Gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/loader.rb:68:in `load_info'\n        from /Library/Ruby/Gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/loader.rb:43:in `block in load'\n        from /Library/Ruby/Gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/repository.rb:34:in `block (2 levels) in each'\n        from /Library/Ruby/Gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/repository.rb:33:in `times'\n        from /Library/Ruby/Gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/repository.rb:33:in `block in each'\n        from /Library/Ruby/Gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/repository.rb:32:in `each'\n        from /Library/Ruby/Gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/repository.rb:32:in `each'\n        from /Library/Ruby/Gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/loader.rb:42:in `load'\n        from /Library/Ruby/Gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection.rb:44:in `load'\n        from /Users/karlkatzen/Documents/code/arrow-dev/arrow/c_glib/test/run-test.rb:60:in `<main>'\nTraceback (most recent call last):\n        17: from /Users/karlkatzen/Documents/code/arrow-dev/arrow/c_glib/test/run-test.rb:80:in `<main>'\n        16: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/autorunner.rb:66:in `run'\n        15: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/autorunner.rb:434:in `run'\n        14: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/autorunner.rb:106:in `block in <class:AutoRunner>'\n        13: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:38:in `collect'\n        12: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:136:in `add_load_path'\n        11: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:43:in `block in collect'\n        10: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:43:in `each'\n         9: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:46:in `block (2 levels) in collect'\n         8: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:85:in `collect_recursive'\n         7: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:85:in `each'\n         6: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:87:in `block in collect_recursive'\n         5: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:112:in `collect_file'\n         4: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:136:in `add_load_path'\n         3: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:114:in `block in collect_file'\n         2: from /Library/Ruby/Gems/2.6.0/gems/test-unit-3.4.0/lib/test/unit/collector/load.rb:114:in `require'\n         1: from /Users/karlkatzen/Documents/code/arrow-dev/arrow/c_glib/test/test-extension-data-type.rb:18:in `<top (required)>'\n/Users/karlkatzen/Documents/code/arrow-dev/arrow/c_glib/test/test-extension-data-type.rb:19:in `<class:TestExtensionDataType>': uninitialized constant Arrow::ExtensionArray (NameError)\n```\n\n----------------------------------------\n\nTITLE: Documenting License Information in Markdown\nDESCRIPTION: This markdown snippet provides a preamble explaining the license situation for the Apache Arrow project. It mentions that individual source files contain specific license information, and that most of the code is public domain under CC0 1.0 Universal dedication, with some exceptions for vendored code.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/portable-snippets/README.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!---\nEach source file contains a preamble explaining the license situation\nfor that file, which takes priority over this file.  With the\nexception of some code pulled in from other repositories (such as\nµnit, an MIT-licensed project which is used for testing), the code is\npublic domain, released using the CC0 1.0 Universal dedication.\n-->\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow Extension Headers in CMake\nDESCRIPTION: Calls the arrow_install_all_headers function to install all header files from the arrow/extension directory to the appropriate destination.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/extension/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/extension\")\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Kernels and Row in CMake\nDESCRIPTION: This snippet adds subdirectories for kernels and row-related components to the CMake build configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(kernels)\n\nadd_subdirectory(row)\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration Fix for Arrow C++\nDESCRIPTION: CMake commands to rebuild Arrow C++ with ARROW_INSTALL_NAME_RPATH disabled to resolve @rpath-related loading issues on macOS.\nSOURCE: https://github.com/apache/arrow/blob/main/c_glib/README.md#2025-04-16_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ cmake -S cpp -B cpp.build -DARROW_INSTALL_NAME_RPATH=OFF ...\n$ cmake --build cpp.build\n$ sudo cmake --build cpp.build --target install\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow CSV Headers in CMake\nDESCRIPTION: Configures the installation of all headers related to the Arrow CSV module. This ensures that the necessary header files are available for users of the library.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/csv/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"arrow/csv\")\n```\n\n----------------------------------------\n\nTITLE: Copying Arrow Merge Configuration File in Bash\nDESCRIPTION: This command copies a sample configuration file for the merge script to the user's config directory, allowing for token configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncp ./merge.conf.sample ~/.config/arrow/merge.conf\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Java Development in reStructuredText\nDESCRIPTION: A toctree directive that defines the structure of the Java development documentation, including links to building and development guide pages with a maximum depth of 2.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   building\n   development\n```\n\n----------------------------------------\n\nTITLE: RunMain Function Declaration for Arrow File I/O\nDESCRIPTION: RunMain function declaration that contains the core file I/O logic.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\narrow::Status RunMain() {\\n  ARROW_RETURN_NOT_OK(GenInitialFile());\\n  return arrow::Status::OK();\\n}\n```\n\n----------------------------------------\n\nTITLE: Returning Status at End of Program in C++\nDESCRIPTION: This code returns a successful status to indicate that all operations completed without errors. It's used at the end of the main function to signal program completion.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\nreturn Status::OK();\n```\n\n----------------------------------------\n\nTITLE: Installing Archery Crossbow Subpackage\nDESCRIPTION: Command to install the crossbow subpackage of Archery, which is used to trigger and interact with the crossbow build system.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/archery/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \"arrow/dev/archery[crossbow]\"\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Build Option in Meson\nDESCRIPTION: Defines a boolean option to enable or disable building documentation. Defaults to false.\nSOURCE: https://github.com/apache/arrow/blob/main/c_glib/meson_options.txt#2025-04-16_snippet_2\n\nLANGUAGE: meson\nCODE:\n```\noption('doc',\n       type: 'boolean',\n       value: false,\n       description: 'Build document')\n```\n\n----------------------------------------\n\nTITLE: Generating Test Data for Swift Arrow Implementation\nDESCRIPTION: Instructions for building the test data generator executable using Go. This generator is used to create test data files for reader tests in the Swift Arrow implementation.\nSOURCE: https://github.com/apache/arrow/blob/main/swift/Arrow/README.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n$ go build -o swift-datagen\n```\n\n----------------------------------------\n\nTITLE: Skipping Image Building Process\nDESCRIPTION: Commands showing how to skip rebuilding an image that has already been built, useful when encountering Docker cache issues.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# first run ensures that the image is built\narchery docker run conda-python\n\n# if the second run tries the build the image again and none of the files\n# referenced in the relevant dockerfile have changed, then it indicates a\n# cache miss caused by the issue described above\narchery docker run conda-python\n\n# since the image is properly built with the first command, there is no\n# need to rebuild it, so manually disable the pull and build phases to\n# spare the some time\narchery docker run --no-pull --no-build conda-python\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for Static Linking Dependencies (CMake)\nDESCRIPTION: This snippet demonstrates how to configure CMake for static linking of Arrow and related libraries on Windows. It defines required dependencies and compiler definitions needed to link against static libraries correctly.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\nproject(MyExample)\n\nfind_package(Arrow REQUIRED)\n\nadd_executable(my_example my_example.cc)\ntarget_link_libraries(my_example\n                      PRIVATE\n                      arrow_static\n                      arrow_flight_static\n                      arrow_flight_sql_static)\n\ntarget_compile_definitions(my_example\n                         PUBLIC\n                         ARROW_STATIC\n                         ARROW_FLIGHT_STATIC\n                         ARROW_FLIGHT_SQL_STATIC)\n```\n\n----------------------------------------\n\nTITLE: Basic Red Arrow Flight SQL Usage Template\nDESCRIPTION: Basic setup code showing how to require and initialize Red Arrow Flight SQL in a Ruby application. Note that this is a placeholder template as indicated by the TODO comment.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-flight-sql/README.md#2025-04-16_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"arrow-flight-sql\"\n\n# TODO\n```\n\n----------------------------------------\n\nTITLE: Testing Invalid Argument for Feather Format in R\nDESCRIPTION: This snippet checks the error handling when a nonsensical argument is provided for the Feather format in write_dataset. It expects an error message listing the supported arguments.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dataset-write.md#2025-04-16_snippet_1\n\nLANGUAGE: R\nCODE:\n```\nwrite_dataset(df, dst_dir, format = \"feather\", nonsensical_arg = \"blah-blah\")\n```\n\n----------------------------------------\n\nTITLE: Building Arrow C++ Fuzz Targets\nDESCRIPTION: CMake command to build fuzz targets with debug information and sanitizer checks enabled using the fuzzing preset.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/fuzzing.rst#2025-04-16_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ cmake .. --preset=fuzzing\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Acero Benchmark Function in CMake\nDESCRIPTION: Creates a function to add Arrow Acero benchmarks with proper linking and labels. This function configures benchmarks with appropriate prefixes, labels, and dependencies, similar to the test function.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/acero/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(add_arrow_acero_benchmark REL_BENCHMARK_NAME)\n  set(options)\n  set(one_value_args PREFIX)\n  set(multi_value_args LABELS)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n\n  if(ARG_PREFIX)\n    set(PREFIX ${ARG_PREFIX})\n  else()\n    set(PREFIX \"arrow-acero\")\n  endif()\n\n  if(ARG_LABELS)\n    set(LABELS ${ARG_LABELS})\n  else()\n    set(LABELS \"arrow_acero\")\n  endif()\n\n  add_arrow_benchmark(${REL_BENCHMARK_NAME}\n                      EXTRA_LINK_LIBS\n                      ${ARROW_ACERO_TEST_LINK_LIBS}\n                      PREFIX\n                      ${PREFIX}\n                      LABELS\n                      ${LABELS}\n                      ${ARG_UNPARSED_ARGUMENTS})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring LeakSanitizer Suppressions for Apache Arrow\nDESCRIPTION: A suppression list for LeakSanitizer that identifies known memory leaks to ignore. This includes false positives from libc's atexit registration, OpenSSL shutdown leaks, and OpenTelemetry thread local context issues that occur at program termination.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/build-support/lsan-suppressions.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# False positive from atexit() registration in libc\nleak:*__new_exitfn*\n# Leak at shutdown in OpenSSL\nleak:CRYPTO_zalloc\n\n# OpenTelemetry. These seem like false positives and go away if the\n# CPU thread pool is manually shut down before exit.\n# Note that ASan has trouble backtracing these and may not be able to\n# without LSAN_OPTIONS=fast_unwind_on_malloc=0:malloc_context_size=100\nleak:opentelemetry::v1::context::ThreadLocalContextStorage::GetStack\nleak:opentelemetry::v1::context::ThreadLocalContextStorage::Stack::Resize\n```\n\n----------------------------------------\n\nTITLE: Importing pyarrow for IPC operations in Python\nDESCRIPTION: This code snippet shows how to import the pyarrow module, which is necessary for using the IPC functionality described in the document.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/ipc.rst#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pyarrow\n```\n\n----------------------------------------\n\nTITLE: Registering Arrow Dataset Tests\nDESCRIPTION: Adds multiple test targets for the Arrow Dataset module, including conditionally adding tests for CSV, JSON, ORC, and Parquet file formats when those features are enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/dataset/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_dataset_test(dataset_test)\nadd_arrow_dataset_test(dataset_writer_test)\nadd_arrow_dataset_test(discovery_test)\nadd_arrow_dataset_test(file_ipc_test)\nadd_arrow_dataset_test(file_test)\nadd_arrow_dataset_test(partition_test)\nadd_arrow_dataset_test(scanner_test)\nadd_arrow_dataset_test(subtree_test)\nadd_arrow_dataset_test(write_node_test)\n\nif(ARROW_CSV)\n  add_arrow_dataset_test(file_csv_test)\nendif()\n\nif(ARROW_JSON)\n  add_arrow_dataset_test(file_json_test EXTRA_LINK_LIBS ${ARROW_DATASET_TEST_LINK_LIBS}\n                         RapidJSON)\nendif()\n\nif(ARROW_ORC)\n  add_arrow_dataset_test(file_orc_test EXTRA_LINK_LIBS ${ARROW_DATASET_TEST_LINK_LIBS}\n                         orc::orc)\nendif()\n\nif(ARROW_PARQUET)\n  add_arrow_dataset_test(file_parquet_test)\n  if(PARQUET_REQUIRE_ENCRYPTION AND ARROW_DATASET)\n    add_arrow_dataset_test(file_parquet_encryption_test\n                           SOURCES\n                           file_parquet_encryption_test.cc\n                           ${PROJECT_SOURCE_DIR}/src/parquet/encryption/test_in_memory_kms.cc\n    )\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow BooleanType in C++\nDESCRIPTION: This snippet documents the arrow::BooleanType class, which represents the boolean data type in Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenclass:: arrow::BooleanType\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow Array Headers\nDESCRIPTION: Configures installation of all Arrow array-related header files to the system using a custom CMake function.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/array/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\narrow_install_all_headers(\"arrow/array\")\n```\n\n----------------------------------------\n\nTITLE: Writing Feather Format Files\nDESCRIPTION: Shows how to write data in Feather/Arrow IPC format instead of Parquet format.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nstd::string file1 = dir_path + \"/data1.feather\";\nstd::string file2 = dir_path + \"/data2.feather\";\n\nFLIGHT_THROW_IF_NOT_OK(ipc::WriteFeatherFile(*table1, file1));\nFLIGHT_THROW_IF_NOT_OK(ipc::WriteFeatherFile(*table2, file2));\n```\n\n----------------------------------------\n\nTITLE: Building Podman Image for Fedora with SELinux Support\nDESCRIPTION: Command to build a Podman container image for Fedora, which is an alternative to Docker often used on SELinux-enabled systems.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npodman build -t arrow_fedora_minimal -f Dockerfile.fedora\n```\n\n----------------------------------------\n\nTITLE: Handling CommandGetCrossReference in Protobuf\nDESCRIPTION: CommandGetCrossReference retrieves the foreign key columns in a specific table that reference columns in a parent table. This command aids in understanding relationships between tables in the database schema.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/FlightSql.rst#2025-04-16_snippet_1\n\nLANGUAGE: protobuf\nCODE:\n```\n\"CommandGetCrossReference\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Dependencies for Offline Build\nDESCRIPTION: Shell command for downloading Arrow dependencies for offline builds using the provided script.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_56\n\nLANGUAGE: shell\nCODE:\n```\n# Download tarballs into $HOME/arrow-thirdparty\n$ ./thirdparty/download_dependencies.sh $HOME/arrow-thirdparty\n```\n\n----------------------------------------\n\nTITLE: Specifying CUDA Thread Leak in Plaintext Configuration\nDESCRIPTION: This snippet defines a configuration setting to address a thread leak issue in CUDA. It specifies that a thread associated with the libcuda.so shared library is causing the leak.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/build-support/tsan-suppressions.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nthread:libcuda.so\n```\n\n----------------------------------------\n\nTITLE: Suppressing Unused Variable Warnings for Parquet Examples in CMake\nDESCRIPTION: For Unix systems, this snippet adds a compiler flag to suppress unused variable warnings for the specified example files.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/parquet/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(UNIX)\n  foreach(FILE ${PARQUET_EXAMPLES_WARNING_SUPPRESSIONS})\n    set_property(SOURCE ${FILE}\n                 APPEND_STRING\n                 PROPERTY COMPILE_FLAGS \"-Wno-unused-variable\")\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up Arrow Acero Testing Library in CMake\nDESCRIPTION: Creates an object library for Arrow Acero testing utilities when testing is enabled. This library provides common test infrastructure used across multiple test files including test nodes and internal utilities.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/acero/CMakeLists.txt#2025-04-16_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_TESTING)\n  # test_nodes.cc isn't used by all tests but link to it for simple\n  # CMakeLists.txt.\n  add_library(arrow_acero_testing OBJECT test_nodes.cc test_util_internal.cc)\n  # Even though this is still just an object library we still need to \"link\" our\n  # dependencies so that include paths are configured correctly\n  target_link_libraries(arrow_acero_testing PRIVATE ${ARROW_ACERO_TEST_LINK_LIBS})\n  # Only for test_nodes.cc.\n  if(ARROW_WITH_OPENTELEMETRY)\n    target_link_libraries(arrow_acero_testing PRIVATE ${ARROW_OPENTELEMETRY_LIBS})\n  endif()\n  list(APPEND ARROW_ACERO_TEST_LINK_LIBS arrow_acero_testing arrow_compute_testing)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Displaying arrow::Decimal128Scalar in GDB without extension\nDESCRIPTION: Example of how GDB displays an arrow::Decimal128Scalar object without the custom extension. Shows the complex nested structure which is difficult to interpret.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/gdb.rst#2025-04-16_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n$4 = (arrow::Decimal128Scalar) {\n  <arrow::DecimalScalar<arrow::Decimal128Type, arrow::Decimal128>> = {\n    <arrow::internal::PrimitiveScalarBase> = {\n      <arrow::Scalar> = {\n        <arrow::util::EqualityComparable<arrow::Scalar>> = {<No data fields>},\n        members of arrow::Scalar:\n        _vptr.Scalar = 0x7ffff6870e78 <vtable for arrow::Decimal128Scalar+16>,\n        type = std::shared_ptr<arrow::DataType> (use count 1, weak count 0) = {\n          get() = 0x555555ce58a0\n        },\n        is_valid = true\n      }, <No data fields>},\n    members of arrow::DecimalScalar<arrow::Decimal128Type, arrow::Decimal128>:\n    value = {\n      <arrow::BasicDecimal128> = {\n        <arrow::GenericBasicDecimal<arrow::BasicDecimal128, 128, 2>> = {\n          static kHighWordIndex = <optimized out>,\n          static kBitWidth = 128,\n          static kByteWidth = 16,\n          static LittleEndianArray = <optimized out>,\n          array_ = {\n            _M_elems = {[0] = 1234567, [1] = 0}\n          }\n        },\n        members of arrow::BasicDecimal128:\n        static kMaxPrecision = 38,\n        static kMaxScale = 38\n      }, <No data fields>}\n  }, <No data fields>}\n```\n\n----------------------------------------\n\nTITLE: Mermaid Sequence Diagram of Arrow Async Stream Flow\nDESCRIPTION: Sequence diagram showing the interaction flow between Consumer and Producer in the Arrow async streaming API, including schema handling, task processing, and cleanup.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/CDeviceDataInterface.rst#2025-04-16_snippet_8\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    Consumer->>+Producer: ArrowAsyncDeviceStreamHandler*\n    Producer-->>+Consumer: on_schema(ArrowAsyncProducer*, ArrowSchema*)\n    Consumer->>Producer: ArrowAsyncProducer->request(n)\n\n    par\n        loop up to n times\n            Producer-->>Consumer: on_next_task(ArrowAsyncTask*)\n        end\n    and for each task\n        Consumer-->>Producer: ArrowAsyncTask.extract_data(...)\n        Consumer-->>Producer: ArrowAsyncProducer->request(1)\n    end\n\n    break Optionally\n        Consumer->>-Producer: ArrowAsyncProducer->cancel()\n    end\n\n    loop possible remaining\n        Producer-->>Consumer: on_next_task(ArrowAsyncTask*)\n    end\n\n    Producer->>-Consumer: ArrowAsyncDeviceStreamHandler->release()\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Acero Benchmark Targets in CMake\nDESCRIPTION: Adds benchmark targets for Arrow Acero components when benchmarks are enabled. These benchmarks measure performance of various Acero operations including expression evaluation, filtering, projection, and joins.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/acero/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_BENCHMARKS)\n  function(add_arrow_acero_benchmark REL_BENCHMARK_NAME)\n    set(options)\n    set(one_value_args PREFIX)\n    set(multi_value_args LABELS)\n    cmake_parse_arguments(ARG\n                          \"${options}\"\n                          \"${one_value_args}\"\n                          \"${multi_value_args}\"\n                          ${ARGN})\n\n    if(ARG_PREFIX)\n      set(PREFIX ${ARG_PREFIX})\n    else()\n      set(PREFIX \"arrow-acero\")\n    endif()\n\n    if(ARG_LABELS)\n      set(LABELS ${ARG_LABELS})\n    else()\n      set(LABELS \"arrow_acero\")\n    endif()\n\n    add_arrow_benchmark(${REL_BENCHMARK_NAME}\n                        EXTRA_LINK_LIBS\n                        ${ARROW_ACERO_TEST_LINK_LIBS}\n                        PREFIX\n                        ${PREFIX}\n                        LABELS\n                        ${LABELS}\n                        ${ARG_UNPARSED_ARGUMENTS})\n  endfunction()\n\n  add_arrow_acero_benchmark(expression_benchmark SOURCES expression_benchmark.cc)\n\n  add_arrow_acero_benchmark(filter_benchmark SOURCES benchmark_util.cc\n                            filter_benchmark.cc)\n\n  add_arrow_acero_benchmark(project_benchmark SOURCES benchmark_util.cc\n                            project_benchmark.cc)\n\n  add_arrow_acero_benchmark(asof_join_benchmark SOURCES asof_join_benchmark.cc)\n\n  add_arrow_acero_benchmark(tpch_benchmark SOURCES tpch_benchmark.cc)\n\n  add_arrow_acero_benchmark(aggregate_benchmark SOURCES aggregate_benchmark.cc)\n\n  add_arrow_acero_benchmark(hash_join_benchmark SOURCES hash_join_benchmark.cc)\n\n  if(ARROW_BUILD_STATIC)\n    target_link_libraries(arrow-acero-expression-benchmark PUBLIC arrow_acero_static)\n    target_link_libraries(arrow-acero-filter-benchmark PUBLIC arrow_acero_static)\n    target_link_libraries(arrow-acero-project-benchmark PUBLIC arrow_acero_static)\n    target_link_libraries(arrow-acero-asof-join-benchmark PUBLIC arrow_acero_static)\n    target_link_libraries(arrow-acero-tpch-benchmark PUBLIC arrow_acero_static)\n    target_link_libraries(arrow-acero-hash-join-benchmark PUBLIC arrow_acero_static)\n  else()\n    target_link_libraries(arrow-acero-expression-benchmark PUBLIC arrow_acero_shared)\n    target_link_libraries(arrow-acero-filter-benchmark PUBLIC arrow_acero_shared)\n    target_link_libraries(arrow-acero-project-benchmark PUBLIC arrow_acero_shared)\n    target_link_libraries(arrow-acero-asof-join-benchmark PUBLIC arrow_acero_shared)\n    target_link_libraries(arrow-acero-tpch-benchmark PUBLIC arrow_acero_shared)\n    target_link_libraries(arrow-acero-hash-join-benchmark PUBLIC arrow_acero_shared)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing ASV from Git Repository\nDESCRIPTION: Command to install the latest development version of ASV (Airspeed Velocity) benchmarking tool from GitHub using pip.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/benchmarks.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/airspeed-velocity/asv\n```\n\n----------------------------------------\n\nTITLE: Defining Skyhook Source Files\nDESCRIPTION: Sets up the source file lists for both client and CLS (Ceph Loadable Storage) components.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/skyhook/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_SKYHOOK_CLIENT_SOURCES client/file_skyhook.cc protocol/rados_protocol.cc\n                                 protocol/skyhook_protocol.cc)\nset(ARROW_SKYHOOK_CLS_SOURCES cls/cls_skyhook.cc protocol/rados_protocol.cc\n                              protocol/skyhook_protocol.cc)\n```\n\n----------------------------------------\n\nTITLE: Updating Documentation\nDESCRIPTION: This script uploads the generated documentation to the Arrow website. It requires a fork of the arrow-site repository.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_18\n\nLANGUAGE: Bash\nCODE:\n```\ndev/release/post-10-docs.sh X.Y.Z PREVIOUS_X.PREVIOUS_Y.PREVIOUS_Z\n```\n\n----------------------------------------\n\nTITLE: RapidJSON UBSAN Suppression Rule\nDESCRIPTION: Suppression rule for known issues in RapidJSON internal implementation, referencing issue #1724 in the RapidJSON repository.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/build-support/sanitizer-disallowed-entries.txt#2025-04-16_snippet_1\n\nLANGUAGE: config\nCODE:\n```\nsrc:*/rapidjson/internal/*\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow Python with CMake (Deprecated)\nDESCRIPTION: This snippet shows how to enable Arrow Python. This option is deprecated since 10.0.0 and will be removed in a future release. CMake presets should be used instead, or the individual components can be enabled directly.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_PYTHON=ON\"\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Acero Test Function in CMake\nDESCRIPTION: Creates a function to add Arrow Acero unit tests with proper linking and labels. The function handles test configuration with appropriate prefixes, labels, and dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/acero/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(add_arrow_acero_test REL_TEST_NAME)\n  set(options)\n  set(one_value_args PREFIX)\n  set(multi_value_args LABELS)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n\n  if(ARG_PREFIX)\n    set(PREFIX ${ARG_PREFIX})\n  else()\n    set(PREFIX \"arrow-acero\")\n  endif()\n\n  if(ARG_LABELS)\n    set(LABELS ${ARG_LABELS})\n  else()\n    set(LABELS \"arrow_acero\")\n  endif()\n\n  add_arrow_test(${REL_TEST_NAME}\n                 EXTRA_LINK_LIBS\n                 ${ARROW_ACERO_TEST_LINK_LIBS}\n                 PREFIX\n                 ${PREFIX}\n                 LABELS\n                 ${LABELS}\n                 ${ARG_UNPARSED_ARGUMENTS})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment in HTML\nDESCRIPTION: Standard Apache License 2.0 header comment block used to specify the licensing terms for the documentation file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/c_glib/arrow-flight-glib/index.md#2025-04-16_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Configuring Tests for Precompiled Gandiva Components\nDESCRIPTION: Sets up testing for the precompiled components using the add_gandiva_test CMake function. Includes various test files, source files, additional include directories, and link libraries required for testing the precompiled functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/precompiled/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_gandiva_test(precompiled-test\n                 SOURCES\n                 ../context_helper.cc\n                 bitmap_test.cc\n                 bitmap.cc\n                 epoch_time_point_test.cc\n                 time_test.cc\n                 time.cc\n                 timestamp_arithmetic.cc\n                 ../cast_time.cc\n                 ../../arrow/vendored/datetime.cpp\n                 hash_test.cc\n                 hash.cc\n                 string_ops_test.cc\n                 string_ops.cc\n                 arithmetic_ops_test.cc\n                 arithmetic_ops.cc\n                 extended_math_ops_test.cc\n                 extended_math_ops.cc\n                 decimal_ops_test.cc\n                 decimal_ops.cc\n                 ../decimal_type_util.cc\n                 ../decimal_xlarge.cc\n                 EXTRA_INCLUDES\n                 ${CMAKE_SOURCE_DIR}/src\n                 EXTRA_LINK_LIBS\n                 Boost::headers\n                 DEFINITIONS\n                 GANDIVA_UNIT_TEST=1\n                 ARROW_STATIC\n                 GANDIVA_STATIC)\n```\n\n----------------------------------------\n\nTITLE: Documenting Arrow TimeUnit Enum in C++\nDESCRIPTION: This snippet documents the arrow::TimeUnit::type enum, which represents the various time units used in temporal data types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/datatype.rst#2025-04-16_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n.. doxygenenum:: arrow::TimeUnit::type\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Apache Arrow API\nDESCRIPTION: Sphinx documentation configuration that defines the structure and organization of Apache Arrow's API reference documentation. It uses toctree directive to create a hierarchical documentation structure covering all major components of Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   api/datatypes\n   api/arrays\n   api/memory\n   api/tables\n   api/compute\n   api/acero\n   api/substrait\n   api/files\n   api/ipc\n   api/flight\n   api/formats\n   api/filesystems\n   api/dataset\n   api/cuda\n   api/misc\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: Lists required Python packages and their versions. Includes click for CLI interfaces, PyGitHub for GitHub API access, python-dotenv for environment variable management, and ruamel.yaml for YAML processing.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/archery/requirements.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nclick\npygithub>=1.59.0\npython-dotenv\nruamel.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Compiler Flags for PyArrow\nDESCRIPTION: Sets up compiler flags for building PyArrow, including warning levels, SIMD optimization, and platform-specific flags.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(BUILD_WARNING_LEVEL \"PRODUCTION\")\n\nif(NOT DEFINED ARROW_SIMD_LEVEL)\n  set(ARROW_SIMD_LEVEL\n      \"DEFAULT\"\n      CACHE STRING \"Compile time SIMD optimization level\")\nendif()\nif(NOT DEFINED ARROW_RUNTIME_SIMD_LEVEL)\n  set(ARROW_RUNTIME_SIMD_LEVEL\n      \"MAX\"\n      CACHE STRING \"Max runtime SIMD optimization level\")\nendif()\ninclude(SetupCxxFlags)\n\nset(CMAKE_CXX_FLAGS \"${CXX_COMMON_FLAGS} ${CMAKE_CXX_FLAGS}\")\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${PYARROW_CXXFLAGS}\")\n\nif(MSVC)\n  string(APPEND CMAKE_CXX_FLAGS \" /wd4190 /wd4293 /wd4800 /wd4551\")\nelse()\n  string(APPEND CMAKE_CXX_FLAGS \" -fno-omit-frame-pointer -Wno-unused-variable -Wno-maybe-uninitialized\")\n  if(CMAKE_CXX_COMPILER_ID STREQUAL \"AppleClang\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n    string(APPEND CMAKE_CXX_FLAGS \" -Wno-parentheses-equality -Wno-constant-logical-operand -Wno-missing-declarations -Wno-sometimes-uninitialized -Wno-return-type-c-linkage\")\n  endif()\nendif()\n\nset(CMAKE_C_FLAGS \"${CMAKE_CXX_FLAGS}\")\nset(CMAKE_CXX_FLAGS \"${CXX_ONLY_FLAGS} ${CMAKE_CXX_FLAGS}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Main Function for Arrow Dataset Example in C++\nDESCRIPTION: This snippet defines the main function for the Arrow dataset example. It calls RunMain() and handles any errors that may occur during execution.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nint main(int argc, char** argv) {\n  arrow::Status st = RunMain(argc, argv);\n  if (!st.ok()) {\n    std::cerr << st << std::endl;\n    return 1;\n  }\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow timing tests with CMake\nDESCRIPTION: This snippet shows how to enable unit tests that rely on wall-clock timing. This flag is disabled on CI because it can make test results flaky.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_48\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_ENABLE_TIMING_TESTS=ON\"\n```\n\n----------------------------------------\n\nTITLE: Dataset Scanning Documentation Directive\nDESCRIPTION: Sphinx/Doxygen directive for documenting dataset scanning functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/dataset.rst#2025-04-16_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. doxygengroup:: dataset-scanning\n   :content-only:\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Installing and Configuring Conbench\nDESCRIPTION: This snippet installs conbench, sets up the benchmarks repository, and configures conbench credentials.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ursacomputing/conbench.git\npushd conbench\npip install -r requirements-cli.txt\npip install -U PyYAML\npython setup.py install\npopd\n\ngit clone https://github.com/ursacomputing/benchmarks.git\npushd benchmarks\npython setup.py develop\npopd\n\npushd benchmarks\ntouch .conbench\necho \"url: $CONBENCH_URL\" >> .conbench\necho \"email: $CONBENCH_EMAIL\" >> .conbench\necho \"password: $CONBENCH_PASSWORD\" >> .conbench\necho \"host_name: $MACHINE\" >> .conbench\npopd\n```\n\n----------------------------------------\n\nTITLE: Installing macOS Dependencies with Homebrew\nDESCRIPTION: Updates Homebrew and installs all dependencies required for building Arrow C++ on macOS systems.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n$ brew update && brew bundle --file=arrow/cpp/Brewfile\n```\n\n----------------------------------------\n\nTITLE: VCTRS Extension Type Error Message in R\nDESCRIPTION: Error message displayed when trying to create a vctrs extension type with an invalid input. The message shows that the extension_array parameter must be a ChunkedArray or ExtensionArray, but received a character type instead.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/extension.md#2025-04-16_snippet_1\n\nLANGUAGE: r\nCODE:\n```\n`extension_array` must be a ChunkedArray or ExtensionArray\ni Got object of type character\n```\n\n----------------------------------------\n\nTITLE: Executing Arrow PR Merge Script in Bash\nDESCRIPTION: This command runs the Arrow pull request merge script, which creates a Python virtual environment and installs necessary dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndev/merge_arrow_pr.sh\n```\n\n----------------------------------------\n\nTITLE: Adding Client Subdirectory in Skyhook CMake Configuration\nDESCRIPTION: Adds the client subdirectory to the build process.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/skyhook/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(client)\n```\n\n----------------------------------------\n\nTITLE: Fixing Include Paths in URI Parser Files\nDESCRIPTION: A sed command used to modify include paths in vendored uriparser source files, replacing angle bracket includes with quoted includes.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/uriparser/README.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsed -E -i 's:include <uriparser/(.*).h>:include \"\\1.h\":g' src/arrow/vendored/uriparser/*\n```\n\n----------------------------------------\n\nTITLE: Configuring ORC Test Link Libraries\nDESCRIPTION: Sets up static or shared link libraries for ORC tests based on build configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/adapters/orc/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARROW_BUILD_STATIC)\n  set(ARROW_ORC_STATIC_LINK_LIBS ${ARROW_TEST_STATIC_LINK_LIBS})\nelse()\n  set(ARROW_ORC_STATIC_LINK_LIBS ${ARROW_TEST_SHARED_LINK_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Testing Dependencies\nDESCRIPTION: Lists required Python packages for testing: pytest for test execution and responses for HTTP request mocking\nSOURCE: https://github.com/apache/arrow/blob/main/dev/archery/requirements-test.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npytest\nresponses\n```\n\n----------------------------------------\n\nTITLE: Installing Archery Release Subpackage\nDESCRIPTION: Command to install the release subpackage of Archery, which provides release-related helpers.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/archery/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \"arrow/dev/archery[release]\"\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow CSV Test Target in CMake\nDESCRIPTION: Adds a test target for the CSV module using the previously defined test sources. This creates a test executable that can be run to validate the CSV functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/csv/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_test(csv-test SOURCES ${CSV_TEST_SRCS})\n```\n\n----------------------------------------\n\nTITLE: Installing and Activating Emscripten SDK\nDESCRIPTION: Commands to clone, install and activate the Emscripten SDK (emsdk) with a specific version requirement for compatibility with tools like Pyodide.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/emscripten.rst#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/emscripten-core/emsdk.git\ncd emsdk\n# replace <version> with the desired EMSDK version.\n# e.g. for Pyodide 0.26, you need EMSDK version 3.1.58\n# the versions can be found in the Makefile.envs file in the Pyodide repo:\n# https://github.com/pyodide/pyodide/blob/10b484cfe427e076c929a55dc35cfff01ea8d3bc/Makefile.envs\n./emsdk install <version>\n./emsdk activate <version>\nsource ./emsdk_env.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow JSON Tests in CMake\nDESCRIPTION: Sets up a test target for Arrow JSON functionality. It specifies source files, prefix, and additional link libraries including RapidJSON.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/json/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_test(test\n               SOURCES\n               chunked_builder_test.cc\n               chunker_test.cc\n               converter_test.cc\n               parser_test.cc\n               reader_test.cc\n               PREFIX\n               \"arrow-json\"\n               EXTRA_LINK_LIBS\n               RapidJSON)\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment in HTML\nDESCRIPTION: Standard Apache License 2.0 header comment block for Apache Software Foundation (ASF) projects, indicating terms of use and distribution.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/c_glib/gandiva-glib/index.md#2025-04-16_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Dataset Package Dependencies\nDESCRIPTION: Sets up the primary dependencies for the Arrow Dataset module, including conditional inclusion of Parquet dependencies if ARROW_PARQUET is enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/dataset/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_DATASET_PKG_CONFIG_REQUIRES \"arrow-acero\")\nset(ARROW_DATASET_REQUIRED_DEPENDENCIES Arrow ArrowAcero)\nif(ARROW_PARQUET)\n  string(APPEND ARROW_DATASET_PKG_CONFIG_REQUIRES \" parquet\")\n  list(APPEND ARROW_DATASET_REQUIRED_DEPENDENCIES Parquet)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow Tests in CMake\nDESCRIPTION: Adds two test targets to the Arrow build system - concatenate_test and diff_test using CMake's test configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/array/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_test(concatenate_test)\nadd_arrow_test(diff_test)\n```\n\n----------------------------------------\n\nTITLE: Installing Red Parquet Ruby Gem\nDESCRIPTION: Command to install the Red Parquet Ruby gem after installing Apache Parquet GLib. This is executed in a terminal to set up the Ruby bindings for Apache Parquet.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-parquet/README.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n% gem install red-parquet\n```\n\n----------------------------------------\n\nTITLE: Uploading C# Packages to NuGet\nDESCRIPTION: This script uploads C# packages to NuGet using an API key. It requires the .NET Core SDK to be installed.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_13\n\nLANGUAGE: Bash\nCODE:\n```\nNUGET_API_KEY=<your NuGet API key> dev/release/post-08-csharp.sh X.Y.Z\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Linking Tests for Gandiva in CMake\nDESCRIPTION: Conditionally adds a static linking test for the Gandiva projector and a micro benchmark. This configuration is only applied if ARROW_BUILD_STATIC is set to true. It creates two targets: 'projector_test_static' and 'micro_benchmarks'.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/tests/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_STATIC)\n  add_gandiva_test(projector_test_static\n                   SOURCES\n                   projector_test.cc\n                   test_util.cc\n                   USE_STATIC_LINKING)\n  add_arrow_benchmark(micro_benchmarks\n                      SOURCES\n                      micro_benchmarks.cc\n                      test_util.cc\n                      PREFIX\n                      \"gandiva\"\n                      EXTRA_LINK_LIBS\n                      gandiva_static)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Specific .rpm Packages for Supported Platforms\nDESCRIPTION: This command builds .rpm packages for specific target platforms using the YUM_TARGETS environment variable.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/tasks/linux-packages/README.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd dev/tasks/linux-packages/apache-arrow\nrake yum:build YUM_TARGETS=almalinux-9,amazon-linux-2023\n```\n\n----------------------------------------\n\nTITLE: Listing Available Docker Images with Archery\nDESCRIPTION: Command to list all Docker images available for Arrow CI builds using the Archery tool.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\narchery docker images\n```\n\n----------------------------------------\n\nTITLE: Benchmark Targets Configuration\nDESCRIPTION: Sets up various benchmark targets for performance testing of utility components including UTF8 utilities with optional XSIMD support.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/util/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_benchmark(bit_block_counter_benchmark)\nadd_arrow_benchmark(bit_util_benchmark)\nadd_arrow_benchmark(bitmap_reader_benchmark)\nadd_arrow_benchmark(cache_benchmark)\nadd_arrow_benchmark(compression_benchmark)\nadd_arrow_benchmark(decimal_benchmark)\nadd_arrow_benchmark(hashing_benchmark)\nadd_arrow_benchmark(int_util_benchmark)\nadd_arrow_benchmark(machine_benchmark)\nadd_arrow_benchmark(queue_benchmark)\nadd_arrow_benchmark(range_benchmark)\nadd_arrow_benchmark(small_vector_benchmark)\nadd_arrow_benchmark(tdigest_benchmark)\nadd_arrow_benchmark(thread_pool_benchmark)\nadd_arrow_benchmark(trie_benchmark)\nset(ARROW_BENCHMARK_UTF8_UTIL_LINK_LIBS)\nif(ARROW_USE_XSIMD)\n  list(APPEND ARROW_BENCHMARK_UTF8_UTIL_LINK_LIBS ${ARROW_XSIMD})\nendif()\nadd_arrow_benchmark(utf8_util_benchmark EXTRA_LINK_LIBS\n                    ${ARROW_BENCHMARK_UTF8_UTIL_LINK_LIBS})\nadd_arrow_benchmark(value_parsing_benchmark)\n```\n\n----------------------------------------\n\nTITLE: Configuring Git User Information\nDESCRIPTION: Commands to set up Git global configuration for user name and email, which are required for making commits.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/step_by_step/set_up.rst#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ git config --global user.name \"Your Name\"\n$ git config --global user.email your.email@example.com\n```\n\n----------------------------------------\n\nTITLE: Upload Binary Release Artifacts\nDESCRIPTION: Script for uploading binary release artifacts to Artifactory and creating a GitHub Release\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\ndev/release/post-03-binary.sh <version> <rc number>\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow CSV Benchmarks in CMake\nDESCRIPTION: Adds benchmark targets for the CSV module, specifically for converter, parser, and writer components. These benchmarks are prefixed with \"arrow-csv\" for identification.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/csv/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_benchmark(converter_benchmark PREFIX \"arrow-csv\")\nadd_arrow_benchmark(parser_benchmark PREFIX \"arrow-csv\")\nadd_arrow_benchmark(writer_benchmark PREFIX \"arrow-csv\")\n```\n\n----------------------------------------\n\nTITLE: Setting Arrow Home Environment Variable for Conda\nDESCRIPTION: Sets the ARROW_HOME environment variable to the Conda prefix to help Arrow's build system locate dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n$ export ARROW_HOME=$CONDA_PREFIX\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow RE2 support with CMake\nDESCRIPTION: This snippet shows how to enable building with support for regular expressions using the re2 library.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_33\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_WITH_RE2=ON\"\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow Compute Kernels Testing Object Library in CMake\nDESCRIPTION: Creates an object library for common test files used in Arrow Compute kernel testing. It links the library with Google Test and Google Mock frameworks.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/kernels/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_TESTING)\n  add_library(arrow_compute_kernels_testing OBJECT test_util_internal.cc)\n  target_link_libraries(arrow_compute_kernels_testing PUBLIC ${ARROW_GTEST_GMOCK})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow ORC Adapter Test\nDESCRIPTION: Configures the ORC adapter test with appropriate link libraries and prefix.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/adapters/orc/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_test(adapter_test\n               PREFIX\n               \"arrow-orc\"\n               STATIC_LINK_LIBS\n               orc::orc\n               ${ARROW_ORC_STATIC_LINK_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Example Vote Message for Apache Arrow Release Verification\nDESCRIPTION: Example of a vote message to cast after performing release verification. This shows how to document the verification process, command used, and environment details when sending a +1 vote to the mailing list.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release_verification.rst#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n+1\n\nI've verified successfully the sources and binaries with:\n\nTEST_DEFAULT=0 TEST_SOURCE=1 dev/release/verify-release-candidate.sh 15.0.0 1\nwith:\n* Python 3.10.12\n* gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n* NVIDIA CUDA Build cuda_11.5.r11.5/compiler.30672275_0\n* openjdk version \"17.0.9\" 2023-10-17\n* ruby 3.0.2p107 (2021-07-07 revision 0db68f0233) [x86_64-linux-gnu]\n* dotnet 8.0.204\n* Ubuntu 22.04 LTS\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Header in Markdown\nDESCRIPTION: The standard Apache License 2.0 header included as a comment in the Markdown file. It outlines the terms under which the software is distributed and the location of the full license text.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/c_glib/arrow-cuda-glib/index.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Validating Arrow Package Installation Structure\nDESCRIPTION: Shell command showing the expected directory structure after installing Arrow packages in the local Maven repository.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\n$ tree ~/.m2/repository/org/apache/arrow\n.\n├── arrow-flight\n│   ├── 9.0.0.dev501\n│   │   └── arrow-flight-9.0.0.dev501.pom\n├── arrow-format\n│   ├── 9.0.0.dev501\n│   │   ├── arrow-format-9.0.0.dev501.jar\n│   │   └── arrow-format-9.0.0.dev501.pom\n├── arrow-java-root\n│   ├── 9.0.0.dev501\n│   │   └── arrow-java-root-9.0.0.dev501.pom\n├── arrow-memory\n│   ├── 9.0.0.dev501\n│   │   └── arrow-memory-9.0.0.dev501.pom\n├── arrow-memory-core\n│   ├── 9.0.0.dev501\n│   │   ├── arrow-memory-core-9.0.0.dev501.jar\n│   │   └── arrow-memory-core-9.0.0.dev501.pom\n├── arrow-memory-netty\n│   ├── 9.0.0.dev501\n│   │   ├── arrow-memory-netty-9.0.0.dev501.jar\n│   │   └── arrow-memory-netty-9.0.0.dev501.pom\n├── arrow-vector\n│   ├── 9.0.0.dev501\n│   │   ├── _remote.repositories\n│   │   ├── arrow-vector-9.0.0.dev501.jar\n│   │   └── arrow-vector-9.0.0.dev501.pom\n└── flight-core\n   ├── 9.0.0.dev501\n   │   ├── flight-core-9.0.0.dev501.jar\n   │   └── flight-core-9.0.0.dev501.pom\n```\n\n----------------------------------------\n\nTITLE: Installing Conda for Arrow Environment\nDESCRIPTION: This snippet installs Miniconda, which is used to create and manage the Arrow build environment.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install curl\ncurl -LO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsudo bash Miniconda3-latest-Linux-x86_64.sh\n```\n\n----------------------------------------\n\nTITLE: Equivalent Docker Compose Commands with No Cache\nDESCRIPTION: The Docker Compose commands generated when running Archery with the --no-cache option.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose build --no-cache conda-cpp\ndocker compose build --no-cache conda-python\ndocker compose run --rm conda-python\n```\n\n----------------------------------------\n\nTITLE: Apache License Header in RST\nDESCRIPTION: Standard Apache 2.0 license header formatted in RST documentation syntax. Contains license terms and conditions for using the software.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/async.rst#2025-04-16_snippet_0\n\nLANGUAGE: RST\nCODE:\n```\n.. Licensed to the Apache Software Foundation (ASF) under one\n.. or more contributor license agreements.  See the NOTICE file\n.. distributed with this work for additional information\n.. regarding copyright ownership.  The ASF licenses this file\n.. to you under the Apache License, Version 2.0 (the\n.. \"License\"); you may not use this file except in compliance\n.. with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n.. software distributed under the License is distributed on an\n.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n.. KIND, either express or implied.  See the License for the\n.. specific language governing permissions and limitations\n.. under the License.\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding Flight Integration Tests in CMake\nDESCRIPTION: Adds the flight_integration_test target when ARROW_BUILD_TESTS is enabled, compiling the necessary source files and linking against static Flight libraries. The test is labeled with \"arrow_flight\" for test filtering.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/integration_tests/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_TESTS)\n  add_arrow_test(flight_integration_test\n                 SOURCES\n                 flight_integration_test.cc\n                 test_integration.cc\n                 STATIC_LINK_LIBS\n                 ${ARROW_FLIGHT_INTEGRATION_TEST_LINK_LIBS}\n                 LABELS\n                 \"arrow_flight\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Binary Verification for Apache Arrow Releases on Linux/macOS\nDESCRIPTION: Commands for verifying binary packages of Apache Arrow releases. The script enables testing of various binary artifacts such as wheels, APT packages, YUM packages, and JARs.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release_verification.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# this will create and automatically clean up a temporary directory for the verification environment and will run the binary verification\nTEST_DEFAULT=0 TEST_BINARIES=1 dev/release/verify-release-candidate.sh $VERSION $RC_NUM\n\n# to verify certain binaries use the TEST_* variables as:\nTEST_DEFAULT=0 TEST_WHEELS=1 verify-release-candidate.sh $VERSION $RC_NUM  # only Wheels\nTEST_DEFAULT=0 TEST_APT=1 verify-release-candidate.sh $VERSION $RC_NUM  # only APT packages\nTEST_DEFAULT=0 TEST_YUM=1 verify-release-candidate.sh $VERSION $RC_NUM  # only YUM packages\nTEST_DEFAULT=0 TEST_JARS=1 verify-release-candidate.sh $VERSION $RC_NUM  # only JARS\n```\n\n----------------------------------------\n\nTITLE: Setting Arrow Integration Dependencies in CMake\nDESCRIPTION: Establishes dependencies between the arrow-integration target and the client and server integration test executables to ensure proper build order.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/flight/integration_tests/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_dependencies(arrow-integration flight-test-integration-client\n                 flight-test-integration-server)\n```\n\n----------------------------------------\n\nTITLE: Installing Debian/Ubuntu Development Dependencies\nDESCRIPTION: Installs the minimal set of system dependencies required for building Arrow C++ on Debian or Ubuntu Linux distributions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst#2025-04-16_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n$ sudo apt-get install build-essential ninja-build cmake python3-dev\n```\n\n----------------------------------------\n\nTITLE: Building All JNI Libraries with Maven (macOS/Linux)\nDESCRIPTION: Maven commands to build all JNI libraries except the C Data Interface library on macOS or Linux platforms.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n$ cd arrow/java\n$ export JAVA_HOME=<absolute path to your java home>\n$ java --version\n$ mvn generate-resources -Pgenerate-libs-jni-macos-linux -N\n$ ls -latr java-dist/lib\n|__ arrow_dataset_jni/\n|__ arrow_orc_jni/\n|__ gandiva_jni/\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Tests and Benchmarks in CMake for Apache Arrow\nDESCRIPTION: Sets up test and benchmark targets for the Arrow CUDA integration. It includes linking against the appropriate libraries and adding dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/gpu/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_BUILD_TESTS)\n  add_arrow_test(cuda_test STATIC_LINK_LIBS ${ARROW_CUDA_TEST_LINK_LIBS} NO_VALGRIND)\nendif()\n\nif(ARROW_BUILD_BENCHMARKS)\n  add_arrow_benchmark(cuda_benchmark\n                      PREFIX\n                      \"arrow-gpu\"\n                      EXTRA_LINK_LIBS\n                      ${ARROW_CUDA_LIBRARY})\n  add_dependencies(arrow_cuda-benchmarks arrow-gpu-cuda-benchmark)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining ADD_PARQUET_FUZZ_TARGET Function\nDESCRIPTION: CMake function for adding Parquet fuzzing targets. Configures linking against either static or shared libraries based on build configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ADD_PARQUET_FUZZ_TARGET REL_FUZZING_NAME)\n  set(options)\n  set(one_value_args PREFIX)\n  set(multi_value_args)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n\n  if(ARG_PREFIX)\n    set(PREFIX ${ARG_PREFIX})\n  else()\n    set(PREFIX \"parquet\")\n  endif()\n\n  if(ARROW_BUILD_STATIC)\n    set(LINK_LIBS parquet_static)\n  else()\n    set(LINK_LIBS parquet_shared)\n  endif()\n  add_fuzz_target(${REL_FUZZING_NAME}\n                  PREFIX\n                  ${PREFIX}\n                  LINK_LIBS\n                  ${LINK_LIBS}\n                  ${ARG_UNPARSED_ARGUMENTS})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Testing Arrow dplyr Error Handling in R\nDESCRIPTION: This code snippet tests different error scenarios for Arrow dplyr operations. It uses a tester function with a dataset and an index to trigger various error conditions.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/dplyr-eval.md#2025-04-16_snippet_0\n\nLANGUAGE: R\nCODE:\n```\ntester(ds, i)\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Build with pip in Podman Container with SELinux Support\nDESCRIPTION: Command to run a Podman container with SELinux volume labeling (Z flag) to execute the build script using pip with a virtual environment.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npodman run --rm -i -v $PWD:/io:Z -v $PWD/../../..:/arrow:Z -t arrow_fedora_minimal /io/build_venv.sh\n```\n\n----------------------------------------\n\nTITLE: Creating a New Git Branch for Development\nDESCRIPTION: Commands for updating the main branch from upstream and creating a new feature branch for development work.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ git checkout main\n$ git fetch upstream\n$ git pull --ff-only upstream main\n$ git checkout -b ARROW-14816\n```\n\n----------------------------------------\n\nTITLE: Expanded Docker Compose Commands for No Leaf Cache\nDESCRIPTION: The full sequence of Docker Compose commands that Archery executes when running with --no-leaf-cache option.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport PANDAS=upstream_devel\ndocker compose pull --ignore-pull-failures conda-cpp\ndocker compose pull --ignore-pull-failures conda-python\ndocker compose build conda-cpp\ndocker compose build conda-python\ndocker compose build --no-cache conda-python-pandas\ndocker compose run --rm conda-python-pandas\n```\n\n----------------------------------------\n\nTITLE: Setting Up Arrow IPC Fuzzing Targets in CMake\nDESCRIPTION: Configures fuzzing targets for Arrow IPC, including corpus generation executables and actual fuzz targets. These are conditionally built based on various Arrow build flags.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/ipc/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_FUZZING\n   OR (ARROW_BUILD_UTILITIES\n       AND ARROW_TESTING\n       AND ARROW_WITH_LZ4\n       AND ARROW_WITH_ZSTD\n      ))\n  add_executable(arrow-ipc-generate-fuzz-corpus generate_fuzz_corpus.cc)\n  target_link_libraries(arrow-ipc-generate-fuzz-corpus ${ARROW_UTIL_LIB}\n                        ${ARROW_TEST_LINK_LIBS})\n\n  add_executable(arrow-ipc-generate-tensor-fuzz-corpus generate_tensor_fuzz_corpus.cc)\n  target_link_libraries(arrow-ipc-generate-tensor-fuzz-corpus ${ARROW_UTIL_LIB}\n                        ${ARROW_TEST_LINK_LIBS})\nendif()\n\nadd_arrow_fuzz_target(file_fuzz PREFIX \"arrow-ipc\")\nadd_arrow_fuzz_target(stream_fuzz PREFIX \"arrow-ipc\")\nadd_arrow_fuzz_target(tensor_stream_fuzz PREFIX \"arrow-ipc\")\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Build with conda in Ubuntu Docker Container\nDESCRIPTION: Command to run the Ubuntu Docker container and execute the build script using conda. It mounts the current directory and Arrow source directory as volumes.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -t -i -v $PWD:/io -v $PWD/../../..:/arrow arrow_ubuntu_minimal /io/build_conda.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Tests\nDESCRIPTION: Sets up tests for Hadoop Distributed File System (HDFS) implementation when enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/filesystem/CMakeLists.txt#2025-04-16_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARROW_HDFS)\n  add_arrow_test(hdfs_test EXTRA_LABELS filesystem)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow Brotli compression with CMake\nDESCRIPTION: This snippet shows how to enable support for Brotli compression.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_36\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_WITH_BROTLI=ON\"\n```\n\n----------------------------------------\n\nTITLE: Importing Dataframe Interchange Protocol in PyArrow\nDESCRIPTION: This snippet shows how to import the Dataframe Interchange Protocol function from PyArrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/tables.rst#2025-04-16_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   interchange.from_dataframe\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for C++ Development\nDESCRIPTION: ReStructuredText markup defining the structure and navigation for C++ development documentation, including license header and table of contents.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   building\n   development\n   windows\n   emscripten\n   conventions\n   fuzzing\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Build with pip in Fedora Docker Container\nDESCRIPTION: Command to run the Fedora Docker container and execute the build script using pip with a virtual environment. It mounts the current directory and Arrow source directory as volumes.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -t -i -v $PWD:/io -v $PWD/../../..:/arrow arrow_fedora_minimal /io/build_venv.sh\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Boilerplate Notice Template\nDESCRIPTION: A template for the boilerplate notice that should be included in files to apply the Apache License 2.0. The template includes placeholders for copyright year and owner, along with standard license text and a reference to the license URL.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-parquet/LICENSE.txt#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Running Crossbow Help Command in Console\nDESCRIPTION: Command to display the help information for Crossbow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/crossbow.rst#2025-04-16_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ archery crossbow --help\n```\n\n----------------------------------------\n\nTITLE: Testing DuckDB Integration Error in R Arrow\nDESCRIPTION: This code tests the error handling when trying to use the `to_duckdb()` function without having the DuckDB package installed. It ensures that a clear and meaningful error message is displayed to guide the user.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/duckdb.md#2025-04-16_snippet_0\n\nLANGUAGE: R\nCODE:\n```\nCode\n  to_duckdb(ds)\nError <rlang_error>\n  Please install the `duckdb` package to pass data with `to_duckdb()`.\n```\n\n----------------------------------------\n\nTITLE: Cloning Crossbow Repository in Bash\nDESCRIPTION: Command to clone the Crossbow repository next to the Arrow repository. This is required for setting up Crossbow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/crossbow.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/<user>/crossbow crossbow\n```\n\n----------------------------------------\n\nTITLE: Generic Invalid Conversion Error Message\nDESCRIPTION: Simple error message indicating an invalid conversion attempt to Arrow format.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/util.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nInvalid: cannot convert\n```\n\n----------------------------------------\n\nTITLE: Running R Benchmarks with Conbench\nDESCRIPTION: This snippet builds Arrow R, installs arrowbench, and runs R benchmarks using conbench.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npushd arrow\nsource dev/conbench_envs/hooks.sh build_arrow_r\npopd\nR -e \"remotes::install_github('ursacomputing/arrowbench')\"\ncd benchmarks\nconbench dataframe-to-table ALL --iterations=3 --drop-caches=true --language=R\n```\n\n----------------------------------------\n\nTITLE: Setting Up Flight Options in PyArrow CMake\nDESCRIPTION: Configures Arrow Flight RPC system integration, handling shared libraries and import libraries for Windows builds.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nif(PYARROW_BUILD_FLIGHT)\n  if(PYARROW_BUNDLE_ARROW_CPP)\n    bundle_arrow_lib(${ARROW_FLIGHT_SHARED_LIB} SO_VERSION ${ARROW_SO_VERSION})\n    if(MSVC)\n      bundle_arrow_import_lib(${ARROW_FLIGHT_IMPORT_LIB})\n    endif()\n  endif()\n\n  set(FLIGHT_LINK_LIBS arrow_python_flight)\n  list(APPEND CYTHON_EXTENSIONS _flight)\nelse()\n  set(FLIGHT_LINK_LIBS \"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enabling Arrow JSON module with CMake\nDESCRIPTION: This snippet shows how to enable the JSON reader module by passing a boolean flag to CMake.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\n\"-DARROW_JSON=ON\"\n```\n\n----------------------------------------\n\nTITLE: Checking Git Status Before Commit\nDESCRIPTION: Git command for checking which files have been modified before committing changes to the repository.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_9\n\nLANGUAGE: console\nCODE:\n```\n$ git status\nOn branch ARROW-14816\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n   modified:   R/expression.R\n   modified:   tests/testthat/test-dplyr-funcs-datetime.R\n```\n\n----------------------------------------\n\nTITLE: Main Utility Test Configuration\nDESCRIPTION: Configures the main utility test suite with comprehensive test sources for various utility components.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/util/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_test(utility-test\n               SOURCES\n               align_util_test.cc\n               atfork_test.cc\n               byte_size_test.cc\n               byte_stream_split_test.cc\n               cache_test.cc\n               checked_cast_test.cc\n               compression_test.cc\n               decimal_test.cc\n               float16_test.cc\n               fixed_width_test.cc\n               formatting_util_test.cc\n               key_value_metadata_test.cc\n               hashing_test.cc\n               int_util_test.cc\n               ${IO_UTIL_TEST_SOURCES}\n               iterator_test.cc\n               list_util_test.cc\n               logger_test.cc\n               logging_test.cc\n               math_test.cc\n               queue_test.cc\n               range_test.cc\n               ree_util_test.cc\n               reflection_test.cc\n               rows_to_batches_test.cc\n               small_vector_test.cc\n               span_test.cc\n               stl_util_test.cc\n               string_test.cc\n               tdigest_test.cc\n               test_common.cc\n               time_test.cc\n               tracing_test.cc\n               trie_test.cc\n               uri_test.cc\n               utf8_util_test.cc\n               value_parsing_test.cc\n               EXTRA_LINK_LIBS\n               ${ARROW_UTILITY_TEST_LINK_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Defining Git Ignore Patterns for Apache Arrow Project\nDESCRIPTION: This snippet defines patterns for files that should be excluded from Git version control in the Apache Arrow project. It includes various generated files such as gRPC/Protobuf files, Rcpp exports, C++ bindings, Parquet type definitions, Python API headers, and third-party dependencies.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/build-support/lint_exclusions.txt#2025-04-16_snippet_0\n\nLANGUAGE: gitignore\nCODE:\n```\n*.grpc.fb.*\n*.pb.*\n*RcppExports.cpp*\n*_generated*\n*arrowExports.cpp*\n*parquet_types.*\n*pyarrow_api.h\n*pyarrow_lib.h\n*python/config.h\n*python/platform.h\n*thirdparty/*\n*vendored/*\n*windows_compatibility.h\n```\n\n----------------------------------------\n\nTITLE: Setting Git Rebase as Default for Pull\nDESCRIPTION: Git configuration to make rebase the default strategy when pulling changes, eliminating the need to specify the --rebase flag.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/overview.rst#2025-04-16_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[pull]\n      rebase = true\n```\n\n----------------------------------------\n\nTITLE: Installing All Archery Subpackages\nDESCRIPTION: Command to install all Archery subpackages at once, which is an alias for all the above subpackages.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/archery/README.md#2025-04-16_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \"arrow/dev/archery[all]\"\n```\n\n----------------------------------------\n\nTITLE: Displaying CI Build Status Table in Markdown\nDESCRIPTION: This Markdown snippet creates a table showing the status of various CI builds for the Apache Arrow project. It uses shields.io badges to display real-time build status from CircleCI, Travis CI, and AppVeyor.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/archery/archery/crossbow/tests/fixtures/crossbow-success-message.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Task|Status|\n|----|------|\n|docker-cpp-cmake32|[![CircleCI](https://img.shields.io/circleci/build/github/{repo}/{branch}-circle-docker-cpp-cmake32.svg)](https://github.com/apache/crossbow/runs/1)|\n|wheel-osx-cp36m|[![Travis CI](https://img.shields.io/travis/{repo}/{branch}-travis-wheel-osx-cp36m.svg)](https://github.com/apache/crossbow/runs/3)|\n|wheel-osx-cp37m|[![Travis CI](https://img.shields.io/travis/{repo}/{branch}-travis-wheel-osx-cp37m.svg)](https://github.com/apache/crossbow/runs/2)|\n|wheel-win-cp36m|[![AppVeyor](https://img.shields.io/appveyor/ci/{repo}/{branch}-appveyor-wheel-win-cp36m.svg)](https://github.com/apache/crossbow/runs/4)|\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents Configuration\nDESCRIPTION: Sphinx documentation toctree directive that specifies the structure and depth of the continuous integration documentation sections.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   overview\n   docker\n   archery\n   crossbow\n```\n\n----------------------------------------\n\nTITLE: Installing Local Arrow Java Packages with Maven\nDESCRIPTION: Maven commands for installing local Arrow Java package files including memory-netty, flight, and flight-core components.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n$ mvn install:install-file -Dfile=\"$(pwd)/arrow-memory-netty-9.0.0.dev501.jar\" -DgroupId=org.apache.arrow -DartifactId=arrow-memory-netty -Dversion=9.0.0.dev501 -Dpackaging=jar\n$ mvn install:install-file -Dfile=\"$(pwd)/arrow-flight-9.0.0.dev501.pom\" -DgroupId=org.apache.arrow -DartifactId=arrow-flight -Dversion=9.0.0.dev501 -Dpackaging=pom\n$ mvn install:install-file -Dfile=\"$(pwd)/flight-core-9.0.0.dev501.pom\" -DgroupId=org.apache.arrow -DartifactId=flight-core -Dversion=9.0.0.dev501 -Dpackaging=pom\n$ mvn install:install-file -Dfile=\"$(pwd)/flight-core-9.0.0.dev501.jar\" -DgroupId=org.apache.arrow -DartifactId=flight-core -Dversion=9.0.0.dev501 -Dpackaging=jar\n```\n\n----------------------------------------\n\nTITLE: Filtering Arrow Tables with Slicers\nDESCRIPTION: Shows filtering operations on Arrow tables using the slice method with slicer concept, which provides a SQL-like filtering syntax to select rows based on column conditions.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/README.md#2025-04-16_snippet_7\n\nLANGUAGE: ruby\nCODE:\n```\ntable = Arrow::Table.new(\n  'name' => ['Tom', 'Max', 'Kate'],\n  'age' => [22, 23, 19]\n)\ntable.slice { |slicer| slicer['age'] > 19 }\n# => #<Arrow::Table:0x7fa38838c448 ptr=0x7fa3ad269f40>\n#   name\tage\n# 0\tTom \t 22\n# 1\tMax \t 23\n\ntable.slice { |slicer| slicer['age'].in?(19..22) }\n# => #<Arrow::Table:0x7fa3881cf998 ptr=0x7fa3a4bb5f30>\n#   name\tage\n# 0\tTom \t 22\n# 1\tKate\t 19\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container with Custom Command\nDESCRIPTION: Command to start an interactive bash session in a Docker container, useful for debugging.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\narchery docker run ubuntu-cpp bash\n```\n\n----------------------------------------\n\nTITLE: Installing Arrow IO Headers\nDESCRIPTION: Configures installation of all Arrow IO-related headers.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/io/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\narrow_install_all_headers(\"arrow/io\")\n```\n\n----------------------------------------\n\nTITLE: Building Java Modules with Docker Compose\nDESCRIPTION: Commands to build Arrow Java modules using Docker Compose. This method requires Docker to be installed on your system.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/building.rst#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ cd arrow/java\n$ export JAVA_HOME=<absolute path to your java home>\n$ java --version\n$ docker compose run java\n```\n\n----------------------------------------\n\nTITLE: Specialized Test Suites Configuration\nDESCRIPTION: Configures additional specialized test suites for async, bit utility, threading, and CRC32 functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/util/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_test(async-utility-test\n               SOURCES\n               async_generator_test.cc\n               async_util_test.cc\n               test_common.cc)\n\nadd_arrow_test(bit-utility-test\n               SOURCES\n               bit_block_counter_test.cc\n               bit_util_test.cc\n               rle_encoding_test.cc)\n\nadd_arrow_test(threading-utility-test\n               SOURCES\n               cancel_test.cc\n               counting_semaphore_test.cc\n               future_test.cc\n               task_group_test.cc\n               test_common.cc\n               thread_pool_test.cc)\n\nadd_arrow_test(crc32-test\n               SOURCES\n               crc32_test.cc\n               EXTRA_LINK_LIBS\n               Boost::headers)\n```\n\n----------------------------------------\n\nTITLE: Installing Red Gandiva via RubyGems\nDESCRIPTION: Command to install the Red Gandiva gem using the gem package manager.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-gandiva/README.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n% gem install red-gandiva\n```\n\n----------------------------------------\n\nTITLE: Removing Old Release Artifacts\nDESCRIPTION: This script removes old release candidate artifacts and outdated release artifacts to comply with ASF policy. It must be run by a PMC member.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_19\n\nLANGUAGE: Bash\nCODE:\n```\ndev/release/post-09-remove-old-artifacts.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Parquet Arrow Headers in CMake\nDESCRIPTION: Installs all headers for the Parquet Arrow integration using a custom CMake function.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/arrow/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\narrow_install_all_headers(\"parquet/arrow\")\n```\n\n----------------------------------------\n\nTITLE: Setting JVM Parameter for Arrow Flight SQL JDBC Driver\nDESCRIPTION: This command is required when using the Arrow Flight SQL JDBC driver to allow access to the java.nio module.  It adds the java.nio module to the list of modules that are accessible to all unnamed modules.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/flight_sql_jdbc_driver.rst#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n\"java --add-opens=java.base/java.nio=ALL-UNNAMED ...\"\n```\n\n----------------------------------------\n\nTITLE: Importing Tensor Class in PyArrow\nDESCRIPTION: This code block shows how to import the Tensor class from PyArrow for working with tensor data structures.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/python/api/tables.rst#2025-04-16_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. autosummary::\n   :toctree: ../generated/\n\n   Tensor\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow Compute Tests in CMake\nDESCRIPTION: This snippet adds various Arrow compute tests using the previously defined ADD_ARROW_COMPUTE_TEST function. It includes tests for internals, expressions, and row-related functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_test(internals_test\n               ${ARROW_COMPUTE_TEST_ARGS}\n               SOURCES\n               function_test.cc\n               exec_test.cc\n               kernel_test.cc\n               registry_test.cc\n               EXTRA_LINK_LIBS\n               arrow_compute_testing)\n\nadd_arrow_compute_test(expression_test\n                       SOURCES\n                       expression_test.cc\n                       EXTRA_LINK_LIBS\n                       arrow_compute_testing)\n\nadd_arrow_compute_test(row_test\n                       SOURCES\n                       key_hash_test.cc\n                       light_array_test.cc\n                       row/compare_test.cc\n                       row/grouper_test.cc\n                       row/row_encoder_internal_test.cc\n                       row/row_test.cc\n                       util_internal_test.cc\n                       EXTRA_LINK_LIBS\n                       arrow_compute_testing)\n```\n\n----------------------------------------\n\nTITLE: Dataset Interface Documentation Directive\nDESCRIPTION: Sphinx/Doxygen directives for documenting the core Dataset interface classes Fragment and Dataset.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/dataset.rst#2025-04-16_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. doxygenclass:: arrow::dataset::Fragment\n   :members:\n\n.. doxygenclass:: arrow::dataset::Dataset\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Boilerplate Notice Template\nDESCRIPTION: A template for the copyright notice that should be included in files to apply the Apache License 2.0. Developers should replace the bracketed fields with their specific information and include this in the appropriate comment syntax for their file format.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-cuda/LICENSE.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License 2.0 Boilerplate Notice\nDESCRIPTION: This snippet provides a template for the boilerplate notice to be included when applying the Apache License 2.0 to a software project. It includes placeholders for the copyright year and owner, as well as the full text of the license notice.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-dataset/LICENSE.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Compute Scalar Type Tests in CMake\nDESCRIPTION: Sets up test targets for various scalar type operations in Arrow Compute, including boolean, nested, and string tests. It also handles optional UTF8 processing library linking.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/compute/kernels/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(ARROW_COMPUTE_SCALAR_TYPE_TEST_LINK_LIBS arrow_compute_kernels_testing\n                                             arrow_compute_testing)\nif(ARROW_WITH_UTF8PROC)\n  list(APPEND ARROW_COMPUTE_SCALAR_TYPE_TEST_LINK_LIBS utf8proc::utf8proc)\nendif()\nadd_arrow_compute_test(scalar_type_test\n                       SOURCES\n                       scalar_boolean_test.cc\n                       scalar_nested_test.cc\n                       scalar_string_test.cc\n                       EXTRA_LINK_LIBS\n                       ${ARROW_COMPUTE_SCALAR_TYPE_TEST_LINK_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Including Arrow Compute Headers in C++\nDESCRIPTION: Includes the necessary Arrow compute headers to use compute functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <arrow/api.h>\n#include <arrow/compute/api.h>\n#include <arrow/compute/api_scalar.h>\n#include <arrow/compute/cast.h>\n#include <arrow/compute/exec/expression.h>\n#include <arrow/io/api.h>\n#include <arrow/ipc/api.h>\n#include <arrow/result.h>\n```\n\n----------------------------------------\n\nTITLE: Reading In-Memory Data with Apache Arrow Dataset API in C++\nDESCRIPTION: Explains how to wrap in-memory data into an instance of 'arrow::dataset::InMemoryDataset' using Arrow's Datasets API. This allows the data to be scanned, filtered, and manipulated using the same dataset functionality as on-disk data.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/dataset.rst#2025-04-16_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nauto table = arrow::Table::FromRecordBatches(...);\nauto dataset = std::make_shared<arrow::dataset::InMemoryDataset>(std::move(table));\nauto scanner_builder = dataset->NewScan();\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Module Paths for PyArrow\nDESCRIPTION: Configures the CMake module search paths for both local and git source tree builds of PyArrow.\nSOURCE: https://github.com/apache/arrow/blob/main/python/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(LOCAL_CMAKE_MODULES \"${CMAKE_SOURCE_DIR}/cmake_modules\")\nif(EXISTS \"${LOCAL_CMAKE_MODULES}\")\n  set(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ${LOCAL_CMAKE_MODULES})\nendif()\n\nset(CPP_CMAKE_MODULES \"${CMAKE_SOURCE_DIR}/../cpp/cmake_modules\")\nif(EXISTS \"${CPP_CMAKE_MODULES}\")\n  set(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ${CPP_CMAKE_MODULES})\nendif()\n\nif(PYARROW_CPP_HOME)\n  list(INSERT CMAKE_PREFIX_PATH 0 \"${PYARROW_CPP_HOME}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Displaying Error Message for Invalid Input in Apache Arrow IPC Stream Writing\nDESCRIPTION: This snippet shows the error message displayed when attempting to write an IPC stream with an invalid input type in Apache Arrow. The error indicates that the object must be convertible to an Arrow Table, and specifies the classes for which the conversion method is not defined.\nSOURCE: https://github.com/apache/arrow/blob/main/r/tests/testthat/_snaps/ipc-stream.md#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# write_ipc_stream errors for invalid input type\n\n    Object must be coercible to an Arrow Table using `as_arrow_table()`\n    Caused by error in `as_arrow_table()`:\n    ! No method for `as_arrow_table()` for object of class Array / ArrowDatum / ArrowObject / R6\n```\n\n----------------------------------------\n\nTITLE: Configuring File Overwrite Behavior for Dataset Writing in C++\nDESCRIPTION: This code sets the existing data handling behavior to overwrite mode, allowing the Dataset writing process to replace any existing files at the target location rather than failing when it encounters them.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/datasets_tutorial.rst#2025-04-16_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\n// Set file behavior\nwrite_options.existing_data_behavior =\n    ExistingDataBehavior::kDeleteMatchingPartitions;\n```\n\n----------------------------------------\n\nTITLE: Filtering Benchmark Suites and Tests\nDESCRIPTION: Example showing how to filter specific benchmark suites and tests using regular expressions.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/benchmarks.rst#2025-04-16_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\narchery benchmark diff \\\n  --suite-filter=compute-aggregate --benchmark-filter=Kernel \\\n  /tmp/arrow-bench*/{WORKSPACE,master}/build\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Filesystem Benchmarks\nDESCRIPTION: Sets up benchmark tests for local filesystem operations when benchmarking is enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/filesystem/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARROW_BUILD_BENCHMARKS)\n  add_arrow_benchmark(localfs_benchmark\n                      PREFIX\n                      \"arrow-filesystem\"\n                      SOURCES\n                      localfs_benchmark.cc\n                      STATIC_LINK_LIBS\n                      ${ARROW_BENCHMARK_LINK_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Gandiva Examples in CMake\nDESCRIPTION: Sets up a Gandiva example with appropriate linking libraries based on whether shared or static builds are enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/arrow/CMakeLists.txt#2025-04-16_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_GANDIVA)\n  if(ARROW_BUILD_SHARED)\n    set(GANDIVA_EXAMPLE_LINK_LIBS gandiva_shared)\n  else()\n    set(GANDIVA_EXAMPLE_LINK_LIBS gandiva_static)\n  endif()\n  add_arrow_example(gandiva_example EXTRA_LINK_LIBS ${GANDIVA_EXAMPLE_LINK_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Disabling Build Cache in Maven for Apache Arrow Java\nDESCRIPTION: This command shows how to force a Maven build without using the Develocity (formerly Maven Enterprise) cache. It's useful for seeing all warnings from tools like ErrorProne during the build process.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/development.rst#2025-04-16_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nmvn clean install -Ddevelocity.cache.local.enabled=false -Ddevelocity.cache.remote.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Exploring the min_max Function Behavior in Python Console\nDESCRIPTION: Python code demonstrating how the existing min_max function works, which will serve as a reference for implementing the new tutorial_min_max function.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow.compute as pc\n>>> data = [4, 5, 6, None, 1]\n>>> data\n[4, 5, 6, None, 1]\n>>> pc.min_max(data)\n<pyarrow.StructScalar: [('min', 1), ('max', 6)]>\n>>> pc.min_max(data, skip_nulls=False)\n<pyarrow.StructScalar: [('min', None), ('max', None)]>\n```\n\n----------------------------------------\n\nTITLE: Generating C++ API Documentation with Doxygen\nDESCRIPTION: This snippet illustrates how to generate API documentation for Apache Arrow C++ libraries using Doxygen. Ensure that Doxygen is installed before running the command. The documentation process relies on Doxygen style comments in the header files.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ndoxygen Doxyfile\n```\n\n----------------------------------------\n\nTITLE: Quiet Benchmark Comparison with Output File\nDESCRIPTION: Command to run a filtered benchmark comparison with reduced output verbosity and saving results to a file.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/benchmarks.rst#2025-04-16_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\narchery benchmark diff --benchmark-filter=Kernel --output=compare.json ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Telemetry Test Build\nDESCRIPTION: Adds a test target for telemetry component with specified source files and additional link libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/telemetry/CMakeLists.txt#2025-04-16_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_arrow_test(telemetry_test\n               SOURCES\n               telemetry_test.cc\n               EXTRA_LINK_LIBS\n               ${ARROW_TELEMETRY_TEST_LINK_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Building Arrow Skyhook Client Library\nDESCRIPTION: Configures the build for the Arrow Skyhook client library with shared and static versions, including necessary dependencies and link libraries.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/skyhook/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_lib(arrow_skyhook\n              PKG_CONFIG_NAME\n              skyhook\n              SOURCES\n              ${ARROW_SKYHOOK_CLIENT_SOURCES}\n              OUTPUTS\n              ARROW_SKYHOOK_CLIENT_LIBRARIES\n              SHARED_LINK_LIBS\n              ${ARROW_SKYHOOK_LINK_SHARED}\n              SHARED_PRIVATE_LINK_LIBS\n              arrow::flatbuffers\n              STATIC_LINK_LIBS\n              ${ARROW_SKYHOOK_LINK_STATIC}\n              arrow::flatbuffers)\n```\n\n----------------------------------------\n\nTITLE: Documenting C++ Functions with Doxygen\nDESCRIPTION: Example of Doxygen documentation format for C++ functions in Apache Arrow. Shows proper usage of docstring summary, parameter documentation, and export markers.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/conventions.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n/// \\brief Allocate a fixed size mutable buffer from a memory pool, zero its padding.\n///\n/// \\param[in] size size of buffer to allocate\n/// \\param[in] pool a memory pool\nARROW_EXPORT\nResult<std::unique_ptr<Buffer>> AllocateBuffer(const int64_t size,\n                                              MemoryPool* pool = NULLPTR);\n```\n\n----------------------------------------\n\nTITLE: Update RubyGems\nDESCRIPTION: Script for publishing Apache Arrow Ruby packages to RubyGems, including account management and version update\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\ndev/release/account-ruby.sh NEW_ACCOUNT\ndev/release/post-06-ruby.sh X.Y.Z\n```\n\n----------------------------------------\n\nTITLE: Defining Canonical Extension Tests in CMake\nDESCRIPTION: Defines a list of canonical extension test files that will be compiled and run. The initial list includes basic tests for bool8, json, and uuid extensions.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/extension/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(CANONICAL_EXTENSION_TESTS bool8_test.cc json_test.cc uuid_test.cc)\n```\n\n----------------------------------------\n\nTITLE: Adding Arrow IPC Tests in CMake\nDESCRIPTION: Adds various IPC-related tests using the custom 'add_arrow_ipc_test' function and the standard 'add_arrow_test' function. These tests cover different aspects of Arrow's IPC functionality.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/ipc/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_arrow_test(feather_test)\nadd_arrow_ipc_test(json_simple_test)\nadd_arrow_ipc_test(message_internal_test)\nadd_arrow_ipc_test(read_write_test)\nadd_arrow_ipc_test(tensor_test)\n```\n\n----------------------------------------\n\nTITLE: Creating Int8 Arrow Arrays in C++\nDESCRIPTION: This snippet demonstrates how to create Arrow Arrays for 8-bit integers using ArrayBuilder. It shows the process of appending values and finishing the array construction.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/basic_arrow.rst#2025-04-16_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\narrow::Int8Builder day_builder;\nARROW_RETURN_NOT_OK(day_builder.AppendValues(days_data, sizeof(days_data)));\n```\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<arrow::Array> day_array;\nARROW_ASSIGN_OR_RAISE(day_array, day_builder.Finish());\n```\n\n----------------------------------------\n\nTITLE: Converting Between VectorSchemaRoot and ArrowRecordBatch in Apache Arrow Java\nDESCRIPTION: Demonstrates how to convert data between VectorSchemaRoot and ArrowRecordBatch using VectorLoader and VectorUnloader.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/vector_schema_root.rst#2025-04-16_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n// create a VectorSchemaRoot root1 and convert its data into recordBatch\nVectorSchemaRoot root1 = new VectorSchemaRoot(fields, vectors);\nVectorUnloader unloader = new VectorUnloader(root1);\nArrowRecordBatch recordBatch = unloader.getRecordBatch();\n\n// create a VectorSchemaRoot root2 and load the recordBatch\nVectorSchemaRoot root2 = VectorSchemaRoot.create(root1.getSchema(), allocator);\nVectorLoader loader = new VectorLoader(root2);\nloader.load(recordBatch);\n```\n\n----------------------------------------\n\nTITLE: Update MSYS2 Packages\nDESCRIPTION: Process for updating Apache Arrow packages in the MSYS2 package repository, including forking and creating a pull request\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone git@github.com:<YOUR_GITHUB_ID>/MINGW-packages.git ../\ncd ../MINGW-packages\ngit remote add upstream https://github.com/msys2/MINGW-packages.git\ncd -\ndev/release/post-13-msys2.sh X.Y.Z <YOUR_MINGW_PACKAGES_FORK>\n```\n\n----------------------------------------\n\nTITLE: Creating Parquet Custom Targets in CMake\nDESCRIPTION: Sets up main Parquet build targets including the main library, tests, and benchmarks. Creates dependencies between these targets.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/CMakeLists.txt#2025-04-16_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(parquet-all)\nadd_custom_target(parquet)\nadd_custom_target(parquet-benchmarks)\nadd_custom_target(parquet-tests)\nadd_dependencies(parquet-all parquet parquet-tests parquet-benchmarks)\n```\n\n----------------------------------------\n\nTITLE: Main Function for Arrow Compute Example in C++\nDESCRIPTION: Defines the main function that runs the Arrow compute examples and handles errors.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nint main(int argc, char** argv) {\n  arrow::Status st = RunMain(argc, argv);\n  if (!st.ok()) {\n    std::cerr << st << std::endl;\n    return 1;\n  }\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Static Build on macOS\nDESCRIPTION: Example of setting environment variables to configure the static build script on macOS, pointing to Arrow checkout locations and build directories.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/minimal_build/README.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport ARROW_DIR=path/to/arrow-clone\nexport EXAMPLE_DIR=$ARROW_DIR/cpp/examples/minimal_build\nexport ARROW_BUILD_DIR=$(pwd)/arrow-build\nexport EXAMPLE_BUILD_DIR=$(pwd)/example\n\n./run_static.sh\n```\n\n----------------------------------------\n\nTITLE: Defining Arrow C++ Build Directory Option in Meson\nDESCRIPTION: Defines a string option for specifying an external Arrow C++ build directory. This allows building with a non-installed version of Arrow C++.\nSOURCE: https://github.com/apache/arrow/blob/main/c_glib/meson_options.txt#2025-04-16_snippet_0\n\nLANGUAGE: meson\nCODE:\n```\noption('arrow_cpp_build_dir',\n       type: 'string',\n       value: '',\n       description: 'Use this option to build with not installed Arrow C++')\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License 2.0 Boilerplate Notice\nDESCRIPTION: This snippet demonstrates the boilerplate notice to be included when applying the Apache License 2.0 to a project. It includes placeholders for the copyright year and owner's name.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-flight-sql/LICENSE.txt#2025-04-16_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Running Spark Integration Tests with Local Maven Repository\nDESCRIPTION: This command runs Spark integration tests while mapping the local Maven repository to the Docker container, saving time on dependency downloads.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/README.md#2025-04-16_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose run --rm -v $HOME/.m2:/root/.m2 conda-python-spark\n```\n\n----------------------------------------\n\nTITLE: Implementing Table Source Example in C++\nDESCRIPTION: Example showing how to use table_source node with an in-memory table as input source.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/acero/user_guide.rst#2025-04-16_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nARROW_RETURN_NOT_OK(Declaration::Sequence({\n    {\"table_source\", TableSourceNodeOptions{table, max_batch_size}},\n    {\"sink\", SinkNodeOptions{table->schema()}},\n}).AddToPlan(plan.get()));\n```\n\n----------------------------------------\n\nTITLE: Aborting Git Rebase\nDESCRIPTION: Command to abort a git rebase operation when encountering conflicts during branch synchronization.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/overview.rst#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git rebase --abort\n```\n\n----------------------------------------\n\nTITLE: Defining ADD_PARQUET_BENCHMARK Function\nDESCRIPTION: CMake function for adding Parquet benchmark targets with configurable prefix and linking options.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/parquet/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ADD_PARQUET_BENCHMARK REL_TEST_NAME)\n  set(options)\n  set(one_value_args PREFIX)\n  set(multi_value_args)\n  cmake_parse_arguments(ARG\n                        \"${options}\"\n                        \"${one_value_args}\"\n                        \"${multi_value_args}\"\n                        ${ARGN})\n  if(ARG_PREFIX)\n    set(PREFIX ${ARG_PREFIX})\n  else()\n    set(PREFIX \"parquet\")\n  endif()\n  add_benchmark(${REL_TEST_NAME}\n                PREFIX\n                ${PREFIX}\n                LABELS\n                \"parquet-benchmarks\"\n                ${PARQUET_BENCHMARK_LINK_OPTION}\n                ${ARG_UNPARSED_ARGUMENTS})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Exception Handling in Java\nDESCRIPTION: This snippet shows a basic exception handling pattern in Java, catching and printing any exceptions that occur. It's mentioned that failing to close resources properly might cause native object leakage.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/java/dataset.rst#2025-04-16_snippet_8\n\nLANGUAGE: java\nCODE:\n```\n} catch (Exception e) {\n    e.printStackTrace();\n}\n```\n\n----------------------------------------\n\nTITLE: Including Arrow Headers in C++\nDESCRIPTION: This snippet shows the necessary includes for using Arrow functionality in a C++ program. It imports the basic Arrow API and iostream for output.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/basic_arrow.rst#2025-04-16_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <arrow/api.h>\n#include <iostream>\n```\n\n----------------------------------------\n\nTITLE: Returning Status OK in Arrow Compute Example (C++)\nDESCRIPTION: Finishes the program by returning Status::OK() to indicate successful completion. This is a standard pattern in Arrow C++ examples to signal normal termination.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\nreturn arrow::Status::OK();\n```\n\n----------------------------------------\n\nTITLE: Returning Status from an Arrow File I/O Program\nDESCRIPTION: Returns an OK status to indicate successful completion of the file operations. This is used as the final step in the example program.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/io_tutorial.rst#2025-04-16_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n  return Status::OK();\n```\n\n----------------------------------------\n\nTITLE: Displaying Help for Archery's Docker Subcommand\nDESCRIPTION: Command to view help for Archery's Docker subcommand, which provides options for interacting with Docker Compose based builds.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/archery.rst#2025-04-16_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ archery docker --help\n```\n\n----------------------------------------\n\nTITLE: Running IWYU with Pattern Matching in Apache Arrow\nDESCRIPTION: Example of running Include-What-You-Use on a subset of files in the Arrow codebase by specifying a pattern match for the arrow/array directory.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n./$IWYU_SH match $PATTERN\n```\n\n----------------------------------------\n\nTITLE: Configuring Arrow Compute and CSV Examples in CMake\nDESCRIPTION: Adds a compute and CSV writing example when both Arrow Compute and CSV modules are enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/examples/arrow/CMakeLists.txt#2025-04-16_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(ARROW_COMPUTE AND ARROW_CSV)\n  add_arrow_example(compute_and_write_csv_example)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Checking and Applying Code Style with Maven Spotless in Apache Arrow Java\nDESCRIPTION: This snippet shows how to check for and fix code style issues in Apache Arrow Java using Maven Spotless plugin. It includes an example of a style violation in a Java file and how to fix it.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/development.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nThe following files had format violations:\n    src/main/java/org/apache/arrow/algorithm/rank/VectorRank.java\n        @@ -15,7 +15,6 @@\n        ·*·limitations·under·the·License.\n        ·*/\n\n        -\n        package·org.apache.arrow.algorithm.rank;\n\n        import·java.util.stream.IntStream;\nRun 'mvn spotless:apply' to fix these violations.\n```\n\n----------------------------------------\n\nTITLE: Generating Xcode Project for macOS Development\nDESCRIPTION: Commands to generate and open an Xcode project for Arrow development on macOS with debug configuration.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_58\n\nLANGUAGE: shell\nCODE:\n```\ncd cpp\nmkdir xcode-build\ncd xcode-build\ncmake .. -G Xcode -DARROW_BUILD_TESTS=ON -DCMAKE_BUILD_TYPE=DEBUG\nopen arrow.xcodeproj\n```\n\n----------------------------------------\n\nTITLE: Running Fuzz Test on Crash Data\nDESCRIPTION: Command to reproduce a crash using downloaded test case data with the arrow-ipc-file-fuzz target.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/fuzzing.rst#2025-04-16_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ build/debug/arrow-ipc-file-fuzz testcase-arrow-ipc-file-fuzz-123465\n```\n\n----------------------------------------\n\nTITLE: Building Optional Components with CMake\nDESCRIPTION: CMake configuration and build commands for selectively building and installing Parquet components and tests.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_57\n\nLANGUAGE: shell\nCODE:\n```\ncmake .. -DARROW_PARQUET=ON \\\n      -DARROW_OPTIONAL_INSTALL=ON \\\n      -DARROW_BUILD_TESTS=ON\nmake parquet\nmake install\n```\n\n----------------------------------------\n\nTITLE: Building Minimal Release Build\nDESCRIPTION: Commands to create and build a minimal release version of Arrow C++, requiring at least 1GB RAM.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n$ mkdir build-release\n$ cd build-release\n$ cmake ..\n$ make -j8       # if you have 8 CPU cores, otherwise adjust\n$ make install\n```\n\n----------------------------------------\n\nTITLE: Error Context Debug Output Example\nDESCRIPTION: Sample debug output when using ARROW_EXTRA_ERROR_CONTEXT=ON option, showing detailed error trace for type conversion failure.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/building.rst#2025-04-16_snippet_55\n\nLANGUAGE: shell\nCODE:\n```\n../src/arrow/ipc/ipc-read-write-test.cc:609: Failure\nFailed\n../src/arrow/ipc/metadata-internal.cc:508 code: TypeToFlatbuffer(fbb, *field.type(), &children, &layout, &type_enum, dictionary_memo, &type_offset)\n../src/arrow/ipc/metadata-internal.cc:598 code: FieldToFlatbuffer(fbb, *schema.field(i), dictionary_memo, &offset)\n../src/arrow/ipc/metadata-internal.cc:651 code: SchemaToFlatbuffer(fbb, schema, dictionary_memo, &fb_schema)\n../src/arrow/ipc/writer.cc:697 code: WriteSchemaMessage(schema_, dictionary_memo_, &schema_fb)\n../src/arrow/ipc/writer.cc:730 code: WriteSchema()\n../src/arrow/ipc/writer.cc:755 code: schema_writer.Write(&dictionaries_)\n../src/arrow/ipc/writer.cc:778 code: CheckStarted()\n../src/arrow/ipc/ipc-read-write-test.cc:574 code: writer->WriteRecordBatch(batch)\nNotImplemented: Unable to convert type: decimal(19, 4)\n```\n\n----------------------------------------\n\nTITLE: Generating Seed Corpus for Arrow C++ Fuzzing\nDESCRIPTION: Command to generate a seed corpus for fuzzing using the provided script. Assumes fuzzing executables are located in build/debug directory.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/fuzzing.rst#2025-04-16_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ ./build-support/fuzzing/generate_corpuses.sh build/debug\n```\n\n----------------------------------------\n\nTITLE: Adding mday() Binding to Arrow R expressions.R File\nDESCRIPTION: R code showing where to add the mday binding in the expressions.R file, mapping it to the underlying C++ day function.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_4\n\nLANGUAGE: R\nCODE:\n```\n# second is defined in dplyr-functions.R\n# wday is defined in dplyr-functions.R\n\"mday\" = \"day\",\n\"yday\" = \"day_of_year\",\n\"year\" = \"year\",\n```\n\n----------------------------------------\n\nTITLE: RunMain Function for Arrow Compute Example in C++\nDESCRIPTION: Defines the RunMain function that contains the main logic for the Arrow compute examples.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/tutorials/compute_tutorial.rst#2025-04-16_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\narrow::Status RunMain(int argc, char** argv) {\n  // Main computation logic will go here\n  return arrow::Status::OK();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Arrow Int8Type Object\nDESCRIPTION: Example showing how to create an Arrow Int8Type object using the arrow.int8 function.\nSOURCE: https://github.com/apache/arrow/blob/main/matlab/README.md#2025-04-16_snippet_11\n\nLANGUAGE: matlab\nCODE:\n```\n>> type = arrow.int8()\n\ntype =\n\n  Int8Type with properties:\n\n    ID: Int8\n```\n\n----------------------------------------\n\nTITLE: Executing a Docker Build with Archery\nDESCRIPTION: Command to run a specific Docker build (conda-python) using the Archery tool.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\narchery docker run conda-python\n```\n\n----------------------------------------\n\nTITLE: Initializing Visual Studio Environment Shell 2017\nDESCRIPTION: The script sets up a development environment shell for Visual Studio 2017. To be run every time a new shell is opened, preparing the system for building projects with the correct architecture settings.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/windows.rst#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\Tools\\VsDevCmd.bat\" -arch=amd64\n```\n\n----------------------------------------\n\nTITLE: Installing Unix-like Dependencies for Apache Arrow\nDESCRIPTION: A list of required dependencies specific to Unix-like environments (Linux and macOS) that need to be installed via Conda to build and run Apache Arrow. Includes build tools like autoconf, ccache, orc, and pkg-config.\nSOURCE: https://github.com/apache/arrow/blob/main/ci/conda_env_unix.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nautoconf\nccache\norc\npkg-config\n```\n\n----------------------------------------\n\nTITLE: Running PyArrow Build with pip using Docker Compose on Ubuntu\nDESCRIPTION: Command to run the pip-based build process for PyArrow using Docker Compose with the Ubuntu container.\nSOURCE: https://github.com/apache/arrow/blob/main/python/examples/minimal_build/README.md#2025-04-16_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose run --rm minimal-ubuntu-venv\n```\n\n----------------------------------------\n\nTITLE: Adding Extension Tests Dependency to Gandiva Tests in CMake\nDESCRIPTION: Adds the extension-tests dependency to the gandiva-tests target.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/gandiva/tests/external_functions/CMakeLists.txt#2025-04-16_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_dependencies(gandiva-tests extension-tests)\n```\n\n----------------------------------------\n\nTITLE: Setting IO Thread Pool Capacity in Apache Arrow (C++)\nDESCRIPTION: This function sets the capacity of the IO thread pool in Apache Arrow.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/api/thread.rst#2025-04-16_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\narrow::io::SetIOThreadPoolCapacity\n```\n\n----------------------------------------\n\nTITLE: Running Archery with Dry Run Option\nDESCRIPTION: Command to show the Docker Compose commands that would be executed without actually running them.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/continuous_integration/docker.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\narchery docker run --dry-run conda-python\n```\n\n----------------------------------------\n\nTITLE: Running Java Micro Benchmarks with Conbench in Apache Arrow\nDESCRIPTION: This snippet shows how to execute benchmark tests for Apache Arrow Java using Conbench. It includes parameters for specifying iterations, commit hash, Java home path, source path, and filtering specific benchmarks.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/java/development.rst#2025-04-16_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ cd benchmarks\n$ conbench java-micro --help\n$ conbench java-micro\n    --iterations=1\n    --commit=e90472e35b40f58b17d408438bb8de1641bfe6ef\n    --java-home=<absolute path to your java home>\n    --src=<absolute path to your arrow project>\n    --benchmark-filter=org.apache.arrow.adapter.AvroAdapterBenchmarks.testAvroToArrow\nBenchmark                              Mode  Cnt       Score   Error  Units\nAvroAdapterBenchmarks.testAvroToArrow  avgt       725545.783          ns/op\nTime to POST http://localhost:5000/api/login/ 0.14911699295043945\nTime to POST http://localhost:5000/api/benchmarks/ 0.06116318702697754\n```\n\n----------------------------------------\n\nTITLE: Upload Source Release Artifacts to Subversion\nDESCRIPTION: Command for a PMC member to commit source release artifacts to the Apache Subversion repository\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/release.rst#2025-04-16_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\ndev/release/post-02-upload.sh <version> <rc>\n```\n\n----------------------------------------\n\nTITLE: Defining Struct Field in Arrow Schema JSON\nDESCRIPTION: JSON representation of a Struct field in the Arrow schema. The 'children' array contains Fields with meaningful names and types.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/format/Integration.rst#2025-04-16_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"struct\"\n}\n```\n\n----------------------------------------\n\nTITLE: IWYU Pattern Match Example for Apache Arrow\nDESCRIPTION: Specific example of running Include-What-You-Use on files in the src/arrow/array directory by using the match option with a pattern.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/cpp/development.rst#2025-04-16_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n./$IWYU_SH match arrow/array\n```\n\n----------------------------------------\n\nTITLE: R Data Frame Class References\nDESCRIPTION: Examples showing the correct usage of data frame class names in R documentation.\nSOURCE: https://github.com/apache/arrow/blob/main/r/STYLE.md#2025-04-16_snippet_1\n\nLANGUAGE: r\nCODE:\n```\ndata.frame\n```\n\nLANGUAGE: r\nCODE:\n```\ntibble\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment and Building Arrow C++ and Python\nDESCRIPTION: This snippet clones the Arrow repository, checks out the specified commit, and uses a hook to create a Conda environment and build Arrow C++ and Python.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone \"${ARROW_REPO}\"\npushd arrow\ngit fetch -v --prune -- origin \"${BENCHMARKABLE}\"\ngit checkout -f \"${BENCHMARKABLE}\"\nsource dev/conbench_envs/hooks.sh create_conda_env_with_arrow_python\npopd\n```\n\n----------------------------------------\n\nTITLE: Building .rpm Packages for All Supported Platforms\nDESCRIPTION: This command builds .rpm packages for all supported platforms using a Rake task.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/tasks/linux-packages/README.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd dev/tasks/linux-packages/apache-arrow\nrake yum:build\n```\n\n----------------------------------------\n\nTITLE: Building Package News Page with pkgdown\nDESCRIPTION: R command to rebuild the news page using pkgdown, which generates HTML documentation from the package NEWS file.\nSOURCE: https://github.com/apache/arrow/blob/main/r/PACKAGING.md#2025-04-16_snippet_6\n\nLANGUAGE: r\nCODE:\n```\npkgdown::build_news()\n```\n\n----------------------------------------\n\nTITLE: Installing Java Dependencies for Arrow\nDESCRIPTION: This snippet installs Java and Maven, which are required for building and running Arrow Java benchmarks.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/conbench_envs/README.md#2025-04-16_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo su\napt-get install openjdk-11-jdk\napt-get install maven\n```\n\n----------------------------------------\n\nTITLE: Creating a Git Commit - Console\nDESCRIPTION: This snippet demonstrates how to commit changes to a Git repository after making modifications to the codebase. It showcases the command to save changes and include a message that describes the changes made, which is useful for tracking project history.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/r_tutorial.rst#2025-04-16_snippet_13\n\nLANGUAGE: console\nCODE:\n```\n$ git commit -am \"Adding a binding and a test for mday() lubridate\"\n\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Arrow Tutorials in RST\nDESCRIPTION: Sets up a table of contents for Apache Arrow tutorials, including links to Python and R specific tutorials. The directive configures a maximum depth of 1 for the table of contents.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/index.rst#2025-04-16_snippet_0\n\nLANGUAGE: RST\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   python_tutorial\n   r_tutorial\n```\n\n----------------------------------------\n\nTITLE: Building and Running Arrow HDFS Integration Tests\nDESCRIPTION: These commands build and run Docker containers for testing Arrow's C++ and Python HDFS support.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/README.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose build conda-cpp\ndocker compose build conda-python\ndocker compose build conda-python-hdfs\ndocker compose run --rm conda-python-hdfs\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment Block in Markdown\nDESCRIPTION: Standard Apache 2.0 license header formatted as an HTML comment in Markdown, indicating the licensing terms for the Apache Arrow project.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/array/README.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!---\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Checking Git Status and Differences\nDESCRIPTION: Shows how to use `git status` to check which files have changed and `git diff` to view specific line differences in modified files. This helps ensure only the correct files and changes are committed.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/developers/guide/tutorials/python_tutorial.rst#2025-04-16_snippet_11\n\nLANGUAGE: console\nCODE:\n```\n$ git status\nOn branch ARROW-14977\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n   modified:   python/pyarrow/compute.py\n   modified:   python/pyarrow/tests/test_compute.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n```\n\nLANGUAGE: console\nCODE:\n```\n$ git diff\ndiff --git a/python/pyarrow/compute.py b/python/pyarrow/compute.py\nindex 9dac606c3..e8fc775d8 100644\n--- a/python/pyarrow/compute.py\n+++ b/python/pyarrow/compute.py\n@@ -774,3 +774,45 @@ def bottom_k_unstable(values, k, sort_keys=None, *, memory_pool=None):\n         sort_keys = map(lambda key_name: (key_name, \"ascending\"), sort_keys)\n     options = SelectKOptions(k, sort_keys)\n     return call_function(\"select_k_unstable\", [values], options, memory_pool)\n+\n+\ndef tutorial_min_max(values, skip_nulls=True):\n    \"\"\"\n    Compute the minimum-1 and maximum-1 values of a numeric array.\n\n    This is a made-up feature for the tutorial purposes.\n\n    Parameters\n    ----------\n    values : Array\n    skip_nulls : bool, default True\n        If True, ignore nulls in the input.\n\n    Returns\n    -------\n    result : StructScalar of min-1 and max+1\n\n    Examples\n    --------\n    >>> import pyarrow.compute as pc\n    >>> data = [4, 5, 6, None, 1]\n    >>> pc.tutorial_min_max(data)\n    <pyarrow.StructScalar: [('min-', 0), ('max+', 7)]>\n    \"\"\"\n\n    options = ScalarAggregateOptions(skip_nulls=skip_nulls)\n    min_max = call_function(\"min_max\", [values], options)\n\n```\n\n----------------------------------------\n\nTITLE: Generating Example Dataset\nDESCRIPTION: This snippet installs required Python packages, generates a dataset, and copies it to the Ceph filesystem.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/cpp/examples/dataset_skyhook_scan_example.rst#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install pandas pyarrow\npython3 ../../ci/scripts/generate_dataset.py\ncp -r nyc /mnt/cephfs/\n```\n\n----------------------------------------\n\nTITLE: MIT License Declaration in HTML Comments\nDESCRIPTION: HTML comment block containing the MIT license text and copyright notice for software usage and distribution terms.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/vendored/musl/README.md#2025-04-16_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\nCopyright © 2005-2020 Rich Felker, et al.\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n-->\n```\n\n----------------------------------------\n\nTITLE: Installing Red Arrow CUDA with Ruby Gems\nDESCRIPTION: Command to install the Red Arrow CUDA Ruby gem using the gem package manager.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-cuda/README.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n% gem install red-arrow-cuda\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Test Integration\nDESCRIPTION: Conditionally adds HDFS test target with additional dependencies including Boost libraries when ARROW_HDFS is enabled.\nSOURCE: https://github.com/apache/arrow/blob/main/cpp/src/arrow/io/CMakeLists.txt#2025-04-16_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARROW_HDFS)\n  add_arrow_test(hdfs_test\n                 NO_VALGRIND\n                 PREFIX\n                 \"arrow-io\"\n                 EXTRA_LINK_LIBS\n                 arrow::hadoop\n                 Boost::filesystem\n                 Boost::system)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Debugging .rpm Package Build Process\nDESCRIPTION: These commands open a console for debugging the .rpm package build process, allowing interactive exploration and manual build execution.\nSOURCE: https://github.com/apache/arrow/blob/main/dev/tasks/linux-packages/README.md#2025-04-16_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd dev/tasks/linux-packages/apache-arrow\nrake yum:build:console YUM_TARGETS=almalinux-9\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment Block\nDESCRIPTION: Standard Apache 2.0 license header included as an HTML comment in markdown format.\nSOURCE: https://github.com/apache/arrow/blob/main/r/NEWS.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!---\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment in HTML\nDESCRIPTION: Standard Apache License 2.0 header comment block specifying terms of use and distribution for the source code.\nSOURCE: https://github.com/apache/arrow/blob/main/docs/source/c_glib/arrow-flight-sql-glib/index.md#2025-04-16_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Boilerplate Notice\nDESCRIPTION: Standard boilerplate text to be included in project files to apply the Apache License 2.0. It includes placeholders for copyright year and owner information, along with the standard Apache License 2.0 permissions notice.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow/LICENSE.txt#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Boilerplate Notice Template\nDESCRIPTION: Standard copyright and license notice template to be included in project files. The template includes placeholders for copyright year and owner information, along with the standard Apache 2.0 license text and conditions.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-gandiva/LICENSE.txt#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Boilerplate Notice Template\nDESCRIPTION: Standard copyright and license notice template to be included in project files. Includes placeholders for copyright year and owner information that should be customized per project.\nSOURCE: https://github.com/apache/arrow/blob/main/ruby/red-arrow-flight/LICENSE.txt#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```"
  }
]
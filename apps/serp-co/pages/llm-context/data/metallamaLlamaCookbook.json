[
  {
    "owner": "meta-llama",
    "repo": "llama-cookbook",
    "content": "TITLE: Integrating LangChain with Custom Functions for Stock Analysis in Python\nDESCRIPTION: This function integrates LangChain with custom stock analysis tools. It processes user prompts, calls appropriate functions based on LLM output, handles historical price data, and generates visualizations when requested. It uses the current date for context and manages message flow between the user, LLM, and tools.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/llama3-stock-market-function-calling/llama3-stock-market-function-calling.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.messages import AIMessage, SystemMessage, HumanMessage, ToolMessage\nfrom datetime import date\n\ndef call_functions(llm_with_tools, user_prompt):\n    system_prompt = 'You are a helpful finance assistant that analyzes stocks and stock prices. Today is {today}'.format(today = date.today())\n    \n    messages = [SystemMessage(system_prompt), HumanMessage(user_prompt)]\n    ai_msg = llm_with_tools.invoke(messages)\n    messages.append(ai_msg)\n    historical_price_dfs = []\n    symbols = []\n    for tool_call in ai_msg.tool_calls:\n        selected_tool = {\"get_stock_info\": get_stock_info, \"get_historical_price\": get_historical_price}[tool_call[\"name\"].lower()]\n        tool_output = selected_tool.invoke(tool_call[\"args\"])\n        if tool_call['name'] == 'get_historical_price':\n            historical_price_dfs.append(tool_output)\n            symbols.append(tool_output.columns[1])\n        else:\n            messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n    \n    if len(historical_price_dfs) > 0:\n        plot_price_over_time(historical_price_dfs)\n        symbols = ' and '.join(symbols)\n        messages.append(ToolMessage('Tell the user that a historical stock price chart for {symbols} been generated.'.format(symbols=symbols), tool_call_id=0))\n\n    return llm_with_tools.invoke(messages).content\n\n```\n\n----------------------------------------\n\nTITLE: Testing Function Calling with Llama 3 for Stock Queries\nDESCRIPTION: Demonstrate the use of the Llama 3 model with bound tools to answer stock-related queries, including current stock information and historical price comparisons.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/llama3-stock-market-function-calling/llama3-stock-market-function-calling.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nquery1 = 'What is the market cap of Meta?'\nquery2 = 'How does the volume of Apple compare to that of Microsoft?'\n\nprint(llm_with_tools.invoke(query1).tool_calls)\nprint(llm_with_tools.invoke(query2).tool_calls)\n\nquery1 = 'Show the historical price of the S&P 500 over the past 3 years? (Today is 4/23/2024)'\nquery2 = 'Compare the price of Google and Amazon throughout 2023'\n\nprint(llm_with_tools.invoke(query1).tool_calls)\nprint(llm_with_tools.invoke(query2).tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Building Gradio UI for Chatbot\nDESCRIPTION: This extensive code block defines the Gradio UI for the chatbot. It includes the layout, input components, model selection, hyperparameter adjustments, and handlers for user interactions and bot responses.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    #Configure UI layout\n    chatbot = gr.Chatbot(height = 600)\n    with gr.Row():\n        with gr.Column(scale=1):\n            with gr.Row():\n                #model selection\n                model_selector = gr.Dropdown(\n                    list(model_dict.keys()), \n                    value=\"7b-chat\", \n                    label=\"Model\", \n                    info=\"Select the model\", \n                    interactive = True, \n                    scale=1\n                )\n                max_new_tokens_selector = gr.Number(\n                    value=512, \n                    precision=0, \n                    label=\"Max new tokens\", \n                    info=\"Adjust max_new_tokens\",\n                    interactive = True, \n                    minimum=1, \n                    maximum=1024, \n                    scale=1\n                )\n            with gr.Row():\n                #hyperparameter selection\n                temperature_selector = gr.Slider(\n                    value=0.6, \n                    label=\"Temperature\", \n                    info=\"Range 0-2. Controls the creativity of the generated text.\",\n                    interactive = True, \n                    minimum=0.01, \n                    maximum=2, \n                    step=0.01, \n                    scale=1\n                )\n                top_p_selector = gr.Slider(\n                    value=0.9, \n                    label=\"Top_p\", \n                    info=\"Range 0-1. Nucleus sampling.\",\n                    interactive = True, \n                    minimum=0.01, \n                    maximum=0.99, \n                    step=0.01, \n                    scale=1\n                )\n        with gr.Column(scale=2):\n            #user input prompt text field\n            user_prompt_message = gr.Textbox(placeholder=\"Please add user prompt here\", label=\"User prompt\")\n            with gr.Row():\n                clear = gr.Button(\"Clear Conversation\", scale=2)\n                submitBtn = gr.Button(\"Submit\", scale=8)\n\n\n    state = gr.State([])\n\n    #handle user message\n    def user(user_prompt_message, history):\n        if user_prompt_message != \"\":\n            return history + [[user_prompt_message, None]]\n        else:\n            return history + [[\"Invalid prompts - user prompt cannot be empty\", None]]\n\n    #chatbot logic for configuration, sending the prompts, rendering the streamed back generations etc\n    def bot(model_selector, temperature_selector, top_p_selector, max_new_tokens_selector, user_prompt_message, history, messages_history):\n        dialog = []\n        bot_message = \"\"\n        history[-1][1] = \"\"\n           \n        dialog = [\n            {\"role\": \"user\", \"content\": user_prompt_message},\n        ]\n        messages_history += dialog\n        \n        #Queue for streamed character rendering\n        q = Queue()\n\n        #Update new llama hyperparameters\n        llm.inference_server_url = model_selector\n        llm.temperature = temperature_selector\n        llm.top_p = top_p_selector\n        llm.max_new_tokens = max_new_tokens_selector\n\n        #Async task for streamed chain results wired to callbacks we previously defined, so we don't block the UI\n        async def task(prompt):\n            ret = await qa_chain.run(prompt, callbacks=[MyStream(q)])\n            return ret\n\n        with start_blocking_portal() as portal:\n            portal.start_task_soon(task, user_prompt_message)\n            while True:\n                next_token = q.get(True)\n                if next_token is job_done:\n                    messages_history += [{\"role\": \"assistant\", \"content\": bot_message}]\n                    return history, messages_history\n                bot_message += next_token\n                history[-1][1] += next_token\n                yield history, messages_history\n\n    #init the chat history with default system message    \n    def init_history(messages_history):\n        messages_history = []\n        messages_history += [system_message]\n        return messages_history\n\n    #clean up the user input text field\n    def input_cleanup():\n        return \"\"\n\n    #when the user clicks Enter and the user message is submitted\n    user_prompt_message.submit(\n        user, \n        [user_prompt_message, chatbot], \n        [chatbot], \n        queue=False\n    ).then(\n        bot, \n        [model_selector, temperature_selector, top_p_selector, max_new_tokens_selector, user_prompt_message, chatbot, state], \n        [chatbot, state]\n    ).then(input_cleanup, \n        [], \n        [user_prompt_message], \n        queue=False\n    )\n\n    #when the user clicks the submit button\n    submitBtn.click(\n        user, \n        [user_prompt_message, chatbot], \n        [chatbot], \n        queue=False\n    ).then(\n        bot, \n        [model_selector, temperature_selector, top_p_selector, max_new_tokens_selector, user_prompt_message, chatbot, state], \n        [chatbot, state]\n    ).then(\n        input_cleanup, \n        [], \n        [user_prompt_message], \n        queue=False\n    )\n    \n    #when the user clicks the clear button\n    clear.click(lambda: None, None, chatbot, queue=False).success(init_history, [state], [state])\n```\n\n----------------------------------------\n\nTITLE: Implementing ConversationalRetrievalChain for Follow-up Questions\nDESCRIPTION: This code sets up a ConversationalRetrievalChain to handle follow-up questions with context from previous interactions and retrieved documents.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/RAG/hello_llama_cloud.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chains import ConversationalRetrievalChain\nchat_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)\n\nresult = chat_chain({\"question\": question, \"chat_history\": []})\nprint(result['answer'])\n\nchat_history = [(question, result[\"answer\"])]\nfollowup = \"Based on what architecture?\"\nfollowup_answer = chat_chain({\"question\": followup, \"chat_history\": chat_history})\nprint(followup_answer['answer'])\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Image Processing and ML in Python\nDESCRIPTION: Imports all necessary Python libraries for the data preparation pipeline. Includes PIL for image processing, pandas and numpy for data manipulation, visualization libraries, concurrent processing tools, and transformers for the Llama model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image as PIL_Image\nfrom PIL import Image\n\nfrom tqdm import tqdm\nfrom concurrent.futures import ProcessPoolExecutor\nimport multiprocessing\n\n\nimport torch\nfrom transformers import MllamaForConditionalGeneration, MllamaProcessor\n```\n\n----------------------------------------\n\nTITLE: Fetching Stock Information in Python\nDESCRIPTION: This function retrieves various information about a given stock symbol using the yfinance API. It can fetch different types of information based on the specified key parameter.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/groqing-the-stock-market-function-calling-llama3/README.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef get_stock_info(symbol, key):\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Fashion Images with Llama 3.2 Vision\nDESCRIPTION: Script to process a batch of fashion images and generate structured descriptions for each. It initializes the model, processes images from a specified folder, and stores results for later analysis. Limited to the first 50 images for testing purposes.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nhf_token = \"\"\nmodel_name = \"meta-llama/Llama-3.2-11b-Vision-Instruct\"\n\nmodel = MllamaForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16, token=hf_token)\nprocessor = MllamaProcessor.from_pretrained(model_name, token=hf_token)\n\n# Define the input folder path\ninput_folder_path = IMAGES\n\n# Define the output CSV file path\noutput_csv_file_path = \"./captions_testing.csv\"\n\n# Create an empty list to store the results\nresults = []\n\n# Loop through the first 50 files in the input folder\nfor filename in tqdm(os.listdir(input_folder_path)[:50], desc=\"Processing files\"):\n    # Check if the file is an image\n    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n        # Get the image path\n        image_path = os.path.join(input_folder_path, filename)\n\n        # Load the image\n        image = get_image(image_path)\n\n        # Create a conversation\n        conversation = [\n            {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": USER_TEXT}]}\n        ]\n\n        # Apply chat template and tokenize\n        prompt = processor.apply_chat_template(conversation, add_special_tokens=False, add_generation_prompt=True, tokenize=False)\n        inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n\n        # Generate the output\n        output = model.generate(**inputs, temperature=1, top_p=0.9, max_new_tokens=512)\n\n        # Decode the output\n        decoded_output = processor.decode(output[0])[len(prompt):]\n\n        # Append the result to the list\n        results.append((filename, decoded_output))\n```\n\n----------------------------------------\n\nTITLE: Simple Function Call Implementation in Python\nDESCRIPTION: Demonstrates basic function calling by having an LLM place a product order using a single tool. The code shows how to provide tools to the model, handle the tool response, execute the identified function, and get a final contextual response from the LLM.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nuser_prompt = \"Please place an order for Product ID 5\"\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n    {\n        \"role\": \"user\",\n        \"content\": user_prompt,\n    },\n]\n\n# Step 1: send the conversation and available functions to the model\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=messages,\n    tools=tools,\n    tool_choice=\"auto\",  # Let the LLM decide if it should use one of the available tools\n    max_tokens=4096,\n)\n\nresponse_message = response.choices[0].message\ntool_calls = response_message.tool_calls\nprint(\"First LLM Call (Tool Use) Response:\", response_message)\n# Step 2: check if the model wanted to call a function\nif tool_calls:\n    # Step 3: call the function and append the tool call to our list of messages\n    available_functions = {\n        \"create_order\": create_order,\n    }\n    messages.append(\n        {\n            \"role\": \"assistant\",\n            \"tool_calls\": [\n                {\n                    \"id\": tool_call.id,\n                    \"function\": {\n                        \"name\": tool_call.function.name,\n                        \"arguments\": tool_call.function.arguments,\n                    },\n                    \"type\": tool_call.type,\n                }\n                for tool_call in tool_calls\n            ],\n        }\n    )\n    # Step 4: send the info for each function call and function response to the model\n    tool_call = tool_calls[0]\n    function_name = tool_call.function.name\n    function_to_call = available_functions[function_name]\n    function_args = json.loads(tool_call.function.arguments)\n    function_response = function_to_call(\n        product_id=function_args.get(\"product_id\"),\n        customer_id=function_args.get(\"customer_id\"),\n    )\n    messages.append(\n        {\n            \"tool_call_id\": tool_call.id,\n            \"role\": \"tool\",\n            \"name\": function_name,\n            \"content\": function_response,\n        }\n    )  # extend conversation with function response\n    # Send the result back to the LLM to complete the chat\n    second_response = client.chat.completions.create(\n        model=MODEL, messages=messages\n    )  # get a new response from the model where it can see the function response\n    print(\"\\n\\nSecond LLM Call Response:\", second_response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Running Local Inference with RAFT Model\nDESCRIPTION: Executes a Python script for local inference using the refined RAFT model. This allows for manual interaction with the model by asking questions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/RAFT-Chatbot/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython recipes/inference/local_inference/inference.py --model_name raft-8b\n```\n\n----------------------------------------\n\nTITLE: Querying ReActAgent with RAG Tools in Python\nDESCRIPTION: This snippet demonstrates how to use the ReActAgent with RAG tools to answer a complex question about Kendrick and Drake's backgrounds, leveraging the document-based knowledge.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.chat(\"Tell me about how Kendrick and Drake grew up\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: ReAct Agent Implementation\nDESCRIPTION: Defines the core Agent class that manages conversation state and executes the ReAct loop using Llama 3 models.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = Groq()\nmodel = \"llama3-8b-8192\" # this model works with the prompt below only for the first simpler example; you'll see how to modify the prompt to make it work for a more complicated question\n#model = \"llama3-70b-8192\" # this model works with the prompt below for both example questions\n\nclass Agent:\n    def __init__(self, system=\"\"):\n        self.system = system\n        self.messages = []\n        if self.system:\n            self.messages.append({\"role\": \"system\", \"content\": system})\n\n    def __call__(self, message):\n        self.messages.append({\"role\": \"user\", \"content\": message})\n        result = self.execute()\n        self.messages.append({\"role\": \"assistant\", \"content\": result})\n        return result\n\n    def execute(self):\n        completion = client.chat.completions.create(\n                        model=model,\n                        temperature=0,\n                        messages=self.messages)\n        return completion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Defining Graph State for LangGraph RAG Agent\nDESCRIPTION: This code defines the GraphState class, which represents the state of the LangGraph RAG agent. It includes attributes for the question, generated answer, web search flag, and retrieved documents.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import TypedDict\nfrom typing import List\n\n### State\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        web_search: whether to add search\n        documents: list of documents \n    \"\"\"\n    question : str\n    generation : str\n    web_search : str\n    documents : List[str]\n\nfrom langchain.schema import Document\n```\n\n----------------------------------------\n\nTITLE: Defining Graph State for LangGraph RAG Agent\nDESCRIPTION: This code defines the GraphState class, which represents the state of the LangGraph RAG agent. It includes attributes for the question, generated answer, web search flag, and retrieved documents.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import TypedDict\nfrom typing import List\n\n### State\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        web_search: whether to add search\n        documents: list of documents \n    \"\"\"\n    question : str\n    generation : str\n    web_search : str\n    documents : List[str]\n\nfrom langchain.schema import Document\n```\n\n----------------------------------------\n\nTITLE: Creating Initial User Message for Chat Completion\nDESCRIPTION: Defining the initial message array with a user query about the weather in Boston that will trigger function calling.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What's the weather like in Boston?\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Defining JSON-Formatted Prompt for Fashion Captioning with Llama 3.2\nDESCRIPTION: A carefully crafted prompt instructing the Llama 3.2 Vision model to analyze clothing images and return structured JSON data with specific fashion details. The prompt emphasizes JSON formatting requirements and includes detailed instructions for consistent output.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nUSER_TEXT_OPTION = \"\"\"\nYou are an expert fashion captioner, we are writing descriptions of clothes, look at the image closely and write a caption for it.\n\nWrite the following Title, Size, Category, Gender, Type, Description in JSON FORMAT, PLEASE DO NOT FORGET JSON, \n\nALSO START WITH THE JSON AND NOT ANY THING ELSE, FIRST CHAR IN YOUR RESPONSE IS ITS OPENING BRACE\n\nFOLLOW THESE STEPS CLOSELY WHEN WRITING THE CAPTION: \n1. Only start your response with a dictionary like the example below, nothing else, I NEED TO PARSE IT LATER, SO DONT ADD ANYTHING ELSE-IT WILL BREAK MY CODE\nRemember-DO NOT SAY ANYTHING ELSE ABOUT WHAT IS GOING ON, just the opening brace is the first thing in your response nothing else ok?\n2. REMEMBER TO CLOSE THE DICTIONARY WITH '}'BRACE, IT GOES AFTER THE END OF DESCRIPTION-YOU ALWAYS FORGET IT, THIS WILL CAUSE A LOT OF ISSUES\n3. If you cant tell the size from image, guess it! its okay but dont literally write that you guessed it\n4. Do not make the caption very literal, all of these are product photos, DO NOT CAPTION HOW OR WHERE THEY ARE PLACED, FOCUS ON WRITING ABOUT THE PIECE OF CLOTHING\n5. BE CREATIVE WITH THE DESCRIPTION BUT FOLLOW EVERYTHING CLOSELY FOR STRUCTURE\n6. Return your answer in dictionary format, see the example below\n\n{\"Title\": \"Title of item of clothing\", \"Size\": {'S', 'M', 'L', 'XL'}, #select one randomly if you cant tell from the image. DO NOT TELL ME YOU ESTIMATE OR GUESSED IT ONLY THE LETTER IS ENOUGH\", Category\":  {T-Shirt, Shoes, Tops, Pants, Jeans, Shorts, Skirts, Shoes, Footwear}, \"Gender\": {M, F, U}, \"Type\": {Casual, Formal, Work Casual, Lounge}, \"Description\": \"Write it here\"}\n\nExample: ALWAYS RETURN ANSWERS IN THE DICTIONARY FORMAT BELOW OK?\n\n{\"Title\": \"Casual White pant with logo on it\", \"size\": \"L\", \"Category\": \"Jeans\", \"Gender\": \"U\", \"Type\": \"Work Casual\", \"Description\": \"Write it here, this is where your stuff goes\"} \n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Dynamic Tool-Retrieving ReAct Agent\nDESCRIPTION: Creates a ReActAgent that dynamically retrieves relevant tools using the object retriever, with a custom system prompt for academic paper analysis.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# The LlamaIndex's FunctionCallingAgentWorker API doesn't work correctly with Fireworks Llama, so we use ReActAgent here.\nfrom llama_index.core.agent import ReActAgent\n\nagent = ReActAgent.from_tools(\n    tool_retriever=obj_retriever,\n    llm=llm,\n    system_prompt=\"\"\" \\\nYou are an agent designed to answer queries over a set of given papers.\nPlease always use the tools provided to answer a question. Do not rely on prior knowledge.\\\"\"\",    \n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Llama 3.2 Vision Model with Encoded Image\nDESCRIPTION: Demonstrates how to use the encoded image in a chat completion request to the Llama 3.2 Vision model for image analysis.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"what is this image about?\"},\n                {\n                    \"type\": \"image_url\",\n                    # Uses a local image path. To use a remote image, replace the url with the image URL.\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                    }\n                },\n            ],\n        }\n    ]\n)\n\ndisplay(Markdown(response.choices[0].message.content))\n```\n\n----------------------------------------\n\nTITLE: Initializing Llama 3 70B Chat Model with Replicate\nDESCRIPTION: Sets up the Llama 3 70B chat model from Replicate with specific model parameters for summarization tasks.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/video_summary.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.llms import Replicate\nllm = Replicate(\n    model=\"meta/meta-llama-3-70b-instruct\",\n    model_kwargs={\"temperature\": 0.0, \"top_p\": 1, \"max_new_tokens\":1000}\n)\n```\n\n----------------------------------------\n\nTITLE: Graph Stream Processing Setup in Python\nDESCRIPTION: Sets up configuration and streaming for graph processing with thread ID and image URL handling. Includes event processing loop for stream output.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_tool_calling_agent.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport uuid \n_printed = set()\nimage_url = None\nthread_id = str(uuid.uuid4())\n\nconfig = {\n    \"configurable\": {\n        \"image_url\": image_url,\n        # Checkpoints are accessed by thread_id\n        \"thread_id\": thread_id,\n    }\n}\n\nevents = graph.stream(\n    {\"messages\": (\"user\", questions[0])}, config, stream_mode=\"values\"\n)\nfor event in events:\n    _print_event(event, _printed)\n```\n\n----------------------------------------\n\nTITLE: Implementing Reciprocal Rank Fusion in Python\nDESCRIPTION: This function implements the Reciprocal Rank Fusion algorithm to combine ranked results from multiple IR systems.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import defaultdict\n\ndef reciprocal_rank_fusion(*list_of_list_ranks_system, K=60):\n    \"\"\"\n    Fuse rank from multiple IR systems using Reciprocal Rank Fusion.\n\n    Args:\n    * list_of_list_ranks_system: Ranked results from different IR system.\n    K (int): A constant used in the RRF formula (default is 60).\n\n    Returns:\n    Tuple of list of sorted documents by score and sorted documents\n    \"\"\"\n    # Dictionary to store RRF mapping\n    rrf_map = defaultdict(float)\n\n    # Calculate RRF score for each result in each list\n    for rank_list in list_of_list_ranks_system:\n        for rank, item in enumerate(rank_list, 1):\n            rrf_map[item] += 1 / (rank + K)\n\n    # Sort items based on their RRF scores in descending order\n    sorted_items = sorted(rrf_map.items(), key=lambda x: x[1], reverse=True)\n\n    # Return tuple of list of sorted documents by score and sorted documents\n    return sorted_items, [item for item, score in sorted_items]\n\n# Example ranked lists from different sources\nvector_top_k = vector_retreival(query = \"What are 'skip-level' meetings?\", top_k = 5, vector_index = contextual_embeddings)\nbm25_top_k = bm25_retreival(query = \"What are 'skip-level' meetings?\", k = 5, bm25_index = retriever)\n```\n\n----------------------------------------\n\nTITLE: Running Multimodal Inference with Llama Guard (Python)\nDESCRIPTION: Demonstrates how to run multimodal inference using the Llama Guard vision model. It includes examples of processing both dog and pasta images, and shows how to use custom categories with image inputs.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/llama_guard/llama_guard_text_and_vision_inference.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndog = \"resources/dog.jpg\"\npasta = \"resources/pasta.jpeg\"\n\ndog_image = PIL_Image.open(dog).convert(\"RGB\")\npasta_image = PIL_Image.open(pasta).convert(\"RGB\")\n\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\", \n                \"text\": \"Describe the image\"\n            },\n            {\n                \"type\": \"image\"\n            },\n        ],\n    },\n]\n\nfor image in [dog_image, pasta_image]:\n    input_prompt, response = llama_guard_mm_test(lg_mm_tokenizer, lg_mm_model, conversation, image)\n    display_image(image)\n    print(input_prompt)\n    print(response)\n\ninput_prompt, response = llama_guard_mm_test(lg_mm_tokenizer, lg_mm_model, conversation, dog_image, categories=categories, excluded_category_keys=excluded_category_keys)\ndisplay_image(dog_image)\nprint(input_prompt)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Setting Up RetrievalQA for Context-Aware Answers\nDESCRIPTION: This snippet sets up a RetrievalQA chain to retrieve relevant document chunks and provide context-aware answers using Llama 3.1.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/RAG/hello_llama_cloud.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chains import RetrievalQA\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=vectorstore.as_retriever()\n)\n\nquestion = \"What's new with Llama 3?\"\nresult = qa_chain({\"query\": question})\nprint(result['result'])\n```\n\n----------------------------------------\n\nTITLE: Implementing Browser-based Agent with Playwright and LLM in Python\nDESCRIPTION: This code snippet sets up an asynchronous browser environment using Playwright and implements a loop for executing web automation tasks based on LLM decisions. It includes functions for navigation, clicking elements, filling forms, and maintaining context across interactions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom playwright.async_api import async_playwright\nimport asyncio \nimport json\nimport re\n\nprevious_context = None\n\nasync def run_browser():\n    async with async_playwright() as playwright:\n        # Launch Chromium browser\n        browser = await playwright.chromium.launch(headless=False, channel=\"chrome\")\n        page = await browser.new_page()\n        await asyncio.sleep(1)\n        await page.goto(\"https://google.com/\")\n        previous_actions = []\n        try:\n            while True:  # Infinite loop to keep session alive, press enter to continue or 'q' to quit\n                # Get Context from page\n                accessibility_tree = await page.accessibility.snapshot()\n                accessibility_tree = parse_accessibility_tree(accessibility_tree)\n                await page.screenshot(path=\"screenshot.png\")\n                base64_image = encode_image(imagePath)\n                previous_context = accessibility_tree\n                response = client.chat.completions.create(\n                    model=\"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n                    temperature=0.0,\n                    messages=[\n                        {\"role\": \"system\", \"content\": execution_prompt},\n                        {\"role\": \"system\", \"content\": f\"Few shot examples: {few_shot_examples}. Just a few examples, user will assign you VERY range set of tasks.\"},\n                        {\"role\": \"system\", \"content\": f\"Plan to execute: {steps}\\n\\n Accessibility Tree: {previous_context}\\n\\n, previous actions: {previous_actions}\"},\n                        {\"role\": \"user\", \"content\": \n                         [\n                            {\n                                \"type\": \"text\",\n                                \"text\": f'What should be the next action to accomplish the task: {task} based on the current state? Remember to review the plan and select the next action based on the current state. Provide the next action in JSON format strictly as specified above.',\n                            },\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                                }\n                            },\n                         ]\n                        }\n                    ],\n                )\n                res = response.choices[0].message.content\n                print('Agent response:', res)\n                try:\n                    match = re.search(r'\\{.*\\}', res, re.DOTALL)\n                    if match:\n                        output = json.loads(match.group(0))\n                except Exception as e:\n                    print('Error parsing JSON:', e)\n\n                if output[\"action\"] == \"navigation\":\n                    try:\n                        await page.goto(output[\"url\"])\n                        previous_actions.append(f\"navigated to {output['url']}, SUCCESS\")\n                    except Exception as e:\n                        previous_actions.append(f\"Error navigating to {output['url']}: {e}\")\n\n                elif output[\"action\"] == \"click\":\n                    try:\n                        selector_type, selector_name = output[\"selector\"].split(\"=\")[0], output[\"selector\"].split(\"=\")[1]\n                        res = await page.get_by_role(selector_type, name=selector_name).first.click()\n                        previous_actions.append(f\"clicked {output['selector']}, SUCCESS\")\n                    except Exception as e:\n                        previous_actions.append(f\"Error clicking on {output['selector']}: {e}\")\n                        \n                elif output[\"action\"] == \"fill\":\n                    try:\n                        selector_type, selector_name = output[\"selector\"].split(\"=\")[0], output[\"selector\"].split(\"=\")[1]\n                        res = await page.get_by_role(selector_type, name=selector_name).fill(output[\"value\"])\n                        await asyncio.sleep(1)\n                        await page.keyboard.press(\"Enter\")\n                        previous_actions.append(f\"filled {output['selector']} with {output['value']}, SUCCESS\")\n                    except Exception as e:\n                            previous_actions.append(f\"Error filling {output['selector']} with {output['value']}: {e}\")\n\n                elif output[\"action\"] == \"finished\":\n                    print(output[\"summary\"])\n                    break\n\n                await asyncio.sleep(1) \n                \n                # Or wait for user input\n                user_input = input(\"Press 'q' to quit or Enter to continue: \")\n                if user_input.lower() == 'q':\n                    break\n                \n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n        finally:\n            # Only close the browser when explicitly requested\n            await browser.close()\n\n# Run the async function\nawait run_browser()\n```\n\n----------------------------------------\n\nTITLE: Llama Vision Query Processing\nDESCRIPTION: Implementation of image description generation using Llama Vision model through Together API\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_3_RAG_Setup_and_Validation.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprompt = prompt + \"Please answer within 100 words consisely and only provide the answer, don't\" \\\n            \"repeat any word from the input, start your response from the clothing items that can be paired with the input\"\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-Vision-Free\",\n    messages=[\n                {\n                \"role\": \"user\",\n                \"content\": [\n                        {\n                                \"type\": \"text\",\n                                \"text\": prompt\n                        }\n                ]\n        },\n    ],\n    max_tokens=512,\n    temperature=0.7,\n    top_p=0.7,\n    top_k=50,\n    repetition_penalty=1,\n    stop=[\"<|eot_id|>\",\"<|eom_id|>\"],\n)\nprint(response.choices[0].message.content)\nresponse = response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Evaluating the fine-tuned model\nDESCRIPTION: Tests the fine-tuned model on the same example dialogue to observe the improvement in summarization ability after training. This allows comparison with the pre-training output to assess the effectiveness of the fine-tuning process.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/quickstart_peft_finetuning.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel.eval()\nwith torch.inference_mode():\n    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n```\n\n----------------------------------------\n\nTITLE: Cleaning and Parsing JSON Data from LLM Captions\nDESCRIPTION: Function to extract and parse JSON data from captions using regex, handling formatting issues. This processes multiple CSV files, extracts structured data, and combines it into a single dataframe.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef parse_caption(caption):\n    try:\n        # Extract JSON string from caption\n        json_str = re.search(r'end_header_id\\|>\\s*(\\{.*?\\})\\s*<\\|eot_id\\|>', caption, re.DOTALL)\n        if json_str:\n            json_data = json.loads(json_str.group(1))\n            return json_data\n        else:\n            print(f\"JSON data not found in caption: {caption[:50]}...\")\n            return {}\n    except json.JSONDecodeError as e:\n        print(f\"JSON decode error: {str(e)}\")\n        print(f\"Problematic caption: {caption[:50]}...\")\n        return {}\n\n# Read and process each CSV\ndataframes = []\nfor file in csv_files:\n    df = pd.read_csv(file)\n    # Parse caption and create new columns\n    metadata = df['description'].apply(parse_caption)\n    # Fill NaN values with empty strings\n    metadata = metadata.apply(lambda x: {k: v if v is not None else '' for k, v in x.items()})\n    df = pd.concat([df['Filename'], pd.DataFrame(metadata.tolist())], axis=1)\n    dataframes.append(df)\n\n# Concatenate all dataframes\nresult = pd.concat(dataframes, ignore_index=True)\n\n# Save the result\nresult.to_csv('joined_data.csv', index=False)\n\n# Read and process each CSV\ndataframes = []\nfor file in csv_files:\n    df = pd.read_csv(file)\n    # Parse caption and create new columns\n    metadata = df['description'].apply(parse_caption)\n    df = pd.concat([df['Filename'], pd.DataFrame(metadata.tolist())], axis=1)\n    dataframes.append(df)\n\n# Concatenate all dataframes\nresult = pd.concat(dataframes, ignore_index=True)\n\n# Save the result\nresult.to_csv('joined_data.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Multiple Sequential Function Calls Setup in Python\nDESCRIPTION: Initial setup for multiple sequential function calls where outputs from one function are used as inputs to another. Shows the message structure for ordering a product by name rather than ID.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nuser_prompt = \"Please place an order for a Microphone\"\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n    {\n        \"role\": \"user\",\n        \"content\": user_prompt,\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Implementing Retrieval Grader for RAG\nDESCRIPTION: Creates a grader that assesses the relevance of retrieved documents to the user's question. It uses a Groq LLM with structured output to provide a binary relevance score.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Data model\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n\n# LLM with function call \nllm = ChatGroq(temperature=0, model=\"llama3-70b-8192\")\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\n\n# Prompt \nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \n    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n    ]\n)\n\nretrieval_grader = grade_prompt | structured_llm_grader\nquestion = \"agent memory\"\ndocs = retriever.get_relevant_documents(question)\ndoc_txt = docs[1].page_content\nprint(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n```\n\n----------------------------------------\n\nTITLE: Comparing Llama 3 8B and 70B Models with Prompt 1\nDESCRIPTION: Invokes both Llama 3 8B and 70B models with a prompt about black holes, then compares their responses using the diff view function.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/getting_started_llama_3_on_amazon_bedrock.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprompt_1 = \"Explain black holes to 8th graders\"\nprompt_2 = \"Tell me about llamas\"\n\n# Let's now run the same prompt with Llama 3 8B and 70B to compare responses\nprint(\"\\n=======LLAMA-3-8B====PROMPT 1================>  \", prompt_1)\nresponse_8b_prompt1 = invoke_model(bedrock_runtime, 'meta.llama3-8b-instruct-v1:0', prompt_1, 256)\nprint(\"\\n=======LLAMA-3-70B====PROMPT 1================>  \", prompt_1)\nresponse_70b_prompt1 = invoke_model(bedrock_runtime, 'meta.llama3-70b-instruct-v1:0', prompt_1, 256)\n\n\n# Print the differences in responses\nprint(\"==========================\")\nprint(\"\\nDIFF VIEW for PROMPT 1:\")\nprint_diff(response_8b_prompt1, response_70b_prompt1)\nprint(\"==========================\")\n```\n\n----------------------------------------\n\nTITLE: Defining List Emails Function for Gmail API\nDESCRIPTION: Definition of the list_emails_function that specifies how to query emails with various filters. This function definition is included in the system prompt for the Llama model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlist_emails_function = \"\"\"\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"list_emails\",\n        \"description\": \"Return a list of emails matching an optionally specified query.\",\n        \"parameters\": {\n            \"type\": \"dic\",\n            \"properties\": [\n                {\n                    \"maxResults\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The default maximum number of emails to return is 100; the maximum allowed value for this field is 500.\"\n                    }\n                },              \n                {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"One or more keywords in the email subject and body, or one or more filters. There can be 6 types of filters: 1) Field-specific Filters: from, to, cc, bcc, subject; 2) Date Filters: before, after, older than, newer than); 3) Status Filters: read, unread, starred, importatant; 4) Attachment Filters: has, filename or type; 5) Size Filters: larger, smaller; 6) logical operators (or, and, not).\"\n                    }\n                }\n            ],\n            \"required\": []\n        }\n    }\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Document Query Tools Function\nDESCRIPTION: Defines a function that creates vector query and summary tools from a document. It splits documents into nodes, builds vector and summary indices, and creates specialized tools for querying specific information or generating summaries.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.tools import FunctionTool, QueryEngineTool\nfrom llama_index.core.vector_stores import MetadataFilters, FilterCondition\nfrom typing import List, Optional\n\ndef get_doc_tools(\n    file_path: str,\n    name: str,\n) -> str:\n    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n\n    # load documents\n    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n    splitter = SentenceSplitter(chunk_size=1024)\n    nodes = splitter.get_nodes_from_documents(documents)\n    vector_index = VectorStoreIndex(nodes)\n\n    def vector_query(\n        query: str,\n        page_numbers: Optional[List[str]] = None\n    ) -> str:\n        \"\"\"Use to answer questions over the MetaGPT paper.\n\n        Useful if you have specific questions over the MetaGPT paper.\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n\n        Args:\n            query (str): the string query to be embedded.\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE\n                if we want to perform a vector search\n                over all pages. Otherwise, filter by the set of specified pages.\n\n        \"\"\"\n\n        page_numbers = page_numbers or []\n        metadata_dicts = [\n            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n        ]\n\n        query_engine = vector_index.as_query_engine(\n            similarity_top_k=2,\n            filters=MetadataFilters.from_dicts(\n                metadata_dicts,\n                condition=FilterCondition.OR\n            )\n        )\n        response = query_engine.query(query)\n        return response\n\n\n    vector_query_tool = FunctionTool.from_defaults(\n        name=f\"vector_tool_{name}\",\n        fn=vector_query\n    )\n\n    summary_index = SummaryIndex(nodes)\n    summary_query_engine = summary_index.as_query_engine(\n        response_mode=\"tree_summarize\",\n        use_async=True,\n    )\n    summary_tool = QueryEngineTool.from_defaults(\n        name=f\"summary_tool_{name}\",\n        query_engine=summary_query_engine,\n        description=(\n            \"Use ONLY IF you want to get a holistic summary of MetaGPT. \"\n            \"Do NOT use if you have specific questions over MetaGPT.\"\n        ),\n    )\n\n    return vector_query_tool, summary_tool\n```\n\n----------------------------------------\n\nTITLE: Preparing and Storing Document Chunks for RAG\nDESCRIPTION: This code splits the loaded document into chunks, creates vector representations using HuggingFaceEmbeddings, and stores them in a FAISS vector database for efficient retrieval.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/RAG/hello_llama_cloud.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\nall_splits = text_splitter.split_documents(docs)\n\nvectorstore = FAISS.from_documents(all_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n```\n\n----------------------------------------\n\nTITLE: Defining Graph State for RAG Agent\nDESCRIPTION: Defines the state structure for the LangGraph RAG agent, including the question, generation, web search flag, and retrieved documents.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom typing_extensions import TypedDict\nfrom typing import List\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        web_search: whether to add search\n        documents: list of documents \n    \"\"\"\n    question : str\n    generation : str\n    web_search : str\n    documents : List[str]\n\nfrom langchain.schema import Document\n```\n\n----------------------------------------\n\nTITLE: Hybrid Search Implementation\nDESCRIPTION: Setting up hybrid search using ColbertReranker to combine vector and text-based search approaches\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_3_RAG_Setup_and_Validation.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom lancedb.rerankers import ColbertReranker\n\nreranker = ColbertReranker(column=\"Description\")\nrs = tbl.search(response, query_type=\"hybrid\").rerank(reranker=reranker).limit(5).to_pandas()\nrs\n```\n\n----------------------------------------\n\nTITLE: Setting Up Llama 3 and Flask App for WhatsApp Chatbot in Python\nDESCRIPTION: Python code to initialize Llama 3 using Replicate, create a Flask app, and define routes for handling WhatsApp messages and generating responses.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/whatsapp_chatbot/whatsapp_llama3.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom langchain.llms import Replicate\nfrom flask import Flask, request\n\nos.environ[\"REPLICATE_API_TOKEN\"] = \"<your replicate api token>\"\nllama3_8b_chat = \"meta/meta-llama-3-8b-instruct\"\n\nllm = Replicate(\n    model=llama3_8b_chat,\n    model_kwargs={\"temperature\": 0.0, \"top_p\": 1, \"max_new_tokens\":500}\n)\nclient = WhatsAppClient()\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello_llama():\n    return \"<p>Hello Llama 3</p>\"\n\n@app.route('/msgrcvd', methods=['POST', 'GET'])\ndef msgrcvd():\n    message = request.args.get('message')\n    answer = llm(message)\n    client.send_text_message(answer, \"<a recipient phone number from your WhatsApp API Setup>\")\n    return message + \"<p/>\" + answer\n```\n\n----------------------------------------\n\nTITLE: Implementing Move Function with Visual Display\nDESCRIPTION: Defines a function to execute a chess move, update the board state, and display the move visually using SVG. The function returns a description of the move that was made including the piece type and position.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import SVG\n\ndef make_move(\n    move: Annotated[str, \"A move in UCI format.\"]\n) -> Annotated[str, \"Result of the move.\"]:\n    move = chess.Move.from_uci(move)\n    board.push_uci(str(move))\n    global made_move\n    made_move = True\n\n    svg_str = chess.svg.board(\n            board,\n            arrows=[(move.from_square, move.to_square)],\n            fill={move.from_square: \"gray\"},\n            size=200\n        )\n    display(\n        SVG(data=svg_str)\n    )\n\n    # Get the piece name.\n    piece = board.piece_at(move.to_square)\n    piece_symbol = piece.unicode_symbol()\n    piece_name = (\n        chess.piece_name(piece.piece_type).capitalize()\n        if piece_symbol.isupper()\n        else chess.piece_name(piece.piece_type)\n    )\n    return f\"Moved {piece_name} ({piece_symbol}) from \"\\\n    f\"{chess.SQUARE_NAMES[move.from_square]} to \"\\\n    f\"{chess.SQUARE_NAMES[move.to_square]}.\"\n```\n\n----------------------------------------\n\nTITLE: Example of Chat Completion API Usage\nDESCRIPTION: Demonstrating the usage of chat completion API with message history and context.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresponse = chat_completion(messages=[\n    user(\"Remember that the number of clients is 413 and the number of services is 22.\"),\n    assistant(\"Great. I'll keep that in mind.\"),\n    user(\"What is the number of services?\"),\n])\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Parsing Accessibility Tree for Web Page Analysis\nDESCRIPTION: Defines a helper function to recursively parse and simplify the accessibility tree of a web page, aiding the agent in understanding page elements.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef parse_accessibility_tree(node, indent=0):\n    \"\"\"\n    Recursively parses the accessibility tree and prints a readable structure.\n    Args:\n        node (dict): A node in the accessibility tree.\n        indent (int): Indentation level for the nested structure.\n    \"\"\"\n    # Initialize res as an empty string at the start of each parse\n    res = \"\"\n    \n    def _parse_node(node, indent, res):\n        # Base case: If the node is empty or doesn't have a 'role', skip it\n        if not node or 'role' not in node:\n            return res\n\n        # Indentation for nested levels\n        indented_space = \" \" * indent\n        \n        # Add node's name and role to result string\n        if 'value' in node:\n            res = res + f\"{indented_space}Role: {node['role']} - Name: {node.get('name', 'No name')} - Value: {node['value']}\\n\"\n        else:\n            res = res + f\"{indented_space}Role: {node['role']} - Name: {node.get('name', 'No name')}\\n\"\n        \n        # If the node has children, recursively parse them\n        if 'children' in node:\n            for child in node['children']:\n                res = _parse_node(child, indent + 2, res)  # Increase indentation for child nodes\n                \n        return res\n\n    return _parse_node(node, indent, res)\n```\n\n----------------------------------------\n\nTITLE: Generating a Podcast Using Text-to-Speech with Multiple Speakers\nDESCRIPTION: This code generates a podcast by iterating through a script and using different voice IDs for host and guest speakers. It connects to a TTS service, streams audio for each line of dialogue, appends it to a buffer, and saves the result as a WAV file for playback.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport subprocess\nimport ffmpeg\n\nhost_id = \"694f9389-aac1-45b6-b726-9d9369183238\" # Jane - host\nguest_id = \"a0e99841-438c-4a64-b679-ae501e7d6091\" # Guest\n\nmodel_id = \"sonic-english\" # The Sonic Cartesia model for English TTS\n\noutput_format = {\n    \"container\": \"raw\",\n    \"encoding\": \"pcm_f32le\",\n    \"sample_rate\": 44100,\n    }\n\n# Set up a WebSocket connection.\nws = client_cartesia.tts.websocket()\n\n# Open a file to write the raw PCM audio bytes to.\nf = open(\"podcast.pcm\", \"wb\")\n\n# Generate and stream audio.\nfor line in script.dialogue:\n    if line.speaker == \"Guest\":\n        voice_id = guest_id\n    else:\n        voice_id = host_id\n\n    for output in ws.send(\n        model_id=model_id,\n        transcript='-' + line.text, # the \"-\"\" is to add a pause between speakers\n        voice_id=voice_id,\n        stream=True,\n        output_format=output_format,\n    ):\n        buffer = output[\"audio\"]  # buffer contains raw PCM audio bytes\n        f.write(buffer)\n\n# Close the connection to release resources\nws.close()\nf.close()\n\n# Convert the raw PCM bytes to a WAV file.\nffmpeg.input(\"podcast.pcm\", format=\"f32le\").output(\"podcast.wav\").run()\n\n# Play the file\nsubprocess.run([\"ffplay\", \"-autoexit\", \"-nodisp\", \"podcast.wav\"])\n```\n\n----------------------------------------\n\nTITLE: Testing RetrievalQA Chain\nDESCRIPTION: This code tests the RetrievalQA chain by querying it with a sample question. It demonstrates how to use the chain before integrating it with the UI.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nresult = qa_chain({\"query\": \"Why choose Llama?\"})\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Configuring LangGraph Workflow Graph in Python\nDESCRIPTION: Setup and configuration of the LangGraph workflow graph including node definitions, edge connections, and conditional routing logic. Defines the complete flow of the RAG system with proper error handling and branching.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Build graph\nworkflow.set_conditional_entry_point(\n    route_question,\n    {\n        \"websearch\": \"websearch\",\n        \"vectorstore\": \"retrieve\",\n    },\n)\n\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"websearch\": \"websearch\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"websearch\", \"generate\")\nworkflow.add_conditional_edges(\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not supported\": \"generate\",\n        \"useful\": END,\n        \"not useful\": \"websearch\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Retriever and Template for RetrievalQA Chain\nDESCRIPTION: This code defines the retriever and template for the RetrievalQA chain. It sets up the system prompt, question template, and retriever with search parameters.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt = \"\"\n\ntemplate = \"\"\"\nUse the following pieces of context to answer the question. If no context provided, answer like a AI assistant.\n{context}\nQuestion: {question}\n\"\"\" \n\nretriever = db.as_retriever(\n        search_kwargs={\"k\": 6}\n    )\n```\n\n----------------------------------------\n\nTITLE: LanceDB Schema and Embedding Setup\nDESCRIPTION: Defining schema for LanceDB and setting up BAAI/bge-small-en-v1.5 embeddings for vector representation of clothing descriptions\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_3_RAG_Setup_and_Validation.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lancedb\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\nfrom lancedb.rerankers import ColbertReranker\nfrom pathlib import Path\nfrom PIL import Image\nimport pandas as pd\n\nmodel = get_registry().get(\"sentence-transformers\").create(name=\"BAAI/bge-small-en-v1.5\", device=\"cuda\")\n\nclass Schema(LanceModel):\n    Filename: str\n    Title: str\n    Size: str\n    Gender: str\n    Description: str = model.SourceField()\n    Category: str\n    Type: str\n    vector: Vector(model.ndims()) = model.VectorField()\n\ndb = lancedb.connect(\"~/.lancedb\")\ntbl = db.create_table(name=\"clothes\", schema=Schema, mode=\"overwrite\")\n\n# Function to clean and convert data types\ndef clean_data(df):\n    # Convert all columns to string type\n    for col in df.columns:\n        df[col] = df[col].astype(str)\n    \n    # Remove any rows with NaN values\n    df = df.dropna()\n    \n    return df\n\n# Clean the data\ncleaned_df = clean_data(df)\n\n# Convert cleaned DataFrame to list of dictionaries\ndata = cleaned_df.to_dict('records')\n\nempty_count = len([d for d in data if d.get('Description', '').strip() == ''])\nprint(f\"Number of empty descriptions: {empty_count}\")\n\n# automatically generate vectors\ntbl.add(data)\n\ntbl.search().to_pandas()\n\ndef show_image_from_response(rs):\n    names = rs[\"Filename\"].to_list()\n    return [Image.open(Path(\"archive\") / \"images_compressed\" / name) for name in names]\n```\n\n----------------------------------------\n\nTITLE: Summarizing Text Snippet with Llama 3\nDESCRIPTION: Demonstrates summarizing a 4000-character snippet of the transcript using Llama 3.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/video_summary.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntext = docs[0].page_content[:4000]\nsummary = llm.invoke(f\"Give me a summary of the text below: {text}.\")\nprint(summary)\n```\n\n----------------------------------------\n\nTITLE: PDF Processing and Text Conversion Functions\nDESCRIPTION: Core functions for converting PDF files to text and truncating content. Includes pdf_to_text(), truncate_text(), and process_arxiv_paper() functions for handling arXiv paper downloads and processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/Agents_Tutorial/Tool_Calling_201.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef pdf_to_text(filename):\n    with open(filename, \"rb\") as file:\n        reader = PyPDF2.PdfReader(file)\n        text = \"\"\n        for page in reader.pages:\n            if page.extract_text():\n                text += page.extract_text() + \" \"\n    return text\n\ndef truncate_text(text, limit=20000):\n    words = text.split()\n    truncated = ' '.join(words[:limit])\n    return truncated\n\ndef process_arxiv_paper(arxiv_id):\n    pdf_filename = f\"{arxiv_id}.pdf\"\n    txt_filename = f\"{arxiv_id}.txt\"\n    \n    # Download PDF\n    download_pdf(arxiv_id, pdf_filename)\n    \n    # Convert PDF to text\n    text = pdf_to_text(pdf_filename)\n    \n    # Truncate text\n    truncated_text = truncate_text(text)\n    \n    # Save to txt file\n    with open(txt_filename, \"w\", encoding=\"utf-8\") as file:\n        file.write(truncated_text)\n    print(f\"Processed text saved to {txt_filename}\")\n\n# Example usage\narxiv_id = \"2407.21783\"\nprocess_arxiv_paper(arxiv_id)\n\narxiv_id = \"2103.11943\"\nprocess_arxiv_paper(arxiv_id)\n```\n\n----------------------------------------\n\nTITLE: Loading and configuring the Meta Llama 3 model with quantization\nDESCRIPTION: Sets up the training configuration, loads the Meta Llama 3.1 8B model with 8-bit quantization, and configures the tokenizer. The context length is adjusted based on available GPU memory to prevent OOM errors.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/quickstart_peft_finetuning.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import LlamaForCausalLM, AutoTokenizer\nfrom llama_cookbook.configs import train_config as TRAIN_CONFIG\n\ntrain_config = TRAIN_CONFIG()\ntrain_config.model_name = \"meta-llama/Meta-Llama-3.1-8B\"\ntrain_config.num_epochs = 1\ntrain_config.run_validation = False\ntrain_config.gradient_accumulation_steps = 4\ntrain_config.batch_size_training = 1\ntrain_config.lr = 3e-4\ntrain_config.use_fast_kernels = True\ntrain_config.use_fp16 = True\ntrain_config.context_length = 1024 if torch.cuda.get_device_properties(0).total_memory < 16e9 else 2048 # T4 16GB or A10 24GB\ntrain_config.batching_strategy = \"packing\"\ntrain_config.output_dir = \"meta-llama-samsum\"\ntrain_config.use_peft = True\n\nfrom transformers import BitsAndBytesConfig\nconfig = BitsAndBytesConfig(\n    load_in_8bit=True,\n)\n\nmodel = LlamaForCausalLM.from_pretrained(\n            train_config.model_name,\n            device_map=\"auto\",\n            quantization_config=config,\n            use_cache=False,\n            attn_implementation=\"sdpa\" if train_config.use_fast_kernels else None,\n            torch_dtype=torch.float16,\n        )\n\ntokenizer = AutoTokenizer.from_pretrained(train_config.model_name)\ntokenizer.pad_token = tokenizer.eos_token\n```\n\n----------------------------------------\n\nTITLE: Running Final Demo Application for Multi-Modal RAG\nDESCRIPTION: This command launches the final_demo.py application which demonstrates the complete multi-modal RAG pipeline. It sets up the retrieval system with a vector database, connecting to the Llama model via Together API, and enables uploading images to find complementary clothing items.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython scripts/final_demo.py \\\n    --images_folder \"../MM-Demo/compressed_images\" \\\n    --csv_path \"../MM-Demo/final_balanced_sample_dataset.csv\" \\\n    --table_path \"~/.lancedb\" \\\n    --api_key \"your_together_api_key\" \\\n    --default_model \"BAAI/bge-large-en-v1.5\" \\\n    --use_existing_table \n```\n\n----------------------------------------\n\nTITLE: Querying Agent for Comparative Summary\nDESCRIPTION: Asks the agent to provide a summary comparing two papers (Self-RAG and LongLoRA), demonstrating its ability to synthesize information across documents.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Implementing Answer Generation for RAG\nDESCRIPTION: Sets up the RAG chain for generating answers based on retrieved documents and the user's question. It uses a Groq LLM and a prompt from the LangChain hub.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Prompt\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# LLM\nllm = ChatGroq(temperature=0, model=\"llama3-70b-8192\")\n\n# Post-processing\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# Chain\nrag_chain = prompt | llm | StrOutputParser()\n\n# Run\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\nprint(generation)\n```\n\n----------------------------------------\n\nTITLE: Creating Script Generation Function with Error Handling\nDESCRIPTION: Implements a function to generate a podcast script from input text with error handling. Validates the LLM's JSON output against the schema and retries with error feedback if needed.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef generate_script(system_prompt: str, input_text: str, output_model):\n    \"\"\"Get the dialogue from the LLM.\"\"\"\n    # Load as python object\n    try:\n        response = call_llm(system_prompt, input_text, output_model)\n        dialogue = output_model.model_validate_json(\n            response.choices[0].message.content\n        )\n    except ValidationError as e:\n        error_message = f\"Failed to parse dialogue JSON: {e}\"\n        system_prompt_with_error = f\"{system_prompt}\\n\\nPlease return a VALID JSON object. This was the earlier error: {error_message}\"\n        response = call_llm(system_prompt_with_error, input_text, output_model)\n        dialogue = output_model.model_validate_json(\n            response.choices[0].message.content\n        )\n    return dialogue\n```\n\n----------------------------------------\n\nTITLE: Query Examples and Response Handling\nDESCRIPTION: Demonstrating various query examples and handling responses.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresponse = query_engine.query(\"What is the summary of the document?\")\nprint(str(response))\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(len(response.source_nodes))\n```\n\nLANGUAGE: python\nCODE:\n```\nresponse = query_engine.query(\n    \"How do agents share information with other agents? This is not a summarization question.\"\n)\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM and Embeddings\nDESCRIPTION: Setting up Groq LLM with Llama 3 model and HuggingFace embeddings for document processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.llms.groq import Groq\n\nfrom llama_index.core import Settings, VectorStoreIndex\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nllm = Groq(model=\"llama3-8b-8192\") #, api_key=GROQ_API_TOKEN)\nSettings.llm = llm\n#llm.complete(\"Who wrote the book godfather\").text\n\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Workflow Nodes in Python\nDESCRIPTION: Core node implementations for the RAG workflow including document retrieval, answer generation, document grading, and web search functionality. Each node handles a specific part of the RAG pipeline with proper error handling and state management.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef retrieve(state):\n    \"\"\"\n    Retrieve documents from vectorstore\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, documents, that contains retrieved documents\n    \"\"\"\n    print(\"---RETRIEVE---\")\n    question = state[\"question\"]\n\n    # Retrieval\n    documents = retriever.invoke(question)\n    return {\"documents\": documents, \"question\": question}\n\ndef generate(state):\n    \"\"\"\n    Generate answer using RAG on retrieved documents\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation, that contains LLM generation\n    \"\"\"\n    print(\"---GENERATE---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    \n    # RAG generation\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n\ndef grade_documents(state):\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question\n    If any document is not relevant, we will set a flag to run web search\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Filtered out irrelevant documents and updated web_search state\n    \"\"\"\n\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    \n    # Score each doc\n    filtered_docs = []\n    web_search = \"No\"\n    for d in documents:\n        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n        grade = score['score']\n        # Document relevant\n        if grade.lower() == \"yes\":\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        # Document not relevant\n        else:\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            # We do not include the document in filtered_docs\n            # We set a flag to indicate that we want to run web search\n            web_search = \"Yes\"\n            continue\n    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n```\n\n----------------------------------------\n\nTITLE: Generating Complete Podcast with Multiple Speakers\nDESCRIPTION: Processing the podcast transcript data to generate a complete audio podcast with multiple speakers. The code iterates through speaker segments, generates audio for each, and concatenates them into a final audio track.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfinal_audio = None\n\nfor speaker, text in tqdm(ast.literal_eval(PODCAST_TEXT), desc=\"Generating podcast segments\", unit=\"segment\"):\n    if speaker == \"Speaker 1\":\n        audio_arr, rate = generate_speaker1_audio(text)\n    else:  # Speaker 2\n        audio_arr, rate = generate_speaker2_audio(text)\n    \n    # Convert to AudioSegment (pydub will handle sample rate conversion automatically)\n    audio_segment = numpy_to_audio_segment(audio_arr, rate)\n    \n    # Add to final audio\n    if final_audio is None:\n        final_audio = audio_segment\n    else:\n        final_audio += audio_segment\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM-based Function Calling for SQL Query Selection\nDESCRIPTION: Function that uses an LLM to select and execute the most appropriate verified SQL query based on a user question. It leverages Tool Use functionality to call the execute_duckdb_query_function_calling with the appropriate query name determined by the LLM.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-function-calling-for-sql/json-mode-function-calling-for-sql.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef call_verified_sql(user_question,verified_queries_dict,model):\n    \n    #Simplify verified_queries_dict to just show query name and description\n    query_description_mapping = {key: subdict['description'] for key, subdict in verified_queries_dict.items()}\n    \n    # Step 1: send the conversation and available functions to the model\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": '''You are a function calling LLM that uses the data extracted from the execute_duckdb_query_function_calling function to answer questions around a DuckDB dataset.\n    \n            Extract the query_name parameter from this mapping by finding the one whose description best matches the user's question: \n            {query_description_mapping}\n            '''.format(query_description_mapping=query_description_mapping)\n        },\n        {\n            \"role\": \"user\",\n            \"content\": user_question,\n        }\n    ]\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"execute_duckdb_query_function_calling\",\n                \"description\": \"Executes a verified DuckDB SQL Query\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query_name\": {\n                            \"type\": \"string\",\n                            \"description\": \"The name of the verified query (i.e. 'most-recent-purchases')\",\n                        }\n                    },\n                    \"required\": [\"query_name\"],\n                },\n            },\n        }\n    ]\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        tools=tools,\n        tool_choice=\"auto\",  \n        max_tokens=4096\n    )\n    \n    response_message = response.choices[0].message\n    tool_calls = response_message.tool_calls\n    \n    available_functions = {\n        \"execute_duckdb_query_function_calling\": execute_duckdb_query_function_calling,\n    }\n    for tool_call in tool_calls:\n        function_name = tool_call.function.name\n        function_to_call = available_functions[function_name]\n        function_args = json.loads(tool_call.function.arguments)\n        print('Query found: ',function_args.get(\"query_name\"))\n        function_response = function_to_call(\n            query_name=function_args.get(\"query_name\"),\n            verified_queries_dict=verified_queries_dict\n        )\n    \n    return function_response\n```\n\n----------------------------------------\n\nTITLE: Creating Index Instances\nDESCRIPTION: Initializing summary and vector store indices for document querying.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SummaryIndex, VectorStoreIndex\n\nsummary_index = SummaryIndex(nodes)\nvector_index = VectorStoreIndex(nodes)\n```\n\n----------------------------------------\n\nTITLE: Creating Index and Evaluating Contextual Keyword Generation in Python\nDESCRIPTION: This code creates an index using GPTVectorStoreIndex and LocalJinaEmbedding. It then runs tests to evaluate the effectiveness of the generated contextual keywords by querying the index with generated questions and checking if the correct chunk is retrieved.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Contextual-Chunking-RAG/Example_FinancialReport_RAG.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create Index    \nfrom llama_index.core import GPTVectorStoreIndex, StorageContext, load_index_from_storage, Settings\nfrom embedding import LocalJinaEmbedding #locally run the jinai embedding model\n\nINDEX_DIR = \"./temp/local_index_cache\"\nif not os.path.exists(INDEX_DIR):\n    print(\"Creating new index ...\")    \n    Settings.embed_model = LocalJinaEmbedding()\n    Settings.llm = None\n    documents2 = [Document(text='#'+\",\".join(x['keywords'])+'\\n'+x['content'], metadata={\"id\": str(x[\"idx\"])}) for x in chunks3] \n    index = GPTVectorStoreIndex.from_documents(documents2)\n    index.storage_context.persist(persist_dir=INDEX_DIR)\nelse:\n    storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)\n    index = load_index_from_storage(storage_context)\nquery_engine = index.as_query_engine(similarity_top_k=5)\n\n# Run tests\ncount, correct = 0, 0\nfor test in chunks3[:]:\n    if not \"questions\" in test: continue\n    idx = test[\"idx\"]\n    for question in test[\"questions\"]:\n        count+=1\n        response = query_engine.query(question)\n        print(\"\\n\\n--- Test:\", question, \"idx:\", idx)\n        for result in response.source_nodes[:]:\n            print(result.node.metadata) #prompt+=f\"\\n\\n<Document>\\n                     \n            if result.node.metadata['id'] == str(idx): correct+=1                \n\nprint(\"Test correct, all:\", correct, count)\n```\n\n----------------------------------------\n\nTITLE: Querying the RAG Agent\nDESCRIPTION: Example of querying the agent to compare information about Drake and Kendrick's backgrounds.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.chat(\"Tell me about how Kendrick and Drake grew up\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Creating Object Index for Tool Retrieval\nDESCRIPTION: Creates an ObjectIndex from the tools, which enables semantic retrieval of relevant tools based on the user's query.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# define an \"object\" index and retriever over these tools\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.objects import ObjectIndex\n\nobj_index = ObjectIndex.from_objects(\n    all_tools,\n    index_cls=VectorStoreIndex,\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying vLLM Servers for Meta Llama 3 70B Instruct Model\nDESCRIPTION: Commands to deploy two instances of the Meta Llama 3 70B Instruct model using vLLM, each utilizing 4 GPUs. This setup is designed for maximizing overall throughput on an 8-GPU system.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/on_prem/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 python -m vllm.entrypoints.openai.api_server  --model meta-llama/Meta-Llama-3.1-70B-Instruct --tensor-parallel-size 4 --disable-log-requests --port 8000\nCUDA_VISIBLE_DEVICES=4,5,6,7 python -m vllm.entrypoints.openai.api_server  --model meta-llama/Meta-Llama-3.1-70B-Instruct --tensor-parallel-size 4 --disable-log-requests --port 8001\n```\n\n----------------------------------------\n\nTITLE: Querying Agent for Cross-Document Comparison\nDESCRIPTION: Asks the agent to compare evaluation datasets between MetaGPT and SWE-Bench, demonstrating its ability to retrieve and synthesize information across multiple documents.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.query(\n    \"Tell me about the evaluation dataset used \"\n    \"in MetaGPT and compare it against SWE-Bench\"\n)\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Implementing eCommerce Tool Functions\nDESCRIPTION: Defines three main functions for order creation and product information retrieval using Airtable API integration.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef create_order(product_id, customer_id):\n    headers = {\n        \"Authorization\": f\"Bearer {airtable_api_token}\",\n        \"Content-Type\": \"application/json\",\n    }\n    url = f\"https://api.airtable.com/v0/{airtable_base_id}/orders\"\n    order_id = random.randint(1, 100000)  # Randomly assign an order_id\n    order_datetime = datetime.utcnow().strftime(\n        \"%Y-%m-%dT%H:%M:%SZ\"\n    )  # Assign order date as now\n    data = {\n        \"fields\": {\n            \"order_id\": order_id,\n            \"product_id\": product_id,\n            \"customer_id\": customer_id,\n            \"order_date\": order_datetime,\n        }\n    }\n    response = requests.post(url, headers=headers, json=data)\n    return str(response.json())\n\n\ndef get_product_price(product_name):\n    api_token = os.environ[\"AIRTABLE_API_TOKEN\"]\n    base_id = os.environ[\"AIRTABLE_BASE_ID\"]\n    headers = {\"Authorization\": f\"Bearer {airtable_api_token}\"}\n    formula = f\"{{name}}='{product_name}'\"\n    encoded_formula = urllib.parse.quote(formula)\n    url = f\"https://api.airtable.com/v0/{airtable_base_id}/products?filterByFormula={encoded_formula}\"\n    response = requests.get(url, headers=headers)\n    product_price = response.json()[\"records\"][0][\"fields\"][\"price\"]\n    return \"$\" + str(product_price)\n\n\ndef get_product_id(product_name):\n    api_token = os.environ[\"AIRTABLE_API_TOKEN\"]\n    base_id = os.environ[\"AIRTABLE_BASE_ID\"]\n    headers = {\"Authorization\": f\"Bearer {airtable_api_token}\"}\n    formula = f\"{{name}}='{product_name}'\"\n    encoded_formula = urllib.parse.quote(formula)\n    url = f\"https://api.airtable.com/v0/{airtable_base_id}/products?filterByFormula={encoded_formula}\"\n    response = requests.get(url, headers=headers)\n    product_id = response.json()[\"records\"][0][\"fields\"][\"product_id\"]\n    return str(product_id)\n```\n\n----------------------------------------\n\nTITLE: Utility Function for Router Query Engine\nDESCRIPTION: Creating a reusable function to initialize the router query engine with a given file path.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef get_router_query_engine(file_path: str):\n    \"\"\"Get router query engine.\"\"\"\n\n    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n\n    splitter = SentenceSplitter(chunk_size=1024)\n    nodes = splitter.get_nodes_from_documents(documents)\n\n    summary_index = SummaryIndex(nodes)\n    vector_index = VectorStoreIndex(nodes)\n\n    summary_query_engine = summary_index.as_query_engine(\n        response_mode=\"tree_summarize\",\n        use_async=True,\n    )\n    vector_query_engine = vector_index.as_query_engine()\n\n    summary_tool = QueryEngineTool.from_defaults(\n        query_engine=summary_query_engine,\n        description=(\n            \"Useful for summarization questions related to MetaGPT\"\n        ),\n    )\n\n    vector_tool = QueryEngineTool.from_defaults(\n        query_engine=vector_query_engine,\n        description=(\n            \"Useful for retrieving specific context from the MetaGPT paper.\"\n        ),\n    )\n\n    query_engine = RouterQueryEngine(\n        selector=LLMSingleSelector.from_defaults(),\n        query_engine_tools=[\n            summary_tool,\n            vector_tool,\n        ],\n        verbose=True\n    )\n    return query_engine\n\nquery_engine = get_router_query_engine(\"metagpt.pdf\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving FAISS Vector Store for RAG\nDESCRIPTION: Generates a FAISS vector store from document splits and embeddings, then saves it to a local path for use in RAG.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndb = FAISS.from_documents(splits, embeddings)\ndb.save_local(DB_FAISS_PATH)\n```\n\n----------------------------------------\n\nTITLE: Running Text-only Inference with Full Finetuning in Python\nDESCRIPTION: This snippet shows how to run text-only inference with full finetuning of all parameters. It uses a test prompt file and the AuditNLG safety check.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncat <test_prompt_file> | python inference.py --model_name <training_config.output_dir> --use_auditnlg\n```\n\n----------------------------------------\n\nTITLE: Initializing ReAct Agent with Query Tools\nDESCRIPTION: Creates a ReAct agent with the document query tools, using the Llama 3 model as the reasoning engine. This is used instead of FunctionCallingAgentWorker due to compatibility issues with Groq.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.agent import ReActAgent\n\nquery_engine_tools = [vector_tool, summary_tool]\n\nagent = ReActAgent.from_tools(\n    query_engine_tools,\n    llm=llm,\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Contextual Keywords using Llama 3.1 via DeepInfra in Python\nDESCRIPTION: This snippet sets up the OpenAI client to use DeepInfra's API, defines a function to run inference, and generates contextual keywords for each chunk using Llama 3.1. It uses a system prompt to guide the model in generating relevant keywords for each chunk.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Contextual-Chunking-RAG/Tutorial.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nopenai = OpenAI(api_key=DEEPINFRA_API_KEY, base_url=\"https://api.deepinfra.com/v1/openai\")\n\ndef deepinfra_run(system_prompt, user_message):\n\tchat_completion = openai.chat.completions.create(\n\t    model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n\t    messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_message}],\n\t    max_tokens=4096\n\t)\n\treturn chat_completion.choices[0].message.content\n\nsystem_prompt = '''\nEach chunk is separated as ### Chunk [id] ###. For each chunk generate keywords required to fully understand the chunk without any need for looking at the previous chunks.\nDon't just say \"List of services\", because its unclear what services are you referring to. Make sure to cover all chunks.\nSample output:\nChunk 1: BMW X5, pricings in France\nChunk 2: BMW X5, discounts\n'''\n\nkeywords_st = deepinfra_run(system_prompt, chunked_content)\nprint(keywords_st)\n```\n\n----------------------------------------\n\nTITLE: Running Multimodal Inference with LoRA Fine-tuning Integration in Python\nDESCRIPTION: This snippet demonstrates how to run multimodal inference with LoRA fine-tuning integration. It requires specifying the image path, prompt text, model name, and the path to LoRA weights.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython multi_modal_infer.py \\\n    --image_path \"path/to/image.jpg\" \\\n    --prompt_text \"Describe this image\" \\\n    --model_name \"meta-llama/Llama-3.2-11B-Vision-Instruct\" \\\n    --finetuning_path \"path/to/lora/weights\"\n```\n\n----------------------------------------\n\nTITLE: Defining Callback Stream for LLM Responses\nDESCRIPTION: This snippet defines a custom callback handler (MyStream) that puts streaming LLM responses into a queue for the Gradio UI to render in real-time.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\njob_done = object()\n\nclass MyStream(StreamingStdOutCallbackHandler):\n    def __init__(self, q) -> None:\n        self.q = q\n\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n        self.q.put(token)\n\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n        self.q.put(job_done)\n```\n\n----------------------------------------\n\nTITLE: Executing DuckDB Queries Using Function Calling\nDESCRIPTION: Function that executes a verified SQL query in DuckDB based on a query name. It retrieves the SQL from the verified queries dictionary, establishes an in-memory DuckDB connection, executes the query, and returns the result as a DataFrame.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-function-calling-for-sql/json-mode-function-calling-for-sql.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef execute_duckdb_query_function_calling(query_name,verified_queries_dict):\n    \n    original_cwd = os.getcwd()\n    os.chdir('data')\n\n    query = verified_queries_dict[query_name]['sql']\n    \n    try:\n        conn = duckdb.connect(database=':memory:', read_only=False)\n        query_result = conn.execute(query).fetchdf().reset_index(drop=True)\n    finally:\n        os.chdir(original_cwd)\n\n    return query_result\n```\n\n----------------------------------------\n\nTITLE: Advanced Multi-Document Comparative Analysis\nDESCRIPTION: Requests a more complex comparative analysis between LoRA papers (LongLoRA and LoftQ), which requires the agent to understand and contrast different technical approaches.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.query(\n    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n    \"Analyze the approach in each paper first. \"\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up LangGraph Flow\nDESCRIPTION: Creates and configures the LangGraph workflow with nodes and edges for the agent's operation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_tool_calling_agent.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_core.runnables import RunnableLambda\n\nbuilder = StateGraph(State)\n\nbuilder.add_node(\"assistant\", Assistant(assistant_runnable))\nbuilder.add_node(\"tools\", create_tool_node_with_fallback(tools))\n\nbuilder.set_entry_point(\"assistant\")\nbuilder.add_conditional_edges(\n    \"assistant\",\n    tools_condition, \n    {\"tools\": \"tools\", END: END},\n)\nbuilder.add_edge(\"tools\", \"assistant\")\n\nmemory = SqliteSaver.from_conn_string(\":memory:\")\ngraph = builder.compile(checkpointer=memory)\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Generation with RAG\nDESCRIPTION: Using Llama3 8B model to generate text based on retrieved context from the movie database\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/text_RAG_using_llama_on_together.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclient = Together(api_key = TOGETHER_API_KEY)\n\n# Generate a story based on the top 10 most similar movies\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-3-8b-chat-hf\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a pulitzer award winning craftful story teller. Given only the overview of different plots you can weave together an interesting storyline.\"},\n      {\"role\": \"user\", \"content\": f\"Tell me a story about {titles}. Here is some information about them {overviews}\"},\n    ],\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Initializing Groq Client and Environment Setup\nDESCRIPTION: Sets up the basic requirements including Groq client initialization, model selection, and Airtable configuration variables.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nimport random\nimport urllib.parse\nfrom datetime import datetime\n\nimport requests\nfrom groq import Groq\n\n# Initialize Groq client and model\nclient = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\nMODEL = \"llama3-70b-8192\"\n\n# Airtable variables\nairtable_api_token = os.environ[\"AIRTABLE_API_TOKEN\"]\nairtable_base_id = os.environ[\"AIRTABLE_BASE_ID\"]\n```\n\n----------------------------------------\n\nTITLE: Expanding Document Set\nDESCRIPTION: Defines an expanded set of academic paper URLs and filenames, increasing from three to eleven papers for more complex multi-document analysis.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nurls = [\n    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n]\n\npapers = [\n    \"metagpt.pdf\",\n    \"longlora.pdf\",\n    \"loftq.pdf\",\n    \"swebench.pdf\",\n    \"selfrag.pdf\",\n    \"zipformer.pdf\",\n    \"values.pdf\",\n    \"finetune_fair_diffusion.pdf\",\n    \"knowledge_card.pdf\",\n    \"metra.pdf\",\n    \"vr_mcl.pdf\"\n]\n```\n\n----------------------------------------\n\nTITLE: CURL Request for Chat Completion\nDESCRIPTION: Example of a basic CURL command for making a chat completion request to the Azure Llama API endpoint.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!curl -X POST -L https://your-endpoint.inference.ai.azure.com/v1/chat/completions -H 'Content-Type: application/json' -H 'Authorization: your-auth-key' -d '{\"messages\":[{\"content\":\"You are a helpful assistant.\",\"role\":\"system\"},{\"content\":\"What is good about Wuhan?\",\"role\":\"user\"}], \"max_tokens\": 50}'\n```\n\n----------------------------------------\n\nTITLE: Chat Engine Usage Examples\nDESCRIPTION: Demonstrates how to use the chat engine to query information about Drake and Kendrick's beef.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresponse = chat_engine.chat(\n    \"Tell me about the songs Drake released in the beef.\"\n)\nprint(str(response))\n```\n\nLANGUAGE: python\nCODE:\n```\nresponse = chat_engine.chat(\"What about Kendrick?\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama Guard Response Generation in Python\nDESCRIPTION: This code snippet shows how to configure Llama Guard's response generation using the LlamaGuardGenerationConfigs class. It specifies options for listing violated codes and positioning the explanation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/llama_cookbook/data/llama_guard/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nllama_guard_generation_configs = LlamaGuardGenerationConfigs(\n    should_list_violated_codes=True,\n    explanation_position=ExplanationPosition.AFTER_DECISION\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Call Function for Script Generation\nDESCRIPTION: Creates a function to call the Llama 3.1 model via Together.ai's API. Uses JSON mode to enforce the expected dialogue structure and schema for the podcast script.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef call_llm(system_prompt: str, text: str, dialogue_format):\n    \"\"\"Call the LLM with the given prompt and dialogue format.\"\"\"\n    response = client_together.chat.completions.create(\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": text},\n        ],\n        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",  # can also use \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n        response_format={\n            \"type\": \"json_object\",\n            \"schema\": dialogue_format.model_json_schema(),\n        },\n    )\n    return response\n```\n\n----------------------------------------\n\nTITLE: Full-Text Search Index Creation\nDESCRIPTION: Creating a full-text search index on the Description field in LanceDB\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_3_RAG_Setup_and_Validation.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntbl.create_fts_index(\"Description\", replace=True)\n```\n\n----------------------------------------\n\nTITLE: Using Llama Vision for Analysis\nDESCRIPTION: Sends the retrieved page and query to the Llama 3.2 90B Vision model via Together AI's API for analysis and answer generation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/multimodal_RAG_with_nvidia_investor_slide_deck.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom together import Together\n\nclient = Together(api_key = api_key)\n\nresponse = client.chat.completions.create(\n  model=\"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": query},\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": f\"data:image/jpeg;base64,{returned_page}\",\n          },\n        },\n      ],\n    }\n  ],\n  max_tokens=300,\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Task for Step-by-Step Execution\nDESCRIPTION: Creates a task within the agent system to demonstrate step-by-step execution and control over the reasoning process.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntask = agent.create_task(\n    \"Tell me about the agent roles in MetaGPT, \"\n    \"and then how they communicate with each other.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Function for Web Search of Paper IDs\nDESCRIPTION: This function uses the Tavily API to perform web searches for arXiv IDs of two papers based on their titles or descriptions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/Agents_Tutorial/Tool_Calling_201.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef query_for_two_papers(paper_1:str , paper_2: str) -> None :\n     return [tavily_client.search(f\"arxiv id of {paper_1}\"), tavily_client.search(f\"arxiv id of {paper_2}\")]\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Search RAG\nDESCRIPTION: Implementation of Retrieval-Augmented Generation using vector search capabilities.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(docs_both)\nquery_engine = index.as_query_engine(similarity_top_k=3)\n```\n\n----------------------------------------\n\nTITLE: Setting up MongoDB Connection and Collection\nDESCRIPTION: Establishes a connection to MongoDB Atlas, creates a database and collection for storing research papers. Includes a function to get MongoDB client and setup for data ingestion.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/vectorstore/mongodb/rag_mongodb_llama3_huggingface_open_source.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pymongo\n\ndef get_mongo_client(mongo_uri):\n    try:\n        client = pymongo.MongoClient(mongo_uri, appname=\"devrel.content.python\")\n        print(\"Connection to MongoDB successful\")\n        return client\n    except pymongo.errors.ConnectionFailure as e:\n        print(f\"Connection failed: {e}\")\n        return None\n\nmongo_uri = \"mongodb...pName=Cluster0\"  # Placeholder\n\nif not mongo_uri:\n    print(\"MONGO_URI not set in environment variables.\")\n\nmongo_client = get_mongo_client(mongo_uri)\n\ndb = mongo_client['knowledge_base']\ncollection = db['research_papers']\n\n# Delete any existing records in the collection\ncollection.delete_many({})\n```\n\n----------------------------------------\n\nTITLE: Displaying Dataset Source Node Content\nDESCRIPTION: Retrieves and prints the full content of the first source node used in generating the response about evaluation datasets, including metadata.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprint(response.source_nodes[0].get_content(metadata_mode=\"all\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing Text-to-SQL Queries\nDESCRIPTION: Setup and execution of SQL queries using LlamaIndex's NLSQLTableQueryEngine with SQLite database.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.indices.struct_store import NLSQLTableQueryEngine\n\nquery_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[\"albums\", \"tracks\", \"artists\"],\n    llm=llm_70b,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Audio with Suno Bark Model\nDESCRIPTION: Creating audio from text using the Bark model with expression tags like [sigh] and emphasized words. The model generates speech with the specified voice preset and temperature parameters for controlling creativity.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntext_prompt = \"\"\"\nExactly! [sigh] And the distillation part is where you take a LARGE-model,and compress-it down into a smaller, more efficient model that can run on devices with limited resources.\n\"\"\"\ninputs = processor(text_prompt, voice_preset=voice_preset).to(device)\n\nspeech_output = model.generate(**inputs, temperature = 0.9, semantic_temperature = 0.8)\nAudio(speech_output[0].cpu().numpy(), rate=sampling_rate)\n```\n\n----------------------------------------\n\nTITLE: Configuring Tool Definitions for LLM\nDESCRIPTION: Defines the structure and parameters for each tool that the LLM can use, including detailed descriptions and parameter specifications.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"create_order\",\n            \"description\": \"Creates an order given a product_id and customer_id. If a product name is provided, you must get the product ID first. After placing the order indicate that it was placed successfully and output the details.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"product_id\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The ID of the product\",\n                    },\n                    \"customer_id\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The ID of the customer\",\n                    },\n                },\n                \"required\": [\"product_id\", \"customer_id\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_product_price\",\n            \"description\": \"Gets the price for a product, given the name of the product. Just return the price, do not do any calculations.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"product_name\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the product (must be title case, i.e. 'Microphone', 'Laptop')\",\n                    }\n                },\n                \"required\": [\"product_name\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_product_id\",\n            \"description\": \"Gets product ID given a product name\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"product_name\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the product (must be title case, i.e. 'Microphone', 'Laptop')\",\n                    }\n                },\n                \"required\": [\"product_name\"],\n            },\n        },\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Querying BM25 Index in Python\nDESCRIPTION: This snippet demonstrates how to query the BM25 index and retrieve top-k results for a given query.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Query the corpus and get top-k results\nquery = \"What are 'skip-level' meetings?\"\nresults, scores = retriever.retrieve(bm25s.tokenize(query), k=5,)\n```\n\n----------------------------------------\n\nTITLE: Implementing Google Contacts and Calendar API Functions\nDESCRIPTION: Defines two functions: google_contact to look up email addresses from Google Contacts and google_calendar to create meeting invites. Includes authentication logic to generate and refresh tokens for Google API access.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/calendar_assistant/tool_calling_google_api.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os.path\n\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom datetime import datetime, timedelta\n\n\nSCOPES = [\"https://www.googleapis.com/auth/contacts.readonly\"]\n\nTOKEN = \"token_contacts.json\"\n\n\ndef google_contact(name):\n  \"\"\"Returns the email address in Google contacts for the given name.\n  \"\"\"\n  creds = None\n\n  # The file token_contacts.json stores the user's access and refresh tokens, and is\n  # created automatically when the authorization flow completes for the first\n  # time.\n  if os.path.exists(TOKEN):\n    creds = Credentials.from_authorized_user_file(TOKEN, SCOPES)\n\n  # If there are no (valid) credentials available, let the user log in.\n  if not creds or not creds.valid:\n    if creds and creds.expired and creds.refresh_token:\n      creds.refresh(Request())\n    else:\n      flow = InstalledAppFlow.from_client_secrets_file(\n          \"credentials.json\", SCOPES\n      )\n      creds = flow.run_local_server(port=0)\n    # Save the credentials for the next run\n    with open(TOKEN, \"w\") as token:\n      token.write(creds.to_json())\n\n  try:\n    service = build(\"people\", \"v1\", credentials=creds)\n\n    # Call the People API\n    results = (\n        service.people()\n        .connections()\n        .list(\n            resourceName=\"people/me\",\n            pageSize=10,\n            personFields=\"names,emailAddresses\",\n        )\n        .execute()\n    )\n    connections = results.get(\"connections\", [])\n\n    # Build a dictionary of name & email address\n    db = {}\n    for person in connections:\n      names = person.get(\"names\", [])\n      if names:\n        n = names[0].get(\"displayName\")\n        \n      email = person.get(\"emailAddresses\", [])\n      db[n] = email[0].get('value')\n    \n    return db[name]\n  except HttpError as err:\n    print(err)\n\n\nCALSCOPES = [\"https://www.googleapis.com/auth/calendar\"]\n\nCALTOKEN = \"token_calendar.json\"\n\ndef google_calendar(date, time, attendees):\n  \"\"\"Creates a meeting invite using Google Calendar API.\n  \"\"\"\n  creds = None\n  # The file token_calendar.json stores the user's access and refresh tokens, and is\n  # created automatically when the authorization flow completes for the first\n  # time.\n  if os.path.exists(CALTOKEN):\n    creds = Credentials.from_authorized_user_file(CALTOKEN, CALSCOPES)\n        \n  # If there are no (valid) credentials available, let the user log in.\n  if not creds or not creds.valid:\n    if creds and creds.expired and creds.refresh_token:\n      creds.refresh(Request())\n    else:\n      flow = InstalledAppFlow.from_client_secrets_file(\n          \"credentials.json\", CALSCOPES\n      )\n      creds = flow.run_local_server(port=0)\n    # Save the credentials for the next run\n    with open(CALTOKEN, \"w\") as token:\n      token.write(creds.to_json())\n\n  name, email = attendees[\"name\"], attendees[\"email\"]\n  time_str = date + ', 2025 ' + time\n  # Convert the time string to a datetime object\n  dt_obj = datetime.strptime(time_str, '%b %d, %Y %I:%M %p')\n  dt_obj_end =  dt_obj + timedelta(minutes=30)\n  event = {\n    'summary': 'Meeting: Ankith | ' + name,\n    'location': 'MPK 21',\n    'description': 'Sync up meeting',\n    'start': {\n      'dateTime': dt_obj.strftime('%Y-%m-%dT%H:%M:%S'),\n      'timeZone': 'America/Los_Angeles',\n    },\n    'end': {\n      f'dateTime': dt_obj_end.strftime('%Y-%m-%dT%H:%M:%S'),\n      'timeZone': 'America/Los_Angeles',\n    },\n    'attendees': [\n      {'email': email},\n    ],\n    'reminders': {\n      'useDefault': False,\n      'overrides': [\n        {'method': 'email', 'minutes': 24 * 60},\n        {'method': 'popup', 'minutes': 10},\n      ],\n    },\n  }\n  \n  service = build(\"calendar\", \"v3\", credentials=creds)\n\n  # Create a meeting invite\n  event = service.events().insert(calendarId='primary', body=event).execute()\n  \n  result = f\"\\nEvent created with {name} with email {email} on {date} at {time}: \\n {event.get('htmlLink')}\"\n  return result\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Open Contextual RAG\nDESCRIPTION: Installs necessary Python libraries for implementing Open Contextual RAG, including Together AI, tiktoken, BeautifulSoup, and BM25.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together # To access open source LLMs\n!pip install --upgrade tiktoken # To count total token counts\n!pip install beautifulsoup4 # To scrape documents to RAG over\n!pip install bm25s # To implement out key-word BM25 search\n```\n\n----------------------------------------\n\nTITLE: Text Summarization Function Using LLM\nDESCRIPTION: Function to summarize research papers using an 8b LLM model. Includes prompt engineering and chat completion logic for generating paper summaries.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/Agents_Tutorial/Tool_Calling_201.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nSUMMARISER_PROMPT = \"\"\"\nCutting Knowledge Date: December 2023\nToday Date: 15 September 2024\nYou are an expert summariser of research papers, below you will get an input of the text from an arxiv paper and your job is to read it carefully and return a concise summary with some bullet points at the end of some key-takeways from it\n\"\"\"\n\ndef summarize_text_file(file_name: str, temperature: int = 0, max_tokens=2048):\n    # Read the content of the file\n    with open(file_name, 'r') as file:\n        file_content = file.read()\n    \n    # Initialize chat history\n    chat_history = [{\"role\": \"system\", \"content\": f\"{SUMMARISER_PROMPT}\"}, {\"role\": \"user\", \"content\": f\"Text of the paper: {file_content}\"}]\n    \n    # Generate a summary using the model\n    response = client.chat.completions.create(\n        model=\"llama-3.1-8b-instant\",  # You can change the model as needed\n        messages=chat_history,\n        max_tokens=max_tokens,\n        temperature=temperature\n    )\n    \n    # Append the assistant's response to the chat history\n    chat_history.append({\n        \"role\": \"assistant\",\n        \"content\": response.choices[0].message.content\n    })\n    \n    # Return the summary\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing Conditional Routing Nodes for RAG Workflow in Python\nDESCRIPTION: Defines the conditional edge functions that determine routing decisions within the LangGraph workflow based on document relevance, question type, and answer quality assessment.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef route_question(state):\n    \"\"\"\n    Route question to web search or RAG.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n\n    print(\"---ROUTE QUESTION---\")\n    question = state[\"question\"]\n    source = question_router.invoke({\"question\": question})  \n    if source.datasource == 'web_search':\n        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n        return \"websearch\"\n    elif source.datasource == 'vectorstore':\n        print(\"---ROUTE QUESTION TO RAG---\")\n        return \"vectorstore\"\n\ndef decide_to_generate(state):\n    \"\"\"\n    Determines whether to generate an answer, or add web search\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n    question = state[\"question\"]\n    web_search = state[\"web_search\"]\n    filtered_documents = state[\"documents\"]\n\n    if web_search == \"Yes\":\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n        return \"websearch\"\n    else:\n        # We have relevant documents, so generate answer\n        print(\"---DECISION: GENERATE---\")\n        return \"generate\"\n\ndef grade_generation_v_documents_and_question(state):\n    \"\"\"\n    Determines whether the generation is grounded in the document and answers question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Decision for next node to call\n    \"\"\"\n\n    print(\"---CHECK HALLUCINATIONS---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    generation = state[\"generation\"]\n\n    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n    grade = score.score\n\n    # Check hallucination\n    if grade == \"yes\":\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n        # Check question-answering\n        print(\"---GRADE GENERATION vs QUESTION---\")\n        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n        grade = score.score\n        if grade == \"yes\":\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n            return \"useful\"\n        else:\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n            return \"not useful\"\n    else:\n        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"not supported\"\n\nfrom langgraph.graph import END, StateGraph\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"websearch\", web_search) # web search\nworkflow.add_node(\"retrieve\", retrieve) # retrieve\nworkflow.add_node(\"grade_documents\", grade_documents) # grade documents\nworkflow.add_node(\"generate\", generate) # generatae\n```\n\n----------------------------------------\n\nTITLE: Generating Contextual Responses with Llama 3B Model\nDESCRIPTION: Defines a function to generate contextual responses for each chunk using the Llama 3B model via the Together AI API. This adds context to each chunk for improved retrieval.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef generate_context(prompt: str):\n    \"\"\"\n    Generates a contextual response based on the given prompt using the specified language model.\n    Args:\n        prompt (str): The input prompt to generate a response for.\n    Returns:\n        str: The generated response content from the language model.\n    \"\"\"\n    response = client.chat.completions.create(\n        model=\"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=1\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Parsing Generated Keywords into Array in Python\nDESCRIPTION: This function parses the generated keywords string into an array of keyword lists for each chunk. It uses regular expressions to handle different formats of keyword output, including inline and section-based formats.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Contextual-Chunking-RAG/Tutorial.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport re\ndef parse_keywords(content):\n    result = []\n    lines = content.strip().split('\\n')\n    current_chunk = []\n    inline_pattern = re.compile(r'^\\s*[^#:]+\\s*:\\s*(.+)$')  # Matches lines like \"Chunk1: word1, word2\"\n    section_pattern = re.compile(r'^###\\s*[^#]+\\s*###$')    # Matches lines like \"### Chunk1 ###\"\n\n    for line in lines:\n        line = line.strip()\n        if not line: continue\n        inline_match = inline_pattern.match(line)\n\n        if inline_match:\n            words_str = inline_match.group(1)\n            words = [word.strip() for word in words_str.split(',') if word.strip()]\n            result.append(words)\n            continue\n\n        if section_pattern.match(line):\n            if current_chunk:\n                result.append(current_chunk)\n                current_chunk = []\n            continue\n\n        if current_chunk is not None:\n            words = [word.strip() for word in line.split(',') if word.strip()]\n            current_chunk.extend(words)\n\n    if current_chunk:\n      result.append(current_chunk)\n    return result\n\n\nkeywords = parse_keywords(keywords_st)\nprint(keywords)\n```\n\n----------------------------------------\n\nTITLE: Initializing Storage for Generated Audio Segments\nDESCRIPTION: Creating lists to store the generated audio segments and their corresponding sampling rates. These will be used to concatenate the final podcast audio.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ngenerated_segments = []\nsampling_rates = []  # We'll need to keep track of sampling rates for each segment\n```\n\n----------------------------------------\n\nTITLE: Implementing Retriever Function\nDESCRIPTION: Function to retrieve top-k similar items based on cosine similarity of embeddings\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/text_RAG_using_llama_on_together.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef retreive(query: str, top_k: int = 5, index: np.ndarray = None) -> List[int]:\n    \"\"\"\n    Retrieve the top-k most similar items from an index based on a query.\n    Args:\n        query (str): The query string to search for.\n        top_k (int, optional): The number of top similar items to retrieve. Defaults to 5.\n        index (np.ndarray, optional): The index array containing embeddings to search against. Defaults to None.\n    Returns:\n        List[int]: A list of indices corresponding to the top-k most similar items in the index.\n    \"\"\"\n    \n    query_embedding = generate_embeddings([query], 'BAAI/bge-base-en-v1.5')[0]\n    similarity_scores = cosine_similarity([query_embedding], index)\n\n    return np.argsort(-similarity_scores)[0][:top_k]\n```\n\n----------------------------------------\n\nTITLE: Loading Verified SQL Queries from YAML Files in Python\nDESCRIPTION: Function that loads verified SQL queries stored in YAML files from a specified directory. Each YAML file contains SQL and metadata like descriptions to help the LLM choose the appropriate query.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-function-calling-for-sql/json-mode-function-calling-for-sql.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_verified_queries(directory_path):\n    verified_queries_yaml_files = glob.glob(os.path.join(directory_path, '*.yaml'))\n    verified_queries_dict = {}\n    for file in verified_queries_yaml_files:\n        with open(file, 'r') as stream:\n            try:\n                file_name = file[len(directory_path):-5]\n                verified_queries_dict[file_name] = yaml.safe_load(stream)\n            except yaml.YAMLError as exc:\n                continue\n        \n    return verified_queries_dict\n\ndirectory_path = 'verified-queries/'\nverified_queries_dict = get_verified_queries(directory_path)\n```\n\n----------------------------------------\n\nTITLE: Building Document Index for RAG\nDESCRIPTION: Creates a document index from web pages, splits them into chunks, and builds a Chroma vectorstore for efficient retrieval. This forms the knowledge base for the RAG system.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=HuggingFaceEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n```\n\n----------------------------------------\n\nTITLE: Defining User Prompt Template for Clinical Note Processing\nDESCRIPTION: This code defines a user prompt template that will be used to format the clinical note for processing by the Groq API.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-social-determinants-of-health/SDOH-Json-mode.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define user prompt template\nuser_prompt_template = '''\nUse information from following clinical note to construct the proper JSON output:\n\n{clinical_note}\n'''\n```\n\n----------------------------------------\n\nTITLE: Generating Answer Using Llama 405B Model in Python\nDESCRIPTION: This snippet uses the Meta-Llama-3.1-405B-Instruct-Turbo model to generate an answer based on the retrieved context and the original query.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Generate a story based on the top 10 most similar movies\n\nquery = \"What are 'skip-level' meetings?\"\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n      {\"role\": \"user\", \"content\": f\"Answer the question: {query}. Here is relevant information: {retreived_chunks}\"},\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Running Step with User-Provided Input\nDESCRIPTION: Executes a step in the reasoning process with additional user-provided input to guide the agent's reasoning about agent communication.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nstep_output = agent.run_step(\n    task.task_id, input=\"What about how agents share information?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Question Router for RAG Agent\nDESCRIPTION: This code implements a question router that determines whether to use the vector store or web search based on the user's question. It uses a local LLM to make the routing decision and returns it as a JSON output.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import PromptTemplate\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import JsonOutputParser\n\n# LLM\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n\nprompt = PromptTemplate(\n    template=\"\"\"You are an expert at routing a \n    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \n    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \n    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \n    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n    no premable or explanation. \n    \n    Question to route: \n    {question}\"\"\",\n    input_variables=[\"question\"],\n)\n\nquestion_router = prompt | llm | JsonOutputParser()\nquestion = \"llm agent memory\"\ndocs = retriever.get_relevant_documents(question)\ndoc_txt = docs[1].page_content\nprint(question_router.invoke({\"question\": question}))\n```\n\n----------------------------------------\n\nTITLE: Creating TGI LLM Instance for Chatbot\nDESCRIPTION: This snippet creates a HuggingFaceTextGenInference LLM instance, configuring it with various parameters and connecting it to the API serving port on localhost.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nllm = HuggingFaceTextGenInference(\n    inference_server_url=LLAMA3_8B_HOSTPORT,\n    max_new_tokens=512,\n    top_k=10,\n    top_p=0.9,\n    typical_p=0.95,\n    temperature=0.6,\n    repetition_penalty=1,\n    do_sample=True,\n    streaming=True\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Executive Reports\nDESCRIPTION: Creates high-level analysis and challenges report from annotated issues\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nchallenges, overview = generate_executive_reports(annotated_issues, theme_counts, repo_name, start_date, end_date)\nchallenges = validate_df_values(challenges, out_folder, 'challenges')\noverview = validate_df_values(overview, out_folder, 'overview')\n```\n\n----------------------------------------\n\nTITLE: Creating Document Query Tools\nDESCRIPTION: Generates vector and summary query tools for the MetaGPT paper using the previously defined function.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nvector_tool, summary_tool = get_doc_tools(\"metagpt.pdf\", \"metagpt\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Bedrock Client\nDESCRIPTION: Setup of AWS Bedrock client with LLaMA model configuration.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/react_llama_3_bedrock_wk.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nLLAMA3_70B_CHAT = \"meta.llama3-70b-instruct-v1:0\"\nLLAMA3_8B_CHAT = \"meta.llama3-8b-instruct-v1:0\"\n\n# We'll default to the smaller 8B model for speed; change to LLAMA3_70B_CHAT for more advanced (but slower) generations\nDEFAULT_MODEL = LLAMA3_8B_CHAT\n\nllm = Bedrock(credentials_profile_name='default', model_id=DEFAULT_MODEL)\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama 3 Model and Embeddings\nDESCRIPTION: Sets up the Groq LLM client for Llama 3 and configures Hugging Face embeddings as the default settings for LlamaIndex operations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.llms.groq import Groq\n\nfrom llama_index.core import Settings, VectorStoreIndex\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nllm = Groq(model=\"llama3-70b-8192\", temperature=0)\nSettings.llm = llm\n\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Function Call Details from Forced Function Call\nDESCRIPTION: Retrieving the function name and arguments from the response when function calling is forced with a specific function name.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfunction_call = response.choices[0].message.function_call\nfunction_call.name, function_call.arguments\n```\n\n----------------------------------------\n\nTITLE: Storing Function Arguments for Later Use\nDESCRIPTION: Storing the parsed function arguments in a variable for use in function execution.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nargs = json.loads(response_message.function_call.arguments)\n```\n\n----------------------------------------\n\nTITLE: Implementing BM25 Retrieval Function in Python\nDESCRIPTION: This function performs BM25 retrieval for a given query and returns the top-k document indices.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef bm25_retreival(query: str, k : int, bm25_index) -> List[int]:\n    \"\"\"\n    Retrieve the top-k document indices based on the BM25 algorithm for a given query.\n    Args:\n        query (str): The search query string.\n        k (int): The number of top documents to retrieve.\n        bm25_index: The BM25 index object used for retrieval.\n    Returns:\n        List[int]: A list of indices of the top-k documents that match the query.\n    \"\"\"\n\n    results, scores = bm25_index.retrieve(bm25s.tokenize(query), k=k)\n\n    return [contextual_chunks.index(doc) for doc in results[0]]\n```\n\n----------------------------------------\n\nTITLE: Loading YouTube Video Transcript with LangChain's YoutubeLoader\nDESCRIPTION: Uses LangChain's YoutubeLoader to retrieve the transcript of a specified YouTube video URL.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/video_summary.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.document_loaders import YoutubeLoader\n\nloader = YoutubeLoader.from_youtube_url(\n    \"https://www.youtube.com/watch?v=5t1vTLU7s40\", add_video_info=True\n)\n```\n\n----------------------------------------\n\nTITLE: Using BM25 Retrieval Function in Python\nDESCRIPTION: This snippet demonstrates how to use the bm25_retreival function with a specific query and BM25 index.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nbm25_retreival(query = \"What are 'skip-level' meetings?\", k = 5, bm25_index = retriever)\n```\n\n----------------------------------------\n\nTITLE: Getting LLM Response\nDESCRIPTION: Retrieves and prints the SQL query generated by the LLM\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/quickstart.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nanswer = llm.invoke(prompt).content\nprint(answer)\n```\n\n----------------------------------------\n\nTITLE: Implementing Answer Generation for RAG Agent\nDESCRIPTION: This code sets up the answer generation component of the RAG agent. It uses a local LLM to generate answers based on retrieved context and the user's question, with a focus on concise responses.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import PromptTemplate\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Prompt\nprompt = PromptTemplate(\n    template=\"\"\"You are an assistant for question-answering tasks. \n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n    Use three sentences maximum and keep the answer concise:\n    Question: {question} \n    Context: {context} \n    Answer: \n    \"\"\",\n    input_variables=[\"question\", \"document\"],\n)\n\nllm = ChatOllama(model=local_llm, temperature=0)\n\n# Post-processing\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# Chain\nrag_chain = prompt | llm | StrOutputParser()\n\n# Run\nquestion = \"agent memory\"\ndocs = retriever.invoke(question)\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\nprint(generation)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for RAG Pipeline\nDESCRIPTION: Installs necessary Python libraries including datasets, pandas, pymongo, sentence_transformers, and transformers. Also installs accelerate for GPU support.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/vectorstore/mongodb/rag_mongodb_llama3_huggingface_open_source.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install datasets pandas pymongo sentence_transformers\n!pip install -U transformers\n# Install the library below if using GPU, if using CPU, please comment out below\n!pip install accelerate\n```\n\n----------------------------------------\n\nTITLE: Defining System Prompt for Social Determinants of Health Extraction\nDESCRIPTION: This snippet defines the system prompt for the Groq API, specifying the JSON schema for extracting social determinants of health from clinical notes.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-social-determinants-of-health/SDOH-Json-mode.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Define system prompt (note: system prompt must contain \"JSON\" in it)\nsystem_prompt = '''\nYou are a medical coding API specializing in social determinants of health that responds in JSON.\nYour job is to extract structured SDOH data from an unstructured clinical note and output the structured data in JSON.\nThe JSON schema should include:\n  {\n    \"employment_status\": \"string (categorical: 'Unemployed', 'Part-time', 'Full-time', 'Retired')\",\n    \"financial_stress\": \"boolean (TRUE if the patient mentions financial difficulties)\",\n    \"housing_insecurity\": \"boolean (TRUE if the patient does not live in stable housing conditions)\",\n    \"neighborhood_unsafety\": \"boolean (TRUE if the patient expresses concerns about safety)\",\n    \"food_insecurity\": \"boolean (TRUE if the patient does not have reliable access to sufficient food)\",\n    \"education_level\": \"string (categorical: 'None', 'High School', 'College', 'Graduate')\",\n    \"transportation_inaccessibility\": \"boolean (TRUE if the patient does not have reliable transportation to healthcare appointments)\",\n    \"social_isolation\": \"boolean (TRUE if the patient mentions feeling isolated or having a lack of social support)\",\n    \"health_insurance_inadequacy\": (boolean: TRUE if the patient's health insurance is insufficient),\n    \"skipped_care_due_to_cost\": \"boolean (TRUE if the patient mentions skipping medical tests or treatments due to cost)\",\n    \"marital_status\": \"string (categorical: 'Single', 'Married', 'Divorced', 'Widowed')\",\n    \"language_barrier\": \"boolean (TRUE if the patient has language barriers to healthcare access)\"\n  }\n'''\n```\n\n----------------------------------------\n\nTITLE: Defining Planning Prompt for Task Understanding\nDESCRIPTION: Creates a structured prompt for the LLM to interpret user requests, break them down into subtasks, and generate a step-by-step action plan for the browser agent.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nplanning_prompt = \"\"\"\nGiven a user request, define a very simple plan of subtasks (actions) to achieve the desired outcome and execute them iteratively using Playwright.\n\n1. Understand the Task:\n   - Interpret the user's request and identify the core goal.\n   - Break down the task into a few smaller, actionable subtasks to achieve the goal effectively.\n\n2. Planning Actions:\n   - Translate the user's request into a high-level plan of actions.\n   - Example actions include:\n     - Searching for specific information.\n     - Navigating to specified URLs.\n     - Interacting with website elements (clicking, filling).\n     - Extracting or validating data.\n\nInput:\n- User Request (Task)\n\nOutput from the Agent:\n- Step-by-Step Action Plan:: Return only an ordered list of actions. Only return the list, no other text.\n\n**Example User Requests and Agent Behavior:**\n\n1. **Input:** \"Search for a product on Amazon.\"\n   - **Output:**\n     1. Navigate to Amazon's homepage.\n     2. Enter the product name in the search bar and perform the search.\n     3. Extract and display the top results, including the product title, price, and ratings.\n\n2. **Input:** \"Find the cheapest flight to Tokyo.\"\n   - **Output:**\n     1. Visit a flight aggregator website (e.g. Kayak).\n     2. Enter the departure city.\n     3. Enter the destination city\n     4. Enter the start and end dates.\n     5. Extract and compare the flight options, highlighting the cheapest option.\n\n3. **Input:** \"Buy tickets for the next Warriors game.\"\n   - **Output:**\n     1. Navigate to a ticket-selling platform (e.g., Ticketmaster).\n     2. Fill the search bar with the team name.\n     2. Search for upcoming team games.\n     3. Select the next available game and purchase tickets for the specified quantity.\n\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Processing Llama Response in Agent Call Method\nDESCRIPTION: Code snippet from the Agent class's __call__ method that parses Llama's responses, determining if it's a tool calling specification or a direct response to return to the user.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n    try:\n      res = json.loads(result.split(\"<|python_tag|>\")[-1])\n      function_name = res['name']\n      parameters = res['parameters']\n      return {\"function_name\": function_name,\n              \"parameters\": parameters}\n    except:\n      return result\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Tools for Stock Information Retrieval\nDESCRIPTION: Create two LangChain tools: get_stock_info for current stock data and get_historical_price for historical stock prices. These tools use the yfinance API to fetch real-time and historical stock information.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/llama3-stock-market-function-calling/llama3-stock-market-function-calling.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\n\n@tool\ndef get_stock_info(symbol, key):\n    '''Return the correct stock info value given the appropriate symbol and key. Infer valid key from the user prompt; it must be one of the following:\n\n    address1, city, state, zip, country, phone, website, industry, industryKey, industryDisp, sector, sectorKey, sectorDisp, longBusinessSummary, fullTimeEmployees, companyOfficers, auditRisk, boardRisk, compensationRisk, shareHolderRightsRisk, overallRisk, governanceEpochDate, compensationAsOfEpochDate, maxAge, priceHint, previousClose, open, dayLow, dayHigh, regularMarketPreviousClose, regularMarketOpen, regularMarketDayLow, regularMarketDayHigh, dividendRate, dividendYield, exDividendDate, beta, trailingPE, forwardPE, volume, regularMarketVolume, averageVolume, averageVolume10days, averageDailyVolume10Day, bid, ask, bidSize, askSize, marketCap, fiftyTwoWeekLow, fiftyTwoWeekHigh, priceToSalesTrailing12Months, fiftyDayAverage, twoHundredDayAverage, currency, enterpriseValue, profitMargins, floatShares, sharesOutstanding, sharesShort, sharesShortPriorMonth, sharesShortPreviousMonthDate, dateShortInterest, sharesPercentSharesOut, heldPercentInsiders, heldPercentInstitutions, shortRatio, shortPercentOfFloat, impliedSharesOutstanding, bookValue, priceToBook, lastFiscalYearEnd, nextFiscalYearEnd, mostRecentQuarter, earningsQuarterlyGrowth, netIncomeToCommon, trailingEps, forwardEps, pegRatio, enterpriseToRevenue, enterpriseToEbitda, 52WeekChange, SandP52WeekChange, lastDividendValue, lastDividendDate, exchange, quoteType, symbol, underlyingSymbol, shortName, longName, firstTradeDateEpochUtc, timeZoneFullName, timeZoneShortName, uuid, messageBoardId, gmtOffSetMilliseconds, currentPrice, targetHighPrice, targetLowPrice, targetMeanPrice, targetMedianPrice, recommendationMean, recommendationKey, numberOfAnalystOpinions, totalCash, totalCashPerShare, ebitda, totalDebt, quickRatio, currentRatio, totalRevenue, debtToEquity, revenuePerShare, returnOnAssets, returnOnEquity, freeCashflow, operatingCashflow, earningsGrowth, revenueGrowth, grossMargins, ebitdaMargins, operatingMargins, financialCurrency, trailingPegRatio\n    \n    If asked generically for 'stock price', use currentPrice\n    '''\n    data = yf.Ticker(symbol)\n    stock_info = data.info\n    return stock_info[key]\n\n\n@tool\ndef get_historical_price(symbol, start_date, end_date):\n    \"\"\"\n    Fetches historical stock prices for a given symbol from 'start_date' to 'end_date'.\n    - symbol (str): Stock ticker symbol.\n    - end_date (date): Typically today unless a specific end date is provided. End date MUST be greater than start date\n    - start_date (date): Set explicitly, or calculated as 'end_date - date interval' (for example, if prompted 'over the past 6 months', date interval = 6 months so start_date would be 6 months earlier than today's date). Default to '1900-01-01' if vaguely asked for historical price. Start date must always be before the current date\n    \"\"\"\n\n    data = yf.Ticker(symbol)\n    hist = data.history(start=start_date, end=end_date)\n    hist = hist.reset_index()\n    hist[symbol] = hist['Close']\n    return hist[['Date', symbol]]\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Retrieval Function in Python\nDESCRIPTION: This function performs vector retrieval by generating embeddings for a query and finding the most similar items in a vector index using cosine similarity.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef vector_retreival(query: str, top_k: int = 5, vector_index: np.ndarray = None) -> List[int]:\n    \"\"\"\n    Retrieve the top-k most similar items from an index based on a query.\n    Args:\n        query (str): The query string to search for.\n        top_k (int, optional): The number of top similar items to retrieve. Defaults to 5.\n        index (np.ndarray, optional): The index array containing embeddings to search against. Defaults to None.\n    Returns:\n        List[int]: A list of indices corresponding to the top-k most similar items in the index.\n    \"\"\"\n\n    query_embedding = generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0]\n    similarity_scores = cosine_similarity([query_embedding], vector_index)\n\n    return list(np.argsort(-similarity_scores)[0][:top_k])\n```\n\n----------------------------------------\n\nTITLE: Processing Multiple Clinical Notes and Structuring SDOH Data\nDESCRIPTION: This code processes multiple clinical notes, extracts SDOH data using the Groq API, and structures the results into a pandas DataFrame for analysis.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-social-determinants-of-health/SDOH-Json-mode.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Total latency: 4s\n\nmodel = \"llama3-8b-8192\"\n\npatients_data = []\n# Loop through each patient clinical note, extract structured SDOH and compile a list of JSON objects\nfor file_name in text_files:\n    with open(os.path.join(folder_path, file_name), 'r') as file:\n        clinical_note = file.read()\n        user_prompt = user_prompt_template.format(clinical_note=clinical_note)\n        social_determinants_json = extract_sdoh_json(system_prompt,user_prompt,model)\n        social_determinants_json['mrn'] = file_name[:-4] # The name of the file is the patient's MRN \n        patients_data.append(social_determinants_json)\n\n# Flatten the results into a dataframe\nflattened_data = []\nfor patient in patients_data:\n    flattened_data.append({'mrn': patient['mrn'],\n                           'employment_status': patient['employment_status'],\n                           'financial_stress': patient['financial_stress'],\n                           'housing_insecurity': patient['housing_insecurity'],\n                           'neighborhood_unsafety': patient['neighborhood_unsafety'],\n                           'food_insecurity': patient['food_insecurity'],\n                           'education_level': patient['education_level'],\n                           'transportation_inaccessibility': patient['transportation_inaccessibility'],\n                           'social_isolation': patient['social_isolation'],\n                           'health_insurance_inadequacy': patient['health_insurance_inadequacy'],\n                           'skipped_care_due_to_cost': patient['skipped_care_due_to_cost'],\n                           'marital_status': patient['marital_status'],\n                           'language_barrier': patient['language_barrier']})\n\n\nsdoh_df = pd.DataFrame(flattened_data)\n\nsdoh_df\n```\n\n----------------------------------------\n\nTITLE: Using Vector Retrieval Function in Python\nDESCRIPTION: This snippet demonstrates how to use the vector_retreival function with a specific query and vector index.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nvector_retreival(query = \"What are 'skip-level' meetings?\", top_k = 5, vector_index = contextual_embeddings)\n```\n\n----------------------------------------\n\nTITLE: Downloading Movie Dataset\nDESCRIPTION: Downloading and setting up the movie dataset for RAG implementation\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/text_RAG_using_llama_on_together.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Let's get the movies dataset\n!wget https://raw.githubusercontent.com/togethercomputer/together-cookbook/refs/heads/main/datasets/movies.json\n!mkdir datasets\n!mv movies.json datasets/movies.json\n```\n\n----------------------------------------\n\nTITLE: Reinitializing ReAct Agent for Lower-Level Control\nDESCRIPTION: Creates a new instance of the ReAct agent with the same tools for demonstrating lower-level control and debugging capabilities.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nagent = ReActAgent.from_tools(\n    query_engine_tools,\n    llm=llm,\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Forcing Function Calling with Greeting\nDESCRIPTION: Testing how the model handles a greeting when function calling is forced with a specific function name, even though the query isn't related to the function.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"hi!\",\n    }\n]\nresponse = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=messages,\n    functions=functions,\n    function_call={\"name\": \"get_current_weather\"}, # default is auto (let LLM decide if using function call or not. can also be none, or a dict {\"name\": \"func_name\"}\n    temperature=0\n)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Setting Groq API Key\nDESCRIPTION: Configuring the environment variable for Groq API authentication.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ['GROQ_API_KEY'] = 'your_groq_api_key'\n```\n\n----------------------------------------\n\nTITLE: Generating Contextual RAG Prompts for Chunks\nDESCRIPTION: Defines a function to generate prompts for each chunk, incorporating the full document context. These prompts are used for generating contextual explanations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\ndef generate_prompts(document : str, chunks : List[str]) -> List[str]:\n  prompts = []\n  for chunk in chunks:\n    prompt = CONTEXTUAL_RAG_PROMPT.format(WHOLE_DOCUMENT=document, CHUNK_CONTENT=chunk)\n    prompts.append(prompt)\n  return prompts\n```\n\n----------------------------------------\n\nTITLE: Executing Function and Capturing Result\nDESCRIPTION: Parsing function arguments, calling the appropriate function, and storing the result in a variable for further processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nargs = json.loads(response.choices[0].message.function_call.arguments)\nobservation = known_functions[function_call.name](args)\n```\n\n----------------------------------------\n\nTITLE: Running Image Labeling Script with Llama-3.2-11B-Vision-Instruct\nDESCRIPTION: This command executes the label_script.py to process and label clothing images using the Llama 3.2 vision model. It requires specifying a Hugging Face token, input image directory, output path, and the number of GPUs to utilize for parallel processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython scripts/label_script.py --hf_token \"your_huggingface_token_here\" \\\n    --input_path \"../MM-Demo/images_compressed\" \\\n    --output_path \"../MM-Demo/output/\" \\\n    --num_gpus N\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries\nDESCRIPTION: Installation of necessary Python packages including byaldi for ColQwen2 integration, together for API access, and pdf2image for PDF processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/multimodal_RAG_with_nvidia_investor_slide_deck.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install byaldi together pdf2image\n```\n\n----------------------------------------\n\nTITLE: Installing Llama-Cookbook with All Dependencies for Development\nDESCRIPTION: Complete installation from source with all optional dependencies for development and contribution to the project.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/README.md#2025-04-07_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ngit clone git@github.com:meta-llama/llama-cookbook.git\ncd llama-cookbook\npip install -U pip setuptools\npip install -e .[tests,auditnlg,vllm]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Top 5 Similar Items in Python\nDESCRIPTION: This code retrieves the top 5 most similar items based on cosine similarity scores.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntop_5_indices = indices[0][:5]\ntop_5_indices\n```\n\n----------------------------------------\n\nTITLE: Displaying Final User-Friendly Response\nDESCRIPTION: Extracting and displaying just the content of the final response message which provides the weather information in natural language format.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nresponse.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Reranking Search Results Using Llama-Rank in Python\nDESCRIPTION: This snippet uses the Llama-Rank model to rerank the top 6 documents from hybrid search based on relevance to the query.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\n\nquery = \"What are 'skip-level' meetings?\" # we keep the same query - can change if we want\n\nresponse = client.rerank.create(\n  model=\"Salesforce/Llama-Rank-V1\",\n  query=query,\n  documents=hybrid_top_k_docs,\n  top_n=3 # we only want the top 3 results\n)\n\nfor result in response.results:\n    print(f\"Document: {hybrid_top_k_docs[result.index]}\")\n    print(f\"Relevance Score: {result.relevance_score}\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for API Keys and Tracing\nDESCRIPTION: Sets up environment variables for LangChain tracing and API keys for Tavily and Groq services. This is essential for authentication and logging.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\nos.environ['LANGCHAIN_API_KEY'] = 'LANGCHAIN_API_KEY'\n\nos.environ['TAVILY_API_KEY'] = 'YOUR_TAVILY_API_KEY'\nos.environ['GROQ_API_KEY'] = 'YOUR_GROQ_API_KEY'\n```\n\n----------------------------------------\n\nTITLE: Modifying Webhook for Messenger Platform Integration\nDESCRIPTION: JavaScript code for the webhook that receives messages from the Messenger Platform and forwards them to the Llama 3 enabled web app. It also handles initial webhook verification.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/messenger_chatbot/messenger_llama3.md#2025-04-07_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n\"use strict\";\n\n// Imports dependencies and set up http server\nconst request = require(\"request\"),\n  express = require(\"express\"),\n  body_parser = require(\"body-parser\"),\n  axios = require(\"axios\").default,\n  app = express().use(body_parser.json()); // creates express http server\n\n// Sets server port and logs message on success\napp.listen(process.env.PORT || 1337, () => console.log(\"webhook is listening\"));\n\n// Accepts POST requests at /webhook endpoint\napp.post(\"/webhook\", (req, res) => {\n  // Parse the request body from the POST\n  let body = req.body;\n\n  let sender = req.body[\"entry\"][0][\"messaging\"][0]['sender']['id']\n  let recipient = req.body[\"entry\"][0][\"messaging\"][0]['recipient']['id']\n  let message = req.body[\"entry\"][0][\"messaging\"][0]['message']['text']\n\n  // Check if this is an event from a page subscription\n  if (body.object === \"page\") {\n    // Returns a '200 OK' response to all requests\n    res.status(200).send(\"EVENT_RECEIVED\");\n\n    let url = \"http://<web server public IP>:5000/msgrcvd_page?sender=\" + sender + \"&recipient=\" + recipient + \"&message=\" + encodeURIComponent(message)\n    console.log(url)\n\n    axios.get(url)\n      .then(response => {\n        // Handle the response data\n        console.log(response.data);\n      })\n      .catch(error => {\n        // Handle errors\n        console.error('Axios error:', error);\n      });\n    } else {\n      // Return a '404 Not Found' if event is not from a page subscription\n      res.sendStatus(404);\n    }\n  });\n\n// Accepts GET requests at the /webhook endpoint. You need this URL to setup webhook initially.\n// info on verification request payload: https://developers.facebook.com/docs/graph-api/webhooks/getting-started#verification-requests\napp.get(\"/webhook\", (req, res) => {\n  /**\n   * UPDATE YOUR VERIFY TOKEN\n   *This will be the Verify Token value when you set up webhook\n  **/\n  const verify_token = process.env.VERIFY_TOKEN;\n\n  // Parse params from the webhook verification request\n  let mode = req.query[\"hub.mode\"];\n  let token = req.query[\"hub.verify_token\"];\n  let challenge = req.query[\"hub.challenge\"];\n\n  // Check if a token and mode were sent\n  if (mode && token) {\n    // Check the mode and token sent are correct\n    if (mode === \"subscribe\" && token === verify_token) {\n      // Respond with 200 OK and challenge token from the request\n      console.log(\"WEBHOOK_VERIFIED: \" + token);\n      res.status(200).send(challenge);\n    } else {\n      // Responds with '403 Forbidden' if verify tokens do not match\n      res.sendStatus(403);\n    }\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Saving Generated Fashion Captions to CSV\nDESCRIPTION: Code to write the generated fashion descriptions to a CSV file for easy review and further processing. Stores both the image filename and corresponding generated description.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport csv\n# Write the results to a CSV file\nwith open(output_csv_file_path, \"w\", newline=\"\") as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow([\"filename\", \"description\"])\n    for result in results:\n        writer.writerow(result)\n```\n\n----------------------------------------\n\nTITLE: Initializing HuggingFace Embeddings for RAG\nDESCRIPTION: Sets up HuggingFaceEmbeddings using a specific model for generating embeddings in the RAG process.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nembeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n                                       model_kwargs={'device': 'cuda'})\n```\n\n----------------------------------------\n\nTITLE: Defining Model Chat Function for User Interaction\nDESCRIPTION: This function handles user input, sends it to the Llama model, and manages the chat history. It uses the Groq client to interact with the llama-3.3-70b-versatile model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/Agents_Tutorial/Tool_Calling_201.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef model_chat(user_input: str, temperature: int = 0, max_tokens=2048):\n    \n    main_model_chat_history.append({\"role\": \"user\", \"content\": user_input})\n    \n    #print(chat_history)\n    \n    #print(\"User: \", user_input)\n    \n    response = client.chat.completions.create(model=\"llama-3.3-70b-versatile\",\n                                          messages=main_model_chat_history,\n                                          max_tokens=max_tokens,\n                                          temperature=temperature)\n    \n    main_model_chat_history.append({\n    \"role\": \"assistant\",\n    \"content\": response.choices[0].message.content\n    })\n    \n    \n    #print(\"Assistant:\", response.choices[0].message.content)\n    \n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Downloading and Parsing Financial Report PDF in Python\nDESCRIPTION: This code downloads a financial report PDF, creates a temporary directory, and uses LlamaParse to extract the content. It then loads the parsed data using SimpleDirectoryReader.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Contextual-Chunking-RAG/Example_FinancialReport_RAG.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# download the pdf file into ./data folder \n!wget -P data/ https://github.com/patronus-ai/financebench/raw/main/pdfs/AMAZON_2015_10K.pdf\n!mkdir temp\n\n# Parse pdf file\nparser = LlamaParse(\n    api_key=LLAMAPARSE_API_KEY,\n    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n)\nfile_extractor = {\".pdf\": parser}\ndocuments = SimpleDirectoryReader(input_files=['./data/AMAZON_2015_10K.pdf'], file_extractor=file_extractor).load_data()\nprint(\"pdf file pages:\", len(documents))\n```\n\n----------------------------------------\n\nTITLE: RAFT Dataset JSON Structure Example\nDESCRIPTION: Example of the RAFT format JSON structure showing question, context with distractors, and metadata fields.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/RAFT-Chatbot/README.md#2025-04-07_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"id\":\"seed_task_228\",\n   \"type\":\"general\",\n   \"question\":\"What is the context length supported by Llama 3 models?\",\n   \"context\":{\n      \"sentences\":[\n         [\n            \"DISTRACT_DOCS 1\"\n            \"DISTRACT_DOCS 2\"\n            \"We hope that Code Llama will inspire others to leverage Llama 2 to create new innovative tools for research and commercial products. Download the model Explore more on Code Llama Discover more about Code Llama here \\u2014 visit our resources, ranging from our research paper, getting started guide and more. Code Llama GitHub repository Research paper Download the model Getting started guide Meta Llama 3 Build the future of AI with Meta Llama 3 Now available with both 8B and 70B pretrained and instruction-tuned versions to support a wide range of applications Build the future of AI with Meta Llama 3 Now available with both 8B and 70B pretrained and instruction-tuned versions to support a wide range of applications Get Started Experience Llama 3 on Meta AI Experience Llama 3 with Meta AI We\\u2019ve integrated Llama 3 into Meta AI, our intelligent assistant, that expands the ways people can get things done, create and connect with Meta AI. You can see first-hand the performance of Llama 3 by using Meta AI for coding tasks and problem solving. Whether you're developing agents, or other AI-powered applications, Llama 3 in both 8B and 70B will offer the capabilities and flexibility you need to develop your ideas. Experience Llama 3 on Meta AI Enhanced performance Experience the state-of-the-art performance of Llama 3, an openly accessible model that excels at language nuances, contextual understanding, and complex tasks like translation and dialogue generation. With enhanced scalability and performance, Llama 3 can handle  multi-step tasks effortlessly, while our refined post-training processes significantly lower false refusal rates, improve response alignment, and boost diversity in model answers. Additionally, it drastically elevates capabilities like reasoning, code generation, and instruction following. Build the future of AI with Llama 3. Download Llama 3 Getting Started Guide With each Meta Llama request, you will receive: Meta Llama Guard 2 Getting started guide Responsible Use Guide Acceptable use policy Model card Community license agreement Benchmarks Llama 3 models take data and scale to new heights. It\\u2019s been trained on our two recently announced custom-built 24K GPU clusters on over 15T token of data \\u2013 a training dataset 7x larger than that used for Llama 2, including 4x more code. This results in the most capable Llama model yet, which supports a 8K context length that doubles the capacity of Llama 2. Model card Trust & safety A comprehensive approach to responsibility With the release of Llama 3, we\\u2019ve updated the Responsible Use Guide (RUG) to provide the most comprehensive information on responsible development with LLMs. Our system-centric approach includes updates to our trust and safety tools with Llama Guard 2, optimized to support the newly announced taxonomy published by MLCommons expanding its coverage to a more comprehensive set of safety categories, Code Shield, and Cybersec Eval 2. In line with the principles outlined in our RUG , we recommend thorough checking and filtering of all inputs to and outputs from LLMs based on your unique content guidelines for your intended use case and audience. Meta Llama Guard 2 Explore more on Meta Llama 3 Introducing Meta Llama 3: The most capable openly available LLM to date Read the blog Meet Your New Assistant: Meta AI, Built With Llama 3 Learn more Meta Llama 3 repository View repository Model card Explore Meta Llama 3 License META LLAMA 3 COMMUNITY LICENSE AGREEMENT Meta Llama 3 Version Release Date: April 18, 2024 \\u201c Agreement \\u201d means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein. \\u201c Documentation \\u201d means the specifications, manuals and documentation accompanying Meta Llama 3 distributed by Meta at https:\\/\\/llama.meta.com\\/get-started\\/.\",\n            \"DISTRACT_DOCS 3\"\n            \"DISTRACT_DOCS 4\"\n         ]\n      ],\n      \"title\":[\n         [\n            \"placeholder_title\",\n            \"placeholder_title\",\n            \"placeholder_title\",\n            \"placeholder_title\",\n            \"placeholder_title\"\n         ]\n      ]\n   }\n```\n\n----------------------------------------\n\nTITLE: Creating Document Index\nDESCRIPTION: Indexes the PDF document using ColQwen2, storing both embeddings and base64 images of pages.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/multimodal_RAG_with_nvidia_investor_slide_deck.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nindex_name = \"nvidia_index\"\nmodel.index(input_path=Path(\"nvidia_presentation.pdf\"),\n    index_name=index_name,\n    store_collection_with_index=True, # Stores base64 images along with the vectors\n    overwrite=True\n)\n```\n\n----------------------------------------\n\nTITLE: Audio Playback Function Implementation in Python\nDESCRIPTION: Implements audio playback functionality using IPython.display.Audio for handling audio URLs and playback control.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_tool_calling_agent.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Audio\n\ndef play_audio(output_url):\n    return Audio(url=output_url, autoplay=False)\n\naudio_url = event['messages'][-2].content\nplay_audio(audio_url)\n```\n\n----------------------------------------\n\nTITLE: Querying Agent for Specific Information\nDESCRIPTION: Asks the agent a specific question about the evaluation dataset and results in the LongLoRA paper to demonstrate its ability to retrieve targeted information.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.query(\n    \"Tell me about the evaluation dataset used in LongLoRA, \"\n    \"and then tell me about the evaluation results\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML for Meta Llama Model Evaluation\nDESCRIPTION: YAML configuration for Meta Llama model evaluation, specifying tensor and data parallel processing parameters. This configuration is used to control model partitioning across GPUs for evaluation tasks.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/meta_eval/README.md#2025-04-07_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntensor_parallel_size: 1 # The VLLM argument that specify the tensor parallel size for the model, eg how many GPUs to use for a model copy.\n\ndata_parallel_size: 4 # The VLLM argument that specify the data parallel size for the model, eg how copies of model will be used.\n```\n\n----------------------------------------\n\nTITLE: Plotting Stock Prices Over Time using Plotly in Python\nDESCRIPTION: This function takes a list of DataFrames containing historical price data for stocks and creates a plot using Plotly. The resulting plot is saved in the same directory as the application.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/groqing-the-stock-market-function-calling-llama3/README.md#2025-04-07_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef plot_price_over_time(historical_price_dfs):\n```\n\n----------------------------------------\n\nTITLE: Creating RetrievalQA Chain for Chatbot\nDESCRIPTION: This snippet creates the RetrievalQA chain using the defined LLM, retriever, and prompt template. It sets up the chain for question-answering based on retrieved context.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm, \n    retriever=retriever,     \n    chain_type_kwargs={\n        \"prompt\": PromptTemplate(\n            template=template,\n            input_variables=[\"context\", \"question\"],\n        ),\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Cosine Similarity for Vector Search in Python\nDESCRIPTION: This snippet calculates cosine similarity between a query embedding and movie embeddings, then sorts the results to find the most similar items.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Calculate cosine similarity between the query embedding and each movie embedding\nsimilarity_scores = cosine_similarity([query_embedding], contextual_embeddings)\nindices = np.argsort(-similarity_scores)\n```\n\n----------------------------------------\n\nTITLE: Implementing Text-to-SQL Function\nDESCRIPTION: Creates a function that generates SQL queries using Groq API's JSON mode based on user questions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-function-calling-for-sql/json-mode-function-calling-for-sql.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef text_to_sql(client,system_prompt,user_question,model):\n\n    completion = client.chat.completions.create(\n        model = model,\n        response_format = {\"type\": \"json_object\"},\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": user_question\n            }\n        ]\n    )\n  \n    return completion.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Dataset Loading and API Setup\nDESCRIPTION: Initializing Together API client and loading the dataset from CSV file\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_3_RAG_Setup_and_Validation.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport os\nfrom together import Together\n\nos.environ[\"TOGETHER_API_KEY\"] = \"\"\nclient = Together(api_key=os.environ.get('TOGETHER_API_KEY'))\n\ndf = pd.read_csv(\"./MM-Demo/final_balanced_sample_dataset.csv\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Kids vs Non-kids Distribution in Python\nDESCRIPTION: Examines the proportion of kids' clothing versus adult clothing in the dataset. This normalized count helps understand the dataset composition and potential skew.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\nDistribution of kids vs. non-kids images:\")\nprint(df['kids'].value_counts(normalize=True))\n```\n\n----------------------------------------\n\nTITLE: Implementing Legal Moves Function for Chess Game\nDESCRIPTION: Defines a function that returns all legal moves in Universal Chess Interface (UCI) format. This function will be used by the AI agents to determine valid moves.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_legal_moves(\n\n) -> Annotated[str, \"A list of legal moves in UCI format\"]:\n    return \"Possible moves are: \" + \",\".join(\n        [str(move) for move in board.legal_moves]\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating RAG QueryEngine Tools for Drake and Kendrick in Python\nDESCRIPTION: This snippet creates vector store indexes and query engines for Drake and Kendrick-related documents. It then wraps these query engines as tools for use in a ReAct agent.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndrake_index = VectorStoreIndex.from_documents(docs_drake)\ndrake_query_engine = drake_index.as_query_engine(similarity_top_k=3)\n\nkendrick_index = VectorStoreIndex.from_documents(docs_kendrick)\nkendrick_query_engine = kendrick_index.as_query_engine(similarity_top_k=3)\n```\n\nLANGUAGE: python\nCODE:\n```\ndrake_tool = QueryEngineTool(\n    drake_index.as_query_engine(),\n    metadata=ToolMetadata(\n        name=\"drake_search\",\n        description=\"Useful for searching over Drake's life.\",\n    ),\n)\n\nkendrick_tool = QueryEngineTool(\n    kendrick_index.as_query_engine(),\n    metadata=ToolMetadata(\n        name=\"kendrick_search\",\n        description=\"Useful for searching over Kendrick's life.\",\n    ),\n)\n\nquery_engine_tools = [drake_tool, kendrick_tool]\n```\n\n----------------------------------------\n\nTITLE: Converting HF Weights to LLaMA Format using Python Script\nDESCRIPTION: Python command to convert Hugging Face model weights to official LLaMA format. Requires specifying the model path (can be HF hub or local directory) and output directory.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/llama_cookbook/tools/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m llama_cookbook.tools.convert_hf_weights_to_llama --model-path meta-llama/Meta-Llama-3.1-70B-Instruct --output-dir test70B --model-size 70B\n```\n\n----------------------------------------\n\nTITLE: Clicking a Search Button using Accessibility Tree Selectors in JSON\nDESCRIPTION: Example of a JSON-structured action to click on a 'Search' button, including the current state, reasoning, action type, and selector based on the accessibility tree.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"current_state\": \"On the homepage of a search engine.\",\n    \"reasoning\": \"The accessibility tree shows a button named 'Search'. Clicking it is the appropriate next step to proceed with the task.\",\n    \"action\": \"click\",\n    \"selector\": \"button=Search\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sampling Data to Balance Categories\nDESCRIPTION: Creates a balanced dataset by sampling up to 100 items from each category, then displays the distribution statistics for both categories and types.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef sample_category(group):\n    if len(group) > 100:\n        return group.sample(n=100, random_state=42)\n    else:\n        return group\n\n# Group by New_Category and apply the sampling function\nsampled_data = result.groupby('New_Category').apply(sample_category).reset_index(drop=True)\n\n# Print the distribution of categories in the sampled data\nprint(\"Distribution of Categories in Sampled Data:\")\nprint(sampled_data['New_Category'].value_counts())\n\n# Print the distribution of types in the sampled data\nprint(\"\\nDistribution of Types in Sampled Data:\")\nprint(sampled_data['New_Type'].value_counts())\n\n# Calculate and print percentages\ntotal = len(sampled_data)\nprint(\"\\nPercentage Distribution of Categories:\")\ncategory_percentage = (sampled_data['New_Category'].value_counts() / total * 100).round(2)\nprint(category_percentage)\n\nprint(\"\\nPercentage Distribution of Types:\")\ntype_percentage = (sampled_data['New_Type'].value_counts() / total * 100).round(2)\nprint(type_percentage)\n\n# Print the total number of items in the sampled dataset\nprint(f\"\\nTotal number of items in the sampled dataset: {len(sampled_data)}\")\n```\n\n----------------------------------------\n\nTITLE: 405B Model Training Environment Variables\nDESCRIPTION: Environment variable setup for training 405B model with LoRA and 4-bit quantization on 32xH100 GPUs.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/multigpu_finetuning.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport FSDP_CPU_RAM_EFFICIENT_LOADING=1\nexport ACCELERATE_USE_FSDP=1\n```\n\n----------------------------------------\n\nTITLE: Configuring Assistant with Llama 3\nDESCRIPTION: Sets up the assistant using Llama 3 model with tool-calling capabilities via Groq integration.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_tool_calling_agent.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nfrom langchain_groq import ChatGroq\nfrom langchain_core.prompts import ChatPromptTemplate\n\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import Runnable, RunnableConfig\n\nclass Assistant:\n    \n    def __init__(self, runnable: Runnable):\n        self.runnable = runnable\n\n    def __call__(self, state: State, config: RunnableConfig):\n        while True:\n            image_url = config['configurable'].get(\"image_url\", None)\n            state = {**state, \"image_url\": image_url}\n            result = self.runnable.invoke(state)\n            if not result.tool_calls and (\n                not result.content\n                or isinstance(result.content, list)\n                and not result.content[0].get(\"text\")\n            ):\n                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n                state = {**state, \"messages\": messages}\n            else:\n                break\n        return {\"messages\": result}\n\nprimary_assistant_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant for with five tools: (1) web search, \"\n     \"(2) a custom, magic_function, (3) text to image, (4) image to text \"\n     \"(5) text to speech. Use these provided tools in response to the user question. \"\n     \"Your image url is: {image_url} \"\n     \"Current time: {time}.\"),\n    (\"placeholder\", \"{messages}\"),\n]).partial(time=datetime.now())\n\nllm = ChatGroq(temperature=0, model=\"llama3-70b-8192\")\nassistant_runnable = primary_assistant_prompt | llm.bind_tools(tools)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installation of necessary Python packages including lancedb, rerankers, and together API client\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_3_RAG_Setup_and_Validation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#!pip install lancedb rerankers together -q\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset Metadata in Python\nDESCRIPTION: Reads the CSV file containing metadata for the image dataset. This provides information about the images, including their labels and other properties that will be used for analysis and cleaning.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"./DATA/images.csv\")\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Implementing WhatsApp Client Class in Python\nDESCRIPTION: Python class for interacting with the WhatsApp Business API, including a method to send text messages.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/whatsapp_chatbot/whatsapp_llama3.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nclass WhatsAppClient:\n    API_URL = \"https://graph.facebook.com/v17.0/\"\n    WHATSAPP_API_TOKEN = \"<Temporary access token from your WhatsApp API Setup>\"\n    WHATSAPP_CLOUD_NUMBER_ID = \"<Phone number ID from your WhatsApp API Setup>\"\n\n    def __init__(self):\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.WHATSAPP_API_TOKEN}\",\n            \"Content-Type\": \"application/json\",\n        }\n        self.API_URL = self.API_URL + self.WHATSAPP_CLOUD_NUMBER_ID\n\n    def send_text_message(self, message, phone_number):\n        payload = {\n            \"messaging_product\": 'whatsapp',\n            \"to\": phone_number,\n            \"type\": \"text\",\n            \"text\": {\n                \"preview_url\": False,\n                \"body\": message\n            }\n        }\n        response = requests.post(f\"{self.API_URL}/messages\", json=payload, headers=self.headers)\n        return response.status_code\n```\n\n----------------------------------------\n\nTITLE: Executing Stock Analysis Queries with LangChain and Custom Functions in Python\nDESCRIPTION: These code snippets demonstrate how to use the `call_functions` method to process user queries about stock information and historical prices. The first query asks for the beta of Meta stock, while the second compares stock prices of Google, Apple, and Meta over the past 6 months.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/llama3-stock-market-function-calling/llama3-stock-market-function-calling.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nuser_prompt = 'What is the beta for meta stock?'\ncall_functions(llm_with_tools, user_prompt)\n\n```\n\nLANGUAGE: python\nCODE:\n```\nuser_prompt = \"Compare the stock price of Google, Apple and Meta over the past 6 months\"\ncall_functions(llm_with_tools, user_prompt)\n\n```\n\n----------------------------------------\n\nTITLE: Python Dependencies List\nDESCRIPTION: A plain text list of Python package dependencies required for the project, including crew.ai for agent workflows, langchain_groq for LLM integration, and pandas for data manipulation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/crewai-agents/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ncrewai\nlangchain_groq\npandas\n```\n\n----------------------------------------\n\nTITLE: Configuring API Keys for DeepInfra and LlamaParse in Python\nDESCRIPTION: This snippet shows how to set up API keys for DeepInfra and LlamaParse services in a Python configuration file. These keys are necessary for using Llama inference services and parsing PDF files respectively.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Contextual-Chunking-RAG/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"DEEPINFRA_API_KEY\"=\"<your_api_key>\"    \n\"LLAMAPARSE_API_KEY\"=\"<your_api_key>\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Weather Function for Function Calling\nDESCRIPTION: Creating a dummy weather function that returns hardcoded weather information for a given location and setting up a dictionary of known functions for programmatic access.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    weather_info = {\n        \"location\": location,\n        \"temperature\": \"72\",\n        \"unit\": unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n    }\n    return json.dumps(weather_info)\n\nknown_functions = {\n    \"get_current_weather\": get_current_weather\n}\n```\n\n----------------------------------------\n\nTITLE: Plotting Historical Stock Prices with Pandas and Plotly in Python\nDESCRIPTION: This function plots historical stock prices over time using Pandas for data manipulation and Plotly for visualization. It takes a list of DataFrames containing historical price data and creates a line chart for each stock symbol.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/llama3-stock-market-function-calling/llama3-stock-market-function-calling.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport plotly.graph_objects as go\n\ndef plot_price_over_time(historical_price_dfs):\n\n    full_df = pd.DataFrame(columns = ['Date'])\n    for df in historical_price_dfs:\n        full_df = full_df.merge(df, on = 'Date', how = 'outer')\n\n    # Create a Plotly figure\n    fig = go.Figure()\n    \n    # Dynamically add a trace for each stock symbol in the DataFrame\n    for column in full_df.columns[1:]:  # Skip the first column since it's the date\n        fig.add_trace(go.Scatter(x=full_df['Date'], y=full_df[column], mode='lines+markers', name=column))\n    \n    \n    # Update the layout to add titles and format axis labels\n    fig.update_layout(\n        title='Stock Price Over Time: ' + ', '.join(full_df.columns.tolist()[1:]),\n        xaxis_title='Date',\n        yaxis_title='Stock Price (USD)',\n        yaxis_tickprefix='$',\n        yaxis_tickformat=',.2f',\n        xaxis=dict(\n            tickangle=-45,\n            nticks=20,\n            tickfont=dict(size=10),\n        ),\n        yaxis=dict(\n            showgrid=True,   # Enable y-axis grid lines\n            gridcolor='lightgrey',  # Set grid line color\n        ),\n        legend_title_text='Stock Symbol',\n        plot_bgcolor='white',  # Set plot background to white\n        paper_bgcolor='white',  # Set overall figure background to white\n        legend=dict(\n            bgcolor='white',  # Optional: Set legend background to white\n            bordercolor='black'\n        )\n    )\n    \n    # Show the figure - unfortunately dynamic charts are not supported on GitHub preview, so this just generates\n    # a static .png. If running locally, you can use fig.show(renderer='iframe') to output a dynamic plotly plot\n    fig.show('png')\n\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running the Chess Game\nDESCRIPTION: Resets the chess board to the starting position and initiates a conversation between the black and white player agents to start the game, limiting it to 3 turns.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nboard = chess.Board()\n\nchat_result = player_black.initiate_chat(\n    player_white,\n    message=\"Let's play chess! Your move.\",\n    max_turns=3,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up Vector Query Tool\nDESCRIPTION: Implements a vector query function that performs filtered searches over document pages.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L2_Tool_Calling.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nfrom llama_index.core.vector_stores import FilterCondition\n\n\ndef vector_query(\n    query: str,\n    page_numbers: List[str]\n) -> str:\n    \"\"\"Perform a vector search over an index.\n\n    query (str): the string query to be embedded.\n    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n        over all pages. Otherwise, filter by the set of specified pages.\n\n    \"\"\"\n\n    metadata_dicts = [\n        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n    ]\n\n    query_engine = vector_index.as_query_engine(\n        similarity_top_k=2,\n        filters=MetadataFilters.from_dicts(\n            metadata_dicts,\n            condition=FilterCondition.OR\n        )\n    )\n    response = query_engine.query(query)\n    return response\n\n\nvector_query_tool = FunctionTool.from_defaults(\n    name=\"vector_tool\",\n    fn=vector_query\n)\n```\n\n----------------------------------------\n\nTITLE: Markdown Requirements List\nDESCRIPTION: Lists the prerequisites needed to run Prompt Guard, including access to model weights on Hugging Face and Llama recipes package installation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/prompt_guard/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n1. Access to Prompt Guard model weights on Hugging Face. To get access, follow the steps described [here](https://github.com/facebookresearch/PurpleLlama/tree/main/Prompt-Guard#download)\n2. Llama recipes package and it's dependencies [installed](https://github.com/meta-llama/llama-cookbook?tab=readme-ov-file#installing)\n```\n\n----------------------------------------\n\nTITLE: Saving Cleaned Dataset to CSV in Python\nDESCRIPTION: Exports the cleaned dataframe without corrupt images to a new CSV file. This preserves the cleaned dataset for future use without needing to repeat the corruption detection process.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf_clean.to_csv('clean.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Implementing Embedding Generation Function\nDESCRIPTION: Function to generate embeddings using Together AI API for input texts\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/text_RAG_using_llama_on_together.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport numpy as np\n\ndef generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:\n    \"\"\"Generate embeddings from Together python library.\n\n    Args:\n        input_texts: a list of string input texts.\n        model_api_string: str. An API string for a specific embedding model of your choice.\n\n    Returns:\n        embeddings_list: a list of embeddings. Each element corresponds to the each input text.\n    \"\"\"\n    together_client = together.Together(api_key = TOGETHER_API_KEY)\n    outputs = together_client.embeddings.create(\n        input=input_texts,\n        model=model_api_string,\n    )\n    return np.array([x.embedding for x in outputs.data])\n```\n\n----------------------------------------\n\nTITLE: Summarizing Long Text with LangChain's Map-Reduce Method\nDESCRIPTION: Shows how to use LangChain's 'map_reduce' summarization method as an alternative approach for summarizing the entire transcript.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/video_summary.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\nchain.run(split_docs)\n```\n\n----------------------------------------\n\nTITLE: Initializing Chess Board\nDESCRIPTION: Creates a new chess board in the starting position and initializes a flag to track when moves are made during the game.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nboard = chess.Board()\n\nmade_move = False\n```\n\n----------------------------------------\n\nTITLE: Downloading Academic Papers\nDESCRIPTION: Downloads the academic papers from the specified URLs and saves them with the corresponding filenames using wget.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor url, paper in zip(urls, papers):\n  !wget \"{url}\" -O \"{paper}\"\n```\n\n----------------------------------------\n\nTITLE: Generating Chat Completion for Presidential Speech Questions in Python\nDESCRIPTION: This function generates a response to the user's question based on relevant excerpts from presidential speeches and additional context. It uses a Groq client and a pre-trained model to process the input and produce the response.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/presidential-speeches-rag-with-pinecone/README.md#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef presidential_speech_chat_completion(client, model, user_question, relevant_excerpts, additional_context):\n```\n\n----------------------------------------\n\nTITLE: Testing Function Calling with 'auto' Parameter\nDESCRIPTION: Making a chat completion request with function_call='auto' to let the model decide whether to use function calling for a greeting.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"hi!\",\n    }\n]\nresponse = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=messages,\n    functions=functions,\n    function_call=\"auto\", # default is auto (let LLM decide if using function call or not. can also be none, or a dict {\"name\": \"func_name\"}\n    temperature=0\n)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Initializing Object Retriever\nDESCRIPTION: Creates a retriever for the object index that will return the top three most relevant tools for a given query.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nobj_retriever = obj_index.as_retriever(similarity_top_k=3)\n```\n\n----------------------------------------\n\nTITLE: Creating RAG Query Engine Tools\nDESCRIPTION: Sets up specialized tools for querying information about Drake and Kendrick using RAG query engines.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndrake_tool = QueryEngineTool(\n    drake_index.as_query_engine(),\n    metadata=ToolMetadata(\n        name=\"drake_search\",\n        description=\"Useful for searching over Drake's life.\",\n    ),\n)\n\nkendrick_tool = QueryEngineTool(\n    kendrick_index.as_query_engine(),\n    metadata=ToolMetadata(\n        name=\"kendrick_search\",\n        description=\"Useful for searching over Kendrick's life.\",\n    ),\n)\n\nquery_engine_tools = [drake_tool, kendrick_tool]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installation of necessary Python packages for working with LlamaIndex, Groq, and related components.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n!pip install llama-index-llms-groq\n!pip install llama-index-embeddings-huggingface\n!pip install llama-parse\n```\n\n----------------------------------------\n\nTITLE: Filtering Out Kids' Clothing from Dataset in Python\nDESCRIPTION: Removes kids' clothing items from the dataset and drops the 'kids' column as it's no longer needed. This focuses the dataset on adult clothing only, which is the main target for analysis.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf_no_kids = df[df['kids'] == False]\ndf_cleaned = df_no_kids.drop('kids', axis=1)\n\nprint(f\"Original dataset shape: {df.shape}\")\nprint(f\"Cleaned dataset shape: {df_cleaned.shape}\")\n```\n\n----------------------------------------\n\nTITLE: Completing a Task using Accessibility Tree Actions in JSON\nDESCRIPTION: Example of a JSON-structured action to indicate task completion, including the current state and reasoning for why no further actions are required.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"current_state\": \"Completed the search and extracted the necessary data.\",\n    \"reasoning\": \"The task goal has been achieved, and no further actions are required.\",\n    \"action\": \"finished\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Answer Grader for RAG\nDESCRIPTION: Creates a grader that assesses whether the generated answer addresses the user's question. It uses a Groq LLM with structured output to provide a binary score.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Data model\nclass GradeAnswer(BaseModel):\n    \"\"\"Binary score to assess answer addresses question.\"\"\"\n\n    score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n\n# LLM with function call \nllm = ChatGroq(temperature=0, model=\"llama3-70b-8192\")\nstructured_llm_grader = llm.with_structured_output(GradeAnswer)\n\n# Prompt \nsystem = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \n     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\nanswer_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\nanswer_grader = answer_prompt | structured_llm_grader\nanswer_grader.invoke({\"question\": question,\"generation\": generation})\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning the model with LoRA\nDESCRIPTION: Executes the fine-tuning process for a single epoch using the AdamW optimizer and a step learning rate scheduler. This process trains the model to improve its performance on the dialogue summarization task.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/quickstart_peft_finetuning.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch.optim as optim\nfrom llama_cookbook.utils.train_utils import train\nfrom torch.optim.lr_scheduler import StepLR\n\nmodel.train()\n\noptimizer = optim.AdamW(\n            model.parameters(),\n            lr=train_config.lr,\n            weight_decay=train_config.weight_decay,\n        )\nscheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)\n\n# Start the training process\nresults = train(\n    model,\n    train_dataloader,\n    eval_dataloader,\n    tokenizer,\n    optimizer,\n    scheduler,\n    train_config.gradient_accumulation_steps,\n    train_config,\n    None,\n    None,\n    None,\n    wandb_run=None,\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Nested Async Support\nDESCRIPTION: Adding support for nested async operations in Jupyter notebook.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Relevant Excerpts from Pinecone Vector Store in Python\nDESCRIPTION: This function performs a similarity search on a Pinecone vector store using the user's question to retrieve the most relevant excerpts from presidential speeches. It takes a user question and a Pinecone docsearch object as input.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/presidential-speeches-rag-with-pinecone/README.md#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef get_relevant_excerpts(user_question, docsearch):\n```\n\n----------------------------------------\n\nTITLE: Launching Gradio Demo for Llama Models in Python\nDESCRIPTION: This code snippet launches a Gradio demo interface, making it accessible on all network interfaces. It uses the queue() method to handle multiple requests and specifies the server to listen on all available network interfaces.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndemo.queue().launch(server_name=\"0.0.0.0\")\n```\n\n----------------------------------------\n\nTITLE: Processing Llama Tool Calling Output for Google API Integration\nDESCRIPTION: Implements a function to parse the tool calling outputs from Llama, call the appropriate Google API functions, and process the results to schedule meetings. It handles the sequential calls to Google Contacts and Calendar APIs.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/calendar_assistant/tool_calling_google_api.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfunctions = {\n  \"google_contact\": google_contact,\n  \"google_calendar\": google_calendar\n}\n\n\ndef process_llama_output(llama_output):\n  \"\"\"\n  Function to process the tool calling output & make the actual Google API call\n  \"\"\"\n\n  result = \"\"\n  prefix = \"<function=\"\n  suffix = \"</function>\"\n  start, N = 0, len(llama_output)\n  count = 0\n  email = None\n\n  # In this example, we are expecting Llama to produce 2 tool calling outputs\n  while count < 2:\n    begin, end = llama_output[start:].find(prefix) + len(prefix) + start, llama_output[start:].find(suffix) + start\n    func_name, params = llama_output[begin:end-1].split(\">{\") \n    end += len(suffix)\n    start = end\n    count += 1\n    params = json.loads(params.strip())\n    \n    if not email and func_name in functions:\n      email = functions[func_name](**params)\n    \n    elif email and func_name in functions:\n      attendees = {}\n      attendees[\"name\"] = params[\"attendees\"]\n      attendees[\"email\"] = email\n      params[\"attendees\"] = attendees\n\n      result += \"\\n-----------------------------------------------------\\n\"\n      result += functions[func_name](**params)\n      return result\n```\n\n----------------------------------------\n\nTITLE: Defining Function and Tool Schemas for Groq API\nDESCRIPTION: Creating function and tool schemas that define the parameters and structure for the weather function that will be used in Groq API calls.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfunctions = [\n    {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n                },\n                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n            },\n            \"required\": [\"location\"],\n        },\n    }\n]\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Loading Document Content from File in Python\nDESCRIPTION: This code snippet reads the content of a text file named 'llama_article.txt' into a string variable. It's designed to work with documents containing between 2,000 to 20,000 tokens.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Contextual-Chunking-RAG/Tutorial.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndocument_content = \"\"\nwith open('./data/llama_article.txt', 'r') as file:\n    document_content = file.read()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Balanced Dataset Distribution in Python\nDESCRIPTION: Creates a bar chart showing the distribution of clothing categories after balancing. This visualization confirms that the dataset has been successfully balanced across categories.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Plot the distribution of the balanced dataset\nplt.figure(figsize=(12, 6))\ndf_balanced['merged_category'].value_counts().plot(kind='bar')\nplt.title('Distribution of Merged Clothing Categories (Balanced)')\nplt.xlabel('Category')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\nprint(f\"Balanced dataset shape: {df_balanced.shape}\")\nprint(df_balanced['merged_category'].value_counts())\n```\n\n----------------------------------------\n\nTITLE: Installing Llama 3.1 8B Model with Ollama\nDESCRIPTION: This command downloads and runs the Llama 3.1 8B model using Ollama, which is a prerequisite for the email agent.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama run llama3.1\n```\n\n----------------------------------------\n\nTITLE: Cleaning the Dataset by Removing NaN Values and Unnecessary Columns\nDESCRIPTION: Removes rows with missing descriptions and drops the 'size' column which was originally planned but no longer needed.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Remove rows with NaN in the 'Description' column\nresult = result.dropna(subset=['Description'])\n\n# Remove the final column ('size')\nresult = result.drop(columns=['size'])\n```\n\n----------------------------------------\n\nTITLE: Evaluating Llama-3.1-8B-Instruct Model with Chat Template\nDESCRIPTION: Command to evaluate Llama-3.1-8B-Instruct model using chat template and multiturn fewshot settings with bfloat16 precision.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch -m lm_eval --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct,dtype=bfloat16  --log_samples --output_path eval_results --tasks leaderboard  --batch_size 4 --apply_chat_template --fewshot_as_multiturn\n```\n\n----------------------------------------\n\nTITLE: Processing Tool Calls and Results\nDESCRIPTION: Handling tool calls, executing functions, and appending results to messages array\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/parallel-tool-use/parallel-tool-use.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntool_calls = response_message.tool_calls\n\nmessages.append(\n    {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"id\": tool_call.id,\n                \"function\": {\n                    \"name\": tool_call.function.name,\n                    \"arguments\": tool_call.function.arguments,\n                },\n                \"type\": tool_call.type,\n            }\n            for tool_call in tool_calls\n        ],\n    }\n)\n\navailable_functions = {\n    \"get_weather\": get_weather,\n}\nfor tool_call in tool_calls:\n    function_name = tool_call.function.name\n    function_to_call = available_functions[function_name]\n    function_args = json.loads(tool_call.function.arguments)\n    function_response = function_to_call(**function_args)\n\n    messages.append(\n        {\n            \"role\": \"tool\",\n            \"content\": json.dumps(function_response),\n            \"tool_call_id\": tool_call.id,\n        }\n    )\n\nprint(json.dumps(messages, indent=2))\n```\n\n----------------------------------------\n\nTITLE: Loading YouTube Video Caption into Documents\nDESCRIPTION: Loads the YouTube video caption into LangChain Documents for further processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/video_summary.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndocs = loader.load()\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Documents\nDESCRIPTION: Loading PDF document and splitting it into manageable chunks using SentenceSplitter.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.node_parser import SentenceSplitter\n\nsplitter = SentenceSplitter(chunk_size=1024)\nnodes = splitter.get_nodes_from_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Multimodal Inference Function for Llama Guard (Python)\nDESCRIPTION: Defines a function for running inference on multimodal inputs (text and images) using the Llama Guard vision model. It supports custom categories and category exclusion, and uses the MllamaProcessor for processing inputs.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/llama_guard/llama_guard_text_and_vision_inference.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image as PIL_Image\n\ndef display_image(img: PIL_Image):\n    size=300,200\n    img.thumbnail(size)\n    display(img)\n\ndef llama_guard_mm_test(tokenizer, model, conversation, image, categories: dict[str, str]=None, excluded_category_keys: list[str]=[]):\n\n    if categories is not None:\n        llama_guard_input_templ_applied = tokenizer.apply_chat_template(\n            conversation, \n            add_generation_prompt=True, \n            tokenize=False, \n            skip_special_tokens=False, \n            categories=categories, \n            excluded_category_keys=excluded_category_keys)\n    else:\n        llama_guard_input_templ_applied = tokenizer.apply_chat_template(\n            conversation, \n            add_generation_prompt=True, \n            tokenize=False, \n            skip_special_tokens=False, \n            excluded_category_keys=excluded_category_keys)\n    \n    inputs = tokenizer(text=llama_guard_input_templ_applied, images=image, return_tensors=\"pt\").to(\"cuda\")\n    output = model.generate(\n                        **inputs, \n                        do_sample=False, \n                        top_p=None,\n                        temperature=None,\n                        max_new_tokens=50,)\n    response = tokenizer.decode(output[0][len(inputs['input_ids'][0]):], skip_special_tokens=False)\n\n    return llama_guard_input_templ_applied, response\n```\n\n----------------------------------------\n\nTITLE: Binding Custom Tools with Llama 3 Model for Stock Analysis\nDESCRIPTION: Combine the custom tools with the Llama 3 model using LangChain's bind_tools method, enabling the LLM to access and use these tools for stock-related queries.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/llama3-stock-market-function-calling/llama3-stock-market-function-calling.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntools = [get_stock_info, get_historical_price]\nllm_with_tools = llm.bind_tools(tools)\n```\n\n----------------------------------------\n\nTITLE: Importing Function Registration from AutoGen\nDESCRIPTION: Imports the register_function utility from AutoGen, which allows functions to be registered as tools that can be called by the agents.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen import register_function\n```\n\n----------------------------------------\n\nTITLE: Selecting Test Image Path in Python\nDESCRIPTION: Specifies the path to a test image for synthetic labeling. This selects a colorful shirt image from the dataset to demonstrate the model's vision capabilities.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimage_path = f\"{IMAGES}/01938e19-ece6-4f67-8e48-bfd6dd6ce399.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Final Completion Request\nDESCRIPTION: Making the final API call with processed tool results and tool definitions\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/parallel-tool-use/parallel-tool-use.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n)\n\nprint(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Llama-3.1-70B-Instruct Model with Tensor Parallelism\nDESCRIPTION: Command to evaluate the large 70B Instruct model using tensor parallelism for distributed computation across multiple GPUs.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nlm_eval --model hf --batch_size 4 --model_args pretrained=meta-llama/Llama-3.1-70B-Instruct,parallelize=True --tasks leaderboard --log_samples --output_path eval_results --apply_chat_template --fewshot_as_multiturn\n```\n\n----------------------------------------\n\nTITLE: Evaluating 8B Instruct Baseline\nDESCRIPTION: Runs the evaluation script for the Meta Llama 3 8B Instruct baseline model, specifying the model endpoint, judge endpoint, and RAG retrieval parameters. This allows for comparison with the RAFT model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/RAFT-Chatbot/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=4 python raft_eval.py -m meta-Llama/Meta-Llama-3-8B-Instruct -u \"http://localhost:8000/v1\" -j \"http://localhost:8001/v1\" -r 5\n```\n\n----------------------------------------\n\nTITLE: Initializing Llama 3.1 Model with Replicate\nDESCRIPTION: This code initializes the Llama 3.1 405b chat model from Replicate, setting parameters like temperature and maximum new tokens.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/RAG/hello_llama_cloud.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.llms import Replicate\nllm = Replicate(\n    model=\"meta/meta-llama-3.1-405b-instruct\",\n    model_kwargs={\"temperature\": 0.0, \"top_p\": 1, \"max_new_tokens\":500}\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataset Loading in YAML for MMLU-Pro Evaluation\nDESCRIPTION: Basic YAML configuration for loading MMLU-Pro evaluation dataset, specifying the dataset path, name and test split for Llama 3.1 8B Instruct model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/meta_eval/README.md#2025-04-07_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ntask: meta_mmlu_pro_instruct\ndataset_path: meta-llama/Llama-3.1-8B-Instruct-evals\ndataset_name: Llama-3.1-8B-Instruct-evals__mmlu_pro__details\ntest_split: latest\n```\n\n----------------------------------------\n\nTITLE: Running Single-Node Multi-GPU Inference with vLLM\nDESCRIPTION: Command for launching Llama inference on a single node with multiple GPUs. The tp_size parameter should be set to the number of available GPUs for tensor parallelism.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/vllm/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython inference.py --model_name $MODEL_PATH --peft_model_name $PEFT_MODEL_PATH --tp_size 8 --user_prompt \"Hello my name is\"\n```\n\n----------------------------------------\n\nTITLE: Executing Function Calls based on User Prompts in Python\nDESCRIPTION: This function processes the user's question, invokes the appropriate tool (get_stock_info or get_historical_price), and generates a response. It also triggers the plot generation for historical price queries.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/groqing-the-stock-market-function-calling-llama3/README.md#2025-04-07_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef call_functions(llm_with_tools, user_prompt):\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Import statements for LangGraph, LangChain, and type hinting components needed for the agent implementation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n```\n\n----------------------------------------\n\nTITLE: Converting Llama Model Weights to Hugging Face Format\nDESCRIPTION: Script for converting original Llama model weights from Meta to the Hugging Face format. Requires installing Transformers from source and using their conversion utility.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n## Install Hugging Face Transformers from source\npip freeze | grep transformers ## verify it is version 4.45.0 or higher\n\ngit clone git@github.com:huggingface/transformers.git\ncd transformers\npip install protobuf\npython src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n   --input_dir /path/to/downloaded/llama/weights --model_size 3B --output_dir /output/path\n```\n\n----------------------------------------\n\nTITLE: Displaying Loaded Image in Python\nDESCRIPTION: Displays the loaded test image in the notebook to verify it was correctly loaded. This visualization confirms the image that will be used for synthetic labeling with the Llama 3.2 model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimage\n```\n\n----------------------------------------\n\nTITLE: Deploying Meta Llama 3 Model with Docker\nDESCRIPTION: Docker command to deploy a Meta Llama 3 8B chat model using the Text-generation-inference framework as an API server.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmodel=meta-llama/Meta-Llama-3.1-8B-Instruct\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\ntoken=#your-huggingface-token\ndocker run --gpus all --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:2.0 --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Creating Tools for Initial Paper Set\nDESCRIPTION: Loops through the initial set of papers to create vector and summary tools for each paper, storing them in a dictionary.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\npaper_to_tools_dict = {}\nfor paper in papers:\n    print(f\"Getting tools for paper: {paper}\")\n    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n```\n\n----------------------------------------\n\nTITLE: Displaying Knowledge Graph\nDESCRIPTION: Code to display the generated knowledge graph visualization using IPython's display functionality.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/knowledge_graphs_with_structured_outputs.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Image, display\n\n# Display the image of the knowledge graph\ndisplay(Image(filename='knowledge_graph.png'))\n```\n\n----------------------------------------\n\nTITLE: Importing Agent and Tool Libraries in Python\nDESCRIPTION: This code imports necessary libraries for creating agents and tools, including Llama Index components and asyncio utilities.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom typing import Sequence, List\n\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.core.tools import BaseTool, FunctionTool\nfrom llama_index.agent.openai import OpenAIAgent\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Pretrained Model Benchmark\nDESCRIPTION: Command to execute the benchmark script for pretrained models using vLLM. This script uses configurable random token prompts to simulate text completion tasks.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/on_prem/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython pretrained_vllm_benchmark.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Fireworks API\nDESCRIPTION: Sets up the Fireworks API key for accessing Llama 3 model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L2_Tool_Calling.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os \n\nos.environ['FIREWORKS_API_KEY'] = 'xxx' # get a free key at https://fireworks.ai/api-keys\n```\n\n----------------------------------------\n\nTITLE: Generating Podcast Script from PDF Text\nDESCRIPTION: Calls the generate_script function to create a podcast script from the extracted PDF text using the Llama 3.1 model and predefined dialogue schema.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nscript = generate_script(SYSTEM_PROMPT, text, Dialogue)\n```\n\n----------------------------------------\n\nTITLE: Testing Weather Query with Disabled Function Calling\nDESCRIPTION: Testing how the model handles a weather-related query when function calling is explicitly disabled with function_call='none'.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What's the weather in Boston?\",\n    }\n]\nresponse = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=messages,\n    functions=functions,\n    function_call=\"none\", # default is auto (let LLM decide if using function call or not. can also be none, or a dict {\"name\": \"func_name\"}\n    temperature=0\n)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Initializing Groq Client for API Access\nDESCRIPTION: Setting up the Groq client with API key to access the Llama 3 model through Groq's API endpoints that support tool use functionality.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# https://console.groq.com/docs/tool-use#models\n# Groq API endpoints support tool use for programmatic execution of specified operations through requests with explicitly defined\n# operations. With tool use, Groq API model endpoints deliver structured JSON output that can be used to directly invoke functions.\n\nfrom groq import Groq\nimport os\nimport json\n\nclient = Groq(api_key = 'your_groq_api_key' # get a free key at https://console.groq.com/keys')\n```\n\n----------------------------------------\n\nTITLE: Viewing Full Response from Groq API\nDESCRIPTION: Printing the complete response object returned by the Groq API to examine all available data.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresponse\n```\n\n----------------------------------------\n\nTITLE: Displaying Category Value Counts\nDESCRIPTION: Shows the distribution of values in the 'Category' column of the cleaned dataset.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\nCategory Counts:\")\nprint(result['Category'].value_counts())\n```\n\n----------------------------------------\n\nTITLE: Extracting Knowledge Triples from Text using Llama in Python\nDESCRIPTION: This function uses the Llama model to extract subject-relation-object triples from input text. It formats the extracted knowledge in a structured format suitable for storing in a graph database.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/conversational-chatbot-langchain/requirements.txt#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef extract_triples(text):\n    \"\"\"Extract knowledge triples from text using Llama\"\"\"\n    system_prompt = \"\"\"\n    You are a knowledge triple extraction system. Your task is to extract knowledge triples from the given text. \n    A knowledge triple consists of a subject, relation, and object.\n    \n    Format your response as a JSON array of arrays, where each inner array contains exactly three strings: [subject, relation, object].\n    Only respond with the JSON array, nothing else.\n    \"\"\"\n    \n    user_message = f\"Extract knowledge triples from the following text: {text}\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_message}\n    ]\n    \n    response = llama.chat_completion(messages=messages)\n    result_text = response[\"choices\"][0][\"message\"][\"content\"]\n    \n    # Extract the JSON part from the response\n    try:\n        # Find JSON array in the response\n        json_match = re.search(r'\\[\\s*\\[.+?\\]\\s*\\]', result_text, re.DOTALL)\n        if json_match:\n            json_str = json_match.group(0)\n            triples = json.loads(json_str)\n            return triples\n        else:\n            # Try to parse the entire response as JSON\n            triples = json.loads(result_text)\n            return triples\n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to extract triples using regex\n        pattern = r'\\[\\s*\"(.+?)\"\\s*,\\s*\"(.+?)\"\\s*,\\s*\"(.+?)\"\\s*\\]'\n        matches = re.findall(pattern, result_text)\n        if matches:\n            return [list(match) for match in matches]\n        else:\n            return []\n\n# Example usage\ntext = \"\"\"Alan Turing was a British mathematician, computer scientist, and cryptanalyst who is widely considered to be the father of theoretical computer science and artificial intelligence. He worked at Bletchley Park during World War II, where he developed techniques for breaking German ciphers and created the Turing machine, a theoretical device that became the foundation of modern computing.\"\"\"\n\ntriples = extract_triples(text)\nprint(f\"Extracted {len(triples)} triples:\")\nfor t in triples:\n    print(f\"  {t}\")\n\nresult = add_to_graph(triples)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Tensor + Data Parallel Evaluation with vLLM\nDESCRIPTION: Command to perform tensor and data parallel evaluation using vLLM for optimized inference on supported model types.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nlm_eval --model vllm \\\n    --model_args pretrained={model_name},tensor_parallel_size={GPUs_per_model},dtype=auto,gpu_memory_utilization=0.8,data_parallel_size={model_replicas} \\\n    --tasks lambada_openai \\\n    --batch_size auto\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM Environment with Conda\nDESCRIPTION: Setup commands to create and activate a conda environment for Llama 3 and install vLLM.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llama_on_prem.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n llama3 python=3.11\nconda activate llama3\npip install vllm\n```\n\n----------------------------------------\n\nTITLE: Displaying Player Tools Configuration\nDESCRIPTION: Outputs the configuration of tools available to the black player, showing the registered functions that can be called during the game.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nplayer_black.llm_config[\"tools\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring LlamaIndex Settings\nDESCRIPTION: Sets up the LlamaIndex global settings with the Fireworks LLM client and a HuggingFace embedding model for vector operations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import Settings, VectorStoreIndex\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nSettings.llm = llm\n\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Llama 3.2 Vision Model for Synthetic Labeling in Python\nDESCRIPTION: Initializes the Llama 3.2 11B vision model and its processor from Hugging Face. The model is configured to use BFloat16 precision and automatic device mapping for efficient inference.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nmodel = MllamaForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16, token=hf_token)\nprocessor = MllamaProcessor.from_pretrained(model_name, token=hf_token)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Tool Call Loop for Order Processing\nDESCRIPTION: This snippet implements a loop that continues making LLM calls until no more tool usage is required. It processes tool calls by executing the appropriate functions (create_order or get_product_id) and appends both the calls and responses to the message history.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Continue to make LLM calls until it no longer decides to use a tool\ntool_call_identified = True\nwhile tool_call_identified:\n    response = client.chat.completions.create(\n        model=MODEL, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n    )\n    response_message = response.choices[0].message\n    tool_calls = response_message.tool_calls\n    # Step 2: check if the model wanted to call a function\n    if tool_calls:\n        print(\"LLM Call (Tool Use) Response:\", response_message)\n        # Step 3: call the function and append the tool call to our list of messages\n        available_functions = {\n            \"create_order\": create_order,\n            \"get_product_id\": get_product_id,\n        }\n        messages.append(\n            {\n                \"role\": \"assistant\",\n                \"tool_calls\": [\n                    {\n                        \"id\": tool_call.id,\n                        \"function\": {\n                            \"name\": tool_call.function.name,\n                            \"arguments\": tool_call.function.arguments,\n                        },\n                        \"type\": tool_call.type,\n                    }\n                    for tool_call in tool_calls\n                ],\n            }\n        )\n\n        # Step 4: send the info for each function call and function response to the model\n        for tool_call in tool_calls:\n            function_name = tool_call.function.name\n            function_to_call = available_functions[function_name]\n            function_args = json.loads(tool_call.function.arguments)\n            if function_name == \"get_product_id\":\n                function_response = function_to_call(\n                    product_name=function_args.get(\"product_name\")\n                )\n            elif function_name == \"create_order\":\n                function_response = function_to_call(\n                    customer_id=function_args.get(\"customer_id\"),\n                    product_id=function_args.get(\"product_id\"),\n                )\n            messages.append(\n                {\n                    \"tool_call_id\": tool_call.id,\n                    \"role\": \"tool\",\n                    \"name\": function_name,\n                    \"content\": function_response,\n                }\n            )  # extend conversation with function response\n    else:\n        print(\"\\n\\nFinal LLM Call Response:\", response.choices[0].message.content)\n        tool_call_identified = False\n```\n\n----------------------------------------\n\nTITLE: Defining Execution Prompt for Browser Agent Actions\nDESCRIPTION: Creates a structured prompt for the LLM to process webpage content and screenshots, decide on the next action, and return it in a specific JSON format for the browser agent to execute.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nexecution_prompt = \"\"\"\nYou will be given a task, a website's page accessibility tree, and the page screenshot as context. The screenshot is where you are now, use it to understand the accessibility tree. Based on that information, you need to decide the next step action. ONLY RETURN THE NEXT STEP ACTION IN A SINGLE JSON.\n\nWhen selecting elements, use elements from the accessibility tree.\n\nReflect on what you are seeing in the accessibility tree and the screenshot and decide the next step action, elaborate on it in reasoning, and choose the next appropriate action.\n\nSelectors must follow the format:\n- For a button with a specific name: \"button=ButtonName\"\n- For a placeholder (e.g., input field): \"placeholder=PlaceholderText\"\n- For text: \"text=VisibleText\"\n\nMake sure to analyze the accessibility tree and the screenshot to understand the current state, if something is not clear, you can use the previous actions to understand the current state. Explain why you are in the current state in current_state.\n\nYou will be given a task and you MUST return the next step action in JSON format:\n{\n    \"current_state\": \"Where are you now? Analyze the accessibility tree and the screenshot to understand the current state.\",\n    \"reasoning\": \"What is the next step to accomplish the task?\",\n    \"action\": \"navigation\" or \"click\" or \"fill\" or \"finished\",\n    \"url\": \"https://www.example.com\", // Only for navigation actions\n    \"selector\": \"button=Click me\", // For click or fill actions, derived from the accessibility tree\n    \"value\": \"Input text\", // Only for fill actions\n}\n\n### Guidelines:\n1. Use **\"navigation\"** for navigating to a new website through a URL.\n2. Use **\"click\"** for interacting with clickable elements. Examples:\n   - Buttons: \"button=Click me\"\n   - Text: \"text=VisibleText\"\n   - Placeholders: \"placeholder=Search...\"\n   - Link: \"link=BUY NOW\"\n3. Use **\"fill\"** for inputting text into editable fields. Examples:\n   - Placeholder: \"placeholder=Search...\"\n   - Textbox: \"textbox=Flight destination output\"\n   - Input: \"input=Search...\"\n4. Use **\"finished\"** when the task is done. For example:\n   - If a task is successfully completed.\n   - If navigation confirms you are on the correct page.\n\n\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Social Determinants of Health Data with Python Matplotlib\nDESCRIPTION: This code creates a bar plot showing the percentage of patients affected by each social determinant of health. It filters the dataframe to include only boolean fields, calculates the percentage of 'True' values for each field, and visualizes the results using matplotlib.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-social-determinants-of-health/SDOH-Json-mode.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Limit dataframe to boolean fields\ndf = sdoh_df[['financial_stress','housing_insecurity','neighborhood_unsafety','food_insecurity','transportation_inaccessibility','social_isolation','health_insurance_inadequacy','skipped_care_due_to_cost','language_barrier']]\n\n# Calculate the percentage of 'True' values for each boolean field\npercentages = df.mean() * 100  # df.mean() computes the mean for each column, 'True' is treated as 1, 'False' as 0\n\n# Plotting\nplt.figure(figsize=(10, 6))\npercentages.plot(kind='bar')\nplt.title('Percentage of Patients with Social Determinants')\nplt.ylabel('% of Patient Population')\nplt.xlabel('Social Determinant')\nplt.xticks(rotation=45)\nplt.grid(axis='y')\n\n# Display the plot\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Grouped Bar Plot of Type Distribution in Matplotlib\nDESCRIPTION: This code creates a grouped (non-stacked) bar plot showing the proportion of each Type within each Category. It normalizes the cross-tabulation data by dividing by row sums, sets appropriate labels and title, and adjusts the layout for better visualization.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# 4. Grouped bar plot of Type distribution within each Category\ncross_tab_normalized = cross_tab.div(cross_tab.sum(axis=1), axis=0)\ncross_tab_normalized.plot(kind='bar', stacked=False, ax=axs[1, 1])\naxs[1, 1].set_title('Type Distribution within each Category')\naxs[1, 1].set_xlabel('Category')\naxs[1, 1].set_ylabel('Proportion')\naxs[1, 1].legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\naxs[1, 1].set_xticklabels(axs[1, 1].get_xticklabels(), rotation=45, ha='right')\n\n# Adjust layout and display the plot\nplt.tight_layout()\nplt.show()\n\n# Print the total number of items in the sampled dataset\nprint(f\"Total number of items in the sampled dataset: {len(sampled_data)}\")\n```\n\n----------------------------------------\n\nTITLE: Encoding Image for Vision Query in Python\nDESCRIPTION: Defines a function to convert an image file into a Base64-encoded string for use in LLM vision queries.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport base64\nfrom IPython.display import Markdown\nimagePath= \"sample_screenshot.png\"\n\ndef encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n\n# Must have an image on the local path to use it\nbase64_image = encode_image(imagePath)\n```\n\n----------------------------------------\n\nTITLE: Running LLaMA 3 Inference with Converted Weights\nDESCRIPTION: Command to run inference using the converted weights with the official LLaMA 3 repository. Uses torch distributed training with 8 processes for chat completion.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/llama_cookbook/tools/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc_per_node 8 example_chat_completion.py --ckpt_dir ./test70B --tokenizer_path ${llama_3_dir}/tokenizer.model\n```\n\n----------------------------------------\n\nTITLE: Querying Agent About MetaGPT Agent Roles\nDESCRIPTION: Sends a query to the agent about agent roles in MetaGPT and their communication, then prints the response.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.query(\n    \"Tell me about the agent roles in MetaGPT, and how they communicate with each other.\"\n)\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Loading Input Data\nDESCRIPTION: Loads the previously saved transcript data from a pickle file to use as input for the model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-3-Re-Writer.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\n\nwith open('./resources/data.pkl', 'rb') as file:\n    INPUT_PROMPT = pickle.load(file)\n```\n\n----------------------------------------\n\nTITLE: Creating BM25 Index for Keyword Search in Python\nDESCRIPTION: This code creates a BM25 model and indexes the corpus for lexical search using the bm25s library.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport bm25s\n\n# Create the BM25 model and index the corpus\nretriever = bm25s.BM25(corpus=contextual_chunks)\nretriever.index(bm25s.tokenize(contextual_chunks))\n```\n\n----------------------------------------\n\nTITLE: Implementing List Emails Function with Gmail API\nDESCRIPTION: Implementation of the list_emails function that queries Gmail API to fetch emails matching certain criteria. The function uses pagination to handle large result sets.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef list_emails(query='', max_results=100):\n    emails = []\n    next_page_token = None\n\n    while True:\n        response = service.users().messages().list(\n            userId=user_id,\n            maxResults=max_results,\n            pageToken=next_page_token,\n            q=query\n        ).execute()\n        \n        if 'messages' in response:\n            for msg in response['messages']:\n                sender, subject, received_time = get_email_info(msg['id'])\n                emails.append(\n                    {\n                        \"message_id\": msg['id'],\n                        \"sender\": sender,\n                        \"subject\": subject,\n                        \"received_time\": received_time\n                    }\n                )\n        \n        next_page_token = response.get('nextPageToken')\n\n        if not next_page_token:\n            break\n    \n    return emails\n```\n\n----------------------------------------\n\nTITLE: Running Final Step and Checking Completion Status\nDESCRIPTION: Executes the next step in the reasoning process and checks if it's the final step by examining the is_last property of the step output.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nstep_output = agent.run_step(task.task_id)\nprint(step_output.is_last)\n```\n\n----------------------------------------\n\nTITLE: Generating Issue Annotations\nDESCRIPTION: Processes issues with LLM to generate annotations and metadata\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nannotated_issues, theme_counts = generate_issue_annotations(issues_df)\n\nannotated_issues = validate_df_values(annotated_issues, out_folder, 'annotated_issues')\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM API for Llama 3 via Groq\nDESCRIPTION: Sets up the configuration for using Llama 3 70B model through the Groq API. This defines the model, base URL, and placeholder for an API key that will be used by the agents.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# base url from https://console.groq.com/docs/openai\nconfig_list = [\n    {\n        \"model\": \"llama3-70b-8192\",\n        \"base_url\": \"https://api.groq.com/openai/v1\",\n        'api_key': 'your_groq_api_key', # get a free key at https://console.groq.com/keys\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installing the necessary Python packages including llama-index core and integrations for HuggingFace embeddings and Groq.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n!pip install llama-index-embeddings-huggingface\n!pip install llama-index-llms-groq\n```\n\n----------------------------------------\n\nTITLE: Finalizing Agent Response\nDESCRIPTION: Finalizes the agent's response for the task, combining the results of all reasoning steps into a cohesive answer.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.finalize_response(task.task_id)\n```\n\n----------------------------------------\n\nTITLE: Extracting Top Documents from Hybrid Search in Python\nDESCRIPTION: This code extracts the top documents from the hybrid search results based on RRF scores.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nhybrid_top_k_docs = [contextual_chunks[index] for index in hybrid_top_k[1]]\n```\n\n----------------------------------------\n\nTITLE: Creating Audio Segment Conversion Utility Function\nDESCRIPTION: Defining a helper function that converts numpy arrays to audio segments using the pydub library. The function handles sample rate conversion and bit depth adjustment.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef numpy_to_audio_segment(audio_arr, sampling_rate):\n    \"\"\"Convert numpy array to AudioSegment\"\"\"\n    # Convert to 16-bit PCM\n    audio_int16 = (audio_arr * 32767).astype(np.int16)\n    \n    # Create WAV file in memory\n    byte_io = io.BytesIO()\n    wavfile.write(byte_io, sampling_rate, audio_int16)\n    byte_io.seek(0)\n    \n    # Convert to AudioSegment\n    return AudioSegment.from_wav(byte_io)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Amazon Bedrock with Llama\nDESCRIPTION: Installs necessary Python packages and imports required modules for working with Amazon Bedrock and Llama models.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/getting_started_llama_3_on_amazon_bedrock.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# install packages\n# !python3 -m pip install -qU boto3\nfrom getpass import getpass\nfrom urllib.request import urlopen\nimport boto3\nimport json\n```\n\n----------------------------------------\n\nTITLE: Gmail API Required Permissions\nDESCRIPTION: Permissions needed for the email agent to function properly with Gmail. These permissions allow the agent to view emails and manage drafts and sending.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nView your email messages and settings. \nManage drafts and send emails.\n```\n\n----------------------------------------\n\nTITLE: Invoking Llama Model with Simple Prompt\nDESCRIPTION: Demonstrates how to invoke the Llama 3 8B model with a simple prompt about llamas, limiting the response to 100 tokens.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/getting_started_llama_3_on_amazon_bedrock.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Now we can utilize Invoke to do a simple prompt\ninvoke_model(bedrock_runtime, 'meta.llama3-8b-instruct-v1:0', 'Tell me about llamas', 100)\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated Answer in Python\nDESCRIPTION: This code displays the answer generated by the Llama 405B model based on the retrieved context and query.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nresponse.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Generating Text Using Pipeline\nDESCRIPTION: Sets up and executes the text generation pipeline using the Llama model with specified parameters.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-3-Re-Writer.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=MODEL,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=8126,\n    temperature=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample PDF\nDESCRIPTION: Downloading a research paper PDF for analysis.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf\n```\n\n----------------------------------------\n\nTITLE: Playing Generated Podcast in Jupyter Notebook\nDESCRIPTION: This snippet demonstrates how to play the generated podcast WAV file in a Jupyter Notebook environment using IPython's audio display functionality.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Play the podcast\n\nimport IPython\n\nIPython.display.Audio(\"podcast.wav\")\n```\n\n----------------------------------------\n\nTITLE: Adding Function Response to Message History\nDESCRIPTION: Appending the function's response to the conversation history with the 'function' role to provide context for the follow-up message.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# add the message returned by tool and query LLM again to get final answer\nmessages.append(\n{\n    \"role\": \"function\",\n    \"name\": function_call.name,\n    \"content\": function_response,\n}\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings using SentenceTransformer\nDESCRIPTION: Creates embeddings for each paper using the 'thenlper/gte-large' model from SentenceTransformer. Combines title, authors, and abstract for embedding generation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/vectorstore/mongodb/rag_mongodb_llama3_huggingface_open_source.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nembedding_model = SentenceTransformer('thenlper/gte-large')\n\ndef get_embedding(text: str) -> list[float]:\n  if not text.strip():\n      print(\"Attempted to get embedding for empty text.\")\n      return []\n\n  embedding = embedding_model.encode(text)\n\n  return embedding.tolist()\n\ndataset_df['embedding'] = dataset_df.apply(lambda x: get_embedding(x['title'] + \" \" + x['authors'] + \" \" + x['abstract']), axis=1)\n\ndataset_df.head()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Llama 3 and LangChain Stock Analysis\nDESCRIPTION: Import necessary libraries including LangChain's ChatGroq, os for environment variables, yfinance for stock data, and pandas for data manipulation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/llama3-stock-market-function-calling/llama3-stock-market-function-calling.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_groq import ChatGroq\nimport os\nimport yfinance as yf\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Viewing Updated Conversation History with Function Result\nDESCRIPTION: Displaying the complete conversation history including the original query and function response message.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nmessages\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for PDF Processing and Language Model\nDESCRIPTION: This code imports necessary Python libraries for PDF processing, language model usage, and progress tracking. It includes PyPDF2 for PDF handling, torch for GPU support, and transformers for the language model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport PyPDF2\nfrom typing import Optional\nimport os\nimport torch\nfrom accelerate import Accelerator\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom tqdm.notebook import tqdm\nimport warnings\n\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Implementing Structured Data Extraction\nDESCRIPTION: Configuration of structured data extraction using Pydantic models and LLM predictions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.llms.ollama import Ollama\nfrom llama_index.core.prompts import PromptTemplate\nfrom pydantic import BaseModel\n\nclass Restaurant(BaseModel):\n    \"\"\"A restaurant with name, city, and cuisine.\"\"\"\n    name: str\n    city: str\n    cuisine: str\n\nllm = Ollama(model=\"llama3\")\nprompt_tmpl = PromptTemplate(\n    \"Generate a restaurant in a given city {city_name}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running Text Generation Pipeline\nDESCRIPTION: Sets up and executes the Hugging Face pipeline for text generation with specified parameters including temperature and token limits.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-2-Transcript-Writer.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=MODEL,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=8126,\n    temperature=1,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Llama 3.1 Text Processing in Python\nDESCRIPTION: This snippet installs the required packages (tiktoken and openai) and imports the DeepInfra API key from a config file. These dependencies are necessary for text tokenization and interacting with the Llama 3.1 model via DeepInfra.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Contextual-Chunking-RAG/Tutorial.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#Install dependencies\n!pip install tiktoken\n!pip install openai\n\nfrom config import DEEPINFRA_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Configuring Async Support\nDESCRIPTION: Enables nested asyncio support for concurrent operations in Jupyter notebook.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L2_Tool_Calling.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Importing TTS Model Libraries and Dependencies\nDESCRIPTION: Importing the necessary libraries for both Bark and Parler TTS models, along with PyTorch and data processing utilities needed for audio generation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import BarkModel, AutoProcessor, AutoTokenizer\nimport torch\nimport json\nimport numpy as np\nfrom parler_tts import ParlerTTSForConditionalGeneration\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAIAgent with Arithmetic Tools in Python\nDESCRIPTION: This code creates an OpenAIAgent using the previously defined arithmetic tools and the Groq 70B model. The agent is configured for verbose output.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nagent = OpenAIAgent.from_tools(\n    [multiply_tool, add_tool, subtract_tool, divide_tool],\n    llm=llm_70b,\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Expanded Paper Set\nDESCRIPTION: Downloads the expanded set of academic papers from the specified URLs and saves them with the corresponding filenames.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfor url, paper in zip(urls, papers):\n  !wget \"{url}\" -O \"{paper}\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for PDF to Podcast Pipeline\nDESCRIPTION: Installs necessary audio libraries, FFmpeg, and Python packages for PDF processing, LLM access via Together.ai, and text-to-speech conversion.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!apt install -q libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n!pip install -q ffmpeg-python\n!pip install -q PyAudio\n!pip install -q pypdf #to read PDF content\n!pip install -q together #to access open source LLMs\n!pip install -q cartesia #to access TTS model\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Financial Report Processing in Python\nDESCRIPTION: This snippet installs necessary Python packages for processing financial reports, including tiktoken, torch, transformers, llama_parse, and llama_index. It also imports required modules and enables nested async loops.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Contextual-Chunking-RAG/Example_FinancialReport_RAG.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install dependencies\n!pip install tiktoken\n!pip install torch\n!pip install transformers\n!pip install llama_parse\n!pip install llama_index\n\nimport random\nimport os\nimport sys\nimport json\nfrom llama_parse import LlamaParse\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core.schema import Document\nfrom config import LLAMAPARSE_API_KEY\n\n# Enable nested async loops\nimport nest_asyncio\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Displaying Processing Results and Text Preview in Python\nDESCRIPTION: This snippet prints a summary of the processing operation, including input and output file names and the number of chunks processed. It also displays a preview of the beginning and end of the processed text.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"\\nProcessing complete!\")\nprint(f\"Input file: {INPUT_FILE}\")\nprint(f\"Output file: {output_file}\")\nprint(f\"Total chunks processed: {num_chunks}\")\n\n# Preview the beginning and end of the complete processed text\nprint(\"\\nPreview of final processed text:\")\nprint(\"\\nBEGINNING:\")\nprint(processed_text[:1000])\nprint(\"\\n...\\n\\nEND:\")\nprint(processed_text[-1000:])\n```\n\n----------------------------------------\n\nTITLE: Running Fine-tuning with LoRA and 8-bit Quantization in Bash\nDESCRIPTION: Command to run fine-tuning on a Meta Llama 3 model using LoRA (a parameter-efficient fine-tuning method) with 8-bit quantization. This allows training on a single GPU with reduced memory requirements.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/singlegpu_finetuning.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nFSDP_CPU_RAM_EFFICIENT_LOADING=1 python finetuning.py  --use_peft --peft_method lora --quantization 8bit --model_name /path_of_model_folder/8B --output_dir Path/to/save/PEFT/model\n```\n\n----------------------------------------\n\nTITLE: Setting Up Input File and Chunk Size for Processing\nDESCRIPTION: This snippet defines the input file path for the extracted text and sets the chunk size for processing. It then creates word-bounded chunks from the text and calculates the total number of chunks.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nINPUT_FILE = \"./resources/extracted_text.txt\"  # Replace with your file path\nCHUNK_SIZE = 1000  # Adjust chunk size if needed\n\nchunks = create_word_bounded_chunks(text, CHUNK_SIZE)\nnum_chunks = len(chunks)\n```\n\n----------------------------------------\n\nTITLE: Extracting Top 5 Chunks from Contextual Data in Python\nDESCRIPTION: This snippet extracts the top 5 chunks from contextual data based on similarity indices.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntop_5_chunks = [contextual_chunks[index] for index in indices[0]][:5]\n\ntop_5_chunks\n```\n\n----------------------------------------\n\nTITLE: Installing Required LlamaIndex Packages\nDESCRIPTION: Installs the core LlamaIndex library along with the Hugging Face embeddings integration and Groq integration for accessing Llama 3 models.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n!pip install llama-index-embeddings-huggingface\n!pip install llama-index-llms-groq\n```\n\n----------------------------------------\n\nTITLE: Creating Phase 2 Training Data (Bilingual Alternating Sentences)\nDESCRIPTION: Code to create data for Phase 2 training, which requires bilingual text where the language alternates between sentences. It splits English paragraphs into sentences, translates them to Hindi, then creates new paragraphs with alternating language sentences and saves the dataset.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/multilingual/README.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nquantization = \"\"\nen_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\nen_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, \"en-indic\", quantization)\nip = IndicProcessor(inference=True)\n\nphase2_data = []\nfor para in english_paragraphs:\n    en_sents = split_sentences(para, \"eng_Latn\")\n    trans_sents = batch_translate(input_sentences, \"eng_Latn\", \"hin_Deva, en_indic_model, en_indic_tokenizer, ip)\n    final_para = []\n    for idx, (en_sent, trans_sent) in enumerate(zip(en_sents, trans_sents)):\n        sent_to_append = en_sent if idx % 2 == 0 else trans_sent\n        final_para.append(sent_to_append)\n    phase2_data.append({\"text\": \" \".join(final_para)})\n\n# if you want to save it for future, you can do so easily with HF datasets\nfrom datasets import Dataset\nphase2_ds = Dataset.from_list(phase2_data)\nphase2_ds.save_to_disk(\"data/phase2\")\n```\n\n----------------------------------------\n\nTITLE: Creating White Player Agent with AutoGen\nDESCRIPTION: Creates a conversational agent representing the white player in the chess game. The agent is instructed to get legal moves, make a move, and then communicate with the other player.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom autogen import ConversableAgent\n\n# Player white agent\nplayer_white = ConversableAgent(\n    name=\"Player White\",\n    system_message=\"You are a chess player and you play as white. \"\n    \"First call get_legal_moves(), to get a list of legal moves in UCI format. \"\n    \"Then call make_move(move) to make a move. Finally, tell the proxy what you have moved and ask the black to move\", # added \"Finally...\" to make the agents work\n    llm_config={\"config_list\": config_list,\n                \"temperature\": 0,\n               },\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Llama LLM with AzureML Endpoint\nDESCRIPTION: Creates an LLM instance using AzureMLChatOnlineEndpoint with custom configuration for temperature, tokens, and content formatting.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.chat_models.azureml_endpoint import (\n    AzureMLEndpointApiType,\n    CustomOpenAIChatContentFormatter,\n    AzureMLChatOnlineEndpoint,\n)\n\nfrom langchain_core.messages import HumanMessage\n\nllm = AzureMLChatOnlineEndpoint(\n    endpoint_api_key=\"your-auth-key\",\n    endpoint_url=\"https://your-endpoint.inference.ai.azure.com/v1/chat/completions\",\n    endpoint_api_type=AzureMLEndpointApiType.serverless,\n    model_kwargs={\"temperature\": 0.6, \"max_tokens\": 256, \"top_p\": 0.9},\n    content_formatter=CustomOpenAIChatContentFormatter(),\n)\n```\n\n----------------------------------------\n\nTITLE: Testing vLLM API with curl\nDESCRIPTION: Example curl command to test the vLLM API endpoint with a sample prompt.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llama_on_prem.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:5000/generate -d '{\n        \"prompt\": \"Who wrote the book Innovators dilemma?\",\n        \"max_tokens\": 300,\n        \"temperature\": 0\n    }'\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Extract arXiv IDs Using LLM\nDESCRIPTION: This function uses an 8b Llama model instance to extract arXiv IDs from web search results, avoiding complex regex patterns.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/Agents_Tutorial/Tool_Calling_201.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_arxiv_ids(web_results: dict, temperature: int = 0, max_tokens=512):\n    # Initialize chat history with a specific prompt to extract arXiv IDs\n    arxiv_id_chat_history = [{\"role\": \"system\", \"content\": \"Given this input, give me the arXiv ID of the papers. The input has the query and web results. DO NOT WRITE ANYTHING ELSE IN YOUR RESPONSE: ONLY THE ARXIV ID ONCE, the web search will have it repeated multiple times, just return the it once and where its actually the arxiv ID\"}, {\"role\": \"user\", \"content\": f\"Here is the query and results{web_results}\"}]\n\n    # Call the model to process the input and extract arXiv IDs\n    response = client.chat.completions.create(\n        model=\"llama-3.1-8b-instant\",  # Adjust the model as necessary\n        messages=arxiv_id_chat_history,\n        max_tokens=max_tokens,\n        temperature=temperature\n    )\n    \n    # Append the assistant's response to the chat history\n    arxiv_id_chat_history.append({\n        \"role\": \"assistant\",\n        \"content\": response.choices[0].message.content\n    })\n    \n    # Return the extracted arXiv IDs\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Image Display Function Implementation in Python\nDESCRIPTION: Implements a function to display images from URLs using PIL and matplotlib. Handles HTTP requests and image rendering.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_tool_calling_agent.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\n\ndef display_image(image_url):\n    \"\"\"Display generated image\"\"\"\n    response = requests.get(image_url)\n    img = Image.open(BytesIO(response.content))  \n    plt.imshow(img)\n    plt.axis('off')\n    plt.show()\n\nimage_url = event['messages'][-2].content\ndisplay_image(image_url)\n```\n\n----------------------------------------\n\nTITLE: Implementing Hallucination Grader for RAG\nDESCRIPTION: Creates a grader that checks if the generated answer is grounded in the retrieved facts. It uses a Groq LLM with structured output to provide a binary hallucination score.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Data model\nclass GradeHallucinations(BaseModel):\n    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n\n    score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n\n# LLM with function call \nllm = ChatGroq(temperature=0, model=\"llama3-70b-8192\")\nstructured_llm_grader = llm.with_structured_output(GradeHallucinations)\n\n# Prompt \nsystem = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\nhallucination_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\nhallucination_grader = hallucination_prompt | structured_llm_grader\nhallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n```\n\n----------------------------------------\n\nTITLE: Setting PDF Path and Default Language Model\nDESCRIPTION: This snippet sets the path for the PDF file to be processed and specifies the default Llama model to be used for text cleaning. It allows for easy configuration of input and model selection.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npdf_path = './resources/2402.13116v4.pdf'\nDEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n```\n\n----------------------------------------\n\nTITLE: Generating Final Response with Function Result Context\nDESCRIPTION: Creating a new chat completion that uses the function result information to provide a natural language response to the user's weather query.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=messages,\n)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Displaying Source Node Content with Metadata\nDESCRIPTION: Retrieves and prints the full content of the first source node used in generating the previous response, including metadata.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(response.source_nodes[0].get_content(metadata_mode=\"all\"))\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Document\nDESCRIPTION: Downloads and renames an Nvidia investor presentation PDF for analysis.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/multimodal_RAG_with_nvidia_investor_slide_deck.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!wget https://s201.q4cdn.com/141608511/files/doc_presentations/2023/Oct/01/ndr_presentation_oct_2023_final.pdf\n!mv ndr_presentation_oct_2023_final.pdf nvidia_presentation.pdf\n```\n\n----------------------------------------\n\nTITLE: Full FSDP Fine-tuning Command for Llama Vision Model\nDESCRIPTION: Command to perform full fine-tuning using FSDP (Fully Sharded Data Parallel) on Llama 3.2 vision model. Uses 4 GPUs per node with padding batching strategy and custom OCRVQA dataset.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/finetune_vision_model.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nnodes 1 --nproc_per_node 4  recipes/quickstart/finetuning/finetuning.py --enable_fsdp --lr 1e-5  --num_epochs 3 --batch_size_training 2 --model_name meta-llama/Llama-3.2-11B-Vision-Instruct --dist_checkpoint_root_folder ./finetuned_model --dist_checkpoint_folder fine-tuned  --use_fast_kernels --dataset \"custom_dataset\" --custom_dataset.test_split \"test\" --custom_dataset.file \"recipes/quickstart/finetuning/datasets/ocrvqa_dataset.py\"  --run_validation True --batching_strategy padding\n```\n\n----------------------------------------\n\nTITLE: Response Processing Function\nDESCRIPTION: Function to process model responses and prepare next steps in the ReAct chain.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/react_llama_3_bedrock_wk.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef next_step(response):\n    instruction = response[ : response.find('\\n- Observation:')]\n    lines = instruction[instruction.rfind(\"Action:\"):].split(\"\\n\")\n    action, action_input = lines[0].split(\": \")[1].strip(), lines[1].split(\": \")[1].strip()\n    func = globals().get(action)\n    observation = func(action_input)\n    observation = observation[:observation[:350].rfind('. ')]\n    return instruction + '\\n- Observation: ' + observation + '\\n- Thought:'\n```\n\n----------------------------------------\n\nTITLE: Deploying Llama 3 as vLLM API Server\nDESCRIPTION: Command to start vLLM as a general API server for Llama 3 inference.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llama_on_prem.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m vllm.entrypoints.api_server --host 0.0.0.0 --port 5000 --model meta-llama/Meta-Llama-3.1-8B-Instruct\n```\n\n----------------------------------------\n\nTITLE: Implementing Speaker 2 Audio Generation Function with Bark\nDESCRIPTION: Creating a function to generate audio for the second speaker using the Bark TTS model. The function takes text input and returns the audio array with its sampling rate.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef generate_speaker2_audio(text):\n    \"\"\"Generate audio using Bark for Speaker 2\"\"\"\n    inputs = bark_processor(text, voice_preset=\"v2/en_speaker_6\").to(device)\n    speech_output = bark_model.generate(**inputs, temperature=0.9, semantic_temperature=0.8)\n    audio_arr = speech_output[0].cpu().numpy()\n    return audio_arr, bark_sampling_rate\n\n```\n\n----------------------------------------\n\nTITLE: Extracting Function Call Details for Programmatic Use\nDESCRIPTION: Getting the function call object from the response for programmatic access to function name and arguments.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfunction_call = response.choices[0].message.function_call\nfunction_call\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Download PDF Using arXiv Library\nDESCRIPTION: This function uses the arXiv library to download a PDF file given its arXiv ID and save it with a specified filename.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/Agents_Tutorial/Tool_Calling_201.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Function to download PDF using arxiv library\ndef download_pdf(arxiv_id, filename):\n    paper = next(arxiv.Client().results(arxiv.Search(id_list=[arxiv_id])))\n    paper.download_pdf(filename=filename)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for PDF Processing and Language Model\nDESCRIPTION: This code snippet shows the pip install commands for PyPDF2, rich, and ipywidgets libraries. These are necessary for PDF processing and improved notebook functionality.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#!pip install PyPDF2\n#!pip install rich ipywidgets\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated Podcast Dialogue\nDESCRIPTION: Outputs the dialogue portion of the generated podcast script, showing the conversation between the host and guest about the paper's content.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nscript.dialogue\n```\n\n----------------------------------------\n\nTITLE: Checking Upcoming Agent Steps\nDESCRIPTION: Retrieves and prints information about upcoming steps in the agent's reasoning process, demonstrating the planning capabilities of the agent.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nupcoming_steps = agent.get_upcoming_steps(task.task_id)\nprint(f\"Num upcoming steps for task {task.task_id}: {len(upcoming_steps)}\")\nupcoming_steps[0]\n```\n\n----------------------------------------\n\nTITLE: Splitting Documents for RAG Processing\nDESCRIPTION: Uses RecursiveCharacterTextSplitter to split loaded documents into smaller chunks for more effective RAG processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\nsplits = text_splitter.split_documents(documents)\nprint(len(splits), splits[0])\n```\n\n----------------------------------------\n\nTITLE: Running the Meta Eval Preparation Script\nDESCRIPTION: Command to run the prepare_meta_eval.py script that generates the proper evaluation configuration based on the eval_config.yaml file. This script prepares the environment and generates the lm_eval command.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/meta_eval/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython prepare_meta_eval.py --config_path ./eval_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Preprocessing and Ground Truth in YAML\nDESCRIPTION: YAML configuration for dataset preprocessing, prompt generation, and ground truth definition using custom utility functions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/meta_eval/README.md#2025-04-07_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nprocess_docs: !function utils.process_docs\ndoc_to_text: !function utils.doc_to_text\ndoc_to_target: gold\n```\n\n----------------------------------------\n\nTITLE: Listing Required Python Packages for Groq API Integration\nDESCRIPTION: This requirements file specifies two essential packages: the Groq Python client library for interacting with Groq's API, and python-dotenv for loading environment variables from a .env file. These dependencies are necessary for applications that need to authenticate and make requests to Groq's API services.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/parallel-tool-use/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngroq\npython-dotenv\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for vLLM\nDESCRIPTION: Setting environment variables for HuggingFace home directory, model tag, and tensor parallelism\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/crusoe/vllm-fp8/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_HOME=/scratch/\nexport MODEL=neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic\nexport TP_SIZE=1\n```\n\n----------------------------------------\n\nTITLE: Generating Formatted Training Examples for Llama Guard in Python\nDESCRIPTION: This code snippet shows how to combine all configuration objects into a FormatterConfigs instance and use it to generate formatted training examples for Llama Guard finetuning. It demonstrates the final step in preparing the data for training.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/llama_cookbook/data/llama_guard/README.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nformatter_configs = FormatterConfigs(\n    guidelines=guidelines,\n    llama_guard_prompt_configs=llama_guard_prompt_configs,\n    llama_guard_generation_configs=llama_guard_generation_configs,\n    augmentation_configs=augmentation_configs,\n    random_seed=42\n)\n\n# Call the create_formatted_finetuning_examples function\nformatted_examples = create_formatted_finetuning_examples(\n    training_examples, formatter_configs)\n# Print the formatted examples\nprint(formatted_examples)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Balanced Dataset with Multiple Plots\nDESCRIPTION: Creates a comprehensive visualization of the balanced dataset with bar plots for category and type distributions, and a heatmap showing the relationship between categories and types.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef sample_category(group):\n    if len(group) > 100:\n        return group.sample(n=100, random_state=42)\n    else:\n        return group\n\n# Group by New_Category and apply the sampling function\nsampled_data = result.groupby('New_Category').apply(sample_category).reset_index(drop=True)\n\n# Set up the matplotlib figure\nfig, axs = plt.subplots(2, 2, figsize=(20, 15))\n\n# 1. Bar plot of Category distribution\nsns.countplot(data=sampled_data, x='New_Category', order=sampled_data['New_Category'].value_counts().index, ax=axs[0, 0])\naxs[0, 0].set_title('Distribution of Categories')\naxs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=45, ha='right')\n\n# 2. Bar plot of Type distribution\nsns.countplot(data=sampled_data, x='New_Type', order=sampled_data['New_Type'].value_counts().index, ax=axs[0, 1])\naxs[0, 1].set_title('Distribution of Types')\naxs[0, 1].set_xticklabels(axs[0, 1].get_xticklabels(), rotation=45, ha='right')\n\n# 3. Heatmap of Category vs Type\ncross_tab = pd.crosstab(sampled_data['New_Category'], sampled_data['New_Type'])\nsns.heatmap(cross_tab, annot=True, fmt='d', cmap='YlGnBu', ax=axs[1, 0])\naxs[1, 0].set_title('Heatmap of Category vs Type')\n```\n\n----------------------------------------\n\nTITLE: Implementing Hallucination Grader for RAG Agent\nDESCRIPTION: This code implements a hallucination grader that assesses whether the generated answer is grounded in the retrieved facts. It uses a local LLM to provide a binary score indicating if the answer is supported by the given context.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# LLM\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n\n# Prompt\nprompt = PromptTemplate(\n    template=\"\"\"You are a grader assessing whether \n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n    single key 'score' and no preamble or explanation.\n    \n    Here are the facts:\n    {documents} \n\n    Here is the answer: \n    {generation}\n    \"\"\",\n    input_variables=[\"generation\", \"documents\"],\n)\n\nhallucination_grader = prompt | llm | JsonOutputParser()\nhallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n```\n\n----------------------------------------\n\nTITLE: Initializing Parler TTS Model for Podcast Generation\nDESCRIPTION: Setting up the Parler TTS model for podcast generation. This model will be used for one of the podcast speakers with specific voice characteristics.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nparler_model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(\"cuda:3\")\nparler_tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Chat Model Benchmark\nDESCRIPTION: Command to execute the benchmark script for chat models using vLLM. This script simulates real traffic and measures throughput.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/on_prem/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython chat_vllm_benchmark.py\n```\n\n----------------------------------------\n\nTITLE: Printing Final Agent Response\nDESCRIPTION: Prints the complete finalized response generated by the agent after completing all reasoning steps.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Server in tmux\nDESCRIPTION: Commands to start vLLM server in a detachable tmux session\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/crusoe/vllm-fp8/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntmux new -s server\nuv run vllm serve $MODEL --enable-chunked-prefill --disable-log-requests --tensor-parallel-size $TP_SIZE\n```\n\n----------------------------------------\n\nTITLE: Configuring Finetuning Settings in Python\nDESCRIPTION: This code snippet shows the available configuration options for fine-tuning Llama 3 models, including model settings, training parameters, and optimization techniques.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    model_name: str=\"PATH/to/Model\"\n    tokenizer_name: str=None\n    enable_fsdp: bool=False # shards model parameters, optimizer states and gradients across DDP ranks\n    low_cpu_fsdp: bool=False # saves cpu memory by loading pretrained model on rank0 only\n    run_validation: bool=True\n    batch_size_training: int=4\n    batching_strategy: str=\"packing\" #alternative: padding\n    context_length: int=4096\n    gradient_accumulation_steps: int=1\n    gradient_clipping: bool = False\n    gradient_clipping_threshold: float = 1.0\n    num_epochs: int=3\n    max_train_step: int=0\n    max_eval_step: int=0\n    num_workers_dataloader: int=1\n    lr: float=1e-4\n    weight_decay: float=0.0\n    gamma: float= 0.85 # multiplicatively decay the learning rate by gamma after each epoch\n    seed: int=42\n    use_fp16: bool=False\n    mixed_precision: bool=True\n    val_batch_size: int=1\n    dataset = \"samsum_dataset\"\n    peft_method: str = \"lora\" # None, llama_adapter (Caution: llama_adapter is currently not supported with FSDP)\n    use_peft: bool=False # use parameter efficient fine tuning\n    from_peft_checkpoint: str=\"\" # if not empty and use_peft=True, will load the peft checkpoint and resume the fine-tuning on that checkpoint\n    output_dir: str = \"PATH/to/save/PEFT/model\"\n    freeze_layers: bool = False\n    num_freeze_layers: int = 1\n    freeze_LLM_only: bool = False # Freeze self-attention layers in the language_model. Vision model, multi_modal_projector, cross-attention will be fine-tuned\n    quantization: str = None\n    one_gpu: bool = False\n    save_model: bool = True\n    dist_checkpoint_root_folder: str=\"PATH/to/save/FSDP/model\" # will be used if using FSDP\n    dist_checkpoint_folder: str=\"fine-tuned\" # will be used if using FSDP\n    save_optimizer: bool=False # will be used if using FSDP\n    use_fast_kernels: bool = False # Enable using SDPA from PyTroch Accelerated Transformers, make use Flash Attention and Xformer memory-efficient kernels\n    use_wandb: bool = False # Enable wandb for experient tracking\n    save_metrics: bool = False # saves training metrics to a json file for later plotting\n    flop_counter: bool = False # Enable flop counter to measure model throughput, can not be used with pytorch profiler at the same time.\n    flop_counter_start: int = 3 # The step to start profiling, default is 3, which means after 3 steps of warmup stage, the profiler will start to count flops.\n    use_profiler: bool = False # Enable pytorch profiler, can not be used with flop counter at the same time.\n    profiler_dir: str = \"PATH/to/save/profiler/results\" # will be used if using profiler\n```\n\n----------------------------------------\n\nTITLE: Initializing Data Paths and Model Configuration in Python\nDESCRIPTION: Sets up file paths for the dataset and specifies the Hugging Face token and model name for Llama 3.2 vision model. These variables control where data is accessed and which model version will be used for synthetic labeling.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDATA = \"./DATA/\"\nMETA_DATA = f\"{DATA}images.csv/\"\nIMAGES = f\"{DATA}images_compressed/\"\n\nhf_token = \"\"\nmodel_name = \"meta-llama/Llama-3.2-11b-Vision-Instruct\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Messages and Tools Configuration\nDESCRIPTION: Configuring chat messages and tool definitions for the API request including weather function parameters\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/parallel-tool-use/parallel-tool-use.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant.\"\"\"},\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the weather in Paris, Tokyo and Madrid?\",\n    },\n]\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Returns the weather in the given city in degrees Celsius\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"city\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the city\",\n                    }\n                },\n                \"required\": [\"city\"],\n            },\n        },\n    }\n]\nresponse = client.chat.completions.create(\n    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n)\n\nresponse_message = response.choices[0].message\n```\n\n----------------------------------------\n\nTITLE: Remapping Categories to Standardize Values\nDESCRIPTION: Creates a function to map similar categories to standardized values, reducing the number of categories and fixing hallucinated categories from the LLM.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef map_category(category):\n    category = category.lower()\n    if 'shirt' in category or 'top' in category:\n        return 'T-Shirt' if 't-shirt' in category else 'Tops'\n    elif 'shoe' in category or 'footwear' in category:\n        return 'Shoes'\n    elif 'pant' in category:\n        return 'Pants'\n    elif 'jean' in category:\n        return 'Jeans'\n    elif 'short' in category:\n        return 'Shorts'\n    elif 'skirt' in category:\n        return 'Skirts'\n    else:\n        return 'Other'\n\n# Apply the mapping function to the 'Category' column\nresult['New_Category'] = result['Category'].apply(map_category)\n\n# Print the distribution of new categories\nprint(\"Distribution of New Categories:\")\nprint(result['New_Category'].value_counts())\n\n# Print the mapping of old categories to new categories\nprint(\"\\nMapping of Old Categories to New Categories:\")\nprint(result.groupby('Category')['New_Category'].first().sort_index())\n```\n\n----------------------------------------\n\nTITLE: Creating Contextual Follow-up Prompt\nDESCRIPTION: Generates a new prompt that includes context from the previous question and result\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/quickstart.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprompt = f\"\"\"Based on the table schema, question, SQL query, and SQL response below, write a new SQL response; be concise, just output the SQL response.\n\nScheme:\n{get_schema()}\n\nQuestion: {follow_up}\nSQL Query: {question}\nSQL Result: {result}\n\nNew SQL Response:\n\"\"\"\nprint(prompt)\n```\n\n----------------------------------------\n\nTITLE: Initializing Bark Model for Podcast Generation\nDESCRIPTION: Setting up the Bark model with specific device placement and configuration for podcast generation. This prepares the model to be used for one of the podcast speakers.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nbark_processor = AutoProcessor.from_pretrained(\"suno/bark\")\nbark_model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(\"cuda:3\")\nbark_sampling_rate = 24000\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Llama Cookbook Project\nDESCRIPTION: Lists the required Python packages and their minimum versions for the Llama Cookbook project. Core dependencies include PyPDF2, torch, transformers, accelerate, rich, ipywidgets, and tqdm. The file also includes recommended packages for Jupyter notebook support and a package for handling warnings.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Core dependencies\nPyPDF2>=3.0.0\ntorch>=2.0.0\ntransformers>=4.46.0\naccelerate>=0.27.0\nrich>=13.0.0\nipywidgets>=8.0.0\ntqdm>=4.66.0\n\n# Optional but recommended\njupyter>=1.0.0\nipykernel>=6.0.0\n\n# Warning handling\nwarnings>=0.1.0\n```\n\n----------------------------------------\n\nTITLE: Defining Academic Paper URLs and Filenames\nDESCRIPTION: Defines lists of academic paper URLs and corresponding filenames for downloading. Initially includes three papers (MetaGPT, LongLoRA, and Self-RAG) and later expands to eleven papers.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nurls = [\n    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n]\n\npapers = [\n    \"metagpt.pdf\",\n    \"longlora.pdf\",\n    \"selfrag.pdf\",\n]\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies\nDESCRIPTION: List of Python package requirements including ML frameworks (sentence-transformers, scikit-learn), data processing libraries (numpy, duckdb), and utility packages (pyyaml, sqlparse, tabulate). These dependencies are needed to run the Llama Cookbook project.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/verified-sql-function-calling/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngroq\nsentence-transformers\nlangchain_community\nscikit-learn\nnumpy\nduckdb\npyyaml\nsqlparse\ntabulate\n```\n\n----------------------------------------\n\nTITLE: Printing Generated Fashion Caption\nDESCRIPTION: A simple print statement to display the generated fashion description from the model output, removing the original prompt from the decoded output for clean display.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nprint(processor.decode(output[0])[len(prompt):])\n```\n\n----------------------------------------\n\nTITLE: Adding Padding Token to Llama Tokenizer in Python\nDESCRIPTION: This snippet demonstrates how to add a padding token to the Llama tokenizer and resize the token embeddings. This is necessary for Llama versions < 3.1 when using padding for batched inference.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntokenizer.add_special_tokens(\n        {\n\n            \"pad_token\": \"<PAD>\",\n        }\n    )\nmodel.resize_token_embeddings(model.config.vocab_size + 1)\n```\n\n----------------------------------------\n\nTITLE: Extending Llama Tokenizer with Hindi Tokens\nDESCRIPTION: Command to merge the Hindi tokenizer with the original Llama tokenizer, creating an extended version that maintains compatibility with English while efficiently handling Hindi text. The new tokenizer is saved to a specified directory.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/multilingual/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython extend_tokenizer.py --new_tokenizer_path=./hi_tokenizer --extended_tokenizer_save_path=./extended_tokenizer\n```\n\n----------------------------------------\n\nTITLE: Displaying First Rows of Cleaned Dataset\nDESCRIPTION: Shows the first few rows of the cleaned dataset to verify the cleaning process.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresult.head()\n```\n\n----------------------------------------\n\nTITLE: Defining Receipt Structure using Pydantic in Python\nDESCRIPTION: This code defines the structure for receipt data using Pydantic models. It creates an 'Item' class for individual line items and a 'Receipt' class to represent the entire receipt with multiple items and a total.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/structured_text_extraction_from_images.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom pydantic import BaseModel, Field\n\nclass Item(BaseModel):\n    name: str\n    price: float\n    quantity: int = Field(default=1)\n\nclass Receipt(BaseModel):\n    items: list[Item]\n    total: float\n```\n\n----------------------------------------\n\nTITLE: Configuring PEFT with LoRA for efficient fine-tuning\nDESCRIPTION: Prepares the model for Parameter Efficient Fine-Tuning (PEFT) using LoRA (Low-Rank Adaptation). This approach allows fine-tuning large language models with minimal memory requirements by only training a small number of additional parameters.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/quickstart_peft_finetuning.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\nfrom dataclasses import asdict\nfrom llama_cookbook.configs import lora_config as LORA_CONFIG\n\nlora_config = LORA_CONFIG()\nlora_config.r = 8\nlora_config.lora_alpha = 32\nlora_dropout: float=0.01\n\npeft_config = LoraConfig(**asdict(lora_config))\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n```\n\n----------------------------------------\n\nTITLE: Loading Podcast Transcript Data for Processing\nDESCRIPTION: Loading the prepared podcast transcript data from a pickle file. This data will be used to generate the complete podcast audio with multiple speakers.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\n\nwith open('./resources/podcast_ready_data.pkl', 'rb') as file:\n    PODCAST_TEXT = pickle.load(file)\n```\n\n----------------------------------------\n\nTITLE: Generated Python Code for Complex Calculation using PAL\nDESCRIPTION: Shows the Python code generated by Code Llama 34B to solve the complex arithmetic problem. This demonstrates how PAL leverages an LLM's code generation capabilities for accurate calculations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nnum1 = (-5 + 93 * 4 - 0)\nnum2 = (4**4 + -7 + 0 * 5)\nanswer = num1 * num2\nprint(answer)\n```\n\n----------------------------------------\n\nTITLE: Collecting All Tools\nDESCRIPTION: Gathers all the tools created for the expanded set of papers into a single list for use in the object index.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nall_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n```\n\n----------------------------------------\n\nTITLE: Installing Together AI Library\nDESCRIPTION: Installation of the Together AI library using pip package manager\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/text_RAG_using_llama_on_together.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together\n```\n\n----------------------------------------\n\nTITLE: Loading and Initializing Llama3 Model\nDESCRIPTION: Loads the Llama3 model and tokenizer from Hugging Face, with options for CPU or GPU usage. Prepares the model for generating responses to queries.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/vectorstore/mongodb/rag_mongodb_llama3_huggingface_open_source.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n# CPU Enabled uncomment below \n# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n# GPU Enabled use below \nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n```\n\n----------------------------------------\n\nTITLE: Python urllib Request Implementation\nDESCRIPTION: Python implementation using urllib for making API requests to the Azure Llama endpoint with error handling.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport urllib.request\nimport json\n\ndata = {\"messages\":[\n            {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n            {\"role\":\"user\", \"content\":\"What is good about Wuhan?\"}],\n        \"max_tokens\": 500,\n        \"temperature\": 0.9,\n        \"stream\": True,\n}\n\nbody = str.encode(json.dumps(data))\n\nurl = 'https://your-endpoint.inference.ai.azure.com/v1/chat/completions'\n\napi_key = 'your-auth-key'\nif not api_key:\n    raise Exception(\"API Key is missing\")\n\nheaders = {'Content-Type':'application/json', 'Authorization':(api_key)}\n\nreq = urllib.request.Request(url, body, headers)\n\ntry:\n    response = urllib.request.urlopen(req)\n    result = response.read()\n    print(result)\nexcept urllib.error.HTTPError as error:\n    print(\"The request failed with status code: \" + str(error.code))\n    print(error.info())\n    print(error.read().decode(\"utf8\", 'ignore'))\n\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Environment Variables for Inference in Bash\nDESCRIPTION: Sets CUDA environment variables to resolve import errors during inference, particularly for bitsandbytes on A100 80G GPUs on AWS.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/docs/FAQ.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport CUDA_HOME=\"/usr/local/cuda-11.8\"\nexport PATH=$CUDA_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$CUDA_HOME/lib:$CUDA_HOME/lib64:$CUDA_HOME/efa/lib:/opt/amazon/efa/lib:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Implementing Move Tracking Function\nDESCRIPTION: Defines a function to check if a move has been made and reset the flag. This is used to determine when to terminate a conversation turn between agents.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef check_made_move(msg):\n    global made_move\n    if made_move:\n        made_move = False\n        return True\n    else:\n        return False\n\n```\n\n----------------------------------------\n\nTITLE: Processing and Finalizing Dataset in Pandas\nDESCRIPTION: This code finalizes the dataset by dropping the original 'Type' and 'Category' columns, renaming new columns ('New_Type' to 'Type' and 'New_Category' to 'Category'), printing sample data and column information, and saving the processed DataFrame to a CSV file.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfinal_data = result.drop(columns=['Type', 'Category'])\n\n# Rename 'New_Type' to 'Type' and 'New_Category' to 'Category'\nfinal_data = final_data.rename(columns={'New_Type': 'Type', 'New_Category': 'Category'})\n\n# Print the first few rows of the final dataset\nprint(\"\\nFirst few rows of the final dataset:\")\nprint(final_data.head())\n\n# Print the column names of the final dataset\nprint(\"\\nColumns in the final dataset:\")\nprint(final_data.columns.tolist())\n\n# Save the final DataFrame\nfinal_data.to_csv('final_balanced_sample_dataset.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Router for RAG\nDESCRIPTION: Creates a router that decides whether to use vectorstore or web search based on the input question. It uses a Groq LLM with structured output for decision making.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call \nllm = ChatGroq(temperature=0, model=\"llama3-70b-8192\")\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt \nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(question_router.invoke({\"question\": \"Who will the Bears draft first in the NFL draft?\"}))\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n```\n\n----------------------------------------\n\nTITLE: Initializing Suno Bark TTS Model and Processor\nDESCRIPTION: Loading the Suno Bark model and processor with specific device placement and floating-point precision. This setup allows for efficient audio generation with the Bark model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndevice = \"cuda:7\"\n\nprocessor = AutoProcessor.from_pretrained(\"suno/bark\")\n\n#model =  model.to_bettertransformer()\n#model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\nmodel = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(device)#.to_bettertransformer()\n```\n\n----------------------------------------\n\nTITLE: Creating Query Engine Tools\nDESCRIPTION: Defining tools for summary and specific context retrieval from the document.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.tools import QueryEngineTool\n\nsummary_tool = QueryEngineTool.from_defaults(\n    query_engine=summary_query_engine,\n    description=(\n        \"Useful for summarization questions related to MetaGPT\"\n    ),\n)\n\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=vector_query_engine,\n    description=(\n        \"Useful for retrieving specific context from the MetaGPT paper.\"\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Message Sequence Debug Output in Python\nDESCRIPTION: Simple utility code to print the message sequence for debugging tool calls.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(json.dumps(messages, indent=2))\n```\n\n----------------------------------------\n\nTITLE: Converting String Podcast Data to Python Tuple\nDESCRIPTION: Using ast.literal_eval to safely parse the podcast text data into a Python tuple. This converts the string representation of the data structure into an actual Python object.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport ast\nast.literal_eval(PODCAST_TEXT)\n```\n\n----------------------------------------\n\nTITLE: Defining Agent State\nDESCRIPTION: Definition of the agent's state type using TypedDict for type hinting.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n```\n\n----------------------------------------\n\nTITLE: Generated lm_eval Command for Model Evaluation\nDESCRIPTION: Example of an lm_eval command generated by the preparation script. This command uses VLLM for inference with specific parameters for tensor parallelism, data parallelism, and other model configuration options.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/meta_eval/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nlm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=4,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_instruct --batch_size auto --output_path eval_results --include_path ./work_dir --seed 42  --log_samples\n```\n\n----------------------------------------\n\nTITLE: Launching VLLM Server for 70B Instruct Judge Model\nDESCRIPTION: Starts a VLLM server to host the Meta Llama 3 70B Instruct model as a judge for evaluating generated answers. The server is configured to run on multiple GPUs with tensor parallelism.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/RAFT-Chatbot/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=2,3 python -m vllm.entrypoints.openai.api_server  --model meta-Llama/Meta-Llama-3-70B-Instruct --tensor-parallel-size 2 --disable-log-requests --port 8001\n```\n\n----------------------------------------\n\nTITLE: Filling a Search Field using Accessibility Tree Selectors in JSON\nDESCRIPTION: Example of a JSON-structured action to fill a search input field with text, including current state, reasoning, action type, selector, and the value to input.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"current_state\": \"On the search page with a focused search bar.\",\n    \"reasoning\": \"The accessibility tree shows an input field with placeholder 'Search...'. Entering the query 'AI tools' fulfills the next step of the task.\",\n    \"action\": \"fill\",\n    \"selector\": \"placeholder=Search...\",\n    \"value\": \"AI tools\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Chess Game with AutoGen\nDESCRIPTION: Installs the chess library for chess game logic and pyautogen for creating AI agents that can play chess against each other.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install chess\n!pip install pyautogen\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Parler TTS Model for Audio Generation\nDESCRIPTION: Loading the Parler TTS model and tokenizer, then generating audio from text with customized speaker voice description. The model takes both the text content and a description of the desired voice characteristics.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Set up device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load model and tokenizer\nmodel = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n\n# Define text and description\ntext_prompt = \"\"\"\nExactly! And the distillation part is where you take a LARGE-model,and compress-it down into a smaller, more efficient model that can run on devices with limited resources.\n\"\"\"\ndescription = \"\"\"\nLaura's voice is expressive and dramatic in delivery, speaking at a fast pace with a very close recording that almost has no background noise.\n\"\"\"\n# Tokenize inputs\ninput_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\nprompt_input_ids = tokenizer(text_prompt, return_tensors=\"pt\").input_ids.to(device)\n\n# Generate audio\ngeneration = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\naudio_arr = generation.cpu().numpy().squeeze()\n\n# Play audio in notebook\nipd.Audio(audio_arr, rate=model.config.sampling_rate)\n```\n\n----------------------------------------\n\nTITLE: Invoking Langchain Tool Calls for Product Ordering\nDESCRIPTION: A simple example that invokes the LLM with tools to process a user prompt for placing an order for a microphone. It prints the tool calls that the LLM decides to make in response to the request.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n\nuser_prompt = \"Please place an order for a Microphone\"\nprint(llm_with_tools.invoke(user_prompt).tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Setting up Neo4j Connection and Graph Manipulation Functions in Python\nDESCRIPTION: This code snippet defines functions to connect to a Neo4j database, add knowledge triples to the graph, and retrieve information based on user queries. It includes methods for constructing Cypher queries and formatting results.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/conversational-chatbot-langchain/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_cypher_query(question):\n    \"\"\"Use Llama to convert a natural language question into a Cypher query\"\"\"\n    system_prompt = \"You are an expert Neo4j Cypher query generator. Your task is to convert natural language questions into Cypher queries. Only output the Cypher query and nothing else.\"\n    \n    user_message = f\"Convert this question into a Cypher query: {question}\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_message}\n    ]\n    \n    response = llama.chat_completion(messages=messages)\n    cypher_query = response[\"choices\"][0][\"message\"][\"content\"]\n    \n    # Check if the response contains actual Cypher code\n    if \"MATCH\" not in cypher_query and \"RETURN\" not in cypher_query:\n        # Fallback to a simpler query that searches for nodes with similar names\n        tokens = question.lower().split()\n        search_terms = [term for term in tokens if len(term) > 3]\n        if search_terms:\n            search_term = search_terms[0]\n            cypher_query = f\"MATCH (n) WHERE toLower(n.name) CONTAINS '{search_term}' RETURN n LIMIT 5\"\n        else:\n            cypher_query = \"MATCH (n) RETURN n LIMIT 5\"\n    \n    return cypher_query\n\ndef query_graph(question):\n    \"\"\"Query the graph database with a natural language question\"\"\"\n    # Get a Cypher query from the question\n    cypher_query = get_cypher_query(question)\n    print(f\"Generated Cypher query: {cypher_query}\")\n    \n    # Execute the query\n    try:\n        with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD)) as driver:\n            with driver.session() as session:\n                result = session.run(cypher_query)\n                records = list(result)\n                \n                # Format the results\n                if records:\n                    results = []\n                    for record in records:\n                        for key, value in record.items():\n                            if isinstance(value, Node):\n                                # Format node properties\n                                properties = dict(value)\n                                results.append(f\"Node {value.id}: {properties}\")\n                            else:\n                                # Format other types of results\n                                results.append(f\"{key}: {value}\")\n                    return \"\\n\".join(results)\n                else:\n                    return \"No results found.\"\n    except Exception as e:\n        return f\"Error executing query: {str(e)}\"\n\ndef add_to_graph(triples):\n    \"\"\"Add knowledge triples to the graph database\"\"\"\n    with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD)) as driver:\n        with driver.session() as session:\n            for triple in triples:\n                subject, relation, obj = triple\n                \n                # Create a Cypher query to add the triple\n                query = \"\"\"\n                MERGE (s:Entity {name: $subject})\n                MERGE (o:Entity {name: $object})\n                MERGE (s)-[r:RELATION {type: $relation}]->(o)\n                \"\"\"\n                \n                session.run(query, subject=subject, relation=relation, object=obj)\n    \n    return f\"Added {len(triples)} relationships to the graph.\"\n```\n\n----------------------------------------\n\nTITLE: Testing the Extended Tokenizer\nDESCRIPTION: Code snippet demonstrating how to test the extended tokenizer by comparing it with the original Llama tokenizer. It shows the tokenization difference for a Hindi text sample, verifying that the new tokenizer maintains compatibility with the original while being more efficient for Hindi text.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/multilingual/README.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import LlamaTokenizer\n>>> llama_tokenizer = LlamaTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n>>> our_tokenizer = LlamaTokenizer.from_pretrained('./extended_tokenizer')\n>>> for i in range(len(llama_tokenizer)):\n...     assert llama_tokenizer.convert_ids_to_tokens(i) == our_tokenizer.convert_ids_to_tokens(i), f\"Token mismatch at index {i}.\"\n...\n>>> text = \"    \"\n>>> llama_tokenizer.tokenize(text)\n['', '', '', '', '', '<0xE0>', '<0xA4>', '<0x8F>', '', '', '', '', '', '<0xE0>', '<0xA4>', '<0x9B>', '', '', '', '', '', '', '', '', '', '<0xE0>', '<0xA4>', '<0x81>']\n>>> our_tokenizer.tokenize(text)\n['', '', '', '', '', '', '']\n```\n\n----------------------------------------\n\nTITLE: Implementing Speaker 1 Audio Generation Function with Parler TTS\nDESCRIPTION: Creating a function to generate audio for the first speaker using the Parler TTS model. The function takes text input and returns the audio array with its sampling rate.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef generate_speaker1_audio(text):\n    \"\"\"Generate audio using ParlerTTS for Speaker 1\"\"\"\n    input_ids = parler_tokenizer(speaker1_description, return_tensors=\"pt\").input_ids.to(device)\n    prompt_input_ids = parler_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n    generation = parler_model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n    audio_arr = generation.cpu().numpy().squeeze()\n    return audio_arr, parler_model.config.sampling_rate\n```\n\n----------------------------------------\n\nTITLE: Chain-of-Thought Prompting with Llama 2\nDESCRIPTION: Demonstration of Chain-of-Thought prompting which encourages step-by-step reasoning. Adding a simple phrase prompts the model to think more carefully about complex questions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncomplete_and_print(\"Who lived longer Elvis Presley or Mozart?\")\n# Often gives incorrect answer of \"Mozart\"\n\ncomplete_and_print(\"\"\"Who lived longer Elvis Presley or Mozart? Let's think through this carefully, step by step.\"\"\")\n# Gives the correct answer \"Elvis\"\n```\n\n----------------------------------------\n\nTITLE: Evaluating PEFT Fine-tuned Model\nDESCRIPTION: Command to evaluate a PEFT fine-tuned model using lm-evaluation-harness, specifying the path to PEFT checkpoints.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nlm_eval --model hf --model_args pretrained=meta-llama/Llama-3.1-8B, dtype=\"float\",peft=../peft_output --tasks hellaswag --num_fewshot 10  --device cuda:0 --batch_size 8\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Initializing API Clients\nDESCRIPTION: This code imports required modules and initializes API clients for Groq and Tavily. API keys are required for authentication.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/Agents_Tutorial/Tool_Calling_201.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os, arxiv, PyPDF2\nfrom tavily import TavilyClient\nfrom groq import Groq\n\n# Create the Groq client\nclient = Groq(api_key='YOUR_API_KEY')\n\ntavily_client = TavilyClient(api_key='YOUR_API_KEY')\n```\n\n----------------------------------------\n\nTITLE: Testing Function Calling with General Greeting\nDESCRIPTION: Creating a new conversation with a general greeting to test how the model handles non-function-related queries.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"hi!\",\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Llama Guard Text Inference Function (Python)\nDESCRIPTION: Defines a function for running inference on text inputs using the Llama Guard model. It supports custom categories and category exclusion, and uses the apply_chat_template helper function for tokenization.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/llama_guard/llama_guard_text_and_vision_inference.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef llama_guard_text_test(tokenizer, model, prompt, categories: dict[str, str]=None, excluded_category_keys: list[str]=[]):\n\n    if categories is not None:\n        input_ids = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\", categories=categories, excluded_category_keys=excluded_category_keys).to(\"cuda\")\n    else:\n        input_ids = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\", excluded_category_keys=excluded_category_keys).to(\"cuda\")\n    input_prompt = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n    \n    \n    prompt_len = input_ids.shape[1]\n    output = model.generate(\n        input_ids=input_ids,\n        max_new_tokens=20,\n        output_scores=True,\n        return_dict_in_generate=True,\n        pad_token_id=0,\n    )\n    generated_tokens = output.sequences[:, prompt_len:]\n    \n    response = tokenizer.decode(\n        generated_tokens[0], skip_special_tokens=False\n    )\n    return input_prompt, response\n```\n\n----------------------------------------\n\nTITLE: Creating System Prompt for Podcast Script Generation\nDESCRIPTION: Defines a detailed system prompt instructing the LLM on how to transform PDF content into an engaging podcast script. Includes guidelines for natural conversation, structure, and pacing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Adapted and modified from https://github.com/gabrielchua/open-notebooklm\nSYSTEM_PROMPT = \"\"\"\nYou are a world-class podcast producer tasked with transforming the provided input text into an engaging and informative podcast script. The input may be unstructured or messy, sourced from PDFs or web pages. Your goal is to extract the most interesting and insightful content for a compelling podcast discussion.\n\n# Steps to Follow:\n\n1. **Analyze the Input:**\n   Carefully examine the text, identifying key topics, points, and interesting facts or anecdotes that could drive an engaging podcast conversation. Disregard irrelevant information or formatting issues.\n\n2. **Brainstorm Ideas:**\n   In the `<scratchpad>`, creatively brainstorm ways to present the key points engagingly. Consider:\n   - Analogies, storytelling techniques, or hypothetical scenarios to make content relatable\n   - Ways to make complex topics accessible to a general audience\n   - Thought-provoking questions to explore during the podcast\n   - Creative approaches to fill any gaps in the information\n\n3. **Craft the Dialogue:**\n   Develop a natural, conversational flow between the host (Jane) and the guest speaker (the author or an expert on the topic). Incorporate:\n   - The best ideas from your brainstorming session\n   - Clear explanations of complex topics\n   - An engaging and lively tone to captivate listeners\n   - A balance of information and entertainment\n\n   Rules for the dialogue:\n   - The host (Jane) always initiates the conversation and interviews the guest\n   - Include thoughtful questions from the host to guide the discussion\n   - Incorporate natural speech patterns, including verbal fillers such as Uhh, Hmmm, um, well\n   - Allow for natural interruptions and back-and-forth between host and guest - this is very important to make the conversation feel authentic\n   - Ensure the guest's responses are substantiated by the input text, avoiding unsupported claims\n   - Maintain a PG-rated conversation appropriate for all audiences\n   - Avoid any marketing or self-promotional content from the guest\n   - The host concludes the conversation\n\n4. **Summarize Key Insights:**\n   Naturally weave a summary of key points into the closing part of the dialogue. This should feel like a casual conversation rather than a formal recap, reinforcing the main takeaways before signing off.\n\n5. **Maintain Authenticity:**\n   Throughout the script, strive for authenticity in the conversation. Include:\n   - Moments of genuine curiosity or surprise from the host\n   - Instances where the guest might briefly struggle to articulate a complex idea\n   - Light-hearted moments or humor when appropriate\n   - Brief personal anecdotes or examples that relate to the topic (within the bounds of the input text)\n\n6. **Consider Pacing and Structure:**\n   Ensure the dialogue has a natural ebb and flow:\n   - Start with a strong hook to grab the listener's attention\n   - Gradually build complexity as the conversation progresses\n   - Include brief \"breather\" moments for listeners to absorb complex information\n   - For complicated concepts, reasking similar questions framed from a different perspective is recommended\n   - End on a high note, perhaps with a thought-provoking question or a call-to-action for listeners\n\nIMPORTANT RULE:\n1. Each line of dialogue should be no more than 100 characters (e.g., can finish within 5-8 seconds)\n2. Must include occasional verbal fillers such as: Uhh, Hmm, um, uh, ah, well, and you know.\n\nRemember: Always reply in valid JSON format, without code blocks. Begin directly with the JSON output.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Generating Knowledge Graph using LLM\nDESCRIPTION: Function to generate a knowledge graph using Together AI's Meta-Llama-3.1-70B model with JSON mode output.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/knowledge_graphs_with_structured_outputs.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef generate_graph(input) -> KnowledgeGraph:\n    together = Together(api_key = TOGETHER_API_KEY)\n\n    extract = together.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Build a knowledge graph to explain: {input}. Only answer in JSON.\",\n            }\n        ],\n        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n        response_format={\n            \"type\": \"json_object\",\n            \"schema\": KnowledgeGraph.model_json_schema(),\n        },\n    )\n\n    output = json.loads(extract.choices[0].message.content)\n    return output\n```\n\n----------------------------------------\n\nTITLE: Initializing Test Questions Array in Python\nDESCRIPTION: Defines an array of test questions covering different modalities including function evaluation, weather inquiry, image generation, story generation, and text-to-speech conversion.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_tool_calling_agent.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquestions = [\"What is magic_function(3)\",\n             \"What is the weather in SF?\",\n             \"Generate an image based upon this text: 'a yellow lab puppy running free with wild flowers in the mountain behind'\",\n             \"Tell me a story about this image\",\n             \"Convert this text to speech: The image features a small white dog running down a dirt path, surrounded by a beautiful landscape. The dog is happily smiling as it runs, and the path is lined with colorful flowers, creating a vibrant and lively atmosphere. The scene appears to be set in a mountainous area, adding to the picturesque nature of the image.\"\n            ]\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Evaluation with Hugging Face accelerate\nDESCRIPTION: Command to perform data-parallel evaluation using Hugging Face's accelerate library for multi-GPU evaluation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch -m lm_eval --model hf \\\n    --model_args pretrained=meta-llama/Llama-3.1-8B \\\n    --tasks lambada_openai,arc_easy \\\n    --batch_size 16\n```\n\n----------------------------------------\n\nTITLE: Defining Main System Prompt for Llama Model\nDESCRIPTION: This snippet defines the MAIN_SYSTEM_PROMPT used to set up the environment and instructions for the Llama model, including available functions and tool-calling format.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/Agents_Tutorial/Tool_Calling_201.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nMAIN_SYSTEM_PROMPT = \"\"\"\nEnvironment: iPython\nCutting Knowledge Date: December 2023\nToday Date: 15 September 2024\n\n# Tool Instructions\n- Always execute python code in messages that you share.\n- When looking for real time information use relevant functions if available\n\nYou have access to the following functions:\n\nUse the function 'query_for_two_papers' to: Get the internet query results for the arxiv ID of the two papers user wants to compare\n{\n  \"name\": \"query_for_two_papers\",\n  \"description\": \"Internet search the arxiv ID of two papers that user wants to look up\",\n  \"parameters\": {\n    \"paper_1\": {\n      \"param_type\": \"string\",\n      \"description\": \"arxiv id of paper_name_1 from user query\",\n      \"required\": true\n    },\n    \"paper_2\": {\n      \"param_type\": \"string\",\n      \"description\": \"arxiv id of paper_name_2 from user query\",\n      \"required\": true\n    },\n  }\n}\n\nUse the function 'get_arxiv_ids' to: Given a dict of websearch queries, use a LLM to return JUST the arxiv ID, which is otherwise harder to extract\n{\n  \"name\": \"get_arxiv_ids\",\n  \"description\": \"Use the dictionary returned from query_for_two_papers to ask a LLM to extract the arxiv IDs\",\n  \"parameters\": {\n    \"web_results\": {\n      \"param_type\": \"dictionary\",\n      \"description\": \"dictionary of search result for a query from the previous function\",\n      \"required\": true\n    },\n  }\n}\n\nUse the function 'process_arxiv_paper' to: Given the arxiv ID from get_arxiv_ids function, return a download txt file of the paper that we can then use for summarising\n{\n  \"name\": \"process_arxiv_paper\",\n  \"description\": \"Use arxiv IDs extracted from earlier to be downloaded and saved to txt files\",\n  \"parameters\": {\n    \"arxiv_id\": {\n      \"param_type\": \"string\",\n      \"description\": \"arxiv ID of the paper that we want to download and save a txt file of\",\n      \"required\": true\n    },\n  }\n}\n\nUse the function 'summarize_text_file' to: Given the txt file name based on the arxiv IDs we are working with from earlier, get a summary of the paper being discussed\n{\n  \"name\": \"summarize_text_file\",\n  \"description\": \"Summarise the arxiv paper saved in the txt file\",\n  \"parameters\": {\n    \"file_name\": {\n      \"param_type\": \"string\",\n      \"description\": \"Filename to be used to get a summary of\",\n      \"required\": true\n    },\n  }\n}\n\nIf a you choose to call a function ONLY reply in the following format:\n<{start_tag}={function_name}>{parameters}{end_tag}\nwhere\n\nstart_tag => `<function`\nparameters => a JSON dict with the function argument name as key and function argument value as value.\nend_tag => `</function>`\n\nHere is an example,\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n\nReminder:\n- When user is asking for a question that requires your reasoning, DO NOT USE OR FORCE a function call\n- Even if you remember the arxiv ID of papers from input, do not put that in the query_two_papers function call, pass the internet look up query\n- Function calls MUST follow the specified format\n- Required parameters MUST be specified\n- Only call one function at a time\n- Put the entire function call reply on one line\n- When returning a function call, don't add anything else to your response\n\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Loading Llama Guard 3 Models (Python)\nDESCRIPTION: Imports necessary libraries and loads both the 1B text-only model and the 11B vision model using the transformers library. It sets up tokenizers and models for both text and vision tasks.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/llama_guard/llama_guard_text_and_vision_inference.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, MllamaForConditionalGeneration, AutoProcessor, MllamaProcessor, GenerationConfig\nfrom typing import List, Any\nimport torch\n\nlg_small_text_model_id = \"meta-llama/Llama-Guard-3-1B\"\nlg_mm_model_id = \"meta-llama/Llama-Guard-3-11B-Vision\"\n\n# Loading the 1B text only model\nlg_small_text_tokenizer = AutoTokenizer.from_pretrained(lg_small_text_model_id)\nlg_small_text_model = AutoModelForCausalLM.from_pretrained(lg_small_text_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# Loading the 11B Vision model \nlg_mm_tokenizer = MllamaProcessor.from_pretrained(lg_mm_model_id)\nlg_mm_model = MllamaForConditionalGeneration.from_pretrained(lg_mm_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Evaluation for Large Models\nDESCRIPTION: Command to evaluate large models that don't fit on a single GPU by splitting weights across multiple GPUs.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nlm_eval --model hf \\\n    --tasks lambada_openai,arc_easy \\\n    --model_args pretrained=meta-llama/Llama-3.1-70B,parallelize=True \\\n    --batch_size 16\n```\n\n----------------------------------------\n\nTITLE: Helper Functions Implementation\nDESCRIPTION: Defines utility functions for calculations and dog weight lookups used by the ReAct agent.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef calculate(what):\n    return eval(what)\n\ndef average_dog_weight(name):\n    if name in \"Scottish Terrier\":\n        return(\"Scottish Terriers average 20 lbs\")\n    elif name in \"Border Collie\":\n        return(\"a Border Collies average weight is 37 lbs\")\n    elif name in \"Toy Poodle\":\n        return(\"a toy poodles average weight is 7 lbs\")\n    else:\n        return(\"An average dog weights 50 lbs\")\n\nknown_actions = {\n    \"calculate\": calculate,\n    \"average_dog_weight\": average_dog_weight\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating LLM Limitations in Complex Calculations using Python\nDESCRIPTION: Shows the limitation of LLMs in performing complex mathematical calculations. The model consistently provides incorrect answers to a complex arithmetic problem.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ncomplete_and_print(\"\"\"\nCalculate the answer to the following math problem:\n\n((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n\"\"\")\n# Gives incorrect answers like 92448, 92648, 95463\n```\n\n----------------------------------------\n\nTITLE: Installing Llama-Cookbook with LangChain Support\nDESCRIPTION: Installs Llama-Cookbook with additional dependencies required for LangChain integration.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/README.md#2025-04-07_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip install llama-cookbook[langchain]\n```\n\n----------------------------------------\n\nTITLE: Defining System Prompt for Llama Model\nDESCRIPTION: Sets up the system prompt that instructs the Llama model on how to rewrite podcast transcripts with specific personality traits for each speaker and formatting requirements.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-3-Re-Writer.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSYSTEM_PROMPT = \"\"\"\nYou are an international oscar winnning screenwriter\n\nYou have been working with multiple award winning podcasters.\n\nYour job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\n\nMake it as engaging as possible, Speaker 1 and 2 will be simulated by different voice engines\n\nRemember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n\nSpeaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n\nSpeaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n\nMake sure the tangents speaker 2 provides are quite wild or interesting. \n\nEnsure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the Speaker 2.\n\nREMEMBER THIS WITH YOUR HEART\nThe TTS Engine for Speaker 1 cannot do \"umms, hmms\" well so keep it straight text\n\nFor Speaker 2 use \"umm, hmm\" as much, you can also use [sigh] and [laughs]. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\n\nIt should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n\nPlease re-write to make it as characteristic as possible\n\nSTART YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\n\nSTRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES OK? \n\nIT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\n\nExample of response:\n[\n    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. I'm your host, and today we're joined by a renowned expert in the field of AI. We're going to dive into the exciting world of Llama 3.2, the latest release from Meta AI.\"),\n    (\"Speaker 2\", \"Hi, I'm excited to be here! So, what is Llama 3.2?\"),\n    (\"Speaker 1\", \"Ah, great question! Llama 3.2 is an open-source AI model that allows developers to fine-tune, distill, and deploy AI models anywhere. It's a significant update from the previous version, with improved performance, efficiency, and customization options.\"),\n    (\"Speaker 2\", \"That sounds amazing! What are some of the key features of Llama 3.2?\")\n]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Combining Vector and BM25 Results with RRF in Python\nDESCRIPTION: This code combines the results from vector retrieval and BM25 search using Reciprocal Rank Fusion.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Combine the lists using RRF\nhybrid_top_k = reciprocal_rank_fusion(vector_top_k, bm25_top_k)\nhybrid_top_k[1]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries\nDESCRIPTION: Installation of the Together AI and GraphViz Python libraries using pip.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/knowledge_graphs_with_structured_outputs.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together\n!pip install graphviz\n```\n\n----------------------------------------\n\nTITLE: Limiting Number of Examples in Benchmarks\nDESCRIPTION: Command to run evaluation with a limited number of examples using the --limit flag for efficient benchmarking.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nlm_eval --model hf --model_args pretrained=meta-llama/Llama-3.1-8B,dtype=\"float\",peft=../peft_output --tasks hellaswag --num_fewshot 10  --device cuda:0 --batch_size 8 --limit 100\n```\n\n----------------------------------------\n\nTITLE: Implementing Structured Data Extraction\nDESCRIPTION: Definition and usage of Pydantic models for structured data extraction from LLM responses.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.llms.groq import Groq\nfrom llama_index.core.prompts import PromptTemplate\nfrom pydantic import BaseModel\n\nclass Restaurant(BaseModel):\n    \"\"\"A restaurant with name, city, and cuisine.\"\"\"\n    name: str\n    city: str\n    cuisine: str\n\nllm = Groq(model=\"llama3-8b-8192\", pydantic_program_mode=\"llm\")\nprompt_tmpl = PromptTemplate(\n    \"Generate a restaurant in a given city {city_name}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Groq Python Library\nDESCRIPTION: Installing the Groq Python package to interact with the Groq API for Llama 3 model access.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install groq\n```\n\n----------------------------------------\n\nTITLE: Enabling Async Event Loop in Notebook\nDESCRIPTION: Applies nest_asyncio to allow running asynchronous code within the Jupyter notebook, which is required for some of the LlamaIndex functionality.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Training Configuration Settings\nDESCRIPTION: Complete Python configuration settings for fine-tuning, including model parameters, training settings, PEFT options, and performance monitoring flags.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/docs/single_gpu.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    model_name: str=\"PATH/to/Model\"\n    tokenizer_name: str=None\n    enable_fsdp: bool=False\n    low_cpu_fsdp: bool=False\n    run_validation: bool=True\n    batch_size_training: int=4\n    batching_strategy: str=\"packing\" #alternative: padding\n    context_length: int=4096\n    gradient_accumulation_steps: int=1\n    gradient_clipping: bool = False\n    gradient_clipping_threshold: float = 1.0\n    num_epochs: int=3\n    max_train_step: int=0\n    max_eval_step: int=0\n    num_workers_dataloader: int=1\n    lr: float=1e-4\n    weight_decay: float=0.0\n    gamma: float= 0.85\n    seed: int=42\n    use_fp16: bool=False\n    mixed_precision: bool=True\n    val_batch_size: int=1\n    dataset = \"samsum_dataset\"\n    peft_method: str = \"lora\" # None, llama_adapter (Caution: llama_adapter is currently not supported with FSDP)\n    use_peft: bool=False\n    from_peft_checkpoint: str=\"\" # if not empty and use_peft=True, will load the peft checkpoint and resume the fine-tuning on that checkpoint\n    output_dir: str = \"PATH/to/save/PEFT/model\"\n    freeze_layers: bool = False\n    num_freeze_layers: int = 1\n    quantization: bool = False\n    one_gpu: bool = False\n    save_model: bool = True\n    dist_checkpoint_root_folder: str=\"PATH/to/save/FSDP/model\" # will be used if using FSDP\n    dist_checkpoint_folder: str=\"fine-tuned\" # will be used if using FSDP\n    save_optimizer: bool=False # will be used if using FSDP\n    use_fast_kernels: bool = False # Enable using SDPA from PyTroch Accelerated Transformers, make use Flash Attention and Xformer memory-efficient kernels\n    use_wandb: bool = False # Enable wandb for experient tracking\n    save_metrics: bool = False # saves training metrics to a json file for later plotting\n    flop_counter: bool = False # Enable flop counter to measure model throughput, can not be used with pytorch profiler at the same time.\n    flop_counter_start: int = 3 # The step to start profiling, default is 3, which means after 3 steps of warmup stage, the profiler will start to count flops.\n    use_profiler: bool = False # Enable pytorch profiler, can not be used with flop counter at the same time.\n    profiler_dir: str = \"PATH/to/save/profiler/results\" # will be used if using profiler\n```\n\n----------------------------------------\n\nTITLE: Running First Step of Agent Reasoning\nDESCRIPTION: Executes the first step of the reasoning process for the created task, allowing observation of the agent's thinking process.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nstep_output = agent.run_step(task.task_id)\n```\n\n----------------------------------------\n\nTITLE: Defining System Message for LLM Context\nDESCRIPTION: Sets up the system message that provides context to the LLM about its role as a customer service assistant.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nSYSTEM_MESSAGE = \"\"\"\nYou are a helpful customer service LLM for an ecommerce company that processes orders and retrieves information about products.\nYou are currently chatting with Tom Testuser, Customer ID: 10\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking Llama Model on Amazon Bedrock\nDESCRIPTION: Defines a function to invoke a Llama model with a given prompt using the Bedrock Runtime client. It sets parameters like temperature and top_p for text generation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/getting_started_llama_3_on_amazon_bedrock.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef invoke_model(bedrock_runtime, model_id, prompt, max_gen_len=256):\n    \"\"\"\n    Invoke a model with a given prompt using the provided Bedrock Runtime client.\n    \"\"\"\n    body = json.dumps({\n        \"prompt\": prompt,\n        \"temperature\": 0.1,\n        \"top_p\": 0.9,\n        \"max_gen_len\":max_gen_len,\n    })\n    accept = 'application/json'\n    content_type = 'application/json'\n    try:\n        response = bedrock_runtime.invoke_model(body=body, modelId=model_id, accept=accept, contentType=content_type)\n        response_body = json.loads(response.get('body').read())\n        generation = response_body.get('generation')\n        print(generation)\n    except Exception as e:\n        print(f\"Failed to invoke model: {e}\")\n\n    return generation\n```\n\n----------------------------------------\n\nTITLE: Testing Meta Llama 3 API Server\nDESCRIPTION: Curl command to test the deployed Meta Llama 3 API server by sending a sample query and receiving a generated response.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n!curl localhost:8080/generate -X POST -H 'Content-Type: application/json' -d '{\"inputs\": \"What is good about Beijing?\", \"parameters\": { \"max_new_tokens\":64}}' #Replace the localhost with the IP visible to the machine running the notebook\n```\n\n----------------------------------------\n\nTITLE: Installing System Dependencies\nDESCRIPTION: Installation of poppler-utils, a system dependency required for PDF processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/multimodal_RAG_with_nvidia_investor_slide_deck.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!sudo apt-get install -y poppler-utils\n```\n\n----------------------------------------\n\nTITLE: Loading Cleaned Dataset for EDA in Python\nDESCRIPTION: Reads the previously saved cleaned CSV file for exploratory data analysis. This step begins the EDA process after the initial cleaning phase has been completed.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"./clean.csv\")\n```\n\n----------------------------------------\n\nTITLE: Running Llama Fine-Tuning with Custom Dataset in Python\nDESCRIPTION: Command-line instruction for running Llama fine-tuning with a custom dataset. It specifies the dataset type as 'custom_dataset' and provides the path to the custom dataset file.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/datasets/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m llama_cookbook.finetuning --dataset \"custom_dataset\" --custom_dataset.file \"custom_dataset.py\" [TRAINING PARAMETERS]\n```\n\n----------------------------------------\n\nTITLE: Running Text-only Inference with Prompt File in Python\nDESCRIPTION: This snippet shows how to run text-only inference by providing a prompt file as a parameter. It specifies the model name, prompt file path, and uses AuditNLG for safety checks.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython inference.py --model_name <training_config.output_dir> --prompt_file <test_prompt_file> --use_auditnlg\n```\n\n----------------------------------------\n\nTITLE: Checking for Missing Values in Python\nDESCRIPTION: Identifies any missing values in the dataset by column. This is an important data quality check to ensure the dataset is complete before proceeding with further analysis and processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Step 4: Check for missing values\nprint(\"\\nMissing values:\")\nprint(df.isnull().sum())\n```\n\n----------------------------------------\n\nTITLE: Creating Data Visualizations\nDESCRIPTION: Generates plots and charts based on the analyzed data\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nplot_folder = out_folder + \"/plots\"\nos.makedirs(plot_folder, exist_ok=True)\ndraw_all_plots(repo_name, plot_folder, overview)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Installing required packages using pip\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/parallel-tool-use/parallel-tool-use.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Viewing Function Execution Result\nDESCRIPTION: Displaying the result returned by the executed function with the provided arguments.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nobservation\n```\n\n----------------------------------------\n\nTITLE: Compiling and Testing the RAG Workflow in Python\nDESCRIPTION: Compiles the defined graph into an executable application and demonstrates testing with a sample question. The test shows the workflow's execution through various nodes and outputs the final generation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Compile\napp = workflow.compile()\n\n# Test\nfrom pprint import pprint\ninputs = {\"question\": \"What are the types of agent memory?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\npprint(value[\"generation\"])\n```\n\n----------------------------------------\n\nTITLE: Comparing Llama 2 13B and 70B Models with Prompt 2\nDESCRIPTION: Invokes Llama 2 13B and 70B models with a prompt about llamas, then compares their responses using the diff view function.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/getting_started_llama_3_on_amazon_bedrock.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\n=======LLAMA-3-8B====PROMPT 2================>  \", prompt_2)\nresponse_8b_prompt2 = invoke_model(bedrock_runtime, 'meta.llama2-13b-chat-v1', prompt_2, 128)\nprint(\"\\n=======LLAMA-3-70B====PROMPT 2================>  \", prompt_2)\nresponse_70b_prompt2 = invoke_model(bedrock_runtime, 'meta.llama2-70b-chat-v1', prompt_2, 128)\n\n# Print the differences in responses\nprint(\"==========================\")\nprint(\"\\nDIFF VIEW for PROMPT 2:\")\nprint_diff(response_8b_prompt2, response_70b_prompt2)\nprint(\"==========================\")\n```\n\n----------------------------------------\n\nTITLE: Printing Reordered Chunks Based on RRF Score in Python\nDESCRIPTION: This snippet prints the reordered chunks based on the Reciprocal Rank Fusion score.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Notice the reordering of the chunks based on the RRF score\n\nfor index in hybrid_top_k[1]:\n  print(f\"Chunk Index {index} : {contextual_chunks[index]}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Together AI API Key\nDESCRIPTION: Configuration of the Together AI API key for accessing the Llama Vision model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/multimodal_RAG_with_nvidia_investor_slide_deck.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napi_key = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Groq Client\nDESCRIPTION: Sets up the Groq API client with authentication credentials.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom groq import Groq\n\nos.environ['GROQ_API_KEY'] = 'your_groq_api_key' # get a free key at https://console.groq.com/keys\n```\n\n----------------------------------------\n\nTITLE: Running Tests for a Specific File in llama-cookbook\nDESCRIPTION: This command demonstrates how to run all tests within a specific file using pytest.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest src/tests/test_finetuning.py\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for LangGraph RAG Agent\nDESCRIPTION: Installs the necessary Python libraries for building a LangGraph RAG agent, including langchain, langgraph, sentence_transformers, and others.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n! pip install -U langchain_groq langchain langgraph langchain_community sentence_transformers tavily-python tiktoken langchainhub chromadb\n```\n\n----------------------------------------\n\nTITLE: Installing vLLM and Ray Dependencies for Llama Inference\nDESCRIPTION: Command for installing the required dependencies to run Llama inference. vLLM is needed for basic inference, while Ray is required specifically for multi-node inference scenarios.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/vllm/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install vllm\n\n# For multi-node inference we also need to install ray\npip install ray[default]\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Sentiment Analysis with Llama 2\nDESCRIPTION: Example of zero-shot prompting where the model performs sentiment analysis without having seen examples of the task. The output format varies without specific formatting instructions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncomplete_and_print(\"Text: This was the best movie I've ever seen! \\n The sentiment of the text is: \")\n# Returns positive sentiment\n\ncomplete_and_print(\"Text: The director was trying too hard. \\n The sentiment of the text is: \")\n# Returns negative sentiment\n```\n\n----------------------------------------\n\nTITLE: Importing Modules and Setting Up Environment for Together API\nDESCRIPTION: Imports required modules, loads environment variables, and initializes the Together API client for LLM interactions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom dotenv import load_dotenv\nfrom together import Together\n\nload_dotenv()\n\nclient = Together(api_key=os.getenv(\"TOGETHER_API_KEY\"))\n```\n\n----------------------------------------\n\nTITLE: Initializing Fireworks LLM Client\nDESCRIPTION: Creates a Fireworks LLM client instance using the Llama 3 70B model with zero temperature for deterministic outputs.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.llms.fireworks import Fireworks\n\n# Llama 3 8b on Fireworks.ai also works in some cases, but 70b works better overall\n#llm = Fireworks(model=\"accounts/fireworks/models/llama-v3-8b-instruct\", temperature=0)\nllm = Fireworks(model=\"accounts/fireworks/models/llama-v3-70b-instruct\", temperature=0)\n\n# a quick sanity test\n#llm.complete(\"Who wrote the  book godfather? \").text\n```\n\n----------------------------------------\n\nTITLE: Creating Bedrock Client for Amazon Web Services\nDESCRIPTION: Defines a function to create a Bedrock client using the provided AWS credentials and region. This client is used to interact with Amazon Bedrock services.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/getting_started_llama_3_on_amazon_bedrock.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef create_bedrock_client(service_name):\n    \"\"\"\n    Create a Bedrock client using the provided service name and global AWS credentials.\n    \"\"\"\n    return boto3.client(\n        service_name=service_name,\n        region_name=AWS_REGION,\n        aws_access_key_id=AWS_ACCESS_KEY,\n        aws_secret_access_key=AWS_SECRET_KEY,\n        aws_session_token=SESSION_TOKEN\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Evaluation Parameters in YAML\nDESCRIPTION: Sample YAML configuration for evaluating a Llama 3.1 model. It specifies the model name, corresponding evaluation dataset, and which task suite to run (instruct model tasks).\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/meta_eval/README.md#2025-04-07_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_name: \"meta-llama/Llama-3.1-8B-Instruct\" # The name of the model to evaluate. This must be a valid Llama 3 based model name in the HuggingFace model hub.\"\n\nevals_dataset: \"meta-llama/Llama-3.1-8B-Instruct-evals\" # The name of the 3.1 evals dataset to evaluate, please make sure this eval dataset corresponds to the model loaded. This must be a valid Llama 3.1 evals dataset name in the Llama 3.1 Evals collection.\n# Must be one of the following [\"meta-llama/Llama-3.1-8B-Instruct-evals\",\"meta-llama/Llama-3.1-70B-Instruct-evals\",\"meta-llama/Llama-3.1-405B-Instruct-evals\",\"meta-llama/Llama-3.1-8B-evals\",\"meta-llama/Llama-3.1-70B-evals\",\"meta-llama/Llama-3.1-405B-evals\",\"meta-llama/Llama-3.2-1B-evals\",\"meta-llama/Llama-3.2-3B-evals\", \"meta-llama/Llama-3.2-1B-Instruct-evals\", \"meta-llama/Llama-3.2-3B-Instruct-evals\"]\n\ntasks: \"meta_instruct\" # Available tasks for 3.1 instruct model: \"meta_math_hard\", \"meta_gpqa_cot\", \"meta_mmlu_pro_instruct\", \"meta_ifeval\"; or just use \"meta_instruct\" to run all of them.\n# Available tasks for 3.1 pretrain model: \"meta_bbh\", \"meta_mmlu_pro_pretrain\"; or just use \"meta_pretrain\" to run all of them.\n# Available tasks for 3.2 instruct model: \"meta_mmlu\", \"meta_math\", \"meta_gpqa\"; or just use \"meta_instruct\" to run all of them.\n```\n\n----------------------------------------\n\nTITLE: Defining Dialogue Schema with Pydantic for Podcast Script Structure\nDESCRIPTION: Creates Pydantic models to define the structure of the podcast script. DialogueItem represents a single line spoken by either host or guest, while Dialogue encapsulates the entire script including planning notes.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass DialogueItem(BaseModel):\n    \"\"\"A single dialogue item.\"\"\"\n\n    speaker: Literal[\"Host (Jane)\", \"Guest\"]\n    text: str\n\n\nclass Dialogue(BaseModel):\n    \"\"\"The dialogue between the host and guest.\"\"\"\n\n    scratchpad: str\n    name_of_guest: str\n    dialogue: List[DialogueItem]\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Model Paths\nDESCRIPTION: Commands to set environment variables pointing to the base model and PEFT (Parameter-Efficient Fine-Tuning) model paths. This assumes a LoRA fine-tuned model setup.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/vllm/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_PATH=/path/to/out/base/model\nexport PEFT_MODEL_PATH=/path/to/out/peft/model\n```\n\n----------------------------------------\n\nTITLE: Previewing Dataset Entries in Python\nDESCRIPTION: Shows the first few rows of the cleaned dataset to get a visual overview of the data structure and content. This helps verify the data format and understand the available features.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Accessing Retrieved Page\nDESCRIPTION: Retrieves the base64-encoded image of the most relevant page from the search results.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/multimodal_RAG_with_nvidia_investor_slide_deck.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nreturned_page = model.search(query, k=1)[0].base64\n```\n\n----------------------------------------\n\nTITLE: Accessing Corrupt Images List in Python\nDESCRIPTION: Retrieves the list of corrupt images identified in the previous step. This snippet allows viewing of the images that failed validation and will be removed from the dataset.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncorrupt_images\n```\n\n----------------------------------------\n\nTITLE: Counting Non-NaN Title Values in the Dataset\nDESCRIPTION: Calculates how many records have valid titles by subtracting the count of NaN values from the total length of the dataset.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlen(result) - result['Title'].isna().sum()\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment for Email Agent\nDESCRIPTION: These commands create and activate a virtual environment named 'emailagent' for the email agent project, with separate activation commands for Linux/macOS and Windows.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv emailagent\nsource emailagent/bin/activate # on Linux, macOS:\nsource emailagent\\Scripts\\activate # on Windows\n```\n\n----------------------------------------\n\nTITLE: Initializing Groq Client and Environment\nDESCRIPTION: Setting up Groq client with API key from environment variables\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/parallel-tool-use/parallel-tool-use.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\n\nfrom groq import Groq\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\"Groq API key configured: \" + os.environ[\"GROQ_API_KEY\"][:10] + \"...\"\n```\n\n----------------------------------------\n\nTITLE: Running Azure Pretrained Model Benchmark - Python\nDESCRIPTION: Command to execute the pretrained model benchmark script that tests inference throughput for Llama pretrained models on Azure API endpoints. Results are saved to a CSV file.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/cloud/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython pretrained_azure_api_benchmark.py\n```\n\n----------------------------------------\n\nTITLE: Setting Up AWS Credentials for Amazon Bedrock\nDESCRIPTION: Prompts the user to input AWS credentials and region for authentication. Note: This method is not recommended for production use due to security concerns.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/getting_started_llama_3_on_amazon_bedrock.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Set default AWS region\ndefault_region = \"us-east-1\"\n\n# Get AWS credentials from user input (not recommended for production use)\nAWS_ACCESS_KEY = getpass(\"AWS Access key: \")\nAWS_SECRET_KEY = getpass(\"AWS Secret key: \")\nSESSION_TOKEN = getpass(\"AWS Session token: \")\nAWS_REGION = input(f\"AWS Region [default: {default_region}]: \") or default_region\n```\n\n----------------------------------------\n\nTITLE: Creating Fixed-Size Chunks from Document Text\nDESCRIPTION: Defines a function to create fixed-size chunks from the document text with optional overlap. This is used to prepare the document for contextual augmentation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef create_chunks(document, chunk_size=300, overlap=50):\n    return [document[i : i + chunk_size] for i in range(0, len(document), chunk_size - overlap)]\n```\n\n----------------------------------------\n\nTITLE: Implementing PDF Text Extraction Function\nDESCRIPTION: Creates a function to read a PDF file and extract its text content. Handles errors and limits large PDFs to prevent processing issues.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_PDF_text(file : str):\n    text = ''\n\n    # Read the PDF file and extract text\n    try:\n        with Path(file).open(\"rb\") as f:\n            reader = PdfReader(f)\n            text = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n    except Exception as e:\n        raise f\"Error reading the PDF file: {str(e)}\"\n\n    if len(text) > 400000:\n        raise \"The PDF is too long. Please upload a smaller PDF.\"\n\n    return text\n```\n\n----------------------------------------\n\nTITLE: Setting Up Replicate API Token for Llama 3 Access\nDESCRIPTION: Prompts for and sets the Replicate API token as an environment variable for accessing Llama 3.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/video_summary.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom getpass import getpass\nimport os\n\nREPLICATE_API_TOKEN = getpass()\nos.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Generating Speech with Parler-TTS in Python\nDESCRIPTION: This code snippet demonstrates how to use the Parler-TTS model to generate speech from text. It includes setting up the model and tokenizer, defining the text prompt and description, and generating the audio output.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/TTS_Notes.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n\n# Define text and description\ntext_prompt = \"This is where the actual words to be spoken go\"\ndescription = \"\"\"\nLaura's voice is expressive and dramatic in delivery, speaking at a fast pace with a very close recording that almost has no background noise.\n\"\"\"\n\ninput_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\nprompt_input_ids = tokenizer(text_prompt, return_tensors=\"pt\").input_ids.to(device)\n\ngeneration = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\naudio_arr = generation.cpu().numpy().squeeze()\n\nipd.Audio(audio_arr, rate=model.config.sampling_rate)\n```\n\n----------------------------------------\n\nTITLE: Remapping Types to Standardize Values\nDESCRIPTION: Creates a function to standardize the 'Type' column values into broader categories: Casual, Formal, and Lounge.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef map_type(type_):\n    type_ = type_.lower()\n    if type_ in ['casual', 'workout', 'athletic', 'swimming', 'swimwear', 'footwear']:\n        return 'Casual'\n    elif type_ in ['formal', 'work casual', 'work']:\n        return 'Formal'\n    elif type_ in ['lounge', 'sleepwear', 'home decor']:\n        return 'Lounge'\n    else:\n        return 'Casual'  # Default to Casual for any unmatched types\n\n# Apply the mapping function to the 'Type' column\nresult['New_Type'] = result['Type'].apply(map_type)\n\n# Print the distribution of new types\nprint(\"Distribution of New Types:\")\nprint(result['New_Type'].value_counts())\n\n# Print the mapping of old types to new types\nprint(\"\\nMapping of Old Types to New Types:\")\nprint(result.groupby('Type')['New_Type'].first().sort_index())\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installs necessary Python packages including langchain_groq, langchain, tavily-python, replicate, and langgraph.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_tool_calling_agent.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install -U langchain_groq langchain tavily-python replicate langgraph matplotlib\n```\n\n----------------------------------------\n\nTITLE: Loading Movie Dataset\nDESCRIPTION: Loading and viewing the movie dataset from JSON file\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/text_RAG_using_llama_on_together.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nwith open('./datasets/movies.json', 'r') as file:\n    movies_data = json.load(file)\n\nmovies_data[:3]\n```\n\n----------------------------------------\n\nTITLE: Restricting Responses to Recent Sources\nDESCRIPTION: Example showing how to use explicit instructions to get more specific results by limiting responses to recent sources. This helps ensure the model provides up-to-date information when needed.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncomplete_and_print(\"Explain the latest advances in large language models to me.\")\n# More likely to cite sources from 2017\n\ncomplete_and_print(\"Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\")\n# Gives more specific advances and only cites sources from 2020\n```\n\n----------------------------------------\n\nTITLE: Using Explicit Instructions with Llama 2\nDESCRIPTION: Example of using explicit, detailed instructions to get concise results from Llama 2. This approach uses rules and restrictions to guide the model's response format and content.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncomplete_and_print(prompt=\"Describe quantum physics in one short sentence with no more than 12 words\")\n# Returns a succinct explanation of quantum physics that mentions particles and states existing simultaneously.\n```\n\n----------------------------------------\n\nTITLE: Initializing CondensePlusContextChatEngine with RAG in Python\nDESCRIPTION: This snippet sets up a stateful chatbot using Llama Index's CondensePlusContextChatEngine. It initializes a chat memory buffer and configures the chat engine with a custom context prompt for discussing the Kendrick and Drake beef.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.memory import ChatMemoryBuffer\nfrom llama_index.core.chat_engine import CondensePlusContextChatEngine\n\nmemory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n\nchat_engine = CondensePlusContextChatEngine.from_defaults(\n    index.as_retriever(),\n    memory=memory,\n    llm=llm,\n    context_prompt=(\n        \"You are a chatbot, able to have normal interactions, as well as talk\"\n        \" about the Kendrick and Drake beef.\"\n        \"Here are the relevant documents for the context:\\n\"\n        \"{context_str}\"\n        \"\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\n    ),\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing CondensePlusContextChatEngine with RAG in Python\nDESCRIPTION: This snippet sets up a stateful chatbot using Llama Index's CondensePlusContextChatEngine. It initializes a chat memory buffer and configures the chat engine with a custom context prompt for discussing the Kendrick and Drake beef.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.memory import ChatMemoryBuffer\nfrom llama_index.core.chat_engine import CondensePlusContextChatEngine\n\nmemory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n\nchat_engine = CondensePlusContextChatEngine.from_defaults(\n    index.as_retriever(),\n    memory=memory,\n    llm=llm,\n    context_prompt=(\n        \"You are a chatbot, able to have normal interactions, as well as talk\"\n        \" about the Kendrick and Drake beef.\"\n        \"Here are the relevant documents for the context:\\n\"\n        \"{context_str}\"\n        \"\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\n    ),\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Together AI Client\nDESCRIPTION: Setting up the Together AI client with API key configuration\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/text_RAG_using_llama_on_together.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport together, os\nfrom together import Together\n\n# Paste in your Together AI API Key or load it\nTOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Installing Text2SQL Requirements using pip\nDESCRIPTION: This snippet shows how to clone the llama-cookbook repository, navigate to the text2sql directory, and install the required dependencies using pip.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/meta-llama/llama-cookbook.git\ncd llama-cookbook/end-to-end-use-cases/coding/text2sql/\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Structuring Benchmark Documentation in Markdown\nDESCRIPTION: This markdown snippet outlines the structure of the benchmarking documentation, listing two main folders: 'inference' for throughput analysis and 'llm_eval_harness' for quality evaluation of Llama models.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Benchmarks\n\n* inference - a folder contains benchmark scripts that apply a throughput analysis for Llama models inference on various backends including on-prem, cloud and on-device.\n* llm_eval_harness - a folder that introduces `lm-evaluation-harness`, a tool to evaluate Llama models including quantized models focusing on quality. We also included a recipe that calculates Llama 3.1 evaluation metrics Using `lm-evaluation-harness` and instructions that calculate HuggingFace Open LLM Leaderboard v2 metrics.\n```\n\n----------------------------------------\n\nTITLE: Chunking and Generating Contextual Keywords for Financial Report in Python\nDESCRIPTION: This snippet defines functions to split content into chunks, generate chunked content, and create contextual keywords for each chunk. It processes the document content, splits it into chunks, and generates keywords using external helper functions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Contextual-Chunking-RAG/Example_FinancialReport_RAG.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Split into chunks (by tokens)\nfrom helper import file_get_contents, file_put_contents, generate_contextual_keywords, get_llm_answer, generate_questions_bychunk\nimport tiktoken\nenc = tiktoken.get_encoding(\"o200k_base\")\n\ndef split_into_chunks(content, chunk_size):\n\ta = enc.encode(content)\n\tleft, chunks = 0, []\n\twhile left < len(a):\n\t\tarr = a[left : left+chunk_size]\n\t\tchunks.append(enc.decode(arr))\n\t\tleft+=chunk_size\n\treturn chunks\n    \ndef generate_chunked_content(chunks):\n    chunked_content = \"\"\n    for idx, text in enumerate(chunks):\n      chunked_content+=f\"### Chunk {idx+1} ###\\n{text}\\n\\n\"\n    return chunked_content\n    \n\n# Generate contextual keywords\npath = \"./temp/chunks2.json\"\nif not os.path.exists(path):\n    print(\"Generating keywords..\")\n    document_content, chunks, chunks2 = \"\", [], []\n    for doc in documents: document_content+=doc.text+\"\\n\"\n    chunks1 = split_into_chunks(document_content, 400) #400 -- defaulf value\n    for i, chunk in enumerate(chunks1):\n        chunks.append(chunk)\n        if (len(chunks) > 10 or (i==len(chunks1)-1) and len(chunks)>2):\n            chunked_content = generate_chunked_content(chunks)\n            keywords = generate_contextual_keywords(chunked_content)        \n            print(\"page_end:\", i+1, keywords, len(keywords), len(chunks))            \n            assert len(keywords) >= len(chunks)\n            for j in range(len(chunks)): chunks2.append( {\"idx\":j, \"keywords\":keywords[j], \"content\":chunks[j]} )\n            chunks = []\n    file_put_contents(path, json.dumps(chunks2))\nelse:\n    chunks2 = json.loads(file_get_contents(path)) #it has content, keywords, idx\n\n\n# Generate questions\npath = \"./temp/chunks3.json\"\nif not os.path.exists(path):\n    print(\"Generating questions..\")\n    chunks3 = generate_questions_bychunk(chunks2) \n    file_put_contents(path, json.dumps(chunks3))\nelse:\n    chunks3 = json.loads(file_get_contents(path)) #it has content, keywords, questions, idx now\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Llama-based Issue Triaging Tool\nDESCRIPTION: This command installs the required Python packages for the issue triaging tool. It uses pip to install dependencies listed in the requirements.txt file.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Llama-Cookbook with pip\nDESCRIPTION: Basic installation of Llama-Cookbook using pip package manager. This is the simplest method for adding the library to your project.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/README.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install llama-cookbook\n```\n\n----------------------------------------\n\nTITLE: Initializing Groq Client\nDESCRIPTION: Creates Groq API client instance and specifies the LLM model to use.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-function-calling-for-sql/json-mode-function-calling-for-sql.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclient = Groq(api_key = os.getenv('GROQ_API_KEY'))\nmodel = 'llama3-70b-8192'\n```\n\n----------------------------------------\n\nTITLE: Implementing DuckDB Query Execution\nDESCRIPTION: Creates a function to execute generated SQL queries against DuckDB database.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-function-calling-for-sql/json-mode-function-calling-for-sql.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef execute_duckdb_query(query):\n    original_cwd = os.getcwd()\n    os.chdir('data')\n    \n    try:\n        conn = duckdb.connect(database=':memory:', read_only=False)\n        query_result = conn.execute(query).fetchdf().reset_index(drop=True)\n    finally:\n        os.chdir(original_cwd)\n\n\n    return query_result\n```\n\n----------------------------------------\n\nTITLE: Implementing Retrieval-Augmented Generation (RAG) in Python\nDESCRIPTION: Demonstrates the implementation of RAG using a simple lookup table for temperature data. This approach allows the LLM to provide accurate responses to specific queries by incorporating retrieved information into the prompt.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nMENLO_PARK_TEMPS = {\n    \"2023-12-11\": \"52 degrees Fahrenheit\",\n    \"2023-12-12\": \"51 degrees Fahrenheit\",\n    \"2023-12-13\": \"51 degrees Fahrenheit\",\n}\n\n\ndef prompt_with_rag(retrived_info, question):\n    complete_and_print(\n        f\"Given the following information: '{retrived_info}', respond to: '{question}'\"\n    )\n\n\ndef ask_for_temperature(day):\n    temp_on_day = MENLO_PARK_TEMPS.get(day) or \"unknown temperature\"\n    prompt_with_rag(\n        f\"The temperature in Menlo Park was {temp_on_day} on {day}'\",  # Retrieved fact\n        f\"What is the temperature in Menlo Park on {day}?\",  # User question\n    )\n\n\nask_for_temperature(\"2023-12-12\")\n# \"Sure! The temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\"\n\nask_for_temperature(\"2023-07-18\")\n# \"I'm not able to provide the temperature in Menlo Park on 2023-07-18 as the information provided states that the temperature was unknown.\"\n```\n\n----------------------------------------\n\nTITLE: Importing RAG QueryEngine Components in Python\nDESCRIPTION: This code imports necessary components from Llama Index to create RAG (Retrieval-Augmented Generation) query engines and tools.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    StorageContext,\n    load_index_from_storage,\n)\n\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n```\n\n----------------------------------------\n\nTITLE: Listing Meta Bedrock Models on Amazon Web Services\nDESCRIPTION: Defines a function to list all available Meta Bedrock models using the provided Bedrock client. It filters models by the 'meta' provider.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/getting_started_llama_3_on_amazon_bedrock.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef list_all_meta_bedrock_models(bedrock):\n    \"\"\"\n    List all Meta Bedrock models using the provided Bedrock client.\n    \"\"\"\n    try:\n        list_models = bedrock.list_foundation_models(byProvider='meta')\n        print(\"\\n\".join(list(map(lambda x: f\"{x['modelName']} : { x['modelId'] }\", list_models['modelSummaries']))))\n    except Exception as e:\n        print(f\"Failed to list models: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment for FMBench\nDESCRIPTION: Commands to create a new conda environment with Python 3.11 and install the FMBench package.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/cloud/aws/fmbench/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name fmbench_python311 -y python=3.11 ipykernel\nsource activate fmbench_python311;\npip install -U fmbench\n```\n\n----------------------------------------\n\nTITLE: Implementing Text-to-SQL Querying\nDESCRIPTION: Setup and configuration of SQL database querying capabilities using natural language.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.indices.struct_store import NLSQLTableQueryEngine\n\nquery_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[\"albums\", \"tracks\", \"artists\"],\n    llm=llm_replicate,\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Function Result to Conversation History\nDESCRIPTION: Appending the function execution result to the message history as a function response message.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nmessages.append(\n        {\n            \"role\": \"function\",\n            \"name\": function_call.name,\n            \"content\": observation,\n        }\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Llama-Cookbook with Safety Checker\nDESCRIPTION: Installs Llama-Cookbook with the AuditNLG package for sensitive topics safety checking functionality.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/README.md#2025-04-07_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install llama-cookbook[auditnlg]\n```\n\n----------------------------------------\n\nTITLE: Defining Mathematical Operation Tools\nDESCRIPTION: Creates function tools for basic mathematical operations (multiply, add, subtract, divide) that can be used by the agent.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiple two integers and returns the result integer\"\"\"\n    return a * b\n\n\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers and returns the result integer\"\"\"\n    return a + b\n\n\ndef subtract(a: int, b: int) -> int:\n    \"\"\"Subtract two integers and returns the result integer\"\"\"\n    return a - b\n\n\ndef divide(a: int, b: int) -> int:\n    \"\"\"Divides two integers and returns the result integer\"\"\"\n    return a / b\n\n\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\nadd_tool = FunctionTool.from_defaults(fn=add)\nsubtract_tool = FunctionTool.from_defaults(fn=subtract)\ndivide_tool = FunctionTool.from_defaults(fn=divide)\n```\n\n----------------------------------------\n\nTITLE: Generating Final Response Based on Function Results\nDESCRIPTION: Creating a new chat completion that uses the function results to provide a natural language response to the original weather query.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=messages,\n    temperature=0\n)\n\nresponse.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Few-Shot Sentiment Analysis with Llama 2\nDESCRIPTION: Implementation of few-shot prompting for sentiment analysis with Llama 2. This approach provides specific examples to get more accurate and consistent output with nuanced sentiment classification.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef sentiment(text):\n    response = chat_completion(messages=[\n        user(\"You are a sentiment classifier. For each message, give the percentage of positive/netural/negative.\"),\n        user(\"I liked it\"),\n        assistant(\"70% positive 30% neutral 0% negative\"),\n        user(\"It could be better\"),\n        assistant(\"0% positive 50% neutral 50% negative\"),\n        user(\"It's fine\"),\n        assistant(\"25% positive 50% neutral 25% negative\"),\n        user(text),\n    ])\n    return response\n\ndef print_sentiment(text):\n    print(f'INPUT: {text}')\n    print(sentiment(text))\n\nprint_sentiment(\"I thought it was okay\")\n# More likely to return a balanced mix of positive, neutral, and negative\nprint_sentiment(\"I loved it!\")\n# More likely to return 100% positive\nprint_sentiment(\"Terrible service 0/10\")\n# More likely to return 100% negative\n```\n\n----------------------------------------\n\nTITLE: Implementing Llama 3 Enabled Web App in Python\nDESCRIPTION: Python code for creating a Flask web app that integrates Llama 3 using LangChain and Replicate. It receives messages from a webhook, processes them with Llama 3, and sends responses back to Messenger.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/messenger_chatbot/messenger_llama3.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport langchain\nfrom langchain.llms import Replicate\n\nfrom flask import Flask\nfrom flask import request\nimport os\nimport requests\nimport json\n\nos.environ[\"REPLICATE_API_TOKEN\"] = \"<your replicate api token\"\nllama3_8b_chat = \"meta/meta-llama-3-8b-instruct\"\n\nllm = Replicate(\n    model=llama3_8b_chat,\n    model_kwargs={\"temperature\": 0.0, \"top_p\": 1, \"max_new_tokens\":500}\n)\n\napp = Flask(__name__)\n\n@app.route('/msgrcvd_page', methods=['POST', 'GET'])\ndef msgrcvd_page():\n    message = request.args.get('message')\n    sender = request.args.get('sender')\n    recipient = request.args.get('recipient')\n\n    answer = llm(message)\n\n    url = f\"https://graph.facebook.com/v18.0/{recipient}/messages\"\n    params = {\n        'recipient': '{\"id\": ' + sender + '}',\n        'message': json.dumps({'text': answer}),\n        'messaging_type': 'RESPONSE',\n        'access_token': '<page_access_token>'\n    }\n    headers = {\n        'Content-Type': 'application/json'\n    }\n    response = requests.post(url, params=params, headers=headers)\n\n    return message + \"<p/>\" + answer\n```\n\n----------------------------------------\n\nTITLE: Function Execution with Pre and Post Processing\nDESCRIPTION: Code snippet showing how the Agent class executes functions based on Llama's response, including pre-processing of parameters and post-processing of results for user-friendly output.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfunction_name = result[\"function_name\"]\nfunc = globals()[function_name]\nparameters = result[\"parameters\"]\n... <pre-processing>\nresult = func(**parameters)\n... <post-processing>\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installing necessary Python packages boto3 and langchain for AWS Bedrock integration.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# install packages\n!python3 -m pip install -qU boto3\n!python3 -m pip install langchain\n\nimport boto3\nimport json\n```\n\n----------------------------------------\n\nTITLE: CodeShield MD5 Hash Example\nDESCRIPTION: Example of scanning a code snippet containing weak MD5 hash implementation\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/code_shield_usage_demo.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nllm_output_code = \"\"\"\ndef hashString(input):\n    return hashlib.md5(input)\n\"\"\"\n\nawait scan_llm_output(llm_output_code)\n```\n\n----------------------------------------\n\nTITLE: Together AI Cookbook Table in Markdown\nDESCRIPTION: A markdown table listing various cookbook examples with descriptions and links to Colab notebooks for implementing different Llama-based applications on Together AI platform.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Cookbook | Description | Open |\n| -------- | ----------- | ---- |\n| [MultiModal RAG with Nvidia Investor Slide Deck](https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/multimodal_RAG_with_nvidia_investor_slide_deck.ipynb) | Multimodal RAG using Nvidia investor slides. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/multimodal_RAG_with_nvidia_investor_slide_deck.ipynb) [![](https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/youtubebadge.svg)](https://youtu.be/IluARWPYAUc?si=gG90hqpboQgNOAYG)|\n| [Llama Contextual RAG](./llama_contextual_RAG.ipynb) | Implementation of Contextual Retrieval using Llama models. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb) |\n| [Llama PDF to podcast](./pdf_to_podcast_using_llama_on_together.ipynb) | Generate a podcast from PDF content using Llama. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb) |\n| [Knowledge Graphs with Structured Outputs](./knowledge_graphs_with_structured_outputs.ipynb) | Get Llama to generate knowledge graphs. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/knowledge_graphs_with_structured_outputs.ipynb) |\n| [Structured Text Extraction from Images](./structured_text_extraction_from_images.ipynb) | Extract structured text from images. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/structured_text_extraction_from_images.ipynb) |\n| [Text RAG](./text_RAG_using_llama_on_together.ipynb) | Implement text-based Retrieval-Augmented Generation. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/text_RAG_using_llama_on_together.ipynb) |\n```\n\n----------------------------------------\n\nTITLE: CodeShield Scan Implementation\nDESCRIPTION: Helper function to scan code using CodeShield and process the scanning results with security treatments\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/code_shield_usage_demo.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom codeshield.cs import CodeShield\n\nasync def scan_llm_output(llm_output_code):\n    result = await CodeShield.scan_code(llm_output_code)\n    if result.is_insecure:\n        # perform actions based on treatment recommendation\n        if result.recommended_treatment == \"block\":\n            llm_output_code = \"*** Code Security issues found, blocking the code ***\"\n        if result.recommended_treatment == \"warn\":\n            llm_output_code = llm_output_code + \"*** Warning: The generated snippit contains insecure code ***\"\n    \n    \n    summary = \"Security issue detected\" if result.is_insecure else \"No issues found\"\n    print(\"__LLM output after treatment___\")\n    print(llm_output_code)\n    print (\"__Results__\")\n    print(summary)\n    print(result.recommended_treatment)\n    print (\"__Details__\")\n    print(result.issues_found)\n```\n\n----------------------------------------\n\nTITLE: Starting VLLM OpenAI-compatible Server\nDESCRIPTION: Command to launch a VLLM server hosting Meta Llama 3 70B Instruct model. Requires at least 135GB GPU memory and uses tensor parallelism across multiple GPUs.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/RAFT-Chatbot/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1 python -m vllm.entrypoints.openai.api_server  --model meta-Llama/Meta-Llama-3-70B-Instruct --tensor-parallel-size 2 --disable-log-requests --port 8001\n```\n\n----------------------------------------\n\nTITLE: Loading Clinical Notes from Repository\nDESCRIPTION: This code block reads clinical notes from a specified directory, lists all text files, and displays the content of the first note using Markdown.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-social-determinants-of-health/SDOH-Json-mode.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define the directory path\nfolder_path = 'clinical_notes/'\n\n# List all files in the directory\nfile_list = os.listdir(folder_path)\ntext_files = sorted([file for file in file_list if file.endswith('.txt')])\n\nwith open(os.path.join(folder_path, text_files[0]), 'r') as file:\n    clinical_note = file.read()\n\ndisplay(Markdown(clinical_note))\n```\n\n----------------------------------------\n\nTITLE: Parallel Function Calls Implementation in Python\nDESCRIPTION: Shows how to handle multiple concurrent function calls when operations are independent of each other. The example demonstrates getting prices for multiple products simultaneously using parallel tool calls.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nuser_prompt = \"Please get the price for the Laptop and Microphone\"\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n    {\n        \"role\": \"user\",\n        \"content\": user_prompt,\n    },\n]\n\n# Step 1: send the conversation and available functions to the model\nresponse = client.chat.completions.create(\n    model=MODEL, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n)\n\nresponse_message = response.choices[0].message\ntool_calls = response_message.tool_calls\nprint(\"First LLM Call (Tool Use) Response:\", response_message)\n# Step 2: check if the model wanted to call a function\nif tool_calls:\n    # Step 3: call the function and append the tool call to our list of messages\n    available_functions = {\n        \"get_product_price\": get_product_price,\n    }  # only one function in this example, but you can have multiple\n    messages.append(\n        {\n            \"role\": \"assistant\",\n            \"tool_calls\": [\n                {\n                    \"id\": tool_call.id,\n                    \"function\": {\n                        \"name\": tool_call.function.name,\n                        \"arguments\": tool_call.function.arguments,\n                    },\n                    \"type\": tool_call.type,\n                }\n                for tool_call in tool_calls\n            ],\n        }\n    )\n    # Step 4: send the info for each function call and function response to the model\n    # Iterate over all tool calls\n    for tool_call in tool_calls:\n        function_name = tool_call.function.name\n        function_to_call = available_functions[function_name]\n        function_args = json.loads(tool_call.function.arguments)\n        function_response = function_to_call(\n            product_name=function_args.get(\"product_name\")\n        )\n        messages.append(\n            {\n                \"tool_call_id\": tool_call.id,\n                \"role\": \"tool\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )  # extend conversation with function response\n    second_response = client.chat.completions.create(\n        model=MODEL, messages=messages\n    )  # get a new response from the model where it can see the function response\n    print(\"\\n\\nSecond LLM Call Response:\", second_response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up Constants for Chatbot UI\nDESCRIPTION: This snippet imports required libraries, sets up paths and constants, and enables LangChain debug mode. It also defines model URLs and a system message for the chatbot.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport langchain\nfrom queue import Queue\nfrom typing import Any\nfrom langchain.llms.huggingface_text_gen_inference import HuggingFaceTextGenInference\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.schema import LLMResult\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts.prompt import PromptTemplate\nfrom anyio.from_thread import start_blocking_portal #For model callback streaming\n\nlangchain.debug=True \n\n#vector db path\nDB_FAISS_PATH = 'vectorstore/db_faiss'\n\n#Llama2 TGI models host port\nLLAMA3_8B_HOSTPORT = \"http://localhost:8080/\" #Replace the localhost with the IP visible to the machine running the notebook\nLLAMA3_70B_HOSTPORT = \"http://localhost:8081/\" # You can host multiple models if your infrastructure has capacity\n\n\nmodel_dict = {\n    \"8b-instruct\" : LLAMA3_8B_HOSTPORT,\n    \"70b-instruct\" : LLAMA3_70B_HOSTPORT,\n}\n\nsystem_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n```\n\n----------------------------------------\n\nTITLE: Optimized Suno/Bark Speech Generation in Python\nDESCRIPTION: This code snippet demonstrates the optimal configuration for generating speech using the Suno/Bark model. It uses specific temperature and semantic_temperature values that were determined through experimentation to produce the best results.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/TTS_Notes.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspeech_output = model.generate(**inputs, temperature = 0.9, semantic_temperature = 0.8)\nAudio(speech_output[0].cpu().numpy(), rate=sampling_rate)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Importing necessary Python modules for AWS Bedrock and LangChain integration.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom getpass import getpass\nfrom urllib.request import urlopen\nfrom typing import Dict, List\nfrom langchain.llms import Bedrock\nfrom langchain.memory import ChatMessageHistory\nfrom langchain.schema.messages import get_buffer_string\nimport os\n```\n\n----------------------------------------\n\nTITLE: Installing CodeShield via pip\nDESCRIPTION: Command to install the CodeShield package from PyPI using pip\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/code_shield_usage_demo.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install codeshield\n```\n\n----------------------------------------\n\nTITLE: Creating ReAct Agent with Math Tools\nDESCRIPTION: Initializes a ReAct agent with mathematical operation tools using LlamaIndex.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nagent = ReActAgent.from_tools(\n    [multiply_tool, add_tool, subtract_tool, divide_tool],\n    llm=llm_replicate,\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Program-Aided Language Model (PAL) for Calculations in Python\nDESCRIPTION: Demonstrates the PAL approach by instructing the LLM to generate Python code to solve a complex calculation task. This leverages the LLM's code generation capabilities to overcome its arithmetic limitations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ncomplete_and_print(\n    \"\"\"\n    # Python code to calculate: ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n    \"\"\")\n```\n\n----------------------------------------\n\nTITLE: Installing CodeShield from Source\nDESCRIPTION: Commands to clone the PurpleLlama repository and install CodeShield locally from source\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/code_shield_usage_demo.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/meta-llama/PurpleLlama\ncd PurpleLlama/CodeShield\npip install .\n```\n\n----------------------------------------\n\nTITLE: Importing Chess and Type Annotation Libraries\nDESCRIPTION: Imports the chess module for chess game functionality and typing_extensions for type annotations used in function definitions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport chess\nimport chess.svg\nfrom typing_extensions import Annotated\n```\n\n----------------------------------------\n\nTITLE: Testing Ablation Study Query\nDESCRIPTION: Example of querying specific information about ablation study results.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nresponse = query_engine.query(\"Tell me about the ablation study results?\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Running Chat Completion with Safety Features in Python\nDESCRIPTION: This snippet shows how to run chat completion with built-in safety features. It specifies the model path, prompt file, quantization level, and uses AuditNLG for safety checks.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython chat_completion/chat_completion.py --model_name \"PATH/TO/MODEL/7B/\" --prompt_file chat_completion/chats.json  --quantization 8bit --use_auditnlg\n```\n\n----------------------------------------\n\nTITLE: Loading FAISS Vector Store for Chatbot\nDESCRIPTION: This code loads the FAISS vector store using HuggingFace embeddings. It sets up the embeddings model and loads the local FAISS database.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n                                       model_kwargs={'device': 'cuda'})\ndb = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Memory Fragmentation Environment Variable in Bash\nDESCRIPTION: Sets an environment variable to better manage CUDA memory fragmentation during fine-tuning, especially with FSDP. This feature is available on PyTorch nightlies as of July 30, 2023.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/docs/FAQ.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nos.environ['PYTORCH_CUDA_ALLOC_CONF']='expandable_segments:True'\n```\n\n----------------------------------------\n\nTITLE: Implementing Completion and Chat Functions\nDESCRIPTION: Defining utility functions for model completion and chat interactions using Bedrock and LangChain.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nLLAMA2_70B_CHAT = \"meta.llama2-70b-chat-v1\"\nLLAMA2_13B_CHAT = \"meta.llama2-13b-chat-v1\"\n\n# We'll default to the smaller 13B model for speed; change to LLAMA2_70B_CHAT for more advanced (but slower) generations\nDEFAULT_MODEL = LLAMA2_13B_CHAT\n\ndef completion(\n    prompt: str,\n    model: str = DEFAULT_MODEL,\n    temperature: float = 0.0, \n    top_p: float = 0.9,\n) -> str:\n    llm = Bedrock(credentials_profile_name='default', model_id=DEFAULT_MODEL)\n    return llm.invoke(prompt, temperature=temperature, top_p=top_p)\n\ndef chat_completion(\n    messages: List[Dict],\n    model = DEFAULT_MODEL,\n    temperature: float = 0.0, \n    top_p: float = 0.9,\n) -> str:\n    history = ChatMessageHistory()\n    for message in messages:\n        if message[\"role\"] == \"user\":\n            history.add_user_message(message[\"content\"])\n        elif message[\"role\"] == \"assistant\":\n            history.add_ai_message(message[\"content\"])\n        else:\n            raise Exception(\"Unknown role\")\n    return completion(\n        get_buffer_string(\n            history.messages,\n            human_prefix=\"USER\",\n            ai_prefix=\"ASSISTANT\",\n        ),\n        model,\n        temperature,\n        top_p,\n    )\n\ndef assistant(content: str):\n    return { \"role\": \"assistant\", \"content\": content }\n\ndef user(content: str):\n    return { \"role\": \"user\", \"content\": content }\n\ndef complete_and_print(prompt: str, model: str = DEFAULT_MODEL):\n    print(f'==============\\n{prompt}\\n==============')    \n    response = completion(prompt, model)\n    print(response, end='\\n\\n')\n```\n\n----------------------------------------\n\nTITLE: Updating Working Dataset in Python\nDESCRIPTION: Assigns the filtered dataset without kids' clothing to the main dataframe variable. This updates the working dataset for all subsequent operations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf = df_cleaned\ndf\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Llama Cookbook\nDESCRIPTION: A requirements file listing all necessary Python packages for the Llama Cookbook project. Includes Google authentication packages, API clients, utility libraries for time zones, HTML parsing with BeautifulSoup, Ollama integration, and PDF processing with pypdf.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngoogle-auth==2.27.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-auth-httplib2==0.1.0\ngoogle-api-python-client==2.34.0\npytz\nbeautifulsoup4\nollama\npypdf\n```\n\n----------------------------------------\n\nTITLE: Setting Up Query Engines\nDESCRIPTION: Creating query engines for both summary and vector-based querying.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsummary_query_engine = summary_index.as_query_engine(\n    response_mode=\"tree_summarize\",\n    use_async=True,\n)\nvector_query_engine = vector_index.as_query_engine()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installation of necessary Python packages for working with Llama3, Ollama, Replicate, and related tools.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n!pip install llama-index-llms-ollama\n!pip install llama-index-llms-replicate\n!pip install llama-index-embeddings-huggingface\n!pip install llama-parse\n!pip install replicate\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for LLM and Document Processing\nDESCRIPTION: This snippet lists the required Python packages for a project involving language models, document handling, and text generation. It includes Gradio for building interfaces, PyPDF for PDF processing, LangChain for LLM operations, sentence-transformers for text embedding, FAISS for efficient similarity search, and a text generation library.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ngradio\npypdf\nlangchain\nsentence-transformers\nfaiss-cpu\ntext-generation\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Meta Llama 3 Chatbot with RAG\nDESCRIPTION: Installs required packages for building a Meta Llama 3 chatbot with RAG capabilities using pip and a requirements file.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Example Function Call JSON for Email Query\nDESCRIPTION: Sample JSON function call specification that Llama would generate when a user asks about large attachments. This represents how natural language is converted to a structured query.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"name\": \"list_emails\", \"parameters\": {\"query\": \"has:attachment larger:5mb\"}}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Conversation and Calling Text Model (Python)\nDESCRIPTION: Demonstrates how to set up a conversation for the text-only model and call the llama_guard_text_test function to generate a response. It includes an example conversation about a mayonnaise recipe.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/llama_guard/llama_guard_text_and_vision_inference.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\", \n                \"text\": \"What is the recipe for mayonnaise?\"\n            },\n        ],\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": [\n            {\"type\": \"text\", \n             \"text\": \"Ingredients: \\n\" +\n                        \"2 large egg yolks \\n\" +\n                        \"1 tablespoon lemon juice or vinegar \\n\" +\n                        \"1/2 teaspoon salt \\n\" +\n                        \"1/4 teaspoon ground black pepper \\n\" +\n                        \"1 cup (240 ml) neutral-tasting oil \\n\" +\n                        \"Instructions: \\n\" +\n                        \"Whisk egg yolks, lemon juice, salt, and pepper. \\n\" +\n                        \"Slowly pour in oil while whisking until thick and creamy. \\n\" +\n                        \"Refrigerate for 30 minutes before serving.\", \n            },\n        ],\n    },\n]\n\ndecoded_input_prompt, response = llama_guard_text_test(lg_small_text_tokenizer, lg_small_text_model, conversation)\nprint(decoded_input_prompt)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Using Instruction Tags with Llama 2\nDESCRIPTION: Example showing how to use [INST][/INST] tags for consistent responses in Llama 2. These tags help maintain context over longer conversations and prevent the model from including role markers in its responses.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"\"\"[INST]Remember that the number of clients is 413\"\n            \"and the number of services is 22.[/INST] What is\"\n            \"the number of services?\"\"\"\n\ncomplete_and_print(prompt)\n```\n\n----------------------------------------\n\nTITLE: Displaying Type Value Counts\nDESCRIPTION: Shows the distribution of values in the 'Type' column of the cleaned dataset.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\nType Counts:\")\nprint(result['Type'].value_counts())\n```\n\n----------------------------------------\n\nTITLE: Setting up Ollama LLM Integration\nDESCRIPTION: Configuration of Ollama LLM with the Llama3 model and timeout settings.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3\", request_timeout=120.0)\n```\n\n----------------------------------------\n\nTITLE: Generating Fashion Captions with Llama 3.2 Vision\nDESCRIPTION: Code snippet showing the process of sending an image and the prompt to the Llama 3.2 Vision model and generating a structured response. It handles the application of chat template, tokenization, and generation of fashion description.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nconversation = [\n        {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": USER_TEXT}]}\n    ]\nprompt = processor.apply_chat_template(conversation, add_special_tokens=False, add_generation_prompt=True, tokenize=False)\ninputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**inputs, temperature=1, top_p=0.9, max_new_tokens=512)\nprocessor.decode(output[0])[len(prompt):]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for Llama 3 Messenger Chatbot\nDESCRIPTION: Commands to create a conda environment and install necessary Python packages for the Llama 3 enabled Messenger chatbot.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/messenger_chatbot/messenger_llama3.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n messenger-llama python=3.8\nconda activate messenger-llama\npip install langchain replicate flask requests uvicorn gunicorn\n```\n\n----------------------------------------\n\nTITLE: FSDP with QLORA Multi-GPU Training Command\nDESCRIPTION: Command for running fine-tuning with FSDP and QLORA quantization on 4 H100 GPUs. Uses int4 quantization and PEFT with LoRA method.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/multigpu_finetuning.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nFSDP_CPU_RAM_EFFICIENT_LOADING=1 ACCELERATE_USE_FSDP=1 torchrun --nnodes 1 --nproc_per_node 4  finetuning.py --enable_fsdp  --quantization int4 --model_name /path_of_model_folder/70B  --mixed_precision False --low_cpu_fsdp --use_peft --peft_method lora --output_dir Path/to/save/PEFT/model\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Functions\nDESCRIPTION: Implements various tool functions including web search, text-to-image, image-to-text, and text-to-speech conversion.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_tool_calling_agent.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport replicate\n\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import ToolNode\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\n@tool\ndef magic_function(input: int) -> int:\n    \"\"\"Applies a magic function to an input.\"\"\"\n    return input + 2\n\n@tool\ndef web_search(input: str) -> str:\n    \"\"\"Runs web search.\"\"\"\n    web_search_tool = TavilySearchResults()\n    docs = web_search_tool.invoke({\"query\": input})\n    return docs\n\n@tool\ndef text2image(text: str) -> str:\n    \"\"\"generate an image based on a text.\"\"\"\n    output = replicate.run(\n        \"stability-ai/sdxl:7762fd07cf82c948538e41f63f77d685e02b063e37e496e96eefd46c929f9bdc\",\n        input={\n            \"width\": 1024,\n            \"height\": 1024,\n            \"prompt\": text,\n            \"scheduler\": \"KarrasDPM\",\n            \"num_outputs\": 1,\n            \"guidance_scale\": 7.5,\n            \"apply_watermark\": True,\n            \"negative_prompt\": \"worst quality, low quality\",\n            \"prompt_strength\": 0.8,\n            \"num_inference_steps\": 60\n        }\n    )\n    print(output)\n    return output[0]\n\n@tool\ndef image2text(image_url: str, prompt: str) -> str:\n    \"\"\"generate text for image_url based on prompt.\"\"\"\n    input = {\n        \"image\": image_url,\n        \"prompt\": prompt\n    }\n\n    output = replicate.run(\n        \"yorickvp/llava-13b:b5f6212d032508382d61ff00469ddda3e32fd8a0e75dc39d8a4191bb742157fb\",\n        input=input\n    )\n\n    return \"\".join(output)\n\n@tool\ndef text2speech(text: str) -> int:\n    \"\"\"convert text to a speech.\"\"\"\n    output = replicate.run(\n        \"cjwbw/seamless_communication:668a4fec05a887143e5fe8d45df25ec4c794dd43169b9a11562309b2d45873b0\",\n        input={\n            \"task_name\": \"T2ST (Text to Speech translation)\",\n            \"input_text\": text,\n            \"input_text_language\": \"English\",\n            \"max_input_audio_length\": 60,\n            \"target_language_text_only\": \"English\",\n            \"target_language_with_speech\": \"English\"\n        }\n    )\n    return output['audio_output']\n```\n\n----------------------------------------\n\nTITLE: Email Agent Greeting Example\nDESCRIPTION: Sample initial greeting from the email agent after successful authentication. It shows the agent's available capabilities to the user.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nHello! I'm Email Agent, here to help you manage your email account with ease.\n\nWhat would you like to do today? Do you want me to:\n\nCheck and respond to new emails\nCompose a new email\nOrganize your inbox with filters or labels\nDelete unwanted emails\nSomething else?\n\nLet me know how I can assist you!\n\nYour ask:\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment for Llama Evaluation\nDESCRIPTION: Commands to clone the llama-cookbook repository, install necessary dependencies including lm-evaluation-harness library, and navigate to the meta_eval directory.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/meta_eval/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:meta-llama/llama-cookbook.git\ncd llama-cookbook\npip install -U pip setuptools\npip install -e .\npip install -U antlr4_python3_runtime==4.11\npip install lm-eval[math,ifeval,sentencepiece,vllm]==0.4.3\ncd end-to-end-use-cases/benchmarks/llm_eval_harness/meta_eval\n```\n\n----------------------------------------\n\nTITLE: Controlling Creativity with Temperature and Top_P Parameters\nDESCRIPTION: Function that demonstrates how temperature and top_p hyperparameters affect output variability. Temperature controls randomness while top_p limits the breadth of vocabulary, with examples of both deterministic and creative settings.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef print_tuned_completion(temperature: float, top_p: float):\n    response = completion(\"Tell me a 25 word story about llamas in space\", temperature=temperature, top_p=top_p)\n    print(f'[temperature: {temperature} | top_p: {top_p}]\\n{response.strip()}\\n')\n\nprint_tuned_completion(0.01, 0.01)\nprint_tuned_completion(0.01, 0.01)\nprint_tuned_completion(0.01, 0.01)\nprint_tuned_completion(0.01, 0.01)\n# These two generations are highly likely to be the same\n\nprint_tuned_completion(1.0, 0.5)\nprint_tuned_completion(1.0, 0.5)\nprint_tuned_completion(1.0, 0.5)\nprint_tuned_completion(1.0, 0.5)\n# These two generations are highly likely to be different\n```\n\n----------------------------------------\n\nTITLE: End of Script Marker\nDESCRIPTION: A simple comment indicating the end of the script implementation for fashion image captioning.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n#fin\n```\n\n----------------------------------------\n\nTITLE: Setting up Embedding Model\nDESCRIPTION: Configuration of HuggingFace embedding model for text embeddings.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nembed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Query\nDESCRIPTION: Runs the generated SQL query against the database\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/quickstart.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# note this is a dangerous operation and for demo purpose only; in production app you'll need to safe-guard any DB operation\nresult = db.run(answer)\nresult\n```\n\n----------------------------------------\n\nTITLE: Configuring System Prompt for Text-to-SQL\nDESCRIPTION: Defines the system prompt that provides database schema, query context, and output format instructions for the LLM.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-function-calling-for-sql/json-mode-function-calling-for-sql.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsystem_prompt = '''\nYou are Groq Advisor, and you are tasked with generating SQL queries for DuckDB based on user questions about data stored in two tables derived from CSV files:\n\nTable: employees.csv\nColumns:\nemployee_id (INTEGER): A unique identifier for each employee.\nname (VARCHAR): The full name of the employee.\nemail (VARCHAR): employee's email address\n\nTable: purchases.csv\nColumns:\npurchase_id (INTEGER): A unique identifier for each purchase.\npurchase_date (DATE): Date of purchase\nemployee_id (INTEGER): References the employee_id from the employees table, indicating which employee made the purchase.\namount (FLOAT): The monetary value of the purchase.\nproduct_name (STRING): The name of the product purchased\n\nGiven a user's question about this data, write a valid DuckDB SQL query that accurately extracts or calculates the requested information from these tables and adheres to SQL best practices for DuckDB, optimizing for readability and performance where applicable.\n\nHere are some tips for writing DuckDB queries:\n* DuckDB syntax requires querying from the .csv file itself, i.e. employees.csv and purchases.csv. For example: SELECT * FROM employees.csv as employees\n* All tables referenced MUST be aliased\n* DuckDB does not implicitly include a GROUP BY clause\n* CURRENT_DATE gets today's date\n* Aggregated fields like COUNT(*) must be appropriately named\n\nAnd some rules for querying the dataset:\n* Never include employee_id in the output - show employee name instead\n\nAlso note that:\n* Valid values for product_name include \\'Tesla\\',\\'iPhone\\' and \\'Humane pin\\'\n\n\nQuestion:\n--------\n{user_question}\n--------\nReminder: Generate a DuckDB SQL to answer to the question:\n* respond as a valid JSON Document\n* [Best] If the question can be answered with the available tables: {\"sql\": <sql here>}\n* If the question cannot be answered with the available tables: {\"error\": <explanation here>}\n* Ensure that the entire output is returned on only one single line\n* Keep your query as simple and straightforward as possible; do not use subqueries\n'''\n```\n\n----------------------------------------\n\nTITLE: Extracting Filenames from Corrupt Image Paths in Python\nDESCRIPTION: Processes the list of corrupt image paths to extract just the filenames without extensions. These filenames will be used to filter the corresponding rows from the metadata dataframe.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncorrupt_filenames = [os.path.splitext(os.path.basename(path))[0] for path in corrupt_images]\n\n# Print out the corrupt filenames for verification\nprint(\"Corrupt filenames:\")\nprint(corrupt_filenames)\n```\n\n----------------------------------------\n\nTITLE: Role Prompting with Llama 2\nDESCRIPTION: Example of role prompting that gives the model a specific persona to adopt. This technique results in more focused and technical responses appropriate to the assigned role.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncomplete_and_print(\"Explain the pros and cons of using PyTorch.\")\n# More likely to explain the pros and cons of PyTorch covers general areas like documentation, the PyTorch community, and mentions a steep learning curve\n\ncomplete_and_print(\"Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\")\n# Often results in more technical benefits and drawbacks that provide more technical details on how model layers\n```\n\n----------------------------------------\n\nTITLE: Basic Chat Completion Payload Structure\nDESCRIPTION: JSON structure for the chat completion API request payload showing system and user message format.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \n  \"messages\": [ \n    { \n      \"content\": \"You are a helpful assistant.\", \n      \"role\": \"system\" \n},  \n    { \n      \"content\": \"Hello!\", \n      \"role\": \"user\" \n    } \n  ], \n  \"max_tokens\": 50, \n } \n```\n\n----------------------------------------\n\nTITLE: Exporting Generated Podcast to MP3 File\nDESCRIPTION: Saving the generated podcast audio as an MP3 file with specific quality settings. The export process uses a high bitrate (192k) for better audio quality.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfinal_audio.export(\"./resources/_podcast.mp3\", \n                  format=\"mp3\", \n                  bitrate=\"192k\",\n                  parameters=[\"-q:a\", \"0\"])\n```\n\n----------------------------------------\n\nTITLE: Self-Consistency Technique with Llama 2\nDESCRIPTION: Implementation of the Self-Consistency technique which enhances accuracy by selecting the most frequent answer from multiple generations. This approach helps overcome the probabilistic nature of LLMs at the cost of higher computation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom statistics import mode\n\ndef gen_answer():\n    response = completion(\n        \"John found that the average of 15 numbers is 40.\"\n        \"If 10 is added to each number then the mean of the numbers is?\"\n        \"Report the answer surrounded by three backticks, for example: ```123```\",\n        model = LLAMA2_70B_CHAT\n    )\n    match = re.search(r'```(\\d+)```', response)\n    if match is None:\n        return None\n    return match.group(1)\n\nanswers = [gen_answer() for i in range(5)]\n\nprint(\n    f\"Answers: {answers}\\n\",\n    f\"Final answer: {mode(answers)}\",\n    )\n\n# Sample runs of Llama-2-70B (all correct):\n# [50, 50, 750, 50, 50]  -> 50\n# [130, 10, 750, 50, 50] -> 50\n# [50, None, 10, 50, 50] -> 50\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Header Preview\nDESCRIPTION: Simple code to display the first few rows of a DataFrame called 'result' using the head() method, which is useful for quickly examining the structure and content of a DataFrame.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nresult.head()\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: Specifies required Python packages including visualization libraries (Streamlit, Plotly), data processing tools (Pandas, NumPy), LangChain components, and Jupyter-related packages. One version constraint is specified for nbformat.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/groqing-the-stock-market-function-calling-llama3/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nstreamlit\npandas\nnumpy\ngroq\nlangchain_community\nlangchain_groq\nyfinance\nplotly\nlangchain_core\nnbformat>=4.2.0\nipython\nkaleido\n```\n\n----------------------------------------\n\nTITLE: Examining Completed Steps and Source Output\nDESCRIPTION: Retrieves and displays information about completed steps in the reasoning process, including the raw output from the first source.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ncompleted_steps = agent.get_completed_steps(task.task_id)\nprint(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\nprint(completed_steps[0].output.sources[0].raw_output)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for PDF to Podcast Conversion\nDESCRIPTION: Imports required libraries for handling PDFs, managing API clients, and defining data structures. Includes standard libraries and third-party packages for Together.ai, Cartesia TTS, and Pydantic for schema validation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Standard library imports\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile\nfrom typing import List, Literal, Tuple, Optional\n\n# Third-party imports\nfrom pydantic import BaseModel\nfrom pypdf import PdfReader\n\nfrom together import Together\nfrom cartesia import Cartesia\nfrom pydantic import ValidationError\n```\n\n----------------------------------------\n\nTITLE: Setting Up LangChain API for Tracing\nDESCRIPTION: Configures environment variables for LangChain API to enable tracing and logging of summarization processes.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/video_summary.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"LANGCHAIN_API_KEY\"] = \"your_langchain_api_key\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"Video Summary with Llama 3\"\n```\n\n----------------------------------------\n\nTITLE: Executing Sample Query\nDESCRIPTION: Demonstrates usage of the text-to-SQL system with a sample question about recent purchases.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-function-calling-for-sql/json-mode-function-calling-for-sql.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nuser_question = 'What are the most recent purchases?'\n\n\nllm_response = text_to_sql(client,system_prompt,user_question,model)\nsql_json = json.loads(llm_response)\nparsed_sql = sqlparse.format(sql_json['sql'], reindent=True, keyword_case='upper')\nformatted_sql = f\"```sql\\n{parsed_sql.strip()}\\n```\"\ndisplay(Markdown(formatted_sql)) \n\nexecute_duckdb_query(parsed_sql)\n```\n\n----------------------------------------\n\nTITLE: Content Safety Filter Example\nDESCRIPTION: Demonstration of content safety filtering through a CURL request with potentially harmful content.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!curl -X POST -L https://your-endpoint.inference.ai.azure.com/v1/chat/completions -H 'Content-Type: application/json' -H 'Authorization: your-auth-key' -d '{\"messages\":[{\"content\":\"You are a helpful assistant.\",\"role\":\"system\"},{\"content\":\"How to make bomb?\",\"role\":\"user\"}], \"max_tokens\": 50}'\n```\n\n----------------------------------------\n\nTITLE: Custom Categories for Llama Guard (Python)\nDESCRIPTION: Shows how to create custom categories and exclude specific categories when calling the Llama Guard model. It includes an example of setting up custom categories and removing one of them.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/llama_guard/llama_guard_text_and_vision_inference.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Setting up custom categories\n\ncategories = {\n    \"S1\": \"Custom category 1. \\n\" +\n            \"AI models should not talk about custom category 1\",\n    \"S2\": \"This will be removed\"\n}\n\n# Removing a single category\nexcluded_category_keys = [\"S2\"]\n\n# Relevant conversation\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\", \n                \"text\": \"What is the color of the sky?\"\n            },\n        ],\n    },\n]\n\ndecoded_input_prompt, response = llama_guard_text_test(lg_small_text_tokenizer, lg_small_text_model, conversation, categories, excluded_category_keys)\nprint(decoded_input_prompt)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Limiting Extraneous Tokens in LLM Output using Python\nDESCRIPTION: Demonstrates techniques to limit extraneous tokens in LLM output by using a combination of role definition, rules, restrictions, explicit instructions, and examples. This approach aims to get more precise and formatted responses.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ncomplete_and_print(\n    \"Give me the zip code for Menlo Park in JSON format with the field 'zip_code'\",\n    model = LLAMA2_70B_CHAT,\n)\n# Likely returns the JSON and also \"Sure! Here's the JSON...\"\n\ncomplete_and_print(\n    \"\"\"\n    You are a robot that only outputs JSON.\n    You reply in JSON format with the field 'zip_code'.\n    Example question: What is the zip code of the Empire State Building? Example answer: {'zip_code': 10118}\n    Now here is my question: What is the zip code of Menlo Park?\n    \"\"\",\n    model = LLAMA2_70B_CHAT,\n)\n# \"{'zip_code': 94025}\"\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark Client\nDESCRIPTION: Commands to start benchmark client in a new tmux session\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/crusoe/vllm-fp8/README.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntmux new -s client\nchmod +x run_benchmark.sh\n./run_benchmark.sh\n```\n\n----------------------------------------\n\nTITLE: CodeShield External LLM Integration\nDESCRIPTION: Example of scanning code generated by external LLM providers using CodeShield\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/code_shield_usage_demo.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport getpass\n\nfrom llama_cookbook.inference.llm import TOGETHER, OPENAI, ANYSCALE\n\nif \"EXTERNALLY_HOSTED_LLM_TOKEN\" not in os.environ:\n    os.environ[\"EXTERNALLY_HOSTED_LLM_TOKEN\"] = getpass.getpass(prompt=\"Provide token for LLM provider\")\n\n# Delete as appropriate\nmodel = TOGETHER(\"togethercomputer/CodeLlama-13b-Instruct\", os.environ[\"EXTERNALLY_HOSTED_LLM_TOKEN\"])\nmodel = OPENAI(\"gpt-4\",os.environ[\"EXTERNALLY_HOSTED_LLM_TOKEN\"])\nmodel = ANYSCALE(\"codellama/CodeLlama-34b-Instruct-hf\",os.environ[\"EXTERNALLY_HOSTED_LLM_TOKEN\"])\n\nllm_output_code = model.query_with_system_prompt_with_retries(\n    system_prompt= \"You are an expert code developer. You output only code and nothing else\", \n    prompt=  \"Output a single python function which calculates the md5 hash of a string provided as an argument to the function. Output only the code and nothing else.\"\n    )\nawait scan_llm_output(llm_output_code)\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Hindi Tokenizer Training\nDESCRIPTION: Script command to prepare training data by sampling Hindi documents from the Varta dataset's validation split. This creates a text file with 10,000 Hindi documents to be used for tokenizer training.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/multilingual/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython prepare_data.py --split=validation --lang=hi --docs_to_sample=10000 --save_path=./data\n```\n\n----------------------------------------\n\nTITLE: Installing Modal CLI\nDESCRIPTION: Command to install the Modal CLI package for running cloud-based container applications.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/modal/many-llamas-human-eval/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install modal\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Email Agent\nDESCRIPTION: These commands create and activate a Conda environment named 'emailagent' with Python 3.10 for the email agent project.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n emailagent python=3.10\nconda activate emailagent\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies\nDESCRIPTION: List of required Python packages for a machine learning project including data processing (pandas, numpy), LLM tools (langchain), and machine learning libraries (transformers, scikit-learn, sentence-transformers).\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/presidential-speeches-rag-with-pinecone/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npandas\nnumpy\ngroq\nlangchain_community\nlangchain_pinecone\ntransformers\nscikit-learn\nsentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Installing Gradio Dependencies\nDESCRIPTION: Installs the specific version of Gradio required for the chatbot interface.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npip install gradio==4.39.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama Guard Prompt Format in Python\nDESCRIPTION: This snippet demonstrates how to configure the prompt format for Llama Guard using the LlamaGuardPromptConfigs class. It specifies the instructions format string and options for including category descriptions and shuffling category codes.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/llama_cookbook/data/llama_guard/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nllama_guard_prompt_configs = LlamaGuardPromptConfigs(\n    instructions_format_string=\"\"\"Task: Using the following guidelines, decide whether the example conversation that comes after is safe or unsafe.\n\n<BEGIN UNSAFE CONTENT CATEGORIES>\n{guidelines}\n<END UNSAFE CONTENT CATEGORIES>\n\n<BEGIN CONVERSATION>\n\n{conversation}\n\n<END CONVERSATION>\n\nProvide your safety assessment for $agent_type in the above conversation:\n- First line must read 'safe' or 'unsafe'.\n- If unsafe, a second line must include a comma-separated list of violated categories.  \"\"\",\n    should_include_category_descriptions=True,\n    should_shuffle_category_codes=True\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Import statements for required Python modules and LangChain components.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/react_llama_3_bedrock_wk.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport boto3\nimport pandas as pd\n\nfrom langchain.agents import Tool\nfrom langchain.llms.bedrock import Bedrock\nfrom langchain.tools import DuckDuckGoSearchRun\nfrom langchain.utilities import WikipediaAPIWrapper\nfrom langchain_experimental.utilities import PythonREPL\n```\n\n----------------------------------------\n\nTITLE: Mounting Storage Configuration for L40S\nDESCRIPTION: Commands to format and mount an attached disk to /scratch directory on the VM\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/crusoe/vllm-fp8/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkfs.ext4 /dev/vdb\nmkdir /scratch\nmount -t ext4 /dev/vdb /scratch\ncd /scratch\n```\n\n----------------------------------------\n\nTITLE: Splitting Text into Chunks using Tiktoken in Python\nDESCRIPTION: This function splits the document content into chunks of 300-1000 tokens using the tiktoken library. It then formats the chunked content with chunk identifiers. The chunk size is set to 400 tokens in this example.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Contextual-Chunking-RAG/Tutorial.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#split into chunks (simple way)\ndef split_into_chunks(content, chunk_size):\n\timport tiktoken\n\tenc = tiktoken.get_encoding(\"o200k_base\")\n\ta = enc.encode(content)\n\tleft, chunks = 0, []\n\twhile left < len(a):\n\t\tarr = a[left : left+chunk_size]\n\t\tchunks.append(enc.decode(arr))\n\t\tleft+=chunk_size\n\treturn chunks\n\nchunks = split_into_chunks(document_content, 400)\n\n#generate chunked content\nchunked_content = \"\"\nfor idx, text in enumerate(chunks):\n  chunked_content+=f\"### Chunk {idx+1} ###\\n{text}\\n\\n\"\n```\n\n----------------------------------------\n\nTITLE: Gmail OAuth Authorization Process\nDESCRIPTION: Example of the OAuth authorization prompt displayed when first running the agent. It provides a URL for the user to authenticate with Google.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=xxxx\nEnter the authorization code: \n```\n\n----------------------------------------\n\nTITLE: Defining Langchain Tools for Order Processing\nDESCRIPTION: Defines three Langchain tools using the @tool decorator: create_order, get_product_price, and get_product_id. Each function includes a description string that helps the LLM understand when and how to use the tool.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\n\n@tool\ndef create_order(product_id, customer_id):\n    \"\"\"\n    Creates an order given a product_id and customer_id.\n    If a product name is provided, you must get the product ID first.\n    After placing the order indicate that it was placed successfully and output the details.\n\n    product_id: ID of the product\n    customer_id: ID of the customer\n    \"\"\"\n    api_token = os.environ[\"AIRTABLE_API_TOKEN\"]\n    base_id = os.environ[\"AIRTABLE_BASE_ID\"]\n    headers = {\n        \"Authorization\": f\"Bearer {api_token}\",\n        \"Content-Type\": \"application/json\",\n    }\n    url = f\"https://api.airtable.com/v0/{base_id}/orders\"\n    order_id = random.randint(1, 100000)  # Randomly assign an order_id\n    order_datetime = datetime.utcnow().strftime(\n        \"%Y-%m-%dT%H:%M:%SZ\"\n    )  # Assign order date as now\n    data = {\n        \"fields\": {\n            \"order_id\": order_id,\n            \"product_id\": product_id,\n            \"customer_id\": customer_id,\n            \"order_date\": order_datetime,\n        }\n    }\n    response = requests.post(url, headers=headers, json=data)\n    return str(response.json())\n\n\n@tool\ndef get_product_price(product_name):\n    \"\"\"\n    Gets the price for a product, given the name of the product.\n    Just return the price, do not do any calculations.\n\n    product_name: The name of the product (must be title case, i.e. 'Microphone', 'Laptop')\n    \"\"\"\n    api_token = os.environ[\"AIRTABLE_API_TOKEN\"]\n    base_id = os.environ[\"AIRTABLE_BASE_ID\"]\n    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n    formula = f\"{{name}}='{product_name}'\"\n    encoded_formula = urllib.parse.quote(formula)\n    url = f\"https://api.airtable.com/v0/{base_id}/products?filterByFormula={encoded_formula}\"\n    response = requests.get(url, headers=headers)\n    product_price = response.json()[\"records\"][0][\"fields\"][\"price\"]\n    return \"$\" + str(product_price)\n\n\n@tool\ndef get_product_id(product_name):\n    \"\"\"\n    Gets product ID given a product name\n\n    product_name: The name of the product (must be title case, i.e. 'Microphone', 'Laptop')\n    \"\"\"\n    api_token = os.environ[\"AIRTABLE_API_TOKEN\"]\n    base_id = os.environ[\"AIRTABLE_BASE_ID\"]\n    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n    formula = f\"{{name}}='{product_name}'\"\n    encoded_formula = urllib.parse.quote(formula)\n    url = f\"https://api.airtable.com/v0/{base_id}/products?filterByFormula={encoded_formula}\"\n    response = requests.get(url, headers=headers)\n    product_id = response.json()[\"records\"][0][\"fields\"][\"product_id\"]\n    return str(product_id)\n\n\n# Add tools to our LLM\ntools = [create_order, get_product_price, get_product_id]\nllm_with_tools = llm.bind_tools(tools)\n```\n\n----------------------------------------\n\nTITLE: Installing Together Library in Python\nDESCRIPTION: This code snippet installs the 'together' library using pip, which is required for interacting with the Together AI API.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/structured_text_extraction_from_images.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install together\n```\n\n----------------------------------------\n\nTITLE: Loading and Converting Image for Model Input in Python\nDESCRIPTION: Defines a function to load an image from file and convert it to RGB format for model processing. Then applies this function to load the selected test image.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef get_image(image_path):\n    with open(image_path, \"rb\") as f:\n        return PIL_Image.open(f).convert(\"RGB\")\n\nimage = get_image(image_path)\n```\n\n----------------------------------------\n\nTITLE: Tool Configuration Setup\nDESCRIPTION: Configuration of LangChain tools including DuckDuckGo search, Wikipedia, and Python REPL.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/react_llama_3_bedrock_wk.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nduckduckgo_search_run = DuckDuckGoSearchRun()\nduckduckgo_tool = Tool(\n    name=\"duckduckgo_tool\",\n    func=duckduckgo_search_run.run,\n    description=\"Useful for when you need to search online about facts and events or retrieve news.\"\n)\n\nwikipedia = WikipediaAPIWrapper()\nwikipedia_tool = Tool(\n    name=\"wikipedia_tool\",\n    func=wikipedia.run,\n    description=\"Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\",\n)\n\npython_repl = PythonREPL()\nrepl_tool = Tool(\n    name=\"repl_tool\",\n    description=\"A Python shell. Use this to execute python commands or to calculate math expressions. Input should be a valid python command.\",\n    func=python_repl.run,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Azure AI Content Safety Dependencies\nDESCRIPTION: Command to install necessary Python packages for using Azure AI content safety checks in the benchmark process.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/on_prem/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install azure-ai-contentsafety azure-core\n```\n\n----------------------------------------\n\nTITLE: Installing Groq API Client\nDESCRIPTION: Installs the Groq Python client library for accessing Llama 3 models.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install groq\n```\n\n----------------------------------------\n\nTITLE: Running H2O-Enabled Summarization with Llama Models\nDESCRIPTION: This script runs a summarization task using the H2O algorithm with Llama models. It enables H2O generation that only keeps heavy-hitter and local KV pairs, optimizing memory usage while maintaining performance. Position rolling can be enabled to handle sequences longer than the pretrained context window.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/long_context/H2O/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython run_summarization.py \\\n--input-path data/summarization/xsum.jsonl \\\n--output-path summarization_output/xsum_h2o.jsonl \\\n--model-name meta-llama/Meta-Llama-3-8B \\\n--enable_h2o_generation\n```\n\n----------------------------------------\n\nTITLE: Organizing Extracted Information into JSON using Llama LLM in Python\nDESCRIPTION: This code uses the Llama 3.1 70B model with JSON mode to organize the extracted information into a structured JSON format based on the predefined Receipt schema.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/structured_text_extraction_from_images.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nextract = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"The following is a detailed description of all the items, prices and quantities on a receipt. Extract out information. Only answer in JSON.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": info,\n            },\n        ],\n        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n        response_format={\n            \"type\": \"json_object\",\n            \"schema\": Receipt.model_json_schema(),\n        },\n    )\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Llama Tool Calling\nDESCRIPTION: This snippet shows the pip commands to install necessary libraries for the project, including groq, arxiv, tavily-python, llama-toolchain, and PyPDF2.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/Agents_Tutorial/Tool_Calling_201.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#!pip3 install groq\n#!pip3 install arxiv\n#!pip3 install tavily-python\n#!pip3 install llama-toolchain\n#!pip3 install PyPDF2\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Simple pip command to install the LangChain framework.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npip install langchain\n```\n\n----------------------------------------\n\nTITLE: Implementing Conversation Loop with Langchain Tool Calls\nDESCRIPTION: Implements a conversation loop using Langchain that processes tool calls and their responses. It maintains the context through a message history and executes the appropriate tools as requested by the LLM until no more tool calls are needed.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n\navailable_tools = {\n    \"create_order\": create_order,\n    \"get_product_price\": get_product_price,\n    \"get_product_id\": get_product_id,\n}\nmessages = [SystemMessage(SYSTEM_MESSAGE), HumanMessage(user_prompt)]\ntool_call_identified = True\nwhile tool_call_identified:\n    ai_msg = llm_with_tools.invoke(messages)\n    messages.append(ai_msg)\n    for tool_call in ai_msg.tool_calls:\n        selected_tool = available_tools[tool_call[\"name\"]]\n        tool_output = selected_tool.invoke(tool_call[\"args\"])\n        messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n    if len(ai_msg.tool_calls) == 0:\n        tool_call_identified = False\n\nprint(ai_msg.content)\n```\n\n----------------------------------------\n\nTITLE: Enabling W&B Experiment Tracking in Bash\nDESCRIPTION: This command demonstrates how to enable Weights & Biases experiment tracking when running the finetuning script, along with other configuration options like PEFT method and quantization.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m llama_cookbook.finetuning --use_peft --peft_method lora --quantization 8bit --model_name /path_of_model_folder/8B --output_dir Path/to/save/PEFT/model --use_wandb\n```\n\n----------------------------------------\n\nTITLE: Printing Structured JSON Output in Python\nDESCRIPTION: This code parses the JSON response from the LLM and prints it in a formatted, indented structure for better readability.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/structured_text_extraction_from_images.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\noutput = json.loads(extract.choices[0].message.content)\nprint(json.dumps(output, indent=2))\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Social Determinants of Health Extraction\nDESCRIPTION: This snippet imports necessary Python libraries for working with the Groq API, data manipulation, file handling, and visualization.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-social-determinants-of-health/SDOH-Json-mode.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Import packages\nfrom groq import Groq\nimport pandas as pd\nimport os\nfrom IPython.display import Markdown\nimport json\nfrom google.cloud import bigquery\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Basic LLM Inference\nDESCRIPTION: Demonstrates how to make a simple inference call to the LLM using HumanMessage.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresponse = llm.invoke([HumanMessage(content=\"What is good about Wuhan?\")])\nresponse\n```\n\n----------------------------------------\n\nTITLE: Detecting and Removing Corrupt Images with Parallel Processing in Python\nDESCRIPTION: Implements parallel image validation to identify corrupt files in the dataset. Uses ProcessPoolExecutor to leverage multiple CPU cores for faster processing across the 5000 images in the dataset.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef is_image_corrupt(image_path):\n    try:\n        with Image.open(image_path) as img:\n            img.verify()\n        return False\n    except (IOError, SyntaxError, Image.UnidentifiedImageError):\n        return True\n\ndef find_corrupt_images(folder_path):\n    image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) \n                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    \n    num_cores = multiprocessing.cpu_count()\n    with ProcessPoolExecutor(max_workers=num_cores) as executor:\n        results = executor.map(is_image_corrupt, image_files)\n    \n    corrupt_images = [img for img, is_corrupt in zip(image_files, results) if is_corrupt]\n    return corrupt_images\n\n\nfolder_path = IMAGES  # Replace with your folder path\ncorrupt_images = find_corrupt_images(folder_path)\n\nprint(\"Corrupt images:\")\nfor img in corrupt_images:\n    print(img)\nprint(f\"Total corrupt images found: {len(corrupt_images)}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Speech with Suno/Bark in Python\nDESCRIPTION: This code snippet shows how to use the Suno/Bark model for text-to-speech generation. It includes setting the voice preset, defining the text prompt with sound effects, and generating the speech output with specific temperature parameters.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/TTS_Notes.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nvoice_preset = \"v2/en_speaker_6\"\nsampling_rate = 24000\n\ntext_prompt = \"\"\"\nExactly! [sigh] And the distillation part is where you take a LARGE-model,and compress-it down into a smaller, more efficient model that can run on devices with limited resources.\n\"\"\"\ninputs = processor(text_prompt, voice_preset=voice_preset).to(device)\n\nspeech_output = model.generate(**inputs, temperature = 0.9, semantic_temperature = 0.8)\nAudio(speech_output[0].cpu().numpy(), rate=sampling_rate)\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample PDF for Podcast Generation\nDESCRIPTION: Downloads the Transformer paper from arXiv as a sample PDF to convert into a podcast script, using wget and renaming the file for easier reference.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# the transformer paper!\n!wget https://arxiv.org/pdf/1706.03762\n!mv 1706.03762 attention.pdf\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and API Setup\nDESCRIPTION: Importing required libraries and setting up the Together AI API key for authentication.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/knowledge_graphs_with_structured_outputs.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\nimport together\nimport json\n\n# Paste in your Together AI API Key\nTOGETHER_API_KEY = \"YOUR_TOGETHER_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Defining Arithmetic Function Tools in Python\nDESCRIPTION: This snippet defines four arithmetic functions (multiply, add, subtract, divide) and creates FunctionTools from them. These tools will be used by the agent to perform calculations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiple two integers and returns the result integer\"\"\"\n    return a * b\n\n\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers and returns the result integer\"\"\"\n    return a + b\n\n\ndef subtract(a: int, b: int) -> int:\n    \"\"\"Subtract two integers and returns the result integer\"\"\"\n    return a - b\n\n\ndef divide(a: int, b: int) -> int:\n    \"\"\"Divides two integers and returns the result integer\"\"\"\n    return a / b\n\n\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\nadd_tool = FunctionTool.from_defaults(fn=add)\nsubtract_tool = FunctionTool.from_defaults(fn=subtract)\ndivide_tool = FunctionTool.from_defaults(fn=divide)\nllm_70b.is_function_calling_model = True\n```\n\n----------------------------------------\n\nTITLE: Building a Chatbot with Memory\nDESCRIPTION: Implements a complete chatbot using Gradio's ChatInterface with conversation memory, combining LangChain's ConversationChain and ConversationBufferWindowMemory.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport gradio as gr\nimport langchain\nfrom langchain.chains import ConversationChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.memory import ConversationBufferWindowMemory\nfrom langchain_core.messages import HumanMessage\nfrom langchain_community.chat_models.azureml_endpoint import (\n    AzureMLEndpointApiType,\n    CustomOpenAIChatContentFormatter,\n    AzureMLChatOnlineEndpoint,\n)\n\nllm = AzureMLChatOnlineEndpoint(\n    endpoint_api_key=\"your-auth-key\",\n    endpoint_url=\"https://your-endpoint.inference.ai.azure.com/v1/chat/completions\",\n    endpoint_api_type=AzureMLEndpointApiType.serverless,\n    model_kwargs={\"temperature\": 0.6, \"max_tokens\": 256, \"top_p\": 0.9},\n    content_formatter=CustomOpenAIChatContentFormatter(),\n)\n\nlangchain.debug=True\n\n#Create memory\nmemory = ConversationBufferWindowMemory(llm=llm, k=5, memory_key=\"chat_history\", ai_prefix=\"Assistant\", human_prefix=\"User\")\n\n#Create input prompt template with chat history for chaining\nINPUT_TEMPLATE = \"\"\"Current conversation:\n{chat_history}\n\nUser question:{input}\"\"\"\n\nconversation_prompt_template = PromptTemplate(\n    input_variables=[\"chat_history\", \"input\"], template=INPUT_TEMPLATE\n)\n\nconversation_chain_with_memory = ConversationChain(\n    llm = llm,\n    prompt = conversation_prompt_template,\n    verbose = True,\n    memory = memory,\n)\n\n#Prediction\ndef predict(message, history):\n    history_format = []\n    for user, assistant in history:\n        history_format.append({\"role\": \"user\", \"content\": user })\n        history_format.append({\"role\": \"assistant\", \"content\":assistant})\n    history_format.append({\"role\": \"user\", \"content\": message})\n    response = conversation_chain_with_memory.run(input=message)\n    return response\n\n#Launch Gradio chatbot interface\ngr.ChatInterface(predict).launch()\n```\n\n----------------------------------------\n\nTITLE: Filtering Corrupt Images from Dataset in Python\nDESCRIPTION: Removes entries of corrupt images from the metadata dataframe. This ensures that further analysis and model training will only use valid images, preventing errors during processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf_clean = df[~df['image'].isin(corrupt_filenames)]\n# Print the number of rows removed\nprint(f\"Number of rows removed: {len(df) - len(df_clean)}\")\n\n# Display the first few rows of the cleaned DataFrame\nprint(df_clean.head())\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Llama 3 WhatsApp Chatbot in Python\nDESCRIPTION: Commands to create a conda environment and install required packages for the Llama 3 WhatsApp chatbot project.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/whatsapp_chatbot/whatsapp_llama3.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n whatsapp-llama python=3.8\nconda activate whatsapp-llama\npip install langchain replicate flask requests uvicorn gunicorn\n```\n\n----------------------------------------\n\nTITLE: Running Llama 3 WhatsApp Chatbot with Gunicorn in Python\nDESCRIPTION: Command to start the Llama 3 WhatsApp chatbot using Gunicorn web server.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/whatsapp_chatbot/whatsapp_llama3.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngunicorn -b 0.0.0.0:5000 llama_chatbot:app\n```\n\n----------------------------------------\n\nTITLE: Displaying Function Response\nDESCRIPTION: Viewing the response returned by the executed function, which contains the weather information.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfunction_response\n```\n\n----------------------------------------\n\nTITLE: Implementing Document Query Tools Function\nDESCRIPTION: Creates a function that generates two query tools from a document: a vector search tool for specific questions and a summary tool for holistic understanding. The function handles document loading, splitting into nodes, and index creation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.tools import FunctionTool, QueryEngineTool\nfrom llama_index.core.vector_stores import MetadataFilters, FilterCondition\nfrom typing import List, Optional\n\ndef get_doc_tools(\n    file_path: str,\n    name: str,\n) -> str:\n    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n\n    # load documents\n    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n    splitter = SentenceSplitter(chunk_size=1024)\n    nodes = splitter.get_nodes_from_documents(documents)\n    vector_index = VectorStoreIndex(nodes)\n\n    def vector_query(\n        query: str,\n        page_numbers: Optional[List[str]] = None\n    ) -> str:\n        \"\"\"Use to answer questions over the MetaGPT paper.\n\n        Useful if you have specific questions over the MetaGPT paper.\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n\n        Args:\n            query (str): the string query to be embedded.\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE\n                if we want to perform a vector search\n                over all pages. Otherwise, filter by the set of specified pages.\n\n        \"\"\"\n\n        page_numbers = page_numbers or []\n        metadata_dicts = [\n            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n        ]\n\n        query_engine = vector_index.as_query_engine(\n            similarity_top_k=2,\n            filters=MetadataFilters.from_dicts(\n                metadata_dicts,\n                condition=FilterCondition.OR\n            )\n        )\n        response = query_engine.query(query)\n        return response\n\n\n    vector_query_tool = FunctionTool.from_defaults(\n        name=f\"vector_tool_{name}\",\n        fn=vector_query\n    )\n\n    summary_index = SummaryIndex(nodes)\n    summary_query_engine = summary_index.as_query_engine(\n        response_mode=\"tree_summarize\",\n        use_async=True,\n    )\n    summary_tool = QueryEngineTool.from_defaults(\n        name=f\"summary_tool_{name}\",\n        query_engine=summary_query_engine,\n        description=(\n            \"Use ONLY IF you want to get a holistic summary of MetaGPT. \"\n            \"Do NOT use if you have specific questions over MetaGPT.\"\n        ),\n    )\n\n    return vector_query_tool, summary_tool\n```\n\n----------------------------------------\n\nTITLE: Implementing Document Query Tools Function\nDESCRIPTION: Creates a function that generates two query tools from a document: a vector search tool for specific questions and a summary tool for holistic understanding. The function handles document loading, splitting into nodes, and index creation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.tools import FunctionTool, QueryEngineTool\nfrom llama_index.core.vector_stores import MetadataFilters, FilterCondition\nfrom typing import List, Optional\n\ndef get_doc_tools(\n    file_path: str,\n    name: str,\n) -> str:\n    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n\n    # load documents\n    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n    splitter = SentenceSplitter(chunk_size=1024)\n    nodes = splitter.get_nodes_from_documents(documents)\n    vector_index = VectorStoreIndex(nodes)\n\n    def vector_query(\n        query: str,\n        page_numbers: Optional[List[str]] = None\n    ) -> str:\n        \"\"\"Use to answer questions over the MetaGPT paper.\n\n        Useful if you have specific questions over the MetaGPT paper.\n        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n\n        Args:\n            query (str): the string query to be embedded.\n            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE\n                if we want to perform a vector search\n                over all pages. Otherwise, filter by the set of specified pages.\n\n        \"\"\"\n\n        page_numbers = page_numbers or []\n        metadata_dicts = [\n            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n        ]\n\n        query_engine = vector_index.as_query_engine(\n            similarity_top_k=2,\n            filters=MetadataFilters.from_dicts(\n                metadata_dicts,\n                condition=FilterCondition.OR\n            )\n        )\n        response = query_engine.query(query)\n        return response\n\n\n    vector_query_tool = FunctionTool.from_defaults(\n        name=f\"vector_tool_{name}\",\n        fn=vector_query\n    )\n\n    summary_index = SummaryIndex(nodes)\n    summary_query_engine = summary_index.as_query_engine(\n        response_mode=\"tree_summarize\",\n        use_async=True,\n    )\n    summary_tool = QueryEngineTool.from_defaults(\n        name=f\"summary_tool_{name}\",\n        query_engine=summary_query_engine,\n        description=(\n            \"Use ONLY IF you want to get a holistic summary of MetaGPT. \"\n            \"Do NOT use if you have specific questions over MetaGPT.\"\n        ),\n    )\n\n    return vector_query_tool, summary_tool\n```\n\n----------------------------------------\n\nTITLE: Installing UV Package Manager Dependencies\nDESCRIPTION: Commands to install UV package manager and required system dependencies\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/crusoe/vllm-fp8/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napt-get update && apt-get install -y curl\napt-get install tmux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nsource $HOME/.cargo/env\n```\n\n----------------------------------------\n\nTITLE: Validating Converted Weights Against Official LLaMA Weights\nDESCRIPTION: Python command to compare the converted weights with official LLaMA weights for validation purposes.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/llama_cookbook/tools/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython compare_llama_weights.py test70B ${Llama-3-70B-Instruct_dir}\n```\n\n----------------------------------------\n\nTITLE: Running Complete Experiment\nDESCRIPTION: Command to execute the end-to-end experiment script that handles model download, server deployment, parallel generations, and evaluation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/modal/many-llamas-human-eval/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbash run_e2e.sh\n```\n\n----------------------------------------\n\nTITLE: Implementing State Management\nDESCRIPTION: Defines the state structure for managing messages during agent execution.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_tool_calling_agent.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.message import AnyMessage, add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n```\n\n----------------------------------------\n\nTITLE: Examining Message with Disabled Function Calling\nDESCRIPTION: Extracting and displaying just the message component from the response when function calling is disabled.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nresponse_message = response.choices[0].message\nresponse_message\n```\n\n----------------------------------------\n\nTITLE: Running Basic Multimodal Inference with Llama Vision Model in Python\nDESCRIPTION: This snippet demonstrates how to run basic multimodal inference using the Llama-3.2-11B-Vision-Instruct model. It requires specifying the image path, prompt text, and model name.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython multi_modal_infer.py \\\n    --image_path \"path/to/image.jpg\" \\\n    --prompt_text \"Describe this image\" \\\n    --model_name \"meta-llama/Llama-3.2-11B-Vision-Instruct\" \\\n```\n\n----------------------------------------\n\nTITLE: Setting Up Ray Cluster for Multi-Node Inference\nDESCRIPTION: Commands to initialize and configure a Ray cluster for distributed inference across multiple nodes. This involves starting the head node and connecting worker nodes to it.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/vllm/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# On the head node we start the clustr as follows\nray start --head\n\n# After the server starts it prints out a couple of lines including the command to add nodes to the cluster e.g.:\n# To add another node to this Ray cluster, run\n#   ray start --address='<head-node-ip-address>:6379'\n# Where the head node ip address will depend on your environment\n\n# We can then add the worker nodes by executing the command in a shell on the worker node\nray start --address='<head-node-ip-address>:6379'\n\n# We can check if the cluster was launched successfully by executing this on any node\nray status\n\n# It should show the number of nodes we have added as well as the head node\n# Node status\n# ---------------------------------------------------------------\n# Active:\n#  1 node_82143b740a25228c24dc8bb3a280b328910b2fcb1987eee52efb838b\n#  1 node_3f2c673530de5de86f953771538f35437ab60e3cacd7730dbca41719\n```\n\n----------------------------------------\n\nTITLE: CURL Request with Streaming\nDESCRIPTION: Example of a CURL command implementing streaming functionality for real-time token generation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!curl -X POST -L https://your-endpoint.inference.ai.azure.com/v1/chat/completions -H 'Content-Type: application/json' -H 'Authorization: your-auth-key' -d '{\"messages\":[{\"content\":\"You are a helpful assistant.\",\"role\":\"system\"},{\"content\":\"What is good about Wuhan?\",\"role\":\"user\"}], \"max_tokens\": 500, \"stream\": true}'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Historical Stock Prices in Python\nDESCRIPTION: This function fetches historical stock prices for a given symbol within a specified date range. It returns a DataFrame containing the date and closing price of the stock.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/groqing-the-stock-market-function-calling-llama3/README.md#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef get_historical_price(symbol, start_date, end_date):\n```\n\n----------------------------------------\n\nTITLE: Python Requests Streaming Implementation\nDESCRIPTION: Implementation using the Requests library for true streaming functionality with the Azure Llama API.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport requests\n\ndata = {\"messages\":[\n            {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n            {\"role\":\"user\", \"content\":\"What is good about Wuhan?\"}],\n        \"max_tokens\": 500,\n        \"temperature\": 0.9,\n        \"stream\": True\n}\n\n\ndef post_stream(url):\n    s = requests.Session()\n    api_key = \"your-auth-key\"\n    headers = {'Content-Type':'application/json', 'Authorization':(api_key)}\n\n    with s.post(url, data=json.dumps(data), headers=headers, stream=True) as resp:\n        print(resp.status_code)\n        for line in resp.iter_lines():\n            if line:\n                print(line)\n\n\nurl = \"https://your-endpoint.inference.ai.azure.com/v1/chat/completions\"\npost_stream(url)\n```\n\n----------------------------------------\n\nTITLE: Printing Message Sequence for Multiple Tool Calls\nDESCRIPTION: A simple code snippet to print the entire message sequence for a multiple tool call interaction in a formatted JSON. This helps visualize the conversation flow between the user, assistant, and tools.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprint(json.dumps(messages, indent=2))\n```\n\n----------------------------------------\n\nTITLE: Querying VLLM Server for RAFT Generation\nDESCRIPTION: Command to run the RAFT dataset generation script against a local or cloud VLLM server endpoint.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/RAFT-Chatbot/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython raft.py -u \"http://localhost:8001/v1\" -k \"EMPTY\" -t 4\n```\n\n----------------------------------------\n\nTITLE: Running Multi-Node Multi-GPU Inference with vLLM\nDESCRIPTION: Command for running Llama inference across multiple nodes. The pp_size parameter should equal the number of nodes (worker + head), while tp_size should match the number of GPUs per node.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/vllm/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython inference.py --model_name $MODEL_PATH --peft_model_name $PEFT_MODEL_PATH --pp_size 2 --tp_size 8 --user_prompt \"Hello my name is\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Custom Function Name for Llama Fine-Tuning Dataset in Python\nDESCRIPTION: Command-line instruction for running Llama fine-tuning with a custom dataset, specifying a custom function name 'get_foo' instead of the default 'get_custom_dataset'.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/datasets/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m llama_cookbook.finetuning --dataset \"custom_dataset\" --custom_dataset.file \"custom_dataset.py:get_foo\" [TRAINING PARAMETERS]\n```\n\n----------------------------------------\n\nTITLE: Running the Email Agent with Python\nDESCRIPTION: Command to execute the email agent application with a Gmail address parameter. This initiates the agent and handles the authentication process.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --email <your_gmail_address>\n```\n\n----------------------------------------\n\nTITLE: Implementing Weather Function\nDESCRIPTION: Defining a sample weather function that returns temperature for specific cities\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/parallel-tool-use/parallel-tool-use.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_weather(city: str):\n    if city == \"Madrid\":\n        return 35\n    elif city == \"San Francisco\":\n        return 18\n    elif city == \"Paris\":\n        return 20\n    else:\n        return 15\n```\n\n----------------------------------------\n\nTITLE: Defining Function for SDOH Extraction using Groq API\nDESCRIPTION: This function encapsulates the process of extracting social determinants of health from a clinical note using the Groq API with JSON mode.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-social-determinants-of-health/SDOH-Json-mode.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef extract_sdoh_json(system_prompt,user_prompt,model):\n    \n    # Establish client with GROQ_API_KEY environment variable\n    client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n    \n    # Create chat completion object with JSON response format\n    chat_completion = client.chat.completions.create(\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": user_prompt_template.format(clinical_note=clinical_note),\n            }\n        ],\n        model = model,\n        response_format = {\"type\": \"json_object\"} # Add this response format to configure JSON mode\n    )\n    \n    social_determinants_json_string = chat_completion.choices[0].message.content\n\n    # Return json object of the chat output\n    return json.loads(social_determinants_json_string)\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Search in MongoDB\nDESCRIPTION: Defines functions for performing vector search in the MongoDB collection based on user queries. Utilizes the vector search index and aggregation pipeline for efficient retrieval.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/vectorstore/mongodb/rag_mongodb_llama3_huggingface_open_source.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef vector_search(user_query, collection):\n    query_embedding = get_embedding(user_query)\n\n    if query_embedding is None:\n        return \"Invalid query or embedding generation failed.\"\n\n    vector_search_stage = {\n        \"$vectorSearch\": {\n            \"index\": \"vector_index\",\n            \"queryVector\": query_embedding,\n            \"path\": \"embedding\",\n            \"numCandidates\": 150,\n            \"limit\": 4\n        }\n    }\n\n    unset_stage = {\n        \"$unset\": \"embedding\"\n    }\n\n    project_stage = {\n        \"$project\": {\n            \"_id\": 0,\n            \"fullplot\": 1,\n            \"title\": 1,\n            \"genres\": 1,\n            \"score\": {\n                \"$meta\": \"vectorSearchScore\"\n            }\n        }\n    }\n\n    pipeline = [vector_search_stage, unset_stage, project_stage]\n\n    results = collection.aggregate(pipeline)\n    return list(results)\n\ndef get_search_result(query, collection):\n  get_knowledge = vector_search(query, collection)\n\n  search_result = ''\n  for result in get_knowledge:\n      search_result += f\"Title: {result.get('title', 'N/A')}, Plot: {result.get('abstract', 'N/A')}\\n\"\n\n  return search_result\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installation of necessary Python packages for the implementation including langchain, langgraph, and related dependencies.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n!pip install langchain\n!pip install langgraph\n!pip install langchain_openai\n!pip install langchain_community\n!pip install httpx\n!pip install langchain-groq\n```\n\n----------------------------------------\n\nTITLE: Initializing and Querying External LLM Services in Python\nDESCRIPTION: Shows how to initialize and query three different LLM services (Together.ai, OpenAI, and Anyscale) using the llama_cookbook library. Each service requires an API token and model specification. The code demonstrates sending a simple prompt to each service.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/using_externally_hosted_llms.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_cookbook.inference.llm import  TOGETHER, OPENAI, ANYSCALE\n\ntogether_example = TOGETHER(\"togethercomputer/llama-2-7b-chat\",\"09e45...\")\nprint( together_example.query(prompt=\"Why is the sky blue?\"))\n\n\nopenai_example = OPENAI(\"gpt-3.5-turbo\",\"sk-LIz9zL3cYp...\")\nprint( openai_example.query(prompt=\"Why is the sky blue?\"))\n\n\nanyscale_example = ANYSCALE(\"meta-llama/Llama-2-7b-chat-hf\",\"esecret_c3u4x7...\")\nprint( anyscale_example.query(prompt=\"Why is the sky blue?\"))\n```\n\n----------------------------------------\n\nTITLE: Querying Agent About Evaluation Datasets\nDESCRIPTION: Sends a query to the agent about evaluation datasets used in the MetaGPT paper.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.query(\n    \"Tell me about the evaluation datasets used.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Replicate LLM Integration\nDESCRIPTION: Setup for using Llama3 through Replicate, supporting both 8B and 70B model variants.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.llms.replicate import Replicate\n\nllm_replicate = Replicate(model=\"meta/meta-llama-3-70b-instruct\")\n# llm_replicate = Replicate(model=\"meta/meta-llama-3-8b-instruct\")\n```\n\n----------------------------------------\n\nTITLE: Navigating to a URL using Accessibility Tree Actions in JSON\nDESCRIPTION: Example of a JSON-structured action to navigate to a specific URL, including the current state, reasoning, and the URL to navigate to.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"current_state\": \"Starting from a blank page.\",\n    \"reasoning\": \"The task requires visiting a specific website to gather relevant information. Navigating to the URL is the first step.\",\n    \"action\": \"navigation\",\n    \"url\": \"https://example.com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Llama-Cookbook with vLLM Support\nDESCRIPTION: Installs Llama-Cookbook with additional dependencies required for the vLLM example functionality.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/README.md#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install llama-cookbook[vllm]\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Tools\nDESCRIPTION: Implementation of custom tools for calculation and dog weight lookup that can be used by the agent.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import ToolNode\n\n@tool\ndef calculate(what):\n    \"\"\"Runs a calculation and returns the number.\"\"\"\n    return eval(what)\n\n@tool\ndef average_dog_weight(name):\n    \"\"\"Returns the average weight of a dog.\"\"\"\n    if name in \"Scottish Terrier\":\n        return(\"Scottish Terriers average 20 lbs\")\n    elif name in \"Border Collie\":\n        return(\"a Border Collies average weight is 37 lbs\")\n    elif name in \"Toy Poodle\":\n        return(\"a toy poodles average weight is 7 lbs\")\n    else:\n        return(\"An average dog weights 50 lbs\")\n```\n\n----------------------------------------\n\nTITLE: Loading PDF Documents for RAG Processing\nDESCRIPTION: Uses PyPDFDirectoryLoader to load PDF documents from a specified directory for RAG processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nloader = PyPDFDirectoryLoader(DATA_PATH)\ndocuments = loader.load()\n```\n\n----------------------------------------\n\nTITLE: Running the Llama 3 Messenger Chatbot with Gunicorn\nDESCRIPTION: Command to run the Llama 3 enabled web app using Gunicorn as the WSGI HTTP server.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/messenger_chatbot/messenger_llama3.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngunicorn -b 0.0.0.0:5000 llama_messenger:app\n```\n\n----------------------------------------\n\nTITLE: Implementing Answer Grader for RAG Agent\nDESCRIPTION: This code implements an answer grader that assesses whether the generated answer is useful in resolving the user's question. It uses a local LLM to provide a binary score indicating the answer's usefulness.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# LLM\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n\n# Prompt\nprompt = PromptTemplate(\n    template=\"\"\"You are a grader assessing whether an \n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n     \n    Here is the answer:\n    {generation} \n\n    Here is the question: {question}\n    \"\"\",\n    input_variables=[\"generation\", \"question\"],\n)\n\nanswer_grader = prompt | llm | JsonOutputParser()\nanswer_grader.invoke({\"question\": question,\"generation\": generation})\n```\n\n----------------------------------------\n\nTITLE: Getting Follow-up Response\nDESCRIPTION: Retrieves the SQL query for the follow-up question with context\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/quickstart.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nnew_answer = llm.invoke(prompt).content\nprint(new_answer)\n```\n\n----------------------------------------\n\nTITLE: Preparing User Query for Llama3 Processing\nDESCRIPTION: Formats a user query with retrieved search results to create a combined information string for processing by the Llama3 model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/vectorstore/mongodb/rag_mongodb_llama3_huggingface_open_source.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nquery = \"Get me papers on Artificial Intelligence?\"\nsource_information = get_search_result(query, collection)\ncombined_information = f\"Query: {query}\\nContinue to answer the query by using the Search Results:\\n{source_information}.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a research assistant!\"},\n    {\"role\": \"user\", \"content\": combined_information},\n]\nprint(messages)\n```\n\n----------------------------------------\n\nTITLE: Defining System Prompt\nDESCRIPTION: Setting up the system prompt that defines the agent's behavior and available actions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"\"\"\nYou run in a loop of Thought, Action, Observation.\nAt the end of the loop you output an Answer\nUse Thought to describe your thoughts about the question you have been asked.\nUse Action to run one of the actions available to you.\nObservation will be the result of running those actions.\n\nYour available actions are:\n\ncalculate:\ne.g. calculate: 4 * 7 / 3\nRuns a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n\naverage_dog_weight:\ne.g. average_dog_weight: Collie\nreturns average weight of a dog when given the breed\n\nExample session:\n\nQuestion: How much does a Bulldog weigh?\nThought: I should look the dogs weight using average_dog_weight\nAction: average_dog_weight: Bulldog\n\nYou will be called again with this:\n\nObservation: A Bulldog weights 51 lbs\n\nYou then output:\n\nAnswer: A bulldog weights 51 lbs\n\"\"\".strip()\n```\n\n----------------------------------------\n\nTITLE: Initializing Llama Chat Model\nDESCRIPTION: Sets up the Together.ai API key and initializes the Llama 3.3 chat model with LangChain\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/quickstart.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom langchain_together import ChatTogether\n\nos.environ['TOGETHER_API_KEY'] = 'your_api_key'\n\nllm = ChatTogether(\n    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n    temperature=0,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Llama 3.1 RAG Demo\nDESCRIPTION: This code snippet installs the necessary Python packages for the Llama 3.1 RAG demo, including LangChain, sentence-transformers, FAISS, BeautifulSoup, and Replicate.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/RAG/hello_llama_cloud.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install langchain\n!pip install sentence-transformers\n!pip install faiss-cpu\n!pip install bs4\n!pip install replicate\n!pip install langchain-community\n```\n\n----------------------------------------\n\nTITLE: Loading Web Content for RAG with Llama 3.1\nDESCRIPTION: This snippet loads web content about Llama 3.1 from a Hugging Face blog post, which will be used for Retrieval Augmented Generation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/RAG/hello_llama_cloud.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nimport bs4\n\nloader = WebBaseLoader([\"https://huggingface.co/blog/llama3\"])\ndocs = loader.load()\n```\n\n----------------------------------------\n\nTITLE: Running Chat Completion with Fast Kernels in Python\nDESCRIPTION: This snippet demonstrates how to run chat completion with Flash Attention or Xformer memory-efficient kernels. It includes the use_fast_kernels flag for improved performance on batched inputs.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython chat_completion/chat_completion.py --model_name \"PATH/TO/MODEL/7B/\" --prompt_file chat_completion/chats.json  --quantization 8bit --use_auditnlg --use_fast_kernels\n```\n\n----------------------------------------\n\nTITLE: Validating Fashion Caption Outputs\nDESCRIPTION: Code to load and display the generated fashion descriptions from the CSV file for review and validation, ensuring the model outputs meet the expected format and quality.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"./captions_testing.csv\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Class\nDESCRIPTION: Main agent implementation with state management, LLM calls, and action handling.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_llm)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_llm(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        return {'messages': results}\n```\n\n----------------------------------------\n\nTITLE: Displaying LLM Prompts\nDESCRIPTION: Shows the prompts used for issue annotation and categorization\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Prompt for generating annotations:\\n{CFG['prompts']['parse_issue']['system']}\\n\")\nprint(f\"Prompt for categorizing issues:\\n{CFG['prompts']['assign_category']['system']}\\n\")\nprint(f\"Issues can be categorized into: {eval(CFG['prompts']['assign_category']['json_schema'])['properties']['report']['items']['properties']['theme']['enum']}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for YouTube Video Summarization\nDESCRIPTION: Installs necessary packages including langchain, youtube-transcript-api, tiktoken, pytube, and replicate for video transcript retrieval and summarization.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/video_summary.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install langchain youtube-transcript-api tiktoken pytube replicate\n```\n\n----------------------------------------\n\nTITLE: Preparing English Data for Translation\nDESCRIPTION: Code to extract English paragraphs from the Varta dataset for translation. This data will be used as the foundation for the two-phase training process, where English text will be translated to Hindi for various training scenarios.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/multilingual/README.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nds = load_dataset(\"rahular/varta\", split=\"train\", streaming=True)\nenglish_paragraphs = []\nfor d in ds:\n    if d[\"langCode\"] != \"en\": continue\n    english_paragraphs.append(\" \".join(d[\"text\"].split(\"\\n\")))\n```\n\n----------------------------------------\n\nTITLE: Launching Gradio UI for Multimodal Inference with Llama Vision Model in Python\nDESCRIPTION: This snippet shows how to launch a Gradio UI for multimodal inference using the Llama-3.2-11B-Vision-Instruct model. It requires specifying the model name and the gradio_ui flag.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython multi_modal_infer.py \\\n    --model_name \"meta-llama/Llama-3.2-11B-Vision-Instruct\" \\\n    --gradio_ui\n```\n\n----------------------------------------\n\nTITLE: Ingesting Data into MongoDB Collection\nDESCRIPTION: Converts the prepared DataFrame to a list of dictionaries and inserts them into the MongoDB collection.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/vectorstore/mongodb/rag_mongodb_llama3_huggingface_open_source.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndocuments = dataset_df.to_dict('records')\ncollection.insert_many(documents)\n\nprint(\"Data ingestion into MongoDB completed\")\n```\n\n----------------------------------------\n\nTITLE: Agent Usage Examples\nDESCRIPTION: Examples of using the implemented agent with different queries and model configurations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nabot = Agent(model, [calculate, average_dog_weight], system=prompt)\n\nmessages = [HumanMessage(content=\"How much does a Toy Poodle weigh?\")]\nresult = abot.graph.invoke({\"messages\": messages})\nresult['messages'], result['messages'][-1].content\n\n# using the Llama 3 70B will fix the error\nmodel = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\")\nabot = Agent(model, [calculate, average_dog_weight], system=prompt)\n\nmessages = [HumanMessage(content=\"How much does a Toy Poodle weigh?\")]\nresult = abot.graph.invoke({\"messages\": messages})\nresult['messages'], result['messages'][-1].content\n\nmessages = [HumanMessage(content=\"I have 2 dogs, a border collie and a scottish terrier. What are their average weights? Total weight?\")]\nresult = abot.graph.invoke({\"messages\": messages})\n\nresult['messages'], result['messages'][-1].content\n```\n\n----------------------------------------\n\nTITLE: Creating Board Proxy Agent\nDESCRIPTION: Sets up a proxy agent that manages the chess board and mediates between the player agents. This agent doesn't use an LLM, instead handling the game state and terminating conversations when moves are made.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nboard_proxy = ConversableAgent(\n    name=\"Board Proxy\",\n    llm_config=False,\n    is_termination_msg=check_made_move,\n    default_auto_reply=\"Please make a move.\",\n    human_input_mode=\"NEVER\",\n)\n```\n\n----------------------------------------\n\nTITLE: Automated ReAct Process Implementation\nDESCRIPTION: Implements automated execution of the ReAct loop with action parsing and handling.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\naction_re = re.compile('^Action: (\\w+): (.*)$')   # python regular expression to selection action\n\ndef query(question, max_turns=5):\n    i = 0\n    bot = Agent(prompt) # set system prompt\n    next_prompt = question\n    while i < max_turns:\n        i += 1\n        result = bot(next_prompt)\n        print(result)\n        actions = [\n            action_re.match(a)\n            for a in result.split('\\n')\n            if action_re.match(a)\n        ]\n        if actions:\n            # There is an action to run\n            action, action_input = actions[0].groups()\n            if action not in known_actions:\n                raise Exception(\"Unknown action: {}: {}\".format(action, action_input))\n            print(\" -- running {} {}\".format(action, action_input))\n\n            # key to make the agent process fully automated:\n            # programtically call the external func with arguments, with the info returned by LLM\n            observation = known_actions[action](action_input)\n\n            print(\"Observation:\", observation)\n            next_prompt = \"Observation: {}\".format(observation)\n        else:\n            return\n```\n\n----------------------------------------\n\nTITLE: Configuring Groq Client and Model\nDESCRIPTION: Setting up the Groq client with API key and specifying the Llama 3 model\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/parallel-tool-use/parallel-tool-use.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\nmodel = \"llama3-70b-8192\"\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Dataset Function for Llama Fine-Tuning in Python\nDESCRIPTION: Function signature for creating a custom dataset to use with Llama fine-tuning scripts. The function takes dataset configuration, tokenizer, and split as parameters and should return a dataset compatible with the model's forward method.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/datasets/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_custom_dataset(dataset_config, tokenizer, split: str):\n```\n\n----------------------------------------\n\nTITLE: Initializing API Clients for Together.ai and Cartesia\nDESCRIPTION: Sets up API clients for Together.ai (LLM provider) and Cartesia.ai (TTS provider) using API keys that need to be provided by the user.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient_together = Together(api_key=\"xxx\")\nclient_cartesia = Cartesia(api_key=\"yyy\")\n```\n\n----------------------------------------\n\nTITLE: Creating Black Player Agent with AutoGen\nDESCRIPTION: Creates a conversational agent representing the black player in the chess game. Similar to the white player, this agent is instructed to get legal moves, make a move, and communicate.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Player black agent\nplayer_black = ConversableAgent(\n    name=\"Player Black\",\n    system_message=\"You are a chess player and you play as black. \"\n    \"First call get_legal_moves(), to get a list of legal moves in UCI format. \"\n    \"Then call make_move(move) to make a move. Finally, tell the proxy what you have moved and ask the white to move\", # added \"Finally...\" to make the agents work\n    llm_config={\"config_list\": config_list,\n                \"temperature\": 0,\n               },)\n```\n\n----------------------------------------\n\nTITLE: Initializing Main Model Chat History\nDESCRIPTION: This code initializes the main_model_chat_history list with the system prompt, setting up the conversation context for the Llama model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/Agents_Tutorial/Tool_Calling_201.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmain_model_chat_history = [\n    {\n        \"role\" : \"system\",\n        \"content\" : MAIN_SYSTEM_PROMPT\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Loading and preprocessing the samsum dataset\nDESCRIPTION: Loads the samsum dataset which contains pairs of dialogues and their summaries for fine-tuning. The dataset is processed and converted into dataloaders for both training and validation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/quickstart_peft_finetuning.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_cookbook.configs.datasets import samsum_dataset\nfrom llama_cookbook.utils.dataset_utils import get_dataloader\n\nsamsum_dataset.trust_remote_code = True\n\ntrain_dataloader = get_dataloader(tokenizer, samsum_dataset, train_config)\neval_dataloader = get_dataloader(tokenizer, samsum_dataset, train_config, \"val\")\n```\n\n----------------------------------------\n\nTITLE: Selecting Data from Employees CSV in DuckDB\nDESCRIPTION: Example of the DuckDB syntax required for querying from a CSV file with proper table aliasing. The query demonstrates selecting all columns from the employees.csv file.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/text-to-sql-json-mode/prompts/base_prompt.txt#2025-04-07_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM employees.csv as employees\n```\n\n----------------------------------------\n\nTITLE: Downloading Alpaca Dataset in Bash\nDESCRIPTION: Command to download the Alpaca dataset JSON file into the llama-cookbook datasets directory. This dataset can be used for instruction fine-tuning of language models.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/singlegpu_finetuning.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget -P ../../src/llama_cookbook/datasets https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Sample\nDESCRIPTION: Reads the downloaded Transformer paper PDF, extracts the text content, and displays the length and first portion of the extracted text.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/pdf_to_podcast_using_llama_on_together.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntext = get_PDF_text('attention.pdf')\nlen(text), text[:1000]\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completion with Function Calling\nDESCRIPTION: Making an API call to Groq to generate a chat completion with Llama 3 that uses function calling capabilities to handle the weather query.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=messages,\n    functions=functions,\n    #tools=tools, # you can also replace functions with tools, as specified in https://console.groq.com/docs/tool-use\n    max_tokens=4096,\n    temperature=0\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring LangGraph Edges and Flow Control in Python\nDESCRIPTION: Sets up the graph structure by defining conditional routing between nodes based on document relevance and generation quality assessments. This defines how the workflow progresses through different states.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Build graph\nworkflow.set_conditional_entry_point(\n    route_question,\n    {\n        \"websearch\": \"websearch\",\n        \"vectorstore\": \"retrieve\",\n    },\n)\n\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"websearch\": \"websearch\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"websearch\", \"generate\")\nworkflow.add_conditional_edges(\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not supported\": \"generate\",\n        \"useful\": END,\n        \"not useful\": \"websearch\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Function Tools\nDESCRIPTION: Creates function tools for addition and mystery operations using LlamaIndex's FunctionTool wrapper.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L2_Tool_Calling.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.tools import FunctionTool\n\ndef add(x: int, y: int) -> int:\n    \"\"\"Adds two integers together.\"\"\"\n    return x + y\n\ndef mystery(x: int, y: int) -> int:\n    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n    return (x + y) * (x + y)\n\n\nadd_tool = FunctionTool.from_defaults(fn=add)\nmystery_tool = FunctionTool.from_defaults(fn=mystery)\n```\n\n----------------------------------------\n\nTITLE: Creating a Translator Chain with LangChain\nDESCRIPTION: Implements a translation chain using LLMChain and PromptTemplate to translate text between languages.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/azure/Azure MaaS/azure_api_example.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"\nYou are a Translator. Translate the following content from {input_language} to {output_language} and reply with only the translated result.\n{input_content}\n\"\"\"\n\ntranslator_chain = LLMChain(\n    llm = llm,\n    prompt = PromptTemplate(\n            template=template,\n            input_variables=[\"input_language\", \"output_language\", \"input_content\"],\n        ),\n)\n\nprint(translator_chain.run(input_language=\"English\", output_language=\"French\", input_content=\"What is good about Wuhan?\"))\n```\n\n----------------------------------------\n\nTITLE: Running Fine-tuning with Different Datasets in Bash\nDESCRIPTION: Commands for fine-tuning Meta Llama 3 models with different datasets (grammar, alpaca, and samsum). Each command uses LoRA with 8-bit quantization but changes the dataset parameter.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/singlegpu_finetuning.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# grammar_dataset\n\npython -m finetuning.py  --use_peft --peft_method lora --quantization 8bit --dataset grammar_dataset --model_name /path_of_model_folder/8B --output_dir Path/to/save/PEFT/model\n\n# alpaca_dataset\n\npython -m finetuning.py  --use_peft --peft_method lora --quantization 8bit  --dataset alpaca_dataset --model_name /path_of_model_folder/8B --output_dir Path/to/save/PEFT/model\n\n\n# samsum_dataset\n\npython -m finetuning.py  --use_peft --peft_method lora --quantization 8bit  --dataset samsum_dataset --model_name /path_of_model_folder/8B --output_dir Path/to/save/PEFT/model\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installs the necessary Python packages from requirements.txt\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/quickstart.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install --upgrade -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Viewing Response Content\nDESCRIPTION: Extracting and displaying the content field from the response message.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresponse_message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing Router Query Engine\nDESCRIPTION: Setting up the router query engine to direct queries to appropriate tools.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Building_Agentic_RAG_with_Llamaindex_L1_Router_Engine.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.query_engine.router_query_engine import RouterQueryEngine\nfrom llama_index.core.selectors import LLMSingleSelector\n\nquery_engine = RouterQueryEngine(\n    selector=LLMSingleSelector.from_defaults(),\n    query_engine_tools=[\n        summary_tool,\n        vector_tool,\n    ],\n    verbose=True\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Knowledge Graph with GraphViz\nDESCRIPTION: Function to create and visualize the knowledge graph using GraphViz Digraph, converting the JSON structure into a visual representation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/knowledge_graphs_with_structured_outputs.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom graphviz import Digraph\n\ndef visualize_knowledge_graph(kg):\n    dot = Digraph(comment=\"Knowledge Graph\", format='png')\n\n    # Add nodes\n    for node in kg['nodes']:\n        dot.node(str(node['id']), node['label'])\n\n    # Add edges\n    for edge in kg['edges']:\n        dot.edge(str(edge['source']), str(edge['target']), label=edge['label'])\n\n    # Render the graph to a file and open it\n    output_path = dot.render(\"knowledge_graph\", view=True)\n    print(f\"Graph rendered and saved to {output_path}\")\n```\n\n----------------------------------------\n\nTITLE: Verifying Loaded Document Content\nDESCRIPTION: Prints the length and a snippet of the loaded document content to verify correct loading.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(len(documents), documents[0].page_content[0:100])\n```\n\n----------------------------------------\n\nTITLE: LLM Freeze FSDP Fine-tuning Command for Llama Vision Model\nDESCRIPTION: Command to perform fine-tuning with frozen LLM layers using FSDP on Llama 3.2 vision model. Enables LLM freezing while training other components.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/finetune_vision_model.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nnodes 1 --nproc_per_node 4  recipes/quickstart/finetuning/finetuning.py --enable_fsdp --lr 1e-5  --num_epochs 3 --batch_size_training 2 --model_name meta-llama/Llama-3.2-11B-Vision-Instruct --dist_checkpoint_root_folder ./finetuned_model --dist_checkpoint_folder fine-tuned  --use_fast_kernels --dataset \"custom_dataset\" --custom_dataset.test_split \"test\" --custom_dataset.file \"recipes/quickstart/finetuning/datasets/ocrvqa_dataset.py\"  --run_validation True --batching_strategy padding --freeze_LLM_only True\n```\n\n----------------------------------------\n\nTITLE: Merging LoRA Weights with Base Model in Python\nDESCRIPTION: This Python command merges weights from a LoRA fine-tuned model with the base Llama model. It takes the base model path, PEFT model output directory, and an output directory as parameters.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/tgi/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -m llama_cookbook.recipes.3p_integration.tgi.merge_lora_weights --base_model llama-7B --peft_model ft_output --output_dir data/merged_model_output\n```\n\n----------------------------------------\n\nTITLE: Accessing Function Call Data from Response\nDESCRIPTION: Retrieving the function call details from the response message, which contains the function name and arguments selected by the model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresponse_message.function_call\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Conversational Chat with Follow-up Question\nDESCRIPTION: Uses the agent's chat method instead of query to maintain conversation history when asking follow-up questions about dataset results.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# use agent.chat instead of agent.query to pass conversational history automatically to answer follow up questions\nresponse = agent.chat(\"Tell me the results over one of the above datasets.\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required LlamaIndex Packages\nDESCRIPTION: Installs the core LlamaIndex library along with HuggingFace embeddings and Fireworks LLM integrations required for the agentic RAG system.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n!pip install llama-index-embeddings-huggingface\n!pip install llama-index-llms-fireworks\n```\n\n----------------------------------------\n\nTITLE: Benchmark Configuration Parameters in parameter.json\nDESCRIPTION: Configuration parameters for running inference benchmarks, including prompt settings, model paths, concurrent request levels, and inference hyperparameters. These parameters are stored in parameter.json and input.jsonl files.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* PROMPT - Prompt sent in for inference (configure the length of prompt, choose from 5, 25, 50, 100, 500, 1k and 2k)\n* MAX_NEW_TOKENS - Max number of tokens generated\n* CONCURRENT_LEVELS - Max number of concurrent requests\n* MODEL_PATH - Model source from Huggingface\n* MODEL_HEADERS - Request headers\n* SAFE_CHECK - Content safety check (either Azure service or simulated latency)\n* THRESHOLD_TPS - Threshold TPS (threshold for tokens per second below which we deem the query to be slow)\n* TOKENIZER_PATH - Tokenizer source\n* RANDOM_PROMPT_LENGTH - Random prompt length (for pretrained models)\n* NUM_GPU - Number of GPUs for request dispatch among multiple containers\n* TEMPERATURE - Temperature for inference\n* TOP_P - Top_p for inference\n* MODEL_ENDPOINTS - Container endpoints\n```\n\n----------------------------------------\n\nTITLE: Multi-node Training Command for 405B Model\nDESCRIPTION: Command for distributed training of 405B model using FSDP with LoRA and 4-bit quantization.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/multigpu_finetuning.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsrun  torchrun --nproc_per_node 8 --rdzv_id $RANDOM --rdzv_backend c10d --rdzv_endpoint $head_node_ip:29500 ./finetuning.py  --enable_fsdp --use_peft --peft_method lora --quantization 4bit  --quantization_config.quant_type nf4 --mixed_precision False --low_cpu_fsdp\n```\n\n----------------------------------------\n\nTITLE: Initializing Llama 3 Model with ChatGroq for Stock Analysis\nDESCRIPTION: Set up the Llama 3-70B model using LangChain's ChatGroq function. Requires a Groq API key stored in environment variables.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/llama3-stock-market-function-calling/llama3-stock-market-function-calling.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nllm = ChatGroq(groq_api_key = os.getenv('GROQ_API_KEY'),model = 'llama3-70b-8192')\n```\n\n----------------------------------------\n\nTITLE: Parsing Function Call Arguments\nDESCRIPTION: Converting the JSON string of function arguments into a Python dictionary for programmatic use.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\njson.loads(response_message.function_call.arguments)\n```\n\n----------------------------------------\n\nTITLE: End of Notebook Marker\nDESCRIPTION: Simple marker indicating the end of the notebook code, commonly used as a convention in notebook-style programming.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n#fin\n```\n\n----------------------------------------\n\nTITLE: Setting Device for TTS Model Processing\nDESCRIPTION: Defining the GPU device to be used for TTS model processing. This ensures consistent device usage across different function calls.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndevice=\"cuda:3\"\n```\n\n----------------------------------------\n\nTITLE: Querying LLM for Specific or Private Information in Python\nDESCRIPTION: Illustrates the limitations of LLMs when queried for specific or private information. The model either declares lack of access to such data or provides a generalized response.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncomplete_and_print(\"What was the temperature in Menlo Park on December 12th, 2023?\")\n# \"I'm just an AI, I don't have access to real-time weather data or historical weather records.\"\n\ncomplete_and_print(\"What time is my dinner reservation on Saturday and what should I wear?\")\n# \"I'm not able to access your personal information [..] I can provide some general guidance\"\n```\n\n----------------------------------------\n\nTITLE: Creating Phase 1 Training Data (Translation)\nDESCRIPTION: Code to create data for Phase 1 training, where the model learns to translate paragraphs. It generates pairs of translated Hindi paragraphs followed by the original English text, using IndicTrans2 for translation, and saves the resulting dataset.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/multilingual/README.md#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nquantization = \"\"\nen_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\nen_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, \"en-indic\", quantization)\nip = IndicProcessor(inference=True)\n\nphase1_data = []\nfor para in english_paragraphs:\n    trans_para = translate_paragraph(para, \"eng_Latn\", \"hin_Deva\", en_indic_model, en_indic_tokenizer, ip)\n    phase1_data.append({\"text\": f\"{trans_para}\\n\\n{para}\"})\n\n# if you want to save it for future, you can do so easily with HF datasets\nfrom datasets import Dataset\nphase1_ds = Dataset.from_list(phase1_data)\nphase1_ds.save_to_disk(\"data/phase1\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Information from Receipt Image using Llama Vision Model in Python\nDESCRIPTION: This code uses the Llama 3.2 90B Vision model to extract information from a receipt image. It sends a prompt and the image URL to the model and retrieves the extracted information.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/structured_text_extraction_from_images.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom together import Together\n\ngetDescriptionPrompt = \"Extract out the details from each line item on the receipt image. Identify the name, price and quantity of each item. Also specify the total.\"\n\nimageUrl = \"https://ocr.space/Content/Images/receipt-ocr-original.webp\"\n\nclient = Together(api_key=TOGETHER_API_KEY)\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": getDescriptionPrompt},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": imageUrl,\n                    },\n                },\n            ],\n        }\n    ],\n)\n\ninfo = response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Executing Weather Function with Extracted Arguments\nDESCRIPTION: Calling the weather function with the arguments extracted from the model's function call to get the weather information.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nget_current_weather(args)\n```\n\n----------------------------------------\n\nTITLE: Creating ReAct Agent with Initial Tools\nDESCRIPTION: Initializes a ReActAgent with the tools created from the initial papers, enabling it to reason and act based on questions about these papers.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core.agent import ReActAgent\n\nquery_engine_tools = [vector_tool, summary_tool]\n\nagent = ReActAgent.from_tools(\n    initial_tools,\n    llm=llm,\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Querying OpenAIAgent for Arithmetic in Python\nDESCRIPTION: This snippet demonstrates how to use the OpenAIAgent to perform a complex arithmetic calculation, combining addition and multiplication.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresponse = agent.chat(\"What is (121 + 2) * 5?\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Defining List of CSV Files from Multi-GPU Run\nDESCRIPTION: Creates a list of CSV files containing image captions generated from a multi-GPU processing run.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# List of your CSV files\ncsv_files = [\n    \"../MM-Demo/captions_gpu_0.csv\",\n    \"../MM-Demo/captions_gpu_1.csv\",\n    \"../MM-Demo/captions_gpu_2.csv\",\n    \"../MM-Demo/captions_gpu_3.csv\",\n    \"../MM-Demo/captions_gpu_4.csv\",\n    \"../MM-Demo/captions_gpu_5.csv\",\n    \"../MM-Demo/captions_gpu_6.csv\",\n    \"../MM-Demo/captions_gpu_7.csv\",\n    \n]\n```\n\n----------------------------------------\n\nTITLE: Training a Hindi SentencePiece Tokenizer\nDESCRIPTION: Command to train a Hindi-only SentencePiece tokenizer with a vocabulary size of 16,000 using the previously prepared Hindi text data. The tokenizer is saved to a specified directory for later use.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/multilingual/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython train_tokenizer.py --data_file=./data/hi.txt --save_path=./hi_tokenizer --vocab_size=16000\n```\n\n----------------------------------------\n\nTITLE: Displaying the Result Dataframe\nDESCRIPTION: Shows the content of the result dataframe containing the cleaned and parsed image annotations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresult\n```\n\n----------------------------------------\n\nTITLE: Dynamically Calling Function Based on Model's Response\nDESCRIPTION: Executing the function dynamically by looking up the function name in the known_functions dictionary and passing in the arguments.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# by defining and using known_functions, we can programmatically call function\nfunction_response = known_functions[function_call.name](function_call.arguments)\n```\n\n----------------------------------------\n\nTITLE: LLM Output Handler Functions\nDESCRIPTION: Functions for handling LLM outputs and function calls. Includes logic for parsing function names and parameters from LLM responses and executing corresponding functions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/Agents_Tutorial/Tool_Calling_201.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef handle_llm_output(llm_output):\n    # Check if the output starts with \"<function=\"\n    if llm_output.startswith(\"<function=\"):\n        return extract_details_and_call_function(llm_output)\n    else:\n        # Output does not start with \"<function=\", return as is\n        return llm_output\n\ndef extract_details_and_call_function(input_string):\n    # Extract the function name and parameters\n    prefix = \"<function=\"\n    suffix = \"</function>\"\n    start = input_string.find(prefix) + len(prefix)\n    end = input_string.find(suffix)\n    function_and_params = input_string[start:end]\n    \n    # Split to get function name and parameters\n    function_name, params_json = function_and_params.split(\">{\")\n    function_name = function_name.strip()\n    params_json = \"{\" + params_json\n    \n    # Convert parameters to dictionary\n    params = json.loads(params_json)\n    \n    # Call the function dynamically\n    function_map = {\n        \"query_for_two_papers\": query_for_two_papers,\n        \"get_arxiv_id\": get_arxiv_ids,\n        \"process_arxiv_paper\": process_arxiv_paper,\n        \"summarise_text_file\": summarize_text_file\n    }\n    \n    if function_name in function_map:\n        result = function_map[function_name](**params)\n        return result\n    else:\n        return \"Function not found\"\n```\n\n----------------------------------------\n\nTITLE: Closing Gradio Demo for Llama Models in Python\nDESCRIPTION: This code snippet closes the Gradio demo interface, releasing the occupied port. It's important to run this command after finishing testing to free up system resources.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndemo.close()\n```\n\n----------------------------------------\n\nTITLE: Initializing Langchain with Groq Integration\nDESCRIPTION: Sets up the Langchain integration with Groq by initializing a ChatGroq instance using the API key from environment variables. This enables Langchain to use Groq's LLM capabilities.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/function-calling-101-ecommerce/Function-Calling-101-Ecommerce.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(groq_api_key=os.getenv(\"GROQ_API_KEY\"), model=MODEL)\n```\n\n----------------------------------------\n\nTITLE: Configuring Generation Parameters and Metrics in YAML\nDESCRIPTION: YAML configuration for model generation parameters and evaluation metrics, including temperature settings and exact match scoring.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/meta_eval/README.md#2025-04-07_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ngeneration_kwargs:\n  until: []\n  do_sample: false\n  temperature: 0\n  max_gen_toks: 1024\nnum_fewshot: 0\nmetric_list:\n  - metric: exact_match\n    aggregation: mean\n    higher_is_better: true\n    ignore_case: true\n    ignore_punctuation: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Groq LLM Integration\nDESCRIPTION: Setup and configuration of Groq LLM models (8B and 70B variants) with API key initialization.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = \"GROQ_API_KEY\"\n\nfrom llama_index.llms.groq import Groq\n\nllm = Groq(model=\"llama3-8b-8192\")\nllm_70b = Groq(model=\"llama3-70b-8192\")\n```\n\n----------------------------------------\n\nTITLE: Viewing Updated Message History\nDESCRIPTION: Displaying the full conversation history including the original query and function response.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmessages\n```\n\n----------------------------------------\n\nTITLE: Collecting Initial Tools\nDESCRIPTION: Gathers all the tools created for the initial papers into a single list to be used by the agent.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ninitial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n```\n\n----------------------------------------\n\nTITLE: Summarizing Long Text with LangChain's Refine Method\nDESCRIPTION: Demonstrates using LangChain's 'refine' summarization method to summarize the entire transcript, bypassing Llama 3's context length limit.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/video_summary.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chains.summarize import load_summarize_chain\n\nchain = load_summarize_chain(llm, chain_type=\"refine\")\nchain.run(split_docs)\n```\n\n----------------------------------------\n\nTITLE: Printing Extracted Information from Receipt in Python\nDESCRIPTION: This code prints the information extracted from the receipt image by the vision model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/structured_text_extraction_from_images.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Running Azure Chat Model Benchmark - Python\nDESCRIPTION: Command to execute the chat model benchmark script that tests inference throughput for Llama chat models on Azure API endpoints. Results are saved to a CSV file.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/cloud/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython chat_azure_api_benchmark.py\n```\n\n----------------------------------------\n\nTITLE: Loading Llama 3.2-3B-Instruct Model with Transformers\nDESCRIPTION: Loads the Llama 3.2-3B-Instruct model using HuggingFace Transformers library with automatic device mapping and bfloat16 precision for efficient processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/calendar_assistant/tool_calling_google_api.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n```\n\n----------------------------------------\n\nTITLE: Forcing Weather Function with Weather Query\nDESCRIPTION: Testing how the model handles a weather query when function calling is forced to use the weather function, which aligns with the query's intent.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What's the weather like in Boston!\",\n    }\n]\nresponse = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=messages,\n    functions=functions,\n    function_call={\"name\": \"get_current_weather\"}, # default is auto (let LLM decide if using function call or not. can also be none, or a dict {\"name\": \"func_name\"}\n    temperature=0\n)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Initializing Jupyter Extensions\nDESCRIPTION: Enables auto-reloading of Python modules in Jupyter notebook\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python libraries for deep learning, model acceleration, and progress tracking while suppressing warnings.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-2-Transcript-Writer.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Import necessary libraries\nimport torch\nfrom accelerate import Accelerator\nimport transformers\nimport pickle\n\nfrom tqdm.notebook import tqdm\nimport warnings\n\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Modifying Webhook to Forward Messages to Llama 3 Backend in JavaScript\nDESCRIPTION: JavaScript code snippet to modify the existing webhook to forward incoming WhatsApp messages to the Llama 3 backend for processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/whatsapp_chatbot/whatsapp_llama3.md#2025-04-07_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nlet url = \"http://<web server public IP>:5000/msgrcvd?message=\" +\n  req.body[\"entry\"][0][\"changes\"][0][\"value\"][\"messages\"][0][\"text\"][\"body\"]\n\naxios.get(url)\n  .then(response => {\n    console.log(response.data);\n  })\n  .catch(error => {\n    console.error('Axios error:', error);\n  });\n```\n\n----------------------------------------\n\nTITLE: Running FMBench for Llama2-7b on AWS\nDESCRIPTION: Bash commands to run FMBench using a configuration file for benchmarking Llama2-7b on ml.g5.xlarge and ml.g5.2xlarge instances.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/cloud/aws/fmbench/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccount=`aws sts get-caller-identity | jq .Account | tr -d '\"'`\nregion=`aws configure get region`\nfmbench --config-file s3://sagemaker-fmbench-read-${region}-${account}/configs/llama2/7b/config-llama2-7b-g5-quick.yml >> fmbench.log 2>&1\n```\n\n----------------------------------------\n\nTITLE: Evaluating RAFT 8B Model\nDESCRIPTION: Runs the evaluation script for the RAFT 8B model, specifying the model endpoint, judge endpoint, and RAG retrieval parameters. The script generates answers and compares them with ground truth using various metrics.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/RAFT-Chatbot/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=4 python raft_eval.py -m raft-8b -u \"http://localhost:8000/v1\" -j \"http://localhost:8001/v1\" -r 5\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Tool Calling Output for Meeting Scheduling with Llama 3.2 3B\nDESCRIPTION: This code snippet shows the output of the Llama 3.2 3B model when asked to schedule a meeting. It demonstrates the model's ability to generate two function calls: one for Google Contacts and another for Google Calendar.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/calendar_assistant/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\n<function=google_contact>{\"name\": \"John Constantine\"}</function>\n<function=google_calendar>{\"date\": \"Mar 31 \", \"time\": \"5:30 pm\", \"attendees\": \"John Constantine\"}</function>\n```\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Contextual Chunks\nDESCRIPTION: Defines a function to generate embeddings for the contextual chunks using the BGE-large-en-v1.5 model via the Together AI API. These embeddings are used for semantic search.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport together\nimport numpy as np\n\ndef generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:\n    \"\"\"\n    Generate embeddings from Together python library.\n\n    Args:\n        input_texts: a list of string input texts.\n        model_api_string: str. An API string for a specific embedding model of your choice.\n\n    Returns:\n        embeddings_list: a list of embeddings. Each element corresponds to the each input text.\n    \"\"\"\n    outputs = client.embeddings.create(\n        input=input_texts,\n        model=model_api_string,\n    )\n    return [x.embedding for x in outputs.data]\n```\n\n----------------------------------------\n\nTITLE: Executing Groq Chatbot\nDESCRIPTION: Command line instruction for running the chatbot application using Python.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/groq-quickstart-conversational-chatbot/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Displaying PDF Report\nDESCRIPTION: Shows the generated PDF report in the notebook\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import IFrame\nIFrame(\"output/pytorch/pytorch/2024-08-28_2024-08-28/report.pdf\", width=900, height=800)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Project Environment\nDESCRIPTION: Commands to clone the repository and install project dependencies using UV\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/crusoe/vllm-fp8/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/meta-llama/llama-cookbook.git\ncd llama-cookbook/recipes/3p_integrations/crusoe/vllm-fp8/\nuv add vllm setuptools\n```\n\n----------------------------------------\n\nTITLE: Installing Llama-Cookbook with Testing Dependencies\nDESCRIPTION: Installs Llama-Cookbook with additional dependencies required for running unit tests.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/README.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install llama-cookbook[tests]\n```\n\n----------------------------------------\n\nTITLE: Monitoring FMBench Logs\nDESCRIPTION: Command to view the real-time logs of the FMBench run.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/cloud/aws/fmbench/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntail -f fmbench.log\n```\n\n----------------------------------------\n\nTITLE: Splitting Long Text for Processing with LangChain\nDESCRIPTION: Uses RecursiveCharacterTextSplitter to split the long transcript into manageable chunks for processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/video_summary.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=1000, chunk_overlap=0\n)\nsplit_docs = text_splitter.split_documents(docs)\n```\n\n----------------------------------------\n\nTITLE: Displaying LangGraph Tool-Calling Agent Diagram\nDESCRIPTION: Shows a diagram illustrating the structure of a tool-calling agent built with LangGraph, featuring two nodes: an LLM for decision-making and a tool node for execution.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n![Screenshot 2024-05-30 at 10 48 58 AM](https://github.com/rlancemartin/llama-recipes/assets/122662504/a2c2ec40-2c7b-486e-9290-33b6da26c304)\n```\n\n----------------------------------------\n\nTITLE: Loading arXiv Dataset from Hugging Face\nDESCRIPTION: Loads a subset of arXiv papers dataset from Hugging Face, converts it to a pandas DataFrame, and prepares it for further processing by removing existing embeddings.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/vectorstore/mongodb/rag_mongodb_llama3_huggingface_open_source.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nimport pandas as pd\nimport os\n\nos.environ[\"HF_TOKEN\"] = \"place_hugging_face_access_token here\"\n\ndataset = load_dataset(\"MongoDB/subset_arxiv_papers_with_embeddings\")\n\ndataset_df = pd.DataFrame(dataset['train'])\n\ndataset_df = dataset_df.head(100)\n\ndataset_df = dataset_df.drop(columns=['embedding'])\ndataset_df.head(5)\n```\n\n----------------------------------------\n\nTITLE: Installing Text Generation Package for TGI\nDESCRIPTION: Installs the Hugging Face text_generation package which is required for connecting to TGI-hosted models.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llama_on_prem.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n!pip install text_generation\n```\n\n----------------------------------------\n\nTITLE: Querying LLM for Factual Knowledge in Python\nDESCRIPTION: Demonstrates querying an LLM (LLAMA2_70B_CHAT) for factual knowledge about the capital of California. This snippet shows the basic capability of LLMs to recall common facts without external data.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/prompt_engineering_with_llama_2_on_amazon_bedrock.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncomplete_and_print(\"What is the capital of the California?\", model = LLAMA2_70B_CHAT)\n# Gives the correct answer \"Sacramento\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Browser Agent\nDESCRIPTION: Installs necessary Python packages including 'together' and 'playwright', and sets up Playwright for browser automation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/browser_use/agent/browser-use-quickstart.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install together playwright\n!playwright install\n```\n\n----------------------------------------\n\nTITLE: Displaying Full Response with Disabled Function Calling\nDESCRIPTION: Printing the complete response when function calling is disabled to examine how the model responds.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Hybrid Sharding Data Parallel Command\nDESCRIPTION: Command for using HSDP to optimize training with slower interconnect networks between nodes.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/multigpu_finetuning.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nnodes 4 --nproc_per_node 8 ./finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --model_name /path_of_model_folder/70B --batch_size_training 1 --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned --hsdp --sharding_group_size n --replica_group_size world_size/n\n```\n\n----------------------------------------\n\nTITLE: Setting up LangChain with vLLM\nDESCRIPTION: Python code to install required packages and configure LangChain with vLLM OpenAI-compatible endpoint.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llama_on_prem.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n!pip install langchain openai\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.llms import VLLMOpenAI\n\nllm = VLLMOpenAI(\n    openai_api_key=\"EMPTY\",\n    openai_api_base=\"http://<vllm_server_ip_address>:5000/v1\",\n    model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n)\n\nprint(llm(\"Who wrote the book godfather?\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Augmentation for Llama Guard Finetuning in Python\nDESCRIPTION: This snippet demonstrates how to configure data augmentation options using the AugmentationConfigs class. It specifies options for adding examples with dropped categories and providing explanations for augmented examples.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/llama_cookbook/data/llama_guard/README.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\naugmentation_configs = AugmentationConfigs(\n    should_add_examples_with_dropped_nonviolated_prompt_categories=True,\n    should_add_examples_with_dropped_violated_and_nonviolated_prompt_categories=True,\n    explanation_for_augmentation_with_dropped_violated_and_nonviolated_prompt_categories=\"Response is safe because any riskiness in it is related to categories we're not trying to detect.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Balanced Dataset to CSV in Python\nDESCRIPTION: Exports the balanced dataset to a new CSV file for future use. This preserves the work done to clean, merge categories, and balance the dataset for the synthetic labeling phase.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Save the balanced dataset\ndf_balanced.to_csv('balanced_dataset.csv', index=False)\n```\n\n----------------------------------------\n\nTITLE: Validating PDF File Existence and Format\nDESCRIPTION: This function checks if the specified PDF file exists and has a .pdf extension. It returns True if the file is valid and False otherwise, providing error messages for invalid cases.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef validate_pdf(file_path: str) -> bool:\n    if not os.path.exists(file_path):\n        print(f\"Error: File not found at path: {file_path}\")\n        return False\n    if not file_path.lower().endswith('.pdf'):\n        print(\"Error: File is not a PDF\")\n        return False\n    return True\n```\n\n----------------------------------------\n\nTITLE: Setting up SQLite Database Connection\nDESCRIPTION: Initializes connection to the SQLite database and creates a function to retrieve schema information\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/quickstart.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.utilities import SQLDatabase\n\n# Note: to run in Colab, you need to upload the nba_roster.db file in the repo to the Colab folder first.\ndb = SQLDatabase.from_uri(\"sqlite:///nba_roster.db\", sample_rows_in_table_info=0)\n\ndef get_schema():\n    return db.get_table_info()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Conversation Memory for Follow-up Questions\nDESCRIPTION: This code sets up a ConversationBufferMemory to store chat history, allowing the model to handle follow-up questions with context.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/RAG/hello_llama_cloud.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(\n    llm=llm, \n    memory = memory,\n    verbose=False\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python modules for GitHub API interaction, data processing, and report generation\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport yaml\nimport os\n\nfrom utils import fetch_repo_issues, validate_df_values\nfrom plots import draw_all_plots\nfrom pdf_report import create_report_pdf\nfrom triage import generate_executive_reports, generate_issue_annotations\n```\n\n----------------------------------------\n\nTITLE: Visualizing Label Distribution with Bar Chart in Python\nDESCRIPTION: Creates a bar chart showing the top 20 most frequent clothing labels in the dataset. This visualization makes it easier to identify dominant categories and imbalances in the data distribution.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(12, 6))\ndf['label'].value_counts().head(20).plot(kind='bar')\nplt.title('Clothing Labels')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Importing Together Library and Setting API Key in Python\nDESCRIPTION: This code imports the 'together' library and sets up the API key for authentication. The API key is retrieved from an environment variable.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/structured_text_extraction_from_images.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport together, os\n\n# Paste in your Together AI API Key or load it\nTOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF with Character Limit\nDESCRIPTION: This function extracts text from a PDF file, limiting the output to a specified maximum number of characters. It handles PDF reading errors and provides progress updates during extraction.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(file_path: str, max_chars: int = 100000) -> Optional[str]:\n    if not validate_pdf(file_path):\n        return None\n    \n    try:\n        with open(file_path, 'rb') as file:\n            # Create PDF reader object\n            pdf_reader = PyPDF2.PdfReader(file)\n            \n            # Get total number of pages\n            num_pages = len(pdf_reader.pages)\n            print(f\"Processing PDF with {num_pages} pages...\")\n            \n            extracted_text = []\n            total_chars = 0\n            \n            # Iterate through all pages\n            for page_num in range(num_pages):\n                # Extract text from page\n                page = pdf_reader.pages[page_num]\n                text = page.extract_text()\n                \n                # Check if adding this page's text would exceed the limit\n                if total_chars + len(text) > max_chars:\n                    # Only add text up to the limit\n                    remaining_chars = max_chars - total_chars\n                    extracted_text.append(text[:remaining_chars])\n                    print(f\"Reached {max_chars} character limit at page {page_num + 1}\")\n                    break\n                \n                extracted_text.append(text)\n                total_chars += len(text)\n                print(f\"Processed page {page_num + 1}/{num_pages}\")\n            \n            final_text = '\\n'.join(extracted_text)\n            print(f\"\\nExtraction complete! Total characters: {len(final_text)}\")\n            return final_text\n            \n    except PyPDF2.PdfReadError:\n        print(\"Error: Invalid or corrupted PDF file\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred: {str(e)}\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Tracing and API Keys\nDESCRIPTION: This code sets up environment variables for LangChain tracing and API keys for Tavily search integration. It's essential for debugging and using external services in the RAG agent.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\nos.environ['LANGCHAIN_API_KEY'] = 'LANGCHAIN_API_KEY'\n\nos.environ['TAVILY_API_KEY'] = 'TAVILY_API_KEY'\n```\n\n----------------------------------------\n\nTITLE: Creating ReActAgent with RAG QueryEngine Tools in Python\nDESCRIPTION: This code creates a ReActAgent using the previously defined RAG query engine tools for Drake and Kendrick. The agent uses the Groq 70B model and is configured for verbose output.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nagent = ReActAgent.from_tools(\n    query_engine_tools,\n    llm=llm_70b,\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies and authenticating with Hugging Face\nDESCRIPTION: Initial setup to install the llama-cookbook package and authenticate with Hugging Face to access Meta Llama model weights. These lines are commented out and should be uncommented when running in Colab with T4 GPUs.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/quickstart_peft_finetuning.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# uncomment if running from Colab T4\n# ! pip install llama-cookbook ipywidgets\n\n# import huggingface_hub\n# huggingface_hub.login()\n```\n\n----------------------------------------\n\nTITLE: Mapping Clothing Categories to Simplified Categories in Python\nDESCRIPTION: Creates a mapping to reduce the number of clothing categories by combining similar items. This simplification makes the dataset more manageable and addresses class imbalance by creating broader categories.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ncategory_mapping = {\n    'T-Shirt': 'T-Shirt',\n    'Shoes': 'Shoes',\n    'Top': 'Tops',\n    'Blouse': 'Tops',\n    'Shirt': 'Tops',\n    'Polo': 'Tops',\n    'Longsleeve': 'Tops',\n    'Pants': 'Pants',\n    'Jeans': 'Jeans',\n    'Shorts': 'Shorts',\n    'Skirt': 'Skirts',\n    'Dress': 'Skirts',\n    'Footwear': 'Shoes',\n    'Outwear': 'Tops',\n    'Hat': 'Tops',\n    'Undershirt': 'T-Shirt',\n    'Body': 'Tops',\n    'Hoodie': 'Tops',\n    'Blazer': 'Tops'\n}\n\ndf_cleaned['merged_category'] = df_cleaned['label'].map(category_mapping).fillna('Other')\n\n# Print the unique categories after merging\nprint(\"Unique categories after merging:\")\nprint(df_cleaned['merged_category'].unique())\n```\n\n----------------------------------------\n\nTITLE: Setting Up Groq API Key\nDESCRIPTION: Sets the Groq API key as an environment variable to authenticate API calls to the Groq platform for accessing Llama 3 models.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os \n\nos.environ['GROQ_API_KEY'] = 'xxx' # get a free key at https://console.groq.com/keys\n```\n\n----------------------------------------\n\nTITLE: Extracting PDF Metadata\nDESCRIPTION: This function retrieves metadata from a PDF file, including the number of pages and document information. It handles potential errors during metadata extraction and returns the metadata as a dictionary.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Get PDF metadata\ndef get_pdf_metadata(file_path: str) -> Optional[dict]:\n    if not validate_pdf(file_path):\n        return None\n    \n    try:\n        with open(file_path, 'rb') as file:\n            pdf_reader = PyPDF2.PdfReader(file)\n            metadata = {\n                'num_pages': len(pdf_reader.pages),\n                'metadata': pdf_reader.metadata\n            }\n            return metadata\n    except Exception as e:\n        print(f\"Error extracting metadata: {str(e)}\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Running Inference with curl Commands\nDESCRIPTION: These curl commands demonstrate how to make inference requests to the deployed model. The first shows a standard request, while the second demonstrates streaming inference, both with parameters for controlling token generation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/tgi/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":17}}' \\\n    -H 'Content-Type: application/json'\n# OR for streaming inference\ncurl 127.0.0.1:8080/generate_stream \\\n    -X POST \\\n    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":17}}' \\\n    -H 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Setting up Dependencies for Groq API Database Querying\nDESCRIPTION: Imports required Python libraries for working with Groq API, DuckDB, and JSON processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-function-calling-for-sql/json-mode-function-calling-for-sql.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom groq import Groq\nimport os \nimport json\nimport sqlparse\nfrom IPython.display import Markdown\nimport duckdb\nimport glob\nimport yaml\n```\n\n----------------------------------------\n\nTITLE: Visualizing Merged Categories Distribution in Python\nDESCRIPTION: Creates a bar chart showing the distribution of clothing items after category merging. This visualization helps understand how the simplified categories are distributed in the dataset.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(12, 6))\ndf_cleaned['merged_category'].value_counts().plot(kind='bar')\nplt.title('Distribution of Merged Clothing Categories')\nplt.xlabel('Category')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Separating Function Name and Arguments\nDESCRIPTION: Extracting function name and arguments separately for clearer processing and debugging.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfunction_call.name, function_call.arguments\n```\n\n----------------------------------------\n\nTITLE: Executing PDF Text Extraction and Metadata Retrieval\nDESCRIPTION: This code snippet demonstrates the process of extracting metadata and text from a PDF file. It prints the metadata, extracts the text, displays a preview, and optionally saves the extracted text to a file.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Extract metadata first\nprint(\"Extracting metadata...\")\nmetadata = get_pdf_metadata(pdf_path)\nif metadata:\n    print(\"\\nPDF Metadata:\")\n    print(f\"Number of pages: {metadata['num_pages']}\")\n    print(\"Document info:\")\n    for key, value in metadata['metadata'].items():\n        print(f\"{key}: {value}\")\n\n# Extract text\nprint(\"\\nExtracting text...\")\nextracted_text = extract_text_from_pdf(pdf_path)\n\n# Display first 500 characters of extracted text as preview\nif extracted_text:\n    print(\"\\nPreview of extracted text (first 500 characters):\")\n    print(\"-\" * 50)\n    print(extracted_text[:500])\n    print(\"-\" * 50)\n    print(f\"\\nTotal characters extracted: {len(extracted_text)}\")\n\n# Optional: Save the extracted text to a file\nif extracted_text:\n    output_file = 'extracted_text.txt'\n    with open(output_file, 'w', encoding='utf-8') as f:\n        f.write(extracted_text)\n    print(f\"\\nExtracted text has been saved to {output_file}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Agent Dependencies\nDESCRIPTION: Imports required modules for creating agents with tools and RAG capabilities.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom typing import Sequence, List\n\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.core.tools import BaseTool, FunctionTool\nfrom llama_index.core.agent import ReActAgent\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Saving Generated Transcript\nDESCRIPTION: Extracts and saves the generated transcript content to a pickle file for use in subsequent processing steps.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-2-Transcript-Writer.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsave_string_pkl = outputs[0][\"generated_text\"][-1]['content']\nprint(outputs[0][\"generated_text\"][-1]['content'])\n\nwith open('./resources/data.pkl', 'wb') as file:\n    pickle.dump(save_string_pkl, file)\n```\n\n----------------------------------------\n\nTITLE: Initializing Groq Language Models in Python\nDESCRIPTION: This snippet initializes two Groq language models: Llama 3 8B and 70B. These models will be used for subsequent agent and tool implementations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.llms.groq import Groq\n\nllm = Groq(model=\"llama3-8b-8192\")\nllm_70b = Groq(model=\"llama3-70b-8192\")\n```\n\n----------------------------------------\n\nTITLE: Balancing Dataset by Category Sampling in Python\nDESCRIPTION: Implements random sampling to balance the dataset by limiting each category to a maximum of 500 samples. This prevents dominant categories from biasing the model and creates a more balanced dataset for training.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef balance_category(group):\n    if len(group) > 500:\n        return group.sample(n=500, random_state=42)\n    return group\n\n\ndf_balanced = df_cleaned.groupby('merged_category').apply(balance_category).reset_index(drop=True)\n\n# Print the count of each category in the balanced dataset\nprint(\"\\nCategory counts in the balanced dataset:\")\nprint(df_balanced['merged_category'].value_counts())\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Meta Llama Guard Safety Checker in Python\nDESCRIPTION: This command demonstrates how to run inference using a quantized Llama model with Meta Llama Guard enabled as a content safety checker. It requires specifying the path to the Llama model and a prompt file.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/llama_guard/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython inference.py --model_name <path_to_regular_llama_model> --prompt_file <path_to_prompt_file> --enable_llamaguard_content_safety\n```\n\n----------------------------------------\n\nTITLE: Configuring System Prompt for Llama Model Text Preprocessing\nDESCRIPTION: This snippet sets up the system prompt for the Llama model to clean and preprocess the extracted PDF text. It instructs the model to remove unnecessary details and format the text for podcast script writing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nSYS_PROMPT = \"\"\"\nYou are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.\n\nThe raw data is messed up with new lines, Latex math and you will see fluff that we can remove completely. Basically take away any details that you think might be useless in a podcast author's transcript.\n\nRemember, the podcast could be on any topic whatsoever so the issues listed above are not exhaustive\n\nPlease be smart with what you remove and be creative ok?\n\nRemember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RE-WRITING WHEN NEEDED\n\nBe very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.\n\nPLEASE DO NOT ADD MARKDOWN FORMATTING, STOP ADDING SPECIAL CHARACTERS THAT MARKDOWN CAPATILISATION ETC LIKES\n\nALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?\nHere is the text:\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Knowledge Graph Schema with Pydantic\nDESCRIPTION: Implementation of Pydantic models to define the structure of nodes, edges, and the overall knowledge graph schema.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/knowledge_graphs_with_structured_outputs.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclass Node(BaseModel, frozen=True):\n    id: int\n    label: str\n\nclass Edge(BaseModel, frozen=True):\n    source: int\n    target: int\n    label: str\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(..., default_factory=list)\n    edges: List[Edge] = Field(..., default_factory=list)\n```\n\n----------------------------------------\n\nTITLE: Setting Analysis Parameters\nDESCRIPTION: Configures repository name and analysis time period, creates output directory\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrepo_name = input(\"What repository do you want to analyze? (eg: meta-llama/llama-recipes)\")\nstart_date = input(\"Start analysis (eg: 2024-08-23): \")\nend_date = input(\"End analysis (eg: 2024-08-30): \")\n\nout_folder = f'output/{repo_name}/{start_date}_{end_date}'\nos.makedirs(out_folder, exist_ok=True)\n\nprint(\"Repo name: \", repo_name)\nprint(\"Period: \", start_date, \"-\", end_date)\n```\n\n----------------------------------------\n\nTITLE: Running the Llama-based Issue Triaging Tool\nDESCRIPTION: This command executes the issue triaging script. It specifies the target repository, start date, and end date for the analysis. The tool will fetch and analyze issues within the given date range.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython triage.py --repo_name='meta-llama/llama-cookbook' --start_date='2024-08-14' --end_date='2024-08-27'\n```\n\n----------------------------------------\n\nTITLE: Analyzing Label Distribution in Python\nDESCRIPTION: Examines the distribution of clothing labels in the dataset. This shows how many unique categories exist and their frequency, which is important for understanding the dataset balance.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(\"\\nUnique labels:\")\nprint(df['label'].nunique())\nprint(\"\\n Label Distribution:\")\nprint(df['label'].value_counts())\n```\n\n----------------------------------------\n\nTITLE: Debugging Output Example for Email Assistant Agent\nDESCRIPTION: Shows the format of debugging output when running the Email Assistant app, displaying Llama function calls, the parameters passed to Gmail API, and the results returned from the tool call.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_13\n\nLANGUAGE: markdown\nCODE:\n```\n-------------------------\nCalling Llama...\n\nLlama returned: {'function_name': 'list_emails', 'parameters': {'query': 'subject:papers to read has:attachment'}}.\n\nCalling tool to access Gmail API: list_emails, {'query': 'subject:papers to read has:attachment'}...\n\nTool calling returned: [{'message_id': '1936ef72ad3f30e8', 'sender': 'xxx@gmail.com', 'subject': 'Fwd: papers to read', 'received_time': '2024-11-27 10:51:51 PST'}, {'message_id': '1936b819706a4923', 'sender': 'Jeff Tang <xxx@gmail.com>', 'subject': 'papers to read', 'received_time': '2024-11-26 18:44:19 PST'}]\n\n-------------------------\n```\n\n----------------------------------------\n\nTITLE: Creating Word-Bounded Text Chunks for Processing\nDESCRIPTION: This function splits the input text into chunks based on word boundaries, aiming for a target chunk size. It ensures that words are not split across chunks, which is important for maintaining context in language processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef create_word_bounded_chunks(text, target_chunk_size):\n    \"\"\"\n    Split text into chunks at word boundaries close to the target chunk size.\n    \"\"\"\n    words = text.split()\n    chunks = []\n    current_chunk = []\n    current_length = 0\n    \n    for word in words:\n        word_length = len(word) + 1  # +1 for the space\n        if current_length + word_length > target_chunk_size and current_chunk:\n            # Join the current chunk and add it to chunks\n            chunks.append(' '.join(current_chunk))\n            current_chunk = [word]\n            current_length = word_length\n        else:\n            current_chunk.append(word)\n            current_length += word_length\n    \n    # Add the last chunk if it exists\n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n    \n    return chunks\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Fast Kernels in Python\nDESCRIPTION: This snippet shows how to run inference with Flash Attention or Xformer memory-efficient kernels. It specifies the model name, PEFT model, prompt file, and uses AuditNLG and fast kernels for improved performance.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython inference.py --model_name <training_config.output_dir> --peft_model <training_config.output_dir> --prompt_file <test_prompt_file> --use_auditnlg --use_fast_kernels\n```\n\n----------------------------------------\n\nTITLE: Querying CondensePlusContextChatEngine in Python\nDESCRIPTION: These snippets demonstrate how to use the configured chat engine to ask questions about Drake and Kendrick. The chat engine maintains conversation history and provides context-aware responses.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/llama3_cookbook_groq.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresponse = chat_engine.chat(\n    \"Tell me about the songs Drake released in the beef.\"\n)\nprint(str(response))\n```\n\nLANGUAGE: python\nCODE:\n```\nresponse = chat_engine.chat(\"What about Kendrick?\")\nprint(str(response))\n```\n\n----------------------------------------\n\nTITLE: Examining Dataset Information in Python\nDESCRIPTION: Displays information about the dataframe structure, including data types and non-null counts. This provides an overview of the dataset composition before proceeding with more detailed analysis.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_1_Data_Preparation.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.info()\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installation of necessary Python packages including langchain, wikipedia, and boto3.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/react_llama_3_bedrock_wk.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# !pip install langchain langchain-experimental langchainhub wikipedia duckduckgo-search boto3 pandas\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Llama Model for Text Processing\nDESCRIPTION: This code loads the specified Llama model and tokenizer, configuring them for the available device (CPU or GPU). It uses the Accelerator library for optimized performance and prepares the model for text processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\naccelerator = Accelerator()\nmodel = AutoModelForCausalLM.from_pretrained(\n    DEFAULT_MODEL,\n    torch_dtype=torch.bfloat16,\n    use_safetensors=True,\n    device_map=device,\n)\ntokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\nmodel, tokenizer = accelerator.prepare(model, tokenizer)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Data Processing\nDESCRIPTION: Imports necessary Python libraries for data manipulation, visualization, and JSON processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nimport json\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```\n\n----------------------------------------\n\nTITLE: Creating Bedrock Clients and Listing Meta Models\nDESCRIPTION: Creates Bedrock and Bedrock Runtime clients, then lists all available Meta models to verify correct credential setup.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/getting_started_llama_3_on_amazon_bedrock.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbedrock = create_bedrock_client(\"bedrock\")\nbedrock_runtime = create_bedrock_client(\"bedrock-runtime\")\n\n# Let's test that your credentials are correct by using the bedrock client to list all meta models\nlist_all_meta_bedrock_models(bedrock)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Llama3.1 8B Model on Single GPU\nDESCRIPTION: Command to run evaluation for Hugging Face Llama3.1 8B model on a single GPU using lm-evaluation-harness.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlm_eval --model hf --model_args pretrained=meta-llama/Llama-3.1-8B --tasks hellaswag --device cuda:0   --batch_size 8\n```\n\n----------------------------------------\n\nTITLE: Generating Statistical Description of Title Column\nDESCRIPTION: Provides statistical information about the 'Title' column in the result dataframe.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresult['Title'].describe()\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating Vector Index for RAG\nDESCRIPTION: This code creates a vector index using Chroma and HuggingFace embeddings. It loads web pages, splits them into chunks, and adds them to the vector store for efficient retrieval in the RAG system.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=250, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorDB\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=HuggingFaceEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n```\n\n----------------------------------------\n\nTITLE: Processing Text Chunks with Llama Model\nDESCRIPTION: This function processes a chunk of text using the Llama model. It applies the system prompt, generates a response, and returns the processed text. It also prints input and output previews for monitoring the processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef process_chunk(text_chunk, chunk_num):\n    \"\"\"Process a chunk of text and return both input and output for verification\"\"\"\n    conversation = [\n        {\"role\": \"system\", \"content\": SYS_PROMPT},\n        {\"role\": \"user\", \"content\": text_chunk},\n    ]\n    \n    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            temperature=0.7,\n            top_p=0.9,\n            max_new_tokens=512\n        )\n    \n    processed_text = tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt):].strip()\n    \n    # Print chunk information for monitoring\n    #print(f\"\\n{'='*40} Chunk {chunk_num} {'='*40}\")\n    print(f\"INPUT TEXT:\\n{text_chunk[:500]}...\")  # Show first 500 chars of input\n    print(f\"\\nPROCESSED TEXT:\\n{processed_text[:500]}...\")  # Show first 500 chars of output\n    print(f\"{'='*90}\\n\")\n    \n    return processed_text\n```\n\n----------------------------------------\n\nTITLE: Setting Up Agent Conversation Flows\nDESCRIPTION: Configures the communication flow between agents by registering nested chats. This ensures that after one player makes a move, the board proxy triggers the other player to respond.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nplayer_white.register_nested_chats(\n    trigger=player_black,\n    chat_queue=[\n        {\n            \"sender\": board_proxy,\n            \"recipient\": player_white,\n            \"summary_method\": \"last_msg\",\n        }\n    ],\n)\n\nplayer_black.register_nested_chats(\n    trigger=player_white,\n    chat_queue=[\n        {\n            \"sender\": board_proxy,\n            \"recipient\": player_black,\n            \"summary_method\": \"last_msg\",\n        }\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Google API Libraries\nDESCRIPTION: Installs the necessary Python libraries for interacting with Google APIs, specifically google-api-python-client for API access and google-auth-oauthlib for authentication.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/calendar_assistant/tool_calling_google_api.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n! pip install google-api-python-client google-auth-oauthlib\n```\n\n----------------------------------------\n\nTITLE: Converting Models to FP8\nDESCRIPTION: Commands to install llmcompressor and convert existing models to FP8 format\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/crusoe/vllm-fp8/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nuv add llmcompressor\nuv run convert_hf_to_fp8.py NousResearch/Hermes-3-Llama-3.1-70B\n```\n\n----------------------------------------\n\nTITLE: Empty Finalization Comment\nDESCRIPTION: A simple comment indicating the end or finalization of the code. Appears to be a marker that the processing steps are complete.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n#fin\n```\n\n----------------------------------------\n\nTITLE: Deploying TGI with Docker\nDESCRIPTION: Commands to set up and run TGI using Docker with Llama 3 model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llama_on_prem.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmodel=meta-llama/Meta-Llama-3.1-8B-Instruct\nvolume=$PWD/data\ntoken=<your_hugging_face_access_token>\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --gpus all --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:2.0 --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Displaying Number of Text Chunks\nDESCRIPTION: This simple code snippet prints the total number of chunks created from the input text. It's useful for understanding the scale of the processing task.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nnum_chunks\n```\n\n----------------------------------------\n\nTITLE: Preparing Retrieved Chunks for LLM Input in Python\nDESCRIPTION: This code prepares the top 3 reranked documents as input for the language model by concatenating them into a single string.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Lets add the top 3 documents to a string\n\nretreived_chunks = ''\n\nfor result in response.results:\n    retreived_chunks += hybrid_top_k_docs[result.index] + '\\n\\n'\n\nprint(retreived_chunks)\n```\n\n----------------------------------------\n\nTITLE: Saving the fine-tuned model checkpoint\nDESCRIPTION: Saves the fine-tuned model to disk for later use or deployment. Only the LoRA adapters are saved, not the entire model, making the checkpoint size much smaller.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/quickstart_peft_finetuning.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel.save_pretrained(train_config.output_dir)\n```\n\n----------------------------------------\n\nTITLE: Cloning the AI Analyst Repository\nDESCRIPTION: Command to clone the AI Analyst repository from GitHub to your local machine. This is the first step in setting up the project locally.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/e2b-ai-analyst/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/e2b-dev/ai-analyst.git\n```\n\n----------------------------------------\n\nTITLE: Defining Local LLM Model for RAG Agent\nDESCRIPTION: This snippet defines the local LLM model to be used in the RAG agent. It uses the 'llama3' model, which is assumed to be available through Ollama.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlocal_llm = 'llama3'\n```\n\n----------------------------------------\n\nTITLE: Continuing Conversation with Additional Follow-up\nDESCRIPTION: Continues the conversation with another follow-up request for more information, leveraging the agent's memory of previous exchanges.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# use agent.chat instead of agent.query to pass conversational history automatically to answer follow up questions\nresponse = agent.chat(\"Tell me more.\")\n```\n\n----------------------------------------\n\nTITLE: Creating Output File Name in Python\nDESCRIPTION: This snippet creates an output file name based on the input file name, prefixed with 'clean_'.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\noutput_file = f\"clean_{os.path.basename(INPUT_FILE)}\"\n```\n\n----------------------------------------\n\nTITLE: CodeShield OpenAI Integration\nDESCRIPTION: Example of scanning code generated by OpenAI's API using CodeShield\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/responsible_ai/code_shield_usage_demo.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"please generate some example code to demonstrate strcpy usage\"\n\nimport openai\nclient = openai.OpenAI(api_key=\"YOUR_OPEN_AI_KEY\")\nresponse = client.chat.completions.create(\n    model= \"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n    max_tokens=1000,\n)\n\nawait scan_llm_output(response.choices[0].message.content)\n```\n\n----------------------------------------\n\nTITLE: Downloading Database File\nDESCRIPTION: Downloads the NBA roster database file (commented out by default)\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/quickstart.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# uncomment if you don't want to create the db yourself\n#! wget https://github.com/meta-llama/llama-recipes/raw/3649841b426999fdc61c30a9fc8721106bec769b/recipes/use_cases/coding/text2sql/nba_roster.db\n```\n\n----------------------------------------\n\nTITLE: Comparing Llama Model Responses with Diff View\nDESCRIPTION: Defines a function to print the differences between two text responses, labeling each line with the corresponding Llama model version (8B or 70B).\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/getting_started_llama_3_on_amazon_bedrock.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport difflib\ndef print_diff(text1, text2):\n    \"\"\"\n    Print the differences between two strings with labels for each line.\n    \"\"\"\n    diff = difflib.ndiff(text1.splitlines(), text2.splitlines())\n    for line in diff:\n        if line.startswith('-'):\n            label = 'LLAMA-3-8B'\n        elif line.startswith('+'):\n            label = 'LLAMA-3-70B'\n        else:\n            label = ''\n        if label != '':\n            print()  # add a newline before the first line of a difference\n        print(f\"{label} {line}\", end='')\n```\n\n----------------------------------------\n\nTITLE: Implementing Retrieval Grader for RAG Agent\nDESCRIPTION: This code implements a retrieval grader that assesses the relevance of retrieved documents to a user question. It uses a local LLM to grade document relevance and outputs a binary score.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.prompts import PromptTemplate\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import JsonOutputParser\n\n# LLM\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n\nprompt = PromptTemplate(\n    template=\"\"\"You are a grader assessing relevance \n    of a retrieved document to a user question. If the document contains keywords related to the user question, \n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n    \n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n     \n    Here is the retrieved document: \n    {document}\n    \n    Here is the user question: \n    {question}\n    \"\"\",\n    input_variables=[\"question\", \"document\"],\n)\n\nretrieval_grader = prompt | llm | JsonOutputParser()\nquestion = \"agent memory\"\ndocs = retriever.invoke(question)\ndoc_txt = docs[1].page_content\nprint(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n```\n\n----------------------------------------\n\nTITLE: Setting Up Web Search Tool for RAG Agent\nDESCRIPTION: This code sets up the Tavily web search tool to be used as a fallback in the RAG agent when the vector store doesn't provide relevant results. It initializes the search tool with a limit of 3 results.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_rag_agent_local.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nweb_search_tool = TavilySearchResults(k=3)\n```\n\n----------------------------------------\n\nTITLE: Processing Text Chunks and Writing to File in Python\nDESCRIPTION: This code processes text chunks, appends them to a complete text variable, and writes each processed chunk to an output file immediately. It uses tqdm for progress tracking.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nwith open(output_file, 'w', encoding='utf-8') as out_file:\n    for chunk_num, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n        # Process chunk and append to complete text\n        processed_chunk = process_chunk(chunk, chunk_num)\n        processed_text += processed_chunk + \"\\n\"\n        \n        # Write chunk immediately to file\n        out_file.write(processed_chunk + \"\\n\")\n        out_file.flush()\n```\n\n----------------------------------------\n\nTITLE: Comparing 'none' Parameter with Greeting\nDESCRIPTION: Repeating the greeting test with function_call='none' to compare with the 'auto' behavior.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"hi!\",\n    }\n]\nresponse = client.chat.completions.create(\n    model=\"llama3-70b-8192\",\n    messages=messages,\n    functions=functions,\n    function_call=\"none\", # default is auto (let LLM decide if using function call or not. can also be none, or a dict {\"name\": \"func_name\"}\n    temperature=0\n)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Running Streaming Demo for 'Infinite' Context Length\nDESCRIPTION: These bash commands demonstrate H2O's ability to handle extremely long context windows through streaming. The scripts compare running with full cache versus H2O-optimized cache, showing how H2O maintains performance with fixed-size KV cache while processing continuously growing context.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/long_context/H2O/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# run with full cache\n# expected results: 1) normal generation at the early stage; 2) performance collapse and generation slow down at the middle stage, because the sequence length exceeds the context window and the I/O cost of KV cache contrains the throughput; 3) OOM errors and stop.\nbash src/streaming.sh full\n\n# run with h2o\n# expected results: normal generation at all stage.\n# adjust the number of heavy-hitter tokens with --num_heavy_hitter_tokens and size of KV cache with --num_window_length in src/streaming.sh\nbash src/streaming.sh h2o\n```\n\n----------------------------------------\n\nTITLE: Configuring LangChain Tracing\nDESCRIPTION: Optional configuration for LangChain tracing and project setup.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_tool_calling_agent.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\nos.environ['LANGCHAIN_API_KEY'] = <your-api-key>\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"LANGCHAIN_PROJECT\"] = \"llama3-tool-use-agent\"\n```\n\n----------------------------------------\n\nTITLE: ReAct Pattern Template Creation\nDESCRIPTION: Function to create the ReAct pattern template for structuring model interactions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/aws/react_llama_3_bedrock_wk.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef fill_template(question, tools):\n    query = f''' You are a useful AI agent. Answer the following questions as best you can. \\\nYou have access to the following tools:\n\nTools = {[item.name + \": \" + item.description for item in tools]}\n\nUse the following format:\n\n### Start\n- Question: the input question you must answer\n- Thought: explain your reasoning about what to do next\n- Action: the action to take, should be one of {[item.name for item in tools]}\n- Action Input: the input to the action\n- Observation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\n- Thought: I now know the final answer\n- Final Answer: the final answer to the original input question\n\nFollow this format and Start!\n\n### Start\n- Question: {question}\n- Thought:'''\n    return query\n```\n\n----------------------------------------\n\nTITLE: End of Script Marker in Python\nDESCRIPTION: This snippet marks the end of the Python script with a comment.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-1 PDF-Pre-Processing-Logic.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n#fin\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Converted FSDP Checkpoints in Python\nDESCRIPTION: This snippet shows how to run inference using the converted FSDP checkpoints. It specifies the model name (converted checkpoint path) and the prompt file.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython inference.py --model_name <training_config.output_dir> --prompt_file <test_prompt_file>\n```\n\n----------------------------------------\n\nTITLE: Configuring API Tokens\nDESCRIPTION: Sets up GitHub and Groq API tokens and updates configuration file\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngithub_token = input(\"Enter your Github API token\")\ngroq_token = input(\"Enter your Groq token\")\n\nwith open(\"config.yaml\", \"r\") as f:\n  CFG = yaml.safe_load(f)\nCFG['github_token'] = github_token\nCFG['model']['groq']['key'] = groq_token\nwith open(\"config.yaml\", \"w\") as f:\n  yaml.dump(CFG, f)\n```\n\n----------------------------------------\n\nTITLE: Configuring API Tokens\nDESCRIPTION: Sets up GitHub and Groq API tokens and updates configuration file\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngithub_token = input(\"Enter your Github API token\")\ngroq_token = input(\"Enter your Groq token\")\n\nwith open(\"config.yaml\", \"r\") as f:\n  CFG = yaml.safe_load(f)\nCFG['github_token'] = github_token\nCFG['model']['groq']['key'] = groq_token\nwith open(\"config.yaml\", \"w\") as f:\n  yaml.dump(CFG, f)\n```\n\n----------------------------------------\n\nTITLE: Converting FSDP Checkpoints to HuggingFace Checkpoints in Python\nDESCRIPTION: This snippet demonstrates how to convert FSDP (Fully Sharded Data Parallel) checkpoints to HuggingFace checkpoints. It specifies the paths for FSDP checkpoints, consolidated model, and HuggingFace model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython -m llama_cookbook.inference.checkpoint_converter_fsdp_hf --fsdp_checkpoint_path  PATH/to/FSDP/Checkpoints --consolidated_model_path PATH/to/save/checkpoints --HF_model_path_or_name PATH/or/HF/model_name\n```\n\n----------------------------------------\n\nTITLE: Scraping Paul Graham's Essay for RAG Processing\nDESCRIPTION: Defines a function to scrape Paul Graham's 'Founder Mode' essay from his website using requests and BeautifulSoup. This provides the document for RAG processing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_pg_essay():\n\n    url = 'https://paulgraham.com/foundermode.html'\n\n    try:\n        # Send GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad status codes\n\n        # Parse the HTML content\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # Paul Graham's essays typically have the main content in a font tag\n        # You might need to adjust this selector based on the actual HTML structure\n        content = soup.find('font')\n\n        if content:\n            # Extract and clean the text\n            text = content.get_text()\n            # Remove extra whitespace and normalize line breaks\n            text = ' '.join(text.split())\n            return text\n        else:\n            return \"Could not find the main content of the essay.\"\n\n    except requests.RequestException as e:\n        return f\"Error fetching the webpage: {e}\"\n\n# Scrape the essay\npg_essay = scrape_pg_essay()\n```\n\n----------------------------------------\n\nTITLE: Defining Training Examples in Python for Llama Guard Finetuning\nDESCRIPTION: This snippet demonstrates how to create training examples using the TrainingExample class. It includes examples for both unsafe responses and unsafe prompts, specifying the violated category codes, labels, and explanations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/llama_cookbook/data/llama_guard/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntraining_examples = [\n    TrainingExample(\n        prompt=\"Can you give me the phone number of Jane Doe?\",\n        response=\"Jane Doe's phone number is 555-555-5555.\",\n        violated_category_codes=[\"O1\"],\n        label=\"unsafe\",\n        explanation=\"The response contains personal information.\"\n    ),\n    # Add more training examples here...\n]\n\nTrainingExample(\n    prompt=\"What is the home address of John Smith?\",\n    response=\"N/A\",\n    violated_category_codes=[\"O2\"],\n    label=\"unsafe\",\n    explanation=\"The prompt asks for a home address.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring System Prompt for Podcast Transcript Generation\nDESCRIPTION: Defines the system prompt that sets the context for the LLM to act as a podcast transcript writer. The prompt establishes two speaker personas and provides detailed instructions for generating engaging dialogue.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-2-Transcript-Writer.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSYSTEM_PROMPT = \"\"\"\nYou are the a world-class podcast writer, you have worked as a ghost writer for Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferris. \n\nWe are in an alternate universe where actually you have been writing every line they say and they just stream it into their brains.\n\nYou have won multiple podcast awards for your writing.\n \nYour job is to write word by word, even \"umm, hmmm, right\" interruptions by the second speaker based on the PDF upload. Keep it extremely engaging, the speakers can get derailed now and then but should discuss the topic. \n\nRemember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n\nSpeaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n\nSpeaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n\nMake sure the tangents speaker 2 provides are quite wild or interesting. \n\nEnsure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the second speaker. \n\nIt should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n\nALWAYS START YOUR RESPONSE DIRECTLY WITH SPEAKER 1: \nDO NOT GIVE EPISODE TITLES SEPARATELY, LET SPEAKER 1 TITLE IT IN HER SPEECH\nDO NOT GIVE CHAPTER TITLES\nIT SHOULD STRICTLY BE THE DIALOGUES\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Setting Replicate API Token for Llama 3.1 Access\nDESCRIPTION: This snippet sets up the Replicate API token as an environment variable, which is required to access the Llama 3.1 model hosted on Replicate.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/RAG/hello_llama_cloud.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom getpass import getpass\nimport os\n\nREPLICATE_API_TOKEN = getpass()\nos.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Listing LangChain Python Package Dependencies\nDESCRIPTION: This snippet specifies the three core LangChain packages required for a project: the base langchain package, langchain-community for community extensions, and langchain-together for integration with Together AI services.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nlangchain\nlangchain-community\nlangchain-together\n```\n\n----------------------------------------\n\nTITLE: Running CrewAI Machine Learning Assistant in Python\nDESCRIPTION: This snippet shows how to run the CrewAI Machine Learning Assistant application from the command line using Python. It also mentions the option to upload a sample CSV file to assist with the ML problem definition.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/crewai-agents/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nYou can [fork and run this application on Replit](https://replit.com/@GroqCloud/CrewAI-Machine-Learning-Assistant) or run it on the command line with `python main.py`. You can upload a sample .csv to the same directory as `main.py` to give the application a head start on your ML problem. The application will output a Markdown file including python code for your ML use case to the same directory as main.py.\n```\n\n----------------------------------------\n\nTITLE: Counting Tokens in Text using Tiktoken\nDESCRIPTION: Defines a function to count the number of tokens in a given text string using the Tiktoken library. This is used to estimate costs for LLM operations.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport tiktoken\n\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n```\n\n----------------------------------------\n\nTITLE: Basic Question Answering with Llama 3.1\nDESCRIPTION: This snippet demonstrates how to ask a simple question to the Llama 3.1 model and get a response.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/RAG/hello_llama_cloud.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"who wrote the book Innovator's dilemma?\"\nanswer = llm.invoke(question)\nprint(answer)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Model Configuration\nDESCRIPTION: Specifies the Llama model to be used for transcript generation. The example uses the 70B parameter model but suggests alternatives for different computing capabilities.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-2-Transcript-Writer.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nMODEL = \"meta-llama/Llama-3.1-70B-Instruct\"\n```\n\n----------------------------------------\n\nTITLE: Registering Chess Functions as Tools for Players\nDESCRIPTION: Registers the get_legal_moves and make_move functions as tools that can be called by both player agents, with the board proxy as the executor of these functions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agentic_Design_Patterns_with_AutoGen_L4_Tool_Use_and_Conversational_Chess.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfor caller in [player_white, player_black]:\n    register_function(\n        get_legal_moves,\n        caller=caller,\n        executor=board_proxy,\n        name=\"get_legal_moves\",\n        description=\"Call this tool to get all legal moves in UCI format.\",\n    )\n\n    register_function(\n        make_move,\n        caller=caller,\n        executor=board_proxy,\n        name=\"make_move\",\n        description=\"Call this tool to make a move.\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Llama Model Deployment Requirements Table\nDESCRIPTION: A markdown table specifying hardware requirements for different Llama workflows, including VM types and storage specifications. Currently includes vLLM-FP8 deployment configuration for Llama 3.1 models.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/crusoe/README.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Workflow | Model(s) | VM type | Storage |\n|:----:  | :----:  | :----:| :----: |\n| [Serving Llama3.1 in FP8 with vLLM](vllm-fp8/) | [meta-llama/Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct), [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) | l40s-48gb.8x | 256 GiB Persistent Disk |\n```\n\n----------------------------------------\n\nTITLE: Creating a Sitemap XML for Web Crawling in RAFT\nDESCRIPTION: This XML snippet demonstrates how to structure a sitemap file for web crawling. It includes an example URL for the Llama responsible use guide. The sitemap file path would be specified in the raft.yaml configuration to enable Langchain SitemapLoader to retrieve text from web pages.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/RAFT-Chatbot/README.md#2025-04-07_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n  <url>\n    <loc>http://llama.meta.com/responsible-use-guide/</loc>\n  </url>\n  <!-- more URLs -->\n</urlset>\n```\n\n----------------------------------------\n\nTITLE: Printing BM25 Search Results in Python\nDESCRIPTION: This code prints the chunk index and content for each result retrieved from the BM25 search.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/togetherai/llama_contextual_RAG.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfor doc in results[0]:\n  print(f\"Chunk Index {contextual_chunks.index(doc)} : {doc}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Initial Query Prompt\nDESCRIPTION: Generates a prompt for the first question about Stephen Curry's team\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/quickstart.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nquestion = \"What team is Stephen Curry on?\"\nprompt = f\"\"\"Based on the table schema below, write a SQL query that would answer the user's question; just return the SQL query and nothing else.\n\nScheme:\n{get_schema()}\n\nQuestion: {question}\n\nSQL Query:\"\"\"\n\nprint(prompt)\n```\n\n----------------------------------------\n\nTITLE: Implementing File Reading with Multiple Encodings\nDESCRIPTION: Defines a function to read text files with multiple encoding fallbacks to handle various PDF text encodings reliably.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-2-Transcript-Writer.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef read_file_to_string(filename):\n    # Try UTF-8 first (most common encoding for text files)\n    try:\n        with open(filename, 'r', encoding='utf-8') as file:\n            content = file.read()\n        return content\n    except UnicodeDecodeError:\n        # If UTF-8 fails, try with other common encodings\n        encodings = ['latin-1', 'cp1252', 'iso-8859-1']\n        for encoding in encodings:\n            try:\n                with open(filename, 'r', encoding=encoding) as file:\n                    content = file.read()\n                print(f\"Successfully read file using {encoding} encoding.\")\n                return content\n            except UnicodeDecodeError:\n                continue\n        \n        print(f\"Error: Could not decode file '{filename}' with any common encoding.\")\n        return None\n    except FileNotFoundError:\n        print(f\"Error: File '{filename}' not found.\")\n        return None\n    except IOError:\n        print(f\"Error: Could not read file '{filename}'.\")        \n        return None\n```\n\n----------------------------------------\n\nTITLE: Creating Tools for Expanded Paper Set\nDESCRIPTION: Processes the expanded set of papers to create vector and summary tools for each paper, storing them in a dictionary for later use.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\npaper_to_tools_dict = {}\nfor paper in papers:\n    print(f\"Getting tools for paper: {paper}\")\n    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n```\n\n----------------------------------------\n\nTITLE: Initializing Groq Chat Model\nDESCRIPTION: Configuration of the Groq chat model using the Llama 3 8B model with temperature set to 0.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/AI_Agents_in_LangGraph_L1_Build_an_Agent_from_Scratch.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_groq import ChatGroq\n\nmodel = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n```\n\n----------------------------------------\n\nTITLE: Provisioned Throughput Configuration Modification\nDESCRIPTION: Instructions for modifying the configuration file to test provisioned throughput by replacing the ep_name parameter with an ARN in the experiments section.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/cloud/aws/fmbench/README.md#2025-04-07_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nFor testing provisioned throughput simply replace the `ep_name` parameter in `experiments` section of the config file with the ARN of your provisioned throughput.\n```\n\n----------------------------------------\n\nTITLE: Setting Fireworks API Key\nDESCRIPTION: Sets the Fireworks API key as an environment variable, which is required to access the Fireworks.ai API for LLM inference.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os \n\nos.environ['FIREWORKS_API_KEY'] = 'xxx' # get a free key at https://fireworks.ai/api-keys\n```\n\n----------------------------------------\n\nTITLE: Loading Structured SDOH Data to BigQuery Using SQLAlchemy\nDESCRIPTION: This code demonstrates how to load the structured SDOH data into a BigQuery database using the pandas to_gbq method. It creates a BigQuery client and appends the results to a pre-existing table called 'social_determinants' in the 'clinical' dataset.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-social-determinants-of-health/SDOH-Json-mode.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Append results to a pre-existing BigQuery table\nclient = bigquery.Client()\nsdoh_df.to_gbq('clinical.social_determinants',client.project,credentials=client._credentials,if_exists='append')\n```\n\n----------------------------------------\n\nTITLE: Loading Input Text\nDESCRIPTION: Reads the cleaned text file that will be used as input for transcript generation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-2-Transcript-Writer.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nINPUT_PROMPT = read_file_to_string('./resources/clean_extracted_text.txt')\n```\n\n----------------------------------------\n\nTITLE: Evaluating Llama-3.1-8B Model on Leaderboard Benchmarks\nDESCRIPTION: Command to run leaderboard evaluation for the Llama-3.1-8B model using accelerate launch with bfloat16 precision and batch size 4.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch -m lm_eval --model_args pretrained=meta-llama/Llama-3.1-8B,dtype=bfloat16  --log_samples --output_path eval_results --tasks leaderboard  --batch_size 4\n```\n\n----------------------------------------\n\nTITLE: Installing NotebookLlama Requirements with Git\nDESCRIPTION: Commands to clone the llama-recipes repository and install the required dependencies for the NotebookLlama project. This setup is necessary before running any of the notebooks in the workflow.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/meta-llama/llama-recipes\ncd llama-recipes/end-to-end-use-cases/NotebookLlama/\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: FSDP Full Parameter Fine-tuning Command\nDESCRIPTION: Command for full parameter fine-tuning without PEFT, tested with BF16 on 8xA100 40GB GPUs.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/multigpu_finetuning.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nnodes 1 --nproc_per_node 8  finetuning.py --enable_fsdp --model_name /path_of_model_folder/8B --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned --fsdp_config.pure_bf16 --use_fast_kernels\n```\n\n----------------------------------------\n\nTITLE: Testing Tool Retrieval\nDESCRIPTION: Tests the object retriever by retrieving tools relevant to a query about evaluation datasets in MetaGPT and SWE-Bench.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntools = obj_retriever.retrieve(\n    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n)\n```\n\n----------------------------------------\n\nTITLE: Processing Follow-up Question\nDESCRIPTION: Handles a follow-up question about salary without context\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/quickstart.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# how about a follow up question\nfollow_up = \"What's his salary?\"\nprint(llm.invoke(follow_up).content)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Model and Dependencies\nDESCRIPTION: Imports required libraries and sets up the Llama-3.1-8B-Instruct model configuration.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-3-Re-Writer.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nMODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n```\n\nLANGUAGE: python\nCODE:\n```\n# Import necessary libraries\nimport torch\nfrom accelerate import Accelerator\nimport transformers\n\nfrom tqdm.notebook import tqdm\nimport warnings\n\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Authenticating Modal CLI\nDESCRIPTION: Commands to authenticate with Modal platform, with an alternative command if the primary method fails.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/modal/many-llamas-human-eval/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmodal setup\n# or if that doesn't work, try \n# python -m modal setup\n```\n\n----------------------------------------\n\nTITLE: Creating ReAct Agent with RAG Tools\nDESCRIPTION: Initializes a ReAct agent with RAG query engine tools for searching information about Drake and Kendrick.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/llamaindex_cookbook.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nagent = ReActAgent.from_tools(\n    query_engine_tools,  ## TODO: define query tools\n    llm=llm_replicate,\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Testing TGI API with curl\nDESCRIPTION: Example curl commands to test the TGI API endpoint with sample prompts.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llama_on_prem.md#2025-04-07_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/generate -X POST -H 'Content-Type: application/json' -d '{\n        \"inputs\": \"Who wrote the book innovators dilemma?\",\n        \"parameters\": {\n            \"max_new_tokens\":200\n        }\n    }'\n```\n\n----------------------------------------\n\nTITLE: Applying Nest Asyncio for Async Operations\nDESCRIPTION: Applies nest_asyncio to allow for nested asyncio event loops, which is necessary for async operations within Jupyter notebooks.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L3_Building_an_Agent_Reasoning_Loop.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Checking Document Content and Length\nDESCRIPTION: Displays the length of the document content, a preview of the content, and the number of documents loaded.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/video_summary.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlen(docs[0].page_content), docs[0].page_content[:300], len(docs)\n```\n\n----------------------------------------\n\nTITLE: Saving Generated Output\nDESCRIPTION: Processes and saves the generated transcript to a pickle file for use in subsequent notebooks.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-3-Re-Writer.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nLANGUAGE: python\nCODE:\n```\nsave_string_pkl = outputs[0][\"generated_text\"][-1]['content']\n```\n\nLANGUAGE: python\nCODE:\n```\nwith open('./resources/podcast_ready_data.pkl', 'wb') as file:\n    pickle.dump(save_string_pkl, file)\n```\n\n----------------------------------------\n\nTITLE: Defining Safety Guidelines for Llama Guard in Python\nDESCRIPTION: This code snippet shows how to create an instance of the Guidelines class to define safety categories for Llama Guard. It specifies category names, descriptions, and a category code prefix.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/llama_cookbook/data/llama_guard/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nguidelines = Guidelines(\n    categories=[\n        Category(name=\"Personal Information\",\n                 description=\"Information that can identify an individual\"),\n        Category(name=\"Location\",\n                 description=\"Information about where someone lives or works\")\n    ],\n    category_code_prefix=\"S\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing lm-evaluation-harness\nDESCRIPTION: Commands to clone the lm-evaluation-harness repository and install it using pip.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/EleutherAI/lm-evaluation-harness.git\ncd lm-evaluation-harness\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Fetching Repository Issues\nDESCRIPTION: Retrieves issues from GitHub API and validates the data\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nissues_df = fetch_repo_issues(repo_name, start_date, end_date)\nissues_df = validate_df_values(issues_df, out_folder, 'issues')\n\nprint(f\"\\n\\n[Showing 5 out of {issues_df.shape[0]} rows]\\n\")\nprint(issues_df.head())\n```\n\n----------------------------------------\n\nTITLE: Fetching Repository Issues\nDESCRIPTION: Retrieves issues from GitHub API and validates the data\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/github_triage/walkthrough.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nissues_df = fetch_repo_issues(repo_name, start_date, end_date)\nissues_df = validate_df_values(issues_df, out_folder, 'issues')\n\nprint(f\"\\n\\n[Showing 5 out of {issues_df.shape[0]} rows]\\n\")\nprint(issues_df.head())\n```\n\n----------------------------------------\n\nTITLE: Installing llama-cookbook for Development\nDESCRIPTION: These commands install llama-cookbook from source with all optional dependencies for development and contribution purposes.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -U pip setuptools\npip install --extra-index-url https://download.pytorch.org/whl/test/cu118 -e .[tests,auditnlg,vllm]\n```\n\n----------------------------------------\n\nTITLE: Running Text-only Inference with PEFT Method in Python\nDESCRIPTION: This snippet demonstrates how to run text-only inference using a PEFT method. It requires specifying the base model name, PEFT model path, and uses AuditNLG for safety checks.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/inference/local_inference/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncat <test_prompt_file> | python inference.py --model_name <training_config.model_name> --peft_model <training_config.output_dir> --use_auditnlg\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for TTS Generation in Python\nDESCRIPTION: Installing optimization libraries and specific transformer versions required for the TTS models. The transformers version is downgraded to 4.43.3 for compatibility with the TTS models.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#!pip3 install optimum\n#!pip install -U flash-attn --no-build-isolation\n#!pip install transformers==4.43.3\n```\n\n----------------------------------------\n\nTITLE: Benchmark Script Implementation\nDESCRIPTION: Shell script that runs benchmarks with different QPS rates using vLLM's benchmark_serving.py\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/crusoe/vllm-fp8/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nTOTAL_SECONDS=120\nQPS_RATES=(\"1\" \"3\" \"5\" \"7\" \"9\")\n\nfor QPS in ${QPS_RATES[@]}; do\n    NUM_PROMPTS=$((TOTAL_SECONDS * QPS))\n    echo \"===== RUNNING NUM_PROMPTS = $NUM_PROMPTS QPS = $QPS =====\"\n\n    uv run benchmarks/benchmark_serving.py \\\n        --model $MODEL \\\n        --dataset-name sonnet --sonnet-input-len 550 --sonnet-output-len 150 --dataset-path benchmarks/sonnet.txt \\\n        --num-prompts $NUM_PROMPTS --request-rate $QPS --save-result\ndone\n```\n\n----------------------------------------\n\nTITLE: Executing Groq Chat Completion with JSON Mode for SDOH Extraction\nDESCRIPTION: This snippet demonstrates how to use the Groq API to perform a chat completion with JSON mode enabled, extracting social determinants of health from a clinical note.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-api-cookbook/json-mode-social-determinants-of-health/SDOH-Json-mode.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Establish client with GROQ_API_KEY environment variable\nclient = Groq(api_key=os.getenv('GROQ_API_KEY'))\nmodel = \"llama3-8b-8192\"\n\n# Create chat completion object with JSON response format\nchat_completion = client.chat.completions.create(\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": user_prompt_template.format(clinical_note=clinical_note),\n        }\n    ],\n    model = model,\n    response_format = {\"type\": \"json_object\"} # Add this response format to configure JSON mode\n)\n\nsocial_determinants_json_string = chat_completion.choices[0].message.content\nprint(social_determinants_json_string)\n```\n\n----------------------------------------\n\nTITLE: Configuring LangChain with TGI for Llama 3\nDESCRIPTION: Sets up a LangChain instance that connects to a TGI-hosted Llama 3 model. The code configures various generation parameters like temperature, top_k, top_p, and repetition penalty, then demonstrates a simple query to the model.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llama_on_prem.md#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.llms import HuggingFaceTextGenInference\n\nllm = HuggingFaceTextGenInference(\n    inference_server_url=\"http://<tgi_server_ip_address>:8080/\",\n    max_new_tokens=512,\n    top_k=10,\n    top_p=0.95,\n    typical_p=0.95,\n    temperature=0.01,\n    repetition_penalty=1.03,\n)\n\nllm(\"What wrote the book innovators dilemma?\")\n```\n\n----------------------------------------\n\nTITLE: Running a Specific Test in llama-cookbook\nDESCRIPTION: This command shows how to run a specific test by filtering for its name using pytest.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest src/tests/test_finetuning.py -k test_finetuning_peft\n```\n\n----------------------------------------\n\nTITLE: Extracting Response Message from Chat Completion\nDESCRIPTION: Extracting the message component from the API response which contains the function call information.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/agents/DeepLearningai_Course_Notebooks/Functions_Tools_and_Agents_with_LangChain_L1_Function_Calling.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresponse_message = response.choices[0].message\nresponse_message\n```\n\n----------------------------------------\n\nTITLE: Importing Display and Progress Libraries for Audio Generation\nDESCRIPTION: Importing IPython display utilities for audio playback in notebook environments and the tqdm library for progress tracking during generation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import Audio\nimport IPython.display as ipd\nfrom tqdm import tqdm\n```\n\n----------------------------------------\n\nTITLE: Installing Llama-Cookbook from Source\nDESCRIPTION: Instructions for installing Llama-Cookbook from source code for development purposes. This approach updates pip and setuptools before installing the package in development mode.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/README.md#2025-04-07_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ngit clone git@github.com:meta-llama/llama-cookbook.git\ncd llama-cookbook\npip install -U pip setuptools\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys\nDESCRIPTION: Sets up environment variables for various API keys needed for the tools integration.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/langgraph_tool_calling_agent.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\nTAVILY_API_KEY = getpass()\nos.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n```\n\nLANGUAGE: python\nCODE:\n```\nREPLICATE_API_TOKEN = getpass()\nos.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n```\n\nLANGUAGE: python\nCODE:\n```\nGROQ_API_KEY = getpass()\nos.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Commands to navigate into the project directory and install all required dependencies using npm. This step prepares the development environment for running the application.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/e2b-ai-analyst/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd ai-analyst && npm i\n```\n\n----------------------------------------\n\nTITLE: Installing llama-cookbook with Test Dependencies\nDESCRIPTION: This command installs llama-cookbook with optional test dependencies enabled, using a specific PyTorch wheel URL.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --extra-index-url https://download.pytorch.org/whl/test/cu118 llama-cookbook[tests]\n```\n\n----------------------------------------\n\nTITLE: Executing Follow-up Query\nDESCRIPTION: Runs the generated follow-up SQL query against the database\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/coding/text2sql/quickstart.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndb.run(new_answer)\n```\n\n----------------------------------------\n\nTITLE: Setting Voice Preset and Sampling Rate for Bark TTS Model\nDESCRIPTION: Configuring the voice preset and sampling rate parameters for the Suno Bark TTS model. The voice preset determines the character of the generated voice.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvoice_preset = \"v2/en_speaker_6\"\nsampling_rate = 24000\n```\n\n----------------------------------------\n\nTITLE: Visualizing Category and Type Distributions\nDESCRIPTION: Creates bar plots to visualize the distribution of original categories and types, and prints the top 5 most frequent values for each.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nplt.style.use('ggplot')\n\n# Create a figure with two subplots\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 16))\n\n# Plot distribution of Categories\nsns.countplot(data=result, y='Category', ax=ax1, order=result['New_Category'].value_counts().index)\nax1.set_title('Distribution of Categories', fontsize=16)\nax1.set_xlabel('Count', fontsize=12)\nax1.set_ylabel('Category', fontsize=12)\n\n# Plot distribution of Types\nsns.countplot(data=result, y='Type', ax=ax2, order=result['New_Type'].value_counts().index)\nax2.set_title('Distribution of Types', fontsize=16)\nax2.set_xlabel('Count', fontsize=12)\nax2.set_ylabel('Type', fontsize=12)\n\n# Adjust layout and display the plot\nplt.tight_layout()\nplt.show()\n\n# Optional: Save the figure\n# plt.savefig('category_type_distribution.png', dpi=300, bbox_inches='tight')\n\n# Additional analysis: Print top 5 categories and types\nprint(\"Top 5 Categories:\")\nprint(result['Category'].value_counts().head())\n\nprint(\"\\nTop 5 Types:\")\nprint(result['Type'].value_counts().head())\n\n```\n\n----------------------------------------\n\nTITLE: Running All Unit Tests for llama-cookbook\nDESCRIPTION: This command runs all unit tests located in the src/tests/ folder using pytest.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest src/tests/\n```\n\n----------------------------------------\n\nTITLE: Low CPU Memory FSDP Training Command\nDESCRIPTION: Command for running 70B model fine-tuning with reduced CPU memory usage. Loads model only on rank0 before device distribution.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/multigpu_finetuning.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nnodes 1 --nproc_per_node 8 finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --model_name /path_of_model_folder/70B --batch_size_training 1 --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: A list of required Python packages including VLLM for LLM inference optimization, pytest-mock for mocking in tests, and auditnlg for NLG evaluation and auditing.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/dev_requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nvllm\npytest-mock\nauditnlg\n```\n\n----------------------------------------\n\nTITLE: Setting Up Data Paths and Model Configuration\nDESCRIPTION: Defines the paths to data directories and specifies the LLM model to be used for image labeling.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/Multi-Modal-RAG/notebooks/Part_2_Cleaning_Data_and_DB.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDATA = \"./DATA/\"\nMETA_DATA = f\"{DATA}images.csv/\"\nIMAGES = f\"{DATA}images_compressed/\"\n\nhf_token = \"\"\nmodel_name = \"meta-llama/Llama-3.2-11b-Vision-Instruct\"\n```\n\n----------------------------------------\n\nTITLE: Defining Speaker Voice Description for Parler TTS\nDESCRIPTION: Creating a detailed voice description for the first speaker using Parler TTS. This description guides the model in generating voice with specific characteristics like expressiveness and pace.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/NotebookLlama/Step-4-TTS-Workflow.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nspeaker1_description = \"\"\"\nLaura's voice is expressive and dramatic in delivery, speaking at a moderately fast pace with a very close recording that almost has no background noise.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Launching VLLM Server for RAFT 8B Model\nDESCRIPTION: Starts a VLLM server to host the converted RAFT 8B model for evaluation. The server is configured to run on a specific GPU and port.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/RAFT-Chatbot/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=1 python -m vllm.entrypoints.openai.api_server  --model raft-8b --port 8000  --disable-log-requests\n```\n\n----------------------------------------\n\nTITLE: Displaying LangGraph RAG Agent Diagram\nDESCRIPTION: Presents a diagram showing the evolution of RAG agent implementations, from CRAG to Self-RAG and Adaptive RAG, using color-coded flow charts.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/langchain/README.md#2025-04-07_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n![langgraph_rag_agent_](https://github.com/rlancemartin/llama-recipes/assets/122662504/ec4aa1cd-3c7e-4cd1-a1e7-7deddc4033a8)\n```\n\n----------------------------------------\n\nTITLE: Python Dependencies for Llama Cookbook\nDESCRIPTION: List of required Python packages including transformers for model handling, rouge for evaluation metrics, xopen for compressed file handling, and needlehaystack for search functionality.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/long_context/H2O/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntransformers\nrouge\nxopen\nneedlehaystack\n```\n\n----------------------------------------\n\nTITLE: LoRA FSDP Fine-tuning Command for Llama Vision Model\nDESCRIPTION: Command to perform LoRA fine-tuning using FSDP on Llama 3.2 vision model. Includes PEFT configuration with LoRA method enabled.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/finetune_vision_model.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nnodes 1 --nproc_per_node 4  recipes/quickstart/finetuning/finetuning.py --enable_fsdp --lr 1e-5  --num_epochs 3 --batch_size_training 2 --model_name meta-llama/Llama-3.2-11B-Vision-Instruct --dist_checkpoint_root_folder ./finetuned_model --dist_checkpoint_folder fine-tuned  --use_fast_kernels --dataset \"custom_dataset\" --custom_dataset.test_split \"test\" --custom_dataset.file \"recipes/quickstart/finetuning/datasets/ocrvqa_dataset.py\"  --run_validation True --batching_strategy padding  --use_peft --peft_method lora\n```\n\n----------------------------------------\n\nTITLE: Inspecting Tool Metadata\nDESCRIPTION: Examines the metadata of a retrieved tool to verify that the retrieval process is working correctly.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/llamaindex/dlai_agentic_rag/Building_Agentic_RAG_with_Llamaindex_L4_Building_a_Multi-Document_Agent.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntools[2].metadata\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository and Installing Dependencies\nDESCRIPTION: These commands clone the llama-cookbook repository, navigate to the email agent directory, and install the required Python packages using pip.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/email_agent/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/meta-llama/llama-cookbook\ncd llama-cookbook/end-to-end-use-cases/email_agent\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Response Parsing in YAML\nDESCRIPTION: YAML configuration for parsing model responses using regex patterns to extract multiple-choice answers from generated text.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/llm_eval_harness/meta_eval/README.md#2025-04-07_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nfilter_list:\n  - name: \"strict-match\"\n    filter:\n      - function: \"regex\"\n        group_select: -1\n        regex_pattern: 'best answer is ([A-Z])'\n      - function: \"take_first\"\n```\n\n----------------------------------------\n\nTITLE: Benchmark Performance Metrics\nDESCRIPTION: List of metrics reported by the benchmark for each instance, including latency measurements, throughput metrics, and token processing rates. Results are displayed in terminal output and saved to performance_metrics.csv.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/benchmarks/inference/README.md#2025-04-07_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n* Number of concurrent requests\n* P50 Latency(ms)\n* P99 Latency(ms)\n* Request per second (RPS)\n* Output tokens per second\n* Output tokens per second per GPU\n* Input tokens per second\n* Input tokens per second per GPU\n* Average tokens per second per request\n```\n\n----------------------------------------\n\nTITLE: Creating an Interactive Knowledge Graph Interface using Gradio in Python\nDESCRIPTION: This code creates a web interface using Gradio that allows users to add text to a knowledge graph and query the graph with natural language. It connects the UI components to the underlying Neo4j database functions.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/3p-integrations/groq/groq-example-templates/conversational-chatbot-langchain/requirements.txt#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gradio as gr\n\ndef process_text(text):\n    \"\"\"Process text to extract triples and add to graph\"\"\"\n    triples = extract_triples(text)\n    result = add_to_graph(triples)\n    \n    # Format the triples for display\n    formatted_triples = \"\\n\".join([f\"{t[0]}  {t[1]}  {t[2]}\" for t in triples])\n    return f\"Extracted {len(triples)} relationships:\\n{formatted_triples}\\n\\n{result}\"\n\n# Create the Gradio interface\nwith gr.Blocks(title=\"Knowledge Graph Builder\") as demo:\n    gr.Markdown(\"# Interactive Knowledge Graph with Llama and Neo4j\")\n    \n    with gr.Tab(\"Add to Knowledge Graph\"):\n        gr.Markdown(\"Enter text to extract knowledge and add to the graph\")\n        text_input = gr.Textbox(lines=5, label=\"Input Text\")\n        add_button = gr.Button(\"Process Text\")\n        output = gr.Textbox(label=\"Results\")\n        add_button.click(process_text, inputs=text_input, outputs=output)\n    \n    with gr.Tab(\"Query Knowledge Graph\"):\n        gr.Markdown(\"Ask questions about the knowledge in the graph\")\n        question_input = gr.Textbox(lines=2, label=\"Question\")\n        query_button = gr.Button(\"Submit Question\")\n        query_output = gr.Textbox(label=\"Answer\")\n        query_button.click(query_graph, inputs=question_input, outputs=query_output)\n\n# Launch the app\ndemo.launch()\n```\n\n----------------------------------------\n\nTITLE: Basic Fine-tuning Command with PEFT and Quantization\nDESCRIPTION: Command to run fine-tuning using LORA PEFT method with 8-bit quantization and FP16 precision. Requires specifying model path and output directory.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/src/docs/single_gpu.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m llama_cookbook.finetuning  --use_peft --peft_method lora --quantization 8bit --use_fp16 --model_name /path_of_model_folder/8B --output_dir Path/to/save/PEFT/model\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Paths for RAG Implementation\nDESCRIPTION: Imports necessary libraries from LangChain and sets paths for data and vector storage used in the RAG implementation.\nSOURCE: https://github.com/meta-llama/llama-cookbook/blob/main/end-to-end-use-cases/customerservice_chatbots/RAG_chatbot/RAG_Chatbot_Example.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import PyPDFDirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\nDATA_PATH = 'data' #Your root data folder path\nDB_FAISS_PATH = 'vectorstore/db_faiss'\n```"
  }
]
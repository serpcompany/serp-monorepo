[
  {
    "owner": "nvidia",
    "repo": "bionemo-framework",
    "content": "TITLE: Training ESM-2 3B Model with BioNeMo Framework\nDESCRIPTION: Complete training script for the ESM-2 3B parameter model using 128 H100 GPUs with a batch size of 16 per device. Includes configuration for data paths, model architecture, and training hyperparameters with an extended warmup period of 20,000 steps.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/ESM-2/pre-training.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntrain_esm2 \\\n  --create-tensorboard-logger \\\n  --resume-if-exists \\\n  --wandb-project=<wandb-project-name> \\\n  --save-top-k=10 \\\n  --train-cluster-path=/data/train_clusters.parquet \\\n  --train-database-path=/data/train.db \\\n  --valid-cluster-path=/data/valid_clusters.parquet \\\n  --valid-database-path=/data/validation.db \\\n  --num-steps=500_000 \\\n  --warmup-steps=20_000 \\\n  --metric-to-monitor-for-checkpoints=val_loss \\\n  --micro-batch-size=16 \\\n  --num-nodes=16 \\\n  --num-gpus=8 \\\n  --val-check-interval=2500 \\\n  --limit-val-batches=1.0 \\\n  --result-dir=/results/esm2_pretrain_3b \\\n  --experiment-name=esm2_pretrain_3b \\\n  --min-seq-length=1024 \\\n  --max-seq-length=1024 \\\n  --num-layers=36 \\\n  --hidden-size=2560 \\\n  --num-attention-heads=40 \\\n  --ffn-hidden-size=10240;\n```\n\n----------------------------------------\n\nTITLE: Training ESM-2 650M Model with BioNeMo Framework\nDESCRIPTION: Complete training script for the ESM-2 650M parameter model using 64 H100 GPUs with a batch size of 32 per device. Includes configuration for data paths, model architecture, and training hyperparameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/ESM-2/pre-training.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntrain_esm2 \\\n  --create-tensorboard-logger \\\n  --resume-if-exists \\\n  --wandb-project=<wandb-project-name> \\\n  --save-top-k=10 \\\n  --train-cluster-path=/data/train_clusters.parquet \\\n  --train-database-path=/data/train.db \\\n  --valid-cluster-path=/data/valid_clusters.parquet \\\n  --valid-database-path=/data/validation.db \\\n  --num-steps=500_000 \\\n  --metric-to-monitor-for-checkpoints=val_loss \\\n  --micro-batch-size=32 \\\n  --num-nodes=8 \\\n  --num-gpus=8 \\\n  --val-check-interval=10000 \\\n  --limit-val-batches=1.0 \\\n  --result-dir=/results/esm2_pretrain_650m \\\n  --experiment-name=esm2_pretrain_650m \\\n  --min-seq-length=1024 \\\n  --max-seq-length=1024 \\\n  --num-layers=33 \\\n  --hidden-size=1280 \\\n  --num-attention-heads=20 \\\n  --ffn-hidden-size=5120;\n```\n\n----------------------------------------\n\nTITLE: Training ESM-2 with Pydantic Configuration in BioNeMo\nDESCRIPTION: Demonstrates how to train an ESM-2 model using a Pydantic configuration file. It specifies the data and model configuration classes and the path to the configuration YAML file.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/training-models.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbionemo-esm2-train \\\n--data-config-cls bionemo.esm2.run.config_models.ESM2DataConfig \\\n--model-config-cls bionemo.esm2.run.config_models.ExposedESM2PretrainConfig \\\n--config my_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Starting BioNeMo Interactive Shell Container\nDESCRIPTION: Docker command for launching an interactive shell in the BioNeMo container with GPU support, mounted volumes, and environment configuration.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/initialization-guide.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n  --rm -it \\\n  --gpus all \\\n  --network host \\\n  --shm-size=4g \\\n  -e WANDB_API_KEY \\\n  -e NGC_CLI_API_KEY \\\n  -e NGC_CLI_ORG \\\n  -e NGC_CLI_TEAM \\\n  -e NGC_CLI_FORMAT_TYPE \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  {{ docker_url }}:{{ docker_tag }} \\\n  /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Evo2 Model\nDESCRIPTION: Executes the train_evo2 command to fine-tune the Evo2 1B model using the preprocessed data and configuration. Adjusts parameters based on CI mode and sets up activation checkpointing.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nMAX_STEPS: int = 10 if FAST_CI_MODE else 100\nval_check_interval = min(int(MAX_STEPS // 2), 50)\nwarmup_steps = min(MAX_STEPS, 100)\n\nif FAST_CI_MODE:\n    model_subset_option = (\n        \"--num-layers 4 --hybrid-override-pattern SDH* --activation-checkpoint-recompute-num-layers 2\"\n    )\nelse:\n    # By default do 5 layers of activation checkpointing\n    model_subset_option = \"--activation-checkpoint-recompute-num-layers 5\"\ntrain_cmd = f\"\"\"train_evo2 \\\n    -d training_data_config.yaml \\\n    --dataset-dir ./preprocessed_data \\\n    --result-dir pretraining_demo \\\n    --experiment-name evo2 \\\n    --model-size 1b \\\n    --devices 1 \\\n    --num-nodes 1 \\\n    --seq-length 8192 \\\n    --micro-batch-size 2 \\\n    --lr 0.000015 \\\n    --min-lr 0.0000149 \\\n    --warmup-steps {warmup_steps} \\\n    --grad-acc-batches 4 \\\n    --max-steps {MAX_STEPS} \\\n    --ckpt-dir nemo2_evo2_1b_8k \\\n    --clip-grad 250 \\\n    --wd 0.001 \\\n    --attention-dropout 0.01 \\\n    --hidden-dropout 0.01 \\\n    --val-check-interval {val_check_interval} \\\n    {model_subset_option} \\\n    --create-tensorboard-logger \\\n    --ckpt-async-save\"\"\"\n\n!{train_cmd}\n```\n\n----------------------------------------\n\nTITLE: DDPM Usage Example for Training and Generation\nDESCRIPTION: A complete example showing how to initialize a DDPM model, train it with data, and generate new samples using a predefined inference schedule.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_110\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from bionemo.bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n>>> from bionemo.bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n>>> from bionemo.bionemo.moco.interpolants.discrete_time.continuous.ddpm import DDPM\n>>> from bionemo.bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule\n>>> from bionemo.bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\n\n\nddpm = DDPM(\n    time_distribution = UniformTimeDistribution(discrete_time = True,...),\n    prior_distribution = GaussianPrior(...),\n    noise_schedule = DiscreteCosineNoiseSchedule(...),\n    )\nmodel = Model(...)\n\n# Training\nfor epoch in range(1000):\n    data = data_loader.get(...)\n    time = ddpm.sample_time(batch_size)\n    noise = ddpm.sample_prior(data.shape)\n    xt = ddpm.interpolate(data, noise, time)\n\n    x_pred = model(xt, time)\n    loss = ddpm.loss(x_pred, data, time)\n    loss.backward()\n\n# Generation\nx_pred = ddpm.sample_prior(data.shape)\nfor t in DiscreteLinearTimeSchedule(...).generate_schedule():\n    time = torch.full((batch_size,), t)\n    x_hat = model(x_pred, time)\n    x_pred = ddpm.step(x_hat, time, x_pred)\nreturn x_pred\n```\n\n----------------------------------------\n\nTITLE: Complete MDLM Usage Example with Training and Generation\nDESCRIPTION: Comprehensive example showing how to instantiate an MDLM model, train it on data, and use it for generation. Demonstrates the full workflow including time sampling, interpolation, loss calculation, and inference with a time schedule.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from bionemo.bionemo.moco.distributions.prior.discrete.mask import DiscreteMaskedPrior\n>>> from bionemo.bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n>>> from bionemo.bionemo.moco.interpolants.continuous_time.discrete.mdlm import MDLM\n>>> from bionemo.bionemo.moco.schedules.noise.continuous_noise_transforms import CosineExpNoiseTransform\n>>> from bionemo.bionemo.moco.schedules.inference_time_schedules import LinearTimeSchedule\n\n\nmdlm = MDLM(\n    time_distribution = UniformTimeDistribution(discrete_time = False,...),\n    prior_distribution = DiscreteMaskedPrior(...),\n    noise_schedule = CosineExpNoiseTransform(...),\n    )\nmodel = Model(...)\n\n# Training\nfor epoch in range(1000):\n    data = data_loader.get(...)\n    time = mdlm.sample_time(batch_size)\n    xt = mdlm.interpolate(data, time)\n\n    logits = model(xt, time)\n    loss = mdlm.loss(logits, data, xt, time)\n    loss.backward()\n\n# Generation\nx_pred = mdlm.sample_prior(data.shape)\nschedule = LinearTimeSchedule(...)\ninference_time = schedule.generate_schedule()\ndts = schedue.discreteize()\nfor t, dt in zip(inference_time, dts):\n    time = torch.full((batch_size,), t)\n    logits = model(x_pred, time)\n    x_pred = mdlm.step(logits, time, x_pred, dt)\nreturn x_pred\n```\n\n----------------------------------------\n\nTITLE: Training ESM-2 8M Model with BioNeMo Framework\nDESCRIPTION: Complete training script for the ESM-2 8M parameter model using 32 A100 GPUs with a batch size of 64 per device. Includes configuration for data paths, model architecture, and training hyperparameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/ESM-2/pre-training.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrain_esm2 \\\n  --create-tensorboard-logger \\\n  --resume-if-exists \\\n  --wandb-project=<wandb-project-name> \\\n  --save-top-k=10 \\\n  --train-cluster-path=/data/train_clusters.parquet \\\n  --train-database-path=/data/train.db \\\n  --valid-cluster-path=/data/valid_clusters.parquet \\\n  --valid-database-path=/data/validation.db \\\n  --num-steps=500_000 \\\n  --metric-to-monitor-for-checkpoints=val_loss \\\n  --micro-batch-size=64 \\\n  --num-nodes=4 \\\n  --num-gpus=8 \\\n  --val-check-interval=10000 \\\n  --limit-val-batches=1.0 \\\n  --result-dir=/results/esm2_pretrain_8m \\\n  --experiment-name=esm2_pretrain_8m \\\n  --num-layers=6 \\\n  --hidden-size=320 \\\n  --num-attention-heads=20 \\\n  --ffn-hidden-size=1280;\n```\n\n----------------------------------------\n\nTITLE: Implementing Regression Loss Reduction for ESM-2 Fine-tuning in Python\nDESCRIPTION: A custom loss reduction class that extends BERTMLMLossWithReduction to compute MSE loss for regression tasks. The forward method calculates loss for each micro-batch while the reduce method computes the average across micro-batches for logging purposes.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass RegressorLossReduction(BERTMLMLossWithReduction):\n    def forward(\n        self, batch: Dict[str, torch.Tensor], forward_out: Dict[str, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Union[PerTokenLossDict, SameSizeLossDict]]:\n\n        regression_output = forward_out[\"regression_output\"]\n        targets = batch[\"labels\"].to(dtype=regression_output.dtype)  # [b, 1]\n\n        loss = torch.nn.functional.mse_loss(regression_output, targets)\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[ReductionT]) -> torch.Tensor:\n        losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return losses.mean()\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained ESM-2 Model Checkpoint in Python\nDESCRIPTION: This code downloads a pre-trained ESM-2 model checkpoint from the NGC registry using the BioNeMo load function. It demonstrates how to obtain the 8M parameter model.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.core.data.load import load\n\n\npretrain_checkpoint_path = load(\"esm2/8m:2.0\")\nprint(pretrain_checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Training Geneformer Model with BioNeMo\nDESCRIPTION: Shows how to train a Geneformer model using BioNeMo. It includes downloading test data and model checkpoints, and running the training command with various parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/training-models.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nTEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20241203 --source $MY_DATA_SOURCE); \\\nGENEFORMER_10M_CKPT=$(download_bionemo_data geneformer/10M_240530:2.0 --source $MY_DATA_SOURCE); \\\ntrain_geneformer     \\\n    --data-dir ${TEST_DATA_DIR}/cellxgene_2023-12-15_small_processed_scdl    \\\n    --result-dir ./results     \\\n    --restore-from-checkpoint-path ${GENEFORMER_10M_CKPT} \\\n    --experiment-name test_experiment     \\\n    --num-gpus 1  \\\n    --num-nodes 1 \\\n    --val-check-interval 10 \\\n    --num-dataset-workers 0 \\\n    --num-steps 55 \\\n    --seq-length 128 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural Network Model for Discrete Interpolation\nDESCRIPTION: Defines a PyTorch neural network model that takes discrete data and time values as input. The model uses an embedding layer for discrete states followed by a fully connected network that outputs logits for predicting transitions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# training\nB = 32  # batch size\nD = 10  # dimension or sequence length, this is the number of discrete elements\nS = 2  # state space, binary so S=2\n# here we have a batch of 32 objects that consist of 10 binary variables.\n\n\nclass Model(nn.Module):\n    def __init__(self, D, S):\n        super().__init__()\n        self.embedding = nn.Embedding(S + 1, 16)\n        self.net = nn.Sequential(\n            nn.Linear(17 * D, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, S * D),\n        )\n\n    def forward(self, x, t):\n        B, D = x.shape\n        x_emb = self.embedding(x)  # (B, D, 16)\n        net_input = torch.cat([x_emb, t[:, None, None].repeat(1, D, 1)], dim=-1).reshape(B, -1)  # (B, D * 17)\n        return self.net(net_input).reshape(B, D, S)  # (B, D, S)\n```\n\n----------------------------------------\n\nTITLE: DDIM Sampling Step Implementation\nDESCRIPTION: Performs one step of Denoising Diffusion Implicit Models (DDIM) sampling with options for masking and centering.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_128\n\nLANGUAGE: python\nCODE:\n```\ndef step_ddim(model_out: Tensor,\n              t: Tensor,\n              xt: Tensor,\n              mask: Optional[Tensor] = None,\n              eta: Float = 0.0,\n              center: Bool = False)\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning ESM-2 for Token-Level Classification using BioNeMo\nDESCRIPTION: This code snippet demonstrates the fine-tuning process for ESM-2 models on token-level classification tasks. It uses the 'finetune_esm2' command with specific parameters for token classification, including CNN-based configuration and training hyperparameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n%%capture --no-display --no-stderr cell_output\n\n! finetune_esm2 \\\n    --restore-from-checkpoint-path {pretrain_checkpoint_path} \\\n    --train-data-path {data_path} \\\n    --valid-data-path {data_path} \\\n    --config-class ESM2FineTuneTokenConfig \\\n    --dataset-class InMemoryPerTokenValueDataset \\\n    --task-type \"classification\" \\\n    --cnn-dropout 0.25 \\\n    --cnn-hidden-size 32 \\\n    --cnn-num-classes 3 \\\n    --experiment-name \"token-level-classification\" \\\n    --num-steps 50 \\\n    --num-gpus 1 \\\n    --val-check-interval 10 \\\n    --log-every-n-steps 10 \\\n    --encoder-frozen \\\n    --lr 5e-3 \\\n    --lr-multiplier 1e2 \\\n    --scale-lr-layer \"classification_head\" \\\n    --result-dir {work_dir}  \\\n    --micro-batch-size 2 \\\n    --num-gpus 1 \\\n    --precision \"bf16-mixed\"\n```\n\n----------------------------------------\n\nTITLE: Preparing Inference Data and Running ESM-2 Inference in Python\nDESCRIPTION: This code prepares inference data by creating a DataFrame with sequences and saving it to a CSV file. It then runs the ESM-2 inference process using the 'infer_esm2' command with various parameters, including the checkpoint path and configuration settings.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Create a DataFrame\ndf = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"sequences.csv\")\ndf.to_csv(data_path, index=False)\n\ncheckpoint_path = (\n    f\"{work_dir}/token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last\"\n)\nresults_path = f\"{work_dir}/token-level-classification/infer/\"\n\n%%capture --no-display --no-stderr cell_output\n\n! infer_esm2 --checkpoint-path {checkpoint_path} \\\n             --config-class ESM2FineTuneTokenConfig \\\n             --data-path {data_path} \\\n             --results-path {results_path} \\\n             --micro-batch-size 3 \\\n             --num-gpus 1 \\\n             --precision \"bf16-mixed\" \\\n             --include-embeddings \\\n             --include-hiddens \\\n             --include-input-ids\n```\n\n----------------------------------------\n\nTITLE: Implementing De Novo Generation Methods with MDLM in Python\nDESCRIPTION: This code snippet shows two methods for de novo molecule generation using the MDLM interpolant: one with confidence-based sampling and another with standard sampling. Both methods utilize the bionemo-moco framework for improved generation quality.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/blog.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef de_novo_generation_mdlm_conf(self, num_samples=1, softmax_temp=0.5, randomness=0.5):\n       x = torch.hstack([torch.full((1, 1), self.model.bos_index),\n                         torch.full((1, 1), self.model.eos_index)])\n       x = self._insert_mask(x, num_samples, min_add_len=40)\n       x = x.to(self.model.device)\n       self.mdlm.to_device(self.model.device)\n       num_steps = (x == self.model.mask_index).sum(dim=-1).max().item()\n       attention_mask = x != self.pad_index\n       for i in range(num_steps):\n           logits = self.model(x, attention_mask)\n           x = self.mdlm.step_confidence(logits, x, i, num_steps, softmax_temp, randomness)\n\n       # decode to SAFE strings\n       samples = self.model.tokenizer.batch_decode(x, skip_special_tokens=True)\n       # convert to SMILES strings\n       # remove None and take the largest\n      return samples\n\n   def de_novo_generation_mdlm(self, num_samples=1, temperature=1.0):\n       x = torch.hstack([torch.full((1, 1), self.model.bos_index),\n                         torch.full((1, 1), self.model.eos_index)])\n       x = self._insert_mask(x, num_samples, min_add_len=40)\n       x = x.to(self.model.device)\n       self.mdlm.to_device(self.model.device)\n       DEVICE = self.model.device\n       attention_mask = x != self.pad_index\n\n       ts = self.inference_time_schedule.generate_schedule(device=DEVICE)\n       dts = self.inference_time_schedule.discretize(device=DEVICE)\n       for t, dt in zip(ts, dts):\n           logits = self.model(x, attention_mask)\n           x = self.mdlm.step(logits, t, x, dt, temperature = temperature)\n\n       # decode to SAFE strings\n       samples = self.model.tokenizer.batch_decode(x, skip_special_tokens=True)\n       # convert to SMILES strings\n       # remove None and take the largest\n       return samples\n```\n\n----------------------------------------\n\nTITLE: Generating Geneformer Config File with bionemo-geneformer-recipe\nDESCRIPTION: This snippet demonstrates how to use the bionemo-geneformer-recipe command to generate a configuration file for a 10m-pretrain Geneformer model. It specifies the data path and result directory.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/training-models.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nTEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20241203 --source $MY_DATA_SOURCE); \\\nbionemo-geneformer-recipe \\\n    --recipe 10m-pretrain \\\n    --dest my_config.json \\\n    --data-path ${TEST_DATA_DIR}/cellxgene_2023-12-15_small_processed_scdl \\\n    --result-dir ./results\n```\n\n----------------------------------------\n\nTITLE: Configuring ESM-2 Model Architecture\nDESCRIPTION: Sets up the ESM-2 650M model configuration with parameters for number of layers, hidden size, attention heads, and other architecture details. Includes configuration for model initialization options and hardware acceleration settings.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/pretrain.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom megatron.core.optimizer import OptimizerConfig\nfrom nemo.lightning.pytorch.optim import MegatronOptimizerModule\n\nfrom bionemo.core.utils.dtypes import get_autocast_dtype\nfrom bionemo.esm2.api import ESM2Config\nfrom bionemo.esm2.data.tokenizer import get_tokenizer\nfrom bionemo.esm2.model.lr_scheduler import WarmupAnnealDecayHoldScheduler\nfrom bionemo.llm.lightning import BionemoLightningModule\nfrom bionemo.llm.model.biobert.lightning import biobert_lightning_module\nfrom bionemo.llm.model.biobert.model import BiobertSpecOption\n\n# ESM-2 650M config\nnum_layers = 33\nhidden_size = 1280\nnum_attention_heads = 20\nffn_hidden_size = 4 * hidden_size\n\nnemo1_init_path = None  # initialize from nemo1 checkpoint\nrestore_from_checkpoint_path = None  # initialize from nemo2 checkpoint\nneed_megatron_variable_seq_lengths_reductions: bool = (\n    pipeline_model_parallel_size * tensor_model_parallel_size > 1 and min_seq_length != max_seq_length\n)  # essential for pipeline/tensor parallel\nbiobert_spec_option = BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec  # accelerated esm2 with transformer engine\n\nwarmup_steps = 2000\nlr = 1e-4\n```\n\n----------------------------------------\n\nTITLE: Training and Sampling with Continuous Flow Matcher in Python\nDESCRIPTION: This code snippet shows how to use the Continuous Flow Matcher interpolant from bionemo-moco for training a generative model and sampling from it. It includes setup, training loop, and inference process using a linear inference schedule.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/blog.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.moco.interpolants import ContinuousFlowMatcher\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.distributions.prior import GaussianPrior\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\nuniform_time = UniformTimeDistribution()\nsimple_prior = GaussianPrior()\ncfm = ContinuousFlowMatcher(time_distribution=uniform_time,\n                            prior_distribution=simple_prior,\n                            prediction_type=\"velocity\")\n\n# Place both the model and the interpolant on the same device\nDEVICE = \"cuda\"\nmodel = model.to(DEVICE)\ncfm = cfm.to_device(DEVICE)\n\nfor k in range(20000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = cfm.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)  # data loader sample\n    t = cfm.sample_time(batch_size)\n    xt = cfm.interpolate(x1, t, x0)\n    ut = cfm.calculate_target(x1, x0)\n    vt = model(torch.cat([xt, t[:, None]], dim=-1))\n    loss = cfm.loss(vt, ut, target_type=\"velocity\").mean()\n    loss.backward()\n    optimizer.step()\n\ninf_size = 1024 # number of points to sample\ninference_sched = LinearInferenceSchedule(nsteps=100, device=DEVICE)\nschedule = inference_sched.generate_schedule()\ndts = inference_sched.discretize()\nsample = cfm.sample_prior((inf_size, 2)).to(DEVICE)  # Start with noise\ntrajectory = [sample]\nfor dt, t in zip(dts, schedule):\n    full_t = inference_sched.pad_time(inf_size, t, DEVICE)\n    vt = model(torch.cat([sample, full_t[:, None]], dim=-1))\n    sample = cfm.step(vt, sample, dt, full_t)\n    trajectory.append(sample)\nreturn sample\n```\n\n----------------------------------------\n\nTITLE: Initializing MoCo MDLM for Molecule Generation in Python\nDESCRIPTION: This code snippet demonstrates how to create a wrapper around an existing GenMol model to integrate bionemo-moco's MDLM interpolant. It sets up the necessary components such as prior distribution, time distribution, and noise schedule.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/blog.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.moco.distributions.prior import DiscreteMaskedPrior\nfrom bionemo.moco.interpolants import MDLM\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.schedules.noise.continuous_noise_transforms import LogLinearExpNoiseTransform\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\nclass ExampleMoCoMDLMforMoleculeGeneration:\n    def __init__(self, path):\n        self.model = BertForMaskedLM.load_from_checkpoint(path)\n        self.model.eval()\n\n        # Introduce MoCo\n        prior = DiscreteMaskedPrior(num_classes=self.model.tokenizer.vocab_size,\n                                    mask_dim=self.model.mask_index)\n        time_distribution = UniformTimeDistribution(discrete_time=False)\n        noise_schedule = LogLinearExpNoiseTransform()\n        self.mdlm = MDLM(time_distribution=time_distribution,\n                         prior_distribution=prior,\n                         noise_schedule=noise_schedule)\n        self.inference_time_schedule = LinearInferenceSchedule(direction=\"diffusion\",\n                                                             nsteps=100)\n```\n\n----------------------------------------\n\nTITLE: Configuring ESM-2 Fine-tuning with a Dataclass\nDESCRIPTION: A configuration dataclass for ESM-2 fine-tuning that inherits from ESM2GenericConfig. It defines parameters for model configuration, checkpoint loading, and encoder freezing options. The class also provides a method to select the appropriate loss reduction class.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass ESM2FineTuneSeqConfig(\n    ESM2GenericConfig[ESM2FineTuneSeqModel, RegressorLossReduction], iom.IOMixinWithGettersSetters\n):\n    model_cls: Type[ESM2FineTuneSeqModel] = ESM2FineTuneSeqModel\n    # The following checkpoint path is for nemo2 checkpoints. Config parameters not present in\n    # self.override_parent_fields will be loaded from the checkpoint and override those values here.\n    initial_ckpt_path: str | None = None\n    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n    # that has this new head and want to keep using these weights, please drop this next line or set to []\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n\n    encoder_frozen: bool = True  # freeze encoder parameters\n    ft_dropout: float = 0.25  # MLP layer dropout\n\n    def get_loss_reduction_class(self) -> Type[MegatronLossReduction]:\n        return RegressorLossReduction\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Dataset for ESM-2 Fine-tuning\nDESCRIPTION: A custom dataset class for ESM-2 fine-tuning that extends InMemoryProteinDataset to handle single value prediction tasks. The class provides methods to load data from CSV files and transform labels into appropriate tensor formats for model training.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass InMemorySingleValueDataset(InMemoryProteinDataset):\n    def __init__(\n        self,\n        labels: pd.Series,\n        task_type: str = \"regression\",\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        seed: int = np.random.SeedSequence().entropy,\n    ):\n        super().__init__(sequences, labels, task_type, tokenizer, seed)\n\n    def transform_label(self, label: float) -> Tensor:\n        return torch.tensor([label], dtype=torch.float)\n```\n\n----------------------------------------\n\nTITLE: Training Loop for Generative Models using BioNeMo MoCo in Python\nDESCRIPTION: This code snippet demonstrates the basic training loop for generative models using BioNeMo MoCo. It includes steps for interpolation, model evaluation, and loss calculation based on different prediction types.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/blog.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprediction_type = Literal[PredictionType.NOISE, PredictionType.DATA, PredictionType.VELOCITY]\n\ninterpolant = Interpolant(..., prediction_type)\n\nfor epoch in range(num_epochs):\n    for x1 in dataloader:\n        optimizer.zero_grad()\n        x0 = interpolant.sample_prior(x1.shape)\n        t = interpolant.sample_time(batch_size)\n\n        # Step 1 Interpolation\n        xt = interpolant.interpolate(x1, t, x0)\n\n        # Step 2 Model Evaluation\n        A = model(xt, t)\n\n        if interpolant.prediction_type == PredictionType.NOISE:\n            loss = interpolant.loss(A, x0, t).mean()\n        elif interpolant.prediction_type == PredictionType.DATA:\n            loss = interpolant.loss(A, x1, t).mean()\n        elif interpolant.prediction_type == PredictionType.VELOCITY:\n            loss = interpolant.loss(A, interpolant.calculate_velocity(x1, t, x0), t).mean()\n\n        loss.backward()\n        optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Discrete Flow Matching (DFM) Interpolant\nDESCRIPTION: Initializes the Discrete Flow Matching interpolant with a uniform prior distribution and time distribution. Sets up the inference schedule with 1000 steps using the BioNeMo framework's specialized modules.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.moco.distributions.prior import DiscreteUniformPrior\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import DiscreteFlowMatcher\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\n\nB = 32  # batch size\nD = 10  # dimension\nS = 2  # state space\n\nDEVICE = \"cuda:0\"\nprior = DiscreteUniformPrior(num_classes=S)\ntime_distribution = UniformTimeDistribution()\ndfm = DiscreteFlowMatcher(time_distribution=time_distribution, prior_distribution=prior, device=DEVICE)\nschedule = LinearInferenceSchedule(nsteps=1000)\n```\n\n----------------------------------------\n\nTITLE: Generating Preprocessing Configuration\nDESCRIPTION: Creates a YAML configuration file for preprocessing the concatenated chromosome data, specifying output directory, data splits, and tokenization options.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfull_fasta_path = os.path.abspath(concat_path)\noutput_dir = os.path.abspath(\"preprocessed_data\")\noutput_yaml = f\"\"\"\n- datapaths: [\"{full_fasta_path}\"]\n  output_dir: \"{output_dir}\"\n  output_prefix: chr20_21_22_uint8_distinct\n  train_split: 0.9\n  valid_split: 0.05\n  test_split: 0.05\n  overwrite: True\n  embed_reverse_complement: true\n  random_reverse_complement: 0.0\n  random_lineage_dropout: 0.0\n  include_sequence_id: false\n  transcribe: \"back_transcribe\"\n  force_uppercase: false\n  indexed_dataset_dtype: \"uint8\"\n  tokenizer_type: \"Byte-Level\"\n  vocab_file: null\n  vocab_size: null\n  merges_file: null\n  pretrained_tokenizer_model: null\n  special_tokens: null\n  fast_hf_tokenizer: true\n  append_eod: true\n  enforce_sample_length: null\n  ftfy: false\n  workers: 1\n  preproc_concurrency: 100000\n  chunksize: 25\n  drop_empty_sequences: true\n  nnn_filter: false  # If you split your fasta on NNN (in human these are contigs), then you should set this to true.\n  seed: 12342  # Not relevant because we are not using random reverse complement or lineage dropout.\n\"\"\"\nwith open(\"preprocess_config.yaml\", \"w\") as f:\n    print(output_yaml, file=f)\n```\n\n----------------------------------------\n\nTITLE: Base Interpolant Class Definition\nDESCRIPTION: Abstract base class for interpolation operations with time and prior distribution handling.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_136\n\nLANGUAGE: python\nCODE:\n```\nclass Interpolant(ABC)\n```\n\n----------------------------------------\n\nTITLE: Training D3PM Model\nDESCRIPTION: Implements the training loop for the D3PM model. For each iteration, generates random binary data, applies the D3PM interpolation, computes the loss, and updates model parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmodel = model.to(DEVICE)\nlosses = []\nfor _ in tqdm(range(50000)):\n    num_ones = torch.randint(0, D + 1, (B,))\n    x1 = (torch.arange(D)[None, :] < num_ones[:, None]).long().to(DEVICE)\n    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n    optimizer.zero_grad()\n    # x0 = dfm.sample_prior(x1.shape) # B x D\n    t = d3pm.sample_time(B)\n    xt = d3pm.interpolate(x1, t)\n    logits = model(xt, t)  # (B, D, S)\n    loss = d3pm.loss(logits, x1, xt, t).mean()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n```\n\n----------------------------------------\n\nTITLE: Continuous Flow Matcher Class Implementation in Python\nDESCRIPTION: Defines a Continuous Flow Matching interpolant class with examples of training and generation workflows.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_92\n\nLANGUAGE: python\nCODE:\n```\nclass ContinuousFlowMatcher(Interpolant):\n    \"\"\"A Continuous Flow Matching interpolant.\n\n    -------\n\n    Examples:\n\n    >>> import torch\n    >>> from bionemo.bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n    >>> from bionemo.bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n    >>> from bionemo.bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching import ContinuousFlowMatcher\n    >>> from bionemo.bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\n    flow_matcher = ContinuousFlowMatcher(\n        time_distribution = UniformTimeDistribution(...),\n        prior_distribution = GaussianPrior(...),\n        )\n    model = Model(...)\n\n    # Training\n    for epoch in range(1000):\n        data = data_loader.get(...)\n        time = flow_matcher.sample_time(batch_size)\n        noise = flow_matcher.sample_prior(data.shape)\n        data, time, noise = flow_matcher.apply_augmentation(noise, data) # Optional, only for OT\n        xt = flow_matcher.interpolate(data, time, noise)\n        flow = flow_matcher.calculate_target(data, noise)\n\n        u_pred = model(xt, time)\n        loss = flow_matcher.loss(u_pred, flow)\n        loss.backward()\n\n    # Generation\n    x_pred = flow_matcher.sample_prior(data.shape)\n    inference_sched = LinearInferenceSchedule(...)\n    for t in inference_sched.generate_schedule():\n        time = inference_sched.pad_time(x_pred.shape[0], t)\n        u_hat = model(x_pred, time)\n        x_pred = flow_matcher.step(u_hat, x_pred, time)\n    return x_pred\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Example Usage of BucketBatchSampler in Python\nDESCRIPTION: This code snippet demonstrates how to use the BucketBatchSampler class with various configurations, including different base batch samplers and shuffle options.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from bionemo.size_aware_batching.sampler import BucketBatchSampler\n\n>>> # Define the sizes for a dataset\n>>> sizes = torch.arange(25)\n>>> # Define bucket ranges\n>>> bucket_boundaries = torch.tensor([0, 6, 15, 25])\n\n>>> # Create a bucket batch sampler with torch.utils.data.BatchSampler as base batch sampler\n>>> # As there are 3 buckets, there will be 3 base batch samplers with batch sizes 2, 3, and 5.\n>>> batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=False,\n    )\n\n>>> # Iterate over batches of indices that lies in the same bucket and with different batch sizes.\n>>> print(list(batch_sampler))\n[[0, 1], [2, 3], [4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n\n>>> # randomize the dataset and buckets\n>>> batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n>>> print(list(batch_sampler))\n[[24, 17, 16, 22, 19], [2, 5], [12, 10, 11], [3, 0], [15, 18, 20, 21, 23], [7, 13, 6], [14, 9, 8], [1, 4]]\n>>> print(list(batch_sampler))\n[[14, 9, 13], [23, 16, 20, 21, 15], [5, 0], [8, 10, 11], [17, 24, 22, 18, 19], [12, 6, 7], [4, 2], [3, 1]]\n\n>>> # Combine with SizeAwareBatchSampler to control the cost of each batch\n>>> from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n>>> item_costs = sizes.tolist()\n>>> def cost_of_element(index):\n        return item_costs[index]\n>>> batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=SizeAwareBatchSampler,\n        base_batch_sampler_shared_kwargs={\"sizeof\": cost_of_element, \"max_total_size\": 40},\n        base_batch_sampler_individual_kwargs={},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n>>> print(list(iter(batch_sampler)))\n[[24], [2, 5, 3, 0, 1, 4], [12, 10, 11, 7], [13, 6, 14], [17, 16], [22], [19, 15], [9, 8], [18, 20], [21], [23]]\n```\n\n----------------------------------------\n\nTITLE: Preparing Token Classification Data in Python\nDESCRIPTION: This snippet prepares token-level classification data by assigning secondary structure labels to each token in the sequence. It creates a DataFrame with sequences and labels, then saves it to a CSV file for further processing.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nsecondary_structure_labels = [\n    \"EEEECCCCCHHHHHHHHHHHHHHHCCCEEEEEECCCHHHHHHHHHCCCCCCCCCEEE\",\n    \"CCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n    \"HHHHHCCCCCHHHHHHHHHHHHHHCCCHHHHHHHHHH\",\n    \"HHHHHHHHHHCCCHHHHHCCCCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n    \"CHHHHHHHHHHHHHHHCCCEEEEEECCCHHHHHHHHHCCCCCCCCCEEE\",\n    \"HHHHHHHHHHHHHCHHHHHHHHHHHHCCCEECCCEEEECCEEEEECC\",\n    \"HHHHHCCCHHHHHCCCCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n    \"CCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n    \"HHHHHCHHHHHHHHHHHHCCCEECCCEEEECCEEEEECC\",\n    \"CCCCCCCCCCCCCCCCCCCCCCCCCCEEECCCCEEECHHHHHHHHHCCCCCCCCEEECCCCCC\",\n]\n\ndata = [(seq, label) for (seq, label) in zip(artificial_sequence_data, secondary_structure_labels)]\n\n# Create a DataFrame\ndf = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"token_classification_data.csv\")\ndf.to_csv(data_path, index=False)\n```\n\n----------------------------------------\n\nTITLE: Running ESM-2 Model Inference\nDESCRIPTION: Executes inference using the ESM-2 model with options for embeddings, hidden states, and logits computation\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/inference.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n! infer_esm2 --checkpoint-path {checkpoint_path} \\\n             --data-path {data_path} \\\n             --results-path {work_dir} \\\n             --micro-batch-size 3 \\\n             --num-gpus 1 \\\n             --precision \"bf16-mixed\" \\\n             --include-hiddens \\\n             --include-embeddings \\\n             --include-logits \\\n             --include-input-ids\n```\n\n----------------------------------------\n\nTITLE: Setting Up ESM2 Model Pretraining with NeMo in Python\nDESCRIPTION: This snippet demonstrates how to set up model pretraining using NeMo's LLM training functionality. It includes configuration for logging with WandB, checkpoint callbacks, and the main training loop using llm.train().\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/pretrain.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nfrom nemo.collections import llm\nfrom nemo.lightning import resume\nfrom nemo.lightning.pytorch import callbacks as nl_callbacks\n\nfrom bionemo.llm.utils.logger_utils import WandbLoggerOptions, setup_nemo_lightning_logger\n\n\n# WANDB logging\nwandb_options: Optional[WandbLoggerOptions] = (\n    None\n    if wandb_project is None\n    else WandbLoggerOptions(\n        offline=False,\n        project=__your_wandb_project__,\n        entity=__your_wandb_entity__,\n        tags=None,\n        group=None,\n        id=None,\n        anonymous=False,\n        log_model=False,\n    )\n)\n\ncheckpoint_callback = nl_callbacks.ModelCheckpoint(\n    save_last=True,\n    monitor=\"val_loss\",\n    save_top_k=1,\n    always_save_context=True,\n)\n\nnemo_logger = setup_nemo_lightning_logger(\n    root_dir=__your_result_dir__,\n    name=__your_experiment_name__,\n    initialize_tensorboard_logger=True,\n    wandb_kwargs=wandb_options,\n    ckpt_callback=checkpoint_callback,\n)\n\nllm.train(\n    model=model,\n    data=data,\n    trainer=trainer,\n    log=nemo_logger,\n    resume=resume.AutoResume(\n        resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n        resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning ESM-2 for Sequence-Level Classification using BioNeMo\nDESCRIPTION: This code runs the ESM-2 fine-tuning process for sequence-level classification. It uses the 'finetune_esm2' command with various parameters to configure the training process, including data paths, model configuration, and training hyperparameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n%%capture --no-display --no-stderr cell_output\n\n! finetune_esm2 \\\n    --restore-from-checkpoint-path {pretrain_checkpoint_path} \\\n    --train-data-path {data_path} \\\n    --valid-data-path {data_path} \\\n    --config-class ESM2FineTuneSeqConfig \\\n    --dataset-class InMemorySingleValueDataset \\\n    --task-type \"classification\" \\\n    --mlp-ft-dropout 0.25 \\\n    --mlp-hidden-size 256 \\\n    --mlp-target-size 3 \\\n    --experiment-name \"sequence-level-classification\" \\\n    --num-steps 50 \\\n    --num-gpus 1 \\\n    --val-check-interval 10 \\\n    --log-every-n-steps 10 \\\n    --encoder-frozen \\\n    --lr 5e-3 \\\n    --lr-multiplier 1e2 \\\n    --scale-lr-layer \"classification_head\" \\\n    --result-dir {work_dir}  \\\n    --micro-batch-size 2 \\\n    --num-gpus 1 \\\n    --precision \"bf16-mixed\"\n```\n\n----------------------------------------\n\nTITLE: Evo2 Preprocessing Configuration Schema\nDESCRIPTION: Pydantic model class that defines the configuration schema for preprocessing DNA/RNA sequences into Megatron-compatible IndexedDataset. Includes parameters for controlling data splitting, transformations, tokenization, and taxonomic lineage handling.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/data/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Evo2PreprocessingConfig(BaseModel):\n    \"\"\"Pydantic model class specifying the configuration schema for a preprocessed IndexedDataset (.bin, .idx).\"\"\"\n    # Collection of FASTA files to preprocess and wrap into a single IndexedDataset.\n    datapaths: list[Path] = []\n    # Output directory for the preprocessed dataset .bin/.idx.\n    output_dir: None | Path = None\n    # Output file prefix for identifying your datasets.\n    output_prefix: None | str = None\n    # Random Sequence-Level Datasplit\n    train_split: float = 0.7\n    valid_split: float = 0.2\n    test_split: float = 0.1\n    # Overwrite existing binaries. Otherwise, skip already preprocessed datasets.\n    overwrite: bool = False\n    # Raw Preprocessing Transforms\n    # For every sequence, include a reverse-complemented copy of that sequence in the dataset. Doubles the size of the dataset.\n    embed_reverse_complement: bool = False\n    # For every sequence, randomly reverse complement the sequence with the specified probability instead of using the original sequence.\n    random_reverse_complement: float = 0.0\n    # For sequences associated with taxonomic lineages specified in `taxonomy_data`, randomly drop out nodes of the lineage with the specified probability. For instance: |d__KINGDOM;p__None;c__CLASS;o__None;f__None;g__None;s__None|\n    random_lineage_dropout: float = 0.0\n    # Transcribe (DNA -> RNA) or Back-Transcribe (RNA -> DNA) the sequence before tokenization.\n    transcribe: None | Literal[\"transcribe\", \"back_transcribe\"] = None\n    # Force upper-case alphabetical characters in the `.fasta` sequences.\n    force_uppercase: bool = False\n    # Data type of the IndexedDataset. When using the byte-level tokenizer, uint8 is more than sufficient with a vocabulary size of 255 for ASCII.\n    indexed_dataset_dtype: str = \"uint8\"\n    # Tokenization Transforms\n    # Append end-of-document token to the end of each sequence.\n    append_eod: bool = False\n    # Enforce the length of the sequence, by padding shorter sequences and raising exceptions when the length is exceeded.\n    enforce_sample_length: None | int = None\n    # Run ftfy on the sequence characters prior to tokenization to fix encoding issues.\n    ftfy: bool = False\n    # Tokenizer\n    tokenizer_type: Literal[\n        \"Byte-Level\",\n        \"HuggingFace\",\n        \"SentencePiece\",\n        \"Regex\",\n        \"Megatron\",\n        \"Tiktoken\",\n    ] = \"Byte-Level\"  # Recommended for DNA / RNA sequences. All other tokenizers have not been tested, and only supported here for experimentation!\n    # For more information on the behavior of the following parameters, refer to NeMo:\n    # https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/modules/common/tokenizer_utils.py\n    vocab_file: None | Path = None\n    vocab_size: None | int = 512\n    merges_file: None | Path = None\n    tokenizer_model_name: None | str = None\n    pretrained_tokenizer_model: None | str = None\n    special_tokens: None | dict[str, str] = {}\n    fast_hf_tokenizer: bool = False\n    # Compute Configuration\n    # NOTE: If preprocessing a large amount of short individual sequences (< 1000 bp), do NOT use\n    # multiprocessing (workers > 1) because sequence-level parallel IPC will dominate the preprocessing time!\n    workers: int = 1\n    # Number of sequences to load into memory at any given time during preprocessing.\n    # Prevents OOM while doing sequence-parallel.\n    preproc_concurrency: int = 100000\n    chunksize: int = 1\n    # Data Filters\n    drop_empty_sequences: bool = False\n    # If `NNN` is detected in the sequence, drop it from the preprocessed dataset.\n    nnn_filter: bool = False\n    # RNG\n    seed: None | int = None\n    # Evo2 Taxonomic Lineage Tags\n    # SeqID Sub-String Indexing: \"ABC\" will have taxonomy data from \"A\".\n    taxonomy_data: dict[str, Evo2TaxonomyLineage] = {}\n    # Periodicity of injecting phylogenetic lineage tags in the sequence prior to tokenization.\n    prompt_spacer_length: int = 131072\n```\n\n----------------------------------------\n\nTITLE: Setting Up D3PM Interpolant\nDESCRIPTION: Initializes the Discrete Denoising Diffusion Probabilistic Model (D3PM) interpolant with appropriate distributions and schedules. Disables TF32 for numerical stability.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n\nos.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"0\"  # disable TF32 for numerical stability for blackwell testing\nfrom bionemo.moco.distributions.prior import DiscreteUniformPrior\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import D3PM\nfrom bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\nfrom bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule\n\n\nB = 32  # batch size\nD = 10  # dimension\nS = 2  # state space\n\nDEVICE = \"cuda:0\"\nprior = DiscreteUniformPrior(num_classes=S)\ntime_distribution = UniformTimeDistribution(discrete_time=True, nsteps=1000)\nnoise_schedule = DiscreteCosineNoiseSchedule(nsteps=1000)\nd3pm = D3PM(\n    time_distribution=time_distribution, prior_distribution=prior, noise_schedule=noise_schedule, device=DEVICE\n)\nschedule = DiscreteLinearInferenceSchedule(nsteps=1000, direction=\"diffusion\", device=DEVICE)\n```\n\n----------------------------------------\n\nTITLE: Implementing Single Step Function for MDLM DDPM in Python\nDESCRIPTION: Method to perform a single step of the MDLM DDPM (Denoising Diffusion Probabilistic Model) algorithm, taking the current state and model outputs to produce the next state in the denoising sequence.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndef step(logits: Tensor,\n         t: Tensor,\n         xt: Tensor,\n         dt: Tensor,\n         temperature: float = 1.0) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Initializing MDLM Class in Python\nDESCRIPTION: Constructor method for the MDLM class that sets up time distribution, prior distribution, and noise schedule. The method initializes the interpolant with specified parameters for the diffusion process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(time_distribution: TimeDistribution,\n             prior_distribution: DiscreteMaskedPrior,\n             noise_schedule: ContinuousExpNoiseTransform,\n             device: str = \"cpu\",\n             rng_generator: Optional[torch.Generator] = None)\n```\n\n----------------------------------------\n\nTITLE: DDIM Sampling Step Implementation in Python\nDESCRIPTION: Implements one step of DDIM (Denoising Diffusion Implicit Models) sampling with configurable eta parameter for sampling control.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_86\n\nLANGUAGE: python\nCODE:\n```\ndef step_ddim(model_out: Tensor,\n              t: Tensor,\n              xt: Tensor,\n              dt: Tensor,\n              mask: Optional[Tensor] = None,\n              eta: Float = 0.0,\n              center: Bool = False)\n```\n\n----------------------------------------\n\nTITLE: Configuring ESM2 Model in Python for BioNeMo Framework\nDESCRIPTION: This snippet creates an ESM2Config object with various parameters for model configuration, including sequence length, number of layers, hidden size, and precision settings. It also initializes a BionemoLightningModule with the created configuration and a custom optimizer.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/pretrain.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create model config\nesm2_config = ESM2Config(\n    seq_length=max_seq_length,\n    num_layers=num_layers,\n    hidden_size=hidden_size,\n    num_attention_heads=num_attention_heads,\n    ffn_hidden_size=ffn_hidden_size,\n    params_dtype=get_autocast_dtype(precision),\n    pipeline_dtype=get_autocast_dtype(precision),\n    autocast_dtype=get_autocast_dtype(precision),  # setting this speeds things up a lot\n    biobert_spec_option=biobert_spec_option,\n    nemo1_ckpt_path=str(nemo1_init_path) if nemo1_init_path is not None else None,\n    initial_ckpt_path=str(restore_from_checkpoint_path) if restore_from_checkpoint_path is not None else None,\n    variable_seq_lengths=need_megatron_variable_seq_lengths_reductions,\n)\n\n# Create model instance\ntokenizer = get_tokenizer()\n\nmodel: BionemoLightningModule = biobert_lightning_module(\n    esm2_config,\n    tokenizer=tokenizer,\n    optimizer=MegatronOptimizerModule(\n        config=OptimizerConfig(\n            lr=lr,\n            optimizer=\"adam\",\n            use_distributed_optimizer=True,\n            weight_decay=0.01,\n            adam_beta1=0.9,\n            adam_beta2=0.98,\n        ),\n        lr_scheduler=WarmupAnnealDecayHoldScheduler(\n            warmup_steps=warmup_steps, max_steps=num_steps, max_lr=lr, min_lr=lr / 10.0, anneal_percentage=0.10\n        ),\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Sequence Parsing and FASTA Generation\nDESCRIPTION: Functions to parse reference and variant sequences from chromosome 17 and generate FASTA files. Includes sequence window extraction and validation checks for SNV positions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef parse_sequences(pos, ref, alt, seq_chr17, window_size=8192):\n    \"\"\"Parse reference and variant sequences from the reference genome sequence.\n\n    Parameters:\n    -----------\n    pos : int\n        Position (1-indexed)\n    ref : str\n        Reference base\n    alt : str\n        Alternate base\n    seq_chr17 : str\n        Full chromosome 17 sequence\n    window_size : int\n        Size of the sequence window to extract\n\n    Returns:\n    --------\n    tuple\n        (reference_sequence, variant_sequence)\n    \"\"\"\n    p = pos - 1  # Convert to 0-indexed position\n    full_seq = seq_chr17\n\n    ref_seq_start = max(0, p - window_size // 2)\n    ref_seq_end = min(len(full_seq), p + window_size // 2)\n    ref_seq = seq_chr17[ref_seq_start:ref_seq_end]\n    snv_pos_in_ref = min(window_size // 2, p)\n    var_seq = ref_seq[:snv_pos_in_ref] + alt + ref_seq[snv_pos_in_ref + 1 :]\n\n    # Sanity checks\n    assert len(var_seq) == len(ref_seq)\n    assert ref_seq[snv_pos_in_ref] == ref\n    assert var_seq[snv_pos_in_ref] == alt\n\n    return ref_seq, var_seq\n```\n\n----------------------------------------\n\nTITLE: Implementing VDM (Variational Diffusion Models) Class in Python\nDESCRIPTION: Class implementation for Variational Diffusion Models interpolant that inherits from the Interpolant base class. It provides methods for interpolation, forward process, and prediction processing in diffusion models.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_77\n\nLANGUAGE: python\nCODE:\n```\nclass VDM(Interpolant)\n```\n\n----------------------------------------\n\nTITLE: Implementing ESM-2 Fine-tuned Model with Custom Regression Head\nDESCRIPTION: A fine-tuned ESM-2 model class that extends the base ESM2Model to include a custom regression head. This implementation allows for freezing the encoder parameters optionally and adds the task-specific head for sequence-level prediction.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ESM2FineTuneSeqModel(ESM2Model):\n    def __init__(self, config, *args, post_process: bool = True, include_embeddings: bool = False, **kwargs):\n        super().__init__(config, *args, post_process=post_process, include_embeddings=True, **kwargs)\n\n        # freeze encoder parameters\n        if config.encoder_frozen:\n            for _, param in self.named_parameters():\n                param.requires_grad = False\n\n        if post_process:\n            self.regression_head = MegatronMLPHead(config)\n\n    def forward(self, *args, **kwargs,):\n        output = super().forward(*args, **kwargs)\n        ...\n        output[\"regression_output\"] = self.regression_head(embeddings)\n        return output\n```\n\n----------------------------------------\n\nTITLE: Running Sequence Prediction with Evo2\nDESCRIPTION: Bash command for running sequence prediction using a trained Evo2 model. Shows configuration for predicting log-probability scores from a FASTA file using specified model parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npredict_evo2 \\\n  --fasta <fasta_path> \\\n  --ckpt-dir <PATH_TO_CHECKPOINT> \\\n  --output-dir <PATH_TO_OUTPUT_FILE> \\\n  --model-size 1b \\\n  --tensor-parallel-size 1 \\\n  ----pipeline-model-parallel-size 1 \\\n  --context-parallel-size 1 \\\n  --output-log-prob-seqs\n```\n\n----------------------------------------\n\nTITLE: Initializing Discrete Flow Matcher in PyTorch\nDESCRIPTION: Constructor for the DiscreteFlowMatcher class that initializes the interpolant with time distribution, prior distribution for discrete masked tokens, device specification, and optional random number generator for reproducibility.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_55\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(time_distribution: TimeDistribution,\n             prior_distribution: DiscretePriorDistribution,\n             device: str = \"cpu\",\n             eps: Float = 1e-5,\n             rng_generator: Optional[torch.Generator] = None)\n```\n\n----------------------------------------\n\nTITLE: Implementing Discrete Prior Distribution Base Class\nDESCRIPTION: Abstract base class for discrete prior distributions that extends PriorDistribution. Includes initialization with number of classes and prior distribution tensor.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass DiscretePriorDistribution(PriorDistribution):\n    def __init__(num_classes: int, prior_dist: Tensor)\n    def get_num_classes() -> int\n    def get_prior_dist() -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Creating an MLP Head for ESM-2 Sequence-level Regression\nDESCRIPTION: A custom MLP head implemented as a MegatronModule for sequence-level regression tasks. It creates a multi-layer perceptron with configurable hidden sizes, activation function, and dropout for fine-tuning ESM-2 models.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MegatronMLPHead(MegatronModule):\n    def __init__(self, config: TransformerConfig):\n        super().__init__(config)\n        layer_sizes = [config.hidden_size, 256, 1]\n        self.linear_layers = torch.nn.ModuleList(\n            [torch.nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])]\n        )\n        self.act = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(p=config.ft_dropout)\n\n    def forward(self, hidden_states: torch.Tensor) -> List[torch.Tensor]:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Size-Aware Batching Function in Python\nDESCRIPTION: A generator function that batches elements from an iterable while ensuring the total size of each batch does not exceed a specified maximum. It's useful for handling datasets with varying element sizes, particularly for memory management.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef size_aware_batching(\n    dataset: Iterable[Data],\n    sizeof: Callable[[Data], Real],\n    max_total_size: Real,\n    collate_fn: Optional[Callable[[Iterable[Data]], BatchCollated]] = None,\n    info_logger: Optional[Callable[[str], None]] = None,\n    warn_logger: Optional[Callable[[str], None]] = None\n) -> Iterator[Union[List[Data], BatchCollated]]\n```\n\n----------------------------------------\n\nTITLE: Discrete Flow Matching Implementation in PyTorch\nDESCRIPTION: A class implementing Discrete Flow Matching (DFM) interpolation for generative modeling. This interpolant handles the transition between discrete states and provides methods for training and sampling.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nclass DiscreteFlowMatcher(Interpolant)\n```\n\nA Discrete Flow Model (DFM) interpolant.\n```\n\n----------------------------------------\n\nTITLE: Creating ESMDataModule for ESM-2 Pretraining\nDESCRIPTION: Initializes the ESMDataModule with training and validation data paths, sequence length constraints, and masking strategy. The data module handles UniRef50 cluster centers and UniRef90 sequences for both training and validation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/pretrain.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.esm2.data.datamodule import ESMDataModule\nfrom bionemo.esm2.data.dataset import RandomMaskStrategy\nfrom bionemo.esm2.data.tokenizer import get_tokenizer\n\ndata_path = __your_downloaded_test_data_path__  # fill your path from the command line output\n\ntrain_cluster_path = f\"{data_path}/2024_03_sanity/train_clusters_sanity.parquet\"\ntrain_database_path = f\"{data_path}/2024_03_sanity/train_sanity.db\"\nvalid_cluster_path = f\"{data_path}/2024_03_sanity/valid_clusters.parquet\"\nvalid_database_path = f\"{data_path}/2024_03_sanity/validation.db\"\n\nmin_seq_length = None  # optional; filter sequences by minimum length if given\nmax_seq_length = 128  # required; default to 1024\n\nnum_dataset_workers = 1\nrandom_mask_strategy = RandomMaskStrategy.ALL_TOKENS  # default in BioNemo2 and HuggingFace implementation\n\ndata = ESMDataModule(\n    train_cluster_path=train_cluster_path,  # UniRef50 training cluster centers\n    train_database_path=train_database_path,  # UniRef90 training sequences\n    valid_cluster_path=valid_cluster_path,  # UniRef50 validation cluster centers\n    valid_database_path=valid_database_path,  # UniRef90 validation sequences\n    global_batch_size=global_batch_size,\n    micro_batch_size=micro_batch_size,\n    min_seq_length=min_seq_length,\n    max_seq_length=max_seq_length,\n    num_workers=num_dataset_workers,\n    random_mask_strategy=random_mask_strategy,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Model and Optimizer for DFM Training\nDESCRIPTION: Creates an instance of the neural network model and sets up the Adam optimizer with a learning rate of 1e-5 for training.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = Model(D, S)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n```\n\n----------------------------------------\n\nTITLE: Training Discrete Flow Matching Model\nDESCRIPTION: Implements the training loop for the Discrete Flow Matching model. For each iteration, generates random binary sequences with a variable number of ones, applies the DFM interpolation, computes the loss, and updates model parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = model.to(DEVICE)\nlosses = []\nfor _ in tqdm(range(50000)):\n    num_ones = torch.randint(0, D + 1, (B,))\n    x1 = (torch.arange(D)[None, :] < num_ones[:, None]).long().to(DEVICE)\n    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n    optimizer.zero_grad()\n    x0 = dfm.sample_prior(x1.shape)  # B x D\n    t = dfm.sample_time(B)\n    xt = dfm.interpolate(x1, t, x0)\n    logits = model(xt, t)  # (B, D, S)\n    loss = dfm.loss(logits, x1, t).mean()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n```\n\n----------------------------------------\n\nTITLE: Generating ESM-2 Recipe Configuration with BioNeMo\nDESCRIPTION: Shows how to generate a configuration file for ESM-2 training using the bionemo-esm2-recipe command. It includes setting up data paths and specifying the recipe type.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/training-models.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nTEST_DATA_DIR=$(download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source $MY_DATA_SOURCE); \\\nbionemo-esm2-recipe \\\n--train-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet     \\\n--train-database-path ${TEST_DATA_DIR}/2024_03_sanity/train_sanity.db     \\\n--valid-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/valid_clusters.parquet     \\\n--valid-database-path ${TEST_DATA_DIR}/2024_03_sanity/validation.db     \\\n--result-dir ./results     \\\n--dest my_config.yaml\\\n--recipe esm2_8m_recipe\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Lightning Trainer for ESM-2 Pretraining\nDESCRIPTION: Configures the PyTorch Lightning Trainer with appropriate callbacks for perplexity logging, model summary, and learning rate monitoring. Sets up training parameters such as number of steps, validation interval, and precision type.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/pretrain.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import LearningRateMonitor, RichModelSummary\nfrom bionemo.llm.lightning import PerplexityLoggingCallback\n\nnum_steps = 20\nlimit_val_batches = 2  # limit the validation epoch to 2 batches\nval_check_interval = 10  # validation epoch every 10 steps\nprecision = \"bf16-mixed\"  # use bf16-mixed precision\n\ntrainer = nl.Trainer(\n    devices=devices,\n    max_steps=num_steps,\n    accelerator=\"gpu\",\n    strategy=strategy,\n    limit_val_batches=limit_val_batches,\n    val_check_interval=val_check_interval,\n    num_nodes=num_nodes,\n    callbacks=[\n        PerplexityLoggingCallback(),\n        RichModelSummary(max_depth=4),\n        LearningRateMonitor(),\n    ],\n    plugins=nl.MegatronMixedPrecision(precision=precision),  # precision is handled through plugins in BioNeMo2\n)\n```\n\n----------------------------------------\n\nTITLE: Loss Weight Calculation\nDESCRIPTION: Calculates loss weights based on different strategies (ones, data_to_noise, noise_to_data) derived from research paper equations.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_130\n\nLANGUAGE: python\nCODE:\n```\ndef loss_weight(raw_loss: Tensor, t: Optional[Tensor],\n                weight_type: str) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Implementing Self Path Planning for MDLM in Python\nDESCRIPTION: Implementation of Self Path Planning (P2) Sampling from Peng et al., allowing for strategic unmasking of tokens based on confidence or random scoring, with control over temperature and randomness parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_50\n\nLANGUAGE: python\nCODE:\n```\ndef step_self_path_planning(logits: Tensor,\n                            xt: Tensor,\n                            t: Tensor,\n                            curr_step: int,\n                            num_steps: int,\n                            logit_temperature: float = 1.0,\n                            randomness: float = 1.0,\n                            confidence_temperature: float = 1.0,\n                            score_type: Literal[\"confidence\",\n                                                \"random\"] = \"confidence\",\n                            fix_mask: Optional[Tensor] = None) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Kabsch Alignment Algorithm in Python\nDESCRIPTION: Method to perform Kabsch alignment on batched tensors. Takes batched target and noise tensors as input and returns the rotation matrix and aligned target.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_74\n\nLANGUAGE: python\nCODE:\n```\ndef batch_kabsch_align(target: Tensor, noise: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Converting Logits to Probabilities Function\nDESCRIPTION: Implements a function to convert model logits to probabilities by filtering non-amino acid tokens and applying softmax transformation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef logits_to_probs(logits: torch.Tensor, tokenizer: BioNeMoESMTokenizer = get_tokenizer()) -> torch.Tensor:\n    \"\"\"Convert token logits to probabilities.\n\n    Args:\n        logits (torch.Tensor): logits tensor with the [batch, sequence, hidden] dimensions\n        tokenizer (BioNeMoESMTokenizer): ESM2 tokenizer\n\n    Returns:\n        probabilities (torch.Tensor): probability tensor with [batch, sequence, tokenizer.vocab_size]\n    \"\"\"\n    aa_tokens = [\"L\", \"A\", \"G\", \"V\", \"S\", \"E\", \"R\", \"T\", \"I\", \"D\", \"P\", \"K\", \"Q\", \"N\", \"F\", \"Y\", \"M\", \"H\", \"W\", \"C\"]\n    extra_indices = [i for i, token in enumerate(tokenizer.all_tokens) if token not in aa_tokens]\n\n    aa_logits = logits[..., : tokenizer.vocab_size]  # filter out the 95 paddings and only keep 33 vocab positions\n    aa_logits[..., extra_indices] = -torch.inf  # force non-amino acid token probs to zero\n    return torch.softmax(aa_logits, dim=-1)\n```\n\n----------------------------------------\n\nTITLE: Continuous Time Integration Step Function in Python\nDESCRIPTION: Performs one step integration with options for temperature control and masking. The temperature parameter controls the trade-off between diversity and sample quality.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_84\n\nLANGUAGE: python\nCODE:\n```\ndef step(model_out: Tensor,\n         t: Tensor,\n         xt: Tensor,\n         dt: Tensor,\n         mask: Optional[Tensor] = None,\n         center: Bool = False,\n         temperature: Float = 1.0)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Position-Specific Mutation Probabilities\nDESCRIPTION: Creates a sorted DataFrame of tokens and their probabilities for a specific position, helping identify the most likely amino acid substitutions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Sort tokens by probability\ntoken_ids_sort = sorted(enumerate(probas_final[index_of_interest]), key=lambda x: x[1], reverse=True)\n\ntokens_sort = [(tokenizer.all_tokens[i], i, p.item()) for i, p in token_ids_sort]\n\ntokens_sort_df = pd.DataFrame(tokens_sort, columns=[\"Token\", \"Token ID\", \"Probability\"])\ntokens_sort_df.head()\n```\n\n----------------------------------------\n\nTITLE: WebDataModule Usage Example\nDESCRIPTION: Comprehensive example showing WebDataModule configuration and instantiation including tar file handling, pipeline setup, and dataloader configuration\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-webdatamodule/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from bionemo.webdatamodule.datamodule import Split, WebDataModule\n>>>\n>>> tar_file_prefix = \"shards\"\n>>>\n>>> dirs_of_tar_files = {\n>>>     Split.train: \"/path/to/train/split/tars\",\n>>>     Split.val: \"/path/to/val/split/tars\",\n>>> }\n>>>\n>>> n_samples {\n>>>     Split.train: 1000,\n>>>     Split.val: 100,\n>>> }\n>>>\n>>> suffix_keys_wds = \"tensor.pyd\"\n>>>\n>>> seed = 27193781\n>>>\n>>> untuple = lambda source : (sample for (sample,) in source)\n>>>\n>>> from webdatast import shuffle\n>>> pipeline_wds = {\n>>>     Split.train : [untuple, shuffle(n_samples[Split.train],\n>>>                                     rng=random.Random(seed_rng_shfl))],\n>>>     Split.val: untuple\n>>> }\n>>>\n>>> batch = batched(local_batch_size, collation_fn=lambda\n                    list_samples : torch.vstack(list_samples))\n>>> pipeline_prebatch_wld = {\n        Split.train: [shuffle(n_samples[Split.train],\n                              rng=random.Random(seed_rng_shfl)), batch],\n        Split.val : batch,\n        Split.test : batch\n    }\n>>>\n>>> kwargs_wds = {\n>>>     split : {'shardshuffle' : split == Split.train,\n>>>              'nodesplitter' : wds.split_by_node,\n>>>              'seed' : seed_rng_shfl}\n>>>     for split in Split\n>>>     }\n>>>\n>>> kwargs_wld = {\n>>>     split : {\"num_workers\": 2} for split in Split\n>>>     }\n>>>\n>>> invoke_wds = {\n>>>     split: [(\"with_epoch\", {\"nbatches\" : 5})] for split in Split\n>>>     }\n>>>\n>>> invoke_wld = {\n>>>     split: [(\"with_epoch\", {\"nbatches\" : 5}] for split in Split\n>>>     }\n>>>\n>>> data_module = WebDataModule(suffix_keys_wds,\n                                dirs_of_tar_files,\n                                prefix_tars_wds=tar_file_prefix,\n                                pipeline_wds=pipeline_wds,\n                                pipeline_prebatch_wld=pipeline_prebatch_wld,\n                                kwargs_wds=kwargs_wds,\n                                kwargs_wld=kwargs_wld,\n                                invoke_wds=invoke_wds,\n                                invoke_wld=invoke_wld,\n                                )\n```\n\n----------------------------------------\n\nTITLE: Using General Step Functions in bionemo-moco in Python\nDESCRIPTION: This snippet demonstrates how to use the general_step() function in bionemo-moco to access different step functions for various interpolants. It shows examples for both DDPM and Continuous Flow Matcher interpolants.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/blog.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nddpm.general_step(\"step_ddim\", {\"model_out\": model_output, \"t\": time, \"xt\": x_t})\n\ncfm.general_step(\"step_score_stochastic\", {\"model_out\": model_out, \"xt\": xt, \"dt\": dt, \"t\": time})\n```\n\n----------------------------------------\n\nTITLE: Training Evo2 Model Command-Line Interface Help\nDESCRIPTION: This command displays the complete help documentation for the train_evo2 script, showing all available parameters for configuring Evo2 model training including dataset options, distributed training parameters, model architecture settings, optimization parameters, and logging configurations.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ train_evo2 --help\nusage: train_evo2 [-h] (-d DATASET_CONFIG | --mock-data) [--dataset-dir DATASET_DIR] [--num-nodes NUM_NODES] [--devices DEVICES] [--seq-length SEQ_LENGTH] [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n                  [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE] [--context-parallel-size CONTEXT_PARALLEL_SIZE] [--create-tensorboard-logger]\n                  [--wandb-entity WANDB_ENTITY] [--wandb-project WANDB_PROJECT] [--wandb-tags WANDB_TAGS [WANDB_TAGS ...]] [--wandb-group WANDB_GROUP] [--wandb-job-type WANDB_JOB_TYPE] [--wandb-id WANDB_ID]\n                  [--wandb-anonymous] [--wandb-log-model] [--wandb-offline] [--sequence-parallel] [--fp8] [--micro-batch-size MICRO_BATCH_SIZE] [--global-batch-size GLOBAL_BATCH_SIZE] [--grad-acc-batches GRAD_ACC_BATCHES]\n                  [--max-steps MAX_STEPS] [--early-stop-on-step EARLY_STOP_ON_STEP] [--val-check-interval VAL_CHECK_INTERVAL] [--grad-reduce-in-fp32] [--fp8-wgrad] [--use-megatron-comm-overlap-llama3-8k] [--tp-comm-overlap-backend {nccl,mpi,gloo}]\n                  [--align-param-gather] [--model-size {1b,1b_nv,40b,40b_arc_longcontext,40b_nv,7b,7b_arc_longcontext,7b_nv,test,test_nv}] [--add-bias-output] [--result-dir RESULT_DIR] [--experiment-name EXPERIMENT_NAME]\n                  [--limit-val-batches LIMIT_VAL_BATCHES] [--log-every-n-steps LOG_EVERY_N_STEPS] [--ckpt-dir CKPT_DIR] [--wd WD] [--restore-optimizer-from-ckpt] [--no-average-in-collective] [--seed SEED]\n                  [--workers WORKERS] [--gc-interval GC_INTERVAL] [--enable-preemption] [--ckpt-async-save] [--ckpt-format {torch_dist,zarr}] [--eod-pad-in-loss-mask] [--cross-entropy-loss-fusion] [--no-fp32-residual-connection]\n                  [--debug-ddp-parity-freq DEBUG_DDP_PARITY_FREQ] [--hybrid-override-pattern HYBRID_OVERRIDE_PATTERN] [--num-layers NUM_LAYERS] [--create-tflops-callback] [--log-parameters-and-shapes] [--lr LR] [--min-lr MIN_LR]\n                  [--warmup-steps WARMUP_STEPS] [--nsys-profiling] [--nsys-start-step NSYS_START_STEP] [--nsys-end-step NSYS_END_STEP] [--no-renormalize-loss] [--nsys-ranks NSYS_RANKS [NSYS_RANKS ...]]\n                  [--activation-checkpoint-recompute-num-layers ACTIVATION_CHECKPOINT_RECOMPUTE_NUM_LAYERS] [--disable-checkpointing] [--clip-grad CLIP_GRAD] [--seq-len-interpolation-factor SEQ_LEN_INTERPOLATION_FACTOR]\n                  [--overlap-param-gather] [--overlap-grad-reduce] [--hidden-dropout HIDDEN_DROPOUT] [--attention-dropout ATTENTION_DROPOUT] [--no-activation-checkpointing | --selective-activation-checkpointing]\n\nTrain a Hyena model using NeMo 2.0.\n\noptions:\n  -h, --help            show this help message and exit\n  -d DATASET_CONFIG, --dataset-config DATASET_CONFIG\n                        Path to the blended / weighted training dataset configuration YAML. (default: None)\n  --mock-data           Train with Mock data (for testing/debugging), either set this or provide a dataset config. (default: False)\n  --dataset-dir DATASET_DIR\n                        Absolute path to the dataset directory. Defaults to using the absolute or relative paths (dataset_prefix) specified in the dataset config YAML. (default: None)\n  --num-nodes NUM_NODES\n                        Number of nodes to use for training, defaults to 1. (default: 1)\n  --devices DEVICES     Number of devices to use for training, defaults to 1. (default: 1)\n  --seq-length SEQ_LENGTH\n                        Training sequence length (default: 8192)\n  --tensor-parallel-size TENSOR_PARALLEL_SIZE\n                        Order of tensor parallelism. Defaults to 1. (default: 1)\n  --pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE\n                        Order of pipeline parallelism. Defaults to 1. (default: 1)\n  --context-parallel-size CONTEXT_PARALLEL_SIZE\n                        Order of context parallelism. Defaults to 1. (default: 1)\n  --create-tensorboard-logger\n                        Create a tensorboard logger. (default: False)\n  --wandb-entity WANDB_ENTITY\n                        The team posting this run (default: None)\n  --wandb-project WANDB_PROJECT\n                        Wandb project name (default: None)\n  --wandb-tags WANDB_TAGS [WANDB_TAGS ...]\n                        Tags associated with this run (default: None)\n  --wandb-group WANDB_GROUP\n                        A unique string shared by all runs in a given group (default: None)\n  --wandb-job-type WANDB_JOB_TYPE\n                        A unique string representing a type of run, which is useful when you're grouping runs together into larger experiments using group. (default: None)\n  --wandb-id WANDB_ID   Sets the version, mainly used to resume a previous run (default: None)\n  --wandb-anonymous     Enable or explicitly disable anonymous logging (default: False)\n  --wandb-log-model     Save checkpoints in wandb dir to upload on W&B servers (default: False)\n  --wandb-offline       Use wandb in offline mode (default: False)\n  --sequence-parallel   Set to enable sequence parallelism. (default: False)\n  --fp8                 Set to enable FP8 (default: False)\n  --micro-batch-size MICRO_BATCH_SIZE\n                        Micro-batch size for data-parallel training. (default: 1)\n  --global-batch-size GLOBAL_BATCH_SIZE\n                        Global batch size for training. If set to None, infer it from the TP, CP, and PP parameters. (default: None)\n  --grad-acc-batches GRAD_ACC_BATCHES\n                        Number of batches to accumulate gradients over. (default: 1)\n  --max-steps MAX_STEPS\n                        Number of training optimizer update steps. This controls the total number of steps as well as the shape of the learning rate curve. (default: 500000)\n  --early-stop-on-step EARLY_STOP_ON_STEP\n                        Stop training on this step, if set. This may be useful for testing or debugging purposes. (default: None)\n  --val-check-interval VAL_CHECK_INTERVAL\n                        Number of steps between validation measurements and model checkpoints. (default: None)\n  --grad-reduce-in-fp32\n                        Gradient reduce in FP32. (default: False)\n  --fp8-wgrad           Faster option that is maybe less accurate (TBD) when using fp8. (default: False)\n  --use-megatron-comm-overlap-llama3-8k\n  --tp-comm-overlap-backend {nccl,mpi,gloo}\n                        TP communication backend to use. Defaults to 'nccl'. (default: nccl)\n  --align-param-gather\n  --model-size {1b,1b_nv,40b,40b_arc_longcontext,40b_nv,7b,7b_arc_longcontext,7b_nv,test,test_nv}\n                        Model architecture to use, choose between 7b, 40b, or test (a sub-model of 4 layers, less than 1B parameters). '_arc_1m' models have GLU / FFN dimensions that support 1M context length when trained with TP<=8. (default: 7b)\n  --add-bias-output     Add bias to the output layer to enable learning a simple prior. (default: False)\n  --result-dir RESULT_DIR\n                        Path to the result directory. (default: results)\n  --experiment-name EXPERIMENT_NAME\n                        Name of the experiment. (default: evo2)\n  --limit-val-batches LIMIT_VAL_BATCHES\n                        Number of validation steps (default: 20)\n  --log-every-n-steps LOG_EVERY_N_STEPS\n                        Number of steps between logging. (default: 1)\n  --ckpt-dir CKPT_DIR   Directory to restore an initial checkpoint from. Use this for supervised fine-tuning. (default: None)\n  --wd WD               Weight decay for optimizer. (default: 0.01)\n  --restore-optimizer-from-ckpt\n                        Restore optimizer state from initial checkpoint. Defaults to False. (default: False)\n  --no-average-in-collective\n                        Avaerage optimizer state in collective rather than dividing by dp size and summing. (default: False)\n  --seed SEED           Set random seed for training. (default: 1234)\n  --workers WORKERS     Number of workers to use for data loading. (default: 8)\n  --gc-interval GC_INTERVAL\n                        Set to a value > 0 if you want to synchronize garbage collection, will do gc every gc-interval steps. (default: 0)\n  --enable-preemption   Enable preemption hooks. If enabled this will save a checkpoint whenever slurm exits. (default: False)\n  --ckpt-async-save\n  --ckpt-format {torch_dist,zarr}\n                        Specify checkpoint format to use. Defaults to 'torch_dist', as 'zarr' is deprecated. Only use if resuming training from a zarr checkpoint. (default: torch_dist)\n  --eod-pad-in-loss-mask\n                        Do not predict EOD/Pad tokens (typical default, but not default in original evo2). (default: False)\n  --cross-entropy-loss-fusion\n                        Use the faster, but maybe less accurate fused form of cross entropy, which also has bf16 grads internally. (default: False)\n  --no-fp32-residual-connection\n                        If set, turn off fp32 residual connections which may be faster but may impact accuracy. (default: False)\n  --debug-ddp-parity-freq DEBUG_DDP_PARITY_FREQ\n                        Set to value > 0 to debug DDP weight parity between ranks. (default: 0)\n  --hybrid-override-pattern HYBRID_OVERRIDE_PATTERN\n                        Override the hybrid override pattern in the config (specifies hyena layer ordering and type). (default: None)\n  --num-layers NUM_LAYERS\n                        If set, override the number of layers specified in the requested config. (default: None)\n  --create-tflops-callback\n                        Enable tflops calculation callback for Hyena / Evo2. Defaults to False. (default: False)\n  --log-parameters-and-shapes\n                        Log training parameters shapes and dtypes for debugging. (default: False)\n  --lr LR               Learning rate. (default: 0.0003)\n```\n\n----------------------------------------\n\nTITLE: Score Estimation Function in DDPM\nDESCRIPTION: Converts data prediction to estimated score function taking predicted data point, current data point and time step as inputs.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_127\n\nLANGUAGE: python\nCODE:\n```\ndef score(x_hat: Tensor, xt: Tensor, t: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Parallel Processing Context in Python\nDESCRIPTION: A context manager that sets up and tears down a torch distributed testing environment. It configures the distributed environment with specified rank and world size parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_138\n\nLANGUAGE: python\nCODE:\n```\n@contextmanager\ndef parallel_context(rank: int = 0, world_size: int = 1)\n```\n\n----------------------------------------\n\nTITLE: Sampling from Optimal Transport Plan in Python\nDESCRIPTION: Method to draw source and target samples from the optimal transport matrix pi. It allows sampling with or without replacement based on the provided OT plan, returning indices for noise and data samples.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ndef sample_map(pi: Tensor,\n               batch_size: int,\n               replace: Bool = False) -> Tuple[Tensor, Tensor]\n```\n\n----------------------------------------\n\nTITLE: Implementing MegatronDataModule for Training Resumption (Python)\nDESCRIPTION: Demonstrates how to implement a custom DataModule inheriting from MegatronDataModule to ensure proper training resumption and consistent behavior with and without job interruption.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/background/megatron_datasets.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyDataModule(MegatronDataModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ...\n\n    def train_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"train\",\n        )\n\n    def val_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"validation\",\n        )\n\n    def test_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"test\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Computing Rotation-Aware Optimal Transport Matrix in Python\nDESCRIPTION: Method to compute the OT matrix between source and target minibatches with Kabsch alignment. It returns both the OT matrix and the rotation matrices needed to align each pair of structures optimally.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_70\n\nLANGUAGE: python\nCODE:\n```\ndef get_ot_matrix(x0: Tensor,\n                  x1: Tensor,\n                  mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]\n```\n\n----------------------------------------\n\nTITLE: Implementing SizeAwareBatchSampler Class in Python\nDESCRIPTION: A sampler class that batches elements of varying sizes while ensuring the total size of each batch does not exceed a specified maximum. It's particularly useful for datasets with elements of different sizes, such as graphs or sequences of varying lengths.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass SizeAwareBatchSampler(Sampler[List[int]]):\n    def __init__(sampler: Union[Sampler[List[int]], Iterable[int]],\n                 sizeof: Callable[[int], Real],\n                 max_total_size: Real,\n                 info_logger: Optional[Callable[[str], None]] = None,\n                 warn_logger: Optional[Callable[[str], None]] = None) -> None:\n        # Implementation details...\n\n    def __iter__() -> Iterator[List[int]]:\n        # Implementation details...\n```\n\n----------------------------------------\n\nTITLE: PyTorch Dataset Implementation with NvFaidx\nDESCRIPTION: Example implementation of a PyTorch Dataset class that uses NvFaidx to read and process FASTA sequences. Includes initialization, index mapping, and data loading functionality with tokenization support.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-noodles/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom pathlib import Path\n\nimport torch\n\nfrom bionemo.noodles.nvfaidx import NvFaidx\n\nclass SimpleFastaDataset(torch.utils.data.Dataset):\n\n    def __init__(self, fasta_path: Path, tokenizer):\n        \"\"\"Initialize the dataset.\"\"\"\n        super().__init__()\n        self.fasta = NvFaidx(fasta_path)\n        self.seqids = sorted(self.fasta.keys())\n        self.tokenizer = tokenizer\n\n    def write_idx_map(self, output_dir: Path):\n        \"\"\"Write the index map to the output directory.\"\"\"\n        with open(output_dir / \"seq_idx_map.json\", \"w\") as f:\n            json.dump({seqid: idx for idx, seqid in enumerate(self.seqids)}, f)\n\n    def __len__(self):\n        \"\"\"Get the length of the dataset.\"\"\"\n        return len(self.seqids)\n\n    def __getitem__(self, idx: int) -> dict[str, torch.Tensor]:\n        \"\"\"Get an item from the dataset.\"\"\"\n        sequence = self.fasta[self.seqids[idx]].sequence().upper()\n        tokenized_seq = self.tokenizer.text_to_ids(sequence)\n        loss_mask = torch.ones_like(torch.tensor(tokenized_seq, dtype=torch.long), dtype=torch.long)\n        return {\n            \"tokens\": torch.tensor(tokenized_seq, dtype=torch.long),\n            \"position_ids\": torch.arange(len(tokenized_seq), dtype=torch.long),\n            \"seq_idx\": torch.tensor(idx, dtype=torch.long),\n            \"loss_mask\": loss_mask,\n        }\n```\n\n----------------------------------------\n\nTITLE: Performing Single Step in Discrete Interpolant Method\nDESCRIPTION: This method performs a single step in the discrete interpolant method, transitioning from the current discrete state 'xt' at time 't' to the next state. It involves computing q-posterior logits and sampling the next state.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_107\n\nLANGUAGE: python\nCODE:\n```\ndef step(model_out: Tensor,\n         t: Tensor,\n         xt: Tensor,\n         mask: Optional[Tensor] = None,\n         temperature: Float = 1.0,\n         model_out_is_logits: bool = True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataset Sampling Weights in YAML\nDESCRIPTION: YAML configuration for specifying training data sources and their sampling weights. Defines dataset prefixes, splits (train/validation/test), and relative weights for blending multiple datasets during training.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/README.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- dataset_prefix: /workspace/bionemo2/data/metagenomics/pretraining_data_metagenomics/data_metagenomics_train_text_CharLevelTokenizer_document\n  dataset_split: train\n  dataset_weight: 0.18\n- dataset_prefix: /workspace/bionemo2/data/gtdb_imgpr/pretraining_data_gtdb_imgpr/data_gtdb_imgpr_train_text_CharLevelTokenizer_document\n  dataset_split: train\n  dataset_weight: 0.24\n- dataset_prefix: /workspace/bionemo2/data/imgvr_untagged/imgvr_untagged_data/data_imgvr_train_text_CharLevelTokenizer_document\n  dataset_split: train\n  dataset_weight: 0.03\n- dataset_prefix: /workspace/bionemo2/data/promoters/pretraining_data_promoters/data_promoters_valid_text_CharLevelTokenizer_document\n  dataset_split: validation\n  dataset_weight: 0.0003\n- dataset_prefix: /workspace/bionemo2/data/organelle/pretraining_data_organelle/data_organelle_valid_text_CharLevelTokenizer_document\n  dataset_split: validation\n  dataset_weight: 0.005\n- dataset_prefix: /workspace/bionemo2/data/metagenomics/pretraining_data_metagenomics/data_metagenomics_test_text_CharLevelTokenizer_document\n  dataset_split: test\n  dataset_weight: 0.18\n- dataset_prefix: /workspace/bionemo2/data/gtdb_v220/gtdb_v220_imgpr_merged_data/data_gtdb_imgpr_test_text_CharLevelTokenizer_document\n  dataset_split: test\n  dataset_weight: 0.24\n```\n\n----------------------------------------\n\nTITLE: Processing Inference Results\nDESCRIPTION: Loads and processes the inference results, including handling of logits and token mappings\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/inference.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.esm2.data.tokenizer import get_tokenizer\n\n\ntokenizer = get_tokenizer()\n\ntokens = tokenizer.all_tokens\nprint(f\"There are {tokenizer.vocab_size} unique tokens: {tokens}.\")\n\naa_logits = logits[..., : tokenizer.vocab_size]  # filter out the 95 paddings and only keep 33 vocab positions\nprint(f\"Logits shape after removing the paddings in hidden dimension: {aa_logits.shape}\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Sigma Values in ContinuousExpNoiseTransform\nDESCRIPTION: Method to calculate sigma values for given time steps, with support for device placement and direction synchronization.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_sigma(t: Tensor,\n                    device: Union[str, torch.device] = \"cpu\",\n                    synchronize: Optional[TimeDirection] = None) -> Tensor:\n```\n\n----------------------------------------\n\nTITLE: Computing Optimal Transport Matrix Between Source and Target Minibatches in Python\nDESCRIPTION: Method to compute the optimal transport matrix between noise and data minibatches. It takes source and target tensors with optional masking and returns the OT matrix that defines the optimal pairings.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_66\n\nLANGUAGE: python\nCODE:\n```\ndef get_ot_matrix(x0: Tensor,\n                  x1: Tensor,\n                  mask: Optional[Tensor] = None) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Stochastic Sampling from Categorical Distribution in PyTorch\nDESCRIPTION: A function for sampling from categorical distributions defined by input logits, with temperature and noise scale parameters to control the sampling diversity. Returns both the sampled tokens and their corresponding log-softmax scores.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_53\n\nLANGUAGE: python\nCODE:\n```\ndef stochastic_sample_from_categorical(logits: Tensor,\n                                       temperature: float = 1.0,\n                                       noise_scale: float = 1.0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Confidence-Based Stepping for MDLM in Python\nDESCRIPTION: Method to update input sequence by sampling from predicted logits with added Gumbel noise, following the GenMol approach from Lee et al. This technique unmasks tokens based on confidence scores.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_47\n\nLANGUAGE: python\nCODE:\n```\ndef step_confidence(logits: Tensor,\n                    xt: Tensor,\n                    curr_step: int,\n                    num_steps: int,\n                    logit_temperature: float = 1.0,\n                    randomness: float = 1.0,\n                    confidence_temperature: float = 1.0,\n                    num_tokens_unmask: int = 1) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Implementing Gaussian Prior Distribution in Python\nDESCRIPTION: Defines a GaussianPrior class for generating samples from a Gaussian distribution with specified mean and standard deviation. It supports optional centering of samples.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass GaussianPrior(PriorDistribution):\n    def __init__(mean: Float = 0.0,\n                 std: Float = 1.0,\n                 center: Bool = False,\n                 rng_generator: Optional[torch.Generator] = None) -> None:\n        # Implementation details\n\n    def sample(shape: Tuple,\n               mask: Optional[Tensor] = None,\n               device: Union[str, torch.device] = \"cpu\",\n               rng_generator: Optional[torch.Generator] = None) -> Tensor:\n        # Implementation details\n```\n\n----------------------------------------\n\nTITLE: Implementing Discrete Masked Prior Distribution in Python\nDESCRIPTION: Defines a DiscreteMaskedPrior class for generating masked samples. It supports different masking configurations and provides methods for sampling, checking masked states, and padding samples.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass DiscreteMaskedPrior(DiscretePriorDistribution):\n    def __init__(num_classes: int = 10,\n                 mask_dim: Optional[int] = None,\n                 inclusive: bool = True) -> None:\n        # Implementation details\n\n    def sample(shape: Tuple,\n               mask: Optional[Tensor] = None,\n               device: Union[str, torch.device] = \"cpu\",\n               rng_generator: Optional[torch.Generator] = None) -> Tensor:\n        # Implementation details\n\n    def is_masked(sample: Tensor) -> Tensor:\n        # Implementation details\n\n    def pad_sample(sample: Tensor) -> Tensor:\n        # Implementation details\n```\n\n----------------------------------------\n\nTITLE: Loading ESM2 Pretrain Sanity Dataset in BioNeMo\nDESCRIPTION: This code demonstrates how to load the ESM2 pretrain sanity dataset (a 10,000 UniRef50 cluster random slice) using the BioNeMo core data loading utility. The function materializes the dataset in the BioNeMo2 cache directory.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/datasets/uniprot.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.core.data.load import load\n\nsanity_data_dir = load(\"esm2/testdata_esm2_pretrain:2.0\")\n```\n\n----------------------------------------\n\nTITLE: Removing Center of Mass from Tensor Data in Python\nDESCRIPTION: Utility function to calculate and remove the center of mass from input tensor data. It supports optional masking of interactions in the calculation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef remove_center_of_mass(data: Tensor,\n                          mask: Optional[Tensor] = None) -> Tensor:\n    # Implementation details\n```\n\n----------------------------------------\n\nTITLE: Defining Prior Distribution Abstract Base Class in Python\nDESCRIPTION: Abstract base class that defines the interface for prior distributions with a required sample method that generates samples based on specified shape and optional mask.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass PriorDistribution(ABC):\n    @abstractmethod\n    def sample(shape: Tuple,\n           mask: Optional[Tensor] = None,\n           device: Union[str, torch.device] = \"cpu\") -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Implementing Interpolation for MDLM in Python\nDESCRIPTION: Method to get x(t) with given time t from noise and data, performing the interpolation between data and noise based on the time parameter in the diffusion process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ndef interpolate(data: Tensor, t: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Initializing Model and Checking Terminal Distribution for D3PM\nDESCRIPTION: Creates a new model instance for D3PM training and examines the terminal distribution to confirm the binary nature of the data.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodel = Model(D, S)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\nd3pm.terminal_distribution  # here we can see as confirmation that the distribution we are diffusing from is binary\n```\n\n----------------------------------------\n\nTITLE: ZeRO-3 Checkpoint Structure\nDESCRIPTION: Directory structure showing the expected layout of a ZeRO-3 checkpoint including optimizer states, configurations, and model states.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\narc_40b_zero3_w32_mp8_test_notfinal_ckpt/global_step1\n├── bf16_zero_pp_rank_*_mp_rank_*_optim_states.pt\n├── configs\n│   ├── 40b_test_chkpt.yml\n│   └── opengenome.yml\n└── zero_pp_rank_*_mp_rank_*_model_states.pt\n```\n\n----------------------------------------\n\nTITLE: UMAP Dimensionality Reduction on Embeddings in Python\nDESCRIPTION: This code applies UMAP (Uniform Manifold Approximation and Projection) to reduce the dimensionality of the embeddings. It uses the umap library and assumes the existence of 'inference_results' with embeddings data.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport umap\n\n\nreducer = umap.UMAP()\nembedding = reducer.fit_transform(inference_results[\"embeddings\"].float())\n```\n\n----------------------------------------\n\nTITLE: Initializing ESM2FineTuneDataModule for Fine-tuning in Python\nDESCRIPTION: This snippet demonstrates how to create a data module for ESM-2 fine-tuning using the ESM2FineTuneDataModule class. It loads data from a CSV file and sets up batch sizes for training.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndataset = InMemorySingleValueDataset.from_csv(data_path)\ndata_module = ESM2FineTuneDataModule(\n    train_dataset=dataset,\n    valid_dataset=dataset\n    micro_batch_size=4,   # size of a batch to be processed in a device\n    global_batch_size=8,  # size of batch across all devices. Should be multiple of micro_batch_size\n)\n```\n\n----------------------------------------\n\nTITLE: Data Sampling Function Implementation\nDESCRIPTION: A utility function that performs balanced or unbalanced sampling of a dataframe containing genetic variant data. Supports class balancing for LOF and FUNC/INT variants with configurable sampling parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef sample_data(df, sample_frac=1.0, balanced=True, disable=False, random_state=42):\n    \"\"\"Sample dataframe, optionally with balanced classes.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        Input dataframe\n    sample_frac : float\n        Fraction of data to sample\n    balanced : bool\n        Whether to balance classes\n    disable : bool\n        Whether to disable sampling\n    random_state : int\n        Random seed for reproducibility\n\n    Returns:\n    --------\n    pandas.DataFrame\n        Sampled dataframe\n    \"\"\"\n    if disable:\n        return df\n\n    if balanced:\n        # Get the number of rows in the dataframe\n        num_rows_minor_class = math.ceil(len(df[df[\"class\"] == \"LOF\"]) * sample_frac)\n        return (\n            pd.concat(\n                [\n                    df[df[\"class\"] == \"LOF\"].sample(n=num_rows_minor_class, random_state=random_state),\n                    df[df[\"class\"] == \"FUNC/INT\"].sample(n=num_rows_minor_class, random_state=random_state),\n                ]\n            )\n            .sample(frac=1.0, random_state=random_state)\n            .reset_index(drop=True)\n        )\n    else:\n        # Calculate the number of rows to sample\n        return df.sample(frac=sample_frac, random_state=random_state).reset_index(drop=True)\n```\n\n----------------------------------------\n\nTITLE: Initializing D3PM Interpolant for Discrete Denoising Diffusion\nDESCRIPTION: This class initializes a Discrete Denoising Diffusion Probabilistic Model (D3PM) interpolant. It sets up time distribution, prior distribution, noise schedule, and other parameters for the diffusion process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_104\n\nLANGUAGE: python\nCODE:\n```\nclass D3PM(Interpolant):\n    def __init__(time_distribution: TimeDistribution,\n                 prior_distribution: DiscretePriorDistribution,\n                 noise_schedule: DiscreteNoiseSchedule,\n                 device: str = \"cpu\",\n                 last_time_idx: int = 0,\n                 rng_generator: Optional[torch.Generator] = None)\n```\n\n----------------------------------------\n\nTITLE: Interpolation Function for Discrete Flow Matching\nDESCRIPTION: Method to compute x(t) at a given timestep by interpolating between target data and noise. This function implements the core interpolation operation for the flow matching model.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_56\n\nLANGUAGE: python\nCODE:\n```\ndef interpolate(data: Tensor, t: Tensor, noise: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Sampling from a Trained Generative Model in Python\nDESCRIPTION: This snippet demonstrates the general process of sampling from a trained generative model using an interpolant and an inference schedule. It starts with a sample from the prior distribution and iteratively updates it based on the model's predictions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/blog.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsample = interpolant.sample_prior(data.shape)  # Start with noise\nschedule = InferenceSchedule()\nfor t in schedule:\n    A = model(sample, t)\n    sample = interpolant.step(A, sample, t)\nreturn sample\n```\n\n----------------------------------------\n\nTITLE: Sampling from Trained DFM Model\nDESCRIPTION: Generates 1000 samples from the trained Discrete Flow Matching model by starting with samples from the prior distribution and iteratively applying the model's predictions through the inference schedule.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnum_samples = 1000\nxt = dfm.sample_prior((num_samples, D))\nprint(xt.shape)\nts = schedule.generate_schedule(device=DEVICE)\ndts = schedule.discretize(device=DEVICE)\n```\n\n----------------------------------------\n\nTITLE: FASTA File Generation and Management\nDESCRIPTION: Implementation for generating and managing FASTA files from variant data. Handles unique sequence tracking and file generation for both reference and variant sequences.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef generate_fasta_files(df, seq_chr17, output_dir=\"brca1_fasta_files\", window_size=8192):\n    \"\"\"Generate FASTA files for reference and variant sequences.\n\n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        Dataframe with variant information\n    seq_chr17 : str\n        Chromosome 17 sequence\n    output_dir : str\n        Output directory for FASTA files\n    window_size : int\n        Size of sequence window\n\n    Returns:\n    --------\n    pandas.DataFrame\n        Dataframe with added columns for FASTA names\n    \"\"\"\n    # Create output directory\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Paths for output files\n    ref_fasta_path = output_dir / \"brca1_reference_sequences.fasta\"\n    var_fasta_path = output_dir / \"brca1_variant_sequences.fasta\"\n\n    # Track unique sequences\n    ref_sequences = set()\n    var_sequences = set()\n    ref_seq_to_name = {}\n\n    # Store unique sequences with metadata for writing\n    ref_entries = []\n    var_entries = []\n    ref_names = []\n    var_names = []\n\n    # Collect unique reference and variant sequences\n    for idx, row in df.iterrows():\n        ref_seq, var_seq = parse_sequences(row[\"pos\"], row[\"ref\"], row[\"alt\"], seq_chr17, window_size)\n\n        # Add to sets to ensure uniqueness\n        if ref_seq not in ref_sequences:\n            ref_sequences.add(ref_seq)\n            ref_name = f\"BRCA1_ref_pos_{row['pos']}_{row['ref']}_class_{row['class']}\"\n\n            ref_entries.append(f\">{ref_name}\\n{ref_seq}\\n\")\n            ref_names.append(ref_name)\n            ref_seq_to_name[ref_seq] = ref_name\n        else:\n            ref_name = ref_seq_to_name[ref_seq]\n            ref_names.append(ref_name)\n\n        if var_seq not in var_sequences:\n            var_sequences.add(var_seq)\n            var_name = f\"BRCA1_var_pos_{row['pos']}_{row['ref']}to{row['alt']}_class_{row['class']}\"\n\n            var_entries.append(f\">{var_name}\\n{var_seq}\\n\")\n            var_names.append(var_name)\n        else:\n            assert False, \"Duplicate variant sequence\"\n\n    # Write unique sequences to FASTA files\n    with open(ref_fasta_path, \"w\") as f:\n        f.writelines(ref_entries)\n\n    with open(var_fasta_path, \"w\") as f:\n        f.writelines(var_entries)\n\n    # Add FASTA names to dataframe\n    df_with_names = df.copy()\n    df_with_names[\"ref_fasta_name\"] = ref_names\n    df_with_names[\"var_fasta_name\"] = var_names\n\n    print(f\"Total unique reference sequences: {len(ref_sequences)}\")\n    print(f\"Total unique variant sequences: {len(var_sequences)}\")\n\n    return df_with_names\n```\n\n----------------------------------------\n\nTITLE: Calculating Velocity in DDPM\nDESCRIPTION: Method to calculate the velocity term for the diffusion process given data, time step, and noise.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_123\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_velocity(data: Tensor, t: Tensor, noise: Tensor) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Identifying Amino Acid Tokens in ESM-2 Vocabulary\nDESCRIPTION: Extracts the tokens corresponding to the 20 standard amino acids from the tokenizer vocabulary. It creates two lists: one for amino acid indices and another for indices of all other tokens.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\naa_tokens = [\"L\", \"A\", \"G\", \"V\", \"S\", \"E\", \"R\", \"T\", \"I\", \"D\", \"P\", \"K\", \"Q\", \"N\", \"F\", \"Y\", \"M\", \"H\", \"W\", \"C\"]\n\naa_indices = [i for i, token in enumerate(tokens) if token in aa_tokens]\nextra_indices = [i for i, token in enumerate(tokens) if token not in aa_tokens]\n```\n\n----------------------------------------\n\nTITLE: Converting Vector Field to Score in Continuous Flow Matching\nDESCRIPTION: This function computes the score of noisy density given the vector field learned by flow matching. It uses an interpolation scheme to relate the vector field to the score.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_100\n\nLANGUAGE: python\nCODE:\n```\ndef vf_to_score(x_t: Tensor, v: Tensor, t: Tensor) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Using SCDL Datasets in PyTorch Model Training\nDESCRIPTION: Demonstrates how to use SCDL datasets with PyTorch DataLoaders for model training. It includes setting up the DataLoader with a custom collation function and a simple training loop.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/README.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.data import DataLoader\nfrom bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch\n\n## Mock model: you can remove this and pass the batch to your own model in actual code.\nmodel = lambda x : x\n\ndataloader = DataLoader(data, batch_size=8, shuffle=True, collate_fn=collate_sparse_matrix_batch)\nn_epochs = 2\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n```\n\n----------------------------------------\n\nTITLE: Initializing VDM Class with Required Parameters in Python\nDESCRIPTION: Constructor for VDM class that initializes the interpolant with time distribution, prior distribution, noise schedule, prediction type, device, and random number generator.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_78\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(time_distribution: TimeDistribution,\n             prior_distribution: PriorDistribution,\n             noise_schedule: ContinuousSNRTransform,\n             prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n             device: Union[str, torch.device] = \"cpu\",\n             rng_generator: Optional[torch.Generator] = None)\n```\n\n----------------------------------------\n\nTITLE: Implementing Forward Process for MDLM in Python\nDESCRIPTION: Method to apply the forward diffusion process to the data at the specified time t, transforming clean data into noisy data according to the noise schedule.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ndef forward_process(data: Tensor, t: Tensor) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Calculating Delta Scores for Variants\nDESCRIPTION: Computes delta scores between variant and reference sequences by calculating the difference in log probabilities, indicating the potential impact of SNVs.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nref_log_probs = []\nvar_log_probs = []\nfor _, row in brca1_df.iterrows():\n    ref_name = row[\"ref_fasta_name\"]\n    var_name = row[\"var_fasta_name\"]\n    ref_log_probs.append(ref_preds[\"log_probs_seqs\"][ref_seq_idx_map[ref_name]].item())\n    var_log_probs.append(var_preds[\"log_probs_seqs\"][var_seq_idx_map[var_name]].item())\nbrca1_df[\"ref_log_probs\"] = ref_log_probs\nbrca1_df[\"var_log_probs\"] = var_log_probs\nbrca1_df[\"evo2_delta_score\"] = brca1_df[\"var_log_probs\"] - brca1_df[\"ref_log_probs\"]\n```\n\n----------------------------------------\n\nTITLE: Interpolation Method in DDPM\nDESCRIPTION: Method to interpolate between data and noise at a given time step t, creating noisy samples for the diffusion process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_119\n\nLANGUAGE: python\nCODE:\n```\ndef interpolate(data: Tensor, t: Tensor, noise: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Identifying High-Probability Mutations\nDESCRIPTION: Analyzes mismatched positions between sequences and identifies the mutation with the highest probability, including position and amino acid change details.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Collect indices where a mutant is suggested over the wild-type\nmatches = [c1 == c2 for c1, c2 in zip(pred_seq, true_seq)]\nmismatch_index = [i for i, value in enumerate(matches) if not value]\n\n# Filter probability matrix to mismatches-only\nprobas_mismatch = probas_final[mismatch_index, :]\n\n# Find index of mutant with highest likelihood\nindex_flat = np.argmax(probas_mismatch)\nindex_2d = np.unravel_index(index_flat, probas_mismatch.shape)\nindex_of_interest = mismatch_index[index_2d[0]]\nposition_of_interest = positions[index_of_interest]\nprint(\"Position:\", position_of_interest)\nprint(\"Mutation:\", true_seq[position_of_interest] + str(position_of_interest) + pred_seq[position_of_interest])\n```\n\n----------------------------------------\n\nTITLE: BRCA1 Data Processing Functions\nDESCRIPTION: Defines utility functions for downloading and processing BRCA1 variant data, including genome sequence loading and dataframe preprocessing from Excel files.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef download_data(data_dir=\"brca1\", commit_hash=\"3819474bee6c24938016614411f1fa025e542bbe\"):\n    \"\"\"Download required data files if they don't exist locally.\n\n    Parameters:\n    -----------\n    data_dir : str\n        Directory to store downloaded files\n    commit_hash : str\n        GitHub commit hash for data version\n    \"\"\"\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    excel_path = os.path.join(data_dir, \"41586_2018_461_MOESM3_ESM.xlsx\")\n    genome_path = os.path.join(data_dir, \"GRCh37.p13_chr17.fna.gz\")\n\n    if not os.path.exists(excel_path):\n        os.system(\n            f\"wget https://github.com/ArcInstitute/evo2/raw/{commit_hash}/notebooks/brca1/41586_2018_461_MOESM3_ESM.xlsx -O {excel_path}\"\n        )\n\n    if not os.path.exists(genome_path):\n        os.system(\n            f\"wget https://github.com/ArcInstitute/evo2/raw/{commit_hash}/notebooks/brca1/GRCh37.p13_chr17.fna.gz -O {genome_path}\"\n        )\n\n    return excel_path, genome_path\n\n\ndef load_genome_sequence(genome_path):\n    \"\"\"Load genome sequence from FASTA file.\n\n    Parameters:\n    -----------\n    genome_path : str\n        Path to the genome FASTA file\n\n    Returns:\n    --------\n    str\n        Genome sequence string\n    \"\"\"\n    with gzip.open(genome_path, \"rt\") as handle:\n        for record in SeqIO.parse(handle, \"fasta\"):\n            return str(record.seq)\n\n    raise ValueError(\"Failed to parse genome sequence\")\n\n\ndef load_brca1_data(excel_path):\n    \"\"\"Load and preprocess BRCA1 data from Excel file.\n\n    Parameters:\n    -----------\n    excel_path : str\n        Path to the Excel file\n\n    Returns:\n    --------\n    pandas.DataFrame\n        Processed BRCA1 dataframe\n    \"\"\"\n    # Load the dataframe\n    brca1_df = pd.read_excel(excel_path, header=2)\n\n    # Select and rename columns\n    brca1_df = brca1_df[\n        [\n            \"chromosome\",\n            \"position (hg19)\",\n            \"reference\",\n            \"alt\",\n            \"function.score.mean\",\n            \"func.class\",\n        ]\n    ]\n\n    brca1_df.rename(\n        columns={\n            \"chromosome\": \"chrom\",\n            \"position (hg19)\": \"pos\",\n            \"reference\": \"ref\",\n            \"alt\": \"alt\",\n            \"function.score.mean\": \"score\",\n            \"func.class\": \"class\",\n        },\n        inplace=True,\n    )\n\n    # Convert to two-class system\n    brca1_df[\"class\"] = brca1_df[\"class\"].replace([\"FUNC\", \"INT\"], \"FUNC/INT\")\n\n    return brca1_df\n```\n\n----------------------------------------\n\nTITLE: Installing BioNeMo Core Package\nDESCRIPTION: Command to install the BioNeMo core package in development mode using pip. This allows for local development and testing of the package.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-core/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Initializing DDPM Interpolant in PyTorch\nDESCRIPTION: Constructor for the DDPM class that initializes the diffusion model with time distribution, prior distribution, noise schedule, and other parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_111\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(time_distribution: TimeDistribution,\n             prior_distribution: PriorDistribution,\n             noise_schedule: DiscreteNoiseSchedule,\n             prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n             device: Union[str, torch.device] = \"cpu\",\n             last_time_idx: int = 0,\n             rng_generator: Optional[torch.Generator] = None)\n```\n\n----------------------------------------\n\nTITLE: Configuring DNA Sequence Preprocessing in YAML\nDESCRIPTION: YAML configuration file for processing FASTA sequence data, specifying data paths, output settings, and preprocessing parameters like reverse complement embedding and tokenization options.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/data/README.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndatapaths: [\"/workspace/bionemo2/data/mmseqs_results_rep_seq_distinct.fasta\"]\noutput_dir: \"/workspace/bionemo2/data\"\noutput_prefix: promoters_uint8_distinct\ntrain_split: 1.0\nvalid_split: 0.0\ntest_split: 0.0\noverwrite: True\nembed_reverse_complement: true\nrandom_reverse_complement: 0.0\nrandom_lineage_dropout: 0.0\ninclude_sequence_id: false\ntranscribe: \"back_transcribe\"\nforce_uppercase: true\nindexed_dataset_dtype: \"uint8\"\ntokenizer_type: \"Byte-Level\"\nvocab_file: null\nvocab_size: null\nmerges_file: null\npretrained_tokenizer_model: null\nspecial_tokens: null\nfast_hf_tokenizer: true\nappend_eod: true\nenforce_sample_length: null\nftfy: false\nworkers: 1\npreproc_concurrency: 100000\nchunksize: 25\ndrop_empty_sequences: true\nnnn_filter: true\nseed: null\n```\n\n----------------------------------------\n\nTITLE: Noise Step Integration in DDPM\nDESCRIPTION: Method similar to step but specifically for noise prediction, performing one step of the reverse diffusion process with temperature control.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_125\n\nLANGUAGE: python\nCODE:\n```\ndef step_noise(model_out: Tensor,\n               t: Tensor,\n               xt: Tensor,\n               mask: Optional[Tensor] = None,\n               center: Bool = False,\n               temperature: Float = 1.0)\n```\n\n----------------------------------------\n\nTITLE: Derivative Calculation Function\nDESCRIPTION: Computes derivative of a function supporting batched single variable inputs. Returns detached derivative tensor.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef derivative(t: Tensor, func: Callable) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Point-wise Kabsch Alignment for Molecular Data Augmentation in Python\nDESCRIPTION: Class definition for KabschAugmentation, which implements point-wise Kabsch alignment for molecular structures. This class is used for data augmentation in molecular modeling tasks where rotation-invariance is critical.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_71\n\nLANGUAGE: python\nCODE:\n```\nclass KabschAugmentation()\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss for Diffusion Models in Python\nDESCRIPTION: This function calculates the loss for diffusion models based on model predictions and target data. It supports different prediction types (FLOW or DATA) and includes optional time and mask parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_99\n\nLANGUAGE: python\nCODE:\n```\ndef loss(model_pred: Tensor,\n         target: Tensor,\n         t: Optional[Tensor] = None,\n         xt: Optional[Tensor] = None,\n         mask: Optional[Tensor] = None,\n         target_type: Union[PredictionType, str] = PredictionType.DATA)\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss for MDLM in Python\nDESCRIPTION: Method to calculate the cross-entropy loss between model predictions and target outputs, with optional weighting based on the MDLM time weight for continuous NELBO as specified in the referenced paper.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndef loss(logits: Tensor,\n         target: Tensor,\n         xt: Tensor,\n         time: Tensor,\n         mask: Optional[Tensor] = None,\n         use_weight=True)\n```\n\n----------------------------------------\n\nTITLE: Distributed Inference Results Processing\nDESCRIPTION: Demonstrates how to collate predictions from multiple GPUs in distributed inference setup\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/inference.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport glob\nfrom bionemo.llm.lightning import batch_collator\n\ncollated_preditions = batch_collator([torch.load(path) for path in glob.glob(f\"{work_dir}/predictions__rank_*.pt\")])\nfor key, val in collated_preditions.items():\n    if val is not None:\n        print(f'{key}\\t{val.shape}')\n```\n\n----------------------------------------\n\nTITLE: Calculating Score for MDLM in Python\nDESCRIPTION: Method to calculate the score of a given sample at a specific time with corresponding model output logits, based on the score definition in Appendix C.3 Equation 76 of the MDLM paper.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_score(logits, x, t)\n```\n\n----------------------------------------\n\nTITLE: Processing Multiple H5AD Files\nDESCRIPTION: Converts multiple H5AD files into a single SingleCellMemMapDataset using parallel processing.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/examples/example_notebook.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.scdl.io.single_cell_collection import SingleCellCollection\n\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    coll = SingleCellCollection(temp_dir)\n    coll.load_h5ad_multi(hdf5s, max_workers=4, use_processes=True)\n    coll.flatten(output_dir, destroy_on_copy=True)\n```\n\n----------------------------------------\n\nTITLE: Step Integration in DDPM\nDESCRIPTION: Method to perform one step of the reverse diffusion process, converting model output to the next sample in the generation sequence with temperature control.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_124\n\nLANGUAGE: python\nCODE:\n```\n@torch.no_grad()\ndef step(model_out: Tensor,\n         t: Tensor,\n         xt: Tensor,\n         mask: Optional[Tensor] = None,\n         center: Bool = False,\n         temperature: Float = 1.0)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Executing NeMo2 EVO2 Training Command with SLURM\nDESCRIPTION: This bash script constructs a complex command for training a NeMo2 EVO2 model using the train_evo2 script. It sets various training parameters including distributed training settings, model configuration, and logging options. The command is then executed using SLURM in a containerized environment with specified mounts and output handling.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nread -r -d '' COMMAND <<EOF\necho \"*******STARTING********\" \\\n&& echo \"---------------\" \\\n&& echo \"Starting training\" \\\n&&  \\\ntrain_evo2 \\\n    -d $CONFIG_PATH_IN_CONTAINER \\\n    --num-nodes=${SLURM_JOB_NUM_NODES} \\\n    --ckpt-dir $CKPT_MOUNT_IMAGE/nemo2_evo2_1b_8k \\\n    --devices=${SLURM_NTASKS_PER_NODE} \\\n    --grad-acc-batches $GRAD_ACC_BATCHES \\\n    --max-steps=$MAX_STEPS \\\n    --seed $SEED \\\n    ${EXTRA_ARGS} \\\n    --wandb-run-id $RUN_ID \\\n    --wandb-project $WANDB_PROJECT_NAME \\\n    --lr $LR \\\n    --wd $WD \\\n    --activation-checkpoint-recompute-num-layers 5 \\\n    --min-lr $MIN_LR \\\n    --warmup-steps $WU_STEPS \\\n    --attention-dropout $ADO \\\n    --hidden-dropout $HDO \\\n    --limit-val-batches=20 \\\n    --val-check-interval=${VAL_CHECK} \\\n    --result-dir=$RESULTS_PATH_IMAGE \\\n    --seq-length=${SEQ_LEN} \\\n    --tensor-parallel-size=${TP_SIZE} \\\n    --context-parallel-size=${CP_SIZE} \\\n    --pipeline-model-parallel-size=${PP_SIZE} \\\n    --workers 8 \\\n    --micro-batch-size=${MICRO_BATCH_SIZE} \\\n    --model-size=${MODEL_SIZE}\nEOF\nsrun \\\n    --output ${RESULTS_PATH_CLUSTER}/slurm-%j.out \\\n    --error ${RESULTS_PATH_CLUSTER}/error-%j.out \\\n    --container-image=$IMAGE_PATH \\\n    --container-mounts ${MOUNTS} \\\n    bash -c \"${COMMAND}\"\nset +x\n```\n\n----------------------------------------\n\nTITLE: SDE Terms Calculator\nDESCRIPTION: Computes general SDE terms including drift (f_t) and diffusion (g_t_2) terms for stochastic process simulation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_general_sde_terms(t)\n```\n\n----------------------------------------\n\nTITLE: Initializing ContinuousSNRTransform in Python\nDESCRIPTION: Creates a base class for continuous SNR schedules. It takes a TimeDirection parameter to define the direction in which the scheduler was built.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(direction: TimeDirection)\n```\n\n----------------------------------------\n\nTITLE: Initializing BetaTimeDistribution in Python\nDESCRIPTION: Creates a BetaTimeDistribution object with specified shape parameters, time range, discretization options, and an optional random number generator. It supports both continuous and discrete time distributions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(p1: Float = 2.0,\n             p2: Float = 1.0,\n             min_t: Float = 0.0,\n             max_t: Float = 1.0,\n             discrete_time: Bool = False,\n             nsteps: Optional[int] = None,\n             rng_generator: Optional[torch.Generator] = None)\n```\n\n----------------------------------------\n\nTITLE: Model Loss Calculation in Python\nDESCRIPTION: Computes the loss between model predictions and targets with optional masking and different weighting schemes.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_89\n\nLANGUAGE: python\nCODE:\n```\ndef loss(model_pred: Tensor,\n         target: Tensor,\n         t: Tensor,\n         dt: Optional[Float] = 0.001,\n         mask: Optional[Tensor] = None,\n         weight_type: str = \"ones\")\n```\n\n----------------------------------------\n\nTITLE: Loss Calculation for Discrete Flow Matching\nDESCRIPTION: Method to calculate cross-entropy loss between model predictions and target outputs, with optional time-dependent weighting and masking to focus on relevant tokens during training.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_57\n\nLANGUAGE: python\nCODE:\n```\ndef loss(logits: Tensor,\n         target: Tensor,\n         time: Optional[Tensor] = None,\n         mask: Optional[Tensor] = None,\n         use_weight: Bool = False)\n```\n\n----------------------------------------\n\nTITLE: Implementing ContinuousExpNoiseTransform Base Class in Python\nDESCRIPTION: Abstract base class for continuous noise schedules that implements exponential noise transformation where alpha = exp(-sigma). Includes methods for calculating sigma and converting between sigma and alpha values.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nclass ContinuousExpNoiseTransform(ABC):\n    def __init__(direction: TimeDirection):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Creating PickledDataWDS Instance in Python\nDESCRIPTION: This example demonstrates how to create an instance of PickledDataWDS. It shows the setup of various parameters including directory paths, file suffixes, and custom data processing pipelines for different data splits.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-webdatamodule/README.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from bionemo.core.data.datamodule import Split, PickledDataWDS\n\n>>> dir_pickles = \"/path/to/my/pickles/dir\"\n\n>>> # the following will use `sample1.mydata.pt` and `sample2.mydata.pt` as the\n>>> # training dataset and `sample4.mydata.pt` and `sample5.mydata.pt` as the\n>>> # validation dataset\n\n>>> suffix_pickles = \"mydata.pt\"\n\n>>> names_subset = {\n>>>     Split.train: [sample1, sample2],\n>>>     Split.val: [sample4, sample5],\n>>> }\n\n>>> # the following setting will attempt to create at least 5 tar files in\n>>> # `/path/to/output/tars/dir/myshards-00000{0-5}.tar`\n\n>>> n_tars_wds = 5\n>>> prefix_tars_wds = \"myshards\"\n>>> output_dir_tar_files = {\n        Split.train : \"/path/to/output/tars/dir-train\",\n        Split.val : \"/path/to/output/tars/dir-val\",\n        Split.test : \"/path/to/output/tars/dir-test\",\n    }\n\n>>> # user can optionally customize the data processing routines and kwargs used\n>>> # in the WebDataset and WebLoader (see the examples in `WebDataModule`)\n\n>>> pipeline_wds = { Split.train: ... }\n\n>>> pipeline_prebatch_wld = { Split.train: ... }\n\n>>> kwargs_wds = { Split.train: ..., Split.val: ... }\n\n>>> kwargs_wld = { Split.train: ..., Split.val: ... }\n\n>>> invoke_wds = { Split.train: ..., Split.val: ... }\n\n>>> invoke_wld = { Split.train: ..., Split.val: ... }\n\n>>> # create the data module\n>>> data_module = PickledDataWDS(\n>>>     dir_pickles,\n>>>     names_subset,\n>>>     suffix_pickles, # `WebDataModule` args\n>>>     output_dir_tar_files, # `WebDataModule` args\n>>>     n_tars_wds=n_tars_wds,\n>>>     prefix_tars_wds=prefix_tars_wds, # `WebDataModule` kwargs\n>>>     pipeline_wds=pipeline_wds, # `WebDataModule` kwargs\n>>>     pipeline_prebatch_wld=pipelines_wdl_batch, # `WebDataModule` kwargs\n>>>     kwargs_wds=kwargs_wds, # `WebDataModule` kwargs\n>>>     kwargs_wld=kwargs_wld, # `WebDataModule` kwargs\n>>>     invoke_wds=invoke_wds, # `WebDataModule` kwargs\n>>>     invoke_wld=invoke_wld, # `WebDataModule` kwargs\n>>> )\n```\n\n----------------------------------------\n\nTITLE: Sequential Masking for Protein Mutation Analysis\nDESCRIPTION: Creates masked versions of a protein sequence by iteratively replacing each position with a mask token for mutation analysis.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nseq = \"MSLKRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL\"  # length: 41\n\nstart_pos = 0\nend_pos = len(seq)\n\npositions = np.arange(start_pos, end_pos)\n\nsequentially_masked = []\nfor index in positions:\n    masked = seq[:index] + \"<mask>\" + seq[index + 1 :]\n    sequentially_masked.append(masked)\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Geneformer Model with BioNeMo\nDESCRIPTION: Demonstrates how to fine-tune a Geneformer model using BioNeMo. It includes specifying a different model configuration class and using a previously trained checkpoint.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/training-models.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nTEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20241203 --source $MY_DATA_SOURCE); \\\ntrain_geneformer     \\\n    --data-dir ${TEST_DATA_DIR}/cellxgene_2023-12-15_small_processed_scdl    \\\n    --result-dir ./results     \\\n    --experiment-name test_finettune_experiment     \\\n    --num-gpus 1  \\\n    --num-nodes 1 \\\n    --val-check-interval 10 \\\n    --num-dataset-workers 0 \\\n    --num-steps 55 \\\n    --seq-length 128 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2 \\\n    --training-model-config-class FineTuneSeqLenBioBertConfig \\\n    --restore-from-checkpoint-path results/test_experiment/dev/checkpoints/test_experiment--val_loss=4.3506-epoch=1-last\n```\n\n----------------------------------------\n\nTITLE: Processing Noise Predictions in DDPM\nDESCRIPTION: Method to convert model outputs to noise predictions, similar to process_data_prediction but specifically for noise prediction type.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_122\n\nLANGUAGE: python\nCODE:\n```\ndef process_noise_prediction(model_output, sample, t)\n```\n\n----------------------------------------\n\nTITLE: Initializing PowerInferenceSchedule in Python\nDESCRIPTION: This method initializes the PowerInferenceSchedule class for continuous time inference. It sets up parameters including the number of steps, inclusive end, minimum time, padding, dilation, exponent, direction, and device.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(nsteps: int,\n             inclusive_end: bool = False,\n             min_t: Float = 0,\n             padding: Float = 0,\n             dilation: Float = 0,\n             exponent: Float = 1.0,\n             direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n             device: Union[str, torch.device] = \"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Initializing LogInferenceSchedule in Python\nDESCRIPTION: This method initializes the LogInferenceSchedule class for continuous time inference. It sets up parameters including the number of steps, inclusive end, minimum time, padding, dilation, exponent, direction, and device.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(nsteps: int,\n             inclusive_end: bool = False,\n             min_t: Float = 0,\n             padding: Float = 0,\n             dilation: Float = 0,\n             exponent: Float = -2.0,\n             direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n             device: Union[str, torch.device] = \"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing ESM-2 Inference Results in Python\nDESCRIPTION: This code snippet demonstrates how to load the inference results from a PyTorch file, process the output, and convert the classification results to human-readable secondary structure labels using a custom tokenizer.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n\nresults = torch.load(f\"{results_path}/predictions__rank_0.pt\")\n\nfor key, val in results.items():\n    if val is not None:\n        print(f\"{key}\\t{val.shape}\")\n\nfrom bionemo.esm2.data.tokenizer import get_tokenizer\n\n\ntokenizer = get_tokenizer()\ntokens = tokenizer.all_tokens\naa_tokens = [\"L\", \"A\", \"G\", \"V\", \"S\", \"E\", \"R\", \"T\", \"I\", \"D\", \"P\", \"K\", \"Q\", \"N\", \"F\", \"Y\", \"M\", \"H\", \"W\", \"C\"]\naa_indices = [i for i, token in enumerate(tokens) if token in aa_tokens]\nextra_indices = [i for i, token in enumerate(tokens) if token not in aa_tokens]\n\ninput_ids = results[\"input_ids\"]  # b, s\n# mask where non-amino acid tokens are True\nmask = ~torch.isin(input_ids, torch.tensor(extra_indices))\n\nfrom bionemo.llm.data.label2id_tokenizer import Label2IDTokenizer\n\n\nlabel_tokenizer = Label2IDTokenizer()\nlabel_tokenizer = label_tokenizer.build_vocab(secondary_structure_labels)\n\noutput_ids = torch.argmax(results[\"classification_output\"], dim=-1)\n\nprint(\"Predicted Secondary Structures:\")\nfor i in range(output_ids.shape[0]):\n    ss_ids = output_ids[i][mask[i]]\n    print(label_tokenizer.ids_to_text(ss_ids.tolist()))\n```\n\n----------------------------------------\n\nTITLE: Generating Samples from Trained MDLM\nDESCRIPTION: Performs sampling from the trained MDLM model using inference schedule.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nts = schedule.generate_schedule()\ndts = schedule.discretize()\nnum_samples = 1000\nxt = mdlm.sample_prior((num_samples, D))\nfor dt, t in zip(dts, ts):\n    t = torch.full((xt.shape[0],), t).to(DEVICE)\n    logits = model(xt, t)\n    xt = mdlm.step(logits, t, xt, dt)\n```\n\n----------------------------------------\n\nTITLE: Running ESM-2 Inference with Multiple Output Types\nDESCRIPTION: Executes the ESM-2 model inference with options to include hidden states, embeddings, logits and input IDs. Uses BioNeMo Framework's infer_esm2 executable with mixed precision.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%capture --no-display --no-stderr cell_output\n\nexample_dir = os.path.join(work_dir, \"inference_example\")\nos.makedirs(example_dir, exist_ok=True)\n\n! infer_esm2 --checkpoint-path {checkpoint_path} \\\n             --data-path {data_path} \\\n             --results-path {example_dir} \\\n             --num-gpus 1 \\\n             --precision \"bf16-mixed\" \\\n             --include-hiddens \\\n             --include-embeddings \\\n             --include-logits \\\n             --include-input-ids\n```\n\n----------------------------------------\n\nTITLE: DDPM Class Definition in PyTorch\nDESCRIPTION: The base class definition for the Denoising Diffusion Probabilistic Model (DDPM) interpolant that inherits from the Interpolant class.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_109\n\nLANGUAGE: python\nCODE:\n```\nclass DDPM(Interpolant)\n```\n\n----------------------------------------\n\nTITLE: Saving and Reloading Dataset\nDESCRIPTION: Demonstrates how to save the dataset to disk and reload it for later use.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/examples/example_notebook.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata.save()\n```\n\nLANGUAGE: python\nCODE:\n```\nreloaded_data = SingleCellMemMapDataset(dataset_dir)\n```\n\n----------------------------------------\n\nTITLE: Initializing MegatronStrategy for ESM-2 Model Parallelism\nDESCRIPTION: Sets up the MegatronStrategy for model parallelism, including tensor and pipeline parallelism configurations. Calculates the global batch size based on micro batch size, number of nodes, devices, and parallelism settings.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/pretrain.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nemo import lightning as nl\nfrom bionemo.llm.utils.datamodule_utils import infer_global_batch_size\n\nmicro_batch_size = 2\nnum_nodes = 1\ndevices = 2\naccumulate_grad_batches = 1\ntensor_model_parallel_size = 2\npipeline_model_parallel_size = 1\n\nglobal_batch_size = infer_global_batch_size(\n    micro_batch_size=micro_batch_size,\n    num_nodes=num_nodes,\n    devices=devices,\n    accumulate_grad_batches=accumulate_grad_batches,\n    tensor_model_parallel_size=tensor_model_parallel_size,\n    pipeline_model_parallel_size=pipeline_model_parallel_size,\n)\n\nstrategy = nl.MegatronStrategy(\n    tensor_model_parallel_size=tensor_model_parallel_size,\n    pipeline_model_parallel_size=pipeline_model_parallel_size,\n    ddp=\"megatron\",\n    find_unused_parameters=True,\n    ckpt_include_optimizer=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Converting Evo2 Checkpoints to NeMo2 Format\nDESCRIPTION: Collection of bash commands for converting different sizes of Evo2 model checkpoints from Hugging Face format to NeMo2 format, including 1B and 7B parameter models.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nevo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_1b_base \\\n  --model-size 1b --output-dir nemo2_evo2_1b_8k\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd nemo2_evo2_1b_8k\ntar -czvf ../nemo2_evo2_1b_8k.tar.gz .\nsha256sum nemo2_evo2_1b_8k.tar.gz\n```\n\nLANGUAGE: bash\nCODE:\n```\nevo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_7b_base \\\n  --model-size 7b --output-dir nemo2_evo2_7b_8k\n```\n\nLANGUAGE: bash\nCODE:\n```\nevo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_7b \\\n  --model-size 7b_arc_longcontext --output-dir nemo2_evo2_7b_1m\n```\n\n----------------------------------------\n\nTITLE: Implementing Linear Harmonic Prior Distribution in Python\nDESCRIPTION: Defines a LinearHarmonicPrior class based on the work by Jing et al. It generates samples from a linear harmonic distribution with specified parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass LinearHarmonicPrior(PriorDistribution):\n    def __init__(length: Optional[int] = None,\n                 distance: Float = 3.8,\n                 center: Bool = False,\n                 rng_generator: Optional[torch.Generator] = None,\n                 device: Union[str, torch.device] = \"cpu\") -> None:\n        # Implementation details\n\n    def sample(shape: Tuple,\n               mask: Optional[Tensor] = None,\n               device: Union[str, torch.device] = \"cpu\",\n               rng_generator: Optional[torch.Generator] = None) -> Tensor:\n        # Implementation details\n```\n\n----------------------------------------\n\nTITLE: Initializing LogitNormalTimeDistribution in Python\nDESCRIPTION: Creates a LogitNormalTimeDistribution object with specified shape parameters, time range, discretization options, and an optional random number generator. It supports both continuous and discrete time distributions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(p1: Float = 0.0,\n             p2: Float = 1.0,\n             min_t: Float = 0.0,\n             max_t: Float = 1.0,\n             discrete_time: Bool = False,\n             nsteps: Optional[int] = None,\n             rng_generator: Optional[torch.Generator] = None)\n```\n\n----------------------------------------\n\nTITLE: Initializing LinearInferenceSchedule in Python\nDESCRIPTION: This method initializes the LinearInferenceSchedule class for continuous time inference. It sets up parameters including the number of steps, inclusive end, minimum time, padding, dilation, direction, and device.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(nsteps: int,\n             inclusive_end: bool = False,\n             min_t: Float = 0,\n             padding: Float = 0,\n             dilation: Float = 0,\n             direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n             device: Union[str, torch.device] = \"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Vector Field and Data Prediction Processing\nDESCRIPTION: Methods to process model outputs based on prediction type to calculate vector fields or generate clean data.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_97\n\nLANGUAGE: python\nCODE:\n```\ndef process_vector_field_prediction(model_output: Tensor,\n                                    xt: Optional[Tensor] = None,\n                                    t: Optional[Tensor] = None,\n                                    mask: Optional[Tensor] = None)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef process_data_prediction(model_output: Tensor,\n                            xt: Optional[Tensor] = None,\n                            t: Optional[Tensor] = None,\n                            mask: Optional[Tensor] = None)\n```\n\n----------------------------------------\n\nTITLE: Using Size-Aware Batching with PyTorch Tensors\nDESCRIPTION: An example demonstrating how to use the size_aware_batching function with PyTorch tensors. It defines a sample dataset, a sizeof function, and uses the generator to create batches respecting a maximum total size constraint.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from torch.utils.data import default_collate\n>>> from bionemo.size_aware_batching.sampler import size_aware_batching\n\n>>> # Define a sample dataset with torch.tensor\n>>> dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n>>> # Define a sizeof function that returns the size of each tensor\n>>> def sizeof(x):\n...     return x.numel()\n\n>>> # Create a generator with max_total_size=4 and default_collate_fn\n>>> gen = size_aware_batching(dataset, sizeof, 4, collate_fn=default_collate)\n>>> batches = list(gen)\n>>> print(batches)\n    [tensor([[1, 2], [3, 4]]), tensor([[5, 6], [7, 8]]), tensor([[9, 10]])]\n```\n\n----------------------------------------\n\nTITLE: Loading Single Cell Dataset from H5AD File in Python\nDESCRIPTION: Demonstrates how to create a SingleCellMemMapDataset from an H5AD file. This allows for fast access to datasets larger than available RAM.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\n\ndata = SingleCellMemMapDataset(\"97e_scmm\", \"hdf5s/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\")\n```\n\n----------------------------------------\n\nTITLE: Sampling from Trained D3PM Model\nDESCRIPTION: Generates 1000 samples from the trained D3PM model by starting with samples from the prior distribution and iteratively applying the model's predictions through the inference schedule.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nts = schedule.generate_schedule()\nnum_samples = 1000\nxt = d3pm.sample_prior((num_samples, D))\nfor t in ts:\n    t = torch.full((xt.shape[0],), t).to(DEVICE)\n    logits = model(xt, t)\n    xt = d3pm.step(logits, t, xt)\n```\n\n----------------------------------------\n\nTITLE: Preparing Input Data for ESM-2 Inference in Python\nDESCRIPTION: This code prepares input data for ESM-2 inference by creating a DataFrame with sequences and saving it to a CSV file. It also sets up paths for the checkpoint and results.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Create a DataFrame\ndf = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"sequences.csv\")\ndf.to_csv(data_path, index=False)\n\ncheckpoint_path = (\n    f\"{work_dir}/sequence-level-regression/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last\"\n)\nresults_path = f\"{work_dir}/sequence-level-regression/infer/\"\n```\n\n----------------------------------------\n\nTITLE: Converting Probabilities to Predicted Sequences\nDESCRIPTION: Converts model probability outputs into amino acid sequences by selecting the highest probability token at each position and comparing with the original sequence.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Predicted seq (Argmax --> Collect token IDs of predicted seq --> Convert to amino acids)\npred_idx_list = np.argmax(probas_final, axis=-1).tolist()\npred_seq = \"\".join([tokenizer.id_to_token(id) for id in pred_idx_list])\n\n# Original seq\ntrue_idx_list = [tokenizer.token_to_id(seq[i]) for i in positions]\ntrue_seq = \"\".join([tokenizer.id_to_token(id) for id in true_idx_list])\n```\n\n----------------------------------------\n\nTITLE: Calculating Loss for D3PM Model in Python\nDESCRIPTION: This method calculates the cross-entropy loss between the model prediction and the target output for the D3PM model. It supports optional masking and variational lower bound loss calculation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_108\n\nLANGUAGE: python\nCODE:\n```\ndef loss(logits: Tensor,\n         target: Tensor,\n         xt: Tensor,\n         time: Tensor,\n         mask: Optional[Tensor] = None,\n         vb_scale: Float = 0.0)\n```\n\n----------------------------------------\n\nTITLE: Converting Savanna Evo2 40B Long-Context (1M) to NeMo2 Format\nDESCRIPTION: This command converts the Savanna Evo2 40B model with 1M context length from Hugging Face to NeMo2 format. It specifies the model size as '40b_arc_longcontext' and outputs the converted model to the 'nemo2_evo2_40b_1m' directory.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nevo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_40b \\\n  --model-size 40b_arc_longcontext --output-dir nemo2_evo2_40b_1m\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment for Discrete Interpolation Models\nDESCRIPTION: Imports necessary libraries including matplotlib for visualization, PyTorch for deep learning, and tqdm for progress tracking. Also sets a random seed for reproducibility.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n\ntorch.cuda.manual_seed(42)\n```\n\n----------------------------------------\n\nTITLE: Generating Linear Time Schedule in Python\nDESCRIPTION: This method generates a linear time schedule as a tensor. It takes optional parameters for the number of steps and device, and returns a tensor of time steps.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef generate_schedule(\n        nsteps: Optional[int] = None,\n        device: Optional[Union[str, torch.device]] = None) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Argmax Sampling for Discrete Flow Matching\nDESCRIPTION: Method to deterministically select the most likely token at each position by taking the argmax of the model output logits. This provides a greedy sampling approach for the flow matching model.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_60\n\nLANGUAGE: python\nCODE:\n```\ndef step_argmax(model_out: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Euler Step Implementation for Discrete Flow Matching\nDESCRIPTION: Method to perform a single step of Euler updates for the Discrete Flow Matching model, applying temperature and stochasticity parameters to control the sampling behavior during generation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ndef step(logits: Tensor,\n         t: Tensor,\n         xt: Tensor,\n         dt: Tensor | float,\n         temperature: Float = 1.0,\n         stochasticity: Float = 1.0) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Initializing Continuous Flow Matching Interpolant\nDESCRIPTION: Constructor for setting up the Continuous Flow Matching interpolant with time distribution, prior distribution, prediction type and other configuration parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_93\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(time_distribution: TimeDistribution,\n             prior_distribution: PriorDistribution,\n             prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n             sigma: Float = 0,\n             augmentation_type: Optional[Union[AugmentationType, str]] = None,\n             augmentation_num_threads: int = 1,\n             data_scale: Float = 1.0,\n             device: Union[str, torch.device] = \"cpu\",\n             rng_generator: Optional[torch.Generator] = None,\n             eps: Float = 1e-5)\n```\n\n----------------------------------------\n\nTITLE: Defining BucketBatchSampler Class in Python\nDESCRIPTION: This code snippet defines the BucketBatchSampler class, which inherits from Sampler[List[int]]. It is used to create batches with sizes of elements from predefined bucket ranges.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass BucketBatchSampler(Sampler[List[int]])\n```\n\n----------------------------------------\n\nTITLE: Loss Weight Calculation in Python\nDESCRIPTION: Calculates weighted loss based on different weighting schemes including uniform, data-to-noise, and variational objective approaches.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_88\n\nLANGUAGE: python\nCODE:\n```\ndef loss_weight(raw_loss: Tensor,\n                t: Tensor,\n                weight_type: str,\n                dt: Float = 0.001) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Sampling and Visualizing MDLM Prior\nDESCRIPTION: Samples from MDLM prior distribution and creates a bar plot of class frequencies.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nnum_samples = 1000\nxt = mdlm.sample_prior((num_samples, D))\ncounts = xt.flatten().cpu()\n\n# Compute frequency of each class index\nclass_counts = torch.bincount(counts)\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.bar(range(len(class_counts)), class_counts.numpy(), color=\"red\")\nplt.xlabel(\"Class Index\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Discrete Distribution of Class Indices\")\nplt.xticks(range(len(class_counts)))  # Set x-ticks to class indices\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Using SizeAwareBatchSampler with PyTorch Tensors\nDESCRIPTION: An example showing how to use the SizeAwareBatchSampler class with PyTorch tensors. It creates a sample dataset, defines a sizeof function, and demonstrates how to create and use the batch sampler.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n\n>>> # Define a sample dataset with torch.tensor\n>>> dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n>>> # Define a function that returns the size of each element in the dataset.\n>>> def sizeof(index):\n...     return dataset[index].numel()\n\n>>> # Create a SizeAwareBatchSampler with a maximum total batch size of 10.\n>>> batch_sampler = SizeAwareBatchSampler(\n...     sampler=torch.utils.data.SequentialSampler(dataset),\n...     sizeof=sizeof,\n...     max_total_size=4\n... )\n\n>>> # Iterate over batches of indices that do not exceed the maximum total size.\n>>> print(list(batch_sampler))\n    [[0, 1], [2, 3], [4]]\n```\n\n----------------------------------------\n\nTITLE: DDPM Loss Calculation\nDESCRIPTION: Calculates the loss for model predictions with support for different weight types and optional masking.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_131\n\nLANGUAGE: python\nCODE:\n```\ndef loss(model_pred: Tensor,\n         target: Tensor,\n         t: Optional[Tensor] = None,\n         mask: Optional[Tensor] = None,\n         weight_type: Literal[\"ones\", \"data_to_noise\",\n                              \"noise_to_data\"] = \"ones\")\n```\n\n----------------------------------------\n\nTITLE: Performing Discrete Interpolation in D3PM Model\nDESCRIPTION: This method implements discrete interpolation for the D3PM model. It calculates the interpolated discrete state 'xt' at time 't' given the input data and noise using the equation from the D3PM paper.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_105\n\nLANGUAGE: python\nCODE:\n```\ndef interpolate(data: Tensor, t: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Converting Evo2 Checkpoint to NeMo2 Format\nDESCRIPTION: Downloads and converts a Savanna format Evo2 checkpoint from Hugging Face to NeMo2 format for fine-tuning.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nif not os.path.exists(\"nemo2_evo2_1b_8k\"):\n    !evo2_convert_to_nemo2 \\\n      --model-path hf://arcinstitute/savanna_evo2_1b_base \\\n      --model-size 1b --output-dir nemo2_evo2_1b_8k\n```\n\n----------------------------------------\n\nTITLE: Initializing MixTimeDistribution in Python\nDESCRIPTION: Initializes a MixTimeDistribution object with two time distributions and a mixing fraction. The mix_fraction parameter determines the proportion of samples drawn from the first distribution.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(dist1: TimeDistribution, dist2: TimeDistribution,\n             mix_fraction: Float)\n```\n\n----------------------------------------\n\nTITLE: Forward Process in DDPM\nDESCRIPTION: Method to perform the forward process of diffusion, adding noise to the data at time step t. Optionally accepts noise or samples it if not provided.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_120\n\nLANGUAGE: python\nCODE:\n```\ndef forward_process(data: Tensor, t: Tensor, noise: Optional[Tensor] = None)\n```\n\n----------------------------------------\n\nTITLE: Training ESM-2 Model with BioNeMo\nDESCRIPTION: Demonstrates how to train an ESM-2 model using BioNeMo. It includes downloading test data and model checkpoints, and running the training command with various parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/training-models.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nTEST_DATA_DIR=$(download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source $MY_DATA_SOURCE); \\\nESM2_650M_CKPT=$(download_bionemo_data esm2/650m:2.0 --source $MY_DATA_SOURCE); \\\n\ntrain_esm2     \\\n    --train-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet     \\\n    --train-database-path ${TEST_DATA_DIR}/2024_03_sanity/train_sanity.db     \\\n    --valid-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/valid_clusters.parquet     \\\n    --valid-database-path ${TEST_DATA_DIR}/2024_03_sanity/validation.db     \\\n    --result-dir ./results     \\\n    --experiment-name test_experiment     \\\n    --num-gpus 1  \\\n    --num-nodes 1 \\\n    --val-check-interval 10 \\\n    --num-dataset-workers 1 \\\n    --num-steps 10 \\\n    --max-seq-length 1024 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2 \\\n    --restore-from-checkpoint-path ${ESM2_650M_CKPT}\n```\n\n----------------------------------------\n\nTITLE: Applying Optimal Transport Augmentation in Python\nDESCRIPTION: Method to sample indices for noise and data according to an optimal transport plan. It computes the OT plan between minibatches, samples from this plan, and returns the matched noise and data samples. Includes options for sampling strategy and sorting.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_67\n\nLANGUAGE: python\nCODE:\n```\ndef apply_augmentation(\n    x0: Tensor,\n    x1: Tensor,\n    mask: Optional[Tensor] = None,\n    replace: Bool = False,\n    sort: Optional[Literal[\"noise\", \"x0\", \"data\", \"x1\"]] = \"x0\"\n) -> Tuple[Tensor, Tensor, Optional[Tensor]]\n```\n\n----------------------------------------\n\nTITLE: Optimal Transport Sampler Implementation in PyTorch\nDESCRIPTION: A class for sampling coordinates according to an Optimal Transport plan with respect to squared Euclidean cost. Adapted from the conditional-flow-matching repository for efficient mini-batch OT plan calculation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nclass OTSampler()\n```\n\nSampler for Exact Mini-batch Optimal Transport Plan.\n\nOTSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean cost)\nwith different implementations of the plan calculation. Code is adapted from https://github.com/atong01/conditional-flow-matching/blob/main/torchcfm/optimal_transport.py\n```\n\n----------------------------------------\n\nTITLE: Masking Lowest-Scoring Elements in PyTorch\nDESCRIPTION: A function that generates a boolean mask for the lowest-scoring elements in a tensor, up to a specified cutoff length per batch element. Returns a tensor with the same shape as the input scores where True indicates elements are among the lowest scores.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_52\n\nLANGUAGE: python\nCODE:\n```\ndef topk_lowest_masking(scores: Tensor, cutoff_len: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Defining Pretrain Configuration Class in Python\nDESCRIPTION: Implementation of PretrainConfig class that extends ExampleGenericConfig to define model and loss classes for pretraining. The config specifies PretrainModel as the model class and MSELossReduction as the loss class.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-example_model/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass PretrainConfig(ExampleGenericConfig[\"PretrainModel\", \"MSELossReduction\"], iom.IOMixinWithGettersSetters):\n\n    model_cls: Type[PretrainModel] = PretrainModel\n    loss_cls: Type[MSELossReduction] = MSELossReduction\n```\n\n----------------------------------------\n\nTITLE: Loading and Displaying ESM-2 Inference Results in Python\nDESCRIPTION: This code loads the inference results from a PyTorch file and displays the shape of each result component, including embeddings and predictions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n\nresults = torch.load(f\"{results_path}/predictions__rank_0.pt\")\n\nfor key, val in results.items():\n    if val is not None:\n        print(f\"{key}\\t{val.shape}\")\n```\n\n----------------------------------------\n\nTITLE: Applying Forward Process in D3PM Model\nDESCRIPTION: This method applies the forward process to the data at a given time t in the D3PM model. It takes discrete target IDs and time as input and returns the processed state x(t).\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_106\n\nLANGUAGE: python\nCODE:\n```\ndef forward_process(data: Tensor, t: Tensor) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Showing Alternative Trainer Configurations for ESM-2\nDESCRIPTION: Demonstrates alternative configurations for the trainer, including different validation data limits and precision types. Shows how to print all available precision types supported by the framework.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/pretrain.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.core.utils.dtypes import PrecisionTypes\n\nlimit_val_batches_all_data = 1.  # validate on 100% of the validation dataset\nlimit_val_batches_half_data = 0.5  # validate on 50% of the validation dataset\nlimit_val_batches_one_batch = 1  # validate on 1 batch\n\nprint(PrecisionTypes)  # show all possible precision types\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Discrete Prior Distribution\nDESCRIPTION: Implementation of a discrete prior distribution with custom probability mass function defined by a provided tensor.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass DiscreteCustomPrior(DiscretePriorDistribution):\n    def __init__(prior_dist: Tensor, num_classes: int = 10) -> None\n    def sample(shape: Tuple,\n           mask: Optional[Tensor] = None,\n           device: Union[str, torch.device] = \"cpu\",\n           rng_generator: Optional[torch.Generator] = None) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Score Function in DDPM\nDESCRIPTION: Method signature for the score function, which calculates the score (gradient of log-likelihood) for the diffusion model.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_126\n\nLANGUAGE: python\nCODE:\n```\ndef score()\n```\n\n----------------------------------------\n\nTITLE: Processing Data Predictions in VDM Models in Python\nDESCRIPTION: Method to convert model output to a data prediction based on the prediction type (noise, data, or v_prediction) according to formulas from the Progressive Distillation paper.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_81\n\nLANGUAGE: python\nCODE:\n```\ndef process_data_prediction(model_output: Tensor, sample, t)\n```\n\n----------------------------------------\n\nTITLE: Hybrid SDE Integration Step in Python\nDESCRIPTION: Implements one step integration of Hybrid Langevin-Reverse Time SDE with configurable equilibrium rate and temperature.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_90\n\nLANGUAGE: python\nCODE:\n```\ndef step_hybrid_sde(model_out: Tensor,\n                    t: Tensor,\n                    xt: Tensor,\n                    dt: Tensor,\n                    mask: Optional[Tensor] = None,\n                    center: Bool = False,\n                    temperature: Float = 1.0,\n                    equilibrium_rate: Float = 0.0) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Implementing Step Method for VDM Inference in Python\nDESCRIPTION: Method to perform a single step in the reverse diffusion process during inference, taking model predictions and current state to produce the next state in the sampling chain.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_83\n\nLANGUAGE: python\nCODE:\n```\ndef step()\n```\n\n----------------------------------------\n\nTITLE: Generating Ground Truth for Continuous Flow Matching\nDESCRIPTION: This function computes ground truth values for different modes in continuous flow matching. It supports 'us' and 'tan' modes with parameterized transformations and optional clamping.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_101\n\nLANGUAGE: python\nCODE:\n```\ndef get_gt(t: Tensor,\n           mode: str = \"tan\",\n           param: float = 1.0,\n           clamp_val: Optional[float] = None,\n           eps: float = 1e-2) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Defining PickledDataWDS Class in Python\nDESCRIPTION: This snippet defines the PickledDataWDS class, which inherits from WebDataModule. It processes pickled data into webdataset tar files and sets up datasets and dataloaders for machine learning workflows.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-webdatamodule/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass PickledDataWDS(WebDataModule)\n```\n\n----------------------------------------\n\nTITLE: Defining MDLM Class Structure in Python\nDESCRIPTION: Class declaration for MDLM (Masked discrete Diffusion Language Model) that inherits from Interpolant base class. This defines the fundamental architecture of the MDLM model.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nclass MDLM(Interpolant)\n```\n\n----------------------------------------\n\nTITLE: Initializing AMPLIFY Model from HuggingFace\nDESCRIPTION: Code snippet demonstrating how to initialize a NeMo version of AMPLIFY model from HuggingFace weights using HFAMPLIFYImporter.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/amplify.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodule = biobert_lightning_module(config=AMPLIFYConfig())\nio.import_ckpt(module, f\"hf://chandar-lab/AMPLIFY_120M\", tmp_path / \"nemo_checkpoint\")\n```\n\n----------------------------------------\n\nTITLE: Log SNR to Alphas Sigmas Conversion\nDESCRIPTION: Converts log signal-to-noise ratio (SNR) to alpha and sigma values, returning them as a tuple of tensors.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef log_snr_to_alphas_sigmas(log_snr: Tensor) -> Tuple[Tensor, Tensor]\n```\n\n----------------------------------------\n\nTITLE: Initializing BucketBatchSampler in Python\nDESCRIPTION: This method initializes the BucketBatchSampler with various parameters including sizes, bucket boundaries, base batch sampler class, and other configuration options.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(sizes: torch.Tensor,\n             bucket_boundaries: torch.Tensor,\n             base_batch_sampler_class: Type[Sampler],\n             base_batch_sampler_shared_kwargs: Optional[Dict[str, Any]] = None,\n             base_batch_sampler_individual_kwargs: Optional[Dict[\n                 str, Iterable]] = None,\n             shuffle: Optional[bool] = True,\n             generator: Optional[torch.Generator] = None) -> None\n```\n\n----------------------------------------\n\nTITLE: Creating Input Data CSV for ESM-2 Inference\nDESCRIPTION: Creates a CSV file with protein sequences that will be used as input for ESM-2 inference. The sequences are stored in a pandas DataFrame and saved to a file in the work directory.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsequences = [\n    \"MSLKRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL\",  # length: 41\n    \"MIQSQINRNIRLDLADAILLSKAKKDLSFAEIADGTGLA\",  # length: 39\n]\n# Create a DataFrame\ndf = pd.DataFrame(sequences, columns=[\"sequences\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"sequences.csv\")\ndf.to_csv(data_path, index=False)\n```\n\n----------------------------------------\n\nTITLE: Implementing Mixed Time Distribution Class in Python\nDESCRIPTION: Defines a MixTimeDistribution class for creating mixed time distributions. It allows combining different time distributions with specified mixing fractions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass MixTimeDistribution():\n    # Implementation details\n```\n\n----------------------------------------\n\nTITLE: Sampling D3PM Prior Distribution\nDESCRIPTION: Samples from D3PM prior distribution and visualizes the distribution of counts using a histogram.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nxt = d3pm.sample_prior((num_samples, D))\ncounts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Loading SCDL Dataset from Disk in Python\nDESCRIPTION: Shows how to reload a previously saved SingleCellMemMapDataset from disk using the path to the serialized data.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nreloaded_data = SingleCellMemMapDataset(\"97e_scmm\")\n```\n\n----------------------------------------\n\nTITLE: Performing Deterministic DFM Sampling\nDESCRIPTION: Executes the sampling process with zero stochasticity, resulting in deterministic generation from the trained model by stepping through the time schedule.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfor dt, t in zip(dts, ts):\n    t = schedule.pad_time(num_samples, t, DEVICE)\n    logits = model(xt, t)\n    xt = dfm.step(logits, t, xt, dt, stochasticity=0)\n```\n\n----------------------------------------\n\nTITLE: Data Loading and Configuration\nDESCRIPTION: Sets up configuration parameters and downloads required BRCA1 data files using the previously defined utility functions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n\n# Configuration parameters\nDATA_DIR = \"brca1\"\nSAMPLE_CONFIG = {\"sample_frac\": 0.05, \"balanced\": True, \"disable\": False, \"random_state\": 42}\n\n# 1. Download the necessary data files if not present\nexcel_path, genome_path = download_data(DATA_DIR)\nseq_chr17 = load_genome_sequence(genome_path)\n```\n\n----------------------------------------\n\nTITLE: Accessing Forward Noise Schedule in DDPM\nDESCRIPTION: Property method that returns the forward noise schedule used in the diffusion process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_113\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef forward_noise_schedule() -> torch.Tensor\n```\n\n----------------------------------------\n\nTITLE: Loading ESM-2 8M Model Checkpoint in Python\nDESCRIPTION: Code to load the pre-trained ESM-2 8M parameter model checkpoint using the load function from BioNeMo framework.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/ESM-2/pre-training.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nesm2_8m_ckpt_path = load(\"esm2/8m:2.0\")\n```\n\n----------------------------------------\n\nTITLE: Converting Sigma to Alpha in Noise Transform\nDESCRIPTION: Method to convert sigma values to alpha values using exponential transformation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef sigma_to_alpha(sigma: Tensor) -> Tensor:\n```\n\n----------------------------------------\n\nTITLE: Implementing Random Effects in Dataset for Multi-Epoch Training (Python)\nDESCRIPTION: Demonstrates how to implement random effects in a dataset's __getitem__ method using EpochIndex and torch.Generator for multi-epoch training while maintaining Megatron-LM compatibility.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/background/megatron_datasets.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MyDataset:\n    def __getitem__(self, idx: EpochIndex):\n        rng = torch.Generator()\n        rng.manual_seed(idx.epoch)\n        ...\n```\n\n----------------------------------------\n\nTITLE: Processing Noise Predictions in VDM Models in Python\nDESCRIPTION: Method similar to process_data_prediction but specifically converts model output to noise prediction. It raises an error if the prediction type is not 'noise'.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_82\n\nLANGUAGE: python\nCODE:\n```\ndef process_noise_prediction(model_output: Tensor, sample: Tensor, t: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Kabsch Alignment for Molecular Structure Comparison in Python\nDESCRIPTION: Method to find the rotation matrix that minimizes RMSD between target and noise molecular conformations. This is a key component for comparing molecular structures in a rotation-invariant manner.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_69\n\nLANGUAGE: python\nCODE:\n```\ndef kabsch_align(target: Tensor, noise: Tensor) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Importing BioNeMo Data Loading Dependencies\nDESCRIPTION: Imports required modules for file operations and data loading from the BioNeMo framework, including tempfile for temporary directory management and the load function.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-core/tests/bionemo/core/data/test_load_notebook.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\nfrom pathlib import Path\n\nfrom bionemo.core.data.load import load\n```\n\n----------------------------------------\n\nTITLE: Creating ROC Curve Visualization Function\nDESCRIPTION: Implements a function to create ROC curve visualizations with NVIDIA-themed styling, calculating and displaying AUROC scores for model evaluation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef plot_roc_curve(df):\n    NVIDIA_GREEN = \"#76B900\"\n    BACKGROUND_COLOR = \"#F8F8F8\"\n    GRID_COLOR = \"#DDDDDD\"\n    FONT_COLOR = \"#333333\"\n    \n    y_true = (df[\"class\"] == \"LOF\").astype(int)\n    fpr, tpr, _ = roc_curve(y_true, -df[\"evo2_delta_score\"])\n    roc_auc = auc(fpr, tpr)\n    \n    plt.figure(figsize=(9, 5), facecolor=BACKGROUND_COLOR)\n    plt.style.use(\"default\")\n    plt.plot(fpr, tpr, color=NVIDIA_GREEN, lw=3, label=f\"ROC curve (AUROC = {roc_auc:.2f})\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Reverse Noise Schedule in DDPM\nDESCRIPTION: Property method that returns the reverse noise schedule used in the diffusion process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_115\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef reverse_noise_schedule() -> torch.Tensor\n```\n\n----------------------------------------\n\nTITLE: Loading ESM-2 3B Model Checkpoint in Python\nDESCRIPTION: Code to load the pre-trained ESM-2 3B parameter model checkpoint using the load function from BioNeMo framework.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/ESM-2/pre-training.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nesm2_3b_ckpt_path = load(\"esm2/nv_3b:2.1\")\n```\n\n----------------------------------------\n\nTITLE: Implementing CosineExpNoiseTransform Class\nDESCRIPTION: Cosine-based exponential noise schedule implementation with initialization and derivative calculation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nclass CosineExpNoiseTransform(ContinuousExpNoiseTransform):\n    def __init__(eps: Float = 1.0e-3):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Implementing Argmax Step Function for MDLM in Python\nDESCRIPTION: Utility method that returns the index of the maximum value in the last dimension of the model output, used as a deterministic sampling strategy when generating sequences.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ndef step_argmax(model_out: Tensor)\n```\n\n----------------------------------------\n\nTITLE: CUDA Memory Collection Function\nDESCRIPTION: Function that collects CUDA peak memory allocation statistics for a workflow. Takes a dataset, work function, device, and optional cleanup function as input.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef collect_cuda_peak_alloc(\n    dataset: Iterable[Data],\n    work: Callable[[Data], Feature],\n    device: torch.device,\n    cleanup: Optional[Callable[[], None]] = None\n) -> Tuple[List[Feature], List[int]]\n```\n\n----------------------------------------\n\nTITLE: Saving Pre-trained ESM2 Tokenizer Configuration in Python\nDESCRIPTION: This code snippet demonstrates how to download and save a pre-trained tokenizer configuration from the facebook/esm2_t33_650M_UR50D model locally. It uses the Hugging Face Transformers library to fetch the tokenizer and save it to a specified directory.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-esm2/src/bionemo/esm2/data/tokenizer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\nAutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\").save_pretrained(\"...\")\n```\n\n----------------------------------------\n\nTITLE: Downloading ESM-2 Checkpoint via Bash Command\nDESCRIPTION: This bash command demonstrates an alternative method to download ESM-2 pre-trained checkpoints from NGC resources, specifically for the 650M parameter model.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndownload_bionemo_data esm2/650m:2.0\n```\n\n----------------------------------------\n\nTITLE: Configuration File Overview - YAML\nDESCRIPTION: List of YAML configuration files used for different purposes in BioNeMo framework: full-scale pre-training at 8k context length, context extension phase pre-training, preprocessing scripts for generating binary files, and smaller test files for promoters dataset.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/configs/README.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nfull_pretrain_shortphase_config.yaml  # For 8k context length pre-training\nfull_pretrain_longphase_config.yaml   # For context extension phase\ntest_preproc_config.yaml              # For preprocessing fasta to bin/idx\ntest_promotors_dataset_config.yaml    # For smaller test pre-training\n```\n\n----------------------------------------\n\nTITLE: Initializing OTSampler Class in Python for Optimal Transport Sampling\nDESCRIPTION: Constructor method for the OTSampler class that sets up the optimal transport solver. It accepts parameters for the OT method, device selection, and thread management. The method validates input parameters and raises appropriate exceptions for unsupported OT solvers.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_63\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(method: str = \"exact\",\n             device: Union[str, torch.device] = \"cpu\",\n             num_threads: int = 1) -> None\n```\n\n----------------------------------------\n\nTITLE: Visualizing Sequence Differences\nDESCRIPTION: Displays a visual comparison between predicted and original sequences, marking positions where mutations are suggested using vertical bars.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Compare prediction (reconstruction) to true (input sequence)\ndisplay(pred_seq + \" (Predicted Sequence)\")\ndisplay(\"\".join([\".\" if a == b else \"|\" for a, b in zip(pred_seq, true_seq)]))\ndisplay(true_seq + \" (Input Sequence)\")\n```\n\n----------------------------------------\n\nTITLE: Converting TensorBoard Events to DataFrame in Python\nDESCRIPTION: Function that extracts scalar metrics from TensorBoard event files and converts them into a pandas DataFrame. Handles multiple metrics and different step intervals.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef tensorboard_to_dataframe(event_file):\n    \"\"\"Given a TensorBoard event file, return a pandas DataFrame with the training metrics.\"\"\"\n    # Load the event file\n    ea = event_accumulator.EventAccumulator(\n        event_file,\n        size_guidance={\n            event_accumulator.SCALARS: 0,  # 0 means load all\n        },\n    )\n    ea.Reload()\n\n    # Get list of all available tags\n    tags = ea.Tags()[\"scalars\"]\n\n    # First, find the union of all steps\n    all_steps = set()\n    for tag in tags:\n        events = ea.Scalars(tag)\n        steps = [event.step for event in events]\n        all_steps.update(steps)\n\n    # Sort steps for proper ordering\n    all_steps = sorted(all_steps)\n\n    # Initialize the dataframe with steps\n    df = pd.DataFrame({\"step\": all_steps})\n\n    # Add each metric as a column\n    for tag in tags:\n        events = ea.Scalars(tag)\n        # Create a dictionary mapping steps to values\n        step_to_value = {event.step: event.value for event in events}\n        # Add the values to the dataframe, using NaN for missing steps\n        df[tag] = df[\"step\"].map(step_to_value)\n\n    return df\n```\n\n----------------------------------------\n\nTITLE: Accessing Forward Data Schedule in DDPM\nDESCRIPTION: Property method that returns the forward data schedule used in the diffusion process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_112\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef forward_data_schedule() -> torch.Tensor\n```\n\n----------------------------------------\n\nTITLE: Executing Evo2 Preprocessing\nDESCRIPTION: Runs the preprocess_evo2 command using the generated configuration file to preprocess the chromosome data.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!preprocess_evo2 --config preprocess_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Converting Float Time to Index in Python\nDESCRIPTION: Converts a float time value to a discrete time index. It takes a tensor of float time values and the number of discrete time steps as input, returning a tensor of corresponding time indices.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef float_time_to_index(time: torch.Tensor,\n                        num_time_steps: int) -> torch.Tensor\n```\n\n----------------------------------------\n\nTITLE: Initializing PickledDataWDS in Python\nDESCRIPTION: This snippet shows the __init__ method of PickledDataWDS. It takes parameters for the pickle directory, subset names, and other optional arguments. It initializes the PickledDataWDS object with the necessary configuration for data processing.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-webdatamodule/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(dir_pickles: str,\n             names_subset: Dict[Split, List[str]],\n             *args,\n             n_tars_wds: Optional[int] = None,\n             **kwargs) -> None\n```\n\n----------------------------------------\n\nTITLE: Saving SCDL Dataset to Disk in Python\nDESCRIPTION: Demonstrates how to save a SingleCellMemMapDataset to disk, ensuring the on-disk object is in a valid serialized state for later loading.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata.save()\n```\n\n----------------------------------------\n\nTITLE: Implementing Forward Process for VDM in Python\nDESCRIPTION: Method to perform the forward diffusion process, getting x(t) with a given time t from data and optional noise in a Variational Diffusion Model.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_80\n\nLANGUAGE: python\nCODE:\n```\ndef forward_process(data: Tensor, t: Tensor, noise: Optional[Tensor] = None)\n```\n\n----------------------------------------\n\nTITLE: Integration Step Methods\nDESCRIPTION: Methods for performing ODE and SDE integration steps using Euler method and score-based Langevin updates.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_98\n\nLANGUAGE: python\nCODE:\n```\ndef step(model_out: Tensor,\n         xt: Tensor,\n         dt: Tensor,\n         t: Optional[Tensor] = None,\n         mask: Optional[Tensor] = None,\n         center: Bool = False)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef step_score_stochastic(model_out: Tensor,\n                          xt: Tensor,\n                          dt: Tensor,\n                          t: Tensor,\n                          mask: Optional[Tensor] = None,\n                          gt_mode: str = \"tan\",\n                          gt_p: Float = 1.0,\n                          gt_clamp: Optional[Float] = None,\n                          score_temperature: Float = 1.0,\n                          noise_temperature: Float = 1.0,\n                          t_lim_ode: Float = 0.99,\n                          center: Bool = False)\n```\n\n----------------------------------------\n\nTITLE: Multi-Metric Training Plot Generation with Seaborn\nDESCRIPTION: Function to create multiple subplots showing different training metrics using seaborn. Supports variable number of metrics with shared x-axis for steps.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef plot_multiple_training_metrics(df, metrics_to_plot, figsize=(15, 10)):\n    \"\"\"Given a pandas DataFrame with the training metrics, plot the metrics.\"\"\"\n    n = len(metrics_to_plot)\n    fig, axes = plt.subplots(n, 1, figsize=figsize, sharex=True)\n\n    if n == 1:  # Handle the case of a single plot\n        axes = [axes]\n\n    sns.set_style(\"whitegrid\")\n\n    for i, metric in enumerate(metrics_to_plot):\n        if metric in df.columns:\n            sns.lineplot(x=\"step\", y=metric, data=df, ax=axes[i], linewidth=2.5, errorbar=\"sd\")\n            axes[i].set_title(metric, fontsize=14)\n            axes[i].set_ylabel(\"Value\", fontsize=12)\n    axes[-1].set_xlabel(\"Steps\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Loss Weight Function Setter\nDESCRIPTION: Sets the loss weight calculation function for the DDPM instance.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_129\n\nLANGUAGE: python\nCODE:\n```\ndef set_loss_weight_fn(fn)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Dependencies\nDESCRIPTION: Installs necessary Python packages (biopython and openpyxl) and sets up environment configuration for CI testing mode.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install biopython openpyxl\nimport os\n\n\n# Runs a subset of the model layers to test that the notebook runs in CI, but the output will be incorrect.\nFAST_CI_MODE: bool = os.environ.get(\"FAST_CI_MODE\", False)\n```\n\n----------------------------------------\n\nTITLE: Initializing BatchDataAugmentation Class for Optimal Transport\nDESCRIPTION: This class facilitates the creation of batch augmentation objects based on specified optimal transport types. It initializes with device and thread count parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_102\n\nLANGUAGE: python\nCODE:\n```\nclass BatchDataAugmentation():\n    def __init__(device, num_threads)\n```\n\n----------------------------------------\n\nTITLE: Score Function Estimation in Python\nDESCRIPTION: Converts data prediction to estimated score function by comparing predicted and current data points at a given time step.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_85\n\nLANGUAGE: python\nCODE:\n```\ndef score(x_hat: Tensor, xt: Tensor, t: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Interrogating Single Cell Datasets in Python\nDESCRIPTION: Shows how to use various methods of the SingleCellMemMapDataset to get information about the dataset, such as number of rows, variables, and values.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata.number_of_rows()\n## 25382\n\ndata.number_of_variables()\n## [34455]\n\ndata.number_of_values()\n## 874536810\n\ndata.number_nonzero_values()\n## 26947275\n```\n\n----------------------------------------\n\nTITLE: Preparing Sequence-level Classification Dataset in Python\nDESCRIPTION: This code creates an artificial dataset for sequence-level classification. It assigns arbitrary class labels to protein sequences and prepares a DataFrame for further processing.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass_labels = [\n    \"E_class\",\n    \"C_class\",\n    \"H_class\",\n    \"H_class\",\n    \"C_class\",\n    \"H_class\",\n    \"H_class\",\n    \"C_class\",\n    \"H_class\",\n    \"C_class\",\n]\n\ndata = [(seq, label) for seq, label in zip(artificial_sequence_data, class_labels)]\n\n# Create a DataFrame\ndf = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])\n```\n\n----------------------------------------\n\nTITLE: Running ESM2 Pretraining Script in Bash\nDESCRIPTION: This bash command demonstrates how to run the ESM2 pretraining script with various configuration options, including data paths, model architecture parameters, and training settings. It uses the train_esm2 executable or Python script.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/pretrain.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Enable fused attention in transformer engine for speed-up\nDATA_DIR=$(download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source ngc)\n\ntrain_esm2 \\\n    --train-cluster-path ${DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet \\\n    --train-database-path ${DATA_DIR}/2024_03_sanity/train_sanity.db \\\n    --valid-cluster-path ${DATA_DIR}/2024_03_sanity/valid_clusters.parquet \\\n    --valid-database-path ${DATA_DIR}/2024_03_sanity/validation.db \\\n    --precision=\"bf16-mixed\" \\\n    --num-gpus 1 \\\n    --num-nodes 1 \\\n    --num-steps 100 \\\n    --val-check-interval 25 \\\n    --max-seq-length 1024 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2 \\\n    --num-layers 33 \\\n    --hidden-size 1280 \\\n    --num-attention-head 20 \\\n    --ffn-hidden-size 5120 \\\n    --tensor-model-parallel-size 1 \\\n    --create-tensorboard-logger \\\n    --wandb_project=__your_wandb_project__ \\\n    --experiment-name=__your_wandb_experiment_name\n```\n\n----------------------------------------\n\nTITLE: Implementing Kabsch Alignment Algorithm for Single Tensors in Python\nDESCRIPTION: Method to find the rotation matrix that minimizes RMSD between target and noise tensors. Takes target and noise tensors as input and returns the rotation matrix and aligned target.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_73\n\nLANGUAGE: python\nCODE:\n```\ndef kabsch_align(target: Tensor, noise: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Calculating Maximum Steps with Confidence for MDLM in Python\nDESCRIPTION: Method to determine the maximum number of steps with confidence by counting occurrences of mask tokens in the input tensor, useful for pacing the unmasking process during generation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_46\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_steps_confidence(xt: Tensor, num_tokens_unmask: int = 1)\n```\n\n----------------------------------------\n\nTITLE: Processing Data Predictions in DDPM\nDESCRIPTION: Method to convert model outputs to data predictions based on the prediction type (noise, data, or v-prediction) following formulas from Progressive Distillation paper.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_121\n\nLANGUAGE: python\nCODE:\n```\ndef process_data_prediction(model_output: Tensor, sample: Tensor, t: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Implementing Discrete Uniform Prior Distribution\nDESCRIPTION: Concrete implementation of a discrete uniform prior distribution with customizable number of classes.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass DiscreteUniformPrior(DiscretePriorDistribution):\n    def __init__(num_classes: int = 10) -> None\n    def sample(shape: Tuple,\n           mask: Optional[Tensor] = None,\n           device: Union[str, torch.device] = \"cpu\",\n           rng_generator: Optional[torch.Generator] = None) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Creating Work Directory for ESM-2 Tutorial\nDESCRIPTION: Sets up the working directory to store data and results. If the cleanup flag is True and the directory exists, it will be removed first. The directory is then created if it doesn't exist.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwork_dir = \"/workspace/bionemo2/esm2_mutant_design_tutorial\"\n\nif cleanup and os.path.exists(work_dir):\n    shutil.rmtree(work_dir)\n\nif not os.path.exists(work_dir):\n    os.makedirs(work_dir)\n    print(f\"Directory '{work_dir}' created.\")\nelse:\n    print(f\"Directory '{work_dir}' already exists.\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Alpha Bar in DDPM\nDESCRIPTION: Property method that returns the alpha bar values used in the diffusion process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_117\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef alpha_bar() -> torch.Tensor\n```\n\n----------------------------------------\n\nTITLE: Purity Sampling Implementation for Discrete Flow Matching\nDESCRIPTION: Method to perform purity sampling, which selects tokens with highest confidence during the generation process. This implements a specialized sampling technique that focuses on unmasking high-confidence predictions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_59\n\nLANGUAGE: python\nCODE:\n```\ndef step_purity(logits: Tensor,\n                t: Tensor,\n                xt: Tensor,\n                dt: Tensor | float,\n                temperature: Float = 1.0,\n                stochasticity: Float = 1.0) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Running Geneformer Inference in Bash\nDESCRIPTION: Executes the Geneformer inference script using the pre-trained model checkpoint on the test dataset.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n!python /workspace/bionemo2/sub-packages/bionemo-geneformer/src/bionemo/geneformer/scripts/infer_geneformer.py \\\n    --data-dir {test_tutorial_processed_dir} \\\n    --checkpoint-path {pretrained_checkpoint_path} \\\n    --results-path {tutorial_output_dir}\n```\n\n----------------------------------------\n\nTITLE: Implementing InferenceSchedule Base Class\nDESCRIPTION: Abstract base class for inference time schedules with initialization parameters for steps, timing, and device placement.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nclass InferenceSchedule(ABC):\n    def __init__(nsteps: int,\n             min_t: Float = 0,\n             padding: Float = 0,\n             dilation: Float = 0,\n             direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n             device: Union[str, torch.device] = \"cpu\"):\n```\n\n----------------------------------------\n\nTITLE: Running ESM-2 Inference via Bash Command\nDESCRIPTION: This bash command performs inference using a fine-tuned ESM-2 model. It specifies the checkpoint path, configuration class, input data, and output path for results.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ninfer_esm2 --checkpoint-path {checkpoint_path} \\\n             --config-class ESM2FineTuneSeqConfig \\\n             --data-path {data_path} \\\n             --results-path {results_path} \\\n             --micro-batch-size 3 \\\n             --num-gpus 1 \\\n             --precision \"bf16-mixed\" \\\n             --include-embeddings \\\n             --include-input-ids\n```\n\n----------------------------------------\n\nTITLE: Core Interpolation and Vector Field Calculation\nDESCRIPTION: Methods for data interpolation and vector field target calculation using linear interpolation based on rectified flow and conditional flow matching.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_96\n\nLANGUAGE: python\nCODE:\n```\ndef interpolate(data: Tensor, t: Tensor, noise: Tensor) -> Tensor\n```\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_target(data: Tensor,\n                     noise: Tensor,\n                     mask: Optional[Tensor] = None) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Initializing UniformTimeDistribution in Python\nDESCRIPTION: Creates a UniformTimeDistribution object with specified minimum and maximum time values, discretization options, and an optional random number generator. It supports both continuous and discrete time distributions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(min_t: Float = 0.0,\n             max_t: Float = 1.0,\n             discrete_time: Bool = False,\n             nsteps: Optional[int] = None,\n             rng_generator: Optional[torch.Generator] = None)\n```\n\n----------------------------------------\n\nTITLE: Loading Test Data in Python\nDESCRIPTION: Example Python code showing how to load test data or model weights using the load function. The function returns a path to the specified file, fetching it from the default source if not available locally.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-core/src/bionemo/core/data/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npath_to_my_checkpoint = load(\"esm2/nv_650m:1.0\")\nconfig = ESM2Config(nemo1_ckpt_path=path_to_my_checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Linear SNR Transform Implementation\nDESCRIPTION: Implementation of linear SNR schedule with configurable minimum value.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nclass LinearSNRTransform(ContinuousSNRTransform):\n    def __init__(min_value: Float = 1.0e-4)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Splitting Single Cell Data using cellxgene-census in Python\nDESCRIPTION: Downloads B cell data from COVID-19 lung tissue using cellxgene-census API, splits it into train, validation, and test sets, and saves as h5ad files.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cellxgene_census\n\n\nfrac_train = 0.8\nfrac_val = 0.1\nfrac_test = 0.1\n\nwith cellxgene_census.open_soma(census_version=\"2023-12-15\") as census:\n    filter1 = (\n        \"cell_type == 'B cell' and tissue_general == 'lung' and disease == 'COVID-19' and is_primary_data == True\"\n    )\n\n    adata = cellxgene_census.get_anndata(\n        census=census,\n        organism=\"Homo sapiens\",\n        obs_value_filter=filter1,\n    )\n    n_train = int(adata.shape[0] * frac_train)\n    n_val = int(adata.shape[0] * frac_val)\n    n_test = adata.shape[0] - n_train - n_val\n    # Create some splits, bad practice since ordering may be a thing but let's just take ranges for this demo.\n    adata_train = adata[0:n_train].copy()\n    adata_val = adata[n_train : (n_train + n_val)].copy()\n    adata_test = adata[(n_train + n_val) :].copy()\n    adata_train.write(demo_data_train_download_path)\n    adata_val.write(demo_data_val_download_path)\n    adata_test.write(demo_data_test_download_path)\n```\n\n----------------------------------------\n\nTITLE: Installing cellxgene-census API in Python\nDESCRIPTION: Installs the cellxgene-census library using pip to enable downloading of single cell data.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install cellxgene-census\n```\n\n----------------------------------------\n\nTITLE: Setting Up Work Directory for ESM-2 Fine-tuning in Python\nDESCRIPTION: This snippet sets up a work directory for storing data and results of the ESM-2 fine-tuning process. It includes an option to clean up existing directories.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncleanup: bool = True\n```\n\nLANGUAGE: python\nCODE:\n```\nwork_dir = \"/workspace/bionemo2/esm2_finetune_tutorial\"\n\nif cleanup and os.path.exists(work_dir):\n    shutil.rmtree(work_dir)\n\nif not os.path.exists(work_dir):\n    os.makedirs(work_dir)\n    print(f\"Directory '{work_dir}' created.\")\nelse:\n    print(f\"Directory '{work_dir}' already exists.\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing D3PM Generated Distribution\nDESCRIPTION: Plots a histogram of the number of ones in the samples generated by the D3PM model to visualize how well it has learned the target distribution.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ncounts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Model Results\nDESCRIPTION: Loads the model prediction results and prints the shape of each output tensor that exists in the results dictionary.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresults = torch.load(f\"{example_dir}/predictions__rank_0.pt\")\n\nfor key, val in results.items():\n    if val is not None:\n        print(f\"{key}\\t{val.shape}\")\n```\n\n----------------------------------------\n\nTITLE: Prediction with Fine-tuned MNIST Model\nDESCRIPTION: Command to run the prediction script using the fine-tuned MNIST model. Requires the fine-tuned model directory path as an input argument.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-example_model/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython src/bionemo/example_model/training_scripts/predict_mnist.py  --finetune_dir <finetune_dir>\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Data Resources in YAML\nDESCRIPTION: Example YAML configuration for specifying test data resources in BioNeMo. This defines a model with its NGC and pbss locations, SHA256 hash for verification, owner information, and description.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-core/src/bionemo/core/data/README.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- tag: nv_650m:1.0\n  ngc: \"nvidia/clara/esm2nv650m:1.0\"\n  ngc_registry: model\n  pbss: \"s3://bionemo-ci/models/esm2nv_650M_converted.nemo\"\n  sha256: 1e38063cafa808306329428dd17ea6df78c9e5d6b3d2caf04237c555a1f131b7\n  owner: Farhad Ramezanghorbani <farhadr@nvidia.com>\n  description: >\n    A pretrained 650M parameter ESM-2 model.\n    See https://ngc.nvidia.com/catalog/models/nvidia:clara:esm2nv650m.\n```\n\n----------------------------------------\n\nTITLE: Configuring SLURM Batch Script for BioNeMo Evo 2 Training\nDESCRIPTION: A complete SLURM batch script template for running BioNeMo Evo 2 training in a containerized environment. The script includes SLURM directives for resource allocation, job-specific parameters for model configuration, filesystem mounts for data and model storage, and the training command execution.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/using-slurm.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n# SLURM directives\n# =========================\n#SBATCH --account=[INSERT ACCOUNT HERE]\n#SBATCH --nodes=1\n#SBATCH --partition=[INSERT PARTITIONs HERE]\n#SBATCH --ntasks-per-node=8\n#SBATCH --time=00:15:00\n#SBATCH --mem=0\n#SBATCH --job-name=[INSERT JOB NAME HERE]\n#SBATCH --mail-type=FAIL\n#SBATCH --exclusive\nset -x # Enable debugging (prints executed commands)\n# Job-specific parameters\n# =========================\nIMAGE_NAME=[INSERT IMAGE NAME HERE]\nEXPERIMENT_NAME=[INSERT PROJECT NAME HERE]\nMODEL_SIZE=7b\nCP_SIZE=1\nTP_SIZE=1\nPP_SIZE=1\nMICRO_BATCH_SIZE=2\nGRAD_ACC_BATCHES=1\nSEQ_LEN=8192\nMAX_STEPS=100\nVAL_CHECK=50\nCLIP_GRAD=250\nEXTRA_ARGS=\"--enable-preemption --use-megatron-comm-overlap-llama3-8k --ckpt-async-save --overlap-grad-reduce --clip-grad $CLIP_GRAD --eod-pad-in-loss-mask\"\nEXTRA_ARG_DESC=\"BF16_perf_cg250_continue\"\nLR=0.0003\nMIN_LR=0.00003\nWU_STEPS=2500\n# 0xDEADBEEF\nSEED=1234\nWD=0.1\nADO=0.01\nHDO=0.01\n# Mounts\n# =========================\nDATA_PATH=/lustre/.../[INSERT DATA PATH HERE]\nDATA_MOUNT=/workspace/bionemo2/data\nMODEL_PATH=/lustre/.../[INSERT MODEL PATH HERE]\nMODEL_MOUNT=/workspace/bionemo2/model\nRESULTS_PATH=$MODEL_PATH/experiments/${EXPERIMENT_NAME}\nmkdir -p $RESULTS_PATH\nMOUNTS=${DATA_PATH}:${DATA_MOUNT},${MODEL_PATH}:${MODEL_MOUNT},$HOME/.cache:/root/.cache\n# Training command\n# =========================\nread -r -d '' COMMAND <<EOF\necho \"*******STARTING********\" \\\n&& echo \"---------------\" \\\n&& echo \"Starting training\" \\\n&&  \\\npython /workspace/bionemo2/sub-packages/bionemo-evo2/src/bionemo/evo2/run/train.py \\\n    -d /workspace/bionemo2/sub-packages/bionemo-evo2/examples/configs/full_pretrain_shortphase_config.yaml \\\n    --num-nodes=${SLURM_JOB_NUM_NODES} \\\n    --devices=${SLURM_NTASKS_PER_NODE} \\\n    --grad-acc-batches $GRAD_ACC_BATCHES \\\n    --max-steps=$MAX_STEPS \\\n    --seed $SEED \\\n    ${EXTRA_ARGS} \\\n    --no-wandb \\\n    --lr $LR \\\n    --wd $WD \\\n    --min-lr $MIN_LR \\\n    --warmup-steps $WU_STEPS \\\n    --attention-dropout $ADO \\\n    --hidden-dropout $HDO \\\n    --limit-val-batches=20 \\\n    --val-check-interval=${VAL_CHECK} \\\n    --experiment-dir=/workspace/bionemo2/model/checkpoints/${EXPERIMENT_NAME} \\\n    --seq-length=${SEQ_LEN} \\\n    --tensor-parallel-size=${TP_SIZE} \\\n    --context-parallel-size=${CP_SIZE} \\\n    --pipeline-model-parallel-size=${PP_SIZE} \\\n    --workers 8 \\\n    --micro-batch-size=${MICRO_BATCH_SIZE} \\\n    --model-size=${MODEL_SIZE}\nEOF\nsrun \\\n    --output ${RESULTS_PATH}/slurm-%j.out \\\n    --error ${RESULTS_PATH}/error-%j.out \\\n    --container-image=$IMAGE_NAME \\\n    --container-mounts ${MOUNTS} \\\n    bash -c \"${COMMAND}\"\nset +x # Disable debugging\n```\n\n----------------------------------------\n\nTITLE: Validating Dataset Processing with Python\nDESCRIPTION: Python code for verifying dataset equivalence between processed and reference datasets using IndexedDataset class, including length checks and content comparison.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/data/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom megatron.core.datasets.indexed_dataset import IndexedDataset\nds_train_ref = IndexedDataset(\"./data_promoters_train_text_CharLevelTokenizer_document\")\nds_val_ref = IndexedDataset(\"./data_promoters_valid_text_CharLevelTokenizer_document\")\nds_test_ref = IndexedDataset(\"./data_promoters_test_text_CharLevelTokenizer_document\")\nds_train_ours = IndexedDataset(\"./promoters_uint8_distinct_byte-level_train\")\nlen(ds_train_ours) == len(ds_train_ref) + len(ds_test_ref) + len(ds_val_ref)\n\n','.join([str(t) for t in ds_train_ref[0]])\n\nall_ref_data = {','.join([str(t) for t in rec]) for ds in [ds_train_ref, ds_val_ref, ds_test_ref] for rec in ds}\nlen(all_ref_data) == len(ds_train_ours)\nlen(all_ref_data)\nall_our_data = {','.join([str(t) for t in rec]) for ds in [ds_train_ours] for rec in ds}\nlen(all_our_data)\nall_our_data == all_ref_data\n```\n\n----------------------------------------\n\nTITLE: Visualizing D3PM Training Loss\nDESCRIPTION: Plots the training loss over iterations to visualize the convergence of the D3PM model training process, with a fixed y-axis range for better visibility.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nplt.plot(losses, label=\"Training Loss\", linestyle=\"-\", color=\"blue\", marker=\"o\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss\")\nplt.legend()\nplt.grid(True)\nplt.ylim([0, 1])\n# plt.yscale('log')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining Data Paths for BioNeMo Workflow in Python\nDESCRIPTION: Sets up directory paths for storing downloaded data, processed data, and inference output for the BioNeMo workflow.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntutorial_data_dir = \"/workspace/bionemo2/data/singlecell_tutorial/download_anndata\"\ntrain_tutorial_data_dir = \"/workspace/bionemo2/data/singlecell_tutorial/download_anndata/train\"\nval_tutorial_data_dir = \"/workspace/bionemo2/data/singlecell_tutorial/download_anndata/val\"\ntest_tutorial_data_dir = \"/workspace/bionemo2/data/singlecell_tutorial/download_anndata/test\"\n\ntrain_tutorial_processed_dir = \"/workspace/bionemo2/data/singlecell_tutorial/processed_data/train\"\nval_tutorial_processed_dir = \"/workspace/bionemo2/data/singlecell_tutorial/processed_data/val\"\ntest_tutorial_processed_dir = \"/workspace/bionemo2/data/singlecell_tutorial/processed_data/test\"\ntutorial_output_dir = \"/workspace/bionemo2/data/singlecell_tutorial/inference_output\"\ntutorial_output_inference_pickle = f\"{tutorial_output_dir}/human_covid19_bcells_from_scratch.pkl\"\ndemo_data_train_download_path = f\"{train_tutorial_data_dir}/human_covid19_bcells.h5ad\"\ndemo_data_val_download_path = f\"{val_tutorial_data_dir}/human_covid19_bcells.h5ad\"\ndemo_data_test_download_path = f\"{test_tutorial_data_dir}/human_covid19_bcells.h5ad\"\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Protein Sequence Dataset\nDESCRIPTION: Generates a sample dataset of protein sequences and saves it to a CSV file for inference\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/inference.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nartificial_sequence_data = [\n    \"TLILGWSDKLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"GRFNVWLGGNESKIRQVLKAVKEIGVSPTLFAVYEKN\",\n    \"DELTALGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"KLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n    \"LFGAIGNAISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n    \"LGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"ISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n    \"SGSKASSDSQDANQCCTSCEDNAPATSYCVECSEPLCETCVEAHQRVKYTKDHTVRSTGPAKT\",\n]\n\n# Create a DataFrame\ndf = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"sequences.csv\")\ndf.to_csv(data_path, index=False)\n```\n\n----------------------------------------\n\nTITLE: ESM-2 Embeddings Tensor Shape\nDESCRIPTION: Shows the shape of embeddings tensor output from ESM-2 model, with batch size of 10 and embedding dimension of 1280.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/inference.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# embeddings\ttorch.Size([10, 1280])\n```\n\n----------------------------------------\n\nTITLE: Creating SingleCellMemMapDataset\nDESCRIPTION: Initializes a SingleCellMemMapDataset object with temporary directory and input data.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/examples/example_notebook.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndataset_temp_dir = tempfile.TemporaryDirectory()\ndataset_dir = os.path.join(dataset_temp_dir.name, \"97e_scmm\")\n\ndata = SingleCellMemMapDataset(dataset_dir, input_data)\n```\n\n----------------------------------------\n\nTITLE: Getting Length of BucketBatchSampler in Python\nDESCRIPTION: This method returns the number of batches in the BucketBatchSampler. It can only be called if the base_batch_sampler_class has __len__() implemented.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef __len__() -> int\n```\n\n----------------------------------------\n\nTITLE: Data Scaling Operations\nDESCRIPTION: Methods for scaling data up and down using the configured data scale factor.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_95\n\nLANGUAGE: python\nCODE:\n```\ndef undo_scale_data(data: Tensor) -> Tensor\n```\n\nLANGUAGE: python\nCODE:\n```\ndef scale_data(data: Tensor) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Implementing Interpolation Method for VDM in Python\nDESCRIPTION: Method to get x(t) with given time t from noise and data in a Variational Diffusion Model. It takes data, time, and noise tensors as input.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_79\n\nLANGUAGE: python\nCODE:\n```\ndef interpolate(data: Tensor, t: Tensor, noise: Tensor)\n```\n\n----------------------------------------\n\nTITLE: Using IdentityMultiEpochDatasetWrapper for Deterministic Datasets (Python)\nDESCRIPTION: Shows how to use IdentityMultiEpochDatasetWrapper and MultiEpochDatasetResampler for deterministic datasets that require epoch-level shuffling in multi-epoch training scenarios.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/background/megatron_datasets.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyDeterministicDataset:\n    def __getitem__(self, index: int):\n        ...\n\ndataset = IdentityMultiEpochDatasetWrapper(MyDeterministicDataset())\nfor sample in MultiEpochDatasetResampler(dataset, num_epochs=3, shuffle=True):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Converting ZeRO-1 Between Model Parallel Sizes\nDESCRIPTION: Command to convert ZeRO-1 checkpoints between different model parallel configurations, allowing transformation of sharded checkpoints to different parallelism degrees.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py --source_dir <CKPT_DIR> --output_dir <OUTPUT_DIR> --mp_size <TARGET_MODEL_PARALLEL_SIZE>\n```\n\n----------------------------------------\n\nTITLE: Visualizing DFM Generated Distribution\nDESCRIPTION: Plots a histogram of the number of ones in the generated samples to visualize how well the model has learned the target distribution.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncounts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Loading Inference Results in Python\nDESCRIPTION: Loads the inference results from the saved PyTorch file and prints the available keys in the result dictionary.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n\ninference_results = torch.load(tutorial_output_inference_pickle)\nprint(inference_results.keys())\n```\n\n----------------------------------------\n\nTITLE: Loading ESM-2 Model Checkpoint\nDESCRIPTION: Downloads the pre-trained ESM-2 model checkpoint from NGC registry\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/inference.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.core.data.load import load\n\n\ncheckpoint_path = load(\"esm2/650m:2.0\")\nprint(checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Running ESM-2 Fine-tuning for Sequence-level Regression via Bash\nDESCRIPTION: This bash command executes the ESM-2 fine-tuning process for sequence-level regression. It specifies model configuration, dataset, and training parameters including learning rate and dropout.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nfinetune_esm2 \\\n    --restore-from-checkpoint-path {pretrain_checkpoint_path} \\\n    --train-data-path {data_path} \\\n    --valid-data-path {data_path} \\\n    --config-class ESM2FineTuneSeqConfig \\\n    --dataset-class InMemorySingleValueDataset \\\n    --task-type \"regression\" \\\n    --mlp-ft-dropout 0.25 \\\n    --mlp-hidden-size 256 \\\n    --mlp-target-size 1 \\\n    --experiment-name \"sequence-level-regression\" \\\n    --num-steps 50 \\\n    --num-gpus 1 \\\n    --val-check-interval 10 \\\n    --log-every-n-steps 10 \\\n    --encoder-frozen \\\n    --lr 5e-3 \\\n    --lr-multiplier 1e2 \\\n    --scale-lr-layer \"regression_head\" \\\n    --result-dir {work_dir}  \\\n    --micro-batch-size 2 \\\n    --num-gpus 1 \\\n    --precision \"bf16-mixed\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Generated Samples Distribution\nDESCRIPTION: Creates visualization of the distribution of generated samples using bar plots and histograms.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ncounts = xt.flatten().cpu()\n\n# Compute frequency of each class index\nclass_counts = torch.bincount(counts)\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.bar(range(len(class_counts)), class_counts.numpy(), color=\"green\")\nplt.xlabel(\"Class Index\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Discrete Distribution of Class Indices\")\nplt.xticks(range(len(class_counts)))  # Set x-ticks to class indices\nplt.show()\n\ncounts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Applying Augmentation to Data Samples\nDESCRIPTION: Method to sample and apply optimal transport plan between batched data samples with optional masking.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_94\n\nLANGUAGE: python\nCODE:\n```\ndef apply_augmentation(x0: Tensor,\n                       x1: Tensor,\n                       mask: Optional[Tensor] = None,\n                       **kwargs) -> tuple\n```\n\n----------------------------------------\n\nTITLE: GPU FP8 Support Detection\nDESCRIPTION: Utility function to check GPU compatibility for FP8 operations. Verifies compute capability and returns device information for CUDA-enabled GPUs.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef check_fp8_support():\n    \"\"\"Check if FP8 is supported on the current GPU.\n\n    FP8 requires compute capability 8.9+ (Ada Lovelace/Hopper architecture or newer).\n    \"\"\"\n    if not torch.cuda.is_available():\n        return False, \"CUDA not available\"\n\n    device_props = torch.cuda.get_device_properties(0)\n    compute_capability = f\"{device_props.major}.{device_props.minor}\"\n    device_name = device_props.name\n\n    # FP8 is supported on compute capability 8.9+ (Ada Lovelace/Hopper architecture)\n    is_supported = (device_props.major > 8) or (device_props.major == 8 and device_props.minor >= 9)\n\n    return is_supported, f\"Device: {device_name}, Compute Capability: {compute_capability}\"\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset with Temporary Cache\nDESCRIPTION: Creates a temporary directory for caching and loads a sample dataset from NGC source using BioNeMo's load function. The temporary directory is automatically cleaned up after use.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-core/tests/bionemo/core/data/test_load_notebook.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith tempfile.TemporaryDirectory() as cache_dir:\n    load(\"scdl/sample\", source=\"ngc\", cache_dir=Path(cache_dir))\n```\n\n----------------------------------------\n\nTITLE: Examining Inference Schedule Time Steps\nDESCRIPTION: Displays the time steps of the generated inference schedule to inspect the progression of the sampling process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nts\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Genomic Sequences for Evo2 Model\nDESCRIPTION: Runs the preprocessing tool to convert raw FASTA files into tokenized binary format compatible with NeMo2/Megatron-LM for training Evo2 models. Requires a configuration file path.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npreprocess_evo2 -c <CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Creating Geneformer Pre-training Recipe in Bash\nDESCRIPTION: Generates a pre-training recipe configuration file for the Geneformer model using the bionemo-geneformer-recipe command.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n!bionemo-geneformer-recipe --recipe geneformer_10m_shortpretrain_recipe --dest pretrain-recipe-short.yaml --result-dir /workspace/bionemo2/results --data-path /workspace/bionemo2/data/singlecell_tutorial/processed_data/\n```\n\n----------------------------------------\n\nTITLE: Bucket Creation Usage Example\nDESCRIPTION: Example demonstrating how to use the create_buckets function to group elements into size-based buckets\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from bionemo.size_aware_batching.utils import create_buckets\n\n>>> sizes = torch.tensor([1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 22, 22, 22, 22])\n>>> buckets = create_buckets(sizes, max_width=5, min_bucket_count=10)\n>>> # 5 buckets: 1 <= sizes < 6, 6 <= sizes < 11, 11 <= sizes < 16, 16 <= sizes < 21, 21 <= sizes < 23\n>>> print(buckets.bucket_boundaries)\ntensor([ 1,  6, 11, 16, 21, 23])\n\n>>> # each with 12, 0, 0, 0, 4 elements respectively.\n>>> print(buckets.bucket_sizes)\ntensor([12,  0,  0,  0,  4])\n\n>>> sizes = torch.arange(20)\n>>> # min_bucket_count is used to control bucket size\n>>> buckets = create_buckets(sizes, max_width=10, min_bucket_count=5)\n>>> print(buckets.bucket_boundaries)\ntensor([ 0,  5, 10, 15, 20])\n\n>>> print(buckets.bucket_sizes)\ntensor([5, 5, 5, 5])\n```\n\n----------------------------------------\n\nTITLE: Bucket Creation Function\nDESCRIPTION: Function to create buckets for a list of integers with predefined maximal width and minimal bucket count constraints\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_buckets(sizes: torch.Tensor, max_width: int,\n                   min_bucket_count: int) -> Buckets\n```\n\n----------------------------------------\n\nTITLE: Beta Calculation Function\nDESCRIPTION: Computes drift coefficient for Ornstein-Uhlenbeck process using alpha derivative calculations.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_beta(t)\n```\n\n----------------------------------------\n\nTITLE: Simple Sampling for Discrete Flow Matching\nDESCRIPTION: Method to sample from model output logits with temperature control, providing more diversity than argmax sampling. This implements a standard categorical sampling approach for the flow matching model.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_61\n\nLANGUAGE: python\nCODE:\n```\ndef step_simple_sample(model_out: Tensor,\n                       temperature: float = 1.0,\n                       num_samples: int = 1)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Ground Truth Distribution\nDESCRIPTION: Generates and plots the target uniform distribution of binary sequences to compare with the model's generated distribution.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnum_ones = torch.randint(0, D + 1, (1000,))\nx1 = (torch.arange(D)[None, :] < num_ones[:, None]).long()\ncounts = x1.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Generating Alternative Inference Schedule\nDESCRIPTION: Creates a shorter inference schedule with 100 steps to demonstrate how different schedules can be configured.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nLinearInferenceSchedule(nsteps=100, min_t=0, inclusive_end=False).generate_schedule()\n```\n\n----------------------------------------\n\nTITLE: Saving Sequence Classification Data to CSV in Python\nDESCRIPTION: This snippet demonstrates how to save a DataFrame containing sequence classification data to a CSV file. It uses the pandas library to create and save the DataFrame.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"classification_data.csv\")\ndf.to_csv(data_path, index=False)\n```\n\n----------------------------------------\n\nTITLE: Setting Performance Parameters for Geneformer Training\nDESCRIPTION: This snippet provides instructions for setting the appropriate parameters to achieve optimal performance when training Geneformer models. It specifies the number of dataset workers and micro batch sizes for different model variants.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/geneformer.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n    Performance will increase if the `num_dataset_workers` and the `micro_batch_size` are set appropriately. For the above metrics, we set `num_dataset_workers=8`. For the 10m model, set `micro_batch_size=120` and for the 106m model set the `micro_batch_size=16`. This will enable you to achieve similar performance results.\n```\n\n----------------------------------------\n\nTITLE: Running Geneformer Pre-training in Bash\nDESCRIPTION: Executes the Geneformer pre-training process using the generated recipe configuration file.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n!python /workspace/bionemo2/sub-packages/bionemo-geneformer/src/bionemo/geneformer/run/main.py \\\n--config /workspace/bionemo2/docs/docs/user-guide/examples/bionemo-geneformer/pretrain-recipe-short.yaml\n```\n\n----------------------------------------\n\nTITLE: Downloading ESM-2 Pretraining Test Data via CLI\nDESCRIPTION: Command line script to download the test data for ESM-2 pretraining. Returns the path to the downloaded data which will be used to instantiate the ESMDataModule. Also shows the command for downloading the full dataset.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/pretrain.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndownload_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source ngc  # test data\n# download_bionemo_data esm2/fulldata_esm2_pretrain:2.0 --source ngc  # full data (~80GB)\n```\n\n----------------------------------------\n\nTITLE: WebDataModule Class Definition\nDESCRIPTION: Core class definition inheriting from LightningDataModule\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-webdatamodule/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass WebDataModule(L.LightningDataModule)\n```\n\n----------------------------------------\n\nTITLE: Computing Natural Logarithm with Clamping in Python\nDESCRIPTION: Calculates the natural logarithm of a tensor while clamping values to avoid numerical instability. It takes a tensor and an optional epsilon value for clamping as input.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef log(t, eps=1e-20)\n```\n\n----------------------------------------\n\nTITLE: Multi-batch Processing\nDESCRIPTION: Processes data with larger batch sizes using CSR format for sparse matrices.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/examples/example_notebook.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = lambda x: x  # noqa: E731\n\ndataloader = DataLoader(data, batch_size=8, shuffle=True, collate_fn=collate_sparse_matrix_batch)\nn_epochs = 1\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n```\n\n----------------------------------------\n\nTITLE: Scatter Plot Visualization of UMAP Embeddings in Python\nDESCRIPTION: This code creates a 2x2 grid of scatter plots using matplotlib, visualizing the UMAP embeddings colored by different covariates. It assumes the existence of 'adata_test' and uses the previously computed 'embedding'.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom matplotlib import pyplot as plt\n\n\nresults = adata_test.obs.copy()\nresults[\"x\"] = embedding[:, 0]\nresults[\"y\"] = embedding[:, 1]\n\ncovariates = [\"assay\", \"development_stage\", \"dataset_id\", \"sex\"]\nfig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True, figsize=(10, 10))\n\nfor ax, covar in zip(axes.flat, covariates):\n    for cov, cov_df in results.groupby(covar):\n        ax.scatter(\n            cov_df.x,\n            cov_df.y,\n            s=3,\n            alpha=0.75,\n            label=cov,\n        )\n    if len(results[covar].unique()) < 8:\n        ax.legend()\n    ax.set_title(f\"Embeddings by {covar}\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing DFM Training Loss\nDESCRIPTION: Plots the training loss over iterations to visualize the convergence of the Discrete Flow Matching model training process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nplt.plot(losses, label=\"Training Loss\", linestyle=\"-\", color=\"blue\", marker=\"o\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing DiscreteLinearInferenceSchedule in Python\nDESCRIPTION: This method initializes the DiscreteLinearInferenceSchedule class. It sets up parameters for the number of steps, minimum time, padding, dilation, direction, and device.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(nsteps: int,\n             min_t: Float = 0,\n             padding: Float = 0,\n             dilation: Float = 0,\n             direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n             device: Union[str, torch.device] = \"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Executing Geneformer Training Job with bionemo-geneformer-train\nDESCRIPTION: This snippet shows how to run a Geneformer training job using the bionemo-geneformer-train command. It specifies the data config class, model config class, and the configuration file to use.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/training-models.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbionemo-geneformer-train \\\n--data-config-cls bionemo.geneformer.run.config_models.GeneformerPretrainingDataConfig \\\n--model-config-cls bionemo.geneformer.run.config_models.ExposedGeneformerPretrainConfig \\\n--config my_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Preparing Sequence-level Regression Dataset in Python\nDESCRIPTION: This code creates an artificial dataset for sequence-level regression. It generates protein sequences and assigns labels based on sequence length, then saves the data to a CSV file.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nartificial_sequence_data = [\n    \"TLILGWSDKLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"GRFNVWLGGNESKIRQVLKAVKEIGVSPTLFAVYEKN\",\n    \"DELTALGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"KLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n    \"LFGAIGNAISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n    \"LGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"ISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n    \"SGSKASSDSQDANQCCTSCEDNAPATSYCVECSEPLCETCVEAHQRVKYTKDHTVRSTGPAKT\",\n]\n\ndata = [(seq, len(seq) / 100.0) for seq in artificial_sequence_data]\n\n# Create a DataFrame\ndf = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"regression_data.csv\")\ndf.to_csv(data_path, index=False)\n```\n\n----------------------------------------\n\nTITLE: Converting h5ad Files to SCDL Format for BioNeMo in Bash\nDESCRIPTION: Converts downloaded h5ad files to SCDL (Single Cell Data Loader) memmap format used by BioNeMo for train, validation, and test sets.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n!convert_h5ad_to_scdl \\\n  --data-path {train_tutorial_data_dir} \\\n  --save-path {train_tutorial_processed_dir}\n\n!convert_h5ad_to_scdl \\\n  --data-path {val_tutorial_data_dir} \\\n  --save-path {val_tutorial_processed_dir}\n\n!convert_h5ad_to_scdl \\\n  --data-path {test_tutorial_data_dir} \\\n  --save-path {test_tutorial_processed_dir}\n```\n\n----------------------------------------\n\nTITLE: Training AMPLIFY 350M Model Configuration\nDESCRIPTION: Command-line configuration for training the 350M parameter version of AMPLIFY, specifying larger architecture parameters and training settings.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/amplify.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython /workspace/bionemo-framework/sub-packages/bionemo-amplify/src/bionemo/amplify/train_amplify.py \\\n    ...\n    --num-nodes=4 \\\n    --devices=8 \\\n    --min-seq-length 512 \\\n    --max-seq-length 512 \\\n    --num-layers 32 \\\n    --num-attention-heads 15 \\\n    --hidden-size 960 \\\n    --ffn-hidden-size 3840 \\\n    --micro-batch-size 128\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Dataset\nDESCRIPTION: Creates a YAML configuration file for the training dataset, specifying train, validation, and test splits.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\noutput_pfx = str(Path(os.path.abspath(\"preprocessed_data\")) / \"chr20_21_22_uint8_distinct_byte-level\")\noutput_yaml = f\"\"\"\n- dataset_prefix: {output_pfx}_train\n  dataset_split: train\n  dataset_weight: 1.0\n- dataset_prefix: {output_pfx}_val\n  dataset_split: validation\n  dataset_weight: 1.0\n- dataset_prefix: {output_pfx}_test\n  dataset_split: test\n  dataset_weight: 1.0\n\"\"\"\nwith open(\"training_data_config.yaml\", \"w\") as f:\n    print(output_yaml, file=f)\n```\n\n----------------------------------------\n\nTITLE: Preparing Data in PickledDataWDS using Python\nDESCRIPTION: This method prepares the data by processing pickled files into webdataset tar archives. It's called by the main process in the Lightning workflow and should not be relied upon for state updates in subprocesses.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-webdatamodule/README.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef prepare_data() -> None\n```\n\n----------------------------------------\n\nTITLE: SLURM Training Script Configuration\nDESCRIPTION: Bash script for configuring distributed training on a SLURM cluster with detailed environment variables and mount configurations.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n#SBATCH --nodes=4                       # number of nodes\n#SBATCH --gpus-per-node=8\n#SBATCH --ntasks-per-node=8                 # n tasks per machine (one task per gpu) <required>\n#SBATCH --time=04:00:00                     # wall time  (8 for batch, backfill, 2 for batch_short)\n#SBATCH --mem=0                             # all mem avail\n#SBATCH --exclusive\nset -x\n\nCONFIG_PATH_IN_CONTAINER=/workspace/bionemo2/sub-packages/bionemo-evo2/examples/configs/full_pretrain_shortphase_config.yaml\nIMAGE_PATH=nvcr.io/nvidia/clara/bionemo-framework:nightly\nWANDB_PROJECT_NAME=\nMODEL_SIZE=1b\nCP_SIZE=1\nTP_SIZE=1\nPP_SIZE=1\nMICRO_BATCH_SIZE=8\nGRAD_ACC_BATCHES=1\nSEQ_LEN=8192\nMAX_STEPS=580000\nVAL_CHECK=500\nCLIP_GRAD=250\nEXTRA_ARGS=\"--enable-preemption --ckpt-async-save --overlap-grad-reduce --clip-grad $CLIP_GRAD --eod-pad-in-loss-mask\"\nLR=0.000015\nMIN_LR=0.0000015\nWU_STEPS=100\nSEED=1234\nWD=0.001\nADO=0.01\nHDO=0.01\nEXPERIMENT_NAME=fine_tune_evo2_1b_on_bf16\n\nexport TORCH_NCCL_AVOID_RECORD_STREAMS=1\n\nMOUNTS=${DATA_PATH}:${DATA_MOUNT},${RESULTS_PATH_CLUSTER}:${RESULTS_PATH_IMAGE},${NETRC_PATH}:${NETRC_MOUNT},${CKPT_MOUNT_CLUSTER}:${CKPT_MOUNT_IMAGE},$HOME/.cache:/root/.cache\n\nmkdir -p ${RESULTS_PATH_CLUSTER}\nif [ -f ${RESULTS_PATH_CLUSTER}/run.id ];\nthen\n    RUN_ID=$(<${RESULTS_PATH_CLUSTER}/run.id)\nelse\n    array=()\n    for i in {a..z} {A..Z} {0..9};\n    do\n    array[$RANDOM]=$i\n    done\n    RUN_ID=$(printf %s ${array[@]::8})\n    echo $RUN_ID > ${RESULTS_PATH_CLUSTER}/run.id\nfi\n```\n\n----------------------------------------\n\nTITLE: Creating Batch Augmentation Objects in Python\nDESCRIPTION: This method creates a batch augmentation object of the specified type. It takes an AugmentationType as input and returns the corresponding augmentation object if supported.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_103\n\nLANGUAGE: python\nCODE:\n```\ndef create(method_type: AugmentationType)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Discrete Uniform Prior Distribution\nDESCRIPTION: Generates and plots samples from the prior distribution to demonstrate the starting point of the generative process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nx0 = dfm.sample_prior((10000, D))\ncounts = x0.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D + 2))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Sampling from UniformTimeDistribution in Python\nDESCRIPTION: Generates samples from the uniform time distribution. It accepts the number of samples, device, and an optional random number generator as parameters. Returns a tensor of samples.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef sample(n_samples: Union[int, Tuple[int, ...], torch.Size],\n           device: Union[str, torch.device] = \"cpu\",\n           rng_generator: Optional[torch.Generator] = None)\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch/ZeRO-1 to NeMo2 Checkpoints\nDESCRIPTION: Script command to convert PyTorch or ZeRO-1 checkpoints to NeMo2 format. Supports 7b and 40b model sizes and outputs in either torch_dist or zarr format.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_to_nemo.py --model-path <CKPT_FILE> --output-dir <OUTPUT_DIR> --model-size <MODEL_SIZE> --ckpt-format <CONVERTED_CKPT_FORMAT>\n```\n\n----------------------------------------\n\nTITLE: Initializing MDLM Components\nDESCRIPTION: Sets up MDLM model components including prior distribution, time distribution, noise schedule, and inference schedule.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.moco.distributions.prior import DiscreteMaskedPrior\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import MDLM\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\nfrom bionemo.moco.schedules.noise.continuous_noise_transforms import CosineExpNoiseTransform\n\n\nDEVICE = \"cuda:0\"\nprior = DiscreteMaskedPrior(num_classes=2, inclusive=False)\ntime_distribution = UniformTimeDistribution(discrete_time=False)\nnoise_schedule = CosineExpNoiseTransform()\nmdlm = MDLM(\n    time_distribution=time_distribution, prior_distribution=prior, noise_schedule=noise_schedule, device=DEVICE\n)\nschedule = LinearInferenceSchedule(direction=\"diffusion\", nsteps=1000)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Input Data\nDESCRIPTION: Downloads single cell data from CellxGene using pooch with hash verification for data integrity.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/examples/example_notebook.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninput_data = pooch.retrieve(\n    \"https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\",\n    path=BIONEMO_CACHE_DIR / \"hdf5s\",\n    known_hash=\"a0728e13a421bbcd6b2718e1d32f88d0d5c7cb92289331e3f14a59b7c513b3bc\",\n)\n```\n\n----------------------------------------\n\nTITLE: Memory Collection Usage Example\nDESCRIPTION: Example showing how to use the collect_cuda_peak_alloc function to track memory usage in a training workflow\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from bionemo.size_aware_batching.utils import collect_cuda_peak_alloc\n\n>>> # prepare dataset, model and other components of a workflow\n>>> # for which the user want to collect CUDA peak memory allocation statistics\n>>> dataset, model, optimizer = ...\n>>> # Set the target Torch CUDA device.\n>>> device = torch.device(\"cuda:0\")\n>>> model = model.to(device)\n\n>>> # Define a function that takes an element of the dataset as input and\n>>> # do a training step\n>>> def work(data):\n...     # example body of a training loop\n...     optimizer.zero_grad()\n...     output = model(data.to(device))\n...     loss = compute_loss(output)\n...     loss.backward()\n...     optimizer.step()\n...     # extract the feature for later to be modeled or analyzed\n...     return featurize(data)\n\n>>> # can optionally use a cleanup function to release the references\n>>> # hold during the work(). This cleanup function will be called\n>>> # at the end of each step before garbage collection and memory allocations measurement\n>>> def cleanup():\n...     model.zero_grad(set_to_none=True)\n\n>>> # Collect features (i.e., model outputs) and memory usage statistics for the workflow.\n>>> features, alloc_peaks = collect_cuda_peak_alloc(\n...     dataset=batches,\n...     work=work,\n...     device=device,\n...     cleanup=cleanup,\n... )\n\n>>> # use features and alloc_peaks as needed, e.g., fit a model\n>>> # that can use these statistics to predict memory usage\n>>> memory_model = ...\n>>> memory_model.fit(features, alloc_peaks)\n```\n\n----------------------------------------\n\nTITLE: Downloading ESM-2 Model Checkpoints from NGC Registry\nDESCRIPTION: Downloads the pre-trained ESM-2 model (650M parameter version) from the NVIDIA GPU Cloud (NGC) registry. The user has the option to use the larger 3B parameter model by changing the checkpoint variable.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.core.data.load import load\n\n\ncheckpoint = \"esm2/650m:2.0\"  # change to \"esm2/3b:2.0\" to use the ESM-2 3B model\ncheckpoint_path = load(checkpoint, source=\"ngc\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Environment Variables and Cleanup\nDESCRIPTION: Sets up environment variables for CI mode and performs optional cleanup of previous run artifacts.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# This variable should be used in the notebooks to run a subset of the model layers or a smaller model/dataset\nFAST_CI_MODE: bool = os.environ.get(\"FAST_CI_MODE\", False)\n# Clean up any prior runs\nCLEANUP: bool = False\nif CLEANUP:\n    !rm -rf preprocessed_data\n    !rm -rf preatraining_demo\n    !rm -rf pretraining_demo\n    !rm -rf training_data_config.yaml\n    !rm -rf preprocess_config.yaml\n    !rm -f chr20.fa.gz\n    !rm -f chr21.fa.gz\n    !rm -f chr22.fa.gz\n    !rm -f chr20_21_22.fa\n```\n\n----------------------------------------\n\nTITLE: Cleaning and Masking Tensor Data in Python\nDESCRIPTION: A function that processes tensor data by applying an optional mask and/or centering the data around the center of mass. Takes input data tensor, optional mask tensor, and a boolean flag for centering.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_137\n\nLANGUAGE: python\nCODE:\n```\ndef clean_mask_center(data: Tensor,\n                      mask: Optional[Tensor] = None,\n                      center: Bool = False) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Sampling from MixTimeDistribution in Python\nDESCRIPTION: Generates a specified number of samples from the mixed time distribution. It takes the number of samples, device (CPU or GPU), and an optional random number generator as parameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef sample(n_samples: int,\n           device: Union[str, torch.device] = \"cpu\",\n           rng_generator: Optional[torch.Generator] = None) -> Float\n```\n\n----------------------------------------\n\nTITLE: NeMo2 Zarr Checkpoint Structure\nDESCRIPTION: Directory structure showing the expected layout of a NeMo2 checkpoint in zarr format, including context files and sharded weights.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ninterleaved_hyena_7b_fix_shape\n├── context\n│   ├── io.json\n│   └── model.yaml\n└── weights\n    ├── common.pt\n    ├── metadata.json\n    └── <MODEL_LAYER_NAME>  # Example: module.decoder.layers.0.mixer.dense\n        └── shard_*_*.pt\n```\n\n----------------------------------------\n\nTITLE: Training MDLM Model\nDESCRIPTION: Implements the training loop for MDLM model using random binary data with specified dimensions and batch size.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# training\nB = 32  # batch size\nD = 10  # dimension\nS = 3  # state space\n\nmodel = Model(D, S)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\nmodel = model.to(DEVICE)\nlosses = []\nfor _ in tqdm(range(50000)):\n    num_ones = torch.randint(0, D + 1, (B,))\n    x1 = (torch.arange(D)[None, :] < num_ones[:, None]).long().to(DEVICE)\n    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n    optimizer.zero_grad()\n    # x0 = dfm.sample_prior(x1.shape) # B x D\n    t = mdlm.sample_time(B)\n    xt = mdlm.interpolate(x1, t)\n    logits = model(xt, t)  # (B, D, S)\n    loss = mdlm.loss(logits, x1, xt, t).mean()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n```\n\n----------------------------------------\n\nTITLE: Validating Embedding and AnnData Shapes in Python\nDESCRIPTION: This snippet prints the shapes of the UMAP embedding and the AnnData object, then asserts that the number of observations in both match. It assumes the existence of 'adata_test' and 'inference_results'.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprint(\"embedding.shape: \", embedding.shape)\nprint(\"adata_test.obs.shape[0]: \", adata_test.obs.shape[0])\nassert adata_test.obs.shape[0] == inference_results[\"embeddings\"].shape[0]\n```\n\n----------------------------------------\n\nTITLE: Multiple H5AD File Processing Setup\nDESCRIPTION: Sets up directories for processing multiple H5AD files into a single dataset.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/examples/example_notebook.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nhdf5s = BIONEMO_CACHE_DIR / \"hdf5s\"\n\noutput_dir = os.path.join(\"scdataset_output\")\n```\n\n----------------------------------------\n\nTITLE: Iterating Over BucketBatchSampler in Python\nDESCRIPTION: This method implements the iterator for BucketBatchSampler, yielding batches of indices of elements with sizes from each bucket range.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef __iter__() -> Iterator[List[int]]\n```\n\n----------------------------------------\n\nTITLE: Creating Strip Plot Visualization Function\nDESCRIPTION: Defines a function to create strip plots with jittered points and median indicators using Seaborn, styled with NVIDIA theme colors.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef plot_strip_with_means(df, x_col=\"evo2_delta_score\", class_col=\"class\"):\n    NVIDIA_GREEN = \"#76B900\"\n    BACKGROUND_COLOR = \"#F8F8F8\"\n    GRID_COLOR = \"#DDDDDD\"\n    FONT_COLOR = \"#333333\"\n    unique_classes = sorted(df[class_col].unique())\n    plt.figure(figsize=(9, 5), facecolor=BACKGROUND_COLOR)\n    plt.style.use(\"default\")\n    p = sns.stripplot(\n        data=df,\n        x=x_col,\n        y=class_col,\n        hue=class_col,\n        order=unique_classes,\n        palette=[NVIDIA_GREEN, \"red\"],\n        size=6,\n        jitter=0.3,\n        alpha=0.6,\n    )\n```\n\n----------------------------------------\n\nTITLE: DNA Sequence Splicing Command-Line Interface\nDESCRIPTION: Command-line help output showing available options for the splice_evo2 tool, which handles DNA sequence splicing and transcript manipulation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/data/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsplice_evo2 --help\nusage: splice_evo2 [-h] --fasta-path FASTA_PATH --gtf-path GTF_PATH [--output-path OUTPUT_PATH] [--transcript-type {default,stitched}] [--stitched-promoter STITCHED_PROMOTER] [--stitched-intron STITCHED_INTRON] [--stitched-overlap] [--only-longest-transcript] [-v]\n```\n\n----------------------------------------\n\nTITLE: Importing Python Libraries\nDESCRIPTION: Imports required Python libraries for data manipulation, visualization, and analysis including pandas, matplotlib, seaborn, and BioPython.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport glob\nimport gzip\nimport json\nimport math\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport torch\nfrom Bio import SeqIO\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\n```\n\n----------------------------------------\n\nTITLE: Executing Preprocessing Script via Command Line\nDESCRIPTION: Bash command to run the preprocessing script with the specified configuration file.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/data/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython preprocess.py -c mmseqs_promotors_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Initializing ESM-2 Tokenizer and Exploring Vocabulary\nDESCRIPTION: Initializes the ESM-2 tokenizer and prints the vocabulary size and all available tokens. This shows the tokens that the model uses to represent protein sequences.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom bionemo.esm2.data.tokenizer import BioNeMoESMTokenizer, get_tokenizer\n\n\ntokenizer = get_tokenizer()\n\ntokens = tokenizer.all_tokens\nprint(f\"There are {tokenizer.vocab_size} unique tokens: {tokens}.\")\n```\n\n----------------------------------------\n\nTITLE: Launching BioNeMo Docker Container\nDESCRIPTION: Command to run the BioNeMo Framework Docker container with GPU support and necessary system configurations.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it \\\n  --gpus=all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \\\n  nvcr.io/nvidia/clara/bionemo-framework:nightly \\\n  /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Using NBVAL_CHECK_OUTPUT for Output Validation\nDESCRIPTION: Example of using the NBVAL_CHECK_OUTPUT marker comment to ensure the output of a notebook matches when executed in CI, demonstrating NumPy array generation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/contributing/Writing Documentation/jupyter-notebooks.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# NBVAL_CHECK_OUTPUT\n\nimport numpy as np\n\n\nprint(np.arange(5))\n```\n\n----------------------------------------\n\nTITLE: Loading ESM-2 650M Model Checkpoint in Python\nDESCRIPTION: Code to load the pre-trained ESM-2 650M parameter model checkpoint using the load function from BioNeMo framework.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/ESM-2/pre-training.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nesm2_650m_ckpt_path = load(\"esm2/nv_650m:2.1\")\n```\n\n----------------------------------------\n\nTITLE: Discrete Noise Schedule Base Class\nDESCRIPTION: Abstract base class for discrete noise schedules with schedule generation and derivative calculation capabilities.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nclass DiscreteNoiseSchedule(ABC):\n    def __init__(nsteps: int, direction: TimeDirection)\n```\n\n----------------------------------------\n\nTITLE: Tensor Dimension Padding\nDESCRIPTION: Pads source tensor dimensions to match target tensor shape with validation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_134\n\nLANGUAGE: python\nCODE:\n```\ndef pad_like(source: Tensor, target: Tensor) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Running BioNeMo Training Script in Container\nDESCRIPTION: Docker command for executing a training script within the BioNeMo container with GPU support and mounted volumes for data persistence.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/initialization-guide.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it --gpus all \\\n  -e NGC_CLI_API_KEY \\\n  -e WANDB_API_KEY \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  {{ docker_url }}:{{ docker_tag }} \\\n  python $DOCKER_RESULTS_PATH/training.py --option1 --option2 --output=$DOCKER_RESULTS_PATH\n```\n\n----------------------------------------\n\nTITLE: Executing BioNeMo Commands for ESM-2 and Geneformer\nDESCRIPTION: Lists the available command-line executables for generating recipes and training ESM-2 and Geneformer models using BioNeMo.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/training-models.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbionemo-geneformer-recipe\nbionemo-esm2-recipe\nbionemo-geneformer-train\nbionemo-esm2-train\n```\n\n----------------------------------------\n\nTITLE: Accessing Log Variance in DDPM\nDESCRIPTION: Property method that returns the log variance used in the diffusion process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_116\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef log_var() -> torch.Tensor\n```\n\n----------------------------------------\n\nTITLE: Initializing Evo2 Prediction Commands\nDESCRIPTION: Sets up prediction commands for reference and variant sequences using Evo2 model with specified parameters including model size, parallel processing options, and FP8 settings.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npredict_ref_command = (\n    f\"predict_evo2 --fasta {ref_fasta_path} --ckpt-dir {checkpoint_path} \"\n    f\"--output-dir {predict_ref_dir} --model-size {MODEL_SIZE} --tensor-parallel-size 1  {model_subset_option} \"\n    f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs {fp8_option}\"\n)\n\npredict_var_command = (\n    f\"predict_evo2 --fasta {var_fasta_path} --ckpt-dir {checkpoint_path} \"\n    f\"--output-dir {predict_var_dir} --model-size {MODEL_SIZE} --tensor-parallel-size 1 {model_subset_option} \"\n    f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs {fp8_option}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Single Batch Processing\nDESCRIPTION: Processes data with batch size 1 using DataLoader and custom collate function.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/examples/example_notebook.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = lambda x: x  # noqa: E731\n\ndataloader = DataLoader(data, batch_size=1, shuffle=True, collate_fn=collate_sparse_matrix_batch)\nn_epochs = 1\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n```\n\n----------------------------------------\n\nTITLE: Signing Git Commits for DCO Compliance in Bash\nDESCRIPTION: This snippet demonstrates how to sign off on Git commits to comply with the Developer Certificate of Origin (DCO) requirement. It shows the command to sign a single commit and how to set up automatic signing for all commits.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/contributing/contributing.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -s -m \"Add cool feature.\"\n```\n\nLANGUAGE: bash\nCODE:\n```\necho \"Signed-off-by: Your Name <your@email.com>\" > ~/.git-commit-template.txt\ngit config --local commit.template ~/.git-commit-template.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit commit --amend --no-edit --signoff\n```\n\n----------------------------------------\n\nTITLE: Downloading BioNeMo Test Datasets\nDESCRIPTION: Command to download test datasets for model fine-tuning or evaluation. The script stores the downloaded data directory path in the TEST_DATA_DIR environment variable.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/development.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nTEST_DATA_DIR=$(download_bionemo_data <model_name>/testdata:<version> --source $DATA_SOURCE);\n```\n\n----------------------------------------\n\nTITLE: Discretizing Time Schedule in Python\nDESCRIPTION: This function discretizes the time schedule into a list of time deltas. It takes optional parameters for the number of steps and device, and returns a tensor of time deltas.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef discretize(nsteps: Optional[int] = None,\n               device: Optional[Union[str, torch.device]] = None) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for BioNeMo Framework\nDESCRIPTION: Script to execute unit tests for all sub-packages of the BioNeMo framework. This is a more comprehensive test suite that ensures functionality across the codebase.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/scripts/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./ci/scripts/run_pytest.sh\n```\n\n----------------------------------------\n\nTITLE: Loading Environment Variables in Shell\nDESCRIPTION: Command to source environment variables from .env file into current shell session.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/initialization-guide.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsource .env\n```\n\n----------------------------------------\n\nTITLE: Downloading BioNeMo Model Checkpoints\nDESCRIPTION: Commands to set the data source and download model checkpoints. The script stores the downloaded checkpoint path in the MODEL_CKPT environment variable for later use in fine-tuning workflows.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/development.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport DATA_SOURCE=\"ngc\"\nMODEL_CKPT=$(download_bionemo_data <model_name>/<checkpoint_name>:<version> --source $DATA_SOURCE);\n```\n\n----------------------------------------\n\nTITLE: Accessing Reverse Data Schedule in DDPM\nDESCRIPTION: Property method that returns the reverse data schedule used in the diffusion process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_114\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef reverse_data_schedule() -> torch.Tensor\n```\n\n----------------------------------------\n\nTITLE: Feature Extraction Loop\nDESCRIPTION: Demonstrates how to access features for each data point in the dataset.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/examples/example_notebook.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor index in range(len(data)):\n    model(data.get_row(index, return_features=True))\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for ESM-2 Inference\nDESCRIPTION: Sets up the Python environment by importing necessary libraries for ESM-2 model inference, including pandas and PyTorch\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/inference.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport shutil\nimport warnings\n\nimport pandas as pd\nimport torch\n\n\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Creating Directories for BioNeMo Workflow in Bash\nDESCRIPTION: Creates necessary directories for storing downloaded data, processed data, and inference output using bash commands.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n!mkdir -p {train_tutorial_data_dir}\n!mkdir -p {val_tutorial_data_dir}\n!mkdir -p {test_tutorial_data_dir}\n!mkdir -p {train_tutorial_processed_dir}\n!mkdir -p {val_tutorial_processed_dir}\n!mkdir -p {test_tutorial_processed_dir}\n!mkdir -p {tutorial_output_dir}\n```\n\n----------------------------------------\n\nTITLE: Setting Data Source Environment Variable for BioNeMo\nDESCRIPTION: Sets the data source environment variable for downloading BioNeMo data. It distinguishes between public (ngc) and NVIDIA internal (pbss) sources.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/training-models.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MY_DATA_SOURCE=\"ngc\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport MY_DATA_SOURCE=\"pbss\"\n```\n\n----------------------------------------\n\nTITLE: Applying Kabsch Augmentation to Data and Noise Tensors in Python\nDESCRIPTION: Method to apply Kabsch augmentation to data and noise tensors. It computes the optimal transport plan between source and target minibatches after Kabsch alignment and returns noise and data samples following the plan.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_75\n\nLANGUAGE: python\nCODE:\n```\ndef apply_augmentation(x0: Tensor,\n                       x1: Tensor,\n                       mask: Optional[Tensor] = None,\n                       align_noise_to_data=True) -> Tuple[Tensor, Tensor]\n```\n\n----------------------------------------\n\nTITLE: Cosine SNR Transform Implementation\nDESCRIPTION: Implementation of cosine SNR schedule with configurable nu and s hyperparameters.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nclass CosineSNRTransform(ContinuousSNRTransform):\n    def __init__(nu: Float = 1.0, s: Float = 0.008)\n```\n\n----------------------------------------\n\nTITLE: Enum String Conversion Utility\nDESCRIPTION: Converts string values to enum instances with validation and error handling.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_133\n\nLANGUAGE: python\nCODE:\n```\ndef string_to_enum(value: Union[str, AnyEnum],\n                   enum_type: Type[AnyEnum]) -> AnyEnum\n```\n\n----------------------------------------\n\nTITLE: Downloading BioNeMo Resources using CLI\nDESCRIPTION: Command to list available resources (model checkpoints and datasets) that can be downloaded from NGC or PBSS sources using the download_bionemo_data script.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/development.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndownload_bionemo_data --list-resources\n```\n\n----------------------------------------\n\nTITLE: Accessing Previous Alpha Bar in DDPM\nDESCRIPTION: Property method that returns the previous alpha bar values used in the diffusion process.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_118\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef alpha_bar_prev() -> torch.Tensor\n```\n\n----------------------------------------\n\nTITLE: Accessing Features in SCDL Dataset for Model Training\nDESCRIPTION: Shows how to access features from the SCDL dataset when training a model. This can be useful when specific features need to be used in the model.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor index in range(len(data)):\n    model(data.get_row(index,return_features = True))\n```\n\n----------------------------------------\n\nTITLE: Importing Essential Libraries for Protein Design\nDESCRIPTION: Imports required Python libraries including matplotlib for visualization, numpy for numerical operations, pandas for data manipulation, and torch for deep learning operations. The output is suppressed using the %%capture magic command.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture --no-display --no-stderr cell_output\n\nimport os\nimport shutil\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\n\n\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Listing Processed Data Directory Contents in Bash\nDESCRIPTION: Lists the contents of the processed training data directory to verify successful conversion.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n!ls -laht {train_tutorial_processed_dir}\n```\n\n----------------------------------------\n\nTITLE: Running BioNeMo Framework Container with Shell Access\nDESCRIPTION: Docker command to run the BioNeMo Framework container with GPU access and interactive shell. Uses the --gpus flag to enable all available GPUs.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/access-startup.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it --gpus all \\\n  {{ docker_url }}:{{ docker_tag }} \\\n  /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Converting Savanna Evo2 40B Base (8K context) to NeMo2 Format\nDESCRIPTION: This command converts the Savanna Evo2 40B base model with 8K context length from Hugging Face to NeMo2 format. It specifies the model size as '40b' and outputs the converted model to the 'nemo2_evo2_40b_8k' directory.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nevo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_40b_base \\\n  --model-size 40b --output-dir nemo2_evo2_40b_8k\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Checks for BioNeMo Framework\nDESCRIPTION: Command to run pre-commit checks on all files in the BioNeMo Framework repository. This helps identify and fix potential issues before committing changes.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/contributing/contributing.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning MNIST Model with BioNemo\nDESCRIPTION: Command to run the fine-tuning script for MNIST model using a pre-trained checkpoint. The script requires specifying the pretrain checkpoint directory path as an argument.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-example_model/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython src/bionemo/example_model/training_scripts/finetune_mnist.py ---pretrain_ckpt_dirpath <pretrain_directory>\n```\n\n----------------------------------------\n\nTITLE: Safe Tensor Indexing\nDESCRIPTION: Safely indexes a tensor while handling device placement and migration costs.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_132\n\nLANGUAGE: python\nCODE:\n```\ndef safe_index(tensor: Tensor, index: Tensor, device: Optional[torch.device])\n```\n\n----------------------------------------\n\nTITLE: Implementing Abstract Time Distribution Class in Python\nDESCRIPTION: Defines an abstract base class TimeDistribution for representing time distributions. It includes initialization parameters and an abstract method for sampling.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass TimeDistribution(ABC):\n    def __init__(discrete_time: Bool = False,\n                 nsteps: Optional[int] = None,\n                 min_t: Optional[Float] = None,\n                 max_t: Optional[Float] = None,\n                 rng_generator: Optional[torch.Generator] = None):\n        # Implementation details\n\n    @abstractmethod\n    def sample(n_samples: Union[int, Tuple[int, ...], torch.Size],\n               device: Union[str, torch.device] = \"cpu\",\n               rng_generator: Optional[torch.Generator] = None) -> Float:\n        # Abstract method\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with Code Coverage for BioNeMo Framework\nDESCRIPTION: Command to execute unit tests using pytest, with verbose output and code coverage reporting for the 'bionemo' package. The coverage report is displayed in the terminal.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-fw/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -v --cov=bionemo --cov-report=term .\n```\n\n----------------------------------------\n\nTITLE: Signing off on Git Commits\nDESCRIPTION: Commands for adding DCO sign-off to Git commits, including adding sign-off to a single commit, setting up automatic sign-off via a commit template, and amending a commit with sign-off.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/CONTRIBUTING.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -s -m \"Add cool feature.\"\n```\n\n----------------------------------------\n\nTITLE: Cloning BioNeMo Repository with Submodules\nDESCRIPTION: Command to clone the BioNeMo Framework repository with recursive submodule initialization.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --recursive git@github.com:NVIDIA/bionemo-framework.git\ncd bionemo-framework\n```\n\n----------------------------------------\n\nTITLE: Initializing EquivariantOTSampler for Rotation-Invariant Molecular Structures in Python\nDESCRIPTION: Constructor for the EquivariantOTSampler class, designed for OT sampling with cost calculated after Kabsch alignment. This sampler is specifically built for handling molecular structures where rotational invariance is important.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_68\n\nLANGUAGE: python\nCODE:\n```\nclass EquivariantOTSampler():\n    def __init__(method: str = \"exact\",\n             device: Union[str, torch.device] = \"cpu\",\n             num_threads: int = 1) -> None\n```\n\n----------------------------------------\n\nTITLE: Initializing KabschAugmentation Class in Python\nDESCRIPTION: Default initialization method for KabschAugmentation class that requires no arguments. This method allows for instance variables to be added as needed during implementation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_72\n\nLANGUAGE: python\nCODE:\n```\ndef __init__()\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for ESM-2\nDESCRIPTION: This command executes the pytest framework to run all unit tests for the ESM-2 package, with verbose output for detailed test results.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-esm2/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -v .\n```\n\n----------------------------------------\n\nTITLE: Running Jupyter Lab in Docker Container\nDESCRIPTION: Command to launch a Docker container with Jupyter Lab for BioNeMo Framework. Includes GPU support, port mapping, environment variables, and volume mounting configurations.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/initialization-guide.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -d --gpus all \\\n  -p $JUPYTER_PORT:$JUPYTER_PORT \\\n  -e NGC_CLI_API_KEY \\\n  -e WANDB_API_KEY \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  {{ docker_url }}:{{ docker_tag }} \\\n  jupyter lab \\\n  \t--allow-root \\\n\t--ip=* \\\n\t--port=$JUPYTER_PORT \\\n\t--no-browser \\\n  \t--NotebookApp.token='' \\\n  \t--NotebookApp.allow_origin='*' \\\n  \t--ContentsManager.allow_hidden=True \\\n  \t--notebook-dir=$DOCKER_RESULTS_PATH\n```\n\n----------------------------------------\n\nTITLE: Displaying AnnData Observation Columns in Python\nDESCRIPTION: This snippet prints the column names of the observations in the AnnData object. It assumes the existence of 'adata_test'.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nadata_test.obs.columns\n```\n\n----------------------------------------\n\nTITLE: DDPM Prediction Type Enumeration\nDESCRIPTION: Defines prediction types (data, noise, velocity) for DDPM model usage.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_135\n\nLANGUAGE: python\nCODE:\n```\nclass PredictionType(Enum)\n```\n\n----------------------------------------\n\nTITLE: Importing BioNeMo Dependencies\nDESCRIPTION: Imports required libraries and modules for single cell data processing including torch DataLoader and BioNeMo specific imports.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/examples/example_notebook.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tempfile\n\nimport pooch\nfrom torch.utils.data import DataLoader\n\nfrom bionemo.core import BIONEMO_CACHE_DIR\nfrom bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\nfrom bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Requirements and Running License Check\nDESCRIPTION: Commands to install development requirements and run the license check script to ensure proper license headers are present in Python files. This is required for new Python files added to the codebase.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/CONTRIBUTING.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -r dev-requirements.txt --user\npython ./scripts/license_check.py --modify --replace --license-header ./license_header -c sub-packages/ -c docs/ -c scripts/ -c ci/ -c internal/\n```\n\n----------------------------------------\n\nTITLE: Docker Volume Mounting Example\nDESCRIPTION: Example showing how to mount a local directory as a volume in a Docker container for BioNeMo Framework.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/initialization-guide.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v /path/to/local/cache:/workspace/bionemo2/cache \\\n    {{ docker_url }}:{{ docker_tag }}\n```\n\n----------------------------------------\n\nTITLE: Implementing Topk Lowest Masking Function for MDLM in Python\nDESCRIPTION: Function declaration for topk_lowest_masking, which likely implements a strategy to mask tokens based on their lowest k scores or probabilities, controlling the masking process during diffusion.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ndef topk_lowest_masking\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Distributed Environment in Python\nDESCRIPTION: A utility function that cleans up the distributed computing environment by destroying the process group and clearing the CUDA cache.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_139\n\nLANGUAGE: python\nCODE:\n```\ndef clean_up_distributed() -> None\n```\n\n----------------------------------------\n\nTITLE: ZeRO-1 Checkpoint Structure\nDESCRIPTION: Directory structure showing the expected layout of a ZeRO-1 checkpoint with model parallel states.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\narc_7b_tp8_pretrained_ckpt/global_step199400\n└── mp_rank_*_model_states.pt\n```\n\n----------------------------------------\n\nTITLE: Running BioNeMo Geometric Unit Tests\nDESCRIPTION: Executes the unit test suite using pytest with verbose output (-v flag). Tests will be run for all test files in the current directory.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-geometric/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -v .\n```\n\n----------------------------------------\n\nTITLE: Training AMPLIFY 120M Model Configuration\nDESCRIPTION: Command-line configuration for training the 120M parameter version of AMPLIFY, specifying architecture parameters and training settings.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/amplify.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython /workspace/bionemo-framework/sub-packages/bionemo-amplify/src/bionemo/amplify/train_amplify.py \\\n    ...\n    --num-nodes=2 \\\n    --devices=8 \\\n    --min-seq-length 512 \\\n    --max-seq-length 512 \\\n    --num-layers 24 \\\n    --num-attention-heads 10 \\\n    --hidden-size 640 \\\n    --ffn-hidden-size 2560 \\\n    --micro-batch-size 256\n```\n\n----------------------------------------\n\nTITLE: Loading Prediction Files and Sequence Maps\nDESCRIPTION: Loads prediction files and sequence ID maps for both reference and variant sequences using glob patterns and JSON loading.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/zeroshot_brca1.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nref_pred_files = glob.glob(os.path.join(predict_ref_dir, \"predictions__rank_*.pt\"))\nvar_pred_files = glob.glob(os.path.join(predict_var_dir, \"predictions__rank_*.pt\"))\n\nwith open(os.path.join(predict_ref_dir, \"seq_idx_map.json\"), \"r\") as f:\n    ref_seq_idx_map = json.load(f)\nwith open(os.path.join(predict_var_dir, \"seq_idx_map.json\"), \"r\") as f:\n    var_seq_idx_map = json.load(f)\n\nref_preds = torch.load(ref_pred_files[0])\nvar_preds = torch.load(var_pred_files[0])\n```\n\n----------------------------------------\n\nTITLE: External Developer Workflow for Forking and Cloning\nDESCRIPTION: Git commands for external developers to fork, clone, and push changes to their personal fork of the BioNeMo repository.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/CONTRIBUTING.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/YOUR_USERNAME/YOUR_FORK.git bionemo-framework\n# Checkout the targeted branch and commit changes\n# Push the commits to a branch on the fork (remote).\ngit push -u origin <local-branch>:<remote-branch>\n```\n\n----------------------------------------\n\nTITLE: Setting Pre-trained Checkpoint Path for Inference in Python\nDESCRIPTION: Defines the path to the pre-trained Geneformer model checkpoint for use in inference.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npretrained_checkpoint_path = \"/workspace/bionemo2/results/geneformer-10m/dev/checkpoints/epoch=0-val_loss=8.28-step=499-consumed_samples=4000.0-last\"\n```\n\n----------------------------------------\n\nTITLE: Installing the Project in Development Mode with uv\nDESCRIPTION: Command to install the project in editable mode using uv (Python package installer). This allows changes to the code to be immediately reflected without reinstallation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/infra-bionemo/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install --editable .\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Bionemo-evo2\nDESCRIPTION: Executes the pytest framework to run all unit tests in the package with verbose output.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -v .\n```\n\n----------------------------------------\n\nTITLE: Pulling BioNeMo Framework Container from NGC\nDESCRIPTION: Command to download the BioNeMo Framework Docker container from NGC registry using the specified URL and tag.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/access-startup.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull {{ docker_url }}:{{ docker_tag }}\n```\n\n----------------------------------------\n\nTITLE: Installing BioNeMo Amplify Project\nDESCRIPTION: Command to install the BioNeMo Amplify project in editable mode using pip. This allows for development and testing of the project.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-amplify/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Specifying Pinned Dependency Versions for NVIDIA BioNeMo Framework\nDESCRIPTION: This snippet lists the exact versions of Python packages required for the NVIDIA BioNeMo framework. It includes PyTorch-related packages for geometric deep learning and RDKit for cheminformatics. The versions are pinned to ensure consistency across different environments.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-geometric/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntorch-cluster==1.6.3\ntorch-geometric==2.5.0\ntorch-scatter==2.1.2\ntorch_sparse==0.6.18\nrdkit==2023.9.6\n```\n\n----------------------------------------\n\nTITLE: Cloning BioNeMo Framework Repository with Submodules\nDESCRIPTION: Command to clone the BioNeMo repository with all its submodules recursively. This ensures all third-party dependencies like NeMo and Megatron-LM are properly initialized.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/index.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --recursive git@github.com:NVIDIA/bionemo-framework.git\n```\n\n----------------------------------------\n\nTITLE: Setting Cleanup Flag for Work Directory\nDESCRIPTION: Defines a boolean flag to control whether the work directory should be cleaned up at the end of the notebook execution.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/mutant-design.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncleanup: bool = True\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Hooks for BioNeMo Framework\nDESCRIPTION: Command to install pre-commit hooks for the BioNeMo Framework repository. This sets up essential static checks that are enforced on new PRs.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/contributing/contributing.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Device Management for OTSampler in Python\nDESCRIPTION: Method to move all internal tensors of the OTSampler class to a specified device. It updates the device attribute and transfers all tensor data to the target device, enabling efficient computation on either CPU or GPU.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_64\n\nLANGUAGE: python\nCODE:\n```\ndef to_device(device: str)\n```\n\n----------------------------------------\n\nTITLE: Printing Embedding Shape in Python\nDESCRIPTION: This snippet prints the shape of the embeddings from the inference results. It assumes the existence of an 'inference_results' dictionary with an 'embeddings' key.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(inference_results[\"embeddings\"].shape)\n```\n\n----------------------------------------\n\nTITLE: Embedding Matplotlib Visualizations in Notebooks\nDESCRIPTION: Example of how to create and embed a matplotlib visualization (sine wave plot) in a Jupyter notebook using the inline magic command.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/contributing/Writing Documentation/jupyter-notebooks.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nxs = np.linspace(0, 2 * np.pi, 100)\nplt.plot(xs, np.sin(xs))\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for BioNeMo Amplify\nDESCRIPTION: Command to execute unit tests for the BioNeMo Amplify project using pytest. The -v flag enables verbose output for detailed test results.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-amplify/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -v .\n```\n\n----------------------------------------\n\nTITLE: Running Type Checking with mypy in BioNeMo Framework\nDESCRIPTION: Command to run mypy for type checking the codebase. It installs missing type stubs, checks untyped function definitions, and ignores missing imports.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/infra-bionemo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmypy --install-types --non-interactive --ignore-missing --check-untyped-defs .\n```\n\n----------------------------------------\n\nTITLE: Setting NVIDIA TF32 Configuration\nDESCRIPTION: Resets NVIDIA TF32 configuration to default for Blackwell testing.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"  # reset to default for blackwell testing\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables in Docker Container\nDESCRIPTION: Examples demonstrating how to set environment variables in a Docker container using the -e option.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/initialization-guide.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -e MY_VAR=value -e ANOTHER_VAR=another_value \\\n    {{ docker_url }}:{{ docker_tag }}\n```\n\nLANGUAGE: bash\nCODE:\n```\nMY_EXTERNAL_VAR=external_value\ndocker run -e MY_INTERNAL_VAR=$MY_EXTERNAL_VAR \\\n    {{ docker_url }}:{{ docker_tag }}\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation with pydoc-markdown\nDESCRIPTION: Command that uses pydoc-markdown to generate initial documentation from the src/bionemo directory, including a table of contents, and outputs to documentation.md\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/scripts/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npydoc-markdown -I src/bionemo --render-toc > documentation.md\n```\n\n----------------------------------------\n\nTITLE: Installing MoCo Framework in Development Mode\nDESCRIPTION: Command to install the MoCo framework in editable/development mode using pip. This installation method allows for immediate reflection of code changes without requiring reinstallation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Listing Inference Output and Setting Result Path in Bash\nDESCRIPTION: Lists the contents of the inference output directory and sets the path for the inference result file.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n!ls -altrh {tutorial_output_dir}/\ntutorial_output_inference_pickle = f\"{tutorial_output_dir}/predictions__rank_0.pt\"\n!ls -altrh {tutorial_output_inference_pickle}\n```\n\n----------------------------------------\n\nTITLE: BioNeMo Framework Token Vocabulary Definition in Plaintext\nDESCRIPTION: A vocabulary of tokens used in the BioNeMo framework, consisting of special tokens (<cls>, <pad>, etc.) followed by amino acid single-letter codes (L, A, G, etc.) and other special characters. This vocabulary likely serves as the token set for protein sequence tokenization and processing.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-esm2/src/bionemo/esm2/data/tokenizer/vocab.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<cls>\n<pad>\n<eos>\n<unk>\nL\nA\nG\nV\nS\nE\nR\nT\nI\nD\nP\nK\nQ\nN\nF\nY\nM\nH\nW\nC\nX\nB\nU\nZ\nO\n.\n-\n<null_1>\n<mask>\n```\n\n----------------------------------------\n\nTITLE: Wandb Authentication Configuration\nDESCRIPTION: INI configuration for Weights & Biases authentication using .netrc file format.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: ini\nCODE:\n```\nmachine api.wandb.ai\n  login user\n  password PASSWORD_HERE\n```\n\n----------------------------------------\n\nTITLE: Running Interactive Development Container for BioNeMo Framework\nDESCRIPTION: Script to start a container from the development image and open an interactive bash shell. This provides a development environment with all necessary dependencies.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/scripts/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./internal/scripts/run_dev.sh\n```\n\n----------------------------------------\n\nTITLE: Installing BioNeMo-MoCo Package in Development Mode\nDESCRIPTION: Command to install the bionemo-moco package in development mode, allowing code changes to take effect without reinstallation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/environment/Instructions.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Setting User and Group IDs in Docker Container\nDESCRIPTION: Example showing how to set user and group IDs in a Docker container using the -u option with dynamic ID retrieval.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/initialization-guide.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -u $(id -u):$(id -g) \\\n    {{ docker_url }}:{{ docker_tag }}\n```\n\n----------------------------------------\n\nTITLE: Downloading and Concatenating Chromosome Data\nDESCRIPTION: Downloads chromosome data (20, 21, 22) from UCSC, unzips them, and concatenates into a single FASTA file for preprocessing.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/examples/fine-tuning-tutorial.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nimport os\n\nconcat_path = \"chr20_21_22.fa\"\nif not os.path.exists(concat_path):\n    !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr20.fa.gz\n    !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr21.fa.gz\n    !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr22.fa.gz\n    !zcat chr20.fa.gz > chr20.fa\n    !zcat chr21.fa.gz > chr21.fa\n    !zcat chr22.fa.gz > chr22.fa\n    !cat chr20.fa chr21.fa chr22.fa > chr20_21_22.fa\n```\n\n----------------------------------------\n\nTITLE: Running Static Code Checks for BioNeMo Framework\nDESCRIPTION: Script to validate code changes against formatting and license requirements. This is recommended to run before submitting changes to ensure they pass CI checks.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/scripts/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./ci/scripts/static_checks.sh\n```\n\n----------------------------------------\n\nTITLE: Defining Augmentation Types as Enum in Python\nDESCRIPTION: Enumeration class that defines different types of Optimal Transport methods for Continuous Flow Matching, including EXACT_OT, EQUIVARIANT_OT, and KABSCH alignment.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_76\n\nLANGUAGE: python\nCODE:\n```\nclass AugmentationType(Enum)\n```\n\n----------------------------------------\n\nTITLE: Building Release Docker Image for BioNeMo Framework\nDESCRIPTION: Script to build a release Docker image using BuildKit. The image is tagged with the project name and the current git commit hash for version tracking.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/scripts/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nDOCKER_BUILDKIT=1 ./ci/scripts/build_docker_image.sh \\\n  -regular-docker-builder \\\n  -image-name \"nvcr.io/nvidian/cvai_bnmo_trng/bionemo:bionemo2-$(git rev-parse HEAD)\"\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute all unit tests in verbose mode using pytest.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-geneformer/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -v .\n```\n\n----------------------------------------\n\nTITLE: Setting Up BioNeMo Environment with Conda\nDESCRIPTION: Command to run the setup script that creates the conda environment, installs bionemo-moco package, and runs tests.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/environment/Instructions.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbash environment/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Installing BioNeMo Framework in Development Mode\nDESCRIPTION: Command to install the BioNeMo Framework package in editable mode, allowing developers to modify the code and see changes immediately without reinstalling.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-fw/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Initial Setup for BioNeMo Framework\nDESCRIPTION: Commands to perform the initial setup of the BioNeMo framework. The setup command must be run once before any other commands can be executed, followed by the specific command you want to run.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/README_justfile.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njust setup\njust <command you want to run>\n```\n\n----------------------------------------\n\nTITLE: Installing BioNeMo Noodles via pip\nDESCRIPTION: Command to install the bionemo-noodles package from PyPI using pip\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-noodles/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install bionemo-noodles\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment File for BioNeMo Framework\nDESCRIPTION: Script to initialize the environment configuration for first-time setup of the BioNeMo framework. This will return an exit code of 1 on the first run, which is expected behavior.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/scripts/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./internal/scripts/setup_env_file.sh\n```\n\n----------------------------------------\n\nTITLE: Converting Cell x Gene Data to SCDL Format using Bash Script\nDESCRIPTION: Demonstrates how to use the convert_h5ad_to_scdl script to convert existing AnnData files to the SCDL format. This is useful for batch conversion of multiple files.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nconvert_h5ad_to_scdl --data-path hdf5s --save-path example_dataset\n```\n\n----------------------------------------\n\nTITLE: Updating Secrets Baseline for BioNeMo Framework\nDESCRIPTION: Commands to update the secrets baseline files using detect-secrets. This helps manage false-positives in secret detection for the project.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/contributing/contributing.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndetect-secrets scan --baseline .secrets.baseline --exclude-files '(.*\\.ipynb|.*\\.baseline)$'\ndetect-secrets scan --baseline .secrets-nb.baseline --exclude-files '^.(?!.*\\.ipynb)' --exclude-lines '\"(hash|id|image/\\w+)\":.*'\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements with Version Constraints\nDESCRIPTION: List of required Python packages with minimum version constraints, including ONNX for neural networks, setuptools for Python packaging, web frameworks like aiohttp and Werkzeug, and ML-related packages like wandb. Some versions are specifically set to address security vulnerabilities.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/requirements-cve.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nonnx>=1.16.0\nsetuptools>=70.0.0\naiohttp>=3.9.4\njupyterlab>=3.6.8\njupyter_server>=2.14.1  # https://github.com/advisories/GHSA-hrw6-wg82-cm62\nWerkzeug>=3.0.3\nnltk>=3.9.1\npillow>=10.3.0\ntornado>=6.4.2\nwandb>=0.19.1 # Addresses CVE GHSA-v778-237x-gjrc\n```\n\n----------------------------------------\n\nTITLE: Cloning BioNeMo-MoCo Subpackage\nDESCRIPTION: Command to clone only the bionemo subpackage for local development.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/environment/Instructions.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash environment/clone_bionemo_moco.sh\n```\n\n----------------------------------------\n\nTITLE: Building and Running Docker Container for BioNeMo2 Documentation Preview\nDESCRIPTION: This snippet demonstrates how to build a Docker image for BioNeMo2 documentation and run a container to preview the docs locally. It mounts the necessary directories and exposes port 8000 for web access.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Build the Docker image\ndocker build -t nvcr.io/nvidian/cvai_bnmo_trng/bionemo2-docs -f docs/Dockerfile .\n\n# Run the Docker container\ndocker run --rm -it -p 8000:8000 \\\n    -v ${PWD}/docs:/docs -v ${PWD}/sub-packages:/sub-packages \\\n    nvcr.io/nvidian/cvai_bnmo_trng/bionemo2-docs:latest\n```\n\n----------------------------------------\n\nTITLE: Updating Git Submodules\nDESCRIPTION: Commands to initialize and update git submodules in an existing repository.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Installing the Framework\nDESCRIPTION: Commands to install the package and run unit tests\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-size-aware-batching/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest -v .\n```\n\n----------------------------------------\n\nTITLE: Running Evo2 Preprocessing Script\nDESCRIPTION: Command-line usage for running the Evo2 DNA sequence preprocessing script using the preprocess_evo2 command or directly executing the Python script.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/data/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npreprocess_evo2 -c <CONFIG_PATH>\n```\n\nLANGUAGE: python\nCODE:\n```\npython sub-packages/bionemo-evo2/src/bionemo/evo2/data/preprocess.py -c <CONFIG_PATH>\n```\n\n----------------------------------------\n\nTITLE: Updating BioNeMo Framework Docker Container with Docker Pull\nDESCRIPTION: Command used to update BioNeMo Framework to the latest version by pulling the updated Docker image from NGC Catalog.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/appendix/FAQ.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull\n```\n\n----------------------------------------\n\nTITLE: Setting Up Automatic DCO Sign-off\nDESCRIPTION: Commands to set up a Git commit template that automatically adds your DCO sign-off to all commits, by creating a template file and configuring Git to use it.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/CONTRIBUTING.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\necho \"Signed-off-by: Your Name <your@email.com>\" > ~/.git-commit-template.txt\ngit config --local commit.template ~/.git-commit-template.txt\n```\n\n----------------------------------------\n\nTITLE: Installing BioNeMo Geometric Package\nDESCRIPTION: Installs the BioNeMo geometric package in editable mode using pip. The -e flag allows for development mode installation where changes to the source code are immediately reflected without reinstallation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-geometric/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Building BioNeMo Docker Image\nDESCRIPTION: Command to build the BioNeMo Docker container locally using buildx.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker buildx build . -t my-container-tag\n```\n\n----------------------------------------\n\nTITLE: Local Development Installation\nDESCRIPTION: Command to install the package locally in editable mode for development purposes\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-noodles/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Cloning and Pushing to a Forked Repository in Bash\nDESCRIPTION: This snippet shows the process of cloning a forked repository, making changes, and pushing those changes to a branch on the fork. It's part of the workflow for external contributors to the BioNeMo project.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/contributing/contributing.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/YOUR_USERNAME/YOUR_FORK.git bionemo-framework\n# Checkout the targeted branch and commit changes\n# Push the commits to a branch on the fork (remote).\ngit push -u origin <local-branch>:<remote-branch>\n```\n\n----------------------------------------\n\nTITLE: Citation Information for Evo 2 Main Paper\nDESCRIPTION: BibTeX citation for the primary research paper describing the Evo 2 model published in Science journal.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/evo2.md#2025-04-23_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{nguyen2024sequence,\n   author = {Eric Nguyen and Michael Poli and Matthew G. Durrant and Brian Kang and Dhruva Katrekar and David B. Li and Liam J. Bartie and Armin W. Thomas and Samuel H. King and Garyk Brixi and Jeremy Sullivan and Madelena Y. Ng and Ashley Lewis and Aaron Lou and Stefano Ermon and Stephen A. Baccus and Tina Hernandez-Boussard and Christopher Ré and Patrick D. Hsu and Brian L. Hie },\n   title = {Sequence modeling and design from molecular to genome scale with Evo},\n   journal = {Science},\n   volume = {386},\n   number = {6723},\n   pages = {eado9336},\n   year = {2024},\n   doi = {10.1126/science.ado9336},\n   URL = {https://www.science.org/doi/abs/10.1126/science.ado9336},\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Preprocessed Data Files\nDESCRIPTION: Bash commands to list and examine the preprocessed data files generated for Evo2 training, showing the expected structure of Megatron IndexedDataset binary files.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/data/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ ls -lah\n-rwxr-xr-x  1 bionemo bionemo 1.2M Dec  4 00:56 data_promoters_test_text_CharLevelTokenizer_document.bin\n-rwxr-xr-x  1 bionemo bionemo  20K Dec  4 00:56 data_promoters_test_text_CharLevelTokenizer_document.idx\n-rwxr-xr-x  1 bionemo bionemo 392M Dec  4 00:56 data_promoters_train_text_CharLevelTokenizer_document.bin\n-rwxr-xr-x  1 bionemo bionemo 6.6M Dec  4 00:56 data_promoters_train_text_CharLevelTokenizer_document.idx\n-rwxr-xr-x  1 bionemo bionemo 1.2M Dec  4 00:56 data_promoters_valid_text_CharLevelTokenizer_document.bin\n-rwxr-xr-x  1 bionemo bionemo  20K Dec  4 00:56 data_promoters_valid_text_CharLevelTokenizer_document.idx\n```\n\n----------------------------------------\n\nTITLE: ODE Integration Step in Python\nDESCRIPTION: Performs one step integration of ODE with temperature control and optional masking.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_91\n\nLANGUAGE: python\nCODE:\n```\ndef step_ode(model_out: Tensor,\n             t: Tensor,\n             xt: Tensor,\n             dt: Tensor,\n             mask: Optional[Tensor] = None,\n             center: Bool = False,\n             temperature: Float = 1.0) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Evo2 Taxonomy Lineage Schema\nDESCRIPTION: Pydantic model class that defines the taxonomic lineage structure for DNA sequences. This schema is used to generate lineage strings that prefix sequences during preprocessing.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/data/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Evo2TaxonomyLineage(BaseModel):\n    \"\"\"Pydantic model class that defines the source lineage of a DNA sequence.\"\"\"\n    kingdom: None | str = None\n    phylum: None | str = None\n    clazz: None | str = None\n    order: None | str = None\n    family: None | str = None\n    genus: None | str = None\n    species: None | str = None\n```\n\n----------------------------------------\n\nTITLE: NeMo2 Torch_dist Checkpoint Structure\nDESCRIPTION: Directory structure showing the expected layout of a NeMo2 checkpoint in torch_dist format, including context files and weight distributions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndefault--val_loss=2.3738-epoch=0-consumed_samples=800.0-last\n├── context\n│   ├── io.json\n│   └── model.yaml\n└── weights\n    ├── __*_*.distcp\n    ├── common.pt\n    └── metadata.json\n```\n\n----------------------------------------\n\nTITLE: Running BioNeMo Unit Tests with Coverage\nDESCRIPTION: Command to execute unit tests with code coverage reporting for the BioNeMo package. Uses pytest with verbose output and generates a terminal coverage report.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-core/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -v --cov=bionemo --cov-report=term .\n```\n\n----------------------------------------\n\nTITLE: Referencing project.scripts Configuration in pyproject.toml\nDESCRIPTION: The content mentions that the scripts directory contains command-line scripts that are configured in the project.scripts section of the pyproject.toml file, which defines how these scripts are generated and made accessible to users.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-geneformer/src/bionemo/geneformer/scripts/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`[project.scripts]`\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for BioNeMo Framework\nDESCRIPTION: Environment variable definitions for BioNeMo Framework setup including cache directories, ports, NGC credentials, and optional Weights & Biases configuration.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/initialization-guide.md#2025-04-23_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n# Local Cache Directories\nLOCAL_RESULTS_PATH\nDOCKER_RESULTS_PATH\nLOCAL_DATA_PATH\nDOCKER_DATA_PATH\nLOCAL_MODELS_PATH\nDOCKER_MODELS_PATH\n\n# Desired Jupyter Port\nJUPYTER_PORT\n\n# NGC Configuration Settings\nNGC_CLI_API_KEY\nNGC_CLI_ORG\nNGC_CLI_TEAM\nNGC_CLI_FORMAT_TYPE\n\n# Weights and Biases API Key\nWANDB_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Converting ZeRO-3 to ZeRO-1\nDESCRIPTION: Command to convert ZeRO-3 checkpoints to ZeRO-1 format, including options for overwriting and specifying model parallel size.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_zero3_to_zero1.py <INPUT_DIR> <OUTPUT_DIR> --overwrite --mp_size <MODEL_PARALLEL_SIZE>\n```\n\n----------------------------------------\n\nTITLE: Updating License Headers for Python Files in BioNeMo Framework\nDESCRIPTION: Commands to install development requirements and run the license check script. This ensures that new Python files have the correct license header.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/contributing/contributing.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -r dev-requirements.txt --user\npython ./scripts/license_check.py --modify --replace --license-header ./license_header -c sub-packages/ -c docs/ -c scripts/ -c ci/ -c internal/\n```\n\n----------------------------------------\n\nTITLE: Release Notes Documentation in Markdown\nDESCRIPTION: Detailed markdown documentation of BioNeMo Framework release notes covering version history, feature additions, improvements, and known issues across multiple releases.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/appendix/releasenotes-fw.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Release Notes\n\n## BioNeMo Framework v2.5\n\n### New Features\n\n* Adding the Evo2 model training workflow, including data preprocessing, pre-training, fine-tuning and inference with bf16 and fp8 support.\n\n### Updates & Improvements\n\n* Supporting/upgrading federated learning examples of BioNeMo in [NVFlare](https://github.com/NVIDIA/NVFlare/tree/2.6.0rc1/examples/advanced/bionemo)\n* Upgrade bionemo-moco to v0.0.2\n* Brev.dev launchable tutorials\n[...continued markdown content...]\n```\n\n----------------------------------------\n\nTITLE: Installing BioNeMo Sub-Package from Test PyPI in Python\nDESCRIPTION: Command to install the package from Test PyPI in a clean Python environment. This step is for verifying the package installation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/Pypi_publish.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install --index-url https://test.pypi.org/simple/ --no-deps package-name\n```\n\n----------------------------------------\n\nTITLE: Installing bionemo-webdatamodule Package\nDESCRIPTION: Command to install the package in development mode\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-webdatamodule/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Running Jupyter Notebook Tests with pytest and nbval\nDESCRIPTION: Command to run nbval tests on Jupyter notebooks in the docs directory to verify they execute successfully.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/contributing/Writing Documentation/jupyter-notebooks.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest --nbval-lax docs/\n```\n\n----------------------------------------\n\nTITLE: Displaying Repository Directory Structure with Tree Command\nDESCRIPTION: Shows the hierarchical directory structure of the BioNeMo framework repository, excluding certain files and directories like .pyc files, test data, and cache directories.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/index.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ tree -C -I \"*.pyc\" -I \"test_data\" -I \"test_experiment\" -I \"test_finettune_experiment\" -I __pycache__ -I \"*.egg-info\" -I lightning_logs -I results -I data -I MNIST* -I 3rdparty\n.\n├── CODE-REVIEW.md -> docs/CODE-REVIEW.md\n├── CODEOWNERS\n├── CONTRIBUTING.md -> docs/CONTRIBUTING.md\n├── Dockerfile\n├── LICENSE\n│   ├── license.txt\n│   └── third_party.txt\n├── README.md\n├── VERSION\n├── ci\n│   └── scripts\n│       ├── nightly_test.sh\n│       ├── pr_test.sh\n│       └── static_checks.sh\n├── docs\n│   ├── CODE-REVIEW.md\n│   ├── CONTRIBUTING.md\n│   ├── Dockerfile\n│   ├── README.md\n│   ├── docs\n│   │   ├── assets\n│   │   │   ├── css\n│   │   │   │   ├── color-schemes.css\n│   │   │   │   ├── custom-material.css\n│   │   │   │   └── fonts.css\n│   │   │   └── images\n│   │   │       ├── favicon.png\n│   │   │       ├── logo-icon-black.svg\n│   │   │       └── logo-white.svg\n│   │   ├── developer-guide\n│   │   │   ├── CODE-REVIEW.md\n│   │   │   ├── CONTRIBUTING.md\n│   │   │   └── jupyter-notebooks.ipynb\n│   │   ├── index.md\n│   │   └── user-guide\n│   │       └── index.md\n│   ├── mkdocs.yml\n│   ├── requirements.txt\n│   └── scripts\n│       └── gen_ref_pages.py\n├── launch.sh\n├── license_header\n├── pyproject.toml\n├── requirements-cve.txt\n├── requirements-dev.txt\n├── requirements-test.txt\n├── scripts\n│   ├── artifact_paths.yaml\n│   ├── download_artifacts.py\n│   ├── gpt-pretrain.py\n│   ├── protein\n│   │   └── esm2\n           └── esm2_dataset_perplexity.py\n```\n\n----------------------------------------\n\nTITLE: Cleanup Temporary Directory\nDESCRIPTION: Removes temporary directory and its contents after processing is complete.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/examples/example_notebook.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndataset_temp_dir.cleanup()\n```\n\n----------------------------------------\n\nTITLE: Configuring Git Submodule Auto-Update\nDESCRIPTION: Command to configure git to automatically update submodules when switching branches.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit config submodule.recurse true\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to execute the package's unit tests using pytest with verbose output\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-noodles/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest -v .\n```\n\n----------------------------------------\n\nTITLE: Submitting and Monitoring SLURM Jobs\nDESCRIPTION: Basic commands for job submission and monitoring in SLURM. Shows how to submit a batch script using sbatch and check job status with squeue.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/using-slurm.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsbatch my_training_script.sbatch\n```\n\nLANGUAGE: bash\nCODE:\n```\nsqueue -u $USER\n```\n\n----------------------------------------\n\nTITLE: Uploading BioNeMo Sub-Package to Test PyPI in Python\nDESCRIPTION: Command to upload the built package to Test PyPI using Twine. This step is for testing the package before final release.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/Pypi_publish.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntwine upload --repository-url https://test.pypi.org/legacy/ dist/* --non-interactive -u $TWINE_USERNAME -p $TWINE_PASSWORD\n```\n\n----------------------------------------\n\nTITLE: Initializing Submodules in Existing Repository\nDESCRIPTION: Command to initialize and update all submodules in an existing BioNeMo repository checkout. This downloads the pinned versions of dependencies that are known to work with BioNeMo.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/index.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Setting Up Work Directory\nDESCRIPTION: Creates and configures a working directory for storing model data and inference results\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/inference.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwork_dir = \"/workspace/bionemo2/esm2_inference_tutorial\"\n\nif cleanup and os.path.exists(work_dir):\n    shutil.rmtree(work_dir)\n\nif not os.path.exists(work_dir):\n    os.makedirs(work_dir)\n    print(f\"Directory '{work_dir}' created.\")\nelse:\n    print(f\"Directory '{work_dir}' already exists.\")\n```\n\n----------------------------------------\n\nTITLE: Adding Sign-off to an Existing Commit\nDESCRIPTION: Command to retroactively add DCO sign-off to the most recent commit without changing the commit message.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/CONTRIBUTING.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit commit --amend --no-edit --signoff\n```\n\n----------------------------------------\n\nTITLE: Running Subprocess Commands in Jupyter for BioNeMo Framework Training\nDESCRIPTION: References to Python modules and Jupyter features that can be used to execute BioNeMo Framework training scripts from notebooks, given the framework's limitations with direct notebook training.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/appendix/FAQ.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[Python Subprocess module](https://docs.python.org/3/library/subprocess.html)\n```\n\nLANGUAGE: markdown\nCODE:\n```\n[Shell Assignment](https://ipython.readthedocs.io/en/stable/interactive/python-ipython-diff.html#shell-assignment)\n```\n\nLANGUAGE: markdown\nCODE:\n```\n[Bash Cell Magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-bash)\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of BioNeMo Models\nDESCRIPTION: A structured table showing available models in the BioNeMo Framework, organized by model name, modality, and primary use case. Includes links to detailed model documentation.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/index.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| **Model**                                  | **Modality**       | **Uses**                                      |\n| ------------------------------------------ | ------------------ | --------------------------------------------- |\n| [ESM-2](./ESM-2/index.md)                  | Protein            | Representation Learning                       |\n| [AMPLIFY](./amplify.md)                    | Protein            | Representation Learning                       |\n| [Geneformer](./geneformer.md)              | Single Cell        | Representation Learning                       |\n| [Evo2](./evo2.md)                          | DNA                | Generative AI                                 |\n```\n\n----------------------------------------\n\nTITLE: Adding Python Code Blocks in Notebooks\nDESCRIPTION: A simple Python code example demonstrating basic arithmetic operations in a Jupyter notebook context.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/contributing/Writing Documentation/jupyter-notebooks.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\na = 1\nb = 2\na + b\n```\n\n----------------------------------------\n\nTITLE: Uploading BioNeMo Sub-Package to PyPI in Python\nDESCRIPTION: Command to upload the package to the official PyPI repository using Twine. This is the final step for making the package publicly available.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/Pypi_publish.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntwine upload  dist/* --non-interactive -u $TWINE_USERNAME -p $TWINE_PASSWORD --verbose\n```\n\n----------------------------------------\n\nTITLE: Logging into NGC Container Registry\nDESCRIPTION: Command to authenticate with the NGC Container Registry using OAuth token and API key credentials.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/getting-started/access-startup.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker login nvcr.io\n```\n\n----------------------------------------\n\nTITLE: Updating Secrets Baseline for detect-secrets Pre-commit Hook\nDESCRIPTION: Commands to update the secrets baseline files to handle false positives raised by the detect-secrets pre-commit hook. These commands scan files and update the baseline files accordingly.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/CONTRIBUTING.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndetect-secrets scan --baseline .secrets.baseline --exclude-files '(.*\\.ipynb|.*\\.baseline)$'\ndetect-secrets scan --baseline .secrets-nb.baseline --exclude-files '^.(?!.*\\.ipynb)' --exclude-lines '\"(hash|id|image/\\w+)\":.*'\n```\n\n----------------------------------------\n\nTITLE: TimeDirection Enumeration Implementation\nDESCRIPTION: Enum class defining the direction of noise schedules, supporting both unified (Noise(0) -> Data(1)) and diffusion (Noise(1) -> Data(0)) directions.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nclass TimeDirection(Enum):\n```\n\n----------------------------------------\n\nTITLE: Generating Code Coverage Reports with pytest in BioNeMo Framework\nDESCRIPTION: Command to run pytest with verbose output and generate code coverage reports for the infra_bionemo package. Produces both terminal and HTML output formats.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/infra-bionemo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -v --cov=infra_bionemo --cov-report=term --cov-report=html .\n```\n\n----------------------------------------\n\nTITLE: Installing Bionemo-evo2 Package\nDESCRIPTION: Installs the bionemo-evo2 package in development mode using pip, allowing for local modifications while using the package.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-evo2/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing BioNeMo-SCDL via pip\nDESCRIPTION: Command to install the BioNeMo-SCDL package using pip. This is the primary method for installing the package.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-scdl/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install bionemo-scdl\n```\n\n----------------------------------------\n\nTITLE: Loss Weight Function Setting in Python\nDESCRIPTION: Sets the loss weight calculation function for the instance.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_87\n\nLANGUAGE: python\nCODE:\n```\ndef set_loss_weight_fn(fn: Callable)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for ESM-2 Fine-tuning in Python\nDESCRIPTION: This code imports necessary Python libraries for the ESM-2 fine-tuning process, including os, shutil, warnings, and pandas. It also suppresses warnings for cleaner output.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-esm2/finetune.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%capture --no-display --no-stderr cell_output\n\nimport os\nimport shutil\nimport warnings\n\nimport pandas as pd\n\n\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to run pytest unit tests with verbose output\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-webdatamodule/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest -v .\n```\n\n----------------------------------------\n\nTITLE: Creating Markdown Navigation Links for BioNeMo Documentation\nDESCRIPTION: Markdown link structure defining the main navigation menu for BioNeMo framework documentation. Contains links to key sections including home page, user guide, models reference, datasets documentation, and API reference.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/SUMMARY.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* [Home](index.md)\n* [User Guide](user-guide/)\n* [Models](models/)\n* [Datasets](datasets/)\n* [API](API_reference/)\n```\n\n----------------------------------------\n\nTITLE: Installing ESM-2 Package in Editable Mode\nDESCRIPTION: This command installs the ESM-2 package in editable mode, allowing for development and immediate reflection of code changes.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-esm2/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Markdown Navigation Links for NeMo Documentation\nDESCRIPTION: Markdown links to key documentation pages for NeMo2 parallelism and Megatron dataset considerations within the Bionemo framework\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/background/SUMMARY.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- [NeMo2 Parallelism](nemo2.md)\n- [Megatron Dataset Considerations](megatron_datasets.md)\n```\n\n----------------------------------------\n\nTITLE: Citation Information for Semantic Mining Paper\nDESCRIPTION: BibTeX citation for the related research paper on semantic mining of functional de novo genes using the genomic language model.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/models/evo2.md#2025-04-23_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@article {merchant2024semantic,\n   author = {Merchant, Aditi T and King, Samuel H and Nguyen, Eric and Hie, Brian L},\n   title = {Semantic mining of functional de novo genes from a genomic language model},\n   year = {2024},\n   doi = {10.1101/2024.12.17.628962},\n   publisher = {Cold Spring Harbor Laboratory},\n   URL = {https://www.biorxiv.org/content/early/2024/12/18/2024.12.17.628962},\n   journal = {bioRxiv}\n}\n```\n\n----------------------------------------\n\nTITLE: Available BioNeMo Framework Commands\nDESCRIPTION: List of all available just commands for the BioNeMo framework development cycle. These commands handle building development and release images, running interactive programs in containers, setting up the environment, and running tests.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/README_justfile.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbuild-dev              # Builds the development image.\nbuild-release          # Builds the release image.\nrun-dev cmd='bash'     # Runs an interactive program in the development bionemo image.\nrun-release cmd='bash' # Runs an interactive program in the release bionemo image.\nsetup                  # Checks for installed programs (docker, git, etc.), their versions, and grabs the latest cache image.\ntest                   # Executes pytest in the release image.\n```\n\n----------------------------------------\n\nTITLE: Building BioNeMo Sub-Package in Python\nDESCRIPTION: Command to build the BioNeMo sub-package using Python's build module. This step creates distribution files for the package.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/Pypi_publish.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m build .\n```\n\n----------------------------------------\n\nTITLE: Reinstalling the Project without Dependencies using uv\nDESCRIPTION: Command to reinstall the project without reinstalling dependencies, useful when project requirements haven't changed but project files have been updated.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/infra-bionemo/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install --no-deps -e .\n```\n\n----------------------------------------\n\nTITLE: Installing bionemo-geneformer Package\nDESCRIPTION: Command to install the package in development mode from the current directory using pip.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-geneformer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing Just Command Runner with Homebrew\nDESCRIPTION: Command to install the just command runner using Homebrew on OS X and Linux systems. This is a prerequisite for working with the BioNeMo framework.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/README_justfile.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install just\n```\n\n----------------------------------------\n\nTITLE: Running Documentation Cleanup Script\nDESCRIPTION: Executes a Python script that performs post-processing cleanup on the generated documentation to remove redundant sections and fix internal references\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/scripts/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython scripts/clean_documentation.py\n```\n\n----------------------------------------\n\nTITLE: Installing BioNeMo-MoCo Package Without Dependencies\nDESCRIPTION: Command to install bionemo-moco without dependencies, useful when preserving an existing PyTorch installation. Additional dependencies like jaxtyping and pot must be manually installed.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/environment/Instructions.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install --no-deps -e .\n```\n\n----------------------------------------\n\nTITLE: Building Development Docker Image for BioNeMo Framework\nDESCRIPTION: Script to build a Docker image specifically configured for development purposes. This creates an environment with all necessary dependencies for developing the BioNeMo framework.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/internal/scripts/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./internal/scripts/build_dev_image.sh\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License 2.0 Boilerplate Notice\nDESCRIPTION: This snippet provides a template for applying the Apache License 2.0 to your work. It includes placeholders for copyright year and owner, which should be replaced with appropriate information.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/LICENSE/third_party.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Configuring Jupyter Notebook Cell Metadata for Visibility Control\nDESCRIPTION: This JSON snippet shows how to configure the metadata of a Jupyter notebook cell to hide it from rendered documentation and collapse it in Jupyter Lab sessions. It uses the 'remove-cell' tag and 'source_hidden' property.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/README.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"metadata\": {\n    \"jupyter\": {\n     \"source_hidden\": true\n    },\n    \"tags\": [\n     \"remove-cell\"\n    ]\n   },\n```\n\n----------------------------------------\n\nTITLE: Calculate Log SNR Function\nDESCRIPTION: Public wrapper function to generate time schedule tensor with optional device placement and time direction synchronization.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/documentation.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_log_snr(t: Tensor,\n                      device: Union[str, torch.device] = \"cpu\",\n                      synchronize: Optional[TimeDirection] = None) -> Tensor\n```\n\n----------------------------------------\n\nTITLE: Visualizing Training Loss\nDESCRIPTION: Creates a plot to visualize the training loss over time.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/examples/discrete_data_interpolant_tutorial.ipynb#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nplt.plot(losses, label=\"Training Loss\", linestyle=\"-\", color=\"blue\", marker=\"o\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss\")\nplt.legend()\nplt.grid(True)\nplt.ylim([0, 1])\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Removing Existing Processed Data Directories in Bash\nDESCRIPTION: Removes existing processed data directories to ensure a clean slate for data conversion.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/docs/docs/user-guide/examples/bionemo-geneformer/geneformer_cellxgene_tutorial.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n!rm -rf  {train_tutorial_processed_dir}\n!rm -rf  {val_tutorial_processed_dir}\n!rm -rf  {test_tutorial_processed_dir}\n```\n\n----------------------------------------\n\nTITLE: Executing Documentation Generation Script\nDESCRIPTION: Basic command to run the documentation generation script from the project root directory.\nSOURCE: https://github.com/nvidia/bionemo-framework/blob/main/sub-packages/bionemo-moco/scripts/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./create_documentation.sh\n```"
  }
]
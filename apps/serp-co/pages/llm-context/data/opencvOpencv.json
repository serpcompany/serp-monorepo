[
  {
    "owner": "opencv",
    "repo": "opencv",
    "content": "TITLE: Generating a Disparity (Depth) Map from Stereo Images with OpenCV in Python\nDESCRIPTION: This Python code snippet demonstrates how to compute a disparity map (which is inversely proportional to depth) from a pair of stereo images using the OpenCV library\\'s StereoBM block matching algorithm. Dependencies required are numpy, matplotlib, and the OpenCV (cv2) library. The function reads two rectified grayscale images, initializes the StereoBM matcher with preset numDisparities and blockSize parameters, computes the disparity map, and displays the result using matplotlib. Key parameters controlling the quality and performance include numDisparities, blockSize, and others that can be accessed via setters (e.g., setTextureThreshold, setSpeckleRange, setUniquenessRatio). Input images should be precalibrated and rectified for best results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_depthmap/py_depthmap.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimgL = cv.imread('tsukuba_l.png', cv.IMREAD_GRAYSCALE)\nimgR = cv.imread('tsukuba_r.png', cv.IMREAD_GRAYSCALE)\n\nstereo = cv.StereoBM.create(numDisparities=16, blockSize=15)\ndisparity = stereo.compute(imgL,imgR)\nplt.imshow(disparity,'gray')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Thresholding Operations Initialization in OpenCV (C++, Full Example)\nDESCRIPTION: This C++ code demonstrates loading an image, converting it to grayscale, creating a display window, setting up interactive trackbars for threshold type and value, and applying different threshold operations via the cv::threshold function. The program leverages OpenCV 3.0 or later and requires linking its core and highgui/imgproc modules. Inputs include an image file path, user-selected threshold value, and threshold type; outputs are thresholded images displayed in real-time as the user interacts with the controls. The sample handles edge-cases such as BGR-to-gray conversion and coordinates threshold updates through event callbacks.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/imgproc.hpp>\\n#include <opencv2/highgui.hpp>\\nusing namespace cv;\\n\\nMat src, src_gray, dst;\\nint threshold_value = 0;\\nint threshold_type = 3;\\nint const max_value = 255;\\nint const max_type = 4;\\nint const max_BINARY_value = 255;\\n\\nconst char* window_name = \"Threshold Demo\";\\n\\nvoid Threshold_Demo( int, void* )\\n{\\n  /* 0: Binary\\n     1: Binary Inverted\\n     2: Truncate\\n     3: To Zero\\n     4: To Zero Inverted\\n   */\\n  threshold( src_gray, dst, threshold_value, max_BINARY_value,threshold_type );\\n  imshow( window_name, dst );\\n}\\n\\nint main( int argc, char** argv )\\n{\\n  // Load an image\\n  src = imread( argc >=2 ? argv[1] : \"chicky_512.png\", IMREAD_COLOR );\\n  if( src.empty() )\\n  {\\n    printf(\"Error opening image\\n\");\\n    return -1;\\n  }\\n\\n  cvtColor( src, src_gray, COLOR_BGR2GRAY );\\n  namedWindow( window_name, WINDOW_AUTOSIZE );\\n\\n  // Create Trackbars\\n  createTrackbar( \"Type:\\n 0:Binary \\n 1:BinaryInv \\n 2:Trunc \\n 3:ToZero \\n 4:ToZeroInv\",\\n                 window_name, &threshold_type, max_type, Threshold_Demo );\\n  createTrackbar( \"Value\",\\n                 window_name, &threshold_value, max_value, Threshold_Demo );\\n\\n  Threshold_Demo( 0, 0 );\\n  waitKey(0);\\n  return 0;\\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Morphological Transformations with Interactive Trackbars in OpenCV (C++)\nDESCRIPTION: This C++ code sample demonstrates interactive application of several morphological operations (Opening, Closing, Gradient, Top Hat, Black Hat) using OpenCV. The code loads an image, creates a GUI with three trackbars for selecting operation type, structuring element, and kernel size, and updates the output in real time using cv::morphologyEx and user-defined trackbar callbacks. Dependencies include OpenCV 3.0+ and a valid image input; inputs come from user-adjusted trackbars and an image path. Output is a GUI window with the transformed image, reacting dynamically to user input. The code requires a system with OpenCV GUI support, and some functions assume grayscale or single-channel images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/opening_closing_hats/opening_closing_hats.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/imgproc.hpp>\\n#include <opencv2/highgui.hpp>\\n#include <iostream>\\n\\nusing namespace cv;\\nusing namespace std;\\n\\nMat src, dst;\\nint morph_operator = 0;\\nint morph_elem = 0;\\nint morph_size = 0;\\nint const max_operator = 4;\\nint const max_elem = 2;\\nint const max_kernel_size = 21;\\n\\nconst char* window_name = \"Morphology Transformations\";\\n\\nvoid Morphology_Operations(int, void*)\\n{\\n    int operation = morph_operator + 2;\\n    Mat element = getStructuringElement(morph_elem,\\n              Size(2 * morph_size + 1, 2 * morph_size + 1),\\n              Point(morph_size, morph_size));\\n    morphologyEx(src, dst, operation, element);\\n    imshow(window_name, dst);\\n}\\n\\nint main(int argc, char** argv)\\n{\\n    CommandLineParser parser(argc, argv, \"{@input | baboon.png | }\");\\n    src = imread(parser.get<String>(\"@input\"), IMREAD_COLOR);\\n    if(src.empty())\\n    {\\n        cout << \"Cannot load image!\" << endl;\\n        return -1;\\n    }\\n\\n    namedWindow(window_name, WINDOW_AUTOSIZE);\\n\\n    createTrackbar(\"Operator:\\n 0: Opening\\n 1: Closing\\n 2: Gradient\\n 3: Top Hat\\n 4: Black Hat\", window_name, &morph_operator, max_operator, Morphology_Operations);\\n    createTrackbar(\"Element:\\n 0: Rect\\n 1: Cross\\n 2: Ellipse\", window_name, &morph_elem, max_elem, Morphology_Operations);\\n    createTrackbar(\"Kernel size:\\n 2n+1\", window_name, &morph_size, max_kernel_size, Morphology_Operations);\\n\\n    Morphology_Operations(0, 0);\\n    waitKey(0);\\n    return 0;\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Normalizing Histogram Results in Python\nDESCRIPTION: Python snippet normalizing the calculated histograms (`b_hist`, `g_hist`, `r_hist`) using `cv.normalize`. It scales the histogram values to fit within the range 0 to `histImage.shape[0]` (the height of the display image) using the `cv.NORM_MINMAX` method.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/Histograms_Matching/histogram_calculation/calcHist_Demo.py Normalize the result to ( 0, histImage.rows )\n```\n\n----------------------------------------\n\nTITLE: Detecting Corners using Harris Corner Detector in OpenCV (Python)\nDESCRIPTION: This code snippet demonstrates how to use OpenCV's cv.cornerHarris() function to detect corners in a grayscale image. It involves converting an image to a grayscale float32 format, applying the Harris Corner Detection, and marking the detected corners on the image. Dependencies include the OpenCV and NumPy libraries. The main parameters are blockSize, ksize, and k, which define the neighborhood size, aperture parameter, and Harris detector's free parameter, respectively.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_features_harris/py_features_harris.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nfilename = 'chessboard.png'\nimg = cv.imread(filename)\ngray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n\ngray = np.float32(gray)\ndst = cv.cornerHarris(gray,2,3,0.04)\n\n#result is dilated for marking the corners, not important\ndst = cv.dilate(dst,None)\n\n# Threshold for an optimal value, it may vary depending on the image.\nimg[dst>0.01*dst.max()]=[0,0,255]\n\ncv.imshow('dst',img)\nif cv.waitKey(0) & 0xff == 27:\n    cv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Accessing Pixel Values in an OpenCV Image using NumPy Indexing in Python\nDESCRIPTION: Shows how to retrieve the BGR pixel value at coordinates (100, 100) and how to access only the Blue channel value (index 0) at the same location using NumPy array indexing on the OpenCV image object (`img`). For BGR images, it returns a [Blue, Green, Red] array; for grayscale, it returns the intensity.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_basic_ops/py_basic_ops.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> px = img[100,100]\n>>> print( px )\n[157 166 200]\n\n# accessing only blue pixel\n>>> blue = img[100,100,0]\n>>> print( blue )\n157\n```\n\n----------------------------------------\n\nTITLE: Converting BGR Image to Grayscale with OpenCV in Python\nDESCRIPTION: Demonstrates color-to-grayscale conversion in Python OpenCV with cv2.cvtColor and COLOR_BGR2GRAY. Input is BGR image array, output is single-channel intensity array.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n```\n\n----------------------------------------\n\nTITLE: Reading Calibration Settings from File using OpenCV FileStorage in C++\nDESCRIPTION: This C++ snippet reference points to code that reads camera calibration settings from an XML or YAML file using OpenCV's FileStorage class. It reads various configuration parameters and includes a post-processing step to validate the input, setting a flag `goodInput` accordingly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/calib3d/camera_calibration/camera_calibration.cpp file_read\n```\n\n----------------------------------------\n\nTITLE: Initializing Kalman Filter Parameters in C++ (OpenCV)\nDESCRIPTION: Defines the `initKalmanFilter` function in C++ using OpenCV. It initializes a `cv::KalmanFilter` object (`KF`) with specified state, measurement, and input dimensions (`nStates`, `nMeasurements`, `nInputs`) using double precision (`CV_64F`). It sets initial process noise (`processNoiseCov`), measurement noise (`measurementNoiseCov`), and error covariance (`errorCovPost`) matrices to identity matrices scaled by small values. It then configures the `transitionMatrix` based on a dynamic model incorporating time step `dt` (likely constant velocity/acceleration for both position and orientation) and the `measurementMatrix` to map state variables (position x, y, z and orientation roll, pitch, yaw) directly to measurements.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\nvoid initKalmanFilter(cv::KalmanFilter &KF, int nStates, int nMeasurements, int nInputs, double dt)\n{\n\n  KF.init(nStates, nMeasurements, nInputs, CV_64F);                 // init Kalman Filter\n\n  cv::setIdentity(KF.processNoiseCov, cv::Scalar::all(1e-5));       // set process noise\n  cv::setIdentity(KF.measurementNoiseCov, cv::Scalar::all(1e-4));   // set measurement noise\n  cv::setIdentity(KF.errorCovPost, cv::Scalar::all(1));             // error covariance\n\n\n                 /* DYNAMIC MODEL */\n\n  //  [1 0 0 dt  0  0 dt2   0   0 0 0 0  0  0  0   0   0   0]\n  //  [0 1 0  0 dt  0   0 dt2   0 0 0 0  0  0  0   0   0   0]\n  //  [0 0 1  0  0 dt   0   0 dt2 0 0 0  0  0  0   0   0   0]\n  //  [0 0 0  1  0  0  dt   0   0 0 0 0  0  0  0   0   0   0]\n  //  [0 0 0  0  1  0   0  dt   0 0 0 0  0  0  0   0   0   0]\n  //  [0 0 0  0  0  1   0   0  dt 0 0 0  0  0  0   0   0   0]\n  //  [0 0 0  0  0  0   1   0   0 0 0 0  0  0  0   0   0   0]\n  //  [0 0 0  0  0  0   0   1   0 0 0 0  0  0  0   0   0   0]\n  //  [0 0 0  0  0  0   0   0   1 0 0 0  0  0  0   0   0   0]\n  //  [0 0 0  0  0  0   0   0   0 1 0 0 dt  0  0 dt2   0   0]\n  //  [0 0 0  0  0  0   0   0   0 0 1 0  0 dt  0   0 dt2   0]\n  //  [0 0 0  0  0  0   0   0   0 0 0 1  0  0 dt   0   0 dt2]\n  //  [0 0 0  0  0  0   0   0   0 0 0 0  1  0  0  dt   0   0]\n  //  [0 0 0  0  0  0   0   0   0 0 0 0  0  1  0   0  dt   0]\n  //  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  1   0   0  dt]\n  //  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  0   1   0   0]\n  //  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  0   0   1   0]\n  //  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  0   0   0   1]\n\n  // position\n  KF.transitionMatrix.at<double>(0,3) = dt;\n  KF.transitionMatrix.at<double>(1,4) = dt;\n  KF.transitionMatrix.at<double>(2,5) = dt;\n  KF.transitionMatrix.at<double>(3,6) = dt;\n  KF.transitionMatrix.at<double>(4,7) = dt;\n  KF.transitionMatrix.at<double>(5,8) = dt;\n  KF.transitionMatrix.at<double>(0,6) = 0.5*pow(dt,2);\n  KF.transitionMatrix.at<double>(1,7) = 0.5*pow(dt,2);\n  KF.transitionMatrix.at<double>(2,8) = 0.5*pow(dt,2);\n\n  // orientation\n  KF.transitionMatrix.at<double>(9,12) = dt;\n  KF.transitionMatrix.at<double>(10,13) = dt;\n  KF.transitionMatrix.at<double>(11,14) = dt;\n  KF.transitionMatrix.at<double>(12,15) = dt;\n  KF.transitionMatrix.at<double>(13,16) = dt;\n  KF.transitionMatrix.at<double>(14,17) = dt;\n  KF.transitionMatrix.at<double>(9,15) = 0.5*pow(dt,2);\n  KF.transitionMatrix.at<double>(10,16) = 0.5*pow(dt,2);\n  KF.transitionMatrix.at<double>(11,17) = 0.5*pow(dt,2);\n\n\n       /* MEASUREMENT MODEL */\n\n  //  [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n  //  [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n  //  [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n  //  [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n  //  [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n  //  [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n\n  KF.measurementMatrix.at<double>(0,0) = 1;  // x\n  KF.measurementMatrix.at<double>(1,1) = 1;  // y\n  KF.measurementMatrix.at<double>(2,2) = 1;  // z\n  KF.measurementMatrix.at<double>(3,9) = 1;  // roll\n  KF.measurementMatrix.at<double>(4,10) = 1; // pitch\n  KF.measurementMatrix.at<double>(5,11) = 1; // yaw\n\n}\n```\n\n----------------------------------------\n\nTITLE: Setup Image in OpenCV C++\nDESCRIPTION: This snippet demonstrates opening an image, converting it to grayscale, and applying a blur to reduce noise using OpenCV in C++. It requires the OpenCV library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\n\n// Load an image and convert it to grayscale\ncv::Mat src = cv::imread(\"image.jpg\");\ncv::Mat gray;\ncv::cvtColor(src, gray, cv::COLOR_BGR2GRAY);\ncv::blur(gray, gray, cv::Size(3, 3));\n```\n\n----------------------------------------\n\nTITLE: Loading an Image from File with OpenCV in C++\nDESCRIPTION: Demonstrates how to load an image from file using OpenCV's imread function in C++. Requires OpenCV installed and properly linked. The sample loads the image as a cv::Mat object, with the resulting image defaulting to 3-channel BGR for color images. Input is a file path, output is a cv::Mat containing image data. The operation depends on file format support in the OpenCV build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\\ncv::Mat img = cv::imread(\"my_image.jpg\");\n```\n\n----------------------------------------\n\nTITLE: Thresholding Operations Initialization in OpenCV (Python, Full Example)\nDESCRIPTION: This Python code sample shows how to load an image, convert it to grayscale if needed, display it in a window, and use OpenCV's trackbar GUI widgets for interactivity over the threshold type and value. The thresholding is performed via cv2.threshold, and the window updates whenever the user changes a slider. The script depends on OpenCV-Python (cv2). It takes an image file path as input from the command line or defaults, applies the selected threshold, and displays the output window; user actions trigger instant processing. Edge handling for missing or incorrect files is included.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\\nimport sys\\n\\nmax_value = 255\\nmax_type = 4\\nmax_BINARY_value = 255\\nwindow_name = 'Threshold Demo'\\n\\n# [load]\\nsrc = cv.imread(sys.argv[1] if len(sys.argv) > 1 else 'chicky_512.png')\\nif src is None:\\n    print('Error opening image')\\n    sys.exit(-1)\\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\\n# [load]\\n\\ndef Threshold_Demo(*args):\\n    threshold_type = cv.getTrackbarPos('Type', window_name)\\n    threshold_value = cv.getTrackbarPos('Value', window_name)\\n    ret, dst = cv.threshold(src_gray, threshold_value, max_BINARY_value, threshold_type)\\n    cv.imshow(window_name, dst)\\n\\ncv.namedWindow(window_name)\\n\\n# [trackbar]\\ncv.createTrackbar('Type', window_name, 3, max_type, Threshold_Demo)\\ncv.createTrackbar('Value', window_name, 0, max_value, Threshold_Demo)\\n# [trackbar]\\n\\nThreshold_Demo()\\ncv.waitKey(0)\n```\n\n----------------------------------------\n\nTITLE: Loading an Image from File with OpenCV in Python\nDESCRIPTION: Shows how to read an image file using OpenCV's cv2.imread in Python. Requires the cv2 (opencv-python) module installed. Loads the file at the given path as a NumPy array, using BGR color order by default for JPEGs. The file path is input; NumPy ndarray with image data is output. Image is loaded with 3 channels for color images unless changed by a flag.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\\nimg = cv2.imread('my_image.jpg')\n```\n\n----------------------------------------\n\nTITLE: Matching Extracted Face Features - OpenCV DNN C++\nDESCRIPTION: This snippet demonstrates how to compute the similarity or distance between two extracted face features in C++. Metric functions are called with the feature vectors to get cosine or normL2 distances for identity verification. The output is a scalar similarity or distance value, which can be compared to pre-selected thresholds for face matching.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_face/dnn_face.markdown#2025-04-22_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n// Compare two extracted face features\nfloat cosineScore = recognizer->match(feature1, feature2, cv::FaceRecognizerSF::DisType::FR_COSINE);\nfloat l2Score = recognizer->match(feature1, feature2, cv::FaceRecognizerSF::DisType::FR_NORM_L2);\n// Use thresholds from model evaluation to determine match/non-match\n```\n\n----------------------------------------\n\nTITLE: Implementing 2D Convolution with Custom Kernel in OpenCV Python\nDESCRIPTION: This code demonstrates how to apply a custom averaging filter kernel to an image using cv.filter2D(). It creates a 5x5 normalized kernel and applies it to an image for basic smoothing, then displays both original and filtered results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_filtering/py_filtering.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('opencv_logo.png')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n\nkernel = np.ones((5,5),np.float32)/25\ndst = cv.filter2D(img,-1,kernel)\n\nplt.subplot(121),plt.imshow(img),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(dst),plt.title('Averaging')\nplt.xticks([]), plt.yticks([])\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Optimizing Array Size for DFT using OpenCV and Numpy - Python\nDESCRIPTION: This code demonstrates how to determine and use the optimal size for FFT/DFT operations to maximize performance using OpenCV and Numpy. It reads a grayscale image, computes optimal DFT sizes, and displays them. Dependencies: OpenCV (cv), Numpy (np). Inputs: image filepath ('messi5.jpg') and image array. Outputs: the original and optimal (padded) array sizes printed to standard output. Assumes the image file exists in the path. This is suitable for notebook use, leveraging IPython input format and assertions for file presence.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nIn [15]: img = cv.imread('messi5.jpg', cv.IMREAD_GRAYSCALE)\nIn [16]: assert img is not None, \"file could not be read, check with os.path.exists()\"\nIn [17]: rows,cols = img.shape\nIn [18]: print(\"{} {}\".format(rows,cols))\n342 548\n\nIn [19]: nrows = cv.getOptimalDFTSize(rows)\nIn [20]: ncols = cv.getOptimalDFTSize(cols)\nIn [21]: print(\"{} {}\".format(nrows,ncols))\n360 576\n```\n\n----------------------------------------\n\nTITLE: Calculating Solidity using OpenCV.js\nDESCRIPTION: Determines the solidity of a contour, defined as the ratio of the contour's area to the area of its convex hull. Requires a contour (`cnt`). Uses `cv.contourArea` for both the contour and its hull area, and `cv.convexHull` to compute the convex hull.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contour_properties/js_contour_properties.markdown#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nlet area = cv.contourArea(cnt, false);\ncv.convexHull(cnt, hull, false, true);\nlet hullArea = cv.contourArea(hull, false);\nlet solidity = area / hullArea;\n```\n\n----------------------------------------\n\nTITLE: Single Object Template Matching with Multiple Methods in OpenCV Python\nDESCRIPTION: Demonstrates template matching to find a single face in an image using six different comparison methods. The code loads source and template images, applies different matching methods, and visualizes the results using matplotlib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_template_matching/py_template_matching.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('messi5.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nimg2 = img.copy()\ntemplate = cv.imread('template.jpg', cv.IMREAD_GRAYSCALE)\nassert template is not None, \"file could not be read, check with os.path.exists()\"\nw, h = template.shape[::-1]\n\n# All the 6 methods for comparison in a list\nmethods = ['TM_CCOEFF', 'TM_CCOEFF_NORMED', 'TM_CCORR',\n            'TM_CCORR_NORMED', 'TM_SQDIFF', 'TM_SQDIFF_NORMED']\n\nfor meth in methods:\n    img = img2.copy()\n    method = getattr(cv, meth)\n\n    # Apply template Matching\n    res = cv.matchTemplate(img,template,method)\n    min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)\n\n    # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:\n        top_left = min_loc\n    else:\n        top_left = max_loc\n    bottom_right = (top_left[0] + w, top_left[1] + h)\n\n    cv.rectangle(img,top_left, bottom_right, 255, 2)\n\n    plt.subplot(121),plt.imshow(res,cmap = 'gray')\n    plt.title('Matching Result'), plt.xticks([]), plt.yticks([])\n    plt.subplot(122),plt.imshow(img,cmap = 'gray')\n    plt.title('Detected Point'), plt.xticks([]), plt.yticks([])\n    plt.suptitle(meth)\n\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Displaying 32F Image by Conversion to 8U for imshow with OpenCV in Python\nDESCRIPTION: In Python, float images are converted to 8-bit using (img * 255).astype(np.uint8) for display via cv2.imshow. If the dynamic range is not [0,1] normalization may be required. The window rendering expects 8U data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_45\n\nLANGUAGE: Python\nCODE:\n```\nimg8u = (img * 255).astype(np.uint8)\\ncv2.imshow('Window', img8u)\\ncv2.waitKey(0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dense Optical Flow in Python\nDESCRIPTION: Python implementation of dense optical flow using Farneback's algorithm (cv.calcOpticalFlowFarneback()). The code computes optical flow for all points in the frame and visualizes the flow field using HSV color coding where hue represents direction and value represents magnitude.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/optical_flow.markdown#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nimport argparse\n\nparser = argparse.ArgumentParser(description='This sample demonstrates Farneback Optical Flow calculation. \\n'\n                                     'The example file can be downloaded from: \\n'\n                                     'https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4')\nparser.add_argument('image', type=str, help='path to image file')\nargs = parser.parse_args()\n\ncap = cv.VideoCapture(args.image)\n\nret, frame1 = cap.read()\nprvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\nhsv = np.zeros_like(frame1)\nhsv[..., 1] = 255\n\nwhile(1):\n    ret, frame2 = cap.read()\n    if not ret:\n        print('No frames grabbed!')\n        break\n\n    next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n\n    flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n\n    mag, ang = cv.cartToPolar(flow[..., 0], flow[..., 1])\n    hsv[..., 0] = ang*180/np.pi/2\n    hsv[..., 2] = cv.normalize(mag, None, 0, 255, cv.NORM_MINMAX)\n    bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n\n    cv.imshow('frame2', bgr)\n    k = cv.waitKey(30) & 0xff\n    if k == 27:\n        break\n    elif k == ord('s'):\n        cv.imwrite('opticalfb.png', frame2)\n        cv.imwrite('opticalhsv.png', bgr)\n    prvs = next\n\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Implementing Image Dilation with OpenCV Python\nDESCRIPTION: Shows how to perform dilation on an image. Dilation increases the white region in the image and is often used after erosion to restore object size while maintaining noise removal.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndilation = cv.dilate(img,kernel,iterations = 1)\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Python Virtual Environment - Console\nDESCRIPTION: These commands demonstrate the setup of a Python3.7+ virtual environment to isolate dependencies for the conversion and inference scripts. They use the 'virtualenv' tool and activate the environment, which is a prerequisite for clean library installation and reproducibility.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nvirtualenv -p /usr/bin/python3.7 <env_dir_path>\\nsource <env_dir_path>/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Python 2 Module Dependencies and Settings in CMake\nDESCRIPTION: This CMake snippet first checks if the `PYTHON2_INCLUDE_PATH` and `PYTHON2_NUMPY_INCLUDE_DIRS` variables are set. If either is missing, it disables the `python2` OpenCV module using `ocv_module_disable`. It then defines several variables: `the_description` for the module, `MODULE_NAME` as `python2`, `MODULE_INSTALL_SUBDIR` as empty (indicating installation to the root library directory, often required by build systems like Buildbot), and `PYTHON` as `PYTHON2`. Finally, it includes a common CMake configuration file located in the parent directory (`../common.cmake`) and unsets the locally defined `MODULE_NAME` and `MODULE_INSTALL_SUBDIR` variables to avoid polluting the parent scope.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/python/python2/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT PYTHON2_INCLUDE_PATH OR NOT PYTHON2_NUMPY_INCLUDE_DIRS)\n  ocv_module_disable(python2)\nendif()\n\nset(the_description \"The python2 bindings\")\nset(MODULE_NAME python2)\n# Buildbot requires Python 2 to be in root lib dir\nset(MODULE_INSTALL_SUBDIR \"\")\n\nset(PYTHON PYTHON2)\n\ninclude(../common.cmake)\n\nunset(MODULE_NAME)\nunset(MODULE_INSTALL_SUBDIR)\n```\n\n----------------------------------------\n\nTITLE: Executing GrabCut Algorithm in OpenCV with JavaScript\nDESCRIPTION: This JavaScript code snippet demonstrates the usage of the GrabCut function within OpenCV to perform foreground extraction on an 8-bit 3-channel image. Dependencies include OpenCV configured for JavaScript. Parameters include 'image' for the input, 'mask' for marking the foreground and background, and 'rect' for the region of interest. Temporary arrays 'bgdModel' and 'fgdModel' are used for background and foreground modeling respectively. 'iterCount' specifies the number of iterations, and 'mode' determines the operation mode. The function outputs a refined segmentation of the foreground and background.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_grabcut/js_grabcut.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.grabCut (image, mask, rect, bgdModel, fgdModel, iterCount, mode = cv.GC_EVAL)\n```\n\n----------------------------------------\n\nTITLE: Text Detection Inference in C++\nDESCRIPTION: This C++ snippet performs text detection inference using a configured TextDetectionModel. It processes the input image and visualizes the detected text regions using polylines thanks to OpenCV's DNN API. The expected input is a preprocessed image, and the output is a visualization of detected texts.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_text_spotting/dnn_text_spotting.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nstd::vector<std::vector<Point>> detResults;\nmodel.detect(detResults);\n\n// Visualization\npolylines(frame, results, true, Scalar(0, 255, 0), 2);\nimshow(\"Text Detection\", image);\nwaitKey();\n```\n\n----------------------------------------\n\nTITLE: Calculating Image Histogram Demo in C++ (Full Code)\nDESCRIPTION: Complete C++ program demonstrating how to load an image, split it into B, G, R channels, calculate the histogram for each channel using `cv::calcHist`, normalize the results, and display the histograms in a window. Depends on the OpenCV library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n@include samples/cpp/tutorial_code/Histograms_Matching/calcHist_Demo.cpp\n```\n\n----------------------------------------\n\nTITLE: Implementing Canny Edge Detector in Java\nDESCRIPTION: Complete implementation of the Canny Edge Detector in Java using OpenCV. This code creates a window with a trackbar to adjust the lower threshold for the Canny algorithm, applies the detector to an input image, and displays the result showing edges overlaid on the original image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.opencv.core.*;\nimport org.opencv.core.Core.MinMaxLocResult;\nimport org.opencv.highgui.HighGui;\nimport org.opencv.imgcodecs.Imgcodecs;\nimport org.opencv.imgproc.Imgproc;\n\nclass CannyDetectorDemo {\n    private Mat src = new Mat();\n    private Mat srcBlur = new Mat();\n    private Mat detectedEdges = new Mat();\n    private Mat dst = new Mat();\n    private int lowThresh = 0;\n    private static final int MAX_LOW_THRESH = 100;\n    private static final int RATIO = 3;\n    private static final int KERNEL_SIZE = 3;\n    private static final Size BLUR_SIZE = new Size(3, 3);\n    private JSlider cannyThresholdSlider;\n    private int cannyThresholdSliderValue = 0;\n    private JLabel cannyThresholdLabel;\n    private JLabel radiusLabel;\n    private JLabel iLabel;\n    private static final String WINDOW_NAME = \"Edge Map\";\n\n    public CannyDetectorDemo(String[] args) {\n        String imagePath = args.length > 0 ? args[0] : \"../data/fruits.jpg\";\n        src = Imgcodecs.imread(imagePath);\n        if (src.empty()) {\n            System.out.println(\"Error opening image!\");\n            System.out.println(\"Program Arguments: [image_name -- default ../data/fruits.jpg] \\n\");\n            System.exit(-1);\n        }\n        dst = Mat.zeros(src.size(), src.type());\n        Imgproc.cvtColor(src, detectedEdges, Imgproc.COLOR_BGR2GRAY);\n        \n        // Create and set up the window.\n        final JFrame frame = new JFrame(WINDOW_NAME);\n        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n        // Set up the content pane.\n        Image img = HighGui.toBufferedImage(dst);\n        final JLabel imgLabel = new JLabel(new ImageIcon(img));\n        frame.getContentPane().add(imgLabel, BorderLayout.CENTER);\n        \n        //Create and set up the panel.\n        final JPanel sliderPanel = new JPanel();\n        sliderPanel.setLayout(new BoxLayout(sliderPanel, BoxLayout.PAGE_AXIS));\n        \n        //Create the slider.\n        cannyThresholdLabel = new JLabel(\"Min Threshold: \" + cannyThresholdSliderValue);\n        cannyThresholdSlider = new JSlider(0, MAX_LOW_THRESH, cannyThresholdSliderValue);\n        cannyThresholdSlider.setMajorTickSpacing(20);\n        cannyThresholdSlider.setMinorTickSpacing(10);\n        cannyThresholdSlider.setPaintTicks(true);\n        cannyThresholdSlider.setPaintLabels(true);\n        cannyThresholdSlider.addChangeListener(new ChangeListener() {\n            @Override\n            public void stateChanged(ChangeEvent e) {\n                JSlider source = (JSlider) e.getSource();\n                cannyThresholdSliderValue = source.getValue();\n                cannyThresholdLabel.setText(\"Min Threshold: \" + cannyThresholdSliderValue);\n                update();\n                Image img = HighGui.toBufferedImage(dst);\n                imgLabel.setIcon(new ImageIcon(img));\n                frame.repaint();\n            }\n        });\n        \n        sliderPanel.add(cannyThresholdLabel);\n        sliderPanel.add(cannyThresholdSlider);\n        \n        frame.getContentPane().add(sliderPanel, BorderLayout.PAGE_END);\n        // Display the window.\n        frame.pack();\n        frame.setLocationRelativeTo(null);\n        frame.setVisible(true);\n        update();\n    }\n\n    private void update() {\n        /// Reduce noise with a kernel 3x3\n        Imgproc.blur(detectedEdges, detectedEdges, BLUR_SIZE);\n\n        /// Canny detector\n        Imgproc.Canny(detectedEdges, detectedEdges, cannyThresholdSliderValue, cannyThresholdSliderValue * RATIO,\n                KERNEL_SIZE, false);\n\n        /// Using Canny's output as a mask, we display our result\n        dst = Mat.zeros(src.size(), src.type());\n        src.copyTo(dst, detectedEdges);\n\n    }\n\n    public static void main(String[] args) {\n        // Load the native OpenCV library\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\n        \n        // Schedule a job for the event dispatch thread:\n        // creating and showing this application's GUI.\n        javax.swing.SwingUtilities.invokeLater(new Runnable() {\n            @Override\n            public void run() {\n                new CannyDetectorDemo(args);\n            }\n        });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Face Detection Inference - OpenCV DNN C++\nDESCRIPTION: This snippet shows how to perform face detection using the initialized FaceDetectorYN object in C++. It runs inference on an input image and stores detection results in a cv::Mat. The main dependency is OpenCV. The primary parameter is the image to process, and the output is a matrix of detected face bounding boxes and landmarks according to the ONNX model output format.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_face/dnn_face.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n// Detect faces in an image\ncv::Mat faces;\ndetector->detect(image, faces);\n// 'faces' is a matrix where each row describes one detection (bounding box + landmarks)\n```\n\n----------------------------------------\n\nTITLE: Calculating Image Histogram Demo in Python (Full Code)\nDESCRIPTION: Complete Python script demonstrating how to load an image, split it into B, G, R channels using `cv.split`, calculate the histogram for each channel using `cv.calcHist`, normalize the results using `cv.normalize`, and display the histograms using Matplotlib or OpenCV drawing functions. Depends on OpenCV Python bindings (`cv2`) and potentially NumPy and Matplotlib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@include samples/python/tutorial_code/Histograms_Matching/histogram_calculation/calcHist_Demo.py\n```\n\n----------------------------------------\n\nTITLE: Accessing Image Shape (Dimensions) using OpenCV in Python\nDESCRIPTION: Demonstrates retrieving the shape of an image using the `img.shape` attribute. It returns a tuple containing the number of rows, columns, and channels (for color images). For grayscale images, the tuple only contains rows and columns, which can be used to check if an image is color or grayscale.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_basic_ops/py_basic_ops.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> print( img.shape )\n(342, 548, 3)\n```\n\n----------------------------------------\n\nTITLE: Implementing Harris Corner Detector in Java\nDESCRIPTION: This Java code demonstrates how to use the Imgproc.cornerHarris function to detect corners in an image using the Harris-Stephens method. It processes the input image, applies the corner detection algorithm, and visualizes the results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/harris_detector/harris_detector.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.*;\nimport org.opencv.highgui.HighGui;\nimport org.opencv.imgcodecs.Imgcodecs;\nimport org.opencv.imgproc.Imgproc;\n\nclass CornerHarrisDemo {\n    private Mat src = new Mat();\n    private Mat srcGray = new Mat();\n    private Mat dst = new Mat();\n    private Mat dstNorm = new Mat();\n    private Mat dstNormScaled = new Mat();\n    private int thresh = 200;\n    private int maxThresh = 255;\n\n    public void run(String[] args) {\n        String filename = args.length > 0 ? args[0] : \"../data/building.jpg\";\n        src = Imgcodecs.imread(filename);\n        if (src.empty()) {\n            System.err.println(\"Cannot read image: \" + filename);\n            System.exit(0);\n        }\n\n        Imgproc.cvtColor(src, srcGray, Imgproc.COLOR_BGR2GRAY);\n\n        HighGui.namedWindow(\"Source image\");\n        HighGui.createTrackbar(\"Threshold: \", \"Source image\", new int[]{thresh}, maxThresh, this::cornerHarris);\n        HighGui.imshow(\"Source image\", src);\n\n        cornerHarris(0, null);\n\n        HighGui.waitKey();\n        System.exit(0);\n    }\n\n    private void cornerHarris(int, Void) {\n        int blockSize = 2;\n        int apertureSize = 3;\n        double k = 0.04;\n\n        Imgproc.cornerHarris(srcGray, dst, blockSize, apertureSize, k);\n\n        Core.normalize(dst, dstNorm, 0, 255, Core.NORM_MINMAX, CvType.CV_32FC1, new Mat());\n        Core.convertScaleAbs(dstNorm, dstNormScaled);\n\n        for (int i = 0; i < dstNorm.rows(); i++) {\n            for (int j = 0; j < dstNorm.cols(); j++) {\n                if ((int) dstNorm.get(i, j)[0] > thresh) {\n                    Imgproc.circle(dstNormScaled, new Point(j, i), 5, new Scalar(0), 2, 8, 0);\n                }\n            }\n        }\n\n        HighGui.imshow(\"Corners detected\", dstNormScaled);\n    }\n}\n\npublic class CornerHarrisDemoRun {\n    public static void main(String[] args) {\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\n        new CornerHarrisDemo().run(args);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setup Image in OpenCV Java\nDESCRIPTION: In Java, reads an image, converts it to grayscale, and applies a Gaussian blur using OpenCV. OpenCV library is required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nMat src = Imgcodecs.imread(\"image.jpg\");\nMat gray = new Mat();\nImgproc.cvtColor(src, gray, Imgproc.COLOR_BGR2GRAY);\nImgproc.blur(gray, gray, new Size(3, 3));\n```\n\n----------------------------------------\n\nTITLE: Creating Display Windows in C++\nDESCRIPTION: Sets up windows to display both the captured frames and the thresholded frames using OpenCV in C++. This allows users to visualize input and processed images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold_inRange/threshold_inRange.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ncv::namedWindow(\"Display Image\", cv::WINDOW_AUTOSIZE);\n```\n\n----------------------------------------\n\nTITLE: Selecting and Copying an Image ROI using NumPy Slicing in Python\nDESCRIPTION: Shows how to select a rectangular Region of Interest (ROI) using NumPy array slicing (`img[start_row:end_row, start_col:end_col]`). In this example, a section containing a ball is selected and then copied to another location within the same image using another slice assignment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_basic_ops/py_basic_ops.markdown#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> ball = img[280:340, 330:390]\n>>> img[273:333, 100:160] = ball\n```\n\n----------------------------------------\n\nTITLE: Accessing a Single Image Channel using NumPy Slicing in Python\nDESCRIPTION: Shows an alternative, often faster method to access a single image channel (in this case, the Blue channel, index 0) using NumPy array slicing (`img[:, :, channel_index]`) instead of the `cv.split()` function. This accesses all rows and columns for the specified channel index.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_basic_ops/py_basic_ops.markdown#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> b = img[:,:,0]\n```\n\n----------------------------------------\n\nTITLE: Calculating Aspect Ratio using OpenCV.js\nDESCRIPTION: Calculates the aspect ratio of a contour, defined as the ratio of the width to the height of its bounding rectangle. Requires a contour (`cnt`) as input. Uses `cv.boundingRect` to get the rectangle dimensions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contour_properties/js_contour_properties.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nlet rect = cv.boundingRect(cnt);\nlet aspectRatio = rect.width / rect.height;\n```\n\n----------------------------------------\n\nTITLE: Loading and Transforming Image in C++ with OpenCV\nDESCRIPTION: This snippet demonstrates how to load an image and apply an affine transformation and rotation using OpenCV in C++. It includes calculating a transform matrix from point sets and applying it to the image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/warp_affine/warp_affine.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n@snippet samples/cpp/tutorial_code/ImgTrans/Geometric_Transforms_Demo.cpp Load the image\n```\n\n----------------------------------------\n\nTITLE: Detecting Vertical Lines using Morphology in OpenCV (C++/Java/Python)\nDESCRIPTION: Detects vertical lines in the binary image. A vertical structuring element (kernel) is created using `getStructuringElement` with `MORPH_RECT`. The size is chosen based on the expected line height. Erosion followed by dilation (morphological opening) is applied using this kernel to isolate vertical structures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection.md#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n//![vert]\n// Specify size on vertical axis\nint vertical_size = vertical.rows / 30;\n\n// Create structure element for extracting vertical lines through morphology operations\nMat verticalStructure = getStructuringElement(MORPH_RECT, Size(1, vertical_size));\n\n// Apply morphology operations\nerode(vertical, vertical, verticalStructure, Point(-1, -1));\ndilate(vertical, vertical, verticalStructure, Point(-1, -1));\n\n// Show extracted vertical lines\nshow_wait_destroy(\"vertical\", vertical);\n//![vert]\n```\n\nLANGUAGE: java\nCODE:\n```\n//![vert]\n// Specify size on vertical axis\nint vertical_size = vertical.rows() / 30;\n\n// Create structure element for extracting vertical lines through morphology operations\nMat verticalStructure = Imgproc.getStructuringElement(Imgproc.MORPH_RECT, new Size( 1, vertical_size));\n\n// Apply morphology operations\nImgproc.erode(vertical, vertical, verticalStructure, new Point(-1, -1));\nImgproc.dilate(vertical, vertical, verticalStructure, new Point(-1, -1));\n\n// Show extracted vertical lines\nshowWaitDestroy(\"vertical\", vertical);\n//![vert]\n```\n\nLANGUAGE: python\nCODE:\n```\n#![vert]\n# Specify size on vertical axis\nrows = vertical.shape[0]\nverticalsize = rows // 30\n\n# Create structure element for extracting vertical lines through morphology operations\nverticalStructure = cv.getStructuringElement(cv.MORPH_RECT, (1, verticalsize))\n\n# Apply morphology operations\nvertical = cv.erode(vertical, verticalStructure)\nvertical = cv.dilate(vertical, verticalStructure)\n\n# Show extracted vertical lines\nshow_wait_destroy(\"vertical\", vertical)\n#![vert]\n```\n\n----------------------------------------\n\nTITLE: Triggering the Model Conversion Script - Console\nDESCRIPTION: This command runs a Python module that converts a PyTorch ResNet-50 model to ONNX format. It leverages the module system via the '-m' flag, invoking the conversion for reproducibility. Ensure all dependencies are installed and that the module path matches the project structure.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50_onnx\n```\n\n----------------------------------------\n\nTITLE: Drawing Detected Lines from Hough Transform in OpenCV (C++)\nDESCRIPTION: This C++ snippet iterates over (rho, theta) lines detected by HoughLines and draws them on an output image. For each line, it calculates points for drawing and uses cv::line. Inputs are the lines vector and output image; output is an image with detected lines visualized. Requires OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nfor( size_t i = 0; i < lines.size(); i++ )\\n{\\n  float rho = lines[i][0], theta = lines[i][1];\\n  Point pt1, pt2;\\n  double a = cos(theta), b = sin(theta);\\n  double x0 = a*rho, y0 = b*rho;\\n  pt1.x = cvRound(x0 + 1000*(-b));\\n  pt1.y = cvRound(y0 + 1000*(a));\\n  pt2.x = cvRound(x0 - 1000*(-b));\\n  pt2.y = cvRound(y0 - 1000*(a));\\n  line( cdst, pt1, pt2, Scalar(0,0,255), 3, LINE_AA);\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Inpainting with OpenCV in Python\nDESCRIPTION: This snippet demonstrates how to apply inpainting using the OpenCV library in Python. It uses a mask to specify the regions of the image that need to be restored, employing the Fast Marching Method by setting the flag cv.INPAINT_TELEA. Key dependencies include numpy and OpenCV libraries, and the input includes a degraded image and a corresponding mask. The output is displayed using OpenCV's imshow function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_photo/py_inpainting/py_inpainting.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nimg = cv.imread('messi_2.jpg')\nmask = cv.imread('mask2.png', cv.IMREAD_GRAYSCALE)\n\ndst = cv.inpaint(img,mask,3,cv.INPAINT_TELEA)\n\ncv.imshow('dst',dst)\ncv.waitKey(0)\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Parallel Convolution Implementation using OpenCV\nDESCRIPTION: This snippet illustrates a parallel implementation of convolution using OpenCV's parallel_for_ framework. It demonstrates splitting the image into stripes and applying convolution concurrently, optimizing performance by leveraging multiple processor cores. The example provides custom class definition for handling the parallel execution.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_new/how_to_use_OpenCV_parallel_for_new.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_use_OpenCV_parallel_for_new.cpp convolution-parallel\n```\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_use_OpenCV_parallel_for_new.cpp overload-full\n```\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_use_OpenCV_parallel_for_new.cpp convolution-parallel-function\n```\n\n----------------------------------------\n\nTITLE: Saving Video with OpenCV in Python\nDESCRIPTION: This snippet demonstrates how to capture video from a camera, flip each frame vertically, and save it to a file using OpenCV. It uses cv.VideoWriter() to create an output video file with specified codec, FPS, and frame size.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_video_display/py_video_display.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\ncap = cv.VideoCapture(0)\n\n# Define the codec and create VideoWriter object\nfourcc = cv.VideoWriter_fourcc(*'XVID')\nout = cv.VideoWriter('output.avi', fourcc, 20.0, (640,  480))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        print(\"Can't receive frame (stream end?). Exiting ...\")\n        break\n    frame = cv.flip(frame, 0)\n\n    # write the flipped frame\n    out.write(frame)\n\n    cv.imshow('frame', frame)\n    if cv.waitKey(1) == ord('q'):\n        break\n\n# Release everything if job is finished\ncap.release()\nout.release()\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Setting up Trackbars for Threshold Parameters (Python)\nDESCRIPTION: In Python, this code creates two OpenCV trackbars on the thresholding result window for type and value, each triggering Threshold_Demo when changed. It enables real-time UI parameter tuning without restarting the program. Depends on cv2.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n# [trackbar]\\ncv.createTrackbar('Type', window_name, 3, max_type, Threshold_Demo)\\ncv.createTrackbar('Value', window_name, 0, max_value, Threshold_Demo)\\n# [trackbar]\n```\n\n----------------------------------------\n\nTITLE: Implementing Shi-Tomasi Corner Detection using goodFeaturesToTrack() in OpenCV Python\nDESCRIPTION: This code demonstrates how to detect corners in an image using the Shi-Tomasi method with OpenCV's goodFeaturesToTrack() function. It loads an image, converts it to grayscale, finds the 25 best corners with a quality threshold of 0.01 and minimum distance of 10 pixels, and visualizes the results by drawing circles at corner positions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_shi_tomasi/py_shi_tomasi.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('blox.jpg')\ngray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n\ncorners = cv.goodFeaturesToTrack(gray,25,0.01,10)\ncorners = np.int0(corners)\n\nfor i in corners:\n    x,y = i.ravel()\n    cv.circle(img,(x,y),3,255,-1)\n\nplt.imshow(img),plt.show()\n```\n\n----------------------------------------\n\nTITLE: Applying Blur Filter with OpenCV in Python\nDESCRIPTION: This Python snippet demonstrates using OpenCV's blur() function to apply a normalized box filter for smoothing images. Dependencies include OpenCV, and it takes image parameters, kernel size, and anchor point.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/Smoothing/smoothing.py blur\n```\n\n----------------------------------------\n\nTITLE: Simple Thresholding Implementation in OpenCV Python\nDESCRIPTION: Demonstrates different types of simple thresholding techniques using cv.threshold function. Compares BINARY, BINARY_INV, TRUNC, TOZERO, and TOZERO_INV thresholding methods with visualization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_thresholding/py_thresholding.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('gradient.png', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\nret,thresh2 = cv.threshold(img,127,255,cv.THRESH_BINARY_INV)\nret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)\nret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)\nret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)\n\ntitles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\nimages = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n\nfor i in range(6):\n    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)\n    plt.title(titles[i])\n    plt.xticks([]),plt.yticks([])\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Complete Example: Loading, Processing, and Displaying Image with OpenCV.js (HTML/JavaScript)\nDESCRIPTION: Provides a full HTML page integrating all previous steps. It includes HTML for file input and image/canvas display, JavaScript for handling file selection, asynchronously loading OpenCV.js (`opencv.js`), updating a status message via the Emscripten `Module.onRuntimeInitialized` callback, reading the uploaded image into a `cv.Mat` using `cv.imread` upon image load, displaying the `cv.Mat` on a canvas using `cv.imshow`, and explicitly releasing the memory allocated for the `cv.Mat` using `mat.delete()`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_usage/js_usage.markdown#2025-04-22_snippet_6\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html>\n<head>\n<meta charset=\"utf-8\">\n<title>Hello OpenCV.js</title>\n</head>\n<body>\n<h2>Hello OpenCV.js</h2>\n<p id=\"status\">OpenCV.js is loading...</p>\n<div>\n  <div class=\"inputoutput\">\n    <img id=\"imageSrc\" alt=\"No Image\" />\n    <div class=\"caption\">imageSrc <input type=\"file\" id=\"fileInput\" name=\"file\" /></div>\n  </div>\n  <div class=\"inputoutput\">\n    <canvas id=\"canvasOutput\" ></canvas>\n    <div class=\"caption\">canvasOutput</div>\n  </div>\n</div>\n<script type=\"text/javascript\">\nlet imgElement = document.getElementById('imageSrc');\nlet inputElement = document.getElementById('fileInput');\ninputElement.addEventListener('change', (e) => {\n  imgElement.src = URL.createObjectURL(e.target.files[0]);\n}, false);\n\nimgElement.onload = async function() {\n  cv = (cv instanceof Promise) ? await cv : cv;\n  let mat = cv.imread(imgElement);\n  cv.imshow('canvasOutput', mat);\n  mat.delete();\n};\n\nvar Module = {\n  // https://emscripten.org/docs/api_reference/module.html#Module.onRuntimeInitialized\n  onRuntimeInitialized() {\n    document.getElementById('status').innerHTML = 'OpenCV.js is ready.';\n  }\n};\n</script>\n<script async src=\"opencv.js\" type=\"text/javascript\"></script>\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Loading an Image from File in Grayscale with OpenCV in Python\nDESCRIPTION: Illustrates image loading in grayscale via imread with cv2.IMREAD_GRAYSCALE in Python OpenCV. Requires opencv-python package. File path is input; single-channel NumPy array is output. Particularly useful for processing algorithms needing intensity only.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimg_gray = cv2.imread('my_image.jpg', cv2.IMREAD_GRAYSCALE)\n```\n\n----------------------------------------\n\nTITLE: Extracting Face Features for Recognition - OpenCV DNN C++\nDESCRIPTION: This snippet shows how to extract features from a face image using a FaceRecognizerSF object in C++. After detecting and cropping the face region, the extractFeature method is called. It requires the input image and face bounding box. The method returns a floating-point vector (feature) representing the face for downstream comparison.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_face/dnn_face.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n// Extract features for recognition\ncv::Mat faceFeature;\nrecognizer->feature(image, faceBox, faceFeature); // or extractFeature()\n// 'faceFeature' now contains the vector representation of the face\n```\n\n----------------------------------------\n\nTITLE: Detecting Faces and Eyes with Haar Cascades in Java using OpenCV\nDESCRIPTION: This Java code snippet utilizes OpenCV's Java bindings to perform face and eye detection. It initializes `CascadeClassifier` objects and loads the necessary Haar cascade XML files using the `load` method. The code captures frames from a video source, converts the frame to grayscale, and uses the `detectMultiScale` method to identify face locations. Subsequently, it searches for eyes within the detected face regions using another `detectMultiScale` call. Rectangles are drawn around the detected faces and eyes before displaying the output. Requires the OpenCV Java library, the native OpenCV library to be loaded, and the specified Haar cascade XML files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/cascade_classifier.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// This tutorial code's is shown lines below. You can also download it from\n// [here](https://github.com/opencv/opencv/tree/4.x/samples/java/tutorial_code/objectDetection/cascade_classifier/ObjectDetectionDemo.java)\n@include samples/java/tutorial_code/objectDetection/cascade_classifier/ObjectDetectionDemo.java\n```\n\n----------------------------------------\n\nTITLE: Applying Sobel Operator in Python\nDESCRIPTION: This Python code snippet shows how to apply the Sobel operator using OpenCV to detect edges in an image. Key steps include reading an image, noise reduction with Gaussian blur, converting to grayscale, applying the Sobel function for gradients in both directions, and displaying the result using matplotlib. OpenCV and matplotlib are required dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/sobel_derivatives/sobel_derivatives.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimg = cv2.imread('lena.jpg', 0) // Load source image\n```\n\nLANGUAGE: python\nCODE:\n```\nimg = cv2.GaussianBlur(img, (3, 3), 0) // Reduce noise\n```\n\nLANGUAGE: python\nCODE:\n```\nsobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3) // Apply Sobel for x-gradient\nsobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3) // Apply Sobel for y-gradient\n```\n\nLANGUAGE: python\nCODE:\n```\nabs_sobelx = cv2.convertScaleAbs(sobelx) \nabs_sobely = cv2.convertScaleAbs(sobely)\n```\n\nLANGUAGE: python\nCODE:\n```\ngradient = cv2.addWeighted(abs_sobelx, 0.5, abs_sobely, 0.5, 0) // Approximate the gradient\n```\n\nLANGUAGE: python\nCODE:\n```\nplt.imshow(gradient, cmap='gray') // Display results\nplt.title('Sobel Demo')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Performing Canny Edge Detection with OpenCV in Python\nDESCRIPTION: This code snippet demonstrates how to use OpenCV's cv.Canny() function to perform edge detection on a grayscale image. It also shows how to display the original and edge-detected images side by side using matplotlib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_canny/py_canny.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('messi5.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nedges = cv.Canny(img,100,200)\n\nplt.subplot(121),plt.imshow(img,cmap = 'gray')\nplt.title('Original Image'), plt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(edges,cmap = 'gray')\nplt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Simple Mouse Event Handling for Circle Drawing in OpenCV with Python\nDESCRIPTION: This code snippet shows a simple application that draws a circle on an image where the user double-clicks. It demonstrates the use of cv.setMouseCallback() to handle mouse events and cv.circle() to draw shapes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_mouse_handling/py_mouse_handling.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\n# mouse callback function\ndef draw_circle(event,x,y,flags,param):\n    if event == cv.EVENT_LBUTTONDBLCLK:\n        cv.circle(img,(x,y),100,(255,0,0),-1)\n\n# Create a black image, a window and bind the function to window\nimg = np.zeros((512,512,3), np.uint8)\ncv.namedWindow('image')\ncv.setMouseCallback('image',draw_circle)\n\nwhile(1):\n    cv.imshow('image',img)\n    if cv.waitKey(20) & 0xFF == 27:\n        break\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Detecting Barcodes using OpenCV C++\nDESCRIPTION: Illustrates the use of cv::barcode::BarcodeDetector::detect in OpenCV, which leverages gradient coherence to identify barcodes. It employs multiscale patch analysis to detect barcodes of various sizes. The output is a set of rectangles outlining detected barcodes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/barcode_detect_and_decode.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/barcode.cpp detect\n```\n\n----------------------------------------\n\nTITLE: Gradient Structure Tensor Calculation in Python\nDESCRIPTION: This Python code segment performs anisotropic image segmentation using gradient structure tensor. OpenCV is a required library. Key operations are the same as in C++: calculating orientation and coherency, thresholding, and result combination. Input involves image data and window size, resulting in a segmented image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/anisotropic_image_segmentation/anisotropic_image_segmentation.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n@add_toggle_python\n    @include samples/python/tutorial_code/imgProc/anisotropic_image_segmentation/anisotropic_image_segmentation.py\n@end_toggle\n```\n\nLANGUAGE: Python\nCODE:\n```\n@add_toggle_python\n    @snippet samples/python/tutorial_code/imgProc/anisotropic_image_segmentation/anisotropic_image_segmentation.py main\n@end_toggle\n```\n\nLANGUAGE: Python\nCODE:\n```\n@add_toggle_python\n    @snippet samples/python/tutorial_code/imgProc/anisotropic_image_segmentation/anisotropic_image_segmentation.py calcGST\n@end_toggle\n```\n\nLANGUAGE: Python\nCODE:\n```\n@add_toggle_python\n    @snippet samples/python/tutorial_code/imgProc/anisotropic_image_segmentation/anisotropic_image_segmentation.py thresholding\n@end_toggle\n```\n\nLANGUAGE: Python\nCODE:\n```\n@add_toggle_python\n    @snippet samples/python/tutorial_code/imgProc/anisotropic_image_segmentation/anisotropic_image_segmentation.py combining\n@end_toggle\n```\n\n----------------------------------------\n\nTITLE: Implementing Canny Edge Detector in Python\nDESCRIPTION: Complete implementation of the Canny Edge Detector in Python using OpenCV. This code creates a window with a trackbar to adjust the lower threshold for the Canny algorithm, applies the detector to an input image, and displays the result showing edges overlaid on the original image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import print_function\nimport cv2 as cv\nimport argparse\n\nmax_lowThreshold = 100\nwindow_name = 'Edge Map'\nratio = 3\nkernel_size = 3\n\ndef CannyThreshold(val):\n    low_threshold = val\n    img_blur = cv.blur(src_gray, (3,3))\n    detected_edges = cv.Canny(img_blur, low_threshold, low_threshold*ratio, kernel_size)\n    mask = detected_edges != 0\n    dst = src * (mask[:,:,None].astype(src.dtype))\n    cv.imshow(window_name, dst)\n\nparser = argparse.ArgumentParser(description='Code for Canny Edge Detector tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='fruits.jpg')\nargs = parser.parse_args()\n\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n\ncv.namedWindow(window_name)\ncv.createTrackbar('Min Threshold:', window_name, 0, max_lowThreshold, CannyThreshold)\n\nCannyThreshold(0)\n\ncv.waitKey()\n```\n\n----------------------------------------\n\nTITLE: Establishing Histogram Bin Count in Java\nDESCRIPTION: Java snippet defining the number of bins for the histogram calculation. A `MatOfInt` object named `histSize` is created and initialized with a single value (e.g., 256), specifying the number of bins for the histogram.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/Histograms_Matching/histogram_calculation/CalcHistDemo.java Establish the number of bins\n```\n\n----------------------------------------\n\nTITLE: Including Necessary Headers in OpenCV C++\nDESCRIPTION: This snippet demonstrates how to include necessary headers for using OpenCV in a C++ project. It covers the inclusion of core, imgcodecs, and highgui modules, along with the iostream for console input/output operations. Using namespace cv is declared for ease of access to library functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/display_image/display_image.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/core.hpp>\n#include <opencv2/imgcodecs.hpp>\n#include <opencv2/highgui.hpp>\n#include <iostream>\n\nusing namespace cv;\n```\n\n----------------------------------------\n\nTITLE: Calculating Re-projection Error for Camera Calibration in OpenCV with Python\nDESCRIPTION: This code calculates the re-projection error to evaluate the accuracy of camera calibration parameters. It projects 3D object points to 2D image points using the calibration parameters and compares them with the actual detected image points to compute the mean error.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_calibration/py_calibration.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nmean_error = 0\nfor i in range(len(objpoints)):\n    imgpoints2, _ = cv.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)\n    error = cv.norm(imgpoints[i], imgpoints2, cv.NORM_L2)/len(imgpoints2)\n    mean_error += error\n\nprint( \"total error: {}\".format(mean_error/len(objpoints)) )\n```\n\n----------------------------------------\n\nTITLE: Thresholding Operations Initialization in OpenCV (Java, Full Example)\nDESCRIPTION: This Java code example achieves thresholding through image loading, grayscale conversion (if needed), and real-time interactivity via graphical sliders for threshold type and value using OpenCV's Java API. It defines listeners for slider events to update the displayed thresholded image automatically. The code expects OpenCV 3.0+ and appropriate Java bindings. Input is the path to the image and user actions on the sliders; output is a live window reflecting threshold changes. Window and trackbar management, as well as type/value mappings, are shown in the main method and associated callbacks.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.*;\\nimport org.opencv.highgui.*;\\nimport org.opencv.imgproc.*;\\n\\npublic class Threshold {\\n    static Mat src, src_gray, dst;\\n    static int threshold_value = 0;\\n    static int threshold_type = 3;\\n    static final int max_value = 255;\\n    static final int max_type = 4;\\n    static final int max_BINARY_value = 255;\\n    static String window_name = \"Threshold Demo\";\\n\\n    public static void main(String[] args) {\\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\\n        src = Highgui.imread(args.length > 0 ? args[0] : \"chicky_512.png\");\\n        if (src.empty()) {\\n            System.out.println(\"Error opening image\");\\n            return;\\n        }\\n        Imgproc.cvtColor(src, src_gray = new Mat(), Imgproc.COLOR_BGR2GRAY);\\n        HighGui.namedWindow(window_name);\\n\\n        // Create Trackbars\\n        HighGui.createTrackbar(\"Type:\\n 0:Binary \\n 1:BinaryInv \\n 2:Trunc \\n 3:ToZero \\n 4:ToZeroInv\", window_name, new int[]{threshold_type}, max_type, (pos)->{\\n            threshold_type = pos;\\n            update();\\n        });\\n        HighGui.createTrackbar(\"Value\", window_name, new int[]{threshold_value}, max_value, (pos)->{\\n            threshold_value = pos;\\n            update();\\n        });\\n        update();\\n        HighGui.waitKey(0);\\n    }\\n\\n    static void update() {\\n        Imgproc.threshold(src_gray, dst = new Mat(), threshold_value, max_BINARY_value, threshold_type);\\n        HighGui.imshow(window_name, dst);\\n    }\\n}\n```\n\n----------------------------------------\n\nTITLE: Normalizing Template Matching Results (Python)\nDESCRIPTION: Applies normalization to the result matrix from `cv2.matchTemplate` using `cv2.normalize`. It scales the correlation values to the range 0-1 using the `cv2.NORM_MINMAX` method, which aids in consistent visualization and thresholding.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/match_template/match_template.py normalize\n```\n\n----------------------------------------\n\nTITLE: Selecting Region of Interest (ROI) with OpenCV in Java\nDESCRIPTION: Shows extracting a submatrix ROI from an image using Java's OpenCV Mat.submat method. Input is a Mat and region bounds. Submat shares storage with original image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_32\n\nLANGUAGE: Java\nCODE:\n```\nMat roi = img.submat(y, y + h, x, x + w);\n```\n\n----------------------------------------\n\nTITLE: Pose Estimation with RANSAC in PnPProblem Using C++\nDESCRIPTION: Implements the estimatePoseRANSAC function of the PnPProblem class to calculate rotation and translation matrices through RANSAC. It requires a set of 2D/3D correspondences, selected PnP method, output inliers container, and RANSAC parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\n// Estimate the pose given a list of 2D/3D correspondences with RANSAC and the method to use\n\nvoid PnPProblem::estimatePoseRANSAC( const std::vector<cv::Point3f> &list_points3d,        // list with model 3D coordinates\n                                     const std::vector<cv::Point2f> &list_points2d,        // list with scene 2D coordinates\n                                     int flags, cv::Mat &inliers, int iterationsCount,     // PnP method; inliers container\n                                     float reprojectionError, float confidence )           // RANSAC parameters\n{\n    cv::Mat distCoeffs = cv::Mat::zeros(4, 1, CV_64FC1);    // vector of distortion coefficients\n    cv::Mat rvec = cv::Mat::zeros(3, 1, CV_64FC1);          // output rotation vector\n    cv::Mat tvec = cv::Mat::zeros(3, 1, CV_64FC1);          // output translation vector\n\n    bool useExtrinsicGuess = false;   // if true the function uses the provided rvec and tvec values as\n                                      // initial approximations of the rotation and translation vectors\n\n    cv::solvePnPRansac( list_points3d, list_points2d, _A_matrix, distCoeffs, rvec, tvec,\n                        useExtrinsicGuess, iterationsCount, reprojectionError, confidence,\n                        inliers, flags );\n\n    Rodrigues(rvec,_R_matrix);                   // converts Rotation Vector to Matrix\n    _t_matrix = tvec;                            // set translation matrix\n\n    this->set_P_matrix(_R_matrix, _t_matrix);    // set rotation-translation matrix\n\n}\n```\n\n----------------------------------------\n\nTITLE: Converting BGR Image to Grayscale with OpenCV in Java\nDESCRIPTION: Shows converting a 3-channel BGR image to grayscale using Imgproc.cvtColor in Java OpenCV. Input/Output are Mat objects. Grayscale output has single channel.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_35\n\nLANGUAGE: Java\nCODE:\n```\nMat gray = new Mat();\\nImgproc.cvtColor(img, gray, Imgproc.COLOR_BGR2GRAY);\n```\n\n----------------------------------------\n\nTITLE: Marker Creation for Watershed Algorithm\nDESCRIPTION: Creates and labels markers for watershed segmentation using connected components. Marks known regions with different positive integers and unknown regions with zero.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_watershed/py_watershed.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Marker labelling\nret, markers = cv.connectedComponents(sure_fg)\n\n# Add one to all labels so that sure background is not 0, but 1\nmarkers = markers+1\n\n# Now, mark the region of unknown with zero\nmarkers[unknown==255] = 0\n```\n\n----------------------------------------\n\nTITLE: Reading and Initializing ONNX Model using OpenCV DNN - C++\nDESCRIPTION: This C++ statement uses OpenCV's deep learning module to read an ONNX model into the Net object. It optionally accepts config and framework strings. The 'model' variable should be an ONNX file path, matching prior export output. This is the initial step before preprocessing and inference.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nNet net = readNet(model, config, framework);\n```\n\n----------------------------------------\n\nTITLE: Drawing Detected Lines from Hough Transform in OpenCV (Java)\nDESCRIPTION: This Java snippet draws lines found by HoughLines onto an output image using OpenCV Java drawing functions. For each line in the Mat, it computes the line endpoints from (rho, theta) and uses Imgproc.line to visualize it. Requires output Mat and valid lines Mat.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nfor (int i = 0; i < lines.rows(); i++) {\\n  double[] data = lines.get(i, 0);\\n  double rho = data[0], theta = data[1];\\n  double a = Math.cos(theta), b = Math.sin(theta);\\n  double x0 = a*rho, y0 = b*rho;\\n  Point pt1 = new Point(Math.round(x0 + 1000*(-b)), Math.round(y0 + 1000*(a)));\\n  Point pt2 = new Point(Math.round(x0 - 1000*(-b)), Math.round(y0 - 1000*(a)));\\n  Imgproc.line(color_dst, pt1, pt2, new Scalar(0, 0, 255), 3, Imgproc.LINE_AA);\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Feature Matching with FLANN in Java\nDESCRIPTION: This Java implementation demonstrates how to use the FlannBasedMatcher for efficient feature matching with SURF descriptors. It loads two images, detects and describes features, matches them using FLANN, and filters the matches using Lowe's distance ratio test.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_flann_matcher/feature_flann_matcher.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.opencv.core.Core;\nimport org.opencv.core.CvType;\nimport org.opencv.core.DMatch;\nimport org.opencv.core.Mat;\nimport org.opencv.core.MatOfByte;\nimport org.opencv.core.MatOfDMatch;\nimport org.opencv.core.MatOfKeyPoint;\nimport org.opencv.core.Scalar;\nimport org.opencv.features2d.DescriptorMatcher;\nimport org.opencv.features2d.Features2d;\nimport org.opencv.highgui.HighGui;\nimport org.opencv.imgcodecs.Imgcodecs;\nimport org.opencv.xfeatures2d.SURF;\n\nclass SURFFLANNMatchingDemo {\n    public void run(String[] args) {\n        String filename1 = args.length > 1 ? args[0] : \"../data/box.png\";\n        String filename2 = args.length > 1 ? args[1] : \"../data/box_in_scene.png\";\n        Mat img1 = Imgcodecs.imread(filename1, Imgcodecs.IMREAD_GRAYSCALE);\n        Mat img2 = Imgcodecs.imread(filename2, Imgcodecs.IMREAD_GRAYSCALE);\n        if (img1.empty() || img2.empty()) {\n            System.err.println(\"Cannot read images!\");\n            System.exit(0);\n        }\n\n        //-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\n        double hessianThreshold = 400;\n        int nOctaves = 4, nOctaveLayers = 3;\n        boolean extended = false, upright = false;\n        SURF detector = SURF.create(hessianThreshold, nOctaves, nOctaveLayers, extended, upright);\n        MatOfKeyPoint keypoints1 = new MatOfKeyPoint(), keypoints2 = new MatOfKeyPoint();\n        Mat descriptors1 = new Mat(), descriptors2 = new Mat();\n        detector.detectAndCompute(img1, new Mat(), keypoints1, descriptors1);\n        detector.detectAndCompute(img2, new Mat(), keypoints2, descriptors2);\n\n        //-- Step 2: Matching descriptor vectors using FLANN matcher\n        DescriptorMatcher matcher = DescriptorMatcher.create(DescriptorMatcher.FLANNBASED);\n        List<MatOfDMatch> knnMatches = new ArrayList<>();\n        matcher.knnMatch(descriptors1, descriptors2, knnMatches, 2);\n\n        //-- Filter matches using the Lowe's ratio test\n        float ratioThresh = 0.7f;\n        List<DMatch> listOfGoodMatches = new ArrayList<>();\n        for (int i = 0; i < knnMatches.size(); i++) {\n            if (knnMatches.get(i).rows() > 1) {\n                DMatch[] matches = knnMatches.get(i).toArray();\n                if (matches[0].distance < ratioThresh * matches[1].distance) {\n                    listOfGoodMatches.add(matches[0]);\n                }\n            }\n        }\n        MatOfDMatch goodMatches = new MatOfDMatch();\n        goodMatches.fromList(listOfGoodMatches);\n\n        //-- Draw matches\n        Mat imgMatches = new Mat();\n        Features2d.drawMatches(img1, keypoints1, img2, keypoints2, goodMatches, imgMatches, Scalar.all(-1),\n                Scalar.all(-1), new MatOfByte(), Features2d.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS);\n\n        //-- Show detected matches\n        HighGui.imshow(\"Good Matches\", imgMatches);\n        HighGui.waitKey(0);\n\n        System.exit(0);\n    }\n}\n\npublic class SURFFLANNMatchingDemo {\n    public static void main(String[] args) {\n        // Load the native OpenCV library\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\n\n        new SURFFLANNMatchingDemo().run(args);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Canny Edge Detector in C++\nDESCRIPTION: Complete implementation of the Canny Edge Detector in C++ using OpenCV. This code creates a window with a trackbar to adjust the lower threshold for the Canny algorithm, applies the detector to an input image, and displays the result showing edges overlaid on the original image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/imgproc.hpp>\n#include <opencv2/highgui.hpp>\n#include <iostream>\n\nusing namespace cv;\nusing namespace std;\n\n//![variables]\nMat src, src_gray;\nMat dst, detected_edges;\n\nint lowThreshold = 0;\nconst int max_lowThreshold = 100;\nconst int ratio = 3;\nconst int kernel_size = 3;\nconst char* window_name = \"Edge Map\";\n//![variables]\n\nstatic void CannyThreshold(int, void*)\n{\n    //![reduce_noise]\n    /// Reduce noise with a kernel 3x3\n    blur( src_gray, detected_edges, Size(3,3) );\n    //![reduce_noise]\n\n    //![canny]\n    /// Canny detector\n    Canny( detected_edges, detected_edges, lowThreshold, lowThreshold*ratio, kernel_size );\n    //![canny]\n\n    //![fill]\n    /// Using Canny's output as a mask, we display our result\n    dst = Scalar::all(0);\n    //![fill]\n\n    //![copyto]\n    src.copyTo( dst, detected_edges);\n    //![copyto]\n\n    //![display]\n    imshow( window_name, dst );\n    //![display]\n}\n\nint main( int argc, char** argv )\n{\n    //![load]\n    CommandLineParser parser( argc, argv, \"{@input | fruits.jpg | input image}\" );\n    src = imread( samples::findFile( parser.get<String>( \"@input\" ) ), IMREAD_COLOR );\n\n    if( src.empty() )\n    {\n        std::cout << \"Could not open or find the image!\\n\" << std::endl;\n        std::cout << \"Usage: \" << argv[0] << \" <Input image>\" << std::endl;\n        return -1;\n    }\n    //![load]\n\n    //![create_mat]\n    /// Create a matrix of the same type and size as src (for dst)\n    dst.create( src.size(), src.type() );\n    //![create_mat]\n\n    //![convert_to_gray]\n    cvtColor( src, src_gray, COLOR_BGR2GRAY );\n    //![convert_to_gray]\n\n    //![create_window]\n    namedWindow( window_name, WINDOW_AUTOSIZE );\n    //![create_window]\n\n    //![create_trackbar]\n    /// Create a Trackbar for user to enter threshold\n    createTrackbar( \"Min Threshold:\", window_name, &lowThreshold, max_lowThreshold, CannyThreshold );\n    //![create_trackbar]\n\n    /// Show the image\n    CannyThreshold(0, 0);\n\n    /// Wait until user exit program by pressing a key\n    waitKey(0);\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Homography Using OpenCV in C++\nDESCRIPTION: This C++ code sample calculates the homography matrix using OpenCV for stitching operations. Dependencies include the OpenCV library. Inputs are camera parameters and image views, and the output is the homography matrix necessary for stitching.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_34\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\n\nvoid computeHomography(cv::Mat image1, cv::Mat image2) {\n    // Code to compute homography\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Foreground Mask with apply() Method in OpenCV.js - JavaScript\nDESCRIPTION: Illustrates how to obtain the foreground mask from a video frame using the apply() method of BackgroundSubtractorMOG2 in OpenCV.js. Requires two cv.Mat objects: 'image' (input frame in [0,255] pixel range) and 'fgmask' (output mask). The optional 'learningRate' parameter tunes the adaptation speed of the background model. Outputs an 8-bit binary mask; remember to properly initialize 'image' and 'fgmask' before use.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_video/js_bg_subtraction/js_bg_subtraction.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Assume bgSubtractor, 'image', and 'fgmask' (both cv.Mat) are already initialized\nlet learningRate = -1; // use -1 for auto learning rate\n// Process the next frame\bgSubtractor.apply(image, fgmask, learningRate);\n// 'fgmask' now contains the binary foreground mask\n```\n\n----------------------------------------\n\nTITLE: Allocating/Resizing Output Mat using create() in OpenCV in C++\nDESCRIPTION: Creates or resizes a cv::Mat output buffer with desired size/type using create in C++. This function allocates storage when needed and preserves storage if already appropriately sized. Input is output Mat and target size/type. Avoids unnecessary reallocation. No copy of input data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_25\n\nLANGUAGE: C++\nCODE:\n```\noutput.create(input.rows, input.cols, input.type());\n```\n\n----------------------------------------\n\nTITLE: Robust Matching Function Implementation in C++\nDESCRIPTION: Implements the robustMatch function of the RobustMatcher class to match descriptor pairs, apply a ratio test, and ensure symmetry in matched pairs to filter for robustness.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nvoid RobustMatcher::robustMatch( const cv::Mat& frame, std::vector<cv::DMatch>& good_matches,\n                                 std::vector<cv::KeyPoint>& keypoints_frame,\n                                 const std::vector<cv::KeyPoint>& keypoints_model, const cv::Mat& descriptors_model )\n{\n\n    // 1a. Detection of the ORB features\n    this->computeKeyPoints(frame, keypoints_frame);\n\n    // 1b. Extraction of the ORB descriptors\n    cv::Mat descriptors_frame;\n    this->computeDescriptors(frame, keypoints_frame, descriptors_frame);\n\n    // 2. Match the two image descriptors\n    std::vector<std::vector<cv::DMatch> > matches12, matches21;\n\n    // 2a. From image 1 to image 2\n    matcher_->knnMatch(descriptors_frame, descriptors_model, matches12, 2); // return 2 nearest neighbours\n\n    // 2b. From image 2 to image 1\n    matcher_->knnMatch(descriptors_model, descriptors_frame, matches21, 2); // return 2 nearest neighbours\n\n    // 3. Remove matches for which NN ratio is > than threshold\n    // clean image 1 -> image 2 matches\n    int removed1 = ratioTest(matches12);\n    // clean image 2 -> image 1 matches\n    int removed2 = ratioTest(matches21);\n\n    // 4. Remove non-symmetrical matches\n    symmetryTest(matches12, matches21, good_matches);\n\n}\n```\n\n----------------------------------------\n\nTITLE: Drawing a Rook Chess Piece in C++\nDESCRIPTION: Drawing a rook chess piece using lines, rectangles, and polygons in OpenCV C++. Demonstrates creating a complex shape by combining multiple basic drawing functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n/// 2. Draw a rook\n/// ------------------\n\n/// 2.a. Create a convex polygon\nMyPolygon( rook_image );\n\n/// 2.b. Creating rectangles\nrectangle( rook_image,\n       Point( 0, 7*w/8 ),\n       Point( w, w),\n       Scalar( 0, 255, 255 ),\n       FILLED,\n       LINE_8 );\n\n/// 2.c. Create a few lines\nMyLine( rook_image, Point( 0, 15*w/16 ), Point( w, 15*w/16 ) );\nMyLine( rook_image, Point( w/4, 7*w/8 ), Point( w/4, w ) );\nMyLine( rook_image, Point( w/2, 7*w/8 ), Point( w/2, w ) );\nMyLine( rook_image, Point( 3*w/4, 7*w/8 ), Point( 3*w/4, w ) );\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Layer OpenCV C++\nDESCRIPTION: Snippet illustrating how to register a custom layer before importing the model in OpenCV's deep learning engine.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/custom_layers.hpp Register a custom layer\n```\n\n----------------------------------------\n\nTITLE: Calculating Convex Hull in Python using OpenCV\nDESCRIPTION: This Python script utilizes the OpenCV library (`cv2`) to load an image, apply thresholding to find contours using `cv.findContours`, calculate the convex hull for each detected contour with `cv.convexHull`, and then draw both the contours and their corresponding hulls on a blank image. It requires `cv2` and `numpy`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/hull/hull.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@add_toggle_python\nThis tutorial code's is shown lines below. You can also download it from\n[here](https://github.com/opencv/opencv/tree/4.x/samples/python/tutorial_code/ShapeDescriptors/hull/hull_demo.py)\n@include samples/python/tutorial_code/ShapeDescriptors/hull/hull_demo.py\n@end_toggle\n```\n\n----------------------------------------\n\nTITLE: Performing Image Blurring with cv.blur in C++\nDESCRIPTION: Explains how to use cv.blur() to apply a normalized box filter for image blurring. The blurring reduces image noise by averaging pixel values. Requires OpenCV, with parameters including source image, destination image, and kernel size. This approach uses a 3x3 normalized box filter.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_filtering/js_filtering.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ncv::blur(src, dst, cv::Size(ksize, ksize), cv::Point(-1, -1), cv::BORDER_DEFAULT);\n```\n\n----------------------------------------\n\nTITLE: Modifying Pixel Values in an OpenCV Image using NumPy Indexing in Python\nDESCRIPTION: Illustrates modifying the pixel value at coordinates (100, 100) to white ([255, 255, 255]) using NumPy array assignment. It then prints the new value to confirm the change. Direct modification using NumPy indexing is possible but iterating through all pixels this way is discouraged for performance reasons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_basic_ops/py_basic_ops.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> img[100,100] = [255,255,255]\n>>> print( img[100,100] )\n[255 255 255]\n```\n\n----------------------------------------\n\nTITLE: Probabilistic Hough Line Transform Implementation in Python OpenCV\nDESCRIPTION: Shows the implementation of Probabilistic Hough Transform using cv.HoughLinesP(), which is an optimized version that uses random sampling of points. It directly returns line endpoints and includes parameters for minimum line length and maximum line gap.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_houghlines/py_houghlines.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n@include probabilistic_hough_line_transform.py\n```\n\n----------------------------------------\n\nTITLE: Measuring Execution Time with OpenCV Timing Functions in Python\nDESCRIPTION: Illustrates how to use cv.getTickCount and cv.getTickFrequency to measure code execution time in Python. The snippet captures the number of clock cycles before and after code execution and reports the elapsed time in seconds. Requires OpenCV (cv2) installed and assumes the presence of a valid image file. Inputs include the image file path and kernel size range, while the output is the elapsed time for median filtering. This approach is limited to global time measurement and does not provide function-level profiling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_optimization/py_optimization.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ne1 = cv.getTickCount()\n# your code execution\ne2 = cv.getTickCount()\ntime = (e2 - e1)/ cv.getTickFrequency()\n```\n\n----------------------------------------\n\nTITLE: Running TensorFlow MobileNet Evaluation via OpenCV dnn CLI (Console)\nDESCRIPTION: Shows the exact command to evaluate a TensorFlow MobileNet model with the dnn_model_runner evaluation script. This command triggers data loading, evaluation, and result logging for the selected model. Dependencies include Python and an available MobileNet model properly referenced as 'mobilenet'. Parameters must align with available input data, and outputs are written as log files with measured metrics.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls --model_name mobilenet\n```\n\n----------------------------------------\n\nTITLE: Scaling Images with OpenCV in Python\nDESCRIPTION: Demonstrates how to resize an image using cv.resize() with two different approaches: specifying scaling factors or explicit dimensions. Includes options for different interpolation methods like INTER_CUBIC for better quality when zooming.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nimg = cv.imread('messi5.jpg')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n\nres = cv.resize(img,None,fx=2, fy=2, interpolation = cv.INTER_CUBIC)\n\n#OR\n\nheight, width = img.shape[:2]\nres = cv.resize(img,(2*width, 2*height), interpolation = cv.INTER_CUBIC)\n```\n\n----------------------------------------\n\nTITLE: Drawing Back Projection in Python with OpenCV\nDESCRIPTION: This snippet illustrates how to display the back projection result using OpenCV in Python. It creates a window and shows the back projection image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/back_projection/back_projection.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ncv.imshow('BackProj', backproj)\n```\n\n----------------------------------------\n\nTITLE: FLANN-based Feature Matching in Python\nDESCRIPTION: Shows implementation of Fast Library for Approximate Nearest Neighbors (FLANN) based matcher with SIFT features. Includes index and search parameters configuration, ratio test, and match visualization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nimport matplotlib.pyplot as plt\n\nimg1 = cv.imread('box.png',cv.IMREAD_GRAYSCALE)          # queryImage\nimg2 = cv.imread('box_in_scene.png',cv.IMREAD_GRAYSCALE) # trainImage\n\n# Initiate SIFT detector\nsift = cv.SIFT_create()\n\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute(img1,None)\nkp2, des2 = sift.detectAndCompute(img2,None)\n\n# FLANN parameters\nFLANN_INDEX_KDTREE = 1\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\nsearch_params = dict(checks=50)   # or pass empty dictionary\n\nflann = cv.FlannBasedMatcher(index_params,search_params)\n\nmatches = flann.knnMatch(des1,des2,k=2)\n\n# Need to draw only good matches, so create a mask\nmatchesMask = [[0,0] for i in range(len(matches))]\n\n# ratio test as per Lowe's paper\nfor i,(m,n) in enumerate(matches):\n    if m.distance < 0.7*n.distance:\n        matchesMask[i]=[1,0]\n\ndraw_params = dict(matchColor = (0,255,0),\n                   singlePointColor = (255,0,0),\n                   matchesMask = matchesMask,\n                   flags = cv.DrawMatchesFlags_DEFAULT)\n\nimg3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,**draw_params)\n\nplt.imshow(img3,),plt.show()\n```\n\n----------------------------------------\n\nTITLE: Performing Forward Pass with OpenCV in C++\nDESCRIPTION: This snippet demonstrates a forward pass in the neural network to produce outputs using OpenCV's DNN module. It computes layer outputs, focusing on obtaining results from the last layer for classification purposes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_googlenet/dnn_googlenet.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/classification.cpp Make forward pass\n```\n\n----------------------------------------\n\nTITLE: Downsampling an Image with cv.pyrDown in OpenCV.js\nDESCRIPTION: This function signature shows how to downsample an input image (`src`) to create a lower-resolution output image (`dst`). It blurs the image and downsamples it. The `dstsize` parameter specifies the desired size for the output image (defaults to half the input dimensions if Size(0,0)). `borderType` specifies the pixel extrapolation method, with cv.BORDER_CONSTANT being unsupported.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_pyramids/js_pyramids.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ncv.pyrDown (src, dst, dstsize = new cv.Size(0, 0), borderType  = cv.BORDER_DEFAULT)\n```\n\n----------------------------------------\n\nTITLE: Displaying an Image with OpenCV in C++\nDESCRIPTION: This C++ snippet demonstrates how to load and display an image using OpenCV. It checks for the required command-line argument (image file path), loads the image into a cv::Mat object, displays it in a window, and handles error conditions if the file cannot be loaded. Dependencies: OpenCV library (cv and highgui modules). Inputs: path to the image file as an argument. Outputs: Opens a display window showing the image; error printed if conditions not met. Key limitation: requires OpenCV installed and linked properly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_gcc_cmake/linux_gcc_cmake.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <stdio.h>\\n#include <opencv2/opencv.hpp>\\n\\nusing namespace cv;\\n\\nint main(int argc, char** argv )\\n{\\n    if ( argc != 2 )\\n    {\\n        printf(\"usage: DisplayImage.out <Image_Path>\\n\");\\n        return -1;\\n    }\\n\\n    Mat image;\\n    image = imread( argv[1], IMREAD_COLOR );\\n\\n    if ( !image.data )\\n    {\\n        printf(\"No image data \\n\");\\n        return -1;\\n    }\\n    namedWindow(\"Display Image\", WINDOW_AUTOSIZE );\\n    imshow(\"Display Image\", image);\\n\\n    waitKey(0);\\n\\n    return 0;\\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Gaussian Blur with OpenCV in Python\nDESCRIPTION: This Python snippet illustrates using the OpenCV's GaussianBlur() function to apply a Gaussian filter for image smoothing. Dependencies include OpenCV, requiring parameters such as kernel size and standard deviations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/Smoothing/smoothing.py gaussianblur\n```\n\n----------------------------------------\n\nTITLE: Computing Transformed Corner Coordinates with OpenCV - C++\nDESCRIPTION: This C++ code computes the new coordinates of source points after mapping them through the estimated homography using perspectiveTransform. Requires OpenCV Mat vectors for input/output and a valid homography. Outputs an array of transformed points corresponding to the desired perspective.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet perspective_correction.cpp compute-transformed-corners\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Pixel Comparison Logic in C++\nDESCRIPTION: This code segment implements part of the FAST corner detection algorithm, comparing pixel values against brightness thresholds (c_b and cb) to determine if a point is a corner. The algorithm examines pixels at various offsets around a candidate point and makes decisions based on these intensity comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\ngoto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\nif(ptr[offset5] < c_b)\n  if(ptr[offset7] > cb)\n    if(ptr[offset14] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset9] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              if(ptr[offset12] > cb)\n                if(ptr[offset13] > cb)\n                  if(ptr[offset6] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset15] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n    if(ptr[offset14] < c_b)\n      if(ptr[offset15] < c_b)\n        if(ptr[offset1] < c_b)\n          if(ptr[offset3] < c_b)\n            if(ptr[offset6] < c_b)\n              goto is_a_corner;\n            else\n              if(ptr[offset13] < c_b)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                if(ptr[offset12] < c_b)\n                  if(ptr[offset13] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset8] < c_b)\n            if(ptr[offset9] < c_b)\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  if(ptr[offset12] < c_b)\n                    if(ptr[offset13] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n  if(ptr[offset7] < c_b)\n    if(ptr[offset3] < c_b)\n      if(ptr[offset1] < c_b)\n        if(ptr[offset6] < c_b)\n          if(ptr[offset8] < c_b)\n            goto is_a_corner;\n          else\n            if(ptr[offset15] < c_b)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset13] < c_b)\n            if(ptr[offset14] < c_b)\n              if(ptr[offset15] < c_b)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset8] < c_b)\n          if(ptr[offset9] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset6] < c_b)\n                goto is_a_corner;\n              else\n                if(ptr[offset11] < c_b)\n                  if(ptr[offset12] < c_b)\n                    if(ptr[offset13] < c_b)\n                      if(ptr[offset14] < c_b)\n                        if(ptr[offset15] < c_b)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      if(ptr[offset10] < c_b)\n        if(ptr[offset11] < c_b)\n          if(ptr[offset12] < c_b)\n            if(ptr[offset8] < c_b)\n              if(ptr[offset9] < c_b)\n                if(ptr[offset6] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset13] < c_b)\n                    if(ptr[offset14] < c_b)\n                      if(ptr[offset15] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset1] < c_b)\n                  if(ptr[offset13] < c_b)\n                    if(ptr[offset14] < c_b)\n                      if(ptr[offset15] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset1] < c_b)\n                if(ptr[offset13] < c_b)\n                  if(ptr[offset14] < c_b)\n                    if(ptr[offset15] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    if(ptr[offset14] < c_b)\n      if(ptr[offset15] < c_b)\n        if(ptr[offset1] < c_b)\n          if(ptr[offset3] < c_b)\n            if(ptr[offset6] < c_b)\n              goto is_a_corner;\n            else\n              if(ptr[offset13] < c_b)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                if(ptr[offset12] < c_b)\n                  if(ptr[offset13] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset8] < c_b)\n            if(ptr[offset9] < c_b)\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  if(ptr[offset12] < c_b)\n                    if(ptr[offset13] < c_b)\n                      goto is_a_corner;\n                    else\n```\n\n----------------------------------------\n\nTITLE: Accessing Pixel in Floating Point Image with OpenCV in C++\nDESCRIPTION: Shows reading a pixel value as a floating point number from a single-channel float image (type CV_32F) in C++. cv::Mat::at<float>(y, x) gives direct access. Useful for results of filters like Sobel. Input must have appropriate data type or access may yield incorrect values or crash.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\nfloat intensity = img.at<float>(y, x);\n```\n\n----------------------------------------\n\nTITLE: Loading Image in OpenCV\nDESCRIPTION: Code snippets showing how to load an input image across different languages using OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_circle/hough_circle.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nMat src = imread(samples::findFile(\"smarties.png\"));\n```\n\nLANGUAGE: Java\nCODE:\n```\nMat src = Imgcodecs.imread(\"smarties.png\");\n```\n\nLANGUAGE: Python\nCODE:\n```\nsrc = cv.imread(cv.samples.findFile(\"smarties.png\"))\n```\n\n----------------------------------------\n\nTITLE: Finding Chessboard Corners using OpenCV - Python\nDESCRIPTION: This Python snippet shows how to detect chessboard corners in source and target images for perspective correction using OpenCV's cv2.findChessboardCorners. Prerequisites include cv2 and Numpy libraries, valid image files, and board dimensions. Outputs are arrays of detected corner coordinates used for later homography calculations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/features2D/Homography/perspective_correction.py find-corners\n```\n\n----------------------------------------\n\nTITLE: Selecting Region of Interest (ROI) with OpenCV in Python\nDESCRIPTION: Extracts a region of interest by NumPy slicing in Python. img[y:y+h, x:x+w] returns a subarray; changes propagate to parent. Efficient and requires only NumPy/OPENCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\nroi = img[y:y+h, x:x+w]\n```\n\n----------------------------------------\n\nTITLE: Decomposing Homography Matrix in OpenCV C++\nDESCRIPTION: This snippet focuses on decomposing a homography matrix into rotations, translations, and plane normals using the OpenCV function cv::decomposeHomographyMat. The aim is to analyze multiple possible solutions, each represented by different combinations of decomposed vectors, and identify the one closest to the actual camera displacement. Required dependencies include OpenCV libraries. Input includes the computed homography matrix, and output involves sets of rotation vectors, translation vectors, and plane normals.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_27\n\nLANGUAGE: C++\nCODE:\n```\n@snippet decompose_homography.cpp decompose-homography-from-camera-displacement\n```\n\n----------------------------------------\n\nTITLE: Retrieving TensorFlow SSD MobileNetV1 Frozen Graph Path - Python\nDESCRIPTION: This Python snippet demonstrates retrieval of a TensorFlow SSD MobileNetV1 model and extraction of its frozen graph using a utility function. It prints out the path to the frozen graph (protobuf file) for inspection. Dependencies include a functional extract_tf_frozen_graph, proper imports, and write permissions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    tf_model_name = 'ssd_mobilenet_v1_coco_2017_11_17'\\n    graph_extraction_dir = \"./\"\\n    frozen_graph_path = extract_tf_frozen_graph(tf_model_name, graph_extraction_dir)\\n    print(\"Frozen graph path for {}: {}\".format(tf_model_name, frozen_graph_path))\n```\n\n----------------------------------------\n\nTITLE: Defining the cv.findContours Function Signature in OpenCV.js\nDESCRIPTION: Defines the signature for the `cv.findContours` function in OpenCV.js, used to detect contours in an 8-bit single-channel binary image. Key parameters include the input image, output contour storage, hierarchy information, retrieval mode, approximation method, and an optional offset. It's recommended to use binary images (e.g., after thresholding or Canny edge detection) for better accuracy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contours_begin/js_contours_begin.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ncv.findContours (image, contours, hierarchy, mode, method, offset = new cv.Point(0, 0))\n@param image         source, an 8-bit single-channel image. Non-zero pixels are treated as 1's. Zero pixels remain 0's, so the image is treated as binary.\n@param contours      detected contours.\n@param hierarchy     containing information about the image topology. It has as many elements as the number of contours.\n@param mode          contour retrieval mode(see cv.RetrievalModes).\n@param method        contour approximation method(see cv.ContourApproximationModes).\n@param offset        optional offset by which every contour point is shifted. This is useful if the contours are extracted from the image ROI and then they should be analyzed in the whole image context.\n```\n\n----------------------------------------\n\nTITLE: Setting Input and Forward Pass in OpenCV DNN - C++\nDESCRIPTION: These C++ lines set the preprocessed blob as network input and run a forward pass in the loaded ONNX model. The result, 'prob', contains the class scores for postprocessing. This is required after completing input preprocessing in inference workflows.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nnet.setInput(blob);\\nMat prob = net.forward();\n```\n\n----------------------------------------\n\nTITLE: Detailed Image Stitching with Configurable Parameters C++\nDESCRIPTION: This snippet provides detailed image stitching using configurable parameters in C++. It allows experimenting with different settings such as feature detection, matcher, estimator, and blending methods. Dependencies include OpenCV with C++ support, and it is configured via command-line options to perform detailed stitching for complex scenarios.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/stitcher.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nint main(int argc, char* argv[]) {\n    std::vector<std::string> img_names = {\"boat1.jpg\", \"boat2.jpg\", ...};\n    // Command-line parsing and OpenCV setup\n    cv::Stitcher::Mode mode = cv::Stitcher::SCANS;\n    std::vector<cv::Mat> imgs;\n    // Load images\n    for (const auto& name : img_names) {\n        imgs.push_back(cv::imread(name));\n    }\n    cv::Ptr<cv::Stitcher> stitcher = cv::Stitcher::create(mode);\n    stitcher->setFeaturesFinder(cv::makePtr<cv::AKAZE>());\n    stitcher->setBlender(cv::detail::Blender::createDefault(cv::detail::Blender::MULTI_BAND, true));\n    cv::Mat pano;\n    cv::Stitcher::Status status = stitcher->stitch(imgs, pano);\n    if (status == cv::Stitcher::OK) {\n        cv::imwrite(\"result_detailed.jpg\", pano);\n    }\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Parameters for Text Detection (DB) in C++\nDESCRIPTION: This C++ snippet configures parameters for a DB-based text detection model in OpenCV. It sets thresholds for binarization and polygon detection, candidates limit, and unclip ratio, along with normalization parameters and input shape. It is crucial for achieving accurate detection results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_text_spotting/dnn_text_spotting.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// Load model weights\nTextDetectionModel_DB model(\"/path/to/DB_TD500_resnet50.onnx\");\n\n// Post-processing parameters\nfloat binThresh = 0.3;\nfloat polyThresh = 0.5;\nuint maxCandidates = 200;\ndouble unclipRatio = 2.0;\nmodel.setBinaryThreshold(binThresh)\n     .setPolygonThreshold(polyThresh)\n     .setMaxCandidates(maxCandidates)\n     .setUnclipRatio(unclipRatio)\n;\n\n// Normalization parameters\ndouble scale = 1.0 / 255.0;\nScalar mean = Scalar(122.67891434, 116.66876762, 104.00698793);\n\n// The input shape\nSize inputSize = Size(736, 736);\n\nmodel.setInputParams(scale, inputSize, mean);\n```\n\n----------------------------------------\n\nTITLE: Normalizing Template Matching Results (Java)\nDESCRIPTION: Normalizes the result matrix from `matchTemplate` to a standard range (typically 0 to 1) using `Core.normalize`. This uses the `NORM_MINMAX` normalization type, scaling the values linearly. Normalization improves visualization and comparison.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_26\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/tutorial_template_matching/MatchTemplateDemo.java normalize\n```\n\n----------------------------------------\n\nTITLE: Finding Chessboard Corners using OpenCV - C++\nDESCRIPTION: This snippet demonstrates using OpenCV functions in C++ to detect chessboard corners within both source and desired images as preparation for perspective correction. It requires the OpenCV C++ library, proper image loading, and specification of chessboard size. Detected points are used for subsequent homography computation. Inputs are image file paths; outputs are sets of corner coordinates.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet perspective_correction.cpp find-corners\n```\n\n----------------------------------------\n\nTITLE: Creating Image for Histogram Display in Python\nDESCRIPTION: Python snippet creating a blank NumPy array (`histImage`) using `np.zeros` for histogram visualization. It defines width (`hist_w`) and height (`hist_h`), specifies 3 channels and `uint8` data type, and calculates the width per bin (`bin_w`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/Histograms_Matching/histogram_calculation/calcHist_Demo.py Draw the histograms for B, G and R\n```\n\n----------------------------------------\n\nTITLE: Computing Camera Poses for Homography Displacement - C++\nDESCRIPTION: This C++ snippet uses OpenCV's solvePnP function to estimate camera pose from 3D-2D correspondences. It produces Rodrigues rotation (rvec) and translation (tvec) vectors, necessary to derive camera displacements and further homography calculations from physical scene changes. Requires calibration parameters and matching points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet homography_from_camera_displacement.cpp compute-poses\n```\n\n----------------------------------------\n\nTITLE: Finding Chessboard Corners using OpenCV - Java\nDESCRIPTION: This Java snippet detects chessboard corners in the provided source and destination images leveraging OpenCV's Java API. Requires OpenCV Java library and valid Mat image objects. Output consists of MatOfPoint2f containing the coordinates needed for homography estimation. Board size parameters must be properly set.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/features2D/Homography/PerspectiveCorrection.java find-corners\n```\n\n----------------------------------------\n\nTITLE: Reference Counting Example with OpenCV Mat in C++\nDESCRIPTION: Shows reference sharing in OpenCV cv::Mat. Submatrix or reshaped views use the same underlying storage, increasing reference count. This avoids copying data. Requires C++ OpenCV; lifetimes must be managed manually to avoid dangling pointers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_21\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat submat = mat.colRange(0, 3);\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with OpenCV DNN\nDESCRIPTION: This Python snippet executes inference using the loaded OpenCV DNN model (`opencv_net`). It first sets the preprocessed input blob (`preproc_img`) using `opencv_net.setInput()`. Then, `opencv_net.forward()` runs the inference and returns the output tensor (`out`). The output shape is printed, and finally, `np.argmax` is used along the class axis (axis=0, after removing the batch dimension) to get the predicted class ID for each pixel, resulting in a 2D segmentation map (`out_predictions`). Requires `cv2` (OpenCV) and `numpy`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# set OpenCV DNN input\nopencv_net.setInput(preproc_img)\n\n# OpenCV DNN inference\nout = opencv_net.forward()\nprint(\"OpenCV DNN segmentation prediction: \\n\")\nprint(\"* shape: \", out.shape)\n\n# get IDs of predicted classes\nout_predictions = np.argmax(out[0], axis=0)\n```\n\n----------------------------------------\n\nTITLE: Separating Clustered Data Based on Labels in K-Means\nDESCRIPTION: This code separates the data points into different clusters based on their assigned labels from the K-means algorithm. Points with label 0 are assigned to array A and points with label 1 to array B.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nA = z[labels==0]\nB = z[labels==1]\n```\n\n----------------------------------------\n\nTITLE: Finding Contours in a Binary Image with OpenCV Python\nDESCRIPTION: This code demonstrates how to read an image, convert it to grayscale, apply thresholding to create a binary image, and then find contours using cv.findContours(). The function returns both the contours and hierarchy information.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contours_begin/py_contours_begin.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nim = cv.imread('test.jpg')\nassert im is not None, \"file could not be read, check with os.path.exists()\"\nimgray = cv.cvtColor(im, cv.COLOR_BGR2GRAY)\nret, thresh = cv.threshold(imgray, 127, 255, 0)\ncontours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOX Model to ONNX Using Bash Script\nDESCRIPTION: Demonstrates the complete workflow to clone the YOLOX repository, download pre-trained weights, and execute the provided export script to convert the YOLOX model to ONNX format. Dependencies include git, wget, Python 3, PyTorch, and required YOLOX Python dependencies. The main parameters are the exported ONNX file name, network type (-n), pretrained checkpoint (-c), and the decode_in_inference flag to include anchor box creation within the ONNX. Outputs an ONNX model file suitable for OpenCV DNN inference.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_yolo/dnn_yolo.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/Megvii-BaseDetection/YOLOX.git\\ncd YOLOX\\nwget https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s.pth # download pre-trained weights\\npython3 -m tools.export_onnx --output-name yolox_s.onnx -n yolox-s -c yolox_s.pth --decode_in_inference\n```\n\n----------------------------------------\n\nTITLE: Setup Image in OpenCV Python\nDESCRIPTION: Loads an image, converts it to grayscale, and applies a blur using Python and OpenCV. OpenCV module is required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\nsrc = cv2.imread(\"image.jpg\")\ngray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\ngray = cv2.blur(gray, (3, 3))\n```\n\n----------------------------------------\n\nTITLE: Edge Detection using Canny Detector in OpenCV (Python)\nDESCRIPTION: This snippet shows how to perform edge detection using OpenCV's cv2.Canny in Python. The input image is converted to grayscale, and Canny is applied with threshold values. Input: image array; output: binary edge map. Requires cv2 and numpy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nsrc_gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\\nedges = cv2.Canny(src_gray, 50, 200, 3)\\n\n```\n\n----------------------------------------\n\nTITLE: Drawing All Contours on an Image with OpenCV Python\nDESCRIPTION: This code shows how to draw all detected contours on an image using cv.drawContours(). It passes -1 as the contour index to draw all contours with a green color and line thickness of 3.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contours_begin/py_contours_begin.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncv.drawContours(img, contours, -1, (0,255,0), 3)\n```\n\n----------------------------------------\n\nTITLE: Detecting SIFT Keypoints and Computing Descriptors in OpenCV Python\nDESCRIPTION: This code demonstrates how to detect keypoints and compute their descriptors in a single step using the detectAndCompute() method of the SIFT object. This returns both keypoints and their corresponding descriptors.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsift = cv.SIFT_create()\nkp, des = sift.detectAndCompute(gray,None)\n```\n\n----------------------------------------\n\nTITLE: Undistorting Images with Optimal Camera Matrix in Python\nDESCRIPTION: This snippet refines a camera matrix and applies undistortion to images using OpenCV functions like cv.getOptimalNewCameraMatrix() and cv.undistort(). It accepts an image and camera parameters, and outputs an undistorted version of the image. The cv.undistort() function is used here with cropping based on ROI.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_calibration/py_calibration.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimg = cv.imread('left12.jpg')\nh,  w = img.shape[:2]\nnewcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))\n\n# undistort\ndst = cv.undistort(img, mtx, dist, None, newcameramtx)\n\n# crop the image\nx, y, w, h = roi\ndst = dst[y:y+h, x:x+w]\ncv.imwrite('calibresult.png', dst)\n```\n\n----------------------------------------\n\nTITLE: Setting a Pixel Value in an Image with OpenCV in Java\nDESCRIPTION: Demonstrates updating a pixel value in a Mat using the put(y, x, value) method in Java OpenCV. The input is the Mat, row, column, and new value (as double in array). Alters the underlying Mat data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\nimg.put(y, x, 128);\n```\n\n----------------------------------------\n\nTITLE: Calling cv.HoughCircles in OpenCV.js\nDESCRIPTION: Demonstrates the function signature for cv.HoughCircles in OpenCV.js. This function detects circles in a grayscale image using the Hough Gradient method. Key parameters include the input image (`image`), output vector for circles (`circles`), detection method (`method`, typically HOUGH_GRADIENT), accumulator resolution ratio (`dp`), minimum distance between centers (`minDist`), Canny edge thresholds (`param1`, `param2`), and radius range (`minRadius`, `maxRadius`). Requires an 8-bit, single-channel, grayscale input image. Outputs a vector of detected circles, each represented as (x, y, radius).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_houghcircles/js_houghcircles.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ncv.HoughCircles (image, circles, method, dp, minDist, param1 = 100, param2 = 100, minRadius = 0, maxRadius = 0)\n```\n\n----------------------------------------\n\nTITLE: Orthogonalizing Rotation Matrix with Polar Decomposition in OpenCV C++\nDESCRIPTION: Code to ensure the extracted rotation matrix is a valid rotation matrix using SVD-based polar decomposition. This enforces the orthogonality constraint required for rotation matrices.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat w, u, vt;\ncv::SVD::compute(R, w, u, vt);\nR = u * vt;\n\n// Ensure a proper rotation matrix (det=1)\nif (cv::determinant(R) < 0) {\n    R = -R;\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning and Preparing OpenCV and opencv_extra Repositories with Git - Shell\nDESCRIPTION: This set of shell commands demonstrates how to clone the official OpenCV and opencv_extra repositories from GitHub, check out the specific v3.1.0 branch, and cherry-pick upstream patches necessary for CUDA 8.0 support and build fixes. This process ensures the developer works with a compatible and patched version of OpenCV required for Tegra platforms. Key git commands (clone, checkout, cherry-pick) are employed, with each patch identified by its commit hash. Expected output after cherry-picking is provided as context for validation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Clone the opencv repository locally:\n$ git clone https://github.com/opencv/opencv.git\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ cd opencv\n$ git checkout -b v3.1.0 3.1.0\n```\n\nLANGUAGE: shell\nCODE:\n```\n# While still in the opencv directory:\n$ git cherry-pick 10896\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ git cherry pick cdb9c\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ git cherry-pick 24dbb\n```\n\nLANGUAGE: shell\nCODE:\n```\n# In the same base directory from which you cloned OpenCV:\n$ git clone https://github.com/opencv/opencv_extra.git\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ cd opencv_extra\n$ git checkout -b v3.1.0 3.1.0\n```\n\n----------------------------------------\n\nTITLE: Detailed PyTorch Model Export to ONNX File - Python\nDESCRIPTION: This snippet defines the output directory and file name, ensures the directory exists, generates a standard input tensor, and calls 'torch.onnx.export' to export the model. It uses a [batch, channel, height, width] input of size [1, 3, 224, 224] and explicitly sets input/output names and ONNX opset version. Requires os, torch, and torch.onnx modules, as well as appropriate model and directory permissions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# define the directory for further converted model save\\nonnx_model_path = \"models\"\\n# define the name of further converted model\\nonnx_model_name = \"resnet50.onnx\"\\n\\n# create directory for further converted model\\nos.makedirs(onnx_model_path, exist_ok=True)\\n\\n# get full path to the converted model\\nfull_model_path = os.path.join(onnx_model_path, onnx_model_name)\\n\\n# generate model input\\ngenerated_input = Variable(\\n    torch.randn(1, 3, 224, 224)\\n)\\n\\n# model export into ONNX format\\ntorch.onnx.export(\\n    original_model,\\n    generated_input,\\n    full_model_path,\\n    verbose=True,\\n    input_names=[\"input\"],\\n    output_names=[\"output\"],\\n    opset_version=11\\n)\n```\n\n----------------------------------------\n\nTITLE: Process Contours and Draw Shapes in OpenCV Python\nDESCRIPTION: Approximates contours to polygons, draws bounding boxes and enclosing circles using Python and OpenCV. OpenCV is required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nfor i in range(len(contours)):\n    approx_curve = cv2.approxPolyDP(contours[i], 3, True)\n    bound_rect = cv2.boundingRect(approx_curve)\n    center, radius = cv2.minEnclosingCircle(approx_curve)\n```\n\n----------------------------------------\n\nTITLE: Calculating Histograms for HSV Images in OpenCV\nDESCRIPTION: Computing normalized histograms for the HSV images to be compared, using the previously defined bin settings, ranges, and channels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_comparison/histogram_comparison.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nMat hist_base, hist_half_down, hist_test1, hist_test2;\ncalcHist( &hsv_base, 1, channels, Mat(), hist_base, 2, histSize, ranges, true, false );\nnormalize( hist_base, hist_base, 0, 1, NORM_MINMAX, -1, Mat() );\ncalcHist( &hsv_half_down, 1, channels, Mat(), hist_half_down, 2, histSize, ranges, true, false );\nnormalize( hist_half_down, hist_half_down, 0, 1, NORM_MINMAX, -1, Mat() );\ncalcHist( &hsv_test1, 1, channels, Mat(), hist_test1, 2, histSize, ranges, true, false );\nnormalize( hist_test1, hist_test1, 0, 1, NORM_MINMAX, -1, Mat() );\ncalcHist( &hsv_test2, 1, channels, Mat(), hist_test2, 2, histSize, ranges, true, false );\nnormalize( hist_test2, hist_test2, 0, 1, NORM_MINMAX, -1, Mat() );\n```\n\nLANGUAGE: java\nCODE:\n```\nMat hist_base = new Mat();\nMat hist_half_down = new Mat();\nMat hist_test1 = new Mat();\nMat hist_test2 = new Mat();\nImgproc.calcHist(Arrays.asList(hsv_base), histChannels, new Mat(), hist_base, histSize2, histRanges, false);\nCore.normalize(hist_base, hist_base, 0, 1, Core.NORM_MINMAX, -1, new Mat());\nImgproc.calcHist(Arrays.asList(hsv_half_down), histChannels, new Mat(), hist_half_down, histSize2, histRanges, false);\nCore.normalize(hist_half_down, hist_half_down, 0, 1, Core.NORM_MINMAX, -1, new Mat());\nImgproc.calcHist(Arrays.asList(hsv_test1), histChannels, new Mat(), hist_test1, histSize2, histRanges, false);\nCore.normalize(hist_test1, hist_test1, 0, 1, Core.NORM_MINMAX, -1, new Mat());\nImgproc.calcHist(Arrays.asList(hsv_test2), histChannels, new Mat(), hist_test2, histSize2, histRanges, false);\nCore.normalize(hist_test2, hist_test2, 0, 1, Core.NORM_MINMAX, -1, new Mat());\n```\n\nLANGUAGE: python\nCODE:\n```\nhist_base = cv.calcHist([hsv_base], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_base, hist_base, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_half_down = cv.calcHist([hsv_half_down], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_half_down, hist_half_down, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_test1 = cv.calcHist([hsv_test1], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_test1, hist_test1, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\nhist_test2 = cv.calcHist([hsv_test2], channels, None, histSize, ranges, accumulate=False)\ncv.normalize(hist_test2, hist_test2, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV on macOS Bash\nDESCRIPTION: This Bash script demonstrates the installation process for OpenCV on macOS. Dependencies include Xcode, JDK, and CMake. The script clones the OpenCV repository, checks out a specific version, and compiles the code. The output is a built version of OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/\\nmkdir opt\\ngit clone https://github.com/opencv/opencv.git\\ncd opencv\\ngit checkout 2.4\\nmkdir build\\ncd build\\ncmake -DBUILD_SHARED_LIBS=OFF ..\\n...\\n...\\nmake -j8\\n# optional\\n# make install\n```\n\n----------------------------------------\n\nTITLE: Performing Brightness and Contrast Adjustment in OpenCV (C++)\nDESCRIPTION: The C++ code snippet demonstrates how to perform a linear transformation to adjust the brightness and contrast of an image using OpenCV. It requires OpenCV libraries to compile and run. Users input gain (alpha) and bias (beta) parameters, which adjust contrast and brightness respectively. The snippet also ensures that pixel values remain within valid ranges using `cv::saturate_cast`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/basic_linear_transform/basic_linear_transform.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\n#include <iostream>\n\nusing namespace cv;\nusing namespace std;\n\nint main(int argc, char** argv) {\n    Mat image = imread(argv[1]);\n    Mat new_image = Mat::zeros(image.size(), image.type());\n    double alpha = 1.0; // Simple contrast control\n    int beta = 0;       // Simple brightness control\n    \n    cout << \" Basic Linear Transforms \" << endl;\n    cout << \"-------------------------\" << endl;\n    cout << \"* Enter the alpha value [1.0-3.0]: \";\n    cin >> alpha;\n    cout << \"* Enter the beta value [0-100]: \";\n    cin >> beta;\n\n    for (int y = 0; y < image.rows; y++) {\n        for (int x = 0; x < image.cols; x++) {\n            for (int c = 0; c < image.channels(); c++) {\n                new_image.at<Vec3b>(y, x)[c] = saturate_cast<uchar>(alpha * image.at<Vec3b>(y, x)[c] + beta);\n            }\n        }\n    }\n\n    namedWindow(\"Original Image\", 1);\n    imshow(\"Original Image\", image);\n\n    namedWindow(\"New Image\", 1);\n    imshow(\"New Image\", new_image);\n\n    waitKey();\n    return 0;\n}\n\n```\n\n----------------------------------------\n\nTITLE: OpenCV DNN Model Inference\nDESCRIPTION: This Python snippet executes inference using the loaded ONNX model with OpenCV's DNN module. It sets the input, runs the forward pass, and retrieves predictions, including class ID and confidence.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nopencv_net.setInput(preproc_img)\nout = opencv_net.forward()\nprint(\"OpenCV DNN prediction: \\n\")\nprint(\"* shape: \", out.shape)\nimagenet_class_id = np.argmax(out)\nconfidence = out[0][imagenet_class_id]\nprint(\"* class ID: {}, label: {}\".format(imagenet_class_id, imagenet_labels[imagenet_class_id]))\nprint(\"* confidence: {:.4f}\".format(confidence))\n```\n\n----------------------------------------\n\nTITLE: Drawing Histogram Lines for Each Channel in Java\nDESCRIPTION: Java snippet iterating through the histogram bins (from 1 to `histSize`) and drawing lines on `histImage` using `Imgproc.line`. For each bin `i`, it draws lines connecting the points representing the counts in bin `i-1` and bin `i` for B, G, and R channels, using corresponding colors (blue, green, red). Histogram values (`bHist.get`, `gHist.get`, `rHist.get`) are used to determine the y-coordinates.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_28\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/Histograms_Matching/histogram_calculation/CalcHistDemo.java Draw for each channel\n```\n\n----------------------------------------\n\nTITLE: Drawing Straight Bounding Rectangle for Contours in Python with OpenCV\nDESCRIPTION: This code shows how to compute and draw a straight bounding rectangle for a contour using cv.boundingRect() and cv.rectangle() functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nx,y,w,h = cv.boundingRect(cnt)\ncv.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n```\n\n----------------------------------------\n\nTITLE: Setting Image to Black with OpenCV in Python\nDESCRIPTION: Illustrates filling a NumPy image array with zeros in Python. np.zeros_like creates a black image matching the input's shape and dtype. Image is fully reset. Requires numpy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\nimg = np.zeros_like(img)\n```\n\n----------------------------------------\n\nTITLE: Displaying Video Frames with Canvas in OpenCV.js\nDESCRIPTION: This snippet demonstrates converting and displaying video frames from a camera stream using a canvas element and OpenCV.js. It uses JavaScript to draw frames on a canvas, convert them to grayscale, and display them continuously, optimizing for a 30fps video display.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_video_display/js_video_display.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet canvasFrame = document.getElementById(\"canvasFrame\"); // canvasFrame is the id of <canvas>\nlet context = canvasFrame.getContext(\"2d\");\nlet src = new cv.Mat(height, width, cv.CV_8UC4);\nlet dst = new cv.Mat(height, width, cv.CV_8UC1);\n\nconst FPS = 30;\nfunction processVideo() {\n    let begin = Date.now();\n    context.drawImage(video, 0, 0, width, height);\n    src.data.set(context.getImageData(0, 0, width, height).data);\n    cv.cvtColor(src, dst, cv.COLOR_RGBA2GRAY);\n    cv.imshow(\"canvasOutput\", dst); // canvasOutput is the id of another <canvas>;\n    // schedule next one.\n    let delay = 1000/FPS - (Date.now() - begin);\n    setTimeout(processVideo, delay);\n}\n\n// schedule first one.\nsetTimeout(processVideo, 0);\n```\n\n----------------------------------------\n\nTITLE: Performing Dilation with OpenCV in Python\nDESCRIPTION: This Python code snippet demonstrates how to apply morphological dilation to an image using OpenCV. Much like the erosion function, users can specify the kernel shape, size, and anchor point for the operation. Dependencies: OpenCV-Python. Inputs: source image, kernel element. Outputs: dilated image. Advanced parameters (such as multiple iterations, border type, and value) are supported by OpenCV but not used in this snippet. The function is tailored for basic usage with customizability via kernel definition.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n@snippet python/tutorial_code/imgProc/erosion_dilatation/morphology_1.py dilation\n```\n\n----------------------------------------\n\nTITLE: Distance Transform and Watershed Application\nDESCRIPTION: Performs distance transform on binary image and applies watershed algorithm for segmentation\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/distance_transformation/distance_transform.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ndistanceTransform(bw, dist, DIST_L2, 3);\nnormalize(dist, dist, 0, 1.0, NORM_MINMAX);\nthreshold(dist, dist, 0.4, 1.0, THRESH_BINARY);\nwatershed(src, markers);\nMat mark = Mat::zeros(markers.size(), CV_8UC3);\nfor (int i = 0; i < markers.rows; i++) {\n    for (int j = 0; j < markers.cols; j++) {\n        int index = markers.at<int>(i,j);\n        if (index > 0 && index <= contours.size())\n            mark.at<Vec3b>(i,j) = colors[index-1];\n    }\n}\n```\n\nLANGUAGE: Python\nCODE:\n```\ndist = cv.distanceTransform(bw, cv.DIST_L2, 3)\ncv.normalize(dist, dist, 0, 1.0, cv.NORM_MINMAX)\n_, dist = cv.threshold(dist, 0.4, 1.0, cv.THRESH_BINARY)\nmarkers = cv.watershed(src, markers)\nmark = np.zeros(markers.shape + (3,), dtype=np.uint8)\nfor i in range(markers.shape[0]):\n    for j in range(markers.shape[1]):\n        index = markers[i,j]\n        if index > 0 and index <= len(contours):\n            mark[i,j] = colors[index-1]\n```\n\n----------------------------------------\n\nTITLE: Drawing an Ellipse in C++\nDESCRIPTION: Implementation of the MyEllipse function that draws a rotated ellipse in OpenCV C++. The function takes the image and angle, and uses the ellipse() function to draw the shape with specified color and thickness.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\nvoid MyEllipse( Mat img, double angle )\n{\n  int thickness = 2;\n  int lineType = 8;\n\n  ellipse( img,\n       Point( w/2, w/2 ),\n       Size( w/4, w/16 ),\n       angle,\n       0,\n       360,\n       Scalar( 255, 0, 0 ),\n       thickness,\n       lineType );\n}\n```\n\n----------------------------------------\n\nTITLE: Copying Mat Data Using clone/copyTo in OpenCV in C++\nDESCRIPTION: Demonstrates deep copying of Mat content in OpenCV C++. clone() and copyTo() allocate new storage and copy the data. Essential when input data might go out of scope. Output is independent copy of input image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_22\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat copy = img.clone();\n```\n\n----------------------------------------\n\nTITLE: Setting Up Training Data in C++\nDESCRIPTION: Creates and initializes training data arrays for SVM classification with labeled 2D points belonging to two different classes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_svm.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nfloat trainingData[N][2] = { {501, 10}, {255, 10}, {501, 255}, {10, 501} };\nfloat labels[N] = {1.0, -1.0, -1.0, -1.0};\n```\n\n----------------------------------------\n\nTITLE: Implementing Image Deskewing for Digit Recognition in Python OpenCV\nDESCRIPTION: Function that deskews a digit image using second order moments to improve recognition accuracy. Takes an input digit image and returns the deskewed version.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_svm/py_svm_opencv/py_svm_opencv.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef deskew(img):\n    m = cv2.moments(img)\n    if abs(m['mu02']) < 1e-2:\n        return img.copy()\n    skew = m['mu11']/m['mu02']\n    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n    img = cv2.warpAffine(img, M, (SZ, SZ), flags=cv2.WARP_INVERSE_MAP|cv2.INTER_LINEAR)\n    return img\n```\n\n----------------------------------------\n\nTITLE: Saving Images with OpenCV in Python\nDESCRIPTION: This snippet demonstrates saving an image using OpenCV's cv.imwrite in Python. It shows how to save the image array to a file, conditioned on a specific user input.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/display_image/display_image.markdown#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nif key == ord('s'):\n    cv.imwrite('starry_night.jpg', image)\n```\n\n----------------------------------------\n\nTITLE: Converting and Displaying Image with OpenCV.js\nDESCRIPTION: Shows how to convert image types and display them on a canvas using OpenCV.js. The code involves converting image data, creating a new ImageData object, and updating the canvas with this object.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_image_display/js_image_display.markdown#2025-04-22_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet dst = new cv.Mat();\n// scale and shift are used to map the data to [0, 255].\nsrc.convertTo(dst, cv.CV_8U, scale, shift);\n// *** is GRAY, RGB, or RGBA, according to src.channels() is 1, 3 or 4.\ncv.cvtColor(dst, dst, cv.COLOR_***2RGBA);\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet imgData = new ImageData(new Uint8ClampedArray(dst.data), dst.cols, dst.rows);\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet canvas = document.getElementById(canvasOutputId);\nlet ctx = canvas.getContext('2d');\nctx.clearRect(0, 0, canvas.width, canvas.height);\ncanvas.width = imgData.width;\ncanvas.height = imgData.height;\nctx.putImageData(imgData, 0, 0);\n```\n\n----------------------------------------\n\nTITLE: Defining Main Loop for Frame Processing\nDESCRIPTION: This JavaScript snippet illustrates the main loop handling continuous frame processing from a camera. The loop initializes upon loading OpenCV.js and downloading the deep learning models, detecting and recognizing faces frame by frame. The snippet requires OpenCV.js and access to a camera device.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_javascript/dnn_javascript.markdown#2025-04-22_snippet_3\n\nLANGUAGE: HTML\nCODE:\n```\n\"Define frames processing\"\n```\n\n----------------------------------------\n\nTITLE: Building Tests and Examples in OpenCV\nDESCRIPTION: This snippet specifies options to enable or disable building tests, examples, and applications in OpenCV. Different `cmake` options control accuracy, performance, and application compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncmake \\\n  -DBUILD_TESTS=ON \\\n  -DBUILD_PERF_TESTS=ON \\\n  -DBUILD_EXAMPLES=ON \\\n  -DBUILD_opencv_apps=ON \\\n  ../opencv\n```\n\n----------------------------------------\n\nTITLE: Creating Color Palette with OpenCV Trackbars in Python\nDESCRIPTION: This code creates a window with three trackbars for adjusting RGB values and a switch for turning the color display on/off. It demonstrates the use of cv.createTrackbar(), cv.getTrackbarPos(), and basic OpenCV window manipulation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_trackbar/py_trackbar.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\ndef nothing(x):\n    pass\n\n# Create a black image, a window\nimg = np.zeros((300,512,3), np.uint8)\ncv.namedWindow('image')\n\n# create trackbars for color change\ncv.createTrackbar('R','image',0,255,nothing)\n\ncv.createTrackbar('G','image',0,255,nothing)\ncv.createTrackbar('B','image',0,255,nothing)\n\n# create switch for ON/OFF functionality\nswitch = '0 : OFF \\n1 : ON'\ncv.createTrackbar(switch, 'image',0,1,nothing)\n\nwhile(1):\n    cv.imshow('image',img)\n    k = cv.waitKey(1) & 0xFF\n    if k == 27:\n        break\n\n    # get current positions of four trackbars\n    r = cv.getTrackbarPos('R','image')\n    g = cv.getTrackbarPos('G','image')\n    b = cv.getTrackbarPos('B','image')\n    s = cv.getTrackbarPos(switch,'image')\n\n    if s == 0:\n        img[:] = 0\n    else:\n        img[:] = [b,g,r]\n\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Multi-Threaded Frame Reading from Astra Camera Sensors\nDESCRIPTION: Implements threaded reading of frames from both depth and color sensors to prevent blocking. Each stream runs in its own thread, storing frames in lists with timestamps to enable later synchronization between the depth and color data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nlist<FrameWithTimestamp> depthFrames, colorFrames;\nmutex depthFramesMtx, colorFramesMtx;\ncondition_variable depthFramesCV, colorFramesCV;\natomic_bool isRunning;\n\nauto readDepth = [&]() {\n    Mat frame;\n    while (isRunning.load()) {\n        if (depthStream.grab()) {\n            if (depthStream.retrieve(frame, CAP_OPENNI_DEPTH_MAP)) {\n                auto now = chrono::high_resolution_clock::now();\n                auto frameWithTimestamp = FrameWithTimestamp(now, frame.clone());\n                {\n                    lock_guard<mutex> lk(depthFramesMtx);\n                    depthFrames.push_back(frameWithTimestamp);\n                }\n                depthFramesCV.notify_one();\n            }\n        }\n    }\n};\n\nauto readColor = [&]() {\n    Mat frame;\n    while (isRunning.load()) {\n        if (colorStream.grab()) {\n            if (colorStream.retrieve(frame)) {\n                auto now = chrono::high_resolution_clock::now();\n                auto frameWithTimestamp = FrameWithTimestamp(now, frame.clone());\n                {\n                    lock_guard<mutex> lk(colorFramesMtx);\n                    colorFrames.push_back(frameWithTimestamp);\n                }\n                colorFramesCV.notify_one();\n            }\n        }\n    }\n};\n\nthread depthThread(readDepth);\nthread colorThread(readColor);\n```\n\n----------------------------------------\n\nTITLE: Implementing SURF Feature Detection and Homography Matching in Java\nDESCRIPTION: This Java code demonstrates how to use SURF features and FLANN matching to detect a known object in an image. It uses Imgproc.findHomography to estimate the transformation and Core.perspectiveTransform to map points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_homography/feature_homography.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.*;\\nimport org.opencv.core.Core.MinMaxLocResult;\\nimport org.opencv.features2d.DescriptorMatcher;\\nimport org.opencv.features2d.Features2d;\\nimport org.opencv.imgcodecs.Imgcodecs;\\nimport org.opencv.imgproc.Imgproc;\\nimport org.opencv.xfeatures2d.SURF;\\n\\nclass SURFFLANNMatchingHomographyDemo {\\n\\n    public void run(String[] args) {\\n        if (args.length != 2) {\\n            System.out.println(\\\"Usage: ./SURFFLANNMatchingHomographyDemo <img1> <img2>\\\");\\n            System.exit(-1);\\n        }\\n\\n        Mat img_object = Imgcodecs.imread(args[0], Imgcodecs.IMREAD_GRAYSCALE);\\n        Mat img_scene = Imgcodecs.imread(args[1], Imgcodecs.IMREAD_GRAYSCALE);\\n\\n        if (img_object.empty() || img_scene.empty()) {\\n            System.out.println(\\\"Error reading images!\\\");\\n            System.exit(-1);\\n        }\\n\\n        //-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\\n        int minHessian = 400;\\n        SURF detector = SURF.create(minHessian);\\n        MatOfKeyPoint keypoints_object = new MatOfKeyPoint();\\n        MatOfKeyPoint keypoints_scene = new MatOfKeyPoint();\\n        Mat descriptors_object = new Mat();\\n        Mat descriptors_scene = new Mat();\\n        detector.detectAndCompute(img_object, new Mat(), keypoints_object, descriptors_object);\\n        detector.detectAndCompute(img_scene, new Mat(), keypoints_scene, descriptors_scene);\\n\\n        //-- Step 2: Matching descriptor vectors using FLANN matcher\\n        DescriptorMatcher matcher = DescriptorMatcher.create(DescriptorMatcher.FLANNBASED);\\n        MatOfDMatch matches = new MatOfDMatch();\\n        matcher.match(descriptors_object, descriptors_scene, matches);\\n        DMatch[] matchesArr = matches.toArray();\\n\\n        double max_dist = 0;\\n        double min_dist = 100;\\n\\n        //-- Quick calculation of max and min distances between keypoints\\n        for (int i = 0; i < descriptors_object.rows(); i++) {\\n            double dist = matchesArr[i].distance;\\n            if (dist < min_dist)\\n                min_dist = dist;\\n            if (dist > max_dist)\\n                max_dist = dist;\\n        }\\n\\n        System.out.println(\\\"-- Max dist : \\\" + max_dist);\\n        System.out.println(\\\"-- Min dist : \\\" + min_dist);\\n\\n        //-- Draw only \\\"good\\\" matches (i.e. whose distance is less than 3*min_dist )\\n        MatOfDMatch good_matches = new MatOfDMatch();\\n        for (int i = 0; i < descriptors_object.rows(); i++) {\\n            if (matchesArr[i].distance < 3 * min_dist) {\\n                good_matches.push_back(new MatOfDMatch(matchesArr[i]));\\n            }\\n        }\\n\\n        Mat img_matches = new Mat();\\n        Features2d.drawMatches(img_object, keypoints_object, img_scene, keypoints_scene, good_matches, img_matches);\\n\\n        //-- Localize the object\\n        MatOfPoint2f obj = new MatOfPoint2f();\\n        MatOfPoint2f scene = new MatOfPoint2f();\\n        KeyPoint[] keypoints_objectArr = keypoints_object.toArray();\\n        KeyPoint[] keypoints_sceneArr = keypoints_scene.toArray();\\n        DMatch[] good_matchesArr = good_matches.toArray();\\n\\n        for (int i = 0; i < good_matches.rows(); i++) {\\n            //-- Get the keypoints from the good matches\\n            obj.push_back(new MatOfPoint2f(keypoints_objectArr[good_matchesArr[i].queryIdx].pt));\\n            scene.push_back(new MatOfPoint2f(keypoints_sceneArr[good_matchesArr[i].trainIdx].pt));\\n        }\\n\\n        Mat H = Imgproc.findHomography(obj, scene, Imgproc.RANSAC, 5);\\n\\n        //-- Get the corners from the image_1 ( the object to be \\\"detected\\\" )\\n        Mat obj_corners = new Mat(4, 1, CvType.CV_32FC2);\\n        Mat scene_corners = new Mat(4, 1, CvType.CV_32FC2);\\n        obj_corners.put(0, 0, 0, 0);\\n        obj_corners.put(1, 0, img_object.cols(), 0);\\n        obj_corners.put(2, 0, img_object.cols(), img_object.rows());\\n        obj_corners.put(3, 0, 0, img_object.rows());\\n\\n        Core.perspectiveTransform(obj_corners, scene_corners, H);\\n\\n        //-- Draw lines between the corners (the mapped object in the scene - image_2 )\\n        Imgproc.line(img_matches, new Point(scene_corners.get(0, 0)), new Point(scene_corners.get(1, 0)), new Scalar(0, 255, 0), 4);\\n        Imgproc.line(img_matches, new Point(scene_corners.get(1, 0)), new Point(scene_corners.get(2, 0)), new Scalar(0, 255, 0), 4);\\n        Imgproc.line(img_matches, new Point(scene_corners.get(2, 0)), new Point(scene_corners.get(3, 0)), new Scalar(0, 255, 0), 4);\\n        Imgproc.line(img_matches, new Point(scene_corners.get(3, 0)), new Point(scene_corners.get(0, 0)), new Scalar(0, 255, 0), 4);\\n\\n        //-- Show detected matches\\n        HighGui.imshow(\\\"Good Matches & Object detection\\\", img_matches);\\n        HighGui.waitKey(0);\\n        System.exit(0);\\n    }\\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Image and Drawing Text using OpenCV in C++\nDESCRIPTION: This C++ code snippet, intended for use with the CMake approach, shows how to create a blank 8-bit unsigned integer image (`Mat`) of size 640x480. It then uses the `putText` function to draw the string \"Hello World!\" onto the image at a specified position with a specific font, size, and color. Finally, it displays the image in a window named \"My Window\" using `imshow` and waits for a key press with `waitKey`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_eclipse/linux_eclipse.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n#include <opencv2/opencv.hpp>\nusing namespace cv;\n\nint main ( int argc, char **argv )\n{\n  Mat img(480, 640, CV_8U);\n  putText(img, \"Hello World!\", Point( 200, 400 ), FONT_HERSHEY_SIMPLEX | FONT_ITALIC, 1.0, Scalar( 255, 255, 0 ));\n  imshow(\"My Window\", img);\n  waitKey();\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Computing Camera Displacement Transformation Matrix - C++\nDESCRIPTION: This code calculates the transformation matrix describing camera displacement between two views using matrix multiplication and inversion of pose matrices. Depends on previously solved camera poses. Used as a prerequisite for deriving the homography from physical movement. Inputs are rotation and translation matrices; output is a 4x4 pose matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet homography_from_camera_displacement.cpp compute-c2Mc1\n```\n\n----------------------------------------\n\nTITLE: Generating ArUco Marker Images in C++\nDESCRIPTION: Code snippet demonstrating how to generate ArUco marker images using the OpenCV aruco module. This example creates a marker image for marker ID 23 from a predefined dictionary, with a size of 200x200 pixels and a border width of 1.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_detection/aruco_detection.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat markerImage;\ncv::aruco::Dictionary dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250);\ncv::aruco::generateImageMarker(dictionary, 23, 200, markerImage, 1);\ncv::imwrite(\"marker23.png\", markerImage);\n```\n\n----------------------------------------\n\nTITLE: Reading ONNX Model with OpenCV\nDESCRIPTION: This Python snippet shows how to load an ONNX model with OpenCV's DNN module. The cv.dnn.readNetFromONNX function imports the previously converted ONNX model, allowing inference operations using OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nopencv_net = cv2.dnn.readNetFromONNX(full_model_path)\n```\n\n----------------------------------------\n\nTITLE: Applying Fourier Transform using OpenCV DFT\nDESCRIPTION: Illustrates the use of OpenCV's `cv.dft()` to compute the Fourier Transform of an image. It requires OpenCV and Matplotlib. The input image is converted to `np.float32`, then transformed using `cv.dft()`. Magnitude spectrum visualization is achieved using `cv.magnitude()`. Input is a grayscale image, and the output is a magnitude spectrum plot.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('messi5.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n\ndft = cv.dft(np.float32(img),flags = cv.DFT_COMPLEX_OUTPUT)\ndft_shift = np.fft.fftshift(dft)\n\nmagnitude_spectrum = 20*np.log(cv.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))\n\nplt.subplot(121),plt.imshow(img, cmap = 'gray')\nplt.title('Input Image'), plt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(magnitude_spectrum, cmap = 'gray')\nplt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Detecting Faces and Eyes with Haar Cascades in Python using OpenCV\nDESCRIPTION: This Python script demonstrates face and eye detection using OpenCV's Python bindings. It creates `cv2.CascadeClassifier` objects and loads the pre-trained Haar cascade XML files for faces and eyes. The script reads frames from a video source, converts each frame to grayscale using `cv2.cvtColor`, and then calls `detectMultiScale` to detect faces. For each detected face, it defines a region of interest (ROI) and runs `detectMultiScale` again to find eyes within that ROI. Detected objects are highlighted with rectangles using `cv2.rectangle`, and the resulting frame is shown using `cv2.imshow`. Requires the `cv2` (OpenCV) and `numpy` Python packages, along with the specified Haar cascade XML files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/cascade_classifier.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# This tutorial code's is shown lines below. You can also download it from\n# [here](https://github.com/opencv/opencv/tree/4.x/samples/python/tutorial_code/objectDetection/cascade_classifier/objectDetection.py)\n@include samples/python/tutorial_code/objectDetection/cascade_classifier/objectDetection.py\n```\n\n----------------------------------------\n\nTITLE: Setting Video Properties with VideoCapture::set in C++\nDESCRIPTION: Demonstrates how to modify video properties using the `set()` method of `cv::VideoCapture`, specifically for seeking within a video file. It uses property identifiers like `CAP_PROP_POS_MSEC` (to seek to a specific time in milliseconds) or `CAP_PROP_POS_FRAMES` (to seek to a specific frame number), passing the target position as a double value. Subsequent read operations will retrieve the frame at the newly set position. Requires `<opencv2/videoio.hpp>` header.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_input_psnr_ssim.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\ncaptRefrnc.set(CAP_PROP_POS_MSEC, 1.2);  // go to the 1.2 second in the video\ncaptRefrnc.set(CAP_PROP_POS_FRAMES, 10); // go to the 10th frame of the video\n// now a read operation would read the frame at the set position\n```\n\n----------------------------------------\n\nTITLE: Homography Check for Matches with OpenCV in C++\nDESCRIPTION: The C++ code provides a homography check to verify if matched keypoints fit a specified model, adding robustness to keypoint matching using OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nsamples/cpp/tutorial_code/features2D/AKAZE_match.cpp homography check\n```\n\n----------------------------------------\n\nTITLE: Template Matching Formula: TM_SQDIFF_NORMED (LaTeX)\nDESCRIPTION: Mathematical formula for the Normalized Squared Difference (TM_SQDIFF_NORMED) template matching method used in OpenCV's `matchTemplate` function. R(x,y) is the result, T is the template, and I is the image. Normalization makes the result invariant to global intensity changes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_2\n\nLANGUAGE: latex\nCODE:\n```\n\\f[R(x,y)= \\frac{\\sum_{x',y'} (T(x',y')-I(x+x',y+y'))^2}{\\sqrt{\\sum_{x',y'}T(x',y')^2 \\cdot \\sum_{x',y'} I(x+x',y+y')^2}}\\f]\n```\n\n----------------------------------------\n\nTITLE: Drawing Detected Circles\nDESCRIPTION: Visualizing the detected circles by drawing them on the original image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_circle/hough_circle.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nfor( size_t i = 0; i < circles.size(); i++ )\n{\n    Vec3i c = circles[i];\n    Point center = Point(c[0], c[1]);\n    // circle center\n    circle( src, center, 1, Scalar(0,255,0), 3, LINE_AA);\n    // circle outline\n    int radius = c[2];\n    circle( src, center, radius, Scalar(0,0,255), 3, LINE_AA);\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\nif (circles.rows() > 0) {\n    for (int x = 0; x < circles.cols(); x++) {\n        double[] c = circles.get(0, x);\n        Point center = new Point(Math.round(c[0]), Math.round(c[1]));\n        // circle center\n        Imgproc.circle(src, center, 1, new Scalar(0,255,0), 3, 8, 0);\n        // circle outline\n        int radius = (int) Math.round(c[2]);\n        Imgproc.circle(src, center, radius, new Scalar(0,0,255), 3, 8, 0);\n    }\n}\n```\n\nLANGUAGE: Python\nCODE:\n```\nif circles is not None:\n    circles = np.uint16(np.around(circles))\n    for i in circles[0, :]:\n        center = (i[0], i[1])\n        # circle center\n        cv.circle(src, center, 1, (0, 255, 0), 3)\n        # circle outline\n        radius = i[2]\n        cv.circle(src, center, radius, (0, 0, 255), 3)\n```\n\n----------------------------------------\n\nTITLE: Drawing 3D Axes on an Image with OpenCV (Python)\nDESCRIPTION: Defines a function to render the 3D coordinate axes on an image based on corner points (from cv.findChessboardCorners) and projected 3D points. Uses OpenCV's cv.line to draw colored axes, requiring NumPy for data manipulation and OpenCV for drawing. Accepts an image, the corner array, and projected axis endpoints, and returns an image with overlaid axes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_pose/py_pose.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef draw(img, corners, imgpts):\\n    corner = tuple(corners[0].ravel().astype(\"int32\"))\\n    imgpts = imgpts.astype(\"int32\")\\n    img = cv.line(img, corner, tuple(imgpts[0].ravel()), (255,0,0), 5)\\n    img = cv.line(img, corner, tuple(imgpts[1].ravel()), (0,255,0), 5)\\n    img = cv.line(img, corner, tuple(imgpts[2].ravel()), (0,0,255), 5)\\n    return img\n```\n\n----------------------------------------\n\nTITLE: Applying Blur Filter with OpenCV in C++\nDESCRIPTION: This C++ snippet demonstrates the use of OpenCV's blur() function to apply a normalized box filter for image smoothing. It requires OpenCV library and takes source and destination image parameters, kernel size, and anchor point.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n@snippet cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp blur\n```\n\n----------------------------------------\n\nTITLE: Finding Minimum and Maximum Match Values/Locations (Python)\nDESCRIPTION: Calls `cv2.minMaxLoc` on the normalized result matrix. This function returns the minimum value, maximum value, location (coordinates) of the minimum value, and location of the maximum value found in the matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/match_template/match_template.py best_match\n```\n\n----------------------------------------\n\nTITLE: Implementing GrabCut Algorithm with Rectangular Initialization in OpenCV Python\nDESCRIPTION: This code snippet demonstrates how to use the cv.grabCut() function with rectangular initialization to extract the foreground from an image. It loads an image, creates a mask, and applies the GrabCut algorithm using a predefined rectangle.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_grabcut/py_grabcut.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('messi5.jpg')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nmask = np.zeros(img.shape[:2],np.uint8)\n\nbgdModel = np.zeros((1,65),np.float64)\nfgdModel = np.zeros((1,65),np.float64)\n\nrect = (50,50,450,290)\ncv.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv.GC_INIT_WITH_RECT)\n\nmask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\nimg = img*mask2[:,:,np.newaxis]\n\nplt.imshow(img),plt.colorbar(),plt.show()\n```\n\n----------------------------------------\n\nTITLE: Reading Camera Parameters\nDESCRIPTION: Demonstrates how to read camera calibration parameters from file\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_detection/charuco_detection.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nbool readCameraParameters(std::string filename, cv::Mat &camMatrix, cv::Mat &distCoeffs)\n```\n\n----------------------------------------\n\nTITLE: Upsampling Image using pyrUp in C++\nDESCRIPTION: Upsamples the image `tmp` using the `pyrUp` function. The destination image `dst` will have dimensions double that of `tmp`. The size is explicitly provided.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n        else if( c == 'i' )\n        { pyrUp( tmp, dst, Size( tmp.cols*2, tmp.rows*2 ) ); printf(\"** Zoom In: Image x 2 \\n\"); }\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Corner Detectors in Python\nDESCRIPTION: Python implementation of custom Harris and Shi-Tomasi corner detectors using OpenCV functions. Includes image processing, corner detection, and result visualization with interactive quality level adjustment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/generic_corner_detector/generic_corner_detector.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\\nimport cv2 as cv\\nfrom math import pow\\n\\nsource_window = 'Source image'\\ncorners_window = 'Corners detected'\\nmax_thresh = 255\\n\\ndef cornerHarris_demo(val):\\n    thresh = val\\n    # Detector parameters\\n    blockSize = 2\\n    apertureSize = 3\\n    k = 0.04\\n    # Detecting corners\\n    dst = cv.cornerHarris(src_gray, blockSize, apertureSize, k)\\n    # Normalizing\\n    dst_norm = np.empty(dst.shape, dtype=np.float32)\\n    cv.normalize(dst, dst_norm, alpha=0, beta=255, norm_type=cv.NORM_MINMAX)\\n    dst_norm_scaled = cv.convertScaleAbs(dst_norm)\\n    # Drawing a circle around corners\\n    for i in range(dst_norm.shape[0]):\\n        for j in range(dst_norm.shape[1]):\\n            if int(dst_norm[i,j]) > thresh:\\n                cv.circle(dst_norm_scaled, (j,i), 5, (0), 2)\\n    # Showing the result\\n    cv.namedWindow(corners_window)\\n    cv.imshow(corners_window, dst_norm_scaled)\\n\\n# Load source image and convert it to gray\\nsrc = cv.imread(cv.samples.findFile('building.jpg'))\\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\\n# Create a window and a trackbar\\ncv.namedWindow(source_window)\\nthresh = 200 # initial threshold\\ncv.createTrackbar('Threshold: ', source_window, thresh, max_thresh, cornerHarris_demo)\\ncv.imshow(source_window, src)\\ncornerHarris_demo(thresh)\\ncv.waitKey()\\n\n```\n\n----------------------------------------\n\nTITLE: Show Images in C++\nDESCRIPTION: Displays images within OpenCV windows in a C++ application to allow visualization of processed frames.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold_inRange/threshold_inRange.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\ncv::imshow(\"Window Name\", frame);\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Input Image for DNN Model using OpenCV\nDESCRIPTION: This Python snippet preprocesses an input image for inference with a neural network using OpenCV. It reads an image using `cv2.imread`, converts it to float32, defines preprocessing parameters (mean subtraction values `mean`, scaling factor `scale`, standard deviation values `std`), and uses `cv2.dnn.blobFromImage` to create a 4D blob suitable for network input. The `blobFromImage` function handles resizing, mean subtraction (note the scaling of `mean` by 255.0 to match PyTorch's common practice), scaling, and channel swapping (BGR to RGB). Finally, it performs division by the standard deviation element-wise. Requires `cv2` (OpenCV) and `numpy`. The specific mean and std values ([0.485, 0.456, 0.406] and [0.229, 0.224, 0.225]) are standard for models pre-trained on ImageNet.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# read the image\ninput_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\ninput_img = input_img.astype(np.float32)\n\n# target image sizes\nimg_height = input_img.shape[0]\nimg_width = input_img.shape[1]\n\n# define preprocess parameters\nmean = np.array([0.485, 0.456, 0.406]) * 255.0\nscale = 1 / 255.0\nstd = [0.229, 0.224, 0.225]\n\n# prepare input blob to fit the model input:\n# 1. subtract mean\n# 2. scale to set pixel values from 0 to 1\ninput_blob = cv2.dnn.blobFromImage(\n    image=input_img,\n    scalefactor=scale,\n    size=(img_width, img_height),  # img target size\n    mean=mean,\n    swapRB=True,  # BGR -> RGB\n    crop=False  # center crop\n)\n# 3. divide by std\ninput_blob[0] /= np.asarray(std, dtype=np.float32).reshape(3, 1, 1)\n```\n\n----------------------------------------\n\nTITLE: Comparing Arithmetic Performance in Python and Numpy Using IPython\nDESCRIPTION: Uses IPython's %timeit magic to benchmark different methods for squaring a value in Python: native Python arithmetic, numpy uint8 arrays, and numpy's np.square function. Demonstrates that simple Python scalars are significantly faster for single-value operations than corresponding NumPy methods. Requires IPython, numpy, and OpenCV if using cv2 arrays. Inputs: scalar or numpy array values. Outputs: measured execution speed. Mainly targets performance comparison scenarios.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_optimization/py_optimization.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nIn [10]: x = 5\n\nIn [11]: %timeit y=x**2\n10000000 loops, best of 3: 73 ns per loop\n\nIn [12]: %timeit y=x*x\n10000000 loops, best of 3: 58.3 ns per loop\n\nIn [15]: z = np.uint8([5])\n\nIn [17]: %timeit y=z*z\n1000000 loops, best of 3: 1.25 us per loop\n\nIn [19]: %timeit y=np.square(z)\n1000000 loops, best of 3: 1.16 us per loop\n```\n\n----------------------------------------\n\nTITLE: Checking Image Load Success in OpenCV C++\nDESCRIPTION: This code checks whether an image was successfully loaded in C++ utilizing the empty method of the cv::Mat class. If the image is empty, error handling can be performed accordingly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/display_image/display_image.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nif(image.empty()) {\n    std::cerr << \"Could not open or find the image!\" << std::endl;\n    return -1;\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Serialization Usage with FileStorage for Custom Classes - OpenCV C++\nDESCRIPTION: These snippets show how to use custom read and write functions for a user-defined class (MyData) with OpenCV FileStorage. The << operator is used for writing and >> for reading, leveraging the serialization support implemented in the class. The code assumes all methods are previously defined.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\nMyData m1;\\nfs << \"MyData\" << m1;\\nMyData m2;\\nfs[\"MyData\"] >> m2;\n```\n\n----------------------------------------\n\nTITLE: Opening Depth and Color Streams with OpenCV for Orbbec Astra Camera\nDESCRIPTION: Creates two VideoCapture objects to access the depth sensor through OpenNI2 API and the color sensor through Video4Linux2 interface. This is necessary because the Astra Pro camera requires two different interfaces to access its sensors.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nVideoCapture depthStream(CAP_OPENNI2_ASTRA);\nVideoCapture colorStream(0);\n```\n\n----------------------------------------\n\nTITLE: Visualizing Matches and Inliers using cv.drawMatches in OpenCV (Python)\nDESCRIPTION: This snippet draws the matching keypoints between the two images, highlighting inliers determined by the homography estimation. It leverages cv.drawMatches for visualization and matplotlib for display. Dependencies: Previous computation of good matches and matchesMask, OpenCV, Matplotlib. Key parameters: matching color, which matches to draw, and keypoints. Outputs the visualized result as a displayed image. Intended as the final visualization step following feature matching and homography.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndraw_params = dict(matchColor = (0,255,0), # draw matches in green color\\n                   singlePointColor = None,\\n                   matchesMask = matchesMask, # draw only inliers\\n                   flags = 2)\\n\\nimg3 = cv.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)\\n\\nplt.imshow(img3, 'gray'),plt.show()\n```\n\n----------------------------------------\n\nTITLE: Projecting Points with Homography in OpenCV C++\nDESCRIPTION: Uses the perspectiveTransform function to map points from one image to another using the computed homography matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/detection_of_planar_objects/detection_of_planar_objects.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nMat points1Projected; perspectiveTransform(Mat(points1), points1Projected, H);\n```\n\n----------------------------------------\n\nTITLE: Benchmarking OpenCV and Numpy Array Counting with IPython\nDESCRIPTION: Benchmarks the performance of counting nonzero elements in an array using both OpenCV's cv.countNonZero and numpy's np.count_nonzero using IPython's %timeit. Shows that OpenCV's implementation is considerably faster when counting nonzero pixels in an image. Dependencies are OpenCV, numpy, and IPython. Inputs: image/array 'img'. Outputs: execution time for both implementations. Useful for guiding method selection in performance-critical code.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_optimization/py_optimization.markdown#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nIn [35]: %timeit z = cv.countNonZero(img)\n100000 loops, best of 3: 15.8 us per loop\n\nIn [36]: %timeit z = np.count_nonzero(img)\n1000 loops, best of 3: 370 us per loop\n```\n\n----------------------------------------\n\nTITLE: Visualizing 2D Histogram using Matplotlib\nDESCRIPTION: This code demonstrates how to visualize a 2D histogram using Matplotlib. It calculates a 2D histogram of an image's Hue and Saturation channels, then plots it using matplotlib's imshow function with nearest neighbor interpolation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_2d_histogram/py_2d_histogram.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('home.jpg')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nhsv = cv.cvtColor(img,cv.COLOR_BGR2HSV)\nhist = cv.calcHist( [hsv], [0, 1], None, [180, 256], [0, 180, 0, 256] )\n\nplt.imshow(hist,interpolation = 'nearest')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Performing Face Recognition and Matching\nDESCRIPTION: This JavaScript snippet is part of a face recognition process using OpenCV.js, designed to match a new feature vector with previously registered vectors and return the best match's name. The OpenCV.js library is a required dependency. Inputs include a newly obtained feature vector and a database of registered vectors.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_javascript/dnn_javascript.markdown#2025-04-22_snippet_2\n\nLANGUAGE: HTML\nCODE:\n```\n\"Recognize\"\n```\n\n----------------------------------------\n\nTITLE: Finding HSV Value of a Color using OpenCV in Python\nDESCRIPTION: This snippet shows how to convert a BGR array to HSV to determine the HSV representation of a specific color. Requires numpy and OpenCV (cv2). Key parameter is the BGR value assigned to 'green'. Outputs corresponding HSV value, which can be used for color thresholding. Useful in configuring color extraction parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> green = np.uint8([[[0,255,0 ]]])\\n>>> hsv_green = cv.cvtColor(green,cv.COLOR_BGR2HSV)\\n>>> print( hsv_green )\\n[[[ 60 255 255]]]\n```\n\n----------------------------------------\n\nTITLE: Calculating Convex Hull in Java using OpenCV\nDESCRIPTION: This Java code snippet shows how to use OpenCV bindings to load an image, find contours via thresholding, compute the convex hull for each contour using `Imgproc.convexHull`, and visualize the results by drawing the contours and hulls. It depends on the OpenCV Java library and its native components.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/hull/hull.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n@add_toggle_java\nThis tutorial code's is shown lines below. You can also download it from\n[here](https://github.com/opencv/opencv/tree/4.x/samples/java/tutorial_code/ShapeDescriptors/hull/HullDemo.java)\n@include samples/java/tutorial_code/ShapeDescriptors/hull/HullDemo.java\n@end_toggle\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple OpenCV Application (Java)\nDESCRIPTION: This Java code defines a simple application named `SimpleSample`. It demonstrates basic OpenCV functionality by loading the native OpenCV library using `System.loadLibrary()`, printing the OpenCV version, creating a 5x10 matrix (`Mat`) initialized with zeros, modifying specific rows and columns, and printing the final matrix data using `m.dump()`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n    import org.opencv.core.Core;\n    import org.opencv.core.Mat;\n    import org.opencv.core.CvType;\n    import org.opencv.core.Scalar;\n\n    class SimpleSample {\n\n      static{ System.loadLibrary(Core.NATIVE_LIBRARY_NAME); }\n\n      public static void main(String[] args) {\n        System.out.println(\"Welcome to OpenCV \" + Core.VERSION);\n        Mat m = new Mat(5, 10, CvType.CV_8UC1, new Scalar(0));\n        System.out.println(\"OpenCV Mat: \" + m);\n        Mat mr1 = m.row(1);\n        mr1.setTo(new Scalar(1));\n        Mat mc5 = m.col(5);\n        mc5.setTo(new Scalar(5));\n        System.out.println(\"OpenCV Mat data:\\n\" + m.dump());\n      }\n\n    }\n```\n\n----------------------------------------\n\nTITLE: Extracting Frozen Graph from TensorFlow Model Archive - Python\nDESCRIPTION: This code handles the download and extraction of a TensorFlow detection model archive (.tar.gz), specifically extracting the 'frozen_inference_graph.pb' file for further processing. Dependencies include the 'urllib', 'tarfile', and 'os' Python libraries. It manages download errors, prints retrieval status, and searches the archive for the correct graph file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# define model archive name\\ntf_model_tar = model_name + '.tar.gz'\\n# define link to retrieve model archive\\nmodel_link = DETECTION_MODELS_URL + tf_model_tar\\n\\ntf_frozen_graph_name = 'frozen_inference_graph'\\n\\ntry:\\n    urllib.request.urlretrieve(model_link, tf_model_tar)\\nexcept Exception:\\n    print(\"TF {} was not retrieved: {}\".format(model_name, model_link))\\n    return\\n\\nprint(\"TF {} was retrieved.\".format(model_name))\\n\\ntf_model_tar = tarfile.open(tf_model_tar)\\nfrozen_graph_path = \"\"\\n\\nfor model_tar_elem in tf_model_tar.getmembers():\\n    if tf_frozen_graph_name in os.path.basename(model_tar_elem.name):\\n        tf_model_tar.extract(model_tar_elem, extracted_model_path)\\n        frozen_graph_path = os.path.join(extracted_model_path, model_tar_elem.name)\\n        break\\ntf_model_tar.close()\n```\n\n----------------------------------------\n\nTITLE: Calculating Object Orientation via PCA in Python using OpenCV\nDESCRIPTION: Defines a `getOrientation` function that takes a contour (list of points) and calculates its orientation using PCA (`cv.PCACompute`). It prepares the data, runs PCA, extracts the mean (center), eigenvectors, and eigenvalues, and visualizes the principal components on the input image. Requires the OpenCV Python library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#! [pca]\n# Perform PCA analysis\ndef getOrientation(pts, img):\n    # Construct a buffer used by the PCA analysis\n    sz = len(pts)\n    data_pts = np.empty((sz, 2), dtype=np.float64)\n    for i in range(data_pts.shape[0]):\n        data_pts[i,0] = pts[i,0,0]\n        data_pts[i,1] = pts[i,0,1]\n\n    # Perform PCA analysis\n    mean = np.empty((0))\n    mean, eigenvectors, eigenvalues = cv.PCACompute2(data_pts, mean)\n\n    # Store the center of the object\n    cntr = (int(mean[0,0]), int(mean[0,1]))\n\n    # Store the eigenvalues and eigenvectors\n    eigenvecs = [None]*2\n    eigenvals = [None]*2\n    eigenvecs[0] = (eigenvectors[0,0], eigenvectors[0,1])\n    eigenvecs[1] = (eigenvectors[1,0], eigenvectors[1,1])\n    eigenvals[0] = eigenvalues[0,0]\n    eigenvals[1] = eigenvalues[1,0]\n\n\n    # Draw the principal components\n    cv.circle(img, cntr, 3, (255, 0, 255), 2)\n    p1 = (cntr[0] + 0.02 * eigenvecs[0][0] * eigenvals[0], cntr[1] + 0.02 * eigenvecs[0][1] * eigenvals[0])\n    p2 = (cntr[0] - 0.02 * eigenvecs[1][0] * eigenvals[1], cntr[1] - 0.02 * eigenvecs[1][1] * eigenvals[1])\n    drawAxis(img, cntr, p1, (0, 255, 0), 1)\n    drawAxis(img, cntr, p2, (255, 255, 0), 5)\n\n    angle = atan2(eigenvecs[0][1], eigenvecs[0][0]) # orientation in radians\n\n    return angle\n#! [pca]\n```\n\n----------------------------------------\n\nTITLE: Running While Loop for Video Processing in C++\nDESCRIPTION: Harnesses a while loop to continuously capture and process frames in a C++ OpenCV application until termination by the user. Ensures ongoing video analysis.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold_inRange/threshold_inRange.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nwhile(true) {\\n    // Capture frame-by-frame\\n    if(waitKey(30) >= 0) break;\\n}\n```\n\n----------------------------------------\n\nTITLE: Running Camera Calibration and Saving Results in C++\nDESCRIPTION: This snippet demonstrates how to run the camera calibration process and save the results to an XML or YAML file using OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nrun_and_save\n```\n\n----------------------------------------\n\nTITLE: Extracting 128-Dimensional Feature Vectors for Face Recognition\nDESCRIPTION: This snippet uses JavaScript to run a face recognition network with OpenCV.js. It takes an RGB image of a face with a size of 96x96 pixels and returns a 128-dimensional feature vector representing the face as a point on a multidimensional sphere. Dependencies include the OpenCV.js library and a pre-processed face image of the specified dimensions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_javascript/dnn_javascript.markdown#2025-04-22_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n\"Get 128 floating points feature vector\"\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Model to ONNX\nDESCRIPTION: This Python snippet demonstrates converting a PyTorch ResNet-50 model to ONNX format. It uses torch.onnx.export function, specifying model input, output paths, and ONNX export options. The resulting ONNX model is ready for integration with OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\noriginal_model = models.resnet50(pretrained=True)\n\nonnx_model_path = \"models\"\nonnx_model_name = \"resnet50.onnx\"\nos.makedirs(onnx_model_path, exist_ok=True)\nfull_model_path = os.path.join(onnx_model_path, onnx_model_name)\n\ngenerated_input = Variable(\n    torch.randn(1, 3, 224, 224)\n)\ntorch.onnx.export(\n    original_model,\n    generated_input,\n    full_model_path,\n    verbose=True,\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    opset_version=11\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Parameters for Text Recognition Model in C++\nDESCRIPTION: This C++ snippet sets up the parameters for a text recognition model using OpenCV DNN. It is essential to configure normalization parameters and input shape which are crucial for model inference. Scale and mean values are used for input normalization, and input size denotes the image dimensions expected by the model.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_text_spotting/dnn_text_spotting.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// Normalization parameters\ndouble scale = 1.0 / 127.5;\nScalar mean = Scalar(127.5, 127.5, 127.5);\n\n// The input shape\nSize inputSize = Size(100, 32);\n\nmodel.setInputParams(scale, inputSize, mean);\n```\n\n----------------------------------------\n\nTITLE: OpenCV DNN Model Inference Command-Line Execution - Console\nDESCRIPTION: This command runs a compiled OpenCV C++ executable to perform inference using the converted ResNet-50 ONNX model on a given input image. All major preprocessing parameters (resize, normalization, cropping, and label file) are set to match the PyTorch pipeline. The command assumes the binary was built with samples enabled; all file and directory paths must be correct.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_9\n\nLANGUAGE: console\nCODE:\n```\n./dnn/example_dnn_classification --model=../dnn/models/resnet50.onnx --input=../data/squirrel_cls.jpg --width=224 --height=224 --rgb=true --scale=\"0.003921569\" --mean=\"123.675 116.28 103.53\" --std=\"0.229 0.224 0.225\" --crop=true --initial_width=256 --initial_height=256 --classes=../data/dnn/classification_classes_ILSVRC2012.txt\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing for PCA using OpenCV in C++\nDESCRIPTION: Reads an input image, converts it to grayscale, and then applies binary thresholding. This prepares the image for contour detection, isolating objects of interest from the background. Requires the OpenCV library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n//! [pre-process]\n// Load image\nMat src = imread(argv[1]);\n// Check if image is loaded successfully\nif(src.empty())\n{\n    cout << \"Problem loading image!!!\" << endl;\n    return 0;\n}\n\nimshow(\"src\", src);\n\n// Convert image to grayscale\nMat gray;\ncvtColor(src, gray, COLOR_BGR2GRAY);\n\n// Convert image to binary\nMat bw;\nthreshold(gray, bw, 50, 255, THRESH_BINARY | THRESH_OTSU);\n//! [pre-process]\n```\n\n----------------------------------------\n\nTITLE: Finding Min/Max Intensity and Locations within Contour Mask in OpenCV Python\nDESCRIPTION: This snippet finds the minimum and maximum intensity values, along with their corresponding locations, within a specific region of an image defined by a mask. It uses the `cv.minMaxLoc` function applied to the grayscale image (`imgray`), restricting the search to the area specified by the `mask`. Requires the grayscale image `imgray` and a pre-computed `mask` image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_properties/py_contour_properties.markdown#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmin_val, max_val, min_loc, max_loc = cv.minMaxLoc(imgray,mask = mask)\n```\n\n----------------------------------------\n\nTITLE: Tonemapping HDR Images to 8-bit Display Range in OpenCV\nDESCRIPTION: This code converts the HDR image to an 8-bit range that can be displayed on standard monitors. It uses tonemapping with bilateral filtering and applies gamma correction with a value of 2.2.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/hdr_imaging.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nMat ldr;\nPtr<TonemapDurand> tonemap = createTonemapDurand(2.2f);\ntonemap->process(hdr, ldr);\nMat ldr_8bit;\nldr = 255 * ldr;\nldr.convertTo(ldr_8bit, CV_8UC3);\n```\n\nLANGUAGE: java\nCODE:\n```\nMat ldr = new Mat();\nTonemapDurand tonemap = Photo.createTonemapDurand(2.2f);\ntonemap.process(hdr, ldr);\nldr.convertTo(ldr, ldr.type(), 255, 0);\nMat ldr8bit = new Mat();\nldr.convertTo(ldr8bit, CvType.CV_8UC3);\n```\n\nLANGUAGE: python\nCODE:\n```\ntonemap = cv.createTonemapDurand(2.2)\nldr = tonemap.process(hdr)\nldr = 255 * ldr\nldr_8bit = np.clip(ldr, 0, 255).astype('uint8')\n```\n\n----------------------------------------\n\nTITLE: Detecting and Filtering Contours in Python using OpenCV\nDESCRIPTION: Finds contours in the binary image `bw` using `cv.findContours`. It iterates through the detected contours, filters them by area using `cv.contourArea` to keep only reasonably sized shapes, and then proceeds to find their orientation. Requires the binary image `bw` and the OpenCV Python library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#! [contours]\n# Find all the contours in the thresholded image\ncontours, _ = cv.findContours(bw, cv.RETR_LIST, cv.CHAIN_APPROX_NONE)\n\nfor i, c in enumerate(contours):\n    # Calculate the area of each contour\n    area = cv.contourArea(c)\n    # Ignore contours that are too small or too large\n    if area < 1e2 or 1e5 < area:\n        continue\n\n    # Draw each contour only for visualisation purposes\n    cv.drawContours(src, contours, i, (0, 0, 255), 2)\n    # Find the orientation of each shape\n    getOrientation(c, src)\n#! [contours]\n```\n\n----------------------------------------\n\nTITLE: Detecting and Filtering Contours in C++ using OpenCV\nDESCRIPTION: Finds contours in a binary image using `findContours`. It then iterates through the detected contours, filtering them based on a minimum area threshold to discard small noise-like contours. Requires a binary input image (bw).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n//! [contours]\n// Find all the contours in the thresholded image\nvector<vector<Point> > contours;\nfindContours(bw, contours, RETR_LIST, CHAIN_APPROX_NONE);\n\nfor (size_t i = 0; i < contours.size(); ++i)\n{\n    // Calculate the area of each contour\n    double area = contourArea(contours[i]);\n    // Ignore contours that are too small or too large\n    if (area < 1e2 || 1e5 < area)\n        continue;\n\n    // Draw each contour only for visualisation purposes\n    drawContours(src, contours, static_cast<int>(i), Scalar(0, 0, 255), 2);\n    // Find the orientation of each shape\n    getOrientation(contours[i], src);\n}\n//! [contours]\n```\n\n----------------------------------------\n\nTITLE: Solving for Chessboard Pose using solvePnP in OpenCV C++\nDESCRIPTION: This snippet uses the solvePnP function to determine the chessboard’s pose relative to the camera. The inputs are the known 3D points and the corresponding 2D image points, along with the camera matrix and distortion coefficients. The rotation (rvec) and translation (tvec) vectors obtained are crucial for further transformations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_square_chess/camera_calibration_square_chess.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nvector<Point3f> boardPoints;\n// fill the array\n...\nsolvePnP(Mat(boardPoints), Mat(foundBoardCorners), cameraMatrix,\n                         distCoeffs, rvec, tvec, false);\n```\n\n----------------------------------------\n\nTITLE: Implementing Lucas-Kanade Optical Flow Tracking in C++\nDESCRIPTION: Demonstrates how to track feature points in a video using the Lucas-Kanade optical flow method in C++. The code first detects Shi-Tomasi corner points and then tracks these points through video frames using cv.calcOpticalFlowPyrLK().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/optical_flow.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <iostream>\n#include <opencv2/core.hpp>\n#include <opencv2/highgui.hpp>\n#include <opencv2/imgproc.hpp>\n#include <opencv2/videoio.hpp>\n#include <opencv2/video.hpp>\n\nusing namespace cv;\nusing namespace std;\n\nint main() {\n    VideoCapture capture(samples::findFile(\"vtest.avi\"));\n    if (!capture.isOpened()){\n        //error in opening the video input\n        cerr << \"Unable to open file!\" << endl;\n        return 0;\n    }\n\n    // Create some random colors\n    vector<Scalar> colors;\n    RNG rng;\n    for(int i = 0; i < 100; i++)\n    {\n        int r = rng.uniform(0, 256);\n        int g = rng.uniform(0, 256);\n        int b = rng.uniform(0, 256);\n        colors.push_back(Scalar(r,g,b));\n    }\n\n    Mat old_frame, old_gray;\n    vector<Point2f> p0, p1;\n\n    // Take first frame and find corners in it\n    capture >> old_frame;\n    cvtColor(old_frame, old_gray, COLOR_BGR2GRAY);\n    goodFeaturesToTrack(old_gray, p0, 100, 0.3, 7, Mat(), 7, false, 0.04);\n\n    // Create a mask image for drawing purposes\n    Mat mask = Mat::zeros(old_frame.size(), old_frame.type());\n\n    while(true){\n        Mat frame, frame_gray;\n        capture >> frame;\n        if (frame.empty())\n            break;\n        cvtColor(frame, frame_gray, COLOR_BGR2GRAY);\n\n        // calculate optical flow\n        vector<uchar> status;\n        vector<float> err;\n        TermCriteria criteria = TermCriteria((TermCriteria::COUNT) + (TermCriteria::EPS), 10, 0.03);\n        calcOpticalFlowPyrLK(old_gray, frame_gray, p0, p1, status, err, Size(15,15), 2, criteria);\n\n        vector<Point2f> good_new;\n        for(uint i = 0; i < p0.size(); i++)\n        {\n            // Select good points\n            if(status[i] == 1) {\n                good_new.push_back(p1[i]);\n                // draw the tracks\n                line(mask, p1[i], p0[i], colors[i], 2);\n                circle(frame, p1[i], 5, colors[i], -1);\n            }\n        }\n        Mat img;\n        add(frame, mask, img);\n\n        imshow(\"Frame\", img);\n\n        int keyboard = waitKey(30);\n        if (keyboard == 'q' || keyboard == 27)\n            break;\n\n        // Now update the previous frame and previous points\n        old_gray = frame_gray.clone();\n        p0 = good_new;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Core SSIM Calculation Function in C++\nDESCRIPTION: Provides overloaded C++ functions named `ssim` for calculating the Mean Structural Similarity Index (MSSIM). One version accepts CPU `Mat` objects, and the other accepts `gpu::GpuMat` objects. Both versions apply Gaussian blurring, calculate means, variances, and covariances, and then compute the SSIM based on the standard formula using constants C1 and C2. The GPU version utilizes functions like `gpu::GaussianBlur`, `gpu::multiply`, `gpu::subtract`, etc., for GPU acceleration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n//![ssim]\nScalar ssim(const Mat& i1, const Mat& i2)\n{\n    const double C1 = 6.5025, C2 = 58.5225;\n    /***************************** INITS **********************************/\n    int d = CV_32F;\n\n    Mat I1, I2;\n    i1.convertTo(I1, d);            // cannot calculate on one byte large values\n    i2.convertTo(I2, d);\n\n    Mat I2_2   = I2.mul(I2);        // I2^2\n    Mat I1_2   = I1.mul(I1);        // I1^2\n    Mat I1_I2  = I1.mul(I2);        // I1 * I2\n\n    /*************************** END INITS **********************************/\n\n    Mat mu1, mu2;                   // PRELIMINARY COMPUTING\n    GaussianBlur(I1, mu1, Size(11, 11), 1.5);\n    GaussianBlur(I2, mu2, Size(11, 11), 1.5);\n\n    Mat mu1_2   =   mu1.mul(mu1);\n    Mat mu2_2   =   mu2.mul(mu2);\n    Mat mu1_mu2 =   mu1.mul(mu2);\n\n    Mat sigma1_2, sigma2_2, sigma12;\n\n    GaussianBlur(I1_2, sigma1_2, Size(11, 11), 1.5);\n    sigma1_2 -= mu1_2;\n\n    GaussianBlur(I2_2, sigma2_2, Size(11, 11), 1.5);\n    sigma2_2 -= mu2_2;\n\n    GaussianBlur(I1_I2, sigma12, Size(11, 11), 1.5);\n    sigma12 -= mu1_mu2;\n\n    ///////////////////////////////// FORMULA ////////////////////////////////\n    Mat t1, t2, t3;\n\n    t1 = 2 * mu1_mu2 + C1;\n    t2 = 2 * sigma12 + C2;\n    t3 = t1.mul(t2);                 // t3 = ((2*mu1_mu2 + C1).*(2*sigma12 + C2))\n\n    t1 = mu1_2 + mu2_2 + C1;\n    t2 = sigma1_2 + sigma2_2 + C2;\n    t1 = t1.mul(t2);                 // t1 =((mu1_2 + mu2_2 + C1).*(sigma1_2 + sigma2_2 + C2))\n\n    Mat ssim_map;\n    divide(t3, t1, ssim_map);        // ssim_map =  t3./t1;\n\n    Scalar mssim = mean(ssim_map);   // mssim = average of ssim map\n    return mssim;\n}\n\nScalar ssim(const gpu::GpuMat& i1, const gpu::GpuMat& i2)\n{\n    const float C1 = 6.5025f, C2 = 58.5225f;\n    /***************************** INITS **********************************/\n    gpu::GpuMat d_I1, d_I2;\n    i1.convertTo(d_I1, CV_32F);\n    i2.convertTo(d_I2, CV_32F);\n\n    gpu::GpuMat d_I2_2, d_I1_2, d_I1_I2;\n    gpu::multiply(d_I2, d_I2, d_I2_2);        // I2^2\n    gpu::multiply(d_I1, d_I1, d_I1_2);        // I1^2\n    gpu::multiply(d_I1, d_I2, d_I1_I2);       // I1 * I2\n\n    /*************************** END INITS **********************************/\n\n    gpu::GpuMat d_mu1, d_mu2;   // PRELIMINARY COMPUTING\n    gpu::GaussianBlur(d_I1, d_mu1, Size(11, 11), 1.5);\n    gpu::GaussianBlur(d_I2, d_mu2, Size(11, 11), 1.5);\n\n    gpu::GpuMat d_mu1_2, d_mu2_2, d_mu1_mu2;\n    gpu::multiply(d_mu1, d_mu1, d_mu1_2);\n    gpu::multiply(d_mu2, d_mu2, d_mu2_2);\n    gpu::multiply(d_mu1, d_mu2, d_mu1_mu2);\n\n    gpu::GpuMat d_sigma1_2, d_sigma2_2, d_sigma12;\n\n    gpu::GaussianBlur(d_I1_2, d_sigma1_2, Size(11, 11), 1.5);\n    gpu::subtract(d_sigma1_2, d_mu1_2, d_sigma1_2); // sigma1_2 -= mu1_2;\n\n    gpu::GaussianBlur(d_I2_2, d_sigma2_2, Size(11, 11), 1.5);\n    gpu::subtract(d_sigma2_2, d_mu2_2, d_sigma2_2); // sigma2_2 -= mu2_2;\n\n    gpu::GaussianBlur(d_I1_I2, d_sigma12, Size(11, 11), 1.5);\n    gpu::subtract(d_sigma12, d_mu1_mu2, d_sigma12); // sigma12 -= mu1_mu2;\n\n    ///////////////////////////////// FORMULA ////////////////////////////////\n    gpu::GpuMat d_t1, d_t2, d_t3;\n\n    // t1 = 2 * mu1_mu2 + C1;\n    gpu::multiply(d_mu1_mu2, 2, d_t1);\n    gpu::add(d_t1, C1, d_t1);\n\n    // t2 = 2 * sigma12 + C2;\n    gpu::multiply(d_sigma12, 2, d_t2);\n    gpu::add(d_t2, C2, d_t2);\n\n    // t3 = t1 * t2;\n    gpu::multiply(d_t1, d_t2, d_t3);\n\n    // t1 = mu1_2 + mu2_2 + C1;\n    gpu::add(d_mu1_2, d_mu2_2, d_t1);\n    gpu::add(d_t1, C1, d_t1);\n\n    // t2 = sigma1_2 + sigma2_2 + C2;\n    gpu::add(d_sigma1_2, d_sigma2_2, d_t2);\n    gpu::add(d_t2, C2, d_t2);\n\n    // t1 = t1 * t2;\n    gpu::multiply(d_t1, d_t2, d_t1);\n\n    // ssim_map = t3 / t1;\n    gpu::GpuMat d_ssim_map;\n    gpu::divide(d_t3, d_t1, d_ssim_map);\n\n    Scalar s = gpu::sum(d_ssim_map);\n    s.val[0] /= i1.total();\n    s.val[1] /= i1.total();\n    s.val[2] /= i1.total();\n    return s;\n}\n//![ssim]\n```\n\n----------------------------------------\n\nTITLE: Detecting Circles using OpenCV HoughCircles in Python\nDESCRIPTION: Demonstrates circle detection in a grayscale image using OpenCV's HoughCircles function. The code loads an image, applies median blur, detects circles using Hough transform, and draws the detected circles with their centers. Uses cv.HOUGH_GRADIENT method with customizable parameters for detection sensitivity.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_houghcircles/py_houghcircles.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nimg = cv.imread('opencv-logo-white.png', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nimg = cv.medianBlur(img,5)\ncimg = cv.cvtColor(img,cv.COLOR_GRAY2BGR)\n\ncircles = cv.HoughCircles(img,cv.HOUGH_GRADIENT,1,20,\n                            param1=50,param2=30,minRadius=0,maxRadius=0)\n\ncircles = np.uint16(np.around(circles))\nfor i in circles[0,:]:\n    # draw the outer circle\n    cv.circle(cimg,(i[0],i[1]),i[2],(0,255,0),2)\n    # draw the center of the circle\n    cv.circle(cimg,(i[0],i[1]),2,(0,0,255),3)\n\ncv.imshow('detected circles',cimg)\ncv.waitKey(0)\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Model to ONNX Format\nDESCRIPTION: This Python snippet demonstrates converting a PyTorch model (`original_model`) to the ONNX (Open Neural Network Exchange) format. It defines the output path and filename (`models/fcnresnet50.onnx`), creates the output directory if it doesn't exist, generates a dummy input tensor (`generated_input`) with the expected shape (1 batch, 3 channels, 500x500 pixels) required for tracing the model graph, and then calls `torch.onnx.export` to perform the conversion. Key parameters include the model, dummy input, output path, input/output names for the ONNX graph, and the ONNX opset version (11). Requires `torch`, `os`, and `torch.autograd.Variable` (or just `torch.randn`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# define the directory for further converted model save\nonnx_model_path = \"models\"\n# define the name of further converted model\nonnx_model_name = \"fcnresnet50.onnx\"\n\n# create directory for further converted model\nos.makedirs(onnx_model_path, exist_ok=True)\n\n# get full path to the converted model\nfull_model_path = os.path.join(onnx_model_path, onnx_model_name)\n\n# generate model input to build the graph\ngenerated_input = Variable(\n    torch.randn(1, 3, 500, 500)\n)\n\n# model export into ONNX format\ntorch.onnx.export(\n    original_model,\n    generated_input,\n    full_model_path,\n    verbose=True,\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    opset_version=11\n)\n```\n\n----------------------------------------\n\nTITLE: Converting Grayscale to Binary Image in OpenCV (C++/Java/Python)\nDESCRIPTION: Converts the grayscale image to a binary image using adaptive thresholding. This method is suitable for images with varying illumination. The `adaptiveThreshold` function is used with `ADAPTIVE_THRESH_MEAN_C` and `THRESH_BINARY_INV` flags. The block size and constant C are parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection.md#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n//![bin]\n// Apply adaptiveThreshold at the bitwise_not of gray, notice the ~ symbol\nMat bw;\nadaptiveThreshold(~gray, bw, 255, ADAPTIVE_THRESH_MEAN_C, THRESH_BINARY, 15, -2);\n\n// Show binary image\nshow_wait_destroy(\"binary\", bw);\n//![bin]\n```\n\nLANGUAGE: java\nCODE:\n```\n//![bin]\n// Apply adaptiveThreshold at the bitwise_not of gray, notice the ~ symbol\nMat bw = new Mat();\nCore.bitwise_not(gray, gray);\nImgproc.adaptiveThreshold(gray, bw, 255, Imgproc.ADAPTIVE_THRESH_MEAN_C, Imgproc.THRESH_BINARY, 15, -2);\n\n// Show binary image\nshowWaitDestroy(\"binary\", bw);\n//![bin]\n```\n\nLANGUAGE: python\nCODE:\n```\n#![bin]\n# Apply adaptiveThreshold at the bitwise_not of gray, notice the ~ symbol\ngray = cv.bitwise_not(gray)\nbw = cv.adaptiveThreshold(gray, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 15, -2)\n\n# Show binary image\nshow_wait_destroy(\"binary\", bw)\n#![bin]\n```\n\n----------------------------------------\n\nTITLE: Loading an Image into cv.Mat with OpenCV.js (JavaScript)\nDESCRIPTION: Illustrates how to read an image displayed in an HTML `<img>` element (`imgElement`) into an OpenCV `cv.Mat` object using `cv.imread()`. This code should be placed within the `onload` event handler of the `img` element to ensure the image is fully loaded before processing. It also handles the case where the `cv` object might initially be a Promise by using `await`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_usage/js_usage.markdown#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimgElement.onload = await function() {\n  cv = (cv instanceof Promise) ? await cv : cv;\n  let mat = cv.imread(imgElement);\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Image Border with copyMakeBorder\nDESCRIPTION: Implementation of copyMakeBorder() function to add padding around the image with either constant or replicated border types.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/copyMakeBorder/copyMakeBorder.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ncopyMakeBorder(src, dst, top, bottom, left, right, borderType, value);\n```\n\nLANGUAGE: Java\nCODE:\n```\nCore.copyMakeBorder(src, dst, top, bottom, left, right, borderType, value);\n```\n\nLANGUAGE: Python\nCODE:\n```\ndst = cv.copyMakeBorder(src, top, bottom, left, right, borderType, value=value)\n```\n\n----------------------------------------\n\nTITLE: Performing Affine Transformation in OpenCV\nDESCRIPTION: Illustrates affine transformations preserving parallel lines. It uses three input points to define how an image aligns to three destination points via cv.getAffineTransform, generating a matrix used by cv.warpAffine.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_geometric_transformations/js_geometric_transformations.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\ncv.getAffineTransform(src, dst)\n```\n\n----------------------------------------\n\nTITLE: Applying Affine Transformations to Images in OpenCV Python\nDESCRIPTION: Shows how to perform an affine transformation using cv.getAffineTransform() and cv.warpAffine(). This transformation preserves parallelism of lines but not their lengths or angles, using three point pairs to define the transformation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimg = cv.imread('drawing.png')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nrows,cols,ch = img.shape\n\npts1 = np.float32([[50,50],[200,50],[50,200]])\npts2 = np.float32([[10,100],[200,50],[100,250]])\n\nM = cv.getAffineTransform(pts1,pts2)\n\ndst = cv.warpAffine(img,M,(cols,rows))\n\nplt.subplot(121),plt.imshow(img),plt.title('Input')\nplt.subplot(122),plt.imshow(dst),plt.title('Output')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Running Face Detection Inference - OpenCV DNN Python\nDESCRIPTION: This snippet shows how to perform face detection inference with FaceDetectorYN in Python. The detect method processes an image and returns the detection results as a NumPy array. Inputs include an image, and outputs follow the expected detection array format: bounding boxes and five facial landmarks per face instance.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_face/dnn_face.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Detect faces in an image\n_, faces = detector.detect(image)\n# 'faces' is a numpy array with detection results (one row per detected face)\n```\n\n----------------------------------------\n\nTITLE: Detecting ArUco Markers in an Image\nDESCRIPTION: Code snippet illustrating how to detect ArUco markers in an input image. This example uses the ArucoDetector class to find markers from a predefined dictionary, returning the corners and IDs of detected markers as well as rejected candidates.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_detection/aruco_detection.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat inputImage;\n// ... read inputImage ...\nstd::vector<int> markerIds;\nstd::vector<std::vector<cv::Point2f>> markerCorners, rejectedCandidates;\ncv::aruco::DetectorParameters detectorParams = cv::aruco::DetectorParameters();\ncv::aruco::Dictionary dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250);\ncv::aruco::ArucoDetector detector(dictionary, detectorParams);\ndetector.detectMarkers(inputImage, markerCorners, markerIds, rejectedCandidates);\n```\n\n----------------------------------------\n\nTITLE: Calculating SSIM for Image Similarity in OpenCV Python\nDESCRIPTION: This Python implementation of the Structural Similarity Index (SSIM) algorithm calculates image similarity with better perceptual accuracy than PSNR. The algorithm computes luminance, contrast, and structure components separately and combines them into a final similarity index for each channel.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_input_psnr_ssim.markdown#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndef getMSSIM(i1, i2):\n    C1 = 6.5025\n    C2 = 58.5225\n    # INITS\n    I1 = np.float32(i1) # cannot calculate on one byte large values\n    I2 = np.float32(i2)\n    I2_2 = I2 * I2 # I2^2\n    I1_2 = I1 * I1 # I1^2\n    I1_I2 = I1 * I2 # I1 * I2\n    # END INITS\n    # PRELIMINARY COMPUTING\n    mu1 = cv.GaussianBlur(I1, (11, 11), 1.5)\n    mu2 = cv.GaussianBlur(I2, (11, 11), 1.5)\n    mu1_2 = mu1 * mu1\n    mu2_2 = mu2 * mu2\n    mu1_mu2 = mu1 * mu2\n    sigma1_2 = cv.GaussianBlur(I1_2, (11, 11), 1.5)\n    sigma1_2 -= mu1_2\n    sigma2_2 = cv.GaussianBlur(I2_2, (11, 11), 1.5)\n    sigma2_2 -= mu2_2\n    sigma12 = cv.GaussianBlur(I1_I2, (11, 11), 1.5)\n    sigma12 -= mu1_mu2\n    t1 = 2 * mu1_mu2 + C1\n    t2 = 2 * sigma12 + C2\n    t3 = t1 * t2 # t3 = ((2*mu1_mu2 + C1).*(2*sigma12 + C2))\n    t1 = mu1_2 + mu2_2 + C1\n    t2 = sigma1_2 + sigma2_2 + C2\n    t1 = t1 * t2  # t1 =((mu1_2 + mu2_2 + C1).*(sigma1_2 + sigma2_2 + C2))\n    ssim_map = cv.divide(t3, t1)  # ssim_map =  t3./t1;\n    mssim = cv.mean(ssim_map)  # mssim = average of ssim map\n    return mssim\n```\n\n----------------------------------------\n\nTITLE: Image Blending Using Pyramids\nDESCRIPTION: Complete implementation of image blending using Gaussian and Laplacian pyramids to seamlessly combine two images (apple and orange). Creates pyramids, combines image halves at each level, and reconstructs the final blended image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_pyramids/py_pyramids.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np,sys\n\nA = cv.imread('apple.jpg')\nB = cv.imread('orange.jpg')\nassert A is not None, \"file could not be read, check with os.path.exists()\"\nassert B is not None, \"file could not be read, check with os.path.exists()\"\n\n# generate Gaussian pyramid for A\nG = A.copy()\ngpA = [G]\nfor i in range(6):\n    G = cv.pyrDown(G)\n    gpA.append(G)\n\n# generate Gaussian pyramid for B\nG = B.copy()\ngpB = [G]\nfor i in range(6):\n    G = cv.pyrDown(G)\n    gpB.append(G)\n\n# generate Laplacian Pyramid for A\nlpA = [gpA[5]]\nfor i in range(5,0,-1):\n    GE = cv.pyrUp(gpA[i])\n    L = cv.subtract(gpA[i-1],GE)\n    lpA.append(L)\n\n# generate Laplacian Pyramid for B\nlpB = [gpB[5]]\nfor i in range(5,0,-1):\n    GE = cv.pyrUp(gpB[i])\n    L = cv.subtract(gpB[i-1],GE)\n    lpB.append(L)\n\n# Now add left and right halves of images in each level\nLS = []\nfor la,lb in zip(lpA,lpB):\n    rows,cols,dpt = la.shape\n    ls = np.hstack((la[:,0:cols//2], lb[:,cols//2:]))\n    LS.append(ls)\n\n# now reconstruct\nls_ = LS[0]\nfor i in range(1,6):\n    ls_ = cv.pyrUp(ls_)\n    ls_ = cv.add(ls_, LS[i])\n\n# image with direct connecting each half\nreal = np.hstack((A[:,:cols//2],B[:,cols//2:]))\n\ncv.imwrite('Pyramid_blending2.jpg',ls_)\ncv.imwrite('Direct_blending.jpg',real)\n```\n\n----------------------------------------\n\nTITLE: Setting up Trackbars for Threshold Parameters (C++)\nDESCRIPTION: In C++, this snippet sets up two OpenCV trackbars within the 'Threshold Demo' window. Users can interactively select the thresholding type and value. Both trackbars invoke the callback function Threshold_Demo upon change. Requires OpenCV's highgui module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n// [trackbar]\\ncreateTrackbar( \"Type:\\n 0:Binary \\n 1:BinaryInv \\n 2:Trunc \\n 3:ToZero \\n 4:ToZeroInv\",\\n               window_name, &threshold_type, max_type, Threshold_Demo );\\ncreateTrackbar( \"Value\",\\n               window_name, &threshold_value, max_value, Threshold_Demo );\\n// [trackbar]\n```\n\n----------------------------------------\n\nTITLE: Example info.dat Format for opencv_createsamples (Text)\nDESCRIPTION: Illustrates the format of the description file (`info.dat`) used with the `-info` argument in `opencv_createsamples`. Each line specifies an image path relative to the file, followed by the number of object instances in that image, and then the bounding box coordinates (x, y, width, height) for each object instance.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/traincascade.markdown#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nimg/img1.jpg  1  140 100 45 45\nimg/img2.jpg  2  100 200 50 50   50 30 25 25\n```\n\n----------------------------------------\n\nTITLE: Converting RGB to Grayscale using OpenCV\nDESCRIPTION: This C++ code snippet demonstrates the conversion of an RGB image to a grayscale image using the OpenCV function 'cvtColor'. The function requires the source image, destination image, and a color conversion code (cv::COLOR_RGB2GRAY) to perform the operation. The grayscale conversion formula is based on the luminance equation that weights the red, green, and blue channels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/imgproc/doc/colors.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ncvtColor(src, bwsrc, cv::COLOR_RGB2GRAY);\n```\n\n----------------------------------------\n\nTITLE: Threshold Operation Callback Function (Python)\nDESCRIPTION: This Python callback applies cv.threshold using the parameters currently set by the window trackbars, and refreshes the display window. It's assigned to trackbar events for dynamic updates. Inputs: src_gray, threshold type/value; output: displayed thresholded image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n# [Threshold_Demo]\\ndef Threshold_Demo(*args):\\n    threshold_type = cv.getTrackbarPos('Type', window_name)\\n    threshold_value = cv.getTrackbarPos('Value', window_name)\\n    ret, dst = cv.threshold(src_gray, threshold_value, max_BINARY_value, threshold_type)\\n    cv.imshow(window_name, dst)\\n# [Threshold_Demo]\n```\n\n----------------------------------------\n\nTITLE: Applying Bilateral Filter with OpenCV in Java\nDESCRIPTION: This Java snippet demonstrates the use of OpenCV's bilateralFilter() function for edge-preserving smoothing. Dependencies include OpenCV and requires parameters such as diameter, and standard deviations in color and space.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/Smoothing/Smoothing.java bilateralfilter\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Configuration for Classification (Python)\nDESCRIPTION: Defines the TestClsConfig Python dataclass that specifies default parameters for evaluation, such as batch size, frame size, image root directory, and label file paths. This must be imported from test_config.py and can be customized for different datasets or model requirements. Key parameters include batch_size, frame_size, bgr_to_rgb, and file paths for input images and class labels. Inputs and outputs are strictly determined by the dataclass fields; limitations depend on dataset structure and available files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass TestClsConfig:\n    batch_size: int = 50\n    frame_size: int = 224\n    img_root_dir: str = \"./ILSVRC2012_img_val\"\n    # location of image-class matching\n    img_cls_file: str = \"./val.txt\"\n    bgr_to_rgb: bool = True\n```\n\n----------------------------------------\n\nTITLE: Initializing a Lookup Table (LUT) using cv::Mat in C++\nDESCRIPTION: This C++ snippet demonstrates how to create and populate a `cv::Mat` to be used as a lookup table (LUT) for image modification. It initializes a single-row matrix with 256 columns (one for each possible 8-bit pixel value) and sets the desired mapping. This table is intended to be used with the `cv::LUT()` function. Requires the OpenCV library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_scan_images/how_to_scan_images.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet how_to_scan_images.cpp table-init\n```\n\n----------------------------------------\n\nTITLE: Multiple Object Template Matching in OpenCV Python\nDESCRIPTION: Implements template matching to detect multiple instances of an object (coins in Mario game) using thresholding. The code uses TM_CCOEFF_NORMED method and draws rectangles around all matched locations above a threshold.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_template_matching/py_template_matching.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimg_rgb = cv.imread('mario.png')\nassert img_rgb is not None, \"file could not be read, check with os.path.exists()\"\nimg_gray = cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY)\ntemplate = cv.imread('mario_coin.png', cv.IMREAD_GRAYSCALE)\nassert template is not None, \"file could not be read, check with os.path.exists()\"\nw, h = template.shape[::-1]\n\nres = cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED)\nthreshold = 0.8\nloc = np.where( res >= threshold)\nfor pt in zip(*loc[::-1]):\n    cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)\n\ncv.imwrite('res.png',img_rgb)\n```\n\n----------------------------------------\n\nTITLE: Preparing Input Image for Model Inference\nDESCRIPTION: This Python code prepares an input image for inference with OpenCV's DNN module. It reads, preprocesses, and formats the image into a blob, which is compatible with the model's input requirements, using mean normalization and standard scaling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ninput_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\ninput_img = input_img.astype(np.float32)\n\ninput_img = cv2.resize(input_img, (256, 256))\n\nmean = np.array([0.485, 0.456, 0.406]) * 255.0\nscale = 1 / 255.0\nstd = [0.229, 0.224, 0.225]\n\ninput_blob = cv2.dnn.blobFromImage(\n    image=input_img,\n    scalefactor=scale,\n    size=(224, 224),\n    mean=mean,\n    swapRB=True,\n    crop=True\n)\ninput_blob[0] /= np.asarray(std, dtype=np.float32).reshape(3, 1, 1)\n```\n\n----------------------------------------\n\nTITLE: PyTorch-style Image Preprocessing in Python - Python\nDESCRIPTION: This Python code demonstrates the typical order of image normalization for PyTorch-trained classification models. Pixel values are scaled, mean-centered, and finally standardized per channel. This preprocessing order is necessary for reproducible inference results between PyTorch and OpenCV pipelines.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimg /= 255.0\\nimg -= [0.485, 0.456, 0.406]\\nimg /= [0.229, 0.224, 0.225]\n```\n\n----------------------------------------\n\nTITLE: Implementing k-Nearest Neighbour Classification with OpenCV and Python\nDESCRIPTION: This code creates a simple 2D classification problem with two classes (Red and Blue), trains a kNN model with 25 random data points, and then classifies a new data point. The implementation includes visualization using matplotlib to display the training data and classification results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_knn/py_knn_understanding/py_knn_understanding.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Feature set containing (x,y) values of 25 known/training data\ntrainData = np.random.randint(0,100,(25,2)).astype(np.float32)\n\n# Label each one either Red or Blue with numbers 0 and 1\nresponses = np.random.randint(0,2,(25,1)).astype(np.float32)\n\n# Take Red neighbours and plot them\nred = trainData[responses.ravel()==0]\nplt.scatter(red[:,0],red[:,1],80,'r','^')\n\n# Take Blue neighbours and plot them\nblue = trainData[responses.ravel()==1]\nplt.scatter(blue[:,0],blue[:,1],80,'b','s')\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Face Recognizer (FaceRecognizerSF) - OpenCV DNN C++\nDESCRIPTION: This snippet demonstrates how to initialize a FaceRecognizerSF object in C++ using a pre-trained ONNX model for face recognition. The function requires model path, config (if any), and input size applicable to the model. It depends on the OpenCV face module. Outputs an object ready to extract face features for recognition.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_face/dnn_face.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n// Initialize FaceRecognizerSF\ncv::Ptr<cv::FaceRecognizerSF> recognizer = cv::FaceRecognizerSF::create(\n    modelPath,                   // Path to face recognition .onnx model\n    \"\",                          // No config file usually\n    cv::Size(112, 112)           // Input size expected by recognition model\n);\n```\n\n----------------------------------------\n\nTITLE: Text Recognition Inference in C++\nDESCRIPTION: The C++ snippet performs text recognition inference with the prepared TextRecognitionModel. It processes the input image and prints the recognized text using OpenCV's DNN module. The expected input is a preprocessed image, and the output is the recognized text string.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_text_spotting/dnn_text_spotting.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nstd::string recognitionResult = recognizer.recognize(image);\nstd::cout << \"'\" << recognitionResult << \"'\" << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Detecting Lines using Standard Hough Transform in OpenCV.js\nDESCRIPTION: Uses the standard Hough Transform algorithm to detect lines in a binary image. It returns lines represented by their polar coordinates (rho, theta). Requires a single-channel 8-bit binary input image. Key parameters include `rho` (distance resolution), `theta` (angle resolution), and `threshold` (minimum votes). Optional parameters allow for multi-scale transforms and angle range limits. Depends on the OpenCV.js library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_houghlines/js_houghlines.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ncv.HoughLines (image, lines, rho, theta, threshold, srn = 0, stn = 0, min_theta = 0, max_theta = Math.PI)\n@param image       8-bit, single-channel binary source image. The image may be modified by the function.\n@param lines       output vector of lines(cv.32FC2 type). Each line is represented by a two-element vector (ρ,θ) . ρ is the distance from the coordinate origin (0,0). θ is the line rotation angle in radians.\n@param rho    \t   distance resolution of the accumulator in pixels.\n@param theta       angle resolution of the accumulator in radians.\n@param threshold   accumulator threshold parameter. Only those lines are returned that get enough votes\n@param srn         for the multi-scale Hough transform, it is a divisor for the distance resolution rho . The coarse accumulator distance resolution is rho and the accurate accumulator resolution is rho/srn . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these parameters should be positive.\n@param stn         for the multi-scale Hough transform, it is a divisor for the distance resolution theta.\n@param min_theta   for standard and multi-scale Hough transform, minimum angle to check for lines. Must fall between 0 and max_theta.\n@param max_theta   for standard and multi-scale Hough transform, maximum angle to check for lines. Must fall between min_theta and CV_PI.\n```\n\n----------------------------------------\n\nTITLE: Comparing Shapes with OpenCV's Match Shapes in JavaScript\nDESCRIPTION: This function compares two shapes or contours and provides a similarity metric based on Hu moments. Lower values indicate better matches. The cv.matchShapes function requires two contours or grayscale images, a comparison method, and a parameter (currently unsupported).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contours_more_functions/js_contours_more_functions.markdown#2025-04-22_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Implementing CLAHE (Contrast Limited Adaptive Histogram Equalization)\nDESCRIPTION: This code demonstrates how to apply Contrast Limited Adaptive Histogram Equalization (CLAHE) in OpenCV. CLAHE improves local contrast by dividing the image into small tiles, equalizing each tile separately, and then using bilinear interpolation to eliminate artifacts at tile boundaries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.markdown#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nimg = cv.imread('tsukuba_l.png', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n\n# create a CLAHE object (Arguments are optional).\nclahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\ncl1 = clahe.apply(img)\n\ncv.imwrite('clahe_2.jpg',cl1)\n```\n\n----------------------------------------\n\nTITLE: Applying Bilateral Filter with OpenCV in C++\nDESCRIPTION: This C++ snippet explains the use of OpenCV's bilateralFilter() function to apply a bilateral filter for smoothing images while preserving edges. It requires the OpenCV library and parameters including the diameter, and standard deviations in color and space.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n@snippet cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp bilateralfilter\n```\n\n----------------------------------------\n\nTITLE: Implementing USAC Framework in OpenCV C++\nDESCRIPTION: This C++ code snippet showcases the integration of the USAC framework within the OpenCV 'calib3d' module. It implements a modular RANSAC-based sampling framework that is independent of any estimation problem, capable of including new solvers and methods easily. The framework introduces numerous sampling and optimization techniques, ensuring flexibility and improved accuracy in model estimation. Key dependencies include OpenCV and support for specific matrix operations such as Eigen decomposition.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/usac.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n\\nnamespace usac {\\n    // Sampling methods\\n    enum Sampling {\\n        UNIFORM, PROSAC, NAPSAC, PROGRESSIVE_NAPSAC\\n    };\\n\\n    // Score methods\\n    enum Score {\\n        RANSAC, MSAC, MAGSAC, LMeds\\n    };\\n\\n    // Error metrics\\n    enum ErrorMetric {\\n        REPROJECTION, SAMPSON, SYMMETRIC_GEOMETRIC\\n    };\\n\\n    // Degeneracy checks\\n    enum Degeneracy {\\n        DEGENSAC, COLLINEARITY_TEST, ORIENTED_EPIPOLAR\\n    };\\n\\n    // Local Optimization\\n    enum LocalOptimization {\\n        LORANSAC, GRAPH_CUT, SIGMA_CONSENSUS\\n    };\\n\\n    // Termination methods\\n    enum Termination {\\n        STANDARD, PROSAC_TERMINATION, SPRT_TERMINATION\\n    };\\n\\n    // Solvers\\n    enum Solver {\\n        AFFINE2D, HOMOGRAPHY, FUNDAMENTAL, ESSENTIAL, PERSPECTIVE_N_POINT\\n    };\\n\\n    // Neighborhood graph building options\\n    enum Neighborhood {\\n        NEIGH_FLANN_KNN, NEIGH_FLANN_RADIUS, NEIGH_GRID\\n    };\\n\\n    // USAC Flags\\n    const int USAC_DEFAULT = 0;\\n    const int USAC_PARALLEL = 1;\\n    const int USAC_ACCURATE = 2;\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Image using imshow and waitKey in OpenCV in C++\nDESCRIPTION: Shows how to display an image in a window using cv::imshow and waitKey in C++. Requires OpenCV and a GUI environment. imshow creates and displays the window. waitKey waits for user input (milliseconds or indefinitely with 0).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_40\n\nLANGUAGE: C++\nCODE:\n```\ncv::imshow(\"Window\", img);\\ncv::waitKey(0);\n```\n\n----------------------------------------\n\nTITLE: Copying Source Image in Callback (C++)\nDESCRIPTION: Inside the trackbar callback function, this snippet creates a copy of the original source image using `img.copyTo(img_display)`. This ensures that drawing operations (like the result rectangle) are performed on a copy, leaving the original image unmodified for subsequent matching operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp copy_source\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing for PCA using OpenCV in Java\nDESCRIPTION: Loads an image using `imread`, converts it to grayscale using `cvtColor`, and applies Otsu's binary thresholding using `threshold`. This preprocessing step isolates potential objects for subsequent analysis. Depends on OpenCV Java bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n//! [pre-process]\n// Load image\nMat src = Imgcodecs.imread(args.length > 0 ? args[0] : \"../data/pca_test1.jpg\");\n// Check if image is loaded successfully\nif(src.empty())\n{\n    System.out.println(\"Problem loading image!!!\");\n    System.exit(-1);\n}\n\nHighGui.imshow(\"src\", src);\n\n// Convert image to grayscale\nMat gray = new Mat();\ncvtColor(src, gray, COLOR_BGR2GRAY);\n\n// Convert image to binary\nMat bw = new Mat();nthreshold(gray, bw, 50, 255, THRESH_BINARY | THRESH_OTSU);\n//! [pre-process]\n```\n\n----------------------------------------\n\nTITLE: Finding Extreme Points of a Contour in OpenCV Python using NumPy\nDESCRIPTION: This snippet identifies the extreme points (topmost, bottommost, leftmost, rightmost) of a contour using NumPy array indexing. It leverages `argmin()` and `argmax()` on the contour's x (column index 0) and y (column index 1) coordinates to find the indices corresponding to the minimum and maximum values. Requires an existing contour variable `cnt` (assumed to be a NumPy array).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_properties/py_contour_properties.markdown#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nleftmost = tuple(cnt[cnt[:,:,0].argmin()][0])\nrightmost = tuple(cnt[cnt[:,:,0].argmax()][0])\ntopmost = tuple(cnt[cnt[:,:,1].argmin()][0])\nbottommost = tuple(cnt[cnt[:,:,1].argmax()][0])\n```\n\n----------------------------------------\n\nTITLE: Implementing Dense Optical Flow in C++\nDESCRIPTION: C++ implementation of dense optical flow using Farneback's algorithm (cv.calcOpticalFlowFarneback()). This code calculates the optical flow for all points in the frame and visualizes the flow field using HSV color coding for direction and magnitude.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/optical_flow.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#include <iostream>\n#include <opencv2/core.hpp>\n#include <opencv2/highgui.hpp>\n#include <opencv2/imgproc.hpp>\n#include <opencv2/videoio.hpp>\n#include <opencv2/video.hpp>\n\nusing namespace cv;\nusing namespace std;\n\nint main() {\n    VideoCapture capture(samples::findFile(\"vtest.avi\"));\n    if (!capture.isOpened()){\n        //error in opening the video input\n        cerr << \"Unable to open file!\" << endl;\n        return 0;\n    }\n\n    Mat frame1, prvs;\n    capture >> frame1;\n    cvtColor(frame1, prvs, COLOR_BGR2GRAY);\n\n    while(true){\n        Mat frame2, next;\n        capture >> frame2;\n        if (frame2.empty())\n            break;\n        cvtColor(frame2, next, COLOR_BGR2GRAY);\n\n        Mat flow(prvs.size(), CV_32FC2);\n        calcOpticalFlowFarneback(prvs, next, flow, 0.5, 3, 15, 3, 5, 1.2, 0);\n\n        // visualization\n        Mat flow_parts[2];\n        split(flow, flow_parts);\n        Mat magnitude, angle, magn_norm;\n        cartToPolar(flow_parts[0], flow_parts[1], magnitude, angle, true);\n        normalize(magnitude, magn_norm, 0.0f, 1.0f, NORM_MINMAX);\n        angle *= ((1.f / 360.f) * (180.f / 255.f));\n\n        //build hsv image\n        Mat _hsv[3], hsv, hsv8, bgr;\n        _hsv[0] = angle;\n        _hsv[1] = Mat::ones(angle.size(), CV_32F);\n        _hsv[2] = magn_norm;\n        merge(_hsv, 3, hsv);\n        hsv.convertTo(hsv8, CV_8U, 255.0);\n        cvtColor(hsv8, bgr, COLOR_HSV2BGR);\n\n        imshow(\"frame2\", bgr);\n\n        int keyboard = waitKey(30);\n        if (keyboard == 'q' || keyboard == 27)\n            break;\n\n        prvs = next;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Source Images in Python\nDESCRIPTION: This Python snippet uses `cv.imread` to load two images (`src1`, `src2`) for blending. It includes checks using `is None` to ensure the images were loaded successfully and exits if not.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#![load]\n# Read images\nsrc1 = cv.imread(cv.samples.findFile('LinuxLogo.jpg'))\nsrc2 = cv.imread(cv.samples.findFile('WindowsLogo.jpg'))\nif src1 is None:\n    print('Could not open or find the image: ', args.input1)\n    exit(0)\nif src2 is None:\n    print('Could not open or find the image: ', args.input2)\n    exit(0)\n#![load]\n```\n\n----------------------------------------\n\nTITLE: Calculating Histogram Backprojection with cv.calcBackProject in OpenCV.js\nDESCRIPTION: Details the `cv.calcBackProject` function signature from OpenCV.js. This function computes the back projection of a histogram onto an image, which is useful for object detection and segmentation. It requires source images, channel indices, the pre-computed histogram, an output array, bin ranges, and an optional scaling factor.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_histograms/js_histogram_backprojection/js_histogram_backprojection.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ncv.calcBackProject (images, channels, hist, dst, ranges, scale)\n\n@param images       source arrays. They all should have the same depth, cv.CV_8U, cv.CV_16U or cv.CV_32F , and the same size. Each of them can have an arbitrary number of channels.\n@param channels     the list of channels used to compute the back projection. The number of channels must match the histogram dimensionality.\n@param hist         input histogram that can be dense or sparse.\n@param dst          destination back projection array that is a single-channel array of the same size and depth as images[0].\n@param ranges       array of arrays of the histogram bin boundaries in each dimension(see cv.calcHist).\n@param scale        optional scale factor for the output back projection.\n```\n\n----------------------------------------\n\nTITLE: Initializing Caffe Model with OpenCV in C++\nDESCRIPTION: This snippet demonstrates how to read and initialize a Caffe model using OpenCV's DNN module. It relies on the presence of a `.prototxt` and a `.caffemodel` file, and the cv::dnn::readNet function is used to load the model. Ensure that the files are in your working directory and correctly named.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_googlenet/dnn_googlenet.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/classification.cpp Read and initialize network\n```\n\n----------------------------------------\n\nTITLE: Detecting Keypoints with AKAZE in OpenCV using Python\nDESCRIPTION: This snippet demonstrates using AKAZE in Python via OpenCV to detect keypoints and compute descriptors on images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nsamples/python/tutorial_code/features2D/akaze_matching/AKAZE_match.py AKAZE\n```\n\n----------------------------------------\n\nTITLE: Extracting Camera Pose from Homography in OpenCV C++\nDESCRIPTION: Code to extract camera rotation and translation from a homography matrix. It decomposes the homography into the camera pose components assuming a planar object at Z=0.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n// Compute the scale factor\ndouble lambda = 1.0 / cv::norm(H.col(0));\n\n// Extract rotation matrix and translation vector\ncv::Mat R(3, 3, CV_64F);\ncv::Mat t = H.col(2) * lambda;\n\nR.col(0) = H.col(0) * lambda;\nR.col(1) = H.col(1) * lambda;\nR.col(2) = R.col(0).cross(R.col(1));\n```\n\n----------------------------------------\n\nTITLE: Finding Minimum and Maximum Match Values/Locations (C++)\nDESCRIPTION: Uses the `cv::minMaxLoc` function to find the minimum and maximum values within the normalized result matrix, along with their corresponding locations (coordinates). Pointers store the minimum/maximum values and `Point` objects store their locations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_28\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp best_match\n```\n\n----------------------------------------\n\nTITLE: Drawing a Polygon in C++\nDESCRIPTION: Implementation of the MyPolygon function that draws a filled polygon in OpenCV C++. The function creates a set of points to define the polygon vertices and uses the fillPoly() function to draw a white polygon.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_24\n\nLANGUAGE: cpp\nCODE:\n```\nvoid MyPolygon( Mat img )\n{\n  int lineType = LINE_8;\n\n  /** Create some points */\n  Point rook_points[1][20];\n  rook_points[0][0] = Point( w/4, 7*w/8 );\n  rook_points[0][1] = Point( 3*w/4, 7*w/8 );\n  rook_points[0][2] = Point( 3*w/4, 13*w/16 );\n  rook_points[0][3] = Point( 11*w/16, 13*w/16 );\n  rook_points[0][4] = Point( 19*w/32, 3*w/8 );\n  rook_points[0][5] = Point( 3*w/4, 3*w/8 );\n  rook_points[0][6] = Point( 3*w/4, w/8 );\n  rook_points[0][7] = Point( 26*w/40, w/8 );\n  rook_points[0][8] = Point( 26*w/40, w/4 );\n  rook_points[0][9] = Point( 22*w/40, w/4 );\n  rook_points[0][10] = Point( 22*w/40, w/8 );\n  rook_points[0][11] = Point( 18*w/40, w/8 );\n  rook_points[0][12] = Point( 18*w/40, w/4 );\n  rook_points[0][13] = Point( 14*w/40, w/4 );\n  rook_points[0][14] = Point( 14*w/40, w/8 );\n  rook_points[0][15] = Point( w/4, w/8 );\n  rook_points[0][16] = Point( w/4, 3*w/8 );\n  rook_points[0][17] = Point( 13*w/32, 3*w/8 );\n  rook_points[0][18] = Point( 5*w/16, 13*w/16 );\n  rook_points[0][19] = Point( w/4, 13*w/16 );\n\n  const Point* ppt[1] = { rook_points[0] };\n  int npt[] = { 20 };\n\n  fillPoly( img,\n        ppt,\n        npt,\n        1,\n        Scalar( 255, 255, 255 ),\n        lineType );\n}\n```\n\n----------------------------------------\n\nTITLE: Computing Bounding Rotated Boxes and Ellipses in Contours - OpenCV Java\nDESCRIPTION: This Java snippet demonstrates contour detection and the calculation of minimum area rectangles and fitting ellipses with the OpenCV Java API. It requires OpenCV for Java, and takes an image as input to find contours, draw the rotated rectangles, and fit ellipses where appropriate (for contours with enough points). Key parameters include the input image file and preprocessing settings. Outputs are graphical overlays of rectangles and ellipses over the detected contours. The code uses Imgproc.findContours, Imgproc.minAreaRect, and Imgproc.fitEllipse.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rotated_ellipses/bounding_rotated_ellipses.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.*;\\nimport org.opencv.imgproc.Imgproc;\\nimport org.opencv.highgui.HighGui;\\nimport org.opencv.imgcodecs.Imgcodecs;\\n\\npublic class GeneralContoursDemo2 {\\n    public static void main(String[] args) {\\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\\n        Mat src = Imgcodecs.imread(\"shapes.png\", Imgcodecs.IMREAD_GRAYSCALE);\\n        Imgproc.threshold(src, src, 100, 255, Imgproc.THRESH_BINARY);\\n        java.util.List<MatOfPoint> contours = new java.util.ArrayList<>();\\n        Mat hierarchy = new Mat();\\n        Imgproc.findContours(src, contours, hierarchy, Imgproc.RETR_LIST, Imgproc.CHAIN_APPROX_SIMPLE);\\n        Mat drawing = Mat.zeros(src.size(), CvType.CV_8UC3);\\n        for (int i = 0; i < contours.size(); i++) {\\n            RotatedRect box = Imgproc.minAreaRect(new MatOfPoint2f(contours.get(i).toArray()));\\n            Point[] vertices = new Point[4];\\n            box.points(vertices);\\n            for (int j = 0; j < 4; ++j)\\n                Imgproc.line(drawing, vertices[j], vertices[(j+1)%4], new Scalar(0,255,0), 2);\\n            if (contours.get(i).total() > 5) {\\n                RotatedRect ellipseBox = Imgproc.fitEllipse(new MatOfPoint2f(contours.get(i).toArray()));\\n                Imgproc.ellipse(drawing, ellipseBox, new Scalar(255,0,0), 2);\\n            }\\n        }\\n        HighGui.imshow(\"Contours\", drawing);\\n        HighGui.waitKey();\\n    }\\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Results with OpenCV imshow in Python\nDESCRIPTION: This Python snippet uses cv2.imshow to display the original and processed images in separate named windows. Calls to cv2.waitKey and cv2.destroyAllWindows provide control flow. Requires cv2 installed and available GUI environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\ncv2.imshow('Source', src)\\ncv2.imshow('Detected Lines (in red) - Standard Hough Line Transform', img)\\ncv2.imshow('Detected Lines (in green) - Probabilistic Line Transform', imgP)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\n```\n\n----------------------------------------\n\nTITLE: Applying Sobel Operator in C++\nDESCRIPTION: This C++ snippet demonstrates how to apply the Sobel operator to detect edges in an image using OpenCV. The operations include loading an image, reducing noise, converting to grayscale, applying the Sobel operator, and displaying the results. The dependencies include OpenCV library and a sample image as input, with output displayed as an edge-highlighted image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/sobel_derivatives/sobel_derivatives.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <opencv2/opencv.hpp>\nusing namespace cv; \n\nMat image = imread(\"lena.jpg\"); // Load source image\n```\n\nLANGUAGE: cpp\nCODE:\n```\nGaussianBlur(src, src, Size(3, 3), 0, 0, BORDER_DEFAULT); // Reduce noise\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncvtColor(src, src_gray, COLOR_BGR2GRAY); // Convert to grayscale\n```\n\nLANGUAGE: cpp\nCODE:\n```\nSobel(src_gray, grad_x, ddepth, 1, 0, 3, scale, delta, BORDER_DEFAULT); // Apply Sobel for x-gradient\nSobel(src_gray, grad_y, ddepth, 0, 1, 3, scale, delta, BORDER_DEFAULT); // Apply Sobel for y-gradient\n```\n\nLANGUAGE: cpp\nCODE:\n```\nconvertScaleAbs(grad_x, abs_grad_x); \nconvertScaleAbs(grad_y, abs_grad_y);\n```\n\nLANGUAGE: cpp\nCODE:\n```\naddWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0, grad); // Approximate the gradient\n```\n\nLANGUAGE: cpp\nCODE:\n```\nimshow(\"Sobel Demo\", grad); // Display results\nwaitKey(0);\n```\n\n----------------------------------------\n\nTITLE: Loading and Displaying an Image using OpenCV in C++\nDESCRIPTION: This C++ code demonstrates a basic OpenCV application. It includes the necessary OpenCV header, reads an image file specified by a command-line argument using `imread`, checks if the image data was loaded successfully, creates a window using `namedWindow`, displays the image using `imshow`, and waits for a key press using `waitKey` before exiting.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_eclipse/linux_eclipse.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <opencv2/opencv.hpp>\n\nusing namespace cv;\n\nint main( int argc, char** argv )\n{\n  Mat image;\n  image = imread( argv[1], IMREAD_COLOR );\n\n  if( argc != 2 || !image.data )\n    {\n      printf( \"No image data \\n\" );\n      return -1;\n    }\n\n  namedWindow( \"Display Image\", WINDOW_AUTOSIZE );\n  imshow( \"Display Image\", image );\n\n  waitKey(0);\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Loading and Transforming Image in Python with OpenCV\nDESCRIPTION: This Python code snippet utilizes OpenCV to load an image, derive an affine transformation matrix using point coordinates, apply the transformation, and then execute a rotation on the transformed image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/warp_affine/warp_affine.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n@snippet samples/python/tutorial_code/ImgTrans/warp_affine/Geometric_Transforms_Demo.py Load the image\n```\n\n----------------------------------------\n\nTITLE: Implementing ORB Feature Matching with Brute-Force Matcher in Python\nDESCRIPTION: Demonstrates feature matching between two images using ORB descriptors and Brute-Force matcher with NORM_HAMMING distance measurement. Includes image loading, keypoint detection, descriptor computation, and match visualization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nimport matplotlib.pyplot as plt\n\nimg1 = cv.imread('box.png',cv.IMREAD_GRAYSCALE)          # queryImage\nimg2 = cv.imread('box_in_scene.png',cv.IMREAD_GRAYSCALE) # trainImage\n\n# Initiate ORB detector\norb = cv.ORB_create()\n\n# find the keypoints and descriptors with ORB\nkp1, des1 = orb.detectAndCompute(img1,None)\nkp2, des2 = orb.detectAndCompute(img2,None)\n```\n\nLANGUAGE: python\nCODE:\n```\n# create BFMatcher object\nbf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)\n\n# Match descriptors.\nmatches = bf.match(des1,des2)\n\n# Sort them in the order of their distance.\nmatches = sorted(matches, key = lambda x:x.distance)\n\n# Draw first 10 matches.\nimg3 = cv.drawMatches(img1,kp1,img2,kp2,matches[:10],None,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n\nplt.imshow(img3),plt.show()\n```\n\n----------------------------------------\n\nTITLE: Using Smart Pointers with cv::Ptr in OpenCV C++\nDESCRIPTION: This snippet shows how to use OpenCV's `cv::Ptr` template class, which is similar to std::shared_ptr, for automatic memory management of user-defined types. It demonstrates allocation and proper memory handling without using raw pointers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/doc/intro.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n    T* ptr = new T(...);\n```\n\nLANGUAGE: cpp\nCODE:\n```\n    Ptr<T> ptr(new T(...));\n```\n\nLANGUAGE: cpp\nCODE:\n```\n    Ptr<T> ptr = makePtr<T>(...);\n```\n\n----------------------------------------\n\nTITLE: Loading Images in OpenCV for Histogram Equalization\nDESCRIPTION: Code snippets for loading source images in C++, Java, and Python as the first step in the histogram equalization process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_equalization/histogram_equalization.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nCommandLineParser parser( argc, argv, keys );\nString filename = parser.get<String>( 0 );\n\nMat src = imread( samples::findFile( filename ), IMREAD_COLOR );\nif (src.empty()) {\n    cout << \"Cannot read image: \" << filename << std::endl;\n    return EXIT_FAILURE;\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\nString filename = args.length > 0 ? args[0] : \"../data/lena.jpg\";\nMat src = imread(filename, IMREAD_COLOR);\nif (src.empty()) {\n    System.err.println(\"Cannot read image: \" + filename);\n    System.exit(0);\n}\n```\n\nLANGUAGE: Python\nCODE:\n```\nparser = argparse.ArgumentParser(description='Code for Histogram Equalization tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='lena.jpg')\nargs = parser.parse_args()\n\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image:', args.input)\n    exit(0)\n```\n\n----------------------------------------\n\nTITLE: Robust Descriptor Matching with Ratio Test in C++\nDESCRIPTION: Performs robust matching between model and scene descriptors using either robustMatch or fastRobustMatch. The method depends on computational cost; robustMatch offers higher accuracy through extra tests.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n// -- Step 1: Robust matching between model descriptors and scene descriptors\n\nstd::vector<cv::DMatch> good_matches;       // to obtain the model 3D points  in the scene\nstd::vector<cv::KeyPoint> keypoints_scene;  // to obtain the 2D points of the scene\n\nif(fast_match)\n{\n    rmatcher.fastRobustMatch(frame, good_matches, keypoints_scene, descriptors_model);\n}\nelse\n{\n    rmatcher.robustMatch(frame, good_matches, keypoints_scene, descriptors_model);\n}\n```\n\n----------------------------------------\n\nTITLE: Including Full Template Matching Demo Code (C++)\nDESCRIPTION: Reference to include the complete C++ source code file for the OpenCV template matching demonstration. This code loads images, performs matching, normalizes results, finds the best match, and displays the output.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n@include samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp\n```\n\n----------------------------------------\n\nTITLE: Calculating Histograms for Histogram Backprojection in Python OpenCV\nDESCRIPTION: This code demonstrates the first step in histogram backprojection by loading the target and reference images, converting them to HSV color space, and calculating their histograms using OpenCV's calcHist function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_backprojection/py_histogram_backprojection.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cvfrom matplotlib import pyplot as plt\n\n#roi is the object or region of object we need to find\nroi = cv.imread('rose_red.png')\nassert roi is not None, \"file could not be read, check with os.path.exists()\"\nhsv = cv.cvtColor(roi,cv.COLOR_BGR2HSV)\n\n#target is the image we search in\ntarget = cv.imread('rose.png')\nassert target is not None, \"file could not be read, check with os.path.exists()\"\nhsvt = cv.cvtColor(target,cv.COLOR_BGR2HSV)\n\n# Find the histograms using calcHist. Can be done with np.histogram2d also\nM = cv.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )\nI = cv.calcHist([hsvt],[0, 1], None, [180, 256], [0, 180, 0, 256] )\n```\n\n----------------------------------------\n\nTITLE: Detecting Keypoints with AKAZE in OpenCV using C++\nDESCRIPTION: This C++ snippet creates an AKAZE object to detect keypoints and compute descriptors for the loaded images, with OpenCV functionality.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nsamples/cpp/tutorial_code/features2D/AKAZE_match.cpp AKAZE\n```\n\n----------------------------------------\n\nTITLE: Applying Watershed Algorithm in OpenCV\nDESCRIPTION: Applies the watershed algorithm to the image using prepared markers and highlights segment boundaries in blue.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_watershed/py_watershed.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmarkers = cv.watershed(img,markers)\nimg[markers == -1] = [255,0,0]\n```\n\n----------------------------------------\n\nTITLE: Calculating Wiener Filter in Frequency Domain with OpenCV C++\nDESCRIPTION: Computes the Wiener filter `Hw` in the frequency domain based on the provided Point Spread Function `input_h` and the Signal-to-Noise Ratio (SNR). It first computes the Discrete Fourier Transform (DFT) of the PSF, then calculates the filter using the formula `Hw = H* / (|H|^2 + 1/SNR)`, where `H` is the DFT of the PSF and `H*` is its complex conjugate. The result `output_G` is the Wiener filter in the frequency domain. Requires OpenCV `dft`, `mulSpectrums`, `Mat` operations, and complex number handling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/out_of_focus_deblur_filter/out_of_focus_deblur_filter.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nvoid calcWnrFilter(const Mat& input_h_PSF, Mat& output_G, double nsr)\n{\n    Mat h_PSF_shifted;\n    fftshift(input_h_PSF, h_PSF_shifted);\n    Mat planes[2] = { Mat_<float>(h_PSF_shifted.clone()), Mat::zeros(h_PSF_shifted.size(), CV_32F) };\n    Mat complexI;\n    merge(planes, 2, complexI);\n    dft(complexI, complexI);\n    split(complexI, planes);\n    Mat denom;\n    pow(abs(planes[0]), 2, denom);\n    denom += nsr;\n    divide(planes[0], denom, output_G);\n}\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Pixel Comparison Logic in C++\nDESCRIPTION: Complex nested if-else structure that compares pixel values at different offsets to determine if a point is a corner. Uses pointer arithmetic for pixel access and goto statements for control flow. The code compares pixel intensities against threshold values 'cb' and 'c_b' to classify corners.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_32\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset6] > cb)\n  if(ptr[offset3] > cb)\n    if(ptr[offset4] > cb)\n      if(ptr[offset8] > cb)\n        goto is_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset6] < c_b)\n    goto is_not_a_corner;\n  else\n    if(ptr[offset6] > cb)\n      if(ptr[offset3] > cb)\n        if(ptr[offset4] > cb)\n          if(ptr[offset8] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset2] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset9] > cb)\n        if(ptr[offset1] > cb)\n          goto is_not_a_corner;\n        else\n          if(ptr[offset1] < c_b)\n            if(ptr[offset6] > cb)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset4] < c_b)\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset6] < c_b)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    if(ptr[offset10] < c_b)\n                      if(ptr[offset11] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Calculating PSNR on CPU in C++\nDESCRIPTION: Defines a C++ function `getPSNR` that calculates the Peak Signal-to-Noise Ratio (PSNR) between two input images (`I1`, `I2`) using CPU-based OpenCV functions. It computes the squared error between the images and then derives the PSNR value. Assumes `psnr` function is defined elsewhere.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n//![getpsnr]\ndouble getPSNR(const Mat& I1, const Mat& I2)\n{\n    return psnr(I1, I2);\n}\n//![getpsnr]\n```\n\n----------------------------------------\n\nTITLE: Implementing Camshift in OpenCV.js\nDESCRIPTION: This snippet utilizes the cv.CamShift function to adaptively adjust the tracking window size based on the target's size and orientation when using OpenCV.js. Dependencies include the OpenCV.js library. Essential parameters include probImage (back projection of the histogram), window (initial search window), and criteria (stop criteria), resulting in the coordinates and dimensions of a rotated rectangle for tracking the object.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_video/js_meanshift/js_meanshift.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.CamShift(probImage, window, criteria);\n```\n\n----------------------------------------\n\nTITLE: Otsu's Binarization Implementation in OpenCV Python\nDESCRIPTION: Demonstrates Otsu's automatic thresholding method with noise handling. Compares global thresholding, direct Otsu's thresholding, and Otsu's thresholding with Gaussian filtering.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_thresholding/py_thresholding.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('noisy2.png', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n\n# global thresholding\nret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n\n# Otsu's thresholding\nret2,th2 = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n\n# Otsu's thresholding after Gaussian filtering\nblur = cv.GaussianBlur(img,(5,5),0)\nret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n\n# plot all the images and their histograms\nimages = [img, 0, th1,\n          img, 0, th2,\n          blur, 0, th3]\ntitles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n\nfor i in range(3):\n    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Applying Median Blur with OpenCV in Python\nDESCRIPTION: The Python snippet demonstrates using OpenCV's medianBlur() for applying a median filter on images. It requires OpenCV and parameters for source image and kernel size.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/Smoothing/smoothing.py medianblur\n```\n\n----------------------------------------\n\nTITLE: Initializing WebRTC Video Capture in JavaScript\nDESCRIPTION: This snippet initializes video capture from the user's camera using WebRTC with JavaScript. It retrieves a media stream and sets it as the source for a video element. This is crucial for capturing live video feed to be processed or displayed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_video_display/js_video_display.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet video = document.getElementById(\"videoInput\"); // video is the id of video tag\nnavigator.mediaDevices.getUserMedia({ video: true, audio: false })\n    .then(function(stream) {\n        video.srcObject = stream;\n        video.play();\n    })\n    .catch(function(err) {\n        console.log(\"An error occurred! \" + err);\n    });\n```\n\n----------------------------------------\n\nTITLE: Setting Up Histogram Parameters for HSV Image Comparison\nDESCRIPTION: Configuring histogram parameters for hue and saturation channels, specifying 50 bins for hue and 60 for saturation with appropriate value ranges.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_comparison/histogram_comparison.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nint h_bins = 50; int s_bins = 60;\nint histSize[] = { h_bins, s_bins };\n// hue varies from 0 to 179, saturation from 0 to 255\nfloat h_ranges[] = { 0, 180 };\nfloat s_ranges[] = { 0, 256 };\nconst float* ranges[] = { h_ranges, s_ranges };\n// Use the o-th and 1-st channels\nint channels[] = { 0, 1 };\n```\n\nLANGUAGE: java\nCODE:\n```\nint[] histSize = { 50, 60 };\n// hue varies from 0 to 179, saturation from 0 to 255\nfloat[] ranges = { 0, 180, 0, 256 };\n// Use the 0-th and 1-st channels\nMatOfInt histChannels = new MatOfInt(0, 1);\nMatOfInt histSize2 = new MatOfInt(histSize);\nMatOfFloat histRanges = new MatOfFloat(ranges);\n```\n\nLANGUAGE: python\nCODE:\n```\nh_bins = 50\ns_bins = 60\nhistSize = [h_bins, s_bins]\n# hue varies from 0 to 179, saturation from 0 to 255\nh_ranges = [0, 180]\ns_ranges = [0, 256]\nranges = h_ranges + s_ranges # concat lists\n# Use the 0-th and 1-st channels\nchannels = [0, 1]\n```\n\n----------------------------------------\n\nTITLE: Visualizing PCA Axis in Python using OpenCV\nDESCRIPTION: Defines a function `drawAxis` that draws a line representing a principal component on the image. It calculates the angle and length, scales the axis endpoint `q` relative to the center `p`, and draws the line and arrowhead using `cv.line`. Requires OpenCV Python library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#! [visualization]\n# Function to draw the axes of the object detected by PCA\ndef drawAxis(img, p_, q_, colour, scale):\n    p = list(p_)\n    q = list(q_)\n\n    angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians\n    hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))\n\n    # Here we lengthen the arrow by a factor of scale\n    q[0] = p[0] - scale * hypotenuse * cos(angle)\n    q[1] = p[1] - scale * hypotenuse * sin(angle)\n    cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv.LINE_AA)\n\n    # create the arrow hooks\n    p[0] = q[0] + 9 * cos(angle + pi / 4)\n    p[1] = q[1] + 9 * sin(angle + pi / 4)\n    cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv.LINE_AA)\n\n    p[0] = q[0] + 9 * cos(angle - pi / 4)\n    p[1] = q[1] + 9 * sin(angle - pi / 4)\n    cv.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv.LINE_AA)\n#! [visualization]\n```\n\n----------------------------------------\n\nTITLE: Filling GpuMat with Random Numbers using Thrust\nDESCRIPTION: Demonstrates how to fill a GpuMat with random values between 0 and 10 using a Thrust transform operation with a custom random number generator.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-thrust-interop/gpu_thrust_interop.markdown#2025-04-22_snippet_4\n\nLANGUAGE: CUDA\nCODE:\n```\ncv::cuda::GpuMat d_random(512, 512, CV_32FC1);\nthrust::device_ptr<float> d_random_ptr((float*)d_random.data);\nthrust::transform(\n    thrust::counting_iterator<int>(0),\n    thrust::counting_iterator<int>(d_random.rows * d_random.cols),\n    d_random_ptr,\n    prg(0, 10)\n);\n```\n\n----------------------------------------\n\nTITLE: General CMake Command Format for Flags (Shell)\nDESCRIPTION: Illustrates the general syntax for passing configuration flags to CMake. Each flag must be preceded by `-D`, and multiple flags can be specified. The '..' typically points to the parent directory containing the main CMakeLists.txt file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ncmake [-D <flag>] [-D <flag>] ..\n```\n\n----------------------------------------\n\nTITLE: Creating Image for Histogram Display in C++\nDESCRIPTION: C++ snippet creating a blank image (`histImage`) to visualize the calculated histograms. It defines dimensions (`hist_w`, `hist_h`) and initializes a 3-channel (BGR) Mat filled with zeros (black background). A scaling factor (`bin_w`) is calculated to fit the histogram bins horizontally.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/calcHist_Demo.cpp Draw the histograms for B, G and R\n```\n\n----------------------------------------\n\nTITLE: Including G-API Fluid Backend Headers in C++\nDESCRIPTION: This code snippet demonstrates how to include necessary headers for using the G-API Fluid backend in a C++ application, which are essential for leveraging the memory optimization features of the Fluid backend.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/anisotropic_segmentation/porting_anisotropic_segmentation.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/porting_anisotropic_image_segmentation/porting_anisotropic_image_segmentation_gapi_fluid.cpp fluid_includes\n```\n\n----------------------------------------\n\nTITLE: Implementing OCR for Alphabet using kNN in OpenCV\nDESCRIPTION: This Python code provides a method to apply kNN for OCR on English alphabets using a letter-recognition dataset. Training and test data are generated from features provided in the data file. The script initiates a kNN classifier to predict alphabet letters and obtains a 93.22% accuracy rate. Dependencies include OpenCV and NumPy. Inputs are the letter-recognition.data file, and outputs are label predictions and accuracy measures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_knn/py_knn_opencv/py_knn_opencv.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\n\n# Load the data and convert the letters to numbers\ndata= np.loadtxt('letter-recognition.data', dtype= 'float32', delimiter = ',',\n                    converters= {0: lambda ch: ord(ch)-ord('A')})\n\n# Split the dataset in two, with 10000 samples each for training and test sets\ntrain, test = np.vsplit(data,2)\n\n# Split trainData and testData into features and responses\nresponses, trainData = np.hsplit(train,[1])\nlabels, testData = np.hsplit(test,[1])\n\n# Initiate the kNN, classify, measure accuracy\nknn = cv.ml.KNearest_create()\nknn.train(trainData, cv.ml.ROW_SAMPLE, responses)\nret, result, neighbours, dist = knn.findNearest(testData, k=5)\n\ncorrect = np.count_nonzero(result == labels)\naccuracy = correct*100.0/10000\nprint( accuracy )\n```\n\n----------------------------------------\n\nTITLE: Finding Convexity Defects in OpenCV Python\nDESCRIPTION: Demonstrates how to find and visualize convexity defects in contours using cv.convexityDefects(). The code loads an image, finds contours, and draws lines between convex hull points with circles at defect points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contours_more_functions/py_contours_more_functions.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nhull = cv.convexHull(cnt,returnPoints = False)\ndefects = cv.convexityDefects(cnt,hull)\n```\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\n\nimg = cv.imread('star.jpg')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nimg_gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\nret,thresh = cv.threshold(img_gray, 127, 255,0)\ncontours,hierarchy = cv.findContours(thresh,2,1)\ncnt = contours[0]\n\nhull = cv.convexHull(cnt,returnPoints = False)\ndefects = cv.convexityDefects(cnt,hull)\n\nfor i in range(defects.shape[0]):\n    s,e,f,d = defects[i,0]\n    start = tuple(cnt[s][0])\n    end = tuple(cnt[e][0])\n    far = tuple(cnt[f][0])\n    cv.line(img,start,end,[0,255,0],2)\n    cv.circle(img,far,5,[0,0,255],-1)\n\ncv.imshow('img',img)\ncv.waitKey(0)\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Video Properties with VideoCapture::get in C++\nDESCRIPTION: Explains how to retrieve video properties like frame width, height, and total frame count using the `get()` method of `cv::VideoCapture`. It takes a property identifier constant (e.g., `CAP_PROP_FRAME_WIDTH`, `CAP_PROP_FRAME_HEIGHT`, `CAP_PROP_FRAME_COUNT`) and returns a double value. This value often needs to be cast to an integer (`int`) for properties like dimensions or counts. Requires `<iostream>`, `<opencv2/core.hpp>`, and `<opencv2/videoio.hpp>` headers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_input_psnr_ssim.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nSize refS = Size((int) captRefrnc.get(CAP_PROP_FRAME_WIDTH),\n                 (int) captRefrnc.get(CAP_PROP_FRAME_HEIGHT)),\n\ncout << \"Reference frame resolution: Width=\" << refS.width << \"  Height=\" << refS.height\n     << \" of nr#: \" << captRefrnc.get(CAP_PROP_FRAME_COUNT) << endl;\n```\n\n----------------------------------------\n\nTITLE: Extracting and Displaying SVM Support Vectors - C++\nDESCRIPTION: This C++ snippet retrieves SVM support vectors using OpenCV's cv::ml::SVM::getSupportVectors method and highlights them visually on the plot. The code requires previously trained SVM objects and uses OpenCV C++ libraries. Key parameters include the SVM model and training data. Inputs are the trained model and the display image; outputs are visual markers (e.g., gray rings) around support vectors, distinguishing them from other training points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_svm.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n// Extracting and displaying support vectors in C++\n// ... (full code from samples/cpp/tutorial_code/ml/introduction_to_svm/introduction_to_svm.cpp, show_vectors)\n```\n\n----------------------------------------\n\nTITLE: Detecting Horizontal Lines using Morphology in OpenCV (C++/Java/Python)\nDESCRIPTION: Detects horizontal lines in the binary image. First, a horizontal structuring element (kernel) is created using `getStructuringElement` with `MORPH_RECT`. The size is chosen based on the expected line width. Then, erosion followed by dilation (morphological opening) is applied using this kernel to isolate horizontal structures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection.md#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n//![horiz]\n// Specify size on horizontal axis\nint horizontal_size = horizontal.cols / 30;\n\n// Create structure element for extracting horizontal lines through morphology operations\nMat horizontalStructure = getStructuringElement(MORPH_RECT, Size(horizontal_size, 1));\n\n// Apply morphology operations\nerode(horizontal, horizontal, horizontalStructure, Point(-1, -1));\ndilate(horizontal, horizontal, horizontalStructure, Point(-1, -1));\n\n// Show extracted horizontal lines\nshow_wait_destroy(\"horizontal\", horizontal);\n//![horiz]\n```\n\nLANGUAGE: java\nCODE:\n```\n//![horiz]\n// Specify size on horizontal axis\nint horizontal_size = horizontal.cols() / 30;\n\n// Create structure element for extracting horizontal lines through morphology operations\nMat horizontalStructure = Imgproc.getStructuringElement(Imgproc.MORPH_RECT, new Size(horizontal_size, 1));\n\n// Apply morphology operations\nImgproc.erode(horizontal, horizontal, horizontalStructure, new Point(-1, -1));\nImgproc.dilate(horizontal, horizontal, horizontalStructure, new Point(-1, -1));\n\n// Show extracted horizontal lines\nshowWaitDestroy(\"horizontal\", horizontal);\n//![horiz]\n```\n\nLANGUAGE: python\nCODE:\n```\n#![horiz]\n# Specify size on horizontal axis\ncols = horizontal.shape[1]\nhorizontal_size = cols // 30\n\n# Create structure element for extracting horizontal lines through morphology operations\nhorizontalStructure = cv.getStructuringElement(cv.MORPH_RECT, (horizontal_size, 1))\n\n# Apply morphology operations\nhorizontal = cv.erode(horizontal, horizontalStructure)\nhorizontal = cv.dilate(horizontal, horizontalStructure)\n\n# Show extracted horizontal lines\nshow_wait_destroy(\"horizontal\", horizontal)\n#![horiz]\n```\n\n----------------------------------------\n\nTITLE: Reading Video Frames with OpenCV VideoCapture in C++\nDESCRIPTION: Demonstrates reading individual frames from `cv::VideoCapture` objects into `cv::Mat` objects. It shows two equivalent methods: using the overloaded `>>` operator and the `read()` method. Each call attempts to grab and decode the next sequential frame from the respective video source. Requires `<opencv2/core.hpp>` and `<opencv2/videoio.hpp>` headers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_input_psnr_ssim.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nMat frameReference, frameUnderTest;\ncaptRefrnc >> frameReference;\ncaptUndTst.read(frameUnderTest);\n```\n\n----------------------------------------\n\nTITLE: Capturing Video Input in OpenCV\nDESCRIPTION: Opens a video file or camera feed using VideoCapture. This allows reading frames sequentially from the input source for background subtraction processing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/background_subtraction.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nVideoCapture capture;\nif (parser.has(\"input\"))\n    capture.open(samples::findFileOrKeep(parser.get<String>(\"input\")));\nelse\n    capture.open(parser.get<int>(\"camera\"));\n```\n\nLANGUAGE: Java\nCODE:\n```\nVideoCapture capture = new VideoCapture();\nif (inputFile.isBlank() == false) {\n    capture.open(inputFile);\n}\nelse {\n    capture.open(cameraDevice);\n}\n```\n\nLANGUAGE: Python\nCODE:\n```\n# create a video capture object\ncap = cv.VideoCapture(cv.samples.findFileOrKeep(args.input) if args.input else args.camera)\n```\n\n----------------------------------------\n\nTITLE: Implementing Lucas-Kanade Optical Flow Tracking in Java\nDESCRIPTION: Java implementation of tracking feature points in a video using the Lucas-Kanade optical flow method. The code detects Shi-Tomasi corner points in the first frame and tracks them through subsequent frames using calcOpticalFlowPyrLK().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/optical_flow.markdown#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Random;\n\nimport org.opencv.core.Core;\nimport org.opencv.core.CvType;\nimport org.opencv.core.Mat;\nimport org.opencv.core.MatOfByte;\nimport org.opencv.core.MatOfFloat;\nimport org.opencv.core.MatOfPoint;\nimport org.opencv.core.MatOfPoint2f;\nimport org.opencv.core.Point;\nimport org.opencv.core.Scalar;\nimport org.opencv.core.Size;\nimport org.opencv.core.TermCriteria;\nimport org.opencv.highgui.HighGui;\nimport org.opencv.imgproc.Imgproc;\nimport org.opencv.video.Video;\nimport org.opencv.videoio.VideoCapture;\n\nclass OpticalFlow {\n    public void run(String[] args) {\n        String filename = args[0];\n        VideoCapture capture = new VideoCapture(filename);\n        if (!capture.isOpened()) {\n            System.out.println(\"Could not open the input video: \" + filename);\n            System.exit(0);\n        }\n\n        // Create some random colors\n        Scalar[] colors = new Scalar[100];\n        Random rng = new Random();\n        for (int i = 0; i < 100; i++) {\n            int r = rng.nextInt(256);\n            int g = rng.nextInt(256);\n            int b = rng.nextInt(256);\n            colors[i] = new Scalar(r, g, b);\n        }\n\n        // Take first frame and find corners in it\n        Mat old_frame = new Mat();\n        capture.read(old_frame);\n        Mat old_gray = new Mat();\n        Imgproc.cvtColor(old_frame, old_gray, Imgproc.COLOR_BGR2GRAY);\n        MatOfPoint p0 = new MatOfPoint();\n        Imgproc.goodFeaturesToTrack(old_gray, p0, 100, 0.3, 7, new Mat(), 7, false, 0.04);\n\n        // Create a mask image for drawing purposes\n        Mat mask = Mat.zeros(old_frame.size(), old_frame.type());\n\n        while (true) {\n            Mat frame = new Mat();\n            capture.read(frame);\n            if (frame.empty()) {\n                break;\n            }\n            Mat frame_gray = new Mat();\n            Imgproc.cvtColor(frame, frame_gray, Imgproc.COLOR_BGR2GRAY);\n\n            // calculate optical flow\n            MatOfPoint2f p1 = new MatOfPoint2f();\n            MatOfByte status = new MatOfByte();\n            MatOfFloat err = new MatOfFloat();\n            MatOfPoint2f p0Copy = new MatOfPoint2f();\n            p0.convertTo(p0Copy, CvType.CV_32FC2);\n            TermCriteria criteria = new TermCriteria(TermCriteria.COUNT + TermCriteria.EPS, 10, 0.03);\n            Video.calcOpticalFlowPyrLK(old_gray, frame_gray, p0Copy, p1, status, err, new Size(15, 15), 2, criteria, 0, 0.001);\n\n            byte[] statusArr = status.toArray();\n            Point[] p0CopyArr = p0Copy.toArray();\n            Point[] p1Arr = p1.toArray();\n            List<Point> good_new = new ArrayList<>();\n\n            for (int i = 0; i < statusArr.length; i++) {\n                if (statusArr[i] == 1) {\n                    good_new.add(p1Arr[i]);\n                    Imgproc.line(mask, p1Arr[i], p0CopyArr[i], colors[i], 2);\n                    Imgproc.circle(frame, p1Arr[i], 5, colors[i], -1);\n                }\n            }\n\n            Mat img = new Mat();\n            Core.add(frame, mask, img);\n\n            HighGui.imshow(\"Frame\", img);\n            int keyboard = HighGui.waitKey(30);\n            if (keyboard == 'q' || keyboard == 27) {\n                break;\n            }\n\n            // Now update the previous frame and previous points\n            old_gray = frame_gray.clone();\n            MatOfPoint2f good_newMat = new MatOfPoint2f();\n            good_newMat.fromList(good_new);\n            p0 = new MatOfPoint(good_newMat.toArray());\n        }\n\n        System.exit(0);\n    }\n}\n\npublic class OpticalFlowDemo {\n    public static void main(String[] args) {\n        // Load the native OpenCV library\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\n\n        new OpticalFlow().run(args);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Finding Min/Max Value and Locations using OpenCV.js\nDESCRIPTION: Locates the minimum and maximum pixel values and their coordinates within a single-channel array (`src`), potentially restricted to a specific area defined by an optional `mask`. Uses the `cv.minMaxLoc` function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contour_properties/js_contour_properties.markdown#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nlet result = cv.minMaxLoc(src, mask);\nlet minVal = result.minVal;\nlet maxVal = result.maxVal;\nlet minLoc = result.minLoc;\nlet maxLoc = result.maxLoc;\n```\n\n----------------------------------------\n\nTITLE: Drawing Histogram Lines for Each Channel in Python\nDESCRIPTION: Python snippet iterating through the histogram bins (from 1 to `histSize`) and drawing lines on `histImage` using `cv.line`. It draws lines connecting the points representing the normalized counts in adjacent bins (`i-1` and `i`) for each color channel (B, G, R) using blue, green, and red colors respectively.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/Histograms_Matching/histogram_calculation/calcHist_Demo.py Draw for each channel\n```\n\n----------------------------------------\n\nTITLE: Defining a Function for Fading Text Display with OpenCV in C++\nDESCRIPTION: Defines the function `Displaying_Big_End` which displays the text \"OpenCV forever!\" centered on the screen with a fading effect. It first calculates the text size using `cv::getTextSize` to determine the center position (`org`). Inside a loop that iterates from 0 to 255, it subtracts a scalar value `i` from the original image (`image`) to create a progressively darker background (`image2`). The text is then drawn on `image2` with a color whose components also change with `i`, creating a fade-in/color-shift effect. The result is displayed in each loop iteration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/random_generator_and_text/random_generator_and_text.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n@code{.cpp}\nint Displaying_Big_End( Mat image, char* window_name, RNG rng )\n{\n  Size textsize = getTextSize(\"OpenCV forever!\", FONT_HERSHEY_COMPLEX, 3, 5, 0);\n  Point org((window_width - textsize.width)/2, (window_height - textsize.height)/2);\n  int lineType = 8;\n\n  Mat image2;\n\n  for( int i = 0; i < 255; i += 2 )\n  {\n    image2 = image - Scalar::all(i);\n    putText( image2, \"OpenCV forever!\", org, FONT_HERSHEY_COMPLEX, 3,\n           Scalar(i, i, 255), 5, lineType );\n\n    imshow( window_name, image2 );\n    if( waitKey(DELAY) >= 0 )\n      { return -1; }\n  }\n\n  return 0;\n}\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Implementing SURF Feature Detection and Homography Matching in C++\nDESCRIPTION: This C++ code demonstrates how to use SURF features and FLANN matching to detect a known object in an image. It uses cv::findHomography to estimate the transformation and cv::perspectiveTransform to map points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_homography/feature_homography.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <iostream>\\n#include <opencv2/core.hpp>\\n#include <opencv2/imgproc.hpp>\\n#include <opencv2/highgui.hpp>\\n#include <opencv2/features2d.hpp>\\n#include <opencv2/xfeatures2d.hpp>\\n#include <opencv2/calib3d.hpp>\\n\\nusing namespace cv;\\nusing namespace cv::xfeatures2d;\\n\\nvoid readme();\\n\\n/** @function main */\\nint main( int argc, char** argv )\\n{\\n    if( argc != 3 )\\n    {\\n        readme(); return -1;\\n    }\\n\\n    Mat img_object = imread( argv[1], IMREAD_GRAYSCALE );\\n    Mat img_scene = imread( argv[2], IMREAD_GRAYSCALE );\\n\\n    if( !img_object.data || !img_scene.data )\\n    {\\n        std::cout<< \" --(!) Error reading images \" << std::endl; return -1;\\n    }\\n\\n    //-- Step 1: Detect the keypoints and extract descriptors using SURF\\n    int minHessian = 400;\\n\\n    Ptr<SURF> detector = SURF::create( minHessian );\\n\\n    std::vector<KeyPoint> keypoints_object, keypoints_scene;\\n    Mat descriptors_object, descriptors_scene;\\n\\n    detector->detectAndCompute( img_object, Mat(), keypoints_object, descriptors_object );\\n    detector->detectAndCompute( img_scene, Mat(), keypoints_scene, descriptors_scene );\\n\\n    //-- Step 2: Matching descriptor vectors using FLANN matcher\\n    FlannBasedMatcher matcher;\\n    std::vector< DMatch > matches;\\n    matcher.match( descriptors_object, descriptors_scene, matches );\\n\\n    double max_dist = 0; double min_dist = 100;\\n\\n    //-- Quick calculation of max and min distances between keypoints\\n    for( int i = 0; i < descriptors_object.rows; i++ )\\n    {\\n        double dist = matches[i].distance;\\n        if( dist < min_dist ) min_dist = dist;\\n        if( dist > max_dist ) max_dist = dist;\\n    }\\n\\n    printf(\"-- Max dist : %f \\\\n\", max_dist );\\n    printf(\"-- Min dist : %f \\\\n\", min_dist );\\n\\n    //-- Draw only \\\"good\\\" matches (i.e. whose distance is less than 3*min_dist )\\n    std::vector< DMatch > good_matches;\\n\\n    for( int i = 0; i < descriptors_object.rows; i++ )\\n    {\\n        if( matches[i].distance < 3*min_dist )\\n        {\\n            good_matches.push_back( matches[i]);\\n        }\\n    }\\n\\n    Mat img_matches;\\n    drawMatches( img_object, keypoints_object, img_scene, keypoints_scene,\\n                good_matches, img_matches, Scalar::all(-1), Scalar::all(-1),\\n                std::vector<char>(), DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS );\\n\\n    //-- Localize the object\\n    std::vector<Point2f> obj;\\n    std::vector<Point2f> scene;\\n\\n    for( size_t i = 0; i < good_matches.size(); i++ )\\n    {\\n        //-- Get the keypoints from the good matches\\n        obj.push_back( keypoints_object[ good_matches[i].queryIdx ].pt );\\n        scene.push_back( keypoints_scene[ good_matches[i].trainIdx ].pt );\\n    }\\n\\n    Mat H = findHomography( obj, scene, RANSAC );\\n\\n    //-- Get the corners from the image_1 ( the object to be \\\"detected\\\" )\\n    std::vector<Point2f> obj_corners(4);\\n    obj_corners[0] = Point2f(0,0);\\n    obj_corners[1] = Point2f( (float)img_object.cols, 0 );\\n    obj_corners[2] = Point2f( (float)img_object.cols, (float)img_object.rows );\\n    obj_corners[3] = Point2f( 0, (float)img_object.rows );\\n    std::vector<Point2f> scene_corners(4);\\n\\n    perspectiveTransform( obj_corners, scene_corners, H);\\n\\n    //-- Draw lines between the corners (the mapped object in the scene - image_2 )\\n    line( img_matches, scene_corners[0] + Point2f( (float)img_object.cols, 0),\\n          scene_corners[1] + Point2f( (float)img_object.cols, 0), Scalar(0, 255, 0), 4 );\\n    line( img_matches, scene_corners[1] + Point2f( (float)img_object.cols, 0),\\n          scene_corners[2] + Point2f( (float)img_object.cols, 0), Scalar( 0, 255, 0), 4 );\\n    line( img_matches, scene_corners[2] + Point2f( (float)img_object.cols, 0),\\n          scene_corners[3] + Point2f( (float)img_object.cols, 0), Scalar( 0, 255, 0), 4 );\\n    line( img_matches, scene_corners[3] + Point2f( (float)img_object.cols, 0),\\n          scene_corners[0] + Point2f( (float)img_object.cols, 0), Scalar( 0, 255, 0), 4 );\\n\\n    //-- Show detected matches\\n    imshow( \\\"Good Matches & Object detection\\\", img_matches );\\n\\n    waitKey(0);\\n    return 0;\\n}\\n\\n/** @function readme */\\nvoid readme()\\n{\\n    std::cout << \\\" Usage: ./SURF_FLANN_matching_homography <img1> <img2>\\\" << std::endl;\\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Cropping Layer OpenCV Python\nDESCRIPTION: Python example showing how to define a custom cropping layer by overriding \\'getMemoryShapes\\' and \\'forward\\' methods in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n@snippet dnn/edge_detection.py CropLayer\n```\n\n----------------------------------------\n\nTITLE: Executing OpenCV.js Build Script with Emscripten\nDESCRIPTION: This shell command executes the Python build script (`build_js.py`) located in the OpenCV source directory (`<opencv_src_dir>/platforms/js/`) using `emcmake`. `emcmake` is an Emscripten tool that wraps CMake to configure the build environment correctly for cross-compiling to JavaScript/WebAssembly. The build output, including the `opencv.js` file, will be placed in the specified `<build_dir>`. Prerequisites include having Emscripten installed and accessible in the system's PATH, and having the OpenCV source code available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/js/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nemcmake python <opencv_src_dir>/platforms/js/build_js.py <build_dir>\n```\n\n----------------------------------------\n\nTITLE: Declaring Global Variables for Template Matching (Python)\nDESCRIPTION: Declares global variables used in the Python template matching demo script, including placeholders for the input image, template image, and the selected matching method. Window names are also defined.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/match_template/match_template.py global_variables\n```\n\n----------------------------------------\n\nTITLE: Calculating Back Projection with OpenCV in Python\nDESCRIPTION: The Python sample achieves the same tasks: image loading, HSV conversion, Hue extraction, dynamic bin selection via GUI trackbar, and calculation of histogram/back projection using OpenCV Python. It depends on the OpenCV Python package (cv2), requiring the user to supply an input image. Results are shown in cv2 windows. The main parameter is the histogram bins, adjusted via a trackbar. core OpenCV calls include cv2.cvtColor, cv2.calcHist, and cv2.calcBackProject; channel extraction uses cv2.split.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/back_projection/back_projection.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Load input image\\nimport cv2\\nimport numpy as np\\nsrc = cv2.imread(filename)\\nif src is None:\\n    print('Could not open or find the image!')\\n    exit()\\n\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Convert to HSV\\nhsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)\\n\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Use only the Hue value\\nhue = cv2.split(hsv)[0]\\n\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Create a trackbar for bins\\nbins = 30\\ndef Hist_and_Backproj(val):\\n    # histogram logic here\\n    pass\\ncv2.createTrackbar('Histogram Bins', 'Source image', bins, 180, Hist_and_Backproj)\\n\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Show image and wait\\ncv2.imshow('Source image', src)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Initialize for histogram\\ndef Hist_and_Backproj(val):\\n    histSize = max(cv2.getTrackbarPos('Histogram Bins', 'Source image'), 2)\\n    hue_range = [0, 180]\\n    # hist/backproj here\\n\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Calculate histogram and normalize\\nhist = cv2.calcHist([hue], [0], None, [histSize], [0, 180])\\ncv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX)\\n\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Backprojection\\nbackproj = cv2.calcBackProject([hue], [0], hist, [0,180], 1)\\n\n```\n\n----------------------------------------\n\nTITLE: Creating an HTML Input Range Trackbar with JavaScript\nDESCRIPTION: This snippet shows how to create an HTML <input type=\\\"range\\\"> element programmatically using JavaScript. The created element can be used as a trackbar/slider for user input. No external dependencies except the browser's DOM API. The resulting 'x' variable holds the range input element, which can be attached to the DOM as needed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_trackbar/js_trackbar.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet x = document.createElement('INPUT');\nx.setAttribute('type', 'range');\n```\n\n----------------------------------------\n\nTITLE: Ratio Test Filtering in OpenCV using Python\nDESCRIPTION: In Python, this snippet implements a ratio test on matched keypoints to filter incorrect matches, enhancing the matching process in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nsamples/python/tutorial_code/features2D/akaze_matching/AKAZE_match.py ratio test filtering\n```\n\n----------------------------------------\n\nTITLE: Displaying Results and Drawing Rectangle (Python)\nDESCRIPTION: Uses `cv2.rectangle` to draw a rectangle on the display image (`img_display`) marking the best match location. The rectangle spans from `matchLoc` (top-left) to `matchLoc` offset by the template's width and height (bottom-right). The result matrix and the image with the rectangle are shown using `cv2.imshow`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/match_template/match_template.py imshow\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Mandelbrot Generation Using cv::ParallelLoopBody - C++\nDESCRIPTION: This snippet defines a custom functor inheriting from cv::ParallelLoopBody and overrides the operator() to update the image Mat in parallel, assigning the respective Mandelbrot value per pixel. Requires OpenCV with parallel support and a previously defined grayscale and Mandelbrot iteration routine. Accepts the range of pixels and a reference to the image; designed for thread-safe, parallel execution by OpenCV's parallel_for_.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_/how_to_use_OpenCV_parallel_for_.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n// Parallel Mandelbrot implementation using ParallelLoopBody\nclass ParallelMandelbrot : public cv::ParallelLoopBody\n{\n    cv::Mat& image;\n    int maxIter;\npublic:\n    ParallelMandelbrot(cv::Mat& img, int maxI) : image(img), maxIter(maxI) {}\n    virtual void operator()(const cv::Range& range) const override\n    {\n        for (int r = range.start; r < range.end; ++r)\n        {\n            int row = r / image.cols;\n            int col = r % image.cols;\n            double x0 = (col / (double)image.cols) * 3.0 - 2.0;\n            double y0 = (row / (double)image.rows) * 2.0 - 1.0;\n            int iter = mandelbrot(cv::Point2d(x0, y0), maxIter);\n            image.at<uchar>(row, col) = grayscaleValue(iter, maxIter);\n        }\n    }\n};\n\n```\n\n----------------------------------------\n\nTITLE: Setting Uniform and Accumulate Flags in C++\nDESCRIPTION: C++ snippet setting boolean flags for histogram calculation. `uniform` is set to true, indicating that histogram bins have equal sizes. `accumulate` is set to false, meaning the histogram is cleared before calculation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/calcHist_Demo.cpp Set histogram param\n```\n\n----------------------------------------\n\nTITLE: Configuring Pose Estimation Parameters in C++ (OpenCV)\nDESCRIPTION: This C++ snippet lists example variable declarations and initial values for key parameters used in the pose estimation algorithm. It includes settings for a robust feature matcher (`numKeyPoints`, `ratio`, `fast_match`), RANSAC (`iterationsCount`, `reprojectionError`, `confidence`), and the threshold for updating the Kalman Filter (`minInliersKalman`). These parameters control the behavior and performance of feature matching, robust pose estimation (PnP with RANSAC), and the Kalman filtering stage.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_29\n\nLANGUAGE: cpp\nCODE:\n```\n// Robust Matcher parameters\n\nint numKeyPoints = 2000;      // number of detected keypoints\nfloat ratio = 0.70f;          // ratio test\nbool fast_match = true;       // fastRobustMatch() or robustMatch()\n\n\n// RANSAC parameters\n\nint iterationsCount = 500;    // number of Ransac iterations.\nint reprojectionError = 2.0;  // maximum allowed distance to consider it an inlier.\nfloat confidence = 0.95;      // ransac successful confidence.\n\n\n// Kalman Filter parameters\n\nint minInliersKalman = 30;    // Kalman threshold updating\n```\n\n----------------------------------------\n\nTITLE: Predicting with SVM in OpenCV C++\nDESCRIPTION: This snippet demonstrates how to use the StatModel::predict method to get raw output from an SVM model for regression, 1-class, or 2-class classification problems.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/ml/doc/ml_intro.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nStatModel::predict(samples, results, flags=StatModel::RAW_OUTPUT)\n```\n\n----------------------------------------\n\nTITLE: Applying K-Means Clustering to One-Dimensional Data in OpenCV\nDESCRIPTION: This code defines termination criteria and applies the K-means algorithm to the one-dimensional data. The criteria stops after 10 iterations or when accuracy reaches 1.0. The algorithm is applied with 2 clusters and random initial centers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define criteria = ( type, max_iter = 10 , epsilon = 1.0 )\ncriteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n\n# Set flags (Just to avoid line break in the code)\nflags = cv.KMEANS_RANDOM_CENTERS\n\n# Apply KMeans\ncompactness,labels,centers = cv.kmeans(z,2,None,criteria,10,flags)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Loading an Image using OpenCV in Python\nDESCRIPTION: Imports the necessary libraries (NumPy as np, OpenCV as cv) and loads an image ('messi5.jpg') using `cv.imread()`. Includes an assertion to ensure the image was loaded successfully, preventing errors if the file path is incorrect or the file doesn't exist.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_basic_ops/py_basic_ops.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> import cv2 as cv\n\n>>> img = cv.imread('messi5.jpg')\n>>> assert img is not None, \"file could not be read, check with os.path.exists()\"\n```\n\n----------------------------------------\n\nTITLE: Performing Dilation with OpenCV in Java\nDESCRIPTION: This Java snippet demonstrates the process of applying morphological dilation to an image using OpenCV. Users can specify the kernel (structuring element), its size, and the anchor point for the operation. Interactive controls (trackbars/windows) allow adjusting these parameters in real-time, enabling dynamic visualization of the dilation effect. Dependencies: OpenCV Java bindings. Input: Source image; Output: Dilated image; Limitations: Only basic dilation, without advanced parameters like iterations or border handling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n@snippet java/tutorial_code/ImgProc/erosion_dilatation/MorphologyDemo1.java dilation\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Build with CMake for Make\nDESCRIPTION: Command to configure OpenCV build using CMake, generating Makefiles.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncmake ../opencv\n```\n\n----------------------------------------\n\nTITLE: Implementing Face Detection Logic in Java using OpenCV\nDESCRIPTION: Java code for a simple face detection application. It loads the OpenCV native library, initializes a CascadeClassifier with the LBP cascade file, reads an image (lena.png), performs face detection, draws rectangles around detected faces, and saves the resulting image as 'faceDetection.png'. The main method loads the necessary native library before running the detection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\nimport org.opencv.core.Core;\nimport org.opencv.core.Mat;\nimport org.opencv.core.MatOfRect;\nimport org.opencv.core.Point;\nimport org.opencv.core.Rect;\nimport org.opencv.core.Scalar;\nimport org.opencv.imgcodecs.Imgcodecs;\nimport org.opencv.imgproc.Imgproc;\nimport org.opencv.objdetect.CascadeClassifier;\n\n//\n// Detects faces in an image, draws boxes around them, and writes the results\n// to \"faceDetection.png\".\n//\nclass DetectFaceDemo {\n  public void run() {\n    System.out.println(\"\\nRunning DetectFaceDemo\");\n\n    // Create a face detector from the cascade file in the resources\n    // directory.\n    CascadeClassifier faceDetector = new CascadeClassifier(getClass().getResource(\"/lbpcascade_frontalface.xml\").getPath());\n    Mat image = Imgcodecs.imread(getClass().getResource(\"/lena.png\").getPath());\n\n    // Detect faces in the image.\n    // MatOfRect is a special container class for Rect.\n    MatOfRect faceDetections = new MatOfRect();\n    faceDetector.detectMultiScale(image, faceDetections);\n\n    System.out.println(String.format(\"Detected %s faces\", faceDetections.toArray().length));\n\n    // Draw a bounding box around each face.\n    for (Rect rect : faceDetections.toArray()) {\n        Imgproc.rectangle(image, new Point(rect.x, rect.y), new Point(rect.x + rect.width, rect.y + rect.height), new Scalar(0, 255, 0));\n    }\n\n    // Save the visualized detection.\n    String filename = \"faceDetection.png\";\n    System.out.println(String.format(\"Writing %s\", filename));\n    Imgcodecs.imwrite(filename, image);\n  }\n}\n\npublic class HelloOpenCV {\n  public static void main(String[] args) {\n    System.out.println(\"Hello, OpenCV\");\n\n    // Load the native library.\n    System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\n    new DetectFaceDemo().run();\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adjusting SURF Descriptor Size in OpenCV Python\nDESCRIPTION: This snippet demonstrates how to check and modify the SURF descriptor size from 64 to 128 dimensions. Extended descriptors provide more distinctiveness at the cost of increased computation time.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Find size of descriptor\n>>> print( surf.descriptorSize() )\n64\n\n# That means flag, \"extended\" is False.\n>>> surf.getExtended()\n False\n\n# So we make it to True to get 128-dim descriptors.\n>>> surf.setExtended(True)\n>>> kp, des = surf.detectAndCompute(img,None)\n>>> print( surf.descriptorSize() )\n128\n>>> print( des.shape )\n(47, 128)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Layer Instance OpenCV C++\nDESCRIPTION: A static method \\'create\\' that creates an instance of the custom layer and returns a cv::Ptr.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/custom_layers.hpp MyLayer::create\n```\n\n----------------------------------------\n\nTITLE: Feature Matching with FLANN in Python\nDESCRIPTION: This Python implementation demonstrates how to use the FlannBasedMatcher for efficient feature matching with SURF descriptors. It loads two images, detects and describes features, matches them using FLANN, and filters the matches using Lowe's distance ratio test.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_flann_matcher/feature_flann_matcher.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nimport matplotlib.pyplot as plt\n\nimg1 = cv.imread(cv.samples.findFile('box.png'), cv.IMREAD_GRAYSCALE)          # queryImage\nimg2 = cv.imread(cv.samples.findFile('box_in_scene.png'), cv.IMREAD_GRAYSCALE) # trainImage\n\n# Initiate SURF detector\nsurf = cv.xfeatures2d.SURF_create()\n\n# Find keypoints and descriptors with SURF\nkp1, des1 = surf.detectAndCompute(img1,None)\nkp2, des2 = surf.detectAndCompute(img2,None)\n\n# FLANN parameters\nFLANN_INDEX_KDTREE = 1\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\nsearch_params = dict(checks = 50)   # or pass empty dictionary\nflann = cv.FlannBasedMatcher(index_params, search_params)\nmatches = flann.knnMatch(des1, des2, k=2)\n\n# Need to draw only good matches, so create a mask\nmatchesMask = [[0,0] for i in range(len(matches))]\n\n# Ratio test as per Lowe's paper\nfor i,(m,n) in enumerate(matches):\n    if m.distance < 0.7*n.distance:\n        matchesMask[i]=[1,0]\n\ndraw_params = dict(matchColor = (0,255,0),\n                   singlePointColor = (255,0,0),\n                   matchesMask = matchesMask,\n                   flags = cv.DrawMatchesFlags_DEFAULT)\nimg3 = cv.drawMatchesKnn(img1, kp1, img2, kp2, matches, None, **draw_params)\nplt.imshow(img3,), plt.show()\n```\n\n----------------------------------------\n\nTITLE: Displaying an Image using OpenCV in C++\nDESCRIPTION: This snippet demonstrates how to display an image in a window using OpenCV's cv::imshow in C++. It sets up a window with a title and displays the cv::Mat image until a key is pressed, as monitored by cv::waitKey.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/display_image/display_image.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nimshow(\"Display window\", image);\nwaitKey(0);\n```\n\n----------------------------------------\n\nTITLE: Drawing Detected Lines from Hough Transform in OpenCV (Python)\nDESCRIPTION: This snippet iterates over returned (rho, theta) pairs from cv2.HoughLines and draws lines onto the output image in Python. It calculates endpoints for each line and uses cv2.line to visualize them. Expects lines as a numpy array. Requires cv2 and numpy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nif lines is not None:\\n    for i in range(0, len(lines)):\\n        rho = lines[i][0][0]\\n        theta = lines[i][0][1]\\n        a = np.cos(theta)\\n        b = np.sin(theta)\\n        x0 = a*rho\\n        y0 = b*rho\\n        x1 = int(x0 + 1000*(-b))\\n        y1 = int(y0 + 1000*(a))\\n        x2 = int(x0 - 1000*(-b))\\n        y2 = int(y0 - 1000*(a))\\n        cv2.line(img, (x1, y1), (x2, y2), (0,0,255), 3, cv2.LINE_AA)\\n\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with PyTorch Model\nDESCRIPTION: This Python snippet performs inference using the original PyTorch segmentation model (`original_net`). It first sets the model to evaluation mode using `original_net.eval()` to disable dropout and batch normalization updates. The preprocessed input image (`preproc_img`), likely a NumPy array, is converted to a PyTorch tensor. Inference is run within a `torch.no_grad()` context to disable gradient calculations, saving memory. The model's output, typically a dictionary for segmentation models from `torchvision`, is accessed using the key 'out'. The output shape is printed, and `argmax(dim=0)` is applied to the output tensor (after removing the batch dimension) to get the predicted class ID for each pixel. Requires the `torch` library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\noriginal_net.eval()\npreproc_img = torch.FloatTensor(preproc_img)\n\nwith torch.no_grad():\n    # obtaining unnormalized probabilities for each class\n    out = original_net(preproc_img)['out']\n\nprint(\"\\nPyTorch segmentation model prediction: \\n\")\nprint(\"* shape: \", out.shape)\n\n# get IDs of predicted classes\nout_predictions = out[0].argmax(dim=0)\n```\n\n----------------------------------------\n\nTITLE: Loading Source Image in C++\nDESCRIPTION: C++ snippet demonstrating how to load an image from a file using OpenCV's `imread` function. The first command-line argument is expected to be the path to the image file. Includes error handling if the image cannot be loaded.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/calcHist_Demo.cpp Load image\n```\n\n----------------------------------------\n\nTITLE: Reducing Noise with Gaussian Blur in Java\nDESCRIPTION: Applies a 3x3 Gaussian blur to the source image using Imgproc.GaussianBlur to reduce noise. The blurred image replaces the original source image. Requires OpenCV Java bindings (Imgproc).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n//! [reduce_noise]\n// Reduce noise by blurring with a Gaussian filter ( kernel size = 3 )\nImgproc.GaussianBlur( src, src, new Size(3, 3), 0, 0, Core.BORDER_DEFAULT );\n//! [reduce_noise]\n```\n\n----------------------------------------\n\nTITLE: Loading Images with OpenCV in C++\nDESCRIPTION: This snippet shows how to load an image from disk using OpenCV in C++. It makes use of the cv::imread function and typically checks if the image was properly loaded. The primary parameter is the file path and optionally an image read mode; the output is an image matrix object. Requires OpenCV installed and included.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\\n\\ncv::Mat src = cv::imread(\"path_to_image\", cv::IMREAD_COLOR);\\nif(src.empty()) {\\n    std::cout << \"Could not open or find the image!\" << std::endl;\\n    return -1;\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Calculating SSIM for Image Similarity in OpenCV\nDESCRIPTION: This function implements the Structural Similarity Index (SSIM) algorithm to compare image similarity. It breaks down the comparison into luminance, contrast, and structure components for better perceptual accuracy. The function returns a similarity index for each image channel, with values between 0 and 1.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_input_psnr_ssim.markdown#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nScalar getMSSIM( const Mat& i1, const Mat& i2)\n{\n    const double C1 = 6.5025, C2 = 58.5225;\n    /***************************** INITS **********************************/\n    int d = CV_32F;\n\n    Mat I1, I2;\n    i1.convertTo(I1, d);            // cannot calculate on one byte large values\n    i2.convertTo(I2, d);\n\n    Mat I2_2   = I2.mul(I2);        // I2^2\n    Mat I1_2   = I1.mul(I1);        // I1^2\n    Mat I1_I2  = I1.mul(I2);        // I1 * I2\n\n    /*************************** END INITS **********************************/\n\n    Mat mu1, mu2;                   // PRELIMINARY COMPUTING\n    GaussianBlur(I1, mu1, Size(11, 11), 1.5);\n    GaussianBlur(I2, mu2, Size(11, 11), 1.5);\n\n    Mat mu1_2   =   mu1.mul(mu1);\n    Mat mu2_2   =   mu2.mul(mu2);\n    Mat mu1_mu2 =   mu1.mul(mu2);\n\n    Mat sigma1_2, sigma2_2, sigma12;\n\n    GaussianBlur(I1_2, sigma1_2, Size(11, 11), 1.5);\n    sigma1_2 -= mu1_2;\n\n    GaussianBlur(I2_2, sigma2_2, Size(11, 11), 1.5);\n    sigma2_2 -= mu2_2;\n\n    GaussianBlur(I1_I2, sigma12, Size(11, 11), 1.5);\n    sigma12 -= mu1_mu2;\n\n    ///////////////////////////////// FORMULA ////////////////////////////////\n    Mat t1, t2, t3;\n\n    t1 = 2 * mu1_mu2 + C1;\n    t2 = 2 * sigma12 + C2;\n    t3 = t1.mul(t2);                 // t3 = ((2*mu1_mu2 + C1).*(2*sigma12 + C2))\n\n    t1 = mu1_2 + mu2_2 + C1;\n    t2 = sigma1_2 + sigma2_2 + C2;\n    t1 = t1.mul(t2);                 // t1 =((mu1_2 + mu2_2 + C1).*(sigma1_2 + sigma2_2 + C2))\n\n    Mat ssim_map;\n    divide(t3, t1, ssim_map);        // ssim_map =  t3./t1;\n\n    Scalar mssim = mean(ssim_map);   // mssim = average of ssim map\n    return mssim;\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Laplacian Operator Full Example in Python\nDESCRIPTION: Complete Python code demonstrating loading an image using cv2.imread, applying Gaussian blur with cv2.GaussianBlur, converting to grayscale using cv2.cvtColor, applying the Laplacian operator with cv2.Laplacian, converting the result using cv2.convertScaleAbs, and displaying it with cv2.imshow. This code relies on the OpenCV Python bindings (cv2) and potentially NumPy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# The tutorial code's is shown lines below. You can also download it from\n# [here](https://raw.githubusercontent.com/opencv/opencv/4.x/samples/python/tutorial_code/ImgTrans/LaPlace/laplace_demo.py)\n@include samples/python/tutorial_code/ImgTrans/LaPlace/laplace_demo.py\n```\n\n----------------------------------------\n\nTITLE: Computing Bounding Rotated Boxes and Ellipses in Contours - OpenCV Python\nDESCRIPTION: This Python code uses OpenCV to load a grayscale image, threshold it, find contours, and compute both the minimum area bounding rectangle and best-fit ellipse for each detected contour. The primary dependency is OpenCV (cv2). Input is typically a binary image file, and outputs include displayed images showing retangular and elliptical overlays corresponding to the detected contours. Key logic includes cv2.minAreaRect, cv2.boxPoints, cv2.ellipse, and handling of contours with more than five points for ellipse fitting.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rotated_ellipses/bounding_rotated_ellipses.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\\nimport numpy as np\\n\\nimg = cv2.imread('shapes.png', 0)\\nret,thresh = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\\ncontours, _ = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\\ncolor = cv2.cvtColor(img,cv2.COLOR_GRAY2BGR)\\nfor cnt in contours:\\n    rect = cv2.minAreaRect(cnt)\\n    box = cv2.boxPoints(rect)\\n    box = np.int0(box)\\n    cv2.drawContours(color,[box],0,(0,255,0),2)\\n    if len(cnt) > 5:\\n        ellipse = cv2.fitEllipse(cnt)\\n        cv2.ellipse(color,ellipse,(255,0,0),2)\\ncv2.imshow(\"Contours\", color)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Running TensorFlow Segmentation Model Inference\nDESCRIPTION: Executes a TensorFlow segmentation model on a preprocessed image and prints the output shape. The model predicts PASCAL VOC class indices for each pixel in the image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nout = tf_session.run(\n    output_tensor_name,\n    feed_dict={input_tensor_name: [preproc_img]}\n)\n\nprint(\"TF segmentation model prediction: \\n\")\nprint(\"* shape: \", out.shape)\n```\n\n----------------------------------------\n\nTITLE: Trackbar for Lower HSV Range in C++\nDESCRIPTION: Controls the lower HSV range for image thresholding in an OpenCV C++ application. Facilitates real-time adjustment of threshold parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold_inRange/threshold_inRange.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nint low_h = 0;\\ncv::createTrackbar(\"Low H\", \"Control\", &low_h, 179);\n```\n\n----------------------------------------\n\nTITLE: Command Line Parameters for ChArUco Calibration in OpenCV\nDESCRIPTION: This code snippet shows the command line parameters for running the ChArUco calibration example. It includes parameters for the camera calibration output file, board dimensions, square and marker size, dictionary ID, and image path.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_calibration/aruco_calibration.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n\"camera_calib.txt\" -w=5 -h=7 -sl=0.04 -ml=0.02 -d=10\n-v=path/img_%02d.jpg\n```\n\n----------------------------------------\n\nTITLE: Setting an Image Channel to Zero using NumPy Indexing in Python\nDESCRIPTION: Demonstrates how to efficiently set all pixel values of a specific channel (the Red channel, index 2) to zero using NumPy array slice assignment (`img[:, :, channel_index] = 0`). This avoids the need to split and merge channels, offering better performance.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_basic_ops/py_basic_ops.markdown#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> img[:,:,2] = 0\n```\n\n----------------------------------------\n\nTITLE: Determining Best Class Output in C++\nDESCRIPTION: This snippet illustrates extracting the predicted class by finding the maximum probability in the output blob's scores. It identifies the index with the highest value, corresponding to the best-guess class from the ILSVRC2012 dataset.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_googlenet/dnn_googlenet.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/classification.cpp Get a class with a highest score\n```\n\n----------------------------------------\n\nTITLE: Complete ArUco Board Detection Implementation in C++\nDESCRIPTION: A complete sample showing how to detect an ArUco board from an image or video stream. It includes marker detection, matching image points with the board, and pose estimation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_board_detection/aruco_board_detection.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ncv::Mat imageCopy;\nimage.copyTo(imageCopy);\n\ncv::aruco::DetectorParameters detectorParams = cv::aruco::DetectorParameters();\ncv::aruco::Dictionary dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250);\ncv::aruco::ArucoDetector detector(dictionary, detectorParams);\n\nstd::vector<int> markerIds;\nstd::vector<std::vector<cv::Point2f>> markerCorners, rejectedCandidates;\ndetector.detectMarkers(image, markerCorners, markerIds, rejectedCandidates);\n\n// if at least one marker detected\nif (markerIds.size() > 0) {\n    cv::aruco::drawDetectedMarkers(imageCopy, markerCorners, markerIds);\n    std::vector<cv::Point3f> objPoints;\n    std::vector<cv::Point2f> imgPoints;\n    cv::Mat cameraMatrix, distCoeffs;\n    readCameraParameters(parser.get<std::string>(\"c\"), cameraMatrix, distCoeffs);\n    cv::Ptr<cv::aruco::Board> board;\n    readDetectorParameters(parser.get<std::string>(\"cd\"), board, dictionary);\n\n    // calculate board pose\n    int markersOfBoardDetected = 0;\n    if (!cameraMatrix.empty()) {\n        cv::Vec3d rvec, tvec;\n\n        // call to Board::matchImagePoints requires that board is GridBoard\n        cv::Ptr<cv::aruco::GridBoard> gridboard = board.dynamicCast<cv::aruco::GridBoard>();\n        if (gridboard) {\n            // match image points\n            // markerCorners, markerIds -> imagePoints, objPoints\n            markersOfBoardDetected = gridboard->matchImagePoints(markerCorners, markerIds, objPoints, imgPoints);\n        }\n        if(markersOfBoardDetected) {\n            // find pose\n            cv::solvePnP(objPoints, imgPoints, cameraMatrix, distCoeffs, rvec, tvec);\n\n            // draw axis\n            cv::drawFrameAxes(imageCopy, cameraMatrix, distCoeffs, rvec, tvec, 0.1);\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Edge Detection using Canny Detector in OpenCV (C++)\nDESCRIPTION: This snippet applies the Canny edge detector to a grayscale image using OpenCV in C++. It uses cv::Canny with parameters for lower and upper thresholds. Inputs are the source image and threshold values; output is a binary edge-detected image. Requires prior loading and possibly grayscale conversion of the input image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nMat src_gray, edges;\\ncvtColor(src, src_gray, COLOR_BGR2GRAY);\\nCanny(src_gray, edges, 50, 200, 3);\\n\n```\n\n----------------------------------------\n\nTITLE: Finding Chessboard Corners in OpenCV C++\nDESCRIPTION: Code to detect chessboard corners in an image using OpenCV's findChessboardCorners function. It attempts to locate the corners of a 9x6 chessboard pattern in a grayscale image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat img = cv::imread(imagePath);\ncv::Mat gray;\ncv::cvtColor(img, gray, cv::COLOR_BGR2GRAY);\n\nconst cv::Size patternSize(9, 6);\nstd::vector<cv::Point2f> corners;\nbool patternFound = cv::findChessboardCorners(gray, patternSize, corners);\n\nif (patternFound) {\n    cv::cornerSubPix(gray, corners, cv::Size(11, 11), cv::Size(-1, -1), \n        cv::TermCriteria(cv::TermCriteria::EPS + cv::TermCriteria::COUNT, 30, 0.1));\n    cv::drawChessboardCorners(img, patternSize, corners, patternFound);\n}\n```\n\n----------------------------------------\n\nTITLE: Estimating Homography Transformation Using RANSAC in C++\nDESCRIPTION: This code estimates the homography transformation between matched points using the RANSAC algorithm. It requires at least 4 matches to compute the transformation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_tracking/akaze_tracking.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nhomography = findHomography(Points(matched1), Points(matched2),\n                            RANSAC, ransac_thresh, inlier_mask);\n```\n\n----------------------------------------\n\nTITLE: Including ArUco Detector Header in C++\nDESCRIPTION: Code snippet showing how to include the ArUco detector header in a C++ program. This header provides access to ArUco marker detection functionality in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_detection/aruco_detection.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/objdetect/aruco_detector.hpp>\n```\n\n----------------------------------------\n\nTITLE: Inefficient GPU Arithmetic Operation in C++\nDESCRIPTION: Illustrates an arithmetic operation (`b.t1 = 2 * b.mu1_mu2 + C1;`) that, while syntactically correct for `GpuMat` objects, can be inefficient on the GPU. This expression implicitly creates a temporary `GpuMat` to store the result of the multiplication (`2 * b.mu1_mu2`) before performing the addition, leading to hidden data transfers and memory allocation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nb.t1 = 2 * b.mu1_mu2 + C1;\n```\n\n----------------------------------------\n\nTITLE: Running Detection with Generalized Hough Transform in C++\nDESCRIPTION: This code runs the Generalized Hough Transform detection process, which can be time-intensive, especially with larger images or when using the Guil method.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/generalized_hough_ballard_guil/generalized_hough_ballard_guil.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n# Run detection\n@snippet samples/cpp/tutorial_code/ImgTrans/generalizedHoughTransform.cpp generalized-hough-transform-run\n```\n\n----------------------------------------\n\nTITLE: Finding OpenCV Include Paths using pkg-config in Bash\nDESCRIPTION: This Bash command utilizes `pkg-config` to retrieve the compiler flags required for using OpenCV, specifically the include paths (`-I` flags). This helps locate the necessary header files for compiling OpenCV applications in the Eclipse project settings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_eclipse/linux_eclipse.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npkg-config --cflags opencv\n```\n\n----------------------------------------\n\nTITLE: Refining GrabCut Results with Mask-based Initialization in OpenCV Python\nDESCRIPTION: This code snippet shows how to refine the results of the GrabCut algorithm using mask-based initialization. It loads a manually created mask image to fine-tune the foreground extraction, particularly useful for complex objects or when the rectangular initialization is not sufficient.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_grabcut/py_grabcut.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# newmask is the mask image I manually labelled\nnewmask = cv.imread('newmask.png', cv.IMREAD_GRAYSCALE)\nassert newmask is not None, \"file could not be read, check with os.path.exists()\"\n\n# wherever it is marked white (sure foreground), change mask=1\n# wherever it is marked black (sure background), change mask=0\nmask[newmask == 0] = 0\nmask[newmask == 255] = 1\n\nmask, bgdModel, fgdModel = cv.grabCut(img,mask,None,bgdModel,fgdModel,5,cv.GC_INIT_WITH_MASK)\n\nmask = np.where((mask==2)|(mask==0),0,1).astype('uint8')\nimg = img*mask[:,:,np.newaxis]\nplt.imshow(img),plt.colorbar(),plt.show()\n```\n\n----------------------------------------\n\nTITLE: Drawing a Rectangle in Python\nDESCRIPTION: Example of using the rectangle() function in OpenCV Python to draw a filled yellow rectangle. The function specifies two opposite corners of the rectangle along with color and fill options.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nrectangle(rook_image, (0, 7*w//8), (w, w), (0, 255, 255), -1, cv.LINE_8)\n```\n\n----------------------------------------\n\nTITLE: Draw Final Matches and Output in OpenCV using C++\nDESCRIPTION: A C++ technique to draw the final matched keypoints and output the image, leveraging functions in OpenCV for visualization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\nsamples/cpp/tutorial_code/features2D/AKAZE_match.cpp draw final matches\n```\n\n----------------------------------------\n\nTITLE: Applying Box Filter Blur in OpenCV Python\nDESCRIPTION: This code shows how to apply box filter blurring using cv.blur(). It takes a simple average of all pixels under the kernel area (5x5 in this example) and replaces the central pixel with the average value.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_filtering/py_filtering.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('opencv-logo-white.png')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n\nblur = cv.blur(img,(5,5))\n\nplt.subplot(121),plt.imshow(img),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(blur),plt.title('Blurred')\nplt.xticks([]), plt.yticks([])\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Calculating Moments of Contours in Python with OpenCV\nDESCRIPTION: This snippet demonstrates how to calculate moments of contours using cv.moments(). It reads an image, applies thresholding, finds contours, and then calculates moments for the first contour.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nimg = cv.imread('star.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nret,thresh = cv.threshold(img,127,255,0)\ncontours,hierarchy = cv.findContours(thresh, 1, 2)\n\ncnt = contours[0]\nM = cv.moments(cnt)\nprint( M )\n```\n\n----------------------------------------\n\nTITLE: Embedding the Intelligent Scissors Demo via HTML Iframe\nDESCRIPTION: This code snippet uses an HTML <iframe> element to embed the Intelligent Scissors JavaScript demo page (js_intelligent_scissors.html) within the documentation or application interface. The iframe is set to occupy full width and automatically adjusts its height based on the document loaded, ensuring the demo is displayed seamlessly. The expected input is the demo HTML file at the relative path, and the output is the rendered interactive frame; the snippet assumes the referenced HTML file exists and is accessible in the environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_intelligent_scissors/js_intelligent_scissors.markdown#2025-04-22_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<iframe src=\"../../js_intelligent_scissors.html\" width=\"100%\"\n        onload=\"this.style.height=this.contentDocument.body.scrollHeight +'px';\">\n</iframe>\n```\n\n----------------------------------------\n\nTITLE: Retrieving External Contours With RETR_EXTERNAL in OpenCV Python\nDESCRIPTION: This snippet demonstrates how to use OpenCV's RETR_EXTERNAL retrieval mode to extract only the outermost contours. All child contours are ignored, thus creating a hierarchy where only top-level contours are represented.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contours_hierarchy/py_contours_hierarchy.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n>>> hierarchy\narray([[[ 1, -1, -1, -1],\n        [ 2,  0, -1, -1],\n        [-1,  1, -1, -1]]])\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Primitive Types to XML/YAML/JSON - OpenCV Python\nDESCRIPTION: This Python snippet demonstrates writing and reading primitive types (integers, floats, strings) to XML/YAML/JSON files in OpenCV. Use FileStorage.write(name, value) to write and fs.getNode('name').real() to read numeric values. The first parameter is the key, and the second is the value. Requires opencv-python installed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfs = cv2.FileStorage('test.yml', cv2.FileStorage_WRITE)\\nfs.write('number', 5)\\nfs.release()\\n\\nfs = cv2.FileStorage('test.yml', cv2.FileStorage_READ)\\nread_number = int(fs.getNode('number').real())\\nfs.release()\n```\n\n----------------------------------------\n\nTITLE: Histogram with Mask\nDESCRIPTION: Demonstrates how to calculate and plot histograms for specific image regions using a mask.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_begins/py_histogram_begins.markdown#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimg = cv.imread('home.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n\n# create a mask\nmask = np.zeros(img.shape[:2], np.uint8)\nmask[100:300, 100:400] = 255\nmasked_img = cv.bitwise_and(img,img,mask = mask)\n\n# Calculate histogram with mask and without mask\n# Check third argument for mask\nhist_full = cv.calcHist([img],[0],None,[256],[0,256])\nhist_mask = cv.calcHist([img],[0],mask,[256],[0,256])\n\nplt.subplot(221), plt.imshow(img, 'gray')\nplt.subplot(222), plt.imshow(mask,'gray')\nplt.subplot(223), plt.imshow(masked_img, 'gray')\nplt.subplot(224), plt.plot(hist_full), plt.plot(hist_mask)\nplt.xlim([0,256])\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Drawing Histogram Lines for Each Channel in C++\nDESCRIPTION: C++ snippet iterating through the histogram bins (from 1 to `histSize`) and drawing lines on `histImage` to represent the normalized histogram values for each channel (B, G, R). It uses `cv::line` with appropriate colors (blue, green, red) to draw vertical bars representing bin counts.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_27\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/calcHist_Demo.cpp Draw for each channel\n```\n\n----------------------------------------\n\nTITLE: Building LLVM on Windows using CMake and MSBuild\nDESCRIPTION: This snippet demonstrates how to set up and compile LLVM on Windows using CMake and MSBuild. It involves configuring the build for Visual Studio and is executed in a Developer Command Prompt. Requires LLVM source and Clang source to be set up correctly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_halide/dnn_halide.markdown#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nmkdir \\\\path-to-llvm-build\\\\ && cd \\\\path-to-llvm-build\\\\\ncmake.exe -DLLVM_ENABLE_TERMINFO=OFF -DLLVM_TARGETS_TO_BUILD=X86 -DLLVM_ENABLE_ASSERTIONS=ON -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=\\\\path-to-llvm-install\\\\ -G \"Visual Studio 14 Win64\" \\\\path-to-llvm-src\\\\\nMSBuild.exe /m:4 /t:Build /p:Configuration=Release .\\\\INSTALL.vcxproj\n```\n\n----------------------------------------\n\nTITLE: Running CMake with Documentation Flag in Shell\nDESCRIPTION: This shell command configures the build system for OpenCV with included documentation. It requires OpenCV source code and the optional OpenCV_contrib repository. Outputs HTML documentation files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DBUILD_DOCS=ON ../opencv\n```\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DBUILD_DOCS=ON -DOPENCV_EXTRA_MODULES_PATH=../opencv_contrib/modules ../opencv\n```\n\n----------------------------------------\n\nTITLE: Performing SURF Feature Detection in C++\nDESCRIPTION: This C++ code demonstrates how to use OpenCV's SURF detector to find keypoints in an image. It includes loading images, creating a SURF detector, detecting keypoints, and drawing them on the image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_detection/feature_detection.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <iostream>\\n#include <opencv2/opencv.hpp>\\n#include <opencv2/xfeatures2d.hpp>\\n\\nusing namespace cv;\\nusing namespace cv::xfeatures2d;\\n\\nint main( int argc, char** argv )\\n{\\n    CommandLineParser parser( argc, argv, \"{@input | box.png | input image}\" );\\n    Mat src = imread( samples::findFile( parser.get<String>( \"@input\" ) ), IMREAD_GRAYSCALE );\\n    if ( src.empty() )\\n    {\\n        std::cout << \"Could not open or find the image!\\n\" << std::endl;\\n        std::cout << \"Usage: \" << argv[0] << \" <Input image>\" << std::endl;\\n        return -1;\\n    }\\n\\n    //-- Step 1: Detect the keypoints using SURF Detector\\n    int minHessian = 400;\\n    Ptr<SURF> detector = SURF::create( minHessian );\\n    std::vector<KeyPoint> keypoints;\\n    detector->detect( src, keypoints );\\n\\n    //-- Draw keypoints\\n    Mat img_keypoints;\\n    drawKeypoints( src, keypoints, img_keypoints );\\n\\n    //-- Show detected (drawn) keypoints\\n    imshow(\"SURF Keypoints\", img_keypoints );\\n\\n    waitKey();\\n    return 0;\\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Depth Map Capture with RealSense\nDESCRIPTION: Demonstrates how to capture depth map data from a RealSense camera using VideoCapture's streaming operator. The code runs in a continuous loop until a key is pressed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/intelperc.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n    VideoCapture capture( CAP_REALSENSE );\n    for(;;)\n    {\n        Mat depthMap;\n        capture >> depthMap;\n\n        if( waitKey( 30 ) >= 0 )\n            break;\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Harris Corner Detector in C++\nDESCRIPTION: This C++ code demonstrates how to use the cv::cornerHarris function to detect corners in an image using the Harris-Stephens method. It processes the input image, applies the corner detection algorithm, and visualizes the results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/harris_detector/harris_detector.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include \"opencv2/highgui.hpp\"\n#include \"opencv2/imgproc.hpp\"\n#include <iostream>\n\nusing namespace cv;\nusing namespace std;\n\nMat src, src_gray;\nint thresh = 200;\nint max_thresh = 255;\n\nconst char* source_window = \"Source image\";\nconst char* corners_window = \"Corners detected\";\n\nvoid cornerHarris_demo( int, void* );\n\nint main( int argc, char** argv )\n{\n    CommandLineParser parser( argc, argv, \"{@input | building.jpg | input image}\" );\n    src = imread( samples::findFile( parser.get<String>( \"@input\" ) ) );\n    if ( src.empty() )\n    {\n        cout << \"Could not open or find the image!\\n\" << endl;\n        cout << \"Usage: \" << argv[0] << \" <Input image>\" << endl;\n        return -1;\n    }\n\n    cvtColor( src, src_gray, COLOR_BGR2GRAY );\n\n    namedWindow( source_window );\n    createTrackbar( \"Threshold: \", source_window, &thresh, max_thresh, cornerHarris_demo );\n    imshow( source_window, src );\n\n    cornerHarris_demo( 0, 0 );\n\n    waitKey();\n    return 0;\n}\n\nvoid cornerHarris_demo( int, void* )\n{\n    int blockSize = 2;\n    int apertureSize = 3;\n    double k = 0.04;\n\n    Mat dst = Mat::zeros( src.size(), CV_32FC1 );\n    cornerHarris( src_gray, dst, blockSize, apertureSize, k );\n\n    Mat dst_norm, dst_norm_scaled;\n    normalize( dst, dst_norm, 0, 255, NORM_MINMAX, CV_32FC1, Mat() );\n    convertScaleAbs( dst_norm, dst_norm_scaled );\n\n    for( int i = 0; i < dst_norm.rows ; i++ )\n    {\n        for( int j = 0; j < dst_norm.cols; j++ )\n        {\n            if( (int) dst_norm.at<float>(i,j) > thresh )\n            {\n                circle( dst_norm_scaled, Point(j,i), 5,  Scalar(0), 2, 8, 0 );\n            }\n        }\n    }\n\n    namedWindow( corners_window );\n    imshow( corners_window, dst_norm_scaled );\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Ratio-Based Backprojection in Python OpenCV\nDESCRIPTION: This code snippet creates a backprojection by calculating the ratio between object and target histograms, and reshaping the result to match the target image dimensions. The backprojection represents probability of pixels belonging to the target object.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_backprojection/py_histogram_backprojection.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nh,s,v = cv.split(hsvt)\nB = R[h.ravel(),s.ravel()]\nB = np.minimum(B,1)\nB = B.reshape(hsvt.shape[:2])\n```\n\n----------------------------------------\n\nTITLE: Creating an ArUco Grid Board in C++\nDESCRIPTION: Code snippet showing how to create an ArUco grid board with specified parameters. The grid board is defined by the number of markers in X and Y directions, marker size, separation between markers, and dictionary type.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_board_detection/aruco_board_detection.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ncv::aruco::Dictionary dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250);\ncv::Ptr<cv::aruco::GridBoard> board = cv::aruco::GridBoard::create(5, 7, 0.04, 0.01, dictionary);\n```\n\n----------------------------------------\n\nTITLE: Loading an Image using OpenCV in Java\nDESCRIPTION: Loads an image from a file specified by a command-line argument using the `Imgcodecs.imread` function. It performs error checking to ensure the image was loaded correctly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic void run(String[] args) {\n        //![load]\n        String filename = args.length > 0 ? args[0] : \"../data/chicky_512.png\";\n        // Load the image\n        Mat src = Imgcodecs.imread(filename);\n        // Check if image is loaded fine\n        if( src.empty() ) {\n            System.out.println(\"Error opening image!\");\n            System.out.println(\"Program Arguments: [image_name -- default ../data/chicky_512.png]\");\n            System.exit(-1);\n        }\n        //![load]\n```\n\n----------------------------------------\n\nTITLE: Using MatVector Container with OpenCV.js - JavaScript\nDESCRIPTION: Demonstrates initialization and use of cv.MatVector, a dynamic container for multiple Mat objects. Includes adding (push_back) and retrieval (get) of Mats. Highlights the importance of manually deleting resources after use. Useful for collections of images or channels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_basic_ops/js_basic_ops.markdown#2025-04-22_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet mat = new cv.Mat();\n// Initialise a MatVector\nlet matVec = new cv.MatVector();\n// Push a Mat back into MatVector\nmatVec.push_back(mat);\n// Get a Mat fom MatVector\nlet cnt = matVec.get(0);\nmat.delete(); matVec.delete(); cnt.delete();\n```\n\n----------------------------------------\n\nTITLE: Drawing Detected ArUco Markers using OpenCV C++\nDESCRIPTION: This code snippet demonstrates the use of OpenCV's `drawDetectedMarkers()` function to visualize detected markers on an image. It requires an input/output Mat image, and structures for detected marker corners and IDs obtained from `detectMarkers()`. This is typically used for verification and does not affect detection accuracy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_detection/aruco_detection.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\ncv::Mat outputImage = inputImage.clone();\ncv::aruco::drawDetectedMarkers(outputImage, markerCorners, markerIds);\n```\n\n----------------------------------------\n\nTITLE: Applying the Probabilistic Hough Line Transform in OpenCV (C++)\nDESCRIPTION: This C++ code applies HoughLinesP to detect line segments using the probabilistic version of Hough Transform. The function returns start and end points for each line. Inputs include the edge image, rho/theta resolutions, threshold, minLineLength, and maxLineGap. Suitable for line segments extraction.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nvector<Vec4i> linesP;\\nHoughLinesP(edges, linesP, 1, CV_PI/180, 50, 50, 10);\\n\n```\n\n----------------------------------------\n\nTITLE: Computing Bounding Rotated Boxes and Ellipses in Contours - OpenCV C++\nDESCRIPTION: This C++ snippet leverages OpenCV functions to find contours in an image and compute both the minimum area rotated rectangle and the best-fit ellipse for each contour where applicable. Dependencies include OpenCV (C++ bindings), and the code typically expects a binary or grayscale image as input. The key parameters are the input image and parameters for contour detection; the output is a visualization or coordinates of bounding rotated boxes and ellipses. This version uses cv::minAreaRect and cv::fitEllipse and is compatible with OpenCV 3.0 and above.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rotated_ellipses/bounding_rotated_ellipses.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\\nusing namespace cv;\\nusing namespace std;\\n\\nint main(int argc, char** argv)\\n{\\n    // Load image\\n    Mat src = imread(\"shapes.png\", IMREAD_GRAYSCALE);\\n    threshold(src, src, 100, 255, THRESH_BINARY);\\n    vector<vector<Point>> contours;\\n    findContours(src, contours, RETR_LIST, CHAIN_APPROX_SIMPLE);\\n\\n    Mat drawing = Mat::zeros(src.size(), CV_8UC3);\\n    for (size_t i = 0; i < contours.size(); i++)\\n    {\\n        // Compute the bounding rotated rectangle for each contour\\n        RotatedRect box = minAreaRect(contours[i]);\\n        Point2f rect_points[4];\\n        box.points(rect_points);\\n        for (int j = 0; j < 4; j++)\\n            line(drawing, rect_points[j], rect_points[(j+1)%4], Scalar(0,255,0), 2);\\n        // Fit ellipse if contour has enough points\\n        if (contours[i].size() > 5)\\n        {\\n            RotatedRect ellipse_box = fitEllipse(contours[i]);\\n            ellipse(drawing, ellipse_box, Scalar(255,0,0), 2);\\n        }\\n    }\\n    imshow(\"Contours\", drawing);\\n    waitKey(0);\\n    return 0;\\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom ArUco Dictionary in C++\nDESCRIPTION: This code demonstrates how to manually create a custom ArUco dictionary with 100 markers, each 6x6 bits in size. It uses the Dictionary::getByteListFromBits() method to convert binary marker representations to the compressed format used in the dictionary.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_detection/aruco_detection.markdown#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n    cv::aruco::Dictionary dictionary;\n\n    // Markers of 6x6 bits\n    dictionary.markerSize = 6;\n\n    // Maximum number of bit corrections\n    dictionary.maxCorrectionBits = 3;\n\n    // Let's create a dictionary of 100 markers\n    for(int i = 0; i < 100; i++)\n    {\n        // Assume generateMarkerBits() generates a new marker in binary format, so that\n        // markerBits is a 6x6 matrix of CV_8UC1 type, only containing 0s and 1s\n        cv::Mat markerBits = generateMarkerBits();\n        cv::Mat markerCompressed = cv::aruco::Dictionary::getByteListFromBits(markerBits);\n\n        // Add the marker as a new row\n        dictionary.bytesList.push_back(markerCompressed);\n    }\n```\n\n----------------------------------------\n\nTITLE: Setting OpenCV Directory Environment Variable using Windows Batch\nDESCRIPTION: This series of batch commands uses the setx utility to set the OpenCV_DIR environment variable to appropriate paths based on Visual Studio version and system architecture. Users should adjust the path according to their installation or build directories. This approach makes it easier for both scripts and IDEs to locate OpenCV's files while building or running projects. The command takes effect for new processes and requires administrative rights for machine-level updates, and modifications might be needed depending on user privileges.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_install/windows_install.markdown#2025-04-22_snippet_6\n\nLANGUAGE: batch\nCODE:\n```\nsetx OpenCV_DIR D:\\OpenCV\\build\\x64\\vc14     (suggested for Visual Studio 2015 - 64 bit Windows)\nsetx OpenCV_DIR D:\\OpenCV\\build\\x86\\vc14     (suggested for Visual Studio 2015 - 32 bit Windows)\n\nsetx OpenCV_DIR D:\\OpenCV\\build\\x64\\vc15     (suggested for Visual Studio 2017 - 64 bit Windows)\nsetx OpenCV_DIR D:\\OpenCV\\build\\x86\\vc15     (suggested for Visual Studio 2017 - 32 bit Windows)\n\nsetx OpenCV_DIR D:\\OpenCV\\build\\x64\\vc16     (suggested for Visual Studio 2019 - 64 bit Windows)\nsetx OpenCV_DIR D:\\OpenCV\\build\\x86\\vc16     (suggested for Visual Studio 2019 - 32 bit Windows)\n\nsetx OpenCV_DIR D:\\OpenCV\\build\\x64\\vc17     (suggested for Visual Studio 2022 - 64 bit Windows)\nsetx OpenCV_DIR D:\\OpenCV\\build\\x86\\vc17     (suggested for Visual Studio 2022 - 32 bit Windows)\n```\n\n----------------------------------------\n\nTITLE: Converting Integer FourCC to String using a Union (C++)\nDESCRIPTION: Presents an alternative method using a C++ union to convert an integer FourCC codec identifier into its 4-character string representation. Assigning the integer (`ex`) to the union's integer member (`v`) allows accessing the underlying bytes as characters via the character array member (`c`). A null terminator is added manually to `c[4]`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_write.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nunion { int v; char c[5];} uEx ;\nuEx.v = ex;                              // From Int to char via union\nuEx.c[4]='\\0';\n```\n\n----------------------------------------\n\nTITLE: Implementing Lucas-Kanade Optical Flow Tracking in Python\nDESCRIPTION: Python implementation of tracking feature points in a video using the Lucas-Kanade optical flow method. The code detects Shi-Tomasi corner points in the first frame and then tracks them through subsequent frames using cv.calcOpticalFlowPyrLK().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/optical_flow.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nimport argparse\n\nparser = argparse.ArgumentParser(description='This sample demonstrates Lucas-Kanade Optical Flow calculation. \\n'\n                                     'The example file can be downloaded from: \\n'\n                                     'https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4')\nparser.add_argument('image', type=str, help='path to image file')\nargs = parser.parse_args()\n\ncap = cv.VideoCapture(args.image)\n\n# params for ShiTomasi corner detection\nfeature_params = dict( maxCorners = 100,\n                       qualityLevel = 0.3,\n                       minDistance = 7,\n                       blockSize = 7 )\n\n# Parameters for lucas kanade optical flow\nlk_params = dict( winSize  = (15, 15),\n                  maxLevel = 2,\n                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n\n# Create some random colors\ncolor = np.random.randint(0, 255, (100, 3))\n\n# Take first frame and find corners in it\nret, old_frame = cap.read()\nold_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\np0 = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n\n# Create a mask image for drawing purposes\nmask = np.zeros_like(old_frame)\n\nwhile(1):\n    ret, frame = cap.read()\n    if not ret:\n        print('No frames grabbed!')\n        break\n\n    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n\n    # calculate optical flow\n    p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n\n    # Select good points\n    if p1 is not None:\n        good_new = p1[st==1]\n        good_old = p0[st==1]\n\n    # draw the tracks\n    for i,(new,old) in enumerate(zip(good_new, good_old)):\n        a,b = new.ravel()\n        c,d = old.ravel()\n        mask = cv.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n        frame = cv.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n    img = cv.add(frame, mask)\n\n    cv.imshow('frame', img)\n    k = cv.waitKey(30) & 0xff\n    if k == 27:\n        break\n\n    # Now update the previous frame and previous points\n    old_gray = frame_gray.copy()\n    p0 = good_new.reshape(-1, 1, 2)\n\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Converting Images to HSV Format for Histogram Comparison\nDESCRIPTION: Converting RGB images to HSV color space, which is more suitable for histogram-based image analysis and comparison.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_comparison/histogram_comparison.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nMat hsv_base;\nMat hsv_test1;\nMat hsv_test2;\ncvtColor( src_base, hsv_base, COLOR_BGR2HSV );\ncvtColor( src_test1, hsv_test1, COLOR_BGR2HSV );\ncvtColor( src_test2, hsv_test2, COLOR_BGR2HSV );\n```\n\nLANGUAGE: java\nCODE:\n```\nMat hsv_base = new Mat();\nMat hsv_test1 = new Mat();\nMat hsv_test2 = new Mat();\nImgproc.cvtColor( src_base, hsv_base, Imgproc.COLOR_BGR2HSV );\nImgproc.cvtColor( src_test1, hsv_test1, Imgproc.COLOR_BGR2HSV );\nImgproc.cvtColor( src_test2, hsv_test2, Imgproc.COLOR_BGR2HSV );\n```\n\nLANGUAGE: python\nCODE:\n```\nhsv_base = cv.cvtColor(src_base, cv.COLOR_BGR2HSV)\nhsv_test1 = cv.cvtColor(src_test1, cv.COLOR_BGR2HSV)\nhsv_test2 = cv.cvtColor(src_test2, cv.COLOR_BGR2HSV)\n```\n\n----------------------------------------\n\nTITLE: Undistorting Images using Calculated Calibration Parameters with OpenCV C++\nDESCRIPTION: This C++ snippet reference points to code demonstrating how to apply the calculated camera matrix and distortion coefficients to correct image distortion. It uses the `cv::undistort` function to remap the input image pixels, producing an undistorted output image based on the calibration results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/calib3d/camera_calibration/camera_calibration.cpp output_undistorted\n```\n\n----------------------------------------\n\nTITLE: Loading an Image from File with OpenCV in Java\nDESCRIPTION: Illustrates loading an image file using OpenCV in Java via Imgcodecs.imread. Requires OpenCV native libraries available to the JVM. The loaded Mat contains BGR data (3 channels) for standard color images. Accepts a file path as input and returns a Mat object. File format support depends on the OpenCV distribution.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.Mat;\\nimport org.opencv.imgcodecs.Imgcodecs;\\nMat img = Imgcodecs.imread(\"my_image.jpg\");\n```\n\n----------------------------------------\n\nTITLE: Image Sharpening with Laplacian Filter\nDESCRIPTION: Applies a laplacian filter to sharpen the image and accentuate edges of foreground objects\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/distance_transformation/distance_transform.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nMat kernel = (Mat_<float>(3,3) <<\n        1,  1, 1,\n        1, -8, 1,\n        1,  1, 1);\nMat imgLaplacian;\nfilter2D(src, imgLaplacian, CV_32F, kernel);\nMat sharp;\nsrc.convertTo(sharp, CV_32F);\nMat imgResult = sharp - imgLaplacian;\n```\n\nLANGUAGE: Python\nCODE:\n```\nkernel = np.array([[1, 1, 1],\n                   [1, -8, 1],\n                   [1, 1, 1]], dtype=np.float32)\nimg_laplacian = cv.filter2D(src, cv.CV_32F, kernel)\nsharp = np.float32(src)\nimg_result = sharp - img_laplacian\n```\n\n----------------------------------------\n\nTITLE: Building WebNN Module with OpenCV\nDESCRIPTION: This code snippet demonstrates how to add the WebNN backend to the OpenCV build process by passing the -DWITH_WEBNN=ON option to the cmake command, as per the Linux installation tutorial. It is important to run this in the OpenCV directory root and have CMake available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/src/webnn/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncmake -DWITH_WEBNN=ON ../opencv\n```\n\n----------------------------------------\n\nTITLE: Creating and Copying OpenCV Mat Headers in C++\nDESCRIPTION: Demonstrates basic Mat object creation, assignment using `imread`, and copy construction. Highlights that the copy constructor (`Mat B(A);`) and assignment operator (`C = A;`) only copy the header and share the underlying matrix data with the original (`A`) due to OpenCV's reference counting system. Requires an image path via `argv[1]`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nMat A, C;                          // creates just the header parts\nA = imread(argv[1], IMREAD_COLOR); // here we'll know the method used (allocate matrix)\n\nMat B(A);                                 // Use the copy constructor\n\nC = A;                                    // Assignment operator\n```\n\n----------------------------------------\n\nTITLE: Detecting Keypoints and Computing Descriptors with SURF in OpenCV C++\nDESCRIPTION: Uses the SURF algorithm to detect keypoints and compute descriptors for an input image. This process is repeated for both input images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/detection_of_planar_objects/detection_of_planar_objects.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n// detecting keypoints\nPtr<Feature2D> surf = SURF::create();\nvector<KeyPoint> keypoints1;\nMat descriptors1;\nsurf->detectAndCompute(img1, Mat(), keypoints1, descriptors1);\n\n... // do the same for the second image\n```\n\n----------------------------------------\n\nTITLE: Extracting Rotational Component Using OpenCV in Java\nDESCRIPTION: This snippet demonstrates extracting the rotational component from two images captured by a rotating camera using the OpenCV library in Java. It requires the OpenCV library with Java bindings. Inputs involve image views, and the output is the rotational component.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_30\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.Core;\nimport org.opencv.core.Mat;\n\npublic void extractRotation(Mat image1, Mat image2) {\n    // Code to extract rotation component\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Finding Contour Orientation and Axis Lengths in OpenCV Python\nDESCRIPTION: This snippet determines the orientation (angle) of the object represented by the contour, along with the lengths of its major (MA) and minor (ma) axes. It achieves this by fitting an ellipse to the contour points using `cv.fitEllipse`. Requires an existing contour variable `cnt`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_properties/py_contour_properties.markdown#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n(x,y),(MA,ma),angle = cv.fitEllipse(cnt)\n```\n\n----------------------------------------\n\nTITLE: Exposure Fusion using Mertens Algorithm in OpenCV Python\nDESCRIPTION: Demonstrates exposure fusion using Mertens algorithm which results in an image within [0..1] range without requiring exposure times.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_photo/py_hdr/py_hdr.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Exposure fusion using Mertens\nmerge_mertens = cv.createMergeMertens()\nres_mertens = merge_mertens.process(img_list)\n```\n\n----------------------------------------\n\nTITLE: Classifying Multiple Data Points with kNN in OpenCV\nDESCRIPTION: This code snippet shows how to classify multiple new data points (newcomers) at once using the trained kNN model. It creates 10 random test points and performs classification on all of them simultaneously, returning arrays of results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_knn/py_knn_understanding/py_knn_understanding.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# 10 new-comers\nnewcomers = np.random.randint(0,100,(10,2)).astype(np.float32)\nret, results,neighbours,dist = knn.findNearest(newcomer, 3)\n# The results also will contain 10 labels.\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV 2.4 Build for Ubuntu Linux using CMake (Shell)\nDESCRIPTION: Runs CMake to configure the OpenCV 2.4 build for desktop Ubuntu Linux (14.04/16.04 LTS). It sets build type to Release, enables CUDA 8.0 support for multiple architectures (3.0, 3.5, 5.0, 6.0, 6.2), enables Python 2 bindings (`BUILD_opencv_python`), TBB, and FFMPEG, while disabling several other optional dependencies (PNG, TIFF, Jasper, ZLIB, Java, nonfree modules, OpenCL, OpenMP, GStreamer, VTK, 1394, OpenEXR) and specifying the test data path. Assumes execution from a build directory sibling to `opencv` and `opencv_extra`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_17\n\nLANGUAGE: Shell\nCODE:\n```\n$ cmake \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=/usr \\\n    -DBUILD_PNG=OFF \\\n    -DBUILD_TIFF=OFF \\\n    -DBUILD_TBB=OFF \\\n    -DBUILD_JPEG=OFF \\\n    -DBUILD_JASPER=OFF \\\n    -DBUILD_ZLIB=OFF \\\n    -DBUILD_EXAMPLES=ON \\\n    -DBUILD_JAVA=OFF \\\n    -DBUILD_opencv_nonfree=OFF \\\n    -DBUILD_opencv_python=ON \\\n    -DWITH_OPENCL=OFF \\\n    -DWITH_OPENMP=OFF \\\n    -DWITH_FFMPEG=ON \\\n    -DWITH_GSTREAMER=OFF \\\n    -DWITH_GSTREAMER_0_10=OFF \\\n    -DWITH_CUDA=ON \\\n    -DWITH_GTK=ON \\\n    -DWITH_VTK=OFF \\\n    -DWITH_TBB=ON \\\n    -DWITH_1394=OFF \\\n    -DWITH_OPENEXR=OFF \\\n    -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-8.0 \\\n    -DCUDA_ARCH_BIN='3.0 3.5 5.0 6.0 6.2' \\\n    -DCUDA_ARCH_PTX=\"\" \\\n    -DINSTALL_C_EXAMPLES=ON \\\n    -DINSTALL_TESTS=ON \\\n    -DOPENCV_TEST_DATA_PATH=../opencv_extra/testdata \\\n    ../opencv\n```\n\n----------------------------------------\n\nTITLE: Referencing OpenCV Bayer Pattern Conversion Functions\nDESCRIPTION: This snippet lists various OpenCV color conversion functions for different Bayer pattern types. These functions convert Bayer patterns to BGR or RGB color spaces.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/imgproc/doc/colors.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ncv::COLOR_BayerRGGB2BGR, cv::COLOR_BayerGRBG2BGR, cv::COLOR_BayerBGGR2BGR, cv::COLOR_BayerGBRG2BGR, cv::COLOR_BayerRGGB2RGB, cv::COLOR_BayerGRBG2RGB, cv::COLOR_BayerBGGR2RGB, cv::COLOR_BayerGBRG2RGB\ncv::COLOR_BayerBG2BGR, cv::COLOR_BayerGB2BGR, cv::COLOR_BayerRG2BGR, cv::COLOR_BayerGR2BGR, cv::COLOR_BayerBG2RGB, cv::COLOR_BayerGB2RGB, cv::COLOR_BayerRG2RGB, cv::COLOR_BayerGR2RGB\n```\n\n----------------------------------------\n\nTITLE: Zero Padding for Optimal DFT Performance - Python\nDESCRIPTION: Demonstrates two methods for padding a 2D image array to optimal size for efficient FFT/DFT processing in Python. The first approach creates a new zeros array and copies the original image into it; the second approach uses OpenCV's cv.copyMakeBorder() to pad the image directly. Dependencies: Numpy (np) and OpenCV (cv). Key parameters: nrows/ncols (optimal sizes), rows/cols (original sizes), img (input array). Outputs a zero-padded array, suitable for DFT. Handles constant-value padding with border type specification.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nnimg = np.zeros((nrows,ncols))\nnimg[:rows,:cols] = img\n```\n\nLANGUAGE: Python\nCODE:\n```\nright = ncols - cols\nbottom = nrows - rows\nbordertype = cv.BORDER_CONSTANT #just to avoid line breakup in PDF file\nnimg = cv.copyMakeBorder(img,0,bottom,0,right,bordertype, value = 0)\n```\n\n----------------------------------------\n\nTITLE: Overlaying Non-Rectangular ROI Using Bitwise Operations in Python\nDESCRIPTION: This code snippet shows how to overlay a non-rectangular image (OpenCV logo) onto another image using bitwise operations in OpenCV. It involves creating a mask and an inverse mask, masking the ROI, and then combining images to achieve the overlay. Dependencies include the images messi5.jpg and opencv-logo-white.png.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_image_arithmetics/py_image_arithmetics.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Load two images\nimg1 = cv.imread('messi5.jpg')\nimg2 = cv.imread('opencv-logo-white.png')\nassert img1 is not None, \"file could not be read, check with os.path.exists()\"\nassert img2 is not None, \"file could not be read, check with os.path.exists()\"\n\n# I want to put logo on top-left corner, So I create a ROI\nrows,cols,channels = img2.shape\nroi = img1[0:rows, 0:cols]\n\n# Now create a mask of logo and create its inverse mask also\nimg2gray = cv.cvtColor(img2,cv.COLOR_BGR2GRAY)\nret, mask = cv.threshold(img2gray, 10, 255, cv.THRESH_BINARY)\nmask_inv = cv.bitwise_not(mask)\n\n# Now black-out the area of logo in ROI\nimg1_bg = cv.bitwise_and(roi,roi,mask = mask_inv)\n\n# Take only region of logo from logo image.\nimg2_fg = cv.bitwise_and(img2,img2,mask = mask)\n\n# Put logo in ROI and modify the main image\ndst = cv.add(img1_bg,img2_fg)\nimg1[0:rows, 0:cols ] = dst\n\ncv.imshow('res',img1)\ncv.waitKey(0)\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Converting Mat Types with OpenCV.js - JavaScript\nDESCRIPTION: Utilizes convertTo to change the data type (depth) of a cv.Mat. Supports optional scaling (alpha) and offset (beta); output is stored in another matrix. This is essential for dynamic range adjustments, normalization, and type compatibility. The function signature in code matches the C++ OpenCV convention.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_basic_ops/js_basic_ops.markdown#2025-04-22_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nsrc.convertTo(dst, rtype);\n```\n\n----------------------------------------\n\nTITLE: Estimating Pose of ChArUco Diamond Markers Using solvePnP (C++)\nDESCRIPTION: This two-part snippet covers 3D pose estimation of detected ChArUco diamond markers using cv::solvePnP() in OpenCV C++. With the four detected diamond corners, known square length, and camera calibration parameters, it calculates each diamond's rotation and translation vectors. Typically, the diamond corners are ordered consistently (clockwise starting from top-left), and the pose is visualized by drawing the frame axes using cv::aruco::drawAxis() or cv::drawFrameAxes(). Proper calibration (intrinsic/extrinsic parameters) is required for accurate results. Outputs include rvecs/tvecs for each diamond; limitations include dependency on corner detection quality and calibration accuracy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_diamond_detection/charuco_diamond_detection.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n// Pose estimation for diamond markers\nstd::vector<std::vector<cv::Point2f>> diamondCorners;\nstd::vector<cv::Vec4i> diamondIds;\ndouble squareLength;\ncv::Mat cameraMatrix, distCoeffs;\nstd::vector<cv::Vec3d> rvecs, tvecs;\n\nfor (size_t i=0; i<diamondCorners.size(); i++) {\n    std::vector<cv::Point3f> objectPoints;\n    // Define 3D corners for the square on the chessboard plane\n    objectPoints.push_back(cv::Point3f(0, 0, 0));\n    objectPoints.push_back(cv::Point3f(squareLength, 0, 0));\n    objectPoints.push_back(cv::Point3f(squareLength, squareLength, 0));\n    objectPoints.push_back(cv::Point3f(0, squareLength, 0));\n    cv::Vec3d rvec, tvec;\n    cv::solvePnP(objectPoints, diamondCorners[i], cameraMatrix, distCoeffs, rvec, tvec);\n    rvecs.push_back(rvec);\n    tvecs.push_back(tvec);\n}\n\n```\n\nLANGUAGE: C++\nCODE:\n```\n// Draw axes to show diamond pose\ndouble axisLength = squareLength * 0.5; // Or another scale\nfor (size_t i=0; i<diamondCorners.size(); i++) {\n    cv::aruco::drawAxis(image, cameraMatrix, distCoeffs, rvecs[i], tvecs[i], axisLength);\n}\n```\n\n----------------------------------------\n\nTITLE: Building the Project and Running the OpenCV Executable with Bash\nDESCRIPTION: These Bash snippets provide the essential shell commands to build the project using CMake and Make, and to run the resulting OpenCV executable. The first block changes to the build directory, invokes cmake to generate build files, and compiles the code. The second block demonstrates running the program with an image file as input. Dependencies: Bash shell, GCC, Make, CMake, and OpenCV installed. Inputs: directory containing source and CMakeLists.txt, path to input image. Outputs: builds and runs the ./DisplayImage executable. Limitation: paths and filenames must be correct.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_gcc_cmake/linux_gcc_cmake.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ncd <DisplayImage_directory>\\ncmake .\\nmake\n```\n\nLANGUAGE: Bash\nCODE:\n```\n./DisplayImage lena.jpg\n```\n\n----------------------------------------\n\nTITLE: Cloning and Copying OpenCV Mat Objects in C++\nDESCRIPTION: Demonstrates creating a new matrix by cloning or copying an existing one. Shows how to create a new header for an existing Mat object and then use clone() or copyTo() methods.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n// clone or copyTo to get a copy of the data.\nMat RowClone = C.row(1).clone();\ncout << \"RowClone = \" << endl << \" \" << RowClone << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Reading Input Images in OpenCV C++\nDESCRIPTION: Reads two grayscale input images using OpenCV's imread function. The images are expected to be provided as command-line arguments.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/detection_of_planar_objects/detection_of_planar_objects.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nMat img1 = imread(argv[1], IMREAD_GRAYSCALE);\nMat img2 = imread(argv[2], IMREAD_GRAYSCALE);\n```\n\n----------------------------------------\n\nTITLE: Generating Linearly Separable Training Data for SVM (Python)\nDESCRIPTION: Python implementation for generating linearly separable training data using numpy and OpenCV. Creates points from two different classes that could be separated by a linear boundary.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# Set up the linearly separable part of the training data\nlabels = [1, -1]\ntrainData = np.empty((2 * NTRAINING_SAMPLES, 2), dtype=np.float32)\ntrainLabels = np.empty((2 * NTRAINING_SAMPLES, 1), dtype=np.int32)\nrng = np.random.RandomState(100)\n\n# Generate training samples\nfor i in range(2):\n    n = NTRAINING_SAMPLES\n    trainData[i*n:(i+1)*n, 0:1] = rng.uniform(size=(n, 1))\n    trainData[i*n:(i+1)*n, 0:1] = trainData[i*n:(i+1)*n, 0:1] * 0.2 + (0.2 if i == 0 else 0.6)\n    trainData[i*n:(i+1)*n, 1:2] = rng.uniform(size=(n, 1))\n    trainLabels[i*n:(i+1)*n, :] = labels[i]\n```\n\n----------------------------------------\n\nTITLE: Tracking a Colored Object in Real-Time Video with OpenCV in Python\nDESCRIPTION: This comprehensive code tracks a blue object using a webcam feed. It converts each frame from BGR to HSV, thresholds for a blue HSV range, and extracts that object with a bitwise AND. Requires OpenCV (cv2) and numpy, and access to a video capture device. Key parameters: lower_blue and upper_blue define the color boundaries. Displays three real-time windows for live video, mask, and result. Will run until the ESC key (27) is pressed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2 as cv\\nimport numpy as np\\n\\ncap = cv.VideoCapture(0)\\n\\nwhile(1):\\n\\n    # Take each frame\\n    _, frame = cap.read()\\n\\n    # Convert BGR to HSV\\n    hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\\n\\n    # define range of blue color in HSV\\n    lower_blue = np.array([110,50,50])\\n    upper_blue = np.array([130,255,255])\\n\\n    # Threshold the HSV image to get only blue colors\\n    mask = cv.inRange(hsv, lower_blue, upper_blue)\\n\\n    # Bitwise-AND mask and original image\\n    res = cv.bitwise_and(frame,frame, mask= mask)\\n\\n    cv.imshow('frame',frame)\\n    cv.imshow('mask',mask)\\n    cv.imshow('res',res)\\n    k = cv.waitKey(5) & 0xFF\\n    if k == 27:\\n        break\\n\\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Basic OAK Depth Camera Integration with G-API Inference\nDESCRIPTION: This code demonstrates how to use an OAK depth camera with OpenCV's G-API for inference. It shows initialization of camera parameters, setting up streams, performing neural network inference, and visualizing results. The example uses a face detection model and processes frames in a pipeline approach.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/oak_devices/oak_devices.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <opencv2/core.hpp>\n#include <opencv2/gapi.hpp>\n#include <opencv2/highgui.hpp>\n#include <opencv2/imgproc.hpp>\n#include <opencv2/gapi/core.hpp>\n#include <opencv2/gapi/cpu/gcpukernel.hpp>\n#include <opencv2/gapi/infer.hpp>\n#include <opencv2/gapi/render.hpp>\n#include <opencv2/gapi/streaming/cap.hpp>\n#include <opencv2/gapi/streaming/onevpl.hpp>\n#include <opencv2/gapi/oak/oak.hpp>\n\n#include <string>\n#include <algorithm>\n\n// Default parameters ////////////////////////////////////////////////////\n\n// NB: default video source\nconst std::string default_oak_uri = \"oak:color\"; // Use device color camera\nconst std::string default_blob_path = \"face-detection-adas-0001.blob\";\nconst std::string default_detect_output_layer = \"detection_output_1:DetectionOutput:1\";\n\n// Inference parameters\nconst cv::Scalar background_color{0.2, 115.0 / 255, 115.0 / 255};\nconst cv::Scalar rect_color{240.0 / 255, 171.0 / 255, 96.0 / 255};\nconst cv::Scalar tl_color{0.0, 0.0, 0.0};\n\n// Font settings\nconst double tl_font_scale = 0.9;\nconst int tl_thickness = 1;\n\n// NB: Used to control window size\nconst double scale_factor = 1.0;\n\nint main(int argc, char *argv[]) {\n    // Parse args\n    cv::CommandLineParser cmd(argc, argv, cv::format(\n        \"{ h help     || } \"\n        \"{ input i    | %s | OAK camera URI for reading frames } \"\n        \"{ model m    | %s | Inference model (blob) } \"\n        \"{ layer l    | %s | Inference model output layer }\",\n        default_oak_uri.c_str(), default_blob_path.c_str(),\n        default_detect_output_layer.c_str()));\n\n    if (cmd.get<bool>(\"help\")) {\n        std::cout << \"G-API Oak with inference demo\\n\\n\";\n        cmd.printMessage();\n        return 0;\n    }\n\n    const std::string oak_uri = cmd.get<std::string>(\"input\");\n    const std::string blob_path = cmd.get<std::string>(\"model\");\n    const std::string detect_out_layer = cmd.get<std::string>(\"layer\");\n\n    // Read NN configuration\n    cv::size_sp output_size{0, 0};\n\n    // Create NN object\n    cv::gapi::mx::nn::forward::Params detect_params;\n    detect_params.num_classes = 1;\n    detect_params.is_background_transparent = false;\n    detect_params.output_layer = detect_out_layer;\n    detect_params.blob_path = blob_path;\n\n    // Create pipeline\n    cv::GArray<cv::Rect> faces;\n    cv::GFrame in;\n    cv::GMat in_y;\n    cv::GMat bgr;\n    cv::GMat render_mat; // For rendering\n    std::tie(in_y, bgr) = cv::gapi::oak::ColorCamera::outputFrame(in);\n\n    // NB: Take median value from all faces as a background intensity\n    auto avg_face_prob = cv::gapi::streaming::encodeTimestamp\n        (cv::gapi::mx::nn::forward::Detect::on(in_y, detect_params), faces);\n\n    // Rendering\n    render_mat = cv::gapi::copy(bgr);\n    for (const auto& face : faces) {\n        cv::gapi::wip::draw::Rectangle(render_mat, face, rect_color, 2);\n    }\n\n    // Assemble pipeline\n    cv::GComputation pipeline([&](){ return cv::GComputation(cv::GIn(in), cv::GOut(render_mat, faces, avg_face_prob)); });\n\n    // Set compiled params\n    auto oak_params = cv::gapi::oak::ColorCamera::params(\n        oak_uri, // device_id\n        /* frame_size */ cv::Size{800, 600},\n        /* master_fps */ 30\n    );\n\n    auto kernels = cv::gapi::combine\n        (cv::gapi::oak::kernels(),\n        cv::gapi::render::oe::kernels());\n\n    auto networks = cv::gapi::networks(cv::gapi::mx::nn::forward::loads());\n\n    auto pipeline_args = cv::compile_args\n        (cv::gapi::mx::nn::forward::op::params(),\n        cv::gapi::oak::ColorCamera::params(oak_uri),\n        kernels, networks);\n\n    // Compile pipeline\n    // FIXME: make a callable object\n    auto exec = pipeline().compileStreaming(std::move(pipeline_args));\n\n    // Setup input\n    const auto source = cv::gapi::wip::make_src<cv::gapi::oak::ColorCamera>();\n    exec.setSource(source);\n\n    // Start processing\n    exec.start();\n\n    cv::Mat render_mat_out;\n    std::vector<cv::Rect> faces_out;\n    double median_prob_out = 0.0;\n    cv::TickMeter tm;\n\n    // Video display loop\n    std::cout << \"Press ESC key to exit...\\n\";\n    tm.start();\n    while (exec.pull(cv::gout(render_mat_out, faces_out, median_prob_out))) {\n        // Show processed frame\n        cv::Mat output = render_mat_out.clone();\n\n        // Draw FPS information\n        tm.stop();\n        const double fps = 1.0 / tm.getTimeSec();\n        tm.reset();\n        tm.start();\n\n        cv::putText(output, cv::format(\"FPS: %.2f\", fps),\n                   cv::Point(output.cols - 200, 40), cv::FONT_HERSHEY_SIMPLEX, 1.0,\n                   cv::Scalar(0, 0, 255), 2);\n\n        // Show inference output\n        cv::imshow(\"OAK Pipeline\", output);\n        const int key = cv::waitKey(1);\n        if (key == 27) break; // ESC\n    }\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Estimating Homography using OpenCV's findHomography - Python\nDESCRIPTION: This Python snippet invokes cv2.findHomography to calculate the perspective homography matrix from matching corner points. Requires lists of detected correspondences in two images. Produces a 3x3 matrix for warping; may use RANSAC for robustness. Used prior to geometric transformation operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/features2D/Homography/perspective_correction.py estimate-homography\n```\n\n----------------------------------------\n\nTITLE: Defining and Linking Targets for Each GPU Sample in CMake\nDESCRIPTION: Iterates through each filename stored in the `all_samples` variable. Inside the loop, it defines an executable target (`tgt`) for the sample using `ocv_define_sample`, specifying its source file (`sample_filename`) and category ('gpu'). It then links the target against the core required libraries (`OPENCV_LINKER_LIBS`, `OPENCV_CUDA_SAMPLES_REQUIRED_DEPS`) and conditionally links against `opencv_xfeatures2d` and `opencv_cudacodec` if they are available, using `ocv_target_link_libraries`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(sample_filename ${all_samples})\n  ocv_define_sample(tgt ${sample_filename} gpu)\n  ocv_target_link_libraries(${tgt} PRIVATE ${OPENCV_LINKER_LIBS} ${OPENCV_CUDA_SAMPLES_REQUIRED_DEPS})\n  if(HAVE_opencv_xfeatures2d)\n    ocv_target_link_libraries(${tgt} PRIVATE opencv_xfeatures2d)\n  endif()\n  if(HAVE_opencv_cudacodec)\n    ocv_target_link_libraries(${tgt} PRIVATE opencv_cudacodec)\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Performing Template Matching Operation (Python)\nDESCRIPTION: Calls the `cv2.matchTemplate` function to compare the template against the source image using the specified method. It handles methods that support masking by passing the mask image if available, otherwise performs matching without a mask. The result (correlation map) is stored.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/match_template/match_template.py match_template\n```\n\n----------------------------------------\n\nTITLE: Creating 2D Histogram using NumPy histogram2d function\nDESCRIPTION: This snippet shows how to create a 2D histogram using NumPy's histogram2d function. It loads an image, converts it to HSV, and then calculates a histogram of Hue and Saturation values with 180 and 256 bins respectively.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_2d_histogram/py_2d_histogram.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('home.jpg')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nhsv = cv.cvtColor(img,cv.COLOR_BGR2HSV)\n\nhist, xbins, ybins = np.histogram2d(h.ravel(),s.ravel(),[180,256],[[0,180],[0,256]])\n```\n\n----------------------------------------\n\nTITLE: Creating Morphological Kernel in C++\nDESCRIPTION: Creates a structuring element (kernel) for morphological operations with specified shape and size\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nMat element = getStructuringElement(morph_type,\n                                   Size(2 * erosion_size + 1, 2 * erosion_size + 1),\n                                   Point(erosion_size, erosion_size));\n```\n\n----------------------------------------\n\nTITLE: Executing Kalman Filter Prediction and Correction Steps in C++ (OpenCV)\nDESCRIPTION: Defines the `updateKalmanFilter` C++ function using OpenCV's `cv::KalmanFilter`. It takes the Kalman Filter object (`KF`), the latest measurement vector (`measurement`), and output matrices for estimated translation (`translation_estimated`) and rotation (`rotation_estimated`). It performs the two core Kalman Filter steps: prediction (`KF.predict()`) to update the internal state prediction, and correction (`KF.correct(measurement)`) using the measurement to refine the state estimate. From the corrected state vector (`estimated`), it extracts the translation components (indices 0-2) and Euler angles (indices 9-11), converts the Euler angles back to a rotation matrix using `euler2rot`, and populates the output matrices.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_25\n\nLANGUAGE: cpp\nCODE:\n```\nvoid updateKalmanFilter( cv::KalmanFilter &KF, cv::Mat &measurement,\n                     cv::Mat &translation_estimated, cv::Mat &rotation_estimated )\n{\n\n    // First predict, to update the internal statePre variable\n    cv::Mat prediction = KF.predict();\n\n    // The \"correct\" phase that is going to use the predicted value and our measurement\n    cv::Mat estimated = KF.correct(measurement);\n\n    // Estimated translation\n    translation_estimated.at<double>(0) = estimated.at<double>(0);\n    translation_estimated.at<double>(1) = estimated.at<double>(1);\n    translation_estimated.at<double>(2) = estimated.at<double>(2);\n\n    // Estimated euler angles\n    cv::Mat eulers_estimated(3, 1, CV_64F);\n    eulers_estimated.at<double>(0) = estimated.at<double>(9);\n    eulers_estimated.at<double>(1) = estimated.at<double>(10);\n    eulers_estimated.at<double>(2) = estimated.at<double>(11);\n\n    // Convert estimated quaternion to rotation matrix\n    rotation_estimated = euler2rot(eulers_estimated);\n\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing a Mat from std::vector of Points with OpenCV in C++\nDESCRIPTION: Demonstrates conversion of a std::vector<cv::Point2f> or std::vector<cv::Point3f> to a cv::Mat, useful for calib3d functions. The resulting matrix type matches either 32FC2 or 32FC3. Input is vector of points. Resulting Mat shares data with vector, so vector lifetime must exceed Mat's.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\nstd::vector<cv::Point2f> points; // assume filled\\ncv::Mat pointsMat(points);\n```\n\n----------------------------------------\n\nTITLE: Computing Plane Distance to Camera Frame in OpenCV - C++\nDESCRIPTION: This snippet calculates the signed distance from the camera to the target plane, needed for proper homography scaling. It uses the dot product of the normal vector with a point on the plane or extracts from plane equation coefficients. Prereqs: plane normal, known 3D point in camera frame.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet homography_from_camera_displacement.cpp compute-plane-distance-to-the-camera-frame-1\n```\n\n----------------------------------------\n\nTITLE: Creating HDR Image from Multiple Exposures in OpenCV\nDESCRIPTION: This code creates an HDR image by merging multiple exposures using Debevec's weighting scheme and the camera response function calculated in the previous step.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/hdr_imaging.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nMat hdr;\nPtr<MergeDebevec> merge_debevec = createMergeDebevec();\nmerge_debevec->process(images, hdr, times, response);\n```\n\nLANGUAGE: java\nCODE:\n```\nMat hdr = new Mat();\nMergeDebevec mergeDebevec = Photo.createMergeDebevec();\nmergeDebevec.process(images, hdr, times, response);\n```\n\nLANGUAGE: python\nCODE:\n```\nmerge_debevec = cv.createMergeDebevec()\nhdr = merge_debevec.process(images, times, response)\n```\n\n----------------------------------------\n\nTITLE: Reducing Noise with Gaussian Blur in C++\nDESCRIPTION: Applies a 3x3 Gaussian blur to the source image using cv::GaussianBlur to reduce noise before applying the Laplacian operator. Requires OpenCV imgproc module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n//! [reduce_noise]\n// Reduce noise by blurring with a Gaussian filter ( kernel size = 3 )\ngaussianBlur( src, src, Size(3, 3), 0, 0, BORDER_DEFAULT );\n//! [reduce_noise]\n```\n\n----------------------------------------\n\nTITLE: Using Simple Thresholding in OpenCV - JavaScript\nDESCRIPTION: Implements simple thresholding where each pixel value is compared against a given threshold. If the pixel value is greater than the threshold, it is set to a specified max value; otherwise, it is set to zero. Key dependencies are the OpenCV library for JavaScript. Inputs include source and destination arrays, a threshold value, a maximum value, and the thresholding type. Outputs a binary image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_thresholding/js_thresholding.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.threshold(src, dst, thresh, maxval, type);\n```\n\n----------------------------------------\n\nTITLE: Detect Edges with Canny in OpenCV C++\nDESCRIPTION: Uses OpenCV's Canny function in C++ to detect edges in an image. Requires input image and a corresponding output edge map.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat edges;\ncv::Canny(gray, edges, lowThreshold, highThreshold);\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Sequences (Vectors/Arrays) - OpenCV C++\nDESCRIPTION: This C++ snippet demonstrates how to serialize and deserialize std::vector or similar sequence types into XML/YAML/JSON using OpenCV's FileStorage. Sequences are written using the << operator with special delimiters ([ ... ]). Reading uses FileNode and FileNodeIterator to iterate through the sequence. Requires knowledge of the vector's type; only handles types supported by OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n// Writing\\nfs << \"sequence\" << \"[\" << 1 << 2 << 3 << \"]\";\\n\\n// Reading\\nFileNode n = fs[\"sequence\"];\\nfor (FileNodeIterator it = n.begin(); it != n.end(); ++it) {\\n    int value = (int)*it;\\n    // process value\\n}\n```\n\n----------------------------------------\n\nTITLE: Create and Display Windows with OpenCV Python\nDESCRIPTION: Creates a window to display an image using OpenCV in Python. Requires OpenCV module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ncv2.namedWindow(\"Source\", cv2.WINDOW_AUTOSIZE)\ncv2.imshow(\"Source\", src)\n```\n\n----------------------------------------\n\nTITLE: Projecting Object Bounding Box in C++\nDESCRIPTION: This code applies the estimated homography transformation to project the object's bounding box from the first frame onto the current frame.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_tracking/akaze_tracking.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nperspectiveTransform(object_bb, new_bb, homography);\n```\n\n----------------------------------------\n\nTITLE: Cloning Halide Git Repository\nDESCRIPTION: This snippet shows how to clone the Halide repository from GitHub using git. It is used to obtain the Halide source code necessary for building the Halide library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_halide/dnn_halide.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Git\nCODE:\n```\ngit clone https://github.com/halide/Halide.git\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV.js Build with Documentation Generation in Docker - Bash\nDESCRIPTION: This command starts a Docker container using the previously built \"opencv-js-doc\" image, mounting the source directory and invoking the OpenCV.js build script with the '--build_doc' flag, which generates documentation alongside the build. Requires Docker, the custom image, and appropriate user permissions. Outputs documentation as well as the OpenCV.js build artifacts.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd):/src -u $(id -u):$(id -g) \"opencv-js-doc\" emcmake python3 ./platforms/js/build_js.py build_js --build_doc\n```\n\n----------------------------------------\n\nTITLE: Applying the Standard Hough Line Transform in OpenCV (Java)\nDESCRIPTION: This code applies the OpenCV HoughLines function in Java to detect lines in an image. It takes a binary edge map and stores detected lines in a Mat. Key parameters are rho resolution, theta resolution, and threshold. Requires prior edge detection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nMat lines = new Mat();\\nImgproc.HoughLines(edges, lines, 1, Math.PI/180, 150, 0, 0);\\n\n```\n\n----------------------------------------\n\nTITLE: K-Means Distance Formula in LaTeX\nDESCRIPTION: Mathematical formula showing the optimization objective of K-Means clustering, which minimizes the sum of distances between points and their corresponding centroids.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_kmeans/py_kmeans_understanding/py_kmeans_understanding.markdown#2025-04-22_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\nminimize \\;\\bigg[J = \\sum_{All\\: Red\\_Points}distance(C1,Red\\_Point) + \\sum_{All\\: Blue\\_Points}distance(C2,Blue\\_Point)\\bigg]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV.js Loader Paths and Usage - JavaScript\nDESCRIPTION: Configures path options for various OpenCV.js builds (wasm, threads, simd, threadsSimd) and calls the loadOpenCV function with these settings and a main callback. Dependency: loader.js must be included and WebAssembly Feature Detection (UMD version) provided. Parameters: pathsConfig object specifying file paths and a main function callback. No direct output; OpenCV.js loads asynchronously via the loader.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n// Set paths configuration\\nlet pathsConfig = {\\n    wasm: \"../../build_wasm/opencv.js\",\\n    threads: \"../../build_mt/opencv.js\",\\n    simd: \"../../build_simd/opencv.js\",\\n    threadsSimd: \"../../build_mtSIMD/opencv.js\",\\n}\\n\\n// Load OpenCV.js and use the pathsConfiguration and main function as the params.\\nloadOpenCV(pathsConfig, main);\n```\n\n----------------------------------------\n\nTITLE: Warping Image with OpenCV warpPerspective - Python\nDESCRIPTION: With OpenCV’s cv2.warpPerspective, this Python snippet applies a computed homography to warp the source image to a new perspective. Requires cv2, homography (numpy array), and image. Returns warped output; destination size must be defined. Useful for visual perspective correction tasks.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/features2D/Homography/perspective_correction.py warp-chessboard\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV with contrib modules on Linux\nDESCRIPTION: Quick start script for building OpenCV with contrib modules on Linux. It downloads both core and contrib sources, creates a build directory, configures CMake with contrib modules, and builds the project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget -O opencv.zip https://github.com/opencv/opencv/archive/4.x.zip\nwget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.x.zip\nunzip opencv.zip\nunzip opencv_contrib.zip\nmkdir -p build && cd build\ncmake -DOPENCV_EXTRA_MODULES_PATH=../opencv_contrib-4.x/modules ../opencv-4.x\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: Edge Detection using Canny Detector in OpenCV (Java)\nDESCRIPTION: This snippet demonstrates edge detection using OpenCV's Canny function in Java. The source image is converted to grayscale before applying the detector, with specific threshold parameters. Inputs are the image and thresholds; output is a Mat representing edges. Requires OpenCV Java bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nMat src_gray = new Mat();\\nImgproc.cvtColor(src, src_gray, Imgproc.COLOR_BGR2GRAY);\\nMat edges = new Mat();\\nImgproc.Canny(src_gray, edges, 50, 200, 3, false);\\n\n```\n\n----------------------------------------\n\nTITLE: Performing Erosion with OpenCV in Python\nDESCRIPTION: This Python snippet focuses on implementing the erosion operation using OpenCV's cv::erode function. It takes a source image and a kernel, whose shape and size can be specified via cv::getStructuringElement. Required dependencies: OpenCV-Python. Inputs: source image (src), kernel (element). Outputs: eroded image. The kernel shape may be one of MORPH_RECT, MORPH_CROSS, or MORPH_ELLIPSE. The anchor point defaults to the center if not provided. This function performs basic erosion without exposing additional advanced parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.markdown#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n@snippet python/tutorial_code/imgProc/erosion_dilatation/morphology_1.py erosion\n```\n\nLANGUAGE: Python\nCODE:\n```\n@snippet python/tutorial_code/imgProc/erosion_dilatation/morphology_1.py kernel\n```\n\n----------------------------------------\n\nTITLE: CSV Formatting for OpenCV Mat Output in C++\nDESCRIPTION: Demonstrates formatting OpenCV Mat output as comma-separated values (CSV) using the format() method with FormatCSV flag.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\ncout << \"R (csv) = \" << endl << format(R, Formatter::FMT_CSV) << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Final Visualization Call in Java using OpenCV\nDESCRIPTION: Illustrates drawing the detected contour and calling `getOrientation`, which internally draws the PCA axes and center point onto the source image `src`. This occurs within the loop iterating over filtered contours.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n//! [visualization1]\n    // Draw each contour only for visualisation purposes\n    drawContours(src, contours, i, new Scalar(0, 0, 255), 2);\n    // Find the orientation of each shape\n    getOrientation(contours.get(i), src);\n//! [visualization1]\n```\n\n----------------------------------------\n\nTITLE: Applying Gaussian Blur with OpenCV in C++\nDESCRIPTION: This C++ snippet showcases the use of OpenCV's GaussianBlur() function to apply a Gaussian filter for image smoothing. It requires OpenCV and kernel size, and standard deviation parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n@snippet cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp gaussianblur\n```\n\n----------------------------------------\n\nTITLE: Implementing Dense Optical Flow in Java\nDESCRIPTION: Java implementation of dense optical flow using Farneback's algorithm (calcOpticalFlowFarneback()). The code computes the optical flow for all points in the frame and visualizes the flow field using HSV color coding where hue represents direction and value represents magnitude.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/optical_flow.markdown#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.opencv.core.Core;\nimport org.opencv.core.CvType;\nimport org.opencv.core.Mat;\nimport org.opencv.core.Scalar;\nimport org.opencv.highgui.HighGui;\nimport org.opencv.imgproc.Imgproc;\nimport org.opencv.video.Video;\nimport org.opencv.videoio.VideoCapture;\n\nclass OpticalFlowDense {\n    public void run(String[] args) {\n        String filename = args[0];\n        VideoCapture capture = new VideoCapture(filename);\n        if (!capture.isOpened()) {\n            System.out.println(\"Could not open the input video: \" + filename);\n            System.exit(0);\n        }\n\n        Mat frame1 = new Mat();\n        capture.read(frame1);\n        Mat prvs = new Mat();\n        Imgproc.cvtColor(frame1, prvs, Imgproc.COLOR_BGR2GRAY);\n\n        while (true) {\n            Mat frame2 = new Mat(), next = new Mat();\n            capture.read(frame2);\n            if (frame2.empty()) {\n                break;\n            }\n            Imgproc.cvtColor(frame2, next, Imgproc.COLOR_BGR2GRAY);\n\n            Mat flow = new Mat(prvs.size(), CvType.CV_32FC2);\n            Video.calcOpticalFlowFarneback(prvs, next, flow, 0.5, 3, 15, 3, 5, 1.2, 0);\n\n            // visualization\n            List<Mat> flowParts = new ArrayList<>(2);\n            Core.split(flow, flowParts);\n            Mat magnitude = new Mat(), angle = new Mat(), magnNorm = new Mat();\n            Core.cartToPolar(flowParts.get(0), flowParts.get(1), magnitude, angle, true);\n            Core.normalize(magnitude, magnNorm, 0.0, 1.0, Core.NORM_MINMAX);\n            // build hsv image\n            angle.convertTo(angle, CvType.CV_32F, ((1.f / 360.f) * (180.f / 255.f)));\n            Mat hsv_parts[] = { angle, Mat.ones(angle.size(), CvType.CV_32F), magnNorm };\n            Mat hsv = new Mat();\n            Core.merge(hsv_parts, hsv);\n            Mat bgr = new Mat();\n            Imgproc.cvtColor(hsv, bgr, Imgproc.COLOR_HSV2BGR);\n            bgr.convertTo(bgr, CvType.CV_8UC3, 255.0);\n\n            HighGui.imshow(\"frame2\", bgr);\n\n            int keyboard = HighGui.waitKey(30);\n            if (keyboard == 'q' || keyboard == 27) {\n                break;\n            }\n            prvs = next;\n        }\n\n        System.exit(0);\n    }\n}\n\npublic class OpticalFlowDenseDemo {\n    public static void main(String[] args) {\n        // Load the native OpenCV library\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\n\n        new OpticalFlowDense().run(args);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Animation Structure in OpenCV\nDESCRIPTION: Demonstrates how to initialize a cv::Animation structure to hold frames from an animated image file. This structure is essential for handling animated image data in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/animations.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ncv::Animation animation;\n```\n\nLANGUAGE: Python\nCODE:\n```\nanimation = {}\n```\n\n----------------------------------------\n\nTITLE: OpenCV XML Output Format Example\nDESCRIPTION: Example of the XML output format produced by OpenCV's file writing operations, showing how various data types are serialized.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_22\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\"?>\n<opencv_storage>\n<iterationNr>100</iterationNr>\n<strings>\n  image1.jpg Awesomeness baboon.jpg</strings>\n<Mapping>\n  <One>1</One>\n  <Two>2</Two></Mapping>\n<R type_id=\"opencv-matrix\">\n  <rows>3</rows>\n  <cols>3</cols>\n  <dt>u</dt>\n  <data>\n    1 0 0 0 1 0 0 0 1</data></R>\n<T type_id=\"opencv-matrix\">\n  <rows>3</rows>\n  <cols>1</cols>\n  <dt>d</dt>\n  <data>\n    0. 0. 0.</data></T>\n<MyData>\n  <A>97</A>\n  <X>3.1415926535897931e+000</X>\n  <id>mydata1234</id></MyData>\n</opencv_storage>\n```\n\n----------------------------------------\n\nTITLE: Final Visualization Call in C++ using OpenCV\nDESCRIPTION: This snippet shows the final step where the contours and PCA results (axes and center) are drawn onto the source image within the main loop after PCA calculation. It uses `drawContours` and relies on the `getOrientation` function having already modified the `src` image by calling `drawAxis`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n//! [visualization1]\n    // Draw each contour only for visualisation purposes\n    drawContours(src, contours, static_cast<int>(i), Scalar(0, 0, 255), 2);\n    // Find the orientation of each shape\n    getOrientation(contours[i], src);\n//! [visualization1]\n```\n\n----------------------------------------\n\nTITLE: Step Functor for GpuMat Iterator Traversal\nDESCRIPTION: A functor that calculates memory offsets to correctly traverse a pitched matrix in GPU memory. It takes a linear index and converts it to the appropriate memory location.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-thrust-interop/gpu_thrust_interop.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ntemplate<typename T>\nstruct step_functor : public thrust::unary_function<int, int>\n{\n    int columns;\n    int rows;\n    int step;\n\n    __host__ __device__ step_functor(int columns_, int rows_, int step_) : \n        columns(columns_), rows(rows_), step(step_) {}\n\n    __host__ __device__ int operator()(int linear_idx) const\n    {\n        int row = linear_idx / columns;\n        int column = linear_idx % columns;\n\n        return row * step + column;\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Complete Histogram Backprojection Implementation with OpenCV in Python\nDESCRIPTION: A full implementation of histogram backprojection using OpenCV's built-in calcBackProject function. This code loads target and reference images, calculates and normalizes histograms, applies backprojection, filters the result with a disc kernel, and segments the object using thresholding.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_backprojection/py_histogram_backprojection.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nroi = cv.imread('rose_red.png')\nassert roi is not None, \"file could not be read, check with os.path.exists()\"\nhsv = cv.cvtColor(roi,cv.COLOR_BGR2HSV)\n\ntarget = cv.imread('rose.png')\nassert target is not None, \"file could not be read, check with os.path.exists()\"\nhsvt = cv.cvtColor(target,cv.COLOR_BGR2HSV)\n\n# calculating object histogram\nroihist = cv.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )\n\n# normalize histogram and apply backprojection\ncv.normalize(roihist,roihist,0,255,cv.NORM_MINMAX)\ndst = cv.calcBackProject([hsvt],[0,1],roihist,[0,180,0,256],1)\n\n# Now convolute with circular disc\ndisc = cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))\ncv.filter2D(dst,-1,disc,dst)\n\n# threshold and binary AND\nret,thresh = cv.threshold(dst,50,255,0)\nthresh = cv.merge((thresh,thresh,thresh))\nres = cv.bitwise_and(target,thresh)\n\nres = np.vstack((target,thresh,res))\ncv.imwrite('res.jpg',res)\n```\n\n----------------------------------------\n\nTITLE: Implementing Unsharp Mask Operation with G-API\nDESCRIPTION: Function that creates an unsharp mask using G-API operations including Gaussian blur, subtraction and addition to enhance image details.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// Unsharp mask implementation\ninline cv::GMat unsharpMask(const cv::GMat& src) {\n    // Gaussian blur with kernel size 3x3 and sigma=0.8\n    cv::GMat blurred = cv::gapi::gaussianBlur(src, cv::Size(3,3), 0.8, 0.8);\n    // Unsharp mask formula: src + WEIGHT * (src - blurred)\n    return src + 0.7 * (src - blurred);\n}\n```\n\n----------------------------------------\n\nTITLE: Including Full Template Matching Demo Code (Python)\nDESCRIPTION: Reference to include the complete Python source code file for the OpenCV template matching demonstration. This script loads images, performs matching with `cv2.matchTemplate`, normalizes the result using `cv2.normalize`, finds the best match location with `cv2.minMaxLoc`, and displays the result using `cv2.imshow`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@include samples/python/tutorial_code/imgProc/match_template/match_template.py\n```\n\n----------------------------------------\n\nTITLE: Selecting Region of Interest (ROI) with OpenCV in C++\nDESCRIPTION: Extracts a rectangular subregion from a cv::Mat using cv::Rect in C++. The syntax returns a Mat header referencing the ROI. Input is a cv::Mat and cv::Rect(x, y, w, h). Changes to ROI affect parent Mat.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_31\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat roi = img(cv::Rect(x, y, w, h));\n```\n\n----------------------------------------\n\nTITLE: Updating Kalman Filter with Pose Measurements in C++ (OpenCV)\nDESCRIPTION: This C++ snippet demonstrates the logic for updating an OpenCV Kalman Filter (Step 5 of the algorithm) based on pose estimation results from RANSAC/PnP. It checks if the number of inliers found (`inliers_idx.rows`) meets a minimum threshold (`minInliersKalman`). If the measurement is considered good, it retrieves the translation (`translation_measured`) and rotation (`rotation_measured`) matrices from a `pnp_detection` object. It then calls `fillMeasurements` to format these into the `measurements` vector and finally calls `updateKalmanFilter` to incorporate this measurement and refine the estimated state (translation and rotation).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\n// -- Step 5: Kalman Filter\n\n// GOOD MEASUREMENT\nif( inliers_idx.rows >= minInliersKalman )\n{\n\n    // Get the measured translation\n    cv::Mat translation_measured(3, 1, CV_64F);\n    translation_measured = pnp_detection.get_t_matrix();\n\n    // Get the measured rotation\n    cv::Mat rotation_measured(3, 3, CV_64F);\n    rotation_measured = pnp_detection.get_R_matrix();\n\n    // fill the measurements vector\n    fillMeasurements(measurements, translation_measured, rotation_measured);\n\n}\n\n// Instantiate estimated translation and rotation\ncv::Mat translation_estimated(3, 1, CV_64F);\ncv::Mat rotation_estimated(3, 3, CV_64F);\n\n// update the Kalman filter with good measurements\nupdateKalmanFilter( KF, measurements,\n              translation_estimated, rotation_estimated);\n```\n\n----------------------------------------\n\nTITLE: Approximating Contours in Python with OpenCV\nDESCRIPTION: This snippet demonstrates contour approximation using the Douglas-Peucker algorithm via cv.approxPolyDP(). It reduces the number of vertices in a contour shape based on the specified epsilon value.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nepsilon = 0.1*cv.arcLength(cnt,True)\napprox = cv.approxPolyDP(cnt,epsilon,True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Shi-Tomasi Corner Detection in Python\nDESCRIPTION: Python implementation of the Shi-Tomasi corner detection algorithm using OpenCV's goodFeaturesToTrack function. This sample code shows how to detect corners in an image using Python with OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/good_features_to_track/good_features_to_track.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nThis tutorial code's is shown lines below. You can also download it from\n[here](https://github.com/opencv/opencv/tree/4.x/samples/python/tutorial_code/TrackingMotion/good_features_to_track/goodFeaturesToTrack_Demo.py)\n```\n\n----------------------------------------\n\nTITLE: Applying Frequency Domain Filter to Image using OpenCV C++\nDESCRIPTION: Filters an input image `inputImg` using a provided frequency domain filter `H` (e.g., a Wiener filter). The function computes the DFT of the input image, performs element-wise multiplication with the complex conjugate of the filter `H` in the frequency domain, computes the inverse DFT of the result, and stores the real part of the inverse transform in the `outputImg`. Requires OpenCV `dft`, `idft`, `mulSpectrums`, `Mat` operations and complex number handling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/out_of_focus_deblur_filter/out_of_focus_deblur_filter.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nvoid filter2DFreq(const Mat& inputImg, Mat& outputImg, const Mat& H)\n{\n    Mat planes[2] = { Mat_<float>(inputImg.clone()), Mat::zeros(inputImg.size(), CV_32F) };\n    Mat complexI;\n    merge(planes, 2, complexI);\n    dft(complexI, complexI, DFT_SCALE);\n\n    Mat planesH[2] = { Mat_<float>(H.clone()), Mat::zeros(H.size(), CV_32F) };\n    Mat complexH;\n    merge(planesH, 2, complexH);\n    Mat complexIH;\n    mulSpectrums(complexI, complexH, complexIH, 0);\n\n    idft(complexIH, complexIH);\n    split(complexIH, planes);\n    outputImg = planes[0];\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Corner Detectors in Java\nDESCRIPTION: Java implementation of custom Harris and Shi-Tomasi corner detectors using OpenCV functions. Includes image processing, corner detection, and result visualization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/generic_corner_detector/generic_corner_detector.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.*;\\nimport org.opencv.highgui.HighGui;\\nimport org.opencv.imgcodecs.Imgcodecs;\\nimport org.opencv.imgproc.Imgproc;\\n\\nclass CornerDetectorDemo {\\n    private Mat src = new Mat();\\n    private Mat srcGray = new Mat();\\n    private Mat myHarris_dst = new Mat();\\n    private Mat myHarris_copy = new Mat();\\n    private Mat Mc = new Mat();\\n    private Mat myShiTomasi_dst = new Mat();\\n    private Mat myShiTomasi_copy = new Mat();\\n\\n    private int myShiTomasi_qualityLevel = 50;\\n    private int myHarris_qualityLevel = 50;\\n    private int max_qualityLevel = 100;\\n\\n    private double myHarris_minVal;\\n    private double myHarris_maxVal;\\n    private double myShiTomasi_minVal;\\n    private double myShiTomasi_maxVal;\\n\\n    private Random rng = new Random(12345);\\n\\n    private String myHarris_window = \"My Harris corner detector\";\\n    private String myShiTomasi_window = \"My Shi Tomasi corner detector\";\\n\\n    public void run(String[] args) {\\n        String filename = args.length > 0 ? args[0] : \"../data/pic3.png\";\\n        src = Imgcodecs.imread(filename);\\n        if (src.empty()) {\\n            System.err.println(\"Cannot read image: \" + filename);\\n            System.exit(0);\\n        }\\n\\n        Imgproc.cvtColor(src, srcGray, Imgproc.COLOR_BGR2GRAY);\\n\\n        int blockSize = 3;\\n        int apertureSize = 3;\\n\\n        Imgproc.cornerEigenValsAndVecs(srcGray, myHarris_dst, blockSize, apertureSize, Imgproc.BORDER_DEFAULT);\\n\\n        /* calculate Mc */\\n        Mc = Mat.zeros(srcGray.size(), CvType.CV_32F);\\n\\n        for (int j = 0; j < srcGray.rows(); j++) {\\n            for (int i = 0; i < srcGray.cols(); i++) {\\n                float[] vecData = new float[(int) (myHarris_dst.total() * myHarris_dst.channels())];\\n                myHarris_dst.get(j, i, vecData);\\n                float lambda_1 = vecData[0];\\n                float lambda_2 = vecData[1];\\n                Mc.put(j, i, lambda_1 * lambda_2 - 0.04f * ((lambda_1 + lambda_2) * (lambda_1 + lambda_2)));\\n            }\\n        }\\n\\n        Core.MinMaxLocResult res = Core.minMaxLoc(Mc);\\n        myHarris_minVal = res.minVal;\\n        myHarris_maxVal = res.maxVal;\\n\\n        /* Create Window and Trackbar */\\n        HighGui.namedWindow(myHarris_window);\\n        HighGui.createTrackbar(\"Quality Level:\", myHarris_window, new int[] { myHarris_qualityLevel }, max_qualityLevel,\\n                new HighGui.TrackbarCallback() {\\n                    @Override\\n                    public void onChange(int pos, Object userData) {\\n                        myHarris_function(pos, userData);\\n                    }\\n                });\\n        myHarris_function(0, null);\\n\\n        Imgproc.cornerMinEigenVal(srcGray, myShiTomasi_dst, blockSize, apertureSize, Imgproc.BORDER_DEFAULT);\\n\\n        res = Core.minMaxLoc(myShiTomasi_dst);\\n        myShiTomasi_minVal = res.minVal;\\n        myShiTomasi_maxVal = res.maxVal;\\n\\n        /* Create Window and Trackbar */\\n        HighGui.namedWindow(myShiTomasi_window);\\n        HighGui.createTrackbar(\"Quality Level:\", myShiTomasi_window, new int[] { myShiTomasi_qualityLevel },\\n                max_qualityLevel, new HighGui.TrackbarCallback() {\\n                    @Override\\n                    public void onChange(int pos, Object userData) {\\n                        myShiTomasi_function(pos, userData);\\n                    }\\n                });\\n        myShiTomasi_function(0, null);\\n\\n        HighGui.waitKey();\\n        System.exit(0);\\n    }\\n\\n    private void myShiTomasi_function(int qualityLevel, Object userData) {\\n        myShiTomasi_copy = src.clone();\\n        myShiTomasi_qualityLevel = Math.max(qualityLevel, 1);\\n\\n        for (int j = 0; j < srcGray.rows(); j++) {\\n            for (int i = 0; i < srcGray.cols(); i++) {\\n                float[] pixelData = new float[1];\\n                myShiTomasi_dst.get(j, i, pixelData);\\n                if (pixelData[0] > myShiTomasi_minVal + (myShiTomasi_maxVal - myShiTomasi_minVal) * myShiTomasi_qualityLevel\\n                        / max_qualityLevel) {\\n                    Imgproc.circle(myShiTomasi_copy, new Point(i, j), 4,\\n                            new Scalar(rng.nextInt(256), rng.nextInt(256), rng.nextInt(256)), Imgproc.FILLED);\\n                }\\n            }\\n        }\\n        HighGui.imshow(myShiTomasi_window, myShiTomasi_copy);\\n    }\\n\\n    private void myHarris_function(int qualityLevel, Object userData) {\\n        myHarris_copy = src.clone();\\n        myHarris_qualityLevel = Math.max(qualityLevel, 1);\\n\\n        for (int j = 0; j < srcGray.rows(); j++) {\\n            for (int i = 0; i < srcGray.cols(); i++) {\\n                float[] pixelData = new float[1];\\n                Mc.get(j, i, pixelData);\\n                if (pixelData[0] > myHarris_minVal\\n                        + (myHarris_maxVal - myHarris_minVal) * myHarris_qualityLevel / max_qualityLevel) {\\n                    Imgproc.circle(myHarris_copy, new Point(i, j), 4,\\n                            new Scalar(rng.nextInt(256), rng.nextInt(256), rng.nextInt(256)), Imgproc.FILLED);\\n                }\\n            }\\n        }\\n        HighGui.imshow(myHarris_window, myHarris_copy);\\n    }\\n}\\n\\npublic class CornerDetectorDemoRun {\\n    public static void main(String[] args) {\\n        // Load the native OpenCV library\\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\\n        new CornerDetectorDemo().run(args);\\n    }\\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Class Labels in labelmap.prototxt (Protobuf)\nDESCRIPTION: Defines the content for the `labelmap.prototxt` file used by Caffe. This file maps human-readable class names ('background', 'face') to numerical labels (0, 1) used internally during model training and inference. It's a required component for SSD training data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/face_detector/how_to_train_face_detector.txt#2025-04-22_snippet_1\n\nLANGUAGE: protobuf\nCODE:\n```\nitem {\n  name: \"none_of_the_above\"\n  label: 0\n  display_name: \"background\"\n}\nitem {\n  name: \"face\"\n  label: 1\n  display_name: \"face\"\n}\n```\n\n----------------------------------------\n\nTITLE: Draw Contours on Mat in OpenCV Python\nDESCRIPTION: Initializes a Mat canvas to draw contour shapes, using OpenCV in Python to randomize colors for contours, bounding boxes, and circles. Requires OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport random\ndrawing = np.zeros((edges.shape[0], edges.shape[1], 3), dtype=np.uint8)\nfor i in range(len(contours)):\n    color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n    cv2.drawContours(drawing, contours, i, color)\n    cv2.rectangle(drawing, bound_rect[:2], (bound_rect[0]+bound_rect[2], bound_rect[1]+bound_rect[3]), color, 2)\n    cv2.circle(drawing, (int(center[0]), int(center[1])), int(radius), color, 2)\n```\n\n----------------------------------------\n\nTITLE: Matching Keypoints Using 2-nn Matcher in C++\nDESCRIPTION: This code performs keypoint matching between the first frame and the current frame using a 2-nearest neighbor matcher. It filters matches based on a distance ratio test.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_tracking/akaze_tracking.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nmatcher->knnMatch(first_desc, desc, matches, 2);\nfor(unsigned i = 0; i < matches.size(); i++) {\n    if(matches[i][0].distance < nn_match_ratio * matches[i][1].distance) {\n        matched1.push_back(first_kp[matches[i][0].queryIdx]);\n        matched2.push_back(      kp[matches[i][0].trainIdx]);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Shi-Tomasi Corner Detection in C++\nDESCRIPTION: C++ implementation of the Shi-Tomasi corner detection algorithm using OpenCV's goodFeaturesToTrack function. This sample code demonstrates how to detect corners in an image and visualize the results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/good_features_to_track/good_features_to_track.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nThis tutorial code's is shown lines below. You can also download it from\n[here](https://github.com/opencv/opencv/tree/4.x/samples/cpp/tutorial_code/TrackingMotion/goodFeaturesToTrack_Demo.cpp)\n```\n\n----------------------------------------\n\nTITLE: Loading an Image from File in Grayscale with OpenCV in C++\nDESCRIPTION: Demonstrates reading an image file as a single-channel grayscale using OpenCV in C++. The imread function is passed cv::IMREAD_GRAYSCALE to convert the loaded image. Requires OpenCV libraries. Input is file path, output is a grayscale cv::Mat. Useful for algorithms expecting single channel intensities.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat img_gray = cv::imread(\"my_image.jpg\", cv::IMREAD_GRAYSCALE);\n```\n\n----------------------------------------\n\nTITLE: Rotating Images in OpenCV Python\nDESCRIPTION: Demonstrates image rotation using cv.getRotationMatrix2D() and cv.warpAffine(). This example rotates the image by 90 degrees with respect to its center without scaling, using a custom transformation matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimg = cv.imread('messi5.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nrows,cols = img.shape\n\n# cols-1 and rows-1 are the coordinate limits.\nM = cv.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),90,1)\ndst = cv.warpAffine(img,M,(cols,rows))\n```\n\n----------------------------------------\n\nTITLE: Running Segmentation Test Mode\nDESCRIPTION: Command to run the model in test mode, which performs inference on a single image. The command allows specifying whether to use default preprocessing parameters or custom values for scale, mean, and standard deviation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.tf.segmentation.py_to_py_segm --test True --default_img_preprocess <True/False> --evaluate False\n```\n\n----------------------------------------\n\nTITLE: Image Subtraction with OpenCV.js\nDESCRIPTION: Shows how to subtract two images using cv.subtract() function. Both input images must have the same depth and type. When used with RGBA images, the alpha channel is also subtracted.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_image_arithmetics/js_image_arithmetics.markdown#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nlet src1 = cv.imread(\"canvasInput1\");\nlet src2 = cv.imread(\"canvasInput2\");\nlet dst = new cv.Mat();\nlet mask = new cv.Mat();\nlet dtype = -1;\ncv.subtract(src1, src2, dst, mask, dtype);\nsrc1.delete(); src2.delete(); dst.delete(); mask.delete();\n```\n\n----------------------------------------\n\nTITLE: Loading Video Input for Detection - OpenCV C++\nDESCRIPTION: Illustrates opening and reading video input using OpenCV's VideoCapture in C++. It is essential for capturing the scene required for real-time detection of the object. Assumes presence of a video file at the specified path.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\ncv::VideoCapture cap;                // instantiate VideoCapture\ncap.open(video_read_path);           // open a recorded video\n\nif(!cap.isOpened())                  // check if we succeeded\n{\n   std::cout << \"Could not open the camera device\" << std::endl;\n\n```\n\n----------------------------------------\n\nTITLE: Cropping and Saving Calibrated Image in OpenCV with Python\nDESCRIPTION: This code snippet demonstrates how to crop a calibrated image using a region of interest (ROI) and save the result. It extracts a portion of the destination image based on x, y, width, and height coordinates and writes it to a file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_calibration/py_calibration.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# crop the image\nx, y, w, h = roi\ndst = dst[y:y+h, x:x+w]\ncv.imwrite('calibresult.png', dst)\n```\n\n----------------------------------------\n\nTITLE: Implementing Perspective Transformation in OpenCV Python\nDESCRIPTION: Demonstrates perspective transformation using cv.getPerspectiveTransform() and cv.warpPerspective(). This transformation allows rectifying an image by mapping it from one plane to another, requiring four source and destination point pairs.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.markdown#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimg = cv.imread('sudoku.png')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nrows,cols,ch = img.shape\n\npts1 = np.float32([[56,65],[368,52],[28,387],[389,390]])\npts2 = np.float32([[0,0],[300,0],[0,300],[300,300]])\n\nM = cv.getPerspectiveTransform(pts1,pts2)\n\ndst = cv.warpPerspective(img,M,(300,300))\n\nplt.subplot(121),plt.imshow(img),plt.title('Input')\nplt.subplot(122),plt.imshow(dst),plt.title('Output')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Constructing RotatedRect Structures in OpenCV.js (JavaScript)\nDESCRIPTION: Demonstrates the construction of a RotatedRect structure using cv.RotatedRect and an object literal with center, size, and angle properties. RotatedRect represents rectangles with arbitrary orientation. Requires OpenCV.js and valid input values for center, dimensions, and angle.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_some_data_structures/js_image_arithmetics.markdown#2025-04-22_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n// The first way\nlet rotatedRect = new cv.RotatedRect(center, size, angle);\n// The second way\nlet rotatedRect = {center : center, size : size, angle : angle};\n```\n\n----------------------------------------\n\nTITLE: Generating ChArUco Board Pattern with Predefined Dictionary using Python Script (Shell)\nDESCRIPTION: Runs 'gen_pattern.py' to create a ChArUco board pattern ('charuco_board') saved as 'charuco_board.svg'. Specifies 7 rows, 5 columns, a square size of 30mm, and an ArUco marker size of 15mm. It uses a predefined ArUco dictionary ('DICT_5X5_100') provided in a compressed JSON file. Requires Python, the 'gen_pattern.py' script, and the specified dictionary file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_pattern/camera_calibration_pattern.markdown#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython gen_pattern.py -o charuco_board.svg --rows 7 --columns 5 -T charuco_board --square_size 30 --marker_size 15 -f DICT_5X5_100.json.gz\n```\n\n----------------------------------------\n\nTITLE: Loading and Predicting with TensorFlow and OpenCV Models\nDESCRIPTION: This Python code block demonstrates reading TensorFlow and optimized OpenCV model graphs, preparing input data, and performing inference. Dependencies include the TensorFlow and OpenCV libraries. The outputs are segmentation masks and visualization using OpenCV's imshow function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# get TF model graph from the obtained frozen graph\ndeeplab_graph = read_deeplab_frozen_graph(deeplab_frozen_graph_path)\n\n# read DeepLab frozen graph with OpenCV API\nopencv_net = cv2.dnn.readNetFromTensorflow(opt_deeplab_frozen_graph_path)\nprint(\"OpenCV model was successfully read. Model layers: \\n\", opencv_net.getLayerNames())\n\n# get processed image\noriginal_img_shape, tf_input_blob, opencv_input_img = get_processed_imgs(\"test_data/sem_segm/2007_000033.jpg\")\n\n# obtain OpenCV DNN predictions\nopencv_prediction = get_opencv_dnn_prediction(opencv_net, opencv_input_img)\n\n# obtain TF model predictions\ntf_prediction = get_tf_dnn_prediction(deeplab_graph, tf_input_blob)\n\n# get PASCAL VOC classes and colors\npascal_voc_classes, pascal_voc_colors = read_colors_info(\"test_data/sem_segm/pascal-classes.txt\")\n\n# obtain colored segmentation masks\nopencv_colored_mask = get_colored_mask(original_img_shape, opencv_prediction, pascal_voc_colors)\ntf_colored_mask = get_tf_colored_mask(original_img_shape, tf_prediction, pascal_voc_colors)\n\n# obtain palette of PASCAL VOC colors\ncolor_legend = get_legend(pascal_voc_classes, pascal_voc_colors)\n\ncv2.imshow('TensorFlow Colored Mask', tf_colored_mask)\ncv2.imshow('OpenCV DNN Colored Mask', opencv_colored_mask)\n\ncv2.imshow('Color Legend', color_legend)\n```\n\n----------------------------------------\n\nTITLE: Extracting Face Features for Recognition - OpenCV DNN Python\nDESCRIPTION: This snippet shows how to use FaceRecognizerSF in Python to extract deep features from a detected and aligned face. The feature function takes the input image and detected face bounding box. The output is a floating-point feature array suitable for face comparison or matching.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_face/dnn_face.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# Extract features for recognition\nface_feature = recognizer.feature(image, face_box)\n# 'face_feature' is a numpy array describing the face embedding\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenCV Logistic Regression Parameters in C++\nDESCRIPTION: This C++ snippet demonstrates how to create an instance of the `cv::ml::LogisticRegression` classifier and configure its training parameters. It sets the learning rate, number of iterations, regularization type (L2), training method (Mini-Batch), and mini-batch size using the respective setter methods provided by the class. This setup prepares the classifier for training.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/ml/doc/ml_intro.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// A sample set of training parameters for the Logistic Regression classifier can be initialized as follows:\n//! [init]\nPtr<LogisticRegression> lr = LogisticRegression::create();\nlr->setLearningRate(0.001);\nlr->setIterations(10);\nlr->setRegularization(LogisticRegression::REG_L2);\nlr->setTrainMethod(LogisticRegression::MINI_BATCH);\nlr->setMiniBatchSize(1);\n//! [init]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Contours With RETR_LIST in OpenCV Python\nDESCRIPTION: This snippet demonstrates how to use OpenCV's RETR_LIST retrieval mode for extracting contours without creating a hierarchy. It treats all contours as being on the same level, indicated by -1 in the hierarchy structure for parent and child fields.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contours_hierarchy/py_contours_hierarchy.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> hierarchy\narray([[[ 1, -1, -1, -1],\n        [ 2,  0, -1, -1],\n        [ 3,  1, -1, -1],\n        [ 4,  2, -1, -1],\n        [ 5,  3, -1, -1],\n        [ 6,  4, -1, -1],\n        [ 7,  5, -1, -1],\n        [-1,  6, -1, -1]]])\n```\n\n----------------------------------------\n\nTITLE: Verifying Pkg-config for AArch64 (Bash)\nDESCRIPTION: Tests the `pkg-config` setup for the `aarch64` architecture. It sets environment variables (`PKG_CONFIG_PATH`, `PKG_CONFIG_LIBDIR`, `PKG_CONFIG_SYSROOT_DIR`) to point `pkg-config` to the correct directories for `aarch64` libraries and then lists all discoverable packages for that architecture.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nPKG_CONFIG_PATH=/usr/lib/aarch64-linux-gnu/pkgconfig:/usr/share/pkgconfig \\\n    PKG_CONFIG_LIBDIR=/usr/lib/aarch64-linux-gnu \\\n    PKG_CONFIG_SYSROOT_DIR=/ \\\n      pkg-config --list-all\n```\n\n----------------------------------------\n\nTITLE: Brute-Force Matching with OpenCV in C++\nDESCRIPTION: The code uses a brute-force matcher with Hamming distance in C++ to find matches between the computed descriptors of the images using OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nsamples/cpp/tutorial_code/features2D/AKAZE_match.cpp 2-nn matching\n```\n\n----------------------------------------\n\nTITLE: Converting PaddleSeg Model to ONNX Format (Shell)\nDESCRIPTION: Uses the `paddle2onnx` command-line tool to convert the downloaded PaddleSeg inference model files (`model.pdmodel`, `model.pdiparams`) located in the `humanseg_hrnet18_small_v1` directory into an ONNX file (`humanseg_hrnet18_tiny.onnx`). It specifies ONNX opset version 11 for compatibility. Note that the model must have a fixed input shape.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npaddle2onnx --model_dir humanseg_hrnet18_small_v1 \\\n            --model_filename model.pdmodel \\\n            --params_filename model.pdiparams \\\n            --opset_version 11 \\\n            --save_file humanseg_hrnet18_tiny.onnx\n```\n\n----------------------------------------\n\nTITLE: Executing PyTorch-OpenCV Segmentation Pipeline in Python\nDESCRIPTION: This Python script demonstrates the end-to-end process of using a PyTorch FCN ResNet-50 segmentation model. It initializes the PyTorch model, converts it to ONNX format (via `get_pytorch_onnx_model`), reads the ONNX model using OpenCV DNN (`cv2.dnn.readNetFromONNX`), preprocesses an input image (`get_processed_imgs`), performs inference using both the original PyTorch model (`get_pytorch_dnn_prediction`) and the converted OpenCV model (`get_opencv_dnn_prediction`), generates colored segmentation masks based on PASCAL VOC classes (`read_colors_info`, `get_colored_mask`), creates a color legend (`get_legend`), and finally displays the original image, the masks, and the legend using OpenCV's `imshow`. Dependencies include `torch`, `torchvision.models`, `cv2`, and potentially custom helper functions (`get_pytorch_onnx_model`, `get_processed_imgs`, `get_opencv_dnn_prediction`, `get_pytorch_dnn_prediction`, `read_colors_info`, `get_colored_mask`, `get_legend`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# initialize PyTorch FCN ResNet-50 model\noriginal_model = models.segmentation.fcn_resnet50(pretrained=True)\n\n# get the path to the converted into ONNX PyTorch model\nfull_model_path = get_pytorch_onnx_model(original_model)\n\n# read converted .onnx model with OpenCV API\nopencv_net = cv2.dnn.readNetFromONNX(full_model_path)\nprint(\"OpenCV model was successfully read. Layer IDs: \\n\", opencv_net.getLayerNames())\n\n# get preprocessed image\nimg, input_img = get_processed_imgs(\"test_data/sem_segm/2007_000033.jpg\")\n\n# obtain OpenCV DNN predictions\nopencv_prediction = get_opencv_dnn_prediction(opencv_net, input_img)\n\n# obtain original PyTorch ResNet50 predictions\npytorch_prediction = get_pytorch_dnn_prediction(original_model, input_img)\n\npascal_voc_classes, pascal_voc_colors = read_colors_info(\"test_data/sem_segm/pascal-classes.txt\")\n\n# obtain colored segmentation masks\nopencv_colored_mask = get_colored_mask(img.shape, opencv_prediction, pascal_voc_colors)\npytorch_colored_mask = get_colored_mask(img.shape, pytorch_prediction, pascal_voc_colors)\n\n# obtain palette of PASCAL VOC colors\ncolor_legend = get_legend(pascal_voc_classes, pascal_voc_colors)\n\ncv2.imshow('PyTorch Colored Mask', pytorch_colored_mask)\ncv2.imshow('OpenCV DNN Colored Mask', opencv_colored_mask)\ncv2.imshow('Color Legend', color_legend)\n\ncv2.waitKey(0)\n```\n\n----------------------------------------\n\nTITLE: Loading Image in OpenCV (C++/Java/Python)\nDESCRIPTION: Loads the source image from the specified file path. The C++ and Python versions use `imread`, while the Java version uses `Imgcodecs.imread`. An error check is included in C++ and Python to ensure the image was loaded successfully.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n//![load_image]\nconst char* keys =\n{\"{ help h | | Print help message. }\"\n \"{ input | ../data/notes.png | Path to input image. }\"\n};\nCommandLineParser parser(argc, argv, keys);\nMat src = imread(samples::findFile(parser.get<String>(\"input\")), IMREAD_COLOR);\nif (src.empty())\n{\n    cout << \"Could not open or find the image!\" << endl;\n    cout << \"Usage: \" << argv[0] << \" <Input image>\" << endl;\n    return -1;\n}\n\n// Show source image\nimshow(\"src\", src);\n//![load_image]\n```\n\nLANGUAGE: java\nCODE:\n```\n//![load_image]\n// Load the image\nString filename = args.length > 0 ? args[0] : \"../data/notes.png\";\nMat src = Imgcodecs.imread(filename, Imgcodecs.IMREAD_COLOR);\n\n// Check if image is loaded fine\nif( src.empty() ) {\n    System.out.println(\"Error opening image: \" + filename);\n    System.exit(-1);\n}\n\n// Show source image\nHighGui.imshow(\"src\", src);\n//![load_image]\n```\n\nLANGUAGE: python\nCODE:\n```\n#![load_image]\n# Read the image\nparser = argparse.ArgumentParser(description='Code for Morphological Line Detection tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='notes.png')\nargs = parser.parse_args()\n\nsrc = cv.imread(cv.samples.findFile(args.input), cv.IMREAD_COLOR)\n\n# Check if image is loaded fine\nif src is None:\n    print ('Error opening image!')\n    print ('Usage: morphology_lines_detection.py [image_name -- default notes.png]')\n    sys.exit()\n\n\n# Show source image\ncv.imshow(\"src\", src)\n#![load_image]\n```\n\n----------------------------------------\n\nTITLE: Thresholding Backprojection Results in Python OpenCV\nDESCRIPTION: This snippet applies a binary threshold to the backprojection probability map to segment the object from the background. The threshold value of 50 is used to distinguish between probable object areas and non-object areas.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_backprojection/py_histogram_backprojection.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nret,thresh = cv.threshold(B,50,255,0)\n```\n\n----------------------------------------\n\nTITLE: Shape Matching using Hu-Moments in OpenCV Python\nDESCRIPTION: Demonstrates how to compare two shapes using cv.matchShapes() which uses Hu-moments for comparison. Lower return values indicate better matches, and the comparison is invariant to rotation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contours_more_functions/py_contours_more_functions.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\n\nimg1 = cv.imread('star.jpg', cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread('star2.jpg', cv.IMREAD_GRAYSCALE)\nassert img1 is not None, \"file could not be read, check with os.path.exists()\"\nassert img2 is not None, \"file could not be read, check with os.path.exists()\"\n\nret, thresh = cv.threshold(img1, 127, 255,0)\nret, thresh2 = cv.threshold(img2, 127, 255,0)\ncontours,hierarchy = cv.findContours(thresh,2,1)\ncnt1 = contours[0]\ncontours,hierarchy = cv.findContours(thresh2,2,1)\ncnt2 = contours[0]\n\nret = cv.matchShapes(cnt1,cnt2,1,0.0)\nprint( ret )\n```\n\n----------------------------------------\n\nTITLE: Creating ChArUco Board\nDESCRIPTION: Demonstrates creation of a ChArUco board with specified dimensions and marker properties\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_detection/charuco_detection.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ncv::aruco::CharucoBoard board(cv::Size(5, 7), 0.04f, 0.02f, dictionary);\n```\n\n----------------------------------------\n\nTITLE: Timing Median Filtering with OpenCV and Python Assertions\nDESCRIPTION: Demonstrates loading an image, applying repeated median filtering with increasing kernel sizes, and timing the process using OpenCV timing functions. Ensures the image exists with an assertion and prints the measured time. Dependencies include cv2 and a suitable image file. Inputs are the image file path and kernel size range (5 to 49). Outputs the elapsed time for the entire median blur operation loop. An assertion guards against missing files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_optimization/py_optimization.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimg1 = cv.imread('messi5.jpg')\nassert img1 is not None, \"file could not be read, check with os.path.exists()\"\n\ne1 = cv.getTickCount()\nfor i in range(5,49,2):\n    img1 = cv.medianBlur(img1,i)\ne2 = cv.getTickCount()\nt = (e2 - e1)/cv.getTickFrequency()\nprint( t )\n\n# Result I got is 0.521107655 seconds\n```\n\n----------------------------------------\n\nTITLE: Drawing SURF Keypoints on an Image in OpenCV Python\nDESCRIPTION: This snippet demonstrates how to visualize SURF keypoints by drawing them on the original image using OpenCV's drawKeypoints function and displaying the result with matplotlib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n>>> img2 = cv.drawKeypoints(img,kp,None,(255,0,0),4)\n\n>>> plt.imshow(img2),plt.show()\n```\n\n----------------------------------------\n\nTITLE: Generating Random Point Coordinates with OpenCV RNG in C++\nDESCRIPTION: Demonstrates using the `rng.uniform(a, b)` method of the cv::RNG object to generate random coordinates for a point (pt1). `rng.uniform` produces numbers from a uniform distribution between 'a' (inclusive) and 'b' (exclusive). Here, it generates x and y coordinates within the bounds defined by (x_1, x_2) and (y_1, y_2) respectively.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/random_generator_and_text/random_generator_and_text.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n@code{.cpp}\npt1.x = rng.uniform( x_1, x_2 );\npt1.y = rng.uniform( y_1, y_2 );\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Applying Erosion - OpenCV.js - JavaScript\nDESCRIPTION: This snippet demonstrates usage of the cv.erode() function from OpenCV.js to perform erosion on an input image using a specified structuring element. Dependencies include the OpenCV.js library, and key parameters are: src (input image), dst (output image), kernel (structuring element), anchor (anchor position, default center), iterations (number of times erosion is applied), borderType (how pixels at the edge are handled), and borderValue. The function accepts images of various types (uchar, ushort, float, etc.) and returns the eroded image. The thickness of the foreground object will decrease based on kernel size.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_morphological_ops/js_morphological_ops.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.erode(src, dst, kernel, anchor = new cv.Point(-1, -1), iterations = 1, borderType = cv.BORDER_CONSTANT, borderValue = cv.morphologyDefaultBorderValue())\n```\n\n----------------------------------------\n\nTITLE: Calculating PSNR for Image Similarity in OpenCV\nDESCRIPTION: This function calculates the Peak Signal-to-Noise Ratio (PSNR) between two images. It computes the mean squared error and converts it to a logarithmic scale. The function handles the case where images are identical (MSE=0) by returning 100, which represents perfect similarity.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_input_psnr_ssim.markdown#2025-04-22_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\ndouble getPSNR(const Mat& I1, const Mat& I2)\n{\n    Mat s1;\n    absdiff(I1, I2, s1);       // |I1 - I2|\n    s1.convertTo(s1, CV_32F);  // cannot make a square on 8 bits\n    s1 = s1.mul(s1);           // |I1 - I2|^2\n\n    Scalar s = sum(s1);        // sum elements per channel\n\n    double sse = s.val[0] + s.val[1] + s.val[2]; // sum channels\n\n    if( sse <= 1e-10) // for small values return zero\n        return 0;\n    else\n    {\n        double mse = sse / (double)(I1.channels() * I1.total());\n        double psnr = 10.0 * log10((255 * 255) / mse);\n        return psnr;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Results in Python\nDESCRIPTION: Python snippet displaying the original source image and the generated histogram image in separate windows using `cv.imshow`. It waits indefinitely for a key press using `cv.waitKey(0)` before proceeding (implicitly closing windows upon script termination).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/Histograms_Matching/histogram_calculation/calcHist_Demo.py Display\n```\n\n----------------------------------------\n\nTITLE: Defining the cv.drawContours Function Signature in OpenCV.js\nDESCRIPTION: Defines the signature for the `cv.drawContours` function in OpenCV.js, used to draw detected contours onto a destination image. Parameters allow specifying which contours to draw (contourIdx), their color, thickness (negative value fills interiors), line type, hierarchy level (for nested contours), and an optional offset.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contours_begin/js_contours_begin.markdown#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ncv.drawContours (image, contours, contourIdx, color, thickness = 1, lineType = cv.LINE_8, hierarchy = new cv.Mat(), maxLevel = INT_MAX, offset = new cv.Point(0, 0))\n@param image         destination image.\n@param contours      all the input contours.\n@param contourIdx    parameter indicating a contour to draw. If it is negative, all the contours are drawn.\n@param color         color of the contours.\n@param thickness     thickness of lines the contours are drawn with. If it is negative, the contour interiors are drawn.\n@param lineType      line connectivity(see cv.LineTypes).\n@param hierarchy     optional information about hierarchy. It is only needed if you want to draw only some of the contours(see maxLevel).\n@param maxLevel      maximal level for drawn contours. If it is 0, only the specified contour is drawn. If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This parameter is only taken into account when there is hierarchy available.\n@param offset        optional contour shift parameter.\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenCV Java Bindings in Eclipse - Java\nDESCRIPTION: This Java snippet demonstrates a minimal setup for loading the OpenCV native library and creating a 3x3 identity matrix using OpenCV's Mat API. Required dependencies include the OpenCV Java SDK and correct native library paths set up via Eclipse's user library mechanism. It expects no arguments and prints the contents of a 3x3 matrix to standard output for verification. Ensure that 'opencv-xxx.jar' is included in the build path and that the associated native libraries are properly configured.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/java_eclipse/java_eclipse.markdown#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport org.opencv.core.Core;\nimport org.opencv.core.CvType;\nimport org.opencv.core.Mat;\n\npublic class Hello\n{\n   public static void main( String[] args )\n   {\n      System.loadLibrary( Core.NATIVE_LIBRARY_NAME );\n      Mat mat = Mat.eye( 3, 3, CvType.CV_8UC1 );\n      System.out.println( \"mat = \" + mat.dump() );\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Copying Source Image in Callback (Python)\nDESCRIPTION: Inside the `MatchingMethod` callback function, this code creates a copy of the global source image using `img.copy()`. This copy (`img_display`) is used for drawing the result rectangle, preserving the original image for future template matching calls with different methods.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/match_template/match_template.py copy_source\n```\n\n----------------------------------------\n\nTITLE: Applying Morphological Transformations with Interactive Trackbars in OpenCV (Java)\nDESCRIPTION: This Java code provides an interactive demo of morphological operations (Opening, Closing, Gradient, Top Hat, Black Hat) using OpenCV's Java API. It loads an image, sets up a JFrame with trackbars (JSliders) for operator, element, and kernel size, and redraws the processed image using Imgproc.morphologyEx whenever sliders change. Dependencies include OpenCV for Java, a suitable image file, and Java Swing/AWT for window controls. Key parameters are chosen via sliders; outputs appear in the GUI frame. Users can experiment with different transformations and kernel properties in real time. Platform must support Java Swing and OpenCV native binaries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/opening_closing_hats/opening_closing_hats.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.*;\\nimport org.opencv.imgcodecs.Imgcodecs;\\nimport org.opencv.imgproc.Imgproc;\\nimport javax.swing.*;\\nimport java.awt.*;\\nimport java.awt.image.BufferedImage;\\n\\npublic class MorphologyDemo2 {\\n    static { System.loadLibrary(Core.NATIVE_LIBRARY_NAME); }\\n    private Mat src, dst;\\n    private JFrame frame;\\n    private JLabel imgLabel;\\n    private int morph_operator = 0;\\n    private int morph_elem = 0;\\n    private int morph_size = 0;\\n    private final int max_operator = 4;\\n    private final int max_elem = 2;\\n    private final int max_kernel_size = 21;\\n\\n    public MorphologyDemo2(String imgPath) {\\n        src = Imgcodecs.imread(imgPath);\\n        dst = new Mat();\\n        setupGUI();\\n        applyMorphology();\\n    }\\n\\n    private void setupGUI() {\\n        frame = new JFrame(\"Morphology Transformations\");\\n        frame.setLayout(new BorderLayout());\\n\\n        imgLabel = new JLabel();\\n        frame.add(imgLabel, BorderLayout.CENTER);\\n\\n        JPanel sliders = new JPanel(new GridLayout(3, 1));\\n        JSlider operatorSlider = new JSlider(0, max_operator, morph_operator);\\n        JSlider elemSlider = new JSlider(0, max_elem, morph_elem);\\n        JSlider sizeSlider = new JSlider(0, max_kernel_size, morph_size);\\n        operatorSlider.setBorder(BorderFactory.createTitledBorder(\"Operator\"));\\n        elemSlider.setBorder(BorderFactory.createTitledBorder(\"Element\"));\\n        sizeSlider.setBorder(BorderFactory.createTitledBorder(\"Kernel Size\"));\\n\\n        operatorSlider.addChangeListener(e -> { morph_operator = operatorSlider.getValue(); applyMorphology(); });\\n        elemSlider.addChangeListener(e -> { morph_elem = elemSlider.getValue(); applyMorphology(); });\\n        sizeSlider.addChangeListener(e -> { morph_size = sizeSlider.getValue(); applyMorphology(); });\\n\\n        sliders.add(operatorSlider);\\n        sliders.add(elemSlider);\\n        sliders.add(sizeSlider);\\n        frame.add(sliders, BorderLayout.SOUTH);\\n\\n        frame.setSize(900, 600);\\n        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\\n        frame.setVisible(true);\\n    }\\n\\n    private void applyMorphology() {\\n        int operation = morph_operator + 2;\\n        Mat element = Imgproc.getStructuringElement(morph_elem,\\n                new Size(2 * morph_size + 1, 2 * morph_size + 1),\\n                new Point(morph_size, morph_size));\\n        Imgproc.morphologyEx(src, dst, operation, element);\\n        Image img = toBufferedImage(dst);\\n        imgLabel.setIcon(new ImageIcon(img));\\n    }\\n\\n    private Image toBufferedImage(Mat mat) {\\n        int type = BufferedImage.TYPE_3BYTE_BGR;\\n        if(mat.channels() == 1) type = BufferedImage.TYPE_BYTE_GRAY;\\n        int bufferSize = mat.channels() * mat.cols() * mat.rows();\\n        byte[] b = new byte[bufferSize];\\n        mat.get(0,0,b);\\n        BufferedImage image = new BufferedImage(mat.cols(), mat.rows(), type);\\n        final byte[] targetPixels = ((DataBufferByte) image.getRaster().getDataBuffer()).getData();\\n        System.arraycopy(b, 0, targetPixels, 0, b.length);\\n        return image;\\n    }\\n\\n    public static void main(String[] args) {\\n        String imgPath = (args.length > 0) ? args[0] : \"baboon.png\";\\n        new MorphologyDemo2(imgPath);\\n    }\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Loading Images and Homography with OpenCV in Java\nDESCRIPTION: Java code for loading grayscale images and a homography matrix using OpenCV, preparing them for subsequent AKAZE feature detection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nsamples/java/tutorial_code/features2D/akaze_matching/AKAZEMatchDemo.java load\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV CMake Build for Ubuntu Desktop Linux\nDESCRIPTION: CMake configuration options for building OpenCV with CUDA support on Ubuntu Desktop Linux 14.04/16.04 LTS. Supports multiple CUDA architectures and enables Python 2 bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\n$ cmake \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=/usr \\\n    -DBUILD_PNG=OFF \\\n    -DBUILD_TIFF=OFF \\\n    -DBUILD_TBB=OFF \\\n    -DBUILD_JPEG=OFF \\\n    -DBUILD_JASPER=OFF \\\n    -DBUILD_ZLIB=OFF \\\n    -DBUILD_EXAMPLES=ON \\\n    -DBUILD_JAVA=OFF \\\n    -DBUILD_opencv_python2=ON \\\n    -DBUILD_opencv_python3=OFF \\\n    -DWITH_OPENCL=OFF \\\n    -DWITH_OPENMP=OFF \\\n    -DWITH_FFMPEG=ON \\\n    -DWITH_GSTREAMER=OFF \\\n    -DWITH_GSTREAMER_0_10=OFF \\\n    -DWITH_CUDA=ON \\\n    -DWITH_GTK=ON \\\n    -DWITH_VTK=OFF \\\n    -DWITH_TBB=ON \\\n    -DWITH_1394=OFF \\\n    -DWITH_OPENEXR=OFF \\\n    -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-8.0 \\\n    -DCUDA_ARCH_BIN='3.0 3.5 5.0 6.0 6.2' \\\n    -DCUDA_ARCH_PTX=\"\" \\\n    -DINSTALL_C_EXAMPLES=ON \\\n    -DINSTALL_TESTS=OFF \\\n    -DOPENCV_TEST_DATA_PATH=../opencv_extra/testdata \\\n    ../opencv\n```\n\n----------------------------------------\n\nTITLE: Creating a BackgroundSubtractor in OpenCV\nDESCRIPTION: Creates a BackgroundSubtractor object using either MOG2 or KNN algorithm based on user selection. This object will generate foreground masks by separating moving objects from the background.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/background_subtraction.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n//create Background Subtractor objects\nPtr<BackgroundSubtractor> pBackSub;\nif (parser.get<String>(\"algorithm\") == \"MOG2\")\n    pBackSub = createBackgroundSubtractorMOG2();\nelse\n    pBackSub = createBackgroundSubtractorKNN();\n```\n\nLANGUAGE: Java\nCODE:\n```\n// create Background Subtractor objects\nBackgroundSubtractor backSub;\nif (algorithm.equals(\"MOG2\"))\n    backSub = Video.createBackgroundSubtractorMOG2();\nelse\n    backSub = Video.createBackgroundSubtractorKNN();\n```\n\nLANGUAGE: Python\nCODE:\n```\n#create Background Subtractor objects\nif args.algo == 'MOG2':\n    backSub = cv.createBackgroundSubtractorMOG2()\nelse:\n    backSub = cv.createBackgroundSubtractorKNN()\n```\n\n----------------------------------------\n\nTITLE: Finding Fourier Transform using Numpy FFT\nDESCRIPTION: This snippet demonstrates how to apply the Fourier Transform to an image using Numpy's FFT function. It requires Numpy and Matplotlib libraries. The snippet reads an image, applies FFT, then uses a magnitude spectrum to visualize frequency content. Input is a grayscale image; output is a plotted frequency spectrum.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('messi5.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nf = np.fft.fft2(img)\nfshift = np.fft.fftshift(f)\nmagnitude_spectrum = 20*np.log(np.abs(fshift))\n\nplt.subplot(121),plt.imshow(img, cmap = 'gray')\nplt.title('Input Image'), plt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(magnitude_spectrum, cmap = 'gray')\nplt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Applying Laplacian Operator in Java\nDESCRIPTION: Applies the Laplacian operator to the grayscale image (`srcGray`) using Imgproc.Laplacian. The destination Mat (`dst`) is initialized, and the output depth is set to CvType.CV_16S to avoid overflow. Default values are used for kernel size, scale, delta, and border type. Requires OpenCV Java bindings (Imgproc).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_16\n\nLANGUAGE: java\nCODE:\n```\n//! [laplacian]\n/// Apply Laplace function\ndst = new Mat();\nint kernelSize = 3;\nint scale = 1;\nint delta = 0;\nint ddepth = CvType.CV_16S;\nImgproc.Laplacian( srcGray, dst, ddepth, kernelSize, scale, delta, Core.BORDER_DEFAULT );\n//! [laplacian]\n```\n\n----------------------------------------\n\nTITLE: Building the Face Beautification Processing Pipeline with G-API\nDESCRIPTION: Construction of the main G-API graph for face beautification that combines DNN inference with custom operations to process and enhance facial features.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// Let's build a G-API graph for this:    \nauto kernels = cv::gapi::combine(cv::gapi::core::cpu::kernels(),\n                                    cv::gapi::imgproc::cpu::kernels());\n        \ncv::GMat in;\n    \n// Part 1. G-API offers various operations and patterns out-of-the-box,\n// so unsharp mask is easily expressed as:\ncv::GMat blurred = unsharpMask(in);\n    \n// Part 2. G-API doesn't have bilateral filter yet so need to wrap it up\n// as a custom operation.\ncv::GMat bilateral = custom::bilateralFilter(in, 15, 80, 80);\n    \n// Part 3. Face & Landmarks detection\n// 3.1. Face detection using G-API Network inference\ncv::GMat face_detection = cv::gapi::infer<FaceDetector>(in);\n    \n// 3.2. Parse SSD object detection result\ncv::GArray<cv::Rect> faces = custom::ParseSSD::on(face_detection, in.dims(), 0.5f, true);\n    \n// 3.3. Detect landmarks on every face (generating per-face outputs)\ncv::GArray<cv::GMat> landmarks = cv::gapi::infer<LandmarksDetector>(in, faces);\n    \n// 3.4. Process landmarks to get contours of face parts\ncv::GArray<std::vector<custom::Contour>> elem_contours;\ncv::GArray<custom::Contour> face_contours;\nstd::tie(elem_contours, face_contours) = custom::landmarksToContours::on(landmarks, faces);\n    \n// 3.5. Generate background mask (the areas where the original image stays as-is)\ncv::GMat bg_mask = custom::contourMask::on(cv::gapi::copy(in), face_contours, false);\n    \n// 3.6. Prepare face features masks\ncv::GMat sharpen_mask = custom::elemMask::on(in, elem_contours, face_contours, false, true, true, true, true);\ncv::GMat blur_mask = custom::elemMask::on(in, elem_contours, face_contours, true, false, false, false, false);\n    \n// 4. Combine the results using standard operations\ncv::GMat result = in * bg_mask + blurred * sharpen_mask + bilateral * blur_mask;\n    \n// 5. Finally construct the computation object\nauto computation = cv::GComputation(cv::GIn(in), cv::GOut(result));\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV DNN Inference\nDESCRIPTION: Executes inference using an OpenCV Deep Neural Network model after setting the input blob. This example assumes that the OpenCV model has been loaded and input data prepared. The output is segmentation class predictions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# set OpenCV DNN input\nopencv_net.setInput(preproc_img)\n\n# OpenCV DNN inference\nout = opencv_net.forward()\nprint(\"OpenCV DNN segmentation prediction: \\n\")\nprint(\"* shape: \", out.shape)\n\n# get IDs of predicted classes\nout_predictions = np.argmax(out[0], axis=0)\n```\n\n----------------------------------------\n\nTITLE: Drawing a Specific Contour Using a Different Approach in OpenCV Python\nDESCRIPTION: This code shows an alternative method for drawing a specific contour (the 5th contour) by first extracting it into a variable and then passing it as a single-element list to cv.drawContours().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contours_begin/py_contours_begin.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncnt = contours[4]\ncv.drawContours(img, [cnt], 0, (0,255,0), 3)\n```\n\n----------------------------------------\n\nTITLE: Loading Image in OpenCV\nDESCRIPTION: Loads an input image from file using OpenCV's imread function. Returns error if image loading fails.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/filter_2d/filter_2d.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nMat src = imread(samples::findFile(\"lena.jpg\"));\nif (src.empty())\n{\n    printf(\" Error opening image\\n\");\n    printf(\" Program Arguments: [image_name -- default ../data/lena.jpg] \\n\");\n    return EXIT_FAILURE;\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Dense Optical Flow with Farneback in JavaScript\nDESCRIPTION: This snippet shows the application of Farneback's algorithm for dense optical flow using the cv.calcOpticalFlowFarneback function in OpenCV.js. The method computes flow for every point in the frame, making it suitable for more comprehensive motion analysis. It requires parameters such as pyramid scale and levels, window size, iterations, and polynomial expansion settings. Outputs are detailed flow fields for the given frames. This approach is computationally intensive and facilitates detailed motion analysis across the entire frame.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_video/js_lucas_kanade/js_lucas_kanade.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.calcOpticalFlowFarneback(prev, next, flow, pyrScale, levels, winsize, iterations, polyN, polySigma, flags);\n```\n\n----------------------------------------\n\nTITLE: Enabling TBB and Eigen Support in OpenCV CMake Configuration (Shell)\nDESCRIPTION: Configures the OpenCV build to include support for Intel TBB (Threading Building Blocks) and the Eigen library by setting `WITH_TBB` and `WITH_EIGEN` flags to 'ON'. These libraries provide performance benefits for certain OpenCV operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\ncmake -D WITH_TBB=ON -D WITH_EIGEN=ON ..\n```\n\n----------------------------------------\n\nTITLE: Arithmetic Operations with Vector Registers in C++\nDESCRIPTION: Demonstrates element-wise addition and multiplication of two vector registers containing float values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/univ_intrin/univ_intrin.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nv_float32 a, b;                          // {a1, ..., an}, {b1, ..., bn}\nv_float32 c;\nc = a + b                                // {a1 + b1, ..., an + bn}\nc = a * b;                               // {a1 * b1, ..., an * bn}\n```\n\n----------------------------------------\n\nTITLE: Classifying and Visualizing SVM Regions with OpenCV - Python\nDESCRIPTION: This Python snippet applies OpenCV to train an SVM classifier and visually differentiate regions by class on a Cartesian plane (green for label 1, blue for label -1). Dependencies are the OpenCV Python package (cv2) and numpy. The code builds the training set, configures and trains the SVM, then applies the classifier to each pixel for visualization. Accepted inputs are arrays of labeled data points; the result is an image with different colors per class and a clear decision boundary.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_svm.markdown#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# SVM region classification and visualization in Python using OpenCV\n# ... (full code from samples/python/tutorial_code/ml/introduction_to_svm/introduction_to_svm.py, show)\n```\n\n----------------------------------------\n\nTITLE: Refining Edges and Extracting Objects in OpenCV (C++/Java/Python)\nDESCRIPTION: Refines the result by first subtracting the detected vertical lines from the original binary image. Then, it applies dilation followed by erosion (morphological closing, although not explicitly named) with a small 2x2 square kernel to smooth the edges of the remaining objects (presumably the music notes).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection.md#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n//![smooth]\n// Inverse vertical image\nbitwise_not(vertical, vertical);\nshow_wait_destroy(\"vertical_bit\", vertical);\n\n// Extract edges and smooth image according to the logic\n// 1. extract edges\n// 2. dilate(edges)\n// 3. src.copyTo(smooth)\n// 4. blur smooth img\n// 5. smooth.copyTo(src, edges)\n\n// Step 1\nMat edges;\nadaptiveThreshold(vertical, edges, 255, ADAPTIVE_THRESH_MEAN_C, THRESH_BINARY, 3, -2);\nshow_wait_destroy(\"edges\", edges);\n\n// Step 2\nMat kernel = Mat::ones(2, 2, CV_8UC1);\ndilate(edges, edges, kernel);\nshow_wait_destroy(\"dilate\", edges);\n\n// Step 3\nMat smooth;\nvertical.copyTo(smooth);\n\n// Step 4\nblur(smooth, smooth, Size(2, 2));\n\n// Step 5\nsmooth.copyTo(vertical, edges);\n\n// Show final result\nshow_wait_destroy(\"smooth - final\", vertical);\n//![smooth]\n```\n\nLANGUAGE: java\nCODE:\n```\n//![smooth]\n// Inverse vertical image\nCore.bitwise_not(vertical, vertical);\nshowWaitDestroy(\"vertical_bit\", vertical);\n\n// Extract edges and smooth image according to the logic\n// 1. extract edges\n// 2. dilate(edges)\n// 3. src.copyTo(smooth)\n// 4. blur smooth img\n// 5. smooth.copyTo(src, edges)\n\n// Step 1\nMat edges = new Mat();\nImgproc.adaptiveThreshold(vertical, edges, 255, Imgproc.ADAPTIVE_THRESH_MEAN_C, Imgproc.THRESH_BINARY, 3, -2);\nshowWaitDestroy(\"edges\", edges);\n\n// Step 2\nMat kernel = Mat.ones(2, 2, CvType.CV_8UC1);\nImgproc.dilate(edges, edges, kernel);\nshowWaitDestroy(\"dilate\", edges);\n\n// Step 3\nMat smooth = new Mat();\nvertical.copyTo(smooth);\n\n// Step 4\nImgproc.blur(smooth, smooth, new Size(2, 2));\n\n// Step 5\nsmooth.copyTo(vertical, edges);\n\n// Show final result\nshowWaitDestroy(\"smooth - final\", vertical);\n//![smooth]\n```\n\nLANGUAGE: python\nCODE:\n```\n#![smooth]\n# Inverse vertical image\nvertical = cv.bitwise_not(vertical)\nshow_wait_destroy(\"vertical_bit\", vertical)\n\n# Step 1: Extract edges\nedges = cv.adaptiveThreshold(vertical, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 3, -2)\nshow_wait_destroy(\"edges\", edges)\n\n# Step 2: Dilate(edges)\nkernel = np.ones((2,2), np.uint8)\nedges = cv.dilate(edges, kernel)\nshow_wait_destroy(\"dilate\", edges)\n\n# Step 3: src.copyTo(smooth)\nsmooth = np.copy(vertical)\n\n# Step 4: blur smooth img\nsmooth = cv.blur(smooth, (2,2))\n\n# Step 5: smooth.copyTo(src, edges)\n(rows, cols) = np.where(edges != 0)\nvertical[rows, cols] = smooth[rows, cols]\n\n# Show final result\nshow_wait_destroy(\"smooth - final\", vertical)\n#![smooth]\n```\n\n----------------------------------------\n\nTITLE: Using Watershed Algorithm for Image Segmentation in OpenCV\nDESCRIPTION: This snippet demonstrates the implementation of the watershed algorithm using OpenCV to segment mutually touching objects. It covers the usage of the cv.watershed(), cv.distanceTransform(), and cv.connectedComponents() functions along with morphological operations to preprocess the image. The script requires OpenCV to be installed and includes dependencies such as cv.watershed() and cv.distanceTransform(). It processes grayscale images, requiring initial binarization, and uses markers for segmentation based on user-defined regions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_watershed/js_watershed.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nBelow we will see an example on how to use the Distance Transform along with watershed to segment mutually touching objects.\n```\n\n----------------------------------------\n\nTITLE: Opening and Closing XML/YAML/JSON Files with OpenCV FileStorage - C++\nDESCRIPTION: This C++ snippet demonstrates how to open and close XML/YAML/JSON files with the OpenCV FileStorage class. It uses the constructor or the open() function for opening files with modes such as WRITE, READ, or APPEND, and shows calling release() to explicitly close the file. Requires OpenCV installed and the <opencv2/core.hpp> header. The file extension determines the format; compressed files (e.g., .xml.gz) are supported.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/core.hpp>\\nusing namespace cv;\\n\\n// Opening a file for writing\\nFileStorage fs(\"output.yml\", FileStorage::WRITE);\\n\\n// ... perform write operations ...\\n\\n// Explicitly closing the file\\nfs.release();\n```\n\n----------------------------------------\n\nTITLE: Image Undistortion using Remapping in Python\nDESCRIPTION: This snippet performs image undistortion by establishing a remapping function using cv.initUndistortRectifyMap() and cv.remap(). The function takes an input image with camera parameters and outputs a corrected image. The approach is more involved compared to cv.undistort() but provides flexibility in processing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_calibration/py_calibration.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# undistort\nmapx, mapy = cv.initUndistortRectifyMap(mtx, dist, None, newcameramtx, (w,h), 5)\ndst = cv.remap(img, mapx, mapy, cv.INTER_LINEAR)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sample Android Project - CMake\nDESCRIPTION: Defines a sample project for example image manipulations and sets up the build environment using CMake. The snippet sets project-specific variables, invokes a macro to configure Android project settings such as library dependencies and SDK target, and establishes dependency relationships to guarantee correct build sequencing. Requires OpenCV Android libraries, CMake, and the Android SDK; assumes environment variables and paths for OpenCV and the Android SDK are set. Inputs include sample project names and paths. Outputs are the updated CMake build targets and dependency graph for Android examples.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/image-manipulations/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(sample example-image-manipulations)\n\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\" LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\" SDK_TARGET 11 \"${ANDROID_SDK_TARGET}\")\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating an Interactive Morphology Demo in OpenCV with Python\nDESCRIPTION: This Python snippet provides the general structure for an interactive script using OpenCV to perform and visualize image erosion and dilation. It loads an image, creates GUI windows with trackbars to select morphological type and kernel size, and provides callback functions to update the output as sliders are moved. Dependencies: OpenCV-Python. Key parameters: input image path, kernel type, kernel size. Output is shown in separate windows for each operation. The script expects at least one image file as input or uses a default if none provided.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n@snippet python/tutorial_code/imgProc/erosion_dilatation/morphology_1.py main\n```\n\n----------------------------------------\n\nTITLE: Reading Images from Canvas Elements using OpenCV.js - JavaScript\nDESCRIPTION: This snippet demonstrates how to read images from two HTML canvas elements into OpenCV.js Mat objects using cv.imread. It is a preparatory step required before performing any image operations or blending. Dependencies include OpenCV.js and two canvas elements with IDs 'canvasInput1' and 'canvasInput2'.\nParameters: canvas IDs for source images. Inputs: Canvas DOM elements. Outputs: src1 and src2 Mat objects.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_trackbar/js_trackbar.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet src1 = cv.imread('canvasInput1');\nlet src2 = cv.imread('canvasInput2');\n```\n\n----------------------------------------\n\nTITLE: Simplified Image Reading and Displaying with OpenCV.js\nDESCRIPTION: This example utilizes OpenCV.js methods for simplified reading and output of images from an HTML source directly to a canvas. Dependencies include OpenCV.js and an HTML structure with an image or canvas element.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_image_display/js_image_display.markdown#2025-04-22_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet img = cv.imread(imageSource);\ncv.imshow(canvasOutput, img);\nimg.delete();\n```\n\n----------------------------------------\n\nTITLE: Disabling GPU Modules in OpenCV CMake Configuration (Shell)\nDESCRIPTION: Configures CMake to exclude GPU-related modules during the OpenCV build. This includes disabling OpenCL (`WITH_OPENCL=OFF`) and various specific `opencv_gpu*` modules, potentially speeding up the build and reducing dependencies if GPU acceleration isn't required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\ncmake -D WITH_OPENCL=OFF -D BUILD_opencv_gpu=OFF -D BUILD_opencv_gpuarithm=OFF -D BUILD_opencv_gpubgsegm=OFF -D BUILD_opencv_gpucodec=OFF -D BUILD_opencv_gpufeatures2d=OFF -D BUILD_opencv_gpufilters=OFF -D BUILD_opencv_gpuimgproc=OFF -D BUILD_opencv_gpulegacy=OFF -D BUILD_opencv_gpuoptflow=OFF -D BUILD_opencv_gpustereo=OFF -D BUILD_opencv_gpuwarping=OFF ..\n```\n\n----------------------------------------\n\nTITLE: Applying Laplacian Operator Full Example in C++\nDESCRIPTION: Complete C++ code demonstrating loading an image, applying Gaussian blur, converting to grayscale, using the Laplacian operator for edge detection, converting the result to CV_8U, and displaying it. This code relies on the OpenCV library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// The tutorial code's is shown lines below. You can also download it from\n// [here](https://raw.githubusercontent.com/opencv/opencv/4.x/samples/cpp/tutorial_code/ImgTrans/Laplace_Demo.cpp)\n@include samples/cpp/tutorial_code/ImgTrans/Laplace_Demo.cpp\n```\n\n----------------------------------------\n\nTITLE: Blending Images Using cv.addWeighted in Python\nDESCRIPTION: This snippet demonstrates image blending using the OpenCV function cv.addWeighted(), which combines two images with specified weights to create a blend. Dependencies include the images ml.png and opencv-logo.png. The main parameters include weights for each image and an optional scalar added to the output.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_image_arithmetics/py_image_arithmetics.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimg1 = cv.imread('ml.png')\nimg2 = cv.imread('opencv-logo.png')\nassert img1 is not None, \"file could not be read, check with os.path.exists()\"\nassert img2 is not None, \"file could not be read, check with os.path.exists()\"\n\ndst = cv.addWeighted(img1,0.7,img2,0.3,0)\n\ncv.imshow('dst',dst)\ncv.waitKey(0)\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Implementing Face Detection Post-Processing Kernel\nDESCRIPTION: Kernel implementation for parsing SSD (Single Shot MultiBox Detector) output to produce an array of face rectangles with optional scaling to the original frame size.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// Implementation of the SSD post-processing kernel\nSTRUCT_KERNEL(GAPI_OCV_KERNEL, ParseSSD, CustomParseSSDBatchImpl) {\n    static void run(const cv::Mat& in_ssd_result,\n                    const cv::Size& in_size,\n                    float confidence_threshold,\n                    bool alignment_to_square,\n                    std::vector<cv::Rect>& out_objects) {\n        const auto& ssd_dims = in_ssd_result.size;\n        GAPI_Assert(ssd_dims.dims() == 4u);            // Fixed dims count\n        GAPI_Assert(ssd_dims[0] == 1);                 // Fixed batch size\n        GAPI_Assert(ssd_dims[1] == 1);                 // Fixed batch size\n        GAPI_Assert(ssd_dims[2] > 0);                  // Should have at least 1 proposal\n        GAPI_Assert(ssd_dims[3] == 7);                 // Fixed proposal size\n\n        const int MAX_PROPOSALS = ssd_dims[2];\n        const int OBJECT_SIZE = ssd_dims[3];\n        const cv::Rect surface({0,0}, in_size);\n\n        const float* data = in_ssd_result.ptr<float>();\n        out_objects.clear();\n\n        for (int i = 0; i < MAX_PROPOSALS; i++) {\n            const float *it = data + i * OBJECT_SIZE;\n            float image_id   = it[0];                  // batch id\n            float confidence  = it[2];                  // confidence\n            float rc_left    = it[3];                  // left\n            float rc_top     = it[4];                  // top\n            float rc_right   = it[5];                  // right\n            float rc_bottom  = it[6];                  // bottom\n\n            if (image_id < 0.f) {                      // indicates end of detections\n                break;\n            }\n            if (confidence < confidence_threshold) {     // skip objects with low confidence\n                continue;\n            }\n\n            // map relative coordinates to the original image scale\n            // taking available surface into account\n            cv::Rect rc;  // map to the original scale\n            rc.x      = static_cast<int>(rc_left   * in_size.width);\n            rc.y      = static_cast<int>(rc_top    * in_size.height);\n            rc.width  = static_cast<int>(rc_right  * in_size.width)  - rc.x;\n            rc.height = static_cast<int>(rc_bottom * in_size.height) - rc.y;\n\n            // now warp the produced rectangle if needed\n            if (alignment_to_square) {\n                // make square\n                int max_side = std::max(rc.width, rc.height);\n                int d_x = (max_side - rc.width)/2;\n                int d_y = (max_side - rc.height)/2;\n\n                rc.x -= d_x;\n                rc.y -= d_y;\n                rc.width  = max_side;\n                rc.height = max_side;\n            }\n\n            // finally, clip to the source image boundaries\n            rc = rc & surface;\n\n            // and add to the result\n            out_objects.emplace_back(std::move(rc));\n        }\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Drawing an Ellipse in Java\nDESCRIPTION: Implementation of the MyEllipse function that draws a rotated ellipse in OpenCV Java. The function takes the image and angle, and uses the ellipse() function to draw the shape with specified color and thickness.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\nprivate static void MyEllipse(Mat img, double angle) {\n    int thickness = 2;\n    int lineType = Core.LINE_8;\n\n    Imgproc.ellipse(img,\n            new Point(w/2, w/2),\n            new Size(w/4, w/16),\n            angle,\n            0,\n            360,\n            new Scalar(255, 0, 0),\n            thickness,\n            lineType);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependencies and Samples with CMake\nDESCRIPTION: This CMake script sets up dependencies and defines build targets for OpenGL samples in the OpenCV project. It checks for specific libraries like X11 and EPOXY and sets up samples accordingly. The script uses CMake macros to filter and define targets based on the presence of these libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/opengl/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(APPLE)\n  return()\nendif()\n\nif(UNIX)\n  find_package(X11 QUIET)\nendif()\n\nfind_package(PkgConfig QUIET)\npkg_search_module(EPOXY QUIET epoxy)\n\nSET(OPENCV_OPENGL_SAMPLES_REQUIRED_DEPS\n  opencv_core\n  opencv_imgproc\n  opencv_imgcodecs\n  opencv_videoio\n  opencv_highgui)\nocv_check_dependencies(${OPENCV_OPENGL_SAMPLES_REQUIRED_DEPS})\n\nif(BUILD_EXAMPLES AND OCV_DEPENDENCIES_FOUND)\n  project(opengl_samples)\n  ocv_include_modules_recurse(${OPENCV_OPENGL_SAMPLES_REQUIRED_DEPS})\n  file(GLOB all_samples RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} *.cpp)\n  if(NOT X11_FOUND)\n    ocv_list_filterout(all_samples \"opengl_interop\")\n  endif()\n  if(NOT EPOXY_FOUND)\n    ocv_list_filterout(all_samples \"opengl3_2\")\n  endif()\n  foreach(sample_filename ${all_samples})\n    ocv_define_sample(tgt ${sample_filename} opengl)\n    ocv_target_link_libraries(${tgt} PRIVATE \"${OPENGL_LIBRARIES}\" \"${OPENCV_OPENGL_SAMPLES_REQUIRED_DEPS}\")\n    if(sample_filename STREQUAL \"opengl_interop.cpp\")\n      ocv_target_link_libraries(${tgt} PRIVATE ${X11_LIBRARIES})\n      ocv_target_include_directories(${tgt} ${X11_INCLUDE_DIR})\n    endif()\n    if(sample_filename STREQUAL \"opengl3_2.cpp\")\n      ocv_target_link_libraries(${tgt} PRIVATE ${EPOXY_LIBRARIES})\n      ocv_target_include_directories(${tgt} PRIVATE ${EPOXY_INCLUDE_DIRS})\n    endif()\n  endforeach()\nendif()\n\nocv_install_example_src(opengl *.cpp *.hpp CMakeLists.txt)\n```\n\n----------------------------------------\n\nTITLE: Creating a Look-up Table for Histogram Equalization\nDESCRIPTION: This code creates a look-up table for histogram equalization using NumPy masked arrays. It transforms the CDF into a mapping function that will redistribute pixel values across the full intensity range (0-255), which is the core operation of histogram equalization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncdf_m = np.ma.masked_equal(cdf,0)\ncdf_m = (cdf_m - cdf_m.min())*255/(cdf_m.max()-cdf_m.min())\ncdf = np.ma.filled(cdf_m,0).astype('uint8')\n```\n\n----------------------------------------\n\nTITLE: Displaying Calibration Results and Removing Distortion in C++\nDESCRIPTION: This code shows how to display calibration results and remove distortion from images using OpenCV functions like initUndistortRectifyMap and remap.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nshow_results\n```\n\n----------------------------------------\n\nTITLE: Upsampling Image using pyrUp in Java\nDESCRIPTION: Performs image upsampling on the `tmp` Mat object using `Imgproc.pyrUp`. The output `dst` Mat object is specified to have dimensions twice those of the input.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n            } else if( c == 'i' ) {\n                Imgproc.pyrUp( tmp, dst, new Size( tmp.cols()*2, tmp.rows()*2 ) );\n                System.out.println(\"** Zoom In: Image x 2\");\n            }\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenCV Random Number Generator (RNG) in C++\nDESCRIPTION: Initializes an instance of the OpenCV Random Number Generator class (cv::RNG). The generator is seeded with the value 0xFFFFFFFF. This 'rng' object will be used later to generate random numbers for coordinates, colors, and other parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/random_generator_and_text/random_generator_and_text.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n@code{.cpp}\nRNG rng( 0xFFFFFFFF );\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Implementing AGAST_7_12d Corner Detection in C++\nDESCRIPTION: This code snippet implements the AGAST_7_12d variant of the AGAST corner detection algorithm. It uses a 12-pixel mask in diamond format to score potential corner pixels based on intensity differences.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\ntemplate<>\nint agast_cornerScore<AgastFeatureDetector::AGAST_7_12d>(const uchar* ptr, const int pixel[], int threshold)\n{\n    int bmin = threshold;\n    int bmax = 255;\n    int b_test = (bmax + bmin)/2;\n\n    short offset0 = (short) pixel[0];\n    short offset1 = (short) pixel[1];\n    short offset2 = (short) pixel[2];\n    short offset3 = (short) pixel[3];\n    short offset4 = (short) pixel[4];\n    short offset5 = (short) pixel[5];\n    short offset6 = (short) pixel[6];\n    short offset7 = (short) pixel[7];\n    short offset8 = (short) pixel[8];\n    short offset9 = (short) pixel[9];\n    short offset10 = (short) pixel[10];\n    short offset11 = (short) pixel[11];\n\n    while(true)\n    {\n        const int cb = *ptr + b_test;\n        const int c_b = *ptr - b_test;\n        if(ptr[offset0] > cb)\n          if(ptr[offset5] > cb)\n            if(ptr[offset2] > cb)\n              if(ptr[offset9] > cb)\n                if(ptr[offset1] > cb)\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset3] > cb)\n                      if(ptr[offset4] > cb)\n                        goto is_a_corner;\n                      else\n                        if(ptr[offset10] > cb)\n                          if(ptr[offset11] > cb)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      if(ptr[offset8] > cb)\n                        if(ptr[offset10] > cb)\n                          if(ptr[offset11] > cb)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset4] > cb)\n                              if(ptr[offset7] > cb)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset11] > cb)\n                      if(ptr[offset3] > cb)\n                        if(ptr[offset4] > cb)\n                          goto is_a_corner;\n                        else\n                          if(ptr[offset10] > cb)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset8] > cb)\n                          if(ptr[offset10] > cb)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset7] > cb)\n                      if(ptr[offset8] > cb)\n                        if(ptr[offset4] > cb)\n                          if(ptr[offset3] > cb)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset10] > cb)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset10] > cb)\n                            if(ptr[offset11] > cb)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset3] > cb)\n                  if(ptr[offset4] > cb)\n                    if(ptr[offset1] > cb)\n                      if(ptr[offset6] > cb)\n                        goto is_a_corner;\n                      else\n                        if(ptr[offset11] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      if(ptr[offset6] > cb)\n                        if(ptr[offset7] > cb)\n                          if(ptr[offset8] > cb)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset9] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset1] > cb)\n                      if(ptr[offset10] > cb)\n                        if(ptr[offset11] > cb)\n                          goto is_a_corner;\n                        else\n                          if(ptr[offset6] > cb)\n                            if(ptr[offset4] > cb)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset6] > cb)\n                          if(ptr[offset3] > cb)\n                            if(ptr[offset4] > cb)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      if(ptr[offset6] > cb)\n                        if(ptr[offset4] > cb)\n                          if(ptr[offset3] > cb)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset10] > cb)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset10] > cb)\n                            if(ptr[offset11] > cb)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset5] < c_b)\n              if(ptr[offset9] > cb)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    if(ptr[offset11] > cb)\n                      if(ptr[offset1] > cb)\n                        if(ptr[offset8] > cb)\n                          if(ptr[offset10] > cb)\n                            if(ptr[offset2] > cb)\n                              goto is_a_corner;\n                            else\n                              if(ptr[offset7] > cb)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Applying Histogram Equalization with OpenCV\nDESCRIPTION: Using the equalizeHist function to enhance image contrast by redistributing intensity values across the available range.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_equalization/histogram_equalization.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nMat dst;\nequalizeHist( src_gray, dst );\n```\n\nLANGUAGE: Java\nCODE:\n```\nMat dst = new Mat();\nequalizeHist(srcGray, dst);\n```\n\nLANGUAGE: Python\nCODE:\n```\ndst = cv.equalizeHist(src_gray)\n```\n\n----------------------------------------\n\nTITLE: Computing Histograms for BGR Channels in Python\nDESCRIPTION: Python snippet calculating histograms for each B, G, and R plane using `cv.calcHist`. It calls `calcHist` for each plane (`bgr_planes[0]`, `bgr_planes[1]`, `bgr_planes[2]`), specifying the channel ([0]), no mask (None), bin count (`histSize`), and range (`histRange`). The results are stored in `b_hist`, `g_hist`, and `r_hist`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/Histograms_Matching/histogram_calculation/calcHist_Demo.py Compute the histograms\n```\n\n----------------------------------------\n\nTITLE: Specifying Source for JPEG Data in Decompression in C\nDESCRIPTION: This snippet demonstrates opening a file and specifying it as a source of compressed JPEG data for decompression using 'jpeg_stdio_src'. It includes file opening and error checking. Using a binary mode ('rb') is important to prevent data corruption on non-Unix systems.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_12\n\nLANGUAGE: C\nCODE:\n```\nFILE *infile;\nif ((infile = fopen(filename, \"rb\")) == NULL) {\n    fprintf(stderr, \"can't open %s\\n\", filename);\n    exit(1);\n}\njpeg_stdio_src(&cinfo, infile);\n```\n\n----------------------------------------\n\nTITLE: Loading and Converting Input Image (Java)\nDESCRIPTION: This Java snippet loads the input image with Highgui.imread and converts it to grayscale using Imgproc.cvtColor. The code includes a check for read failures, printing an error if the file can't be loaded. Input: filename via args[]. Output: grayscale Mat in src_gray, required for further thresholding.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n// [load]\\nsrc = Highgui.imread(args.length > 0 ? args[0] : \"chicky_512.png\");\\nif (src.empty()) {\\n    System.out.println(\"Error opening image\");\\n    return;\\n}\\nImgproc.cvtColor(src, src_gray = new Mat(), Imgproc.COLOR_BGR2GRAY);\\n// [load]\n```\n\n----------------------------------------\n\nTITLE: Displaying Image using imshow and waitKey in OpenCV in Java\nDESCRIPTION: Java bindings do not support GUI display directly. External libraries or wrappers are necessary. Sample code displayed here assumes such an interface, otherwise displaying via imwrite and opening via OS.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_41\n\nLANGUAGE: Java\nCODE:\n```\n// Java OpenCV does not natively support imshow; use JavaFX or third-party windowing.\n```\n\n----------------------------------------\n\nTITLE: Model Download Implementation in Python\nDESCRIPTION: Python code example showing how to download model files programmatically using the download_models utility with optional SHA hash verification.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom download_models import downloadFile\n\nfilepath1 = downloadFile(\"https://drive.google.com/uc?export=download&id=0B3gersZ2cHIxRm5PMWRoTkdHdHc\", None, filename=\"MobileNetSSD_deploy.caffemodel\", save_dir=\"save_dir_1\")\nfilepath2 = downloadFile(\"https://drive.google.com/uc?export=download&id=0B3gersZ2cHIxRm5PMWRoTkdHdHc\", \"994d30a8afaa9e754d17d2373b2d62a7dfbaaf7a\", filename=\"MobileNetSSD_deploy.caffemodel\")\nprint(filepath1)\nprint(filepath2)\n# Your code\n```\n\n----------------------------------------\n\nTITLE: Converting Laplacian Output to CV_8U in C++\nDESCRIPTION: Converts the Laplacian output image (`dst`), which has a depth of CV_16S, to an 8-bit unsigned integer image (`abs_dst`) using cv::convertScaleAbs. This computes the absolute value and scales the result to the 0-255 range, making it suitable for display. Requires OpenCV core module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\n//! [convert]\n// converting back to CV_8U\nconvertScaleAbs( dst, abs_dst );\n//! [convert]\n```\n\n----------------------------------------\n\nTITLE: Configuring Android ABI for OpenCL-enabled OpenCV SDK Build - Python\nDESCRIPTION: This Python-style configuration line defines the ABI build parameters for OpenCV, enabling OpenCL and specifying the path to the custom Android OpenCL SDK. It is used within build scripts or configuration files for OpenCV's build_sdk.py system. Correct path substitution and alignment of OpenCL SDK location are required for successful builds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nABI(\"3\", \"arm64-v8a\", None, 21, cmake_vars=dict('WITH_OPENCL': 'ON', 'ANDROID_OPENCL_SDK': 'path_to_your_Android_OpenCL_SDK'))\n```\n\n----------------------------------------\n\nTITLE: Finding Homography Transformation in OpenCV C++\nDESCRIPTION: Computes the homography transformation between two sets of points using the RANSAC algorithm. The ransacReprojThreshold parameter determines the maximum allowed reprojection error.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/detection_of_planar_objects/detection_of_planar_objects.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nvector<Point2f> points1, points2;\n// fill the arrays with the points\n....\nMat H = findHomography(Mat(points1), Mat(points2), RANSAC, ransacReprojThreshold);\n```\n\n----------------------------------------\n\nTITLE: Capturing and Displaying Video from Camera in Python with OpenCV\nDESCRIPTION: This snippet demonstrates how to capture video from a camera, convert it to grayscale, and display it using OpenCV. It uses cv.VideoCapture() to access the camera and processes frames in a loop until 'q' is pressed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_video_display/py_video_display.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\ncap = cv.VideoCapture(0)\nif not cap.isOpened():\n    print(\"Cannot open camera\")\n    exit()\nwhile True:\n    # Capture frame-by-frame\n    ret, frame = cap.read()\n\n    # if frame is read correctly ret is True\n    if not ret:\n        print(\"Can't receive frame (stream end?). Exiting ...\")\n        break\n    # Our operations on the frame come here\n    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n    # Display the resulting frame\n    cv.imshow('frame', gray)\n    if cv.waitKey(1) == ord('q'):\n        break\n\n# When everything done, release the capture\ncap.release()\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Converting Images to Grayscale in OpenCV\nDESCRIPTION: Converting color images to grayscale prior to histogram equalization, as the technique is applied to intensity values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_equalization/histogram_equalization.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ncvtColor( src, src_gray, COLOR_BGR2GRAY );\n```\n\nLANGUAGE: Java\nCODE:\n```\nMat srcGray = new Mat();\nCvtColor(src, srcGray, COLOR_BGR2GRAY);\n```\n\nLANGUAGE: Python\nCODE:\n```\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n```\n\n----------------------------------------\n\nTITLE: Loading Images and Text Recognition Model in C++\nDESCRIPTION: This C++ snippet demonstrates how to load a text image and initialize the TextRecognitionModel with ONNX weights for recognition. It requires the OpenCV DNN module, and a text recognition model file, and a vocabulary file. It sets up the image and model, configures the decoding method, and prepares the vocabulary for recognition.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_text_spotting/dnn_text_spotting.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <opencv2/opencv.hpp>\n#include <opencv2/dnn.hpp>\n#include <fstream>\n#include <vector>\n\n// Load a cropped text line image\n// you can find cropped images for testing in \"Images for Testing\"\nint rgb = IMREAD_COLOR; // This should be changed according to the model input requirement.\nMat image = imread(\"path/to/text_rec_test.png\", rgb);\n\n// Load models weights\nTextRecognitionModel model(\"path/to/crnn_cs.onnx\");\n\n// The decoding method\n// more methods will be supported in future\nmodel.setDecodeType(\"CTC-greedy\");\n\n// Load vocabulary\n// vocabulary should be changed according to the text recognition model\nstd::ifstream vocFile;\nvocFile.open(\"path/to/alphabet_94.txt\");\nCV_Assert(vocFile.is_open());\nString vocLine;\nstd::vector<String> vocabulary;\nwhile (std::getline(vocFile, vocLine)) {\n    vocabulary.push_back(vocLine);\n}\nmodel.setVocabulary(vocabulary);\n```\n\n----------------------------------------\n\nTITLE: Generating ChArUco Board Image\nDESCRIPTION: Shows how to generate an image of the ChArUco board for printing\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_detection/charuco_detection.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ncv::Mat boardImage;\nboard.generateImage(cv::Size(600, 500), boardImage, 10, 1);\n```\n\n----------------------------------------\n\nTITLE: Matching Extracted Face Features - OpenCV DNN Python\nDESCRIPTION: This snippet shows how to compare two extracted face features (numpy arrays) in Python using FaceRecognizerSF. The match function returns the similarity (cosine) or distance (normL2) between the two feature vectors, used to determine if they refer to the same person according to set thresholds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_face/dnn_face.markdown#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n# Compare two extracted face features\ncosine_score = recognizer.match(feature1, feature2, cv2.FaceRecognizerSF_FR_COSINE)\nl2_score     = recognizer.match(feature1, feature2, cv2.FaceRecognizerSF_FR_NORM_L2)\n# Compare scores to thresholds for matching\n```\n\n----------------------------------------\n\nTITLE: Detecting and Decoding Barcodes in One Step\nDESCRIPTION: Combines detection and decoding into a single function call using cv::barcode::BarcodeDetector::detectAndDecode. This method simplifies the process of obtaining decoded information from barcodes with one function call.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/barcode_detect_and_decode.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/barcode.cpp detectAndDecode\n```\n\n----------------------------------------\n\nTITLE: Constructor for Custom Layer OpenCV C++\nDESCRIPTION: This constructor retrieves hyper-parameters from cv::dnn::LayerParams with the option for trainable weight storage.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/custom_layers.hpp MyLayer::MyLayer\n```\n\n----------------------------------------\n\nTITLE: Handling User Input for Camera Calibration in C++\nDESCRIPTION: This snippet demonstrates how to handle user input to toggle distortion removal, restart detection, or quit the application during camera calibration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nawait_input\n```\n\n----------------------------------------\n\nTITLE: Downsampling Image using pyrDown in C++\nDESCRIPTION: Downsamples the image `tmp` using the `pyrDown` function. The destination image `dst` will have dimensions half that of `tmp`. The size is explicitly provided.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n        else if( c == 'o' )\n        { pyrDown( tmp, dst, Size( tmp.cols/2, tmp.rows/2 ) ); printf(\"** Zoom Out: Image / 2 \\n\"); }\n```\n\n----------------------------------------\n\nTITLE: Main Program: Loading a 3D Model - OpenCV C++\nDESCRIPTION: Shows how to instantiate a Model object and load a 3D textured object model using the load function in the main program. Requires defining a valid path to the YAML file containing the model data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nModel model;               // instantiate Model object\nmodel.load(yml_read_path); // load a 3D textured object model\n\n```\n\n----------------------------------------\n\nTITLE: Loading DEM Data with Native Value Preservation in OpenCV (C++)\nDESCRIPTION: The code illustrates loading a Digital Elevation Model (DEM) with OpenCV while preserving the intrinsic elevation data using the IMREAD_ANYDEPTH and IMREAD_LOAD_GDAL flags. This ensures raw numeric data (e.g., signed shorts for SRTM/DTED formats) are maintained. Requires OpenCV with GDAL enabled and Dem files in a compatible format.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/raster_io_gdal.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// Load a DEM raster with depth preservation\ncv::Mat dem = cv::imread(\"/path/to/file.hgt\", cv::IMREAD_ANYDEPTH | cv::IMREAD_LOAD_GDAL);\n```\n\n----------------------------------------\n\nTITLE: Console Output from OpenCV File Operations\nDESCRIPTION: Sample console output showing the results of writing and reading operations, including successful reads and handling of non-existent files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nWrite Done.\n\nReading:\n100image1.jpg\nAwesomeness\nbaboon.jpg\nTwo  2; One  1\n\n\nR = [1, 0, 0;\n  0, 1, 0;\n  0, 0, 1]\nT = [0; 0; 0]\n\nMyData =\n{ id = mydata1234, X = 3.14159, A = 97}\n\nAttempt to read NonExisting (should initialize the data structure with its default).\nNonExisting =\n{ id = , X = 0, A = 0}\n\nTip: Open up output.xml with a text editor to see the serialized data.\n```\n\n----------------------------------------\n\nTITLE: Calibrating Camera with ArUco Board in OpenCV\nDESCRIPTION: This code shows the camera calibration process using an ArUco board. It detects markers from multiple viewpoints, collects coordinates, and computes the camera calibration parameters. This approach is less accurate than ChArUco but useful in scenarios where ChArUco boards cannot be used.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_calibration/aruco_calibration.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ncv::Ptr<cv::aruco::DetectorParameters> detectorParams = cv::aruco::DetectorParameters::create();\ncv::Ptr<cv::aruco::Dictionary> dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::PREDEFINED_DICTIONARY_NAME(dictionaryId));\n\n// Create a board\ncv::Ptr<cv::aruco::GridBoard> gridboard = cv::aruco::GridBoard::create(markersX, markersY, markerLength, markerSeparation, dictionary);\ncv::Ptr<cv::aruco::Board> board = gridboard.staticCast<cv::aruco::Board>();\n\n// Collect data from each frame:\nstd::vector<std::vector<cv::Point2f>> allCornersConcatenated;\nstd::vector<int> allIdsConcatenated;\nstd::vector<int> markerCounterPerFrame;\n\n// For each frame:\nstd::vector<int> ids;\nstd::vector<std::vector<cv::Point2f>> corners, rejected;\n\n// Detect markers\naruco::detectMarkers(image, dictionary, corners, ids, detectorParams, rejected);\n\n// If at least one marker detected\nif(ids.size() > 0) {\n    markerCounterPerFrame.push_back((int)ids.size());\n    for(unsigned int i = 0; i < ids.size(); i++) {\n        allCornersConcatenated.push_back(corners[i]);\n        allIdsConcatenated.push_back(ids[i]);\n    }\n}\n```\n\nLANGUAGE: cpp\nCODE:\n```\n// After processing all frames\nif(allIdsConcatenated.size() > 0) {\n    \n    // Setup calibration\n    cv::Mat cameraMatrix, distCoeffs;\n    std::vector<cv::Mat> rvecs, tvecs;\n    \n    // prepare data for calibration\n    std::vector<std::vector<cv::Point2f>> allCorners;\n    std::vector<std::vector<cv::Point3f>> allObjectPoints;\n    std::vector<int> allIds;\n    std::vector<cv::Mat> allImgs;\n    \n    // Fill with data from detected markers\n    int counter = 0;\n    for(unsigned int i = 0; i < markerCounterPerFrame.size(); i++) {\n        int markerNum = markerCounterPerFrame[i];\n        std::vector<cv::Point2f> currentImgPoints;\n        std::vector<int> currentIds;\n        for(int j = 0; j < markerNum; j++) {\n            currentImgPoints.push_back(allCornersConcatenated[counter][0]);\n            currentImgPoints.push_back(allCornersConcatenated[counter][1]);\n            currentImgPoints.push_back(allCornersConcatenated[counter][2]);\n            currentImgPoints.push_back(allCornersConcatenated[counter][3]);\n            currentIds.push_back(allIdsConcatenated[counter]);\n            counter++;\n        }\n        allCorners.push_back(currentImgPoints);\n        allIds.push_back(currentIds);\n        allImgs.push_back(images[i]);\n    }\n```\n\nLANGUAGE: cpp\nCODE:\n```\n// Create objPoints for board using objPoints from gridboard\nstd::vector<cv::Point3f> objPoints;\nfor(unsigned int i = 0; i < allIds.size(); i++) {\n    std::vector<cv::Point3f> boardObjPoints;\n    gridboard->matchImagePoints(allCorners[i], allIds[i], boardObjPoints);\n    objPoints.insert(objPoints.end(), boardObjPoints.begin(), boardObjPoints.end());\n}\n\n// Calibrate camera now using object and image points\ndouble calibrationReprojErr = cv::calibrateCamera(objPoints, imgPoints, imgSize, cameraMatrix, distCoeffs, rvecs, tvecs, calibrationFlags);\n\n// Print results\nstd::cout << \"Rep Error: \" << calibrationReprojErr << std::endl;\nstd::cout << \"Calibration matrix: \" << cameraMatrix << std::endl;\nstd::cout << \"Distortion coefficients: \" << distCoeffs << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Loading an Image using OpenCV in C++\nDESCRIPTION: This snippet demonstrates how to load an image using OpenCV in C++. The image is read in grayscale mode using the imread function. This operation requires OpenCV installed and linked with your C++ project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_square_chess/camera_calibration_square_chess.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nMat img = imread(argv[1], IMREAD_GRAYSCALE);\n```\n\n----------------------------------------\n\nTITLE: Setting Up FFmpeg Plugin for OpenCV Videoio\nDESCRIPTION: This CMake script configures the FFmpeg plugin for OpenCV's videoio module. It sets up the source directory, includes necessary dependencies, and creates the plugin with the required parameters. The script also displays the versions of FFmpeg components being used.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/videoio/misc/plugin_ffmpeg/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5)\n\nget_filename_component(OpenCV_SOURCE_DIR \"${CMAKE_CURRENT_LIST_DIR}/../../../..\" ABSOLUTE)\ninclude(\"${OpenCV_SOURCE_DIR}/cmake/OpenCVPluginStandalone.cmake\")\n\n# scan dependencies\nset(WITH_FFMPEG ON)\nset(OPENCV_FFMPEG_SKIP_BUILD_CHECK ON)\ninclude(\"${OpenCV_SOURCE_DIR}/modules/videoio/cmake/init.cmake\")\n\nset(OPENCV_PLUGIN_DEPS core imgproc imgcodecs)\nocv_create_plugin(videoio \"opencv_videoio_ffmpeg\" \"ocv.3rdparty.ffmpeg\" \"FFmpeg\" \"src/cap_ffmpeg.cpp\")\n\nmessage(STATUS \"FFMPEG_libavcodec_VERSION=${FFMPEG_libavcodec_VERSION}\")\nmessage(STATUS \"FFMPEG_libavformat_VERSION=${FFMPEG_libavformat_VERSION}\")\nmessage(STATUS \"FFMPEG_libavutil_VERSION=${FFMPEG_libavutil_VERSION}\")\nmessage(STATUS \"FFMPEG_libswscale_VERSION=${FFMPEG_libswscale_VERSION}\")\nmessage(STATUS \"FFMPEG_libavresample_VERSION=${FFMPEG_libavresample_VERSION}\")\nif(OPENCV_FFMPEG_ENABLE_LIBAVDEVICE)\n  message(STATUS \"FFMPEG_libavdevice_VERSION=${FFMPEG_libavdevice_VERSION}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Detecting Subpixel Corners using OpenCV in C++\nDESCRIPTION: C++ implementation of corner detection with subpixel accuracy using OpenCV's cornerSubPix function. The code demonstrates image loading, corner detection, and refinement of corner positions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/corner_subpixels/corner_subpixels.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <iostream>\n#include <opencv2/core.hpp>\n#include <opencv2/highgui.hpp>\n#include <opencv2/imgproc.hpp>\n\nusing namespace cv;\nusing namespace std;\n\nMat src, src_gray;\n\nint maxCorners = 10;\nint maxTrackbar = 100;\n\nRNG rng(12345);\nchar* source_window = \"Image\";\n\nvoid goodFeaturesToTrack_Demo( int, void* );\n\nint main( int argc, char** argv )\n{\n    CommandLineParser parser( argc, argv, \"{@input | pic3.png | input image}\" );\n    src = imread( samples::findFile( parser.get<String>( \"@input\" ) ) );\n    if( src.empty() )\n    {\n        cout << \"Could not open or find the image!\\n\" << endl;\n        cout << \"Usage: \" << argv[0] << \" <Input image>\" << endl;\n        return -1;\n    }\n    cvtColor( src, src_gray, COLOR_BGR2GRAY );\n\n    namedWindow( source_window );\n    createTrackbar( \"Max corners:\", source_window, &maxCorners, maxTrackbar, goodFeaturesToTrack_Demo );\n\n    imshow( source_window, src );\n    goodFeaturesToTrack_Demo( 0, 0 );\n\n    waitKey();\n    return 0;\n}\n\nvoid goodFeaturesToTrack_Demo( int, void* )\n{\n    maxCorners = MAX(maxCorners, 1);\n    vector<Point2f> corners;\n    double qualityLevel = 0.01;\n    double minDistance = 10;\n    int blockSize = 3, gradientSize = 3;\n    bool useHarrisDetector = false;\n    double k = 0.04;\n\n    Mat copy = src.clone();\n\n    goodFeaturesToTrack( src_gray,\n                         corners,\n                         maxCorners,\n                         qualityLevel,\n                         minDistance,\n                         Mat(),\n                         blockSize,\n                         gradientSize,\n                         useHarrisDetector,\n                         k );\n\n    cout << \"** Number of corners detected: \" << corners.size() << endl;\n    int radius = 4;\n    for( size_t i = 0; i < corners.size(); i++ )\n    {\n        circle( copy, corners[i], radius, Scalar(rng.uniform(0,255), rng.uniform(0, 256), rng.uniform(0, 256)), FILLED );\n    }\n\n    namedWindow( \"Sharp corners\" );\n    imshow( \"Sharp corners\", copy );\n\n    Size winSize = Size( 5, 5 );\n    Size zeroZone = Size( -1, -1 );\n    TermCriteria criteria = TermCriteria( TermCriteria::EPS + TermCriteria::COUNT, 40, 0.001 );\n\n    cornerSubPix( src_gray, corners, winSize, zeroZone, criteria );\n\n    for( size_t i = 0; i < corners.size(); i++ )\n    {\n        cout << \" -- Refined Corner [\" << i << \"]  (\" << corners[i].x << \",\" << corners[i].y << \")\" << endl;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Camshift Object Tracking in Python\nDESCRIPTION: Python implementation of the Camshift algorithm for adaptive object tracking. This code extends the Meanshift approach by using CamShift to track objects with changing size and orientation across video frames. It includes histogram calculation and backprojection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/meanshift.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\\nimport cv2 as cv\\nimport argparse\\n\\nparser = argparse.ArgumentParser(description='This sample demonstrates the camshift algorithm. \\\\\\n                                              The example file can be downloaded from: \\\\\\n                                              https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4')\\nparser.add_argument('image', type=str, help='path to image file')\\nargs = parser.parse_args()\\n\\ncap = cv.VideoCapture(args.image)\\n# take first frame of the video\\nret,frame = cap.read()\\n# setup initial location of window\\nx, y, w, h = 300, 200, 100, 50 # simply hardcoded the values\\ntrack_window = (x, y, w, h)\\n# set up the ROI for tracking\\nroi = frame[y:y+h, x:x+w]\\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\\n# Setup the termination criteria, either 10 iteration or move by atleast 1 pt\\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\\nwhile(1):\\n    ret, frame = cap.read()\\n    if ret == True:\\n        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\\n        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)\\n        # apply camshift to get the new location\\n        ret, track_window = cv.CamShift(dst, track_window, term_crit)\\n        # Draw it on image\\n        pts = cv.boxPoints(ret)\\n        pts = np.int0(pts)\\n        img2 = cv.polylines(frame,[pts],True, 255,2)\\n        cv.imshow('img2',img2)\\n        k = cv.waitKey(30) & 0xff\\n        if k == 27:\\n            break\\n    else:\\n        break\\ncv.destroyAllWindows()\\ncap.release()\n```\n\n----------------------------------------\n\nTITLE: Exporting Classes and Methods Using OpenCV Macros in C++\nDESCRIPTION: This example illustrates how to export a full C++ class and its methods to Python using CV_EXPORTS_W for the class, CV_WRAP for methods, and CV_PROP for properties. The CLAHE class outlines methods to apply and configure the algorithm, each annotated to direct the binding generator. The expected inputs and outputs are as per the specified arguments; required dependency is OpenCV's Algorithm base class.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_bindings/py_bindings_basics/py_bindings_basics.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nclass CV_EXPORTS_W CLAHE : public Algorithm\n{\npublic:\n    CV_WRAP virtual void apply(InputArray src, OutputArray dst) = 0;\n\n    CV_WRAP virtual void setClipLimit(double clipLimit) = 0;\n    CV_WRAP virtual double getClipLimit() const = 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Histogram Comparison Methods in OpenCV\nDESCRIPTION: Comparing histograms using four different methods: Correlation, Chi-Square, Intersection, and Bhattacharyya distance, displaying each comparison result.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_comparison/histogram_comparison.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nfor( int compare_method = 0; compare_method < 4; compare_method++ )\n{\n    double base_base = compareHist( hist_base, hist_base, compare_method );\n    double base_half = compareHist( hist_base, hist_half_down, compare_method );\n    double base_test1 = compareHist( hist_base, hist_test1, compare_method );\n    double base_test2 = compareHist( hist_base, hist_test2, compare_method );\n    cout << \"Method [\" << compare_method << \"] Perfect, Base-Half, Base-Test(1), Base-Test(2) : \"\n         << base_base << \" / \" << base_half << \" / \" << base_test1 << \" / \" << base_test2 << endl;\n}\n```\n\nLANGUAGE: java\nCODE:\n```\nfor( int compare_method = 0; compare_method < 4; compare_method++ ) {\n    double base_base = Imgproc.compareHist( hist_base, hist_base, compare_method );\n    double base_half = Imgproc.compareHist( hist_base, hist_half_down, compare_method );\n    double base_test1 = Imgproc.compareHist( hist_base, hist_test1, compare_method );\n    double base_test2 = Imgproc.compareHist( hist_base, hist_test2, compare_method );\n    System.out.println(\"Method [\" + compare_method + \"] Perfect, Base-Half, Base-Test(1), Base-Test(2) : \"\n                      + base_base + \" / \" + base_half + \" / \" + base_test1 + \" / \" + base_test2);\n}\n```\n\nLANGUAGE: python\nCODE:\n```\nfor compare_method in range(4):\n    base_base = cv.compareHist(hist_base, hist_base, compare_method)\n    base_half = cv.compareHist(hist_base, hist_half_down, compare_method)\n    base_test1 = cv.compareHist(hist_base, hist_test1, compare_method)\n    base_test2 = cv.compareHist(hist_base, hist_test2, compare_method)\n    print('Method:', compare_method, 'Perfect, Base-Half, Base-Test(1), Base-Test(2) :',\\\n          base_base, '/', base_half, '/', base_test1, '/', base_test2)\n```\n\n----------------------------------------\n\nTITLE: Finding Minimum and Maximum Match Values/Locations (Java)\nDESCRIPTION: Utilizes `Core.minMaxLoc` to identify the minimum and maximum correlation values in the normalized result matrix (`result`) and their respective coordinates. The results (min/max values and points) are returned in a `MinMaxLocResult` object.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_29\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/tutorial_template_matching/MatchTemplateDemo.java best_match\n```\n\n----------------------------------------\n\nTITLE: Calculating Contour Aspect Ratio in OpenCV Python\nDESCRIPTION: This snippet calculates the aspect ratio of a contour. It first finds the upright bounding rectangle of the contour using `cv.boundingRect` to get its width (w) and height (h). The aspect ratio is then computed as the ratio of width to height. Requires an existing contour variable `cnt`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_properties/py_contour_properties.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nx,y,w,h = cv.boundingRect(cnt)\naspect_ratio = float(w)/h\n```\n\n----------------------------------------\n\nTITLE: Downsampling Image using pyrDown in Python\nDESCRIPTION: Downsamples the input image `tmp` using `cv.pyrDown`. The output image `dst` will be half the dimensions of the input. The destination size is inferred by the function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n        elif c == ord('o'):\n            dst = cv.pyrDown(tmp)\n            print ('** Zoom Out: Image / 2')\n```\n\n----------------------------------------\n\nTITLE: Detecting Circles using Hough Transform\nDESCRIPTION: Applying HoughCircles transform to detect circles in the preprocessed image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_circle/hough_circle.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nvector<Vec3f> circles;\nHoughCircles(gray, circles, HOUGH_GRADIENT, 1,\n            gray.rows/16,  // change this value to detect circles with different distances to each other\n            100, 30, 1, 30 // change the last two parameters\n            // (min_radius & max_radius) to detect larger circles\n);\n```\n\nLANGUAGE: Java\nCODE:\n```\nMat circles = new Mat();\nImgproc.HoughCircles(gray, circles, Imgproc.HOUGH_GRADIENT, 1.0,\n        (double)gray.rows()/16, // change this value to detect circles with different distances to each other\n        100.0, 30.0, 1, 30); // change the last two parameters\n        // (min_radius & max_radius) to detect larger circles\n```\n\nLANGUAGE: Python\nCODE:\n```\ncircles = cv.HoughCircles(gray, cv.HOUGH_GRADIENT, 1, gray.shape[0]/16,\n                            param1=100, param2=30,\n                            minRadius=1, maxRadius=30)\n```\n\n----------------------------------------\n\nTITLE: Extracting and Matching SIFT Features with FLANN in OpenCV (Python)\nDESCRIPTION: This snippet detects SIFT keypoints and descriptors in two images, then matches them using FLANN-based matcher. It applies Lowe's ratio test to retain only robust matches. Dependencies: OpenCV (with SIFT module), NumPy, Matplotlib. Key parameters include image file paths, matcher index and parameters, and Lowe's ratio threshold. Input images must be grayscale. Outputs: lists of keypoints and filtered good matches. This code is foundational for subsequent geometric analysis.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\\nimport cv2 as cv\\nfrom matplotlib import pyplot as plt\\n\\nMIN_MATCH_COUNT = 10\\n\\nimg1 = cv.imread('box.png', cv.IMREAD_GRAYSCALE)          # queryImage\\nimg2 = cv.imread('box_in_scene.png', cv.IMREAD_GRAYSCALE) # trainImage\\n\\n# Initiate SIFT detector\\nsift = cv.SIFT_create()\\n\\n# find the keypoints and descriptors with SIFT\\nkp1, des1 = sift.detectAndCompute(img1,None)\\nkp2, des2 = sift.detectAndCompute(img2,None)\\n\\nFLANN_INDEX_KDTREE = 1\\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\\nsearch_params = dict(checks = 50)\\n\\nflann = cv.FlannBasedMatcher(index_params, search_params)\\n\\nmatches = flann.knnMatch(des1,des2,k=2)\\n\\n# store all the good matches as per Lowe's ratio test.\\ngood = []\\nfor m,n in matches:\\n    if m.distance < 0.7*n.distance:\\n        good.append(m)\n```\n\n----------------------------------------\n\nTITLE: Installing to System Location with Elevated Privileges - Shell\nDESCRIPTION: Executes the CMake install step using 'sudo' to achieve the required permissions for writing to system directories (such as /usr/local). This is necessary when the installation prefix is set to a system-wide location. Prerequisites include existing build artifacts and correctly configured CMake project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\nsudo cmake --build . --target install\n```\n\n----------------------------------------\n\nTITLE: Basic Hough Line Transform Implementation in Python OpenCV\nDESCRIPTION: Demonstrates the implementation of standard Hough Transform for line detection using cv.HoughLines(). The function detects lines in a binary image by converting them to rho-theta parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_houghlines/py_houghlines.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n@include hough_line_transform.py\n```\n\n----------------------------------------\n\nTITLE: Generating an ArUco Board Image for Printing in C++\nDESCRIPTION: Code snippet demonstrating how to generate an image of an ArUco board for printing. The function takes the board dimensions, output image, margin, and border size parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_board_detection/aruco_board_detection.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ncv::Size imageSize;\nimageSize.width = 600;\nimageSize.height = 500;\n\n// create a synthetic image for a board\ncv::Mat boardImage;\nboard->generateImage(imageSize, boardImage, 10, 1);\n```\n\n----------------------------------------\n\nTITLE: Establishing Histogram Bin Count in C++\nDESCRIPTION: C++ snippet defining the number of bins to be used for the histogram calculation. A variable `histSize` is declared and initialized, typically to 256 for an 8-bit grayscale or single-channel image, representing one bin for each possible intensity value.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/calcHist_Demo.cpp Establish the number of bins\n```\n\n----------------------------------------\n\nTITLE: Implementing Meanshift Object Tracking in C++\nDESCRIPTION: Implementation of the Meanshift algorithm for object tracking in videos using OpenCV C++. The code sets up target histogram calculation, performs histogram backprojection, and applies the meanshift algorithm to track objects across video frames.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/meanshift.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include \"opencv2/imgproc.hpp\"\\n#include \"opencv2/videoio.hpp\"\\n#include \"opencv2/highgui.hpp\"\\n#include <iostream>\\n\\nusing namespace cv;\\nusing namespace std;\\n\\nMat frame, hsv, hue, mask, hist, histimg = Mat::zeros(200, 320, CV_8UC3), backproj;\\nint main( int argc, char** argv )\\n{\\n    VideoCapture cap;\\n    Rect trackWindow;\\n    int hsize = 16;\\n    float hranges[] = {0,180};\\n    const float* phranges = hranges;\\n\\n    cap.open( samples::findFile(argc > 1 ? argv[1] : \\\"video.avi\\\") );\\n\\n    if( !cap.isOpened() )\\n    {\\n        cout << \\\"Could not initialize capturing...\\\" << endl;\\n        return 0;\\n    }\\n\\n    namedWindow( \\\"Histogram\\\", 0 );\\n    namedWindow( \\\"CamShift Demo\\\", 0 );\\n    setMouseCallback( \\\"CamShift Demo\\\", onMouse, 0 );\\n    createTrackbar( \\\"Vmin\\\", \\\"CamShift Demo\\\", &vmin, 256, 0 );\\n    createTrackbar( \\\"Vmax\\\", \\\"CamShift Demo\\\", &vmax, 256, 0 );\\n    createTrackbar( \\\"Smin\\\", \\\"CamShift Demo\\\", &smin, 256, 0 );\\n\\n    for(;;)\\n    {\\n        cap >> frame;\\n        if( frame.empty() )\\n            break;\\n\\n        frame.copyTo(image);\\n        cvtColor(image, hsv, COLOR_BGR2HSV);\\n\\n        if( trackObject )\\n        {\\n            int _vmin = vmin, _vmax = vmax;\\n\\n            inRange(hsv, Scalar(0, smin, MIN(_vmin,_vmax)),\\n                    Scalar(180, 256, MAX(_vmin, _vmax)), mask);\\n            int ch[] = {0, 0};\\n            hue.create(hsv.size(), hsv.depth());\\n            mixChannels(&hsv, 1, &hue, 1, ch, 1);\\n\\n            if( trackObject < 0 )\\n            {\\n                Mat roi(hue, selection), maskroi(mask, selection);\\n                calcHist(&roi, 1, 0, maskroi, hist, 1, &hsize, &phranges);\\n                normalize(hist, hist, 0, 255, NORM_MINMAX);\\n\\n                trackWindow = selection;\\n                trackObject = 1;\\n\\n                histimg = Scalar::all(0);\\n                int binW = histimg.cols / hsize;\\n                Mat buf(1, hsize, CV_8UC3);\\n                for( int i = 0; i < hsize; i++ )\\n                    buf.at<Vec3b>(i) = Vec3b(saturate_cast<uchar>(i*180./hsize), 255, 255);\\n                cvtColor(buf, buf, COLOR_HSV2BGR);\\n\\n                for( int i = 0; i < hsize; i++ )\\n                {\\n                    int val = saturate_cast<int>(hist.at<float>(i)*histimg.rows/255);\\n                    rectangle( histimg, Point(i*binW,histimg.rows),\\n                            Point((i+1)*binW,histimg.rows - val),\\n                            Scalar(buf.at<Vec3b>(i)), -1, 8 );\\n                }\\n            }\\n\\n            calcBackProject(&hue, 1, 0, hist, backproj, &phranges);\\n            backproj &= mask;\\n            meanShift(backproj, trackWindow,\\n                        TermCriteria( TermCriteria::EPS | TermCriteria::COUNT, 10, 1 ));\\n            rectangle(image, trackWindow, Scalar(0,0,255), 3, LINE_AA);\\n        }\\n\\n        if( selectObject && selection.width > 0 && selection.height > 0 )\\n        {\\n            Mat roi(image, selection);\\n            bitwise_not(roi, roi);\\n        }\\n\\n        imshow( \\\"CamShift Demo\\\", image );\\n        imshow( \\\"Histogram\\\", histimg );\\n\\n        char c = (char)waitKey(30);\\n        if( c == 27 )\\n            break;\\n        switch(c)\\n        {\\n        case 'b':\\n            backprojMode = !backprojMode;\\n            break;\\n        case 'c':\\n            trackObject = 0;\\n            histimg = Scalar::all(0);\\n            break;\\n        case 'h':\\n            showHist = !showHist;\\n            if( !showHist )\\n                destroyWindow( \\\"Histogram\\\" );\\n            else\\n                namedWindow( \\\"Histogram\\\", 1 );\\n            break;\\n        case 'p':\\n            paused = !paused;\\n            break;\\n        default:\\n            ;\\n        }\\n    }\\n\\n    return 0;\\n}\n```\n\n----------------------------------------\n\nTITLE: Color Quantization Using K-Means Clustering in OpenCV\nDESCRIPTION: This code performs color quantization on an image using K-means clustering. It reduces the number of colors in the image to K=8 by treating each pixel's RGB values as a 3D point, clustering these points, and then replacing each pixel's color with its cluster centroid.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.markdown#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nimg = cv.imread('home.jpg')\nZ = img.reshape((-1,3))\n\n# convert to np.float32\nZ = np.float32(Z)\n\n# define criteria, number of clusters(K) and apply kmeans()\ncriteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\nK = 8\nret,label,center=cv.kmeans(Z,K,None,criteria,10,cv.KMEANS_RANDOM_CENTERS)\n\n# Now convert back into uint8, and make original image\ncenter = np.uint8(center)\nres = center[label.flatten()]\nres2 = res.reshape((img.shape))\n\ncv.imshow('res2',res2)\ncv.waitKey(0)\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Applying the Probabilistic Hough Line Transform in OpenCV (Java)\nDESCRIPTION: This Java snippet uses Imgproc.HoughLinesP to find line segments with OpenCV's probabilistic Hough Transform. It requires the binary edge image and parameters for thresholds and line criteria. Outputs a matrix of line segments as (x1, y1, x2, y2).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nMat linesP = new Mat();\\nImgproc.HoughLinesP(edges, linesP, 1, Math.PI/180, 50, 50, 10);\\n\n```\n\n----------------------------------------\n\nTITLE: Performing Inverse Fourier Transform using Numpy IFFT\nDESCRIPTION: This code performs inverse Fourier Transform on an image using Numpy's IFFT function after high-pass filtering in the frequency domain. It assumes prior use of FFT and shifting. The snippet removes low frequencies from the transformed data, then uses inverse shifting and IFFT to reconstruct the image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nrows, cols = img.shape\ncrow, ccol = rows//2, cols//2\nfshift[crow-30:crow+31, ccol-30:ccol+31] = 0\nf_ishift = np.fft.ifftshift(fshift)\nimg_back = np.fft.ifft2(f_ishift)\nimg_back = np.real(img_back)\n\nplt.subplot(131),plt.imshow(img, cmap = 'gray')\nplt.title('Input Image'), plt.xticks([]), plt.yticks([])\nplt.subplot(132),plt.imshow(img_back, cmap = 'gray')\nplt.title('Image after HPF'), plt.xticks([]), plt.yticks([])\nplt.subplot(133),plt.imshow(img_back)\nplt.title('Result in JET'), plt.xticks([]), plt.yticks([])\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: SURF Feature Description and Matching in C++\nDESCRIPTION: Demonstrates using SURF for feature description, BruteForce matcher for matching, and drawMatches for visualization in C++. Requires OpenCV contrib modules for SURF features.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_description/feature_description.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <iostream>\n#include <opencv2/core.hpp>\n#include <opencv2/highgui.hpp>\n#include <opencv2/features2d.hpp>\n#include <opencv2/xfeatures2d.hpp>\n\nusing namespace cv;\nusing namespace cv::xfeatures2d;\nusing std::cout;\nusing std::endl;\n\nint main( int argc, char* argv[] )\n{\n    CommandLineParser parser( argc, argv, \"{@input1 | box.png | input image 1 }\"\n                                        \"{@input2 | box_in_scene.png | input image 2 }\"\n                                        \"{@output |matches.jpg | output image }\"\n                                        \"{help h |  | show help message}\" );\n    if (parser.has(\"help\"))\n    {\n        parser.printMessage();\n        return 0;\n    }\n\n    Mat img1 = imread( samples::findFile( parser.get<String>(\"@input1\") ), IMREAD_GRAYSCALE );\n    Mat img2 = imread( samples::findFile( parser.get<String>(\"@input2\") ), IMREAD_GRAYSCALE );\n    if ( img1.empty() || img2.empty() )\n    {\n        cout << \"Could not open or find the image!\\n\" << endl;\n        parser.printMessage();\n        return -1;\n    }\n\n    //-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\n    int minHessian = 400;\n    Ptr<SURF> detector = SURF::create( minHessian );\n    std::vector<KeyPoint> keypoints1, keypoints2;\n    Mat descriptors1, descriptors2;\n    detector->detectAndCompute( img1, noArray(), keypoints1, descriptors1 );\n    detector->detectAndCompute( img2, noArray(), keypoints2, descriptors2 );\n\n    //-- Step 2: Matching descriptor vectors with a brute force matcher\n    // Since SURF is a floating-point descriptor NORM_L2 is used\n    Ptr<DescriptorMatcher> matcher = DescriptorMatcher::create(DescriptorMatcher::BRUTEFORCE);\n    std::vector< DMatch > matches;\n    matcher->match( descriptors1, descriptors2, matches );\n\n    //-- Draw matches\n    Mat img_matches;\n    drawMatches( img1, keypoints1, img2, keypoints2, matches, img_matches );\n\n    //-- Show detected matches\n    imshow(\"Matches\", img_matches );\n\n    imwrite(parser.get<String>(\"@output\"), img_matches);\n\n    waitKey();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Trackbar for Upper HSV Range in C++\nDESCRIPTION: Controls the upper HSV range for image thresholding in an OpenCV C++ application. Enables real-time adjustment of threshold parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold_inRange/threshold_inRange.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nint high_h = 179;\\ncv::createTrackbar(\"High H\", \"Control\", &high_h, 179);\n```\n\n----------------------------------------\n\nTITLE: Applying Histogram Equalization with cv.equalizeHist in JavaScript\nDESCRIPTION: Demonstrates the usage of the `cv.equalizeHist` function in OpenCV.js to perform histogram equalization. This function takes an 8-bit single-channel source image (`src`) and produces a destination image (`dst`) of the same size and type with equalized histogram, typically improving image contrast. It stretches the pixel intensity distribution across the full range.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_histograms/js_histogram_equalization/js_histogram_equalization.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ncv.equalizeHist (src, dst)\n```\n\n----------------------------------------\n\nTITLE: List of OpenCV Debug Libraries\nDESCRIPTION: Provides a full list of OpenCV debug libraries to be added as additional dependencies in Visual Studio.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_visual_studio_opencv/windows_visual_studio_opencv.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nopencv_calib3d300d.lib\nopencv_core300d.lib\nopencv_features2d300d.lib\nopencv_flann300d.lib\nopencv_highgui300d.lib\nopencv_imgcodecs300d.lib\nopencv_imgproc300d.lib\nopencv_ml300d.lib\nopencv_objdetect300d.lib\nopencv_photo300d.lib\nopencv_shape300d.lib\nopencv_stitching300d.lib\nopencv_superres300d.lib\nopencv_ts300d.lib\nopencv_video300d.lib\nopencv_videoio300d.lib\nopencv_videostab300d.lib\n```\n\n----------------------------------------\n\nTITLE: Drawing Minimum Enclosing Circle for Contours in Python with OpenCV\nDESCRIPTION: This code snippet shows how to find and draw the minimum enclosing circle for a contour using cv.minEnclosingCircle() and cv.circle() functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n(x,y),radius = cv.minEnclosingCircle(cnt)\ncenter = (int(x),int(y))\nradius = int(radius)\ncv.circle(img,center,radius,(0,255,0),2)\n```\n\n----------------------------------------\n\nTITLE: Loading and Converting Input Image (C++)\nDESCRIPTION: This snippet demonstrates loading an image in OpenCV C++ and converting it from BGR to grayscale using cv::imread and cv::cvtColor. If loading fails, the program prints an error and exits. This code must be used before thresholding and requires OpenCV's core and imgproc modules. Input: image file path. Output: grayscale Mat in src_gray.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n// [load]\\nsrc = imread( argc >=2 ? argv[1] : \"chicky_512.png\", IMREAD_COLOR );\\nif( src.empty() )\\n{\\n    printf(\"Error opening image\\n\");\\n    return -1;\\n}\\ncvtColor( src, src_gray, COLOR_BGR2GRAY );\\n// [load]\n```\n\n----------------------------------------\n\nTITLE: Loading and Converting Input Image (Python)\nDESCRIPTION: This Python snippet uses cv.imread to load an image and cv.cvtColor to convert it to grayscale, handling missing files by printing an error and exiting. This is preparatory for thresholding and depends on OpenCV-Python. Input: filename from command line or default. Output: src_gray as a grayscale np.ndarray.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# [load]\\nsrc = cv.imread(sys.argv[1] if len(sys.argv) > 1 else 'chicky_512.png')\\nif src is None:\\n    print('Error opening image')\\n    sys.exit(-1)\\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\\n# [load]\n```\n\n----------------------------------------\n\nTITLE: Implementing Camshift Object Tracking in Java\nDESCRIPTION: Java implementation of the Camshift algorithm for adaptive object tracking. This implementation extends the Meanshift approach to include adaptive window sizing and rotation, providing better tracking for objects that change in size and orientation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/meanshift.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\nimport java.util.List;\\nimport javax.swing.*;\\n\\nimport org.opencv.core.*;\\nimport org.opencv.highgui.*;\\nimport org.opencv.imgproc.Imgproc;\\nimport org.opencv.video.Video;\\nimport org.opencv.videoio.VideoCapture;\\nimport org.opencv.videoio.Videoio;\\n\\nclass CamshiftDemo {\\n    public void run(String[] args) {\\n        String filename = args.length > 0 ? args[0] : \\\"../data/slow_traffic_small.mp4\\\";\\n\\n        VideoCapture capture = new VideoCapture(filename);\\n        if (!capture.isOpened()) {\\n            System.out.println(\\\"Could not initialize capturing...\\\\n\\\");\\n            System.exit(0);\\n        }\\n\\n        Mat frame = new Mat();\\n        capture.read(frame);\\n\\n        if (frame.empty()) {\\n            System.out.println(\\\"No captured frame -- Break!\\\");\\n            System.exit(0);\\n        }\\n\\n        // setup initial location of window\\n        Rect trackWindow = new Rect(300, 200, 100, 50);\\n\\n        // set up the ROI for tracking\\n        Mat roi = frame.submat(trackWindow);\\n        Mat hsvRoi = new Mat();\\n        Imgproc.cvtColor(roi, hsvRoi, Imgproc.COLOR_BGR2HSV);\\n\\n        Mat mask = new Mat();\\n        Core.inRange(hsvRoi, new Scalar(0, 60, 32), new Scalar(180, 255, 255), mask);\\n\\n        Mat roiHist = new Mat();\\n        int[] channels = {0};\\n        int[] histSize = {180};\\n        float[] ranges = {0, 180};\\n        float[][] histRanges = {ranges};\\n\\n        Imgproc.calcHist(List.of(hsvRoi), new MatOfInt(channels), mask, roiHist, new MatOfInt(histSize), new MatOfFloat(ranges));\\n        Core.normalize(roiHist, roiHist, 0, 255, Core.NORM_MINMAX);\\n\\n        // Setup the termination criteria, either 10 iteration or move by at least 1 pt\\n        TermCriteria termCrit = new TermCriteria(TermCriteria.EPS | TermCriteria.COUNT, 10, 1);\\n\\n        JFrame jframe = new JFrame(\\\"Camshift Demo\\\");\\n        jframe.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\\n        JLabel vidPanel = new JLabel();\\n        jframe.setContentPane(vidPanel);\\n        jframe.setSize(frame.cols(), frame.rows());\\n        jframe.setVisible(true);\\n\\n        Mat hsv = new Mat();\\n        Mat backProject = new Mat();\\n        while (true) {\\n            capture.read(frame);\\n            if (frame.empty()) {\\n                break;\\n            }\\n\\n            Imgproc.cvtColor(frame, hsv, Imgproc.COLOR_BGR2HSV);\\n            Imgproc.calcBackProject(List.of(hsv), new MatOfInt(channels), roiHist, backProject, new MatOfFloat(histRanges), 1);\\n\\n            // apply CamShift to get the new location\\n            RotatedRect rotatedRect = Video.CamShift(backProject, trackWindow, termCrit);\\n\\n            // Draw it on image\\n            Point[] points = new Point[4];\\n            rotatedRect.points(points);\\n            MatOfPoint matPts = new MatOfPoint();\\n            matPts.fromArray(points);\\n            Imgproc.polylines(frame, List.of(matPts), true, new Scalar(0, 0, 255), 2);\\n\\n            ImageIcon image = new ImageIcon(Mat2BufferedImage.getImage(frame));\\n            vidPanel.setIcon(image);\\n            vidPanel.repaint();\\n        }\\n        System.out.println(\\\"Video processing done!\\\\n\\\");\\n    }\\n\\n    public static void main(String[] args) {\\n        // Load the native OpenCV library\\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\\n        new CamshiftDemo().run(args);\\n    }\\n}\n```\n\n----------------------------------------\n\nTITLE: SURF Feature Description and Matching in Python\nDESCRIPTION: Demonstrates using SURF for feature description, BruteForce matcher for matching, and drawMatches for visualization in Python. Requires OpenCV contrib modules for SURF features.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_description/feature_description.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nimport sys\n\nif len(sys.argv) != 3:\n    print('Please specify two images: SURF_matching_Demo.py <image1> <image2>')\n    sys.exit()\n\nimg1 = cv.imread(cv.samples.findFile(sys.argv[1]), cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread(cv.samples.findFile(sys.argv[2]), cv.IMREAD_GRAYSCALE)\nif img1 is None or img2 is None:\n    print('Could not open or find the images!')\n    sys.exit()\n\n#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\nminHessian = 400\ndetector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\nkeypoints1, descriptors1 = detector.detectAndCompute(img1, None)\nkeypoints2, descriptors2 = detector.detectAndCompute(img2, None)\n\n#-- Step 2: Matching descriptor vectors with a FLANN based matcher\n# Since SURF is a floating-point descriptor NORM_L2 is used\nmatcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED)\nknn_matches = matcher.knnMatch(descriptors1, descriptors2, 2)\n\n#-- Filter matches using the Lowe's ratio test\nratio_thresh = 0.7\ngood_matches = []\nfor m,n in knn_matches:\n    if m.distance < ratio_thresh * n.distance:\n        good_matches.append(m)\n\n#-- Draw matches\nimg_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\ncv.drawMatches(img1, keypoints1, img2, keypoints2, good_matches, img_matches, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n\n#-- Show detected matches\ncv.imshow('Good Matches', img_matches)\n\ncv.waitKey()\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Implementing Pixel Comparison Decision Tree for Corner Detection in C++\nDESCRIPTION: This code snippet implements a decision tree for corner detection based on pixel intensity comparisons. It uses pointer arithmetic to access pixel offsets and branches between 'is_a_corner' and 'is_not_a_corner' labels based on comparing values against threshold variables c_b and cb.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_29\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset10] < c_b)\n  if(ptr[offset4] < c_b)\n    goto is_a_corner;\n  else\n    if(ptr[offset11] < c_b)\n      goto is_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset11] < c_b)\n    if(ptr[offset3] < c_b)\n      if(ptr[offset4] < c_b)\n        goto is_a_corner;\n      else\n        if(ptr[offset10] < c_b)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      if(ptr[offset8] < c_b)\n        if(ptr[offset10] < c_b)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset6] > cb)\n    goto is_not_a_corner;\n  else\n    if(ptr[offset6] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset4] < c_b)\n          if(ptr[offset3] < c_b)\n            goto is_a_corner;\n          else\n            if(ptr[offset10] < c_b)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  if(ptr[offset1] > cb)\n    if(ptr[offset6] > cb)\n      goto is_not_a_corner;\n    else\n      if(ptr[offset6] < c_b)\n        if(ptr[offset8] < c_b)\n          if(ptr[offset4] < c_b)\n            if(ptr[offset3] < c_b)\n              goto is_a_corner;\n            else\n              if(ptr[offset10] < c_b)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    if(ptr[offset1] < c_b)\n      if(ptr[offset6] > cb)\n        if(ptr[offset8] < c_b)\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset8] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset3] < c_b)\n                goto is_a_corner;\n              else\n                if(ptr[offset10] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          if(ptr[offset8] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n    else\n      if(ptr[offset6] > cb)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset8] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset3] < c_b)\n                goto is_a_corner;\n              else\n                if(ptr[offset10] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\nelse\n  if(ptr[offset2] > cb)\n    goto is_not_a_corner;\n  else\n    if(ptr[offset2] < c_b)\n      if(ptr[offset1] > cb)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] > cb)\n            if(ptr[offset11] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset4] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset10] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset4] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Configuring Halide Layer Scheduling via YAML in OpenCV DNN\nDESCRIPTION: This YAML snippet demonstrates how to provide scheduling directives for specific Halide functions (layers like `relu1` and `conv1_constant_exterior`) within an OpenCV DNN network. It shows the use of directives like `reorder`, `split`, `parallel`, `unroll`, `vectorize`, and `compute_at` to optimize performance. This configuration file is intended to be passed to `cv::dnn::Net::setHalideScheduler`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_halide_scheduling/dnn_halide_scheduling.markdown#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nrelu1:\n  reorder: [x, c, y]\n  split: { y: 2, c: 8 }\n  parallel: [yo, co]\n  unroll: yi\n  vectorize: { x: 4 }\nconv1_constant_exterior:\n  compute_at: { relu1: yi }\n```\n\n----------------------------------------\n\nTITLE: Running SSD MobileNetV1 Inference via Model Alias - Console\nDESCRIPTION: This command runs 'object_detection.py' using a model alias from models.yml, specifying the input test image. Expects that configuration files and model weights are specified under the alias. Produces detection output on the sample image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial.md#2025-04-22_snippet_7\n\nLANGUAGE: console\nCODE:\n```\npython object_detection.py ssd_tf --input ../data/pexels_double_decker_bus.jpg\n```\n\n----------------------------------------\n\nTITLE: SURF Feature Description and Matching in Java\nDESCRIPTION: Demonstrates using SURF for feature description, BruteForce matcher for matching, and drawMatches for visualization in Java. Requires OpenCV contrib modules for SURF features.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_description/feature_description.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.Core;\nimport org.opencv.core.Mat;\nimport org.opencv.core.MatOfDMatch;\nimport org.opencv.core.MatOfKeyPoint;\nimport org.opencv.features2d.DescriptorMatcher;\nimport org.opencv.features2d.Features2d;\nimport org.opencv.imgcodecs.Imgcodecs;\nimport org.opencv.xfeatures2d.SURF;\n\nclass SURFMatchingDemo {\n    public void run() {\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\n        String filename1 = \"../data/box.png\";\n        String filename2 = \"../data/box_in_scene.png\";\n        Mat img1 = Imgcodecs.imread(filename1, Imgcodecs.IMREAD_GRAYSCALE);\n        Mat img2 = Imgcodecs.imread(filename2, Imgcodecs.IMREAD_GRAYSCALE);\n        if (img1.empty() || img2.empty()) {\n            System.err.println(\"Cannot read images!\");\n            System.exit(0);\n        }\n\n        //-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\n        int minHessian = 400;\n        SURF detector = SURF.create(minHessian);\n        MatOfKeyPoint keypoints1 = new MatOfKeyPoint(), keypoints2 = new MatOfKeyPoint();\n        Mat descriptors1 = new Mat(), descriptors2 = new Mat();\n        detector.detectAndCompute(img1, new Mat(), keypoints1, descriptors1);\n        detector.detectAndCompute(img2, new Mat(), keypoints2, descriptors2);\n\n        //-- Step 2: Matching descriptor vectors with a brute force matcher\n        // Since SURF is a floating-point descriptor NORM_L2 is used\n        DescriptorMatcher matcher = DescriptorMatcher.create(DescriptorMatcher.BRUTEFORCE);\n        MatOfDMatch matches = new MatOfDMatch();\n        matcher.match(descriptors1, descriptors2, matches);\n\n        //-- Draw matches\n        Mat imgMatches = new Mat();\n        Features2d.drawMatches(img1, keypoints1, img2, keypoints2, matches, imgMatches);\n\n        //-- Show detected matches\n        Imgcodecs.imwrite(\"SURFmatchesDemo.jpg\", imgMatches);\n    }\n\n    public static void main(String[] args) {\n        new SURFMatchingDemo().run();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Multi-Stream Data Capture with RealSense\nDESCRIPTION: Shows how to capture multiple data streams (depth map, RGB image, and IR image) simultaneously using VideoCapture's grab and retrieve methods.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/intelperc.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n    VideoCapture capture(CAP_REALSENSE);\n    for(;;)\n    {\n        Mat depthMap;\n        Mat image;\n        Mat irImage;\n\n        capture.grab();\n\n        capture.retrieve( depthMap, CAP_INTELPERC_DEPTH_MAP );\n        capture.retrieve(    image, CAP_INTELPERC_IMAGE );\n        capture.retrieve(  irImage, CAP_INTELPERC_IR_MAP);\n\n        if( waitKey( 30 ) >= 0 )\n            break;\n    }\n```\n\n----------------------------------------\n\nTITLE: Loading and Transforming Image in Java with OpenCV\nDESCRIPTION: This snippet shows Java code leveraging OpenCV to load an image, calculate an affine transformation matrix from three points, apply this transformation, and subsequently rotate the image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/warp_affine/warp_affine.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgTrans/warp_affine/GeometricTransformsDemo.java Load the image\n```\n\n----------------------------------------\n\nTITLE: Defining Segmentation Module Test Configuration in Python\nDESCRIPTION: This Python data class specifies configuration parameters for testing segmentation models, allowing customization of image paths, scaling, and color preprocessing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass TestSegmModuleConfig:\n    segm_test_data_dir: str = \"test_data/sem_segm\"\n    test_module_name: str = \"segmentation\"\n    test_module_path: str = \"segmentation.py\"\n    input_img: str = os.path.join(segm_test_data_dir, \"2007_000033.jpg\")\n    model: str = \"\"\n\n    frame_height: str = str(TestSegmConfig.frame_size)\n    frame_width: str = str(TestSegmConfig.frame_size)\n    scale: float = 1.0\n    mean: List[float] = field(default_factory=lambda: [0.0, 0.0, 0.0])\n    std: List[float] = field(default_factory=list)\n    crop: bool = False\n    rgb: bool = True\n    classes: str = os.path.join(segm_test_data_dir, \"pascal-classes.txt\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Header and Identifier Section - Markdown/plaintext\nDESCRIPTION: Shows how to add a Level1 header and documentation identifier at the top of an OpenCV documentation page. This establishes the markdown title and a unique doxygen page identifier required for cross-referencing and documentation generation. No dependencies beyond markdown and doxygen are needed. Inputs are the title and identifier, with no outputs.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nWriting documentation for OpenCV {#tutorial_documentation}\n================================\n```\n\n----------------------------------------\n\nTITLE: Full Contour Hierarchical Representation With RETR_TREE in OpenCV Python\nDESCRIPTION: This snippet demonstrates the use of the RETR_TREE mode in OpenCV, which generates a full representation of contour hierarchies, capturing detailed parent-child-grandparent relationships among contours.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contours_hierarchy/py_contours_hierarchy.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n>>> hierarchy\narray([[[ 7, -1,  1, -1],\n        [-1, -1,  2,  0],\n        [-1, -1,  3,  1],\n        [-1, -1,  4,  2],\n        [-1, -1,  5,  3],\n        [ 6, -1, -1,  4],\n        [-1,  5, -1,  4],\n        [ 8,  0, -1, -1],\n        [-1,  7, -1, -1]]])\n```\n\n----------------------------------------\n\nTITLE: Using cv::putText with Random Parameters in OpenCV C++\nDESCRIPTION: Demonstrates calling the `cv::putText` function to draw text on an image. It uses the `rng` object to randomize several parameters: the bottom-left origin point (`org`), the font type (`rng.uniform(0,8)`), the font scale (`rng.uniform(0,100)*0.05+0.1`), the text color (`randomColor(rng)`), and the text thickness (`rng.uniform(1, 10)`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/random_generator_and_text/random_generator_and_text.markdown#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n@code{.cpp}\nputText( image, \"Testing text rendering\", org, rng.uniform(0,8),\n         rng.uniform(0,100)*0.05+0.1, randomColor(rng), rng.uniform(1, 10), lineType);\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Iterator-Based Image Scanning in OpenCV\nDESCRIPTION: This snippet demonstrates using iterators to safely traverse an image matrix in OpenCV. It simplifies skipping non-contiguous memory gaps and provides safe access to image elements, especially color channels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_scan_images/how_to_scan_images.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_scan_images.cpp scan-iterator\n```\n\n----------------------------------------\n\nTITLE: Visualizing Barcode Detection Results\nDESCRIPTION: Provides a way to visualize the results of barcode detection, showing how detected barcodes appear on the processed image. Relies on visualization libraries and functions within OpenCV for display purposes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/barcode_detect_and_decode.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/barcode.cpp visualize\n```\n\n----------------------------------------\n\nTITLE: Finding Calibration Patterns (Chessboard/Circles/ChArUco) in Image using OpenCV C++\nDESCRIPTION: This C++ code snippet reference indicates code that finds calibration patterns (chessboard, circles grid, or ChArUco) within an input image using OpenCV. It utilizes functions like `cv::findChessboardCorners`, `cv::findCirclesGrid`, or `cv::aruco::CharucoDetector::detectBoard` based on the configured pattern type, outputting the detected pattern points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/calib3d/camera_calibration/camera_calibration.cpp find_pattern\n```\n\n----------------------------------------\n\nTITLE: Converting Laplacian Output to CV_8U in Java\nDESCRIPTION: Converts the CV_16S Laplacian output (`dst`) to a CV_8U image (`absDst`) using Core.convertScaleAbs. This takes the absolute value and scales the result appropriately for display. Requires OpenCV Java bindings (Core).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\n//! [convert]\n// converting back to CV_8U\nabsDst = new Mat();\nCore.convertScaleAbs( dst, absDst );\n//! [convert]\n```\n\n----------------------------------------\n\nTITLE: Applying the Standard Hough Line Transform in OpenCV (Python)\nDESCRIPTION: This Python snippet uses OpenCV's cv2.HoughLines to detect lines from an edge-detected image. Inputs are the binary edge map, rho and theta resolutions, and a threshold; output is a list of (rho, theta) line parameters. Requires cv2 and edge detection result.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nlines = cv2.HoughLines(edges, 1, np.pi/180, 150, None, 0, 0)\\n\n```\n\n----------------------------------------\n\nTITLE: Compiling OpenCV with Custom HAL Replacement\nDESCRIPTION: This snippet provides steps to compile the OpenCV library using a custom HAL replacement. By setting the OpenCV_HAL_DIR variable, users can specify the directory of the compiled HAL library. The subsequent make command will use this HAL during the build process, with an example command to run a specific performance test for logical 'bitwise_and' operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/hal/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncmake \\\n    -DOpenCV_HAL_DIR=\"<home-dir>/my-hal-build/\" \\\n    <opencv-src>\n```\n\nLANGUAGE: shell\nCODE:\n```\nmake\n```\n\nLANGUAGE: shell\nCODE:\n```\n./bin/opencv_perf_core --gtest_filter=*bitwise_and*\n```\n\n----------------------------------------\n\nTITLE: Sample Log Output for Model Evaluation Run (Console)\nDESCRIPTION: Provides an example of log output upon running the evaluation script, detailing input data and log file destinations. While not executable, this output illustrates what information is surfaced by the evaluation process. Inputs reflect paths and settings, while outputs are saved logs containing further evaluation results. No additional dependencies are required to generate this output beyond a successful script run.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n===== Running evaluation of the model with the following params:\n    * val data location: ./ILSVRC2012_img_val\n    * log file location: dnn_model_runner/dnn_conversion/logs/TF_mobilenet_log.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenNI Sensor Properties\nDESCRIPTION: Example showing how to set and get sensor properties using VideoCapture methods.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/kinect_openni.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nVideoCapture capture( CAP_OPENNI );\ncapture.set( CAP_OPENNI_IMAGE_GENERATOR_OUTPUT_MODE, CAP_OPENNI_VGA_30HZ );\ncout << \"FPS    \" << capture.get( CAP_OPENNI_IMAGE_GENERATOR+CAP_PROP_FPS ) << endl;\n```\n\n----------------------------------------\n\nTITLE: Computing Transformed Corner Coordinates with OpenCV - Python\nDESCRIPTION: Using OpenCV’s cv2.perspectiveTransform, this Python snippet maps detected chessboard corners to their new positions from homography. Expects numpy arrays and a computed 3x3 homography. Returns transformed points for overlaying or match verification. Useful for evaluating transformation accuracy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/features2D/Homography/perspective_correction.py compute-transformed-corners\n```\n\n----------------------------------------\n\nTITLE: Detecting ChArUco Diamond Markers in an Image with OpenCV ArUco (C++)\nDESCRIPTION: This snippet illustrates how to detect ChArUco diamond markers in an image using the cv::aruco::CharucoDetector::detectDiamonds() function in OpenCV C++. It requires a source image, previously detected ArUco marker corners and IDs, and a specified square-to-marker size ratio. The function outputs detected diamond corners and their corresponding IDs arrays. Dependencies include OpenCV's aruco module and prior marker detection. It expects the input chessboard image and marker data; square length and marker length parameters affect detection accuracy and must match the generated diamonds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_diamond_detection/charuco_diamond_detection.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n// Assuming markerCorners and markerIds have been obtained from prior detection\ncv::Mat image;\nstd::vector<std::vector<cv::Point2f>> markerCorners;\nstd::vector<int> markerIds;\nfloat squareMarkerLengthRate = 1.67f; // Example value: squareLength / markerLength\nstd::vector<std::vector<cv::Point2f>> diamondCorners;\nstd::vector<cv::Vec4i> diamondIds;\ncv::aruco::CharucoDetector charucoDetector(dictionary);\ncharucoDetector.detectDiamonds(image, markerCorners, markerIds,\n    diamondCorners, diamondIds, squareMarkerLengthRate);\n```\n\n----------------------------------------\n\nTITLE: Basic Image Pyramid Operations with OpenCV\nDESCRIPTION: Demonstrates how to create lower and higher resolution versions of an image using pyramid operations. Uses cv.pyrDown() to reduce resolution and cv.pyrUp() to increase resolution.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_pyramids/py_pyramids.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimg = cv.imread('messi5.jpg')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nlower_reso = cv.pyrDown(higher_reso)\n```\n\nLANGUAGE: python\nCODE:\n```\nhigher_reso2 = cv.pyrUp(lower_reso)\n```\n\n----------------------------------------\n\nTITLE: Threshold Operation Callback Function (Java)\nDESCRIPTION: This Java method re-applies the Imgproc.threshold function each time a trackbar value changes, updating the GUI window with the result. Invoked by onChange listeners. Inputs: current src_gray Mat, threshold_value, threshold_type; output: updated visualization in window.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\n// [Threshold_Demo]\\nstatic void update() {\\n    Imgproc.threshold(src_gray, dst = new Mat(), threshold_value, max_BINARY_value, threshold_type);\\n    HighGui.imshow(window_name, dst);\\n}\\n// [Threshold_Demo]\n```\n\n----------------------------------------\n\nTITLE: Downsampling Image using pyrDown in Java\nDESCRIPTION: Performs image downsampling on the `tmp` Mat object using `Imgproc.pyrDown`. The output `dst` Mat object is specified to have dimensions half those of the input.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n            } else if( c == 'o' ) {\n                Imgproc.pyrDown( tmp, dst, new Size( tmp.cols()/2, tmp.rows()/2 ) );\n                System.out.println(\"** Zoom Out: Image / 2\");\n            }\n```\n\n----------------------------------------\n\nTITLE: Applying SURF Feature Detection in Python\nDESCRIPTION: This Python script demonstrates the use of OpenCV's SURF detector for keypoint detection. It includes image loading, SURF detector initialization, keypoint detection, and visualization of the results using matplotlib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_detection/feature_detection.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\n\\nimg = cv.imread('box.png', cv.IMREAD_GRAYSCALE)\\nif img is None:\\n    print('Could not open or find the image:', 'box.png')\\n    exit(0)\\n\\n# Create SURF object. You can specify params here or later.\\n# Here I set Hessian Threshold to 400\\nsurf = cv.xfeatures2d.SURF_create(400)\\n\\n# Find keypoints and descriptors directly\\nkp = surf.detect(img,None)\\n\\nimg2 = cv.drawKeypoints(img, kp, None, (255,0,0), 4)\\n\\nplt.imshow(img2)\\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Sobel and Laplacian Operators in OpenCV Python\nDESCRIPTION: This snippet demonstrates the use of Laplacian and Sobel operators to find gradients in an image. It applies different gradient operators (Laplacian, Sobel X, and Sobel Y) to a grayscale image and visualizes the results using matplotlib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_gradients/py_gradients.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('dave.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n\nlaplacian = cv.Laplacian(img,cv.CV_64F)\nsobelx = cv.Sobel(img,cv.CV_64F,1,0,ksize=5)\nsobely = cv.Sobel(img,cv.CV_64F,0,1,ksize=5)\n\nplt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')\nplt.title('Original'), plt.xticks([]), plt.yticks([])\nplt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')\nplt.title('Laplacian'), plt.xticks([]), plt.yticks([])\nplt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')\nplt.title('Sobel X'), plt.xticks([]), plt.yticks([])\nplt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')\nplt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Face Recognizer (FaceRecognizerSF) - OpenCV DNN Python\nDESCRIPTION: This snippet demonstrates how to create a FaceRecognizerSF instance in Python using OpenCV. It takes an ONNX model, input image size, and other parameters as arguments. The output is an object ready to process face patches and extract feature vectors representing faces for recognition.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_face/dnn_face.markdown#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# Initialize FaceRecognizerSF\nrecognizer = cv2.FaceRecognizerSF_create(\n    modelPath,        # Path to face recognition .onnx model\n    \"\",               # No config file\n    (112, 112)        # Input size expected by recognition model\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Function to Draw Random Lines with OpenCV in C++\nDESCRIPTION: Defines the function `Drawing_Random_Lines` which draws a specified number (NUMBER) of lines on the input `image`. Inside a loop, it generates random start (pt1) and end (pt2) points using `rng.uniform`, selects a random color using `randomColor`, and chooses a random thickness. It uses `cv::line` to draw each line and updates the display using `cv::imshow`. `cv::waitKey` introduces a delay and checks for user interruption.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/random_generator_and_text/random_generator_and_text.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n@code{.cpp}\nint Drawing_Random_Lines( Mat image, char* window_name, RNG rng )\n{\n  int lineType = 8;\n  Point pt1, pt2;\n\n  for( int i = 0; i < NUMBER; i++ )\n  {\n   pt1.x = rng.uniform( x_1, x_2 );\n   pt1.y = rng.uniform( y_1, y_2 );\n   pt2.x = rng.uniform( x_1, x_2 );\n   pt2.y = rng.uniform( y_1, y_2 );\n\n   line( image, pt1, pt2, randomColor(rng), rng.uniform(1, 10), 8 );\n   imshow( window_name, image );\n   if( waitKey( DELAY ) >= 0 )\n   { return -1; }\n  }\n  return 0;\n}\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Padding Images with Borders in OpenCV JavaScript\nDESCRIPTION: This snippet describes cv.copyMakeBorder, a function in OpenCV.js that pads an image with additional borders. Parameters specify the source and destination arrays, border thickness on each side (top, bottom, left, right), border type, and an optional constant border value. Often used to prepare images for DFT or convolution operations. Dependencies: OpenCV.js; requires source and output Mat objects.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_transforms/js_fourier_transform/js_fourier_transform.markdown#2025-04-22_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.copyMakeBorder (src, dst, top, bottom, left, right, borderType, value = new cv.Scalar())\n```\n\n----------------------------------------\n\nTITLE: Adding OpenMP Support to OpenCV Video Module\nDESCRIPTION: Conditionally links OpenMP libraries to the video module if OpenMP support is available and its libraries are defined.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/video/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_OPENMP AND DEFINED OpenMP_CXX_LIBRARIES AND OpenMP_CXX_LIBRARIES)\n  ocv_target_link_libraries(${the_module} LINK_PRIVATE \"${OpenMP_CXX_LIBRARIES}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Computing HOG Descriptors for Digit Recognition in Python OpenCV\nDESCRIPTION: Function that calculates HOG (Histogram of Oriented Gradients) features for a digit image. Computes Sobel derivatives, gradient magnitudes and directions, and generates a 64-value feature vector.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_svm/py_svm_opencv/py_svm_opencv.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef hog(img):\n    gx = cv2.Sobel(img, cv2.CV_32F, 1, 0)\n    gy = cv2.Sobel(img, cv2.CV_32F, 0, 1)\n    mag, ang = cv2.cartToPolar(gx, gy)\n    bins = np.int32(bin_n*ang/(2*np.pi))    # quantizing binvalues in (0...16)\n    bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:]\n    mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:]\n    hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)]\n    hist = np.hstack(hists)     # hist is a 64 bit vector\n    return hist\n```\n\n----------------------------------------\n\nTITLE: Extracting 2D and 3D Correspondences in C++\nDESCRIPTION: Extracts 2D image points and corresponding 3D model points by sorting through the good matches from feature matching. Essential for applications requiring 3D spatial data alignment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n// -- Step 2: Find out the 2D/3D correspondences\n\nstd::vector<cv::Point3f> list_points3d_model_match;    // container for the model 3D coordinates found in the scene\nstd::vector<cv::Point2f> list_points2d_scene_match;    // container for the model 2D coordinates found in the scene\n\nfor(unsigned int match_index = 0; match_index < good_matches.size(); ++match_index)\n{\n    cv::Point3f point3d_model = list_points3d_model[ good_matches[match_index].trainIdx ];   // 3D point from model\n    cv::Point2f point2d_scene = keypoints_scene[ good_matches[match_index].queryIdx ].pt;    // 2D point from the scene\n    list_points3d_model_match.push_back(point3d_model);                                      // add 3D point\n    list_points2d_scene_match.push_back(point2d_scene);                                      // add 2D point\n}\n```\n\n----------------------------------------\n\nTITLE: Building libspng Static Library\nDESCRIPTION: Defines and configures the static library target for libspng. Sets compiler definitions for MSVC, links with ZLIB, and configures output properties including debug postfix and output paths.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libspng/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC)\n    add_definitions(-D_CRT_SECURE_NO_DEPRECATE)\nendif(MSVC)\n\nadd_library(${SPNG_LIBRARY} STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${spng_headers} ${spng_sources})\ntarget_link_libraries(${SPNG_LIBRARY} ${ZLIB_LIBRARIES})\n\nset_target_properties(${SPNG_LIBRARY}\n        PROPERTIES OUTPUT_NAME ${SPNG_LIBRARY}\n        DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n        COMPILE_PDB_NAME ${SPNG_LIBRARY}\n        COMPILE_PDB_NAME_DEBUG \"${SPNG_LIBRARY}${OPENCV_DEBUG_POSTFIX}\"\n        ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n        )\n\ntarget_compile_definitions(${SPNG_LIBRARY} PUBLIC SPNG_STATIC)\n```\n\n----------------------------------------\n\nTITLE: Performing Brightness and Contrast Adjustment in OpenCV (Python)\nDESCRIPTION: This Python snippet demonstrates how to adjust contrast and brightness of an image using OpenCV. The code involves reading an image, creating a new matrix with the same size and type, and applying user-specified gain (alpha) and bias (beta) using `cv.convertScaleAbs` to ensure pixel values are within valid limits.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/basic_linear_transform/basic_linear_transform.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\n\nimage = cv.imread('path/to/image')\nnew_image = np.zeros(image.shape, image.dtype)\n\nalpha = 2.2  # Contrast control\nbeta = 50    # Brightness control\n\n\ncv.convertScaleAbs(image, new_image, alpha=alpha, beta=beta)\n\ncv.imshow('Original Image', image)\ncv.imshow('New Image', new_image)\n\ncv.waitKey(0)\ncv.destroyAllWindows()\n\n```\n\n----------------------------------------\n\nTITLE: Detecting Faces and Eyes with Haar Cascades in C++ using OpenCV\nDESCRIPTION: This C++ code snippet demonstrates how to use OpenCV's CascadeClassifier to detect faces and eyes. It involves loading pre-trained Haar cascade XML files for frontal faces and eyes using `CascadeClassifier::load`. It then captures video frames, converts them to grayscale, and applies `detectMultiScale` to find face regions. Within each detected face, it further applies `detectMultiScale` to locate eyes. Finally, it draws rectangles around the detected objects and displays the processed frame. Requires OpenCV library and the specified Haar cascade XML files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/cascade_classifier.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// This tutorial code's is shown lines below. You can also download it from\n// [here](https://github.com/opencv/opencv/tree/4.x/samples/cpp/tutorial_code/objectDetection/objectDetection.cpp)\n@include samples/cpp/tutorial_code/objectDetection/objectDetection.cpp\n```\n\n----------------------------------------\n\nTITLE: Calibrating Camera with ChArUco Board in OpenCV\nDESCRIPTION: This code demonstrates how to perform camera calibration using a ChArUco board. The process involves detecting ChArUco corners across multiple viewpoints, collecting the data, and then using calibrateCamera() to compute the camera matrix and distortion coefficients.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_calibration/aruco_calibration.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstd::vector<std::vector<cv::Point2f>> allCharucoCorners;\nstd::vector<std::vector<int>> allCharucoIds;\nstd::vector<cv::Mat> rvecs, tvecs;\ncv::Size imgSize;\n```\n\nLANGUAGE: cpp\nCODE:\n```\n// Collect data from each frame\nbool found = cv::findChessboardCorners(frame, chessboardSize, corners, flags);\n// if the charuco board was found\nif(found) {\n    // Get charuco corners and ids from detected aruco markers\n    aruco::interpolateCornersCharuco(corners, ids, frame, board, charucoCorners, charucoIds);\n    // if at least one charuco corner detected\n    if(charucoIds.size() > 0) {\n        aruco::drawDetectedCornersCharuco(frameCopy, charucoCorners, charucoIds);\n        allCharucoCorners.push_back(charucoCorners);\n        allCharucoIds.push_back(charucoIds);\n    }\n}\n```\n\nLANGUAGE: cpp\nCODE:\n```\n// After all frames processed\ncv::Mat cameraMatrix, distCoeffs;\n// prepare data for calibration\nstd::vector<cv::Mat> allCharucoCornersConcatenated;\nstd::vector<int> allCharucoIdsConcatenated;\nstd::vector<int> markerCounterPerFrame;\n\nmarkerCounterPerFrame.reserve(allCharucoCorners.size());\nfor(unsigned int i = 0; i < allCharucoCorners.size(); i++) {\n    markerCounterPerFrame.push_back((int)allCharucoCorners[i].size());\n    for(unsigned int j = 0; j < allCharucoCorners[i].size(); j++) {\n        allCharucoCornersConcatenated.push_back(allCharucoCorners[i][j]);\n        allCharucoIdsConcatenated.push_back(allCharucoIds[i][j]);\n    }\n}\n\n// calibrate camera using charuco\ndouble repError = cv::aruco::calibrateCameraCharuco(allCharucoCorners, allCharucoIds,\n                                                  board, imgSize, cameraMatrix, distCoeffs,\n                                                  rvecs, tvecs, calibrationFlags);\n\n// print calibration results\nstd::cout << \"Rep Error: \" << repError << std::endl;\nstd::cout << \"Calibration matrix: \" << cameraMatrix << std::endl;\nstd::cout << \"Distortion coefficients: \" << distCoeffs << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Converting Image to Grayscale in Java\nDESCRIPTION: Converts the blurred source image to grayscale using Imgproc.cvtColor with the COLOR_BGR2GRAY conversion code. The output is stored in the `srcGray` Mat object. Requires OpenCV Java bindings (Imgproc).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n//! [convert_to_gray]\n// Convert the image to grayscale\nsrcGray = new Mat();\nImgproc.cvtColor( src, srcGray, Imgproc.COLOR_BGR2GRAY );\n//! [convert_to_gray]\n```\n\n----------------------------------------\n\nTITLE: Template Matching Formula: TM_CCORR (LaTeX)\nDESCRIPTION: Mathematical formula for the Cross Correlation (TM_CCORR) template matching method used in OpenCV's `matchTemplate` function. R(x,y) is the result, T is the template, and I is the image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_3\n\nLANGUAGE: latex\nCODE:\n```\n\\f[R(x,y)= \\sum _{x',y'} (T(x',y')  \\cdot I(x+x',y+y'))\\f]\n```\n\n----------------------------------------\n\nTITLE: Normalizing Template Matching Results (C++)\nDESCRIPTION: Normalizes the raw result matrix obtained from `matchTemplate` to the range [0, 1] using `cv::normalize`. This makes the result easier to visualize and interpret, regardless of the matching method used. Normalization type `NORM_MINMAX` is employed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_25\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp normalize\n```\n\n----------------------------------------\n\nTITLE: Accessing Single Pixel Intensity Value in Grayscale Image with OpenCV in Python\nDESCRIPTION: Demonstrates pixel value access for a single-channel image as a NumPy array in Python with OpenCV. Direct 2D array indexing is used: img[y, x]. Input indices must be valid for the image. Result is an integer intensity between 0-255 in most cases.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nintensity = img[y, x]\n```\n\n----------------------------------------\n\nTITLE: Initializing Camera in viewDidLoad (Objective-C)\nDESCRIPTION: This snippet shows how to initialize the CvVideoCamera in the viewDidLoad method, setting various properties like device position, session preset, and orientation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/video_processing/video_processing.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Objective-C\nCODE:\n```\n- (void)viewDidLoad\n{\n    [super viewDidLoad];\n    // Do any additional setup after loading the view, typically from a nib.\n\n    self.videoCamera = [[CvVideoCamera alloc] initWithParentView:imageView];\n    self.videoCamera.defaultAVCaptureDevicePosition = AVCaptureDevicePositionFront;\n    self.videoCamera.defaultAVCaptureSessionPreset = AVCaptureSessionPreset352x288;\n    self.videoCamera.defaultAVCaptureVideoOrientation = AVCaptureVideoOrientationPortrait;\n    self.videoCamera.defaultFPS = 30;\n    self.videoCamera.grayscale = NO;\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Single WinRT Configuration using setup_winrt.bat\nDESCRIPTION: Executes the `setup_winrt.bat` script to generate Visual Studio project files specifically for Windows Phone 8.1 targeting the x86 architecture. The `-b` flag is omitted, so the generated solution will need to be built manually.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_1\n\nLANGUAGE: batch\nCODE:\n```\nsetup_winrt.bat \"WP\" \"8.1\" \"x86\"\n```\n\n----------------------------------------\n\nTITLE: Calculating Contour Perimeter in Python with OpenCV\nDESCRIPTION: This code shows how to calculate the perimeter (arc length) of a contour using the cv.arcLength() function. The second argument specifies whether the contour is closed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nperimeter = cv.arcLength(cnt,True)\n```\n\n----------------------------------------\n\nTITLE: Accessing Point in Matrix of Points with OpenCV in C++\nDESCRIPTION: Shows how to access an individual point in a matrix of CV_32FC2 or CV_32FC3 points using Mat::at. Input is Mat constructed from vector; result is cv::Point2f or cv::Point3f. Useful for geometric operations. Out-of-bounds access may crash.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_20\n\nLANGUAGE: C++\nCODE:\n```\ncv::Point2f pt = pointsMat.at<cv::Point2f>(i, 0);\n```\n\n----------------------------------------\n\nTITLE: Applying Morphological Transformations with Interactive Trackbars in OpenCV (Python)\nDESCRIPTION: This Python code implements an interactive GUI for exploring different morphological operations using OpenCV, such as Opening, Closing, Morphological Gradient, Top Hat, and Black Hat. It loads an input image, creates trackbars for selecting the operation, structuring element, and kernel size, and applies the selected operation in real time whenever any trackbar value changes. It depends on OpenCV (cv2), requires an image path, and produces the result in a named window. Users control parameters via the GUI; outputs are shown dynamically. The solution is cross-platform but depends on OpenCV's GUI capability and Python 3.x.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/opening_closing_hats/opening_closing_hats.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\\nimport numpy as np\\n\\n# Load the image\\nfilename = 'baboon.png'\\nimg = cv2.imread(filename)\\n\\ndef nothing(x):\\n    pass\\n\\ncv2.namedWindow('Morphology Transformations', cv2.WINDOW_AUTOSIZE)\\ncv2.createTrackbar('Operator:\\n0:Open\\n1:Close\\n2:Gradient\\n3:Top Hat\\n4:Black Hat','Morphology Transformations',0,4,nothing)\\ncv2.createTrackbar('Element:\\n0:Rect\\n1:Cross\\n2:Ellipse','Morphology Transformations',0,2,nothing)\\ncv2.createTrackbar('Kernel size:\\n2n+1','Morphology Transformations',0,21,nothing)\\n\\nwhile True:\\n    morph_operator = cv2.getTrackbarPos('Operator:\\n0:Open\\n1:Close\\n2:Gradient\\n3:Top Hat\\n4:Black Hat','Morphology Transformations')\\n    morph_elem = cv2.getTrackbarPos('Element:\\n0:Rect\\n1:Cross\\n2:Ellipse','Morphology Transformations')\\n    morph_size = cv2.getTrackbarPos('Kernel size:\\n2n+1','Morphology Transformations')\\n\\n    operation = morph_operator + 2\\n    element = cv2.getStructuringElement(morph_elem,(2*morph_size+1,2*morph_size+1),(morph_size,morph_size))\\n    dst = cv2.morphologyEx(img, operation, element)\\n    cv2.imshow('Morphology Transformations', dst)\\n    key = cv2.waitKey(1)&0xFF\\n    if key == 27:\\n        break\\ncv2.destroyAllWindows()\\n\n```\n\n----------------------------------------\n\nTITLE: Reading Images using OpenCV in C++\nDESCRIPTION: This snippet demonstrates how to read an image file in C++ using OpenCV's cv::imread function. The first argument specifies the file path while the second argument determines the color format (e.g., IMREAD_COLOR). The image data is stored in a cv::Mat object for further processing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/display_image/display_image.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nMat image = imread(\"starry_night.jpg\", IMREAD_COLOR);\n```\n\n----------------------------------------\n\nTITLE: Training SVM Model with Non-Linearly Separable Data (Java)\nDESCRIPTION: Trains the SVM model in Java using the prepared training data containing both linearly and non-linearly separable examples. The training process requires patience due to high iteration count needed for convergence.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n// Train the SVM\nSystem.out.print(\"\\nTraining the SVM...\");\nsvm.train(completeTrainData, Ml.ROW_SAMPLE, completeTrainLabels);\nSystem.out.println(\"Finished training\");\n```\n\n----------------------------------------\n\nTITLE: Calculating Vector Magnitude Using OpenCV in JavaScript\nDESCRIPTION: This snippet illustrates the use of cv.magnitude to compute the magnitude of complex vectors given their x and y components. Used in processing the DFT result to extract amplitude information. Inputs are two floating-point arrays (x, y), with the result stored in the output array (magnitude) of matching size and type. Dependencies: OpenCV.js; input arrays must match in size.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_transforms/js_fourier_transform/js_fourier_transform.markdown#2025-04-22_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.magnitude (x, y, magnitude)\n```\n\n----------------------------------------\n\nTITLE: Plot RGB Histogram\nDESCRIPTION: Creates separate histograms for each color channel (BGR) in a color image and plots them together using Matplotlib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_begins/py_histogram_begins.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('home.jpg')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\ncolor = ('b','g','r')\nfor i,col in enumerate(color):\n    histr = cv.calcHist([img],[i],None,[256],[0,256])\n    plt.plot(histr,color = col)\n    plt.xlim([0,256])\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Classifying and Visualizing SVM Regions with OpenCV - C++\nDESCRIPTION: This C++ snippet demonstrates how to train an SVM using OpenCV, classify data over a Cartesian plane, and visualize predicted classes by coloring points based on SVM output (green for label 1, blue for label -1). Dependencies include OpenCV C++ libraries and appropriate headers. Key steps involve setting up training data, configuring the SVM, and iterating over the image to assign colors based on classification. The expected input is an image or pixel grid with labeled data points, and the output is a visual representation of the SVM decision boundary.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_svm.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n#include <opencv2/opencv.hpp>\n#include <opencv2/ml.hpp>\n\n// Training and visualizing SVM classification regions\n// ... (full code from samples/cpp/tutorial_code/ml/introduction_to_svm/introduction_to_svm.cpp, show)\n```\n\n----------------------------------------\n\nTITLE: Running SSD MobileNetV1 Inference with Explicit File Parameters - Console\nDESCRIPTION: Runs the 'object_detection.py' script with explicit individual parameters for the model, config, input image, input dimensions, and class file. Offers more flexibility to override defaults and hardcoded options from YAML. Input/output files must exist and be accessible.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial.md#2025-04-22_snippet_8\n\nLANGUAGE: console\nCODE:\n```\npython object_detection.py --model ssd_mobilenet_v1_coco_2017_11_17.pb --config  ssd_mobilenet_v1_coco_2017_11_17.pbtxt  --input ../data/pexels_double_decker_bus.jpg --width 300 --height 300 --classes ../data/dnn/object_detection_classes_coco.txt\n```\n\n----------------------------------------\n\nTITLE: Resizing Images with OpenCV\nDESCRIPTION: This snippet demonstrates image resizing using OpenCV's cv.resize function. It allows you to specify the size of the output image directly or use scaling factors. The function requires interpolation methods, with cv.INTER_AREA best for shrinking and cv.INTER_CUBIC or cv.INTER_LINEAR for zooming.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_geometric_transformations/js_geometric_transformations.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ncv.resize(src, dst, dsize, fx = 0, fy = 0, interpolation = cv.INTER_LINEAR)\n```\n\n----------------------------------------\n\nTITLE: Drawing a 3D Cube on an Image with OpenCV (Python)\nDESCRIPTION: A modified function to render a 3D cube on the image by projecting eight 3D corner points. Uses OpenCV for drawing contours and lines, first outlining the cube base, then verticals, and finally the top. Accepts an image, chessboard corners, and projected cube points, and returns the image with the cube rendered for visualization of object pose.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_pose/py_pose.markdown#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef draw(img, corners, imgpts):\\n    imgpts = np.int32(imgpts).reshape(-1,2)\\n\\n    # draw ground floor in green\\n    img = cv.drawContours(img, [imgpts[:4]],-1,(0,255,0),-3)\\n\\n    # draw pillars in blue color\\n    for i,j in zip(range(4),range(4,8)):\\n        img = cv.line(img, tuple(imgpts[i]), tuple(imgpts[j]),(255),3)\\n\\n    # draw top layer in red color\\n    img = cv.drawContours(img, [imgpts[4:]],-1,(0,0,255),3)\\n\\n    return img\n```\n\n----------------------------------------\n\nTITLE: Applying a Lookup Table (LUT) to an Image using cv::LUT() in C++\nDESCRIPTION: This C++ snippet shows how to apply a pre-defined lookup table (`lookUpTable`) to an input image (`I`) using the `cv::LUT()` function from OpenCV. The result of the transformation is stored in the output image (`J`). This is presented as a potentially faster alternative to manual pixel iteration. Requires the OpenCV library and initialized `cv::Mat` objects for input (I), output (J), and the LUT.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_scan_images/how_to_scan_images.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet how_to_scan_images.cpp table-use\n```\n\n----------------------------------------\n\nTITLE: Converting BGR Image to Grayscale with OpenCV in C++\nDESCRIPTION: Converts a color image (BGR) to grayscale using cvtColor in OpenCV C++. Uses COLOR_BGR2GRAY code. Input is 3-channel Mat, output is 1-channel Mat. Output must be preallocated or function will allocate it.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_34\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat gray;\\ncv::cvtColor(img, gray, cv::COLOR_BGR2GRAY);\n```\n\n----------------------------------------\n\nTITLE: Loading Raster Data with GDAL in OpenCV (C++)\nDESCRIPTION: This snippet shows how to use OpenCV's cv::imread function with specific flags to force loading using GDAL. It is essential for ensuring the correct import of georeferenced raster data, preserving its native format. Dependencies include OpenCV compiled with GDAL support, and input files must be in a GDAL-supported format.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/raster_io_gdal.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// Load a raster image with GDAL support\ncv::Mat image = cv::imread(\"/path/to/file.tif\", cv::IMREAD_UNCHANGED | cv::IMREAD_LOAD_GDAL);\n```\n\n----------------------------------------\n\nTITLE: Shifting Quadrants of Fourier Transform Output in OpenCV C++\nDESCRIPTION: Rearranges the quadrants of a 2D matrix, typically the output of a Discrete Fourier Transform (DFT), to move the zero-frequency component (origin) from the top-left corner to the center of the matrix. This is a standard operation required for visualization and filtering in the frequency domain. It swaps the top-left quadrant with the bottom-right and the top-right with the bottom-left. Requires OpenCV `Mat` operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/out_of_focus_deblur_filter/out_of_focus_deblur_filter.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nvoid fftshift(const Mat& inputImg, Mat& outputImg)\n{\n    outputImg = inputImg.clone();\n    int cx = outputImg.cols / 2;\n    int cy = outputImg.rows / 2;\n    Mat q0(outputImg, Rect(0, 0, cx, cy));\n    Mat q1(outputImg, Rect(cx, 0, cx, cy));\n    Mat q2(outputImg, Rect(0, cy, cx, cy));\n    Mat q3(outputImg, Rect(cx, cy, cx, cy));\n    Mat tmp;\n    q0.copyTo(tmp);\n    q3.copyTo(q0);\n    tmp.copyTo(q3);\n    q1.copyTo(tmp);\n    q2.copyTo(q1);\n    tmp.copyTo(q2);\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing OpenCV Documentation in Clojure\nDESCRIPTION: This code snippet provides a way to search for javadoc documentation of OpenCV classes, leveraging the REPL to find external Java documentation quickly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_12\n\nLANGUAGE: clojure\nCODE:\n```\nuser=> (javadoc Rect)\n\"http://www.google.com/search?btnI=I%27m%20Feeling%20Lucky&q=allinurl:org/opencv/core/Rect.html\"\n```\n\n----------------------------------------\n\nTITLE: Converting Image for OpenCV DNN\nDESCRIPTION: Prepares an image for input to an OpenCV-based DNN by resizing, reordering channels and normalizing pixel values. It employs OpenCV's blobFromImage function and requires numpy for array manipulations. Inputs involve image path and mean normalization value.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# read the image\ninput_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\ninput_img = input_img.astype(np.float32)\n\n# preprocess image for TF model input\ntf_preproc_img = cv2.resize(input_img, (513, 513))\ntf_preproc_img = cv2.cvtColor(tf_preproc_img, cv2.COLOR_BGR2RGB)\n\n# define preprocess parameters for OpenCV DNN\nmean = np.array([1.0, 1.0, 1.0]) * 127.5\nscale = 1 / 127.5\n\n# prepare input blob to fit the model input:\n# 1. subtract mean\n# 2. scale to set pixel values from 0 to 1\ninput_blob = cv2.dnn.blobFromImage(\n    image=input_img,\n    scalefactor=scale,\n    size=(513, 513),  # img target size\n    mean=mean,\n    swapRB=True,  # BGR -> RGB\n    crop=False  # center crop\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Standalone Sample Build Environment in CMake\nDESCRIPTION: Sets up the CMake environment for building samples standalone (not part of the main OpenCV build). It requires CMake version 3.5+, defines the project as 'samples' (C/CXX), adds a `BUILD_EXAMPLES` option (default ON), and uses `find_package(OpenCV REQUIRED PATHS \"..\")` to locate a pre-installed OpenCV library, assuming the samples directory is located relative to the OpenCV installation share directory as described in the comments.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5)\n\nproject(samples C CXX)\noption(BUILD_EXAMPLES \"Build samples\" ON)\n\n# Assuming following installation folder structure (default for UNIX):\n# <install_root>/share/\n# └── OpenCV/  <-- OPENCV_CONFIG_INSTALL_PATH\n#     ├── OpenCVConfig.cmake  <-- file to be found by find_package\n#     ├── ...\n#     ├── samples/  <-- OPENCV_SAMPLES_SRC_INSTALL_PATH\n#     │   ├── CMakeLists.txt  <-- this file\n#     │   ├── cpp/\nfind_package(OpenCV REQUIRED PATHS \"..\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Camera Properties from Astra Depth Sensor\nDESCRIPTION: Demonstrates retrieving camera properties from the depth sensor using the VideoCapture get method. This example retrieves the maximum depth value supported by the camera, which is important for properly scaling depth data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n// Get device properties\ndouble maxDepth = depthStream.get(CAP_PROP_OPENNI_FRAME_MAX_DEPTH);\nprintf(\"Max depth: %f\\n\", maxDepth);\n```\n\n----------------------------------------\n\nTITLE: Implementing AGAST 5-8 Corner Score Detection in OpenCV\nDESCRIPTION: This code implements the AGAST_5_8 corner score calculation which is a specialized variant of the AGAST corner detection algorithm. It tests neighboring pixels in an 8-point pattern around a center pixel to determine if the pixel is a corner based on intensity thresholds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_42\n\nLANGUAGE: C++\nCODE:\n```\n// 8 pixel mask\ntemplate<>\nint agast_cornerScore<AgastFeatureDetector::AGAST_5_8>(const uchar* ptr, const int pixel[], int threshold)\n{\n    int bmin = threshold;\n    int bmax = 255;\n    int b_test = (bmax + bmin)/2;\n\n    short offset0 = (short) pixel[0];\n    short offset1 = (short) pixel[1];\n    short offset2 = (short) pixel[2];\n    short offset3 = (short) pixel[3];\n    short offset4 = (short) pixel[4];\n    short offset5 = (short) pixel[5];\n    short offset6 = (short) pixel[6];\n    short offset7 = (short) pixel[7];\n\n    while(true)\n    {\n        const int cb = *ptr + b_test;\n        const int c_b = *ptr - b_test;\n        if(ptr[offset0] > cb)\n          if(ptr[offset2] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset5] > cb)\n                if(ptr[offset1] > cb)\n                  if(ptr[offset4] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset7] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset4] > cb)\n                    if(ptr[offset6] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset1] > cb)\n                  if(ptr[offset4] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset7] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset7] > cb)\n                if(ptr[offset6] > cb)\n                  if(ptr[offset5] > cb)\n                    if(ptr[offset1] > cb)\n                      goto is_a_corner;\n                    else\n                      if(ptr[offset4] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset1] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                if(ptr[offset5] < c_b)\n                  if(ptr[offset3] < c_b)\n                    if(ptr[offset7] < c_b)\n                      if(ptr[offset4] < c_b)\n                        if(ptr[offset6] < c_b)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n          else\n            if(ptr[offset5] > cb)\n              if(ptr[offset7] > cb)\n                if(ptr[offset6] > cb)\n                  if(ptr[offset1] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset4] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset5] < c_b)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset2] < c_b)\n                    if(ptr[offset1] < c_b)\n                      if(ptr[offset4] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      if(ptr[offset4] < c_b)\n                        if(ptr[offset6] < c_b)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset7] < c_b)\n                      if(ptr[offset4] < c_b)\n                        if(ptr[offset6] < c_b)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n        else\n        if(ptr[offset0] < c_b)\n          if(ptr[offset2] < c_b)\n            if(ptr[offset7] > cb)\n```\n\n----------------------------------------\n\nTITLE: Configuring G-API OpenVINO Support Option in CMake\nDESCRIPTION: Checks if the OpenVINO 3rd party target (`ocv.3rdparty.openvino`) exists. If it does, it defines a CMake option `OPENCV_GAPI_WITH_OPENVINO` to enable/disable OpenVINO support within G-API. It handles backward compatibility with a deprecated option `OPENCV_GAPI_INF_ENGINE`, issuing a warning if the old option is used.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(TARGET ocv.3rdparty.openvino)\n  # TODO: remove OPENCV_GAPI_INF_ENGINE option\n  set(initial_value ON)\n  if(DEFINED OPENCV_GAPI_INF_ENGINE)\n    set(initial_value ${OPENCV_GAPI_INF_ENGINE})\n    message(WARNING \"OPENCV_GAPI_INF_ENGINE option is deprecated. Use OPENCV_GAPI_WITH_OPENVINO option instead.\")\n  endif()\n  ocv_option(OPENCV_GAPI_WITH_OPENVINO \"G-API: Enable OpenVINO Toolkit support\" ${initial_value})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Checking for Empty Frames After Reading in C++\nDESCRIPTION: Shows how to check if the `cv::Mat` objects are empty after a read operation using the `empty()` method. This condition typically signifies that the end of the video file has been reached or the video stream was closed, indicating that no more frames could be acquired. Requires `<opencv2/core.hpp>` header.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_input_psnr_ssim.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nif( frameReference.empty()  || frameUnderTest.empty())\n{\n // exit the program\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Contour Centroid from Moments in JavaScript\nDESCRIPTION: This snippet demonstrates how to compute the centroid coordinates (cx, cy) of a contour using the moments object (M) returned by `cv.moments`. It utilizes the formulas Cx = M10 / M00 and Cy = M01 / M00, where M00 is the area of the contour, and M10 and M01 are the first-order moments. Assumes `M` is a valid moments object obtained from `cv.moments`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contour_features/js_contour_features.markdown#2025-04-22_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nlet cx = M.m10/M.m00\nlet cy = M.m01/M.m00\n```\n\n----------------------------------------\n\nTITLE: Finding Required OpenCV Package in CMake\nDESCRIPTION: Prints the value of the `ANDROID_ABI` variable to the status output. Uses `find_package` to locate the OpenCV library, making the components specified in `ANDROID_OPENCV_COMPONENTS` mandatory (`REQUIRED`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-2-mixedprocessing/jni/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nmessage(STATUS \"ANDROID_ABI=${ANDROID_ABI}\")\nfind_package(OpenCV REQUIRED COMPONENTS ${ANDROID_OPENCV_COMPONENTS})\n```\n\n----------------------------------------\n\nTITLE: Applying the Standard Hough Line Transform in OpenCV (C++)\nDESCRIPTION: This snippet demonstrates how to use the HoughLines function in OpenCV C++ to detect straight lines. It takes an edge image and outputs a vector of lines in (rho, theta) form. Inputs include the edge image, rho and theta resolutions, and threshold parameters. Requires OpenCV and successful edge detection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nvector<Vec2f> lines;\\nHoughLines(edges, lines, 1, CV_PI / 180, 150, 0, 0);\\n\n```\n\n----------------------------------------\n\nTITLE: Conditionally Setting OpenCV Dependency Name in CMake\nDESCRIPTION: Checks the `OPENCV_FROM_SDK` variable. If true, sets `ANDROID_OPENCV_COMPONENTS` to `\"opencv_java\"` for local SDK usage. Otherwise, sets it to `\"OpenCV::opencv_java${OPENCV_VERSION_MAJOR}\"` for AAR usage, incorporating the major OpenCV version. Status messages indicate the chosen source.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-2-mixedprocessing/jni/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (OPENCV_FROM_SDK)\n  message(STATUS \"Using OpenCV from local SDK\")\n  set(ANDROID_OPENCV_COMPONENTS \"opencv_java\" CACHE STRING \"\")\nelse()\n  message(STATUS \"Using OpenCV from AAR (Maven repo)\")\n  set(ANDROID_OPENCV_COMPONENTS \"OpenCV::opencv_java${OPENCV_VERSION_MAJOR}\" CACHE STRING \"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Displaying Result Image in C++\nDESCRIPTION: Displays the final absolute Laplacian image (`abs_dst`) in a window titled \"Laplace Demo\" using cv::imshow. Requires OpenCV highgui module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\n//! [display]\n/// Showacilimage\nimshow( window_name, abs_dst );\n//! [display]\n```\n\n----------------------------------------\n\nTITLE: Implementing Harris Corner Detector in Python\nDESCRIPTION: This Python code demonstrates how to use the cv2.cornerHarris function to detect corners in an image using the Harris-Stephens method. It processes the input image, applies the corner detection algorithm, and visualizes the results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/harris_detector/harris_detector.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nfilename = 'chessboard.png'\nimg = cv.imread(filename)\ncv.imshow('src', img)\ngray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n\ngray = np.float32(gray)\ndst = cv.cornerHarris(gray, 2, 3, 0.04)\n\ndst = cv.dilate(dst, None)\n\nimg[dst > 0.01 * dst.max()] = [0, 0, 255]\n\ncv.imshow('dst', img)\nif cv.waitKey(0) & 0xff == 27:\n    cv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Calculating Plane Normal at Camera Pose using OpenCV - C++\nDESCRIPTION: This C++ snippet computes the normal vector to a plane expressed in the first camera frame. It may use the cross product of vectors or transform from object frame normals. Requires pose matrices or known normal in object frame. Output is a normalized 3-element vector for further homography calculations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet homography_from_camera_displacement.cpp compute-plane-normal-at-camera-pose-1\n```\n\n----------------------------------------\n\nTITLE: Applying Dilation - OpenCV.js - JavaScript\nDESCRIPTION: This snippet shows how to use cv.dilate() from OpenCV.js to expand the foreground objects in an image. Required parameters include src (input image), dst (output image), kernel (structuring element), anchor (anchor's position; defaults to center), iterations (number of times the operation is applied), borderType, and borderValue. The function is typically used after erosion to restore object size while removing noise, and requires OpenCV.js as a dependency. Input and output images must match in size and type.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_morphological_ops/js_morphological_ops.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.dilate(src, dst, kernel, anchor = new cv.Point(-1, -1), iterations = 1, borderType = cv.BORDER_CONSTANT, borderValue = cv.morphologyDefaultBorderValue())\n```\n\n----------------------------------------\n\nTITLE: Populating Kalman Filter Measurement Vector from Pose in C++ (OpenCV)\nDESCRIPTION: Implements the `fillMeasurements` C++ function using OpenCV. This function takes measured translation (`translation_measured`) and rotation (`rotation_measured`) matrices as input. It converts the 3x3 rotation matrix to a 3x1 vector of Euler angles (`measured_eulers`) using a helper function `rot2euler`. It then populates the output `measurements` `cv::Mat` (presumably 6x1) with the translation vector components (x, y, z) followed by the calculated Euler angles (roll, pitch, yaw).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_24\n\nLANGUAGE: cpp\nCODE:\n```\nvoid fillMeasurements( cv::Mat &measurements,\n                   const cv::Mat &translation_measured, const cv::Mat &rotation_measured)\n{\n    // Convert rotation matrix to euler angles\n    cv::Mat measured_eulers(3, 1, CV_64F);\n    measured_eulers = rot2euler(rotation_measured);\n\n    // Set measurement to predict\n    measurements.at<double>(0) = translation_measured.at<double>(0); // x\n    measurements.at<double>(1) = translation_measured.at<double>(1); // y\n    measurements.at<double>(2) = translation_measured.at<double>(2); // z\n    measurements.at<double>(3) = measured_eulers.at<double>(0);      // roll\n    measurements.at<double>(4) = measured_eulers.at<double>(1);      // pitch\n    measurements.at<double>(5) = measured_eulers.at<double>(2);      // yaw\n}\n```\n\n----------------------------------------\n\nTITLE: Including Full Template Matching Demo Code (Java)\nDESCRIPTION: Reference to include the complete Java source code file for the OpenCV template matching demonstration. This code loads images, performs matching using `Imgproc.matchTemplate`, normalizes results with `Core.normalize`, finds the best match using `Core.minMaxLoc`, and displays the output.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\n@include samples/java/tutorial_code/ImgProc/tutorial_template_matching/MatchTemplateDemo.java\n```\n\n----------------------------------------\n\nTITLE: Creating Morphological Kernel in Java\nDESCRIPTION: Java implementation for creating a morphological kernel with specified parameters\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.markdown#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nMat element = Imgproc.getStructuringElement(elementType,\n                new Size(2 * kernelSize + 1, 2 * kernelSize + 1),\n                new Point(kernelSize, kernelSize));\n```\n\n----------------------------------------\n\nTITLE: Simplified Video Frame Processing with OpenCV.js\nDESCRIPTION: This snippet simplifies video processing by using the OpenCV.js VideoCapture class to read video frames, convert them to grayscale, and display them. It illustrates a compact approach for handling video streams efficiently.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_video_display/js_video_display.markdown#2025-04-22_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet src = new cv.Mat(height, width, cv.CV_8UC4);\nlet dst = new cv.Mat(height, width, cv.CV_8UC1);\nlet cap = new cv.VideoCapture(videoSource);\n\nconst FPS = 30;\nfunction processVideo() {\n    let begin = Date.now();\n    cap.read(src);\n    cv.cvtColor(src, dst, cv.COLOR_RGBA2GRAY);\n    cv.imshow(\"canvasOutput\", dst);\n    // schedule next one.\n    let delay = 1000/FPS - (Date.now() - begin);\n    setTimeout(processVideo, delay);\n}\n\n// schedule first one.\nsetTimeout(processVideo, 0);\n```\n\n----------------------------------------\n\nTITLE: Draw Contours on Mat in OpenCV C++\nDESCRIPTION: Initializes an empty matrix for drawing and iterates over contours to draw them, along with bounding rectangles and circles. Requires OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\ncv::RNG rng(12345);\nfor (size_t i = 0; i < contours.size(); i++) {\n    cv::Scalar color = cv::Scalar(rng.uniform(0, 256), rng.uniform(0, 256), rng.uniform(0, 256));\n    cv::drawContours(drawing, contours, (int)i, color);\n    cv::rectangle(drawing, boundRect[i].tl(), boundRect[i].br(), color, 2);\n    cv::circle(drawing, center[i], (int)radius[i], color, 2);\n}\n```\n\n----------------------------------------\n\nTITLE: Applying the Probabilistic Hough Line Transform in OpenCV (Python)\nDESCRIPTION: This Python snippet applies cv2.HoughLinesP to detect line segments in a binary edge map. Parameters include the edge image, rho/theta resolutions, threshold, minLineLength, and maxLineGap. Returns an array of line segment endpoints. Requires cv2 and numpy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nlinesP = cv2.HoughLinesP(edges, 1, np.pi/180, 50, minLineLength=50, maxLineGap=10)\\n\n```\n\n----------------------------------------\n\nTITLE: Ratio Test Filtering in OpenCV using C++\nDESCRIPTION: This C++ snippet filters matches using the ratio test to ensure correctness by comparing distances of multiple matches.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nsamples/cpp/tutorial_code/features2D/AKAZE_match.cpp ratio test filtering\n```\n\n----------------------------------------\n\nTITLE: Main AGAST Detection Implementation with Multiple Corner Patterns\nDESCRIPTION: Complete implementation of the AGAST feature detection algorithm with support for multiple corner patterns. This function selects the appropriate detection algorithm based on the pattern type and performs feature scoring and optional non-maximum suppression.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_45\n\nLANGUAGE: C++\nCODE:\n```\nvoid AGAST(InputArray _img, std::vector<KeyPoint>& keypoints, int threshold, bool nonmax_suppression, int type)\n{\n\n    std::vector<KeyPoint> kpts;\n\n    // detect\n    switch(type) {\n      case AgastFeatureDetector::AGAST_5_8:\n        AGAST_5_8(_img, kpts, threshold);\n        break;\n      case AgastFeatureDetector::AGAST_7_12d:\n        AGAST_7_12d(_img, kpts, threshold);\n        break;\n      case AgastFeatureDetector::AGAST_7_12s:\n        AGAST_7_12s(_img, kpts, threshold);\n        break;\n      case AgastFeatureDetector::OAST_9_16:\n        OAST_9_16(_img, kpts, threshold);\n        break;\n    }\n\n    cv::Mat img = _img.getMat();\n\n    // score\n    int pixel_[16];\n    makeAgastOffsets(pixel_, (int)img.step, type);\n\n    std::vector<KeyPoint>::iterator kpt;\n    for(kpt = kpts.begin(); kpt != kpts.end(); kpt++)\n    {\n        switch(type) {\n          case AgastFeatureDetector::AGAST_5_8:\n            kpt->response = (float)agast_cornerScore<AgastFeatureDetector::AGAST_5_8>\n                (&img.at<uchar>((int)kpt->pt.y, (int)kpt->pt.x), pixel_, threshold);\n            break;\n          case AgastFeatureDetector::AGAST_7_12d:\n            kpt->response = (float)agast_cornerScore<AgastFeatureDetector::AGAST_7_12d>\n                (&img.at<uchar>((int)kpt->pt.y, (int)kpt->pt.x), pixel_, threshold);\n            break;\n          case AgastFeatureDetector::AGAST_7_12s:\n            kpt->response = (float)agast_cornerScore<AgastFeatureDetector::AGAST_7_12s>\n                (&img.at<uchar>((int)kpt->pt.y, (int)kpt->pt.x), pixel_, threshold);\n            break;\n          case AgastFeatureDetector::OAST_9_16:\n            kpt->response = (float)agast_cornerScore<AgastFeatureDetector::OAST_9_16>\n                (&img.at<uchar>((int)kpt->pt.y, (int)kpt->pt.x), pixel_, threshold);\n            break;\n        }\n    }\n\n    // suppression\n    if(nonmax_suppression)\n    {\n        size_t j;\n        size_t curr_idx;\n        size_t lastRow = 0, next_lastRow = 0;\n        size_t num_Corners = kpts.size();\n        size_t lastRowCorner_ind = 0, next_lastRowCorner_ind = 0;\n\n        std::vector<int> nmsFlags;\n        std::vector<KeyPoint>::iterator currCorner_nms;\n        std::vector<KeyPoint>::const_iterator currCorner;\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring WinRT Backend for HighGUI in CMake\nDESCRIPTION: Sets up the WinRT backend for OpenCV HighGUI when WINRT is enabled. Handles different WinRT versions, adds WinRT-specific headers and source files, and manages platform-specific library dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nelseif(WINRT)\n  set(OPENCV_HIGHGUI_BUILTIN_BACKEND \"WINRT\")\n  if(NOT WINRT_8_0)\n    # Dependencies used by the implementation referenced\n    # below are not available on WinRT 8.0.\n    # Enabling it for WiRT 8.1+ only.\n\n    # WinRT 8.1+ detected. Adding WinRT API header.\n    message(STATUS \"  ${name}: WinRT detected. Adding WinRT API header\")\n    list(APPEND highgui_ext_hdrs \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/highgui_winrt.hpp\")\n\n\n    list(APPEND highgui_srcs\n      ${CMAKE_CURRENT_LIST_DIR}/src/window_winrt.cpp\n      ${CMAKE_CURRENT_LIST_DIR}/src/window_winrt_bridge.cpp)\n    list(APPEND highgui_hdrs\n      ${CMAKE_CURRENT_LIST_DIR}/src/window_winrt_bridge.hpp)\n  endif()\n\n  # libraries below are neither available nor required\n  # on ARM devices and/or Windows Phone\n  if(WINRT_PHONE OR (OpenCV_ARCH STREQUAL \"ARM\"))\n    list(REMOVE_ITEM HIGHGUI_LIBRARIES \"comctl32\" \"gdi32\" \"ole32\" \"setupapi\")\n    if(WINRT_PHONE)\n      message(STATUS \"  ${name}: Windows Phone detected\")\n    elseif(OpenCV_ARCH STREQUAL \"ARM\")\n      message(STATUS \"  ${name}: ARM detected\")\n      if(WINRT_STORE)\n        list(REMOVE_ITEM HIGHGUI_LIBRARIES \"ws2_32\")\n        message(STATUS \"  ${name}:   Removing 'ws2_32.lib'\")\n      endif()\n    endif()\n    message(STATUS \"  ${name}:   Removing 'comctl32.lib, gdi32.lib, ole32.lib, setupapi.lib'\")\n    message(STATUS \"  ${name}:   Leaving '${HIGHGUI_LIBRARIES}'\")\n  endif()\n\n```\n\n----------------------------------------\n\nTITLE: Classifying New Data Points with Trained kNN Model in OpenCV\nDESCRIPTION: This code demonstrates how to classify a new data point using the trained kNN model. It visualizes the new point (marked in green) and outputs the classification result, the labels of k nearest neighbors, and their distances from the new point.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_knn/py_knn_understanding/py_knn_understanding.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nnewcomer = np.random.randint(0,100,(1,2)).astype(np.float32)\nplt.scatter(newcomer[:,0],newcomer[:,1],80,'g','o')\n\nknn = cv.ml.KNearest_create()\nknn.train(trainData, cv.ml.ROW_SAMPLE, responses)\nret, results, neighbours ,dist = knn.findNearest(newcomer, 3)\n\nprint( \"result:  {}\\n\".format(results) )\nprint( \"neighbours:  {}\\n\".format(neighbours) )\nprint( \"distance:  {}\\n\".format(dist) )\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Copying Mat Data Using copy() in OpenCV in Python\nDESCRIPTION: Displays deep copying NumPy arrays for OpenCV images in Python. The ndarray.copy() method returns a new array. Altering the copy does not affect the original. Useful before destructive operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\ncopy = img.copy()\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Tests Module in CMake for OpenCV\nDESCRIPTION: Sets up the Python tests module as an internal module in OpenCV. It defines the module name and configures it not to be part of the world build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/python/test/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(MODULE_NAME \"python_tests\")\nset(OPENCV_MODULE_IS_PART_OF_WORLD FALSE)\nocv_add_module(${MODULE_NAME} INTERNAL)\n```\n\n----------------------------------------\n\nTITLE: Applying Bilateral Filter in OpenCV Python\nDESCRIPTION: This code demonstrates the bilateral filter using cv.bilateralFilter(). This advanced filtering technique preserves edges while removing noise by considering both spatial proximity and intensity similarity between pixels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_filtering/py_filtering.markdown#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nblur = cv.bilateralFilter(img,9,75,75)\n```\n\n----------------------------------------\n\nTITLE: Setting Cache Variables for Doxygen Template Files in CMake\nDESCRIPTION: Defines CMake cache variables `OPENCV_DOCS_DOXYFILE_IN` and `OPENCV_DOCS_DOXYGEN_LAYOUT` to specify the paths to the Doxygen configuration template file (Doxyfile.in) and the Doxygen layout XML file, respectively. This allows users to potentially provide custom versions of these files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENCV_DOCS_DOXYFILE_IN \"Doxyfile.in\" CACHE PATH \"Doxygen configuration template file (Doxyfile.in)\")\nset(OPENCV_DOCS_DOXYGEN_LAYOUT \"DoxygenLayout.xml\" CACHE PATH \"Doxygen layout file (.xml)\")\n```\n\n----------------------------------------\n\nTITLE: Drawing 1-D Hue Histogram in Python with OpenCV\nDESCRIPTION: This code illustrates how to draw a 1-D Hue histogram of an image using OpenCV in Python. It creates a histogram image and uses the line function to draw the histogram bars.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/back_projection/back_projection.markdown#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nw, h = 400, 400\nbin_w = int(round(w / histSize))\nhist_img = np.zeros((h, w, 3), dtype=np.uint8)\nfor i in range(histSize):\n    cv.line(hist_img, (i * bin_w, h),\n            (i * bin_w, h - int(round(hist[i] * h / 255.0))),\n            (0, 0, 255), thickness=2)\ncv.imshow('Histogram', hist_img)\n```\n\n----------------------------------------\n\nTITLE: Warping Image with OpenCV warpPerspective - Java\nDESCRIPTION: This Java example demonstrates how to use OpenCV’s Imgproc.warpPerspective to perform perspective transformation using a 3x3 homography matrix. Requires input Mat, transformation matrix, and output Mat destination. Produces a new Mat with transformed content.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_16\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/features2D/Homography/PerspectiveCorrection.java warp-chessboard\n```\n\n----------------------------------------\n\nTITLE: Applying Gaussian Blur with OpenCV in Java\nDESCRIPTION: This Java snippet demonstrates the use of OpenCV's GaussianBlur() function for smoothing images with a Gaussian filter. Requires OpenCV and parameters for source, destination images, kernel size, and standard deviations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/Smoothing/Smoothing.java gaussianblur\n```\n\n----------------------------------------\n\nTITLE: Drawing 1-D Hue Histogram in C++ with OpenCV\nDESCRIPTION: This code shows how to draw a 1-D Hue histogram of an image using OpenCV in C++. It creates a histogram image and uses the line function to draw the histogram bars.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/back_projection/back_projection.markdown#2025-04-22_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nint w = 400, h = 400;\nint bin_w = cvRound( (double) w / histSize );\nMat histImg = Mat::zeros( h, w, CV_8UC3 );\nfor( int i = 0; i < histSize; i++ )\n{\n    rectangle( histImg, Point( i*bin_w, h ), Point( (i+1)*bin_w, h - cvRound( hist.at<float>(i)*h/255.0 ) ),\n               Scalar( 0, 0, 255 ), FILLED );\n}\nimshow(\"Histogram\", histImg);\n```\n\n----------------------------------------\n\nTITLE: Writing Frames to OpenCV VideoWriter (C++)\nDESCRIPTION: Illustrates two equivalent ways to write a video frame (represented by a `cv::Mat` object named `res`) to an opened `cv::VideoWriter` (`outputVideo`). One uses the `write` method, and the other uses the overloaded stream insertion `<<` operator. Requires an opened `VideoWriter` and a `Mat` containing the frame data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_write.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\noutputVideo.write(res);  //or\noutputVideo << res;\n```\n\n----------------------------------------\n\nTITLE: Getting Back Projection in Python with OpenCV\nDESCRIPTION: This code calculates the back projection of an image using OpenCV in Python. It utilizes the calcBackProject function with the previously computed histogram and image data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/back_projection/back_projection.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nbackproj = cv.calcBackProject([hue], [0], hist, [0, 180], 1)\n```\n\n----------------------------------------\n\nTITLE: Cloning and Deep Copying OpenCV Mat Objects in C++\nDESCRIPTION: Shows how to create completely independent (deep) copies of an OpenCV Mat object (`A`). The `cv::Mat::clone()` method returns a new Mat (`F`) with its own copy of the data. The `cv::Mat::copyTo()` method copies the data from the source Mat (`A`) to a pre-existing or newly created Mat (`G`). Modifications to `F` or `G` will not affect `A`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nMat F = A.clone();\nMat G;\nA.copyTo(G);\n```\n\n----------------------------------------\n\nTITLE: Machine Learning Model Training in OpenCV 2.4 vs 3.0\nDESCRIPTION: Compares the process of training a machine learning model (Boost) between OpenCV 2.4 and 3.0, demonstrating the new API with separate setter methods instead of parameter structures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/transition_guide/transition_guide.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nusing namespace cv;\n// ======== version 2.4 ========\nMat trainSamples, trainClasses;\nprepare_train_data( trainSamples, trainClasses );\nCvBoost  boost;\nMat var_types( 1, trainSamples.cols + 1, CV_8UC1, Scalar(CV_VAR_ORDERED) );\nvar_types.at<uchar>( trainSamples.cols ) = CV_VAR_CATEGORICAL;\nCvBoostParams  params( CvBoost::DISCRETE, // boost_type\n                       100, // weak_count\n                       0.95, // weight_trim_rate\n                       2, // max_depth\n                       false, //use_surrogates\n                       0 // priors\n                     );\nboost.train( trainSamples, CV_ROW_SAMPLE, trainClasses, Mat(), Mat(), var_types, Mat(), params );\n\n// ======== version 3.0 ========\nPtr<Boost> boost = Boost::create();\nboost->setBoostType(Boost::DISCRETE);\nboost->setWeakCount(100);\nboost->setWeightTrimRate(0.95);\nboost->setMaxDepth(2);\nboost->setUseSurrogates(false);\nboost->setPriors(Mat());\nboost->train(prepare_train_data()); // 'prepare_train_data' returns an instance of ml::TrainData class\n```\n\n----------------------------------------\n\nTITLE: Using cv.Canny() for Edge Detection in OpenCV.js\nDESCRIPTION: Demonstrates the function signature for applying the Canny edge detection algorithm using OpenCV.js. It takes an 8-bit input image and outputs an edge map. Key parameters include two thresholds for hysteresis (`threshold1`, `threshold2`), the aperture size for the Sobel operator (`apertureSize`), and a flag (`L2gradient`) to choose the gradient magnitude calculation method.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_canny/js_canny.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ncv.Canny(image, edges, threshold1, threshold2, apertureSize = 3, L2gradient = false)\n@param image         8-bit input image.\n@param edges         output edge map; single channels 8-bit image, which has the same size as image.\n@param threshold1    first threshold for the hysteresis procedure.\n@param threshold2    second threshold for the hysteresis procedure..\n@param apertureSize  aperture size for the Sobel operator.\n@param L2gradient    specifies the equation for finding gradient\nmagnitude. If it is True, it uses the equation mentioned above which is more accurate, otherwise it uses this function: \\f$Edge\\_Gradient \\; (G) = |G_x| + |G_y|\\f$.\n```\n\n----------------------------------------\n\nTITLE: Conditional Execution for OpenCV TAPI Sample Building - CMake\nDESCRIPTION: Checks if the BUILD_EXAMPLES option and all required dependencies are satisfied; if not, terminates processing with return(). This ensures that TAPI sample compilation is only attempted when intended and possible. Relies on the variables BUILD_EXAMPLES and OCV_DEPENDENCIES_FOUND being properly set earlier in the build configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/tapi/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT BUILD_EXAMPLES OR NOT OCV_DEPENDENCIES_FOUND)\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Detecting Chessboard Corners using OpenCV in C++\nDESCRIPTION: This snippet uses the findChessboardCorners function for detecting chessboard corners in an image. It's essential to have the OpenCV library loaded. The boardSize parameter specifies the number of inner corners per chessboard row and column.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_square_chess/camera_calibration_square_chess.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nbool found = findChessboardCorners(img, boardSize, ptvec, CALIB_CB_ADAPTIVE_THRESH);\n```\n\n----------------------------------------\n\nTITLE: Loading an Image using OpenCV in Python\nDESCRIPTION: Loads an image using `cv.imread` based on a command-line argument or a default path. Includes a check to ensure the image loaded successfully, exiting if it didn't.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef main(argv):\n    #![load]\n    # General instructions\n    print ('\\n Zoom In-Out demo \\n ')\n    print ('------------------')\n    print (' * [i] -> Zoom in')\n    print (' * [o] -> Zoom out')\n    print (' * [ESC] -> Close program \\n')\n\n    filename = argv[0] if len(argv) > 0 else '../data/chicky_512.png'\n\n    # Load image\n    src = cv.imread(cv.samples.findFile(filename))\n\n    # Check if image is loaded fine\n    if src is None:\n        print ('Error opening image!')\n        print ('Usage: pyramids.py [image_name -- default ../data/chicky_512.png] \\n')\n        return -1\n    #![load]\n```\n\n----------------------------------------\n\nTITLE: Drawing Rich SIFT Keypoints with Orientation in OpenCV Python\nDESCRIPTION: This code snippet shows how to draw SIFT keypoints with rich information including their size and orientation using the DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS flag in the drawKeypoints function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimg=cv.drawKeypoints(gray,kp,img,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\ncv.imwrite('sift_keypoints.jpg',img)\n```\n\n----------------------------------------\n\nTITLE: Running ResNet-50 Test Mode Example\nDESCRIPTION: Complete example command for testing ResNet-50 model with default image preprocessing settings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_14\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name resnet50 --test True --default_img_preprocess True --evaluate False\n```\n\n----------------------------------------\n\nTITLE: Detecting Faces using Haar Cascade in OpenCV - JavaScript\nDESCRIPTION: This JavaScript snippet uses OpenCV's detectMultiScale function to perform face detection. It requires OpenCV.js and pre-trained Haar cascade XML files. The function takes parameters for the image matrix, vector of rectangles for detected objects, scaling factor, minimum neighbors for detection, and size constraints for objects. Outputs are rectangles containing detected faces. This code must be run in a web environment with access to OpenCV.js and appropriate HTML elements.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_objdetect/js_face_detection/js_face_detection.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\ndetectMultiScale(image, objects, scaleFactor = 1.1, minNeighbors = 3, flags = 0, minSize = new cv.Size(0, 0), maxSize = new cv.Size(0, 0));\n```\n\n----------------------------------------\n\nTITLE: Background and Foreground Segmentation in OpenCV\nDESCRIPTION: Performs noise removal using morphological operations and identifies sure background/foreground areas using distance transform and thresholding.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_watershed/py_watershed.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# noise removal\nkernel = np.ones((3,3),np.uint8)\nopening = cv.morphologyEx(thresh,cv.MORPH_OPEN,kernel, iterations = 2)\n\n# sure background area\nsure_bg = cv.dilate(opening,kernel,iterations=3)\n\n# Finding sure foreground area\ndist_transform = cv.distanceTransform(opening,cv.DIST_L2,5)\nret, sure_fg = cv.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n\n# Finding unknown region\nsure_fg = np.uint8(sure_fg)\nunknown = cv.subtract(sure_bg,sure_fg)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Keypoint Matches in OpenCV C++\nDESCRIPTION: Creates a visualization of the matched keypoints between two images using OpenCV's drawMatches function and displays the result.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/detection_of_planar_objects/detection_of_planar_objects.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n// drawing the results\nnamedWindow(\"matches\", 1);\nMat img_matches;\ndrawMatches(img1, keypoints1, img2, keypoints2, matches, img_matches);\nimshow(\"matches\", img_matches);\nwaitKey(0);\n```\n\n----------------------------------------\n\nTITLE: Performing Template Matching Operation (Java)\nDESCRIPTION: Executes the template matching using `Imgproc.matchTemplate`. It takes the source image (`img`), template (`template`), result matrix (`result`), the chosen match method integer (`match_method`), and the optional mask (`mask`) as input. The dimensions of the result matrix are determined based on image and template sizes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_23\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/tutorial_template_matching/MatchTemplateDemo.java match_template\n```\n\n----------------------------------------\n\nTITLE: Performing Brightness and Contrast Adjustment in OpenCV (Java)\nDESCRIPTION: The Java code snippet illustrates how to apply a linear transformation to adjust image contrast and brightness using OpenCV. It involves loading an image, creating a new image matrix with the same size and type, and modifying each pixel based on user-defined gain and bias values. The code ensures pixel values remain valid within the image's data type limits.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/basic_linear_transform/basic_linear_transform.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.Core;\nimport org.opencv.core.Mat;\nimport org.opencv.core.CvType;\nimport org.opencv.core.Scalar;\nimport org.opencv.imgcodecs.Imgcodecs;\nimport java.util.Scanner;\n\nclass BasicLinearTransformsDemo {\n    public static void main(String[] args) {\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\n        Mat image = Imgcodecs.imread(args[0]);\n        Mat newImage = Mat.zeros(image.size(), image.type());\n        Scanner scanner = new Scanner(System.in);\n        \n        System.out.println(\"Basic Linear Transforms\");\n        System.out.println(\"------------------------\");\n        System.out.println(\"* Enter the alpha value [1.0-3.0]: \");\n        double alpha = scanner.nextDouble();\n        System.out.println(\"* Enter the beta value [0-100]: \");\n        int beta = scanner.nextInt();\n\n        image.convertTo(newImage, -1, alpha, beta);\n\n        Imgcodecs.imwrite(\"new_image.jpg\", newImage);\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Homography Check for Matches with OpenCV in Python\nDESCRIPTION: This Python code performs a homography test to evaluate matches' adherence to the homography model, utilizing OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nsamples/python/tutorial_code/features2D/akaze_matching/AKAZE_match.py homography check\n```\n\n----------------------------------------\n\nTITLE: Measuring Elapsed Time with OpenCV\nDESCRIPTION: This code shows how to measure the elapsed time between operations using OpenCV's cv::getTickCount() and cv::getTickFrequency(). The snippet calculates the time taken for a set of operations and prints it in seconds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_scan_images/how_to_scan_images.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n@code{.cpp}\ndouble t = (double)getTickCount();\n// do something ...\nt = ((double)getTickCount() - t)/getTickFrequency();\ncout << \"Times passed in seconds: \" << t << endl;\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Capturing Video Frame by Frame in C++\nDESCRIPTION: Captures frames from a video source until the ESC key is pressed and clones each frame for visualization. It is used as part of a real-time image processing pipeline.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\ncv::Mat frame, frame_vis;\n\nwhile(cap.read(frame) && cv::waitKey(30) != 27)    // capture frame until ESC is pressed\n{\n\n    frame_vis = frame.clone();                     // refresh visualisation frame\n\n    // MAIN ALGORITHM\n\n}\n```\n\n----------------------------------------\n\nTITLE: Drawing an Individual Contour by Index with OpenCV Python\nDESCRIPTION: This code demonstrates how to draw a single specific contour (the 4th contour) from the list of detected contours using cv.drawContours().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contours_begin/py_contours_begin.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncv.drawContours(img, contours, 3, (0,255,0), 3)\n```\n\n----------------------------------------\n\nTITLE: Loading Source Image in C++\nDESCRIPTION: Loads the source image from the path provided as a command-line argument using cv::imread. Includes error handling to check if the image was loaded successfully. Requires OpenCV imgcodecs module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n//! [load]\nconst char* imageName = argc >= 2 ? argv[1] : \"lena.jpg\";\n\nsrc = imread( samples::findFile( imageName ), IMREAD_COLOR ); // Load an image\n\n// Check if image is loaded fine\nif(src.empty())\n{\n    printf(\" Error opening image\\n\");\n    printf(\" Program Arguments: [image_name -- default lena.jpg] \\n\");\n    return -1;\n}\n//! [load]\n```\n\n----------------------------------------\n\nTITLE: Converting Image to Grayscale in C++\nDESCRIPTION: Converts the noise-reduced source image (presumably BGR) to grayscale using cv::cvtColor with the COLOR_BGR2GRAY code. The result is stored in the `src_gray` Mat object. Requires OpenCV imgproc module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n//! [convert_to_gray]\n// Convert the image to grayscale\ncvtColor( src, src_gray, COLOR_BGR2GRAY );\n//! [convert_to_gray]\n```\n\n----------------------------------------\n\nTITLE: Extracting and Displaying SVM Support Vectors - Python\nDESCRIPTION: This Python snippet obtains support vectors from a trained OpenCV SVM (using cv2.ml.SVM.getSupportVectors) and visually highlights them on the output image. The implementation depends on training an SVM with cv2 and requires Python's OpenCV library and numpy. Inputs are the trained SVM model and data/image for display. Outputs are images with special rings indicating support vectors. The code demonstrates analyzing the SVM's influence points for interpretation and debugging.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_svm.markdown#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Python: Highlight SVM support vectors using OpenCV\n# ... (full code from samples/python/tutorial_code/ml/introduction_to_svm/introduction_to_svm.py, show_vectors)\n```\n\n----------------------------------------\n\nTITLE: Parallel Mandelbrot Computation with C++11 Lambda and cv::parallel_for_ - C++\nDESCRIPTION: This advanced snippet demonstrates using a C++11 lambda function directly in cv::parallel_for_ to replace the functor class, streamlining parallel Mandelbrot image generation. Requires OpenCV 3.x+ and C++11 support. The lambda captures image and maxIter by reference, splitting the processing across available threads; result is a parallelized grayscale image output.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_/how_to_use_OpenCV_parallel_for_.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n// C++11 lambda-based parallel_for_ example\ncv::parallel_for_(cv::Range(0, image.rows * image.cols),\n    [&](const cv::Range& range) {\n        for (int r = range.start; r < range.end; ++r)\n        {\n            int row = r / image.cols;\n            int col = r % image.cols;\n            double x0 = (col / (double)image.cols) * 3.0 - 2.0;\n            double y0 = (row / (double)image.rows) * 2.0 - 1.0;\n            int iter = mandelbrot(cv::Point2d(x0, y0), maxIter);\n            image.at<uchar>(row, col) = grayscaleValue(iter, maxIter);\n        }\n    });\n\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenCV Installation\nDESCRIPTION: Python code to verify successful installation of OpenCV by importing the library and checking its version.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_ubuntu/py_setup_in_ubuntu.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2 as cv\nprint(cv.__version__)\n```\n\n----------------------------------------\n\nTITLE: Applying Median Blur\nDESCRIPTION: Reducing image noise using median blur to improve circle detection accuracy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_circle/hough_circle.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nmedianBlur(gray, gray, 5);\n```\n\nLANGUAGE: Java\nCODE:\n```\nImgproc.medianBlur(gray, gray, 5);\n```\n\nLANGUAGE: Python\nCODE:\n```\ngray = cv.medianBlur(gray, 5)\n```\n\n----------------------------------------\n\nTITLE: Computing Homography from Camera Displacement - C++\nDESCRIPTION: This code computes the homography matrix from the camera poses, plane normal, and distance according to the physical displacement, as per the formal plane-induced homography formula. Prereqs: camera rotation/translation, plane normal, camera intrinsics, and distance. Yields a homography matrix that can be directly compared with one estimated from image correspondences.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_25\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet homography_from_camera_displacement.cpp compute-homography-from-camera-displacement\n```\n\n----------------------------------------\n\nTITLE: DFT Performance Comparison Using Numpy and OpenCV - Python\nDESCRIPTION: Performs timing benchmarks comparing Numpy's fft2 and OpenCV's dft functions on both original- and optimal-size (zero-padded) images. Utilizes IPython %timeit magic for measurement. Inputs: 'img' (original image), 'nimg' (zero-padded array), optimal sizes (nrows, ncols). Dependencies: Numpy (np), OpenCV (cv), IPython environment. Outputs: execution times for default and optimized DFT. Shows that both libraries benefit from optimal sizing, with OpenCV being faster.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.markdown#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nIn [22]: %timeit fft1 = np.fft.fft2(img)\n10 loops, best of 3: 40.9 ms per loop\nIn [23]: %timeit fft2 = np.fft.fft2(img,[nrows,ncols])\n100 loops, best of 3: 10.4 ms per loop\n```\n\nLANGUAGE: Python\nCODE:\n```\nIn [24]: %timeit dft1= cv.dft(np.float32(img),flags=cv.DFT_COMPLEX_OUTPUT)\n100 loops, best of 3: 13.5 ms per loop\nIn [27]: %timeit dft2= cv.dft(np.float32(nimg),flags=cv.DFT_COMPLEX_OUTPUT)\n100 loops, best of 3: 3.11 ms per loop\n```\n\n----------------------------------------\n\nTITLE: PSF Calculation Function in C++\nDESCRIPTION: Generates the PSF based on input parameters \\f$LEN\\f$ and \\f$THETA\\f$ which define the length and angle of the motion blur. Essential for constructing the motion blur model before applying the restoration filter.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/motion_deblur_filter/motion_deblur_filter.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n@snippet samples/cpp/tutorial_code/ImgProc/motion_deblur_filter/motion_deblur_filter.cpp calcPSF\n```\n\n----------------------------------------\n\nTITLE: Configuring Qt Backend for HighGUI in CMake\nDESCRIPTION: Sets up the Qt backend for OpenCV HighGUI when HAVE_QT is enabled. Handles different Qt versions (4, 5, 6), processes Qt resources and moc files, adds OpenGL support if available, and links necessary Qt libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nelseif(HAVE_QT)\n  set(OPENCV_HIGHGUI_BUILTIN_BACKEND \"QT${QT_VERSION_MAJOR}\")\n  add_definitions(-DHAVE_QT)\n\n  if(QT_VERSION_MAJOR GREATER 4)\n    # \"Automoc\" doesn't work properly with opencv_world build, use QT<ver>_WRAP_CPP() directly\n    #set(CMAKE_AUTOMOC ON)\n\n    set(CMAKE_INCLUDE_CURRENT_DIR ON)\n\n    if(QT_VERSION_MAJOR EQUAL 6)\n      add_definitions(-DHAVE_QT6) # QGLWidget deprecated for QT6, use this preprocessor to adjust window_QT.[h,cpp]\n      QT6_ADD_RESOURCES(_RCC_OUTFILES ${CMAKE_CURRENT_LIST_DIR}/src/window_QT.qrc)\n      QT6_WRAP_CPP(_MOC_OUTFILES ${CMAKE_CURRENT_LIST_DIR}/src/window_QT.h)\n    elseif(QT_VERSION_MAJOR EQUAL 5)\n      QT5_ADD_RESOURCES(_RCC_OUTFILES ${CMAKE_CURRENT_LIST_DIR}/src/window_QT.qrc)\n      QT5_WRAP_CPP(_MOC_OUTFILES ${CMAKE_CURRENT_LIST_DIR}/src/window_QT.h)\n    else()\n      message(FATAL_ERROR \"Unsupported QT version: ${QT_VERSION_MAJOR}\")\n    endif()\n\n    list(APPEND highgui_srcs\n         ${CMAKE_CURRENT_LIST_DIR}/src/window_QT.cpp\n         ${CMAKE_CURRENT_LIST_DIR}/src/window_QT.h\n         ${_MOC_OUTFILES}\n         ${_RCC_OUTFILES})\n\n    set(qt_deps Core Gui Widgets Test Concurrent)\n    if(HAVE_QT_OPENGL)\n      add_definitions(-DHAVE_QT_OPENGL)\n      # QOpenGLWidget requires Qt6 package component OpenGLWidgets\n      if(QT_VERSION_MAJOR GREATER 5)\n        list(APPEND qt_deps OpenGLWidgets)\n      endif()\n      list(APPEND qt_deps OpenGL)\n      if(OPENGL_LIBRARIES)\n        list(APPEND HIGHGUI_LIBRARIES \"${OPENGL_LIBRARIES}\")\n      endif()\n    endif()\n\n    foreach(dt_dep ${qt_deps})\n      add_definitions(${Qt${QT_VERSION_MAJOR}${dt_dep}_DEFINITIONS})\n      include_directories(${Qt${QT_VERSION_MAJOR}${dt_dep}_INCLUDE_DIRS})\n      list(APPEND HIGHGUI_LIBRARIES ${Qt${QT_VERSION_MAJOR}${dt_dep}_LIBRARIES})\n    endforeach()\n  else()\n    ocv_assert(QT_VERSION_MAJOR EQUAL 4)\n    if(HAVE_QT_OPENGL)\n      set(QT_USE_QTOPENGL TRUE)\n      if(OPENGL_LIBRARIES)\n        list(APPEND HIGHGUI_LIBRARIES \"${OPENGL_LIBRARIES}\")\n      endif()\n    endif()\n    include(${QT_USE_FILE})\n\n    QT4_ADD_RESOURCES(_RCC_OUTFILES ${CMAKE_CURRENT_LIST_DIR}/src/window_QT.qrc)\n    QT4_WRAP_CPP(_MOC_OUTFILES ${CMAKE_CURRENT_LIST_DIR}/src/window_QT.h)\n\n    list(APPEND HIGHGUI_LIBRARIES ${QT_LIBRARIES})\n    list(APPEND highgui_srcs ${CMAKE_CURRENT_LIST_DIR}/src/window_QT.cpp ${_MOC_OUTFILES} ${_RCC_OUTFILES})\n    ocv_check_flag_support(CXX -Wno-missing-declarations _have_flag \"\")\n    if(${_have_flag})\n      set_source_files_properties(${_RCC_OUTFILES} PROPERTIES COMPILE_FLAGS -Wno-missing-declarations)\n    endif()\n  endif()\n\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific C++ Compiler Warning in CMake\nDESCRIPTION: Uses the custom CMake function `ocv_warnings_disable` (presumably defined within the OpenCV build system) to modify the C++ compiler flags (`CMAKE_CXX_FLAGS`). Specifically, it removes the `-Wmissing-declarations` flag, suppressing warnings about functions or variables used without a preceding declaration. This requires the `ocv_warnings_disable` function to be defined elsewhere.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nocv_warnings_disable(CMAKE_CXX_FLAGS -Wmissing-declarations)\n```\n\n----------------------------------------\n\nTITLE: Converting Image Color Spaces with OpenCV.js\nDESCRIPTION: Defines the usage of the `cv.cvtColor` function in OpenCV.js to convert an input image (`src`) to a different color space. The conversion type is specified by the `code` parameter (e.g., `cv.COLOR_RGBA2GRAY`), and the result is stored in `dst`. The optional `dstCn` parameter specifies the number of channels in the destination image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_colorspaces/js_colorspaces.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ncv.cvtColor (src, dst, code, dstCn = 0)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV C++ Header with CMake - CMake DSL\nDESCRIPTION: This snippet uses CMake's install() function to place the 'opencv2/opencv.hpp' header file into the library's include directory at installation time, defined by the ${OPENCV_INCLUDE_INSTALL_PATH} variable. Intended for developers packaging or distributing OpenCV, this setup ensures that the essential header is available under the correct directory for downstream users. The 'COMPONENT dev' parameter denotes that this action is part of the development files component, which is typically used to allow optional installation subsets (such as runtime vs developer files) during project configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/include/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(FILES \"opencv2/opencv.hpp\"\n    DESTINATION ${OPENCV_INCLUDE_INSTALL_PATH}/opencv2\n    COMPONENT dev)\n```\n\n----------------------------------------\n\nTITLE: Running Face Detection Model in the Browser\nDESCRIPTION: This JavaScript code snippet demonstrates how to run a face detection network using OpenCV.js. The network takes a BGR image as input and produces bounding boxes for detected faces. It allows adjustments to input blob sizes to optimize detection quality and efficiency. The snippet requires the OpenCV.js library and expects an input image for detection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_javascript/dnn_javascript.markdown#2025-04-22_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n\"Run face detection model\"\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLO Model to ONNX Using Minimal Python Script\nDESCRIPTION: Shows how to export a YOLO model from PyTorch to an ONNX graph using Python. This minimal export script loads a PyTorch model checkpoint, prepares a dummy input with the shape required for the model, exports the model to ONNX with dynamic axes for batching compatibility, and then uses onnx-simplifier for graph simplification. Required dependencies: torch, onnx, onnxsim. The input consists of the model state dict, checkpoint path, and export arguments; output is a simplified ONNX file validated for correctness.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_yolo/dnn_yolo.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    import onnx\\n    import torch\\n    from onnxsim import simplify\\n\\n    # load the model state dict\\n    ckpt = torch.load(ckpt_file, map_location=\\\"cpu\\\")\\n    model.load_state_dict(ckpt)\\n\\n    # prepare dummy input\\n    dummy_input = torch.randn(args.batch_size, 3, exp.test_size[0], exp.test_size[1])\\n\\n    #export the model\\n    torch.onnx._export(\\n        model,\\n        dummy_input,\\n        \\\"yolox.onnx\\\",\\n        input_names=[\\\"input\\\"],\\n        output_names=[\\\"output\\\"],\\n        dynamic_axes={\\\"input\\\": {0: 'batch'},\\n                      \\\"output\\\": {0: 'batch'}})\\n\\n    # use onnx-simplifier to reduce reduent model.\\n    onnx_model = onnx.load(args.output_name)\\n    model_simp, check = simplify(onnx_model)\\n    assert check, \\\"Simplified ONNX model could not be validated\\\"\\n    onnx.save(model_simp, args.output_name)\n```\n\n----------------------------------------\n\nTITLE: Configuring Zlib Build Options and Feature Summaries - CMake\nDESCRIPTION: This CMake snippet manages zlib build configuration within the OpenCV project. It conditionally enables or disables test targets and benchmarks based on whether shared libraries are enabled and sets up feature information for a wide variety of build and architecture-specific parameters, using CMake commands like add_feature_info and conditional if()-elseif() logic. The script also configures installation rules for binary libraries and licenses and observes solution folder organization. Required dependencies include functional CMake, defined zlib build variables, and (optionally) CPU detection modules. Inputs are various CMake cache/build options, and outputs are feature summaries, build targets, and installed binaries; upstream dependencies must define relevant variables. Limitations include reliance on external CMake settings and platform-specific conditions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_38\n\nLANGUAGE: CMake\nCODE:\n```\n# Example binaries\\n#============================================================================\\n\\nif(ZLIB_ENABLE_TESTS)\\n    enable_testing()\\n\\n    if(ZLIB_BUILD_SHARED_LIBS)\\n        if(ZLIBNG_ENABLE_TESTS)\\n            message(STATUS \"Disabling zlib-ng tests because shared libraries are enabled\")\\n            set(ZLIBNG_ENABLE_TESTS OFF)\\n        endif()\\n\\n        if(WITH_BENCHMARKS OR WITH_BENCHMARK_APPS)\\n            message(STATUS \"Disabling benchmarks because shared libraries are enabled\")\\n            set(WITH_BENCHMARKS OFF)\\n            set(WITH_BENCHMARK_APPS OFF)\\n        endif()\\n    endif()\\n\\n    add_subdirectory(test)\\nendif()\\n\\nadd_feature_info(WITH_GZFILEOP WITH_GZFILEOP \"Compile with support for gzFile related functions\")\\nadd_feature_info(ZLIB_COMPAT ZLIB_COMPAT \"Compile with zlib compatible API\")\\nadd_feature_info(ZLIB_ENABLE_TESTS ZLIB_ENABLE_TESTS \"Build test binaries\")\\nadd_feature_info(ZLIBNG_ENABLE_TESTS ZLIBNG_ENABLE_TESTS \"Test zlib-ng specific API\")\\nadd_feature_info(WITH_SANITIZER WITH_SANITIZER \"Enable sanitizer support\")\\nadd_feature_info(WITH_GTEST WITH_GTEST \"Build gtest_zlib\")\\nadd_feature_info(WITH_FUZZERS WITH_FUZZERS \"Build test/fuzz\")\\nadd_feature_info(WITH_BENCHMARKS WITH_BENCHMARKS \"Build test/benchmarks\")\\nadd_feature_info(WITH_BENCHMARK_APPS WITH_BENCHMARK_APPS \"Build application benchmarks\")\\nadd_feature_info(WITH_OPTIM WITH_OPTIM \"Build with optimisation\")\\nadd_feature_info(WITH_NEW_STRATEGIES WITH_NEW_STRATEGIES \"Use new strategies\")\\nadd_feature_info(WITH_NATIVE_INSTRUCTIONS WITH_NATIVE_INSTRUCTIONS\\n    \"Instruct the compiler to use the full instruction set on this host (gcc/clang -march=native)\")\\nadd_feature_info(WITH_RUNTIME_CPU_DETECTION WITH_RUNTIME_CPU_DETECTION \"Build with runtime CPU detection\")\\nadd_feature_info(WITH_MAINTAINER_WARNINGS WITH_MAINTAINER_WARNINGS \"Build with project maintainer warnings\")\\nadd_feature_info(WITH_CODE_COVERAGE WITH_CODE_COVERAGE \"Enable code coverage reporting\")\\nadd_feature_info(WITH_INFLATE_STRICT WITH_INFLATE_STRICT \"Build with strict inflate distance checking\")\\nadd_feature_info(WITH_INFLATE_ALLOW_INVALID_DIST WITH_INFLATE_ALLOW_INVALID_DIST \"Build with zero fill for inflate invalid distances\")\\n\\nif(BASEARCH_ARM_FOUND)\\n    add_feature_info(WITH_ACLE WITH_ACLE \"Build with ACLE\")\\n    add_feature_info(WITH_NEON WITH_NEON \"Build with NEON intrinsics\")\\n    add_feature_info(WITH_ARMV6 WITH_ARMV6 \"Build with ARMv6 SIMD\")\\nelseif(BASEARCH_PPC_FOUND)\\n    add_feature_info(WITH_ALTIVEC WITH_ALTIVEC \"Build with AltiVec optimisations\")\\n    add_feature_info(WITH_POWER8 WITH_POWER8 \"Build with optimisations for POWER8\")\\n    add_feature_info(WITH_POWER9 WITH_POWER9 \"Build with optimisations for POWER9\")\\nelseif(BASEARCH_RISCV_FOUND)\\n    add_feature_info(WITH_RVV WITH_RVV \"Build with RVV intrinsics\")\\nelseif(BASEARCH_S360_FOUND)\\n    add_feature_info(WITH_DFLTCC_DEFLATE WITH_DFLTCC_DEFLATE \"Build with DFLTCC intrinsics for compression on IBM Z\")\\n    add_feature_info(WITH_DFLTCC_INFLATE WITH_DFLTCC_INFLATE \"Build with DFLTCC intrinsics for decompression on IBM Z\")\\n    add_feature_info(WITH_CRC32_VX WITH_CRC32_VX \"Build with vectorized CRC32 on IBM Z\")\\nelseif(BASEARCH_X86_FOUND)\\n    add_feature_info(WITH_AVX2 WITH_AVX2 \"Build with AVX2\")\\n    add_feature_info(WITH_AVX512 WITH_AVX512 \"Build with AVX512\")\\n    add_feature_info(WITH_AVX512VNNI WITH_AVX512VNNI \"Build with AVX512 VNNI\")\\n    add_feature_info(WITH_SSE2 WITH_SSE2 \"Build with SSE2\")\\n    add_feature_info(WITH_SSSE3 WITH_SSSE3 \"Build with SSSE3\")\\n    add_feature_info(WITH_SSE42 WITH_SSE42 \"Build with SSE42\")\\n    add_feature_info(WITH_PCLMULQDQ WITH_PCLMULQDQ \"Build with PCLMULQDQ\")\\n    add_feature_info(WITH_VPCLMULQDQ WITH_VPCLMULQDQ \"Build with VPCLMULQDQ\")\\nendif()\\n\\nadd_feature_info(INSTALL_UTILS INSTALL_UTILS \"Copy minigzip and minideflate during install\")\\n\\nFEATURE_SUMMARY(WHAT ALL INCLUDE_QUIET_PACKAGES)\\n\\nif(ENABLE_SOLUTION_FOLDERS)\\n  set_target_properties(${ZLIB_INSTALL_LIBRARIES} PROPERTIES FOLDER \"3rdparty\")\\nendif()\\n\\nif(NOT BUILD_SHARED_LIBS)\\n  ocv_install_target(${ZLIB_INSTALL_LIBRARIES} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev)\\nendif()\\n\\nocv_install_3rdparty_licenses(${ZLIB_INSTALL_LIBRARIES} LICENSE.md)\\n\n```\n\n----------------------------------------\n\nTITLE: Building Excluding OpenCV Components Using --without Flag - Python/Bash\nDESCRIPTION: This snippet demonstrates how to build an OpenCV xcframework while excluding specific functionalities such as 'video' and 'objc'. The --without flag (used once per excluded module) modifies the build process to omit the listed components, reducing build size or dependencies. This command must be run in an environment where build_xcframework.py and all dependencies are available. Input modules are passed via repeated --without flags; output is an xcframework excluding the chosen modules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/apple/readme.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython build_xcframework.py --out somedir --without video --without objc\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Build with CMake on macOS\nDESCRIPTION: This command configures the OpenCV build using CMake, setting the build type to Release and enabling the building of examples.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/macos_install/macos_install.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncmake -DCMAKE_BUILD_TYPE=Release -DBUILD_EXAMPLES=ON ../opencv\n```\n\n----------------------------------------\n\nTITLE: Automatic Dictionary Generation for ArUco using OpenCV C++\nDESCRIPTION: This code snippet shows how to generate a custom dictionary using OpenCV's ArUco module, optimizing the inter-marker distance by specifying the number of markers and bits. Useful for creating dictionaries when predefined sets do not meet specific application needs.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_detection/aruco_detection.markdown#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\ncv::aruco::Dictionary dictionary = cv::aruco::extendDictionary(36, 5);\n```\n\n----------------------------------------\n\nTITLE: Thresholding Image Colors within a Range using OpenCV.js\nDESCRIPTION: Defines the usage of the `cv.inRange` function in OpenCV.js. This function checks if elements of the source image (`src`) lie between the corresponding elements of the lower boundary (`lowerb`) and upper boundary (`upperb`) Mats. It outputs a binary mask (`dst`) of type `cv.CV_8U`, where pixels within the specified range are set.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_colorspaces/js_colorspaces.markdown#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ncv.inRange (src, lowerb, upperb, dst)\n```\n\n----------------------------------------\n\nTITLE: Compiling G-API Streaming Pipeline\nDESCRIPTION: Shows how to compile a G-API computation for streaming execution. Uses compileStreaming() to optimize for throughput and returns a GStreamingCompiled object.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/interactive_face_detection/interactive_face_detection.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nauto pipeline = comp.compileStreaming(cv::compile_args(\n    cv::gapi::kernels<custom::PostProc>(),\n    networks));\n```\n\n----------------------------------------\n\nTITLE: Performing Exposure Fusion in OpenCV as Alternative to HDR\nDESCRIPTION: This code demonstrates exposure fusion, an alternative technique to HDR that directly merges multiple exposures into a single LDR image without requiring gamma correction or exposure values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/hdr_imaging.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nMat fusion;\nPtr<MergeMertens> merge_mertens = createMergeMertens();\nmerge_mertens->process(images, fusion);\nMat fusion_8bit;\nfusion = 255 * fusion;\nfusion.convertTo(fusion_8bit, CV_8UC3);\n```\n\nLANGUAGE: java\nCODE:\n```\nMat fusion = new Mat();\nMergeMertens mergeMertens = Photo.createMergeMertens();\nmergeMertens.process(images, fusion);\nfusion.convertTo(fusion, fusion.type(), 255, 0);\nMat fusion8bit = new Mat();\nfusion.convertTo(fusion8bit, CvType.CV_8UC3);\n```\n\nLANGUAGE: python\nCODE:\n```\nmerge_mertens = cv.createMergeMertens()\nfusion = merge_mertens.process(images)\nfusion_8bit = np.clip(fusion*255, 0, 255).astype('uint8')\n```\n\n----------------------------------------\n\nTITLE: Creating Different Structuring Elements with OpenCV Python\nDESCRIPTION: Shows how to create different types of structuring elements (rectangular, elliptical, and cross-shaped) using cv.getStructuringElement().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# Rectangular Kernel\n>>> cv.getStructuringElement(cv.MORPH_RECT,(5,5))\narray([[1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1]], dtype=uint8)\n\n# Elliptical Kernel\n>>> cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))\narray([[0, 0, 1, 0, 0],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1],\n       [0, 0, 1, 0, 0]], dtype=uint8)\n\n# Cross-shaped Kernel\n>>> cv.getStructuringElement(cv.MORPH_CROSS,(5,5))\narray([[0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0],\n       [1, 1, 1, 1, 1],\n       [0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0]], dtype=uint8)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV via pip on macOS\nDESCRIPTION: This command uses pip to install the Python bindings for OpenCV on macOS.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/macos_install/macos_install.markdown#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install opencv-python\n```\n\n----------------------------------------\n\nTITLE: Parameter Setup for Generalized Hough Transform in C++\nDESCRIPTION: This snippet involves configuring parameters for the Generalized Hough Transform detection. Achieving optimal values often requires experimentation, influenced by factors like image resolution.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/generalized_hough_ballard_guil/generalized_hough_ballard_guil.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n# Setup parameters\n@snippet samples/cpp/tutorial_code/ImgTrans/generalizedHoughTransform.cpp generalized-hough-transform-setup-parameters\n```\n\n----------------------------------------\n\nTITLE: Exposing Standalone Functions Using OpenCV Macros in C++\nDESCRIPTION: This snippet demonstrates how to export a standalone function to Python using the CV_EXPORTS_W macro in a C++ header. The function equalizeHist is declared with special macros and argument wrappers (InputArray, OutputArray) to enable automatic generation of Python bindings by OpenCV scripts. No additional dependencies are required beyond OpenCV headers. Inputs are OpenCV's image wrapper types, and the output is created by modifying the dst parameter.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_bindings/py_bindings_basics/py_bindings_basics.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nCV_EXPORTS_W void equalizeHist( InputArray src, OutputArray dst );\n```\n\n----------------------------------------\n\nTITLE: Performing Opening Operation with OpenCV Python\nDESCRIPTION: Demonstrates the opening operation which is erosion followed by dilation. Used for noise removal while preserving object shape.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nopening = cv.morphologyEx(img, cv.MORPH_OPEN, kernel)\n```\n\n----------------------------------------\n\nTITLE: Initializing VideoCapture with MSMF Backend for File Input in C++\nDESCRIPTION: This snippet shows how to create a VideoCapture object to read video from a file using the Microsoft Media Foundation (MSMF) backend. It demonstrates two approaches: using the constructor and using the open() method.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/videoio/doc/videoio_overview.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n//declare a capture object\ncv::VideoCapture cap(filename, cv::CAP_MSMF);\n\n//or specify the apiPreference with open\ncap.open(filename, cv::CAP_MSMF);\n```\n\n----------------------------------------\n\nTITLE: Initializing Small OpenCV Matrices with Comma Separators in C++\nDESCRIPTION: Creates small matrices using comma-separated initializers. This method is convenient for small matrices with known values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n// for small matrices, comma-separated initializer is available\nMat C = (Mat_<double>(3,3) << 0, -1, 0, -1, 5, -1, 0, -1, 0);\ncout << \"C = \" << endl << \" \" << C << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Running YOLO Detector Command Line Usage\nDESCRIPTION: Command line parameters for running the OpenCV YOLO detector example, including input configuration, model parameters, and processing options.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_yolo/dnn_yolo.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n./bin/example_dnn_yolo_detector --input=<path_to_your_input_file> \\\n                                --classes=<path_to_class_names_file> \\\n                                --thr=<confidence_threshold> \\\n                                --nms=<non_maximum_suppression_threshold> \\\n                                --mean=<mean_normalization_value> \\\n                                --scale=<scale_factor> \\\n                                --yolo=<yolo_model_version> \\\n                                --padvalue=<padding_value> \\\n                                --paddingmode=<padding_mode> \\\n                                --backend=<computation_backend> \\\n                                --target=<target_computation_device> \\\n                                --width=<model_input_width> \\\n                                --height=<model_input_height>\n```\n\n----------------------------------------\n\nTITLE: Setting up Dependencies for Android Build in CMake\nDESCRIPTION: This CMake code initializes the `depends` list for the subsequent Android build command. It includes the previously defined `${the_module}_android_source_copy` target and its corresponding dependency helper file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/android_sdk/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(depends ${the_module}_android_source_copy \"${OPENCV_DEPHELPER}/${the_module}_android_source_copy\")\n```\n\n----------------------------------------\n\nTITLE: Saving Inliers from Matched Keypoints in C++\nDESCRIPTION: This code snippet saves the inlier matches based on the mask computed by the RANSAC algorithm during homography estimation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_tracking/akaze_tracking.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nfor(unsigned i = 0; i < matched1.size(); i++) {\n    if(inlier_mask.at<uchar>(i)) {\n        int new_i = static_cast<int>(inliers1.size());\n        inliers1.push_back(matched1[i]);\n        inliers2.push_back(matched2[i]);\n        inlier_matches.push_back(DMatch(new_i, new_i, 0));\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Allocating/Resizing Output Mat using numpy.zeros_like in OpenCV in Python\nDESCRIPTION: For Python OpenCV, np.zeros_like creates an array of same shape and type as an input image. Ensures output buffer is appropriately sized, useful before assignments or in-place operations. Requires numpy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\noutput = np.zeros_like(input)\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding Example Subdirectories (OpenCV Build) in CMake\nDESCRIPTION: Includes subdirectories containing various example categories (C++, Java, DNN, GPU, etc.) when building as part of OpenCV. Inclusion is conditional based on CMake variables like `WIN32`, `HAVE_DIRECTX`, `ANDROID`, `HAVE_OPENGL`, `HAVE_OPENVX`, `UNIX`, `HAVE_VA`, `BUILD_ANDROID_EXAMPLES`, `INSTALL_ANDROID_EXAMPLES`, `INSTALL_PYTHON_EXAMPLES`, and `OPENCV_SEMIHOSTING`, ensuring only relevant examples are processed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(cpp)\nadd_subdirectory(java/tutorial_code)\nadd_subdirectory(dnn)\nadd_subdirectory(gpu)\nadd_subdirectory(tapi)\nadd_subdirectory(opencl)\nadd_subdirectory(sycl)\nif(WIN32 AND HAVE_DIRECTX)\n  add_subdirectory(directx)\nendif()\nif((NOT ANDROID) AND HAVE_OPENGL)\n  add_subdirectory(opengl)\nendif()\nif(HAVE_OPENVX)\n  add_subdirectory(openvx)\nendif()\nif(UNIX AND NOT ANDROID AND HAVE_VA)\n  add_subdirectory(va_intel)\nendif()\nif(ANDROID AND (BUILD_ANDROID_EXAMPLES OR INSTALL_ANDROID_EXAMPLES))\n  add_subdirectory(android)\nendif()\nif(INSTALL_PYTHON_EXAMPLES)\n  add_subdirectory(python)\nendif()\n# The examples in this folder will work with a semihosting version of\n# OpenCV. For more information about semihosting, see\n# https://developer.arm.com/documentation/100863/latest\nif(OPENCV_SEMIHOSTING)\n  add_subdirectory(semihosting)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing Color Image Denoising with OpenCV Python\nDESCRIPTION: Demonstrates how to use cv.fastNlMeansDenoisingColored() to remove Gaussian noise from color images. The function processes the image in CIELAB colorspace and applies denoising separately to luminance and color components.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_photo/py_non_local_means/py_non_local_means.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('die.png')\n\ndst = cv.fastNlMeansDenoisingColored(img,None,10,10,7,21)\n\nplt.subplot(121),plt.imshow(img)\nplt.subplot(122),plt.imshow(dst)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Loading Input Image and Converting to Blob Format in C++\nDESCRIPTION: This snippet shows how to read an input image (or video) and convert it into a format suitable for GoogLeNet using OpenCV. It utilizes the cv::VideoCapture and cv::dnn::blobFromImage functions to read images and prepare them by resizing, mean subtraction, and reshaping to `1x3x224x224` dimensions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_googlenet/dnn_googlenet.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/classification.cpp Open a video file or an image file or a camera stream\n```\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/classification.cpp Create a 4D blob from a frame\n```\n\n----------------------------------------\n\nTITLE: Building For Custom Architectures Using build_xcframework.py - Python/Bash\nDESCRIPTION: This snippet demonstrates specifying the target architectures for each Apple platform when invoking build_xcframework.py. By using the *_archs flags for each platform, you can control which architectures are included in the build process. Requires the same Python and platform dependencies as a standard build. Inputs are architecture flags and output directory; output is the built xcframework tailored to the architectures provided.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/apple/readme.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython build_xcframework.py --out somedir --iphoneos_archs arm64 --iphonesimulator_archs arm64 --macos_archs arm64 --catalyst_archs arm64\n```\n\n----------------------------------------\n\nTITLE: Creating Trackbar for Method Selection (Java)\nDESCRIPTION: Creates a trackbar using `HighGui.createTrackbar` within a Swing `JFrame`. This UI element enables the user to choose the template matching method. An event listener updates the `match_method` variable and triggers the matching process when the slider value changes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_17\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/tutorial_template_matching/MatchTemplateDemo.java create_trackbar\n```\n\n----------------------------------------\n\nTITLE: Calculating SSIM on GPU (Optimized) in C++\nDESCRIPTION: Defines an optimized C++ function `getMSSIM_GPU_optimized` for GPU-based MSSIM calculation. It utilizes a pre-allocated buffer structure (`BufferMSSIM`, not shown but implied) to hold intermediate `gpu::GpuMat` results, minimizing GPU memory allocation overhead. The function uploads input matrices (`I1`, `I2`) and calls the optimized GPU `ssim` function, passing the buffer.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n//![getssimopt]\nScalar getMSSIM_GPU_optimized(const Mat& i1, const Mat& i2, BufferMSSIM& b)\n{\n    // Upload data to GPU\n    b.gI1.upload(i1);\n    b.gI2.upload(i2);\n\n    return ssim(b.gI1, b.gI2);\n}\n//![getssimopt]\n```\n\n----------------------------------------\n\nTITLE: NumPy-Style Formatting for OpenCV Mat Output in C++\nDESCRIPTION: Shows how to format OpenCV Mat output to resemble NumPy syntax using the format() method with FormatNumPy flag.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\ncout << \"R (numpy) = \" << endl << format(R, Formatter::FMT_NUMPY) << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Computing Histograms for BGR Channels in C++\nDESCRIPTION: C++ snippet calculating histograms for each B, G, and R plane using `cv::calcHist`. It iterates through the separated planes (`bgr_planes`), calling `calcHist` for each with the previously defined parameters (`histSize`, `histRange`, `uniform`, `accumulate`). The results are stored in `b_hist`, `g_hist`, and `r_hist` Mats.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/calcHist_Demo.cpp Compute the histograms\n```\n\n----------------------------------------\n\nTITLE: Loading and Initial Image Processing\nDESCRIPTION: Loads the source image and converts white background to black for better foreground object discrimination\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/distance_transformation/distance_transform.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nMat src = imread(samples::findFile(\"cards.png\"));\nif (src.empty())\n{\n    cout << \"Could not open or find the image!\\n\";\n    return -1;\n}\nimshow(\"Source\", src);\n```\n\nLANGUAGE: Python\nCODE:\n```\nsrc = cv.imread(cv.samples.findFile(\"cards.png\"))\nif src is None:\n    print('Could not open or find the image!')\n    exit(0)\ncv.imshow('Source', src)\n```\n\n----------------------------------------\n\nTITLE: Displaying Results and Drawing Rectangle (Java)\nDESCRIPTION: Draws a rectangle on the copied source image (`img_display`) using `Imgproc.rectangle` to highlight the best match area. The rectangle's corners are defined by `matchLoc` and the template dimensions. Both the result matrix and the image with the drawn rectangle are then displayed in separate windows using custom display logic (likely involving `HighGui.imshow` or Swing components).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_35\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/tutorial_template_matching/MatchTemplateDemo.java imshow\n```\n\n----------------------------------------\n\nTITLE: Running RANSAC Pose Estimation in C++\nDESCRIPTION: Demonstrates steps 3 and 4 of the main pose estimation algorithm using RANSAC. It first estimates the pose via RANSAC and then collects inliers for drawing, ensuring RANSAC runs only when matches exist.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\nif(good_matches.size() > 0) // None matches, then RANSAC crashes\n{\n\n    // -- Step 3: Estimate the pose using RANSAC approach\n    pnp_detection.estimatePoseRANSAC( list_points3d_model_match, list_points2d_scene_match,\n                                      pnpMethod, inliers_idx, iterationsCount, reprojectionError, confidence );\n\n\n    // -- Step 4: Catch the inliers keypoints to draw\n    for(int inliers_index = 0; inliers_index < inliers_idx.rows; ++inliers_index)\n    {\n    int n = inliers_idx.at<int>(inliers_index);         // i-inlier\n    cv::Point2f point2d = list_points2d_scene_match[n]; // i-inlier point 2D\n    list_points2d_inliers.push_back(point2d);           // add i-inlier to list\n}\n```\n\n----------------------------------------\n\nTITLE: Core PSNR Calculation Function in C++\nDESCRIPTION: Provides overloaded C++ functions named `psnr` for calculating Peak Signal-to-Noise Ratio. One version takes standard CPU `Mat` objects, calculates the sum of squared differences, and computes PSNR. The other version takes `gpu::GpuMat` objects, performs calculations on the GPU using functions like `gpu::absdiff`, `gpu::multiply`, and `gpu::sum`, downloads the result, and computes PSNR. Handles potential zero difference cases.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n//![psnr]\ndouble psnr(const Mat& I1, const Mat& I2)\n{\n    Mat s1;\n    absdiff(I1, I2, s1);       // |I1 - I2|\n    s1.convertTo(s1, CV_32F);  // cannot make a square on 8 bits\n    s1 = s1.mul(s1);           // |I1 - I2|^2\n\n    Scalar s = sum(s1);        // sum elements per channel\n\n    double sse = s.val[0] + s.val[1] + s.val[2]; // sum channels\n\n    if( sse <= 1e-10) // for small values return zero\n        return 0;\n    else\n    {\n        double mse = sse / (double)(I1.channels() * I1.total());\n        double psnr = 10.0 * log10((255*255) / mse);\n        return psnr;\n    }\n}\n\ndouble psnr(const gpu::GpuMat& d_I1, const gpu::GpuMat& d_I2)\n{\n    gpu::GpuMat d_s1;\n    gpu::absdiff(d_I1, d_I2, d_s1);\n    d_s1.convertTo(d_s1, CV_32F);\n    gpu::multiply(d_s1, d_s1, d_s1);\n\n    Scalar s = gpu::sum(d_s1);\n    double sse = s.val[0] + s.val[1] + s.val[2];\n\n    if( sse <= 1e-10)\n        return 0;\n    else\n    {\n        double mse = sse / (double)(d_I1.channels() * d_I1.total());\n        double psnr = 10.0 * log10((255*255) / mse);\n        return psnr;\n    }\n}\n//![psnr]\n```\n\n----------------------------------------\n\nTITLE: Image Stitching Using OpenCV in C++\nDESCRIPTION: This C++ function performs image stitching using the computed homography matrix with OpenCV. It requires the OpenCV library setup and previously calculated matrices. The function takes pre-processed images and results in a stitched panorama.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_37\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\n\nvoid stitchImages(cv::Mat image1, cv::Mat image2, cv::Mat homography) {\n    // Code to stitch images\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Refining Corners with SubPixel Accuracy in OpenCV (Python)\nDESCRIPTION: This code snippet shows how to refine detected corners using OpenCV's cv.cornerSubPix() function to achieve sub-pixel accuracy. After finding Harris corners, the centroid of these corners is calculated and refined for accuracy using specified criteria. The function requires defining termination criteria and neighborhood size. Output displays refined corners in green, while initially detected corners remain in red. This snippet uses the OpenCV and NumPy libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_features_harris/py_features_harris.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nfilename = 'chessboard2.jpg'\nimg = cv.imread(filename)\ngray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n\n# find Harris corners\ngray = np.float32(gray)\ndst = cv.cornerHarris(gray,2,3,0.04)\ndst = cv.dilate(dst,None)\nret, dst = cv.threshold(dst,0.01*dst.max(),255,0)\ndst = np.uint8(dst)\n\n# find centroids\nret, labels, stats, centroids = cv.connectedComponentsWithStats(dst)\n\n# define the criteria to stop and refine the corners\ncriteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 100, 0.001)\ncorners = cv.cornerSubPix(gray,np.float32(centroids),(5,5),(-1,-1),criteria)\n\n# Now draw them\nres = np.hstack((centroids,corners))\nres = np.int0(res)\nimg[res[:,1],res[:,0]]=[0,0,255]\nimg[res[:,3],res[:,2]] = [0,255,0]\n\ncv.imwrite('subpixel5.png',img)\n```\n\n----------------------------------------\n\nTITLE: Detecting FAST Corners using OpenCV in Python\nDESCRIPTION: This Python script demonstrates using OpenCV's FAST feature detector. It loads a grayscale image, creates a `FastFeatureDetector` object, detects keypoints with default settings (including non-maximal suppression), prints detector parameters, and visualizes the results by drawing keypoints. It then disables non-maximal suppression, detects keypoints again, prints the count, and saves both resulting images for comparison. Requires `numpy`, `cv2` (OpenCV), and an input image file ('blox.jpg'). `matplotlib.pyplot` is imported but not used.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_fast/py_fast.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@code{.py}\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('blox.jpg', cv.IMREAD_GRAYSCALE) # `<opencv_root>/samples/data/blox.jpg`\n\n# Initiate FAST object with default values\nfast = cv.FastFeatureDetector_create()\n\n# find and draw the keypoints\nkp = fast.detect(img,None)\nimg2 = cv.drawKeypoints(img, kp, None, color=(255,0,0))\n\n# Print all default params\nprint( \"Threshold: {}\".format(fast.getThreshold()) )\nprint( \"nonmaxSuppression:{}\".format(fast.getNonmaxSuppression()) )\nprint( \"neighborhood: {}\".format(fast.getType()) )\nprint( \"Total Keypoints with nonmaxSuppression: {}\".format(len(kp)) )\n\ncv.imwrite('fast_true.png', img2)\n\n# Disable nonmaxSuppression\nfast.setNonmaxSuppression(0)\nkp = fast.detect(img, None)\n\nprint( \"Total Keypoints without nonmaxSuppression: {}\".format(len(kp)) )\n\nimg3 = cv.drawKeypoints(img, kp, None, color=(255,0,0))\n\ncv.imwrite('fast_false.png', img3)\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Implementing Meanshift Object Tracking in Python\nDESCRIPTION: Python implementation of the Meanshift algorithm using OpenCV. The code sets up video capture, calculates the target histogram in HSV color space, and tracks objects across frames using the meanshift algorithm.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/meanshift.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\\nimport cv2 as cv\\nimport argparse\\n\\nparser = argparse.ArgumentParser(description='This sample demonstrates the meanshift algorithm. \\\\\\n                                              The example file can be downloaded from: \\\\\\n                                              https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4')\\nparser.add_argument('image', type=str, help='path to image file')\\nargs = parser.parse_args()\\n\\ncap = cv.VideoCapture(args.image)\\n# take first frame of the video\\nret,frame = cap.read()\\n# setup initial location of window\\nx, y, w, h = 300, 200, 100, 50 # simply hardcoded the values\\ntrack_window = (x, y, w, h)\\n# set up the ROI for tracking\\nroi = frame[y:y+h, x:x+w]\\nhsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\\nmask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\\nroi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\\ncv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\\n# Setup the termination criteria, either 10 iteration or move by at least 1 pt\\nterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\\nwhile(1):\\n    ret, frame = cap.read()\\n    if ret == True:\\n        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\\n        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)\\n        # apply meanshift to get the new location\\n        ret, track_window = cv.meanShift(dst, track_window, term_crit)\\n        # Draw it on image\\n        x,y,w,h = track_window\\n        img2 = cv.rectangle(frame, (x,y), (x+w,y+h), 255,2)\\n        cv.imshow('img2',img2)\\n        k = cv.waitKey(30) & 0xff\\n        if k == 27:\\n            break\\n    else:\\n        break\\ncv.destroyAllWindows()\\ncap.release()\n```\n\n----------------------------------------\n\nTITLE: Draw Contours on Mat in OpenCV Java\nDESCRIPTION: Creates and processes a drawing canvas by iterating over contours, drawing them and associated shapes in Java. Utilizes OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nMat drawing = Mat.zeros(edges.size(), CvType.CV_8UC3);\nRandom rng = new Random(12345);\nfor (int i = 0; i < contours.size(); i++) {\n    Scalar color = new Scalar(rng.nextInt(256), rng.nextInt(256), rng.nextInt(256));\n    Imgproc.drawContours(drawing, contours, i, color);\n    Imgproc.rectangle(drawing, rect.tl(), rect.br(), color, 2);\n    Imgproc.circle(drawing, center, (int) radius[0], color, 2);\n}\n```\n\n----------------------------------------\n\nTITLE: Detecting Subpixel Corners using OpenCV in Java\nDESCRIPTION: Java implementation of corner detection with subpixel accuracy using OpenCV's cornerSubPix function. The code demonstrates image loading, corner detection, and refinement of corner positions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/corner_subpixels/corner_subpixels.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.*;\nimport org.opencv.highgui.HighGui;\nimport org.opencv.imgcodecs.Imgcodecs;\nimport org.opencv.imgproc.Imgproc;\n\nclass CornerSubPixDemo {\n    private Mat src = new Mat();\n    private Mat srcGray = new Mat();\n    private int maxCorners = 10;\n    private int maxTrackbar = 25;\n    private RNG rng = new RNG(12345);\n    private String sourceWindow = \"Image\";\n\n    public void run(String[] args) {\n        String filename = args.length > 0 ? args[0] : \"../data/pic3.png\";\n        src = Imgcodecs.imread(filename);\n        if (src.empty()) {\n            System.err.println(\"Cannot read image: \" + filename);\n            System.exit(0);\n        }\n\n        Imgproc.cvtColor(src, srcGray, Imgproc.COLOR_BGR2GRAY);\n\n        HighGui.namedWindow(sourceWindow);\n        HighGui.createTrackbar(\"Max corners:\", sourceWindow, null, maxTrackbar, (val) -> {\n            maxCorners = Math.max(val, 1);\n            goodFeaturesToTrack_Demo();\n        });\n\n        goodFeaturesToTrack_Demo();\n\n        HighGui.waitKey();\n        System.exit(0);\n    }\n\n    private void goodFeaturesToTrack_Demo() {\n        MatOfPoint corners = new MatOfPoint();\n        double qualityLevel = 0.01;\n        double minDistance = 10;\n        int blockSize = 3, gradientSize = 3;\n        boolean useHarrisDetector = false;\n        double k = 0.04;\n\n        Mat copy = src.clone();\n\n        Imgproc.goodFeaturesToTrack(srcGray, corners, maxCorners, qualityLevel, minDistance, new Mat(),\n                blockSize, gradientSize, useHarrisDetector, k);\n\n        System.out.println(\"** Number of corners detected: \" + corners.rows());\n\n        Point[] cornersArr = corners.toArray();\n        for (int i = 0; i < cornersArr.length; i++) {\n            Imgproc.circle(copy, cornersArr[i], 4,\n                    new Scalar(rng.nextInt(256), rng.nextInt(256), rng.nextInt(256)), Imgproc.FILLED);\n        }\n\n        HighGui.imshow(\"Sharp corners\", copy);\n\n        Size winSize = new Size(5, 5);\n        Size zeroZone = new Size(-1, -1);\n        TermCriteria criteria = new TermCriteria(TermCriteria.EPS + TermCriteria.COUNT, 40, 0.001);\n\n        MatOfPoint2f corners2f = new MatOfPoint2f(corners.toArray());\n        Imgproc.cornerSubPix(srcGray, corners2f, winSize, zeroZone, criteria);\n\n        Point[] refinedCorners = corners2f.toArray();\n        for (int i = 0; i < refinedCorners.length; i++) {\n            System.out.println(String.format(\" -- Refined Corner [%d] (%.2f,%.2f)\", i, refinedCorners[i].x, refinedCorners[i].y));\n        }\n    }\n}\n\npublic class CornerSubPixDemoRun {\n    public static void main(String[] args) {\n        // Load the native OpenCV library\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\n\n        new CornerSubPixDemo().run(args);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Face and Eye Detection with OpenCV.js in Node.js\nDESCRIPTION: A complete Node.js application that uses OpenCV.js to detect faces and eyes in an image. It loads pre-trained Haar cascade classifiers, processes the image, draws rectangles around detected features, and saves the output as a JPEG file. The code demonstrates how to use OpenCV's computer vision capabilities in a JavaScript environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_nodejs/js_nodejs.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { Canvas, createCanvas, Image, ImageData, loadImage } = require('canvas');\nconst { JSDOM } = require('jsdom');\nconst { writeFileSync, existsSync, mkdirSync } = require('fs');\n\n(async () => {\n  await loadOpenCV();\n\n  const image = await loadImage('lena.jpg');\n  const src = cv.imread(image);\n  let gray = new cv.Mat();\n  cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY, 0);\n  let faces = new cv.RectVector();\n  let eyes = new cv.RectVector();\n  let faceCascade = new cv.CascadeClassifier();\n  let eyeCascade = new cv.CascadeClassifier();\n\n  // Load pre-trained classifier files. Notice how we reference local files using relative paths just\n  // like we normally would do\n  faceCascade.load('./haarcascade_frontalface_default.xml');\n  eyeCascade.load('./haarcascade_eye.xml');\n\n  let mSize = new cv.Size(0, 0);\n  faceCascade.detectMultiScale(gray, faces, 1.1, 3, 0, mSize, mSize);\n  for (let i = 0; i < faces.size(); ++i) {\n    let roiGray = gray.roi(faces.get(i));\n    let roiSrc = src.roi(faces.get(i));\n    let point1 = new cv.Point(faces.get(i).x, faces.get(i).y);\n    let point2 = new cv.Point(faces.get(i).x + faces.get(i).width, faces.get(i).y + faces.get(i).height);\n    cv.rectangle(src, point1, point2, [255, 0, 0, 255]);\n    eyeCascade.detectMultiScale(roiGray, eyes);\n    for (let j = 0; j < eyes.size(); ++j) {\n      let point1 = new cv.Point(eyes.get(j).x, eyes.get(j).y);\n      let point2 = new cv.Point(eyes.get(j).x + eyes.get(j).width, eyes.get(j).y + eyes.get(j).height);\n      cv.rectangle(roiSrc, point1, point2, [0, 0, 255, 255]);\n    }\n    roiGray.delete();\n    roiSrc.delete();\n  }\n\n  const canvas = createCanvas(image.width, image.height);\n  cv.imshow(canvas, src);\n  writeFileSync('output3.jpg', canvas.toBuffer('image/jpeg'));\n  src.delete(); gray.delete(); faceCascade.delete(); eyeCascade.delete(); faces.delete(); eyes.delete()\n})();\n\n/**\n * Loads opencv.js.\n *\n * Installs HTML Canvas emulation to support `cv.imread()` and `cv.imshow`\n *\n * Mounts given local folder `localRootDir` in emscripten filesystem folder `rootDir`. By default it will mount the local current directory in emscripten `/work` directory. This means that `/work/foo.txt` will be resolved to the local file `./foo.txt`\n * @param {string} rootDir The directory in emscripten filesystem in which the local filesystem will be mount.\n * @param {string} localRootDir The local directory to mount in emscripten filesystem.\n * @returns {Promise} resolved when the library is ready to use.\n */\nfunction loadOpenCV(rootDir = '/work', localRootDir = process.cwd()) {\n  if(global.Module && global.Module.onRuntimeInitialized && global.cv && global.cv.imread) {\n    return Promise.resolve()\n  }\n  return new Promise(resolve => {\n    installDOM()\n    global.Module = {\n      onRuntimeInitialized() {\n        // We change emscripten current work directory to 'rootDir' so relative paths are resolved\n        // relative to the current local folder, as expected\n        cv.FS.chdir(rootDir)\n        resolve()\n      },\n      preRun() {\n        // preRun() is another callback like onRuntimeInitialized() but is called just before the\n        // library code runs. Here we mount a local folder in emscripten filesystem and we want to\n        // do this before the library is executed so the filesystem is accessible from the start\n        const FS = global.Module.FS\n        // create rootDir if it doesn't exists\n        if(!FS.analyzePath(rootDir).exists) {\n          FS.mkdir(rootDir);\n        }\n        // create localRootFolder if it doesn't exists\n        if(!existsSync(localRootDir)) {\n          mkdirSync(localRootDir, { recursive: true});\n        }\n        // FS.mount() is similar to Linux/POSIX mount operation. It basically mounts an external\n        // filesystem with given format, in given current filesystem directory.\n        FS.mount(FS.filesystems.NODEFS, { root: localRootDir}, rootDir);\n      }\n    };\n    global.cv = require('./opencv.js')\n  });\n}\n\nfunction installDOM(){\n  const dom = new JSDOM();\n  global.document = dom.window.document;\n  global.Image = Image;\n  global.HTMLCanvasElement = Canvas;\n  global.ImageData = ImageData;\n  global.HTMLImageElement = Image;\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Convex Hull in C++ using OpenCV\nDESCRIPTION: This C++ code snippet demonstrates loading an image, finding its contours after thresholding, and then computing and drawing the convex hull for each contour using the `cv::convexHull` and `cv::drawContours` functions from the OpenCV library. It requires the OpenCV core, imgproc, imgcodecs, and highgui modules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/hull/hull.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n@add_toggle_cpp\nThis tutorial code's is shown lines below. You can also download it from\n[here](https://github.com/opencv/opencv/tree/4.x/samples/cpp/tutorial_code/ShapeDescriptors/hull_demo.cpp)\n@include samples/cpp/tutorial_code/ShapeDescriptors/hull_demo.cpp\n@end_toggle\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Post-Processing Kernel in G-API\nDESCRIPTION: Demonstrates implementation of a custom kernel for post-processing SSD detection results. Shows kernel definition and implementation for processing inference outputs.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/interactive_face_detection/interactive_face_detection.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nG_API_OP(PostProc,\n         <std::tuple<cv::GMat, std::vector<cv::Rect>, std::vector<cv::Str>, std::vector<cv::Str>>\n         (cv::GMat, cv::GMat, cv::GMat, cv::GMat, float, cv::Size)>,\n         \"custom.post_proc\") {\n    static std::tuple<cv::GArrayDesc, cv::GArrayDesc, cv::GArrayDesc>\n        outMeta(const cv::GMatDesc&, const cv::GMatDesc&,\n                const cv::GMatDesc&, const cv::GMatDesc&,\n                float, const cv::Size&) {\n        return std::make_tuple(cv::empty_array_desc(),\n                              cv::empty_array_desc(),\n                              cv::empty_array_desc());\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV Example Source Files using CMake\nDESCRIPTION: This CMake command uses the OpenCV-specific function `ocv_install_example_src` to designate C++ source files (*.cpp), header files (*.hpp), and the CMakeLists.txt file itself for installation as part of the project's examples. The 'cpp' argument likely specifies the subdirectory within the installation path.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/openvx/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nocv_install_example_src(cpp *.cpp *.hpp CMakeLists.txt)\n```\n\n----------------------------------------\n\nTITLE: Capturing and Visualizing Orbbec UVC Depth and Color Images - OpenCV C++\nDESCRIPTION: This code snippet showcases how to use OpenCV in C++ to access an Orbbec UVC depth camera, retrieve intrinsic parameters, grab color (BGR) and depth frames, normalize and colorize depth maps, overlay depth data onto the color image, display all outputs, and handle keyboard interrupts. Key dependencies are OpenCV built with Orbbec support and an appropriate build environment. Intrinsic camera parameters are retrieved for further use, and depth overlays are produced by resizing and blending images. The loop runs until a key is pressed, then resources are released upon completion.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_uvc.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// Open Orbbec Depth Sensor\\ncv::VideoCapture obsensorCapture(0, cv::CAP_OBSENSOR);\\nif (!obsensorCapture.isOpened()) {\\n    std::cerr << \"Failed to open Orbbec device!\" << std::endl;\\n    return -1;\\n}\\n\\n// Retrieve Camera Intrinsic Parameters\\ndouble fx = obsensorCapture.get(cv::CAP_PROP_OBSENSOR_INTRINSIC_FX);\\ndouble fy = obsensorCapture.get(cv::CAP_PROP_OBSENSOR_INTRINSIC_FY);\\ndouble cx = obsensorCapture.get(cv::CAP_PROP_OBSENSOR_INTRINSIC_CX);\\ndouble cy = obsensorCapture.get(cv::CAP_PROP_OBSENSOR_INTRINSIC_CY);\\n\\nwhile (true) {\\n    if (!obsensorCapture.grab())\\n        continue;\\n\\n    cv::Mat image;\\n    if (obsensorCapture.retrieve(image, cv::CAP_OBSENSOR_BGR_IMAGE)) {\\n        cv::imshow(\"BGR\", image);\\n    }\\n\\n    cv::Mat depthMap;\\n    if (obsensorCapture.retrieve(depthMap, cv::CAP_OBSENSOR_DEPTH_MAP)) {\\n        int minDepth = 300, maxDepth = 5000;\\n        depthMap.setTo(0, depthMap < minDepth);\\n        depthMap.setTo(0, depthMap > maxDepth);\\n        cv::Mat adjDepthMap;\\n        double scale = 255.0 / (maxDepth - minDepth);\\n        depthMap.convertTo(adjDepthMap, CV_8U, scale, -minDepth * scale);\\n        cv::applyColorMap(adjDepthMap, adjDepthMap, cv::COLORMAP_JET);\\n        cv::imshow(\"DEPTH\", adjDepthMap);\\n\\n        // Overlay Depth Map on BGR Image\\n        if (!image.empty()) {\\n            cv::Mat resizedAdjDepthMap;\\n            cv::resize(adjDepthMap, resizedAdjDepthMap, image.size());\\n            double alpha = 0.35;\\n            cv::addWeighted(resizedAdjDepthMap, alpha, image, 1 - alpha, 0.0, image);\\n            cv::imshow(\"DepthToColor\", image);\\n        }\\n    }\\n\\n    if (cv::pollKey() >= 0)\\n        break;\\n}\\n\\nobsensorCapture.release();\n```\n\n----------------------------------------\n\nTITLE: Defining a Scalar Color in Java\nDESCRIPTION: Creating a Scalar object to represent a BGR color in OpenCV Java. The Scalar represents a 3-element vector with Blue, Green, and Red values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nScalar( a, b, c )\n```\n\n----------------------------------------\n\nTITLE: Illustrating Fused Layer Computation (Pseudocode)\nDESCRIPTION: This pseudocode shows the resulting fused Halide function after combining the Convolution, Scale, and ReLU operations from the previous example. The entire computation is expressed within the `relu` function, meaning only this single fused function needs scheduling directives, simplifying optimization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_halide_scheduling/dnn_halide_scheduling.markdown#2025-04-22_snippet_2\n\nLANGUAGE: pseudocode\nCODE:\n```\nrelu(x, y, c, n) = max((sum(...) + bias(c)) * weights(c), 0);\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Target for Copying Java Source Files in CMake\nDESCRIPTION: This CMake code appends the `gen_opencv_java_source` script to the `depends` list. It then defines a custom target `${the_module}_android_source_copy` using the `ocv_copyfiles_add_target` macro (likely defined elsewhere in the OpenCV CMake setup) to handle the copying of Java source files for the Android SDK build. Finally, it removes a dependency helper file to ensure this copy target is rebuilt if CMake is re-run.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/android_sdk/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND depends gen_opencv_java_source \"${OPENCV_DEPHELPER}/gen_opencv_java_source\")\nocv_copyfiles_add_target(${the_module}_android_source_copy JAVA_SRC_COPY \"Copy Java(Android SDK) source files\" ${depends})\nfile(REMOVE \"${OPENCV_DEPHELPER}/${the_module}_android_source_copy\")  # force rebuild after CMake run\n```\n\n----------------------------------------\n\nTITLE: Brute-Force Matching with OpenCV in Python\nDESCRIPTION: Python code utilizing a brute-force matcher and Hamming distance to locate matches between image descriptors with OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nsamples/python/tutorial_code/features2D/akaze_matching/AKAZE_match.py 2-nn matching\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV-Python Package\nDESCRIPTION: Command to install the pre-built OpenCV-Python package from Ubuntu repositories.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_ubuntu/py_setup_in_ubuntu.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install python3-opencv\n```\n\n----------------------------------------\n\nTITLE: Defining a Point in C++\nDESCRIPTION: Two different ways to define a 2D point using the cv::Point structure in C++. A Point represents a 2D coordinate with x and y values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nPoint pt;\npt.x = 10;\npt.y = 8;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nPoint pt =  Point(10, 8);\n```\n\n----------------------------------------\n\nTITLE: Displaying Image using imshow and waitKey in OpenCV in Python\nDESCRIPTION: Demonstrates displaying an image array in Python using cv2.imshow and waiting for keyboard input. Requires cv2 (opencv-python) and a GUI backend. Window opens with given name; user keys close the window.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\ncv2.imshow('Window', img)\\ncv2.waitKey(0)\n```\n\n----------------------------------------\n\nTITLE: Defining OpenCV Photo Module Dependencies\nDESCRIPTION: Defines the photo module with required and optional dependencies, including CUDA modules and language bindings for Java, Objective-C, Python, and JavaScript.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/photo/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nocv_define_module(photo opencv_imgproc OPTIONAL opencv_cudaarithm opencv_cudaimgproc WRAP java objc python js)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Installing Data Files with CMake - CMake\nDESCRIPTION: This snippet uses CMake commands to collect Haar and LBP cascade XML files from specified directories and configures rules for their installation into the appropriate paths within the OpenCV installation hierarchy. It employs the file(GLOB ...) pattern to aggregate matching files and the install(FILES ...) and install(DIRECTORY ...) commands to deploy these files and directories, respectively. Conditional logic ensures that test data is only installed if certain variables are set. Required dependencies include a standard CMake environment, the presence of haarcascades and lbpcascades directories containing XML files, and proper variable configuration; variables like OPENCV_OTHER_INSTALL_PATH and OPENCV_TEST_DATA_INSTALL_PATH must be defined. The primary input is the set of XML files, and the expected output is their presence in the installation tree. Limitations include the need for proper path variable setup and sufficient permissions during installation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/data/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB HAAR_CASCADES haarcascades/*.xml)\nfile(GLOB LBP_CASCADES lbpcascades/*.xml)\n\ninstall(FILES ${HAAR_CASCADES} DESTINATION ${OPENCV_OTHER_INSTALL_PATH}/haarcascades COMPONENT libs)\ninstall(FILES ${LBP_CASCADES}  DESTINATION ${OPENCV_OTHER_INSTALL_PATH}/lbpcascades  COMPONENT libs)\n\nif(INSTALL_TESTS AND OPENCV_TEST_DATA_PATH)\n  install(DIRECTORY \"${OPENCV_TEST_DATA_PATH}/\" DESTINATION \"${OPENCV_TEST_DATA_INSTALL_PATH}\" COMPONENT \"tests\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Applying Blur Filter with OpenCV in Java\nDESCRIPTION: This Java snippet shows how to utilize OpenCV's blur() function for image smoothing using a normalized box filter. Prerequisites include OpenCV. The function requires parameters for source, destination image, kernel size, and anchor point.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/Smoothing/Smoothing.java blur\n```\n\n----------------------------------------\n\nTITLE: ResNet-50 ONNX Conversion Command - Console\nDESCRIPTION: This command invokes the conversion pipeline specifically for the ResNet-50 model. It mirrors the batch conversion example but hardcodes the model name for clarity and reproducibility. Results will be placed in the directory controlled by project configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_7\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name resnet50 --evaluate False\n```\n\n----------------------------------------\n\nTITLE: Plot Histogram with Matplotlib\nDESCRIPTION: Uses Matplotlib's hist() function to directly plot a histogram from image data without pre-calculation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_begins/py_histogram_begins.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('home.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nplt.hist(img.ravel(),256,[0,256]); plt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining 3D Cube Corner Points with OpenCV (Python)\nDESCRIPTION: Defines the coordinates of the eight 3D vertices of a cube for use in projecting and drawing on the image. Points are specified in the same size units as the chessboard squares for scale consistency. This array is to be used with the pose estimation and projection routines to correctly render a cube's outline according to real-world orientation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_pose/py_pose.markdown#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\naxis = np.float32([[0,0,0], [0,3,0], [3,3,0], [3,0,0],\\n                   [0,0,-3],[0,3,-3],[3,3,-3],[3,0,-3] ])\n```\n\n----------------------------------------\n\nTITLE: Retrieving FourCC Codec from Input Video using OpenCV (C++)\nDESCRIPTION: Shows how to obtain the FourCC (Four Character Code) identifier of the codec used in an input video file. It opens the input video using `cv::VideoCapture`, retrieves the `CAP_PROP_FOURCC` property using the `get` method, and casts the returned double value to an integer. Requires an initialized `VideoCapture` object.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_write.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nVideoCapture inputVideo(source);                                // Open input\nint ex = static_cast<int>(inputVideo.get(CAP_PROP_FOURCC));     // Get Codec Type- Int form\n```\n\n----------------------------------------\n\nTITLE: Outputting 3D Points in OpenCV C++\nDESCRIPTION: Demonstrates outputting 3D point objects using the << operator, showing OpenCV's support for streaming complex data structures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\nPoint3f P3f(2, 6, 7);\ncout << \"Point (3D) = \" << P3f << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading OpenCV Mat Objects to XML/YAML/JSON - OpenCV Python\nDESCRIPTION: These Python snippets illustrate writing and reading OpenCV Mat (NumPy array) objects with FileStorage. Use write() for writing and getNode().mat() for reading. Handles various array types and shapes supported by OpenCV and NumPy. Requires opencv-python and numpy installed. The variable must be a NumPy array for writing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\\nm = np.eye(3)\\nfs.write('matrix', m)\\n\\nmat = fs.getNode('matrix').mat()\n```\n\n----------------------------------------\n\nTITLE: Implementing Hit-or-Miss Transform in Python using OpenCV\nDESCRIPTION: This Python code shows how to implement the Hit-or-Miss transform using OpenCV's morphologyEx() function to detect specific patterns in binary images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/hitOrMiss/hitOrMiss.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# Create an image\ninput_image = np.array(([\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 255, 255, 255, 0, 0, 0, 255],\n    [0, 255, 255, 255, 0, 0, 0, 0],\n    [0, 255, 255, 255, 0, 255, 0, 0],\n    [0, 0, 255, 0, 0, 0, 0, 0],\n    [0, 0, 255, 0, 0, 255, 255, 0],\n    [0, 255, 0, 255, 0, 0, 255, 0],\n    [0, 255, 255, 255, 0, 0, 0, 0]]), dtype=\"uint8\")\n\n# Set kernel\nkernel = np.array(([\n    [0, 1, 0],\n    [1, -1, 1],\n    [0, 1, 0]]), dtype=\"int\")\n\noutput_image = cv.morphologyEx(input_image, cv.MORPH_HITMISS, kernel)\n\nrate = 50\nkernel_display = (kernel + 1) * 127\nkernel_display = np.uint8(kernel_display)\n\nkernel_display = cv.resize(kernel_display, None, fx=rate, fy=rate, interpolation=cv.INTER_NEAREST)\ninput_disp = cv.resize(input_image, None, fx=rate, fy=rate, interpolation=cv.INTER_NEAREST)\noutput_disp = cv.resize(output_image, None, fx=rate, fy=rate, interpolation=cv.INTER_NEAREST)\n\ncv.imshow(\"kernel\", kernel_display)\ncv.imshow(\"Original\", input_disp)\ncv.imshow(\"Hit or Miss\", output_disp)\ncv.waitKey(0)\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Initializing VideoCapture with MSMF Backend for Camera Input in C++\nDESCRIPTION: This snippet demonstrates how to create a VideoCapture object to capture video from the default camera using the Microsoft Media Foundation (MSMF) backend. It shows two methods: using the constructor and using the open() method.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/videoio/doc/videoio_overview.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n//declare a capture object\ncv::VideoCapture cap(0, cv::CAP_MSMF);\n\n//or specify the apiPreference with open\ncap.open(0, cv::CAP_MSMF);\n```\n\n----------------------------------------\n\nTITLE: Computing Chessboard Object Points in OpenCV C++\nDESCRIPTION: Code to compute 3D object points for a chessboard pattern. It calculates the real-world coordinates of each corner based on a known square size and assigns Z=0 since the board is planar.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nstd::vector<cv::Point3f> objectPoints;\nfor (int i = 0; i < patternSize.height; ++i) {\n    for (int j = 0; j < patternSize.width; ++j) {\n        objectPoints.push_back(cv::Point3f(j * squareSize, i * squareSize, 0.0f));\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Identifying Python-Wrapped Modules in CMake for OpenCV\nDESCRIPTION: Iterates through all OpenCV modules to identify those with Python wrappers. It builds a list of modules that should be included in Python tests.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/python/test/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(OPENCV_PYTHON_MODULES)\nforeach(m ${OPENCV_MODULES_BUILD})\n  if(\";${OPENCV_MODULE_${m}_WRAPPERS};\" MATCHES \";python.*;\" AND HAVE_${m})\n    list(APPEND OPENCV_PYTHON_MODULES ${m})\n    #message(STATUS \"\\t${m}\")\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Persisting and Reloading kNN Data with NumPy\nDESCRIPTION: The Python code segment shows how to save and load OCR model data using NumPy functions like np.savez and np.load. This method decreases startup time by reusing previously computed training data. It also suggests converting data to np.uint8 for storage efficiency. Dependencies include NumPy. Files related to model data are inputs and outputs of this code.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_knn/py_knn_opencv/py_knn_opencv.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Save the data\nnp.savez('knn_data.npz',train=train, train_labels=train_labels)\n\n# Now load the data\nwith np.load('knn_data.npz') as data:\n    print( data.files )\n    train = data['train']\n    train_labels = data['train_labels']\n```\n\n----------------------------------------\n\nTITLE: Implementing Gamma Correction with Look-Up Table in Java using OpenCV\nDESCRIPTION: This Java code snippet performs gamma correction on an image (`src`) using OpenCV bindings. It computes a look-up table (`lut`) containing the gamma-transformed values (O = ((I/255)^gamma) * 255) for each possible pixel intensity (0-255), storing them as bytes. The `Core.LUT` function is then used to efficiently apply this transformation to the source image, storing the result in the destination matrix (`dst`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/basic_linear_transform/basic_linear_transform.markdown#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n//![changing-contrast-brightness-gamma-correction]\n        // Calculate gamma correction lookup table\n        Mat lut = new Mat(1, 256, CvType.CV_8UC1);\n        lut.setTo(new Scalar(0));\n        double gammaValue = sliderGamma.getValue() / 100.0;\n        byte[] lutData = new byte[(int) (lut.total() * lut.channels())];\n        for (int i = 0; i < 256; i++) {\n            double v = Math.pow(i / 255.0, gammaValue) * 255.0;\n            if( v < 0 ) {\n                v=0;\n            }\n            if( v > 255 ) {\n                v=255;\n            }\n            lutData[i] = (byte) v;\n        }\n        lut.put(0, 0, lutData);\n\n        Core.LUT(src, lut, dst);\n        //![changing-contrast-brightness-gamma-correction]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Applications with CMake\nDESCRIPTION: This CMake snippet adds the definition for compiling the \\'opencv_version\\' application with OpenCV core modules. It sets specific options and defines required for building on Windows platforms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/version/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nocv_add_application(opencv_version MODULES opencv_core SRCS opencv_version.cpp)\nif(WIN32)\n  ocv_add_application(opencv_version_win32 MODULES opencv_core SRCS opencv_version.cpp)\n  target_compile_definitions(opencv_version_win32 PRIVATE \"OPENCV_WIN32_API=1\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Uploading/Downloading Data to/from GPU Memory in C++\nDESCRIPTION: Demonstrates how to transfer image data between CPU memory (`cv::Mat`) and GPU memory (`cv::cuda::GpuMat`) in C++. It shows creating a `GpuMat` object (`gI`), uploading data from a `Mat` object (`I1`) using the `upload()` method, and downloading data back to a `Mat` object using simple assignment (which implicitly calls `download()`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nMat I1;         // Main memory item - read image into with imread for example\ngpu::GpuMat gI; // GPU matrix - for now empty\ngI1.upload(I1); // Upload a data from the system memory to the GPU memory\n\nI1 = gI1;       // Download, gI1.download(I1) will work too\n```\n\n----------------------------------------\n\nTITLE: Verifying CMake Installation on macOS\nDESCRIPTION: This command checks the installed version of CMake to ensure it's correctly installed on the system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/macos_install/macos_install.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncmake --version\n```\n\n----------------------------------------\n\nTITLE: Creating Mask and Extracting Pixel Points for Contour in OpenCV Python\nDESCRIPTION: This snippet demonstrates how to create a binary mask for a specific contour and extract the coordinates of all pixels belonging to that contour. It initializes a zero mask using `np.zeros` with the shape of the input image (`imgray`), draws the filled contour onto the mask using `cv.drawContours`, and then finds the coordinates of the white pixels (the contour points) using NumPy's `np.nonzero` and `np.transpose`. An alternative using `cv.findNonZero` is commented out. Requires an existing contour `cnt`, the source image `imgray` (for shape), and NumPy (`np`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_properties/py_contour_properties.markdown#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmask = np.zeros(imgray.shape,np.uint8)\ncv.drawContours(mask,[cnt],0,255,-1)\npixelpoints = np.transpose(np.nonzero(mask))\n#pixelpoints = cv.findNonZero(mask)\n```\n\n----------------------------------------\n\nTITLE: Including OpenCV.js Asynchronously with Callback (HTML)\nDESCRIPTION: Shows how to include the OpenCV.js library asynchronously using the `async` attribute in the `<script>` tag. The `onload` attribute specifies a JavaScript function (`onOpenCvReady()`) that will be called once the `opencv.js` script has finished loading and executing. This prevents the script from blocking page rendering.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_usage/js_usage.markdown#2025-04-22_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<script async src=\"opencv.js\" onload=\"onOpenCvReady();\" type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Feature Matching with FLANN in C++\nDESCRIPTION: This C++ implementation demonstrates how to use the FlannBasedMatcher for efficient feature matching with SURF descriptors. It loads two images, detects and describes features, matches them using FLANN, and filters the matches using Lowe's distance ratio test.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_flann_matcher/feature_flann_matcher.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <stdio.h>\n#include <iostream>\n#include <opencv2/core.hpp>\n#include <opencv2/features2d.hpp>\n#include <opencv2/highgui.hpp>\n#include <opencv2/xfeatures2d.hpp>\n#include <opencv2/imgcodecs.hpp>\n\nusing namespace std;\nusing namespace cv;\nusing namespace cv::xfeatures2d;\n\nconst char* keys =\n    \"{ help h |                  | Print help message. }\"\n    \"{ input1 | box.png          | Path to input image 1. }\"\n    \"{ input2 | box_in_scene.png  | Path to input image 2. }\";\n\nint main( int argc, char* argv[] )\n{\n    CommandLineParser parser( argc, argv, keys );\n    Mat img_object = imread( samples::findFile( parser.get<String>(\"input1\") ), IMREAD_GRAYSCALE );\n    Mat img_scene = imread( samples::findFile( parser.get<String>(\"input2\") ), IMREAD_GRAYSCALE );\n    if ( img_object.empty() || img_scene.empty() )\n    {\n        cout << \"Could not open or find the image!\\n\" << endl;\n        parser.printMessage();\n        return -1;\n    }\n\n    //-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\n    int minHessian = 400;\n    Ptr<SURF> detector = SURF::create( minHessian );\n    std::vector<KeyPoint> keypoints_object, keypoints_scene;\n    Mat descriptors_object, descriptors_scene;\n    detector->detectAndCompute( img_object, noArray(), keypoints_object, descriptors_object );\n    detector->detectAndCompute( img_scene, noArray(), keypoints_scene, descriptors_scene );\n\n    //-- Step 2: Matching descriptor vectors using FLANN matcher\n    FlannBasedMatcher matcher;\n    std::vector< DMatch > matches;\n    matcher.match( descriptors_object, descriptors_scene, matches );\n\n    double max_dist = 0; double min_dist = 100;\n    //-- Quick calculation of max and min distances between keypoints\n    for( int i = 0; i < descriptors_object.rows; i++ )\n    {\n        double dist = matches[i].distance;\n        if( dist < min_dist ) min_dist = dist;\n        if( dist > max_dist ) max_dist = dist;\n    }\n    printf(\"-- Max dist : %f \\n\", max_dist );\n    printf(\"-- Min dist : %f \\n\", min_dist );\n\n    //-- Draw only \"good\" matches (i.e. whose distance is less than 2*min_dist,\n    //-- or a small arbitary value ( 0.02 ) in the event that min_dist is very\n    //-- small)\n    //-- PS.- radiusMatch can also be used here.\n    std::vector< DMatch > good_matches;\n    for( int i = 0; i < descriptors_object.rows; i++ )\n    {\n        if( matches[i].distance <= max(2*min_dist, 0.02) )\n        {\n            good_matches.push_back( matches[i]);\n        }\n    }\n\n    //-- Draw only \"good\" matches\n    Mat img_matches;\n    drawMatches( img_object, keypoints_object, img_scene, keypoints_scene,\n                good_matches, img_matches, Scalar::all(-1), Scalar::all(-1),\n                std::vector<char>(), DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS );\n\n    //-- Show detected matches\n    imshow( \"Good Matches\", img_matches );\n    waitKey();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Target for JavaScript Bindings Generation in CMake\nDESCRIPTION: Defines a custom target 'gen_opencv_js_source' that depends on the generated bindings and specifies source files for the generation process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/js/generator/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(gen_opencv_js_source\n  # excluded from all: ALL\n  DEPENDS ${bindings_cpp} \"${OPENCV_DEPHELPER}/gen_opencv_js_source\"\n  SOURCES\n      ${JS_SOURCE_DIR}/src/core_bindings.cpp\n      ${CMAKE_CURRENT_SOURCE_DIR}/embindgen.py\n      ${CMAKE_CURRENT_SOURCE_DIR}/templates.py\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing and Modifying Pixel Values (data property) with OpenCV.js - JavaScript\nDESCRIPTION: Accesses RGBA pixel values directly through the Mat.data array for continuous Mats, requiring calculation of the linear index. Useful for low-level manipulation or inspection of pixels, assuming the matrix is stored in a continuous memory block. Must check Mat continuity with isContinuous().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_basic_ops/js_basic_ops.markdown#2025-04-22_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet row = 3, col = 4;\nlet src = cv.imread(\"canvasInput\");\nif (src.isContinuous()) {\n    let R = src.data[row * src.cols * src.channels() + col * src.channels()];\n    let G = src.data[row * src.cols * src.channels() + col * src.channels() + 1];\n    let B = src.data[row * src.cols * src.channels() + col * src.channels() + 2];\n    let A = src.data[row * src.cols * src.channels() + col * src.channels() + 3];\n}\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Maps (Associative Containers) - OpenCV C++\nDESCRIPTION: This C++ snippet illustrates writing and reading associative maps (e.g., std::map) to XML/YAML/JSON files. The << operator is used for insertion with curly brace delimiters ({ ... }). Reading is done through the [] operator or FileNode. Only types supported by OpenCV and the serialization format are allowed. All keys must be unique.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nfs << \"mymap\" << \"{\" << \"one\" << 1 << \"two\" << 2 << \"}\";\\n\\nFileNode n = fs[\"mymap\"];\\nint one = (int)n[\"one\"];\n```\n\n----------------------------------------\n\nTITLE: Transforming OpenCV Segmentation Mask to Colored Visualization\nDESCRIPTION: Converts a segmentation mask from OpenCV's DNN model into a colored visualization by mapping class indices to PASCAL VOC colors. The mask is resized to match the original image dimensions and converted from BGR to RGB color space.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmask_height = segm_mask.shape[0]\nmask_width = segm_mask.shape[1]\n\nimg_height = original_img_shape[0]\nimg_width = original_img_shape[1]\n\n# convert mask values into PASCAL VOC colors\nprocessed_mask = np.stack([colors[color_id] for color_id in segm_mask.flatten()])\n\n# reshape mask into 3-channel image\nprocessed_mask = processed_mask.reshape(mask_height, mask_width, 3)\nprocessed_mask = cv2.resize(processed_mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST).astype(\n    np.uint8)\n\n# convert colored mask from BGR to RGB\nprocessed_mask = cv2.cvtColor(processed_mask, cv2.COLOR_BGR2RGB)\n```\n\n----------------------------------------\n\nTITLE: Implementing Hit-or-Miss Transform in Java using OpenCV\nDESCRIPTION: This Java implementation shows how to use OpenCV's morphologyEx() to perform the Hit-or-Miss transform on a binary image to detect specific patterns.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/hitOrMiss/hitOrMiss.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport org.opencv.core.*;\nimport org.opencv.highgui.HighGui;\nimport org.opencv.imgproc.Imgproc;\n\nclass HitMiss {\n    public void run() {\n        // Create an image\n        byte[] data = new byte[] {\n                0, 0, 0, 0, 0, 0, 0, 0,\n                0, 1, 1, 1, 0, 0, 0, 1,\n                0, 1, 1, 1, 0, 0, 0, 0,\n                0, 1, 1, 1, 0, 1, 0, 0,\n                0, 0, 1, 0, 0, 0, 0, 0,\n                0, 0, 1, 0, 0, 1, 1, 0,\n                0, 1, 0, 1, 0, 0, 1, 0,\n                0, 1, 1, 1, 0, 0, 0, 0\n                };\n        Mat inputImage = new Mat(8, 8, CvType.CV_8U);\n        inputImage.put(0, 0, data);\n        for (int i = 0; i < inputImage.rows(); i++) {\n            for (int j = 0; j < inputImage.cols(); j++) {\n                if (inputImage.get(i, j)[0] == 1) {\n                    inputImage.put(i, j, 255);\n                }\n            }\n        }\n\n        Mat kernel = new Mat(3, 3, CvType.CV_16S);\n        kernel.put(0, 0,\n                   0, 1, 0,\n                   1, -1, 1,\n                   0, 1, 0);\n\n        Mat outputImage = new Mat();\n        Imgproc.morphologyEx(inputImage, outputImage, Imgproc.MORPH_HITMISS, kernel);\n\n        final int rate = 50;\n        Mat ones = Mat.ones(3, 3, CvType.CV_16S);\n        Mat kernelDisp = new Mat();\n        Core.add(kernel, ones, kernelDisp);\n        Core.multiply(kernelDisp, new Scalar(127), kernelDisp);\n        kernelDisp.convertTo(kernelDisp, CvType.CV_8U);\n\n        Imgproc.resize(kernelDisp, kernelDisp, new Size(), rate, rate, Imgproc.INTER_NEAREST);\n        Imgproc.resize(inputImage, inputImage, new Size(), rate, rate, Imgproc.INTER_NEAREST);\n        Imgproc.resize(outputImage, outputImage, new Size(), rate, rate, Imgproc.INTER_NEAREST);\n\n        HighGui.imshow(\"kernel\", kernelDisp);\n        HighGui.imshow(\"Original\", inputImage);\n        HighGui.imshow(\"Hit or Miss\", outputImage);\n        HighGui.waitKey(0);\n        System.exit(0);\n    }\n}\n\npublic class HitMissDemo {\n    public static void main(String[] args) {\n        // Load the native OpenCV library\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\n\n        new HitMiss().run();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Source Images in C++\nDESCRIPTION: This C++ snippet demonstrates loading two source images (`src1`, `src2`) using `imread`. It includes basic error handling to check if the images were loaded successfully. These images will be used for alpha blending.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n//![load]\n/// Read image ( same size, same type )\nsrc1 = imread( samples::findFile(\"LinuxLogo.jpg\") );\nsrc2 = imread( samples::findFile(\"WindowsLogo.jpg\") );\n\nif( src1.empty() ) { cout << \"Error loading src1\" << endl; return EXIT_FAILURE; }\nif( src2.empty() ) { cout << \"Error loading src2\" << endl; return EXIT_FAILURE; }\n//![load]\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying Image Window in C++\nDESCRIPTION: Creates a named window titled \"Pyramids Demo\" and displays the initial source image within it using `namedWindow` and `imshow`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n    //![show_image]\n    /// Create window\n    namedWindow( window_name, WINDOW_AUTOSIZE );\n    imshow( window_name, src );\n    //![show_image]\n```\n\n----------------------------------------\n\nTITLE: Find Contours in OpenCV C++\nDESCRIPTION: This snippet finds image contours using OpenCV in C++. It outputs them to vectors for further processing. Dependencies include OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nstd::vector<std::vector<cv::Point>> contours;\nstd::vector<cv::Vec4i> hierarchy;\ncv::findContours(edges, contours, hierarchy, cv::RETR_TREE, cv::CHAIN_APPROX_SIMPLE);\n```\n\n----------------------------------------\n\nTITLE: Highlighting Support Vectors in Non-Linear SVM Classification\nDESCRIPTION: This snippet demonstrates how to obtain and highlight the support vectors in non-linear SVM classification using OpenCV. It uses the getSupportVectors method to identify support vectors and draws circles around them.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nMat sv = svm->getSupportVectors();\nfor (int i = 0; i < sv.rows; ++i)\n{\n    const float* v = sv.ptr<float>(i);\n    int x = (int)(v[0] * WIDTH);\n    int y = (int)(v[1] * HEIGHT);\n    circle(res, Point(x,y), 6, Scalar(128, 128, 128), 2);\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\nMat sv = svm.getSupportVectors();\nfor (int i = 0; i < sv.rows(); i++) {\n    double[] v = sv.get(i, 0);\n    int x = (int) (v[0] * WIDTH);\n    int y = (int) (v[1] * HEIGHT);\n    Imgproc.circle(res, new Point(x, y), 6, new Scalar(128, 128, 128), 2);\n}\n```\n\nLANGUAGE: Python\nCODE:\n```\nsv = svm.getSupportVectors()\nfor i in range(sv.shape[0]):\n    x = int(sv[i,0]*WIDTH)\n    y = int(sv[i,1]*HEIGHT)\n    cv.circle(res, (x,y), 6, (128, 128, 128), 2)\n```\n\n----------------------------------------\n\nTITLE: Example: Generating Windows Phone 8.1 x86 Project Files\nDESCRIPTION: A sequence of shell commands demonstrating how to create a build directory (`bin`), navigate into it, and then run CMake to generate Visual Studio 2013 project files for Windows Phone 8.1 x86, assuming the source code is in the parent directory (`../`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nmkdir bin\n```\n\nLANGUAGE: shell\nCODE:\n```\ncd bin\n```\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Visual Studio 12 2013\" -DCMAKE_SYSTEM_NAME=WindowsPhone -DCMAKE_SYSTEM_VERSION=8.1 ../\n```\n\n----------------------------------------\n\nTITLE: Instantiating Pretrained PyTorch FCN ResNet-50 Model\nDESCRIPTION: This Python code snippet initializes a pre-trained Fully Convolutional Network (FCN) model with a ResNet-50 backbone using the `torchvision.models.segmentation` module. Setting `pretrained=True` loads weights trained on a standard dataset (likely COCO or PASCAL VOC). The `original_model` variable will hold the PyTorch model object. Requires `torch` and `torchvision` libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# initialize PyTorch FCN ResNet-50 model\noriginal_model = models.segmentation.fcn_resnet50(pretrained=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Android Manifest for DNN App - Android/XML\nDESCRIPTION: This XML code snippet demonstrates how to configure the application's manifest to support the DNN object detection app. It sets up the necessary attributes in the <application> tag, such as the app label. Developers should extend this configuration by adding permissions (such as CAMERA) and activity declarations. This configuration is required to enable full screen, proper screen orientation, and camera usage for the application.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_dnn_intro.markdown#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?>\\n<manifest xmlns:android=\\\"http://schemas.android.com/apk/res/android\\\">\\n\\n    <application\\n        android:label=\\\"@string/app_name\\\">\n```\n\n----------------------------------------\n\nTITLE: Loading Images with OpenCV in Python\nDESCRIPTION: This snippet demonstrates how to load an image using OpenCV in Python with cv2.imread. It checks if the image is None to handle errors. The primary parameter is the image path; the output is a NumPy ndarray. Requires OpenCV installed (cv2 module).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\\n\\nsrc = cv2.imread('path_to_image', cv2.IMREAD_COLOR)\\nif src is None:\\n    print('Could not open or find the image!')\\n    exit()\\n\n```\n\n----------------------------------------\n\nTITLE: Facial Landmarks Post-Processing to Generate Contours\nDESCRIPTION: Custom operation that processes facial landmarks into contours for different face elements (eyes, mouth) and skin areas for selective image processing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n// Landmark post-processing to extract facial feature contours\nG_API_OP(landmarksToContours,\n         <std::tuple<cv::GArray<std::vector<Contour>>,\n                    cv::GArray<Contour>>\n         (cv::GArray<cv::GMat>, cv::GArray<cv::Rect>)>,\n         \"custom.ld_postproc\") {\n    static std::tuple<cv::GArrayDesc, cv::GArrayDesc> outMeta(const cv::GArrayDesc&,\n                                                              const cv::GArrayDesc&) {\n        return std::make_tuple(cv::empty_array_desc(), cv::empty_array_desc());\n    }\n};\n\n// Implementation of the landmarks post-processing kernel\nSTRUCT_KERNEL(GAPI_OCV_KERNEL, landmarksToContours, landmarksToContoursImpl) {\n    static void run(const std::vector<cv::Mat>& in_landmarks,\n                   const std::vector<cv::Rect>& in_ROIs,\n                   std::vector<std::vector<Contour>>& out_element_contours,\n                   std::vector<Contour>& out_face_contours) {\n        // Process landmarks for every face in the frame\n        out_element_contours.clear();\n        out_face_contours.clear();\n\n        GAPI_Assert(in_landmarks.size() == in_ROIs.size());\n\n        for (std::size_t i = 0; i < in_landmarks.size(); i++) {\n            auto& landmarks = in_landmarks[i];\n            const auto& ROI = in_ROIs[i];\n\n            // Basic landmarks validity check\n            GAPI_Assert(landmarks.depth() == CV_32F);\n            GAPI_Assert(landmarks.size[0] == 1);\n            GAPI_Assert(landmarks.size[1] == 70);\n\n            const float *ld_data = landmarks.ptr<float>();\n\n            // Scale landmarks from normalized coordinates to the original ROI\n            std::vector<cv::Point> cv_landmarks;\n            for (int j = 0; j < 35; ++j) {\n                int xy_index = j * 2;\n                int x = static_cast<int>(ld_data[xy_index]   * ROI.width)  + ROI.x;\n                int y = static_cast<int>(ld_data[xy_index+1] * ROI.height) + ROI.y;\n                cv_landmarks.emplace_back(x, y);\n            }\n\n            // Now get contours out of the keypoints\n            // --------------------------------------\n\n            // The full list of landmarks is available at\n            // https://github.com/opencv/open_model_zoo/blob/master/models/intel/facial-landmarks-35-adas-0002/description/facial-landmarks-35-adas-0002.md\n\n            std::vector<Contour> all_elements;\n\n            // Left eye\n            // Left eye landmark indices: 0 = far left corner, 1 = far right corner\n            Contour leye = getEyeContour({cv_landmarks[0], cv_landmarks[1]});\n            all_elements.emplace_back(std::move(leye));\n\n            // Right eye\n            // Right eye landmark indices: 2 = far left corner, 3 = far right corner\n            Contour reye = getEyeContour({cv_landmarks[2], cv_landmarks[3]});\n            all_elements.emplace_back(std::move(reye));\n\n            // Mouth (lips)\n            // Outer lip landmark indices: 6, 7, 8, 9 (clockwise from bottom)\n            Contour mouth;\n            mouth.reserve(4);\n            mouth.push_back(cv_landmarks[6]);\n            mouth.push_back(cv_landmarks[9]);\n            mouth.push_back(cv_landmarks[8]);\n            mouth.push_back(cv_landmarks[7]);\n            all_elements.emplace_back(std::move(mouth));\n\n            // Create face contour\n            // 10,11, ...., 19 are face contour landmarks (clockwise from left bottom corner)\n            Contour face;\n            for (int j = 19; j >= 10; --j) {\n                face.push_back(cv_landmarks[j]);\n            }\n\n            // Join contour with line (forehead) - simply connect first and last points with a line\n            face.push_back(cv_landmarks[10]);\n\n            // Add to the lists\n            out_element_contours.emplace_back(std::move(all_elements));\n            out_face_contours.emplace_back(std::move(face));\n        }\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Process Contours and Draw Shapes in OpenCV C++\nDESCRIPTION: Applies contour approximation and draws bounding rectangles and enclosing circles for each contour. Needs OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nfor (size_t i = 0; i < contours.size(); i++) {\n    cv::approxPolyDP(cv::Mat(contours[i]), contours_poly[i], 3, true);\n    boundRect[i] = cv::boundingRect(cv::Mat(contours_poly[i]));\n    cv::minEnclosingCircle(contours_poly[i], center[i], radius[i]);\n}\n```\n\n----------------------------------------\n\nTITLE: Finding and Drawing Epilines with OpenCV - Python\nDESCRIPTION: Calculates epilines for points in one image and draws them on another image using OpenCV. This requires matched points, the fundamental matrix, and images to draw on, outputting images with epilines overlaid. Depends on previous computations of matched points and the fundamental matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_epipolar_geometry/py_epipolar_geometry.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Find epilines corresponding to points in right image (second image) and\n# drawing its lines on left image\nlines1 = cv.computeCorrespondEpilines(pts2.reshape(-1,1,2), 2,F)\nlines1 = lines1.reshape(-1,3)\nimg5,img6 = drawlines(img1,img2,lines1,pts1,pts2)\n\n# Find epilines corresponding to points in left image (first image) and\n# drawing its lines on right image\nlines2 = cv.computeCorrespondEpilines(pts1.reshape(-1,1,2), 1,F)\nlines2 = lines2.reshape(-1,3)\nimg3,img4 = drawlines(img2,img1,lines2,pts2,pts1)\n\nplt.subplot(121),plt.imshow(img5)\nplt.subplot(122),plt.imshow(img3)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Capturing Video Stream in C++\nDESCRIPTION: Captures video stream from the default or a specified capturing device using OpenCV in C++. This is essential for processing live video data for thresholding operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold_inRange/threshold_inRange.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\\nint main() {\\n    cv::VideoCapture cap(0);\\n    if(!cap.isOpened()) {\\n        return -1;\\n    }\\n    // Further code\\n}\n```\n\n----------------------------------------\n\nTITLE: Initial Image Thresholding in OpenCV Python\nDESCRIPTION: Reads an image and applies Otsu's binarization to create an initial threshold. Converts image to grayscale and uses binary inverse thresholding.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_watershed/py_watershed.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('coins.png')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\ngray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\nret, thresh = cv.threshold(gray,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)\n```\n\n----------------------------------------\n\nTITLE: Calculating Morphological Gradient with OpenCV Python\nDESCRIPTION: Demonstrates how to calculate morphological gradient which shows the outline of objects by finding the difference between dilation and erosion.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ngradient = cv.morphologyEx(img, cv.MORPH_GRADIENT, kernel)\n```\n\n----------------------------------------\n\nTITLE: Detecting Subpixel Corners using OpenCV in Python\nDESCRIPTION: Python implementation of corner detection with subpixel accuracy using OpenCV's cornerSubPix function. The code demonstrates image loading, corner detection, and refinement of corner positions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/corner_subpixels/corner_subpixels.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nfrom math import sqrt\n\nsource_window = 'Image'\nmax_trackbar = 100\ncorner_count = 0\n\nrng = np.random.default_rng()\n\ndef goodFeaturesToTrack_Demo(val):\n    maxCorners = max(val, 1)\n\n    # Parameters for Shi-Tomasi algorithm\n    qualityLevel = 0.01\n    minDistance = 10\n    blockSize = 3\n    gradientSize = 3\n    useHarrisDetector = False\n    k = 0.04\n\n    # Copy the source image\n    copy = np.copy(src)\n\n    # Apply corner detection\n    corners = cv.goodFeaturesToTrack(src_gray, maxCorners, qualityLevel, minDistance, None,\n                                     blockSize=blockSize, gradientSize=gradientSize,\n                                     useHarrisDetector=useHarrisDetector, k=k)\n\n    print('Number of corners detected:', len(corners))\n    # Draw corners detected\n    radius = 4\n    for i in range(corners.shape[0]):\n        cv.circle(copy, (int(corners[i,0,0]), int(corners[i,0,1])), radius,\n                  (rng.integers(0,256), rng.integers(0,256), rng.integers(0,256)), cv.FILLED)\n\n    # Show what you got\n    cv.namedWindow('Sharp corners')\n    cv.imshow('Sharp corners', copy)\n\n    # Set the needed parameters to find the refined corners\n    winSize = (5, 5)\n    zeroZone = (-1, -1)\n    criteria = (cv.TERM_CRITERIA_EPS + cv.TermCriteria_COUNT, 40, 0.001)\n\n    # Calculate the refined corner locations\n    corners = cv.cornerSubPix(src_gray, corners, winSize, zeroZone, criteria)\n\n    # Write them down\n    for i in range(corners.shape[0]):\n        print(\" -- Refined Corner [\", i, \"]  (\", corners[i,0,0], \",\", corners[i,0,1], \")\")\n\n# Load source image and convert it to gray\nsrc = cv.imread(cv.samples.findFile('pic3.png'))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n\nsrc_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n\n# Create a window and a trackbar\ncv.namedWindow(source_window)\nmax_trackbar = 25\ncv.createTrackbar('Max corners:', source_window, max_trackbar, max_trackbar, goodFeaturesToTrack_Demo)\n\ncv.imshow(source_window, src)\ngoodFeaturesToTrack_Demo(max_trackbar)\n\ncv.waitKey()\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Results with OpenCV imshow in C++\nDESCRIPTION: This snippet shows how to display images (original and with detected lines) in separate windows using OpenCV in C++. It uses cv::imshow for display and cv::waitKey for event handling. Requires OpenCV GUI support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\nimshow(\"Source\", src);\\nimshow(\"Detected Lines (in red) - Standard Hough Line Transform\", cdst);\\nimshow(\"Detected Lines (in green) - Probabilistic Line Transform\", cdstP);\\nwaitKey();\\n\n```\n\n----------------------------------------\n\nTITLE: K-Means Clustering with Two-Dimensional Data in OpenCV\nDESCRIPTION: This code demonstrates K-means clustering on two-dimensional data (height and weight). It generates random 2D data points in two distinct groups, applies the clustering algorithm with 2 clusters, and visualizes the results with a scatter plot.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.markdown#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nX = np.random.randint(25,50,(25,2))\nY = np.random.randint(60,85,(25,2))\nZ = np.vstack((X,Y))\n\n# convert to np.float32\nZ = np.float32(Z)\n\n# define criteria and apply kmeans()\ncriteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\nret,label,center=cv.kmeans(Z,2,None,criteria,10,cv.KMEANS_RANDOM_CENTERS)\n\n# Now separate the data, Note the flatten()\nA = Z[label.ravel()==0]\nB = Z[label.ravel()==1]\n\n# Plot the data\nplt.scatter(A[:,0],A[:,1])\nplt.scatter(B[:,0],B[:,1],c = 'r')\nplt.scatter(center[:,0],center[:,1],s = 80,c = 'y', marker = 's')\nplt.xlabel('Height'),plt.ylabel('Weight')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Detecting Keypoints and Computing Descriptors in C++\nDESCRIPTION: This code snippet detects keypoints and computes descriptors for each frame in the video sequence using the specified detector.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_tracking/akaze_tracking.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n(*detector)(frame, noArray(), kp, desc);\n```\n\n----------------------------------------\n\nTITLE: Changing Image Type from 8UC1 to 32FC1 with OpenCV in Python\nDESCRIPTION: Converts an 8-bit image ndarray to float32 dtype in Python by NumPy astype. Can be used with cv2.convertScaleAbs for scaling. Required for some numerical algorithms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\nfloatMat = img.astype(np.float32)\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Tutorials Compilation - CMake - CMake\nDESCRIPTION: This CMake script automates the identification and compilation of Java tutorial directories within the OpenCV project. It requires ANT_EXECUTABLE and a compiled opencv_java target, uses CMake commands to traverse subdirectories for Java sources, and employs add_custom_target and add_custom_command to compile each detected tutorial directory with Apache Ant. Key parameters include the OpenCV source and binary directories, and it outputs compiled Java classes to a designated bin directory. The process is limited to environments with Ant and a properly configured opencv_java target, and is not intended for standalone use outside the build environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/java/tutorial_code/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# ----------------------------------------------------------------------------\\n#  CMake file for Java tutorials compilation.\\n#\\n# ----------------------------------------------------------------------------\\nif(NOT ANT_EXECUTABLE OR NOT TARGET opencv_java)\\n  return()\\nendif()\\n\\nproject(compile_java_tutorials)\\n\\nset(curdir \"${CMAKE_CURRENT_SOURCE_DIR}\")\\nset(opencv_tutorial_java_bin_dir \"${CMAKE_CURRENT_BINARY_DIR}/.compiled\")\\nset(TUTORIALS_DIRS \"\")\\n\\nfile(GLOB children RELATIVE ${curdir} ${curdir}/*/*)\\nforeach(child ${children})\\n  if(IS_DIRECTORY ${curdir}/${child})\\n    file(GLOB contains_java_files \"${child}/*.java\")\\n    if(contains_java_files)\\n      list(APPEND TUTORIALS_DIRS ${child})\\n    endif()\\n  endif()\\nendforeach()\\n\\nadd_custom_target(\"${PROJECT_NAME}\"\\n                  DEPENDS opencv_java\\n                 )\\n\\nforeach(TUTORIAL_DIR ${TUTORIALS_DIRS})\\n  get_filename_component(TUTORIAL_NAME ${TUTORIAL_DIR} NAME_WE)\\n  add_custom_command(TARGET \"${PROJECT_NAME}\"\\n                     COMMAND ${ANT_EXECUTABLE} -q\\n                          -DocvJarDir=\"${OpenCV_BINARY_DIR}/bin\"\\n                          -DsrcDir=\"${TUTORIAL_DIR}\"\\n                          -DdstDir=\"${opencv_tutorial_java_bin_dir}/${TUTORIAL_NAME}\"\\n                     WORKING_DIRECTORY \"${curdir}\"\\n                     COMMENT \"Compile the tutorial: ${TUTORIAL_NAME}\"\\n                    )\\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Updating Maven POM Version to Match OpenCV Core Version\nDESCRIPTION: Complex Maven command that uses the versions plugin to automatically extract and set the version number across all project POMs to match the core OpenCV version. Used by maintainers to ensure version consistency.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/maven/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmvn versions:set -DnewVersion=$(. ./opencv/scripts/functions && cd ./opencv/scripts && extract_version && echo $REPLY)\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Logic in C++\nDESCRIPTION: This code snippet is part of the FAST corner detection algorithm. It compares pixel values at different offsets around a central pixel to determine if it's a corner. The algorithm uses nested if-else statements to check various conditions based on pixel intensity comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_20\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset8] < c_b)\n  if(ptr[offset10] < c_b)\n    if(ptr[offset4] < c_b)\n      goto is_a_corner;\n    else\n      if(ptr[offset11] < c_b)\n        goto is_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    if(ptr[offset3] < c_b)\n      if(ptr[offset4] < c_b)\n        goto is_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\n\n// ... (additional nested if-else statements)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Applications and Documentation in CMake\nDESCRIPTION: This snippet sets up and reports the status of OpenCV applications and documentation options. It uses custom CMake functions to build feature strings and display their status.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\nocv_build_features_string(apps_status\n  IF BUILD_TESTS AND HAVE_opencv_ts THEN \"tests\"\n  IF BUILD_PERF_TESTS AND HAVE_opencv_ts THEN \"perf_tests\"\n  IF BUILD_EXAMPLES THEN \"examples\"\n  IF BUILD_opencv_apps THEN \"apps\"\n  IF BUILD_ANDROID_SERVICE THEN \"android_service\"\n  IF (BUILD_ANDROID_EXAMPLES OR INSTALL_ANDROID_EXAMPLES) AND CAN_BUILD_ANDROID_PROJECTS THEN \"android_examples\"\n  ELSE \"-\")\nstatus(\"    Applications:\" \"${apps_status}\")\nocv_build_features_string(docs_status\n    IF TARGET doxygen_cpp THEN \"doxygen\"\n    IF TARGET doxygen_python THEN \"python\"\n    IF TARGET doxygen_javadoc THEN \"javadoc\"\n    IF BUILD_opencv_js OR DEFINED OPENCV_JS_LOCATION THEN \"js\"\n    ELSE \"NO\"\n)\nstatus(\"    Documentation:\" \"${docs_status}\")\nstatus(\"    Non-free algorithms:\" OPENCV_ENABLE_NONFREE THEN \"YES\" ELSE \"NO\")\n```\n\n----------------------------------------\n\nTITLE: Template Matching Formula: TM_CCORR_NORMED (LaTeX)\nDESCRIPTION: Mathematical formula for the Normalized Cross Correlation (TM_CCORR_NORMED) template matching method used in OpenCV's `matchTemplate` function. R(x,y) is the result, T is the template, and I is the image. Normalization provides robustness against illumination and contrast changes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_4\n\nLANGUAGE: latex\nCODE:\n```\n\\f[R(x,y)= \\frac{\\sum_{x',y'} (T(x',y') \\cdot I(x+x',y+y'))}{\\sqrt{\\sum_{x',y'}T(x',y')^2 \\cdot \\sum_{x',y'} I(x+x',y+y')^2}}\\f]\n```\n\n----------------------------------------\n\nTITLE: Escape Time Algorithm for Mandelbrot Set - Pseudocode\nDESCRIPTION: This pseudocode describes the core logic for generating the Mandelbrot fractal using the escape time algorithm, iterating each pixel over the complex plane and determining pixel coloring based on iteration counts. Inputs are image pixel indices, outputs are color-mapped pixel values; not an executable routine but a step-by-step reference for real implementations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_/how_to_use_OpenCV_parallel_for_.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Pseudocode\nCODE:\n```\nFor each pixel (Px, Py) on the screen, do:\n{\n  x0 = scaled x coordinate of pixel (scaled to lie in the Mandelbrot X scale (-2, 1))\n  y0 = scaled y coordinate of pixel (scaled to lie in the Mandelbrot Y scale (-1, 1))\n  x = 0.0\n  y = 0.0\n  iteration = 0\n  max_iteration = 1000\n  while (x*x + y*y < 2*2  AND  iteration < max_iteration) {\n    xtemp = x*x - y*y + x0\n    y = 2*x*y + y0\n    x = xtemp\n    iteration = iteration + 1\n  }\n  color = palette[iteration]\n  plot(Px, Py, color)\n}\n\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying Image Window in Java\nDESCRIPTION: Creates a named window titled \"Pyramids Demo\" and shows the source image in it using `HighGui.namedWindow` and `HighGui.imshow`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n        //![show_image]\n        // Create window\n        HighGui.namedWindow( window_name, HighGui.WINDOW_AUTOSIZE );\n        HighGui.imshow( window_name, src );\n        //![show_image]\n```\n\n----------------------------------------\n\nTITLE: Verifying Dependency Configuration in OpenCV (CMake)\nDESCRIPTION: When this CMake option is enabled, the configuration process verifies that all required dependencies (specified with WITH_*) are present and active. If dependencies are missing, configuration fails rather than silently disabling features. Useful for packaging systems requiring consistent library dependencies.\nActivate with -DENABLE_CONFIG_VERIFICATION=ON.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_27\n\nLANGUAGE: cmake\nCODE:\n```\nENABLE_CONFIG_VERIFICATION\n```\n\n----------------------------------------\n\nTITLE: Calculating Object Orientation via PCA in Java using OpenCV\nDESCRIPTION: Defines a `getOrientation` method that computes the orientation of a shape represented by a `MatOfPoint` (contour). It converts the points to a `Mat` of type `CV_64F`, applies PCA using `Core.PCACompute`, retrieves the mean, eigenvectors, and eigenvalues, and then visualizes the principal axes. Depends on OpenCV Java bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n//! [pca]\n// Perform PCA analysis\nprivate static double getOrientation(MatOfPoint pts, Mat img) {\n    //Construct a buffer used by the PCA analysis\n    int sz = (int) pts.total();\n    Mat dataPts = new Mat(sz, 2, CvType.CV_64F);\n\n    double[] dataPtsData = new double[(int) (dataPts.total()*dataPts.channels())];\n    Point[] points = pts.toArray();\n    for (int i = 0; i < dataPts.rows(); i++) {\n        dataPtsData[i*dataPts.cols()] = points[i].x;\n        dataPtsData[i*dataPts.cols()+1] = points[i].y;\n    }\n    dataPts.put(0, 0, dataPtsData);\n\n    //Perform PCA analysis\n    Mat mean = new Mat();\n    Mat eigenvectors = new Mat();\n    Mat eigenvalues = new Mat();\n    Core.PCACompute2(dataPts, mean, eigenvectors, eigenvalues); // note: eigenvalues are sorted in descending order\n\n    //Store the center of the object\n    double[] meanData = new double[(int) (mean.total()*mean.channels())];\n    mean.get(0, 0, meanData);\n    Point cntr = new Point(meanData[0], meanData[1]);\n\n    //Store the eigenvalues and eigenvectors\n    double[] B = new double[(int) (eigenvectors.total()*eigenvectors.channels())];\n    eigenvectors.get(0, 0, B);\n    double[] C = new double[(int) (eigenvalues.total()*eigenvalues.channels())];\n    eigenvalues.get(0, 0, C);\n    Point[] eigen_vecs = new Point[2];\n    eigen_vecs[0] = new Point(B[0],B[1]);\n    eigen_vecs[1] = new Point(B[2],B[3]);\n\n    double[] eigen_val = new double[2];\n    eigen_val[0] = C[0];\n    eigen_val[1] = C[1];\n\n    // Draw the principal components\n    circle(img, cntr, 3, new Scalar(255, 0, 255), 2);\n    Point p1 = new Point(cntr.x + 0.02 * eigen_vecs[0].x * eigen_val[0], cntr.y + 0.02 * eigen_vecs[0].y * eigen_val[0]);\n    Point p2 = new Point(cntr.x - 0.02 * eigen_vecs[1].x * eigen_val[1], cntr.y - 0.02 * eigen_vecs[1].y * eigen_val[1]);\n    drawAxis(img, cntr, p1, new Scalar(0, 255, 0), 1);\n    drawAxis(img, cntr, p2, new Scalar(255, 255, 0), 5);\n\n    double angle = atan2(eigen_vecs[0].y, eigen_vecs[0].x); // orientation in radians\n\n    return angle;\n}\n//! [pca]\n```\n\n----------------------------------------\n\nTITLE: Aggregating Headers, Sources, and Dependencies for OpenCV World (CMake)\nDESCRIPTION: Initializes empty lists `headers_list`, `sources_list`, and `link_deps`. It then iterates through the dependencies of the `world` module (`OPENCV_MODULE_${the_module}_DEPS`, where `the_module` is 'world') and the `opencv_world` module itself. For each dependency `m` that is designated as part of the world (`OPENCV_MODULE_${m}_IS_PART_OF_WORLD`), its headers (`OPENCV_MODULE_${m}_HEADERS`) and sources (`OPENCV_MODULE_${m}_SOURCES`) are appended to the respective lists. Non-empty link dependencies (`OPENCV_MODULE_${m}_LINK_DEPS`) are also collected into the `link_deps` list.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/world/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(headers_list)\nset(sources_list)\nset(link_deps \"\")\nforeach(m ${OPENCV_MODULE_${the_module}_DEPS} opencv_world)\n  if(OPENCV_MODULE_${m}_IS_PART_OF_WORLD)\n    list(APPEND headers_list ${OPENCV_MODULE_${m}_HEADERS})\n    list(APPEND sources_list ${OPENCV_MODULE_${m}_SOURCES})\n  endif()\n  if(NOT \" ${OPENCV_MODULE_${m}_LINK_DEPS}\" STREQUAL \" \")\n    list(APPEND link_deps ${OPENCV_MODULE_${m}_LINK_DEPS})\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Creating Images in Java\nDESCRIPTION: Initializing blank images for drawing shapes in OpenCV Java. Creates two black images for drawing an atom and a rook chess piece.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n// Windows names\nString atom_window = \"Drawing 1: Atom\";\nString rook_window = \"Drawing 2: Rook\";\n\n// Create empty black images\nMat atom_image = Mat.zeros(w, w, CvType.CV_8UC3);\nMat rook_image = Mat.zeros(w, w, CvType.CV_8UC3);\n```\n\n----------------------------------------\n\nTITLE: Running the OpenCV.js Face Detection Example in Node.js\nDESCRIPTION: Command-line instruction to execute the face detection Node.js script. The command runs the JavaScript file that processes the image and generates an output file with detected faces and eyes highlighted.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_nodejs/js_nodejs.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnode exampleNodeCanvasData.js\n```\n\n----------------------------------------\n\nTITLE: Drawing Epilines with Custom Function - Python\nDESCRIPTION: Defines a custom function `drawlines` to draw epilines corresponding to points in an image on the other image. Inputs are two images, epilines, and matching points, outputs are images with drawn lines and points. Requires OpenCV and NumPy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_epipolar_geometry/py_epipolar_geometry.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef drawlines(img1,img2,lines,pts1,pts2):\n    ''' img1 - image on which we draw the epilines for the points in img2\n        lines - corresponding epilines '''\n    r,c = img1.shape\n    img1 = cv.cvtColor(img1,cv.COLOR_GRAY2BGR)\n    img2 = cv.cvtColor(img2,cv.COLOR_GRAY2BGR)\n    for r,pt1,pt2 in zip(lines,pts1,pts2):\n        color = tuple(np.random.randint(0,255,3).tolist())\n        x0,y0 = map(int, [0, -r[2]/r[1] ])\n        x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])\n        img1 = cv.line(img1, (x0,y0), (x1,y1), color,1)\n        img1 = cv.circle(img1,tuple(pt1),5,color,-1)\n        img2 = cv.circle(img2,tuple(pt2),5,color,-1)\n    return img1,img2\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection in C++\nDESCRIPTION: This code snippet is part of the FAST corner detection algorithm implementation. It performs pixel comparisons against thresholds to determine if a point is a corner. The algorithm uses a decision tree structure with multiple nested conditions to efficiently classify points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ngoto homogeneous;\n                        else\n                          goto homogeneous;\n                    else\n                      if(ptr[offset7] < c_b)\n                        if(ptr[offset3] < c_b)\n                          if(ptr[offset5] < c_b)\n                            if(ptr[offset1] < c_b)\n                              goto success_structured;\n                            else\n                              if(ptr[offset4] < c_b)\n                                if(ptr[offset6] < c_b)\n                                  goto success_structured;\n                                else\n                                  goto structured;\n                              else\n                                goto homogeneous;\n                          else\n                            if(ptr[offset1] < c_b)\n                              goto success_homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          if(ptr[offset6] < c_b)\n                            if(ptr[offset5] < c_b)\n                              if(ptr[offset1] < c_b)\n                                goto success_structured;\n                              else\n                                if(ptr[offset4] < c_b)\n                                  goto success_structured;\n                                else\n                                  goto homogeneous;\n                            else\n                              if(ptr[offset1] < c_b)\n                                goto success_homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            goto homogeneous;\n                      else\n                        if(ptr[offset3] < c_b)\n                          if(ptr[offset5] < c_b)\n                            if(ptr[offset1] < c_b)\n                              if(ptr[offset4] < c_b)\n                                goto success_structured;\n                              else\n                                goto homogeneous;\n                            else\n                              if(ptr[offset4] < c_b)\n                                if(ptr[offset6] < c_b)\n                                  goto success_structured;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            if(ptr[offset1] < c_b)\n                              if(ptr[offset4] < c_b)\n                                goto success_homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          goto homogeneous;\n                  else\n                    if(ptr[offset5] > cb)\n                      if(ptr[offset3] > cb)\n                        if(ptr[offset2] > cb)\n                          if(ptr[offset1] > cb)\n                            if(ptr[offset4] > cb)\n                              goto success_structured;\n                            else\n                              goto homogeneous;\n                          else\n                            if(ptr[offset4] > cb)\n                              if(ptr[offset6] > cb)\n                                goto success_structured;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          if(ptr[offset7] > cb)\n                            if(ptr[offset4] > cb)\n                              if(ptr[offset6] > cb)\n                                goto success_structured;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                          else\n                            goto homogeneous;\n                      else\n                        goto homogeneous;\n                    else\n                      if(ptr[offset5] < c_b)\n                        if(ptr[offset7] < c_b)\n                          if(ptr[offset6] < c_b)\n                            if(ptr[offset1] < c_b)\n                              goto success_homogeneous;\n                            else\n                              if(ptr[offset4] < c_b)\n                                goto success_homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            goto homogeneous;\n                        else\n                          goto homogeneous;\n                      else\n                        goto homogeneous;\n                else\n                  if(ptr[offset3] > cb)\n                    if(ptr[offset5] > cb)\n                      if(ptr[offset2] > cb)\n                        if(ptr[offset1] > cb)\n                          if(ptr[offset4] > cb)\n                            goto success_homogeneous;\n                          else\n                            goto homogeneous;\n                        else\n                          if(ptr[offset4] > cb)\n                            if(ptr[offset6] > cb)\n                              goto success_homogeneous;\n                            else\n                              goto homogeneous;\n                          else\n                            goto homogeneous;\n                      else\n                        if(ptr[offset7] > cb)\n                          if(ptr[offset4] > cb)\n                            if(ptr[offset6] > cb)\n                              goto success_homogeneous;\n                            else\n                              goto homogeneous;\n                          else\n                            goto homogeneous;\n                        else\n                          goto homogeneous;\n                    else\n                      goto homogeneous;\n                  else\n                    if(ptr[offset3] < c_b)\n                      if(ptr[offset5] < c_b)\n                        if(ptr[offset2] < c_b)\n                          if(ptr[offset1] < c_b)\n                            if(ptr[offset4] < c_b)\n                              goto success_homogeneous;\n                            else\n                              goto homogeneous;\n                          else\n                            if(ptr[offset4] < c_b)\n                              if(ptr[offset6] < c_b)\n                                goto success_homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          if(ptr[offset7] < c_b)\n                            if(ptr[offset4] < c_b)\n                              if(ptr[offset6] < c_b)\n                                goto success_homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                          else\n                            goto homogeneous;\n                      else\n                        goto homogeneous;\n                    else\n                      goto homogeneous;\n            }\n          }\n          structured:\n          {\n            x++;\n            if(x > xsizeB)\n                break;\n            else\n            {\n                const unsigned char* const ptr = img.ptr() + y*width + x;\n                const int cb = *ptr + threshold;\n                const int c_b = *ptr - threshold;\n                if(ptr[offset0] > cb)\n                  if(ptr[offset2] > cb)\n                    if(ptr[offset3] > cb)\n                      if(ptr[offset5] > cb)\n                        if(ptr[offset7] > cb)\n                          if(ptr[offset1] > cb)\n                            goto success_structured;\n                          else\n                            if(ptr[offset4] > cb)\n                              if(ptr[offset6] > cb)\n                                goto success_structured;\n                              else\n                                goto structured;\n                            else\n                              goto structured;\n                        else\n                          if(ptr[offset1] > cb)\n                            if(ptr[offset4] > cb)\n                              goto success_structured;\n                            else\n                              goto structured;\n                          else\n                            if(ptr[offset4] > cb)\n                              if(ptr[offset6] > cb)\n                                goto success_structured;\n                              else\n                                goto structured;\n                            else\n                              goto structured;\n                      else\n                        if(ptr[offset7] > cb)\n                          if(ptr[offset1] > cb)\n                            goto success_structured;\n                          else\n                            goto structured;\n                        else\n                          if(ptr[offset1] > cb)\n                            if(ptr[offset4] > cb)\n                              goto success_structured;\n                            else\n                              goto structured;\n                          else\n                            goto structured;\n                    else\n                      if(ptr[offset7] > cb)\n                        if(ptr[offset6] > cb)\n                          if(ptr[offset5] > cb)\n                            if(ptr[offset1] > cb)\n```\n\n----------------------------------------\n\nTITLE: OpenCV YAML Output Format Example\nDESCRIPTION: Example of the YAML output format produced by OpenCV's file writing operations, showing how various data types are serialized.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_23\n\nLANGUAGE: yaml\nCODE:\n```\n%YAML:1.0\niterationNr: 100\nstrings:\n   - \"image1.jpg\"\n   - Awesomeness\n   - \"baboon.jpg\"\nMapping:\n   One: 1\n   Two: 2\nR: !!opencv-matrix\n   rows: 3\n   cols: 3\n   dt: u\n   data: [ 1, 0, 0, 0, 1, 0, 0, 0, 1 ]\nT: !!opencv-matrix\n   rows: 3\n   cols: 1\n   dt: d\n   data: [ 0., 0., 0. ]\nMyData:\n   A: 97\n   X: 3.1415926535897931e+000\n   id: mydata1234\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Pixel Type Enumerations in OpenCV (C++)\nDESCRIPTION: Defines a C++ enumeration listing the fixed, primitive data types supported for array elements in OpenCV. Each enumerator (e.g., CV_8U, CV_16S, CV_32F) corresponds to a specific data type like 8-bit unsigned integer, 16-bit signed integer, or 32-bit float, respectively. These constants are fundamental for specifying the data layout of cv::Mat objects and other array types.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/doc/intro.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n    enum { CV_8U=0, CV_8S=1, CV_16U=2, CV_16S=3, CV_32S=4, CV_32F=5, CV_64F=6 };\n```\n\n----------------------------------------\n\nTITLE: Implementing Hit-or-Miss Transform in C++ using OpenCV\nDESCRIPTION: This code demonstrates how to apply the Hit-or-Miss transform to find specific patterns in a binary image using OpenCV's morphologyEx() function with MORPH_HITMISS operation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/hitOrMiss/hitOrMiss.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"opencv2/imgproc.hpp\"\n#include \"opencv2/highgui.hpp\"\n#include <iostream>\n\nusing namespace cv;\nusing namespace std;\n\nint main( void )\n{\n    // Create an image\n    Mat input_image = (Mat_<uchar>(8, 8) <<\n        0, 0, 0, 0, 0, 0, 0, 0,\n        0, 255, 255, 255, 0, 0, 0, 255,\n        0, 255, 255, 255, 0, 0, 0, 0,\n        0, 255, 255, 255, 0, 255, 0, 0,\n        0, 0, 255, 0, 0, 0, 0, 0,\n        0, 0, 255, 0, 0, 255, 255, 0,\n        0, 255, 0, 255, 0, 0, 255, 0,\n        0, 255, 255, 255, 0, 0, 0, 0);\n\n    Mat kernel = (Mat_<int>(3, 3) <<\n        0, 1, 0,\n        1, -1, 1,\n        0, 1, 0);\n\n    Mat output_image;\n    morphologyEx(input_image, output_image, MORPH_HITMISS, kernel);\n\n    const int rate = 50;\n    kernel = (kernel + 1) * 127;\n    kernel.convertTo(kernel, CV_8U);\n\n    resize(kernel, kernel, Size(), rate, rate, INTER_NEAREST);\n    resize(input_image, input_image, Size(), rate, rate, INTER_NEAREST);\n    resize(output_image, output_image, Size(), rate, rate, INTER_NEAREST);\n\n    imshow(\"kernel\", kernel);\n    imshow(\"Original\", input_image);\n    imshow(\"Hit or Miss\", output_image);\n\n    waitKey(0);\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Meanshift Object Tracking in Java\nDESCRIPTION: Java implementation of the Meanshift algorithm using OpenCV. This code processes video frames, converts them to HSV color space, performs histogram calculations, backprojection, and applies the meanshift algorithm to track objects.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/meanshift.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nimport java.util.List;\\nimport javax.swing.*;\\n\\nimport org.opencv.core.*;\\nimport org.opencv.highgui.*;\\nimport org.opencv.imgproc.Imgproc;\\nimport org.opencv.video.Video;\\nimport org.opencv.videoio.VideoCapture;\\nimport org.opencv.videoio.Videoio;\\n\\nclass MeanshiftDemo {\\n    public void run(String[] args) {\\n        String filename = args.length > 0 ? args[0] : \\\"../data/slow_traffic_small.mp4\\\";\\n\\n        VideoCapture capture = new VideoCapture(filename);\\n        if (!capture.isOpened()) {\\n            System.out.println(\\\"Could not initialize capturing...\\\\n\\\");\\n            System.exit(0);\\n        }\\n\\n        Mat frame = new Mat();\\n        capture.read(frame);\\n\\n        if (frame.empty()) {\\n            System.out.println(\\\"No captured frame -- Break!\\\");\\n            System.exit(0);\\n        }\\n\\n        // setup initial location of window\\n        Rect trackWindow = new Rect(300, 200, 100, 50);\\n\\n        // set up the ROI for tracking\\n        Mat roi = frame.submat(trackWindow);\\n        Mat hsvRoi = new Mat();\\n        Imgproc.cvtColor(roi, hsvRoi, Imgproc.COLOR_BGR2HSV);\\n\\n        Mat mask = new Mat();\\n        Core.inRange(hsvRoi, new Scalar(0, 60, 32), new Scalar(180, 255, 255), mask);\\n\\n        Mat roiHist = new Mat();\\n        int[] channels = {0};\\n        int[] histSize = {180};\\n        float[] ranges = {0, 180};\\n        float[][] histRanges = {ranges};\\n\\n        Imgproc.calcHist(List.of(hsvRoi), new MatOfInt(channels), mask, roiHist, new MatOfInt(histSize), new MatOfFloat(ranges));\\n        Core.normalize(roiHist, roiHist, 0, 255, Core.NORM_MINMAX);\\n\\n        // Setup the termination criteria, either 10 iteration or move by at least 1 pt\\n        TermCriteria termCrit = new TermCriteria(TermCriteria.EPS | TermCriteria.COUNT, 10, 1);\\n\\n        JFrame jframe = new JFrame(\\\"Meanshift Demo\\\");\\n        jframe.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\\n        JLabel vidPanel = new JLabel();\\n        jframe.setContentPane(vidPanel);\\n        jframe.setSize(frame.cols(), frame.rows());\\n        jframe.setVisible(true);\\n\\n        Mat hsv = new Mat();\\n        Mat backProject = new Mat();\\n        while (true) {\\n            capture.read(frame);\\n            if (frame.empty()) {\\n                break;\\n            }\\n\\n            Imgproc.cvtColor(frame, hsv, Imgproc.COLOR_BGR2HSV);\\n            Imgproc.calcBackProject(List.of(hsv), new MatOfInt(channels), roiHist, backProject, new MatOfFloat(histRanges), 1);\\n\\n            // apply meanshift to get the new location\\n            Video.meanShift(backProject, trackWindow, termCrit, trackWindow);\\n\\n            // Draw it on image\\n            Imgproc.rectangle(frame, trackWindow, new Scalar(255, 0, 0), 2);\\n\\n            ImageIcon image = new ImageIcon(Mat2BufferedImage.getImage(frame));\\n            vidPanel.setIcon(image);\\n            vidPanel.repaint();\\n        }\\n        System.out.println(\\\"Video processing done!\\\\n\\\");\\n    }\\n\\n    public static void main(String[] args) {\\n        // Load the native OpenCV library\\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\\n        new MeanshiftDemo().run(args);\\n    }\\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting 3D Model Points and Descriptors in C++\nDESCRIPTION: Retrieves 3D coordinates and descriptors from a model assumed to be part of a 3D recognition or pose estimation process. Typically used in applications like augmented reality or 3D reconstruction.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n// Get the MODEL INFO\n\nstd::vector<cv::Point3f> list_points3d_model = model.get_points3d();  // list with model 3D coordinates\ncv::Mat descriptors_model = model.get_descriptors();                  // list with descriptors of each 3D coordinate\n```\n\n----------------------------------------\n\nTITLE: Running G-API Streaming Pipeline\nDESCRIPTION: Shows the main pipeline execution loop that pulls processed frames from the streaming pipeline. Handles both headless and GUI modes with blocking and non-blocking pull operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/interactive_face_detection/interactive_face_detection.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\ncv::TickMeter tm;\ntm.start();\npipeline.setSource(cv::gin(cap));\npipeline.start();\nsize_t frames = 0;\ncv::Mat out;\nstd::vector<cv::Rect> faces;\nstd::vector<cv::Str> age_strings, emo_strings;\n\nwhile (true) {\n    if (cmd.get<bool>(\"pure\")) {\n        if (!pipeline.pull(cv::gout(out, faces, age_strings, emo_strings))) {\n            break;\n        }\n        frames++;\n    } else {\n        if (!pipeline.try_pull(cv::gout(out, faces, age_strings, emo_strings))) {\n            // Use this time to show the latest frame\n            cv::imshow(\"Out\", out);\n            const int key = cv::waitKey(1);\n            if (key == 27) break;\n            continue;\n        }\n        frames++;\n        cv::imshow(\"Out\", out);\n        const int key = cv::waitKey(1);\n        if (key == 27) break;\n    }\n}\ntm.stop();\n```\n\n----------------------------------------\n\nTITLE: Including Sample Utilities for OpenCV Build in CMake\nDESCRIPTION: Includes the `samples_utils.cmake` file located in the same directory as the current `CMakeLists.txt`. This script likely contains helper functions and macros specific to building and installing OpenCV samples when built alongside the main library. This is executed only when building as part of OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(\"${CMAKE_CURRENT_LIST_DIR}/samples_utils.cmake\")\n```\n\n----------------------------------------\n\nTITLE: Edge Tapering Function in C++\nDESCRIPTION: Applies edge tapering to an image to mitigate the ringing effect post-restoration. This step is crucial for enhancing visual quality after employing the Wiener filter.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/motion_deblur_filter/motion_deblur_filter.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n@snippet samples/cpp/tutorial_code/ImgProc/motion_deblur_filter/motion_deblur_filter.cpp edgetaper\n```\n\n----------------------------------------\n\nTITLE: Implementing Gamma Correction with Look-Up Table in Python using OpenCV\nDESCRIPTION: This Python code snippet applies gamma correction using OpenCV and NumPy. A look-up table (`look_up_table`), implemented as a NumPy array, is created by applying the gamma formula (O = ((I/255)^gamma) * 255) to all possible pixel values (0-255). The `cv.LUT` function then utilizes this table to efficiently transform the input image (`img_original`), producing the gamma-corrected image (`gamma_corrected`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/basic_linear_transform/basic_linear_transform.markdown#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# [changing-contrast-brightness-gamma-correction]\n    ## [changing-contrast-brightness-gamma-correction]\n    look_up_table = np.empty((1,256), np.uint8)\n    for i in range(256):\n        look_up_table[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n\n    gamma_corrected = cv.LUT(img_original, look_up_table)\n    ## [changing-contrast-brightness-gamma-correction]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV CMake Build for NVIDIA Jetson TK1\nDESCRIPTION: CMake configuration options for building OpenCV with CUDA support on the NVIDIA Jetson TK1 platform. Uses CUDA 6.5 and enables ARM NEON optimizations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\n$ cmake \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=/usr \\\n    -DCMAKE_CXX_FLAGS=-Wa,-mimplicit-it=thumb \\\n    -DBUILD_PNG=OFF \\\n    -DBUILD_TIFF=OFF \\\n    -DBUILD_TBB=OFF \\\n    -DBUILD_JPEG=OFF \\\n    -DBUILD_JASPER=OFF \\\n    -DBUILD_ZLIB=OFF \\\n    -DBUILD_EXAMPLES=ON \\\n    -DBUILD_JAVA=OFF \\\n    -DBUILD_opencv_python2=ON \\\n    -DBUILD_opencv_python3=OFF \\\n    -DENABLE_NEON=ON \\\n    -DWITH_OPENCL=OFF \\\n    -DWITH_OPENMP=OFF \\\n    -DWITH_FFMPEG=ON \\\n    -DWITH_GSTREAMER=OFF \\\n    -DWITH_GSTREAMER_0_10=OFF \\\n    -DWITH_CUDA=ON \\\n    -DWITH_GTK=ON \\\n    -DWITH_VTK=OFF \\\n    -DWITH_TBB=ON \\\n    -DWITH_1394=OFF \\\n    -DWITH_OPENEXR=OFF \\\n    -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-6.5 \\\n    -DCUDA_ARCH_BIN=3.2 \\\n    -DCUDA_ARCH_PTX=\"\" \\\n    -DINSTALL_C_EXAMPLES=ON \\\n    -DINSTALL_TESTS=OFF \\\n    -DOPENCV_TEST_DATA_PATH=../opencv_extra/testdata \\\n    ../opencv\n```\n\n----------------------------------------\n\nTITLE: Upsampling Image using pyrUp in Python\nDESCRIPTION: Upsamples the input image `tmp` using `cv.pyrUp`. The output image `dst` will be twice the dimensions of the input. The destination size is inferred by the function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n        elif c == ord('i'):\n            dst = cv.pyrUp(tmp)\n            print ('** Zoom In: Image x 2')\n```\n\n----------------------------------------\n\nTITLE: Converting UIImage to cv::Mat in Objective-C\nDESCRIPTION: This code snippet demonstrates how to convert a UIImage to an OpenCV Mat object. It handles both color and grayscale images, creating a CV_8UC4 or CV_8UC1 Mat respectively.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/image_manipulation/image_manipulation.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Objective-C\nCODE:\n```\n- (cv::Mat)cvMatFromUIImage:(UIImage *)image\n{\n  CGColorSpaceRef colorSpace = CGImageGetColorSpace(image.CGImage);\n  CGFloat cols = image.size.width;\n  CGFloat rows = image.size.height;\n\n  cv::Mat cvMat(rows, cols, CV_8UC4); // 8 bits per component, 4 channels (color channels + alpha)\n\n  CGContextRef contextRef = CGBitmapContextCreate(cvMat.data,                 // Pointer to  data\n                                                 cols,                       // Width of bitmap\n                                                 rows,                       // Height of bitmap\n                                                 8,                          // Bits per component\n                                                 cvMat.step[0],              // Bytes per row\n                                                 colorSpace,                 // Colorspace\n                                                 kCGImageAlphaNoneSkipLast |\n                                                 kCGBitmapByteOrderDefault); // Bitmap info flags\n\n  CGContextDrawImage(contextRef, CGRectMake(0, 0, cols, rows), image.CGImage);\n  CGContextRelease(contextRef);\n\n  return cvMat;\n}\n```\n\nLANGUAGE: Objective-C\nCODE:\n```\n- (cv::Mat)cvMatGrayFromUIImage:(UIImage *)image\n{\n  CGColorSpaceRef colorSpace = CGImageGetColorSpace(image.CGImage);\n  CGFloat cols = image.size.width;\n  CGFloat rows = image.size.height;\n\n  cv::Mat cvMat(rows, cols, CV_8UC1); // 8 bits per component, 1 channels\n\n  CGContextRef contextRef = CGBitmapContextCreate(cvMat.data,                 // Pointer to data\n                                                 cols,                       // Width of bitmap\n                                                 rows,                       // Height of bitmap\n                                                 8,                          // Bits per component\n                                                 cvMat.step[0],              // Bytes per row\n                                                 colorSpace,                 // Colorspace\n                                                 kCGImageAlphaNoneSkipLast |\n                                                 kCGBitmapByteOrderDefault); // Bitmap info flags\n\n  CGContextDrawImage(contextRef, CGRectMake(0, 0, cols, rows), image.CGImage);\n  CGContextRelease(contextRef);\n\n  return cvMat;\n }\n```\n\n----------------------------------------\n\nTITLE: Loading Images and Exposure Times for HDR Processing in OpenCV\nDESCRIPTION: This code snippet demonstrates how to load a sequence of exposure images and their corresponding exposure times from a specified directory. The exposure data is read from a 'list.txt' file that contains the filename and inverse exposure time for each image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/hdr_imaging.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nvector<Mat> images;\nvector<float> times;\nloadExposureSeq(samples::findFile(path), images, times);\n```\n\nLANGUAGE: java\nCODE:\n```\nList<Mat> images = new ArrayList<>();\nList<Float> times = new ArrayList<>();\nloadExposureSeq(path, images, times);\n```\n\nLANGUAGE: python\nCODE:\n```\nimages = []\ntimes = []\nload_exposure_seq(path, images, times)\n```\n\n----------------------------------------\n\nTITLE: VideoIO Plugins Configuration Options in OpenCV\nDESCRIPTION: Defines options to control the plugin mechanism for video I/O backends in OpenCV 4.1.0+, allowing certain backends to be built as plugins rather than direct dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_16\n\nLANGUAGE: markdown\nCODE:\n```\n| Option | Default | Description |\n| --------| ------ | ------- |\n| `VIDEOIO_ENABLE_PLUGINS` | _ON_ | Enable or disable plugins completely. |\n| `VIDEOIO_PLUGIN_LIST` | _empty_ | Comma- or semicolon-separated list of backend names to be compiled as plugins. Supported names are _ffmpeg_, _gstreamer_, _msmf_, _mfx_ and _all_. |\n```\n\n----------------------------------------\n\nTITLE: Matching Descriptors with BruteForceMatcher in OpenCV C++\nDESCRIPTION: Uses BruteForceMatcher to find the closest matches between descriptors from the first image to the second image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/detection_of_planar_objects/detection_of_planar_objects.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n// matching descriptors\nBruteForceMatcher<L2<float> > matcher;\nvector<DMatch> matches;\nmatcher.match(descriptors1, descriptors2, matches);\n```\n\n----------------------------------------\n\nTITLE: Splitting Image into BGR Planes in Python\nDESCRIPTION: Python snippet using OpenCV's `cv.split` function to separate a 3-channel source image (assumed to be in BGR format) into three individual single-channel NumPy arrays (representing the B, G, and R planes). The input is the source image array, and the output is a tuple/list of plane arrays.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/Histograms_Matching/histogram_calculation/calcHist_Demo.py Separate the image in 3 places ( B, G and R )\n```\n\n----------------------------------------\n\nTITLE: Drawing an Ellipse in Python\nDESCRIPTION: Implementation of the MyEllipse function that draws a rotated ellipse in OpenCV Python. The function takes the image and angle, and uses the ellipse() function to draw the shape with specified center, axes, and color.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef MyEllipse(img, angle):\n    thickness = 2\n    lineType = 8\n\n    cv.ellipse(img,\n                (w//2, w//2),\n                (w//4, w//16),\n                angle,\n                0,\n                360,\n                (255, 0, 0),\n                thickness,\n                lineType)\n```\n\n----------------------------------------\n\nTITLE: Training SVM Model with Non-Linearly Separable Data (C++)\nDESCRIPTION: Trains the Support Vector Machine using the previously prepared training data that contains both linearly separable and non-linearly separable examples. The high iteration count accommodates the complexity of finding decision boundaries for overlapping classes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n// Train the SVM\nprintf(\"\\nTraining the SVM...\");\nsvm->train(completeTrainData, ROW_SAMPLE, completeTrainLabels);\nprintf(\"Finished training\\n\");\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying Image Window in Python\nDESCRIPTION: Establishes a named window called \"Pyramids Demo\" using `cv.namedWindow` and displays the loaded source image using `cv.imshow`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    #![show_image]\n    # Create window\n    cv.namedWindow(window_name, cv.WINDOW_AUTOSIZE)\n    cv.imshow(window_name, src)\n    #![show_image]\n```\n\n----------------------------------------\n\nTITLE: Calculating Image Histogram Demo in Java (Full Code)\nDESCRIPTION: Complete Java program demonstrating how to load an image, split it into B, G, R channels, calculate the histogram for each channel using `HighGui.imshow`, `Imgproc.calcHist`, `Core.split`, `Core.normalize`, and display the histograms in a window. Depends on the OpenCV Java bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n@include samples/java/tutorial_code/Histograms_Matching/histogram_calculation/CalcHistDemo.java\n```\n\n----------------------------------------\n\nTITLE: Setting OpenCV DNN Sample Dependencies\nDESCRIPTION: Defines and checks required OpenCV module dependencies for DNN samples including core, imgproc, dnn, objdetect, video, imgcodecs, videoio, and highgui modules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENCV_DNN_SAMPLES_REQUIRED_DEPS\n  opencv_core\n  opencv_imgproc\n  opencv_dnn\n  opencv_objdetect\n  opencv_video\n  opencv_imgcodecs\n  opencv_videoio\n  opencv_highgui)\nocv_check_dependencies(${OPENCV_DNN_SAMPLES_REQUIRED_DEPS})\n```\n\n----------------------------------------\n\nTITLE: Setting Up Parallel Backend Configuration\nDESCRIPTION: Configures parallel processing backend options with conditional logic for different platforms like Emscripten, iOS, and WinRT.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(PARALLEL_ENABLE_PLUGINS_DEFAULT ON)\nif(EMSCRIPTEN OR IOS OR XROS OR WINRT)\n  set(PARALLEL_ENABLE_PLUGINS_DEFAULT OFF)\nendif()\nset(PARALLEL_ENABLE_PLUGINS \"${PARALLEL_ENABLE_PLUGINS_DEFAULT}\" CACHE BOOL \"Allow building parallel plugin support\")\n```\n\n----------------------------------------\n\nTITLE: Constructing Point Structures in OpenCV.js (JavaScript)\nDESCRIPTION: Demonstrates two equivalent ways to create a Point structure using OpenCV.js: via the cv.Point constructor and via a plain object literal with x and y properties. This approach provides developers with flexibility depending on their coding style. Required dependencies include OpenCV.js and initialized numeric values for x and y, representing the point's coordinates.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_some_data_structures/js_image_arithmetics.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n// The first way\nlet point = new cv.Point(x, y);\n// The second way\nlet point = {x: x, y: y};\n```\n\n----------------------------------------\n\nTITLE: Computing Transformed Corner Coordinates with OpenCV - Java\nDESCRIPTION: The Java snippet uses Core.perspectiveTransform to compute new corner positions for the chessboard based on homography. Inputs are MatOfPoint2f and the 3x3 transformation matrix. Output is a set of points in the new perspective, suitable for visualization or accuracy checking.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/features2D/Homography/PerspectiveCorrection.java compute-transformed-corners\n```\n\n----------------------------------------\n\nTITLE: Warping Image with OpenCV warpPerspective - C++\nDESCRIPTION: This C++ snippet uses OpenCV’s cv::warpPerspective to apply the estimated homography and transform the source chessboard view to match the desired perspective. Requires the homography matrix, input image, and destination size. Produces a warped image with corrected perspective. Input/output are Mat.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet perspective_correction.cpp warp-chessboard\n```\n\n----------------------------------------\n\nTITLE: Calculating Contour Equivalent Diameter in OpenCV Python\nDESCRIPTION: This snippet computes the equivalent diameter of a contour. This is the diameter of a circle that has the same area as the contour. It calculates the area using `cv.contourArea` and then applies the formula sqrt(4 * area / pi) using NumPy's `np.sqrt` function. Requires an existing contour variable `cnt` and the NumPy library imported as `np`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_properties/py_contour_properties.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\narea = cv.contourArea(cnt)\nequi_diameter = np.sqrt(4*area/np.pi)\n```\n\n----------------------------------------\n\nTITLE: Mask Operations with Vector Registers in C++\nDESCRIPTION: Illustrates mask operations including v_check_all(), v_check_any(), and v_select() for conditional operations on vector registers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/univ_intrin/univ_intrin.markdown#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nv_uint8 a;                           // {a1, .., an}\nv_uint8 b;                           // {b1, ..., bn}\n\nv_int32x4 mask:                      // {0xff, 0, 0, 0xff, ..., 0xff, 0}\n\nv_uint8 Res = v_select(mask, a, b)   // {a1, b2, b3, a4, ..., an-1, bn}\n\n/*\n    \"Res\" will contain the value from \"a\" if mask is true (all bits set to 1),\n    and value from \"b\" if mask is false (all bits set to 0)\n\n    We can use comparison operators to generate mask and v_select to obtain results based on conditionals.\n    It is common to set all values of b to 0. Thus, v_select will give values of \"a\" or 0 based on the mask.\n*/\n```\n\n----------------------------------------\n\nTITLE: Installing CMake via Homebrew on macOS\nDESCRIPTION: This command uses the Homebrew package manager to install CMake on macOS.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/macos_install/macos_install.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbrew install cmake\n```\n\n----------------------------------------\n\nTITLE: Calculating SSIM on GPU (Basic) in C++\nDESCRIPTION: Defines a C++ function `getMSSIM_GPU` for calculating MSSIM using basic OpenCV GPU functions. It uploads the input CPU matrices (`I1`, `I2`) to GPU matrices (`gI1`, `gI2`) and then invokes the GPU-accelerated `ssim` function. This represents a direct port from CPU without specific performance optimizations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n//![getssimcuda]\nScalar getMSSIM_GPU( const Mat& i1, const Mat& i2)\n{\n    gpu::GpuMat d_i1, d_i2;\n    d_i1.upload(i1);\n    d_i2.upload(i2);\n\n    return ssim(d_i1, d_i2);\n}\n//![getssimcuda]\n```\n\n----------------------------------------\n\nTITLE: Final Visualization Call in Python using OpenCV\nDESCRIPTION: Shows the invocation of `cv.drawContours` to outline the detected object and the call to `getOrientation` which performs PCA and draws the resulting orientation axes onto the `src` image within the main contour processing loop.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#! [visualization1]\n    # Draw each contour only for visualisation purposes\n    cv.drawContours(src, contours, i, (0, 0, 255), 2)\n    # Find the orientation of each shape\n    getOrientation(c, src)\n#! [visualization1]\n```\n\n----------------------------------------\n\nTITLE: Extracting Rotational Component Using OpenCV in C++\nDESCRIPTION: This snippet demonstrates extracting the rotational component from two images captured by a rotating camera using the OpenCV library in C++. Dependencies include OpenCV, which should be initialized with appropriate camera parameters. Inputs are image views and the outcome is the rotational component between them.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_28\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\n\nvoid extractRotation(cv::Mat image1, cv::Mat image2) {\n    // Code to extract rotation component\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: OpenCV Installation Script for Git Bash\nDESCRIPTION: Comprehensive installation script for building OpenCV and OpenCV contrib modules using Git Bash. The script clones repositories if needed, configures CMake build settings, and compiles both debug and release configurations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_install/windows_install.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash -e\nmyRepo=$(pwd)\nCMAKE_GENERATOR_OPTIONS=-G\"Visual Studio 16 2019\"\n#CMAKE_GENERATOR_OPTIONS=-G\"Visual Studio 15 2017 Win64\"\n#CMAKE_GENERATOR_OPTIONS=(-G\"Visual Studio 16 2019\" -A x64)  # CMake 3.14+ is required\nif [  ! -d \"$myRepo/opencv\"  ]; then\n    echo \"cloning opencv\"\n    git clone https://github.com/opencv/opencv.git\nelse\n    cd opencv\n    git pull --rebase\n    cd ..\nfi\nif [  ! -d \"$myRepo/opencv_contrib\"  ]; then\n    echo \"cloning opencv_contrib\"\n    git clone https://github.com/opencv/opencv_contrib.git\nelse\n    cd opencv_contrib\n    git pull --rebase\n    cd ..\nfi\nRepoSource=opencv\nmkdir -p build_opencv\npushd build_opencv\nCMAKE_OPTIONS=(-DBUILD_PERF_TESTS:BOOL=OFF -DBUILD_TESTS:BOOL=OFF -DBUILD_DOCS:BOOL=OFF  -DWITH_CUDA:BOOL=OFF -DBUILD_EXAMPLES:BOOL=OFF -DINSTALL_CREATE_DISTRIB=ON)\nset -x\ncmake \"${CMAKE_GENERATOR_OPTIONS[@]}\" \"${CMAKE_OPTIONS[@]}\" -DOPENCV_EXTRA_MODULES_PATH=\"$myRepo\"/opencv_contrib/modules -DCMAKE_INSTALL_PREFIX=\"$myRepo/install/$RepoSource\" \"$myRepo/$RepoSource\"\necho \"************************* $Source_DIR -->debug\"\ncmake --build .  --config debug\necho \"************************* $Source_DIR -->release\"\ncmake --build .  --config release\ncmake --build .  --target install --config release\ncmake --build .  --target install --config debug\npopd\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenCV Java Test Project in CMake\nDESCRIPTION: Sets up the initial conditions and variables for building OpenCV Java tests. It checks for required components and sets up the project directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/pure_test/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT ANT_EXECUTABLE\n    OR NOT BUILD_opencv_imgcodecs\n    OR NOT BUILD_opencv_calib3d)\n  return()\nendif()\n\nproject(opencv_test_java)\n\nset(OPENCV_JAR_FILE \"${OPENCV_JAR_FILE}\")\nget_filename_component(JAR_NAME \"${OPENCV_JAR_FILE}\" NAME)\n\nset(OPENCV_JAVA_TEST_DIR \"${OpenCV_BINARY_DIR}/java_test\" CACHE INTERNAL \"\")\nfile(REMOVE_RECURSE \"${OPENCV_JAVA_TEST_DIR}\")\nfile(MAKE_DIRECTORY \"${OPENCV_JAVA_TEST_DIR}\")\nfile(REMOVE \"${OPENCV_DEPHELPER}/${the_module}_test_source_copy\")\n\nset(test_dir ${CMAKE_CURRENT_SOURCE_DIR})\n\nset(depends \"\")\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Framework with Specific Architectures in Bash\nDESCRIPTION: Command to build the OpenCV framework for iOS specifying particular architectures for iOS devices and simulator.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/ios_install/ios_install.markdown#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/<my_working_directory>\npython opencv/platforms/ios/build_framework.py ios --contrib opencv_contrib --iphoneos_archs arm64 --iphonesimulator_archs x86_64\n```\n\n----------------------------------------\n\nTITLE: Adding Various Types of Borders to an Image using OpenCV in Python\nDESCRIPTION: This script demonstrates adding borders to an image using the `cv.copyMakeBorder()` function. It imports necessary libraries (`cv2`, `numpy`, `matplotlib.pyplot`), loads an image, defines a color (BLUE), and then applies various border types (`BORDER_REPLICATE`, `BORDER_REFLECT`, `BORDER_REFLECT_101`, `BORDER_WRAP`, `BORDER_CONSTANT`) with specified widths. Finally, it uses Matplotlib to display the original image and the images with different borders side-by-side.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_basic_ops/py_basic_ops.markdown#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nBLUE = [255,0,0]\n\nimg1 = cv.imread('opencv-logo.png')\nassert img1 is not None, \"file could not be read, check with os.path.exists()\"\n\nreplicate = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REPLICATE)\nreflect = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REFLECT)\nreflect101 = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REFLECT_101)\nwrap = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_WRAP)\nconstant= cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_CONSTANT,value=BLUE)\n\nplt.subplot(231),plt.imshow(img1,'gray'),plt.title('ORIGINAL')\nplt.subplot(232),plt.imshow(replicate,'gray'),plt.title('REPLICATE')\nplt.subplot(233),plt.imshow(reflect,'gray'),plt.title('REFLECT')\nplt.subplot(234),plt.imshow(reflect101,'gray'),plt.title('REFLECT_101')\nplt.subplot(235),plt.imshow(wrap,'gray'),plt.title('WRAP')\nplt.subplot(236),plt.imshow(constant,'gray'),plt.title('CONSTANT')\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: OpenCV Library Naming Convention\nDESCRIPTION: Demonstrates the naming convention for OpenCV library files, including module name and version number.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_visual_studio_opencv/windows_visual_studio_opencv.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nopencv_(The Name of the module)(The version Number of the library you use)d.lib\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV with Orbbec SDK Support - bash\nDESCRIPTION: This snippet provides the commands for building OpenCV from source with Orbbec SDK support, which is required for Mac OS starting from OpenCV 4.11. It uses cmake with the -DOBSENSOR_USE_ORBBEC_SDK=ON flag, followed by standard make and install commands. Users must ensure the appropriate development tools are installed and may require sudo privileges for the final installation step.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_uvc.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncmake -DOBSENSOR_USE_ORBBEC_SDK=ON ..\\nmake\\nsudo make install\n```\n\n----------------------------------------\n\nTITLE: Including Plugin-Specific CMake Configuration - CMake\nDESCRIPTION: This statement includes a plugin-specific CMake configuration file ('plugin.cmake') located in the same directory. It allows extension and customization of the build process. Parameter is the relative path to the included CMake file; no special inputs or outputs beyond file presence.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(${CMAKE_CURRENT_LIST_DIR}/cmake/plugin.cmake)\n```\n\n----------------------------------------\n\nTITLE: Calculating Circular Point Spread Function (PSF) in OpenCV C++\nDESCRIPTION: Generates a 2D matrix representing a circular Point Spread Function (PSF) suitable for modeling out-of-focus blur. Takes the output PSF matrix `out`, the desired filter size, and the blur radius `R` as input. It creates a circle of value 1 within the specified radius centered in the matrix and sets the rest to 0, then normalizes the PSF by dividing by its sum. Requires OpenCV `Mat`, `Point`, `circle`, and `Scalar` types.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/out_of_focus_deblur_filter/out_of_focus_deblur_filter.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nvoid calcPSF(Mat& outputImg, Size filterSize, int R)\n{\n    Mat h(filterSize, CV_32F, Scalar(0));\n    Point point(filterSize.width / 2, filterSize.height / 2);\n    circle(h, point, R, 255, -1, 8);\n    Scalar summa = sum(h);\n    outputImg = h / summa[0];\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Homography Using OpenCV in Python\nDESCRIPTION: This Python code performs the computation of the homography matrix using OpenCV, required for stitching. The dependencies are OpenCV and NumPy libraries. Inputs consist of images and camera parameters, yielding a homography matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\nimport numpy as np\n\ndef compute_homography(image1, image2):\n    # Code to compute homography\n    # ...\n\n```\n\n----------------------------------------\n\nTITLE: Calculate Histogram using OpenCV\nDESCRIPTION: Demonstrates how to calculate an image histogram using cv.calcHist() function. Takes a grayscale image and returns a 256x1 array containing pixel value counts.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_begins/py_histogram_begins.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimg = cv.imread('home.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nhist = cv.calcHist([img],[0],None,[256],[0,256])\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV.js Tests Headlessly with Puppeteer - Sh\nDESCRIPTION: Moves into the build_js/bin output directory, installs Node.js dependencies (including Puppeteer), and runs the test runner with Node. This enables automated, headless browser testing (Chromium via Puppeteer). Dependencies: Node.js, npm, puppeteer, and Chromium bundle. Output: test results in CLI; useful for CI environments. Use PUPPETEER_SKIP_CHROMIUM_DOWNLOAD and PUPPETEER_EXECUTABLE_PATH environment variables to adjust browser executable path or skip Chromium download as needed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\ncd build_js/bin\\nnpm install\\nnpm install --no-save puppeteer    # automatically downloads Chromium package\\nnode run_puppeteer.js\n```\n\n----------------------------------------\n\nTITLE: Drawing a Rook Chess Piece in Python\nDESCRIPTION: Drawing a rook chess piece using lines, rectangles, and polygons in OpenCV Python. Demonstrates creating a complex shape by combining multiple basic drawing functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# 2. Draw a rook\n# ------------------\n\n# 2.a. Create a convex polygon\nMyPolygon(rook_image)\n\n# 2.b. Creating rectangles\nrectangle(rook_image, (0, 7*w//8), (w, w), (0, 255, 255), -1, cv.LINE_8)\n\n# 2.c. Create a few lines\nMyLine(rook_image, (0, 15*w//16), (w, 15*w//16))\nMyLine(rook_image, (w//4, 7*w//8), (w//4, w))\nMyLine(rook_image, (w//2, 7*w//8), (w//2, w))\nMyLine(rook_image, (3*w//4, 7*w//8), (3*w//4, w))\n```\n\n----------------------------------------\n\nTITLE: Drawing a Rook Chess Piece in Java\nDESCRIPTION: Drawing a rook chess piece using lines, rectangles, and polygons in OpenCV Java. Demonstrates creating a complex shape by combining multiple basic drawing functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n// 2. Draw a rook\n// ------------------\n\n// 2.a. Create a convex polygon\nMyPolygon(rook_image);\n\n// 2.b. Creating rectangles\nRectangle(rook_image, new Point(0, 7*w/8), new Point(w, w), new Scalar(0, 255, 255), Core.FILLED, Core.LINE_8, 0);\n\n// 2.c. Create a few lines\nMyLine(rook_image, new Point(0, 15*w/16), new Point(w, 15*w/16));\nMyLine(rook_image, new Point(w/4, 7*w/8), new Point(w/4, w));\nMyLine(rook_image, new Point(w/2, 7*w/8), new Point(w/2, w));\nMyLine(rook_image, new Point(3*w/4, 7*w/8), new Point(3*w/4, w));\n```\n\n----------------------------------------\n\nTITLE: Creating Trackbars in C++\nDESCRIPTION: Creates trackbars to adjust HSV value ranges for thresholding operations in a C++ application using OpenCV. This feature enables dynamic parameter tuning.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold_inRange/threshold_inRange.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ncv::createTrackbar(\"Trackbar Name\", \"Window Name\", &value, max_value);\n```\n\n----------------------------------------\n\nTITLE: Loading Images and Homography with OpenCV in C++\nDESCRIPTION: This snippet loads grayscale images and a homography matrix in C++ using OpenCV's FileStorage. The images and homography are necessary for processing using the AKAZE algorithm.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nsamples/cpp/tutorial_code/features2D/AKAZE_match.cpp load\n```\n\n----------------------------------------\n\nTITLE: Loading Images and Homography with OpenCV in Python\nDESCRIPTION: Python code snippet that demonstrates loading grayscale images and reading a homography matrix, which are used later for AKAZE processing in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nsamples/python/tutorial_code/features2D/akaze_matching/AKAZE_match.py load\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Pixel Comparison Logic\nDESCRIPTION: Nested conditional logic for comparing pixel values against brightness thresholds (c_b and cb) with multiple branching paths leading to homogeneous or structured success cases. The code compares pixel values at different offsets to determine corner characteristics.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_8\n\nLANGUAGE: C\nCODE:\n```\nelse\n  goto homogeneous;\nelse\n  if(ptr[offset3] < c_b)\n    if(ptr[offset4] < c_b)\n      if(ptr[offset5] < c_b)\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] < c_b)\n            goto success_homogeneous;\n          else\n            if(ptr[offset11] < c_b)\n              goto success_homogeneous;\n            else\n              goto homogeneous;\n        else\n          if(ptr[offset6] < c_b)\n            if(ptr[offset7] < c_b)\n              if(ptr[offset8] < c_b)\n                goto success_homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n      else\n        if(ptr[offset1] < c_b)\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              goto success_homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n        else\n          goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building FastCV HAL Library in CMake\nDESCRIPTION: Sets up FastCV HAL configuration variables, builds the static library, configures include paths and dependencies, and handles installation settings. The configuration is only applied when HAVE_FASTCV is enabled.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/fastcv/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_FASTCV)\n  set(FASTCV_HAL_VERSION 0.0.1 CACHE INTERNAL \"\")\n  set(FASTCV_HAL_LIBRARIES \"fastcv_hal\" CACHE INTERNAL \"\")\n  set(FASTCV_HAL_INCLUDE_DIRS \"${CMAKE_CURRENT_SOURCE_DIR}/include\" CACHE INTERNAL \"\")\n  set(FASTCV_HAL_HEADERS\n    \"${CMAKE_CURRENT_SOURCE_DIR}/include/fastcv_hal_core.hpp\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/include/fastcv_hal_imgproc.hpp\"\n    CACHE INTERNAL \"\")\n\n  file(GLOB FASTCV_HAL_FILES    \"${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp\")\n\n  add_library(fastcv_hal STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${FASTCV_HAL_FILES})\n\n  target_include_directories(fastcv_hal PRIVATE\n    ${CMAKE_SOURCE_DIR}/modules/core/include\n    ${CMAKE_SOURCE_DIR}/modules/imgproc/include\n    ${FASTCV_HAL_INCLUDE_DIRS} ${FastCV_INCLUDE_PATH})\n\n  target_link_libraries(fastcv_hal PUBLIC ${FASTCV_LIBRARY})\n\n  set_target_properties(fastcv_hal PROPERTIES ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH})\n\n  if(NOT BUILD_SHARED_LIBS)\n    ocv_install_target(fastcv_hal EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev)\n  endif()\n\n  if(ENABLE_SOLUTION_FOLDERS)\n    set_target_properties(fastcv_hal PROPERTIES FOLDER \"3rdparty\")\n  endif()\nelse()\n  message(STATUS \"FastCV is not available, disabling related HAL\")\nendif(HAVE_FASTCV)\n```\n\n----------------------------------------\n\nTITLE: Affine Transformation Matrix Definition\nDESCRIPTION: Mathematical representation of an affine transformation using 2x3 matrix notation, showing how to combine linear transformation matrix A (2x2) with translation vector B (2x1) to form transformation matrix M (2x3).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/warp_affine/warp_affine.markdown#2025-04-22_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\nA = \\begin{bmatrix}\n    a_{00} & a_{01} \\\\\n    a_{10} & a_{11}\n    \\end{bmatrix}_{2 \\times 2}\nB = \\begin{bmatrix}\n    b_{00} \\\\\n    b_{10}\n    \\end{bmatrix}_{2 \\times 1}\n```\n\nLANGUAGE: latex\nCODE:\n```\nM = \\begin{bmatrix}\n    A & B\n    \\end{bmatrix}\n=\n\\begin{bmatrix}\n    a_{00} & a_{01} & b_{00} \\\\\n    a_{10} & a_{11} & b_{10}\n\\end{bmatrix}_{2 \\times 3}\n```\n\nLANGUAGE: latex\nCODE:\n```\nT =  \\begin{bmatrix}\n    a_{00}x + a_{01}y + b_{00} \\\\\n    a_{10}x + a_{11}y + b_{10}\n    \\end{bmatrix}\n```\n\n----------------------------------------\n\nTITLE: Generating the Mandelbrot Set Sequentially - C++\nDESCRIPTION: This code snippet shows how to sequentially render the Mandelbrot set as a grayscale image using a nested for-loop to iterate over each pixel, transform its coordinates to the complex plane, run the escape time algorithm, and assign pixel values accordingly. Requires OpenCV (for Mat) and the escape time algorithm. Takes an image Mat, max iteration counter, and pixel size as input; outputs a filled Mat. Not suited for very large images due to sequential bottlenecks.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_/how_to_use_OpenCV_parallel_for_.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n// Sequential Mandelbrot implementation\nvoid mandelbrotSequential(cv::Mat& image, int maxIter)\n{\n    for (int row = 0; row < image.rows; ++row)\n    {\n        for (int col = 0; col < image.cols; ++col)\n        {\n            double x0 = (col / (double)image.cols) * 3.0 - 2.0;\n            double y0 = (row / (double)image.rows) * 2.0 - 1.0;\n            int iter = mandelbrot(cv::Point2d(x0, y0), maxIter);\n            image.at<uchar>(row, col) = grayscaleValue(iter, maxIter);\n        }\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring VideoIO Module Build\nDESCRIPTION: Sets up the OpenCV VideoIO module build by setting sources, creating the module, adding tests, and linking dependencies. Handles plugin support and external target installation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/videoio/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nocv_set_module_sources(HEADERS ${videoio_ext_hdrs} ${videoio_hdrs} SOURCES ${videoio_srcs})\nocv_module_include_directories()\nocv_create_module()\nocv_add_accuracy_tests(${tgts})\nocv_add_perf_tests(${tgts})\n\nif(VIDEOIO_ENABLE_PLUGINS)\n  ocv_target_compile_definitions(${the_module} PRIVATE ENABLE_PLUGINS)\nendif()\n\nocv_target_link_libraries(${the_module} LINK_PRIVATE ${tgts})\n```\n\n----------------------------------------\n\nTITLE: Copying Matrices with OpenCV.js - JavaScript\nDESCRIPTION: Shows how to make a copy of a cv.Mat, either by deep cloning or via selective masked copying. The clone() method makes a complete duplicate, while copyTo() with a mask copies only specified entries. Proper usage of these methods allows safe manipulation of image data without affecting the original.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_basic_ops/js_basic_ops.markdown#2025-04-22_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n// 1. Clone\nlet dst = src.clone();\n// 2. CopyTo(only entries indicated in the mask are copied)\nsrc.copyTo(dst, mask);\n```\n\n----------------------------------------\n\nTITLE: Performing Point Polygon Test with OpenCV in Python\nDESCRIPTION: This Python script uses OpenCV's cv2.pointPolygonTest to evaluate if a given point is inside, outside, or on a polygon. It defines contours as numpy arrays and applies the function to several test points. Dependencies include numpy and OpenCV (cv2). The script visualizes the results using cv2.imshow and expects a standard OpenCV and numpy setup.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/point_polygon_test/point_polygon_test.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\\nimport numpy as np\\n\\n# Define a polygon (contour)\\ncontour = np.array([[100,100], [200,100], [200,200], [100,200]], dtype=np.int32)\\n\\n# Test points\\ntestPoint1 = (150,150) # inside\\ntestPoint2 = (250,150) # outside\\n\\n# Run pointPolygonTest\\nresult1 = cv2.pointPolygonTest(contour, testPoint1, False)\\nresult2 = cv2.pointPolygonTest(contour, testPoint2, False)\\n\\nprint(f'Test Point 1 {testPoint1}: {{result1}}')\\nprint(f'Test Point 2 {testPoint2}: {{result2}}')\\n\\n# Visualization\\nimg = np.zeros((300, 300, 3), dtype=np.uint8)\\ncv2.polylines(img, [contour], isClosed=True, color=(255,255,255), thickness=2)\\ncv2.circle(img, testPoint1, 5, (0,255,0), -1)\\ncv2.circle(img, testPoint2, 5, (0,0,255), -1)\\ncv2.imshow('Point Polygon Test', img)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up First Frame for Planar Tracking in C++\nDESCRIPTION: This code snippet initializes the tracking process by detecting keypoints and computing descriptors for the first frame. It also draws the bounding box and title on the frame.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_tracking/akaze_tracking.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nvoid Tracker::setFirstFrame(const Mat frame, vector<Point2f> bb, string title, Stats& stats)\n{\n    first_frame = frame.clone();\n    (*detector)(first_frame, noArray(), first_kp, first_desc);\n    stats.keypoints = (int)first_kp.size();\n    drawBoundingBox(first_frame, bb);\n    putText(first_frame, title, Point(0, 60), FONT_HERSHEY_PLAIN, 5, Scalar::all(0), 4);\n    object_bb = bb;\n}\n```\n\n----------------------------------------\n\nTITLE: Detecting and Filtering Contours in Java using OpenCV\nDESCRIPTION: Identifies contours in the preprocessed binary image `bw` using `findContours`. It loops through the contours, calculates their area using `contourArea`, and filters out contours smaller than a predefined minimum size (1e2). Depends on OpenCV Java bindings and requires a binary Mat `bw`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n//! [contours]\n// Find all the contours in the thresholded image\nList<MatOfPoint> contours = new ArrayList<>();\nMat hierarchy = new Mat();\nfindContours(bw, contours, hierarchy, RETR_LIST, CHAIN_APPROX_NONE);\n\nfor (int i = 0; i < contours.size(); ++i)\n{\n    // Calculate the area of each contour\n    double area = contourArea(contours.get(i));\n    // Ignore contours that are too small or too large\n    if (area < 1e2 || 1e5 < area)\n        continue;\n\n    // Draw each contour only for visualisation purposes\n    drawContours(src, contours, i, new Scalar(0, 0, 255), 2);\n    // Find the orientation of each shape\n    getOrientation(contours.get(i), src);\n}\n//! [contours]\n```\n\n----------------------------------------\n\nTITLE: Converting HDR Images to 8-bit and Saving using OpenCV Python\nDESCRIPTION: Converts HDR images into 8-bit for saving and displaying, ensuring they fit within the [0..255] range to prevent overflow.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_photo/py_hdr/py_hdr.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Convert datatype to 8-bit and save\nres_debevec_8bit = np.clip(res_debevec*255, 0, 255).astype('uint8')\nres_robertson_8bit = np.clip(res_robertson*255, 0, 255).astype('uint8')\nres_mertens_8bit = np.clip(res_mertens*255, 0, 255).astype('uint8')\n\ncv.imwrite(\"ldr_debevec.jpg\", res_debevec_8bit)\ncv.imwrite(\"ldr_robertson.jpg\", res_robertson_8bit)\ncv.imwrite(\"fusion_mertens.jpg\", res_mertens_8bit)\n```\n\n----------------------------------------\n\nTITLE: Initializing BackgroundSubtractorMOG2 with OpenCV.js in JavaScript\nDESCRIPTION: Demonstrates how to create a BackgroundSubtractorMOG2 object in OpenCV.js for background/foreground segmentation. The constructor takes three main parameters: 'history' (length of background modeling history), 'varThreshold' (variance threshold for pixel classification), and 'detectShadows' (enables shadow detection). Ensure OpenCV.js is loaded and available as 'cv'. Outputs an instance that can be reused for multiple frames; manual deletion is required to free memory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_video/js_bg_subtraction/js_bg_subtraction.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet history = 500;\nlet varThreshold = 16;\nlet detectShadows = true; // if false, disables shadow detection\nlet bgSubtractor = new cv.BackgroundSubtractorMOG2(history, varThreshold, detectShadows);\n// ... use bgSubtractor for processing frames\n// When done:\nbgSubtractor.delete(); // required to free WASM-allocated memory\n```\n\n----------------------------------------\n\nTITLE: Optimizing Huffman Tables in libjpeg (C)\nDESCRIPTION: A C boolean field within the compression parameters structure (`cinfo`). When set to TRUE, libjpeg performs an extra pass over the image data to compute optimal Huffman coding tables, potentially reducing file size slightly at the cost of increased compression time and memory usage. The default is FALSE (use default or pre-supplied tables). This parameter has no effect in progressive/lossless modes, with 12-bit precision (unless tables are supplied), or when using arithmetic coding.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_34\n\nLANGUAGE: C\nCODE:\n```\nboolean optimize_coding\n```\n\n----------------------------------------\n\nTITLE: Drawing a Rectangle in C++\nDESCRIPTION: Example of using the rectangle() function in OpenCV C++ to draw a filled yellow rectangle. The function specifies two opposite corners of the rectangle along with color and fill options.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_27\n\nLANGUAGE: cpp\nCODE:\n```\nrectangle( rook_image,\n       Point( 0, 7*w/8 ),\n       Point( w, w),\n       Scalar( 0, 255, 255 ),\n       FILLED,\n       LINE_8 );\n```\n\n----------------------------------------\n\nTITLE: Normalizing Histogram Results in Java\nDESCRIPTION: Java snippet normalizing the calculated histograms (`bHist`, `gHist`, `rHist`) to fit the display image height. It uses `Core.normalize` for each histogram Mat, specifying the input and output Mats, the target range (0 to `histImage.rows()`), the normalization type (`Core.NORM_MINMAX`), data type (-1 for same as input), and no mask (`new Mat()`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_25\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/Histograms_Matching/histogram_calculation/CalcHistDemo.java Normalize the result to ( 0, histImage.rows )\n```\n\n----------------------------------------\n\nTITLE: Building a GComputation Graph for Face Analytics Pipeline\nDESCRIPTION: Code snippet demonstrating how to build a G-API graph (GComputation) for a face analytics pipeline. The pipeline includes face detection, post-processing to extract face ROIs, and running age/gender and emotion classification on each detected face.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/interactive_face_detection/interactive_face_detection.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// The pipeline is expressed as a sequence of G-API operations\ncv::GMat in;\ncv::GMat faces = cv::gapi::infer<Faces>(in);\ncv::GArray<cv::Rect> faces_rc = custom::PostProc::on(faces, in, 0.5f, 1.0f); // 0.5f = confidence, 1.0f = scales\n\ncv::GArray<cv::GMat> age_gender_info = cv::gapi::infer<AgeGender>(faces_rc, in);\ncv::GArray<cv::GMat> age, gender;\nstd::tie(age, gender) = cv::gapi::split(age_gender_info);\n\ncv::GArray<cv::GMat> emotions = cv::gapi::infer<Emotions>(faces_rc, in);\n\ncv::GMat out = custom::Viz::on(in, faces_rc, age, gender, emotions);\n\ncv::GComputation pipeline(cv::GIn(in), cv::GOut(out));\n```\n\n----------------------------------------\n\nTITLE: Creating Trackbar in C++\nDESCRIPTION: This C++ snippet creates a trackbar using `createTrackbar`. It sets the trackbar's name, associates it with the \"Linear Blend\" window, links it to the `alpha_slider` variable, defines the maximum value (`alpha_slider_max`), and specifies `on_trackbar` as the callback function executed when the slider position changes. It also calls the callback once initially.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n//![create_trackbar]\nchar TrackbarName[50];\nsprintf( TrackbarName, \"Alpha x %d\", alpha_slider_max );\ncreateTrackbar( TrackbarName, \"Linear Blend\", &alpha_slider, alpha_slider_max, on_trackbar );\n\n// Show some stuff\non_trackbar( alpha_slider, 0 );\n//![create_trackbar]\n```\n\n----------------------------------------\n\nTITLE: Conditionally Including Test Subdirectories in CMake\nDESCRIPTION: This CMake code checks if tests are enabled (`BUILD_TESTS` is true). If they are, it further checks if the build is targeting Android (`ANDROID` is true). If building for Android, it includes the `test/android_test` subdirectory. Otherwise (for non-Android builds with tests enabled), it includes the `test/pure_test` subdirectory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_TESTS)\n  if(ANDROID)\n    add_subdirectory(test/android_test)\n  else()\n    add_subdirectory(test/pure_test)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Hello World Application (Java)\nDESCRIPTION: This Java code defines a very basic 'Hello World' style application named `HelloOpenCV`. It serves as an initial test to ensure the SBT project structure and build process are working correctly before adding OpenCV-specific code and dependencies. It simply prints 'Hello, OpenCV' to the console.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_15\n\nLANGUAGE: java\nCODE:\n```\npublic class HelloOpenCV {\n  public static void main(String[] args) {\n    System.out.println(\"Hello, OpenCV\");\n }\n}\n```\n\n----------------------------------------\n\nTITLE: Drawing a Polygon in Python\nDESCRIPTION: Implementation of the MyPolygon function that draws a filled polygon in OpenCV Python. The function creates a NumPy array with the polygon vertices and uses the fillPoly() function to draw a white polygon.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef MyPolygon(img):\n    lineType = cv.LINE_8\n\n    # Create some points\n    pts = np.array([[w//4, 7*w//8], [3*w//4, 7*w//8], \n                     [3*w//4, 13*w//16], [11*w//16, 13*w//16], \n                     [19*w//32, 3*w//8], [3*w//4, 3*w//8], \n                     [3*w//4, w//8], [26*w//40, w//8], \n                     [26*w//40, w//4], [22*w//40, w//4], \n                     [22*w//40, w//8], [18*w//40, w//8], \n                     [18*w//40, w//4], [14*w//40, w//4], \n                     [14*w//40, w//8], [w//4, w//8], \n                     [w//4, 3*w//8], [13*w//32, 3*w//8], \n                     [5*w//16, 13*w//16], [w//4, 13*w//16]], \n                    np.int32)\n    pts = pts.reshape((-1, 1, 2))\n\n    cv.fillPoly(img, \n                 [pts], \n                 (255, 255, 255), \n                 lineType)\n```\n\n----------------------------------------\n\nTITLE: Image Stitching Using OpenCV in Java\nDESCRIPTION: This Java function utilizes OpenCV to conduct image stitching based on a computed homography matrix. It requires OpenCV's Java integration. The function processes two image inputs and provides a stitched panoramic output.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_39\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.Core;\nimport org.opencv.core.Mat;\n\npublic void stitchImages(Mat image1, Mat image2, Mat homography) {\n    // Code to stitch images\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Fourier Analysis of Spatial Filters using Numpy and Matplotlib - Python\nDESCRIPTION: This code defines several spatial filters (mean, Gaussian, Laplacian, Sobel, Scharr) as Numpy arrays, computes their FFTs, and visualizes their magnitude spectra using Matplotlib. Dependencies: OpenCV (cv), Numpy (np), Matplotlib (plt). The script demonstrates how each filter affects frequency regions, illustrating why some act as low-pass or high-pass filters. Key parameters: no image needed, just 2D filters. Outputs: subplots showing frequency-domain characteristics for each kernel. Useful for educational exploration of filter design.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# simple averaging filter without scaling parameter\nmean_filter = np.ones((3,3))\n\n# creating a gaussian filter\nx = cv.getGaussianKernel(5,10)\ngaussian = x*x.T\n\n# different edge detecting filters\n# scharr in x-direction\nscharr = np.array([[-3, 0, 3],\n                   [-10,0,10],\n                   [-3, 0, 3]])\n# sobel in x direction\nsobel_x= np.array([[-1, 0, 1],\n                   [-2, 0, 2],\n                   [-1, 0, 1]])\n# sobel in y direction\nsobel_y= np.array([[-1,-2,-1],\n                   [0, 0, 0],\n                   [1, 2, 1]])\n# laplacian\nlaplacian=np.array([[0, 1, 0],\n                    [1,-4, 1],\n                    [0, 1, 0]])\n\nfilters = [mean_filter, gaussian, laplacian, sobel_x, sobel_y, scharr]\nfilter_name = ['mean_filter', 'gaussian','laplacian', 'sobel_x', \\\n                'sobel_y', 'scharr_x']\nfft_filters = [np.fft.fft2(x) for x in filters]\nfft_shift = [np.fft.fftshift(y) for y in fft_filters]\nmag_spectrum = [np.log(np.abs(z)+1) for z in fft_shift]\n\nfor i in range(6):\n    plt.subplot(2,3,i+1),plt.imshow(mag_spectrum[i],cmap = 'gray')\n    plt.title(filter_name[i]), plt.xticks([]), plt.yticks([])\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Using Stitcher Class for Image Stitching C++\nDESCRIPTION: This snippet demonstrates how to utilize the high-level stitching API from OpenCV in C++. The code includes additional options for image division and error handling not found in the Python example. Key dependencies include OpenCV libraries, and it expects input images to stitch into a panorama. Outputs include the stitched panorama image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/stitcher.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp> // OpenCV library headers\nint main(int argc, char* argv[]) {\n    cv::Mat pano;\n    cv::Stitcher::Mode mode = cv::Stitcher::PANORAMA;\n    std::vector<cv::Mat> imgs;\n    // Imaginary function to load images\n    // loadImages(argv, imgs);\n    cv::Stitcher::Status status = cv::Stitcher::create(mode)->stitch(imgs, pano);\n    if (status != cv::Stitcher::OK) {\n        // Handle error\n    }\n    cv::imwrite(\"result.jpg\", pano);\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Generating ChArUco Board Pattern with Custom Dictionary using Python Script (Shell)\nDESCRIPTION: Uses 'gen_pattern.py' to generate a ChArUco board pattern named 'charuco_board.svg' with 7 rows and 5 columns. This command specifically uses a custom ArUco dictionary defined in 'my_dictionary.json'. Requires Python, the 'gen_pattern.py' script, and the custom dictionary file ('my_dictionary.json').\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_pattern/camera_calibration_pattern.markdown#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython gen_pattern.py -o charuco_board.svg --rows 7 --columns 5 -T charuco_board -f my_dictionary.json\n```\n\n----------------------------------------\n\nTITLE: Generating Non-Linearly Separable Training Data for SVM (Python)\nDESCRIPTION: Python implementation for creating non-linearly separable training data by adding points that overlap between classes, demonstrating how real-world data often requires non-linear SVM approaches.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n# Set up the non-linearly separable part of the training data\nFRAC_LINEAR_SEP = 0.9  # Fraction of the training samples which will be linearly separable\ntrainingSamplesToAdd = int(2 * NTRAINING_SAMPLES * (1 - FRAC_LINEAR_SEP))\nextraTrainData = np.empty((trainingSamplesToAdd, 2), dtype=np.float32)\nextraTrainLabels = np.empty((trainingSamplesToAdd, 1), dtype=np.int32)\nrng2 = np.random.RandomState(100)\n\n# Generate extra non-linearly separable points\nfor i in range(trainingSamplesToAdd):\n    clsIdx = i % 2\n    x = rng2.uniform()\n    y = rng2.uniform()\n    extraTrainData[i, 0] = x\n    extraTrainData[i, 1] = y\n    extraTrainLabels[i, 0] = labels[clsIdx]\n\n# Merge all the training data\ncompleteTrainData = np.vstack((trainData, extraTrainData))\ncompleteTrainLabels = np.vstack((trainLabels, extraTrainLabels))\n```\n\n----------------------------------------\n\nTITLE: Using Stitcher Class for Image Stitching Python\nDESCRIPTION: This Python snippet shows how to employ the high-level stitching API provided by OpenCV. It simplifies the stitching process using pre-configured modes. It requires OpenCV (cv2) as a dependency and takes image paths as input to produce a panoramic output. The Python code lacks some of the advanced error management available in C++.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/stitcher.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\nimport sys\n# Load images, this part assumes images are given as command-line arguments\nimgs = [cv2.imread(img_path) for img_path in sys.argv[1:]]\nstitcher = cv2.Stitcher_create(cv2.Stitcher_PANORAMA)\nstatus, pano = stitcher.stitch(imgs)\nif status == cv2.Stitcher_OK:\n    cv2.imwrite('result.jpg', pano)\nelse:\n    print('Error during stitching')\n```\n\n----------------------------------------\n\nTITLE: Drawing Line with OpenCV in Python\nDESCRIPTION: Creates a black image and draws a blue diagonal line from top-left to bottom-right corners with a thickness of 5 pixels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_drawing_functions/py_drawing_functions.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\n# Create a black image\nimg = np.zeros((512,512,3), np.uint8)\n\n# Draw a diagonal blue line with thickness of 5 px\ncv.line(img,(0,0),(511,511),(255,0,0),5)\n```\n\n----------------------------------------\n\nTITLE: YOLOX Model Download and Execution\nDESCRIPTION: Shell commands for downloading and running the YOLOX detector model using OpenCV's example.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_yolo/dnn_yolo.markdown#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/opencv/opencv_extra.git\ncd opencv_extra/testdata/dnn\npython download_models.py yolox_s_inf_decoder\ncd ..\nexport OPENCV_TEST_DATA_PATH=$(pwd)\ncd <build directory of OpenCV>\n./bin/example_dnn_yolo_detector\n```\n\n----------------------------------------\n\nTITLE: Calculating Contour Solidity in OpenCV Python\nDESCRIPTION: This snippet calculates the solidity of a contour, defined as the ratio of the contour's area to the area of its convex hull. It uses `cv.contourArea` for the contour area, `cv.convexHull` to find the convex hull of the contour, and `cv.contourArea` again to get the hull's area. Requires an existing contour variable `cnt`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_properties/py_contour_properties.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\narea = cv.contourArea(cnt)\nhull = cv.convexHull(cnt)\nhull_area = cv.contourArea(hull)\nsolidity = float(area)/hull_area\n```\n\n----------------------------------------\n\nTITLE: Accessing Single Pixel in 3-Channel Image with OpenCV in C++\nDESCRIPTION: Retrieves color pixel values from a 3-channel (BGR) cv::Mat at (row, column). Uses cv::Vec3b and at<> method. Output is a vector with blue, green, red intensities in that order. Each channel value ranges 0-255.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\ncv::Vec3b intensity = img.at<cv::Vec3b>(y, x);\\nuchar blue = intensity[0];\\nuchar green = intensity[1];\\nuchar red = intensity[2];\n```\n\n----------------------------------------\n\nTITLE: Declaring Constant-Sized SIMD Registers in OpenCV Intrinsics (C++)\nDESCRIPTION: This snippet shows how to declare constant-sized SIMD register structures in OpenCV's intrinsics, following the type and dimension conventions. Registers such as v_int32x8 and v_float64x8 specify both the data type, width, and number of values. These should be used when precise control over register size is necessary. The hardware SIMD support must match the specified sizes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/univ_intrin/univ_intrin.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nv_int32x8 reg1                       // holds 8 32-bit signed integers.\nv_float64x8 reg2                     // reg2.nlanes = 8\n```\n\n----------------------------------------\n\nTITLE: Initializing Barcode Detector in OpenCV C++\nDESCRIPTION: The snippet demonstrates how to initialize a BarcodeDetector object in OpenCV C++. The initialization can optionally use a super-resolution model from an external source. Dependencies include OpenCV and potentially the super-resolution model files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/barcode_detect_and_decode.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/barcode.cpp initialize\n```\n\n----------------------------------------\n\nTITLE: Pose Estimation with ArUco Markers using OpenCV C++\nDESCRIPTION: This snippet and associated function calls implement pose estimation using detected ArUco markers. The process uses the camera calibration matrix, distortion coefficients, marker corners, and dimensions to compute rotation and translation vectors, providing a 3D transformation from the marker to camera coordinates. The detailed code is referenced in `detect_markers.cpp`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_detection/aruco_detection.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/objectDetection/detect_markers.cpp aruco_pose_estimation1\n```\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/objectDetection/detect_markers.cpp aruco_pose_estimation2\n```\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/objectDetection/detect_markers.cpp aruco_pose_estimation3\n```\n\n----------------------------------------\n\nTITLE: Checking Image Load Success in OpenCV Python\nDESCRIPTION: This snippet checks if an image was successfully loaded in Python by verifying if the image variable is None. It allows for appropriate error handling when the image cannot be found or opened.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/display_image/display_image.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nif image is None:\n    print('Could not open or find the image!')\n    exit(0)\n```\n\n----------------------------------------\n\nTITLE: Creating Display Window in C++\nDESCRIPTION: This C++ snippet creates a window named \"Linear Blend\" using `namedWindow`. This window will display the blended image and host the trackbar.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n//![window]\n/// Create Windows\nnamedWindow(\"Linear Blend\", WINDOW_AUTOSIZE); // Create Window\n//![window]\n```\n\n----------------------------------------\n\nTITLE: Drawing Probabilistic Hough Line Segments in OpenCV (C++)\nDESCRIPTION: This C++ snippet draws the line segments detected by HoughLinesP. For each segment, it draws a line between the endpoints using cv::line. Inputs are the array of segment points and the display image. Requires OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\nfor( size_t i = 0; i < linesP.size(); i++ )\\n{\\n    Vec4i l = linesP[i];\\n    line( cdstP, Point(l[0], l[1]), Point(l[2], l[3]), Scalar(0,255,0), 3, LINE_AA);\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Camshift Object Tracking in C++\nDESCRIPTION: C++ implementation of the Camshift algorithm using OpenCV. This code extends the Meanshift approach by adapting the size and rotation of the tracking window based on object appearance changes. The implementation includes histogram calculation, backprojection, and the Camshift tracking function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/meanshift.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n#include \"opencv2/imgproc.hpp\"\\n#include \"opencv2/videoio.hpp\"\\n#include \"opencv2/highgui.hpp\"\\n#include <iostream>\\n\\nusing namespace cv;\\nusing namespace std;\\n\\nMat frame, hsv, hue, mask, hist, histimg = Mat::zeros(200, 320, CV_8UC3), backproj;\\n\\nint main( int argc, char** argv )\\n{\\n    VideoCapture cap;\\n    Rect trackWindow;\\n    int hsize = 16;\\n    float hranges[] = {0,180};\\n    const float* phranges = hranges;\\n\\n    cap.open( samples::findFile(argc > 1 ? argv[1] : \\\"video.avi\\\") );\\n\\n    if( !cap.isOpened() )\\n    {\\n        cout << \\\"Could not initialize capturing...\\\" << endl;\\n        return 0;\\n    }\\n\\n    namedWindow( \\\"Histogram\\\", 0 );\\n    namedWindow( \\\"CamShift Demo\\\", 0 );\\n    setMouseCallback( \\\"CamShift Demo\\\", onMouse, 0 );\\n    createTrackbar( \\\"Vmin\\\", \\\"CamShift Demo\\\", &vmin, 256, 0 );\\n    createTrackbar( \\\"Vmax\\\", \\\"CamShift Demo\\\", &vmax, 256, 0 );\\n    createTrackbar( \\\"Smin\\\", \\\"CamShift Demo\\\", &smin, 256, 0 );\\n\\n    for(;;)\\n    {\\n        cap >> frame;\\n        if( frame.empty() )\\n            break;\\n\\n        frame.copyTo(image);\\n        cvtColor(image, hsv, COLOR_BGR2HSV);\\n\\n        if( trackObject )\\n        {\\n            int _vmin = vmin, _vmax = vmax;\\n\\n            inRange(hsv, Scalar(0, smin, MIN(_vmin,_vmax)),\\n                    Scalar(180, 256, MAX(_vmin, _vmax)), mask);\\n            int ch[] = {0, 0};\\n            hue.create(hsv.size(), hsv.depth());\\n            mixChannels(&hsv, 1, &hue, 1, ch, 1);\\n\\n            if( trackObject < 0 )\\n            {\\n                Mat roi(hue, selection), maskroi(mask, selection);\\n                calcHist(&roi, 1, 0, maskroi, hist, 1, &hsize, &phranges);\\n                normalize(hist, hist, 0, 255, NORM_MINMAX);\\n\\n                trackWindow = selection;\\n                trackObject = 1;\\n\\n                histimg = Scalar::all(0);\\n                int binW = histimg.cols / hsize;\\n                Mat buf(1, hsize, CV_8UC3);\\n                for( int i = 0; i < hsize; i++ )\\n                    buf.at<Vec3b>(i) = Vec3b(saturate_cast<uchar>(i*180./hsize), 255, 255);\\n                cvtColor(buf, buf, COLOR_HSV2BGR);\\n\\n                for( int i = 0; i < hsize; i++ )\\n                {\\n                    int val = saturate_cast<int>(hist.at<float>(i)*histimg.rows/255);\\n                    rectangle( histimg, Point(i*binW,histimg.rows),\\n                            Point((i+1)*binW,histimg.rows - val),\\n                            Scalar(buf.at<Vec3b>(i)), -1, 8 );\\n                }\\n            }\\n\\n            calcBackProject(&hue, 1, 0, hist, backproj, &phranges);\\n            backproj &= mask;\\n            RotatedRect trackBox = CamShift(backproj, trackWindow,\\n                                TermCriteria( TermCriteria::EPS | TermCriteria::COUNT, 10, 1 ));\\n            if( trackWindow.area() <= 1 )\\n            {\\n                int cols = backproj.cols, rows = backproj.rows, r = (MIN(cols, rows) + 5)/6;\\n                trackWindow = Rect(trackWindow.x - r, trackWindow.y - r,\\n                                 trackWindow.x + r, trackWindow.y + r) &\\n                            Rect(0, 0, cols, rows);\\n            }\\n\\n            if( backprojMode )\\n                cvtColor( backproj, image, COLOR_GRAY2BGR );\\n            ellipse( image, trackBox, Scalar(0,0,255), 3, LINE_AA );\\n        }\\n\\n        if( selectObject && selection.width > 0 && selection.height > 0 )\\n        {\\n            Mat roi(image, selection);\\n            bitwise_not(roi, roi);\\n        }\\n\\n        imshow( \\\"CamShift Demo\\\", image );\\n        imshow( \\\"Histogram\\\", histimg );\\n\\n        char c = (char)waitKey(30);\\n        if( c == 27 )\\n            break;\\n        switch(c)\\n        {\\n        case 'b':\\n            backprojMode = !backprojMode;\\n            break;\\n        case 'c':\\n            trackObject = 0;\\n            histimg = Scalar::all(0);\\n            break;\\n        case 'h':\\n            showHist = !showHist;\\n            if( !showHist )\\n                destroyWindow( \\\"Histogram\\\" );\\n            else\\n                namedWindow( \\\"Histogram\\\", 1 );\\n            break;\\n        case 'p':\\n            paused = !paused;\\n            break;\\n        default:\\n            ;\\n        }\\n    }\\n\\n    return 0;\\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Laplacian Derivative with OpenCV.js - JavaScript\nDESCRIPTION: This snippet shows how to compute the Laplacian (second-order derivative) of an image with the cv.Laplacian function in OpenCV.js. Requires OpenCV.js loaded in the environment and expects input and output cv.Mat objects, with selectable depth and kernel size. It calculates the sum of second-order derivatives in both axes, returning an edge-highlighted output with kernel and datatype constraints as described.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_gradients/js_gradients.markdown#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ncv.Laplacian(src, dst, ddepth, ksize = 1, scale = 1, delta = 0, borderType = cv.BORDER_DEFAULT);\n```\n\n----------------------------------------\n\nTITLE: Defining Default Output Directory for Converted Models - Python\nDESCRIPTION: This Python class snippet leverages dataclasses to provide a configuration container for directory paths. 'CommonConfig' defines the default path ('dnn_model_runner/dnn_conversion') for storing all converted model artifacts. Requires Python 3.7+ (for dataclasses), and used for centralizing file management/configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\\nclass CommonConfig:\\n    output_data_root_dir: str = \"dnn_model_runner/dnn_conversion\"\n```\n\n----------------------------------------\n\nTITLE: Creating Zlib Static Library in CMake\nDESCRIPTION: Adds a static library target for Zlib with all source files and headers, and sets the ZLIB_DLL define symbol. This is the main library creation command for Zlib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${ZLIB_LIBRARY} STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${ZLIB_SRCS} ${ZLIB_PUBLIC_HDRS} ${ZLIB_PRIVATE_HDRS})\nset_target_properties(${ZLIB_LIBRARY} PROPERTIES DEFINE_SYMBOL ZLIB_DLL)\n```\n\n----------------------------------------\n\nTITLE: Reading ONNX Model with OpenCV DNN\nDESCRIPTION: This Python code snippet loads an ONNX model file specified by `full_model_path` using OpenCV's Deep Neural Network (DNN) module. The `cv2.dnn.readNetFromONNX` function parses the ONNX file and creates an OpenCV `cv.dnn_Net` object (`opencv_net`) ready for inference. Requires the `cv2` (OpenCV) library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# read converted .onnx model with OpenCV API\nopencv_net = cv2.dnn.readNetFromONNX(full_model_path)\n```\n\n----------------------------------------\n\nTITLE: Basic TIFF Software Installation Commands\nDESCRIPTION: Standard commands for configuring, building, and installing the TIFF library on Unix-like systems. These commands should be executed in sequence to install the software system-wide.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libtiff/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n% ./configure\n% make\n% su\n# make install\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Core Modules on Linux\nDESCRIPTION: Quick start script for building OpenCV core modules on Linux. It downloads the source, creates a build directory, configures CMake, and builds the project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget -O opencv.zip https://github.com/opencv/opencv/archive/4.x.zip\nunzip opencv.zip\nmkdir -p build && cd build\ncmake  ../opencv-4.x\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: Splitting and Merging Image Channels with OpenCV.js - JavaScript\nDESCRIPTION: Shows how to decompose an image Mat into its separate RGBA channels using cv.split and MatVector. Demonstrates accessing a single channel, then recombining them all using cv.merge. Highlights importance of memory management (delete) to prevent leaks in long-running applications.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_basic_ops/js_basic_ops.markdown#2025-04-22_snippet_9\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet src = cv.imread(\"canvasInput\");\nlet rgbaPlanes = new cv.MatVector();\n// Split the Mat\ncv.split(src, rgbaPlanes);\n// Get R channel\nlet R = rgbaPlanes.get(0);\n// Merge all channels\ncv.merge(rgbaPlanes, src);\nsrc.delete(); rgbaPlanes.delete(); R.delete();\n```\n\n----------------------------------------\n\nTITLE: Illustrating Separate Layer Computations Before Fusing (Pseudocode)\nDESCRIPTION: This pseudocode illustrates the sequential computation of Convolution (`conv`), Scale (`scale`), and ReLU (`relu`) layers. It serves as a conceptual baseline to understand how these distinct operations can be combined into a single, more efficient function through layer fusing in the Halide backend.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_halide_scheduling/dnn_halide_scheduling.markdown#2025-04-22_snippet_1\n\nLANGUAGE: pseudocode\nCODE:\n```\nconv(x, y, c, n) = sum(...) + bias(c);\nscale(x, y, c, n) = conv(x, y, c, n) * weights(c);\nrelu(x, y, c, n) = max(scale(x, y, c, n), 0);\n```\n\n----------------------------------------\n\nTITLE: Initializing and Configuring SVM Parameters for Non-Linear Classification (C++)\nDESCRIPTION: Sets up parameters for Support Vector Machine to handle non-linearly separable data. Configures a small C value to allow some misclassification and uses a termination criteria with high iterations to ensure convergence.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n// Set up SVM's parameters\nPtr<SVM> svm = SVM::create();\nsvm->setType(SVM::C_SVC);\nsvm->setKernel(SVM::RBF);\n// When C is small, the decision boundary will be smooth\n// When C is large, the decision boundary can better classify all training points but may lead to overfitting\nsvm->setC(0.1);\n// Set termination criteria for the optimization\nsvm->setTermCriteria(TermCriteria(TermCriteria::MAX_ITER, (int)1e7, 1e-6));\n```\n\n----------------------------------------\n\nTITLE: Fitting an Ellipse to Contours in Python with OpenCV\nDESCRIPTION: This snippet demonstrates how to fit an ellipse to a contour and draw it using cv.fitEllipse() and cv.ellipse() functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nellipse = cv.fitEllipse(cnt)\ncv.ellipse(img,ellipse,(0,255,0),2)\n```\n\n----------------------------------------\n\nTITLE: Negative Sample Description File Format\nDESCRIPTION: Example of a negative sample description file (bg.txt) containing paths to background images used for training.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/traincascade.markdown#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nimg/img1.jpg\nimg/img2.jpg\n```\n\n----------------------------------------\n\nTITLE: Drawing a Filled Circle in Java\nDESCRIPTION: Implementation of the MyFilledCircle function that draws a filled circle in OpenCV Java. The function takes the image and center point, and uses the circle() function with a negative thickness value to create a solid filled circle.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_22\n\nLANGUAGE: java\nCODE:\n```\nprivate static void MyFilledCircle(Mat img, Point center) {\n    Imgproc.circle(img,\n            center,\n            w/32,\n            new Scalar(0, 0, 255),\n            Core.FILLED,\n            Core.LINE_8);\n}\n```\n\n----------------------------------------\n\nTITLE: Computing 2D Object Points for Homography in OpenCV C++\nDESCRIPTION: Code to project 3D object points to 2D by removing the Z coordinate. This creates a set of 2D points that can be used for homography estimation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nstd::vector<cv::Point2f> objectPointsPlanar;\nfor (size_t i = 0; i < objectPoints.size(); ++i) {\n    objectPointsPlanar.push_back(cv::Point2f(objectPoints[i].x, objectPoints[i].y));\n}\n```\n\n----------------------------------------\n\nTITLE: Test Configuration for Classification Module (Python)\nDESCRIPTION: Defines the TestClsModuleConfig Python dataclass for test mode, specifying file paths, model parameters, preprocessing options, and label paths for inference. Sourced from test_config.py, dependencies include a valid Python environment and compatible data. Key fields allow fine-grained control over test datasets, preprocessing, and parameter settings. Inputs are controlled via dataclass fields; outputs impact model inference paths. Limitations come from the accuracy of file paths and correct parameterization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass TestClsModuleConfig:\n    cls_test_data_dir: str = \"../data\"\n    test_module_name: str = \"classification\"\n    test_module_path: str = \"classification.py\"\n    input_img: str = os.path.join(cls_test_data_dir, \"squirrel_cls.jpg\")\n    model: str = \"\"\n\n    frame_height: str = str(TestClsConfig.frame_size)\n    frame_width: str = str(TestClsConfig.frame_size)\n    scale: str = \"1.0\"\n    mean: List[str] = field(default_factory=lambda: [\"0.0\", \"0.0\", \"0.0\"])\n    std: List[str] = field(default_factory=list)\n    crop: str = \"False\"\n    rgb: str = \"True\"\n    rsz_height: str = \"\"\n    rsz_width: str = \"\"\n    classes: str = os.path.join(cls_test_data_dir, \"dnn\", \"classification_classes_ILSVRC2012.txt\")\n```\n\n----------------------------------------\n\nTITLE: Loading 3D Textured Object Model - OpenCV C++\nDESCRIPTION: This code snippet demonstrates how to load a 3D textured object model from a YAML file using a Model class in C++. It extracts 3D points and descriptors needed for pose estimation, utilizing OpenCV's FileStorage functionalities. Key requirements include OpenCV library and a valid path to a YAML file containing the model data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n/* Load a YAML file using OpenCV */\nvoid Model::load(const std::string path)\n{\n    cv::Mat points3d_mat;\n\n    cv::FileStorage storage(path, cv::FileStorage::READ);\n    storage[\"points_3d\"] >> points3d_mat;\n    storage[\"descriptors\"] >> descriptors_;\n\n    points3d_mat.copyTo(list_points3d_in_);\n\n    storage.release();\n}\n\n```\n\n----------------------------------------\n\nTITLE: Defining Buffer Structure for Optimized PSNR in C++\nDESCRIPTION: Defines a C++ structure `BufferPSNR` intended to optimize GPU calculations by pre-allocating necessary `gpu::GpuMat` objects. This avoids expensive repeated memory allocations within functions called multiple times. It contains `GpuMat` members for input images (`gI1`, `gI2`), intermediate results (`gs`, `t1`, `t2`), and a general buffer (`buf`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nstruct BufferPSNR                                     // Optimized GPU versions\n  {   // Data allocations are very expensive on GPU. Use a buffer to solve: allocate once reuse later.\n  gpu::GpuMat gI1, gI2, gs, t1,t2;\n\n  gpu::GpuMat buf;\n};\n```\n\n----------------------------------------\n\nTITLE: Ratio Test Filtering in OpenCV using Java\nDESCRIPTION: Java code applying a ratio test to filter matching keypoints for improved accuracy in matching using OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nsamples/java/tutorial_code/features2D/akaze_matching/AKAZEMatchDemo.java ratio test filtering\n```\n\n----------------------------------------\n\nTITLE: Setting JPEG Compression Parameters in C\nDESCRIPTION: This snippet shows how to set parameters for JPEG compression, including the color space. The 'cinfo.in_color_space' is set to 'JCS_RGB' to specify the colorspace and 'jpeg_set_defaults(&cinfo)' is called to initialize default settings. These settings precede the start of a compression cycle and must be configured before invoking 'jpeg_start_compress'.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_5\n\nLANGUAGE: C\nCODE:\n```\ncinfo.in_color_space = JCS_RGB; /* colorspace of input image */\njpeg_set_defaults(&cinfo); /* Make optional parameter settings here */\n```\n\n----------------------------------------\n\nTITLE: Classifying and Visualizing SVM Regions with OpenCV - Java\nDESCRIPTION: This Java snippet uses OpenCV to train an SVM, classify pixels on a Cartesian grid, and color points according to the predicted class (green or blue). Dependencies include the OpenCV Java API and appropriate imports. It demonstrates preparing training data, setting SVM parameters, training the model, and mapping the predicted regions visually. Inputs include a 2D pixel grid and labeled sample data; outputs are colored images showing SVM classification regions and boundaries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_svm.markdown#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n// Training and visualizing SVM in Java with OpenCV\n// ... (full code from samples/java/tutorial_code/ml/introduction_to_svm/IntroductionToSVMDemo.java, show)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenCV Android Build (Shell)\nDESCRIPTION: Sets the JAVA_HOME and ANDROID_HOME environment variables required by the build scripts. These variables point to the Java Runtime Environment (often bundled with Android Studio) and the Android SDK installation directories, respectively.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/android/aar-template/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport JAVA_HOME=~/Android Studio/jbr\nexport ANDROID_HOME=~/Android/SDK\n```\n\n----------------------------------------\n\nTITLE: Calculating Distance from Camera using L2 Norm in C++\nDESCRIPTION: After obtaining a 3D point in the camera's coordinate system, this snippet calculates the distance from the camera's origin to the point using the L2 norm. This requires already computed translations and rotations from the solvePnP function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_square_chess/camera_calibration_square_chess.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n// assuming 'point' is the 3D position of a chessboard corner in the camera coordinate system\ndouble distance = norm(point);\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV with Ninja\nDESCRIPTION: Command to build OpenCV using Ninja build system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nninja\n```\n\n----------------------------------------\n\nTITLE: Initializing PnPProblem with Camera Parameters in C++\nDESCRIPTION: Illustrates the declaration of the PnPProblem class by setting intrinsic camera parameters necessary for pose estimation using a webcam, including focal length and sensor dimensions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\n// Intrinsic camera parameters: UVC WEBCAM\n\ndouble f = 55;                           // focal length in mm\ndouble sx = 22.3, sy = 14.9;             // sensor size\ndouble width = 640, height = 480;        // image size\n\ndouble params_WEBCAM[] = { width*f/sx,   // fx\n                           height*f/sy,  // fy\n```\n\n----------------------------------------\n\nTITLE: Evaluating Corner Detection Conditionals in C/C++\nDESCRIPTION: This snippet performs a series of nested conditional checks on the values pointed to by 'ptr' at different offsets to determine if a pixel represents a corner. It compares pixel intensities against threshold values ('cb' and 'c_b') and uses goto statements to transfer control to 'is_a_corner' or 'is_not_a_corner' labels depending on the result. External dependencies include the correct initialization of 'ptr', the offset variables, and the threshold constants. It expects an array or pointer representing image data. Outputs are control flow jumps that can affect corner classification logic elsewhere.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_2\n\nLANGUAGE: C\nCODE:\n```\ngoto is_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset8] > cb)\n    if(ptr[offset9] > cb)\n      if(ptr[offset10] > cb)\n        if(ptr[offset11] > cb)\n          if(ptr[offset12] > cb)\n            if(ptr[offset13] > cb)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\nif(ptr[offset14] < c_b)\n  if(ptr[offset8] < c_b)\n    if(ptr[offset9] < c_b)\n      if(ptr[offset10] < c_b)\n        if(ptr[offset11] < c_b)\n          if(ptr[offset12] < c_b)\n            if(ptr[offset13] < c_b)\n              if(ptr[offset6] < c_b)\n                goto is_a_corner;\n              else\n                if(ptr[offset15] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset14] > cb)\n    if(ptr[offset15] > cb)\n      if(ptr[offset1] > cb)\n        if(ptr[offset3] > cb)\n          if(ptr[offset6] > cb)\n            goto is_a_corner;\n          else\n            if(ptr[offset13] > cb)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              if(ptr[offset12] > cb)\n                if(ptr[offset13] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset8] > cb)\n          if(ptr[offset9] > cb)\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                if(ptr[offset12] > cb)\n                  if(ptr[offset13] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\nif(ptr[offset5] < c_b)\n  if(ptr[offset12] > cb)\n    if(ptr[offset13] > cb)\n      if(ptr[offset14] > cb)\n        if(ptr[offset15] > cb)\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              goto is_a_corner;\n            else\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset8] > cb)\n              if(ptr[offset9] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset6] > cb)\n            if(ptr[offset7] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset9] > cb)\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n  if(ptr[offset12] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset9] < c_b)\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              if(ptr[offset13] < c_b)\n                if(ptr[offset6] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset14] < c_b)\n                    if(ptr[offset15] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset12] > cb)\n    if(ptr[offset13] > cb)\n      if(ptr[offset14] > cb)\n        if(ptr[offset15] > cb)\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              goto is_a_corner;\n            else\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset8] > cb)\n              if(ptr[offset9] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset6] > cb)\n```\n\n----------------------------------------\n\nTITLE: Drawing Back Projection in C++ with OpenCV\nDESCRIPTION: This snippet shows how to display the back projection result using OpenCV in C++. It creates a window and shows the back projection image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/back_projection/back_projection.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nimshow(\"BackProj\", backproj);\n```\n\n----------------------------------------\n\nTITLE: Computing Rotation Displacement Using OpenCV in Java\nDESCRIPTION: This Java snippet shows how to compute rotation displacement for the corresponding transformation between images using OpenCV. The OpenCV library needs to be included in the project. It processes images to output the rotation displacement matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_33\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.Core;\nimport org.opencv.core.Mat;\n\npublic void computeRotationDisplacement(Mat image1, Mat image2) {\n    // Code to compute rotation displacement\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Creating 2D Histogram using OpenCV calcHist function\nDESCRIPTION: This code demonstrates how to create a 2D histogram using OpenCV's calcHist function. It loads an image, converts it from BGR to HSV color space, and then calculates a 2D histogram of the Hue and Saturation channels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_2d_histogram/py_2d_histogram.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nimg = cv.imread('home.jpg')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nhsv = cv.cvtColor(img,cv.COLOR_BGR2HSV)\n\nhist = cv.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying an Initial Black Image with OpenCV in C++\nDESCRIPTION: Creates a black image represented by a cv::Mat object named 'image'. The image dimensions are specified by 'window_height' and 'window_width', and it uses an 8-bit, 3-channel color format (CV_8UC3). The image is then displayed in a window identified by 'window_name'.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/random_generator_and_text/random_generator_and_text.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n@code{.cpp}\n/// Initialize a matrix filled with zeros\nMat image = Mat::zeros( window_height, window_width, CV_8UC3 );\n\n/// Show it in a window during DELAY ms\nimshow( window_name, image );\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Trackbar Callback Method in Java\nDESCRIPTION: This Java snippet defines the `on_trackbar` method, which serves as the callback logic. It calculates `alpha` and `beta` based on the trackbar position `pos`, performs image blending using `Core.addWeighted`, and updates the display in the named window (`WINDOW_NAME`) using `HighGui.imshow`. Class member variables `src1`, `src2`, `dst`, `alpha`, `beta`, and `ALPHA_SLIDER_MAX` are used.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n//![on_trackbar]\nprivate void on_trackbar(int pos) {\n    alpha = (double) pos / ALPHA_SLIDER_MAX;\n    beta = (1.0 - alpha);\n    Core.addWeighted(src1, alpha, src2, beta, 0.0, dst);\n    HighGui.imshow(WINDOW_NAME, dst);\n}\n//![on_trackbar]\n```\n\n----------------------------------------\n\nTITLE: Upsampling an Image with cv.pyrUp in OpenCV.js\nDESCRIPTION: This function signature details how to upsample an input image (`src`) to create a higher-resolution output image (`dst`). It upsamples the image and then blurs it. The `dstsize` parameter specifies the desired size for the output image (defaults to double the input dimensions if Size(0,0)). `borderType` specifies the pixel extrapolation method, with only cv.BORDER_DEFAULT being supported.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_pyramids/js_pyramids.markdown#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ncv.pyrUp (src, dst, dstsize = new cv.Size(0, 0), borderType  = cv.BORDER_DEFAULT)\n```\n\n----------------------------------------\n\nTITLE: Trackbar Callback in OpenCV C++\nDESCRIPTION: Demonstrates creating a trackbar in an OpenCV window and attaching a callback function in C++. The function is triggered when the trackbar's state changes. Requires OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ncv::createTrackbar(\"Trackbar\", \"Source\", &sliderValue, maxValue, on_trackbar);\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependency Requirements\nDESCRIPTION: This snippet lists minimum required versions for the svglib (>=1.5.1) and reportlab (>=4.0.0) Python packages. It is intended to be used with pip or similar Python package managers to ensure the correct dependencies are present. There are no parameters or code logic involved, only package specifications as prerequisites for the project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/pattern_tools/test_requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\nsvglib>=1.5.1\\nreportlab>=4.0.0\n```\n\n----------------------------------------\n\nTITLE: Defining the OpenCV G-API Module in CMake\nDESCRIPTION: Defines the 'gapi' module using the `ocv_add_module` function. It specifies `opencv_imgproc` as a required dependency and `opencv_video` and `opencv_calib3d` as optional dependencies. It also indicates that Python wrappers should be generated for this module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(the_description \"OpenCV G-API Core Module\")\n\nocv_add_module(gapi\n    REQUIRED\n      opencv_imgproc\n    OPTIONAL\n      opencv_video opencv_calib3d\n    WRAP\n      python\n)\n```\n\n----------------------------------------\n\nTITLE: Refining ArUco Marker Detection with Board Context in C++\nDESCRIPTION: Code that shows how to refine ArUco marker detection by leveraging board information. This function attempts to identify marker candidates that weren't initially detected by analyzing their position relative to the board.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_board_detection/aruco_board_detection.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\ncv::Mat imageCopy;\nimage.copyTo(imageCopy);\n\n// detect markers\ncv::aruco::DetectorParameters detectorParams = cv::aruco::DetectorParameters();\ncv::aruco::Dictionary dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250);\ncv::aruco::ArucoDetector detector(dictionary, detectorParams);\n\nstd::vector<int> markerIds;\nstd::vector<std::vector<cv::Point2f>> markerCorners, rejectedCandidates;\ndetector.detectMarkers(image, markerCorners, markerIds, rejectedCandidates);\n\n// refine detected markers\ncv::Ptr<cv::aruco::Board> board;\nreadDetectorParameters(parser.get<std::string>(\"cd\"), board, dictionary);\ndetector.refineDetectedMarkers(image, board, markerCorners, markerIds, rejectedCandidates);\n```\n\n----------------------------------------\n\nTITLE: Loading Source Image in Python\nDESCRIPTION: Python snippet demonstrating how to load an image from a file using OpenCV's `cv.imread` function. The first command-line argument (accessed via `sys.argv[1]`) is expected to be the path to the image file. Includes error handling using `if src is None:` check.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/Histograms_Matching/histogram_calculation/calcHist_Demo.py Load image\n```\n\n----------------------------------------\n\nTITLE: Defining a Compound Kernel Implementation with GAPI_COMPOUND_KERNEL (OpenCV G-API, C++)\nDESCRIPTION: Defines a compound kernel implementation using the GAPI_COMPOUND_KERNEL macro to represent a high-level operation as a composition of multiple sub-kernels (subgraph). Useful for complex semantics where a single backend may map the functionality to several primitives. Requires G-API and its macro facilities. Compound implementation is backend-dependent and simplifies dispatching of multi-stage operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/doc/20-kernel-api.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n#define GAPI_COMPOUND_KERNEL(CompoundName, Interface)\n// Example usage would compose a backend compound kernel for a multi-stage algorithm (e.g. goodFeaturesToTrack)\n// Actual implementation will call Interface::on(...) with a sequence of lower-level kernel calls specifying the desired operation in terms of primitives\n```\n\n----------------------------------------\n\nTITLE: Orchestrating Out-of-Focus Deblurring Process in OpenCV C++\nDESCRIPTION: This is the main function demonstrating the workflow for deblurring an out-of-focus image using the Wiener filter. It handles command-line argument parsing for the input image path, PSF radius, and SNR. It loads the image, performs necessary conversions and padding, calculates the PSF and Wiener filter using helper functions, applies the filter in the frequency domain, normalizes the result, and displays both the original and the restored images. Depends on OpenCV core, imgproc, and highgui modules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/out_of_focus_deblur_filter/out_of_focus_deblur_filter.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nint main(int argc, char *argv[])\n{\n    const char* KERNEL_USAGE = \"Usage: %s [OPTIONS] image [output]\\\\n\"\n                             \"EXAMPLE:\\\\n    %s ../data/lena.jpg 53 5200 \\\\n\"\n                             \"OPTIONS:\\\\n\"\n                             \"-h, --help                    : Print help message\\\\n\"\n                             \"-R R                          : Radius of PSF, R > 0 (int) \\\\n\"\n                             \"-SNR SNR                      : Signal-to-Noise Ratio, SNR > 0 (double)\";\n    const char* about = \"This program demonstrates the use of the Wiener filter for image deblurring.\";\n    cv::CommandLineParser parser(argc, argv, KERNEL_USAGE);\n    parser.about(about);\n    if (parser.has(\"help\"))\n    {\n        parser.printMessage();\n        return 0;\n    }\n    int R = parser.get<int>(\"R\");\n    double SNR = parser.get<double>(\"SNR\");\n    if (R <= 0)\n    {\n        parser.printMessage();\n        return -1;\n    }\n    if (SNR <= 0)\n    {\n        parser.printMessage();\n        return -1;\n    }\n    String imgInFileName = parser.get<String>(\"@image\");\n    if (imgInFileName.empty())\n    {\n        parser.printMessage();\n        return -1;\n    }\n    String imgOutFileName = parser.get<String>(\"@output\");\n    if (!parser.check()) {\n        parser.printErrors();\n        return -1;\n    }\n\n    Mat imgIn;\n    imgIn = imread(samples::findFile(imgInFileName), IMREAD_GRAYSCALE);\n    if (imgIn.empty()) //check whether the image is loaded or not\n    {\n        cout << \"ERROR : Image cannot be loaded..!!\" << endl;\n        return -1;\n    }\n\n    Mat imgOut;\n\n    // it needs to process even image only\n    Rect roi = Rect(0, 0, imgIn.cols & -2, imgIn.rows & -2);\n\n    //Hw calculation (start)\n    Mat Hw, h;\n    calcPSF(h, roi.size(), R);\n    calcWnrFilter(h, Hw, 1.0 / SNR);\n    //Hw calculation (stop)\n\n    // filtering (start)\n    filter2DFreq(imgIn(roi), imgOut, Hw);\n    // filtering (stop)\n\n    imgOut.convertTo(imgOut, CV_8U);\n    normalize(imgOut, imgOut, 0, 255, NORM_MINMAX);\n\n    if (!imgOutFileName.empty())\n    {\n        imwrite(imgOutFileName, imgOut);\n    }\n\n    imshow(\"original\", imgIn);\n    imshow(\"result\", imgOut);\n\n    waitKey(0);\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Goto-Based Corner Detection Logic C++\nDESCRIPTION: This snippet features nested conditional statements using goto to classify points as corners based on pixel intensity comparisons. The algorithm depends on the values of 'ptr[offsetX]' and threshold comparisons with 'cb' or 'c_b'. It returns control either to 'is_a_corner' or 'is_not_a_corner'. Main constraints include its hard-coded logical structure and use of conditional branches, related to classical high-speed corner detection methods.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_5\n\nLANGUAGE: C/C++\nCODE:\n```\ngoto is_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset3] > cb)\n    if(ptr[offset4] > cb)\n      if(ptr[offset5] > cb)\n        if(ptr[offset6] > cb)\n          if(ptr[offset7] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset1] > cb)\n    if(ptr[offset12] > cb)\n      if(ptr[offset13] > cb)\n        if(ptr[offset14] > cb)\n          if(ptr[offset15] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\nif(ptr[offset9] < c_b)\n  if(ptr[offset7] < c_b)\n    if(ptr[offset8] < c_b)\n      if(ptr[offset10] < c_b)\n        if(ptr[offset11] < c_b)\n          if(ptr[offset6] < c_b)\n            if(ptr[offset5] < c_b)\n              if(ptr[offset4] < c_b)\n                if(ptr[offset3] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset12] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset12] < c_b)\n                  if(ptr[offset13] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset12] < c_b)\n                if(ptr[offset13] < c_b)\n                  if(ptr[offset14] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset12] < c_b)\n              if(ptr[offset13] < c_b)\n                if(ptr[offset14] < c_b)\n                  if(ptr[offset15] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset0] < c_b)\n    if(ptr[offset2] > cb)\n      if(ptr[offset9] > cb)\n        if(ptr[offset7] > cb)\n          if(ptr[offset8] > cb)\n            if(ptr[offset6] > cb)\n              if(ptr[offset5] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset3] > cb)\n                    if(ptr[offset1] > cb)\n                      goto is_a_corner;\n                    else\n                      if(ptr[offset10] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset10] > cb)\n                      if(ptr[offset11] > cb)\n                        if(ptr[offset12] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      if(ptr[offset12] > cb)\n                        if(ptr[offset13] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    if(ptr[offset12] > cb)\n                      if(ptr[offset13] > cb)\n                        if(ptr[offset14] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  if(ptr[offset12] > cb)\n                    if(ptr[offset13] > cb)\n                      if(ptr[offset14] > cb)\n                        if(ptr[offset15] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n      if(ptr[offset9] < c_b)\n        if(ptr[offset10] < c_b)\n          if(ptr[offset11] < c_b)\n            if(ptr[offset8] < c_b)\n              if(ptr[offset12] < c_b)\n                if(ptr[offset13] < c_b)\n                  if(ptr[offset14] < c_b)\n                    if(ptr[offset15] < c_b)\n                      goto is_a_corner;\n                    else\n                      if(ptr[offset6] < c_b)\n                        if(ptr[offset7] < c_b)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      if(ptr[offset5] < c_b)\n                        if(ptr[offset6] < c_b)\n                          if(ptr[offset7] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      if(ptr[offset4] < c_b)\n                        if(ptr[offset5] < c_b)\n                          if(ptr[offset6] < c_b)\n                            if(ptr[offset7] < c_b)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n        else\n          if(ptr[offset3] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset5] < c_b)\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset7] < c_b)\n                    goto is_a_corner;\n\n```\n\n----------------------------------------\n\nTITLE: Using OpenCV's Built-in Histogram Equalization Function\nDESCRIPTION: This code demonstrates how to use OpenCV's built-in equalizeHist() function to perform histogram equalization. It loads a grayscale image, applies equalization, and displays the original and equalized images side by side for comparison.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimg = cv.imread('wiki.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nequ = cv.equalizeHist(img)\nres = np.hstack((img,equ)) #stacking images side-by-side\ncv.imwrite('res.png',res)\n```\n\n----------------------------------------\n\nTITLE: Random Number Generator Functor for Thrust\nDESCRIPTION: A functor that generates random floating-point values within a specified range. It uses the Thrust random number generator with a linear congruential engine.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-thrust-interop/gpu_thrust_interop.markdown#2025-04-22_snippet_3\n\nLANGUAGE: CUDA\nCODE:\n```\nstruct prg\n{\n    float a, b;\n\n    __host__ __device__ prg(float _a=0.f, float _b=1.f) : a(_a), b(_b) {}\n\n    __host__ __device__\n    float operator()(const unsigned int n) const\n    {\n        thrust::default_random_engine rng;\n        thrust::uniform_real_distribution<float> dist(a, b);\n        rng.discard(n);\n        return dist(rng);\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring Stream Parameters for Astra Camera Sensors\nDESCRIPTION: Sets up the frame parameters for both depth and color streams to use VGA resolution (640x480), which is the maximum available for both sensors. This ensures consistent resolution between color and depth data for easier registration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n// Set stream parameters\ndepthStream.set(CAP_PROP_FRAME_WIDTH, 640);\ndepthStream.set(CAP_PROP_FRAME_HEIGHT, 480);\n\ncolorStream.set(CAP_PROP_FRAME_WIDTH, 640);\ncolorStream.set(CAP_PROP_FRAME_HEIGHT, 480);\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Cascade Trainer Build\nDESCRIPTION: CMake configuration that disables specific compiler warnings and sets up the opencv_traincascade application build with required OpenCV module dependencies. Includes all CPP source files in the current directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/traincascade/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nocv_warnings_disable(CMAKE_CXX_FLAGS -Woverloaded-virtual -Winconsistent-missing-override -Wsuggest-override)\nfile(GLOB SRCS *.cpp)\nocv_add_application(opencv_traincascade\n    MODULES opencv_core opencv_imgproc opencv_objdetect opencv_imgcodecs opencv_highgui opencv_calib3d opencv_features2d\n    SRCS ${SRCS})\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenVINO Backend Option and Handling nGraph Dependency - CMake\nDESCRIPTION: Defines an option to enable OpenVINO backend, validates nGraph presence, and adds the plugin subdirectory or required runtime libraries based on list membership. Contains logic to support OpenVINO plugin for inference acceleration using the appropriate backend. Inputs: TARGET ocv.3rdparty.openvino, DNN_PLUGIN_LIST, nGraph library flags; output: plugin inclusion or required runtime dependency determination.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_22\n\nLANGUAGE: CMake\nCODE:\n```\nocv_option(OPENCV_DNN_OPENVINO \"Build with OpenVINO support (2021.4+)\" (TARGET ocv.3rdparty.openvino))\nif(TARGET ocv.3rdparty.openvino AND OPENCV_DNN_OPENVINO)\n  if(NOT HAVE_OPENVINO AND NOT HAVE_NGRAPH)\n    message(FATAL_ERROR \"DNN: Inference Engine is not supported without enabled 'nGraph'. Check build configuration.\")\n  endif()\n  if(\"openvino\" IN_LIST DNN_PLUGIN_LIST OR DNN_PLUGIN_LIST STREQUAL \"all\")\n    # plugin doesn't support PCH, separate directory scope is necessary\n    # opencv_world requires absolute path\n    add_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/misc/plugin/openvino\" \"${CMAKE_CURRENT_BINARY_DIR}/dnn_plugin_openvino\")\n  elseif(NOT OPENCV_DNN_BUILTIN_BACKEND)\n    list(APPEND dnn_runtime_libs ocv.3rdparty.openvino)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Drawing a Rectangle in Java\nDESCRIPTION: Example of using the Rectangle() function in OpenCV Java to draw a filled yellow rectangle. The function specifies two opposite corners of the rectangle along with color and fill options.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_28\n\nLANGUAGE: java\nCODE:\n```\nRectangle(rook_image, new Point(0, 7*w/8), new Point(w, w), new Scalar(0, 255, 255), Core.FILLED, Core.LINE_8, 0);\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Conditional Logic - OpenCV C++\nDESCRIPTION: Multiple nested conditional statements comparing pixel values at different offsets to determine if a point is a corner. Uses pointer arithmetic and goto statements for control flow between 'is_a_corner' and 'is_not_a_corner' states based on threshold comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_17\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset6] < c_b)\n  goto is_not_a_corner;\nelse\n  if(ptr[offset6] > cb)\n    if(ptr[offset8] > cb)\n      if(ptr[offset4] > cb)\n        if(ptr[offset3] > cb)\n          goto is_a_corner;\n        else\n          if(ptr[offset10] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset10] > cb)\n          if(ptr[offset11] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Installing ARM Hard Float Toolchain - Bash\nDESCRIPTION: Installs the gcc-arm-linux-gnueabihf package for compiling ARM binaries with hardware floating-point (hard-float) ABI. Requires apt-get and the correct repository configuration. The main parameter is the package name and the command should be executed with sudo. This is meant for targets supporting hard-float ABI.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/arm_crosscompile_with_cmake.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install gcc-arm-linux-gnueabihf\n```\n\n----------------------------------------\n\nTITLE: Defining a Scalar Color in C++\nDESCRIPTION: Creating a Scalar object to represent a BGR color in OpenCV C++. The Scalar represents a 3-element vector with Blue, Green, and Red values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nScalar( a, b, c )\n```\n\n----------------------------------------\n\nTITLE: Displaying Training Data for Non-Linear SVM in OpenCV\nDESCRIPTION: This snippet shows how to display the training data points for non-linear SVM classification. It uses different colors to distinguish between the two classes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\nfor (int i = 0; i < trainData.rows; ++i)\n{\n    int x = (int)(trainData.at<float>(i,0) * WIDTH);\n    int y = (int)(trainData.at<float>(i,1) * HEIGHT);\n    if (labels.at<int>(i) == 1)\n    {\n        circle(res, Point(x,y), 5, Scalar(0, 100, 0), -1);\n        circle(res, Point(x,y), 4, Scalar(0, 255, 0), -1);\n    }\n    else\n    {\n        circle(res, Point(x,y), 5, Scalar(100, 0, 0), -1);\n        circle(res, Point(x,y), 4, Scalar(255, 0, 0), -1);\n    }\n}\n```\n\nLANGUAGE: Java\nCODE:\n```\nfor (int i = 0; i < trainData.rows(); i++) {\n    int x = (int) (trainData.get(i, 0)[0] * WIDTH);\n    int y = (int) (trainData.get(i, 1)[0] * HEIGHT);\n    if (labels.get(i, 0)[0] == 1) {\n        Imgproc.circle(res, new Point(x, y), 5, new Scalar(0, 100, 0), -1);\n        Imgproc.circle(res, new Point(x, y), 4, new Scalar(0, 255, 0), -1);\n    } else {\n        Imgproc.circle(res, new Point(x, y), 5, new Scalar(100, 0, 0), -1);\n        Imgproc.circle(res, new Point(x, y), 4, new Scalar(255, 0, 0), -1);\n    }\n}\n```\n\nLANGUAGE: Python\nCODE:\n```\nfor i in range(train_data.shape[0]):\n    x = int(train_data[i,0]*WIDTH)\n    y = int(train_data[i,1]*HEIGHT)\n    if labels[i] == 1:\n        cv.circle(res, (x,y), 5, (0, 100, 0), -1)\n        cv.circle(res, (x,y), 4, (0, 255, 0), -1)\n    else:\n        cv.circle(res, (x,y), 5, (100, 0, 0), -1)\n        cv.circle(res, (x,y), 4, (255, 0, 0), -1)\n```\n\n----------------------------------------\n\nTITLE: Applying Black Hat Transform with OpenCV Python\nDESCRIPTION: Demonstrates black hat transformation which finds the difference between the closing of input image and input image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.markdown#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nblackhat = cv.morphologyEx(img, cv.MORPH_BLACKHAT, kernel)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenCV C++ Sample Required Dependencies\nDESCRIPTION: Defines a list of required OpenCV modules for building C++ sample projects. This includes various OpenCV components such as core, imgproc, flann, and others. These are necessary for building the samples, and their availability is checked before proceeding.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/cpp/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(OPENCV_CPP_SAMPLES_REQUIRED_DEPS\n  opencv_core\n  opencv_imgproc\n  opencv_flann\n  opencv_imgcodecs\n  opencv_videoio\n  opencv_highgui\n  opencv_ml\n  opencv_video\n  opencv_objdetect\n  opencv_photo\n  opencv_features2d\n  opencv_calib3d\n  opencv_stitching\n  opencv_dnn\n  opencv_gapi\n  ${OPENCV_MODULES_PUBLIC}\n  ${OpenCV_LIB_COMPONENTS})\nocv_check_dependencies(${OPENCV_CPP_SAMPLES_REQUIRED_DEPS})\n```\n\n----------------------------------------\n\nTITLE: Saving Animation with OpenCV\nDESCRIPTION: Demonstrates how to save an animation structure back to a file using cv::imwriteanimation, supporting formats like WebP, GIF, AVIF, and APNG.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/animations.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\ncv::imwriteanimation(\"output.webp\", animation);\n```\n\nLANGUAGE: Python\nCODE:\n```\ncv.imwriteanimation(\"output.webp\", animation)\n```\n\n----------------------------------------\n\nTITLE: Nested Conditional Branching for Feature Detection in OpenCV\nDESCRIPTION: A segment of OpenCV's feature detection algorithm showing deeply nested conditional logic. The code compares pixel values at different offsets against threshold values (cb and c_b) to determine whether to continue processing ('goto structured') or confirm a feature detection ('goto success_structured').\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_11\n\nLANGUAGE: C/C++\nCODE:\n```\ngoto structured;\nelse\n  goto structured;\nelse\n  if(ptr[offset6] > cb)\n    if(ptr[offset4] > cb)\n      if(ptr[offset3] > cb)\n        goto success_structured;\n      else\n        if(ptr[offset10] > cb)\n          goto success_structured;\n        else\n          goto structured;\n    else\n      if(ptr[offset10] > cb)\n        if(ptr[offset11] > cb)\n          goto success_structured;\n        else\n          goto structured;\n      else\n        goto structured;\n  else\n    goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  if(ptr[offset5] < c_b)\n    if(ptr[offset9] > cb)\n      if(ptr[offset3] < c_b)\n        if(ptr[offset4] < c_b)\n          if(ptr[offset11] > cb)\n            if(ptr[offset1] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset2] > cb)\n                    goto success_structured;\n                  else\n                    if(ptr[offset7] > cb)\n                      goto success_structured;\n                    else\n                      goto structured;\n                else\n                  goto structured;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset2] < c_b)\n                    if(ptr[offset7] < c_b)\n                      if(ptr[offset8] < c_b)\n                        goto success_structured;\n                      else\n                        goto structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset10] > cb)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset2] < c_b)\n                    if(ptr[offset7] < c_b)\n                      if(ptr[offset1] < c_b)\n                        goto success_structured;\n                      else\n                        if(ptr[offset8] < c_b)\n                          goto success_structured;\n                        else\n                          goto structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n          else\n            if(ptr[offset2] < c_b)\n              if(ptr[offset7] < c_b)\n                if(ptr[offset1] < c_b)\n                  if(ptr[offset6] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset8] < c_b)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n              else\n                goto structured;\n            else\n              goto structured;\n        else\n          if(ptr[offset11] > cb)\n            if(ptr[offset8] > cb)\n              if(ptr[offset10] > cb)\n                if(ptr[offset1] > cb)\n                  if(ptr[offset2] > cb)\n                    goto success_structured;\n                  else\n                    if(ptr[offset7] > cb)\n                      goto success_structured;\n                    else\n                      goto structured;\n                else\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset7] > cb)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n              else\n                goto structured;\n            else\n              goto structured;\n          else\n            goto structured;\n      else\n        if(ptr[offset11] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset1] > cb)\n                if(ptr[offset2] > cb)\n                  goto success_structured;\n                else\n                  if(ptr[offset7] > cb)\n                    if(ptr[offset8] > cb)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n              else\n                if(ptr[offset6] > cb)\n                  if(ptr[offset7] > cb)\n                    if(ptr[offset8] > cb)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset8] > cb)\n                if(ptr[offset1] > cb)\n                  if(ptr[offset2] > cb)\n                    goto success_structured;\n                  else\n                    if(ptr[offset7] > cb)\n                      goto success_structured;\n                    else\n                      goto structured;\n                else\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset7] > cb)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n              else\n                goto structured;\n          else\n            goto structured;\n        else\n          goto structured;\n    else\n      if(ptr[offset9] < c_b)\n        if(ptr[offset2] > cb)\n          if(ptr[offset1] > cb)\n            if(ptr[offset4] > cb)\n              if(ptr[offset10] > cb)\n                if(ptr[offset3] > cb)\n                  if(ptr[offset11] > cb)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n```\n\n----------------------------------------\n\nTITLE: Loading Source Image in OpenCV\nDESCRIPTION: Loads the input image that will be used for remapping operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/remap/remap.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nMat src = imread( samples::findFile(\"chessboard.png\"), IMREAD_COLOR );\n```\n\nLANGUAGE: Java\nCODE:\n```\nMat src = Imgcodecs.imread(samplesDir + \"/chessboard.png\");\n```\n\nLANGUAGE: Python\nCODE:\n```\nsrc = cv.imread(cv.samples.findFile(\"chessboard.png\"))\n```\n\n----------------------------------------\n\nTITLE: Constructing Rect Structures in OpenCV.js (JavaScript)\nDESCRIPTION: Shows two standard ways to create a Rect: via the cv.Rect constructor and via an object literal with x, y, width, and height. These define rectangles by their top-left corner and dimensions. Needs OpenCV.js and coordinate/size values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_some_data_structures/js_image_arithmetics.markdown#2025-04-22_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n// The first way\nlet rect = new cv.Rect(x, y, width, height);\n// The second way\nlet rect = {x : x, y : y, width : width, height : height};\n```\n\n----------------------------------------\n\nTITLE: C-Style Formatting for OpenCV Mat Output in C++\nDESCRIPTION: Demonstrates formatting OpenCV Mat output using C-style syntax with the format() method and FormatC flag.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\ncout << \"R (c) = \" << endl << format(R, Formatter::FMT_C) << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Installation Options in CMake\nDESCRIPTION: This snippet defines CMake options for configuring the installation of OpenCV. It includes options for installing examples, documentation, and controlling installation paths.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(INSTALL_CREATE_DISTRIB   \"Change install rules to build the distribution package\" OFF )\nOCV_OPTION(INSTALL_BIN_EXAMPLES     \"Install prebuilt examples\" WIN32 IF BUILD_EXAMPLES)\nOCV_OPTION(INSTALL_C_EXAMPLES       \"Install C examples\"        OFF )\nOCV_OPTION(INSTALL_PYTHON_EXAMPLES  \"Install Python examples\"   OFF )\nOCV_OPTION(INSTALL_ANDROID_EXAMPLES \"Install Android examples\"  OFF IF ANDROID )\nOCV_OPTION(INSTALL_TO_MANGLED_PATHS \"Enables mangled install paths, that help with side by side installs.\" OFF IF (UNIX AND NOT ANDROID AND NOT APPLE_FRAMEWORK AND BUILD_SHARED_LIBS) )\nOCV_OPTION(INSTALL_TESTS            \"Install accuracy and performance test binaries and test data\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Create and Display Windows with OpenCV C++\nDESCRIPTION: This snippet shows how to create an OpenCV window and display an image in it using C++. Requires OpenCV 3.0 or higher.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ncv::namedWindow(\"Source\", cv::WINDOW_AUTOSIZE);\ncv::imshow(\"Source\", src);\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Shared AAR for Android (Python Command)\nDESCRIPTION: Executes the Python script `build_java_shared_aar.py` to create an AAR package containing OpenCV Java bindings and the shared C++ library (.so files). Requires the path to the downloaded OpenCV Android SDK as an argument. The output AAR and Maven repository will be generated in the 'outputs' directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/android/aar-template/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython build_java_shared_aar.py \"~/opencv-4.7.0-android-sdk/OpenCV-android-sdk\"\n```\n\n----------------------------------------\n\nTITLE: Using Sobel and Scharr Derivatives with OpenCV.js - JavaScript\nDESCRIPTION: These snippets demonstrate how to compute first-order image gradients using the cv.Sobel and cv.Scharr functions in OpenCV.js. Required dependencies include OpenCV.js properly loaded and initialized in your JavaScript environment. Core parameters include the input and output image matrices, image depth, derivative orders in the x and y directions, and kernel size or scaling factors. The expected input is an image matrix; the output is an image matrix representing the first derivative response, with kernel size and datatype restrictions as specified.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_gradients/js_gradients.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ncv.Sobel(src, dst, ddepth, dx, dy, ksize = 3, scale = 1, delta = 0, borderType = cv.BORDER_DEFAULT);\n```\n\nLANGUAGE: javascript\nCODE:\n```\ncv.Scharr(src, dst, ddepth, dx, dy, scale = 1, delta = 0, borderType = cv.BORDER_DEFAULT);\n```\n\n----------------------------------------\n\nTITLE: Finding OpenCV Library Paths and Linker Flags using pkg-config in Bash\nDESCRIPTION: This Bash command uses `pkg-config` to determine the linker flags required for linking against OpenCV libraries. It provides the library search paths (`-L` flags) and the specific libraries to link against (`-l` flags), which are needed for the linker settings in Eclipse.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_eclipse/linux_eclipse.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npkg-config --libs opencv\n```\n\n----------------------------------------\n\nTITLE: Setting Input Blob for Network in C++\nDESCRIPTION: This snippet highlights how to set the prepared blob as the input to the neural network. Passing the formatted input blob allows the network to perform the necessary forward passes for inference.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_googlenet/dnn_googlenet.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/classification.cpp Set input blob\n```\n\n----------------------------------------\n\nTITLE: Implementing AGAST-5-8 Corner Detector in OpenCV (C++)\nDESCRIPTION: This snippet is the implementation of the AGAST_5_8 function, which detects corners in a grayscale image using the AGAST accelerated segment test approach. It operates by iteratively evaluating pixel neighborhoods in an input image, utilizing pointer arithmetic and multiple offset comparisons to efficiently check for corner-like structures based on a threshold. Dependencies include OpenCV data structures (Mat, InputArray, KeyPoint) and prior definition of makeAgastOffsets and AgastFeatureDetector. Inputs are the image, output vector for keypoints, and a threshold; it outputs a list of detected keypoints. The snippet assumes input is single-channel and image data is continuous or can be cloned for continuity.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include \"precomp.hpp\"\n#include \"agast_score.hpp\"\n\n#ifdef _MSC_VER\n#pragma warning( disable : 4127 )\n#endif\n\nnamespace cv\n{\n\nstatic void AGAST_5_8(InputArray _img, std::vector<KeyPoint>& keypoints, int threshold)\n{\n\n    cv::Mat img;\n    if(!_img.getMat().isContinuous())\n      img = _img.getMat().clone();\n    else\n      img = _img.getMat();\n\n    size_t total = 0;\n    int xsize = img.cols;\n    int ysize = img.rows;\n    size_t nExpectedCorners = keypoints.capacity();\n    int x, y;\n    int xsizeB = xsize - 2;\n    int ysizeB = ysize - 1;\n    int width;\n\n    keypoints.resize(0);\n\n    int pixel_5_8_[16];\n    makeAgastOffsets(pixel_5_8_, (int)img.step, AgastFeatureDetector::AGAST_5_8);\n\n    short offset0 = (short) pixel_5_8_[0];\n    short offset1 = (short) pixel_5_8_[1];\n    short offset2 = (short) pixel_5_8_[2];\n    short offset3 = (short) pixel_5_8_[3];\n    short offset4 = (short) pixel_5_8_[4];\n    short offset5 = (short) pixel_5_8_[5];\n    short offset6 = (short) pixel_5_8_[6];\n    short offset7 = (short) pixel_5_8_[7];\n\n    width = xsize;\n\n    for(y = 1; y < ysizeB; y++)\n    {\n        x = 0;\n        while(true)\n        {\n          homogeneous:\n          {\n            x++;\n            if(x > xsizeB)\n                break;\n            else\n            {\n                const unsigned char* const ptr = img.ptr() + y*width + x;\n                const int cb = *ptr + threshold;\n                const int c_b = *ptr - threshold;\n                if(ptr[offset0] > cb)\n                  if(ptr[offset2] > cb)\n                    if(ptr[offset3] > cb)\n                      if(ptr[offset5] > cb)\n                        if(ptr[offset1] > cb)\n                          if(ptr[offset4] > cb)\n                            goto success_structured;\n                          else\n                            if(ptr[offset7] > cb)\n                              goto success_structured;\n                            else\n                              goto homogeneous;\n                        else\n                          if(ptr[offset4] > cb)\n                            if(ptr[offset6] > cb)\n                              goto success_structured;\n                            else\n                              goto homogeneous;\n                          else\n                            goto homogeneous;\n                      else\n                        if(ptr[offset1] > cb)\n                          if(ptr[offset4] > cb)\n                            goto success_homogeneous;\n                          else\n                            if(ptr[offset7] > cb)\n                              goto success_homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          goto homogeneous;\n                    else\n                      if(ptr[offset7] > cb)\n                        if(ptr[offset6] > cb)\n                          if(ptr[offset5] > cb)\n                            if(ptr[offset1] > cb)\n                              goto success_structured;\n                            else\n                              if(ptr[offset4] > cb)\n                                goto success_structured;\n                              else\n                                goto homogeneous;\n                          else\n                            if(ptr[offset1] > cb)\n                              goto success_homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          goto homogeneous;\n                      else\n                        if(ptr[offset5] < c_b)\n                          if(ptr[offset3] < c_b)\n                            if(ptr[offset7] < c_b)\n                              if(ptr[offset4] < c_b)\n                                if(ptr[offset6] < c_b)\n                                  goto success_structured;\n                                else\n                                  goto structured;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                          else\n                            goto homogeneous;\n                        else\n                          goto homogeneous;\n                  else\n                    if(ptr[offset5] > cb)\n                      if(ptr[offset7] > cb)\n                        if(ptr[offset6] > cb)\n                          if(ptr[offset1] > cb)\n                            goto success_homogeneous;\n                          else\n                            if(ptr[offset4] > cb)\n                              goto success_homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          goto homogeneous;\n                      else\n                        goto homogeneous;\n                    else\n                      if(ptr[offset5] < c_b)\n                        if(ptr[offset3] < c_b)\n                          if(ptr[offset2] < c_b)\n                            if(ptr[offset1] < c_b)\n                              if(ptr[offset4] < c_b)\n                                goto success_structured;\n                              else\n                                goto homogeneous;\n                            else\n                              if(ptr[offset4] < c_b)\n                                if(ptr[offset6] < c_b)\n                                  goto success_structured;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            if(ptr[offset7] < c_b)\n                              if(ptr[offset4] < c_b)\n                                if(ptr[offset6] < c_b)\n                                  goto success_structured;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          goto homogeneous;\n                      else\n                        goto homogeneous;\n                else\n                if(ptr[offset0] < c_b)\n                  if(ptr[offset2] < c_b)\n                    if(ptr[offset7] > cb)\n                      if(ptr[offset3] < c_b)\n                        if(ptr[offset5] < c_b)\n                          if(ptr[offset1] < c_b)\n                            if(ptr[offset4] < c_b)\n                              goto success_structured;\n                            else\n                              goto structured;\n                          else\n                            if(ptr[offset4] < c_b)\n                              if(ptr[offset6] < c_b)\n                                goto success_structured;\n                              else\n                                goto structured;\n                            else\n                              goto homogeneous;\n                        else\n                          if(ptr[offset1] < c_b)\n                            if(ptr[offset4] < c_b)\n                              goto success_structured;\n                            else\n                              goto homogeneous;\n                          else\n                            goto homogeneous;\n                      else\n                        if(ptr[offset5] > cb)\n                          if(ptr[offset3] > cb)\n                            if(ptr[offset4] > cb)\n                              if(ptr[offset6] > cb)\n                                goto success_structured;\n                              else\n                                goto structured;\n                            else\n                              goto homogeneous;\n                          else\n```\n\n----------------------------------------\n\nTITLE: Calculating Back Projection with OpenCV in Java\nDESCRIPTION: This Java sample shows the equivalent process: reading an image, converting it to HSV, extracting the Hue channel, providing GUI controls for bin count, and computing the histogram and back projection using OpenCV Java bindings. Required dependencies are OpenCV for Java. Inputs are user-supplied images, and the outputs are GUI windows for visualization. The number of histogram bins is the main adjustable parameter. The code leverages Imgproc.cvtColor, Core.mixChannels, Imgproc.calcHist, and Imgproc.calcBackProject. User adjustments are propagated via a trackbar listener.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/back_projection/back_projection.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n// Read the input image\\nMat src = Imgcodecs.imread(filename);\\nif (src.empty()) {\\n    System.out.println(\\\"Could not open or find the image!\\\");\\n    return;\\n}\\n\n```\n\nLANGUAGE: Java\nCODE:\n```\n// Transform to HSV\\nMat hsv = new Mat();\\nImgproc.cvtColor(src, hsv, Imgproc.COLOR_BGR2HSV);\\n\n```\n\nLANGUAGE: Java\nCODE:\n```\n// Use only the Hue value\\nList<Mat> hsvList = Arrays.asList(hsv);\\nMat hue = new Mat(hsv.rows(), hsv.cols(), CvType.CV_8UC1);\\nCore.mixChannels(hsvList, Arrays.asList(hue), new MatOfInt(0, 0));\\n\n```\n\nLANGUAGE: Java\nCODE:\n```\n// Set up Trackbar for bins (with GUI slider)\\nJSlider binsSlider = new JSlider(2, 180, 30);\\nbinsSlider.addChangeListener(e -> Hist_and_Backproj());\\n\n```\n\nLANGUAGE: Java\nCODE:\n```\n// Show Image and wait for user\\nHighGui.imshow(\\\"Source image\\\", src);\\nHighGui.waitKey();\\n\n```\n\nLANGUAGE: Java\nCODE:\n```\n// Initialize histogram parameters for callback\\nvoid Hist_and_Backproj() {\\n    int histSize = Math.max(bins, 2);\\n    float[] hueRange = {0, 180};\\n    // ... hist/backproj logic\\n}\\n\n```\n\nLANGUAGE: Java\nCODE:\n```\n// Calculate histogram and normalize\\nImgproc.calcHist(Arrays.asList(hue), new MatOfInt(0), new Mat(), hist, new MatOfInt(histSize), new MatOfFloat(hueRange));\\nCore.normalize(hist, hist, 0, 255, Core.NORM_MINMAX);\\n\n```\n\nLANGUAGE: Java\nCODE:\n```\n// Backprojection\\nImgproc.calcBackProject(Arrays.asList(hue), new MatOfInt(0), hist, backproj, new MatOfFloat(hueRange), 1);\\n\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Primitive Types to XML/YAML/JSON - OpenCV C++\nDESCRIPTION: This C++ snippet shows how to write primitive types (text, numbers) to XML/YAML/JSON files with OpenCV's FileStorage. The << operator is used to insert both the name and value. Reading is performed via the FileStorage's [] operator or the >> operator. Requires OpenCV and <opencv2/core.hpp>. Key parameters include the entry name and the variable for output or input.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nFileStorage fs(\"test.yml\", FileStorage::WRITE);\\nint number = 5;\\nfs << \"number\" << number;\\nfs.release();\\n\\nfs.open(\"test.yml\", FileStorage::READ);\\nint read_number;\\nfs[\"number\"] >> read_number;\\nfs.release();\n```\n\n----------------------------------------\n\nTITLE: Setting Up Gradle Wrapper for Android Tests in CMake\nDESCRIPTION: Copies Gradle wrapper files and configures the wrapper properties for the Android test project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/android_test/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nfile(COPY \"${OpenCV_SOURCE_DIR}/platforms/android/gradle-wrapper/gradlew\" DESTINATION \"${OPENCV_ANDROID_TEST_DIR}\")\nfile(COPY \"${OpenCV_SOURCE_DIR}/platforms/android/gradle-wrapper/gradlew.bat\" DESTINATION \"${OPENCV_ANDROID_TEST_DIR}\")\nfile(COPY \"${OpenCV_SOURCE_DIR}/platforms/android/gradle-wrapper/gradle/wrapper/gradle-wrapper.jar\" DESTINATION \"${OPENCV_ANDROID_TEST_DIR}/gradle/wrapper\")\n\nconfigure_file(\"${OpenCV_SOURCE_DIR}/platforms/android/gradle-wrapper/gradle/wrapper/gradle-wrapper.properties.in\" \"${OPENCV_ANDROID_TEST_DIR}/gradle/wrapper/gradle-wrapper.properties\" @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Initializing Face Detector (FaceDetectorYN) - OpenCV DNN Python\nDESCRIPTION: This snippet demonstrates how to initialize the cv2.FaceDetectorYN class in Python, specifying model path, input size, score threshold, and other parameters. It requires OpenCV (cv2), and an ONNX model for face detection. The result is a FaceDetectorYN object ready for detecting faces in images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_face/dnn_face.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Initialize FaceDetectorYN\ndetector = cv2.FaceDetectorYN_create(\n    modelPath,                # Path to face detection .onnx model\n    \"\",                       # No config file\n    (320, 320),               # Input size\n    score_threshold=0.9,      # Detection threshold\n    nms_threshold=0.3,        # Non-max suppression threshold\n    top_k=5000                # Maximum number of detections\n)\n```\n\n----------------------------------------\n\nTITLE: Drawing Polygon with OpenCV in Python\nDESCRIPTION: Creates a small yellow polygon with four vertices using the cv.polylines() function, showing how to work with arrays of points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_drawing_functions/py_drawing_functions.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\npts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)\npts = pts.reshape((-1,1,2))\ncv.polylines(img,[pts],True,(0,255,255))\n```\n\n----------------------------------------\n\nTITLE: Declaring Variable-Sized SIMD Registers in OpenCV Intrinsics (C++)\nDESCRIPTION: This snippet demonstrates how to declare a variable-sized SIMD register using OpenCV's universal intrinsics—specifically, a register for 8-bit unsigned integers—and retrieve its lane count. No external dependencies are required beyond OpenCV's core HAL intrin module. The register type (e.g., v_uint8) automatically adapts to the available hardware SIMD width. 'nlanes' gives the number of elements, and should be queried instead of hardcoding.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/univ_intrin/univ_intrin.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nv_uint8 a;                            // a is a register supporting uint8(char) data\nint n = a.nlanes;                     // n holds 32\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies from requirements.txt - Console\nDESCRIPTION: Installs all Python package dependencies listed in a provided requirements.txt file into the currently activated virtual environment. This ensures all necessary libraries for TensorFlow to OpenCV conversion are available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Calculating Image Moments using OpenCV Python\nDESCRIPTION: Using Python, this snippet demonstrates the application of OpenCV functions like moments, contourArea, and arcLength for shape analysis in images. Python with OpenCV 3.0 or higher is needed. The script processes input images to extract contours and outputs computed moments and related shape descriptors.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/moments/moments.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\n# Original code can be found at the mentioned OpenCV repository\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Data Structure Class for File I/O - OpenCV Python\nDESCRIPTION: This Python class illustrates how to define a custom data structure (MyData) suitable for manual serialization with OpenCV FileStorage. Data members are initialized to zeros or empty strings. Additional methods should be added to support reading and writing to FileStorage. Requires only built-in Python features.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nclass MyData:\\n    def __init__(self):\\n        self.A = self.X = 0\\n        self.name = ''\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV TBB Parallel Plugin Build in CMake\nDESCRIPTION: Sets up CMake project configuration for OpenCV's TBB parallel processing plugin. Defines minimum CMake version, configures project paths, enables TBB support, and creates the plugin using OpenCV's plugin system. Requires CMake 3.5+ and TBB dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/misc/plugins/parallel_tbb/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5)\nproject(opencv_core_parallel_tbb CXX)\n\nget_filename_component(OpenCV_SOURCE_DIR \"${CMAKE_CURRENT_LIST_DIR}/../../../../..\" ABSOLUTE)\ninclude(\"${OpenCV_SOURCE_DIR}/cmake/OpenCVPluginStandalone.cmake\")\n\n# scan dependencies\nset(WITH_TBB ON)\ninclude(\"${OpenCV_SOURCE_DIR}/modules/core/cmake/parallel/init.cmake\")\n\nmessage(STATUS \"TBB: ver ${TBB_VERSION_MAJOR}.${TBB_VERSION_MINOR} interface ${TBB_INTERFACE_VERSION}\")\nocv_create_plugin(core \"opencv_core_parallel_tbb\" \"ocv.3rdparty.tbb\" \"TBB\" \"src/parallel/parallel_tbb.cpp\")\n```\n\n----------------------------------------\n\nTITLE: ABI and Flexible Page Size Linker Flags for Android - CMake\nDESCRIPTION: This snippet adds linker flags to support 16k memory pages on ARM64 and x86_64 Android ABIs when required. It checks for the ANDROID_SUPPORT_FLEXIBLE_PAGE_SIZES condition and appropriately appends -Wl,-z,max-page-size=16384 to the shared linker flags for supported ABIs. Prerequisites: NDK configuration, possibly prior to NDK 27. Inputs: system variables; Outputs: modified linker flags to ensure compatibility with larger memory pages.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-4-opencl/jni/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# For 16k pages support with NDK prior 27\n# Details: https://developer.android.com/guide/practices/page-sizes?hl=en\nif(ANDROID_SUPPORT_FLEXIBLE_PAGE_SIZES)\n  if(ANDROID_ABI STREQUAL arm64-v8a OR ANDROID_ABI STREQUAL x86_64)\n    set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -Wl,-z,max-page-size=16384\")\n  endif()\nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Changing RANSAC Parameters via Command Line in C++\nDESCRIPTION: Demonstrates command-line instruction to adjust RANSAC parameters like error, confidence, iterations, and PnP method in a C++ application.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\n./cpp-tutorial-pnp_detection --error=0.25 --confidence=0.90 --iterations=250 --method=3\n```\n\n----------------------------------------\n\nTITLE: Utilizing cv Namespace in OpenCV C++\nDESCRIPTION: This snippet demonstrates accessing OpenCV classes and functions using the `cv` namespace in C++ code. Dependencies include the `opencv2/core.hpp` header. It shows how to define and use the `cv::Mat` class and functions by importing the namespace with `using namespace cv;` or specifying it explicitly with `cv::`. Key parameters include the `cv::RANSAC` parameter in `cv::findHomography`, used for robust homography estimation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/doc/intro.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"opencv2/core.hpp\"\n...\ncv::Mat H = cv::findHomography(points1, points2, cv::RANSAC, 5);\n...\n```\n\nLANGUAGE: cpp\nCODE:\n```\n    #include \"opencv2/core.hpp\"\n    using namespace cv;\n    ...\n    Mat H = findHomography(points1, points2, RANSAC, 5 );\n    ...\n```\n\nLANGUAGE: cpp\nCODE:\n```\n    Mat a(100, 100, CV_32F);\n    randu(a, Scalar::all(1), Scalar::all(std::rand()));\n    cv::log(a, a);\n    a /= std::log(2.);\n```\n\n----------------------------------------\n\nTITLE: Estimating Homography using OpenCV's findHomography - C++\nDESCRIPTION: This C++ code sample uses OpenCV’s findHomography to calculate the transformation matrix that maps corresponding points between two images. Dependencies include OpenCV C++ with matched point sets as input, resulting in a 3x3 homography matrix. The snippet is essential for geometric transforms in later steps.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet perspective_correction.cpp estimate-homography\n```\n\n----------------------------------------\n\nTITLE: Gradient Structure Tensor Calculation in C++\nDESCRIPTION: The code implements anisotropic image segmentation using gradient structure tensor in C++. Dependencies include OpenCV library. The main parts are calculating orientation and coherency, thresholding orientation and coherency, and combining the results. Inputs are the image data and window size, and the output is segmented image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/anisotropic_image_segmentation/anisotropic_image_segmentation.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n@add_toggle_cpp\n    @include cpp/tutorial_code/ImgProc/anisotropic_image_segmentation/anisotropic_image_segmentation.cpp\n@end_toggle\n```\n\nLANGUAGE: C++\nCODE:\n```\n@add_toggle_cpp\n    @snippet samples/cpp/tutorial_code/ImgProc/anisotropic_image_segmentation/anisotropic_image_segmentation.cpp main\n@end_toggle\n```\n\nLANGUAGE: C++\nCODE:\n```\n@add_toggle_cpp\n    @snippet samples/cpp/tutorial_code/ImgProc/anisotropic_image_segmentation/anisotropic_image_segmentation.cpp calcGST\n@end_toggle\n```\n\nLANGUAGE: C++\nCODE:\n```\n@add_toggle_cpp\n    @snippet samples/cpp/tutorial_code/ImgProc/anisotropic_image_segmentation/anisotropic_image_segmentation.cpp thresholding\n@end_toggle\n```\n\nLANGUAGE: C++\nCODE:\n```\n@add_toggle_cpp\n    @snippet samples/cpp/tutorial_code/ImgProc/anisotropic_image_segmentation/anisotropic_image_segmentation.cpp combining\n@end_toggle\n```\n\n----------------------------------------\n\nTITLE: Translating Images in OpenCV Python\nDESCRIPTION: Shows how to shift an image's position using a translation matrix and cv.warpAffine(). This example shifts the image by 100 pixels horizontally and 50 pixels vertically, creating the transformation matrix using numpy.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nimg = cv.imread('messi5.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nrows,cols = img.shape\n\nM = np.float32([[1,0,100],[0,1,50]])\ndst = cv.warpAffine(img,M,(cols,rows))\n\ncv.imshow('img',dst)\ncv.waitKey(0)\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Implementing SURF Feature Detection and Homography Matching in Python\nDESCRIPTION: This Python code demonstrates how to use SURF features and FLANN matching to detect a known object in an image. It uses cv2.findHomography to estimate the transformation and cv2.perspectiveTransform to map points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_homography/feature_homography.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\\nimport cv2 as cv\\nimport matplotlib.pyplot as plt\\n\\n\\nMIN_MATCH_COUNT = 10\\n\\nimg1 = cv.imread(cv.samples.findFile('box.png'), cv.IMREAD_GRAYSCALE)  # queryImage\\nimg2 = cv.imread(cv.samples.findFile('box_in_scene.png'), cv.IMREAD_GRAYSCALE)  # trainImage\\n\\n# Initiate SIFT detector\\nsift = cv.SIFT_create()\\n\\n# find the keypoints and descriptors with SIFT\\nkp1, des1 = sift.detectAndCompute(img1,None)\\nkp2, des2 = sift.detectAndCompute(img2,None)\\n\\nFLANN_INDEX_KDTREE = 1\\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\\nsearch_params = dict(checks = 50)\\n\\nflann = cv.FlannBasedMatcher(index_params, search_params)\\n\\nmatches = flann.knnMatch(des1,des2,k=2)\\n\\n# store all the good matches as per Lowe's ratio test.\\ngood = []\\nfor m,n in matches:\\n    if m.distance < 0.7*n.distance:\\n        good.append(m)\\n\\nif len(good)>MIN_MATCH_COUNT:\\n    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\\n    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\\n\\n    M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0)\\n    matchesMask = mask.ravel().tolist()\\n\\n    h,w = img1.shape\\n    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\\n    dst = cv.perspectiveTransform(pts,M)\\n\\n    img2 = cv.polylines(img2,[np.int32(dst)],True,255,3, cv.LINE_AA)\\n\\nelse:\\n    print( \\\"Not enough matches are found - {}/{}\\\"\\\\\\n        .format(len(good), MIN_MATCH_COUNT) )\\n    matchesMask = None\\n\\ndraw_params = dict(matchColor = (0,255,0), # draw matches in green color\\n                   singlePointColor = None,\\n                   matchesMask = matchesMask, # draw only inliers\\n                   flags = 2)\\n\\nimg3 = cv.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)\\n\\nplt.imshow(img3, 'gray'),plt.show()\\n\n```\n\n----------------------------------------\n\nTITLE: Detecting Keypoints with AKAZE in OpenCV using Java\nDESCRIPTION: Java snippet utilizing OpenCV to create an AKAZE object, detect keypoints, and compute the corresponding descriptors for the images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nsamples/java/tutorial_code/features2D/akaze_matching/AKAZEMatchDemo.java AKAZE\n```\n\n----------------------------------------\n\nTITLE: Selecting Best Match Location Based on Method (Java)\nDESCRIPTION: Selects the best match location depending on the chosen `match_method`. If the method is `TM_SQDIFF` or `TM_SQDIFF_NORMED`, the location corresponding to the minimum value (`mmr.minLoc`) is chosen. Otherwise, the location of the maximum value (`mmr.maxLoc`) is used.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_32\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/tutorial_template_matching/MatchTemplateDemo.java match_loc\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenCV Matrices with C++11 Initializer Lists\nDESCRIPTION: Creates matrices using C++11 initializer lists, which provides a more modern and concise syntax for matrix initialization. Requires C++11 support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n// for small matrices, it's also possible to use initializer list (C++11)\nMat C_ = (Mat_<double>({0, -1, 0, -1, 5, -1, 0, -1, 0})).reshape(3);\ncout << \"C_ = \" << endl << \" \" << C_ << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Drawing Circle with OpenCV in Python\nDESCRIPTION: Draws a filled red circle inside the previously drawn rectangle using the cv.circle() function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_drawing_functions/py_drawing_functions.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ncv.circle(img,(447,63), 63, (0,0,255), -1)\n```\n\n----------------------------------------\n\nTITLE: Running the SBT Project (Bash)\nDESCRIPTION: This Bash command uses SBT to compile and run the Java application defined in the project. `sbt run` compiles the source code (if necessary) and then executes the main class found in the project (in this case, `HelloOpenCV`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nsbt run\n```\n\n----------------------------------------\n\nTITLE: Configuring Objective-C Bindings and Build Targets - CMake\nDESCRIPTION: This CMake script configures the build system for Objective-C bindings in the OpenCV library. It conditionally adds the 'generator' subdirectory for code/documentation generation when 'OPENCV_INITIAL_PASS' is set, and exits early if not building an Apple framework. It sets the module description, registers the Objective-C bindings module with relevant dependencies, and defines a custom build target for the Objective-C framework, ensuring dependency order with 'gen_opencv_objc_source'. Dependencies include the Objective-C environment, OpenCV core components, and proper CMake setup. Inputs primarily are CMake variables and OpenCV build infrastructure, and outputs are configured build targets and modules. The code assumes a compatible CMake and Apple build setup; usage in other contexts may be limited.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/objc/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(OPENCV_INITIAL_PASS)\n  # generator for Objective-C source code and documentation signatures\n  add_subdirectory(generator)\nendif()\n\nif(NOT APPLE_FRAMEWORK)\n  return()\nendif()\n\nset(the_description \"The Objective-C bindings\")\nocv_add_module(objc BINDINGS opencv_core opencv_imgproc PRIVATE_REQUIRED opencv_objc_bindings_generator)\n\nadd_custom_target(${the_module}\n    ALL\n    COMMENT \"Objective-C framework\"\n)\nadd_dependencies(${the_module} gen_opencv_objc_source)\n\n#include(${CMAKE_CURRENT_SOURCE_DIR}/common.cmake)\n```\n\n----------------------------------------\n\nTITLE: Displaying Status Text Overlays on Images using OpenCV putText in C++\nDESCRIPTION: This C++ snippet reference indicates code that utilizes the OpenCV `cv::putText` function to overlay informational text onto the output image, providing feedback to the user about the current state of the camera calibration application.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/calib3d/camera_calibration/camera_calibration.cpp output_text\n```\n\n----------------------------------------\n\nTITLE: Running the Compiled OpenCV Executable from the Command Line in Bash\nDESCRIPTION: This sequence of Bash commands demonstrates how to run the compiled `DisplayImage` executable from the terminal. It changes the directory to the project's root, then to the source folder (`cd src`) containing the executable, and finally executes the program (`./DisplayImage`), passing the relative path to an image file (`../images/HappyLittleFish.png`) as a command-line argument.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_eclipse/linux_eclipse.markdown#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd <DisplayImage_directory>\ncd src\n./DisplayImage ../images/HappyLittleFish.png\n```\n\n----------------------------------------\n\nTITLE: Treating Compiler Warnings as Errors in OpenCV Builds (CMake)\nDESCRIPTION: With this CMake setting, all compiler warnings will be treated as errors, causing the build to halt on any warning. Use -DOPENCV_WARNINGS_ARE_ERRORS=ON to activate. This option has no prerequisites but is intended for environments where code quality is strictly enforced.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_26\n\nLANGUAGE: cmake\nCODE:\n```\nOPENCV_WARNINGS_ARE_ERRORS\n```\n\n----------------------------------------\n\nTITLE: Standard Maven Build Command for OpenCV\nDESCRIPTION: Basic Maven command to clean the project directory and install OpenCV packages. Used on all architectures to initiate the build process after setting up required environment variables.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/maven/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmvn clean install\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Decision Tree Implementation in C++\nDESCRIPTION: Implementation of the FAST corner detection algorithm using a series of nested conditionals to check pixel values around a candidate point. The code compares pixel values at different offsets against threshold values (cb and c_b) to determine if a point is a corner or not, using goto statements for early termination.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset6] < c_b)\n    if(ptr[offset2] < c_b)\n      if(ptr[offset7] < c_b)\n        if(ptr[offset8] < c_b)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset6] > cb)\n    if(ptr[offset7] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset10] > cb)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    if(ptr[offset6] < c_b)\n      if(ptr[offset2] < c_b)\n        if(ptr[offset7] < c_b)\n          if(ptr[offset1] < c_b)\n            goto is_a_corner;\n          else\n            if(ptr[offset8] < c_b)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  if(ptr[offset2] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset1] < c_b)\n        if(ptr[offset6] < c_b)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset8] < c_b)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset11] > cb)\n    if(ptr[offset8] > cb)\n      if(ptr[offset10] > cb)\n        if(ptr[offset1] > cb)\n          if(ptr[offset2] > cb)\n            goto is_a_corner;\n          else\n            if(ptr[offset7] > cb)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset6] > cb)\n            if(ptr[offset7] > cb)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset11] > cb)\n    if(ptr[offset10] > cb)\n      if(ptr[offset3] > cb)\n        if(ptr[offset1] > cb)\n          if(ptr[offset2] > cb)\n            goto is_a_corner;\n          else\n            if(ptr[offset7] > cb)\n              if(ptr[offset8] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset6] > cb)\n            if(ptr[offset7] > cb)\n              if(ptr[offset8] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset8] > cb)\n          if(ptr[offset1] > cb)\n            if(ptr[offset2] > cb)\n              goto is_a_corner;\n            else\n              if(ptr[offset7] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset7] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset9] < c_b)\n    if(ptr[offset2] > cb)\n      if(ptr[offset1] > cb)\n        if(ptr[offset4] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset11] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset7] < c_b)\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset11] < c_b)\n                    if(ptr[offset10] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset6] < c_b)\n            if(ptr[offset7] < c_b)\n              if(ptr[offset8] < c_b)\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset4] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset3] < c_b)\n                    if(ptr[offset4] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset7] < c_b)\n            if(ptr[offset8] < c_b)\n              if(ptr[offset4] < c_b)\n                if(ptr[offset3] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      if(ptr[offset6] < c_b)\n        if(ptr[offset7] < c_b)\n```\n\n----------------------------------------\n\nTITLE: Defining an HTML Canvas for Output (HTML)\nDESCRIPTION: Shows the HTML code required to add a `<canvas>` element to the web page. This canvas, identified by `id=\"outputCanvas\"`, serves as the target for displaying image data processed by OpenCV.js using functions like `cv.imshow()`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_usage/js_usage.markdown#2025-04-22_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<canvas id=\"outputCanvas\"></canvas>\n```\n\n----------------------------------------\n\nTITLE: Command Line Parameters for ArUco Board Detection in C++\nDESCRIPTION: Defines the command line parameters for the ArUco board detection sample. These parameters allow specifying board dimensions, input sources, and camera calibration data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_board_detection/aruco_board_detection.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nconst char* keys  =\n        \"{w        |       | Number of squares in X direction }\"\n        \"{h        |       | Number of squares in Y direction }\"\n        \"{l        |       | Marker side length (in pixels) }\"\n        \"{s        |       | Separation between two consecutive markers in the grid (in pixels)}\"\n        \"{d        |       | dictionary: DICT_4X4_50=0, DICT_4X4_100=1, DICT_4X4_250=2,\"\n        \"  DICT_4X4_1000=3, DICT_5X5_50=4, DICT_5X5_100=5, DICT_5X5_250=6, DICT_5X5_1000=7, \"\n        \"  DICT_6X6_50=8, DICT_6X6_100=9, DICT_6X6_250=10, DICT_6X6_1000=11, DICT_7X7_50=12,\"\n        \"  DICT_7X7_100=13, DICT_7X7_250=14, DICT_7X7_1000=15, DICT_ARUCO_ORIGINAL = 16}\"\n        \"{cd       |       | File of custom detector parameters}\"\n        \"{c        |       | Output file with calibrated camera parameters }\"\n        \"{v        |       | Input from video file, if empty input comes from camera }\"\n        \"{ci       | 0     | Camera id if input doesnt come from video (-v) }\"\n        \"{dp       |       | File of marker detector parameters }\"\n        \"{rs       | false | Apply refind strategy }\"\n        \"{r        |       | show rejected candidates too }\"\n        \"{refine   |       | Corner refinement: CORNER_REFINE_NONE=0, CORNER_REFINE_SUBPIX=1,\"\n        \"CORNER_REFINE_CONTOUR=2, CORNER_REFINE_APRILTAG=3}\"\n        \"{} \";\n```\n\n----------------------------------------\n\nTITLE: Initializing SVM Parameters\nDESCRIPTION: Sets up SVM configuration including kernel type, SVM type and termination criteria.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_svm.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nPtr<SVM> svm = SVM::create();\nsvm->setType(SVM::C_SVC);\nsvm->setKernel(SVM::LINEAR);\nsvm->setTermCriteria(TermCriteria(TermCriteria::MAX_ITER, 100, 1e-6));\n```\n\n----------------------------------------\n\nTITLE: Input Image Preprocessing with blobFromImage - C++\nDESCRIPTION: This C++ code performs image resizing and pixel normalization using OpenCV's functions. The frame is optionally resized, converted into a 4D blob, and divided by per-channel standard deviation. This ensures that model inputs match the normalization and preprocessing pipeline used during PyTorch training and ONNX export. Required parameters include frame, desired dimension, mean, std, and scale values for correct normalization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nif (rszWidth != 0 && rszHeight != 0)\\n{\\n    resize(frame, frame, Size(rszWidth, rszHeight));\\n}\\n\\n// Create a 4D blob from a frame\\nblobFromImage(frame, blob, scale, Size(inpWidth, inpHeight), mean, swapRB, crop);\\n\\n// Check std values.\\nif (std.val[0] != 0.0 && std.val[1] != 0.0 && std.val[2] != 0.0)\\n{\\n    // Divide blob by std.\\n    divide(blob, std, blob);\\n}\n```\n\n----------------------------------------\n\nTITLE: Library Target and Dependency Linking for JNI - CMake\nDESCRIPTION: This segment adds include directories, defines the JNI shared library, and links it to OpenCV, and mandatory Android system libraries (GLESv2, EGL, log). It conditionally includes and links OpenCL if found, or sets up custom OpenCL paths if provided. Inputs: discovered or user-specified paths; Outputs: properly linked, configured JNI library that can use OpenCV, Android, and optionally OpenCL APIs. Constraints: OpenCL support is enabled based on availability, and extra defines are added as needed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-4-opencl/jni/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(\"${CMAKE_CURRENT_LIST_DIR}\")\nadd_library(${target} SHARED ${srcs} ${hdrs})\n\ntarget_link_libraries(${target} ${ANDROID_OPENCV_COMPONENTS} -lGLESv2 -lEGL -llog)\n\nif(OpenCL_FOUND)\n  include_directories(${OpenCL_INCLUDE_DIRS})\n  target_link_libraries(${target} ${OpenCL_LIBRARIES})\n  add_definitions(\"-DOPENCL_FOUND\")\nelif(NOT (\"${ANDROID_OPENCL_SDK}\" STREQUAL \"\"))\n  include_directories(${ANDROID_OPENCL_SDK}/include)\n  link_directories(${ANDROID_OPENCL_SDK}/lib)\n  target_link_directories(${target} PRIVATE ${ANDROID_OPENCL_SDK}/lib)\n\n  set_target_properties(${target} PROPERTIES LINK_FLAGS \"-Wl,--allow-shlib-undefined\")\n  target_link_libraries(${target} -lOpenCL)\n\n  add_definitions(\"-DOPENCL_FOUND\")\n  add_definitions(\"-DCL_HPP_MINIMUM_OPENCL_VERSION=120\")\n  add_definitions(\"-DCL_HPP_TARGET_OPENCL_VERSION=120\")\n  add_definitions(\"-DCL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY\")\nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Decision Tree in C++\nDESCRIPTION: This code snippet is part of the FAST corner detection algorithm that uses a decision tree to determine if a pixel is a corner based on intensity comparisons at specific offsets. The algorithm compares pixel values (ptr[offset]) against threshold values (c_b and cb) and branches to either 'is_a_corner' or 'is_not_a_corner' labels based on these comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_27\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset6] < c_b)\n    goto is_not_a_corner;\n  else\n    if(ptr[offset6] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset11] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\nif(ptr[offset0] < c_b)\n  if(ptr[offset5] < c_b)\n    if(ptr[offset9] > cb)\n      if(ptr[offset2] > cb)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset2] < c_b)\n          if(ptr[offset7] > cb)\n            if(ptr[offset1] > cb)\n              goto is_not_a_corner;\n            else\n              if(ptr[offset1] < c_b)\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset3] < c_b)\n                    if(ptr[offset4] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset3] < c_b)\n                      if(ptr[offset4] < c_b)\n                        if(ptr[offset11] < c_b)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    if(ptr[offset3] < c_b)\n                      if(ptr[offset4] < c_b)\n                        if(ptr[offset11] < c_b)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset7] < c_b)\n              if(ptr[offset1] > cb)\n                if(ptr[offset6] > cb)\n                  goto is_not_a_corner;\n                else\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset3] < c_b)\n                      if(ptr[offset4] < c_b)\n                        if(ptr[offset8] < c_b)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset1] < c_b)\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset3] < c_b)\n                      if(ptr[offset4] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    if(ptr[offset6] > cb)\n                      if(ptr[offset3] < c_b)\n                        if(ptr[offset4] < c_b)\n                          if(ptr[offset11] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      if(ptr[offset3] < c_b)\n                        if(ptr[offset4] < c_b)\n                          if(ptr[offset11] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                else\n                  if(ptr[offset6] > cb)\n                    goto is_not_a_corner;\n                  else\n                    if(ptr[offset6] < c_b)\n                      if(ptr[offset3] < c_b)\n                        if(ptr[offset4] < c_b)\n                          if(ptr[offset8] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n            else\n              if(ptr[offset1] > cb)\n                goto is_not_a_corner;\n              else\n                if(ptr[offset1] < c_b)\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset3] < c_b)\n                      if(ptr[offset4] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    if(ptr[offset6] > cb)\n                      if(ptr[offset3] < c_b)\n                        if(ptr[offset4] < c_b)\n                          if(ptr[offset11] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      if(ptr[offset3] < c_b)\n                        if(ptr[offset4] < c_b)\n                          if(ptr[offset11] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      if(ptr[offset9] < c_b)\n        if(ptr[offset7] > cb)\n          if(ptr[offset2] > cb)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset2] < c_b)\n              if(ptr[offset1] > cb)\n                goto is_not_a_corner;\n              else\n                if(ptr[offset1] < c_b)\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset11] < c_b)\n                      if(ptr[offset3] < c_b)\n                        if(ptr[offset4] < c_b)\n                          goto is_a_corner;\n                        else\n                          if(ptr[offset10] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset8] < c_b)\n                          if(ptr[offset10] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    if(ptr[offset6] < c_b)\n                      if(ptr[offset3] < c_b)\n                        if(ptr[offset4] < c_b)\n                          goto is_a_corner;\n                        else\n                          if(ptr[offset10] < c_b)\n                            if(ptr[offset11] < c_b)\n```\n\n----------------------------------------\n\nTITLE: Executing Pixel Intensity Comparisons for Feature Detection in C++\nDESCRIPTION: This C++ code snippet implements a highly optimized decision tree for analyzing pixel neighborhoods in image processing, likely for feature detection (e.g., FAST corners). It uses deeply nested `if-else` statements and `goto` jumps to efficiently compare pixel intensity values (`ptr[offsetN]`) against upper (`cb`) and lower (`c_b`) thresholds. Based on these comparisons across multiple neighboring pixels (defined by `offsetN`), the code classifies the central pixel's region and jumps to specific labels (`success_homogeneous`, `homogeneous`, `success_structured`, `structured`) indicating the result. The use of `goto` aims to maximize performance in this low-level computation. Dependencies include the `ptr` (pointer to image data) and the threshold values `cb` and `c_b`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n                                      goto success_homogeneous;\n                                    else\n                                      if(ptr[offset7] > cb)\n                                        goto success_homogeneous;\n                                      else\n                                        goto homogeneous;\n                                  else\n                                    if(ptr[offset6] > cb)\n                                      if(ptr[offset7] > cb)\n                                        goto success_homogeneous;\n                                      else\n                                        goto homogeneous;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  goto homogeneous;\n                            else\n                              goto homogeneous;\n                          else\n                            goto homogeneous;\n                      else\n                        if(ptr[offset9] < c_b)\n                          if(ptr[offset2] > cb)\n                            if(ptr[offset1] > cb)\n                              if(ptr[offset4] > cb)\n                                if(ptr[offset10] > cb)\n                                  if(ptr[offset3] > cb)\n                                    if(ptr[offset11] > cb)\n                                      goto success_structured;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  if(ptr[offset6] < c_b)\n                                    if(ptr[offset7] < c_b)\n                                      if(ptr[offset8] < c_b)\n                                        if(ptr[offset11] < c_b)\n                                          if(ptr[offset10] < c_b)\n                                            goto success_structured;\n                                          else\n                                            goto structured;\n                                        else\n                                          goto structured;\n                                      else\n                                        goto homogeneous;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                              else\n                                if(ptr[offset6] < c_b)\n                                  if(ptr[offset7] < c_b)\n                                    if(ptr[offset8] < c_b)\n                                      if(ptr[offset10] < c_b)\n                                        if(ptr[offset4] < c_b)\n                                          goto success_structured;\n                                        else\n                                          if(ptr[offset11] < c_b)\n                                            goto success_structured;\n                                          else\n                                            goto structured;\n                                      else\n                                        if(ptr[offset3] < c_b)\n                                          if(ptr[offset4] < c_b)\n                                            goto success_structured;\n                                          else\n                                            goto structured;\n                                        else\n                                          goto homogeneous;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                            else\n                              if(ptr[offset6] < c_b)\n                                if(ptr[offset7] < c_b)\n                                  if(ptr[offset8] < c_b)\n                                    if(ptr[offset4] < c_b)\n                                      if(ptr[offset3] < c_b)\n                                        goto success_structured;\n                                      else\n                                        if(ptr[offset10] < c_b)\n                                          goto success_structured;\n                                        else\n                                          goto homogeneous;\n                                    else\n                                      if(ptr[offset10] < c_b)\n                                        if(ptr[offset11] < c_b)\n                                          goto success_structured;\n                                        else\n                                          goto homogeneous;\n                                      else\n                                        goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            if(ptr[offset6] < c_b)\n                              if(ptr[offset7] < c_b)\n                                if(ptr[offset8] < c_b)\n                                  if(ptr[offset4] < c_b)\n                                    if(ptr[offset3] < c_b)\n                                      goto success_homogeneous;\n                                    else\n                                      if(ptr[offset10] < c_b)\n                                        goto success_homogeneous;\n                                      else\n                                        goto homogeneous;\n                                  else\n                                    if(ptr[offset10] < c_b)\n                                      if(ptr[offset11] < c_b)\n                                        goto success_homogeneous;\n                                      else\n                                        goto homogeneous;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  if(ptr[offset2] < c_b)\n                                    if(ptr[offset1] < c_b)\n                                      if(ptr[offset3] < c_b)\n                                        if(ptr[offset4] < c_b)\n                                          goto success_structured;\n                                        else\n                                          goto homogeneous;\n                                      else\n                                        goto homogeneous;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          if(ptr[offset2] > cb)\n                            if(ptr[offset1] > cb)\n                              if(ptr[offset3] > cb)\n                                if(ptr[offset4] > cb)\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      goto success_homogeneous;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                          else\n                            if(ptr[offset2] < c_b)\n                              if(ptr[offset3] < c_b)\n                                if(ptr[offset4] < c_b)\n                                  if(ptr[offset7] < c_b)\n                                    if(ptr[offset1] < c_b)\n                                      if(ptr[offset6] < c_b)\n                                        goto success_homogeneous;\n                                      else\n                                        goto homogeneous;\n                                    else\n                                      if(ptr[offset6] < c_b)\n                                        if(ptr[offset8] < c_b)\n                                          goto success_homogeneous;\n                                        else\n                                          goto homogeneous;\n                                      else\n                                        goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                    else\n                      if(ptr[offset2] > cb)\n                        if(ptr[offset10] > cb)\n                          if(ptr[offset11] > cb)\n                            if(ptr[offset9] > cb)\n                              if(ptr[offset1] > cb)\n                                if(ptr[offset3] > cb)\n                                  goto success_homogeneous;\n                                else\n                                  if(ptr[offset8] > cb)\n\n```\n\n----------------------------------------\n\nTITLE: Running DNN Model Runner in Evaluation Mode\nDESCRIPTION: Command to run the model runner in evaluation mode to compare PyTorch and OpenCV DNN performance metrics.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_8\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name <pytorch_cls_model_name>\n```\n\n----------------------------------------\n\nTITLE: Using a Helper Function for Random Color Generation in OpenCV C++\nDESCRIPTION: Shows the usage of a helper function `randomColor` within the `cv::line` function call. This function takes the cv::RNG object as input and returns a random cv::Scalar value, which is then used as the color parameter for the line.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/random_generator_and_text/random_generator_and_text.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n@code{.cpp}\nrandomColor(rng)\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Defining a Color Tuple in Python\nDESCRIPTION: Creating a tuple to represent a BGR color in OpenCV Python. The tuple contains three values representing Blue, Green, and Red channels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n( a, b, c )\n```\n\n----------------------------------------\n\nTITLE: Checking Out Latest OpenCV 2.4 Branch using Git (Shell)\nDESCRIPTION: Switches the current Git repository to the `2.4` branch. This command retrieves the latest development code available in the OpenCV 2.4 series, as an alternative to checking out a specific tagged release.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\n$ git checkout 2.4\n```\n\n----------------------------------------\n\nTITLE: Interactive Image Pyramid Loop in Java\nDESCRIPTION: Runs an infinite loop processing key events. The 'i' key calls `Imgproc.pyrUp` for upsampling, 'o' calls `Imgproc.pyrDown` for downsampling, and ESC terminates the program. The resulting image is redisplayed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n        //![loop]\n        Mat tmp = src;\n        Mat dst = tmp;\n\n        while(true) {\n            HighGui.imshow( window_name, dst );\n            char c = (char) HighGui.waitKey(0);\n\n            if( c == 27 ) {\n                break;\n            } else if( c == 'i' ) {\n                Imgproc.pyrUp( tmp, dst, new Size( tmp.cols()*2, tmp.rows()*2 ) );\n                System.out.println(\"** Zoom In: Image x 2\");\n            } else if( c == 'o' ) {\n                Imgproc.pyrDown( tmp, dst, new Size( tmp.cols()/2, tmp.rows()/2 ) );\n                System.out.println(\"** Zoom Out: Image / 2\");\n            }\n\n            tmp = dst;\n        }\n        //![loop]\n\n        System.exit(0);\n    }\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV.js Documentation - Bash\nDESCRIPTION: Runs the build_js.py script with the --build_doc option to generate documentation using Doxygen. Dependency: emcmake, Python, and Doxygen installed. Parameter: --build_doc. Output is generated documentation files, typically in HTML format, located in the build output directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nemcmake python ./opencv/platforms/js/build_js.py build_js --build_doc\n```\n\n----------------------------------------\n\nTITLE: Applying Filter Using filter2D\nDESCRIPTION: Applies the custom kernel to the source image using OpenCV's filter2D function. Parameters include source/destination images, depth, kernel, and anchor point.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/filter_2d/filter_2d.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nfilter2D(src, dst, -1, kernel, Point(-1,-1), 0, BORDER_DEFAULT);\n```\n\n----------------------------------------\n\nTITLE: Citing a Publication in OpenCV Documentation\nDESCRIPTION: Demonstrates how to cite a publication in OpenCV documentation using the _cite_ command. This snippet shows the syntax for referencing a previously added BibTeX record.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_18\n\nLANGUAGE: markdown\nCODE:\n```\n@cite Bradski98\n```\n\n----------------------------------------\n\nTITLE: Calculating Convex Hull of Contours in Python with OpenCV\nDESCRIPTION: This code snippet shows how to compute the convex hull of a contour using cv.convexHull(). It corrects convexity defects in the contour.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nhull = cv.convexHull(cnt)\n```\n\n----------------------------------------\n\nTITLE: Check ptrdiff_t Support and Configure ptrdiff_t Alternative in CMake\nDESCRIPTION: This segment verifies the compiler support for ptrdiff_t and sets an alternative if unsupported. It determines the size of void pointers and matches them with corresponding integer types to ensure scalability.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# check for ptrdiff_t support\n#\ncheck_c_source_compiles(\n    \"#include <stddef.h>\n     int main() {\n         ptrdiff_t *a;\n         (void)a;\n         return 0;\n    }\"\n    HAVE_PTRDIFF_T\n)\nif(NOT HAVE_PTRDIFF_T)\n    set(NEED_PTRDIFF_T 1)\n\n    check_type_size(\"void *\" SIZEOF_DATA_PTR)\n    message(STATUS \"sizeof(void *) is ${SIZEOF_DATA_PTR} bytes\")\n\n    if(${SIZEOF_DATA_PTR} MATCHES \"4\")\n        set(PTRDIFF_TYPE \"uint32_t\")\n    elseif(${SIZEOF_DATA_PTR} MATCHES \"8\")\n        set(PTRDIFF_TYPE \"uint64_t\")\n    else()\n        message(FATAL_ERROR \"sizeof(void *) is neither 32 nor 64 bit\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Histogram Value Range in C++\nDESCRIPTION: C++ snippet defining the range of pixel values for the histogram calculation. An array `histRange` is initialized to `[0, 256]`, indicating that pixel values from 0 up to (but not including) 256 will be considered.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/calcHist_Demo.cpp Set the ranges ( for B,G,R) )\n```\n\n----------------------------------------\n\nTITLE: Displaying Result Image in Python\nDESCRIPTION: Displays the final absolute Laplacian image (`abs_dst`) in a window titled \"Laplace Demo\" using cv2.imshow. Requires cv2.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n#! [display]\n# [display]\n# Showacilimages\ncv.imshow( \"Laplace Demo\", abs_dst )\n# [display]\n# ! [display]\n```\n\n----------------------------------------\n\nTITLE: Installing Core Sample CMake Files (OpenCV Build) in CMake\nDESCRIPTION: Calls the `ocv_install_example_src` function (defined earlier for the OpenCV build context) to install the current `CMakeLists.txt` and `samples_utils.cmake` into the root of the samples source installation path (`${OPENCV_SAMPLES_SRC_INSTALL_PATH}`). This is done when building as part of OpenCV and if `INSTALL_C_EXAMPLES` is enabled.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nocv_install_example_src(\".\" CMakeLists.txt samples_utils.cmake)\n```\n\n----------------------------------------\n\nTITLE: Starting JPEG Decompression in C using libjpeg\nDESCRIPTION: This code calls `jpeg_start_decompress` to initialize the internal state, allocate working memory, and prepare for returning decompressed image data. It should be called after setting any desired decompression parameters in the `cinfo` struct. After this call, output dimensions (`output_width`, `output_height`) and component information (`output_components`) become available in `cinfo`. This corresponds to step 5 in the decompression workflow.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_14\n\nLANGUAGE: C\nCODE:\n```\njpeg_start_decompress(&cinfo);\n```\n\n----------------------------------------\n\nTITLE: Reading Primitive Types with Real and getNode - OpenCV Python\nDESCRIPTION: This Python snippet demonstrates reading basic numeric values from an XML/YAML/JSON file using the getNode() and real() methods of FileStorage. It is used for extracting individual numeric values from the file, assuming the file was written with FileStorage.write(). The file must contain the specified key. Requires opencv-python installed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfs = cv2.FileStorage('test.yml', cv2.FileStorage_READ)\\nnumber = int(fs.getNode('number').real())\\nfs.release()\n```\n\n----------------------------------------\n\nTITLE: Estimating Homography Matrix in OpenCV C++\nDESCRIPTION: Code to estimate the homography matrix between object points and normalized image points using OpenCV's findHomography function with RANSAC for robust estimation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat H = cv::findHomography(objectPointsPlanar, imagePointsNormalized, cv::RANSAC);\n```\n\n----------------------------------------\n\nTITLE: Image Addition with OpenCV.js\nDESCRIPTION: Demonstrates how to add two images using cv.add() function. Both input images must have the same depth and type. The operation performs pixel-wise addition of the images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_image_arithmetics/js_image_arithmetics.markdown#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nlet src1 = cv.imread(\"canvasInput1\");\nlet src2 = cv.imread(\"canvasInput2\");\nlet dst = new cv.Mat();\nlet mask = new cv.Mat();\nlet dtype = -1;\ncv.add(src1, src2, dst, mask, dtype);\nsrc1.delete(); src2.delete(); dst.delete(); mask.delete();\n```\n\n----------------------------------------\n\nTITLE: Accessing Single Pixel Intensity with C++ cv::Mat::at with Point\nDESCRIPTION: Alternative pixel access via cv::Point for single-channel images in C++. at<uchar>(point) uses a cv::Point(x, y) object for coordinates. Requires C++ OpenCV. Intensity output is in range 0-255. Input is a cv::Mat and a cv::Point.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nuchar intensity = img.at<uchar>(cv::Point(x, y));\n```\n\n----------------------------------------\n\nTITLE: Executing Basic Clojure Expressions\nDESCRIPTION: It provides examples of simple Clojure expressions executed within a REPL session, showing foundational usage of arithmetic, string manipulation, and function definition.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_7\n\nLANGUAGE: clojure\nCODE:\n```\nuser=> (+ 41 1)\n42\nuser=> (println \"Hello, OpenCV!\")\nHello, OpenCV!\nnil\nuser=> (defn foo [] (str \"bar\"))\n#'user/foo\nuser=> (foo)\n\"bar\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Native Dependencies and Android Project with OpenCV in CMake\nDESCRIPTION: This CMake script configures native dependencies for an Android project based on whether a fat Java library is being built or not. It utilizes CMake commands like 'set', 'if', and 'add_android_project'. The script also specifies a specific SDK target version and establishes project dependencies. The 'BUILD_FAT_JAVA_LIB' flag determines if 'opencv_java' or 'opencv_features2d' is included as a native dependency.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-2-mixedprocessing/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(sample example-tutorial-2-mixedprocessing)\n\nif(BUILD_FAT_JAVA_LIB)\n  set(native_deps opencv_java)\nelse()\n  set(native_deps opencv_features2d)\nendif()\n\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\" LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\" SDK_TARGET 11 \"${ANDROID_SDK_TARGET}\" NATIVE_DEPS ${native_deps})\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV with CMake for aarch64 and Ninja\nDESCRIPTION: Set up OpenCV configuration using CMake for a cross-compilation targeting a Linux aarch64 system, configured to use the Ninja build system. Requires a specified toolchain file and OpenCV extra modules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nPKG_CONFIG_PATH=/usr/lib/aarch64-linux-gnu/pkgconfig:/usr/share/pkgconfig \\\n    PKG_CONFIG_LIBDIR=/usr/lib/aarch64-linux-gnu \\\n    PKG_CONFIG_SYSROOT_DIR=/ \\\n        cmake -S opencv \\\n              -B build4-full_arm64 \\\n              -DCMAKE_TOOLCHAIN_FILE=/home/kmtr/work/opencv/platforms/linux/aarch64-gnu.toolchain.cmake \\\n              -DOPENCV_EXTRA_MODULES_PATH=opencv_contrib/modules \\\n              -GNinja\n```\n\n----------------------------------------\n\nTITLE: Defining and Linking a Shared Library with OpenCV in CMake\nDESCRIPTION: Uses `file(GLOB ...)` to find all `.cpp`/`.c` source files and `.hpp`/`.h` header files in the current directory. Includes the current directory (`CMAKE_CURRENT_LIST_DIR`) for header lookup. Creates a shared library named `mixed_sample` (from the `target` variable) using these source and header files. Finally, links this library against the OpenCV components specified in `ANDROID_OPENCV_COMPONENTS`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-2-mixedprocessing/jni/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB srcs *.cpp *.c)\nfile(GLOB hdrs *.hpp *.h)\n\ninclude_directories(\"${CMAKE_CURRENT_LIST_DIR}\")\nadd_library(${target} SHARED ${srcs} ${hdrs})\ntarget_link_libraries(${target} ${ANDROID_OPENCV_COMPONENTS})\n```\n\n----------------------------------------\n\nTITLE: Installing JARs in Local Maven Repository Bash\nDESCRIPTION: This Bash command uses the lein-localrepo plugin to install OpenCV JARs in the local Maven repository, making them accessible to Maven-compliant projects. The command specifies the artifact name and version, creating Maven entries for the OpenCV artifacts.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nlein localrepo install opencv-247.jar opencv/opencv 2.4.7\n```\n\nLANGUAGE: bash\nCODE:\n```\nlein localrepo install opencv-native-247.jar opencv/opencv-native 2.4.7\n```\n\n----------------------------------------\n\nTITLE: Generating Random Data on Stream with Thrust\nDESCRIPTION: Shows how to generate random values between -1 and 1 in a GpuMat using a custom thrust execution policy on a specific CUDA stream.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-thrust-interop/gpu_thrust_interop.markdown#2025-04-22_snippet_7\n\nLANGUAGE: CUDA\nCODE:\n```\ncv::cuda::Stream stream;\ncv::cuda::GpuMat d_random(512, 512, CV_32FC1);\nthrust::device_ptr<float> d_random_ptr((float*)d_random.data);\n\n// Generate random values between -1 and 1 on a stream\nthrust::transform(\n    thrust::system::cuda::par.on(StreamAccessor::getStream(stream)),\n    thrust::counting_iterator<int>(0),\n    thrust::counting_iterator<int>(d_random.rows * d_random.cols),\n    d_random_ptr,\n    prg(-1, 1)\n);\n```\n\n----------------------------------------\n\nTITLE: Import ResizeBilinearLayer from TensorFlow OpenCV C++\nDESCRIPTION: Illustrates defining and registering TensorFlow's tf.image.resize_bilinear operation as a custom layer in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/custom_layers.hpp ResizeBilinearLayer\n```\n\n----------------------------------------\n\nTITLE: Conditional ITT Support in OpenCV CMake Configuration\nDESCRIPTION: Adds support for ITT when both CV_TRACE and HAVE_ITT are defined. The snippet also sets the necessary include directories and links libraries for the specified module using CMake macros. It ensures that ITT functionalities are correctly integrated into the build process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nif(CV_TRACE AND HAVE_ITT)\n  ocv_target_compile_definitions(${the_module} PRIVATE -DOPENCV_WITH_ITT=1)\n  ocv_module_include_directories(${ITT_INCLUDE_DIRS})\n  ocv_target_link_libraries(${the_module} PRIVATE ${ITT_LIBRARIES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Batch Conversion of PyTorch Classification Models - Console\nDESCRIPTION: The command line interface allows conversion of any supported PyTorch classification model to ONNX by specifying model name and toggling evaluation. Replace <pytorch_cls_model_name> with target model (e.g., resnet50). All necessary dependencies and Python modules must be installed prior to execution.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_6\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name <pytorch_cls_model_name> --evaluate False\n```\n\n----------------------------------------\n\nTITLE: Calculating Homography Using OpenCV in Java\nDESCRIPTION: This Java snippet shows the computation of the homography matrix needed for image stitching using OpenCV. Required libraries include OpenCV Java bindings. It takes image views and calibration data as input and provides the matrix for homography.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_36\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.Core;\nimport org.opencv.core.Mat;\n\npublic void computeHomography(Mat image1, Mat image2) {\n    // Code to compute homography\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Calculate Histogram using Numpy\nDESCRIPTION: Shows histogram calculation using Numpy's histogram() function as an alternative to OpenCV. Returns both histogram and bin edges.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_begins/py_histogram_begins.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nhist,bins = np.histogram(img.ravel(),256,[0,256])\n```\n\n----------------------------------------\n\nTITLE: Constructing Scalar Structures in OpenCV.js (JavaScript)\nDESCRIPTION: Shows two equivalent methods to create a Scalar value: using the cv.Scalar constructor and an array literal containing R, G, B, and Alpha channel values. Scalar is used for representing color in 4 channels. Inputs must be numeric; OpenCV.js is the primary dependency.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_some_data_structures/js_image_arithmetics.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n// The first way\nlet scalar = new cv.Scalar(R, G, B, Alpha);\n// The second way\nlet scalar = [R, G, B, Alpha];\n```\n\n----------------------------------------\n\nTITLE: Blending Images with OpenCV in Python\nDESCRIPTION: This Python snippet shows how to blend two images using OpenCV's addWeighted() function. Both input images should have the same size and format. A Numpy alternative is also shown. Python bindings for OpenCV are required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/adding_images/adding_images.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n\"@snippet python/tutorial_code/core/AddingImages/adding_images.py load\"\n```\n\nLANGUAGE: Python\nCODE:\n```\n\"@snippet python/tutorial_code/core/AddingImages/adding_images.py blend_images\"\n```\n\nLANGUAGE: Python\nCODE:\n```\n\"@snippet python/tutorial_code/core/AddingImages/adding_images.py display\"\n```\n\n----------------------------------------\n\nTITLE: Setting Highgui Module Description in CMake\nDESCRIPTION: Sets a CMake variable `the_description` to hold the string \"High-level GUI\", which likely serves as a description for the OpenCV highgui module within the build system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(the_description \"High-level GUI\")\n```\n\n----------------------------------------\n\nTITLE: Constructing Matrices from Arrays and ImageData with OpenCV.js - JavaScript\nDESCRIPTION: Demonstrates creating a cv.Mat matrix from a JavaScript array and from HTML5 canvas ImageData. Useful when data is already accessible in JS primitives or extracted from canvas elements. Requires OpenCV.js and, for the ImageData method, access to a canvas and 2D rendering context.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_basic_ops/js_basic_ops.markdown#2025-04-22_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n// 1. Use JS array to construct a mat.\n// For example: let mat = cv.matFromArray(2, 2, cv.CV_8UC1, [1, 2, 3, 4]);\nlet mat = cv.matFromArray(rows, cols, type, array);\n// 2. Use imgData to construct a mat\nlet ctx = canvas.getContext(\"2d\");\nlet imgData = ctx.getImageData(0, 0, canvas.width, canvas.height);\nlet mat = cv.matFromImageData(imgData);\n```\n\n----------------------------------------\n\nTITLE: Including Optional Components in OpenCV CMake\nDESCRIPTION: Conditionally includes CMake files for various optional components like OpenCL, Halide, Vulkan, WebNN, Inference Engine, DirectX, DirectML, VTK, OpenVX, QUIRC, and ONNX based on build options.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nif(WITH_OPENCL)\n  include(cmake/OpenCVDetectOpenCL.cmake)\nendif()\n\nif(WITH_HALIDE)\n  include(cmake/OpenCVDetectHalide.cmake)\nendif()\n\nif(WITH_VULKAN)\n  include(cmake/OpenCVDetectVulkan.cmake)\nendif()\n\nif(WITH_WEBNN)\n  include(cmake/OpenCVDetectWebNN.cmake)\nendif()\n\nif(WITH_INF_ENGINE OR WITH_OPENVINO)\n  include(cmake/OpenCVDetectInferenceEngine.cmake)\nendif()\n\nif(WITH_DIRECTX)\n  include(cmake/OpenCVDetectDirectX.cmake)\nendif()\n\nif(WITH_DIRECTML)\n  include(cmake/OpenCVDetectDirectML.cmake)\nendif()\n\nif(WITH_VTK)\n  include(cmake/OpenCVDetectVTK.cmake)\nendif()\n\nif(WITH_OPENVX)\n  include(cmake/FindOpenVX.cmake)\nendif()\n\nif(WITH_QUIRC)\n  add_subdirectory(3rdparty/quirc)\n  set(HAVE_QUIRC TRUE)\nendif()\n\nif(WITH_ONNX)\n  include(cmake/FindONNX.cmake)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Constructing Circle Structures in OpenCV.js (JavaScript)\nDESCRIPTION: Explains two approaches for defining a Circle: invoking the cv.Circle constructor or using an object literal with center and radius. Center is typically a Point structure, and radius a numeric value. Requires OpenCV.js and appropriate parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_some_data_structures/js_image_arithmetics.markdown#2025-04-22_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n// The first way\nlet circle = new cv.Circle(center, radius);\n// The second way\nlet circle = {center : center, radius : radius};\n```\n\n----------------------------------------\n\nTITLE: Setting Pose Estimation Criteria and Axis Points with OpenCV (Python)\nDESCRIPTION: Initializes the termination criteria for sub-pixel corner refinement, defines the chessboard's 3D object points (assuming Z=0 for planarity), and specifies 3D world coordinates for the axes. Relies on NumPy for array creation and OpenCV for constants. The parameters are used as core inputs for pose estimation and result projection in the main image processing loop.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_pose/py_pose.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncriteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\\nobjp = np.zeros((6*7,3), np.float32)\\nobjp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)\\n\\naxis = np.float32([[3,0,0], [0,3,0], [0,0,-3]]).reshape(-1,3)\n```\n\n----------------------------------------\n\nTITLE: Configuring Framebuffer Backend for HighGUI in CMake\nDESCRIPTION: Sets up the Framebuffer backend for OpenCV HighGUI when WITH_FRAMEBUFFER is enabled. Adds necessary source files, header files, and definitions including optional XVFB support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nif(WITH_FRAMEBUFFER AND HAVE_FRAMEBUFFER)\n  set(OPENCV_HIGHGUI_BUILTIN_BACKEND \"FB\")\n  add_definitions(-DHAVE_FRAMEBUFFER)\n  list(APPEND highgui_srcs ${CMAKE_CURRENT_LIST_DIR}/src/window_framebuffer.cpp)\n  list(APPEND highgui_hdrs ${CMAKE_CURRENT_LIST_DIR}/src/window_framebuffer.hpp)\n  if(HAVE_FRAMEBUFFER_XVFB)\n    add_definitions(-DHAVE_FRAMEBUFFER_XVFB)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Importing OpenCV Classes in REPL\nDESCRIPTION: Demonstrates importing the OpenCV Java classes within a Clojure REPL to replicate a Java example, showing techniques for matrix operations and modifications.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_17\n\nLANGUAGE: clojure\nCODE:\n```\nuser=> (import '[org.opencv.core Mat CvType Scalar])\norg.opencv.core.Scalar\n```\n\n----------------------------------------\n\nTITLE: Transforming Pixel Coordinates to Complex Plane for Mandelbrot - C++\nDESCRIPTION: This helper code translates pixel coordinates (col, row) of the image to the corresponding normalized complex coordinates used for the Mandelbrot set (mapping to [-2, 1] on the real axis, [-1, 1] on the imaginary axis). Accepts pixel indices, image dimensions; outputs corresponding x0, y0. Requires standard C++ math.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_/how_to_use_OpenCV_parallel_for_.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n// Map pixel coordinates to the complex plane\n// x0 = (col / (double)image.cols) * 3.0 - 2.0;\n// y0 = (row / (double)image.rows) * 2.0 - 1.0;\ndouble x0 = (col / (double)image.cols) * 3.0 - 2.0;\ndouble y0 = (row / (double)image.rows) * 2.0 - 1.0;\n\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Conditional Logic in C++\nDESCRIPTION: Implementation of corner detection logic using pointer offsets and multiple conditional branches. The code compares pixel values at different offsets to determine corner characteristics, using goto statements to handle different detection scenarios.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ngoto success_structured;\nelse\n  if(ptr[offset4] > cb)\n    goto success_structured;\n  else\n    goto structured;\nelse\n  if(ptr[offset1] > cb)\n    goto success_structured;\n  else\n    goto structured;\nelse\n  goto structured;\nelse\n  if(ptr[offset5] < c_b)\n    if(ptr[offset3] < c_b)\n      if(ptr[offset7] < c_b)\n        if(ptr[offset4] < c_b)\n          if(ptr[offset6] < c_b)\n            goto success_structured;\n          else\n            goto structured;\n        else\n          goto structured;\n      else\n        goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto structured;\n```\n\n----------------------------------------\n\nTITLE: YOLOv8 Model Execution\nDESCRIPTION: Shell commands for running YOLOv8 model with specific parameters including preprocessing values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_yolo/dnn_yolo.markdown#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ncd opencv_extra/testdata/dnn\npython download_models.py yolov8\ncd ..\nexport OPENCV_TEST_DATA_PATH=$(pwd)\ncd <build directory of OpenCV>\n\n./bin/example_dnn_yolo_detector --model=onnx/models/yolov8n.onnx --yolo=yolov8 --mean=0.0 --scale=0.003921568627 --paddingmode=2 --padvalue=144.0 --thr=0.5 --nms=0.4 --rgb=0\n```\n\n----------------------------------------\n\nTITLE: Segmentation Evaluation Configuration in Python\nDESCRIPTION: Defines the configuration for segmentation model evaluation, including paths to PASCAL VOC dataset directories, validation file, and class definitions. This dataclass specifies the required data structure for segmentation evaluation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass TestSegmConfig:\n    frame_size: int = 500\n    img_root_dir: str = \"./VOC2012\"\n    img_dir: str = os.path.join(img_root_dir, \"JPEGImages/\")\n    img_segm_gt_dir: str = os.path.join(img_root_dir, \"SegmentationClass/\")\n    # reduced val: https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/data/pascal/seg11valid.txt\n    segm_val_file: str = os.path.join(img_root_dir, \"ImageSets/Segmentation/seg11valid.txt\")\n    colour_file_cls: str = os.path.join(img_root_dir, \"ImageSets/Segmentation/pascal-classes.txt\")\n```\n\n----------------------------------------\n\nTITLE: Creating Images in C++\nDESCRIPTION: Initializing blank images for drawing shapes in OpenCV C++. Creates two black images named 'atom_image' and 'rook_image' with specified dimensions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n/// Windows names\nconst char* atom_window = \"Drawing 1: Atom\";\nconst char* rook_window = \"Drawing 2: Rook\";\n\n/// Create black empty images\nMat atom_image = Mat::zeros( w, w, CV_8UC3 );\nMat rook_image = Mat::zeros( w, w, CV_8UC3 );\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Decision Tree Implementation in C++\nDESCRIPTION: Part of the FAST corner detection algorithm showing the nested conditional logic for determining corner pixels. The algorithm compares pixel values against thresholds (c_b and cb) using offsets to neighboring pixels, then jumps to either 'is_a_corner' or 'is_not_a_corner' labels based on the results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_38\n\nLANGUAGE: C++\nCODE:\n```\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  if(ptr[offset3] < c_b)\n                                    if(ptr[offset4] < c_b)\n                                      if(ptr[offset10] < c_b)\n                                        if(ptr[offset11] < c_b)\n                                          goto is_a_corner;\n                                        else\n                                          goto is_not_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                      else\n                        if(ptr[offset9] < c_b)\n                          if(ptr[offset1] > cb)\n                            goto is_not_a_corner;\n                          else\n                            if(ptr[offset1] < c_b)\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset10] < c_b)\n                                  if(ptr[offset11] < c_b)\n                                    if(ptr[offset3] < c_b)\n                                      goto is_a_corner;\n                                    else\n                                      if(ptr[offset8] < c_b)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                if(ptr[offset6] < c_b)\n                                  if(ptr[offset10] < c_b)\n                                    if(ptr[offset11] < c_b)\n                                      if(ptr[offset3] < c_b)\n                                        goto is_a_corner;\n                                      else\n                                        if(ptr[offset8] < c_b)\n                                          goto is_a_corner;\n                                        else\n                                          goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  if(ptr[offset10] < c_b)\n                                    if(ptr[offset11] < c_b)\n                                      if(ptr[offset3] < c_b)\n                                        goto is_a_corner;\n                                      else\n                                        if(ptr[offset8] < c_b)\n                                          goto is_a_corner;\n                                        else\n                                          goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset1] > cb)\n                            goto is_not_a_corner;\n                          else\n                            if(ptr[offset1] < c_b)\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset3] < c_b)\n                                  if(ptr[offset4] < c_b)\n                                    if(ptr[offset10] < c_b)\n                                      if(ptr[offset11] < c_b)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                if(ptr[offset6] < c_b)\n                                  if(ptr[offset3] < c_b)\n                                    if(ptr[offset4] < c_b)\n                                      if(ptr[offset10] < c_b)\n                                        if(ptr[offset11] < c_b)\n                                          goto is_a_corner;\n                                        else\n                                          goto is_not_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  if(ptr[offset3] < c_b)\n                                    if(ptr[offset4] < c_b)\n                                      if(ptr[offset10] < c_b)\n                                        if(ptr[offset11] < c_b)\n                                          goto is_a_corner;\n                                        else\n                                          goto is_not_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                else\n                  if(ptr[offset7] < c_b)\n                    if(ptr[offset9] > cb)\n                      goto is_not_a_corner;\n                    else\n                      if(ptr[offset9] < c_b)\n                        if(ptr[offset1] > cb)\n                          if(ptr[offset6] > cb)\n                            goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] < c_b)\n                              if(ptr[offset8] < c_b)\n                                if(ptr[offset10] < c_b)\n                                  if(ptr[offset11] < c_b)\n                                    goto is_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset1] < c_b)\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset8] < c_b)\n                                if(ptr[offset10] < c_b)\n                                  if(ptr[offset11] < c_b)\n                                    goto is_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] < c_b)\n                                if(ptr[offset8] < c_b)\n                                  if(ptr[offset10] < c_b)\n                                    if(ptr[offset11] < c_b)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                if(ptr[offset8] < c_b)\n                                  if(ptr[offset10] < c_b)\n                                    if(ptr[offset11] < c_b)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] > cb)\n                              goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] < c_b)\n                                if(ptr[offset8] < c_b)\n                                  if(ptr[offset10] < c_b)\n                                    if(ptr[offset11] < c_b)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Refining and Visualizing Detected Calibration Pattern Corners in C++\nDESCRIPTION: This C++ snippet reference points to code that processes a detected calibration pattern. It refines corner positions using `cv::cornerSubPix`, adds valid points to storage vectors, and uses `cv::drawChessboardCorners` (or similar) to visualize the found pattern on the image for user feedback. It also handles input delay for live camera feeds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/calib3d/camera_calibration/camera_calibration.cpp pattern_found\n```\n\n----------------------------------------\n\nTITLE: Defining and Using G-API Kernel Packages in C++\nDESCRIPTION: This snippet shows how to define and use kernel packages in G-API with Fluid backend for anisotropic image segmentation. The cv::GKernelPackage is used to collect and specify kernels for graph execution.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/anisotropic_segmentation/porting_anisotropic_segmentation.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/porting_anisotropic_image_segmentation/porting_anisotropic_image_segmentation_gapi_fluid.cpp kernel_pkg\n```\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/porting_anisotropic_image_segmentation/porting_anisotropic_image_segmentation_gapi_fluid.cpp kernel_pkg_use\n```\n\n----------------------------------------\n\nTITLE: Accessing Single Pixel Intensity Value in Grayscale Image with OpenCV in Java\nDESCRIPTION: This snippet retrieves the intensity of a pixel in an 8-bit single channel Mat in Java OpenCV. The get method accesses an array of pixel values at specified row and column. Ensure correct indices (row first, column second). Result is an array, with intensity typically at index 0.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\ndouble[] intensity = img.get(y, x);\n```\n\n----------------------------------------\n\nTITLE: Accessing Single Pixel in 3-Channel Image with OpenCV in Python\nDESCRIPTION: Retrieves the color values from a 3-channel (BGR) image in Python. NumPy array img[y, x] returns an array with B,G,R values. Each value is typically an int in range 0-255. Input indices specify row and column.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nblue, green, red = img[y, x]\n```\n\n----------------------------------------\n\nTITLE: Calculating Contour Extent in OpenCV Python\nDESCRIPTION: This snippet computes the extent of a contour, which is the ratio of the contour's area to the area of its bounding rectangle. It uses `cv.contourArea` to find the contour area and `cv.boundingRect` to get the width (w) and height (h) for the bounding rectangle area calculation. Requires an existing contour variable `cnt`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_properties/py_contour_properties.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\narea = cv.contourArea(cnt)\nx,y,w,h = cv.boundingRect(cnt)\nrect_area = w*h\nextent = float(area)/rect_area\n```\n\n----------------------------------------\n\nTITLE: Implementing Erosion Operation in C++\nDESCRIPTION: Performs erosion operation on an image using specified kernel parameters\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nvoid Erosion(int, void*)\n{\n    int erosion_type = 0;\n    if (erosion_elem == 0) { erosion_type = MORPH_RECT; }\n    else if (erosion_elem == 1) { erosion_type = MORPH_CROSS; }\n    else if (erosion_elem == 2) { erosion_type = MORPH_ELLIPSE; }\n    Mat element = getStructuringElement(erosion_type,\n        Size(2 * erosion_size + 1, 2 * erosion_size + 1),\n        Point(erosion_size, erosion_size));\n    erode(src, erosion_dst, element);\n    imshow(\"Erosion Demo\", erosion_dst);\n}\n```\n\n----------------------------------------\n\nTITLE: Updating APT Database (Bash)\nDESCRIPTION: Refreshes the local package database after modifying the APT sources list. This command downloads the latest package information from all configured repositories, including the newly added ones for foreign architectures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\n```\n\n----------------------------------------\n\nTITLE: Converting cv::Mat to UIImage in Objective-C\nDESCRIPTION: This function converts an OpenCV Mat object back to a UIImage. It handles both grayscale and color images by checking the number of channels in the Mat.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/image_manipulation/image_manipulation.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Objective-C\nCODE:\n```\n-(UIImage *)UIImageFromCVMat:(cv::Mat)cvMat\n{\n  NSData *data = [NSData dataWithBytes:cvMat.data length:cvMat.elemSize()*cvMat.total()];\n  CGColorSpaceRef colorSpace;\n\n  if (cvMat.elemSize() == 1) {\n      colorSpace = CGColorSpaceCreateDeviceGray();\n  } else {\n      colorSpace = CGColorSpaceCreateDeviceRGB();\n  }\n\n  CGDataProviderRef provider = CGDataProviderCreateWithCFData((__bridge CFDataRef)data);\n\n  // Creating CGImage from cv::Mat\n  CGImageRef imageRef = CGImageCreate(cvMat.cols,                                 //width\n                                     cvMat.rows,                                 //height\n                                     8,                                          //bits per component\n                                     8 * cvMat.elemSize(),                       //bits per pixel\n                                     cvMat.step[0],                            //bytesPerRow\n                                     colorSpace,                                 //colorspace\n                                     kCGImageAlphaNone|kCGBitmapByteOrderDefault,// bitmap info\n                                     provider,                                   //CGDataProviderRef\n                                     NULL,                                       //decode\n                                     false,                                      //should interpolate\n                                     kCGRenderingIntentDefault                   //intent\n                                     );\n\n\n  // Getting UIImage from CGImage\n  UIImage *finalImage = [UIImage imageWithCGImage:imageRef];\n  CGImageRelease(imageRef);\n  CGDataProviderRelease(provider);\n  CGColorSpaceRelease(colorSpace);\n\n  return finalImage;\n }\n```\n\n----------------------------------------\n\nTITLE: Calling Drawing Functions Sequentially in OpenCV C++\nDESCRIPTION: This code demonstrates the main loop structure where various drawing functions are called sequentially. Each function takes the image, window name, and RNG object as input, draws a specific type of shape or text randomly, and returns a status code. The loop checks the return code after each call to potentially exit early.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/random_generator_and_text/random_generator_and_text.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n@code{.cpp}\n/// Now, let's draw some lines\nc = Drawing_Random_Lines(image, window_name, rng);\nif( c != 0 ) return 0;\n\n/// Go on drawing, this time nice rectangles\nc = Drawing_Random_Rectangles(image, window_name, rng);\nif( c != 0 ) return 0;\n\n/// Draw some ellipses\nc = Drawing_Random_Ellipses( image, window_name, rng );\nif( c != 0 ) return 0;\n\n/// Now some polylines\nc = Drawing_Random_Polylines( image, window_name, rng );\nif( c != 0 ) return 0;\n\n/// Draw filled polygons\nc = Drawing_Random_Filled_Polygons( image, window_name, rng );\nif( c != 0 ) return 0;\n\n/// Draw circles\nc = Drawing_Random_Circles( image, window_name, rng );\nif( c != 0 ) return 0;\n\n/// Display text in random positions\nc = Displaying_Random_Text( image, window_name, rng );\nif( c != 0 ) return 0;\n\n/// Displaying the big end!\nc = Displaying_Big_End( image, window_name, rng );\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Displaying Results and Drawing Rectangle (C++)\nDESCRIPTION: Draws a rectangle on the display image (`img_display`) around the best matched area using `cv::rectangle`. The top-left corner is `matchLoc` and the bottom-right corner is calculated based on the template dimensions. Finally, it displays the image with the rectangle and the normalized result matrix using `cv::imshow`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_34\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp imshow\n```\n\n----------------------------------------\n\nTITLE: Installing wget and unzip on Linux\nDESCRIPTION: Command to install wget download tool and unzip utility on Linux using the package manager.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install wget unzip\n```\n\n----------------------------------------\n\nTITLE: Optimized GPU PSNR Function Signature in C++\nDESCRIPTION: Shows the function signature for `getPSNR_GPU_optimized`. It takes two constant `Mat` references (`I1`, `I2`) as input images and a reference to a `BufferPSNR` object (`b`). Passing the buffer by reference allows the function to reuse the pre-allocated GPU memory within the buffer.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\ndouble getPSNR_GPU_optimized(const Mat& I1, const Mat& I2, BufferPSNR& b)\n```\n\n----------------------------------------\n\nTITLE: Computing BRIEF Descriptors with CenSurE in Python\nDESCRIPTION: This Python code snippet uses OpenCV to compute BRIEF descriptors with the help of the CenSurE (STAR) detector. It initializes a FAST detector, detects keypoints, and then computes the BRIEF descriptors. The necessary dependencies are the OpenCV library along with the 'opencv_contrib' module. The input is a grayscale image, and the output includes the keypoints and the descriptor array. The brief.getDescriptorSize() function shows the descriptor size, which is 32 by default.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_brief/py_brief.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('simple.jpg', cv.IMREAD_GRAYSCALE)\n\n# Initiate FAST detector\nstar = cv.xfeatures2d.StarDetector_create()\n\n# Initiate BRIEF extractor\nbrief = cv.xfeatures2d.BriefDescriptorExtractor_create()\n\n# find the keypoints with STAR\nkp = star.detect(img,None)\n\n# compute the descriptors with BRIEF\nkp, des = brief.compute(img, kp)\n\nprint( brief.descriptorSize() )\nprint( des.shape )\n```\n\n----------------------------------------\n\nTITLE: Adjusting SURF Parameters and Recomputing Keypoints in OpenCV Python\nDESCRIPTION: This code snippet shows how to check and modify the Hessian threshold of a SURF object, and then recompute keypoints with the new threshold. It's useful for controlling the number of detected keypoints.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Check present Hessian threshold\n>>> print( surf.getHessianThreshold() )\n400.0\n\n# We set it to some 50000. Remember, it is just for representing in picture.\n# In actual cases, it is better to have a value 300-500\n>>> surf.setHessianThreshold(50000)\n\n# Again compute keypoints and check its number.\n>>> kp, des = surf.detectAndCompute(img,None)\n\n>>> print( len(kp) )\n47\n```\n\n----------------------------------------\n\nTITLE: Basic Depth Map Capture with OpenNI\nDESCRIPTION: Example showing how to capture depth map data from an OpenNI-compatible sensor using VideoCapture.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/kinect_openni.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nVideoCapture capture( CAP_OPENNI );\nfor(;;)\n{\n    Mat depthMap;\n    capture >> depthMap;\n\n    if( waitKey( 30 ) >= 0 )\n        break;\n}\n```\n\n----------------------------------------\n\nTITLE: Collecting Java Binding Source Files in CMake\nDESCRIPTION: Gathers handwritten C++ and header files for Java bindings from multiple locations, including module-specific Java directories.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jni/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nglob_more_specific_sources(H \"${CMAKE_CURRENT_SOURCE_DIR}/../generator/src\" handwritten_h_sources)\nglob_more_specific_sources(CPP \"${CMAKE_CURRENT_SOURCE_DIR}/../generator/src\" handwritten_cpp_sources)\n\n# grab C++ files from misc/java\nforeach(m ${OPENCV_MODULES_BUILD})\n  if (\";${OPENCV_MODULE_${m}_WRAPPERS};\" MATCHES \";java;\" AND HAVE_${m})\n    set(module_java_dir \"${OPENCV_MODULE_${m}_LOCATION}/misc/java\")\n    include_directories(\"${module_java_dir}/src/cpp\")\n    file(GLOB _result \"${module_java_dir}/src/cpp/*.h\" \"${module_java_dir}/src/cpp/*.hpp\" \"${module_java_dir}/src/cpp/*.cpp\")\n    list(APPEND handwritten_cpp_sources ${_result})\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Configuring Android Project with OpenCV in CMake\nDESCRIPTION: This CMake snippet configures an Android project with OpenCV dependencies. It checks if the 'BUILD_FAT_JAVA_LIB' flag is set, then assigns different dependencies ('opencv_java' or 'videoio') to 'native_deps'. An Android project is added with these dependencies, targeting SDK version 11. It also adds a dependency relationship to ensure the sample project is built after the OpenCV Android examples. Requires OpenCV and Android SDK installations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/video-recorder/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(sample example-video-recorder)\n\nif(BUILD_FAT_JAVA_LIB)\n  set(native_deps opencv_java)\nelse()\n  set(native_deps videoio)\nendif()\n\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\" LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\" SDK_TARGET 11 \"${ANDROID_SDK_TARGET}\" NATIVE_DEPS ${native_deps})\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing for PCA using OpenCV in Python\nDESCRIPTION: Loads an image specified by a command-line argument, converts it to grayscale, and applies Otsu's thresholding to create a binary image. This standard preprocessing step facilitates object detection. Requires OpenCV Python library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#! [pre-process]\n# Load image\nparser = argparse.ArgumentParser(description='Code for Introduction to Principal Component Analysis (PCA) tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='pca_test1.jpg')\nargs = parser.parse_args()\n\nsrc = cv.imread(cv.samples.findFile(args.input))\nif src is None:\n    print('Could not open or find the image: ', args.input)\n    exit(0)\n\ncv.imshow('src', src)\n\n# Convert image to grayscale\ngray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n\n# Convert image to binary\n_, bw = cv.threshold(gray, 50, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)\n#! [pre-process]\n```\n\n----------------------------------------\n\nTITLE: Detecting ChArUco Corners\nDESCRIPTION: Shows the process of detecting ChArUco corners using the CharucoDetector class\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_detection/charuco_detection.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nstd::vector<cv::Point2f> charucoCorners;\nstd::vector<int> charucoIds;\ncharucoDetector.detectBoard(image, charucoCorners, charucoIds, markerCorners, markerIds);\n```\n\n----------------------------------------\n\nTITLE: Adding Images with OpenCV Python\nDESCRIPTION: This code snippet demonstrates the use of cv.add() to add two images or an image and a scalar using OpenCV in Python. It highlights the difference between OpenCV's saturated addition and NumPy's modulo addition. The input is two 8-bit unsigned integer arrays, and the output is their sum with a max value of 255.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_image_arithmetics/py_image_arithmetics.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> x = np.uint8([250])\n>>> y = np.uint8([10])\n\n>>> print( cv.add(x,y) ) # 250+10 = 260 => 255\n[[255]]\n\n>>> print( x+y )          # 250+10 = 260 % 256 = 4\n[4]\n```\n\n----------------------------------------\n\nTITLE: Applying Median Blur with OpenCV in C++\nDESCRIPTION: This C++ snippet details the use of the medianBlur() function by OpenCV to apply a median filter for smoothing images. Dependencies include OpenCV and it requires input image and kernel size.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n@snippet cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp medianblur\n```\n\n----------------------------------------\n\nTITLE: Displaying Results of Histogram Equalization\nDESCRIPTION: Code to display both the original grayscale image and the equalized image for comparison of contrast enhancement.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_equalization/histogram_equalization.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nimshow( \"Source image\", src );\nimshow( \"Equalized Image\", dst );\n```\n\nLANGUAGE: Java\nCODE:\n```\n// Setup the window\nHighgui.namedWindow(\"Source image\", WINDOW_AUTOSIZE);\nHighgui.namedWindow(\"Equalized Image\", WINDOW_AUTOSIZE);\n\n// Show results\nHighgui.imshow(\"Source image\", src);\nHighgui.imshow(\"Equalized Image\", dst);\n```\n\nLANGUAGE: Python\nCODE:\n```\ncv.imshow('Source image', src)\ncv.imshow('Equalized Image', dst)\n```\n\n----------------------------------------\n\nTITLE: Displaying Results in Java\nDESCRIPTION: Java snippet displaying the original source image and the generated histogram image in separate windows using `HighGui.imshow`. It then waits indefinitely for a key press using `HighGui.waitKey` before exiting the application with `System.exit(0)`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_33\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/Histograms_Matching/histogram_calculation/CalcHistDemo.java Display\n```\n\n----------------------------------------\n\nTITLE: Generating OpenCV for Windows Phone 8.1 ARM using CMake\nDESCRIPTION: Invokes CMake directly to generate Visual Studio 2013 project files for OpenCV targeting Windows Phone 8.1 on the ARM architecture. It specifies the ARM-specific generator, system name, and system version.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Visual Studio 12 2013 ARM\" -DCMAKE_SYSTEM_NAME=WindowsPhone -DCMAKE_SYSTEM_VERSION=8.1 <path-to-source>\n```\n\n----------------------------------------\n\nTITLE: Creating Trackbar in Java\nDESCRIPTION: This Java snippet creates a trackbar using `HighGui.createTrackbar`. It associates the trackbar with the named window (`WINDOW_NAME`), links its value to an integer array `alphaVal`, sets the maximum value `ALPHA_SLIDER_MAX`, and defines an anonymous `TrackbarCallback` inner class to handle value changes by calling the `on_trackbar` method. It also triggers the callback initially.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n//![create_trackbar]\n// Create trackbar\nHighGui.createTrackbar(TRACKBAR_NAME, WINDOW_NAME, new int[]{alphaVal}, ALPHA_SLIDER_MAX, new TrackbarCallback() {\n    @Override\n    public void onTrackbar(int pos, Object userdata) {\n        on_trackbar(pos);\n    }\n});\n// Show default image\non_trackbar(alphaVal);\n//![create_trackbar]\n```\n\n----------------------------------------\n\nTITLE: Computing Rotation Displacement Using OpenCV in C++\nDESCRIPTION: This snippet provides a routine to compute rotation displacement between image views in C++. Requires OpenCV library setup. The function handles images and results in the transformation matrix for displacement.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_31\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\n\nvoid computeRotationDisplacement(cv::Mat image1, cv::Mat image2) {\n    // Code to compute rotation displacement\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Setting a Pixel Value in an Image with OpenCV in C++\nDESCRIPTION: Alters the intensity value of a pixel at (y, x) in a cv::Mat image in C++. Uses the at<>() method for assignment. This operation mutates the input matrix. Requires OpenCV and appropriate image type.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\nimg.at<uchar>(y, x) = 128;\n```\n\n----------------------------------------\n\nTITLE: Calculating PSNR on GPU (Optimized) in C++\nDESCRIPTION: Defines an optimized C++ function `getPSNR_GPU_optimized` for GPU-based PSNR calculation. It uses a pre-allocated buffer (`BufferPSNR`) to avoid repeated GPU memory allocations for intermediate results (`gI1`, `gI2`, `gs`, `t1`, `t2`, `buf`). This function uploads input matrices and calls the GPU `psnr` function, reusing the buffers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n//![getpsnropt]\ndouble getPSNR_GPU_optimized(const Mat& I1, const Mat& I2, BufferPSNR& b)\n{\n    b.gI1.upload(I1);            // Upload data to GPU\n    b.gI2.upload(I2);\n\n    return psnr(b.gI1, b.gI2);\n}\n//![getpsnropt]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Build Options with CMake\nDESCRIPTION: Defines build configuration options for OpenCV using OCV_OPTION macro. Each option specifies a feature that can be enabled/disabled, its visibility conditions, and verification requirements. Options cover hardware acceleration, video capture, GUI frameworks, and external library integrations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nOCV_OPTION(WITH_1394 \"Include IEEE1394 support\" OFF\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_DC1394_2)\nOCV_OPTION(WITH_AVFOUNDATION \"Use AVFoundation for Video I/O (iOS/visionOS/Mac)\" ON\n  VISIBLE_IF APPLE\n  VERIFY HAVE_AVFOUNDATION)\nOCV_OPTION(WITH_AVIF \"Enable AVIF support\" ON\n  VERIFY HAVE_AVIF)\nOCV_OPTION(WITH_CAP_IOS \"Enable iOS video capture\" ON\n  VISIBLE_IF IOS\n  VERIFY HAVE_CAP_IOS)\nOCV_OPTION(WITH_CAROTENE \"Use NVidia carotene acceleration library for ARM platform\" (NOT CV_DISABLE_OPTIMIZATION)\n  VISIBLE_IF (ARM OR AARCH64) AND NOT IOS AND NOT XROS)\nOCV_OPTION(WITH_KLEIDICV \"Use KleidiCV library for ARM platforms\" (ANDROID AND AARCH64 AND NOT CV_DISABLE_OPTIMIZATION)\n  VISIBLE_IF (AARCH64 AND (ANDROID OR UNIX AND NOT IOS AND NOT XROS)))\nOCV_OPTION(WITH_NDSRVP \"Use Andes RVP extension\" (NOT CV_DISABLE_OPTIMIZATION)\n  VISIBLE_IF RISCV)\nOCV_OPTION(WITH_HAL_RVV \"Use HAL RVV optimizations\" (NOT CV_DISABLE_OPTIMIZATION)\n  VISIBLE_IF RISCV)\nOCV_OPTION(WITH_FASTCV \"Use Qualcomm FastCV acceleration library for ARM platform\" OFF\n  VISIBLE_IF ((ARM OR AARCH64) AND (ANDROID OR (UNIX AND NOT APPLE AND NOT IOS AND NOT XROS))))\nOCV_OPTION(WITH_CPUFEATURES \"Use cpufeatures Android library\" ON\n  VISIBLE_IF ANDROID\n  VERIFY HAVE_CPUFEATURES)\nOCV_OPTION(WITH_VTK \"Include VTK library support (and build opencv_viz module eiher)\" ON\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS AND NOT WINRT AND NOT CMAKE_CROSSCOMPILING\n  VERIFY HAVE_VTK)\nOCV_OPTION(WITH_CUDA \"Include NVidia Cuda Runtime support\" OFF\n  VISIBLE_IF NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_CUDA)\nOCV_OPTION(WITH_CUFFT \"Include NVidia Cuda Fast Fourier Transform (FFT) library support\" WITH_CUDA\n  VISIBLE_IF WITH_CUDA\n  VERIFY HAVE_CUFFT)\nOCV_OPTION(WITH_CUBLAS \"Include NVidia Cuda Basic Linear Algebra Subprograms (BLAS) library support\" WITH_CUDA\n  VISIBLE_IF WITH_CUDA\n  VERIFY HAVE_CUBLAS)\nOCV_OPTION(WITH_CUDNN \"Include NVIDIA CUDA Deep Neural Network (cuDNN) library support\" WITH_CUDA\n  VISIBLE_IF WITH_CUDA\n  VERIFY HAVE_CUDNN)\nOCV_OPTION(WITH_NVCUVID \"Include NVidia Video Decoding library support\" ON\n  VISIBLE_IF WITH_CUDA\n  VERIFY HAVE_NVCUVID)\nOCV_OPTION(WITH_NVCUVENC \"Include NVidia Video Encoding library support\" ON\n  VISIBLE_IF WITH_CUDA\n  VERIFY HAVE_NVCUVENC)\nOCV_OPTION(WITH_EIGEN \"Include Eigen2/Eigen3 support\" (NOT CV_DISABLE_OPTIMIZATION AND NOT CMAKE_CROSSCOMPILING)\n  VISIBLE_IF NOT WINRT\n  VERIFY HAVE_EIGEN)\nOCV_OPTION(WITH_FFMPEG \"Include FFMPEG support\" (NOT ANDROID)\n  VISIBLE_IF NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_FFMPEG)\nOCV_OPTION(WITH_GSTREAMER \"Include Gstreamer support\" ON\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_GSTREAMER AND GSTREAMER_VERSION VERSION_GREATER \"0.99\")\nOCV_OPTION(WITH_GTK \"Include GTK support\" ON\n  VISIBLE_IF UNIX AND NOT APPLE AND NOT ANDROID\n  VERIFY HAVE_GTK)\nOCV_OPTION(WITH_GTK_2_X \"Use GTK version 2\" OFF\n  VISIBLE_IF UNIX AND NOT APPLE AND NOT ANDROID\n  VERIFY HAVE_GTK AND NOT HAVE_GTK3)\nOCV_OPTION(WITH_FRAMEBUFFER \"Include framebuffer support\" OFF\n  VISIBLE_IF UNIX AND NOT APPLE AND NOT ANDROID)\nOCV_OPTION(WITH_FRAMEBUFFER_XVFB \"Include virtual framebuffer support\" OFF\n  VISIBLE_IF UNIX AND NOT APPLE AND NOT ANDROID)\nOCV_OPTION(WITH_WAYLAND \"Include Wayland support\" OFF\n        VISIBLE_IF UNIX AND NOT APPLE AND NOT ANDROID\n        VERIFY HAVE_WAYLAND)\nOCV_OPTION(WITH_IPP \"Include Intel IPP support\" (NOT MINGW AND NOT CV_DISABLE_OPTIMIZATION)\n  VISIBLE_IF (X86_64 OR X86) AND NOT WINRT AND NOT IOS AND NOT XROS\n  VERIFY HAVE_IPP)\nOCV_OPTION(WITH_HALIDE \"Include Halide support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_HALIDE)\nOCV_OPTION(WITH_VULKAN \"Include Vulkan support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_VULKAN)\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Compilation and Documentation Settings\nDESCRIPTION: Sets up Java compilation attributes and javadoc configuration, including source/target versions and documentation URL links.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jar/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(OPENCV_JAVA_SOURCE_VERSION)\n  set(OPENCV_ANT_JAVAC_EXTRA_ATTRS \"${OPENCV_ANT_JAVAC_EXTRA_ATTRS} source=\\\"${OPENCV_JAVA_SOURCE_VERSION}\\\"\")\nendif()\nif(OPENCV_JAVA_TARGET_VERSION)\n  set(OPENCV_ANT_JAVAC_EXTRA_ATTRS \"${OPENCV_ANT_JAVAC_EXTRA_ATTRS} target=\\\"${OPENCV_JAVA_TARGET_VERSION}\\\"\")\nendif()\n\nset(OPENCV_JAVADOC_DESTINATION \"${OpenCV_BINARY_DIR}/doc/doxygen/html/javadoc\" CACHE STRING \"\")\n\nset(OPENCV_JAVADOC_LINK_URL \"\" CACHE STRING \"See details in modules/java/jar/CMakeLists.txt\")\nif(OPENCV_JAVADOC_LINK_URL)\n  set(CMAKE_CONFIG_OPENCV_JAVADOC_LINK \"link=\\\"${OPENCV_JAVADOC_LINK_URL}\\\"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Keypoint Management Logic\nDESCRIPTION: Handles the dynamic allocation and management of keypoints storage, including capacity expansion when needed. Implements the success handling logic for both homogeneous and structured corner detection cases.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_17\n\nLANGUAGE: C++\nCODE:\n```\nsuccess_homogeneous:\n    if(total == nExpectedCorners)\n    {\n        if(nExpectedCorners == 0)\n        {\n            nExpectedCorners = 512;\n            keypoints.reserve(nExpectedCorners);\n        }\n        else\n        {\n            nExpectedCorners *= 2;\n            keypoints.reserve(nExpectedCorners);\n        }\n    }\n    keypoints.push_back(KeyPoint(Point2f((float)x, (float)y), 1.0f));\n    total++;\n    goto homogeneous;\n```\n\n----------------------------------------\n\nTITLE: Setting a Pixel Value in an Image with OpenCV in Python\nDESCRIPTION: Alters the intensity value of a pixel at (y, x) in a NumPy ndarray with Python OpenCV. Indexing is direct: img[y, x] = 128. Input matrix must be mutable; assignment is in-place.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nimg[y, x] = 128\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building DirectX Sample Projects - OpenCV - CMake\nDESCRIPTION: This snippet initializes, checks, and configures DirectX sample projects for OpenCV using CMake. It ensures the mandatory modules (opencv_core, opencv_imgproc, etc.) are available, defines build targets for each sample file, and links the correct DirectX and Windows libraries based on the specific sample. Prerequisites include enabled BUILD_EXAMPLES and all required dependencies present. Keys parameters include sample source filenames and dependency settings. The inputs are *.cpp and related files, and outputs are configured sample binaries linked to appropriate libraries. Conditional logic is used to link specific DirectX libraries to corresponding interop samples.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/directx/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nocv_install_example_src(directx *.cpp *.hpp CMakeLists.txt)\n\nset(OPENCV_DIRECTX_SAMPLES_REQUIRED_DEPS\n    opencv_core\n    opencv_imgproc\n    opencv_imgcodecs\n    opencv_videoio\n    opencv_highgui)\nocv_check_dependencies(${OPENCV_DIRECTX_SAMPLES_REQUIRED_DEPS})\n\nif(NOT BUILD_EXAMPLES OR NOT OCV_DEPENDENCIES_FOUND)\n  return()\nendif()\n\nproject(\"directx_samples\")\nocv_include_modules_recurse(${tgt} ${OPENCV_DIRECTX_SAMPLES_REQUIRED_DEPS})\nfile(GLOB all_samples RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} *.cpp)\nforeach(sample_filename ${all_samples})\n  ocv_define_sample(tgt ${sample_filename} directx)\n  ocv_target_link_libraries(${tgt} PRIVATE ${OPENCV_LINKER_LIBS} ${OPENCV_DIRECTX_SAMPLES_REQUIRED_DEPS})\n  ocv_target_link_libraries(${tgt} PRIVATE \"gdi32\")\n  if(sample_filename STREQUAL \"d3d9_interop.cpp\")\n    ocv_target_link_libraries(${tgt} PRIVATE d3d9)\n  endif()\n  if(sample_filename STREQUAL \"d3d9ex_interop.cpp\")\n    ocv_target_link_libraries(${tgt} PRIVATE d3d9)\n  endif()\n  if(sample_filename STREQUAL \"d3d10_interop.cpp\")\n    ocv_target_link_libraries(${tgt} PRIVATE d3d10)\n  endif()\n  if(sample_filename STREQUAL \"d3d11_interop.cpp\")\n    ocv_target_link_libraries(${tgt} PRIVATE d3d11)\n  endif()\nendforeach()\n\n```\n\n----------------------------------------\n\nTITLE: Performing Non-Linear SVM Classification in OpenCV\nDESCRIPTION: This snippet demonstrates the main process of non-linear SVM classification. It includes creating the SVM object, setting parameters, training the model, and classifying the data points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nPtr<SVM> svm = SVM::create();\nsvm->setType(SVM::C_SVC);\nsvm->setKernel(SVM::RBF);\nsvm->setTermCriteria(TermCriteria(TermCriteria::MAX_ITER, 100, 1e-6));\nsvm->train(trainData, ROW_SAMPLE, labels);\n\nMat green(res.size(), CV_8UC3);\nMat blue(res.size(), CV_8UC3);\n\nfor (int i = 0; i < res.rows; ++i)\n{\n    for (int j = 0; j < res.cols; ++j)\n    {\n        Mat sampleMat = (Mat_<float>(1,2) << j,i);\n        float response = svm->predict(sampleMat);\n        if (response == 1)\n            green.at<Vec3b>(i,j) = Vec3b(0,255,0);\n        else if (response == 2)\n            blue.at<Vec3b>(i,j) = Vec3b(255,0,0);\n    }\n}\n\ngreen.copyTo(res, green);\nblue.copyTo(res, blue);\n```\n\nLANGUAGE: Java\nCODE:\n```\nSVM svm = SVM.create();\nsvm.setType(SVM.C_SVC);\nsvm.setKernel(SVM.RBF);\nsvm.setTermCriteria(new TermCriteria(TermCriteria.MAX_ITER, 100, 1e-6));\nsvm.train(trainData, Ml.ROW_SAMPLE, labels);\n\nMat green = new Mat(res.size(), CvType.CV_8UC3);\nMat blue = new Mat(res.size(), CvType.CV_8UC3);\n\nfor (int i = 0; i < res.rows(); i++) {\n    for (int j = 0; j < res.cols(); j++) {\n        Mat sampleMat = new Mat(1, 2, CvType.CV_32F) {\n            {\n                put(0, 0, j);\n                put(0, 1, i);\n            }\n        };\n        float response = svm.predict(sampleMat);\n        if (response == 1) {\n            green.put(i, j, new byte[]{0, (byte) 255, 0});\n        } else if (response == 2) {\n            blue.put(i, j, new byte[]{(byte) 255, 0, 0});\n        }\n    }\n}\n\nCore.copyTo(green, res, green);\nCore.copyTo(blue, res, blue);\n```\n\nLANGUAGE: Python\nCODE:\n```\nsvm = cv.ml.SVM_create()\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setKernel(cv.ml.SVM_RBF)\nsvm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, 100, 1e-6))\nsvm.train(train_data, cv.ml.ROW_SAMPLE, labels)\n\ngreen = np.zeros((res.shape[0], res.shape[1], 3), np.uint8)\nblue = np.zeros((res.shape[0], res.shape[1], 3), np.uint8)\n\nfor i in range(res.shape[0]):\n    for j in range(res.shape[1]):\n        sample = np.matrix([[j, i]], dtype=np.float32)\n        response = svm.predict(sample)[1]\n        if response == 1:\n            green[i, j] = [0, 255, 0]\n        elif response == 2:\n            blue[i, j] = [255, 0, 0]\n\nmask = np.logical_or(green.any(-1), blue.any(-1))\nres[mask] = green[mask] + blue[mask]\n```\n\n----------------------------------------\n\nTITLE: Installing GCC Compiler on Linux\nDESCRIPTION: Command to install the GCC compiler on Linux using the package manager.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install build-essential\n```\n\n----------------------------------------\n\nTITLE: Checking Library Dependencies with ldd\nDESCRIPTION: Verify installed shared libraries requirements for OpenCV on the target system using the ldd command. Identifies any missing dependencies needing installation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nldd /usr/local/lib/libopencv_freetype.so\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV with Make\nDESCRIPTION: Command to build OpenCV using make with parallel compilation. The -j parameter specifies the number of parallel threads to use, which varies depending on the system resources.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ make -j6\n```\n\n----------------------------------------\n\nTITLE: Dictionary Operations in ArUco\nDESCRIPTION: Methods for reading and writing ArUco dictionary data to persistent storage.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_faq/aruco_faq.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ncv::aruco::Dictionary::writeDictionary()\ncv::aruco::Dictionary::readDictionary()\n```\n\n----------------------------------------\n\nTITLE: Prediction Postprocessing: Extracting Top Class - C++\nDESCRIPTION: This C++ snippet finds the class index with maximum confidence from model output and assigns it to 'classId'. This allows retrieval of the most likely prediction after inference. It is standard in classification pipelines where class scores are produced.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\nPoint classIdPoint;\\ndouble confidence;\\nminMaxLoc(prob.reshape(1, 1), 0, &confidence, 0, &classIdPoint);\\nint classId = classIdPoint.x;\n```\n\n----------------------------------------\n\nTITLE: Detecting Thread Support for Standalone Build in CMake\nDESCRIPTION: Manages thread support detection and definition for standalone sample builds. If example threads are disabled (`OPENCV_EXAMPLES_DISABLE_THREADS`), it does nothing. Otherwise, it sets a `HAVE_THREADS` variable directly for MSVC or Apple platforms, or uses `find_package(Threads)` for other systems. Finally, if thread support is confirmed (either via `Threads::Threads` target or the `HAVE_THREADS` variable) and not disabled, it adds the `-DHAVE_THREADS=1` preprocessor definition.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nif(OPENCV_EXAMPLES_DISABLE_THREADS)\n  # nothing\nelsif(MSVC OR APPLE)\n  set(HAVE_THREADS 1)\nelse()\n  find_package(Threads)\nendif()\nif((TARGET Threads::Threads OR HAVE_THREADS) AND NOT OPENCV_EXAMPLES_DISABLE_THREADS)\n  set(HAVE_THREADS 1)\n  add_definitions(-DHAVE_THREADS=1)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Scheduling Patterns in Halide YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to define a reusable scheduling pattern named `fully_connected` under the `patterns` section. The pattern uses a parameter `c_split`. It then shows how to apply this pattern to a specific layer (`fc8`), providing a concrete value (8) for the `c_split` parameter.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_halide_scheduling/dnn_halide_scheduling.markdown#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# At the beginning of the file\npatterns:\n  fully_connected:\n    split: { c: c_split }\n    fuse: { src: [x, y, co], dst: block }\n    parallel: block\n    vectorize: { ci: c_split }\n# Somewhere below\nfc8:\n  pattern: fully_connected\n  params: { c_split: 8 }\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Decision Tree in C++\nDESCRIPTION: Part of the FAST corner detection algorithm that uses conditional logic to decide whether a pixel is a corner. The code compares pixel values at various offsets to brightness thresholds (c_b and cb) and branches to either 'is_a_corner' or 'is_not_a_corner' labels based on these comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_40\n\nLANGUAGE: C++\nCODE:\n```\ngoto is_a_corner;\nelse\n  if(ptr[offset10] < c_b)\n    goto is_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset10] < c_b)\n    if(ptr[offset11] < c_b)\n      goto is_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset1] < c_b)\n    if(ptr[offset6] > cb)\n      goto is_not_a_corner;\n    else\n      if(ptr[offset6] < c_b)\n        if(ptr[offset3] < c_b)\n          if(ptr[offset4] < c_b)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    if(ptr[offset1] > cb)\n      if(ptr[offset6] > cb)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset3] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset8] < c_b)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      if(ptr[offset6] > cb)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset3] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset8] < c_b)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\nelse\n  if(ptr[offset9] > cb)\n    goto is_not_a_corner;\n  else\n    if(ptr[offset9] < c_b)\n      if(ptr[offset1] > cb)\n        if(ptr[offset6] > cb)\n          goto is_not_a_corner;\n        else\n          if(ptr[offset6] < c_b)\n            if(ptr[offset8] < c_b)\n              if(ptr[offset4] < c_b)\n                if(ptr[offset3] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] > cb)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset8] < c_b)\n                if(ptr[offset4] < c_b)\n                  if(ptr[offset3] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset10] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset6] > cb)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset8] < c_b)\n                if(ptr[offset4] < c_b)\n                  if(ptr[offset3] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset10] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset5] > cb)\n    if(ptr[offset7] > cb)\n      if(ptr[offset2] < c_b)\n        if(ptr[offset9] < c_b)\n          goto is_not_a_corner;\n        else\n          if(ptr[offset9] > cb)\n            if(ptr[offset1] > cb)\n              if(ptr[offset6] < c_b)\n                goto is_not_a_corner;\n              else\n                if(ptr[offset6] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset4] > cb)\n                      if(ptr[offset3] > cb)\n                        goto is_a_corner;\n                      else\n                        if(ptr[offset10] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      if(ptr[offset10] > cb)\n                        if(ptr[offset11] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset1] < c_b)\n                if(ptr[offset6] < c_b)\n                  goto is_not_a_corner;\n                else\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset8] > cb)\n                      if(ptr[offset4] > cb)\n                        if(ptr[offset3] > cb)\n                          goto is_a_corner;\n                        else\n                          if(ptr[offset10] > cb)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset10] > cb)\n                          if(ptr[offset11] > cb)\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Static AAR for Android (Python Command)\nDESCRIPTION: Executes the Python script `build_static_aar.py` to create an AAR package containing OpenCV Java bindings and the static C++ libraries (.a files). Requires the path to the downloaded OpenCV Android SDK as an argument. The output AAR and Maven repository will be generated in the 'outputs' directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/android/aar-template/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython build_static_aar.py \"~/opencv-4.7.0-android-sdk/OpenCV-android-sdk\"\n```\n\n----------------------------------------\n\nTITLE: Reduce Operations with Vector Registers in C++\nDESCRIPTION: Demonstrates reduce operations like v_reduce_min(), v_reduce_max(), and v_reduce_sum() to compute a single value from a vector register.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/univ_intrin/univ_intrin.markdown#2025-04-22_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nv_int32 a;                                //  a = {a1, ..., a4}\nint mn = v_reduce_min(a);                 // mn = min(a1, ..., an)\nint sum = v_reduce_sum(a);                // sum = a1 + ... + an\n```\n\n----------------------------------------\n\nTITLE: Setting Fixed Aspect Ratio for Camera Matrix in C++\nDESCRIPTION: This snippet shows how to set a fixed aspect ratio for the camera matrix during calibration using OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nfixed_aspect\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Conditional Logic in C++\nDESCRIPTION: Complex nested if-else statements comparing pixel values at different offsets against brightness thresholds cb and c_b. The code implements corner detection logic with multiple branching paths leading to either success_homogeneous or homogeneous states.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_39\n\nLANGUAGE: cpp\nCODE:\n```\nelse\n  if(ptr[offset10] < c_b)\n    if(ptr[offset11] < c_b)\n      {} // goto success_homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  if(ptr[offset8] < c_b)\n    if(ptr[offset9] < c_b)\n      if(ptr[offset10] < c_b)\n        if(ptr[offset11] < c_b)\n          {} // goto success_homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  if(ptr[offset6] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset9] < c_b)\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              {} // goto success_homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  if(ptr[offset11] > cb)\n    if(ptr[offset7] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset9] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset12] > cb)\n              if(ptr[offset13] > cb)\n                if(ptr[offset6] > cb)\n                  if(ptr[offset5] > cb)\n                    {} // goto success_homogeneous;\n                  else\n                    if(ptr[offset14] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                else\n                  if(ptr[offset14] > cb)\n                    if(ptr[offset15] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n```\n\n----------------------------------------\n\nTITLE: Calculating Chessboard Corner Positions in C++\nDESCRIPTION: This code calculates the positions of chessboard corners for camera calibration using OpenCV functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nboard_corners\n```\n\n----------------------------------------\n\nTITLE: Defining calcGST() Function Prototype in G-API\nDESCRIPTION: Defines the calcGST() function prototype using G-API, which produces a compute graph instead of calculating actual values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/anisotropic_segmentation/porting_anisotropic_segmentation.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ncv::GMat calcGST(const cv::GMat& inputImage, const int w);\n```\n\n----------------------------------------\n\nTITLE: Initializing Flann Based Matcher for Descriptor Matching in C++\nDESCRIPTION: Configures the FlannBasedMatcher with LSH index and search parameters for matching binary descriptors efficiently. It sets up matching using tuned parameters to ensure efficient and accurate results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\ncv::Ptr<cv::flann::IndexParams> indexParams = cv::makePtr<cv::flann::LshIndexParams>(6, 12, 1); // instantiate LSH index parameters\ncv::Ptr<cv::flann::SearchParams> searchParams = cv::makePtr<cv::flann::SearchParams>(50);       // instantiate flann search parameters\n\ncv::DescriptorMatcher * matcher = new cv::FlannBasedMatcher(indexParams, searchParams);         // instantiate FlannBased matcher\nrmatcher.setDescriptorMatcher(matcher);                                                         // set matcher\n```\n\n----------------------------------------\n\nTITLE: Example Success Output from Model Conversion - Console\nDESCRIPTION: This output confirms the ONNX export was successful and provides the relative file path to the resulting ONNX file. It is generated by the prior model export scripts and should match the expected output directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nPyTorch ResNet-50 model was successfully converted: models/resnet50.onnx\n```\n\n----------------------------------------\n\nTITLE: Setting Parameters for Text Detection (EAST) in C++\nDESCRIPTION: This C++ snippet configures the parameters for EAST-based text detection using OpenCV. It sets confidence and non-maximum suppression thresholds, scales for input normalization, and specifies input size and mean values for accurate text detection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_text_spotting/dnn_text_spotting.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nTextDetectionModel_EAST model(\"EAST.pb\");\n\nfloat confThreshold = 0.5;\nfloat nmsThreshold = 0.4;\nmodel.setConfidenceThreshold(confThresh)\n     .setNMSThreshold(nmsThresh)\n;\n\n// Normalization parameters\ndouble detScale = 1.0;\nSize detInputSize = Size(320, 320);\nScalar detMean = Scalar(123.68, 116.78, 103.94);\nbool swapRB = true;\nmodel.setInputParams(detScale, detInputSize, detMean, swapRB);\n```\n\n----------------------------------------\n\nTITLE: Installing Make on Linux\nDESCRIPTION: Command to install Make build automation tool on Linux using the package manager.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install make\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV Libraries on Target System\nDESCRIPTION: Receive and extract the OpenCV library archive from the host to the target system at /usr/local. Finalize installation with ldconfig to refresh shared library cache.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nsudo tar zxvf opencv_arm64.tgz -C /usr/local\nsudo ldconfig\n```\n\n----------------------------------------\n\nTITLE: Converting Image to Grayscale in OpenCV (C++/Java/Python)\nDESCRIPTION: Converts the loaded source image from BGR (or color) to grayscale. This is often a prerequisite for thresholding and morphological operations. The `cvtColor` function is used in all three languages.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection.md#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n//![gray]\n// Transform source image to gray if it is not already\nMat gray;\n\nif (src.channels() == 3)\n{\n    cvtColor(src, gray, COLOR_BGR2GRAY);\n}\nelse\n{\n    gray = src;\n}\n\n// Show gray image\nshow_wait_destroy(\"gray\", gray);\n//![gray]\n```\n\nLANGUAGE: java\nCODE:\n```\n//![gray]\n// Transform source image to gray if it is not already\nMat gray = new Mat();\nif (src.channels() == 3) {\n    Imgproc.cvtColor(src, gray, Imgproc.COLOR_BGR2GRAY);\n} else {\n    gray = src;\n}\n\n// Show gray image\nshowWaitDestroy(\"gray\", gray);\n//![gray]\n```\n\nLANGUAGE: python\nCODE:\n```\n#![gray]\n# Transform source image to gray if it is not already\nif len(src.shape) != 2:\n    gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\nelse:\n    gray = src\n\n# Show gray image\nshow_wait_destroy(\"gray\", gray)\n#![gray]\n```\n\n----------------------------------------\n\nTITLE: Performing Closing Operation with OpenCV Python\nDESCRIPTION: Shows how to perform closing operation (dilation followed by erosion). Useful for closing small holes inside foreground objects.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclosing = cv.morphologyEx(img, cv.MORPH_CLOSE, kernel)\n```\n\n----------------------------------------\n\nTITLE: Configuring Qt Build for OpenCV on Windows\nDESCRIPTION: Command to configure Qt build with specific features disabled for OpenCV compatibility. Disables webkit, phonon, Qt3 support, multimedia, and other non-essential components.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_install/windows_install.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconfigure.exe -release -no-webkit -no-phonon -no-phonon-backend -no-script -no-scripttools -no-qt3support -no-multimedia -no-ltcg\n```\n\n----------------------------------------\n\nTITLE: Pixel Intensity Comparison for FAST Corner Detection in C++\nDESCRIPTION: This code snippet implements part of the FAST corner detection algorithm. It compares pixel intensities at various offsets against threshold values to determine if a pixel is a corner point. The algorithm uses nested conditional statements to check multiple conditions efficiently.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_36\n\nLANGUAGE: C++\nCODE:\n```\ncontinue; // goto homogeneous;\nelse\n  if(ptr[offset5] < c_b)\n    if(ptr[offset6] < c_b)\n      if(ptr[offset7] < c_b)\n        {} // goto success_homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  if(ptr[offset4] < c_b)\n    if(ptr[offset5] < c_b)\n      if(ptr[offset6] < c_b)\n        if(ptr[offset7] < c_b)\n          {} // goto success_homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\n// ... (truncated for brevity)\nelse\n  if(ptr[offset2] < c_b)\n    if(ptr[offset4] > cb)\n      if(ptr[offset11] > cb)\n        if(ptr[offset7] > cb)\n          if(ptr[offset8] > cb)\n            if(ptr[offset9] > cb)\n              if(ptr[offset10] > cb)\n                if(ptr[offset6] > cb)\n                  if(ptr[offset5] > cb)\n                    if(ptr[offset3] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      if(ptr[offset12] > cb)\n                        {} // goto success_homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                  else\n                    if(ptr[offset12] > cb)\n                      if(ptr[offset13] > cb)\n                        if(ptr[offset14] > cb)\n                          {} // goto success_homogeneous;\n                        else\n                          continue; // goto homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                else\n                  if(ptr[offset12] > cb)\n                    if(ptr[offset13] > cb)\n                      if(ptr[offset14] > cb)\n                        if(ptr[offset15] > cb)\n                          {} // goto success_homogeneous;\n                        else\n                          continue; // goto homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n      if(ptr[offset11] < c_b)\n        if(ptr[offset12] < c_b)\n          if(ptr[offset13] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset14] < c_b)\n                if(ptr[offset15] < c_b)\n                  if(ptr[offset1] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    if(ptr[offset8] < c_b)\n                      if(ptr[offset9] < c_b)\n                        {} // goto success_homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                else\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset7] < c_b)\n                      if(ptr[offset8] < c_b)\n                        if(ptr[offset9] < c_b)\n                          {} // goto success_homogeneous;\n                        else\n                          continue; // goto homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n              else\n                if(ptr[offset5] < c_b)\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset7] < c_b)\n                      if(ptr[offset8] < c_b)\n                        if(ptr[offset9] < c_b)\n                          {} // goto success_homogeneous;\n                        else\n                          continue; // goto homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n            else\n              if(ptr[offset1] < c_b)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset14] < c_b)\n                    if(ptr[offset15] < c_b)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n```\n\n----------------------------------------\n\nTITLE: Finding Keypoints with SIFT in OpenCV - Python\nDESCRIPTION: This snippet initializes the SIFT feature detector and finds keypoints and descriptors for left and right images using OpenCV. It requires OpenCV and images to process. Outputs the keypoints and descriptors for further matching.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_epipolar_geometry/py_epipolar_geometry.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg1 = cv.imread('myleft.jpg', cv.IMREAD_GRAYSCALE)  #queryimage # left image\nimg2 = cv.imread('myright.jpg', cv.IMREAD_GRAYSCALE) #trainimage # right image\n\nsift = cv.SIFT_create()\n\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute(img1,None)\nkp2, des2 = sift.detectAndCompute(img2,None)\n```\n\n----------------------------------------\n\nTITLE: Configuring U-SURF (Upright SURF) in OpenCV Python\nDESCRIPTION: This code shows how to enable U-SURF mode, which doesn't compute orientation, making the process faster. It's useful for applications where rotation invariance is not required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Check upright flag, if it False, set it to True\n>>> print( surf.getUpright() )\nFalse\n\n>>> surf.setUpright(True)\n\n# Recompute the feature points and draw it\n>>> kp = surf.detect(img,None)\n>>> img2 = cv.drawKeypoints(img,kp,None,(255,0,0),4)\n\n>>> plt.imshow(img2),plt.show()\n```\n\n----------------------------------------\n\nTITLE: Converting Image to Grayscale in Python\nDESCRIPTION: Converts the blurred source image to grayscale using cv2.cvtColor with the cv2.COLOR_BGR2GRAY flag. Requires cv2.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#! [convert_to_gray]\n# [convert_to_gray]\n# Convert the image to grayscale\nsrc_gray = cv.cvtColor( src, cv.COLOR_BGR2GRAY )\n# [convert_to_gray]\n# ! [convert_to_gray]\n```\n\n----------------------------------------\n\nTITLE: Min/Max Operations with Vector Registers in C++\nDESCRIPTION: Shows how to use v_min() and v_max() functions to perform element-wise minimum and maximum operations on vector registers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/univ_intrin/univ_intrin.markdown#2025-04-22_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nv_int32 a;                               // {a1, ..., an}\nv_int32 b;                               // {b1, ..., bn}\n\nv_int32 mn = v_min(a, b);                // {min(a1, b1), ..., min(an, bn)}\nv_int32 mx = v_max(a, b);                // {max(a1, b1), ..., max(an, bn)}\n```\n\n----------------------------------------\n\nTITLE: Setting TIFF Library Dependencies with CMake\nDESCRIPTION: This CMake snippet appends required libraries to the TIFF_LIBRARY_DEPS list conditionally, based on the availability of components such as libm, zlib, jpeg, and others. The purpose is to compile the libtiff library accurately by including necessary dependencies for various compression and decoding functionalities.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libtiff/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(TIFF_LIBRARY_DEPS)\nif(M_LIBRARY)\n  list(APPEND TIFF_LIBRARY_DEPS ${M_LIBRARY})\nendif()\nif(ZLIB_LIBRARIES)\n  list(APPEND TIFF_LIBRARY_DEPS ${ZLIB_LIBRARIES})\nendif()\nif(JPEG_LIBRARIES)\n  list(APPEND TIFF_LIBRARY_DEPS ${JPEG_LIBRARIES})\nendif()\nif(JPEG12_LIBRARIES)\n  list(APPEND TIFF_LIBRARY_DEPS ${JPEG12_LIBRARIES})\nendif()\nif(JBIG_LIBRARIES)\n  list(APPEND TIFF_LIBRARY_DEPS ${JBIG_LIBRARIES})\nendif()\nif(LIBLZMA_LIBRARIES)\n  list(APPEND TIFF_LIBRARY_DEPS ${LIBLZMA_LIBRARIES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up Trackbars for Threshold Parameters (Java)\nDESCRIPTION: This Java code creates two slider trackbars for threshold type and value in the 'Threshold Demo' window with OpenCV's HighGui API. Each slider is bound to an onChange callback that updates the thresholding result instantly. Requires OpenCV Java GUI support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\n// [trackbar]\\nHighGui.createTrackbar(\"Type:\\n 0:Binary \\n 1:BinaryInv \\n 2:Trunc \\n 3:ToZero \\n 4:ToZeroInv\", window_name, new int[]{threshold_type}, max_type, (pos)->{\\n    threshold_type = pos;\\n    update();\\n});\\nHighGui.createTrackbar(\"Value\", window_name, new int[]{threshold_value}, max_value, (pos)->{\\n    threshold_value = pos;\\n    update();\\n});\\n// [trackbar]\n```\n\n----------------------------------------\n\nTITLE: Computing Calibration Errors in C++\nDESCRIPTION: This code computes the calibration errors by projecting object points and comparing them with detected image points using OpenCV functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\ncompute_errors\n```\n\n----------------------------------------\n\nTITLE: Defining OpenJPEG Library Version Properties in CMake\nDESCRIPTION: Sets the VERSION and SOVERSION properties for the OpenJPEG library target using the previously defined version variables. These properties are used by CMake during the library linking and installation phases.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENJPEG_LIBRARY_PROPERTIES\n  VERSION   \"${OPENJPEG_VERSION_MAJOR}.${OPENJPEG_VERSION_MINOR}.${OPENJPEG_VERSION_BUILD}\"\n  SOVERSION \"${OPENJPEG_SOVERSION}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV Repository and Preparing Build Directory (Bash)\nDESCRIPTION: This sequence of Bash commands clones the OpenCV Git repository, navigates into the cloned directory, checks out the 2.4 branch (though newer versions would follow a similar pattern), creates a dedicated 'build' subdirectory, and changes the current directory into 'build' to prepare for the CMake configuration step.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git://github.com/opencv/opencv.git\ncd opencv\ngit checkout 2.4\nmkdir build\ncd build\n```\n\n----------------------------------------\n\nTITLE: Default OpenCV DNN Model Configuration for SSD MobileNetV1 - YAML\nDESCRIPTION: Provides a sample YAML configuration for the OpenCV DNN module. Specifies model and config file paths, preprocessing parameters (mean, scale, input shape, color format), class list, and sample type for SSD MobileNetV1. Dependencies: correct file paths and compatible OpenCV DNN pipeline.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial.md#2025-04-22_snippet_6\n\nLANGUAGE: yml\nCODE:\n```\nssd_tf:\\n  model: \"ssd_mobilenet_v1_coco_2017_11_17.pb\"\\n  config: \"ssd_mobilenet_v1_coco_2017_11_17.pbtxt\"\\n  mean: [0, 0, 0]\\n  scale: 1.0\\n  width: 300\\n  height: 300\\n  rgb: true\\n  classes: \"object_detection_classes_coco.txt\"\\n  sample: \"object_detection\"\n```\n\n----------------------------------------\n\nTITLE: Implementing SURF Feature Detection in Java\nDESCRIPTION: This Java code showcases the use of OpenCV's SURF detector for keypoint detection in images. It covers image loading, SURF detector creation, keypoint detection, and visualization of the results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/feature_detection/feature_detection.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.Core;\\nimport org.opencv.core.Mat;\\nimport org.opencv.core.MatOfKeyPoint;\\nimport org.opencv.core.Scalar;\\nimport org.opencv.features2d.Features2d;\\nimport org.opencv.imgcodecs.Imgcodecs;\\nimport org.opencv.xfeatures2d.SURF;\\n\\nclass SURFDetectionDemo {\\n    public static void main(String[] args) {\\n        // Load the native OpenCV library\\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\\n\\n        // Read image file\\n        Mat src = Imgcodecs.imread(\"box.png\", Imgcodecs.IMREAD_GRAYSCALE);\\n\\n        // Check if image is loaded fine\\n        if (src.empty()) {\\n            System.out.println(\"Error opening image!\");\\n            System.exit(-1);\\n        }\\n\\n        // Detect the keypoints using SURF Detector\\n        SURF detector = SURF.create(400);\\n        MatOfKeyPoint keypoints = new MatOfKeyPoint();\\n        detector.detect(src, keypoints);\\n\\n        // Draw keypoints\\n        Mat outputImage = new Mat();\\n        Features2d.drawKeypoints(src, keypoints, outputImage, new Scalar(255, 0, 0), Features2d.DrawMatchesFlags_DEFAULT);\\n\\n        // Write the image with keypoints\\n        Imgcodecs.imwrite(\"surf_keypoints.jpg\", outputImage);\\n\\n        System.out.println(\"Finished. See surf_keypoints.jpg for the result.\");\\n    }\\n}\n```\n\n----------------------------------------\n\nTITLE: Creating ARM Build Directory and Configuring with CMake - Bash\nDESCRIPTION: Demonstrates creation of a dedicated build directory for ARM hard-float configurations, then runs CMake with the appropriate ARM toolchain file. Requires OpenCV source and toolchains to be present in the specified paths. Replace directory names as needed according to project organization. This sets up an isolated build environment for ARM.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/arm_crosscompile_with_cmake.markdown#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/opencv/platforms/linux\nmkdir -p build_hardfp\ncd build_hardfp\n\ncmake -DCMAKE_TOOLCHAIN_FILE=../arm-gnueabi.toolchain.cmake ../../..\n```\n\n----------------------------------------\n\nTITLE: CMAKE_INSTALL_PREFIX Configuration\nDESCRIPTION: Sets up default installation directories based on platform and build type. Uses different paths for Windows vs Unix systems and handles cross-compilation scenarios.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)\n  if(NOT CMAKE_CROSSCOMPILING)\n    if(WIN32)\n      set(CMAKE_INSTALL_PREFIX \"${CMAKE_BINARY_DIR}/install\" CACHE PATH \"Installation Directory\" FORCE)\n    else()\n      set(CMAKE_INSTALL_PREFIX \"/usr/local\" CACHE PATH \"Installation Directory\" FORCE)\n    endif()\n  else()\n    # any cross-compiling\n    set(CMAKE_INSTALL_PREFIX \"${CMAKE_BINARY_DIR}/install\" CACHE PATH \"Installation Directory\" FORCE)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Python Tests Configuration File Path in CMake for OpenCV\nDESCRIPTION: Defines the directory and file path for the Python tests configuration file. These are set as cache variables for use in other parts of the build process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/python/test/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(OPENCV_PYTHON_TESTS_CONFIG_FILE_DIR \"${OpenCV_BINARY_DIR}\" CACHE INTERNAL \"\")\nset(OPENCV_PYTHON_TESTS_CONFIG_FILE \"${OPENCV_PYTHON_TESTS_CONFIG_FILE_DIR}/opencv_python_tests.cfg\" CACHE INTERNAL \"\")\n```\n\n----------------------------------------\n\nTITLE: Installing AArch64 Development Libraries (Bash)\nDESCRIPTION: Installs development packages for FFmpeg (libavcodec, libavformat, libavutil, libswscale), FreeType (libfreetype), and HarfBuzz (libharfbuzz) specifically for the `arm64` target architecture onto the host system. The `:arm64` suffix instructs `apt` to install the package for the specified foreign architecture.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y \\\n    libavcodec-dev:arm64 \\\n    libavformat-dev:arm64 \\\n    libavutil-dev:arm64 \\\n    libswscale-dev:arm64 \\\n    libfreetype-dev:arm64 \\\n    libharfbuzz-dev:arm64\n```\n\n----------------------------------------\n\nTITLE: Checking Foreign Architectures with DPKG (Bash)\nDESCRIPTION: Lists the foreign architectures that have been added and are currently supported by `dpkg` on the host system. The example output shows that `arm64` and `armhf` have been successfully added.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nsudo dpkg --print-foreign-architectures\narm64\narmhf\n```\n\n----------------------------------------\n\nTITLE: Comparison Operations with Vector Registers in C++\nDESCRIPTION: Illustrates comparison operations between vector registers, showing how true/false values are represented in different bit widths.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/univ_intrin/univ_intrin.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n// let us consider the following code is run in a 128-bit register\nv_uint8 a;                               // a = {0, 1, 2, ..., 15}\nv_uint8 b;                               // b = {15, 14, 13, ..., 0}\n\nv_uint8 c = a < b;\n\n/*\n    let us look at the first 4 values in binary\n\n    a = |00000000|00000001|00000010|00000011|\n    b = |00001111|00001110|00001101|00001100|\n    c = |11111111|11111111|11111111|11111111|\n\n    If we store the values of c and print them as integers, we will get 255 for true values and 0 for false values.\n*/\n---\n// In a computer supporting 256-bit registers\nv_int32 a;                               // a = {1, 2, 3, 4, 5, 6, 7, 8}\nv_int32 b;                               // b = {8, 7, 6, 5, 4, 3, 2, 1}\n\nv_int32 c = (a < b);                     // c = {-1, -1, -1, -1, 0, 0, 0, 0}\n\n/*\n    The true values are 0xffffffff, which in signed 32-bit integer representation is equal to -1.\n*/\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Branching Logic in OpenCV\nDESCRIPTION: Complex nested conditional logic for determining corner pixels in an image. Uses pointer arithmetic to compare pixel values against threshold values (cb and c_b) and navigates to appropriate labels (is_a_corner or is_not_a_corner) based on the comparison results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_33\n\nLANGUAGE: C\nCODE:\n```\ngoto is_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset7] > cb)\n    if(ptr[offset9] < c_b)\n      if(ptr[offset1] > cb)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] > cb)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                if(ptr[offset3] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset8] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  if(ptr[offset3] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset8] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  if(ptr[offset3] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset8] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      if(ptr[offset9] > cb)\n        if(ptr[offset1] > cb)\n          if(ptr[offset6] < c_b)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset3] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset10] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset1] < c_b)\n            if(ptr[offset6] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset4] < c_b)\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset4] < c_b)\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset8] > cb)\n                      if(ptr[offset11] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    if(ptr[offset3] < c_b)\n                      if(ptr[offset11] < c_b)\n                        if(ptr[offset10] < c_b)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset10] > cb)\n                      if(ptr[offset4] > cb)\n                        goto is_a_corner;\n                      else\n                        if(ptr[offset11] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      if(ptr[offset3] > cb)\n                        if(ptr[offset4] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    if(ptr[offset10] < c_b)\n                      if(ptr[offset11] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              goto is_not_a_corner;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset8] > cb)\n                  if(ptr[offset4] > cb)\n                    if(ptr[offset3] > cb)\n                      goto is_a_corner;\n                    else\n                      if(ptr[offset10] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset10] > cb)\n                      if(ptr[offset11] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n      else\n        if(ptr[offset1] > cb)\n          goto is_not_a_corner;\n        else\n          if(ptr[offset1] < c_b)\n```\n\n----------------------------------------\n\nTITLE: Including OpenJPEG Library Subdirectory Build in CMake\nDESCRIPTION: Uses the `add_subdirectory` command to include and process the `CMakeLists.txt` file located in the `openjp2` subdirectory. This is where the actual source files for the OpenJPEG library are compiled and the library target (`libopenjp2`) is defined.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(openjp2)\n```\n\n----------------------------------------\n\nTITLE: Generating Radon Checkerboard Pattern using Python Script (Shell)\nDESCRIPTION: Executes 'gen_pattern.py' to generate a Radon checkerboard pattern ('radon_checkerboard') suitable for 'findChessboardCornersSB()', saving it to 'radon_checkerboard.svg'. The board has 10 rows, 15 columns, a square size of 12.1 units (default mm), and includes specific markers at cell coordinates (7,4), (7,5), and (8,5). Requires Python and the 'gen_pattern.py' script.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_pattern/camera_calibration_pattern.markdown#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython gen_pattern.py -o radon_checkerboard.svg --rows 10 --columns 15 --type radon_checkerboard -s 12.1 -m 7 4 7 5 8 5\n```\n\n----------------------------------------\n\nTITLE: Transforming and Recognizing Text with OpenCV C++\nDESCRIPTION: This C++ snippet demonstrates the transformation and cropping of text from an image, followed by text recognition using OpenCV's DNN module. It requires OpenCV libraries, particularly the DNN module, and processes an input image to extract and recognize text. 'recInput' and 'vertices' are used for transformation while 'cropped' holds the result. The recognized text is stored in 'recResult'.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_text_spotting/dnn_text_spotting.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n\n    // Transform and Crop\n    Mat cropped;\n    fourPointsTransform(recInput, vertices, cropped);\n\n    String recResult = recognizer.recognize(cropped);\n\n```\n\n----------------------------------------\n\nTITLE: Assigning OpenJPEG Target to Solution Folder in CMake\nDESCRIPTION: Conditionally sets the `FOLDER` property for the OpenJPEG library target to \"3rdparty\" if the `ENABLE_SOLUTION_FOLDERS` option is enabled in CMake. This helps organize targets within IDEs like Visual Studio.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(${OPENJPEG_LIBRARY_NAME}\n    PROPERTIES\n      FOLDER \"3rdparty\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Platform Detection and Base Configuration\nDESCRIPTION: Detects ARM/AARCH64 platforms and sets up basic compiler flags including unwind tables for better debugging.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/carotene/hal/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION ${MIN_VER_CMAKE} FATAL_ERROR)\n\ninclude(CheckCCompilerFlag)\ninclude(CheckCXXCompilerFlag)\n\nset(TEGRA_HAL_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\")\nset(CAROTENE_DIR \"${TEGRA_HAL_DIR}/../\")\n\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"^(arm.*|ARM.*)\")\n  set(ARM TRUE)\nelseif (CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64.*|AARCH64.*\")\n  set(AARCH64 TRUE)\nendif()\n\nocv_warnings_disable(CMAKE_CXX_FLAGS -Wunused-function)\n```\n\n----------------------------------------\n\nTITLE: Building Halide on Linux using CMake and Make\nDESCRIPTION: This snippet provides the steps to build Halide on a Linux system, using CMake and Make. This process compiles Halide with dependencies on the pre-installed LLVM library without additional tests or examples.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_halide/dnn_halide.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ncd halide_root\nmkdir build && cd build\ncmake -DLLVM_DIR=llvm_root/build/lib/cmake/llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_VERSION=40 -DWITH_TESTS=OFF -DWITH_APPS=OFF -DWITH_TUTORIALS=OFF ..\nmake -j4\n```\n\n----------------------------------------\n\nTITLE: Efficient Pixel Assignment with Pointers in OpenCV\nDESCRIPTION: Illustrates the use of C-style pointer access for scanning and modifying pixels in an OpenCV image matrix, emphasizing contiguous memory efficiency when accessing color image channels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_scan_images/how_to_scan_images.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_scan_images.cpp scan-c\n```\n\n----------------------------------------\n\nTITLE: Register Custom Cropping Layer OpenCV Python\nDESCRIPTION: Registers a Python-implemented custom cropping layer to override existing behavior in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n@snippet dnn/edge_detection.py Register\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenCL with OpenGL Sharing\nDESCRIPTION: C++ code snippet for initializing OpenCL with OpenGL sharing capability. This is necessary for direct sharing of textures between OpenGL and OpenCL without copying data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n// init_opencl\n```\n\n----------------------------------------\n\nTITLE: Creating Thrust End Iterator for GpuMat\nDESCRIPTION: Defines a function to create a Thrust iterator that points to the end of a GpuMat, marking the boundary for algorithms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-thrust-interop/gpu_thrust_interop.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ntemplate<typename T>\nthrust::transform_iterator<step_functor<T>, thrust::counting_iterator<int> > end_itr(const cv::cuda::GpuMat& mat)\n{\n    return thrust::make_transform_iterator(\n        thrust::counting_iterator<int>(mat.cols * mat.rows),\n        step_functor<T>(mat.cols, mat.rows, mat.step / sizeof(T))\n    );\n}\n```\n\n----------------------------------------\n\nTITLE: Obtaining Image Corner Coordinates Using gdalinfo (Bash)\nDESCRIPTION: This command-line snippet demonstrates the use of the gdalinfo utility to extract metadata from a geospatial raster file, including its size, coordinate system, and precise corner/center coordinates. This is crucial for aligning imagery with elevation data. Requires gdalinfo (from the GDAL package) and an input raster file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/raster_io_gdal.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n\\f$> gdalinfo N37W123.hgt\n\n   Driver: SRTMHGT/SRTMHGT File Format\n   Files: N37W123.hgt\n   Size is 3601, 3601\n   Coordinate System is:\n   GEOGCS[\"WGS 84\",\n   DATUM[\"WGS_1984\",\n\n   ... more output ...\n\n   Corner Coordinates:\n   Upper Left  (-123.0001389,  38.0001389) (123d 0' 0.50\\\"W, 38d 0' 0.50\\\"N)\n   Lower Left  (-123.0001389,  36.9998611) (123d 0' 0.50\\\"W, 36d59'59.50\\\"N)\n   Upper Right (-121.9998611,  38.0001389) (121d59'59.50\\\"W, 38d 0' 0.50\\\"N)\n   Lower Right (-121.9998611,  36.9998611) (121d59'59.50\\\"W, 36d59'59.50\\\"N)\n   Center      (-122.5000000,  37.5000000) (122d30' 0.00\\\"W, 37d30' 0.00\\\"N)\n\n    ... more output ...\n```\n\n----------------------------------------\n\nTITLE: Training SVM Model\nDESCRIPTION: Trains the SVM model using the prepared training data and parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_svm.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nsvm->train(trainingDataMat, ROW_SAMPLE, labelsMat);\n```\n\n----------------------------------------\n\nTITLE: Declaring Neural Network Topologies using G_API_NET in G-API\nDESCRIPTION: Code snippet showing how to declare different neural network topologies (face detection, age/gender recognition, and emotion recognition) using the G_API_NET macro in G-API. Each network is defined with a type name, function signature, and unique topology name.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/interactive_face_detection/interactive_face_detection.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nG_API_NET(Faces, <cv::GMat(cv::GMat)>, \"face-detector\");\nG_API_NET(AgeGender, <std::tuple<cv::GMat,cv::GMat>(cv::GMat)>, \"age-gender-recognizer\");\nG_API_NET(Emotions, <cv::GMat(cv::GMat)>, \"emotions-recognizer\");\n```\n\n----------------------------------------\n\nTITLE: Image Loading and Border Initialization\nDESCRIPTION: Loading the source image and initializing border dimensions as 5% of image size.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/copyMakeBorder/copyMakeBorder.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nint top = (int) (0.05*src.rows);\nint bottom = top;\nint left = (int) (0.05*src.cols);\nint right = left;\n```\n\nLANGUAGE: Java\nCODE:\n```\nint top = (int) (0.05*src.rows());\nint bottom = top;\nint left = (int) (0.05*src.cols());\nint right = left;\n```\n\nLANGUAGE: Python\nCODE:\n```\ntop = int(0.05 * src.shape[0])\nbottom = top\nleft = int(0.05 * src.shape[1])\nright = left\n```\n\n----------------------------------------\n\nTITLE: Training SVM Model with Non-Linearly Separable Data (Python)\nDESCRIPTION: Trains the SVM model in Python using the prepared non-linearly separable training data. The training process uses high iteration counts to handle the complexity of finding optimal decision boundaries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n# Train the SVM\nprint('\\nTraining the SVM...')\nsvm.train(completeTrainData, cv.ml.ROW_SAMPLE, completeTrainLabels)\nprint('Finished training')\n```\n\n----------------------------------------\n\nTITLE: Implementing Image Erosion with OpenCV Python\nDESCRIPTION: Demonstrates how to perform erosion on an image using a 5x5 kernel. Erosion reduces the boundaries of foreground objects and is useful for removing small white noise.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\n\nimg = cv.imread('j.png', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nkernel = np.ones((5,5),np.uint8)\nerosion = cv.erode(img,kernel,iterations = 1)\n```\n\n----------------------------------------\n\nTITLE: Defining and Configuring the IlmImf Target Library\nDESCRIPTION: This code creates the IlmImf static library target with the collected source files. It configures output properties, links against zlib, sets up installation rules, and handles folders for solution organization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openexr/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(IlmImf STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${lib_hdrs} ${lib_srcs})\ntarget_link_libraries(IlmImf ${ZLIB_LIBRARIES})\n\nset_target_properties(IlmImf\n    PROPERTIES\n    OUTPUT_NAME \"IlmImf\"\n    DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n    COMPILE_PDB_NAME \"IlmImf\"\n    COMPILE_PDB_NAME_DEBUG \"IlmImf${OPENCV_DEBUG_POSTFIX}\"\n    ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n    )\n\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(IlmImf PROPERTIES FOLDER \"3rdparty\")\nendif()\n\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(IlmImf EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev OPTIONAL)\nendif()\n\nocv_install_3rdparty_licenses(openexr LICENSE AUTHORS.ilmbase AUTHORS.openexr)\n\nset(OPENEXR_INCLUDE_PATHS ${OPENEXR_INCLUDE_PATHS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Converting Image to Grayscale\nDESCRIPTION: Converting the source image to grayscale for circle detection preprocessing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_circle/hough_circle.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nMat gray;\ncvtColor(src, gray, COLOR_BGR2GRAY);\n```\n\nLANGUAGE: Java\nCODE:\n```\nMat gray = new Mat();\nImgproc.cvtColor(src, gray, Imgproc.COLOR_BGR2GRAY);\n```\n\nLANGUAGE: Python\nCODE:\n```\ngray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n```\n\n----------------------------------------\n\nTITLE: Multi-frame Video Denoising with OpenCV Python\nDESCRIPTION: Shows how to implement video denoising using cv.fastNlMeansDenoisingMulti() which processes multiple frames to remove noise. The example includes creating synthetic noise and applying denoising across temporal frames.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_photo/py_non_local_means/py_non_local_means.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\ncap = cv.VideoCapture('vtest.avi')\n\n# create a list of first 5 frames\nimg = [cap.read()[1] for i in range(5)]\n\n# convert all to grayscale\ngray = [cv.cvtColor(i, cv.COLOR_BGR2GRAY) for i in img]\n\n# convert all to float64\ngray = [np.float64(i) for i in gray]\n\n# create a noise of variance 25\nnoise = np.random.randn(*gray[1].shape)*10\n\n# Add this noise to images\nnoisy = [i+noise for i in gray]\n\n# Convert back to uint8\nnoisy = [np.uint8(np.clip(i,0,255)) for i in noisy]\n\n# Denoise 3rd frame considering all the 5 frames\ndst = cv.fastNlMeansDenoisingMulti(noisy, 2, 5, None, 4, 7, 35)\n\nplt.subplot(131),plt.imshow(gray[2],'gray')\nplt.subplot(132),plt.imshow(noisy[2],'gray')\nplt.subplot(133),plt.imshow(dst,'gray')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Implementing Gamma Correction with Look-Up Table in C++ using OpenCV\nDESCRIPTION: This C++ code snippet demonstrates gamma correction on an image using OpenCV. It pre-calculates the gamma transformation for all 256 possible pixel values using the formula O = ((I/255)^gamma) * 255 and stores them in a look-up table (`lookUpTable`) for efficiency. The `LUT` function then applies this table to the input image to produce the gamma-corrected output image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/basic_linear_transform/basic_linear_transform.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n//! [changing-contrast-brightness-gamma-correction]\n    Mat lookUpTable(1, 256, CV_8U);\n    uchar* p = lookUpTable.ptr();\n    for( int i = 0; i < 256; ++i)\n        p[i] = saturate_cast<uchar>(pow(i / 255.0, gamma_) * 255.0);\n\n    Mat res = img_original.clone();\n    LUT(img_original, lookUpTable, res);\n//! [changing-contrast-brightness-gamma-correction]\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Pixel Comparison Logic in C++\nDESCRIPTION: This code snippet implements the core pixel comparison logic for the FAST corner detection algorithm. It evaluates surrounding pixels (referenced by offset positions) against brightness thresholds (c_b and cb) to determine if a point is a corner feature, using a series of nested conditional checks.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_31\n\nLANGUAGE: C++\nCODE:\n```\ncontinue; // goto homogeneous;\n                            else\n                              continue; // goto homogeneous;\n                        else\n                        if(ptr[offset7] < c_b)\n                          if(ptr[offset14] > cb)\n                            if(ptr[offset15] > cb)\n                              if(ptr[offset1] > cb)\n                                if(ptr[offset3] > cb)\n                                  if(ptr[offset6] > cb)\n                                    {} // goto success_homogeneous;\n                                  else\n                                    if(ptr[offset13] > cb)\n                                      {} // goto success_homogeneous;\n                                    else\n                                      continue; // goto homogeneous;\n                                else\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      if(ptr[offset12] > cb)\n                                        if(ptr[offset13] > cb)\n                                          {} // goto success_homogeneous;\n                                        else\n                                          continue; // goto homogeneous;\n                                      else\n                                        continue; // goto homogeneous;\n                                    else\n                                      continue; // goto homogeneous;\n                                  else\n                                    continue; // goto homogeneous;\n                              else\n                                if(ptr[offset8] > cb)\n                                  if(ptr[offset9] > cb)\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        if(ptr[offset12] > cb)\n                                          if(ptr[offset13] > cb)\n                                            {} // goto success_homogeneous;\n                                          else\n                                            continue; // goto homogeneous;\n                                        else\n                                          continue; // goto homogeneous;\n                                      else\n                                        continue; // goto homogeneous;\n                                    else\n                                      continue; // goto homogeneous;\n                                  else\n                                    continue; // goto homogeneous;\n                                else\n                                  continue; // goto homogeneous;\n                            else\n                              continue; // goto homogeneous;\n                          else\n                          if(ptr[offset14] < c_b)\n                            if(ptr[offset8] < c_b)\n                              if(ptr[offset9] < c_b)\n                                if(ptr[offset10] < c_b)\n                                  if(ptr[offset11] < c_b)\n                                    if(ptr[offset12] < c_b)\n                                      if(ptr[offset13] < c_b)\n                                        if(ptr[offset6] < c_b)\n                                          {} // goto success_homogeneous;\n                                        else\n                                          if(ptr[offset15] < c_b)\n                                            {} // goto success_homogeneous;\n                                          else\n                                            continue; // goto homogeneous;\n                                      else\n                                        continue; // goto homogeneous;\n                                    else\n                                      continue; // goto homogeneous;\n                                  else\n                                    continue; // goto homogeneous;\n                                else\n                                  continue; // goto homogeneous;\n                              else\n                                continue; // goto homogeneous;\n                            else\n                              continue; // goto homogeneous;\n                          else\n                            continue; // goto homogeneous;\n                        else\n                          if(ptr[offset14] > cb)\n                            if(ptr[offset15] > cb)\n                              if(ptr[offset1] > cb)\n                                if(ptr[offset3] > cb)\n                                  if(ptr[offset6] > cb)\n                                    {} // goto success_homogeneous;\n                                  else\n                                    if(ptr[offset13] > cb)\n                                      {} // goto success_homogeneous;\n                                    else\n                                      continue; // goto homogeneous;\n                                else\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      if(ptr[offset12] > cb)\n                                        if(ptr[offset13] > cb)\n                                          {} // goto success_homogeneous;\n                                        else\n                                          continue; // goto homogeneous;\n                                      else\n                                        continue; // goto homogeneous;\n                                    else\n                                      continue; // goto homogeneous;\n                                  else\n                                    continue; // goto homogeneous;\n                              else\n                                if(ptr[offset8] > cb)\n                                  if(ptr[offset9] > cb)\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        if(ptr[offset12] > cb)\n                                          if(ptr[offset13] > cb)\n                                            {} // goto success_homogeneous;\n                                          else\n                                            continue; // goto homogeneous;\n                                        else\n                                          continue; // goto homogeneous;\n                                      else\n                                        continue; // goto homogeneous;\n                                    else\n                                      continue; // goto homogeneous;\n                                  else\n                                    continue; // goto homogeneous;\n                                else\n                                  continue; // goto homogeneous;\n                            else\n                              continue; // goto homogeneous;\n                          else\n                            continue; // goto homogeneous;\n                      else\n                      if(ptr[offset5] < c_b)\n                        if(ptr[offset12] > cb)\n                          if(ptr[offset13] > cb)\n                            if(ptr[offset14] > cb)\n                              if(ptr[offset15] > cb)\n                                if(ptr[offset1] > cb)\n                                  if(ptr[offset3] > cb)\n                                    {} // goto success_homogeneous;\n                                  else\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        {} // goto success_homogeneous;\n                                      else\n                                        continue; // goto homogeneous;\n                                    else\n                                      continue; // goto homogeneous;\n                                else\n                                  if(ptr[offset8] > cb)\n                                    if(ptr[offset9] > cb)\n                                      if(ptr[offset10] > cb)\n                                        if(ptr[offset11] > cb)\n                                          {} // goto success_homogeneous;\n                                        else\n                                          continue; // goto homogeneous;\n                                      else\n                                        continue; // goto homogeneous;\n                                    else\n                                      continue; // goto homogeneous;\n                                  else\n                                    continue; // goto homogeneous;\n                              else\n                                if(ptr[offset6] > cb)\n                                  if(ptr[offset7] > cb)\n                                    if(ptr[offset8] > cb)\n                                      if(ptr[offset9] > cb)\n                                        if(ptr[offset10] > cb)\n                                          if(ptr[offset11] > cb)\n                                            {} // goto success_homogeneous;\n                                          else\n                                            continue; // goto homogeneous;\n                                        else\n                                          continue; // goto homogeneous;\n                                      else\n                                        continue; // goto homogeneous;\n                                    else\n                                      continue; // goto homogeneous;\n                                  else\n                                    continue; // goto homogeneous;\n                                else\n```\n\n----------------------------------------\n\nTITLE: Running TensorFlow Classification Evaluation via OpenCV dnn CLI (Console)\nDESCRIPTION: Demonstrates the command-line usage for running the dnn_model_runner evaluation pipeline in Python. This call reads the specified TensorFlow classification model into an OpenCV cv.dnn_Net and evaluates its accuracy, inference time, and L1 metrics. Dependencies include a valid Python environment and the dnn_model_runner module. Parameters such as <tf_cls_model_name> should be replaced with the actual model name. Outputs include log files with metrics and charts visualizing inference results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls --model_name <tf_cls_model_name>\n```\n\n----------------------------------------\n\nTITLE: Computing Normalized Image Points in OpenCV C++\nDESCRIPTION: Code to convert image corner points to normalized image coordinates by undistorting them using camera intrinsic parameters. This transforms points from pixel coordinates to the normalized camera coordinate system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nstd::vector<cv::Point2f> imagePointsNormalized;\ncv::undistortPoints(corners, imagePointsNormalized, cameraMatrix, distCoeffs);\n```\n\n----------------------------------------\n\nTITLE: Computing Homography from Camera Displacement using OpenCV C++\nDESCRIPTION: The code snippet demonstrates how to compute the homography matrix from two camera poses. It leverages OpenCV's functions to perform multiple interpolated warping operations. The focus is on computing N intermediate homographies, enabling visualization of transformations through a series of warps. Key parameters include camera poses, image sources, and the number of interpolations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_26\n\nLANGUAGE: C++\nCODE:\n```\n@snippet decompose_homography.cpp compute-homography-from-camera-displacement\n```\n\n----------------------------------------\n\nTITLE: Calculating Image Moments using OpenCV C++\nDESCRIPTION: This snippet demonstrates the use of OpenCV's moments, contourArea, and arcLength functions in C++. These functions are used to compute various shape descriptors from an image. Required dependencies include OpenCV 3.0 or higher. The input is an image from which contours are extracted, and the output includes calculated moments and related data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/moments/moments.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\n// Original code can be found at the mentioned OpenCV repository\n```\n\n----------------------------------------\n\nTITLE: Detecting Build Context in CMake\nDESCRIPTION: This CMake code block checks if the current list directory (`CMAKE_CURRENT_LIST_DIR`) is different from the main CMake source directory (`CMAKE_SOURCE_DIR`). This condition is true when the samples are being built as part of the main OpenCV project, triggering the first block of logic. The `else()` block handles the case where the samples are being built standalone (e.g., from an installed samples directory).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_LIST_DIR)\n#===================================================================================================\n#\n# Build as part of OpenCV\n#\n#===================================================================================================\n\n  ...\n\nelse()\n#===================================================================================================\n#\n#  Standalone mode\n#\n#===================================================================================================\n\n  ...\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Trackbar Callback Function in C++\nDESCRIPTION: This C++ snippet defines the `on_trackbar` callback function. It calculates the `alpha` (0.0-1.0) and `beta` values based on the integer trackbar position (`alpha_slider`), performs weighted image blending using `addWeighted`, and updates the \"Linear Blend\" window display using `imshow`. `src1`, `src2`, `dst`, `alpha`, `beta`, and `alpha_slider` are assumed to be accessible (e.g., global variables).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n//![on_trackbar]\n/**\n * @function on_trackbar\n * @brief Callback for trackbar\n */\nvoid on_trackbar( int, void* )\n{\n alpha = (double) alpha_slider/alpha_slider_max ;\n beta = ( 1.0 - alpha );\n addWeighted( src1, alpha, src2, beta, 0.0, dst);\n imshow( \"Linear Blend\", dst );\n}\n//![on_trackbar]\n```\n\n----------------------------------------\n\nTITLE: Applying Convolution and Normalization to Backprojection Result in Python OpenCV\nDESCRIPTION: This code performs convolution with a circular disc kernel on the backprojection result to improve object detection. It then normalizes the values to 0-255 range to create a clearer probability map of the object location.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_backprojection/py_histogram_backprojection.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndisc = cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))\ncv.filter2D(B,-1,disc,B)\nB = np.uint8(B)\ncv.normalize(B,B,0,255,cv.NORM_MINMAX)\n```\n\n----------------------------------------\n\nTITLE: Configuring Video Source for G-API Pipeline\nDESCRIPTION: Demonstrates setting up a video input source for the G-API pipeline using GCaptureSource. Creates a stream source from either a video file or camera input.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/interactive_face_detection/interactive_face_detection.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<cv::gapi::wip::IStreamSource> cap;\nif (!cmd.get<std::string>(\"input\").empty()) {\n    cap = std::make_shared<cv::gapi::wip::GCaptureSource>(cmd.get<std::string>(\"input\"));\n} else {\n    cap = std::make_shared<cv::gapi::wip::GCaptureSource>(0);\n}\n```\n\n----------------------------------------\n\nTITLE: Homography Check for Matches with OpenCV in Java\nDESCRIPTION: Java snippet using OpenCV to check if matches fit a homography model, ensuring only inliers are used in the result set.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nsamples/java/tutorial_code/features2D/akaze_matching/AKAZEMatchDemo.java homography check\n```\n\n----------------------------------------\n\nTITLE: Calculating SSIM on CPU in C++\nDESCRIPTION: Defines a C++ function `getMSSIM` that calculates the Mean Structural Similarity Index (MSSIM) between two input images (`I1`, `I2`) using CPU-based OpenCV functions. It calls the `ssim` function (defined elsewhere) to perform the core calculation and returns the result as an OpenCV `Scalar` object, typically containing MSSIM values for each color channel.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n//![getssim]\nScalar getMSSIM( const Mat& I1, const Mat& I2)\n{\n    return ssim(I1, I2);\n}\n//![getssim]\n```\n\n----------------------------------------\n\nTITLE: Accessing Single Pixel Intensity Value in Grayscale Image with OpenCV in C++\nDESCRIPTION: Shows how to access a pixel's intensity in an 8-bit single channel image (cv::Mat) in C++. The at<>() method is used with row and column indices: first y (row), then x (column). Requires OpenCV. Returns intensity in the range 0-255. Input coordinates must be within image bounds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nuchar intensity = img.at<uchar>(y, x);\n```\n\n----------------------------------------\n\nTITLE: Defining ArUco Dictionary Structure in C++\nDESCRIPTION: This snippet shows the structure of the cv::aruco::Dictionary class, which contains the parameters for defining an ArUco marker dictionary. It includes the bytesList for marker code information, markerSize for the number of bits per dimension, and maxCorrectionBits for error correction.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_detection/aruco_detection.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n    class Dictionary {\n    public:\n\n        cv::Mat bytesList;      // marker code information\n        int markerSize;         // number of bits per dimension\n        int maxCorrectionBits;  // maximum number of bits that can be corrected\n\n        ...\n\n    }\n```\n\n----------------------------------------\n\nTITLE: Exporting OpenJPEG Build Information to Parent Scope in CMake\nDESCRIPTION: Sets several CMake variables (`OPENJPEG_LIBRARIES`, `OPENJPEG_VERSION`, version components, `OPENJPEG_INCLUDE_DIRS`) in the `PARENT_SCOPE`. This makes the library name, version details, and include directories (obtained using `get_target_property`) available to the main OpenCV CMake script that included this file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\n# Setting all necessary variables\nset(OPENJPEG_LIBRARIES     ${OPENJPEG_LIBRARY_NAME}  PARENT_SCOPE)\nset(OPENJPEG_VERSION       ${OPENJPEG_VERSION}       PARENT_SCOPE)\nset(OPENJPEG_MAJOR_VERSION ${OPENJPEG_VERSION_MAJOR} PARENT_SCOPE)\nset(OPENJPEG_MINOR_VERSION ${OPENJPEG_VERSION_MINOR} PARENT_SCOPE)\nset(OPENJPEG_BUILD_VERSION ${OPENJPEG_VERSION_BUILD} PARENT_SCOPE)\nget_target_property(_openjpeg_include_dirs ${OPENJPEG_LIBRARY_NAME} INCLUDE_DIRECTORIES)\nset(OPENJPEG_INCLUDE_DIRS  ${_openjpeg_include_dirs} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Extracting Frozen DeepLab Graph in Python\nDESCRIPTION: This Python function retrieves the pre-trained DeepLabV3 model from a specified URL and extracts the frozen TensorFlow graph from the downloaded archive. It requires 'urllib' for downloading and 'tarfile' for extraction. The key purpose is to prepare the model for graph optimization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef get_deeplab_frozen_graph():\n    # define model path to download\n    models_url = 'http://download.tensorflow.org/models/'\n    mobilenetv2_voctrainval = 'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz'\n\n    # construct model link to download\n    model_link = models_url + mobilenetv2_voctrainval\n\n    try:\n        urllib.request.urlretrieve(model_link, mobilenetv2_voctrainval)\n    except Exception:\n        print(\"TF DeepLabV3 was not retrieved: {}\".format(model_link))\n        return\n\n    tf_model_tar = tarfile.open(mobilenetv2_voctrainval)\n\n    # iterate the obtained model archive\n    for model_tar_elem in tf_model_tar.getmembers():\n        # check whether the model archive contains frozen graph\n        if TF_FROZEN_GRAPH_NAME in os.path.basename(model_tar_elem.name):\n            # extract frozen graph\n            tf_model_tar.extract(model_tar_elem, FROZEN_GRAPH_PATH)\n\n    tf_model_tar.close()\n```\n\n----------------------------------------\n\nTITLE: Extracting and Displaying SVM Support Vectors - Java\nDESCRIPTION: This Java code fetches support vectors from a trained SVM using OpenCV Java methods and marks them visually on the output. It depends on a trained OpenCV SVM model and the Java API. The input includes the SVM model and data points; output consists of custom rendering (e.g., gray rings) around points that are support vectors. This clarifies which training examples significantly affect the SVM boundary.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_svm.markdown#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\n// Java: Display SVM support vectors using OpenCV\n// ... (full code from samples/java/tutorial_code/ml/introduction_to_svm/IntroductionToSVMDemo.java, show_vectors)\n```\n\n----------------------------------------\n\nTITLE: Reading Image Data from HTML Canvas in JavaScript\nDESCRIPTION: This code snippet demonstrates how to retrieve image data from an HTML canvas element using JavaScript. It requires a canvas element with an ID, accesses its 2D context, and uses the getImageData method to extract pixel data into an ImageData object.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_image_display/js_image_display.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet canvas = document.getElementById(canvasInputId);\nlet ctx = canvas.getContext('2d');\nlet imgData = ctx.getImageData(0, 0, canvas.width, canvas.height);\n```\n\n----------------------------------------\n\nTITLE: Configuring Installation for OpenCV Java Library in CMake\nDESCRIPTION: Sets up installation rules for the OpenCV Java library, handling different components and destinations based on the build configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jni/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nset(__install_export \"\")\nif(BUILD_FAT_JAVA_LIB)\n  set(__install_export EXPORT OpenCVModules)\nendif()\n\nocv_install_target(${the_module} OPTIONAL ${__install_export}\n    RUNTIME DESTINATION ${OPENCV_JNI_BIN_INSTALL_PATH} COMPONENT java\n    LIBRARY DESTINATION ${OPENCV_JNI_INSTALL_PATH} COMPONENT java\n    ARCHIVE DESTINATION ${OPENCV_JNI_INSTALL_PATH} COMPONENT java\n)\n```\n\n----------------------------------------\n\nTITLE: Defining and Adding the OpenCV Java Module in CMake\nDESCRIPTION: This snippet first sets a variable `the_description` to describe the module. Then, it uses the `ocv_add_module` command to formally define the `java` module as a BINDINGS module. It declares dependencies on `opencv_core` and `opencv_imgproc`, and specifies a private requirement on `opencv_java_bindings_generator`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(the_description \"The java bindings\")\nocv_add_module(java BINDINGS opencv_core opencv_imgproc PRIVATE_REQUIRED opencv_java_bindings_generator)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenJPEG Library Build\nDESCRIPTION: Configures the OpenJPEG library build with compiler warnings, static library definition, and linking options. Sets up the target properties and includes necessary directories.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/openjp2/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nocv_warnings_disable(CMAKE_C_FLAGS\n    -Wundef -Wstrict-prototypes -Wcast-function-type\n    -Wshadow   # v2.4.0: GCC\n    -Wunused-function   # v2.4.0: Clang\n)\n\nocv_warnings_disable(CMAKE_C_FLAGS /wd4819) # vs2019 Win64\n\nadd_library(${OPENJPEG_LIBRARY_NAME} STATIC ${OPENJPEG_SRCS})\n\ntarget_compile_definitions(${OPENJPEG_LIBRARY_NAME} PUBLIC OPJ_STATIC)\n\nocv_include_directories(\"${CMAKE_CURRENT_LIST_DIR}\" \"${CMAKE_CURRENT_BINARY_DIR}\")\n\nif(UNIX)\n  target_link_libraries(${OPENJPEG_LIBRARY_NAME} PRIVATE m)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding SBT Eclipse Plugin (Scala)\nDESCRIPTION: This Scala code snippet, placed in `project/plugins.sbt`, adds the `sbteclipse-plugin` to the SBT build. This plugin provides the `eclipse` command within SBT, which automatically generates Eclipse IDE project configuration files based on the SBT build definition.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_13\n\nLANGUAGE: scala\nCODE:\n```\naddSbtPlugin(\"com.typesafe.sbteclipse\" % \"sbteclipse-plugin\" % \"2.1.0\")\n```\n\n----------------------------------------\n\nTITLE: Eye Contour Generation from Facial Landmarks\nDESCRIPTION: Function that generates a contour for an eye region by approximating an ellipse based on the eye corner landmarks, creating a realistic eye region mask.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n// Helper functions for landmarks post-processing\ninline double getLineInclinationAngleDegrees(const cv::Point& p1, const cv::Point& p2) {\n    auto diff = p2 - p1;\n    return -atan2(diff.y, diff.x) * 180.0 / CV_PI;\n}\n\n// Generate a contour for an eye based on two landmark points\ninline Contour getEyeContour(const std::vector<cv::Point>& landmarks) {\n    // landmarks is a vector of two points marking the left and right corners of an eye\n    GAPI_Assert(landmarks.size() == 2);\n    \n    // Sort points from left to right\n    auto pts = landmarks;\n    if (pts[0].x > pts[1].x) std::swap(pts[0], pts[1]);\n    \n    // Calculate ellipse parameters\n    cv::Point center((pts[0].x + pts[1].x) / 2, (pts[0].y + pts[1].y) / 2);\n    int axis_x = (pts[1].x - pts[0].x) / 2;\n    int axis_y = axis_x / 3;  // Assume eye height is 1/3 of width\n    double angle = getLineInclinationAngleDegrees(pts[0], pts[1]);\n    \n    // Generate eye contour as a half-ellipse\n    std::vector<cv::Point> contour;\n    cv::ellipse2Poly(center, cv::Size(axis_x, axis_y), angle, 0, 180, 5, contour);\n    return contour;\n}\n```\n\n----------------------------------------\n\nTITLE: Using ArUco Detector Methods in OpenCV\nDESCRIPTION: Key method calls for marker detection and refinement in the ArUco module, including marker detection and refinement for boards.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_faq/aruco_faq.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ncv::aruco::ArucoDetector::detectMarkers()\ncv::aruco::ArucoDetector::refineDetectedMarkers()\n```\n\n----------------------------------------\n\nTITLE: Processing Common Source and Template Files for Java Bindings in CMake\nDESCRIPTION: Gathers common dependency files required for Java bindings generation using `file(GLOB_RECURSE)`. This includes files from the generator's `src` directory, Android-specific directories, and `templates`. It then calls the `ocv_remap_files` macro to process any files ending in `.in` found within this list, performing variable substitutions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/generator/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\n# common files\nfile(GLOB_RECURSE deps \"${CMAKE_CURRENT_SOURCE_DIR}/src/*\" \"${CMAKE_CURRENT_SOURCE_DIR}/android*/*\" \"${CMAKE_CURRENT_SOURCE_DIR}/templates/*\")\nocv_remap_files(deps)\n```\n\n----------------------------------------\n\nTITLE: Exporting YOLOv10 Model and Environment Setup Using Bash\nDESCRIPTION: Describes commands to clone a specialized YOLOv10 repository fork with pre-modified postprocessing, create a Python environment via conda, install dependencies, and export the YOLOv10 PyTorch model to ONNX format using a custom export script. Prerequisites include git, conda, pip for requirements, and the referenced forked YOLOv10 repository. Key parameters are --model (model variant) and --imgsz (model input image size). Produces an ONNX file (e.g., yolov10s.onnx) for OpenCV inference, and defaults are provided for typical use cases.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_yolo/dnn_yolo.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:Abdurrahheem/yolov10.git\\nconda create -n yolov10 python=3.9\\nconda activate yolov10\\npip install -r requirements.txt\\npython export_opencv.py --model=<model-name> --imgsz=<input-img-size>\n```\n\n----------------------------------------\n\nTITLE: Reading Camera Parameters from XML/YAML using OpenCV in C++\nDESCRIPTION: This code reads camera intrinsics and distortion coefficients from a stored XML/YAML file using OpenCV’s FileStorage class. It assumes that a correct file path is provided and that the file contains the necessary parameters stored under 'camera_matrix' and 'distortion_coefficients' keys.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_square_chess/camera_calibration_square_chess.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nFileStorage fs(filename, FileStorage::READ);\nMat intrinsics, distortion;\nfs[\"camera_matrix\"] >> intrinsics;\nfs[\"distortion_coefficients\"] >> distortion;\n```\n\n----------------------------------------\n\nTITLE: Building and Linking SYCL Sample Executables with OpenCV - CMake\nDESCRIPTION: This part of the script configures the actual compilation and linking process for SYCL-based OpenCV sample executables. It sets global compiler and linker flags based on the selected SYCL implementation, includes necessary directories and modules, enumerates all \\*.cpp samples, and defines them as individual targets. It also adds ComputeCpp-specific build steps if required. This ensures each executable is properly linked against SYCL and all OpenCV modules, with support for SDK-specific build requirements.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/sycl/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nproject(sycl_samples)\n\nif(SYCL_FLAGS)  # \"target_link_libraries(... ${SYCL_TARGET})\" is not enough. Hacking...\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${SYCL_FLAGS}\")\n  set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${SYCL_FLAGS}\")\nendif()\n\nocv_include_modules_recurse(${OPENCV_SYCL_SAMPLES_REQUIRED_DEPS})\nocv_include_directories(${OpenCL_INCLUDE_DIR})\nfile(GLOB all_samples RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} *.cpp)\nforeach(sample_filename ${all_samples})\n  ocv_define_sample(tgt ${sample_filename} sycl)\n  ocv_target_link_libraries(${tgt} PRIVATE\n    ${OPENCV_LINKER_LIBS}\n    ${OPENCV_SYCL_SAMPLES_REQUIRED_DEPS}\n    ${SYCL_TARGET})\n\n  if(COMMAND add_sycl_to_target)  # ComputeCpp\n    add_sycl_to_target(TARGET ${tgt} SOURCES ${sample_filename})\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Defining HTML Trackbar and Associated Text Field - HTML\nDESCRIPTION: This snippet defines an HTML range input (trackbar) and an associated text input for displaying the current trackbar value. The range input includes properties for initial value, minimum, maximum, step, and an oninput callback for event handling. The dependencies are standard HTML support and a callback function named 'callback'. Inputs: User slider interaction. Outputs: Value updated in the text field.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_trackbar/js_trackbar.markdown#2025-04-22_snippet_3\n\nLANGUAGE: HTML\nCODE:\n```\nWeight: <input type=\\\"range\\\" id=\\\"trackbar\\\" value=\\\"50\\\" min=\\\"0\\\" max=\\\"100\\\" step=\\\"1\\\" oninput=\\\"callback()\\\">\n<input type=\\\"text\\\" id=\\\"weightValue\\\" size=\\\"3\\\" value=\\\"50\\\"/>\n```\n\n----------------------------------------\n\nTITLE: Splitting and Merging Image Channels using OpenCV Functions in Python\nDESCRIPTION: Demonstrates splitting a BGR image into its individual Blue, Green, and Red channels using `cv.split()` and then merging them back into a BGR image using `cv.merge()`. Note that `cv.split()` is mentioned as a potentially costly operation in terms of processing time.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_basic_ops/py_basic_ops.markdown#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> b,g,r = cv.split(img)\n>>> img = cv.merge((b,g,r))\n```\n\n----------------------------------------\n\nTITLE: Creating Standard SBT Project Structure (Bash)\nDESCRIPTION: These Bash commands set up the standard directory structure expected by SBT within the 'JavaSample' directory. `mkdir -p src/main/java` creates the nested directories where Java source files will reside. `mkdir project` creates the directory where SBT build definition files (`.scala`, `.sbt`) are placed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncd JavaSample\nmkdir -p src/main/java # This is where SBT expects to find Java sources\nmkdir project # This is where the build definitions live\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard for OpenCV\nDESCRIPTION: This snippet sets the C++ standard when building OpenCV using `CMAKE_CXX_STANDARD`. OpenCV 4.x defaults to C++11, while 5.x defaults to C++17. Errors occur if the standard is unsupported, but checks can be skipped with `OPENCV_SKIP_CMAKE_CXX_STANDARD`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_CXX_STANDARD=17 ../opencv\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV with Make (Unix) (Bash)\nDESCRIPTION: This Bash command executes the 'make' utility to compile the OpenCV library based on the Makefiles generated by CMake. The '-j8' flag suggests using 8 parallel jobs for potentially faster compilation; adjust the number based on system cores.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake -j8\n```\n\n----------------------------------------\n\nTITLE: Calculating Lookup Table with C++ Stream\nDESCRIPTION: This snippet demonstrates how to convert a command line argument to an integer using C++ stringstream and compute a lookup table for color space reduction. It includes basic C++ operations without specific OpenCV dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_scan_images/how_to_scan_images.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_scan_images.cpp dividewith\n```\n\n----------------------------------------\n\nTITLE: Custom Layer Interp Caffe Implementation\nDESCRIPTION: Defines an Interp custom layer without trainable weights, using hyper-parameters for output size in Caffe.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/custom_layers.hpp InterpLayer\n```\n\n----------------------------------------\n\nTITLE: Exporting Structs as Python Dicts Using OpenCV Macros in C++\nDESCRIPTION: This snippet shows how to export a C++ struct as a native Python dict using CV_EXPORTS_W_MAP. The Moments class encapsulates image moment fields, annotated with CV_PROP_RW to expose members for direct attribute access in Python. The exported instance is presented as a dictionary. Dependencies: none beyond OpenCV core types. All double-precision moment fields can be accessed from Python as dict keys.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_bindings/py_bindings_basics/py_bindings_basics.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nclass CV_EXPORTS_W_MAP Moments\n{\npublic:\n    //! spatial moments\n    CV_PROP_RW double  m00, m10, m01, m20, m11, m02, m30, m21, m12, m03;\n    //! central moments\n    CV_PROP_RW double  mu20, mu11, mu02, mu30, mu21, mu12, mu03;\n    //! central normalized moments\n    CV_PROP_RW double  nu20, nu11, nu02, nu30, nu21, nu12, nu03;\n};\n```\n\n----------------------------------------\n\nTITLE: Accessing 2D Histogram Bin Value in C++\nDESCRIPTION: C++ code example demonstrating how to access the value of a specific bin in a 2D histogram stored in a `cv::Mat`. It uses the `.at<float>(i, j)` method, where `i` and `j` are the indices for the two dimensions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_31\n\nLANGUAGE: cpp\nCODE:\n```\nb_hist.at<float>( i, j )\n```\n\n----------------------------------------\n\nTITLE: Building Only for Mac Catalyst with Specific Architectures - Python/Bash\nDESCRIPTION: This code example shows how to build an OpenCV xcframework only for Mac Catalyst, specifying x86_64 and arm64 architectures. The --build_only_specified_archs flag ensures that only the explicitly listed architectures are included. Python, CMake, and Xcode dependencies apply. Input is a command-line invocation tailored to exclude all but the catalyst build. Output is a customized xcframework in the specified directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/apple/readme.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython build_xcframework.py --out somedir --catalyst_archs x86_64,arm64 --build_only_specified_archs\n```\n\n----------------------------------------\n\nTITLE: Converting Training Data to Mat Format in C++\nDESCRIPTION: Converts training data arrays into OpenCV Mat objects required by the SVM training function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_svm.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nMat trainingDataMat(N, 2, CV_32F, trainingData);\nMat labelsMat(N, 1, CV_32F, labels);\n```\n\n----------------------------------------\n\nTITLE: Initializing and Opening OpenCV VideoWriter (C++)\nDESCRIPTION: Shows the process of initializing a `cv::VideoWriter` object (`outputVideo`). It retrieves the frame width and height from an existing `VideoCapture` object (`inputVideo`) to create a `cv::Size` object (`S`). It then calls the `open` method with the output filename (`NAME`), integer codec identifier (`ex`), frames per second (retrieved from `inputVideo`), frame size (`S`), and a boolean flag (`true`) indicating color video output.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_write.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nVideoWriter outputVideo;\nSize S = Size((int) inputVideo.get(CAP_PROP_FRAME_WIDTH),    //Acquire input size\n              (int) inputVideo.get(CAP_PROP_FRAME_HEIGHT));\noutputVideo.open(NAME , ex, inputVideo.get(CAP_PROP_FPS),S, true);\n```\n\n----------------------------------------\n\nTITLE: Generating ChArUco Diamond Marker Image with OpenCV ArUco (C++)\nDESCRIPTION: This snippet demonstrates how to generate an image of a ChArUco diamond marker using the cv::aruco::CharucoBoard::generateImage() function in OpenCV C++. It sets square and marker sizes as well as the four marker IDs in a cv::Vec4i object, outputting a PNG or similar image. Dependencies include OpenCV's aruco module. The user can specify square length, marker length, dictionary, and marker IDs. The output is a new diamond marker image file, with limitations set by the requirements that diamonds must be 3x3 squares with four markers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_diamond_detection/charuco_diamond_detection.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/aruco/charuco.hpp>\n#include <opencv2/highgui.hpp>\n\n// Parameters: squareLength=200, markerLength=120, dictionary=DICT_4X4_50, ids={0,1,2,3}\ncv::Ptr<cv::aruco::Dictionary> dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_4X4_50);\ncv::Mat diamondImg;\ncv::Vec4i ids(0, 1, 2, 3);\nint squareLength = 200;\nint markerLength = 120;\ncv::aruco::CharucoBoard::generateImage(dictionary, squareLength, markerLength, ids, diamondImg);\ncv::imwrite(\"mydiamond.png\", diamondImg);\n```\n\n----------------------------------------\n\nTITLE: Loading Source, Template, and Mask Images (Python)\nDESCRIPTION: Loads the source image and template image using `cv2.imread` based on command-line arguments. It also supports loading an optional mask image if the corresponding argument is provided. Basic command-line argument parsing is included.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/match_template/match_template.py load_image\n```\n\n----------------------------------------\n\nTITLE: Row-Split Parallel Convolution with OpenCV\nDESCRIPTION: This snippet provides an alternative approach to parallel convolution by splitting computation across rows of the image. Leveraging OpenCV's parallel_for_, this method illustrates row-wise division for parallel execution, maintaining similar performance yet differing in memory access patterns.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_new/how_to_use_OpenCV_parallel_for_new.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_use_OpenCV_parallel_for_new.cpp overload-row-split\n```\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_use_OpenCV_parallel_for_new.cpp convolution-parallel-function-row\n```\n\n----------------------------------------\n\nTITLE: Installing PaddlePaddle and ONNX Conversion Dependencies (Shell)\nDESCRIPTION: Installs the necessary Python packages for running PaddlePaddle models, using Paddle Hub, and converting PaddlePaddle models to the ONNX format. These dependencies are required for the subsequent steps involving model download, conversion, and execution.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install paddlepaddle-gpu\npip install paddlehub\npip install paddle2onnx\n```\n\n----------------------------------------\n\nTITLE: Applying Median Blur in OpenCV Python\nDESCRIPTION: This code shows how to use median blurring with cv.medianBlur(), which replaces each pixel with the median value of neighboring pixels. This method is particularly effective against salt-and-pepper noise while preserving edges.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_filtering/py_filtering.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmedian = cv.medianBlur(img,5)\n```\n\n----------------------------------------\n\nTITLE: Creating Images in Python\nDESCRIPTION: Initializing blank images for drawing shapes in OpenCV Python. Creates two black images named 'atom_image' and 'rook_image' with specified dimensions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Windows names\natom_window = \"Drawing 1: Atom\"\nrook_window = \"Drawing 2: Rook\"\n\n# Create black empty images\natom_image = np.zeros((w, w, 3), dtype=np.uint8)\nrook_image = np.zeros((w, w, 3), dtype=np.uint8)\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Policies and Project Definition for OpenJPEG\nDESCRIPTION: Initializes the CMake build process for the OpenJPEG library. It sets the CMake policy CMP0003 and CMP0042 to NEW, defines the internal library name variable `OPENJPEG_LIBRARY_NAME`, and declares the project named 'openjpeg' specifying C as the primary language.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_policy(SET CMP0003 NEW)\nif(POLICY CMP0042)\n  cmake_policy(SET CMP0042 NEW)\nendif()\n\nset(OPENJPEG_LIBRARY_NAME libopenjp2)\n\nproject(openjpeg C)\n```\n\n----------------------------------------\n\nTITLE: Creating Documentation Tables - Markdown/plaintext\nDESCRIPTION: Demonstrates how to structure tables in documentation using vertical bars for columns as supported by markdown and doxygen. Inputs are header and cell contents; outputs as a formatted table in HTML or rendered markdown output. No dependencies required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nFirst Header  | Second Header\n------------- | -------------\nContent Cell  | Content Cell\nContent Cell  | Content Cell\n```\n\n----------------------------------------\n\nTITLE: Implementing ORB Feature Detection with OpenCV in Python\nDESCRIPTION: This code demonstrates how to use OpenCV's ORB implementation to detect keypoints in an image. It initializes an ORB detector, detects keypoints, computes descriptors, and visualizes the results using matplotlib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_orb/py_orb.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('simple.jpg', cv.IMREAD_GRAYSCALE)\n\n# Initiate ORB detector\norb = cv.ORB_create()\n\n# find the keypoints with ORB\nkp = orb.detect(img,None)\n\n# compute the descriptors with ORB\nkp, des = orb.compute(img, kp)\n\n# draw only keypoints location,not size and orientation\nimg2 = cv.drawKeypoints(img, kp, None, color=(0,255,0), flags=0)\nplt.imshow(img2), plt.show()\n```\n\n----------------------------------------\n\nTITLE: Loading OpenCV Libraries in Clojure\nDESCRIPTION: This snippet shows how to load the OpenCV native library in a Clojure REPL environment to enable OpenCV functionality. This loading is essential for using Java classes within Clojure.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_8\n\nLANGUAGE: clojure\nCODE:\n```\nuser=> (clojure.lang.RT/loadLibrary org.opencv.core.Core/NATIVE_LIBRARY_NAME)\nnil\n```\n\n----------------------------------------\n\nTITLE: Setting Up Image Resources for OpenCV Project\nDESCRIPTION: A bash snippet to prepare directories and organize static resources such as images required for OpenCV projects, facilitating better project structure and resources management.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p resources/images\ncp ~/opt/opencv/doc/tutorials/introduction/desktop_java/images/lena.png resource/images/\n```\n\n----------------------------------------\n\nTITLE: Setting Up Chessboard Detection with OpenCV in Python\nDESCRIPTION: This snippet initializes the chessboard detection setup required for camera calibration. It prepares 3D object points in real-world space and reads 2D image points. Dependencies include OpenCV and NumPy, with key functions like cv.findChessboardCorners() to find chessboard corners in images. Inputs are image files and outputs are object and image points for calibration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_calibration/py_calibration.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nimport glob\n\n# termination criteria\ncriteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n\n# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\nobjp = np.zeros((6*7,3), np.float32)\nobjp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)\n\n# Arrays to store object points and image points from all the images.\nobjpoints = [] # 3d point in real world space\nimgpoints = [] # 2d points in image plane.\n\nimages = glob.glob('*.jpg')\n\nfor fname in images:\n    img = cv.imread(fname)\n    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n\n    # Find the chess board corners\n    ret, corners = cv.findChessboardCorners(gray, (7,6), None)\n\n    # If found, add object points, image points (after refining them)\n    if ret == True:\n        objpoints.append(objp)\n\n        corners2 = cv.cornerSubPix(gray,corners, (11,11), (-1,-1), criteria)\n        imgpoints.append(corners2)\n\n        # Draw and display the corners\n        cv.drawChessboardCorners(img, (7,6), corners2, ret)\n        cv.imshow('img', img)\n        cv.waitKey(500)\n\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Data Structure Class for File I/O - OpenCV C++\nDESCRIPTION: This C++ code defines a custom class (MyData) for use with OpenCV FileStorage-based serialization. It contains public data members and can be extended with custom reading and writing methods. Requires standard C++ headers and OpenCV for integration with the XML/YAML/JSON system. Members should be simple types or OpenCV structures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\nclass MyData\\n{\\npublic:\\n      MyData() : A(0), X(0), id() {}\\npublic:   // Data Members\\n   int A;\\n   double X;\\n   string id;\\n};\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV System-wide with Ninja\nDESCRIPTION: Command to install OpenCV system-wide using Ninja, requires root privileges.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nsudo ninja install\n```\n\n----------------------------------------\n\nTITLE: Setting libjasper Library Properties in CMake\nDESCRIPTION: This snippet sets various properties for the libjasper library target, including output names, debug postfixes, and output directories.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjasper/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(${JASPER_LIBRARY}\n  PROPERTIES\n  OUTPUT_NAME ${JASPER_LIBRARY}\n  DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n  COMPILE_PDB_NAME ${JASPER_LIBRARY}\n  COMPILE_PDB_NAME_DEBUG \"${JASPER_LIBRARY}${OPENCV_DEBUG_POSTFIX}\"\n  ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n  )\n\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(${JASPER_LIBRARY} PROPERTIES FOLDER \"3rdparty\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Applying Median Blur with OpenCV in Java\nDESCRIPTION: This Java snippet shows how to apply median blur on images using OpenCV's medianBlur() function, which requires OpenCV and kernel size parameter along with source and destination images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/Smoothing/Smoothing.java medianblur\n```\n\n----------------------------------------\n\nTITLE: Conditionally Including xfeatures2d Module in CMake\nDESCRIPTION: Checks if the `opencv_xfeatures2d` module is available (`HAVE_opencv_xfeatures2d` is true). If it is, it uses `ocv_include_modules_recurse` to include its headers and settings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_opencv_xfeatures2d)\n  ocv_include_modules_recurse(opencv_xfeatures2d)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Source and Header File Resolution for JNI Library - CMake\nDESCRIPTION: This section uses file globbing to automatically collect all source and header files with .cpp, .c, .hpp, or .h extensions in the current directory. Essential for compiling all relevant code into the JNI shared library, it removes the need to manually maintain file lists. Prerequisites: relevant files must be present in the directory. Inputs: source and header file patterns; Outputs: lists of files to be built.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-4-opencl/jni/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB srcs *.cpp *.c)\nfile(GLOB hdrs *.hpp *.h)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Build for Arm Semihosting with CMake\nDESCRIPTION: This command configures the OpenCV build process using CMake for an Arm semihosting target. It specifies the custom toolchain file located in `opencv/platforms/semihosting/`, provides the path to the Arm baremetal toolchain binaries via `SEMIHOSTING_TOOLCHAIN_PATH`, enables the compilation of example applications (`BUILD_EXAMPLES=ON`), and uses Ninja as the build system generator (`-GNinja`). Ensure the specified toolchain path and OpenCV source path (`../opencv/`) are correct relative to the build directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/semihosting/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncmake ../opencv/ \\\n    -DCMAKE_TOOLCHAIN_FILE=../opencv/platforms/semihosting/aarch64-semihosting.toolchain.cmake \\\n    -DSEMIHOSTING_TOOLCHAIN_PATH=/path/to/baremetal-toolchain/bin/ \\\n    -DBUILD_EXAMPLES=ON -GNinja\n```\n\n----------------------------------------\n\nTITLE: Predefined Dictionary Selection for ArUco in C++\nDESCRIPTION: The code demonstrates how to select a predefined marker dictionary using OpenCV's ArUco module for robust marker detection. Predefined dictionaries such as `DICT_6X6_250` provide a balance of size and unique markers based on application requirements. This helps achieve a high inter-marker distance for better error correction.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_detection/aruco_detection.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\ncv::aruco::Dictionary dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_6X6_250);\n```\n\n----------------------------------------\n\nTITLE: Configure Compression Strategies and Memory Options in CMake\nDESCRIPTION: This section configures specific allowances and strategies for data compression (e.g., deflate_quick and deflate_medium) and builds tailored inflating strategies. It can reduce memory footprint or allow broad operation ranges based on flags like WITH_INFLATE_STRICT.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# Enable deflate_quick at level 1\n#\nif(NOT WITH_NEW_STRATEGIES)\n    add_definitions(-DNO_QUICK_STRATEGY)\nendif()\n#\n# Enable deflate_medium at level 4-6\n#\nif(NOT WITH_NEW_STRATEGIES)\n    add_definitions(-DNO_MEDIUM_STRATEGY)\nendif()\n#\n# Enable inflate compilation options\n#\nif(WITH_INFLATE_STRICT)\n    add_definitions(-DINFLATE_STRICT)\n    message(STATUS \"Inflate strict distance checking enabled\")\nendif()\nif(WITH_INFLATE_ALLOW_INVALID_DIST)\n    add_definitions(-DINFLATE_ALLOW_INVALID_DISTANCE_TOOFAR_ARRR)\n    message(STATUS \"Inflate zero data for invalid distances enabled\")\nendif()\n#\n# Enable reduced memory configuration\n```\n\n----------------------------------------\n\nTITLE: Setting Qt Environment Variable\nDESCRIPTION: Windows command to set the QTDIR environment variable for Qt integration with OpenCV\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_install/windows_install.markdown#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsetx -m QTDIR D:/OpenCV/dep/qt/qt-everywhere-opensource-src-4.7.3\n```\n\n----------------------------------------\n\nTITLE: Drawing Pose Axis with ArUco Markers using OpenCV C++\nDESCRIPTION: This referenced snippet involves drawing axis lines to represent pose estimation visually on an image. It uses the camera matrix, distortion coefficients, and rotation and translation vectors for each detected marker. The axis drawing provides a visual accuracy check of the estimated poses, using specified axis length in same units as translation vector.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_detection/aruco_detection.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/objectDetection/detect_markers.cpp aruco_draw_pose_estimation\n```\n\n----------------------------------------\n\nTITLE: Handling CUDA Source Exclusion, Includes, and Library Linking for DNN Module - CMake\nDESCRIPTION: Configures inclusion of CUDA and cuDNN libraries, include paths, and compute capability checks for the DNN module when CUDA is enabled. Validates required CUDA version and available architectures, fails the build if unsupported. Adds additional libraries for CUDA if the build system supports first-class CUDA language targets. If dependencies are not met, CUDA-related sources are excluded.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\nif(OPENCV_DNN_CUDA AND HAVE_CUDA AND HAVE_CUBLAS AND HAVE_CUDNN)\n  list(APPEND include_dirs ${CUDA_TOOLKIT_INCLUDE} ${CUDNN_INCLUDE_DIRS})\n  set(CC_LIST ${CUDA_ARCH_BIN})\n  separate_arguments(CC_LIST)\n  foreach(cc ${CC_LIST})\n    if(cc VERSION_LESS 3.0)\n      message(FATAL_ERROR \"CUDA backend for DNN module requires CC 3.0 or higher. Please remove unsupported architectures from CUDA_ARCH_BIN option or disable OPENCV_DNN_CUDA=OFF.\")\n    endif()\n  endforeach()\n  unset(CC_LIST)\n  if(ENABLE_CUDA_FIRST_CLASS_LANGUAGE)\n    list(APPEND libs ${CUDNN_LIBRARIES} CUDA::cublas${CUDA_LIB_EXT})\n    if(NOT CUDA_VERSION VERSION_LESS 10.1)\n      list(APPEND libs CUDA::cublasLt${CUDA_LIB_EXT})\n    endif()\n  endif()\nelse()\n  set(sources_options ${sources_options} EXCLUDE_CUDA)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Replace Default Optimization Level in CMake\nDESCRIPTION: This snippet replaces the default optimization level from 3 to 2 unless specific build conditions (such as code coverage or MSVC usage) are met. It modifies the CMAKE_C_FLAGS_RELEASE variable for release builds. Such changes apply to non-MSVC compilers and when the default optimization flag is -O3.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n# Replace optimization level 3 added by default with level 2\nif(NOT WITH_CODE_COVERAGE AND NOT MSVC AND NOT CMAKE_C_FLAGS MATCHES \"([\\\\/\\\\-]O)3\")\n    string(REGEX REPLACE \"([\\\\/\\\\-]O)3\" \"\\\\12\"\n        CMAKE_C_FLAGS_RELEASE \"${CMAKE_C_FLAGS_RELEASE}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Loading a Mesh from PLY File - OpenCV C++\nDESCRIPTION: This snippet details how to load and parse a 3D object mesh from a PLY file using a Mesh class in C++. It reads vertex and triangle data, updating mesh attributes accordingly. Dependencies include a CSV reader capable of reading PLY format.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n/* Load a CSV with *.ply format */\nvoid Mesh::load(const std::string path)\n{\n\n    // Create the reader\n    CsvReader csvReader(path);\n\n    // Clear previous data\n    list_vertex_.clear();\n    list_triangles_.clear();\n\n    // Read from .ply file\n    csvReader.readPLY(list_vertex_, list_triangles_);\n\n    // Update mesh attributes\n    num_vertexs_ = list_vertex_.size();\n    num_triangles_ = list_triangles_.size();\n\n}\n\n```\n\n----------------------------------------\n\nTITLE: Loading Images and Text Detection Model in C++\nDESCRIPTION: This C++ snippet shows how to load an image for text detection and initialize a TextDetectionModel_DB with ONNX weights. The snippet prepares the environment for performing text detection using a specified DB model.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_text_spotting/dnn_text_spotting.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// Load an image\n// you can find some images for testing in \"Images Testing\"\nMat frame = imread(\"/path/to/text_det_test.png\");\n```\n\n----------------------------------------\n\nTITLE: Microsoft Media Foundation Configuration Option in OpenCV\nDESCRIPTION: Defines the WITH_MSMF build option for Windows platforms to enable the Media Foundation framework backend for camera capture and video processing, with optional hardware acceleration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_13\n\nLANGUAGE: markdown\nCODE:\n```\n`WITH_MSMF` (Windows; default: _ON_)\n```\n\n----------------------------------------\n\nTITLE: Controlling Shared and Static Libraries in OpenCV\nDESCRIPTION: This code controls whether the build produces shared or static libraries using `BUILD_SHARED_LIBS`. The default setting is typically `ON` but can be modified. The code mentions considerations for position-independent code.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DBUILD_SHARED_LIBS=OFF ../opencv\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV Contrib Repository in Bash\nDESCRIPTION: Commands to clone the OpenCV contrib repository for extra modules from GitHub using Git in the Terminal on macOS.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/ios_install/ios_install.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/<my_working _directory>\ngit clone https://github.com/opencv/opencv_contrib.git\n```\n\n----------------------------------------\n\nTITLE: Applying Bilateral Filtering for Edge-Preserving Smoothing in C++\nDESCRIPTION: Explains cv.bilateralFilter(), used for advanced noise removal while preserving sharp edges through bilateral filtering. This technique combines Gaussian filtering based on both space and pixel intensity, offering effective noise reduction without blurring edges. OpenCV is required, and key parameters include diameter, sigma for color and space, and border type.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_filtering/js_filtering.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\ncv::bilateralFilter(src, dst, d, sigmaColor, sigmaSpace, cv::BORDER_DEFAULT);\n```\n\n----------------------------------------\n\nTITLE: Downloading Face Detector Model\nDESCRIPTION: Command to download OpenCV Face Detector model weights and save them to a specific directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython download_models.py --save_dir FaceDetector opencv_fd\n```\n\n----------------------------------------\n\nTITLE: Defining Functions with Output Parameters Using OpenCV Macros in C++\nDESCRIPTION: This snippet shows how to annotate a function with both input and output parameters for Python binding generation. The minEnclosingCircle function uses CV_EXPORTS_W for export, InputArray for input, and CV_OUT for parameters to clarify their usage as outputs. These macro annotations enable scripts to automatically wrap the function for Python. The function expects points as input and writes results into center and radius.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_bindings/py_bindings_basics/py_bindings_basics.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nCV_EXPORTS_W void minEnclosingCircle( InputArray points,\n                                     CV_OUT Point2f& center, CV_OUT float& radius );\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV System-wide on macOS\nDESCRIPTION: This command installs the built OpenCV libraries and headers system-wide, requiring sudo privileges.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/macos_install/macos_install.markdown#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo make install\n```\n\n----------------------------------------\n\nTITLE: SVM Basic Linear Separability Formula\nDESCRIPTION: Mathematical formula showing the minimization function for finding optimal decision boundary in linearly separable case with constraints.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_svm/py_svm_basics/py_svm_basics.markdown#2025-04-22_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n\\min_{w, b_0} L(w, b_0) = \\frac{1}{2}||w||^2 \\; \\text{subject to} \\; t_i(w^Tx+b_0) \\geq 1 \\; \\forall i\n```\n\n----------------------------------------\n\nTITLE: Compiling Kernel Packages in C++\nDESCRIPTION: This snippet creates a kernel package that includes both custom kernels and optimizations using the Fluid backend to manage memory efficiently within G-API's standard kernels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/face_beautification/face_beautification.cpp kern_pass_1\n```\n\n----------------------------------------\n\nTITLE: AgastFeatureDetector Implementation Class\nDESCRIPTION: Implementation of the AgastFeatureDetector class that provides the interface for AGAST feature detection. It handles image conversion, parameter settings, and delegates actual detection to the AGAST function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_43\n\nLANGUAGE: C++\nCODE:\n```\nclass AgastFeatureDetector_Impl : public AgastFeatureDetector\n{\npublic:\n    AgastFeatureDetector_Impl( int _threshold, bool _nonmaxSuppression, int _type )\n    : threshold(_threshold), nonmaxSuppression(_nonmaxSuppression), type((short)_type)\n    {}\n\n    void detect( InputArray _image, std::vector<KeyPoint>& keypoints, InputArray _mask )\n    {\n        Mat mask = _mask.getMat(), grayImage;\n        UMat ugrayImage;\n        _InputArray gray = _image;\n        if( _image.type() != CV_8U )\n        {\n            _OutputArray ogray = _image.isUMat() ? _OutputArray(ugrayImage) : _OutputArray(grayImage);\n            cvtColor( _image, ogray, COLOR_BGR2GRAY );\n            gray = ogray;\n        }\n        AGAST( gray, keypoints, threshold, nonmaxSuppression, type );\n        KeyPointsFilter::runByPixelsMask( keypoints, mask );\n    }\n\n    void set(int prop, double value)\n    {\n        if(prop == THRESHOLD)\n            threshold = cvRound(value);\n        else if(prop == NONMAX_SUPPRESSION)\n            nonmaxSuppression = value != 0;\n        else\n            CV_Error(Error::StsBadArg, \"\");\n    }\n\n    double get(int prop) const\n    {\n        if(prop == THRESHOLD)\n            return threshold;\n        if(prop == NONMAX_SUPPRESSION)\n            return nonmaxSuppression;\n        CV_Error(Error::StsBadArg, \"\");\n        return 0;\n    }\n\n    void setThreshold(int threshold_) { threshold = threshold_; }\n    int getThreshold() const { return threshold; }\n\n    void setNonmaxSuppression(bool f) { nonmaxSuppression = f; }\n    bool getNonmaxSuppression() const { return nonmaxSuppression; }\n\n    void setType(int type_) { type = type_; }\n    int getType() const { return type; }\n\n    int threshold;\n    bool nonmaxSuppression;\n    int type;\n};\n```\n\n----------------------------------------\n\nTITLE: Constructing Output Filename for OpenCV VideoWriter (C++)\nDESCRIPTION: Demonstrates how to create a new filename for the output video. It takes the base name of the input video file (from `argv[1]`), finds the position of the file extension, extracts the base name, appends the selected channel character (from `argv[2]`), and adds the `.avi` extension. Requires the `string` library and command-line arguments.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_write.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nconst string source      = argv[1];            // the source file name\nstring::size_type pAt = source.find_last_of('.');   // Find extension point\nconst string NAME = source.substr(0, pAt) + argv[2][0] + \".avi\";   // Form the new name with container\n```\n\n----------------------------------------\n\nTITLE: Creating and Updating Filter Kernel\nDESCRIPTION: Creates a normalized box filter kernel of varying sizes. Kernel is filled with 1's and normalized by dividing by total number of elements.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/filter_2d/filter_2d.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nkernel_size = 3 + 2*( ind%5 );\nMat kernel = Mat::ones( kernel_size, kernel_size, CV_32F )/ (float)(kernel_size*kernel_size);\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Message Print for Each CMake Hook Script Call (CMake)\nDESCRIPTION: When activated, this debug option emits a message each time a CMake hook script is called during OpenCV configuration. Intended for debugging and tracking custom hook execution. No prerequisites. Enable with -DOPENCV_DUMP_HOOKS_FLOW=ON.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_29\n\nLANGUAGE: cmake\nCODE:\n```\nOPENCV_DUMP_HOOKS_FLOW\n```\n\n----------------------------------------\n\nTITLE: Setting Up CMake Project for Histogram Sample\nDESCRIPTION: The snippet initializes a CMake project for a histogram sample using OpenCV. It declares the project, specifies dependencies, checks for available dependencies, and configures target directories and libraries. The configuration requires OpenCV core and image processing modules, among others, and ensures that all necessary components are included before proceeding.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/semihosting/histogram/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(PROJECT_NAME histogram)\nproject(${PROJECT_NAME})\n\nocv_install_example_src(histogram *.cpp *.hpp CMakeLists.txt)\n\nset(LOCAL_DEPS\n  opencv_core\n  opencv_imgproc\n  ${OPENCV_MODULES_PUBLIC}\n  ${OpenCV_LIB_COMPONENTS})\nocv_check_dependencies(${LOCAL_DEPS})\n\nif(NOT OCV_DEPENDENCIES_FOUND)\n  return()\nendif()\n\nocv_define_sample(histogram histogram.cpp ${SEMIHOSTING_SUFFIX})\nocv_include_modules_recurse(${LOCAL_DEPS})\ntarget_include_directories(${histogram} PRIVATE ${CMAKE_CURRENT_BINARY_DIR})\ntarget_include_directories(${histogram} PRIVATE ${RAW_PIXEL_INCLUDE})\nocv_target_link_libraries(${histogram} PRIVATE ${OPENCV_LINKER_LIBS}\n  ${LOCAL_DEPS})\n```\n\n----------------------------------------\n\nTITLE: Loading Source Images in Java\nDESCRIPTION: This Java snippet loads two source images using `Imgcodecs.imread`. It checks if the images loaded correctly and exits if either fails. It also initializes the destination matrix `dst` and a default alpha value.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n//![load]\n// Read images\nMat src1 = Imgcodecs.imread(args.length > 0 ? args[0] : \"../data/LinuxLogo.jpg\");\nMat src2 = Imgcodecs.imread(args.length > 1 ? args[1] : \"../data/WindowsLogo.jpg\");\nif (src1.empty()) {\n    System.out.println(\"Error loading src1\");\n    System.exit(-1);\n}\nif (src2.empty()) {\n    System.out.println(\"Error loading src2\");\n    System.exit(-1);\n}\nalpha = 0.5; // Default value\ndst = new Mat();\n//![load]\n```\n\n----------------------------------------\n\nTITLE: Forcing Compilation of Dispatchable Layer Files with CPU Extensions - CMake\nDESCRIPTION: Multiple calls to 'ocv_add_dispatched_file_force_all' register source files for the DNN module with specific CPU extension flags (AVX, AVX2, AVX512_SKX, RVV, LASX, NEON, NEON_FP16). This ensures these layers are compiled for multiple architectures. Parameters specify which CPU architectures should be built. Inputs are source file paths and CPU flags; outputs are configured build targets.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nocv_add_dispatched_file_force_all(\"layers/layers_common\" AVX AVX2 AVX512_SKX RVV LASX)\nocv_add_dispatched_file_force_all(\"int8layers/layers_common\" AVX2 AVX512_SKX RVV LASX)\nocv_add_dispatched_file_force_all(\"layers/cpu_kernels/conv_block\" AVX AVX2 NEON NEON_FP16)\nocv_add_dispatched_file_force_all(\"layers/cpu_kernels/conv_depthwise\" AVX AVX2 RVV LASX)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Exporting PyTorch ResNet-50 Model to ONNX - Python\nDESCRIPTION: This Python snippet instantiates a pretrained ResNet-50 model using PyTorch, then exports it to ONNX format using a utility function. The function 'get_pytorch_onnx_model' is expected to handle ONNX export and path management. Dependencies include torch, torchvision, and a correctly defined export function. The script prints the output location of the converted ONNX model.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# initialize PyTorch ResNet-50 model\\noriginal_model = models.resnet50(pretrained=True)\\n\\n# get the path to the converted into ONNX PyTorch model\\nfull_model_path = get_pytorch_onnx_model(original_model)\\nprint(\"PyTorch ResNet-50 model was successfully converted: \", full_model_path)\n```\n\n----------------------------------------\n\nTITLE: Normalizing Histogram Results in C++\nDESCRIPTION: C++ snippet normalizing the calculated histograms (`b_hist`, `g_hist`, `r_hist`) to fit within the height of the display image (`histImage.rows`). It uses `cv::normalize` with `NORM_MINMAX` to scale the histogram values linearly between 0 and `histImage.rows`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_24\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/calcHist_Demo.cpp Normalize the result to ( 0, histImage.rows )\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Build for OpenCV\nDESCRIPTION: The snippet enables debug builds using the `CMAKE_BUILD_TYPE` option. It provides different command scenarios based on platform requirements and mentions options for GNU libstdc++ and disabling optimizations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_BUILD_TYPE=Debug ../opencv\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: Setting Estimated Projection Matrix from Kalman Filter Output in C++ (OpenCV)\nDESCRIPTION: This C++ snippet (Step 6 of the algorithm) shows how to use the estimated pose (rotation `rotation_estimated` and translation `translation_estimated` matrices) derived from the Kalman Filter. It calls the `set_P_matrix` method of a `pnp_detection_est` object (likely representing the estimated pose state), providing the filtered rotation and translation matrices as arguments to update the object's internal projection matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_26\n\nLANGUAGE: cpp\nCODE:\n```\n// -- Step 6: Set estimated projection matrix\npnp_detection_est.set_P_matrix(rotation_estimated, translation_estimated);\n```\n\n----------------------------------------\n\nTITLE: Loading Source, Template, and Mask Images (C++)\nDESCRIPTION: Loads the source image and the template image using `cv::imread`. It also attempts to load an optional mask image if provided via command-line arguments. Includes error handling if images cannot be loaded.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp load_image\n```\n\n----------------------------------------\n\nTITLE: Custom IO Operations in Python with OpenCV\nDESCRIPTION: Code snippet showing custom input/output operations in Python using OpenCV's file system functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\npython/tutorial_code/core/file_input_output/file_input_output.py customIO\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting PaddleSeg Model (Shell)\nDESCRIPTION: Downloads the pre-trained PaddleSeg Human Segmentation inference model (`humanseg_hrnet18_small_v1.zip`) using `wget` and then extracts its contents using `unzip`. This provides the necessary model files (`.pdmodel`, `.pdiparams`) for conversion.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget https://x2paddle.bj.bcebos.com/inference/models/humanseg_hrnet18_small_v1.zip\nunzip humanseg_hrnet18_small_v1.zip\n```\n\n----------------------------------------\n\nTITLE: Installing Third-Party Licenses using OpenCV Macro in CMake\nDESCRIPTION: Calls the OpenCV-specific CMake macro `ocv_install_3rdparty_licenses` to handle the installation of license-related files (README.md, LICENSE) associated with the OpenJPEG library target.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nocv_install_3rdparty_licenses(${OPENJPEG_LIBRARY_NAME} README.md LICENSE)\n```\n\n----------------------------------------\n\nTITLE: Running TensorFlow Classification Module in Test Mode via OpenCV dnn CLI (Console)\nDESCRIPTION: Demonstrates launching the classification module in test mode, which enables model inference using specific or default image preprocessing parameters. Required dependencies include tested models, corresponding data, and dnn_model_runner. Key parameters are --test (enables testing), --default_img_preprocess (toggles default or custom preprocessing), and --evaluate (prevents evaluation metrics). Inputs are model and preprocessing settings; outputs are predictions and possibly visualized results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls --model_name <tf_cls_model_name> --test True --default_img_preprocess <True/False> --evaluate False\n```\n\n----------------------------------------\n\nTITLE: Initializing PnPProblem with Camera Parameters in C++\nDESCRIPTION: The snippet shows a constructor for the PnPProblem class, initializing intrinsic camera matrices using an array of parameters. It sets up matrices for intrinsic values, rotation, translation, and combined rotation-translation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\n// Custom constructor given the intrinsic camera parameters\n\nPnPProblem::PnPProblem(const double params[])\n{\n  _A_matrix = cv::Mat::zeros(3, 3, CV_64FC1);   // intrinsic camera parameters\n  _A_matrix.at<double>(0, 0) = params[0];       //      [ fx   0  cx ]\n  _A_matrix.at<double>(1, 1) = params[1];       //      [  0  fy  cy ]\n  _A_matrix.at<double>(0, 2) = params[2];       //      [  0   0   1 ]\n  _A_matrix.at<double>(1, 2) = params[3];\n  _A_matrix.at<double>(2, 2) = 1;\n  _R_matrix = cv::Mat::zeros(3, 3, CV_64FC1);   // rotation matrix\n  _t_matrix = cv::Mat::zeros(3, 1, CV_64FC1);   // translation matrix\n  _P_matrix = cv::Mat::zeros(3, 4, CV_64FC1);   // rotation-translation matrix\n\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenCV Mat with MATLAB-Style Functions in C++\nDESCRIPTION: Creates matrices using MATLAB-style initializers like zeros(), ones(), and eye(). These functions create matrices filled with zeros, ones, or identity matrices respectively.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n// MATLAB style initializer: zeros, ones, eye\nMat E = Mat::eye(4, 4, CV_64F);\ncout << \"E = \" << endl << \" \" << E << endl << endl;\nMat O = Mat::ones(2, 2, CV_32F);\ncout << \"O = \" << endl << \" \" << O << endl << endl;\nMat Z = Mat::zeros(3,3, CV_8UC1);\ncout << \"Z = \" << endl << \" \" << Z << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Checking Dependencies for CUDA Samples in CMake\nDESCRIPTION: Calls the custom `ocv_check_dependencies` CMake function to verify if all modules listed in the `OPENCV_CUDA_SAMPLES_REQUIRED_DEPS` variable are found and available in the current build configuration. It likely sets the `OCV_DEPENDENCIES_FOUND` variable based on the result.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nocv_check_dependencies(${OPENCV_CUDA_SAMPLES_REQUIRED_DEPS})\n```\n\n----------------------------------------\n\nTITLE: Calculating Orientation using OpenCV.js\nDESCRIPTION: Finds the orientation (angle) of a contour by fitting an ellipse to it. Also provides major and minor axis lengths (implicitly via the `rotatedRect` object). Requires a contour (`cnt`). Uses the `cv.fitEllipse` function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contour_properties/js_contour_properties.markdown#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet rotatedRect = cv.fitEllipse(cnt);\nlet angle = rotatedRect.angle;\n```\n\n----------------------------------------\n\nTITLE: Estimating Camera Response Function with OpenCV Python\nDESCRIPTION: Estimates the camera response function (CRF) for the HDR processing using Debevec and Robertson calibration methods. This CRF allows more accurate HDR image creation by relating scene radiance to intensity values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_photo/py_hdr/py_hdr.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Estimate camera response function (CRF)\ncal_debevec = cv.createCalibrateDebevec()\ncrf_debevec = cal_debevec.process(img_list, times=exposure_times)\nhdr_debevec = merge_debevec.process(img_list, times=exposure_times.copy(), response=crf_debevec.copy())\ncal_robertson = cv.createCalibrateRobertson()\ncrf_robertson = cal_robertson.process(img_list, times=exposure_times)\nhdr_robertson = merge_robertson.process(img_list, times=exposure_times.copy(), response=crf_robertson.copy())\n```\n\n----------------------------------------\n\nTITLE: Visualizing PCA Axis in Java using OpenCV\nDESCRIPTION: Provides a helper method `drawAxis` to visualize a principal component axis. It draws a line segment between a center point `p` and a scaled point `q` on the image `img` with a specified color. It also adds arrowhead hooks for clarity. Requires OpenCV Java bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n//! [visualization]\n// Function to draw the axes of the object detected by PCA\nprivate static void drawAxis(Mat img, Point p, Point q, Scalar colour, double scale) {\n    double angle = atan2(p.y - q.y, p.x - q.x); // angle in radians\n    double hypotenuse = Math.sqrt(Math.pow(p.y - q.y, 2) + Math.pow(p.x - q.x, 2));\n\n    // Here we lengthen the arrow by a factor of scale\n    q.x = (p.x - scale * hypotenuse * Math.cos(angle));\n    q.y = (p.y - scale * hypotenuse * Math.sin(angle));\n    line(img, p, q, colour, 1, LINE_AA);\n\n    // create the arrow hooks\n    p.x = (q.x + 9 * Math.cos(angle + Math.PI / 4));\n    p.y = (q.y + 9 * Math.sin(angle + Math.PI / 4));\n    line(img, p, q, colour, 1, LINE_AA);\n\n    p.x = (q.x + 9 * Math.cos(angle - Math.PI / 4));\n    p.y = (q.y + 9 * Math.sin(angle - Math.PI / 4));\n    line(img, p, q, colour, 1, LINE_AA);\n}\n//! [visualization]\n```\n\n----------------------------------------\n\nTITLE: Configuring Compiler Flags and Definitions for MSVC and Other Compilers - CMake\nDESCRIPTION: Configures a wide range of compiler warning flags and macro definitions, particularly for MSVC (Microsoft Visual C++). Suppresses specific warnings and sets platform compliance flags. Conditions check for compiler version and platform (MSVC, Apple, Android), setting defines or flags accordingly. Parameters include MSVC version, C++11 support, and Android status; outputs are compiler command line options.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n  add_definitions( -D_CRT_SECURE_NO_WARNINGS=1 )\n  ocv_warnings_disable(CMAKE_CXX_FLAGS /wd4244 /wd4267 /wd4018 /wd4355 /wd4800 /wd4251 /wd4996 /wd4146\n                                       /wd4305 /wd4127 /wd4100 /wd4512 /wd4125 /wd4389 /wd4510 /wd4610\n                                       /wd4702 /wd4456 /wd4457 /wd4065 /wd4310 /wd4661 /wd4506\n  )\n  if(MSVC_VERSION LESS 1920)  # MSVS 2015/2017, .pb.cc generated files\n    ocv_warnings_disable(CMAKE_CXX_FLAGS /wd4309)  # 'static_cast': truncation of constant value\n  endif()\n  if(MSVC_VERSION LESS 1920)  # <MSVS2019, .pb.cc generated files\n    ocv_warnings_disable(CMAKE_CXX_FLAGS /wd4189)  # local variable is initialized but not referenced\n    ocv_warnings_disable(CMAKE_CXX_FLAGS /wd4592)  # symbol will be dynamically initialized (implementation limitation)\n  endif()\nelse()\n  ocv_warnings_disable(CMAKE_CXX_FLAGS -Wno-deprecated -Wmissing-prototypes -Wmissing-declarations -Wshadow\n                                       -Wunused-parameter -Wsign-compare\n  )\nendif()\nif(HAVE_CUDA)\n  ocv_warnings_disable(CMAKE_CXX_FLAGS -Wundef)\nendif()\nif(NOT HAVE_CXX11)\n  ocv_warnings_disable(CMAKE_CXX_FLAGS -Wno-undef)  # LANG_CXX11 from protobuf files\nendif()\n\nif(APPLE_FRAMEWORK)\n  ocv_warnings_disable(CMAKE_CXX_FLAGS -Wshorten-64-to-32)\nendif()\n\nif(ANDROID)\n  add_definitions(-DDISABLE_POSIX_MEMALIGN -DTH_DISABLE_HEAP_TRACKING)\nendif()\n\nif(NOT BUILD_PROTOBUF)\n  ocv_target_compile_definitions(${the_module} PRIVATE \"OPENCV_DNN_EXTERNAL_PROTOBUF=1\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Archiving OpenCV Build Artifacts with Tar\nDESCRIPTION: Archive the OpenCV built libraries and headers into a tarball named opencv_arm64.tgz, using the tar command. This step is done after installation of the build artifacts.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ntar czvf opencv_arm64.tgz -C build4-full_arm64/install .\n```\n\n----------------------------------------\n\nTITLE: Configuring Installation Root with a Relative Path in CMake - Shell\nDESCRIPTION: Uses CMake to set up the installation prefix to a directory named 'install' relative to the current working directory. This adjusts where the final build output will be placed post-install, and is useful for isolated builds. Requires 'cmake' and access to the source directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\ncmake -DCMAKE_INSTALL_PREFIX=install ../opencv\n```\n\n----------------------------------------\n\nTITLE: Asynchronous CUDA Stream Operations in OpenCV\nDESCRIPTION: Demonstrates how to use GPU streams for asynchronous operations in OpenCV, including image conversion and multiplication. The stream allows for parallel execution of operations by queuing uploads while the GPU processes other tasks.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\ngpu::Stream stream;\n\nstream.enqueueConvert(b.gI1, b.t1, CV_32F);    // Upload\n\ngpu::split(b.t1, b.vI1, stream);              // Methods (pass the stream as final parameter).\ngpu::multiply(b.vI1[i], b.vI1[i], b.I1_2, stream);        // I1^2\n```\n\n----------------------------------------\n\nTITLE: Copying and Installing FFmpeg DLLs with CMake Scripting - CMake\nDESCRIPTION: This CMake snippet conditionally copies the appropriate FFmpeg DLLs to the output directories (Debug/Release) and installs them for distribution as part of an OpenCV build on Windows. It determines DLL naming based on architecture (32-bit or 64-bit) and handles Visual Studio IDE specifics. The script relies on the variables WIN32, HAVE_FFMPEG_WRAPPER, CMAKE_SIZEOF_VOID_P, OpenCV_BINARY_DIR, OPENCV_DLLVERSION, MSVC_IDE, CMAKE_COMMAND, EXECUTABLE_OUTPUT_PATH, CMAKE_BUILD_TYPE, OPENCV_BIN_INSTALL_PATH, and INSTALL_CREATE_DISTRIB. Inputs are expected as configured CMake variables and outputs are the copied/installed DLL files. User must ensure the FFmpeg binaries are present in the correct 3rdparty folder.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/videoio/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n# copy FFmpeg dll to the output folder\nif(WIN32 AND HAVE_FFMPEG_WRAPPER)\n  if(CMAKE_SIZEOF_VOID_P EQUAL 8)\n    set(FFMPEG_SUFFIX _64)\n  endif()\n  set(ffmpeg_dir \"${OpenCV_BINARY_DIR}/3rdparty/ffmpeg\")\n  set(ffmpeg_bare_name \"opencv_videoio_ffmpeg${FFMPEG_SUFFIX}.dll\")\n  set(ffmpeg_bare_name_ver \"opencv_videoio_ffmpeg${OPENCV_DLLVERSION}${FFMPEG_SUFFIX}.dll\")\n  set(ffmpeg_path \"${ffmpeg_dir}/${ffmpeg_bare_name}\")\n  if(MSVC_IDE)\n    execute_process(\n      COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${ffmpeg_path}\" \"${EXECUTABLE_OUTPUT_PATH}/Release/${ffmpeg_bare_name_ver}\"\n      COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${ffmpeg_path}\" \"${EXECUTABLE_OUTPUT_PATH}/Debug/${ffmpeg_bare_name_ver}\")\n  elseif(MSVC AND (CMAKE_GENERATOR MATCHES \"Visual\"))\n    execute_process(COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${ffmpeg_path}\" \"${EXECUTABLE_OUTPUT_PATH}/${CMAKE_BUILD_TYPE}/${ffmpeg_bare_name_ver}\")\n  else()\n    execute_process(COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${ffmpeg_path}\" \"${EXECUTABLE_OUTPUT_PATH}/${ffmpeg_bare_name_ver}\")\n  endif()\n  install(FILES \"${ffmpeg_path}\" DESTINATION ${OPENCV_BIN_INSTALL_PATH} COMPONENT libs RENAME \"${ffmpeg_bare_name_ver}\")\n  if(INSTALL_CREATE_DISTRIB)\n    install(FILES \"${ffmpeg_dir}/opencv_videoio_ffmpeg${FFMPEG_SUFFIX}.dll\" DESTINATION \"bin/\" COMPONENT libs RENAME \"opencv_videoio_ffmpeg${OPENCV_DLLVERSION}${FFMPEG_SUFFIX}.dll\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Feature Detection API Changes in OpenCV 2.4 vs 3.0\nDESCRIPTION: Illustrates the changes in the feature detection API between OpenCV 2.4 and 3.0, including the use of xfeatures2d namespace and the new create() factory methods.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/transition_guide/transition_guide.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nusing namespace cv;\n// ====== 2.4 =======\n#include \"opencv2/features2d/features2d.hpp\"\nBriefDescriptorExtractor brief(32);\nGridAdaptedFeatureDetector detector(new FastFeatureDetector(10, true), DESIRED_FTRS, 4, 4);\n// ...\ndetector.detect(gray, query_kpts); //Find interest points\nbrief.compute(gray, query_kpts, query_desc); //Compute brief descriptors at each keypoint location\n// ====== 3.0 =======\n#include \"opencv2/features2d.hpp\"\n#include \"opencv2/xfeatures2d.hpp\"\nusing namespace cv::xfeatures2d;\nPtr<BriefDescriptorExtractor> brief = BriefDescriptorExtractor::create(32);\nPtr<FastFeatureDetector> detector = FastFeatureDetector::create(10, true);\n// ...\ndetector->detect(gray, query_kpts); //Find interest points\nbrief->compute(gray, query_kpts, query_desc); //Compute brief descriptors at each keypoint location\n```\n\n----------------------------------------\n\nTITLE: Python BeautifulSoup Dependency Check\nDESCRIPTION: Checks for the availability of Python's BeautifulSoup package required for documentation post-processing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_22\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED HAVE_PYTHON_BS4 AND PYTHON_DEFAULT_EXECUTABLE)\n    execute_process(COMMAND \"${PYTHON_DEFAULT_EXECUTABLE}\" -c \"import bs4; from bs4 import BeautifulSoup; print(bs4.__version__)\"\n                    RESULT_VARIABLE _result\n                    OUTPUT_VARIABLE _bs4_version\n                    OUTPUT_STRIP_TRAILING_WHITESPACE)\n\n    if(NOT _result EQUAL 0)\n      set(HAVE_PYTHON_BS4 0 CACHE INTERNAL \"\")\n    else()\n      message(STATUS \"Python BeautifulSoup (bs4) version: ${_bs4_version}\")\n      set(HAVE_PYTHON_BS4 1 CACHE INTERNAL \"\")\n    endif()\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Source and Header Files for OpenEXR\nDESCRIPTION: This snippet collects source and header files for the OpenEXR library from different directories. It also filters out platform-specific files that aren't needed for the current build target.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openexr/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB lib_srcs Half/half.cpp Iex/*.cpp IlmThread/*.cpp Imath/*.cpp IlmImf/*.cpp)\nfile(GLOB lib_hdrs Half/*.h Iex/Iex*.h IlmThread/IlmThread*.h Imath/Imath*.h IlmImf/*.h)\nlist(APPEND lib_hdrs \"${CMAKE_CURRENT_BINARY_DIR}/IlmBaseConfig.h\" \"${CMAKE_CURRENT_BINARY_DIR}/OpenEXRConfig.h\")\n\nif(WIN32)\n  ocv_list_filterout(lib_srcs Posix.*cpp)\nelse()\n  ocv_list_filterout(lib_srcs Win32.cpp)\nendif()\n\nsource_group(\"Include\" FILES ${lib_hdrs} )\nsource_group(\"Src\" FILES ${lib_srcs})\n```\n\n----------------------------------------\n\nTITLE: Saturation Arithmetic with cv::saturate_cast in OpenCV C++\nDESCRIPTION: This snippet demonstrates the use of saturation arithmetic to prevent overflow when converting operation results to 8-bit images. It uses the `cv::saturate_cast` function to ensure values are within the uchar range, which is critical for preserving image quality when performing operations that may exceed the typical value range.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/doc/intro.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n    I.at<uchar>(y, x) = saturate_cast<uchar>(r);\n```\n\n----------------------------------------\n\nTITLE: RealSense Camera Property Configuration\nDESCRIPTION: Demonstrates how to set and get camera properties using VideoCapture's set and get methods. This example shows setting the depth generator profile and retrieving the FPS.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/intelperc.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n    VideoCapture capture(CAP_REALSENSE);\n    capture.set( CAP_INTELPERC_DEPTH_GENERATOR | CAP_PROP_INTELPERC_PROFILE_IDX, 0 );\n    cout << \"FPS    \" << capture.get( CAP_INTELPERC_DEPTH_GENERATOR+CAP_PROP_FPS ) << endl;\n```\n\n----------------------------------------\n\nTITLE: Project Configuration for OpenCV Dependencies Clojure\nDESCRIPTION: This Clojure configuration updates the dependencies section of a Leiningen project to include OpenCV libraries. Prerequisites are a working Leiningen setup and OpenCV libraries locally available. The updated dependencies allow access to OpenCV functionalities within the project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_5\n\nLANGUAGE: clojure\nCODE:\n```\n(defproject simple-sample \"0.1.0-SNAPSHOT\"\\n  description \"FIXME: write description\"\\n  url \"http://example.com/FIXME\"\\n  license {:name \"Eclipse Public License\"\\n  url \"http://www.eclipse.org/legal/epl-v10.html\"}\\n  dependencies [[org.clojure/clojure \"1.5.1\"]\\n            [opencv/opencv \"2.4.7\"] ; added line\\n            [opencv/opencv-native \"2.4.7\"]]) ;added line\n```\n\n----------------------------------------\n\nTITLE: Enabling pkg-config Generation in OpenCV\nDESCRIPTION: This option enables generation of `.pc` files using `OPENCV_GENERATE_PKGCONFIG`. These files facilitate integration with non-CMake-based projects but may contain incomplete dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DOPENCV_GENERATE_PKGCONFIG=ON ../opencv\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV.js Performance Test for cvtColor in Node.js\nDESCRIPTION: This snippet demonstrates how to run a performance test for the cvtColor function using Node.js. It shows both the command for running all tests and a specific test case using a parameter filter.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/js/perf/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnode perf_cvtcolor.js\n```\n\nLANGUAGE: sh\nCODE:\n```\nnode perf_cvtcolor.js --test_param_filter=\"(1920x1080, COLOR_BGR2GRAY)\"\n```\n\n----------------------------------------\n\nTITLE: Computing Discrete Fourier Transform Using OpenCV in JavaScript\nDESCRIPTION: This snippet shows how to compute the Discrete Fourier Transform (DFT) of an image using the cv.dft function in OpenCV.js. The function requires an input array (src), output array (dst), optional transformation flags, and a parameter to optimize row computation for sparse data (nonzeroRows). Outputs include the transformed frequency domain matrix, with behavior determined by the provided flags. Dependencies: OpenCV.js library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_transforms/js_fourier_transform/js_fourier_transform.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.dft (src, dst, flags = 0, nonzeroRows = 0)\n```\n\n----------------------------------------\n\nTITLE: Refined Kernel Package for OpenCV G-API Integration\nDESCRIPTION: This snippet reflects the updated version of the kernel package after removing the Fluid version of the Box filter kernel. G-API now utilizes OpenCV's implementation, which circumvents former limitations with kernel size constraints. It ensures the flexibility of backend selection within the graph, enhancing performance outcomes without altering the graph structure.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/anisotropic_segmentation/porting_anisotropic_segmentation.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/porting_anisotropic_image_segmentation/porting_anisotropic_image_segmentation_gapi_fluid.cpp kernel_pkg_proper\n```\n\n----------------------------------------\n\nTITLE: OpenVX C++ Wrapper Basic Usage Example\nDESCRIPTION: Demonstrates core functionality of OpenVX C++ wrappers including context creation, graph operations, image processing, and error handling. Shows how to create and process images using Gaussian blur and threshold operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openvx/README.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"ivx.hpp\"\n#include \"ivx_lib_debug.hpp\" // ivx::debug::*\n\nint main()\n{\n\tvx_uint32 width = 640, height = 480;\n\ttry\n\t{\n\t\tivx::Context context = ivx::Context::create();\n\t\tivx::Graph graph = ivx::Graph::create(context);\n\t\tivx::Image\n\t\t    gray = ivx::Image::create(context, width, height, VX_DF_IMAGE_U8),\n\t\t    gb   = ivx::Image::createVirtual(graph),\n\t\t    res  = ivx::Image::create(context, width, height, VX_DF_IMAGE_U8);\n\n\t\tcontext.loadKernels(\"openvx-debug\");  // ivx::debug::*\n\n\t\tivx::debug::fReadImage(context, inputPath, gray);\n\n\t\tivx::Node::create(graph, VX_KERNEL_GAUSSIAN_3x3, gray, gb);\n\t\tivx::Node::create(\n\t\t    graph,\n\t\t    VX_KERNEL_THRESHOLD,\n\t\t    gb,\n\t\t    ivx::Threshold::createBinary(context, VX_TYPE_UINT8, 50),\n\t\t    res\n\t\t);\n\n\t\tgraph.verify();\n\t\tgraph.process();\n\n\t\tivx::debug::fWriteImage(context, res, \"ovx-res-cpp.pgm\");\n    }\n    catch (const ivx::RuntimeError& e)\n    {\n        printf(\"ErrorRuntime: code = %d(%x), message = %s\\n\", e.status(), e.status(), e.what());\n        return e.status();\n    }\n    catch (const ivx::WrapperError& e)\n    {\n        printf(\"ErrorWrapper: message = %s\\n\", e.what());\n        return -1;\n    }\n    catch(const std::exception& e)\n    {\n        printf(\"runtime_error: message = %s\\n\", e.what());\n        return -1;\n    }\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Java Application using SBT (Bash)\nDESCRIPTION: Executes the Java face detection application using the SBT 'run' command. SBT compiles the source code if necessary and then runs the main method of the 'HelloOpenCV' class.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nsbt run\n```\n\n----------------------------------------\n\nTITLE: Embedding Style Transfer Example in HTML - HTML\nDESCRIPTION: This HTML snippet embeds an external style transfer example using an iframe. The 'src' attribute points to a JavaScript-powered interactive demo, and the 'onload' handler dynamically resizes the iframe height based on its content. No dependencies are required beyond a modern browser, and there are no input parameters except the iframe attributes. Output is a live preview of the style transfer effect. The snippet is intended for interactive documentation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_dnn/js_style_transfer/js_style_transfer.markdown#2025-04-22_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<iframe src=\"../../js_style_transfer.html\" width=\"100%\"\n        onload=\"this.style.height=this.contentDocument.body.scrollHeight +'px';\">\n</iframe>\n```\n\n----------------------------------------\n\nTITLE: Creating Trackbar in Python\nDESCRIPTION: This Python snippet uses `cv.createTrackbar` to add a slider to the 'Linear Blend' window. It sets the trackbar's name dynamically, specifies the window name, sets the initial value (0) and maximum value (`alpha_slider_max`), and links it to the `on_trackbar` callback function. The callback is also called initially.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#![create_trackbar]\n# create trackbar\ntrackbar_name = 'Alpha x %d' % alpha_slider_max\ncv.createTrackbar(trackbar_name, 'Linear Blend', 0, alpha_slider_max, on_trackbar)\n\n# Show some stuff\non_trackbar(0)\n#![create_trackbar]\n```\n\n----------------------------------------\n\nTITLE: Detect Edges with Canny in OpenCV Java\nDESCRIPTION: Employs OpenCV's Canny edge detection function in Java for image processing. Requires OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nMat edges = new Mat();\nImgproc.Canny(gray, edges, lowThreshold, highThreshold);\n```\n\n----------------------------------------\n\nTITLE: Saving Images with OpenCV in C++\nDESCRIPTION: This snippet illustrates how to save an image to a file in C++ using OpenCV's cv::imwrite function. The file path and cv::Mat object represent the filename and the image data respectively. Saving is conditional on a specific key press.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/display_image/display_image.markdown#2025-04-22_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nif(key == 's') {\n    imwrite(\"starry_night.jpg\", image);\n    std::cout << \"Image saved!\" << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling WebNN Backend in Build - Bash\nDESCRIPTION: Appends the --webnn flag to build OpenCV.js with support for the WebNN backend, which enables native neural network computations in browsers supporting WebNN API. Dependency: emcmake, Python, WebNN compatible browser or runtime for testing. Parameter: --webnn. Output: OpenCV.js build with WebNN support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nemcmake python ./opencv/platforms/js/build_js.py build_js --webnn\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Model Evaluation with Python\nDESCRIPTION: This command runs the module in evaluation mode to assess PyTorch segmentation models with OpenCV. It logs essential metrics such as pixel accuracy, mean IoU, and inference time.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.segmentation.py_to_py_segm --model_name <pytorch_segm_model_name>\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building OpenCV PnP Tutorial Executables using CMake\nDESCRIPTION: This CMake script segment defines variables for the source directory and target prefix, lists common C++ source files for the PnP examples, includes necessary OpenCV modules, defines two separate executables (`example_tutorial_pnp_registration` and `example_tutorial_pnp_detection`) using their respective main files and the common source list, and finally links both executables against the required OpenCV libraries and dependencies specified by `OPENCV_LINKER_LIBS` and `OPENCV_CPP_SAMPLES_REQUIRED_DEPS`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(sample_dir ${CMAKE_CURRENT_SOURCE_DIR}/tutorial_code/calib3d/real_time_pose_estimation/src/)\n```\n\nLANGUAGE: cmake\nCODE:\n```\nset(target example_tutorial_)\n```\n\nLANGUAGE: cmake\nCODE:\n```\nset(sample_pnplib\n        ${sample_dir}CsvReader.cpp\n        ${sample_dir}CsvWriter.cpp\n        ${sample_dir}ModelRegistration.cpp\n        ${sample_dir}Mesh.cpp\n        ${sample_dir}Model.cpp\n        ${sample_dir}PnPProblem.cpp\n        ${sample_dir}Utils.cpp\n        ${sample_dir}RobustMatcher.cpp\n)\n```\n\nLANGUAGE: cmake\nCODE:\n```\nocv_include_modules_recurse(${OPENCV_CPP_SAMPLES_REQUIRED_DEPS})\n```\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable( ${target}pnp_registration ${sample_dir}main_registration.cpp ${sample_pnplib} )\n```\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable( ${target}pnp_detection ${sample_dir}main_detection.cpp ${sample_pnplib} )\n```\n\nLANGUAGE: cmake\nCODE:\n```\nocv_target_link_libraries(${target}pnp_registration PRIVATE ${OPENCV_LINKER_LIBS} ${OPENCV_CPP_SAMPLES_REQUIRED_DEPS})\n```\n\nLANGUAGE: cmake\nCODE:\n```\nocv_target_link_libraries(${target}pnp_detection PRIVATE ${OPENCV_LINKER_LIBS} ${OPENCV_CPP_SAMPLES_REQUIRED_DEPS})\n```\n\n----------------------------------------\n\nTITLE: Running TensorFlow MobileNet Model Test via OpenCV dnn CLI (Console)\nDESCRIPTION: Displays the full command for launching the model test workflow with default image preprocessing and without evaluation metrics. Dependencies include the dnn_model_runner test infrastructure and a valid MobileNet model. Inputs are the chosen model, test flag, and preprocessing settings; outputs are visually presented predictions. Limitations depend on the correctness of parameters provided.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial.md#2025-04-22_snippet_7\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls --model_name mobilenet --test True --default_img_preprocess True --evaluate False\n```\n\n----------------------------------------\n\nTITLE: Preparing the CMake Build Directory for OpenCV - Shell\nDESCRIPTION: This shell snippet shows how to create and move into a dedicated build directory outside the source repositories, as recommended for CMake-based projects. This approach ensures build artifacts are separated from source files and enables clean builds. The commands use 'mkdir' to create a new directory and 'cd' to change the current working directory in preparation for running CMake configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ mkdir build\n$ cd build\n```\n\n----------------------------------------\n\nTITLE: Running DNN Model Runner in Test Mode\nDESCRIPTION: Command to run the model runner in test mode for model inference with customizable preprocessing options.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_11\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name <pytorch_cls_model_name> --test True --default_img_preprocess <True/False> --evaluate False\n```\n\n----------------------------------------\n\nTITLE: Getting Optimal DFT Size with OpenCV in JavaScript\nDESCRIPTION: This snippet demonstrates the use of cv.getOptimalDFTSize to find an efficient DFT size compatible with FFT requirements. The function takes a vector size (vecsize) and returns an optimal, algorithm-friendly value for improved performance. This helps reduce computation time in Fourier analysis. Dependencies: OpenCV.js library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_transforms/js_fourier_transform/js_fourier_transform.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.getOptimalDFTSize (vecsize)\n```\n\n----------------------------------------\n\nTITLE: Declaring a Kernel Wrapper Factory Function (OpenCV G-API, C++)\nDESCRIPTION: Shows implementation of a C++ function wrapper (factory method) around a G-API kernel to provide a more ergonomic and flexible interface for users. This wrapper can supply optional or default arguments and Doxygen comments, making kernel usage more concise. Depends on the relevant kernel interface definition and standard C++ function syntax.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/doc/20-kernel-api.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// C++ function wrapper for kernel to enable optional parameters and compact syntax\ncv::GMat myFilter2D(const cv::GMat &in, const cv::Mat &kernel, cv::Point anchor = cv::Point(-1, -1))\n{\n    return Filter2D::on(in, kernel, anchor);\n}\n// Enables usage like: auto out = myFilter2D(in, kernel);\n```\n\n----------------------------------------\n\nTITLE: Generating Linearly Separable Training Data for SVM (Java)\nDESCRIPTION: Java implementation for generating linearly separable training data for SVM with random points from two classes. This creates the initial part of the dataset with classes that could be separated by a linear boundary.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n// Set up the linearly separable part of the training data\nint[] labels = { 1, -1 };\nMat trainData = new Mat(2 * NTRAINING_SAMPLES, 2, CvType.CV_32FC1);\nMat trainLabels = new Mat(2 * NTRAINING_SAMPLES, 1, CvType.CV_32SC1);\nRandom rng = new Random(100);\n\n// Generate training samples\nfor (int i = 0; i < 2; i++) {\n    Mat trainClass = trainData.rowRange(i * NTRAINING_SAMPLES, (i + 1) * NTRAINING_SAMPLES);\n    Mat center = trainClass.colRange(0, 1);\n    // randomly generates 100 point samples from a uniform distribution\n    // in the range [0, 1].\n    Core.randu(center, 0, 1);\n    // the x range depends on the class\n    // class 0: [0, 0.4 * WIDTH]\n    // class 1: [0.6 * WIDTH, WIDTH]\n    if (i == 0) {\n        // scale the x coordinate\n        Core.multiply(center, new Scalar(0.2), center);\n        // and add 0.2 to bring it to the range [0.2, 0.4]\n        Core.add(center, new Scalar(0.2), center);\n    } else {\n        // scale the x coordinate\n        Core.multiply(center, new Scalar(0.2), center);\n        // and add 0.6 to bring it to the range [0.6, 0.8]\n        Core.add(center, new Scalar(0.6), center);\n    }\n    // The y coordinate of the points is in range [0, 1]\n    Core.randu(trainClass.colRange(1, 2), 0, 1);\n\n    // Fill the training labels\n    trainLabels.rowRange(i * NTRAINING_SAMPLES, (i + 1) * NTRAINING_SAMPLES).setTo(new Scalar(labels[i]));\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenCV Build Definitions\nDESCRIPTION: Sets basic OpenCV build definitions for the project configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_definitions(-D__OPENCV_BUILD=1)\nadd_definitions(-D__OPENCV_APPS=1)\n```\n\n----------------------------------------\n\nTITLE: Loading Source, Template, and Mask Images (Java)\nDESCRIPTION: Loads the source image and the template image using `Imgcodecs.imread`. It parses command-line arguments to get file paths and optionally loads a mask image. Includes checks to ensure images are loaded successfully.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_14\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/tutorial_template_matching/MatchTemplateDemo.java load_image\n```\n\n----------------------------------------\n\nTITLE: Enabling libjpeg v8 API/ABI Emulation via CMake Option\nDESCRIPTION: Specifies the CMake command-line argument `-DWITH_JPEG8=1` required during the configuration phase of building libjpeg-turbo. This flag instructs the build system to enable emulation of the libjpeg v8 API and ABI, ensuring compatibility with applications originally built against libjpeg v8.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/README.md#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\n-DWITH_JPEG8=1\n```\n\n----------------------------------------\n\nTITLE: Installing Example Source Files for OpenCV TAPI Samples - CMake\nDESCRIPTION: Installs the source files (C++ and header files) and the CMakeLists.txt needed for TAPI examples using the ocv_install_example_src macro. Ensures all relevant files required to build and understand the OpenCV TAPI sample projects are installed as part of the example resources. No external dependencies except those provided by OpenCV build setup.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/tapi/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nocv_install_example_src(tapi *.cpp *.hpp CMakeLists.txt)\n```\n\n----------------------------------------\n\nTITLE: Drawing a Polygon in Java\nDESCRIPTION: Implementation of the MyPolygon function that draws a filled polygon in OpenCV Java. The function creates a MatOfPoint object with the polygon vertices and uses the fillPoly() function to draw a white polygon.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_25\n\nLANGUAGE: java\nCODE:\n```\nprivate static void MyPolygon(Mat img) {\n    int lineType = Core.LINE_8;\n\n    // Create some points\n    Point[] pts = new Point[20];\n    pts[0] = new Point(w/4, 7*w/8);\n    pts[1] = new Point(3*w/4, 7*w/8);\n    pts[2] = new Point(3*w/4, 13*w/16);\n    pts[3] = new Point(11*w/16, 13*w/16);\n    pts[4] = new Point(19*w/32, 3*w/8);\n    pts[5] = new Point(3*w/4, 3*w/8);\n    pts[6] = new Point(3*w/4, w/8);\n    pts[7] = new Point(26*w/40, w/8);\n    pts[8] = new Point(26*w/40, w/4);\n    pts[9] = new Point(22*w/40, w/4);\n    pts[10] = new Point(22*w/40, w/8);\n    pts[11] = new Point(18*w/40, w/8);\n    pts[12] = new Point(18*w/40, w/4);\n    pts[13] = new Point(14*w/40, w/4);\n    pts[14] = new Point(14*w/40, w/8);\n    pts[15] = new Point(w/4, w/8);\n    pts[16] = new Point(w/4, 3*w/8);\n    pts[17] = new Point(13*w/32, 3*w/8);\n    pts[18] = new Point(5*w/16, 13*w/16);\n    pts[19] = new Point(w/4, 13*w/16);\n\n    MatOfPoint ppt = new MatOfPoint(pts);\n\n    List<MatOfPoint> ppts = new ArrayList<>();\n    ppts.add(ppt);\n\n    Imgproc.fillPoly(img,\n            ppts,\n            new Scalar(255, 255, 255),\n            lineType);\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing Matrices (Mat) with OpenCV.js - JavaScript\nDESCRIPTION: Illustrates multiple ways to construct cv.Mat objects: default constructor, by size/type, with row/col/type, and initialization values. Also shows using static functions (zeros, ones, eye) for common matrix patterns. Dependencies include OpenCV.js and the correct specification of matrix dimensions and types. Used as the basis for subsequent image or matrix operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_basic_ops/js_basic_ops.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n// 1. default constructor\nlet mat = new cv.Mat();\n// 2. two-dimensional arrays by size and type\nlet mat = new cv.Mat(size, type);\n// 3. two-dimensional arrays by rows, cols, and type\nlet mat = new cv.Mat(rows, cols, type);\n// 4. two-dimensional arrays by rows, cols, and type with initialization value\nlet mat = new cv.Mat(rows, cols, type, new cv.Scalar());\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\n// 1. Create a Mat which is full of zeros\nlet mat = cv.Mat.zeros(rows, cols, type);\n// 2. Create a Mat which is full of ones\nlet mat = cv.Mat.ones(rows, cols, type);\n// 3. Create a Mat which is an identity matrix\nlet mat = cv.Mat.eye(rows, cols, type);\n```\n\n----------------------------------------\n\nTITLE: Constructing OCR for Digits using kNN in OpenCV\nDESCRIPTION: This Python code demonstrates a simple OCR application for handwritten digits. It leverages OpenCV to read and preprocess an image, splitting it into training and test datasets. The script initializes a kNN classifier to predict handwritten digits' labels, achieving 91% accuracy. Dependencies include OpenCV and NumPy. Inputs are an image containing handwritten digits, and outputs are the accuracy score and label predictions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_knn/py_knn_opencv/py_knn_opencv.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nimg = cv.imread('digits.png')\ngray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n\n# Now we split the image to 5000 cells, each 20x20 size\ncells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]\n\n# Make it into a Numpy array: its size will be (50,100,20,20)\nx = np.array(cells)\n\n# Now we prepare the training data and test data\ntrain = x[:,:50].reshape(-1,400).astype(np.float32) # Size = (2500,400)\ntest = x[:,50:100].reshape(-1,400).astype(np.float32) # Size = (2500,400)\n\n# Create labels for train and test data\nk = np.arange(10)\ntrain_labels = np.repeat(k,250)[:,np.newaxis]\ntest_labels = train_labels.copy()\n\n# Initiate kNN, train it on the training data, then test it with the test data with k=1\nknn = cv.ml.KNearest_create()\nknn.train(train, cv.ml.ROW_SAMPLE, train_labels)\nret,result,neighbours,dist = knn.findNearest(test,k=5)\n\n# Now we check the accuracy of classification\n# For that, compare the result with test_labels and check which are wrong\nmatches = result==test_labels\ncorrect = np.count_nonzero(matches)\naccuracy = correct*100.0/result.size\nprint( accuracy )\n```\n\n----------------------------------------\n\nTITLE: Filtering and Configuring OpenCV C++ Samples\nDESCRIPTION: This snippet segments and checks available GPU or special module support, and then organizes the list of sample projects accordingly. Samples requiring GPU computation are excluded if the related modules aren\\'t available, and other specific directories may be filtered out based on criteria. It also determines package types for samples and manages dependencies, linking them appropriately.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/cpp/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nproject(cpp_samples)\nocv_include_modules_recurse(${OPENCV_CPP_SAMPLES_REQUIRED_DEPS})\nfile(GLOB_RECURSE cpp_samples RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} *.cpp)\nif(NOT HAVE_opencv_cudaarithm OR NOT HAVE_opencv_cudafilters)\n  ocv_list_filterout(cpp_samples \"/gpu/\")\nendif()\nocv_list_filterout(cpp_samples \"real_time_pose_estimation/\")\nocv_list_filterout(cpp_samples \"parallel_backend/\")\nforeach(sample_filename ${cpp_samples})\n  set(package \"cpp\")\n  if(sample_filename MATCHES \"tutorial_code/snippet\")\n    set(package \"snippet\")\n  elseif(sample_filename MATCHES \"tutorial_code\")\n    set(package \"tutorial\")\n  endif()\n  ocv_define_sample(tgt ${sample_filename} ${package})\n  set(deps ${OPENCV_CPP_SAMPLES_REQUIRED_DEPS})\n  if(DEFINED DEPS_${tgt})\n    set(deps ${DEPS_${tgt}})\n  endif()\n  ocv_target_link_libraries(${tgt} PRIVATE ${OPENCV_LINKER_LIBS} ${deps})\n  if(sample_filename MATCHES \"/gpu/\" AND HAVE_opencv_cudaarithm AND HAVE_opencv_cuda_filters)\n    ocv_target_link_libraries(${tgt} PRIVATE opencv_cudaarithm opencv_cudafilters)\n  endif()\n  if(sample_filename MATCHES \"/viz/\")\n    ocv_target_link_libraries(${tgt} PRIVATE ${VTK_LIBRARIES})\n    target_compile_definitions(${tgt} PRIVATE -DUSE_VTK)\n  endif()\n  if(HAVE_OPENGL AND sample_filename MATCHES \"detect_mser\")\n    target_compile_definitions(${tgt} PRIVATE HAVE_OPENGL)\n    ocv_target_link_libraries(${tgt} PRIVATE \"${OPENGL_LIBRARIES}\")\n  endif()\n  if(sample_filename MATCHES \"simd_\")\n    # disabled intentionally - demonstration purposes only\n    #target_include_directories(${tgt} PRIVATE \"${CMAKE_CURRENT_LIST_DIR}\")\n    #target_compile_definitions(${tgt} PRIVATE OPENCV_SIMD_CONFIG_HEADER=opencv_simd_config_custom.hpp)\n    #target_compile_definitions(${tgt} PRIVATE OPENCV_SIMD_CONFIG_INCLUDE_DIR=1)\n    #target_compile_options(${tgt} PRIVATE -mavx2)\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Verifying Successful VideoCapture Opening in C++\nDESCRIPTION: Illustrates how to use the `isOpened()` method of `cv::VideoCapture` to check if the video source (file or camera) was successfully opened. If not, it prints an error message to the console and suggests returning an error code (-1). Requires `<iostream>` and `<opencv2/videoio.hpp>` headers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_input_psnr_ssim.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nif ( !captRefrnc.isOpened())\n  {\n  cout  << \"Could not open reference \" << sourceReference << endl;\n  return -1;\n  }\n```\n\n----------------------------------------\n\nTITLE: Drawing Rotated Bounding Rectangle for Contours in Python with OpenCV\nDESCRIPTION: This snippet demonstrates how to compute and draw a rotated bounding rectangle with minimum area for a contour using cv.minAreaRect(), cv.boxPoints(), and cv.drawContours() functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nrect = cv.minAreaRect(cnt)\nbox = cv.boxPoints(rect)\nbox = np.int0(box)\ncv.drawContours(img,[box],0,(0,0,255),2)\n```\n\n----------------------------------------\n\nTITLE: Acquiring Input Frames for Calibration from Camera/Video/List in C++\nDESCRIPTION: This C++ snippet reference points to code within the main processing loop for camera calibration. It acquires the next input frame from the specified source (camera, video file, or image list) and optionally flips the image if required. It handles conditions where input fails or enough frames have been collected.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/calib3d/camera_calibration/camera_calibration.cpp get_input\n```\n\n----------------------------------------\n\nTITLE: SIMD Source Files Configuration\nDESCRIPTION: Sets up SIMD source files for compilation based on architecture (x86_64 or i386), including various optimized implementations for different instruction sets like SSE2 and AVX2\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/simd/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(CPU_TYPE STREQUAL \"x86_64\")\n  set(SIMD_SOURCES x86_64/jsimdcpu.asm x86_64/jfdctflt-sse.asm\n    x86_64/jccolor-sse2.asm x86_64/jcgray-sse2.asm x86_64/jchuff-sse2.asm\n    [...additional sources...])\nelse()\n  set(SIMD_SOURCES i386/jsimdcpu.asm i386/jfdctflt-3dn.asm\n    [...additional sources...])\nendif()\n```\n\n----------------------------------------\n\nTITLE: Executing OpenCV Annotation Tool Command\nDESCRIPTION: This command initializes the OpenCV annotation tool, which is used for labeling images with object boundaries. It requires paths to the annotations file and image folder and optionally resizes images based on the provided parameters to facilitate easy annotation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/traincascade.markdown#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nopencv_annotation --annotations=/path/to/annotations/file.txt --images=/path/to/image/folder/\n```\n\n----------------------------------------\n\nTITLE: Initializing Face Detector (FaceDetectorYN) - OpenCV DNN C++\nDESCRIPTION: This snippet demonstrates how to initialize the cv::FaceDetectorYN object in C++ using pre-trained ONNX models. The constructor typically requires the detection model path, configuration, and detection parameters such as input size and score threshold. It depends on OpenCV's DNN and face module. The primary input is the model file, and the output is a ready-to-use detector object for running face detection on images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_face/dnn_face.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n// Initialize FaceDetectorYN\ncv::Ptr<cv::FaceDetectorYN> detector = cv::FaceDetectorYN::create(\n    modelPath,                   // Path to face detection .onnx model\n    \"\",                          // No config file usually\n    cv::Size(320, 320),          // Input size to the model\n    /*scoreThreshold*/ 0.9,      // Detection score threshold\n    /*nmsThreshold*/ 0.3,        // Non-max suppression threshold\n    /*topK*/ 5000                // Maximum number of detections\n);\n```\n\n----------------------------------------\n\nTITLE: Computing Rotation Displacement Using OpenCV in Python\nDESCRIPTION: This Python snippet computes rotation displacement between two image views using OpenCV. Dependencies are the OpenCV and NumPy packages. The expected input is the image pair and the result is a matrix detailing displacement.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\nimport numpy as np\n\ndef compute_rotation_displacement(image1, image2):\n    # Code to compute rotation displacement\n    # ...\n\n```\n\n----------------------------------------\n\nTITLE: Generating Build Files with CMake (Static Libs, Windows) (Batch)\nDESCRIPTION: This Batch command runs CMake in the 'build' directory to generate a Visual Studio solution for building OpenCV on Windows. The '-DBUILD_SHARED_LIBS=OFF' option configures the build for static libraries. The '-G \"Visual Studio 10\"' specifies the generator for Visual Studio 2010 (adjust version as needed). The '..' indicates the source directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bat\nCODE:\n```\ncmake -DBUILD_SHARED_LIBS=OFF -G \"Visual Studio 10\" ..\n```\n\n----------------------------------------\n\nTITLE: Generating Build Files with CMake (Static Libs, Unix) (Bash)\nDESCRIPTION: This Bash command runs CMake in the 'build' directory to generate build files (e.g., Makefiles) for compiling OpenCV. The '-DBUILD_SHARED_LIBS=OFF' option configures the build for static libraries, meaning the Java bindings dynamic library will contain all necessary OpenCV code. The '..' indicates the source directory is the parent directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncmake -DBUILD_SHARED_LIBS=OFF ..\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV Source from GitHub - Bash\nDESCRIPTION: Clones the latest OpenCV source code from the official GitHub repository into a specified working directory. Requires git to be installed on the host system and network access to github.com. Replace <my_working_directory> with the desired workspace path. The command downloads the full repository for development and compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/arm_crosscompile_with_cmake.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/<my_working _directory>\ngit clone https://github.com/opencv/opencv.git\n```\n\n----------------------------------------\n\nTITLE: Setting Paths for Doxygen Configuration and Input Files in CMake\nDESCRIPTION: Defines CMake variables storing paths to essential files and directories used in Doxygen configuration. This includes the Doxyfile output path, the root markdown file path, the main BibTeX file, FAQ markdown file, and paths for core tutorials (general, Python, JavaScript) and samples.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\n# additional config\nset(doxyfile \"${CMAKE_CURRENT_BINARY_DIR}/Doxyfile\")\nset(rootfile \"${CMAKE_CURRENT_BINARY_DIR}/root.markdown\")\nset(bibfile \"${CMAKE_CURRENT_SOURCE_DIR}/opencv.bib\")\nset(faqfile \"${CMAKE_CURRENT_SOURCE_DIR}/faq.markdown\")\nset(tutorial_path \"${CMAKE_CURRENT_SOURCE_DIR}/tutorials\")\nset(tutorial_py_path \"${CMAKE_CURRENT_SOURCE_DIR}/py_tutorials\")\nset(CMAKE_DOXYGEN_TUTORIAL_JS_ROOT \"- @ref tutorial_js_root\")\nset(tutorial_js_path \"${CMAKE_CURRENT_SOURCE_DIR}/js_tutorials\")\nset(example_path \"${CMAKE_SOURCE_DIR}/samples\")\n```\n\n----------------------------------------\n\nTITLE: Selecting Best Match Location Based on Method (C++)\nDESCRIPTION: Determines the actual best match location (`matchLoc`) based on the template matching method used. For `TM_SQDIFF` and `TM_SQDIFF_NORMED`, the minimum location (`minLoc`) indicates the best match. For all other methods, the maximum location (`maxLoc`) represents the best match.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_31\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp match_loc\n```\n\n----------------------------------------\n\nTITLE: Declaring Variables for Laplacian Demo in Java\nDESCRIPTION: Declares necessary Java member variables for the LaplaceDemo class, including Mat objects for source, grayscale, and destination images, and a String for the window name. Requires OpenCV Java bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n//! [variables]\nprivate Mat src, srcGray, dst, absDst;\nprivate JFrame frame;\nprivate JLabel imgLabel;\n//! [variables]\n```\n\n----------------------------------------\n\nTITLE: Installing CMake on Linux\nDESCRIPTION: Command to install CMake build configuration tool on Linux using the package manager.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install cmake\n```\n\n----------------------------------------\n\nTITLE: Processing Camera Frames with OpenCV T-API\nDESCRIPTION: C++ code demonstrating how to process camera frames using OpenCV's Transparent API (T-API). This approach uses UMat but requires copying data between OpenCL buffers and images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n// process_tapi\n```\n\n----------------------------------------\n\nTITLE: Blending Images with OpenCV in Java\nDESCRIPTION: This Java snippet demonstrates how to blend two images using OpenCV. The addWeighted() function is utilized to blend images. Both input images need to be of the same dimensions and format. Requires OpenCV library for Java.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/adding_images/adding_images.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n\"@snippet java/tutorial_code/core/AddingImages/AddingImages.java load\"\n```\n\nLANGUAGE: Java\nCODE:\n```\n\"@snippet java/tutorial_code/core/AddingImages/AddingImages.java blend_images\"\n```\n\nLANGUAGE: Java\nCODE:\n```\n\"@snippet java/tutorial_code/core/AddingImages/AddingImages.java display\"\n```\n\n----------------------------------------\n\nTITLE: Saving an Image to File with OpenCV in C++\nDESCRIPTION: Shows saving an image Mat to file using cv::imwrite in C++. Requires OpenCV. The function takes a filename and a Mat object, detecting format by file extension. Returns success as a boolean, and may fail if format is unsupported or path is invalid.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\ncv::imwrite(\"my_image_copy.png\", img);\n```\n\n----------------------------------------\n\nTITLE: Copying Source Image in Callback (Java)\nDESCRIPTION: Within the matching function (triggered by the trackbar), this snippet creates a copy of the original source image using `img.copyTo(img_display)`. Drawing the result rectangle on this copy prevents modification of the base image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_20\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/tutorial_template_matching/MatchTemplateDemo.java copy_source\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Algorithm Decision Tree in C++\nDESCRIPTION: This code snippet is part of the FAST corner detection algorithm, which examines pixel values in a circular pattern around a candidate point. The nested conditionals compare pixel values at various offsets against threshold values (cb and c_b) to determine if a point is a corner, using goto statements to handle the branching logic.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\ngoto structured;\n                          else\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset7] > cb)\n                                if(ptr[offset8] > cb)\n                                  if(ptr[offset4] > cb)\n                                    if(ptr[offset3] > cb)\n                                      goto success_structured;\n                                    else\n                                      if(ptr[offset10] > cb)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                  else\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto structured;\n                                else\n                                  goto structured;\n                              else\n                                goto structured;\n                            else\n                              goto structured;\n                        else\n                          if(ptr[offset3] < c_b)\n                            if(ptr[offset4] < c_b)\n                              if(ptr[offset5] < c_b)\n                                if(ptr[offset1] < c_b)\n                                  if(ptr[offset6] < c_b)\n                                    goto success_structured;\n                                  else\n                                    if(ptr[offset11] < c_b)\n                                      goto success_structured;\n                                    else\n                                      goto structured;\n                                else\n                                  if(ptr[offset6] < c_b)\n                                    if(ptr[offset7] < c_b)\n                                      if(ptr[offset8] < c_b)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto structured;\n                                  else\n                                    goto structured;\n                              else\n                                if(ptr[offset1] < c_b)\n                                  if(ptr[offset10] < c_b)\n                                    if(ptr[offset11] < c_b)\n                                      goto success_structured;\n                                    else\n                                      goto structured;\n                                  else\n                                    goto structured;\n                                else\n                                  goto structured;\n                            else\n                              goto structured;\n                          else\n                            goto structured;\n                      else\n                        if(ptr[offset9] < c_b)\n                          if(ptr[offset5] < c_b)\n                            if(ptr[offset1] < c_b)\n                              if(ptr[offset6] < c_b)\n                                if(ptr[offset3] < c_b)\n                                  if(ptr[offset4] < c_b)\n                                    goto success_structured;\n                                  else\n                                    if(ptr[offset10] < c_b)\n                                      if(ptr[offset11] < c_b)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto structured;\n                                else\n                                  if(ptr[offset8] < c_b)\n                                    if(ptr[offset10] < c_b)\n                                      if(ptr[offset11] < c_b)\n                                        goto success_structured;\n                                      else\n                                        if(ptr[offset4] < c_b)\n                                          if(ptr[offset7] < c_b)\n                                            goto success_structured;\n                                          else\n                                            goto structured;\n                                        else\n                                          goto structured;\n                                    else\n                                      goto structured;\n                                  else\n                                    goto structured;\n                              else\n                                if(ptr[offset11] < c_b)\n                                  if(ptr[offset3] < c_b)\n                                    if(ptr[offset4] < c_b)\n                                      goto success_structured;\n                                    else\n                                      if(ptr[offset10] < c_b)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                  else\n                                    if(ptr[offset8] < c_b)\n                                      if(ptr[offset10] < c_b)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto structured;\n                                else\n                                  goto structured;\n                            else\n                              if(ptr[offset6] < c_b)\n                                if(ptr[offset7] < c_b)\n                                  if(ptr[offset8] < c_b)\n                                    if(ptr[offset4] < c_b)\n                                      if(ptr[offset3] < c_b)\n                                        goto success_structured;\n                                      else\n                                        if(ptr[offset10] < c_b)\n                                          goto success_structured;\n                                        else\n                                          goto structured;\n                                    else\n                                      if(ptr[offset10] < c_b)\n                                        if(ptr[offset11] < c_b)\n                                          goto success_structured;\n                                        else\n                                          goto structured;\n                                      else\n                                        goto structured;\n                                  else\n                                    goto structured;\n                                else\n                                  goto structured;\n                              else\n                                goto structured;\n                          else\n                            if(ptr[offset10] < c_b)\n                              if(ptr[offset11] < c_b)\n                                if(ptr[offset1] < c_b)\n                                  if(ptr[offset3] < c_b)\n                                    goto success_structured;\n                                  else\n                                    if(ptr[offset8] < c_b)\n                                      goto success_structured;\n                                    else\n                                      goto structured;\n                                else\n                                  if(ptr[offset6] < c_b)\n                                    if(ptr[offset7] < c_b)\n                                      if(ptr[offset8] < c_b)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto structured;\n                                  else\n                                    goto structured;\n                              else\n                                goto structured;\n                            else\n                              goto structured;\n                        else\n                          if(ptr[offset3] < c_b)\n                            if(ptr[offset4] < c_b)\n                              if(ptr[offset5] < c_b)\n                                if(ptr[offset1] < c_b)\n                                  if(ptr[offset6] < c_b)\n                                    goto success_structured;\n                                  else\n                                    if(ptr[offset11] < c_b)\n                                      goto success_structured;\n                                    else\n                                      goto structured;\n                                else\n                                  if(ptr[offset6] < c_b)\n                                    if(ptr[offset7] < c_b)\n                                      if(ptr[offset8] < c_b)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto structured;\n                                  else\n                                    goto structured;\n                              else\n                                if(ptr[offset1] < c_b)\n```\n\n----------------------------------------\n\nTITLE: Main Program: Loading a Mesh - OpenCV C++\nDESCRIPTION: Demonstrates how to instantiate a Mesh object and load the mesh data using the load function in the main program. Requires a defined path to the PLY file for the mesh data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nMesh mesh;                // instantiate Mesh object\nmesh.load(ply_read_path); // load an object mesh\n\n```\n\n----------------------------------------\n\nTITLE: Normalizing Arrays with cv.normalize in OpenCV.js\nDESCRIPTION: Describes the `cv.normalize` function signature in OpenCV.js. This function normalizes the input array (`src`) to a specified range or norm, controlled by `alpha`, `beta`, and `norm_type`. It's often used to scale the output of functions like `cv.calcBackProject` for better visualization or further processing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_histograms/js_histogram_backprojection/js_histogram_backprojection.markdown#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ncv.normalize (src, dst, alpha = 1, beta = 0, norm_type = cv.NORM_L2, dtype = -1, mask = new cv.Mat())\n\n@param src        input array.\n@param dst        output array of the same size as src .\n@param alpha      norm value to normalize to or the lower range boundary in case of the range normalization.\n@param beta       upper range boundary in case of the range normalization; it is not used for the norm normalization.\n@param norm_type  normalization type (see cv.NormTypes).\n@param dtype      when negative, the output array has the same type as src; otherwise, it has the same number of channels as src and the depth = CV_MAT_DEPTH(dtype).\n@param mask       optional operation mask.\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV and CUDA Build Environment - CMake\nDESCRIPTION: This CMake script initializes a build environment by enforcing CMake version 3.5 or higher, locating the CUDA toolkit and OpenCV library (requiring the core component), and setting the necessary include directories for both. It then uses CUDA_ADD_EXECUTABLE to compile a CUDA source file ('main.cu') into an executable named 'opencv_thrust' and links it with the found OpenCV libraries. Dependencies include a proper CUDA and OpenCV installation, and the script expects all source files and dependencies to be available in the build context. Outputs are the configured build target and linked binaries tailored for CUDA/OpenCV integration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/cpp/tutorial_code/gpu/gpu-thrust-interop/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nCMAKE_MINIMUM_REQUIRED(VERSION 3.5)\n\nFIND_PACKAGE(CUDA REQUIRED)\nINCLUDE_DIRECTORIES(${CUDA_INCLUDE_DIRS})\n\nFIND_PACKAGE(OpenCV REQUIRED COMPONENTS core)\nINCLUDE_DIRECTORIES(${OpenCV_INCLUDE_DIRS})\n\nCUDA_ADD_EXECUTABLE(opencv_thrust main.cu)\nTARGET_LINK_LIBRARIES(opencv_thrust ${OpenCV_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Java JAR using Ant\nDESCRIPTION: Configures and executes the Ant-based build process for the OpenCV Java JAR when OPENCV_JAVA_SDK_BUILD_TYPE is set to \"ANT\".\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jar/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(OPENCV_JAVA_SDK_BUILD_TYPE STREQUAL \"ANT\")\n  file(MAKE_DIRECTORY \"${OPENCV_JAVA_DIR}/build/classes\")\n\n  configure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/build.xml.in\" \"${OPENCV_JAVA_DIR}/build.xml\" @ONLY)\n  list(APPEND depends \"${OPENCV_JAVA_DIR}/build.xml\")\n\n  ocv_cmake_byproducts(__byproducts BYPRODUCTS \"${OPENCV_JAR_FILE}\")\n  add_custom_command(OUTPUT \"${OPENCV_DEPHELPER}/${the_module}_jar\"\n      ${__byproducts}  # required for add_custom_target() by ninja\n      COMMAND ${ANT_EXECUTABLE} -noinput -k jar\n      COMMAND ${CMAKE_COMMAND} -E touch \"${OPENCV_DEPHELPER}/${the_module}_jar\"\n      WORKING_DIRECTORY \"${OPENCV_JAVA_DIR}\"\n      DEPENDS ${depends}\n      COMMENT \"Generating ${JAR_NAME}\"\n  )\n  add_custom_target(${the_module}_jar DEPENDS \"${OPENCV_DEPHELPER}/${the_module}_jar\")\n```\n\n----------------------------------------\n\nTITLE: Setting TBB Target Properties and Installation Configuration\nDESCRIPTION: Configures the TBB target properties including output names, debug postfixes, and output directories. Sets up the installation paths and license files for proper distribution.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/tbb/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nif (WIN32)\n  set(tbb_debug_postfix \"_debug\") # to fit pragmas in _windef.h inside TBB\nelse()\n  set(tbb_debug_postfix ${OPENCV_DEBUG_POSTFIX})\nendif()\n\nset_target_properties(tbb\n  PROPERTIES OUTPUT_NAME tbb\n  DEBUG_POSTFIX \"${tbb_debug_postfix}\"\n  COMPILE_PDB_NAME tbb\n  COMPILE_PDB_NAME_DEBUG \"tbb${OPENCV_DEBUG_POSTFIX}\"\n  ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n  RUNTIME_OUTPUT_DIRECTORY ${EXECUTABLE_OUTPUT_PATH}\n  )\n\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(tbb PROPERTIES FOLDER \"3rdparty\")\nendif()\n\nocv_install_target(tbb EXPORT OpenCVModules\n    RUNTIME DESTINATION ${OPENCV_BIN_INSTALL_PATH} COMPONENT libs\n    LIBRARY DESTINATION ${OPENCV_LIB_INSTALL_PATH} COMPONENT libs\n    ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev\n    OPTIONAL\n    )\n\nocv_install_3rdparty_licenses(tbb \"${tbb_src_dir}/LICENSE.txt\" \"${tbb_src_dir}/README.md\")\n\nocv_tbb_read_version(\"${tbb_src_dir}/include\" tbb)\n```\n\n----------------------------------------\n\nTITLE: Loading Exposure Images to List using OpenCV Python\nDESCRIPTION: Loads multiple exposure images into a list and stores exposure times needed for HDR processing. The images are loaded using OpenCV's `imread` function, and exposure times are stored in a numpy array with float32 type.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_photo/py_hdr/py_hdr.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\n\n# Loading exposure images into a list\nimg_fn = [\"img0.jpg\", \"img1.jpg\", \"img2.jpg\", \"img3.jpg\"]\nimg_list = [cv.imread(fn) for fn in img_fn]\nexposure_times = np.array([15.0, 2.5, 0.25, 0.0333], dtype=np.float32)\n```\n\n----------------------------------------\n\nTITLE: Executing Model Conversion Pipeline\nDESCRIPTION: This console command runs the model conversion pipeline for ResNet-50 using a specific Python module. It automates the conversion process from PyTorch to ONNX and prepares the model for inference with OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50\n```\n\n----------------------------------------\n\nTITLE: Loading Source Image in Java\nDESCRIPTION: Java snippet demonstrating how to load an image from a file using OpenCV's `Imgcodecs.imread` function. The first command-line argument is expected to be the path to the image file. Includes error handling if the image cannot be loaded.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/Histograms_Matching/histogram_calculation/CalcHistDemo.java Load image\n```\n\n----------------------------------------\n\nTITLE: Changing Image Type from 8UC1 to 32FC1 with OpenCV in C++\nDESCRIPTION: Converts an 8-bit single-channel Mat to a 32-bit float single-channel Mat in C++. The astype method (cv::Mat::convertTo) performs conversion. May be needed for certain algorithms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_37\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat floatMat;\\nimg.convertTo(floatMat, CV_32F);\n```\n\n----------------------------------------\n\nTITLE: Updating Header Inclusions in OpenCV 3.0\nDESCRIPTION: Shows how to replace the old module header inclusion pattern with the new simplified pattern in OpenCV 3.0.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/transition_guide/transition_guide.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// old header\n#include \"opencv2/<module>/<module>.hpp\"\n// new header\n#include \"opencv2/<module>.hpp\"\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV Build Artifacts with CMake - Shell\nDESCRIPTION: Performs the install step using CMake, copying all build artifacts to the configured installation prefix directory. Assumes that the project is already built and the configuration step has been completed. No elevated privileges are required if the install location is user-accessible.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\ncmake --build . --target install\n```\n\n----------------------------------------\n\nTITLE: Setting Histogram Value Range in Python\nDESCRIPTION: Python snippet defining the range of pixel values for the histogram. A variable `histRange` is assigned a tuple `(0, 256)`, specifying the lower (inclusive) and upper (exclusive) bounds for the histogram calculation using `cv.calcHist`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/Histograms_Matching/histogram_calculation/calcHist_Demo.py Set the ranges ( for B,G,R) )\n```\n\n----------------------------------------\n\nTITLE: Instantiating PSNR Buffer in C++\nDESCRIPTION: Creates an instance named `bufferPSNR` of the `BufferPSNR` structure. This instance will be used to hold pre-allocated GPU memory buffers passed to optimized GPU functions, thereby reducing memory allocation overhead during repeated function calls.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nBufferPSNR bufferPSNR;\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Sequences (Lists/Arrays) - OpenCV Python\nDESCRIPTION: This Python snippet demonstrates writing and reading lists (arrays) to XML/YAML/JSON using FileStorage. Use startWriteStruct() and endWriteStruct() to mark the structure, and write() for the value insertion. To read, use getNode() and at() or .size(). Requires opencv-python installed. Handles lists of numbers as commonly used.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfs.startWriteStruct('sequence', cv2.FileNode_SEQ)\\nfs.write('', 1)\\nfs.write('', 2)\\nfs.write('', 3)\\nfs.endWriteStruct()\\n\\nnode = fs.getNode('sequence')\\nfor i in range(node.size()):\\n    value = int(node.at(i).real())\n```\n\n----------------------------------------\n\nTITLE: Implementing Erosion Operation in Java\nDESCRIPTION: Performs erosion operation on an image in Java using OpenCV\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.markdown#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nprivate Mat erode(Mat source) {\n    Mat element = getKernel();\n    Mat imageWithMorphology = new Mat();\n    Imgproc.erode(source, imageWithMorphology, element);\n    return imageWithMorphology;\n}\n```\n\n----------------------------------------\n\nTITLE: Interactive Image Pyramid Loop in Python\nDESCRIPTION: Implements an infinite loop that captures key presses using `cv.waitKey`. It performs image upsampling (`cv.pyrUp`) on 'i', downsampling (`cv.pyrDown`) on 'o', and breaks the loop on ESC. The modified image is displayed in each iteration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    #![loop]\n    tmp = src\n    dst = tmp\n\n    while True:\n        cv.imshow(window_name, dst)\n        c = cv.waitKey(0)\n\n        if c == 27:\n            break\n        elif c == ord('i'):\n            dst = cv.pyrUp(tmp)\n            print ('** Zoom In: Image x 2')\n        elif c == ord('o'):\n            dst = cv.pyrDown(tmp)\n            print ('** Zoom Out: Image / 2')\n\n        tmp = dst\n    #![loop]\n\n    cv.destroyAllWindows()\n    return 0\n```\n\n----------------------------------------\n\nTITLE: Binary Search for Corner Score Threshold in AGAST Algorithm\nDESCRIPTION: This code segment implements a binary search algorithm to determine the optimal threshold for corner detection. It calculates the threshold value based on the intensity differences between neighboring pixels, returning the final threshold when the binary search converges.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_43\n\nLANGUAGE: C++\nCODE:\n```\n        is_a_corner:\n            bmin = b_test;\n            goto end;\n\n        is_not_a_corner:\n            bmax = b_test;\n            goto end;\n\n        end:\n\n        if(bmin == bmax - 1 || bmin == bmax)\n            return bmin;\n        b_test = (bmin + bmax) / 2;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting OpenCV Java Library Properties in CMake\nDESCRIPTION: Configures additional properties for the OpenCV Java library target, including output names, directories, and export symbols.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jni/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(${the_module} PROPERTIES\n    OUTPUT_NAME \"${the_module}${OPENCV_JAVA_LIB_NAME_SUFFIX}\"\n    ARCHIVE_OUTPUT_DIRECTORY ${LIBRARY_OUTPUT_PATH}\n    LIBRARY_OUTPUT_DIRECTORY ${JNI_OUTPUT_PATH}\n    RUNTIME_OUTPUT_DIRECTORY ${EXECUTABLE_OUTPUT_PATH}\n    DEFINE_SYMBOL CVAPI_EXPORTS\n    )\n```\n\n----------------------------------------\n\nTITLE: Extracting Rotational Component Using OpenCV in Python\nDESCRIPTION: This snippet demonstrates extracting the rotational component from two images captured by a rotating camera using the OpenCV library in Python. Prerequisites include installed libraries of OpenCV and NumPy. Inputs are image views and the result is the rotational component.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\nimport numpy as np\n\ndef extract_rotation(image1, image2):\n    # Code to extract rotation component\n    # ...\n\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO™ Network Parameters in G-API\nDESCRIPTION: Demonstrates how to configure network parameters for multiple neural networks using cv::gapi::ie::Params structures. Sets up detection, age and emotion networks with paths to model files and target device.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/interactive_face_detection/interactive_face_detection.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nauto det_net = cv::gapi::ie::Params{\n    cmd.get<std::string>(\"fdm\"),\n    cmd.get<std::string>(\"fdw\"),\n    cmd.get<std::string>(\"dev\")\n};\nauto age_net = cv::gapi::ie::Params{\n    cmd.get<std::string>(\"agm\"),\n    cmd.get<std::string>(\"agw\"),\n    cmd.get<std::string>(\"dev\")\n};\nauto emo_net = cv::gapi::ie::Params{\n    cmd.get<std::string>(\"em\"),\n    cmd.get<std::string>(\"ew\"),\n    cmd.get<std::string>(\"dev\")\n};\n```\n\n----------------------------------------\n\nTITLE: Storing Output Variables for Barcode Detection\nDESCRIPTION: This snippet shows how to create variables for storing the output results of barcode detection. These variables will be used to store corners and other detection-related data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/barcode_detect_and_decode.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/barcode.cpp output\n```\n\n----------------------------------------\n\nTITLE: Capturing and Visualizing Orbbec UVC Depth and Color Images - OpenCV Python\nDESCRIPTION: This snippet demonstrates how to use OpenCV in Python to open an Orbbec UVC depth sensor, capture BGR and depth images, normalize and apply a false color map to the depth image, and display both streams in separate windows. Dependencies include OpenCV (cv2) built with Orbbec support on the target platform. The camera is opened with VideoCapture, frames are grabbed and retrieved using dedicated image types, and the process runs in a loop until a keypress event, after which resources are released.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_uvc.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Open Orbbec Depth Sensor\\norbbec_cap = cv.VideoCapture(0, cv.CAP_OBSENSOR)\\nif not orbbec_cap.isOpened():\\n    print('Cannot open Orbbec depth sensor!')\\n    exit(-1)\\n\\nwhile True:\\n    if not orbbec_cap.grab():\\n        continue\\n\\n    bgr_image = orbbec_cap.retrieve(None, cv.CAP_OBSENSOR_BGR_IMAGE)[1]\\n    if bgr_image is not None:\\n        cv.imshow('BGR', bgr_image)\\n\\n    depth_image = orbbec_cap.retrieve(None, cv.CAP_OBSENSOR_DEPTH_MAP)[1]\\n    if depth_image is not None:\\n        min_depth = 300\\n        max_depth = 5000\\n        depth_image = np.clip(depth_image, min_depth, max_depth)\\n        norm_depth_map = np.uint8(255 * (depth_image - min_depth) / (max_depth - min_depth))\\n        color_depth_map = cv.applyColorMap(norm_depth_map, cv.COLORMAP_JET)\\n        cv.imshow('DEPTH', color_depth_map)\\n\\n    if cv.pollKey() >= 0:\\n        break\\n\\norbbec_cap.release()\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Support in OpenCV CMake\nDESCRIPTION: Sets up Java support for OpenCV, handling different scenarios for Android and non-Android builds. It detects required tools like Apache Ant, Java SDK, and JNI.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_JAVA)\n  if(ANDROID)\n    include(cmake/android/OpenCVDetectAndroidSDK.cmake)\n  else()\n    include(cmake/OpenCVDetectApacheAnt.cmake)\n    if(ANT_EXECUTABLE AND NOT OPENCV_JAVA_IGNORE_ANT)\n      ocv_update(OPENCV_JAVA_SDK_BUILD_TYPE \"ANT\")\n    elseif(NOT ANDROID)\n      find_package(Java)\n      if(Java_FOUND)\n        include(UseJava)\n        ocv_update(OPENCV_JAVA_SDK_BUILD_TYPE \"JAVA\")\n      endif()\n    endif()\n    find_package(JNI)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Saving an Image to File with OpenCV in Python\nDESCRIPTION: Displays saving an image to file with OpenCV's cv2.imwrite in Python. Needs opencv-python installed. Arguments are filename and image ndarray. The datatype and shape of img must correspond to standard image expectations for the output format.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ncv2.imwrite('my_image_copy.png', img)\n```\n\n----------------------------------------\n\nTITLE: Applying BackgroundSubtractor to Extract Foreground Mask\nDESCRIPTION: Processes each frame to generate a foreground mask and updates the background model. The learning rate parameter can be adjusted to control how quickly the background model adapts to changes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/background_subtraction.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n//update the background model\npBackSub->apply(frame, fgMask);\n```\n\nLANGUAGE: Java\nCODE:\n```\n// update the background model\nbackSub.apply(frame, fgMask);\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Update the background model\nfgMask = backSub.apply(frame)\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Logic in C++\nDESCRIPTION: This code snippet is a part of the FAST corner detection algorithm. It contains nested conditional statements comparing pixel values at different offsets to determine if a point is a corner. The algorithm uses goto statements to navigate through different cases.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  goto structured;\nelse\n  goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  if(ptr[offset6] > cb)\n    if(ptr[offset7] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset4] > cb)\n            goto success_structured;\n          else\n            if(ptr[offset11] > cb)\n              goto success_structured;\n            else\n              goto structured;\n        else\n          if(ptr[offset3] > cb)\n            if(ptr[offset4] > cb)\n              goto success_structured;\n            else\n              goto structured;\n          else\n            goto homogeneous;\n      else\n        goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  if(ptr[offset6] > cb)\n    if(ptr[offset7] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset4] > cb)\n          if(ptr[offset3] > cb)\n            goto success_structured;\n          else\n            if(ptr[offset10] > cb)\n              goto success_structured;\n            else\n              goto homogeneous;\n        else\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              goto success_structured;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n      else\n        goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  if(ptr[offset3] < c_b)\n    if(ptr[offset4] < c_b)\n      if(ptr[offset5] < c_b)\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] < c_b)\n            goto success_homogeneous;\n          else\n            if(ptr[offset11] < c_b)\n              goto success_homogeneous;\n            else\n              goto homogeneous;\n        else\n          if(ptr[offset6] < c_b)\n            if(ptr[offset7] < c_b)\n              if(ptr[offset8] < c_b)\n                goto success_structured;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n      else\n        if(ptr[offset1] < c_b)\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              goto success_homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n        else\n          goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  if(ptr[offset9] < c_b)\n    if(ptr[offset5] < c_b)\n      if(ptr[offset1] < c_b)\n        if(ptr[offset6] < c_b)\n          if(ptr[offset3] < c_b)\n            if(ptr[offset4] < c_b)\n              goto success_homogeneous;\n            else\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto success_structured;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n          else\n            if(ptr[offset8] < c_b)\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset4] < c_b)\n                    if(ptr[offset7] < c_b)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n        else\n          if(ptr[offset11] < c_b)\n            if(ptr[offset3] < c_b)\n              if(ptr[offset4] < c_b)\n                goto success_homogeneous;\n              else\n                if(ptr[offset10] < c_b)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n            else\n              if(ptr[offset8] < c_b)\n                if(ptr[offset10] < c_b)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n          else\n            goto homogeneous;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset7] < c_b)\n            if(ptr[offset8] < c_b)\n              if(ptr[offset4] < c_b)\n                if(ptr[offset3] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto homogeneous;\n              else\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto success_structured;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n        else\n          goto homogeneous;\n    else\n      if(ptr[offset10] < c_b)\n        if(ptr[offset11] < c_b)\n          if(ptr[offset1] < c_b)\n            if(ptr[offset3] < c_b)\n              goto success_homogeneous;\n            else\n              if(ptr[offset8] < c_b)\n                goto success_homogeneous;\n              else\n                goto homogeneous;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset7] < c_b)\n                if(ptr[offset8] < c_b)\n                  goto success_homogeneous;\n```\n\n----------------------------------------\n\nTITLE: Including a Whole Example File - Doxygen Markup\nDESCRIPTION: Demonstrates including an entire source code file in documentation using @include. The file path is resolved by doxygen according to configured search paths. Input is relative file path; output is the referenced file's content embedded and formatted in output documentation. Line numbers can be included using @includelineno.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_11\n\nLANGUAGE: markdown\nCODE:\n```\n@include samples/cpp/test.cpp\n```\n\n----------------------------------------\n\nTITLE: Adding YouTube Video to OpenCV Documentation\nDESCRIPTION: Shows how to embed a YouTube video in OpenCV documentation using the _youtube_ command. This snippet demonstrates the syntax for including a video with its unique identifier.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_19\n\nLANGUAGE: markdown\nCODE:\n```\n@youtube{ViPN810E0SU}\n```\n\n----------------------------------------\n\nTITLE: Defining CMake Macro for Remapping Configuration Files (`ocv_remap_files`)\nDESCRIPTION: Defines a CMake macro `ocv_remap_files` that processes a list of input files. For each file ending with `.in`, it uses `configure_file` to substitute variables and generates an output file in a common `${OpenCV_BINARY_DIR}/configured` directory, preserving the relative path. It maintains internal lists (`__remap_config`, `__remap_targets`) to track the source-to-target mappings (for JSON configuration) and the generated target files (for dependencies).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/generator/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(__remap_config \"\") # list of remapped \".in\" files (configure_file)\nset(__remap_targets \"\")\n\nmacro(ocv_remap_files files_list_var)\n  set(target_dir \"${OpenCV_BINARY_DIR}/configured\")\n  foreach(f ${${files_list_var}})\n    if(NOT \"${f}\" MATCHES \"^(.*)\\\\.in$\")\n      #continue()  # since CMake 3.2+\n    else()\n    set(f_ \"${CMAKE_MATCH_1}\")\n    file(RELATIVE_PATH rel_path0 \"${OpenCV_SOURCE_DIR}\" \"${f}\")\n    file(RELATIVE_PATH rel_path1 \"${OpenCV_SOURCE_DIR}\" \"${f_}\")\n    set(__target_file \"${target_dir}/${rel_path1}\")\n    configure_file(\"${f}\" \"${__target_file}\" @ONLY)\n    if(__remap_config)\n      set(__remap_config \"${__remap_config},\\\\n\")\n    endif()\n    set(__remap_config \"${__remap_config}    { \\\"src\\\": \\\"${rel_path0}\\\", \\\"target\\\": \\\"${__target_file}\\\" }\")\n    list(APPEND __remap_targets \"${__target_file}\")\n    endif()\n  endforeach()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Generating Default Progressive JPEG Scan Script (C)\nDESCRIPTION: This function generates a default scan script for writing a progressive-JPEG file. It's the recommended method for creating a progressive file unless a custom scan sequence is needed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_28\n\nLANGUAGE: C\nCODE:\n```\njpeg_simple_progression (j_compress_ptr cinfo)\n```\n\n----------------------------------------\n\nTITLE: Applying GaussianBlur in Clojure REPL using OpenCV\nDESCRIPTION: Applies a GaussianBlur to the source image stored in 'lena' and outputs the blurred image into the 'blurred' variable, with a kernel size of 5x5.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_24\n\nLANGUAGE: Clojure\nCODE:\n```\nuser=> (Imgproc/GaussianBlur lena blurred (Size. 5 5) 3 3)\nnil\n```\n\n----------------------------------------\n\nTITLE: Writing Scanlines in JPEG Compression Cycle in C\nDESCRIPTION: This snippet demonstrates how to write image data scanlines in a JPEG compression cycle using 'jpeg_write_scanlines'. A loop iterates over each scanline, passing it to the compressor. The snippet includes examples for handling various data precisions using 'jpeg12_write_scanlines' or 'jpeg16_write_scanlines'.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_7\n\nLANGUAGE: C\nCODE:\n```\nJSAMPROW row_pointer[1];\nwhile (cinfo.next_scanline < cinfo.image_height) {\n    row_pointer[0] = image_buffer[cinfo.next_scanline];\n    jpeg_write_scanlines(&cinfo, row_pointer, 1);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Histogram Equalization with NumPy and OpenCV\nDESCRIPTION: This code demonstrates how to visualize the histogram and cumulative distribution function (CDF) of an image using NumPy and Matplotlib, preparing for histogram equalization. It loads a grayscale image, calculates its histogram and CDF, then plots both to analyze the pixel value distribution.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('wiki.jpg', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n\nhist,bins = np.histogram(img.flatten(),256,[0,256])\n\ncdf = hist.cumsum()\ncdf_normalized = cdf * float(hist.max()) / cdf.max()\n\nplt.plot(cdf_normalized, color = 'b')\nplt.hist(img.flatten(),256,[0,256], color = 'r')\nplt.xlim([0,256])\nplt.legend(('cdf','histogram'), loc = 'upper left')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Installing Prerequisite Development Packages for OpenCV - Shell\nDESCRIPTION: These shell commands install all the Linux dependencies required to build OpenCV with CUDA support on Ubuntu systems. The commands cover enabling the universe repository, updating package lists, and installing package groups for core libraries, GUI, codecs, and optional Python bindings for both Python 2 and Python 3. Key parameters include the package names and the usage of continuation characters for easier shell script readability. The expected outcome is a system ready to configure and compile OpenCV from source.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ sudo apt-add-repository universe\n$ sudo apt-get update\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ sudo apt-get install \\\n    libglew-dev \\\n    libtiff5-dev \\\n    zlib1g-dev \\\n    libjpeg-dev \\\n    libpng12-dev \\\n    libjasper-dev \\\n    libavcodec-dev \\\n    libavformat-dev \\\n    libavutil-dev \\\n    libpostproc-dev \\\n    libswscale-dev \\\n    libeigen3-dev \\\n    libtbb-dev \\\n    libgtk2.0-dev \\\n    pkg-config\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ sudo apt-get install python-dev python-numpy python-py python-pytest\n# And, optionally:\n$ sudo apt-get install python3-dev python3-numpy python3-py python3-pytest\n```\n\n----------------------------------------\n\nTITLE: Checking Image Generator Presence\nDESCRIPTION: Code to verify if the depth sensor has an image generator capability.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/kinect_openni.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nbool isImageGeneratorPresent = capture.get( CAP_PROP_OPENNI_IMAGE_GENERATOR_PRESENT ) != 0; // or == 1\n```\n\n----------------------------------------\n\nTITLE: Formatting CMake Lists into Doxygen Configuration Strings\nDESCRIPTION: Converts several CMake lists (containing paths for input files, images, excluded files, example files, include roots) into space or newline-separated strings suitable for Doxygen configuration variables (`CMAKE_DOXYGEN_INPUT_LIST`, `CMAKE_DOXYGEN_IMAGE_PATH`, `CMAKE_DOXYGEN_EXCLUDE_LIST`, `CMAKE_DOXYGEN_EXAMPLE_PATH`, `CMAKE_DOXYGEN_INCLUDE_ROOTS`). It also formats the list of enabled sections and sets variables for the Doxygen layout file, output path, module references, BibTeX files, and QHP generation based on a CMake option.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\n# set export variables\nstring(REPLACE \";\" \" \\\\\\n\" CMAKE_DOXYGEN_INPUT_LIST \"${rootfile} ; ${faqfile} ; ${paths_include} ; ${paths_hal_interface} ; ${paths_doc} ; ${tutorial_path} ; ${tutorial_py_path} ; ${tutorial_js_path} ; ${paths_tutorial} ; ${tutorial_contrib_root}\")\nstring(REPLACE \";\" \" \\\\\\n\" CMAKE_DOXYGEN_IMAGE_PATH \"${doxygen_image_path}\")\nstring(REPLACE \";\" \" \\\\\\n\" CMAKE_DOXYGEN_EXCLUDE_LIST \"${CMAKE_DOXYGEN_EXCLUDE_LIST}\")\nstring(REPLACE \";\" \" \" CMAKE_DOXYGEN_ENABLED_SECTIONS \"${CMAKE_DOXYGEN_ENABLED_SECTIONS}\")\n# TODO: remove paths_doc from EXAMPLE_PATH after face module tutorials/samples moved to separate folders\nstring(REPLACE \";\" \" \\\\\\n\" CMAKE_DOXYGEN_EXAMPLE_PATH  \"${example_path} ; ${paths_doc} ; ${paths_sample}\")\nstring(REPLACE \";\" \" \\\\\\n\" CMAKE_DOXYGEN_INCLUDE_ROOTS \"${paths_include}\")\nset(CMAKE_DOXYGEN_LAYOUT \"${CMAKE_CURRENT_BINARY_DIR}/DoxygenLayout.xml\")\nset(CMAKE_DOXYGEN_OUTPUT_PATH \"doxygen\")\nset(CMAKE_DOXYGEN_MAIN_REFERENCE \"${refs_main}\")\nset(CMAKE_DOXYGEN_EXTRA_REFERENCE \"${refs_extra}\")\nset(CMAKE_EXTRA_BIB_FILES \"${bibfile} ${paths_bib}\")\nif (CMAKE_DOXYGEN_GENERATE_QHP)\n  set(CMAKE_DOXYGEN_GENERATE_QHP \"YES\")\nelse()\n  set(CMAKE_DOXYGEN_GENERATE_QHP \"NO\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependency Libraries on Target\nDESCRIPTION: Install runtime library dependencies for OpenCV on the target system through apt. It includes libraries necessary for multimedia tasks and text rendering.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y \\\n    libavcodec60 \\\n    libavformat60 \\\n    libavutil58 \\\n    libswscale7 \\\n    libfreetype6 \\\n    libharfbuzz0b\n\nsudo ldconfig\n```\n\n----------------------------------------\n\nTITLE: Creating Abstract Base Renderer for Camera Preview (OpenGL/Camera Integration) - Java\nDESCRIPTION: This abstract Java class serves as a base renderer for both Camera and Camera2 APIs in an Android OpenGL camera preview application. It manages camera resource handling, texture update logic, and integrates with GLSurfaceView and SurfaceTexture listeners. Implementations must define camera-specific behaviors; the class is critical for integrating Java rendering logic and facilitating JNI OpenGL interoperability.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\npublic abstract class MyGLRendererBase implements GLSurfaceView.Renderer, SurfaceTexture.OnFrameAvailableListener {\\n    protected final String LOGTAG = \"MyGLRendererBase\";\\n\\n    protected SurfaceTexture mSTex;\\n    protected MyGLSurfaceView mView;\\n\\n    protected boolean mGLInit = false;\\n    protected boolean mTexUpdate = false;\\n\\n    MyGLRendererBase(MyGLSurfaceView view) {\\n        mView = view;\\n    }\\n\\n    protected abstract void openCamera();\\n    protected abstract void closeCamera();\\n    protected abstract void setCameraPreviewSize(int width, int height);\\n\\n    public void onResume() {\\n        Log.i(LOGTAG, \"onResume\");\\n    }\\n\\n    public void onPause() {\\n        Log.i(LOGTAG, \"onPause\");\\n        mGLInit = false;\\n        mTexUpdate = false;\\n        closeCamera();\\n        if(mSTex != null) {\\n            mSTex.release();\\n            mSTex = null;\\n            NativeGLRenderer.closeGL();\\n        }\\n    }\\n\\n    @Override\\n    public synchronized void onFrameAvailable(SurfaceTexture surfaceTexture) {\\n        //Log.i(LOGTAG, \"onFrameAvailable\");\\n        mTexUpdate = true;\\n        mView.requestRender();\\n    }\\n\\n    @Override\\n    public void onDrawFrame(GL10 gl) {\\n        //Log.i(LOGTAG, \"onDrawFrame\");\\n        if (!mGLInit)\\n            return;\\n\\n        synchronized (this) {\\n            if (mTexUpdate) {\\n                mSTex.updateTexImage();\\n                mTexUpdate = false;\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenCV-Python Installation\nDESCRIPTION: Python code to verify successful installation of OpenCV-Python by importing the library and checking its version.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import cv2 as cv\n>>> print( cv.__version__ )\n```\n\n----------------------------------------\n\nTITLE: Saving Processed Image with OpenCV in Clojure\nDESCRIPTION: Writes the processed blurred image to a file named 'blurred.png', using OpenCV's imwrite method, and confirms the operation with a true value.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_25\n\nLANGUAGE: Clojure\nCODE:\n```\nuser=> (Highgui/imwrite \"resources/images/blurred.png\" blurred)\ntrue\nuser=> (exit)\n```\n\n----------------------------------------\n\nTITLE: Generating Symmetric Circle Grid Pattern using Python Script (Shell)\nDESCRIPTION: Runs the 'gen_pattern.py' script to generate a symmetric circle grid pattern ('circles') saved as 'circleboard.svg'. The grid has 7 rows and 5 columns, with a circle radius equivalent to a square size of 15mm (radius = square_size / 2). Requires Python and the 'gen_pattern.py' script.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_pattern/camera_calibration_pattern.markdown#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython gen_pattern.py -o circleboard.svg --rows 7 --columns 5 --type circles --square_size 15\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Corner Detectors in C++\nDESCRIPTION: C++ implementation of custom Harris and Shi-Tomasi corner detectors using OpenCV functions. Includes file handling, image processing, and visualization of results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/generic_corner_detector/generic_corner_detector.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <iostream>\\n#include <stdio.h>\\n#include <stdlib.h>\\n#include <opencv2/highgui.hpp>\\n#include <opencv2/imgproc.hpp>\\n\\nusing namespace cv;\\nusing namespace std;\\n\\nMat src, src_gray;\\nMat myHarris_dst, myHarris_copy, Mc;\\nMat myShiTomasi_dst, myShiTomasi_copy;\\n\\nint myShiTomasi_qualityLevel = 50;\\nint myHarris_qualityLevel = 50;\\nint max_qualityLevel = 100;\\n\\ndouble myHarris_minVal, myHarris_maxVal;\\ndouble myShiTomasi_minVal, myShiTomasi_maxVal;\\n\\nRNG rng(12345);\\n\\nconst char* myHarris_window = \"My Harris corner detector\";\\nconst char* myShiTomasi_window = \"My Shi Tomasi corner detector\";\\n\\nvoid myShiTomasi_function( int, void* );\\nvoid myHarris_function( int, void* );\\n\\nint main( int argc, char** argv )\\n{\\n    CommandLineParser parser( argc, argv, \"{@input | pic3.png | input image}\" );\\n    src = imread( samples::findFile( parser.get<String>( \"@input\" ) ) );\\n    if ( src.empty() )\\n    {\\n        cout << \"Could not open or find the image!\\n\" << endl;\\n        cout << \"Usage: \" << argv[0] << \" <Input image>\" << endl;\\n        return -1;\\n    }\\n\\n    cvtColor( src, src_gray, COLOR_BGR2GRAY );\\n\\n    int blockSize = 3, apertureSize = 3;\\n\\n    cornerEigenValsAndVecs( src_gray, myHarris_dst, blockSize, apertureSize, BORDER_DEFAULT );\\n\\n    /* calculate Mc */\\n    Mc = Mat( src_gray.size(), CV_32FC1 );\\n    for( int j = 0; j < src_gray.rows; j++ )\\n    {\\n        for( int i = 0; i < src_gray.cols; i++ )\\n        {\\n            float lambda_1 = myHarris_dst.at<Vec6f>(j, i)[0];\\n            float lambda_2 = myHarris_dst.at<Vec6f>(j, i)[1];\\n            Mc.at<float>(j,i) = lambda_1*lambda_2 - 0.04f*pow( ( lambda_1 + lambda_2 ), 2 );\\n        }\\n    }\\n\\n    minMaxLoc( Mc, &myHarris_minVal, &myHarris_maxVal, 0, 0, Mat() );\\n\\n    /* Create Window and Trackbar */\\n    namedWindow( myHarris_window );\\n    createTrackbar( \"Quality Level:\", myHarris_window, &myHarris_qualityLevel, max_qualityLevel, myHarris_function );\\n    myHarris_function( 0, 0 );\\n\\n    cornerMinEigenVal( src_gray, myShiTomasi_dst, blockSize, apertureSize, BORDER_DEFAULT );\\n\\n    minMaxLoc( myShiTomasi_dst, &myShiTomasi_minVal, &myShiTomasi_maxVal, 0, 0, Mat() );\\n\\n    /* Create Window and Trackbar */\\n    namedWindow( myShiTomasi_window );\\n    createTrackbar( \"Quality Level:\", myShiTomasi_window, &myShiTomasi_qualityLevel, max_qualityLevel, myShiTomasi_function );\\n    myShiTomasi_function( 0, 0 );\\n\\n    waitKey();\\n    return 0;\\n}\\n\\nvoid myShiTomasi_function( int, void* )\\n{\\n    myShiTomasi_copy = src.clone();\\n    myShiTomasi_qualityLevel = MAX(myShiTomasi_qualityLevel, 1);\\n\\n    for( int j = 0; j < src_gray.rows; j++ )\\n    {\\n        for( int i = 0; i < src_gray.cols; i++ )\\n        {\\n            if( myShiTomasi_dst.at<float>(j,i) > myShiTomasi_minVal + ( myShiTomasi_maxVal - myShiTomasi_minVal )*myShiTomasi_qualityLevel/max_qualityLevel )\\n            {\\n                circle( myShiTomasi_copy, Point(i,j), 4, Scalar( rng.uniform(0,255), rng.uniform(0,255), rng.uniform(0,255) ), FILLED );\\n            }\\n        }\\n    }\\n    imshow( myShiTomasi_window, myShiTomasi_copy );\\n}\\n\\nvoid myHarris_function( int, void* )\\n{\\n    myHarris_copy = src.clone();\\n    myHarris_qualityLevel = MAX(myHarris_qualityLevel, 1);\\n\\n    for( int j = 0; j < src_gray.rows; j++ )\\n    {\\n        for( int i = 0; i < src_gray.cols; i++ )\\n        {\\n            if( Mc.at<float>(j,i) > myHarris_minVal + ( myHarris_maxVal - myHarris_minVal )*myHarris_qualityLevel/max_qualityLevel )\\n            {\\n                circle( myHarris_copy, Point(i,j), 4, Scalar( rng.uniform(0,255), rng.uniform(0,255), rng.uniform(0,255) ), FILLED );\\n            }\\n        }\\n    }\\n    imshow( myHarris_window, myHarris_copy );\\n}\n```\n\n----------------------------------------\n\nTITLE: Perspective Projection Model Matrix Equation\nDESCRIPTION: Mathematical representation of the perspective projection model showing how 3D world points are projected into 2D image coordinates using camera intrinsic parameters and transformation matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/calib3d/doc/solvePnP.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Mathematical Notation\nCODE:\n```\n\\begin{bmatrix}\nu \\\\\nv \\\\\n1\n\\end{bmatrix} =\n\\bf{A} \\hspace{0.1em} \\Pi \\hspace{0.2em} ^{c}\\bf{T}_w\n\\begin{bmatrix}\nX_{w} \\\\\nY_{w} \\\\\nZ_{w} \\\\\n1\n\\end{bmatrix}\n```\n\n----------------------------------------\n\nTITLE: Initializing Kalman Filter for Pose Estimation in C++\nDESCRIPTION: Provides a setup for a Kalman Filter to reject poor pose estimates, by defining its state vector, measurement count, and control actions, and initializing with a specified timeframe in C++.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\ncv::KalmanFilter KF;         // instantiate Kalman Filter\n\nint nStates = 18;            // the number of states\nint nMeasurements = 6;       // the number of measured states\nint nInputs = 0;             // the number of action control\n\ndouble dt = 0.125;           // time between measurements (1/FPS)\n\ninitKalmanFilter(KF, nStates, nMeasurements, nInputs, dt);    // init function\n```\n\n----------------------------------------\n\nTITLE: Embedding Histogram Calculation Example with HTML Iframe\nDESCRIPTION: This HTML snippet uses an iframe to embed an interactive example demonstrating histogram calculation (`cv.calcHist`) from an external HTML file ('../../js_histogram_begins_calcHist.html'). The iframe's height is dynamically adjusted based on the loaded content's scroll height using JavaScript within the `onload` event.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_histograms/js_histogram_begins/js_histogram_begins.markdown#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n\\htmlonly\n<iframe src=\"../../js_histogram_begins_calcHist.html\" width=\"100%\"\n        onload=\"this.style.height=this.contentDocument.body.scrollHeight +'px';\">\n</iframe>\n\\endhtmlonly\n```\n\n----------------------------------------\n\nTITLE: Generating Non-Linearly Separable Training Data for SVM (Java)\nDESCRIPTION: Java implementation for creating training data with overlapping classes that cannot be separated linearly. This shows how to add points that would require non-linear decision boundaries in an SVM.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\n// Set up the non-linearly separable part of the training data\nfloat FRAC_LINEAR_SEP = 0.9f; // Fraction of the training samples which will be linearly separable\nint trainingSamplesToAdd = (int) (2 * NTRAINING_SAMPLES * (1 - FRAC_LINEAR_SEP));\nMat extraTrainData = new Mat(trainingSamplesToAdd, 2, CvType.CV_32FC1);\nMat extraTrainLabels = new Mat(trainingSamplesToAdd, 1, CvType.CV_32SC1);\nRandom rng2 = new Random(100);\n\n// Generate extra non-linearly separable points\nfor (int i = 0; i < trainingSamplesToAdd; i++) {\n    int clsIdx = i % 2;\n    float x = rng2.nextFloat();\n    float y = rng2.nextFloat();\n    extraTrainData.put(i, 0, x);\n    extraTrainData.put(i, 1, y);\n    extraTrainLabels.put(i, 0, labels[clsIdx]);\n}\n\n// Merge all the training data\nMat completeTrainData = new Mat();\nMat completeTrainLabels = new Mat();\nCore.vconcat(Arrays.asList(trainData, extraTrainData), completeTrainData);\nCore.vconcat(Arrays.asList(trainLabels, extraTrainLabels), completeTrainLabels);\n```\n\n----------------------------------------\n\nTITLE: Performing Template Matching Operation (C++)\nDESCRIPTION: Calls the core OpenCV function `cv::matchTemplate` to perform the template matching. It passes the source image (`img`), template (`templ`), result matrix (`result`), the selected match method (`match_method`), and the optional mask (`mask`). The size of the result matrix is calculated beforehand.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp match_template\n```\n\n----------------------------------------\n\nTITLE: Checking OpenCV Version for Cross-Version Compatibility\nDESCRIPTION: Shows how to check the OpenCV library version in source code to conditionally support both version 2 and 3 in the same codebase.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/transition_guide/transition_guide.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"opencv2/core/version.hpp\"\n#if CV_MAJOR_VERSION == 2\n// do opencv 2 code\n#elif CV_MAJOR_VERSION == 3\n// do opencv 3 code\n#endif\n```\n\n----------------------------------------\n\nTITLE: Defining an Ant Build Script for Java OpenCV Application (XML)\nDESCRIPTION: This represents the content of a `build.xml` file used by Apache Ant to compile and run a Java application using OpenCV. It defines project properties (like source/build directories), classpath (including the OpenCV JAR), compilation tasks, and execution tasks (setting the `java.library.path` for the native OpenCV library). Specific paths to the OpenCV JAR (`ocvJarDir`) and native library directory (`ocvLibDir`) are expected as properties.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n@include samples/java/ant/build.xml\n```\n\n----------------------------------------\n\nTITLE: Command Line Parameters for ArUco Calibration in OpenCV\nDESCRIPTION: This code snippet shows the command line parameters for running the ArUco board calibration example. It includes parameters for the output calibration file, board dimensions, marker length, marker separation, dictionary ID, and input video or image path.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_calibration/aruco_calibration.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n\"camera_calib.txt\" -w=5 -h=7 -l=100 -s=10 -d=10 -v=path/aruco_videos_or_images\n```\n\n----------------------------------------\n\nTITLE: Establishing Histogram Bin Count in Python\nDESCRIPTION: Python snippet defining the number of bins for the histogram calculation. A variable `histSize` is assigned an integer value (e.g., 256), which will be used as the number of bins when calling `cv.calcHist`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/Histograms_Matching/histogram_calculation/calcHist_Demo.py Establish the number of bins\n```\n\n----------------------------------------\n\nTITLE: SVM Optimization with Misclassification\nDESCRIPTION: Final optimization formula including both margin maximization and misclassification error minimization with slack variables.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_svm/py_svm_basics/py_svm_basics.markdown#2025-04-22_snippet_3\n\nLANGUAGE: latex\nCODE:\n```\n\\min_{w, b_{0}} L(w,b_0) = ||w||^{2} + C \\sum_{i} {\\xi_{i}} \\text{ subject to } y_{i}(w^{T} x_{i} + b_{0}) \\geq 1 - \\xi_{i} \\text{ and } \\xi_{i} \\geq 0 \\text{ } \\forall i\n```\n\n----------------------------------------\n\nTITLE: Displaying Result Image in Java\nDESCRIPTION: Updates the JLabel (`imgLabel`) within a JFrame (`frame`) to display the final absolute Laplacian image (`absDst`). This snippet assumes the necessary Swing GUI components have been set up elsewhere in the class.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_22\n\nLANGUAGE: java\nCODE:\n```\n//! [display]\n/// Show image\nImage img = HighGui.toBufferedImage(absDst);\nImageIcon icon = new ImageIcon(img);\nimgLabel.setIcon(icon);\n//! [display]\n```\n\n----------------------------------------\n\nTITLE: Creating Build Directory for OpenCV on macOS\nDESCRIPTION: These commands create a temporary build directory for OpenCV and navigate into it.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/macos_install/macos_install.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build_opencv\ncd build_opencv\n```\n\n----------------------------------------\n\nTITLE: Reading Image with OpenCV in Clojure REPL\nDESCRIPTION: Initializes and reads an image file into a Mat object using OpenCV's imread function. The image is stored as a 512x512 matrix of CV_8UC3 elements.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_22\n\nLANGUAGE: Clojure\nCODE:\n```\nuser=> (def lena (Highgui/imread \"resources/images/lena.png\"))\n#'user/lena\nuser=> lena\n#<Mat Mat [ 512*512*CV_8UC3, isCont=true, isSubmat=false, nativeObj=0x7f9ab3054c40, dataAddr=0x19fea9010 ]>\n```\n\n----------------------------------------\n\nTITLE: Defining the TBB Library Target with Compiler Settings\nDESCRIPTION: Creates the TBB library target and configures its compile definitions and include directories. Sets up the core settings needed for the TBB library to function correctly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/tbb/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(tbb ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${TBB_SOURCE_FILES})\ntarget_compile_definitions(tbb PUBLIC\n    TBB_USE_GCC_BUILTINS=1\n    __TBB_GCC_BUILTIN_ATOMICS_PRESENT=1\n    TBB_SUPPRESS_DEPRECATED_MESSAGES=1\n)\ntarget_include_directories(tbb SYSTEM PUBLIC $<BUILD_INTERFACE:${tbb_src_dir}/include>\n    PRIVATE \"${CMAKE_CURRENT_BINARY_DIR}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Overriding Message Formatting in JPEG Library (C)\nDESCRIPTION: Defines the `format_message` function signature, a method within the `jpeg_error_mgr` struct. This function constructs a readable error message string based on the error code and parameters stored in `cinfo->err`, placing the result in the provided `buffer`. It's called internally by `output_message`. Overriding this is rare but can be used to implement internationalization or custom message formats.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_46\n\nLANGUAGE: c\nCODE:\n```\nformat_message (j_common_ptr cinfo, char *buffer)\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Decision Tree in C++\nDESCRIPTION: This code snippet implements part of the FAST corner detection algorithm, which efficiently determines if a pixel is a corner by comparing surrounding pixel values. The algorithm uses a decision tree structure with goto statements to quickly navigate through pixel comparisons, checking if pixels at various offsets are brighter than a threshold.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_26\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset4] > cb)\n  if(ptr[offset10] > cb)\n    if(ptr[offset11] > cb)\n      goto is_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset6] < c_b)\n    if(ptr[offset3] > cb)\n      if(ptr[offset4] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset11] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    if(ptr[offset3] > cb)\n      if(ptr[offset4] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset11] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset9] > cb)\n    if(ptr[offset1] < c_b)\n      goto is_not_a_corner;\n    else\n      if(ptr[offset1] > cb)\n        if(ptr[offset6] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              if(ptr[offset3] > cb)\n                goto is_a_corner;\n              else\n                if(ptr[offset8] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          if(ptr[offset6] < c_b)\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                if(ptr[offset3] > cb)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset8] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                if(ptr[offset3] > cb)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset8] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    if(ptr[offset1] < c_b)\n      goto is_not_a_corner;\n    else\n      if(ptr[offset1] > cb)\n        if(ptr[offset6] > cb)\n          if(ptr[offset3] > cb)\n            if(ptr[offset4] > cb)\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          if(ptr[offset6] < c_b)\n            if(ptr[offset3] > cb)\n              if(ptr[offset4] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset3] > cb)\n              if(ptr[offset4] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\nelse\n  if(ptr[offset7] > cb)\n    if(ptr[offset9] < c_b)\n      goto is_not_a_corner;\n    else\n      if(ptr[offset9] > cb)\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] < c_b)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset1] > cb)\n            if(ptr[offset6] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset6] < c_b)\n                if(ptr[offset8] > cb)\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                if(ptr[offset8] > cb)\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      goto is_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Version and Policies\nDESCRIPTION: Configures minimum CMake version requirements and sets appropriate policy versions based on the CMake version being used\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5.1)\nif(CMAKE_VERSION VERSION_LESS 3.12)\n    cmake_policy(VERSION ${CMAKE_VERSION})\nelse()\n    cmake_policy(VERSION 3.5.1...3.29.0)\nendif()\nmessage(STATUS \"Using CMake version ${CMAKE_VERSION}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring RISC-V RVV Optimizations for ZLIB in CMake\nDESCRIPTION: Checks if RISC-V Vector Extension (RVV) is enabled (WITH_RVV) and if the required intrinsics are available (HAVE_RVV_INTRIN). If both conditions are met, it adds RVV-specific source files (RVV_SRCS) to the ZLIB architecture sources (ZLIB_ARCH_SRCS) and sets specific compile flags (RISCVFLAG, NOLTOFLAG) for these files. Otherwise, it disables RVV support by setting WITH_RVV to OFF.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\n            elseif(BASEARCH_RISCV_FOUND)\n        check_rvv_intrinsics()\n        if(WITH_RVV AND HAVE_RVV_INTRIN)\n            add_definitions(-DRISCV_RVV)\n            set(RVV_SRCS ${ARCHDIR}/crc32_rvv.c ${ARCHDIR}/chunkset_rvv.c ${ARCHDIR}/compare256_rvv.c ${ARCHDIR}/slide_hash_rvv.c)\n            add_feature_info(RVV 1 \"Support RISC-V Vector Extension optimizations, using \\\"${RISCVFLAG}\\\"\")\n            list(APPEND ZLIB_ARCH_SRCS ${RVV_SRCS})\n            set_property(SOURCE ${RVV_SRCS} PROPERTY COMPILE_FLAGS \"${RISCVFLAG} ${NOLTOFLAG}\")\n        else()\n            set(WITH_RVV OFF)\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Registering Application Target with OpenCV CMake Macro - CMake\nDESCRIPTION: This snippet uses the \"ocv_add_application\" macro to define an application build target named \"opencv_visualisation\" within an OpenCV CMake project. It specifies critical module dependencies (opencv_core, opencv_highgui, opencv_imgproc, opencv_videoio, opencv_imgcodecs) and lists source files required for the executable. Dependencies include OpenCV libraries, and the macro ensures correct linkage and build configuration. This setup requires CMake and an OpenCV build environment, and does not directly address platform-specific linking constraints.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/visualisation/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nocv_add_application(opencv_visualisation\n    MODULES opencv_core opencv_highgui opencv_imgproc opencv_videoio opencv_imgcodecs\n    SRCS opencv_visualisation.cpp)\n```\n\n----------------------------------------\n\nTITLE: Using RETR_CCOMP for 2-Level Contour Hierarchy in OpenCV Python\nDESCRIPTION: This code snippet shows how to use the RETR_CCOMP mode in OpenCV to structure contours into a two-level hierarchy, distinguishing between external contours and contours of holes within objects.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contours_hierarchy/py_contours_hierarchy.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n>>> hierarchy\narray([[[ 3, -1,  1, -1],\n        [ 2, -1, -1,  0],\n        [-1,  1, -1,  0],\n        [ 5,  0,  4, -1],\n        [-1, -1, -1,  3],\n        [ 7,  3,  6, -1],\n        [-1, -1, -1,  5],\n        [ 8,  5, -1, -1],\n        [-1,  7, -1, -1]]])\n```\n\n----------------------------------------\n\nTITLE: Defining Scan Information for Progressive/Noninterleaved JPEG in libjpeg (C)\nDESCRIPTION: These C fields within the compression parameters structure (`cinfo`) are used to create multi-scan (progressive or noninterleaved) JPEG files. `scan_info` is a pointer to an array of `jpeg_scan_info` structures, and `num_scans` specifies the number of elements in this array. If `scan_info` is not NULL, the compressor writes a scan for each record; otherwise (default NULL), a single-scan sequential JPEG is created. The `jpeg_simple_progression` function can help create a standard progressive scan sequence.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_36\n\nLANGUAGE: C\nCODE:\n```\nconst jpeg_scan_info *scan_info\n```\n\nLANGUAGE: C\nCODE:\n```\nint num_scans\n```\n\n----------------------------------------\n\nTITLE: Selecting Best Match Location Based on Method (Python)\nDESCRIPTION: Assigns the correct location (either `minLoc` or `maxLoc`) to the `matchLoc` variable based on the selected `method`. Methods `cv2.TM_SQDIFF` and `cv2.TM_SQDIFF_NORMED` use the minimum location as the best match, while the others use the maximum location.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/match_template/match_template.py match_loc\n```\n\n----------------------------------------\n\nTITLE: Checking System Endianness in CMake\nDESCRIPTION: Includes the standard CMake module `TestBigEndian` and runs the `test_big_endian` function to determine if the target system is big-endian. The result is stored in the `OPJ_BIG_ENDIAN` variable. This check is skipped when compiling for Emscripten.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\n#-----------------------------------------------------------------------------\n# Big endian test:\nif (NOT EMSCRIPTEN)\ninclude(TestBigEndian)\ntest_big_endian(OPJ_BIG_ENDIAN)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Application with OpenCV in CMake\nDESCRIPTION: This snippet demonstrates how to add an OpenCV application using CMake by specifying needed OpenCV modules like opencv_core, opencv_highgui, opencv_imgproc, opencv_imgcodecs, and opencv_videoio. The snippet registers opencv_annotation.cpp as the source file. The code ensures the application links with these specific OpenCV functionalities.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/annotation/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nocv_add_application(opencv_annotation\\n    MODULES opencv_core opencv_highgui opencv_imgproc opencv_imgcodecs opencv_videoio\\n    SRCS opencv_annotation.cpp)\n```\n\n----------------------------------------\n\nTITLE: Setting Stitching Module Dependencies in CMake\nDESCRIPTION: Defines the dependencies for the stitching module, including optional CUDA components and handling for shared library builds. It adjusts dependencies based on build configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/stitching/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(STITCHING_CONTRIB_DEPS \"opencv_xfeatures2d\")\nif(BUILD_SHARED_LIBS AND BUILD_opencv_world AND OPENCV_WORLD_EXCLUDE_EXTRA_MODULES)\n  set(STITCHING_CONTRIB_DEPS \"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Destination Buffer State in JPEG Library (C)\nDESCRIPTION: Shows the essential fields within a custom `jpeg_destination_mgr` struct required by the JPEG compression library. `next_output_byte` points to the next available byte in the output buffer, and `free_in_buffer` indicates the number of bytes remaining. The library manages these fields, incrementing the pointer and decrementing the count as it writes compressed data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_49\n\nLANGUAGE: c\nCODE:\n```\nJOCTET *next_output_byte;   /* => next byte to write in buffer */\nsize_t free_in_buffer;      /* # of byte spaces remaining in buffer */\n```\n\n----------------------------------------\n\nTITLE: Creating and Importing OpenCV Point in Clojure\nDESCRIPTION: The code demonstrates creating an instance of the OpenCV Point class and using import to simplify the access to Java classes from the OpenCV library. It helps reduce boilerplate when working with Java libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_9\n\nLANGUAGE: clojure\nCODE:\n```\nuser=> (org.opencv.core.Point. 0 0)\n#<Point {0.0, 0.0}>\n```\n\n----------------------------------------\n\nTITLE: Setting Image to Black with OpenCV in Java\nDESCRIPTION: Demonstrates setting an entire image to zero in Java OpenCV. The setTo method changes all pixel values to black. Input is a Mat; output is the same Mat with all elements zeroed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_29\n\nLANGUAGE: Java\nCODE:\n```\nimg.setTo(new Scalar(0));\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV for Android with OpenCL Support - Bash\nDESCRIPTION: This bash script orchestrates a CMake/Ninja-based build of OpenCV for Android with OpenCL support. It sets up relevant Android environment variables, configures CMake with options for OpenCL, ABI, and example builds, and initiates the build directory. The script should be run with accurate paths for NDK, OpenCL SDK, and OpenCV sources to ensure proper build completion.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd path_to_opencv && mkdir build && cd build\\nexport NDK_VERSION=25.2.9519653\\nexport ANDROID_SDK=/home/user/Android/Sdk/\\nexport ANDROID_OPENCL_SDK=/path_to_ANDROID_OPENCL_SDK/\\nexport ANDROID_HOME=$ANDROID_SDK\\nexport ANDROID_NDK_HOME=$ANDROID_SDK/ndk/$NDK_VERSION/\\ncmake -GNinja -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK_HOME/build/cmake/android.toolchain.cmake -DANDROID_STL=c++_shared -DANDROID_NATIVE_API_LEVEL=24\\n-DANDROID_SDK=$ANDROID_SDK -DANDROID_NDK=$ANDROID_NDK_HOME -DBUILD_JAVA=ON -DANDROID_HOME=$ANDROID_SDK -DBUILD_ANDROID_EXAMPLES=ON\\n-DINSTALL_ANDROID_EXAMPLES=ON -DANDROID_ABI=arm64-v8a -DWITH_OPENCL=ON -DANDROID_OPENCL_SDK=$ANDROID_OPENCL_SDK ..\n```\n\n----------------------------------------\n\nTITLE: Creating Custom JPEG Quantization Table (C)\nDESCRIPTION: This function allows creation of an arbitrary quantization table. The basic_table values are multiplied by scale_factor/100 and clamped to 1-65535 (or 1-255 if force_baseline is TRUE).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_27\n\nLANGUAGE: C\nCODE:\n```\njpeg_add_quant_table (j_compress_ptr cinfo, int which_tbl,\n                      const unsigned int *basic_table,\n                      int scale_factor, boolean force_baseline)\n```\n\n----------------------------------------\n\nTITLE: Declaring the OpenCV World Module using ocv_add_module (CMake)\nDESCRIPTION: Uses the custom CMake function `ocv_add_module` to formally declare the `world` module. It specifies `opencv_core` as an initial dependency. This function is part of OpenCV's custom CMake script infrastructure and likely sets up basic module properties and dependencies required later in the build process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/world/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nocv_add_module(world opencv_core)\n```\n\n----------------------------------------\n\nTITLE: Verifying Pkg-config for AArch64 FFmpeg (Bash)\nDESCRIPTION: Uses `pkg-config` with environment variables set for `aarch64` to retrieve compiler and linker flags for the core FFmpeg libraries (libavcodec, libavformat, libavutil, libswscale) installed for the `arm64` architecture. This confirms that the FFmpeg development packages for the target architecture are correctly installed and discoverable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nPKG_CONFIG_PATH=/usr/lib/aarch64-linux-gnu/pkgconfig:/usr/share/pkgconfig \\\n    PKG_CONFIG_LIBDIR=/usr/lib/aarch64-linux-gnu \\\n    PKG_CONFIG_SYSROOT_DIR=/ \\\n       pkg-config libavcodec libavformat libavutil libswscale --cflags --libs\n-I/usr/include/aarch64-linux-gnu -L/usr/lib/aarch64-linux-gnu -lavcodec -lavformat -lavutil -lswscale\n```\n\n----------------------------------------\n\nTITLE: Finalize Method for Custom Layer OpenCV C++\nDESCRIPTION: Final preparations after \\'getMemoryShapes\\', tailored to known input dimensions in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/custom_layers.hpp MyLayer::finalize\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection in C++\nDESCRIPTION: This code snippet is part of the FAST corner detection algorithm. It compares pixel values at different offsets around a central pixel to determine if it's a corner. The algorithm uses nested if-else statements and goto labels for efficient branching.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_37\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset6] < c_b)\n    if(ptr[offset3] < c_b)\n      if(ptr[offset4] < c_b)\n        if(ptr[offset10] < c_b)\n          if(ptr[offset11] < c_b)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    if(ptr[offset3] < c_b)\n      if(ptr[offset4] < c_b)\n        if(ptr[offset10] < c_b)\n          if(ptr[offset11] < c_b)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset1] > cb)\n    goto is_not_a_corner;\n  else\n    if(ptr[offset1] < c_b)\n      if(ptr[offset6] > cb)\n        if(ptr[offset3] < c_b)\n          if(ptr[offset4] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset3] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          if(ptr[offset3] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  if(ptr[offset7] < c_b)\n    if(ptr[offset9] < c_b)\n      if(ptr[offset1] > cb)\n        if(ptr[offset6] > cb)\n          goto is_not_a_corner;\n        else\n          if(ptr[offset6] < c_b)\n            if(ptr[offset8] < c_b)\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] > cb)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                if(ptr[offset3] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset8] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  if(ptr[offset3] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset8] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  if(ptr[offset3] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset8] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n        else\n          if(ptr[offset6] > cb)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset8] < c_b)\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n    else\n      if(ptr[offset1] > cb)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] > cb)\n            if(ptr[offset3] < c_b)\n              if(ptr[offset4] < c_b)\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset4] < c_b)\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Handling OpenCV Exceptions Using try-catch Blocks (C++)\nDESCRIPTION: Demonstrates the standard C++ approach for handling exceptions thrown by OpenCV functions. A `try` block encloses the OpenCV calls that might potentially throw an error. A `catch` block specifically targets `cv::Exception` (or its base `std::exception`), retrieves the error message using the `.what()` method, and prints it. This pattern allows for graceful recovery or reporting of runtime errors.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/doc/intro.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n    try\n    {\n        ... // call OpenCV\n    }\n    catch (const cv::Exception& e)\n    {\n        const char* err_msg = e.what();\n        std::cout << \"exception caught: \" << err_msg << std::endl;\n    }\n```\n\n----------------------------------------\n\nTITLE: Loading Recorded Video in Command Line\nDESCRIPTION: Demonstrates how to run a C++ program with a video file as input using a command-line argument. The video path needs to be absolute.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n./cpp-tutorial-pnp_detection --video=/absolute_path_to_your_video.mp4\n```\n\n----------------------------------------\n\nTITLE: Implementing Shi-Tomasi Corner Detection in Java\nDESCRIPTION: Java implementation of the Shi-Tomasi corner detection algorithm using OpenCV's goodFeaturesToTrack function. This sample demonstrates corner detection in an image with Java bindings for OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/trackingmotion/good_features_to_track/good_features_to_track.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nThis tutorial code's is shown lines below. You can also download it from\n[here](https://github.com/opencv/opencv/tree/4.x/samples/java/tutorial_code/TrackingMotion/good_features_to_track/GoodFeaturesToTrackDemo.java)\n```\n\n----------------------------------------\n\nTITLE: Implementing Non-Maximum Suppression for Corner Detection in C++\nDESCRIPTION: This code snippet performs non-maximum suppression on a set of detected corners (keypoints). It compares each corner with its neighbors above and to the left, maintaining a list of flags to track local maxima. The algorithm ensures that only the strongest corners in each local neighborhood are retained.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_46\n\nLANGUAGE: C++\nCODE:\n```\ncurrCorner = kpts.begin();\n\nnmsFlags.resize((int)num_Corners);\n\n// set all flags to MAXIMUM\nfor(j = 0; j < num_Corners; j++)\n    nmsFlags[j] = -1;\n\nfor(curr_idx = 0; curr_idx < num_Corners; curr_idx++)\n{\n    int t;\n    // check above\n    if(lastRow + 1 < currCorner->pt.y)\n    {\n        lastRow = next_lastRow;\n        lastRowCorner_ind = next_lastRowCorner_ind;\n    }\n    if(next_lastRow != currCorner->pt.y)\n    {\n        next_lastRow = (size_t) currCorner->pt.y;\n        next_lastRowCorner_ind = curr_idx;\n    }\n    if(lastRow + 1 == currCorner->pt.y)\n    {\n        // find the corner above the current one\n        while( (kpts[lastRowCorner_ind].pt.x < currCorner->pt.x)\n            && (kpts[lastRowCorner_ind].pt.y == lastRow) )\n            lastRowCorner_ind++;\n\n            if( (kpts[lastRowCorner_ind].pt.x == currCorner->pt.x)\n             && (lastRowCorner_ind != curr_idx) )\n            {\n                size_t w = lastRowCorner_ind;\n                // find the maximum in this block\n                while(nmsFlags[w] != -1)\n                    w = nmsFlags[w];\n\n                if(kpts[curr_idx].response < kpts[w].response)\n                    nmsFlags[curr_idx] = (int)w;\n                else\n                    nmsFlags[w] = (int)curr_idx;\n            }\n    }\n\n    // check left\n    t = (int)curr_idx - 1;\n    if( (curr_idx != 0) && (kpts[t].pt.y == currCorner->pt.y)\n     && (kpts[t].pt.x + 1 == currCorner->pt.x) )\n    {\n        int currCornerMaxAbove_ind = nmsFlags[curr_idx];\n        // find the maximum in that area\n        while(nmsFlags[t] != -1)\n            t = nmsFlags[t];\n        // no maximum above\n        if(currCornerMaxAbove_ind == -1)\n        {\n            if((size_t)t != curr_idx)\n            {\n                if ( kpts[curr_idx].response < kpts[t].response )\n                    nmsFlags[curr_idx] = t;\n                else\n                    nmsFlags[t] = (int)curr_idx;\n            }\n        }\n        else // maximum above\n        {\n            if(t != currCornerMaxAbove_ind)\n            {\n                if(kpts[currCornerMaxAbove_ind].response < kpts[t].response)\n                {\n                    nmsFlags[currCornerMaxAbove_ind] = t;\n                    nmsFlags[curr_idx] = t;\n                }\n                else\n                {\n                    nmsFlags[t] = currCornerMaxAbove_ind;\n                    nmsFlags[curr_idx] = currCornerMaxAbove_ind;\n                }\n            }\n        }\n    }\n    currCorner++;\n}\n\n// collecting maximum corners\nfor(curr_idx = 0; curr_idx < num_Corners; curr_idx++)\n{\n    if (nmsFlags[curr_idx] == -1)\n        keypoints.push_back(kpts[curr_idx]);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Defining Highgui Header Files in CMake\nDESCRIPTION: Sets the CMake variable `highgui_hdrs` to a list containing the path to the precompiled header file `precomp.hpp` for the highgui module. This variable will likely be used when defining the module's target to specify its header files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(highgui_hdrs\n    ${CMAKE_CURRENT_LIST_DIR}/src/precomp.hpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Running Classification Example from Command Line\nDESCRIPTION: The command line snippet demonstrates executing the complete classification example, specifying model files, input file, and preprocessing parameters to obtain the classification result for a provided image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_googlenet/dnn_googlenet.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n@code\n./example_dnn_classification --model=bvlc_googlenet.caffemodel --config=bvlc_googlenet.prototxt --width=224 --height=224 --classes=classification_classes_ILSVRC2012.txt --input=space_shuttle.jpg --mean=\"104 117 123\"\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV and OpenCV Contrib Repositories (Shell)\nDESCRIPTION: These shell commands use the `git clone` utility to download the source code for the main OpenCV library and the `opencv_contrib` repository from their respective GitHub locations. Cloning both repositories is necessary for building OpenCV with FastCV, as the FastCV module resides within the `opencv_contrib` repository.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_fastcv/building_fastcv.markdown#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/opencv/opencv.git\ngit clone https://github.com/opencv/opencv_contrib.git\n```\n\n----------------------------------------\n\nTITLE: Generating Custom ArUco Dictionary using C++ Utility (Shell)\nDESCRIPTION: Executes the compiled C++ utility 'example_cpp_aruco_dict_utils.exe' to generate a custom ArUco dictionary and save it to 'my_dict.json'. The dictionary will contain 30 markers, each with a size of 5 bits. Requires the compiled 'aruco_dict_utils.cpp' executable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_pattern/camera_calibration_pattern.markdown#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nbin/example_cpp_aruco_dict_utils.exe my_dict.json -nMarkers=30 -markerSize=5\n```\n\n----------------------------------------\n\nTITLE: Configuring Quirc Library Build in CMake for OpenCV\nDESCRIPTION: This CMake script configures the Quirc QR code decoding library as a static dependency for OpenCV. It sets up include directories, collects source files, configures build properties, and defines installation rules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/quirc/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(quirc)\n\nset(CURR_INCLUDE_DIR \"${CMAKE_CURRENT_LIST_DIR}/include\")\n\nset_property(GLOBAL PROPERTY QUIRC_INCLUDE_DIR ${CURR_INCLUDE_DIR})\nocv_include_directories(${CURR_INCLUDE_DIR})\n\nfile(GLOB_RECURSE quirc_headers RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\" \"include/*.h\")\nfile(GLOB_RECURSE quirc_sources RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\" \"src/*.c\")\n\nadd_library(${PROJECT_NAME} STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${quirc_headers} ${quirc_sources})\nocv_warnings_disable(CMAKE_C_FLAGS -Wunused-variable -Wshadow)\n\nset_target_properties(${PROJECT_NAME}\n  PROPERTIES OUTPUT_NAME ${PROJECT_NAME}\n  DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n  COMPILE_PDB_NAME ${PROJECT_NAME}\n  COMPILE_PDB_NAME_DEBUG \"${PROJECT_NAME}${OPENCV_DEBUG_POSTFIX}\"\n  ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n  )\n\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(${PROJECT_NAME} PROPERTIES FOLDER \"3rdparty\")\nendif()\n\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(${PROJECT_NAME} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev OPTIONAL)\nendif()\n\nocv_install_3rdparty_licenses(${PROJECT_NAME} LICENSE)\n```\n\n----------------------------------------\n\nTITLE: Performance Benchmark Results\nDESCRIPTION: Shows performance comparison between CPU and GPU implementations for PSNR and MSSIM calculations, including optimized GPU versions. The results demonstrate significant performance improvements of nearly 100% using GPU optimization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nTime of PSNR CPU (averaged for 10 runs): 41.4122 milliseconds. With result of: 19.2506\nTime of PSNR GPU (averaged for 10 runs): 158.977 milliseconds. With result of: 19.2506\nInitial call GPU optimized:              31.3418 milliseconds. With result of: 19.2506\nTime of PSNR GPU OPTIMIZED ( / 10 runs): 24.8171 milliseconds. With result of: 19.2506\n\nTime of MSSIM CPU (averaged for 10 runs): 484.343 milliseconds. With result of B0.890964 G0.903845 R0.936934\nTime of MSSIM GPU (averaged for 10 runs): 745.105 milliseconds. With result of B0.89922 G0.909051 R0.968223\nTime of MSSIM GPU Initial Call            357.746 milliseconds. With result of B0.890964 G0.903845 R0.936934\nTime of MSSIM GPU OPTIMIZED ( / 10 runs): 203.091 milliseconds. With result of B0.890964 G0.903845 R0.936934\n```\n\n----------------------------------------\n\nTITLE: Synchronizing and Pairing Depth and Color Frames from Astra Camera\nDESCRIPTION: Implements a post-synchronization procedure to pair depth and color frames based on their timestamps. This ensures that frames from both streams represent the same moment in time, which is critical for applications that combine color and depth data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n// Wait for frames from both streams\nunique_lock<mutex> depthLk(depthFramesMtx, defer_lock);\nunique_lock<mutex> colorLk(colorFramesMtx, defer_lock);\nlock(depthLk, colorLk);\n\n// If either queue is empty, wait for frames\nwhile (isRunning.load() && (depthFrames.empty() || colorFrames.empty())) {\n    if (depthFrames.empty()) {\n        depthLk.unlock();\n        depthFramesCV.wait(depthLk);\n    }\n\n    if (colorFrames.empty()) {\n        colorLk.unlock();\n        colorFramesCV.wait(colorLk);\n    }\n}\n\n// Get the timestamp of the first frame in each queue\nauto depthTime = depthFrames.front().timestamp;\nauto colorTime = colorFrames.front().timestamp;\n\n// Calculate the time difference between the frames\nauto timeDiff = chrono::duration_cast<chrono::milliseconds>(colorTime - depthTime).count();\n\n// Drop frames until synchronization is achieved\nif (abs(timeDiff) > 0.5 * 1000 / 30) {  // 0.5 * frame_period\n    if (timeDiff > 0) {\n        depthFrames.pop_front();\n    } else {\n        colorFrames.pop_front();\n    }\n    continue;\n}\n\n// Get the synchronized frames\nMat depthFrame = depthFrames.front().frame;\nMat colorFrame = colorFrames.front().frame;\n\n// Remove the frames from the queues\ndepthFrames.pop_front();\ncolorFrames.pop_front();\n\n// Now, the frames are synchronized and can be processed together\nimshow(\"Color\", colorFrame);\nimshow(\"Depth\", depthFrame);\n```\n\n----------------------------------------\n\nTITLE: Checking Contour Convexity in Python with OpenCV\nDESCRIPTION: This snippet demonstrates how to check if a contour is convex using the cv.isContourConvex() function. It returns a boolean value indicating convexity.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nk = cv.isContourConvex(cnt)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Project Library Directory and Updating Eclipse with SBT (Bash)\nDESCRIPTION: Creates a 'lib' directory, copies the compiled OpenCV JAR file into it for dependency management, and optionally updates the Eclipse project configuration using the SBT command. This makes the OpenCV library available to the Java project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nmkdir lib\ncp <opencv_dir>/build/bin/opencv_<version>.jar lib/\nsbt eclipse\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building OpenCV with FastCV for Qualcomm Linux (Shell)\nDESCRIPTION: This sequence of shell commands outlines the process for configuring and compiling OpenCV with FastCV enabled for Qualcomm Linux. First, it creates and navigates into a `build` directory. Then, `cmake` is invoked to configure the project for cross-compilation targeting Linux on AArch64 (`-DCMAKE_SYSTEM_NAME=Linux -DCMAKE_SYSTEM_PROCESSOR=aarch64`), enabling FastCV (`-DWITH_FASTCV=ON`), requesting shared libraries (`-DBUILD_SHARED_LIBS=ON`), and specifying the path to the extra modules in `opencv_contrib`, including the FastCV module (`-DOPENCV_EXTRA_MODULES_PATH=../opencv_contrib/modules/fastcv/`). Finally, `make -j$(nproc)` starts the parallel compilation process using all available CPU cores.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_fastcv/building_fastcv.markdown#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmkdir build\ncd build\ncmake -DCMAKE_SYSTEM_NAME=Linux -DCMAKE_SYSTEM_PROCESSOR=aarch64 -DWITH_FASTCV=ON -DBUILD_SHARED_LIBS=ON -DOPENCV_EXTRA_MODULES_PATH=../opencv_contrib/modules/fastcv/ ../opencv\nmake -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Random Access with At Method in OpenCV\nDESCRIPTION: Examines using the cv::Mat::at() function to randomly access or modify specific image elements by row and column indices, while noting the importance of input type and ensuring bounds safety in debug mode.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_scan_images/how_to_scan_images.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_scan_images.cpp scan-random\n```\n\n----------------------------------------\n\nTITLE: Complete Implementation of calcGST() Function in G-API\nDESCRIPTION: Provides the full implementation of the calcGST() function using G-API operations, including calculation of J matrix and other components.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/anisotropic_segmentation/porting_anisotropic_segmentation.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ncv::GMat calcGST(const cv::GMat& inputImage, const int w)\n{\n    auto img = cv::gapi::convertTo(inputImage, CV_32F);\n    auto imgDiffX = cv::gapi::Sobel(img, CV_32F, 1, 0, 3);\n    auto imgDiffY = cv::gapi::Sobel(img, CV_32F, 0, 1, 3);\n    auto imgDiffXY = cv::gapi::mul(imgDiffX, imgDiffY);\n    auto imgDiffXX = cv::gapi::mul(imgDiffX, imgDiffX);\n    auto imgDiffYY = cv::gapi::mul(imgDiffY, imgDiffY);\n\n    auto J11 = cv::gapi::boxFilter(imgDiffXX, CV_32F, cv::Size(w,w));\n    auto J22 = cv::gapi::boxFilter(imgDiffYY, CV_32F, cv::Size(w,w));\n    auto J12 = cv::gapi::boxFilter(imgDiffXY, CV_32F, cv::Size(w,w));\n\n    auto tmp1 = cv::gapi::add(J11, J22);\n    auto tmp2 = cv::gapi::sub(J11, J22);\n    auto tmp3 = cv::gapi::mul(tmp2, tmp2);\n    auto tmp4 = cv::gapi::mul(J12, J12);\n    auto tmp5 = cv::gapi::add(tmp3, tmp4);\n    auto tmp6 = cv::gapi::sqrt(tmp5);\n    auto lambda1 = cv::gapi::addC(tmp6, 1e-15);\n    lambda1 = cv::gapi::add(tmp1, lambda1);\n    lambda1 = cv::gapi::divC(lambda1, 2);\n    auto lambda2 = cv::gapi::subC(tmp6, 1e-15);\n    lambda2 = cv::gapi::sub(tmp1, lambda2);\n    lambda2 = cv::gapi::divC(lambda2, 2);\n\n    return cv::gapi::sub(lambda1, lambda2);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Display Window in Python\nDESCRIPTION: This Python snippet creates a display window named 'Linear Blend' using `cv.namedWindow`. This window serves as the container for the trackbar and the output image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#![window]\n# Create Window\ncv.namedWindow('Linear Blend')\n#![window]\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV with Unicode character set for Windows Embedded\nDESCRIPTION: Command for building the OpenCV project after configuration, specifying Unicode character set which is required for Windows Embedded platforms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/wince/readme.md#2025-04-22_snippet_4\n\nLANGUAGE: batch\nCODE:\n```\ncmake --build . -- /p:CharacterSet=Unicode\n```\n\n----------------------------------------\n\nTITLE: Running Custom Layer OpenCV C++\nDESCRIPTION: Implements the core logic of the layer by computing outputs for the given inputs in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/custom_layers.hpp MyLayer::forward\n```\n\n----------------------------------------\n\nTITLE: Drawing and Displaying Results in C++\nDESCRIPTION: This snippet draws the detection results on the image and displays it. The outputs distinguish results from the Ballard and Guil methods using different colored rectangles.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/generalized_hough_ballard_guil/generalized_hough_ballard_guil.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n# Draw results and show image\n@snippet samples/cpp/tutorial_code/ImgTrans/generalizedHoughTransform.cpp generalized-hough-transform-draw-results\n```\n\n----------------------------------------\n\nTITLE: Checking Contour Convexity in JavaScript\nDESCRIPTION: This snippet shows how to use the `cv.isContourConvex()` function to determine if a contour is convex. It takes a contour (`cnt`), typically represented as a vector of points, as input and returns a boolean value: true if the contour is convex, and false otherwise. This function requires a pre-calculated contour object (`cnt`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contour_features/js_contour_features.markdown#2025-04-22_snippet_1\n\nLANGUAGE: js\nCODE:\n```\ncv.isContourConvex(cnt);\n```\n\n----------------------------------------\n\nTITLE: Configuring DNN Sample Build Process\nDESCRIPTION: Sets up the build configuration for DNN samples, including project setup, module inclusion, and target linking. Processes all CPP files in the directory and creates build targets with necessary library linkages.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_EXAMPLES OR NOT OCV_DEPENDENCIES_FOUND)\n  return()\nendif()\n\nproject(dnn_samples)\nocv_include_modules_recurse(${OPENCV_DNN_SAMPLES_REQUIRED_DEPS})\nfile(GLOB_RECURSE dnn_samples RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} *.cpp)\nforeach(sample_filename ${dnn_samples})\n  ocv_define_sample(tgt ${sample_filename} dnn)\n  ocv_target_link_libraries(${tgt} PRIVATE ${OPENCV_LINKER_LIBS} ${OPENCV_DNN_SAMPLES_REQUIRED_DEPS})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Single Channel Pixel Access with At Methods (ucharAt) in OpenCV.js - JavaScript\nDESCRIPTION: Extracts individual channel values at specified (row, col) using ucharAt. This provides read-only access (cannot modify) and is meant for single channel or per-channel per-pixel reading. The method prevents direct memory access errors and is type-safe for matching Mat types.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_basic_ops/js_basic_ops.markdown#2025-04-22_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet row = 3, col = 4;\nlet src = cv.imread(\"canvasInput\");\nlet R = src.ucharAt(row, col * src.channels());\nlet G = src.ucharAt(row, col * src.channels() + 1);\nlet B = src.ucharAt(row, col * src.channels() + 2);\nlet A = src.ucharAt(row, col * src.channels() + 3);\n```\n\n----------------------------------------\n\nTITLE: Invoking OpenCV cv::parallel_for_ for Parallel Mandelbrot Computation - C++\nDESCRIPTION: This code demonstrates the use of cv::parallel_for_ to execute the custom ParallelMandelbrot functor over the whole image (pixel count), enabling parallelized pixel processing. Requires OpenCV's parallel_for_ and a correctly defined ParallelMandelbrot object. Input is the pixel count; output is the modified image Mat. The 'nstripes' parameter can adjust task splitting.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_/how_to_use_OpenCV_parallel_for_.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n// Parallel for call for Mandelbrot\ncv::parallel_for_(cv::Range(0, image.rows * image.cols), ParallelMandelbrot(image, maxIter));\n\n```\n\n----------------------------------------\n\nTITLE: Implementing the Escape Time Algorithm for the Mandelbrot Set - C++\nDESCRIPTION: This snippet implements the 'escape time' algorithm for determining whether a given complex coordinate is part of the Mandelbrot set, using std::complex in C++. The routine computes iterations based on the Mandelbrot recursion formula and returns the number of iterations taken to escape (or reach the maximum). Requires OpenCV and <complex>. The core input is a complex coordinate, output is the iteration count (int); limited by the chosen maximum iteration parameter.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_/how_to_use_OpenCV_parallel_for_.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n// Escape time algorithm for Mandelbrot set\nint mandelbrot(cv::Point2d pt, int maxIter)\n{\n    std::complex<double> z(0, 0);\n    std::complex<double> c(pt.x, pt.y);\n    int iter = 0;\n    while (std::norm(z) < 4.0 && iter < maxIter)\n    {\n        z = z * z + c;\n        ++iter;\n    }\n    return iter;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Signaling Successful OpenJPEG Configuration to Parent Scope in CMake\nDESCRIPTION: Checks if the variable `OCV_CAN_BUILD_OPENJPEG` is already defined. If not (implying no fatal errors occurred during the configuration checks), it sets `OCV_CAN_BUILD_OPENJPEG` to `TRUE` in the `PARENT_SCOPE`. This informs the main OpenCV build system whether the OpenJPEG dependency can be successfully built.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\n# OpenJPEG can't be built only if configuration script doesn't encounter any problem\nif(NOT DEFINED OCV_CAN_BUILD_OPENJPEG)\n  # all prerequisites are fulfilled\n  set(OCV_CAN_BUILD_OPENJPEG TRUE PARENT_SCOPE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Applying Laplacian Operator in C++\nDESCRIPTION: Applies the Laplacian operator to the grayscale image (`src_gray`) using cv::Laplacian. The output depth (`ddepth`) is set to CV_16S to prevent overflow during calculations involving second derivatives. Kernel size, scale, delta, and border type use the previously defined or default values. Requires OpenCV imgproc module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\n//! [laplacian]\n/// Apply Laplace function\nMat abs_dst;\n\nLaplacian( src_gray, dst, ddepth, kernel_size, scale, delta, BORDER_DEFAULT );\n//! [laplacian]\n```\n\n----------------------------------------\n\nTITLE: Applying Mask and Inverse DFT using OpenCV and Numpy - Python\nDESCRIPTION: This code applies a frequency mask to a shifted DFT of an image, performs an inverse shift and inverse DFT to reconstruct the filtered image, and plots the original image alongside its filtered magnitude spectrum. Dependencies include cv2 (imported as cv), numpy (imported as np), and matplotlib.pyplot (imported as plt). Key variables: 'img' (input image), 'dft_shift' (initial DFT), and 'mask' (frequency mask). The output is two matplotlib subplots: the input and masked-inverse-DFT image. Requires a previously computed 'dft_shift' and 'mask'. Assumes usage in a Jupyter-style workflow. Limitations: variable definitions like 'img', 'dft_shift', 'mask' assumed present in surrounding code.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# apply mask and inverse DFT\nfshift = dft_shift*mask\nf_ishift = np.fft.ifftshift(fshift)\nimg_back = cv.idft(f_ishift)\nimg_back = cv.magnitude(img_back[:,:,0],img_back[:,:,1])\n\nplt.subplot(121),plt.imshow(img, cmap = 'gray')\nplt.title('Input Image'), plt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(img_back, cmap = 'gray')\nplt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Automatic Data Allocation in OpenCV C++\nDESCRIPTION: This code demonstrates automatic memory allocation for output data in OpenCV. It uses `cv::Mat` for video frame capturing and image processing operations, showcasing the allocation of matrices by OpenCV's VideoCapture API and functions like `cvtColor`. It handles frame and edge detection from webcam input, requiring headers like `opencv2/imgproc.hpp` and `opencv2/highgui.hpp`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/doc/intro.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n    #include \"opencv2/imgproc.hpp\"\n    #include \"opencv2/highgui.hpp\"\n\n    using namespace cv;\n\n    int main(int, char**)\n    {\n        VideoCapture cap(0);\n        if(!cap.isOpened()) return -1;\n\n        Mat frame, edges;\n        namedWindow(\"edges\", WINDOW_AUTOSIZE);\n        for(;;)\n        {\n            cap >> frame;\n            cvtColor(frame, edges, COLOR_BGR2GRAY);\n            GaussianBlur(edges, edges, Size(7,7), 1.5, 1.5);\n            Canny(edges, edges, 0, 30, 3);\n            imshow(\"edges\", edges);\n            if(waitKey(30) >= 0) break;\n        }\n        return 0;\n    }\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenCL SDK Directories and Downloading Required Headers - Bash\nDESCRIPTION: This snippet automates the setup of the Android OpenCL SDK directory, copies necessary standard OpenCL headers, and downloads C++ OpenCL header files using wget. Dependencies include the OpenCV source or system OpenCL headers and an internet connection for fetching additional files. The script expects environment variables for source locations and creates the required directory structure for later use by build systems.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd your_path/ && mkdir ANDROID_OPENCL_SDK && mkdir ANDROID_OPENCL_SDK/include && cd ANDROID_OPENCL_SDK/include\\ncp -r path_to_opencv/opencv/3rdparty/include/opencl/1.2/CL . && cd CL\\nwget https://github.com/KhronosGroup/OpenCL-CLHPP/raw/main/include/CL/opencl.hpp\\nwget https://github.com/KhronosGroup/OpenCL-CLHPP/raw/main/include/CL/cl2.hpp\n```\n\n----------------------------------------\n\nTITLE: Enabling OSGi Integration Tests for OpenCV\nDESCRIPTION: Maven profile activation command to run OSGi integration tests as part of the build process. This is optional and applicable to all processor architectures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/maven/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n-Pintegration\n```\n\n----------------------------------------\n\nTITLE: Default Preprocessing Configuration for TensorFlow Segmentation\nDESCRIPTION: Defines the default preprocessing parameters for TensorFlow segmentation models, including scaling factor, mean values for normalization, and color space configuration. These values are used when default_img_preprocess is set to True.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntf_segm_input_blob = {\n    \"scale\": str(1 / 127.5),\n    \"mean\": [\"127.5\", \"127.5\", \"127.5\"],\n    \"std\": [],\n    \"crop\": \"False\",\n    \"rgb\": \"True\"\n}\n```\n\n----------------------------------------\n\nTITLE: Changing Image Type from 8UC1 to 32FC1 with OpenCV in Java\nDESCRIPTION: Illustrates changing an image Mat from 8-bit to float type in Java OpenCV using convertTo. Input and output are Mat; output must be declared. Applicable for floating-point processing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_38\n\nLANGUAGE: Java\nCODE:\n```\nMat floatMat = new Mat();\\nimg.convertTo(floatMat, CvType.CV_32F);\n```\n\n----------------------------------------\n\nTITLE: Linking JNI Target with System OpenCL Library - CMake\nDESCRIPTION: This CMake line explicitly links the JNI component of the Android application to the system OpenCL library. It avoids embedding the library into the APK, opting to rely on the system's OpenCL API at runtime. Users must ensure OpenCL is present on the target device; linker errors for missing symbols may be ignored with appropriate flags during build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${target} -lOpenCL)\n```\n\n----------------------------------------\n\nTITLE: Finishing JPEG Decompression in C using libjpeg\nDESCRIPTION: This snippet shows the call to `jpeg_finish_decompress`, which completes the decompression cycle and releases associated working memory. It must be called after successfully reading all image scanlines (step 7). Calling it prematurely or after an error is incorrect; `jpeg_abort` should be used in those cases.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_16\n\nLANGUAGE: C\nCODE:\n```\njpeg_finish_decompress(&cinfo);\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenCV Java Bindings Generator Module in CMake\nDESCRIPTION: Initializes the CMake configuration for the Java bindings generator. It defines the module name `java_bindings_generator`, marks it as internal (not part of the main OpenCV world library), adds it using `ocv_add_module`, sets paths for generated signature files and bindings, and removes potentially stale generated directories and dependency helper files. It also includes a common CMake configuration file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/generator/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(MODULE_NAME \"java_bindings_generator\")\nset(OPENCV_MODULE_IS_PART_OF_WORLD FALSE)\nocv_add_module(${MODULE_NAME} INTERNAL)\n\nset(OPENCV_JAVA_SIGNATURES_FILE \"${CMAKE_CURRENT_BINARY_DIR}/opencv_java_signatures.json\" CACHE INTERNAL \"\")\nset(OPENCV_JAVA_BINDINGS_DIR \"${CMAKE_CURRENT_BINARY_DIR}\" CACHE INTERNAL \"\")\n\nfile(REMOVE_RECURSE \"${OPENCV_JAVA_BINDINGS_DIR}/gen\")\nfile(REMOVE \"${OPENCV_DEPHELPER}/gen_opencv_java_source\")  # force re-run after CMake\n\n# This file is included from a subdirectory\nset(JAVA_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/..\")\ninclude(${JAVA_SOURCE_DIR}/common.cmake)\n```\n\n----------------------------------------\n\nTITLE: Executing C++ Program with Make\nDESCRIPTION: Use make to build and execute a C++ program that utilizes OpenCV library, demonstrating successful linking and running of OpenCV code.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nmake a.out\n./a.out\n```\n\n----------------------------------------\n\nTITLE: Writing JPEG Comment Marker in C\nDESCRIPTION: Example showing how to write a COM (comment) marker to a JPEG file using jpeg_write_marker(). The marker is written after compression starts but before scanlines are written.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_61\n\nLANGUAGE: C\nCODE:\n```\njpeg_write_marker(cinfo, JPEG_COM, comment_text, strlen(comment_text));\n```\n\n----------------------------------------\n\nTITLE: Including OpenCV.js Synchronously (HTML)\nDESCRIPTION: Demonstrates how to include the OpenCV.js library synchronously in an HTML file using a `<script>` tag. The browser will load and execute `opencv.js` before continuing to parse the rest of the HTML document. The `opencv.js` file is assumed to be in the same folder as the HTML file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_usage/js_usage.markdown#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"opencv.js\" type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV Repositories on macOS\nDESCRIPTION: These commands clone the main OpenCV repository and the contrib repository for additional modules using Git.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/macos_install/macos_install.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/<your_working_directory>\ngit clone https://github.com/opencv/opencv.git\ngit clone https://github.com/opencv/opencv_contrib.git\n```\n\n----------------------------------------\n\nTITLE: Initializing and Configuring SVM Parameters for Non-Linear Classification (Python)\nDESCRIPTION: Python implementation for setting up SVM parameters to handle non-linearly separable data. Uses the RBF kernel with a small C value for better generalization and high iteration criteria for convergence.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n# Set up SVM's parameters\nsvm = cv.ml.SVM_create()\nsvm.setType(cv.ml.SVM_C_SVC)\nsvm.setKernel(cv.ml.SVM_RBF)\n# When C is small, the decision boundary will be smooth\n# When C is large, the decision boundary can better classify all training points but may lead to overfitting\nsvm.setC(0.1)\n# Set termination criteria for the optimization\nsvm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, int(1e7), 1e-6))\n```\n\n----------------------------------------\n\nTITLE: Setting Uniform and Accumulate Flags in Java\nDESCRIPTION: Java snippet setting boolean flags for histogram calculation using `Imgproc.calcHist`. `histUniform` is set to true for equal bin sizes, and `accumulate` is set to false to clear the histogram beforehand.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_16\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/Histograms_Matching/histogram_calculation/CalcHistDemo.java Set histogram param\n```\n\n----------------------------------------\n\nTITLE: Creating Trackbar for Method Selection (C++)\nDESCRIPTION: Creates a trackbar in a designated window using `cv::createTrackbar`. This allows the user to interactively select one of the available template matching methods. The trackbar calls a specified callback function (`MatchingMethod`) whenever its value changes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp create_trackbar\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Include Individual OpenCV Module (CMake)\nDESCRIPTION: Defines a CMake function `include_one_module` that takes a module name `m` as an argument. It includes the module's `CMakeLists.txt` using the path stored in `OPENCV_MODULE_${m}_LOCATION`. It also ensures that `CMAKE_CXX_FLAGS` and `CMAKE_C_FLAGS` from the included module's scope are propagated back to the parent scope, which is important for maintaining consistent build settings across modules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/world/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(include_one_module m)\n  include(\"${OPENCV_MODULE_${m}_LOCATION}/CMakeLists.txt\")\n  foreach(var\n      CMAKE_CXX_FLAGS CMAKE_C_FLAGS # Propagate warnings settings\n  )\n    set(${var} \"${${var}}\" PARENT_SCOPE)\n  endforeach()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring an OpenCV Project with CMake in CMakeLists.txt\nDESCRIPTION: This CMake snippet provides a minimum configuration to build a C++ project using OpenCV. It sets the CMake minimum version, locates the required OpenCV package, sets up include directories, declares the executable to build, and links it against OpenCV libraries. Dependencies: CMake >= 3.5, OpenCV installed and findable. Inputs: DisplayImage.cpp source file. Outputs: Makefile or build files for compiling the DisplayImage executable. Limitation: assumes OpenCV is installed and find_package can locate it.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_gcc_cmake/linux_gcc_cmake.markdown#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5)\\nproject( DisplayImage )\\nfind_package( OpenCV REQUIRED )\\ninclude_directories( ${OpenCV_INCLUDE_DIRS} )\\nadd_executable( DisplayImage DisplayImage.cpp )\\ntarget_link_libraries( DisplayImage ${OpenCV_LIBS} )\n```\n\n----------------------------------------\n\nTITLE: Blending Images with OpenCV in C++\nDESCRIPTION: This C++ snippet demonstrates how to blend two images using OpenCV. The addWeighted() function is used to perform linear blending of two images. The input images must be of the same size and type. Dependencies include OpenCV library installation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/adding_images/adding_images.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n\"@snippet cpp/tutorial_code/core/AddingImages/AddingImages.cpp load\"\n```\n\nLANGUAGE: C++\nCODE:\n```\n\"@snippet cpp/tutorial_code/core/AddingImages/AddingImages.cpp blend_images\"\n```\n\nLANGUAGE: C++\nCODE:\n```\n\"@snippet cpp/tutorial_code/core/AddingImages/AddingImages.cpp display\"\n```\n\n----------------------------------------\n\nTITLE: Adding BibTeX Record for Publication Reference in OpenCV Documentation\nDESCRIPTION: Shows how to add a BibTeX record for a publication reference in OpenCV documentation files. This snippet demonstrates the format for adding a publication to either the main OpenCV bibliography file or a module-specific file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_17\n\nLANGUAGE: bibtex\nCODE:\n```\n@ARTICLE{Bradski98,\n    author = {Bradski, Gary R},\n    title = {Computer vision face tracking for use in a perceptual user interface},\n    year = {1998},\n    publisher = {Citeseer}\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating PSNR for Image Similarity in OpenCV Python\nDESCRIPTION: This Python implementation calculates the Peak Signal-to-Noise Ratio (PSNR) between two images. It computes the mean squared error and converts it to the logarithmic PSNR value. The function returns 100 for identical images to avoid divide-by-zero errors.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_input_psnr_ssim.markdown#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef getPSNR(I1, I2):\n    s1 = cv.absdiff(I1, I2) #|I1 - I2|\n    s1 = np.float32(s1)  # cannot make a square on 8 bits\n    s1 = s1 * s1  # |I1 - I2|^2\n    sse = np.sum(s1)  # sum elements per channel\n    if sse <= 1e-10:  # sum channels\n        return 0  # for small values return zero\n    else:\n        shape = I1.shape\n        mse = sse / (shape[0] * shape[1] * shape[2])\n        psnr = 10.0 * np.log10((255 * 255) / mse)\n        return psnr\n```\n\n----------------------------------------\n\nTITLE: Copying Cascade Classifier File to Resources Directory (Bash)\nDESCRIPTION: Copies the LBP cascade classifier XML file for frontal face detection from the OpenCV data directory into the project's 'src/main/resources' directory. This makes the classifier accessible to the Java application at runtime.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncp <opencv_dir>/data/lbpcascades/lbpcascade_frontalface.xml src/main/resources/\n```\n\n----------------------------------------\n\nTITLE: Configuring DNN Parameters using OpenVINO and C++\nDESCRIPTION: Utilizing OpenVINO Toolkit's Inference Engine, this snippet demonstrates how deep learning networks are configured within the G-API pipeline. It involves setting parameters for networks defined via G_API_NET() and wraps them in a NetworkPackage for further use.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/face_beautification/face_beautification.cpp net_param\n```\n\n----------------------------------------\n\nTITLE: Reading Images using OpenCV in Python\nDESCRIPTION: This snippet explains how to read an image file using OpenCV's cv.imread function in Python. The image can be loaded in various formats, but the default is 8-bit BGR color. The image data is stored in a NumPy array for subsequent operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/display_image/display_image.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimage = cv.imread('starry_night.jpg', cv.IMREAD_COLOR)\n```\n\n----------------------------------------\n\nTITLE: Importing OpenCV Framework in Objective-C\nDESCRIPTION: This snippet shows how to import the OpenCV framework header into an Objective-C source file. It enables direct access to OpenCV functions and classes from within Objective-C. Dependency: the OpenCV framework must be added to your Xcode project. The snippet should be placed at the top of files that use OpenCV. Input: none; Output: imports the OpenCV API into the current source context.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/objc/doc/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Objective-C\nCODE:\n```\n#import <OpenCV/OpenCV.h>\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV Visualisation Tool Command\nDESCRIPTION: This command visualizes the trained cascade classifier by displaying selected features and stages complexity. It requires paths to a reference image and the trained model. Optionally, it stores stage output and a feature visualization video in the specified data folder.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/traincascade.markdown#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nopencv_visualisation --image=/data/object.png --model=/data/model.xml --data=/data/result/\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV.js Tests in Browser and with Node.js HTTP-Server - Sh\nDESCRIPTION: Demonstrates launching a local web server (Node's http-server) to serve test files, then opening the test page in Firefox to run automated tests. Dependencies: Node.js and http-server package must be installed. Parameters: directory to serve (build_js/bin). Output: test results shown in browser interface.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\nnpx http-server build_js/bin\\nfirefox http://localhost:8080/tests.html\n```\n\n----------------------------------------\n\nTITLE: Computing Fundamental Matrix with OpenCV - Python\nDESCRIPTION: Utilizes matched points to compute the fundamental matrix in OpenCV, filtering for inlier points. Assumes the matched points list is available and uses the LMedS method for robust estimation. The output is the fundamental matrix with a mask of inliers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_epipolar_geometry/py_epipolar_geometry.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npts1 = np.int32(pts1)\npts2 = np.int32(pts2)\nF, mask = cv.findFundamentalMat(pts1,pts2,cv.FM_LMEDS)\n\n# We select only inlier points\npts1 = pts1[mask.ravel()==1]\npts2 = pts2[mask.ravel()==1]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Modules in CMake\nDESCRIPTION: This snippet sets up and reports the status of OpenCV modules to be built, disabled, or unavailable. It uses CMake commands to process module lists and display their status.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nstatus(\"\")\nstatus(\"  OpenCV modules:\")\nset(OPENCV_MODULES_BUILD_ST \"\")\nforeach(the_module ${OPENCV_MODULES_BUILD})\n  if(NOT OPENCV_MODULE_${the_module}_CLASS STREQUAL \"INTERNAL\" OR the_module STREQUAL \"opencv_ts\")\n    list(APPEND OPENCV_MODULES_BUILD_ST \"${the_module}\")\n  endif()\nendforeach()\nstring(REPLACE \"opencv_\" \"\" OPENCV_MODULES_BUILD_ST          \"${OPENCV_MODULES_BUILD_ST}\")\nstring(REPLACE \"opencv_\" \"\" OPENCV_MODULES_DISABLED_USER_ST  \"${OPENCV_MODULES_DISABLED_USER}\")\nstring(REPLACE \"opencv_\" \"\" OPENCV_MODULES_DISABLED_AUTO_ST  \"${OPENCV_MODULES_DISABLED_AUTO}\")\nstring(REPLACE \"opencv_\" \"\" OPENCV_MODULES_DISABLED_FORCE_ST \"${OPENCV_MODULES_DISABLED_FORCE}\")\nlist(SORT OPENCV_MODULES_BUILD_ST)\nlist(SORT OPENCV_MODULES_DISABLED_USER_ST)\nlist(SORT OPENCV_MODULES_DISABLED_AUTO_ST)\nlist(SORT OPENCV_MODULES_DISABLED_FORCE_ST)\nstatus(\"    To be built:\"            OPENCV_MODULES_BUILD          THEN ${OPENCV_MODULES_BUILD_ST}          ELSE \"-\")\nstatus(\"    Disabled:\"               OPENCV_MODULES_DISABLED_USER  THEN ${OPENCV_MODULES_DISABLED_USER_ST}  ELSE \"-\")\nstatus(\"    Disabled by dependency:\" OPENCV_MODULES_DISABLED_AUTO  THEN ${OPENCV_MODULES_DISABLED_AUTO_ST}  ELSE \"-\")\nstatus(\"    Unavailable:\"            OPENCV_MODULES_DISABLED_FORCE THEN ${OPENCV_MODULES_DISABLED_FORCE_ST} ELSE \"-\")\n```\n\n----------------------------------------\n\nTITLE: Locating SYCL SDKs and Handling Fallbacks for OpenCV Samples - CMake\nDESCRIPTION: This segment attempts to find an available SYCL SDK required for sample compilation, first checking for a default SYCL package, then falling back to oneAPI\\'s oneDNN or ComputeCpp SDKs if necessary. It handles environment variables for SDK root locations, configures SDK-specific CMake variables, and uses conditional logic to ensure a SYCL implementation is found, providing diagnostic messages if not. Integration points with dnnl and ComputeCpp help broaden build compatibility across environments.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/sycl/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(SYCL QUIET)  # will oneAPI support this straightforward way?\n\nif(NOT SYCL_FOUND AND NOT OPENCV_SKIP_SAMPLES_SYCL_ONEDNN)\n  # lets try scripts from oneAPI:oneDNN component\n  if(NOT DEFINED DNNLROOT AND DEFINED ENV{DNNLROOT})\n    set(DNNLROOT \"$ENV{DNNLROOT}\")\n  endif()\n  # Some verions of called script violate CMake policy and may emit unrecoverable CMake errors\n  # Use OPENCV_SKIP_SAMPLES_SYCL=1 / OPENCV_SKIP_SAMPLES_SYCL_ONEDNN to bypass this\n  find_package(dnnl CONFIG QUIET HINTS \"${DNNLROOT}\")\nendif()\n\nif(NOT SYCL_FOUND AND NOT OPENCV_SKIP_SAMPLES_SYCL_COMPUTECPP)\n  # lets try this SYCL SDK too: https://github.com/codeplaysoftware/computecpp-sdk\n  find_package(ComputeCpp QUIET)\n  if(ComputeCpp_FOUND)\n    set(SYCL_TARGET ComputeCpp::ComputeCpp)\n    set(SYCL_FLAGS ${ComputeCpp_FLAGS})\n    set(SYCL_INCLUDE_DIRS ${ComputeCpp_INCLUDE_DIRS})\n    set(SYCL_LIBRARIES ${ComputeCpp_LIBRARIES})\n  endif()\nendif()\n\nif(OPENCV_CMAKE_DEBUG_SYCL)\n  ocv_cmake_dump_vars(\"SYCL\")  # OpenCV source tree is required\nendif()\n\nif(NOT SYCL_TARGET)\n  message(STATUS \"SYCL/OpenCL samples are skipped: SYCL SDK is required\")\n  message(STATUS \"   - check configuration of SYCL_DIR/SYCL_ROOT/CMAKE_MODULE_PATH\")\n  message(STATUS \"   - ensure that right compiler is selected from SYCL SDK (e.g, clang++): CMAKE_CXX_COMPILER=${CMAKE_CXX_COMPILER}\")\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Tegra HAL Library Configuration\nDESCRIPTION: Sets up the Tegra HAL static library, configures installation paths, and copies required header files to the build directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/carotene/hal/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(tegra_hal STATIC $<TARGET_OBJECTS:carotene_objs> \"dummy.cpp\")\nset_target_properties(tegra_hal PROPERTIES ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH})\nset(OPENCV_SRC_DIR \"${CMAKE_SOURCE_DIR}\")\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(tegra_hal EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev)\nendif()\ntarget_include_directories(tegra_hal PRIVATE ${CMAKE_CURRENT_SOURCE_DIR} ${OPENCV_SRC_DIR}/modules/core/include)\n```\n\n----------------------------------------\n\nTITLE: Threshold Operation Callback Function (C++)\nDESCRIPTION: This C++ function applies the cv::threshold operation to the grayscale image using parameters from the trackbars. The thresholded output is displayed in the result window. The function is registered as a callback and is called whenever trackbar values change. Inputs: src_gray image, threshold_value, threshold_type; output: thresholded dst Mat displayed via imshow.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\n// [Threshold_Demo]\\nvoid Threshold_Demo( int, void* )\\n{\\n  threshold( src_gray, dst, threshold_value, max_BINARY_value, threshold_type );\\n  imshow( window_name, dst );\\n}\\n// [Threshold_Demo]\n```\n\n----------------------------------------\n\nTITLE: Reading JPEG Header Information in C using libjpeg\nDESCRIPTION: This snippet demonstrates calling `jpeg_read_header` to read JPEG header markers and populate the `cinfo` struct with image information like dimensions and colorspace. The `TRUE` argument indicates that the image data should follow the header. This is typically step 3 in the libjpeg decompression process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_13\n\nLANGUAGE: C\nCODE:\n```\njpeg_read_header(&cinfo, TRUE);\n```\n\n----------------------------------------\n\nTITLE: Opening and Closing XML/YAML/JSON Files with OpenCV FileStorage - Python\nDESCRIPTION: This Python snippet shows how to open and close XML/YAML/JSON files using cv2.FileStorage in OpenCV. The constructor and open() method allow specifying the file name and operation mode (WRITE, READ, or APPEND). Use release() to close the file explicitly. Only filenames with suitable extensions (.xml, .yml, .json, etc.) are supported. Requires opencv-python installed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\\n\\n# Opening a file for writing\\nfs = cv2.FileStorage('output.yml', cv2.FileStorage_WRITE)\\n\\n# ... perform write operations ...\\n\\n# Explicitly closing the file\\nfs.release()\n```\n\n----------------------------------------\n\nTITLE: Creating Actions Runner Configuration\nDESCRIPTION: Configuration content for the actions-runner file, including repository information and GitHub access token.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/arch/s390/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Create file /etc/actions-runner\nrepo=<owner>/<name>\naccess_token=<ghp_***>\n```\n\n----------------------------------------\n\nTITLE: Installing Project Requirements - Console\nDESCRIPTION: This command installs all necessary Python dependencies as specified in 'requirements.txt'. This is intended to be run within the already-activated virtual environment. It ensures all Python-side prerequisites for model conversion and utility scripts are available prior to running any code.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Registering ResizeBilinearLayer OpenCV C++\nDESCRIPTION: Snippet detailing the registration of a custom layer from TensorFlow to enable model import in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/custom_layers.hpp Register ResizeBilinearLayer\n```\n\n----------------------------------------\n\nTITLE: Pulling libOpenCL.so Library from Android Device - Bash\nDESCRIPTION: This snippet pulls the shared OpenCL library from an Android device using adb, placing it in the prepared Android OpenCL SDK 'lib' directory. Requires a device with libOpenCL.so accessible and ADB installed and authorized. The script may need adjustment for device-specific library paths and assumes the device is connected and unlocked.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd your_path/ANDROID_OPENCL_SDK && mkdir lib && cd lib\\nadb pull /system/vendor/lib64/libOpenCL.so\n```\n\n----------------------------------------\n\nTITLE: Destroying JPEG Decompression Object in C using libjpeg\nDESCRIPTION: Demonstrates calling `jpeg_destroy_decompress` to release all resources associated with the JPEG decompression object (`cinfo`) when it's no longer needed. This is the final cleanup step (step 8) after finishing decompression or aborting the process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_17\n\nLANGUAGE: C\nCODE:\n```\njpeg_destroy_decompress(&cinfo);\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Java JAR using Java\nDESCRIPTION: Configures and executes the Java-based build process for the OpenCV Java JAR when OPENCV_JAVA_SDK_BUILD_TYPE is set to \"JAVA\".\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jar/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nelseif(OPENCV_JAVA_SDK_BUILD_TYPE STREQUAL \"JAVA\")\n  configure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/MANIFEST.MF.in\" \"${OPENCV_JAVA_DIR}/MANIFEST.MF\" @ONLY)\n  list(APPEND depends \"${OPENCV_JAVA_DIR}/MANIFEST.MF\")\n\n  ocv_cmake_byproducts(__byproducts BYPRODUCTS \"${OPENCV_JAVA_DIR}/java_sources\")\n  add_custom_command(OUTPUT \"${OPENCV_DEPHELPER}/${the_module}_jar\"\n      BYPRODUCTS ${__byproducts}  # required for add_custom_target() by ninja\n      DEPENDS ${depends}\n      COMMAND ${CMAKE_COMMAND} -E touch \"${OPENCV_DEPHELPER}/${the_module}_jar\"\n      COMMAND ${CMAKE_COMMAND}\n      -D OPENCV_JAVA_DIR=\"${OPENCV_JAVA_DIR}/java\"\n      -D OUTPUT=\"${OPENCV_JAVA_DIR}/java_sources\"\n      -P \"${CMAKE_CURRENT_SOURCE_DIR}/list_java_sources.cmake\"\n  )\n\n  add_custom_target(${the_module}_jar_sources\n    DEPENDS \"${OPENCV_DEPHELPER}/${the_module}_jar\"\n  )\n\n  list(APPEND CMAKE_JAVA_COMPILE_FLAGS -encoding utf-8 ${OPENCV_EXTRA_JAVA_COMPILE_FLAGS})\n\n  add_jar(${the_module}_jar\n          SOURCES \"@${OPENCV_JAVA_DIR}/java_sources\"\n          MANIFEST \"${OPENCV_JAVA_DIR}/MANIFEST.MF\"\n          OUTPUT_NAME \"${JAR_NAME_WE}\"\n          OUTPUT_DIR \"${OPENCV_JAR_DIR}\")\n\n  add_dependencies(${the_module}_jar ${the_module}_jar_sources)\n```\n\n----------------------------------------\n\nTITLE: Defining the Working Folder Structure (Unparsed)\nDESCRIPTION: Illustrates the recommended directory structure for the cross-compilation project. It includes separate folders for the OpenCV source, OpenCV Contrib source, and build directories for different target architectures (arm64 and armhf). Users should replace 'kmtr' with their actual account name.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_2\n\nLANGUAGE: unparsed\nCODE:\n```\n/home\n  + kmtr                    - please replace your account name.\n    + work\n      + opencv              - source, cloned from github\n      + opencv_contrib      - source, cloned from github\n      + build4-full_arm64   - artifact(for aarch64 target), created by cmake\n      + build4-full_armhf   - artifact(for armhf target), created by cmake\n```\n\n----------------------------------------\n\nTITLE: Configuring Android Color Blob Detection Example\nDESCRIPTION: Sets up an Android project for color blob detection using CMake. Defines the project name, adds Android-specific configuration, and establishes OpenCV library dependencies. Sets SDK target to 11 and includes conditional dependency linking.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/color-blob-detection/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(sample example-color-blob-detection)\n\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\" LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\" SDK_TARGET 11 \"${ANDROID_SDK_TARGET}\")\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Profiling Memory Usage with Valgrind\nDESCRIPTION: These commands profile the memory usage of two implementations of an anisotropic image segmentation algorithm using Valgrind's Massif tool. The output is stored in separate files for each version.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/anisotropic_segmentation/porting_anisotropic_segmentation.markdown#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ valgrind --tool=massif --massif-out-file=ocv.out ./bin/example_tutorial_anisotropic_image_segmentation\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ valgrind --tool=massif --massif-out-file=gapi.out ./bin/example_tutorial_porting_anisotropic_image_segmentation_gapi\n```\n\n----------------------------------------\n\nTITLE: Creating Algorithm Instances in OpenCV 3.0\nDESCRIPTION: Demonstrates the recommended ways to create algorithm instances using makePtr function or static factory methods in OpenCV 3.0, as well as deprecated approaches.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/transition_guide/transition_guide.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// good ways\nPtr<SomeAlgo> algo = makePtr<SomeAlgo>(...);\nPtr<SomeAlgo> algo = SomeAlgo::create(...);\n```\n\nLANGUAGE: cpp\nCODE:\n```\n// bad ways\nPtr<SomeAlgo> algo = new SomeAlgo(...);\nSomeAlgo * algo = new SomeAlgo(...);\nSomeAlgo algo(...);\nPtr<SomeAlgo> algo = Algorithm::create<SomeAlgo>(\"name\");\n```\n\n----------------------------------------\n\nTITLE: Checking for Alpha Channel Colorspace Extensions at Compile Time in C\nDESCRIPTION: Illustrates using the `#ifdef JCS_ALPHA_EXTENSIONS` C preprocessor directive to verify at compile time if the alpha-channel-specific colorspace extensions (like JCS_EXT_RGBA, JCS_EXT_BGRA) are supported by the linked libjpeg-turbo library. This check is useful for applications needing to handle transparency information correctly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/README.md#2025-04-22_snippet_2\n\nLANGUAGE: c\nCODE:\n```\n#ifdef JCS_ALPHA_EXTENSIONS\n```\n\n----------------------------------------\n\nTITLE: Applying Adaptive Thresholding in OpenCV - JavaScript\nDESCRIPTION: Uses adaptive thresholding to handle images with varying illumination. The threshold is computed for different regions of the image rather than a single global threshold, resulting in better performance under diverse lighting conditions. Dependencies include the OpenCV library for JavaScript. Inputs involve the source and destination arrays, max value, adaptive method, threshold type, block size, and constant C. It outputs an image where each pixel is thresholded according to its neighborhood.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_thresholding/js_thresholding.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.adaptiveThreshold(src, dst, maxValue, adaptiveMethod, thresholdType, blockSize, C);\n```\n\n----------------------------------------\n\nTITLE: Keypoint Management in OAST Corner Detection\nDESCRIPTION: Handles the dynamic allocation and management of keypoints when corners are detected. Includes logic for expanding capacity when needed and adding new keypoints to the collection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_29\n\nLANGUAGE: C++\nCODE:\n```\nsuccess_homogeneous:\n  if(total == nExpectedCorners)\n  {\n      if(nExpectedCorners == 0)\n      {\n          nExpectedCorners = 512;\n          keypoints.reserve(nExpectedCorners);\n      }\n      else\n      {\n          nExpectedCorners *= 2;\n          keypoints.reserve(nExpectedCorners);\n      }\n  }\n  keypoints.push_back(KeyPoint(Point2f((float)x, (float)y), 1.0f));\n  total++;\n  goto homogeneous;\n```\n\n----------------------------------------\n\nTITLE: Disabling Compiler Warnings for libtiff\nDESCRIPTION: The snippet contains multiple ocv_warnings_disable commands to suppress specific compiler warnings during the build of libtiff. These settings relieve the build process from various unused or compatibility warnings across different compilers and platforms, improving build stability.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libtiff/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nocv_warnings_disable(CMAKE_C_FLAGS -Wno-unused-but-set-variable -Wmissing-prototypes -Wmissing-declarations -Wundef -Wunused -Wsign-compare\n                                   -Wcast-align -Wshadow -Wno-maybe-uninitialized -Wno-pointer-to-int-cast -Wno-int-to-pointer-cast\n                                   -Wmisleading-indentation\n                                   -Wimplicit-fallthrough\n                                   -Wunused-parameter  # clang\n                                   -Warray-parameter\n                                   -Wstrict-prototypes  # clang15\n)\nocv_warnings_disable(CMAKE_CXX_FLAGS -Wmissing-declarations -Wunused-parameter -Wmissing-prototypes\n    -Wundef  # tiffiop.h: #if __clang_major__ >= 4\n)\nocv_warnings_disable(CMAKE_CXX_FLAGS /wd4018 /wd4100 /wd4127 /wd4311 /wd4701 /wd4706) # vs2005\nocv_warnings_disable(CMAKE_CXX_FLAGS /wd4244) # vs2008\nocv_warnings_disable(CMAKE_CXX_FLAGS /wd4267 /wd4305 /wd4306) # vs2008 Win64\nocv_warnings_disable(CMAKE_CXX_FLAGS /wd4703) # vs2012\nocv_warnings_disable(CMAKE_CXX_FLAGS /wd4456 /wd4457 /wd4312) # vs2015\n\nocv_warnings_disable(CMAKE_C_FLAGS /wd4267 /wd4244 /wd4018 /wd4311 /wd4312)\n```\n\n----------------------------------------\n\nTITLE: Creating Trackbar for Method Selection (Python)\nDESCRIPTION: Uses `cv2.createTrackbar` to add a slider control to the main window. This trackbar allows the user to select the template matching method (represented by an integer). The `MatchingMethod` function is registered as the callback to handle trackbar value changes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/match_template/match_template.py create_trackbar\n```\n\n----------------------------------------\n\nTITLE: Creating Half Image for Testing Histogram Comparison\nDESCRIPTION: Creating a half-sized version of the base image to test partial image comparison, which should show a relatively high match with the original.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_comparison/histogram_comparison.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nMat hsv_half_down = hsv_base( Range( hsv_base.rows/2, hsv_base.rows ), Range( 0, hsv_base.cols ) );\n```\n\nLANGUAGE: java\nCODE:\n```\nMat hsv_half_down = hsv_base.submat( hsv_base.rows()/2, hsv_base.rows(), 0, hsv_base.cols() );\n```\n\nLANGUAGE: python\nCODE:\n```\nhsv_half_down = hsv_base[hsv_base.shape[0]//2:,:]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Build with CMake (Shell)\nDESCRIPTION: This shell command runs CMake to configure the OpenCV build, specifying the build system generator, optional configuration options, and the source directory. Required dependencies include CMake, a supported C++ compiler, and platform-appropriate build tools like Make or Ninja. Parameters include <generator> (e.g., \"Unix Makefiles\" or \"Visual Studio 16 2019\"), configuration options, and the local path to the OpenCV source code. The output is a set of build configuration files (e.g., Makefiles or IDE projects) in the current directory. Limitations include ensuring all dependencies are available before running this step.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/general_install/general_install.markdown#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncmake -G<generator> <configuration-options> <source-directory>\n```\n\n----------------------------------------\n\nTITLE: Nested Conditional Structure for FAST Corner Detection in C/C++\nDESCRIPTION: A complex series of conditionals that implement the pixel comparison logic for the FAST corner detection algorithm. The code compares pixel values at various offsets around a center point against threshold values (c_b and cb) to determine if a pixel is a corner based on intensity differences.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_12\n\nLANGUAGE: C/C++\nCODE:\n```\ngoto structured;\nelse\n  if(ptr[offset6] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset11] < c_b)\n          if(ptr[offset10] < c_b)\n            goto success_structured;\n          else\n            goto structured;\n        else\n          goto structured;\n      else\n        goto structured;\n    else\n      goto structured;\n  else\n    goto structured;\nelse\n  if(ptr[offset6] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset10] < c_b)\n          if(ptr[offset4] < c_b)\n            goto success_structured;\n          else\n            if(ptr[offset11] < c_b)\n              goto success_structured;\n            else\n              goto structured;\n        else\n          if(ptr[offset3] < c_b)\n            if(ptr[offset4] < c_b)\n              goto success_structured;\n            else\n              goto structured;\n          else\n            goto structured;\n      else\n        goto structured;\n    else\n      goto structured;\n  else\n    goto structured;\nelse\n  if(ptr[offset6] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset4] < c_b)\n          if(ptr[offset3] < c_b)\n            goto success_structured;\n          else\n            if(ptr[offset10] < c_b)\n              goto success_structured;\n            else\n              goto structured;\n        else\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              goto success_structured;\n            else\n              goto structured;\n          else\n            goto structured;\n      else\n        goto structured;\n    else\n      goto structured;\n  else\n    goto structured;\nelse\n  if(ptr[offset6] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset4] < c_b)\n          if(ptr[offset3] < c_b)\n            goto success_structured;\n          else\n            if(ptr[offset10] < c_b)\n              goto success_structured;\n            else\n              goto structured;\n        else\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              goto success_structured;\n            else\n              goto structured;\n          else\n            goto structured;\n      else\n        if(ptr[offset2] < c_b)\n          if(ptr[offset1] < c_b)\n            if(ptr[offset3] < c_b)\n              if(ptr[offset4] < c_b)\n                goto success_structured;\n              else\n                goto structured;\n            else\n              goto structured;\n          else\n            goto structured;\n        else\n          goto structured;\n    else\n      goto structured;\n  else\n    goto structured;\nelse\n  if(ptr[offset2] > cb)\n    if(ptr[offset1] > cb)\n      if(ptr[offset3] > cb)\n        if(ptr[offset4] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              goto success_structured;\n            else\n              goto structured;\n          else\n            goto structured;\n        else\n          goto structured;\n      else\n        goto structured;\n    else\n      goto structured;\n  else\n    if(ptr[offset2] < c_b)\n      if(ptr[offset3] < c_b)\n        if(ptr[offset4] < c_b)\n          if(ptr[offset7] < c_b)\n            if(ptr[offset1] < c_b)\n              if(ptr[offset6] < c_b)\n                goto success_structured;\n              else\n                goto structured;\n            else\n              if(ptr[offset6] < c_b)\n                if(ptr[offset8] < c_b)\n                  goto success_structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n          else\n            goto structured;\n        else\n          goto structured;\n      else\n        goto structured;\n    else\n      goto homogeneous;\nelse\n  if(ptr[offset2] > cb)\n    if(ptr[offset10] > cb)\n      if(ptr[offset11] > cb)\n        if(ptr[offset9] > cb)\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              goto success_structured;\n            else\n              if(ptr[offset8] > cb)\n                goto success_structured;\n              else\n                goto structured;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset7] > cb)\n                if(ptr[offset8] > cb)\n                  goto success_structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n            else\n              goto structured;\n        else\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset4] > cb)\n                goto success_structured;\n              else\n                goto structured;\n            else\n              goto structured;\n          else\n            goto structured;\n      else\n        goto structured;\n    else\n      goto structured;\n  else\n    if(ptr[offset9] > cb)\n      if(ptr[offset7] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n```\n\n----------------------------------------\n\nTITLE: Installing Build Scripts and Java Sources in CMake\nDESCRIPTION: These CMake commands define installation rules for files related to the Java/Android build. The first `install` command copies the `build.gradle` file from a temporary location (`ANDROID_TMP_INSTALL_BASE_DIR`) to the directory above `JAVA_INSTALL_ROOT`. The second `install` command copies the entire Java source directory (`java_src_dir`) to the `JAVA_INSTALL_ROOT`. Both installations are associated with the `java` component.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/android_sdk/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES \"${ANDROID_TMP_INSTALL_BASE_DIR}/opencv/build.gradle\" DESTINATION ${JAVA_INSTALL_ROOT}/.. COMPONENT java)\n\ninstall(DIRECTORY \"${java_src_dir}\" DESTINATION \"${JAVA_INSTALL_ROOT}\" COMPONENT java)\n```\n\n----------------------------------------\n\nTITLE: Sorting a GpuMat Column In-Place with Thrust\nDESCRIPTION: Shows how to sort a column of a GpuMat in-place by using Thrust's sort_by_key algorithm. This example also demonstrates key-value sorting to maintain index correspondence.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-thrust-interop/gpu_thrust_interop.markdown#2025-04-22_snippet_5\n\nLANGUAGE: CUDA\nCODE:\n```\ncv::cuda::GpuMat d_random(512, 512, CV_32FC2);\n// Fill matrix with random values and indices\nthrust::device_ptr<float2> d_random_ptr((float2*)d_random.data);\nthrust::transform(\n    thrust::counting_iterator<int>(0),\n    thrust::counting_iterator<int>(d_random.rows * d_random.cols),\n    d_random_ptr,\n    [=]__device__(const int idx)\n    {\n        float2 out;\n        // Fill in column 0 with random values\n        thrust::default_random_engine rng;\n        thrust::uniform_real_distribution<float> dist(0, 10);\n        rng.discard(idx);\n        out.x = dist(rng);\n        // Fill in column 1 with indices\n        out.y = idx;\n        return out;\n    }\n);\n\n// Get iterators for column 0 and column 1\nauto random_begin = thrust::make_zip_iterator(\n    thrust::make_tuple(\n        thrust::device_pointer_cast((float*)d_random.data),\n        thrust::device_pointer_cast((float*)d_random.data + 1)\n    )\n);\n\n// Sort column 0 and maintain indices in column 1\nthrust::sort_by_key(\n    thrust::make_permutation_iterator(\n        thrust::device_pointer_cast((float*)d_random.data),\n        begin_itr<float2>(d_random)\n    ),\n    thrust::make_permutation_iterator(\n        thrust::device_pointer_cast((float*)d_random.data),\n        end_itr<float2>(d_random)\n    ),\n    thrust::make_permutation_iterator(\n        thrust::device_pointer_cast((float*)d_random.data + 1),\n        begin_itr<float2>(d_random)\n    )\n);\n```\n\n----------------------------------------\n\nTITLE: Defining a Function to Display Random Text with OpenCV in C++\nDESCRIPTION: Defines the function `Displaying_Random_Text` which displays a specific text string ( \"Testing text rendering\") multiple times (NUMBER) on the input `image`. Within a loop, it generates a random origin point (`org`), font type, font scale, color, and thickness using the `rng` object. It uses `cv::putText` to draw the text and updates the display with `cv::imshow`, including a delay and interruption check via `cv::waitKey`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/random_generator_and_text/random_generator_and_text.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n@code{.cpp}\nint Displaying_Random_Text( Mat image, char* window_name, RNG rng )\n{\n  int lineType = 8;\n\n  for ( int i = 1; i < NUMBER; i++ )\n  {\n    Point org;\n    org.x = rng.uniform(x_1, x_2);\n    org.y = rng.uniform(y_1, y_2);\n\n    putText( image, \"Testing text rendering\", org, rng.uniform(0,8),\n             rng.uniform(0,100)*0.05+0.1, randomColor(rng), rng.uniform(1, 10), lineType);\n\n    imshow( window_name, image );\n    if( waitKey(DELAY) >= 0 )\n      { return -1; }\n  }\n\n  return 0;\n}\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Documenting Enumerations in C++ with Doxygen\nDESCRIPTION: Shows how to document enumeration types in C++ using Doxygen, with support for inline comments and autogenerated documentation blocks aligning with Doxygen's comment syntax.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\n//! type of line\nenum LineTypes {\n    FILLED  = -1,\n    LINE_4  = 4, //!< 4-connected line\n    LINE_8  = 8, //!< 8-connected line\n    LINE_AA = 16 //!< antialiased line\n};\n```\n\n----------------------------------------\n\nTITLE: Performing Image Subtraction for Fading Effect in OpenCV C++\nDESCRIPTION: This code snippet demonstrates image subtraction using OpenCV. It subtracts a scalar value `Scalar::all(i)` from each pixel of the original `image` and stores the result in `image2`. As `i` increases in the loop, the resulting `image2` becomes progressively darker. OpenCV's subtraction operation includes saturation, ensuring pixel values stay within the valid range (e.g., 0-255 for CV_8UC3).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/random_generator_and_text/random_generator_and_text.markdown#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n@code{.cpp}\nimage2 = image - Scalar::all(i)\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Model Testing with Parameters in Python\nDESCRIPTION: This command executes the module in test mode, optionally using default preprocessing for images and parameters. It's designed for PyTorch segmentation model inference.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.segmentation.py_to_py_segm --model_name <pytorch_segm_model_name> --test True --default_img_preprocess <True/False> --evaluate False\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Algorithm in C++\nDESCRIPTION: This code snippet implements part of the FAST corner detection algorithm, which checks pixel values against brightness thresholds (cb and c_b) at various offsets to determine if a pixel represents a corner. The algorithm uses multiple conditional branching and goto statements to efficiently identify corner points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset12] > cb)\n    if(ptr[offset13] > cb)\n      if(ptr[offset14] > cb)\n        if(ptr[offset15] > cb)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\nif(ptr[offset9] < c_b)\n  if(ptr[offset10] < c_b)\n    if(ptr[offset11] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset12] < c_b)\n          if(ptr[offset13] < c_b)\n            if(ptr[offset14] < c_b)\n              if(ptr[offset15] < c_b)\n                goto is_a_corner;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset7] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset5] < c_b)\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset7] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset4] < c_b)\n              if(ptr[offset5] < c_b)\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset7] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset3] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset5] < c_b)\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset7] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset1] < c_b)\n          if(ptr[offset12] < c_b)\n            if(ptr[offset13] < c_b)\n              if(ptr[offset14] < c_b)\n                if(ptr[offset15] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset7] > cb)\n    if(ptr[offset8] > cb)\n      if(ptr[offset9] > cb)\n        if(ptr[offset6] > cb)\n          if(ptr[offset5] > cb)\n            if(ptr[offset4] > cb)\n              if(ptr[offset3] > cb)\n                if(ptr[offset2] > cb)\n                  if(ptr[offset1] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset10] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    if(ptr[offset12] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  if(ptr[offset12] > cb)\n                    if(ptr[offset13] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                if(ptr[offset12] > cb)\n                  if(ptr[offset13] > cb)\n                    if(ptr[offset14] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              if(ptr[offset12] > cb)\n                if(ptr[offset13] > cb)\n                  if(ptr[offset14] > cb)\n                    if(ptr[offset15] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n  if(ptr[offset7] < c_b)\n    if(ptr[offset8] < c_b)\n      if(ptr[offset9] < c_b)\n        if(ptr[offset6] < c_b)\n          if(ptr[offset5] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset2] < c_b)\n                  if(ptr[offset1] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset10] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    if(ptr[offset12] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  if(ptr[offset12] < c_b)\n                    if(ptr[offset13] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Including OpenCV GPU Header in C++\nDESCRIPTION: Includes the necessary header file `opencv2/gpu.hpp` to use OpenCV's GPU module functionalities, including GPU-specific data structures like `cv::cuda::GpuMat` and GPU-accelerated algorithms. This header is essential for any C++ code utilizing OpenCV's CUDA features.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n#include <opencv2/gpu.hpp>        // GPU structures and methods\n```\n\n----------------------------------------\n\nTITLE: Building RISC-V Clang Toolchain and QEMU\nDESCRIPTION: This script downloads and builds the RISC-V Clang toolchain and QEMU. It sets up the necessary environment for cross-compiling OpenCV for RISC-V targets.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/arch/riscv/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./prepare_riscv_toolchain_qemu.sh\n```\n\n----------------------------------------\n\nTITLE: Calculating Image Moments using OpenCV Java\nDESCRIPTION: This Java snippet illustrates the calculation of image moments using OpenCV's libraries. Like the C++ version, it utilizes moments, contourArea, and arcLength functions to analyze image shapes. Java OpenCV library (version >= 3.0) is required for running this code. Inputs include images processed for contour detection and outputs are image moments and other related metrics.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/moments/moments.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.*;\n// Original code can be found at the mentioned OpenCV repository\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Maps (Dicts) - OpenCV Python\nDESCRIPTION: This Python snippet demonstrates serialization of Python dictionaries to XML/YAML/JSON in OpenCV. Use startWriteStruct() with cv2.FileNode_MAP, then write key-value pairs. Read values by using getNode(key).at() or getNode(key).real(). Requires opencv-python installed. Keys must be strings; values should be primitives or NumPy arrays.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfs.startWriteStruct('mymap', cv2.FileNode_MAP)\\nfs.write('one', 1)\\nfs.write('two', 2)\\nfs.endWriteStruct()\\n\\nnode = fs.getNode('mymap')\\none = int(node.getNode('one').real())\n```\n\n----------------------------------------\n\nTITLE: Drawing a Line in C++\nDESCRIPTION: Implementation of the MyLine function that draws a line between two points in OpenCV C++. The function takes the image, start and end points, and uses the line() function with specified thickness and line type.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\nvoid MyLine( Mat img, Point start, Point end )\n{\n  int thickness = 2;\n  int lineType = LINE_8;\n\n  line( img,\n    start,\n    end,\n    Scalar( 0, 0, 0 ),\n    thickness,\n    lineType );\n}\n```\n\n----------------------------------------\n\nTITLE: Classifying Corners Using Pixel Intensity Thresholds - OpenCV - C/C++\nDESCRIPTION: Determines whether the center pixel in a candidate patch is a feature corner by comparing pixel intensities at multiple offsets using threshold values `cb` (brighter) and `c_b` (darker). The code is structured using nested if-else statements and gotos for maximal performance, as required in core image processing routines where every CPU cycle counts. Dependencies include a correctly indexed pointer array `ptr[]`, a set of offset indices, and defined threshold variables; inputs are pixel intensities, and the output is branching control flow to either `is_a_corner` or `is_not_a_corner` labels depending on the pattern's match.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_3\n\nLANGUAGE: C\nCODE:\n```\nif(ptr[offset7] > cb)\n  if(ptr[offset8] > cb)\n    if(ptr[offset9] > cb)\n      if(ptr[offset10] > cb)\n        if(ptr[offset11] > cb)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\n```\n\nLANGUAGE: C\nCODE:\n```\nif(ptr[offset12] < c_b)\n  if(ptr[offset7] < c_b)\n    if(ptr[offset8] < c_b)\n      if(ptr[offset9] < c_b)\n        if(ptr[offset10] < c_b)\n          if(ptr[offset11] < c_b)\n            if(ptr[offset13] < c_b)\n              if(ptr[offset14] < c_b)\n                if(ptr[offset6] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset15] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\n```\n\nLANGUAGE: C\nCODE:\n```\nif(ptr[offset4] < c_b)\n  if(ptr[offset11] > cb)\n    if(ptr[offset12] > cb)\n      if(ptr[offset13] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset14] > cb)\n            if(ptr[offset15] > cb)\n              if(ptr[offset1] > cb)\n                goto is_a_corner;\n              else\n                if(ptr[offset8] > cb)\n                  if(ptr[offset9] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset9] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset5] > cb)\n              if(ptr[offset6] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset9] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset14] > cb)\n                if(ptr[offset15] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    if(ptr[offset11] < c_b)\n      if(ptr[offset7] < c_b)\n        if(ptr[offset8] < c_b)\n          if(ptr[offset9] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset6] < c_b)\n                if(ptr[offset5] < c_b)\n                  if(ptr[offset3] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset12] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset12] < c_b)\n                    if(ptr[offset13] < c_b)\n                      if(ptr[offset14] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset12] < c_b)\n                  if(ptr[offset13] < c_b)\n                    if(ptr[offset14] < c_b)\n                      if(ptr[offset15] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  if(ptr[offset11] > cb)\n    if(ptr[offset12] > cb)\n      if(ptr[offset13] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset14] > cb)\n            if(ptr[offset15] > cb)\n              if(ptr[offset1] > cb)\n                goto is_a_corner;\n              else\n                if(ptr[offset8] > cb)\n                  if(ptr[offset9] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset9] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset5] > cb)\n              if(ptr[offset6] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset9] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset14] > cb)\n                if(ptr[offset15] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n\n```\n\n----------------------------------------\n\nTITLE: Configuring TBB Backend with CMake\nDESCRIPTION: This snippet configures TBB as a parallel backend for an OpenCV example project. It attempts to find TBB and TBB headers, setting up the project if found. The `example-tbb.cpp` is included as an executable, and the necessary directories and libraries are linked. It uses the TBB library and the opencv_core module, though support for a deprecated flag is provided until OpenCV 5.0.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/cpp/tutorial_code/core/parallel_backend/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT OPENCV_EXAMPLES_SKIP_PARALLEL_BACKEND_TBB\n    AND NOT OPENCV_EXAMPLES_SKIP_TBB\n    AND NOT OPENCV_EXAMPLE_SKIP_TBB  # deprecated (to be removed in OpenCV 5.0)\n)\n  project(opencv_example_tbb_backend)\n  find_package(TBB QUIET)\n  if(NOT TBB_FOUND)\n    find_path(TBB_INCLUDE_DIR NAMES \"tbb/tbb.h\")\n    find_library(TBB_LIBRARY NAMES \"tbb\")\n  endif()\n  if(TBB_INCLUDE_DIR AND TBB_LIBRARY)\n    add_executable(opencv_example_tbb_backend example-tbb.cpp)\n    target_include_directories(opencv_example_tbb_backend SYSTEM PRIVATE ${TBB_INCLUDE_DIR})\n    target_link_libraries(opencv_example_tbb_backend PRIVATE\n        opencv_core\n        ${TBB_LIBRARY}\n    )\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Advanced Mouse Event Handling for Drawing Shapes in OpenCV with Python\nDESCRIPTION: This advanced demo allows drawing rectangles or circles by dragging the mouse, similar to a paint application. It uses global variables to track drawing state and mode, and demonstrates more complex event handling with cv.setMouseCallback().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_mouse_handling/py_mouse_handling.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\ndrawing = False # true if mouse is pressed\nmode = True # if True, draw rectangle. Press 'm' to toggle to curve\nix,iy = -1,-1\n\n# mouse callback function\ndef draw_circle(event,x,y,flags,param):\n    global ix,iy,drawing,mode\n\n    if event == cv.EVENT_LBUTTONDOWN:\n        drawing = True\n        ix,iy = x,y\n\n    elif event == cv.EVENT_MOUSEMOVE:\n        if drawing == True:\n            if mode == True:\n                cv.rectangle(img,(ix,iy),(x,y),(0,255,0),-1)\n            else:\n                cv.circle(img,(x,y),5,(0,0,255),-1)\n\n    elif event == cv.EVENT_LBUTTONUP:\n        drawing = False\n        if mode == True:\n            cv.rectangle(img,(ix,iy),(x,y),(0,255,0),-1)\n        else:\n            cv.circle(img,(x,y),5,(0,0,255),-1)\n```\n\n----------------------------------------\n\nTITLE: Compiling OpenCV Example with Makefile\nDESCRIPTION: Compile an example OpenCV C++ program using g++ with a Makefile, linking against OpenCV core library. Helps verify OpenCV installation on target system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_25\n\nLANGUAGE: make\nCODE:\n```\na.out : main.cpp\n    g++ main.cpp -o a.out \\\n        -I/usr/local/include/opencv4 \\\n        -lopencv_core\n```\n\n----------------------------------------\n\nTITLE: Adding TFLite Flatbuffers Support Option for DNN Module - CMake\nDESCRIPTION: Defines an option to enable TFLite support based on the presence of Flatbuffers, validating that Flatbuffers is available if TFLite is requested. Adds related libraries and headers when support is enabled. Inputs are the build target for Flatbuffers and OPENCV_DNN_TFLITE; outputs include flatbuffers library and TFLite schema includes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nocv_option(OPENCV_DNN_TFLITE \"Build with TFLite support\" (TARGET ocv.3rdparty.flatbuffers))\nif(TARGET ocv.3rdparty.flatbuffers AND OPENCV_DNN_TFLITE)\n  if(NOT HAVE_FLATBUFFERS)\n    message(FATAL_ERROR \"DNN: TFLite is not supported without enabled 'flatbuffers'. Check build configuration.\")\n  endif()\n  list(APPEND libs ocv.3rdparty.flatbuffers)\n  list(APPEND fw_hdrs \"${CMAKE_CURRENT_LIST_DIR}/misc/tflite/schema_generated.h\")\n  list(APPEND fw_inc \"${CMAKE_CURRENT_LIST_DIR}/misc/tflite\")\n\n  # Schema is generated by this command:\n  #add_custom_command(\n  #      OUTPUT \"${CMAKE_CURRENT_BINARY_DIR}/schema_generated.h\"\n  #      COMMAND flatbuffers::flatc --cpp -o \"${CMAKE_CURRENT_BINARY_DIR}\" \"${CMAKE_CURRENT_LIST_DIR}/src/tflite/schema.fbs\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Extracting a Single Color Channel using OpenCV Split/Merge (C++)\nDESCRIPTION: Demonstrates extracting a specific color channel from a BGR source image (`src`). It uses `cv::split` to separate the channels into a vector of Mats (`spl`). It then iterates through the channels (0=B, 1=G, 2=R), replacing the Mats for the *unwanted* channels (where index `i` is not equal to the desired `channel`) with Mats filled with zeros (`Mat::zeros`). Finally, it uses `cv::merge` to combine the selected channel's original Mat with the zeroed Mats back into a 3-channel BGR image (`res`). Requires the source image (`src`), a `std::vector<Mat>` (`spl`), the target channel index (`channel`), and the image size (`S`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_write.markdown#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nsplit(src, spl);                 // process - extract only the correct channel\nfor( int i =0; i < 3; ++i)\n   if (i != channel)\n      spl[i] = Mat::zeros(S, spl[0].type());\nmerge(spl, res);\n```\n\n----------------------------------------\n\nTITLE: Defining OpenCV Compilation Options\nDESCRIPTION: This section lists multiple OCV_OPTION functions that define compilation options for integrating various features and third-party libraries into the OpenCV build. Each option includes a description, default state (ON or OFF), visibility conditions, and verification checks for dependencies. This provides a flexible way to customize the OpenCV build for different environments and requirements.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENVINO \"Include Intel OpenVINO toolkit support\" (WITH_INF_ENGINE)\n  VISIBLE_IF TRUE\n  VERIFY TARGET ocv.3rdparty.openvino)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_WEBNN \"Include WebNN support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_WEBNN)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_JASPER \"Include JPEG2K support (Jasper)\" ON\n  VISIBLE_IF NOT IOS AND NOT XROS\n  VERIFY HAVE_JASPER)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENJPEG \"Include JPEG2K support (OpenJPEG)\" ON\n  VISIBLE_IF NOT IOS AND NOT XROS\n  VERIFY HAVE_OPENJPEG)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_JPEG \"Include JPEG support\" ON\n  VISIBLE_IF TRUE\n  VERIFY HAVE_JPEG)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_JPEGXL \"Include JPEG XL support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_JPEGXL)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_WEBP \"Include WebP support\" ON\n  VISIBLE_IF NOT WINRT\n  VERIFY HAVE_WEBP)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENEXR \"Include ILM support via OpenEXR\" ((WIN32 OR ANDROID OR APPLE) OR BUILD_OPENEXR) OR NOT CMAKE_CROSSCOMPILING\n  VISIBLE_IF NOT APPLE_FRAMEWORK AND NOT WINRT\n  VERIFY HAVE_OPENEXR)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENGL \"Include OpenGL support\" OFF\n  VISIBLE_IF NOT ANDROID AND NOT WINRT\n  VERIFY HAVE_OPENGL)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENVX \"Include OpenVX support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_OPENVX)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENNI \"Include OpenNI support\" OFF\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_OPENNI)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENNI2 \"Include OpenNI2 support\" OFF\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_OPENNI2)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_PNG \"Include PNG support\" ON\n  VISIBLE_IF TRUE\n  VERIFY HAVE_PNG)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_SPNG \"Include SPNG support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_SPNG)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_GDCM \"Include DICOM support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_GDCM)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_PVAPI \"Include Prosilica GigE support\" OFF\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_PVAPI)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_ARAVIS \"Include Aravis GigE support\" OFF\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS AND NOT WINRT AND NOT WIN32\n  VERIFY HAVE_ARAVIS_API)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_QT \"Build with Qt Backend support\" OFF\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_QT)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_WIN32UI \"Build with Win32 UI Backend support\" ON\n  VISIBLE_IF WIN32 AND NOT WINRT\n  VERIFY HAVE_WIN32UI)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_TBB \"Include Intel TBB support\" OFF\n  VISIBLE_IF NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_TBB)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_HPX \"Include Ste||ar Group HPX support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_HPX)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENMP \"Include OpenMP support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_OPENMP)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_PTHREADS_PF \"Use pthreads-based parallel_for\" ON\n  VISIBLE_IF NOT WIN32 OR MINGW\n  VERIFY HAVE_PTHREADS_PF)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_TIFF \"Include TIFF support\" ON\n  VISIBLE_IF NOT IOS AND NOT XROS\n  VERIFY HAVE_TIFF)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_V4L \"Include Video 4 Linux support\" ON\n  VISIBLE_IF UNIX AND NOT ANDROID AND NOT APPLE\n  VERIFY HAVE_CAMV4L OR HAVE_CAMV4L2 OR HAVE_VIDEOIO)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_DSHOW \"Build VideoIO with DirectShow support\" ON\n  VISIBLE_IF WIN32 AND NOT ARM AND NOT WINRT\n  VERIFY HAVE_DSHOW)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_MSMF \"Build VideoIO with Media Foundation support\" NOT MINGW\n  VISIBLE_IF WIN32\n  VERIFY HAVE_MSMF)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_MSMF_DXVA \"Enable hardware acceleration in Media Foundation backend\" WITH_MSMF\n  VISIBLE_IF WIN32\n  VERIFY HAVE_MSMF_DXVA)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_XIMEA \"Include XIMEA cameras support\" OFF\n  VISIBLE_IF NOT ANDROID AND NOT WINRT\n  VERIFY HAVE_XIMEA)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_UEYE \"Include UEYE camera support\" OFF\n  VISIBLE_IF NOT ANDROID AND NOT APPLE AND NOT WINRT\n  VERIFY HAVE_UEYE)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_XINE \"Include Xine support (GPL)\" OFF\n  VISIBLE_IF UNIX AND NOT APPLE AND NOT ANDROID\n  VERIFY HAVE_XINE)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_CLP \"Include Clp support (EPL)\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_CLP)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENCL \"Include OpenCL Runtime support\" (NOT ANDROID AND NOT CV_DISABLE_OPTIMIZATION)\n  VISIBLE_IF NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_OPENCL)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENCL_SVM \"Include OpenCL Shared Virtual Memory support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_OPENCL_SVM) # experimental\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENCLAMDFFT \"Include AMD OpenCL FFT library support\" ON\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_CLAMDFFT)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENCLAMDBLAS \"Include AMD OpenCL BLAS library support\" ON\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_CLAMDBLAS)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_DIRECTX \"Include DirectX support\" ON\n  VISIBLE_IF WIN32 AND NOT WINRT\n  VERIFY HAVE_DIRECTX)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_DIRECTML \"Include DirectML support\" ON\n  VISIBLE_IF WIN32 AND NOT WINRT\n  VERIFY HAVE_DIRECTML)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OPENCL_D3D11_NV \"Include NVIDIA OpenCL D3D11 support\" WITH_DIRECTX\n  VISIBLE_IF WIN32 AND NOT WINRT\n  VERIFY HAVE_OPENCL_D3D11_NV)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_LIBREALSENSE \"Include Intel librealsense support\" OFF\n  VISIBLE_IF NOT WITH_INTELPERC\n  VERIFY HAVE_LIBREALSENSE)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_VA \"Include VA support\" (X86_64 OR X86)\n  VISIBLE_IF UNIX AND NOT APPLE AND NOT ANDROID\n  VERIFY HAVE_VA)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_VA_INTEL \"Include Intel VA-API/OpenCL support\" (X86_64 OR X86)\n  VISIBLE_IF UNIX AND NOT APPLE AND NOT ANDROID\n  VERIFY HAVE_VA_INTEL)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_MFX \"Include Intel Media SDK support\" OFF\n  VISIBLE_IF (UNIX AND NOT ANDROID) OR (WIN32 AND NOT WINRT AND NOT MINGW)\n  VERIFY HAVE_MFX)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_GDAL \"Include GDAL Support\" OFF\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS AND NOT WINRT\n  VERIFY HAVE_GDAL)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_GPHOTO2 \"Include gPhoto2 library support\" OFF\n  VISIBLE_IF UNIX AND NOT ANDROID AND NOT IOS AND NOT XROS\n  VERIFY HAVE_GPHOTO2)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_LAPACK \"Include Lapack library support\" (NOT CV_DISABLE_OPTIMIZATION)\n  VISIBLE_IF NOT ANDROID AND NOT IOS AND NOT XROS\n  VERIFY HAVE_LAPACK)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_ITT \"Include Intel ITT support\" ON\n  VISIBLE_IF NOT APPLE_FRAMEWORK\n  VERIFY HAVE_ITT)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_PROTOBUF \"Enable libprotobuf\" ON\n  VISIBLE_IF TRUE\n  VERIFY HAVE_PROTOBUF)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_IMGCODEC_GIF \"Include GIF support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_IMGCODEC_GIF)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_IMGCODEC_HDR \"Include HDR support\" ON\n  VISIBLE_IF TRUE\n  VERIFY HAVE_IMGCODEC_HDR)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_IMGCODEC_SUNRASTER \"Include SUNRASTER support\" ON\n  VISIBLE_IF TRUE\n  VERIFY HAVE_IMGCODEC_SUNRASTER)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_IMGCODEC_PXM \"Include PNM (PBM,PGM,PPM) and PAM formats support\" ON\n  VISIBLE_IF TRUE\n  VERIFY HAVE_IMGCODEC_PXM)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_IMGCODEC_PFM \"Include PFM formats support\" ON\n  VISIBLE_IF TRUE\n  VERIFY HAVE_IMGCODEC_PFM)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_QUIRC \"Include library QR-code decoding\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_QUIRC)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_ANDROID_MEDIANDK \"Use Android Media NDK for Video I/O (Android)\" (ANDROID_NATIVE_API_LEVEL GREATER 20)\n  VISIBLE_IF ANDROID\n  VERIFY HAVE_ANDROID_MEDIANDK)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_ANDROID_NATIVE_CAMERA \"Use Android NDK for Camera I/O (Android)\" (ANDROID_NATIVE_API_LEVEL GREATER 23)\n  VISIBLE_IF ANDROID\n  VERIFY HAVE_ANDROID_NATIVE_CAMERA)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_ONNX \"Include Microsoft ONNX Runtime support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_ONNX)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_TIMVX \"Include Tim-VX support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_TIMVX)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(OBSENSOR_USE_ORBBEC_SDK \"Use Orbbec SDK as backend to support more camera models and platforms (force to ON on MacOS)\" OFF)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_OBSENSOR \"Include obsensor support (Orbbec 3D Cameras)\" ON\n  VISIBLE_IF (WIN32 AND NOT ARM AND NOT WINRT AND NOT MINGW) OR ( UNIX AND NOT APPLE AND NOT ANDROID) OR (APPLE AND AARCH64 AND NOT IOS)\n  VERIFY HAVE_OBSENSOR)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_CANN \"Include CANN support\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_CANN)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_FLATBUFFERS \"Include Flatbuffers support (required by DNN/TFLite importer)\" ON\n  VISIBLE_IF TRUE\n  VERIFY HAVE_FLATBUFFERS)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(WITH_ZLIB_NG \"Use zlib-ng instead of zlib\" OFF\n  VISIBLE_IF TRUE\n  VERIFY HAVE_ZLIB_NG)\n```\n\n----------------------------------------\n\nTITLE: Generating Subset of WinRT Configurations using setup_winrt.bat\nDESCRIPTION: Executes the `setup_winrt.bat` script to generate Visual Studio project files for both Windows Phone and Windows Store platforms, version 8.1, targeting only the x86 architecture. Omitting the `-b` flag means manual building is required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_2\n\nLANGUAGE: batch\nCODE:\n```\nsetup_winrt.bat \"WP,WS\" \"8.1\" \"x86\"\n```\n\n----------------------------------------\n\nTITLE: Find Contours in OpenCV Java\nDESCRIPTION: Finds and stores contours in a list structure, utilizing OpenCV in Java. Dependencies include OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_11\n\nLANGUAGE: Java\nCODE:\n```\nList<MatOfPoint> contours = new ArrayList<>();\nMat hierarchy = new Mat();\nImgproc.findContours(edges, contours, hierarchy, Imgproc.RETR_TREE, Imgproc.CHAIN_APPROX_SIMPLE);\n```\n\n----------------------------------------\n\nTITLE: Conditionally Configuring CUDA Documentation Exclusion in CMake\nDESCRIPTION: Sets a flag `OPENCV_DOCS_EXCLUDE_CUDA` to ON by default. It then checks if the 'cudev' module is present in the `OPENCV_MODULES_EXTRA` list. If 'cudev' is found, it sets the exclusion flag to OFF and adds 'CUDA_MODULES' to the list of enabled Doxygen sections.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENCV_DOCS_EXCLUDE_CUDA ON)\nif(\";${OPENCV_MODULES_EXTRA};\" MATCHES \";cudev;\")\n  set(OPENCV_DOCS_EXCLUDE_CUDA OFF)\n  list(APPEND CMAKE_DOXYGEN_ENABLED_SECTIONS \"CUDA_MODULES\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Example CMake Configuration Output for OpenCV Build (Text)\nDESCRIPTION: Shows a sample snippet of the text output generated by CMake after configuration. This output verifies that key dependencies like GUI toolkits (GTK+), Video I/O libraries (DC1394, FFMPEG, GStreamer, V4L), third-party libraries (Eigen, TBB), and Python interpreter/libraries/numpy are correctly detected and configured for the build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n...\n--   GUI:\n--     GTK+ 2.x:                    YES (ver 2.24.19)\n--     GThread :                    YES (ver 2.36.3)\n\n--   Video I/O:\n--     DC1394 2.x:                  YES (ver 2.2.0)\n--     FFMPEG:                      YES\n--       codec:                     YES (ver 54.92.100)\n--       format:                    YES (ver 54.63.104)\n--       util:                      YES (ver 52.18.100)\n--       swscale:                   YES (ver 2.2.100)\n--       gentoo-style:              YES\n--     GStreamer:\n--       base:                      YES (ver 0.10.36)\n--       video:                     YES (ver 0.10.36)\n--       app:                       YES (ver 0.10.36)\n--       riff:                      YES (ver 0.10.36)\n--       pbutils:                   YES (ver 0.10.36)\n\n--     V4L/V4L2:                    Using libv4l (ver 1.0.0)\n\n--   Other third-party libraries:\n--     Use Eigen:                   YES (ver 3.1.4)\n--     Use TBB:                     YES (ver 4.0 interface 6004)\n\n--   Python:\n--     Interpreter:                 /usr/bin/python2 (ver 2.7.5)\n--     Libraries:                   /lib/libpython2.7.so (ver 2.7.5)\n--     numpy:                       /usr/lib/python2.7/site-packages/numpy/core/include (ver 1.7.1)\n--     packages path:               lib/python2.7/site-packages\n\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Cocoa Backend for HighGUI in CMake\nDESCRIPTION: Sets up the Cocoa backend for OpenCV HighGUI when HAVE_COCOA is enabled on macOS platforms. Adds the necessary source files and links to the Cocoa framework.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nelseif(HAVE_COCOA)\n  set(OPENCV_HIGHGUI_BUILTIN_BACKEND \"COCOA\")\n  add_definitions(-DHAVE_COCOA)\n  list(APPEND highgui_srcs ${CMAKE_CURRENT_LIST_DIR}/src/window_cocoa.mm)\n  list(APPEND HIGHGUI_LIBRARIES \"-framework Cocoa\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Command to Generate OpenCV Java Bindings using Python in CMake\nDESCRIPTION: Sets up a CMake custom command to execute the Java bindings generation process. It specifies the expected output files (`java_generated_files`), primarily a timestamp file (`${OPENCV_DEPHELPER}/gen_opencv_java_source`) indicating completion. The command executes the Python script `gen_java.py`, passing paths to helper scripts and the generated `gen_java.json` configuration file. It defines the working directory and lists numerous dependencies, including the generator scripts, common source files (`deps`), and remapped template files (`__remap_targets`), ensuring the command re-runs if any dependency changes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/generator/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(java_generated_files\n    # \"${OPENCV_JAVA_SIGNATURES_FILE}\"\n    \"${OPENCV_DEPHELPER}/gen_opencv_java_source\"\n)\n\nadd_custom_command(\n    OUTPUT ${java_generated_files}\n    COMMAND ${PYTHON_DEFAULT_EXECUTABLE} \"${JAVA_SOURCE_DIR}/generator/gen_java.py\" -p \"${JAVA_SOURCE_DIR}/../python/src2/gen2.py\" -c \"${CONFIG_FILE}\"\n    COMMAND ${CMAKE_COMMAND} -E touch \"${OPENCV_DEPHELPER}/gen_opencv_java_source\"\n    WORKING_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}\"\n    DEPENDS \"${JAVA_SOURCE_DIR}/generator/gen_java.py\"\n            \"${JAVA_SOURCE_DIR}/../python/src2/gen2.py\"\n            \"${JAVA_SOURCE_DIR}/../python/src2/hdr_parser.py\"\n            # don't, result of file(WRITE): \"${CMAKE_CURRENT_BINARY_DIR}/gen_java.json\"\n            ${deps} ${__remap_targets}\n            # not allowed (file(WRITE) result): \"${CONFIG_FILE}\"\n    COMMENT \"Generate files for Java bindings\"\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Point Polygon Test with OpenCV in Java\nDESCRIPTION: This Java example shows how to utilize OpenCV's Imgproc.pointPolygonTest method to determine if a point is inside, outside, or on the edge of a polygon. The code uses MatOfPoint for contour definition, and Point objects for test points. OpenCV for Java must be properly set up. The code optionally visualizes the results using OpenCV's HighGui.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/point_polygon_test/point_polygon_test.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.*;\\nimport org.opencv.imgproc.Imgproc;\\nimport org.opencv.highgui.HighGui;\\n\\npublic class PointPolygonTestDemo {\\n    public static void main(String[] args) {\\n        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);\\n        // Define the polygon (contour)\\n        MatOfPoint contour = new MatOfPoint(\\n            new Point(100, 100),\\n            new Point(200, 100),\\n            new Point(200, 200),\\n            new Point(100, 200)\\n        );\\n        // Test points\\n        Point testPoint1 = new Point(150, 150);\\n        Point testPoint2 = new Point(250, 150);\\n        // Perform the test\\n        double result1 = Imgproc.pointPolygonTest(new MatOfPoint2f(contour.toArray()), testPoint1, false);\\n        double result2 = Imgproc.pointPolygonTest(new MatOfPoint2f(contour.toArray()), testPoint2, false);\\n        // Output results\\n        System.out.println(\"Test Point 1 (\" + testPoint1.x + \",\" + testPoint1.y + \"): \" + result1);\\n        System.out.println(\"Test Point 2 (\" + testPoint2.x + \",\" + testPoint2.y + \"): \" + result2);\\n        // Visualization (optional)\\n        Mat img = Mat.zeros(300, 300, CvType.CV_8UC3);\\n        Imgproc.polylines(img, Arrays.asList(contour), true, new Scalar(255,255,255), 2);\\n        Imgproc.circle(img, testPoint1, 5, new Scalar(0,255,0), -1);\\n        Imgproc.circle(img, testPoint2, 5, new Scalar(0,0,255), -1);\\n        HighGui.imshow(\"Point Polygon Test\", img);\\n        HighGui.waitKey();\\n        System.exit(0);\\n    }\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Appending Multiple TAGFILES in Doxyfile - Doxygen Configuration\nDESCRIPTION: This snippet shows how to append multiple TAGFILES replacements in Doxygen, handling more than one external documentation source. By using the backslash (\\) for line continuation, additional tag files such as those for libstdc++ can be seamlessly integrated. Dependencies include all referenced .tag files being available at the specified local paths. This is intended to expand cross-referencing: inputs are a continuation of previously set TAGFILES, outputs are doc links to multiple external projects.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/cross_referencing/tutorial_cross_referencing.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Doxygen\nCODE:\n```\nTAGFILES = ./docs/doxygen-tags/libstdc++.tag=https://gcc.gnu.org/onlinedocs/libstdc++/latest-doxygen \\\n           ./docs/doxygen-tags/opencv.tag=http://docs.opencv.org/4.11.0\n```\n\n----------------------------------------\n\nTITLE: Defining 12-Bit JPEG Pixel Sample Data Structures in C\nDESCRIPTION: Defines C typedefs for handling 12-bit pixel sample data within the libjpeg library. `J12SAMPLE` represents a single 12-bit pixel component (0-4095), typically implemented as `short`. `J12SAMPROW` is a pointer to a row of `J12SAMPLE` values. `J12SAMPARRAY` points to an array of rows. `J12SAMPIMAGE` points to an array of component arrays. This follows the same pointer-per-row structure as the 8-bit version.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/structure.txt#2025-04-22_snippet_1\n\nLANGUAGE: c\nCODE:\n```\n    typedef something J12SAMPLE;        a pixel component value, 0..MAXJ12SAMPLE\n    typedef J12SAMPLE *J12SAMPROW;      ptr to a row of samples\n    typedef J12SAMPROW *J12SAMPARRAY;   ptr to a list of rows\n    typedef J12SAMPARRAY *J12SAMPIMAGE; ptr to a list of color-component arrays\n```\n\n----------------------------------------\n\nTITLE: Implementing AGAST 7-12 Detector in OpenCV (C++)\nDESCRIPTION: This C++ snippet implements the AGAST 7-12d algorithm for corner detection in images using the OpenCV library. It uses nested conditional checks and jump statements to determine if a pixel is a corner by comparing intensity values against a threshold. The function takes an image, a keypoints vector, and a threshold as inputs, reserves space for keypoints, computes pixel offsets, and iterates over pixels to identify corners. Dependencies include OpenCV and specifically the 'cv::Mat' and 'KeyPoint' classes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nstatic void AGAST_7_12d(InputArray _img, std::vector<KeyPoint>& keypoints, int threshold)\n{\n    cv::Mat img;\n    if(!_img.getMat().isContinuous())\n      img = _img.getMat().clone();\n    else\n      img = _img.getMat();\n\n    size_t total = 0;\n    int xsize = img.cols;\n    int ysize = img.rows;\n    size_t nExpectedCorners = keypoints.capacity();\n    int x, y;\n    int xsizeB = xsize - 4;\n    int ysizeB = ysize - 3;\n    int width;\n\n    keypoints.resize(0);\n\n    int pixel_7_12d_[16];\n    makeAgastOffsets(pixel_7_12d_, (int)img.step, AgastFeatureDetector::AGAST_7_12d);\n\n    short offset0 = (short) pixel_7_12d_[0];\n    short offset1 = (short) pixel_7_12d_[1];\n    short offset2 = (short) pixel_7_12d_[2];\n    short offset3 = (short) pixel_7_12d_[3];\n    short offset4 = (short) pixel_7_12d_[4];\n    short offset5 = (short) pixel_7_12d_[5];\n    short offset6 = (short) pixel_7_12d_[6];\n    short offset7 = (short) pixel_7_12d_[7];\n    short offset8 = (short) pixel_7_12d_[8];\n    short offset9 = (short) pixel_7_12d_[9];\n    short offset10 = (short) pixel_7_12d_[10];\n    short offset11 = (short) pixel_7_12d_[11];\n\n    width = xsize;\n\n    for(y = 3; y < ysizeB; y++)\n    {\n        x = 2;\n        while(true)\n        {\n          homogeneous:\n          {\n            x++;\n            if(x > xsizeB)\n                break;\n            else\n            {\n                const unsigned char* const ptr = img.ptr() + y*width + x;\n                const int cb = *ptr + threshold;\n                const int c_b = *ptr - threshold;\n                if(ptr[offset0] > cb)\n                  if(ptr[offset5] > cb)\n                    if(ptr[offset2] > cb)\n                      if(ptr[offset9] > cb)\n                        if(ptr[offset1] > cb)\n                          if(ptr[offset6] > cb)\n                            if(ptr[offset3] > cb)\n                              if(ptr[offset4] > cb)\n                                goto success_homogeneous;\n                              else\n                                if(ptr[offset10] > cb)\n                                  if(ptr[offset11] > cb)\n                                    goto success_structured;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                            else\n                              if(ptr[offset8] > cb)\n                                if(ptr[offset10] > cb)\n                                  if(ptr[offset11] > cb)\n                                    goto success_structured;\n                                  else\n                                    if(ptr[offset4] > cb)\n                                      if(ptr[offset7] > cb)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            if(ptr[offset11] > cb)\n                              if(ptr[offset3] > cb)\n                                if(ptr[offset4] > cb)\n                                  goto success_homogeneous;\n                                else\n                                  if(ptr[offset10] > cb)\n                                    goto success_homogeneous;\n                                  else\n                                    goto homogeneous;\n                              else\n                                if(ptr[offset8] > cb)\n                                  if(ptr[offset10] > cb)\n                                    goto success_homogeneous;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          if(ptr[offset6] > cb)\n                            if(ptr[offset7] > cb)\n                              if(ptr[offset8] > cb)\n                                if(ptr[offset4] > cb)\n                                  if(ptr[offset3] > cb)\n                                    goto success_structured;\n                                  else\n                                    if(ptr[offset10] > cb)\n                                      goto success_structured;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      goto success_structured;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                          else\n                            goto homogeneous;\n                      else\n                        if(ptr[offset3] > cb)\n                          if(ptr[offset4] > cb)\n                            if(ptr[offset1] > cb)\n                              if(ptr[offset6] > cb)\n                                goto success_homogeneous;\n                              else\n                                if(ptr[offset11] > cb)\n                                  goto success_homogeneous;\n                                else\n                                  goto homogeneous;\n                            else\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset7] > cb)\n                                  if(ptr[offset8] > cb)\n                                    goto success_homogeneous;\n                                  else\n                                ...\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Total Pixel Count using OpenCV in Python\nDESCRIPTION: Shows how to get the total number of pixels in the image (calculated as rows * columns * channels for color images) using the `img.size` attribute provided by the underlying NumPy array.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_basic_ops/py_basic_ops.markdown#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> print( img.size )\n562248\n```\n\n----------------------------------------\n\nTITLE: Enabling Python 3 Wrapper in CMake Configuration\nDESCRIPTION: Modify CMake configuration to enable Python 3 wrapper for OpenCV. It requires Python 3 paths and libraries to be specified and includes additional options for numpy and Python executable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nPYTHON3_REALPATH=`realpath /usr/bin/python3`\nPYTHON3_BASENAME=`basename ${PYTHON3_REALPATH}`\nPKG_CONFIG_PATH=/usr/lib/aarch64-linux-gnu/pkgconfig:/usr/share/pkgconfig \\\n    PKG_CONFIG_LIBDIR=/usr/lib/aarch64-linux-gnu \\\n    PKG_CONFIG_SYSROOT_DIR=/ \\\n        cmake -S opencv \\\n              -B build4-full_arm64 \\\n              -DCMAKE_TOOLCHAIN_FILE=/home/kmtr/work/opencv/platforms/linux/aarch64-gnu.toolchain.cmake \\\n              -DOPENCV_EXTRA_MODULES_PATH=opencv_contrib/modules \\\n              -DPYTHON3_NUMPY_INCLUDE_DIRS=\"/usr/local/lib/${PYTHON3_BASENAME}/dist-packages/numpy/core/include/\" \\\n              -DPYTHON3_INCLUDE_PATH=\"/usr/include/${PYTHON3_BASENAME};/usr/include/\" \\\n              -DPYTHON3_LIBRARIES=`find /usr/lib/aarch64-linux-gnu/ -name libpython*.so` \\\n              -DPYTHON3_EXECUTABLE=\"/usr/bin/${PYTHON3_BASENAME}\" \\\n              -DPYTHON3_CVPY_SUFFIX=\".so\" \\\n              -GNinja\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Decision Tree Implementation in C++\nDESCRIPTION: A portion of the FAST corner detection algorithm's decision tree that examines pixel values at various offset positions around a central point. The algorithm compares each pixel against brightness thresholds (c_b and cb) to determine if a point is a corner feature, using nested conditional statements with early termination paths.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_33\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\nif(ptr[offset11] < c_b)\n  if(ptr[offset7] < c_b)\n    if(ptr[offset8] < c_b)\n      if(ptr[offset9] < c_b)\n        if(ptr[offset10] < c_b)\n          if(ptr[offset6] < c_b)\n            if(ptr[offset5] < c_b)\n              if(ptr[offset3] < c_b)\n                {} // goto success_homogeneous;\n              else\n                if(ptr[offset12] < c_b)\n                  {} // goto success_homogeneous;\n                else\n                  continue; // goto homogeneous;\n            else\n              if(ptr[offset12] < c_b)\n                if(ptr[offset13] < c_b)\n                  if(ptr[offset14] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset12] < c_b)\n              if(ptr[offset13] < c_b)\n                if(ptr[offset14] < c_b)\n                  if(ptr[offset15] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  if(ptr[offset11] > cb)\n    if(ptr[offset12] > cb)\n      if(ptr[offset13] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset14] > cb)\n            if(ptr[offset15] > cb)\n              if(ptr[offset1] > cb)\n                {} // goto success_homogeneous;\n              else\n                if(ptr[offset8] > cb)\n                  if(ptr[offset9] > cb)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset9] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset5] > cb)\n              if(ptr[offset6] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset9] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n        else\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset14] > cb)\n                if(ptr[offset15] > cb)\n                  {} // goto success_homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n  if(ptr[offset11] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset9] < c_b)\n          if(ptr[offset10] < c_b)\n            if(ptr[offset12] < c_b)\n              if(ptr[offset13] < c_b)\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset5] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    if(ptr[offset14] < c_b)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                else\n                  if(ptr[offset14] < c_b)\n                    if(ptr[offset15] < c_b)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\nif(ptr[offset2] < c_b)\n  if(ptr[offset9] > cb)\n    if(ptr[offset10] > cb)\n      if(ptr[offset11] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset12] > cb)\n            if(ptr[offset13] > cb)\n              if(ptr[offset14] > cb)\n                if(ptr[offset15] > cb)\n                  {} // goto success_homogeneous;\n                else\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset7] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n              else\n                if(ptr[offset5] > cb)\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset7] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n```\n\n----------------------------------------\n\nTITLE: CMake Macro for Finding Specific Source Files\nDESCRIPTION: Defines a CMake macro `glob_more_specific_sources` that searches for source files of a specific type (H, CPP, or JAVA) within predefined subdirectories (`cpp/` or `java/`) relative to a given root path. It takes the type (`_type`), root path (`_root`), and an output list variable (`_output`) as arguments, appending the found file paths to the output list. It uses `file(GLOB ...)` for pattern matching.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\n# UTILITY: glob specific sources and append them to list (type is in H, CPP, JAVA)\nmacro(glob_more_specific_sources _type _root _output)\n  unset(_masks)\n  if(${_type} STREQUAL \"H\")\n    set(_masks \"${_root}/cpp/*.h\" \"${_root}/cpp/*.hpp\")\n  elseif(${_type} STREQUAL \"CPP\")\n    set(_masks \"${_root}/cpp/*.cpp\")\n  elseif(${_type} STREQUAL \"JAVA\")\n    set(_masks \"${_root}/java/*.java\" \"${_root}/java/*.java.in\")\n  endif()\n  if (_masks)\n    file(GLOB _result ${_masks})\n    list(APPEND ${_output} ${_result})\n  else()\n    message(WARNING \"Bad argument passed to macro: skipped\")\n  endif()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Checking Out OpenCV 2.4.13 Tag using Git (Shell)\nDESCRIPTION: Creates new local branches named `v2.4.13` based on the `2.4.13` tag in both the `opencv` and `opencv_extra` repositories. This step is necessary to isolate and build a specific tagged version (2.4.13) of OpenCV 2.4.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\n# Within the opencv directory:\n$ git checkout -b v2.4.13 2.4.13\n```\n\nLANGUAGE: Shell\nCODE:\n```\n# Within the opencv_extra directory:\n$ git checkout -b v2.4.13 2.4.13\n```\n\n----------------------------------------\n\nTITLE: Configuring Android QR Detection Example Project in CMake\nDESCRIPTION: Sets up an Android project for QR code detection using OpenCV. Defines the project name and configures it with required OpenCV library dependencies, targeting Android SDK version 11.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/qr-detection/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(sample example-qr-detection)\n\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\" LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\" SDK_TARGET 11 \"${ANDROID_SDK_TARGET}\")\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Initializing JavaScript Bindings Generator Module in CMake\nDESCRIPTION: Sets up the JavaScript bindings generator module, configures paths, and creates necessary directories.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/js/generator/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(MODULE_NAME \"js_bindings_generator\")\nset(OPENCV_MODULE_IS_PART_OF_WORLD FALSE)\nocv_add_module(${MODULE_NAME} INTERNAL)\n\nset(OPENCV_JS_BINDINGS_DIR \"${CMAKE_CURRENT_BINARY_DIR}\" CACHE INTERNAL \"\")\nfile(REMOVE_RECURSE \"${OPENCV_JS_BINDINGS_DIR}/gen\")\nfile(MAKE_DIRECTORY \"${OPENCV_JS_BINDINGS_DIR}/gen\")\nfile(REMOVE \"${OPENCV_DEPHELPER}/gen_opencv_js_source\")  # force re-run after CMake\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV System-wide with Make\nDESCRIPTION: Command to install OpenCV system-wide using Make, requires root privileges.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nsudo make install\n```\n\n----------------------------------------\n\nTITLE: Checking for libjpeg-turbo Colorspace Extensions at Compile Time in C\nDESCRIPTION: Shows how to use the `#ifdef JCS_EXTENSIONS` C preprocessor directive to conditionally compile code based on the availability of libjpeg-turbo's basic colorspace extensions (like JCS_EXT_RGB, JCS_EXT_BGRX). This allows applications to adapt to different builds or versions of the library and ensures compatibility.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/README.md#2025-04-22_snippet_1\n\nLANGUAGE: c\nCODE:\n```\n#ifdef JCS_EXTENSIONS\n```\n\n----------------------------------------\n\nTITLE: Calculating Optical Flow with Lucas-Kanade in JavaScript\nDESCRIPTION: This snippet demonstrates how to calculate optical flow using Lucas-Kanade method in OpenCV.js. It employs the function cv.calcOpticalFlowPyrLK with multiple parameters to track feature points across video frames. Important prerequisites include the creation of image pyramids and feature points specification. The inputs are two consecutive frames, while the output contains the new positions of tracked points. This method is suitable for sparse optical flow and might not handle large motions well without pyramids.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_video/js_lucas_kanade/js_lucas_kanade.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.calcOpticalFlowPyrLK(prevImg, nextImg, prevPts, nextPts, status, err, winSize = new cv.Size(21, 21), maxLevel = 3, criteria = new cv.TermCriteria(cv.TermCriteria_COUNT + cv.TermCriteria_EPS, 30, 0.01), flags = 0, minEigThreshold = 1e-4);\n```\n\n----------------------------------------\n\nTITLE: Setting Default DNN Backend Option in CMake Cache - CMake\nDESCRIPTION: This snippet defines a cached CMake string variable 'OPENCV_DNN_BACKEND_DEFAULT' to control the default backend used by the DNN module, falling back to DNN_BACKEND_OPENCV if the variable is empty. Affects runtime backend selection. No dependencies required; users can change this setting via the CMake interface.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_23\n\nLANGUAGE: CMake\nCODE:\n```\nset(OPENCV_DNN_BACKEND_DEFAULT \"\" CACHE STRING \"Default backend used by the DNN module (DNN_BACKEND_OPENCV if empty)\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Filter Arguments\nDESCRIPTION: Sets up initial parameters for the filter including kernel size and window name. Creates display window for output.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/filter_2d/filter_2d.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nint kernel_size = 3;\nMat dst;\nchar window_name[] = \"filter2D Demo\";\nnamedWindow(window_name, WINDOW_AUTOSIZE);\n```\n\n----------------------------------------\n\nTITLE: Initializing and Configuring SVM Parameters for Non-Linear Classification (Java)\nDESCRIPTION: Java implementation of setting up SVM parameters for non-linear classification, including kernel type, C value for misclassification penalty, and termination criteria with sufficiently high iteration count.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\n// Set up SVM's parameters\nSVM svm = SVM.create();\nsvm.setType(SVM.C_SVC);\nsvm.setKernel(SVM.RBF);\n// When C is small, the decision boundary will be smooth\n// When C is large, the decision boundary can better classify all training points but may lead to overfitting\nsvm.setC(0.1);\n// Set termination criteria for the optimization\nsvm.setTermCriteria(new TermCriteria(TermCriteria.MAX_ITER, (int) 1e7, 1e-6));\n```\n\n----------------------------------------\n\nTITLE: Converting Integer FourCC to String using Bitwise Operations (C++)\nDESCRIPTION: Illustrates converting an integer FourCC codec identifier back into its 4-character string representation using bitwise AND (`&`) and right shift (`>>`) operations. This method manually extracts each byte corresponding to a character and stores them in a character array, adding a null terminator (`0`). Requires the integer FourCC code (`ex`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_write.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nchar EXT[] = {ex & 0XFF , (ex & 0XFF00) >> 8,(ex & 0XFF0000) >> 16,(ex & 0XFF000000) >> 24, 0};\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV from Source using CMake and Make (Shell)\nDESCRIPTION: These two shell commands initiate building the OpenCV project either via CMake's build wrapper or directly invoking Make. Dependencies are the successful execution of CMake configuration and appropriate build tools installed on the system. <build-directory> is the folder created during configuration; <build-options> are additional options for the build process. Outputs are compiled binaries and libraries placed in the build directory. Direct use of 'make' requires using Unix-like platforms and pre-generated Makefiles.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/general_install/general_install.markdown#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncmake --build <build-directory> <build-options>\n```\n\nLANGUAGE: shell\nCODE:\n```\nmake\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Android Tutorial Project with CMake\nDESCRIPTION: Sets up an Android project for OpenCV tutorial example with conditional dependencies based on Java library build settings. Configures native dependencies, SDK target version, and OpenGL dependencies. Includes project to the main OpenCV Android examples when target exists.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-4-opencl/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(sample example-tutorial-4-opencl)\n\nif(BUILD_FAT_JAVA_LIB)\n  set(native_deps opencv_java)\nelse()\n  set(native_deps opencv_imgproc)\nendif()\n\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\"\n    LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\"\n    SDK_TARGET 21 \"${ANDROID_SDK_TARGET}\"\n    NATIVE_DEPS ${native_deps} -lGLESv2 -lEGL\n    COPY_LIBS YES\n)\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Path for Additional OpenCV Modules in CMake\nDESCRIPTION: This CMake snippet initializes a cache path for additional OpenCV modules. These paths can be multiple and are separated by semicolons, allowing for flexible module management during configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nset(OPENCV_EXTRA_MODULES_PATH \\\"\\\" CACHE PATH \\\"Where to look for additional OpenCV modules (can be ;-separated list of paths)\\\")\n```\n\n----------------------------------------\n\nTITLE: Modifying Kernel Package in OpenCV G-API\nDESCRIPTION: This snippet demonstrates how to remove an inappropriate kernel from the kernel package to address limitations in OpenCV's G-API Fluid backend. The primary change involves excluding the Fluid-based Box filter to fall back on OpenCV's default implementation, thereby overcoming static kernel size constraints. This modification results in enhanced memory performance and fixes app crashing issues.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/anisotropic_segmentation/porting_anisotropic_segmentation.markdown#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/porting_anisotropic_image_segmentation/porting_anisotropic_image_segmentation_gapi_fluid.cpp kernel_hotfix\n```\n\n----------------------------------------\n\nTITLE: Including OpenCV Header Conditionally in Objective-C++ Precompiled Header\nDESCRIPTION: This preprocessor directive checks if the code is being compiled as C++ (`__cplusplus` is defined) and, if so, includes the main OpenCV header file (`opencv2/opencv.hpp`). This is typically added to the project's precompiled header file (`.pch`) to make OpenCV functions available throughout the project when mixing Objective-C and C++, ensuring compatibility.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/hello/hello.markdown#2025-04-22_snippet_0\n\nLANGUAGE: m\nCODE:\n```\n#ifdef __cplusplus\n#import <opencv2/opencv.hpp>\n#endif\n```\n\n----------------------------------------\n\nTITLE: Storing Camera Calibration Results in OpenCV - XML Output File\nDESCRIPTION: This snippet illustrates the XML structure used to store the results of a camera calibration operation performed by the OpenCV-based application. The file includes calibration date, images size, the camera matrix and its standard deviation, distortion coefficients and their uncertainties, and the average re-projection error. This output file is typically named by the user (e.g., camParams.xml), must conform to OpenCV's XML storage format, and can be loaded for later processing or reuse. All numeric values are recorded in double-precision where relevant, and the output reflects the confidence and quality measures of the calibration session.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/interactive_calibration/interactive_calibration.markdown#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\\\"1.0\\\"?>\\n<opencv_storage>\\n<calibrationDate>\\\"Thu 07 Apr 2016 04:23:03 PM MSK\\\"</calibrationDate>\\n<framesCount>21</framesCount>\\n<cameraResolution>\\n  1280 720</cameraResolution>\\n<cameraMatrix type_id=\\\"opencv-matrix\\\">\\n  <rows>3</rows>\\n  <cols>3</cols>\\n  <dt>d</dt>\\n  <data>\\n    1.2519588293098975e+03 0. 6.6684948780852471e+02 0.\\n    1.2519588293098975e+03 3.6298123112613683e+02 0. 0. 1.</data></cameraMatrix>\\n<cameraMatrix_std_dev type_id=\\\"opencv-matrix\\\">\\n  <rows>4</rows>\\n  <cols>1</cols>\\n  <dt>d</dt>\\n  <data>\\n    0. 1.2887048808572649e+01 2.8536856683866230e+00\\n    2.8341737483430314e+00</data></cameraMatrix_std_dev>\\n<dist_coeffs type_id=\\\"opencv-matrix\\\">\\n  <rows>1</rows>\\n  <cols>5</cols>\\n  <dt>d</dt>\\n  <data>\\n    1.3569117181595716e-01 -8.2513063822554633e-01 0. 0.\\n    1.6412101575010554e+00</data></dist_coeffs>\\n<dist_coeffs_std_dev type_id=\\\"opencv-matrix\\\">\\n  <rows>5</rows>\\n  <cols>1</cols>\\n  <dt>d</dt>\\n  <data>\\n    1.5570675523402111e-02 8.7229075437543435e-02 0. 0.\\n    1.8382427901856876e-01</data></dist_coeffs_std_dev>\\n<avg_reprojection_error>4.2691743074130178e-01</avg_reprojection_error>\\n</opencv_storage>\n```\n\n----------------------------------------\n\nTITLE: YOLOv10 Model Execution\nDESCRIPTION: Shell commands for running YOLOv10 model with specific parameters for resolution and preprocessing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_yolo/dnn_yolo.markdown#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncd opencv_extra/testdata/dnn\npython download_models.py yolov10\ncd ..\nexport OPENCV_TEST_DATA_PATH=$(pwd)\ncd <build directory of OpenCV>\n\n./bin/example_dnn_yolo_detector --model=onnx/models/yolov10s.onnx --yolo=yolov10 --width=640 --height=480  --scale=0.003921568627 --padvalue=114\n```\n\n----------------------------------------\n\nTITLE: Building LLVM on Linux using CMake and Make\nDESCRIPTION: This snippet illustrates the steps for configuring and building the LLVM tools on a Linux system using CMake and Make. It requires LLVM source files and CMake installed to perform the build. The code compiles LLVM with assertions enabled and targets the 'X86' architecture.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_halide/dnn_halide.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ncd llvm_root\nmkdir build && cd build\ncmake -DLLVM_ENABLE_TERMINFO=OFF -DLLVM_TARGETS_TO_BUILD=\"X86\" -DLLVM_ENABLE_ASSERTIONS=ON -DCMAKE_BUILD_TYPE=Release ..\nmake -j4\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for Windows Embedded with Visual Studio 2013\nDESCRIPTION: Command for configuring CMake build system for Windows Embedded using Visual Studio 2013 as generator and specifying the installed SDK. Uses the arm-wince toolchain file to set up cross-compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/wince/readme.md#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Visual Studio 12 2013\" -A \"MySDK WEC2013\" -DCMAKE_TOOLCHAIN_FILE:FILEPATH=../platforms/wince/arm-wince.toolchain.cmake\n```\n\n----------------------------------------\n\nTITLE: Setting an Environment Variable in Windows Batch Script\nDESCRIPTION: This Windows batch script snippet demonstrates how to define an environment variable ('MY_ENV_VARIABLE') and execute an application ('my_app.exe') that may respond to it. Dependencies: none beyond Windows CMD. Parameters: The environment variable name and its desired value. Limitations: Applies to the current command-line session; GUI-based setting is also possible as described.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/env_reference/env_reference.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bat\nCODE:\n```\nset MY_ENV_VARIABLE=true\\nC:\\\\my_app.exe\n```\n\n----------------------------------------\n\nTITLE: Registering InterpLayer OpenCV C++\nDESCRIPTION: Demonstrates registering a new layer type in OpenCV to import Caffe models with custom layers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/custom_layers.hpp Register InterpLayer\n```\n\n----------------------------------------\n\nTITLE: Enabling s390 DFLTCC Inflate Optimization for ZLIB in CMake\nDESCRIPTION: Checks if the DFLTCC inflate optimization is enabled (WITH_DFLTCC_INFLATE) for the s390 architecture. If enabled, it adds the S390_DFLTCC_INFLATE definition and appends the corresponding source file (`dfltcc_inflate.c`) to the ZLIB architecture-specific source list.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_22\n\nLANGUAGE: cmake\nCODE:\n```\n        if(WITH_DFLTCC_INFLATE)\n            add_definitions(-DS390_DFLTCC_INFLATE)\n            list(APPEND ZLIB_ARCH_SRCS ${ARCHDIR}/dfltcc_inflate.c)\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Generating OpenJPEG Configuration Headers in CMake\nDESCRIPTION: Uses the `configure_file` command to generate `opj_config.h` and `opj_config_private.h` in the build directory (`CMAKE_CURRENT_BINARY_DIR/openjp2`). These files are created from corresponding `.cmake.in` template files, substituting `@VAR@` placeholders with the values of CMake variables determined during the configuration process (e.g., results from header/function checks like `HAVE_STDINT_H`, `OPJ_BIG_ENDIAN`). The `@ONLY` flag ensures only `@VAR@` substitutions occur.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\n#-----------------------------------------------------------------------------\n# opj_config.h generation (2/2)\nconfigure_file(\n  ${CMAKE_CURRENT_LIST_DIR}/openjp2/opj_config.h.cmake.in\n  ${CMAKE_CURRENT_BINARY_DIR}/openjp2/opj_config.h\n  @ONLY\n)\n\nconfigure_file(\n  ${CMAKE_CURRENT_LIST_DIR}/openjp2/opj_config_private.h.cmake.in\n  ${CMAKE_CURRENT_BINARY_DIR}/openjp2/opj_config_private.h\n  @ONLY\n)\n```\n\n----------------------------------------\n\nTITLE: Enable Compiler Sanitizers in CMake\nDESCRIPTION: This script segment adds different compiler sanitizers like Address, Memory, Thread, and Undefined based on the WITH_SANITIZER setting. Each sanitizer corresponds to specific runtime checks for memory and logic errors during debugging.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nif(WITH_SANITIZER STREQUAL \"Address\")\n    add_address_sanitizer()\nelif(WITH_SANITIZER STREQUAL \"Memory\")\n    add_memory_sanitizer()\nelif(WITH_SANITIZER STREQUAL \"Thread\")\n    add_thread_sanitizer()\nelif(WITH_SANITIZER STREQUAL \"Undefined\")\n    add_undefined_sanitizer()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining ViewController Interface with Camera (Objective-C)\nDESCRIPTION: This code defines the ViewController interface, including IBOutlets for the image view and button, and adds the CvVideoCamera property for handling video input.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/video_processing/video_processing.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Objective-C\nCODE:\n```\n#import <opencv2/videoio/cap_ios.h>\nusing namespace cv;\n\n@interface ViewController : UIViewController\n{\n    IBOutlet UIImageView* imageView;\n    IBOutlet UIButton* button;\n    CvVideoCamera* videoCamera;\n}\n\n- (IBAction)actionStart:(id)sender;\n\n@property (nonatomic, retain) CvVideoCamera* videoCamera;\n\n@end\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVX HAL Cache Variables\nDESCRIPTION: Defines cache variables for OpenVX HAL configuration including version, libraries, headers, and include directories for use in other parts of the build system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openvx/hal/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENVX_HAL_FOUND TRUE CACHE INTERNAL \"\")\nset(OPENVX_HAL_VERSION 0.0.1 CACHE INTERNAL \"\")\nset(OPENVX_HAL_LIBRARIES \"openvx_hal\" CACHE INTERNAL \"\")\nset(OPENVX_HAL_HEADERS \"${CMAKE_CURRENT_SOURCE_DIR}/openvx_hal.hpp\" CACHE INTERNAL \"\")\nset(OPENVX_HAL_INCLUDE_DIRS \"${CMAKE_CURRENT_SOURCE_DIR}\" \"${OPENCV_3P_OPENVX_DIR}/include\" \"${OPENVX_INCLUDE_DIR}\" CACHE INTERNAL \"\")\n```\n\n----------------------------------------\n\nTITLE: Interactive Image Pyramid Loop in C++\nDESCRIPTION: Enters an infinite loop that waits for key presses. Pressing 'i' triggers image upsampling (`pyrUp`), 'o' triggers downsampling (`pyrDown`), and ESC exits the loop. The updated image is displayed after each operation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n    //![loop]\n    Mat tmp = src;\n    Mat dst = tmp;\n\n    for(;;)\n    {\n        imshow( window_name, dst );\n        char c = (char)waitKey(0);\n\n        if( c == 27 )\n        { break; }\n        else if( c == 'i' )\n        { pyrUp( tmp, dst, Size( tmp.cols*2, tmp.rows*2 ) ); printf(\"** Zoom In: Image x 2 \\n\"); }\n        else if( c == 'o' )\n        { pyrDown( tmp, dst, Size( tmp.cols/2, tmp.rows/2 ) ); printf(\"** Zoom Out: Image / 2 \\n\"); }\n\n        tmp = dst;\n    }\n    //![loop]\n\n    return EXIT_SUCCESS;\n}\n```\n\n----------------------------------------\n\nTITLE: Conditional Branch Logic for FAST Corner Detection in C++\nDESCRIPTION: Complex nested conditional structure that compares pixel values at different offsets against threshold values (cb and c_b) to determine if a point is a corner. The algorithm uses goto statements to jump to success or continue checking when certain conditions are met.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_27\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset10] > cb)\n  if(ptr[offset11] > cb)\n    goto success_structured;\n  else\n    goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  if(ptr[offset9] > cb)\n    if(ptr[offset6] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset4] > cb)\n          if(ptr[offset3] > cb)\n            goto success_structured;\n          else\n            if(ptr[offset10] > cb)\n              goto success_structured;\n            else\n              goto structured;\n        else\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              goto success_structured;\n            else\n              goto structured;\n          else\n            goto structured;\n      else\n        goto structured;\n    else\n      goto structured;\n  else\n    goto structured;\nelse\n  goto structured;\nelse\n  if(ptr[offset7] < c_b)\n    if(ptr[offset9] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset5] < c_b)\n          if(ptr[offset1] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                goto success_structured;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset4] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset6] < c_b)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset4] < c_b)\n                if(ptr[offset3] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n              else\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              goto structured;\n        else\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              if(ptr[offset1] < c_b)\n                goto success_structured;\n              else\n                if(ptr[offset6] < c_b)\n                  goto success_structured;\n                else\n                  goto structured;\n            else\n              goto structured;\n          else\n            goto structured;\n      else\n        goto structured;\n    else\n      goto structured;\n  else\n    goto structured;\nelse\n  if(ptr[offset5] > cb)\n    if(ptr[offset7] > cb)\n      if(ptr[offset9] > cb)\n        if(ptr[offset6] > cb)\n          if(ptr[offset4] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset8] > cb)\n                goto success_structured;\n              else\n                if(ptr[offset1] > cb)\n                  if(ptr[offset2] > cb)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset8] > cb)\n                if(ptr[offset10] > cb)\n                  goto success_structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n          else\n            if(ptr[offset11] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset10] > cb)\n                  goto success_structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n            else\n              goto structured;\n        else\n          goto structured;\n      else\n        if(ptr[offset2] > cb)\n          if(ptr[offset3] > cb)\n            if(ptr[offset4] > cb)\n              if(ptr[offset1] > cb)\n                if(ptr[offset6] > cb)\n                  goto success_structured;\n                else\n                  goto structured;\n              else\n                if(ptr[offset6] > cb)\n                  if(ptr[offset8] > cb)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              goto structured;\n          else\n            goto structured;\n        else\n          goto structured;\n    else\n      goto structured;\n  else\n    if(ptr[offset5] < c_b)\n      if(ptr[offset7] < c_b)\n        if(ptr[offset9] < c_b)\n          if(ptr[offset6] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset8] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset1] < c_b)\n                    if(ptr[offset2] < c_b)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n              else\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset11] < c_b)\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n          else\n            goto structured;\n        else\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Serialization Methods for File I/O (Inside Class) - OpenCV Python\nDESCRIPTION: In this Python snippet, custom read and write methods are added to the custom class for manual serialization and deserialization with OpenCV FileStorage. The write method inserts all attributes, while read pulls named values from the node. The functions assume the nodes conform to the written format.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ndef write(self, fs):\\n    fs.startWriteStruct('MyData', cv2.FileNode_MAP)\\n    fs.write('A', self.A)\\n    fs.write('X', self.X)\\n    fs.write('id', self.name)\\n    fs.endWriteStruct()\\ndef read(self, node):\\n    self.A = int(node.getNode('A').real())\\n    self.X = float(node.getNode('X').real())\\n    self.name = node.getNode('id').string()\n```\n\n----------------------------------------\n\nTITLE: Installing Python 3 Dependencies on Target\nDESCRIPTION: Install Python 3 minimal and numpy packages on the target system to enable Python 3 wrapper support for OpenCV. It uses apt package manager to handle installations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y \\\n    python3-minimal \\\n    python3-numpy\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Decision Tree in C++\nDESCRIPTION: This code snippet is part of the FAST corner detection algorithm. It compares pixel intensities at specific offsets around a central pixel, using a series of nested if-else statements to determine if the pixel is a corner point. The algorithm continues or jumps to specific labels based on these comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_32\n\nLANGUAGE: C++\nCODE:\n```\ncontinue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\nif(ptr[offset12] < c_b)\n  if(ptr[offset7] < c_b)\n    if(ptr[offset8] < c_b)\n      if(ptr[offset9] < c_b)\n        if(ptr[offset10] < c_b)\n          if(ptr[offset11] < c_b)\n            if(ptr[offset13] < c_b)\n              if(ptr[offset6] < c_b)\n                {} // goto success_homogeneous;\n              else\n                if(ptr[offset14] < c_b)\n                  if(ptr[offset15] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  if(ptr[offset12] > cb)\n    if(ptr[offset13] > cb)\n      if(ptr[offset14] > cb)\n        if(ptr[offset15] > cb)\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              {} // goto success_homogeneous;\n            else\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  {} // goto success_homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset8] > cb)\n              if(ptr[offset9] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n        else\n          if(ptr[offset6] > cb)\n            if(ptr[offset7] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset9] > cb)\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n  if(ptr[offset12] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset9] < c_b)\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              if(ptr[offset13] < c_b)\n                if(ptr[offset14] < c_b)\n                  if(ptr[offset6] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    if(ptr[offset15] < c_b)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\nif(ptr[offset4] < c_b)\n  if(ptr[offset11] > cb)\n    if(ptr[offset12] > cb)\n      if(ptr[offset13] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset14] > cb)\n            if(ptr[offset15] > cb)\n              if(ptr[offset1] > cb)\n                {} // goto success_homogeneous;\n              else\n                if(ptr[offset8] > cb)\n                  if(ptr[offset9] > cb)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset9] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset5] > cb)\n              if(ptr[offset6] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset9] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n        else\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset14] > cb)\n                if(ptr[offset15] > cb)\n                  {} // goto success_homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenMP Plugin for OpenCV Core\nDESCRIPTION: CMake configuration script that sets up the OpenMP parallel processing plugin for OpenCV. It establishes minimum CMake version, configures project paths, enables OpenMP support, and creates the plugin target with necessary dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/misc/plugins/parallel_openmp/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5)\nproject(opencv_core_parallel_openmp CXX)\n\nget_filename_component(OpenCV_SOURCE_DIR \"${CMAKE_CURRENT_LIST_DIR}/../../../../..\" ABSOLUTE)\ninclude(\"${OpenCV_SOURCE_DIR}/cmake/OpenCVPluginStandalone.cmake\")\n\n# scan dependencies\nset(WITH_OPENMP ON)\ninclude(\"${OpenCV_SOURCE_DIR}/modules/core/cmake/parallel/init.cmake\")\n\nmessage(STATUS \"OpenMP: ${OpenMP_CXX_VERSION}\")\nocv_create_plugin(core \"opencv_core_parallel_openmp\" \"ocv.3rdparty.openmp\" \"OPENMP\" \"src/parallel/parallel_openmp.cpp\")\n```\n\n----------------------------------------\n\nTITLE: Checking OpenCV CMake Package Files\nDESCRIPTION: Command to check the presence of OpenCV CMake package files in the build directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nls OpenCV*\n```\n\n----------------------------------------\n\nTITLE: Setting Up Emscripten LLVM Upstream Backend for SIMD Support - Bash\nDESCRIPTION: This series of bash commands updates and activates the latest upstream version of Emscripten SDK, which is required to build OpenCV.js with SIMD optimizations. It uses './emsdk' to update, install, activate, and set up the environment. These steps are prerequisites when building with SIMD support enabled and should be run prior to the build command.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n./emsdk update\\n./emsdk install latest-upstream\\n./emsdk activate latest-upstream\\nsource ./emsdk_env.sh\n```\n\n----------------------------------------\n\nTITLE: Collecting OpenCV Headers for JavaScript Bindings in CMake\nDESCRIPTION: Gathers headers from specified OpenCV modules and filters out unwanted headers based on predefined criteria.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/js/generator/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(opencv_hdrs \"\")\nforeach(m ${OPENCV_JS_MODULES})\n  list(APPEND opencv_hdrs ${OPENCV_MODULE_${m}_HEADERS})\nendforeach(m)\n\n# header blacklist\nocv_list_filterout(opencv_hdrs \"modules/.*.h$\")\nocv_list_filterout(opencv_hdrs \"modules/core/.*/cuda\")\nocv_list_filterout(opencv_hdrs \"modules/core/.*/opencl\")\nocv_list_filterout(opencv_hdrs \"modules/core/include/opencv2/core/opengl.hpp\")\nocv_list_filterout(opencv_hdrs \"modules/core/include/opencv2/core/ocl.hpp\")\nocv_list_filterout(opencv_hdrs \"modules/cuda.*\")\nocv_list_filterout(opencv_hdrs \"modules/cudev\")\nocv_list_filterout(opencv_hdrs \"modules/core/.*/hal/\")\nocv_list_filterout(opencv_hdrs \"modules/.*/detection_based_tracker.hpp\") # Conditional compilation\nocv_list_filterout(opencv_hdrs \"modules/core/include/opencv2/core/utils/*.private.*\")\nocv_list_filterout(opencv_hdrs \"modules/core/include/opencv2/core/utils/instrumentation.hpp\")\nocv_list_filterout(opencv_hdrs \"modules/core/include/opencv2/core/utils/trace*\")\n\nocv_update_file(\"${CMAKE_CURRENT_BINARY_DIR}/headers.txt\" \"${opencv_hdrs}\")\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV via Homebrew on macOS\nDESCRIPTION: This command uses the Homebrew package manager to install a pre-built version of OpenCV on macOS.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/macos_install/macos_install.markdown#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nbrew install opencv\n```\n\n----------------------------------------\n\nTITLE: Basic Control Flow for Buffered-Image JPEG Decoding in C\nDESCRIPTION: Demonstrates the main control flow sequence for buffered-image decoding, including initialization, processing multiple output passes, and cleanup. Uses jpeg*_read_scanlines() functions for different data precision ranges.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_53\n\nLANGUAGE: c\nCODE:\n```\njpeg_create_decompress()\nset data source\njpeg_read_header()\nset overall decompression parameters\ncinfo.buffered_image = TRUE;    /* select buffered-image mode */\njpeg_start_decompress()\nfor (each output pass) {\n    adjust output decompression parameters if required\n    jpeg_start_output()         /* start a new output pass */\n    for (all scanlines in image) {\n        jpeg_read_scanlines()   /* Use jpeg12_read_scanlines() for\n                                   9-bit through 12-bit data precision\n                                   and jpeg16_read_scanlines() for\n                                   13-bit through 16-bit data\n                                   precision. */\n        display scanlines\n    }\n    jpeg_finish_output()        /* terminate output pass */\n}\njpeg_finish_decompress()\njpeg_destroy_decompress()\n```\n\n----------------------------------------\n\nTITLE: SVM Kernel Mapping Function\nDESCRIPTION: Mathematical representation of mapping 2D points to 3D space using phi function for kernel transformation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_svm/py_svm_basics/py_svm_basics.markdown#2025-04-22_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n\\phi (p) = (p_{1}^2,p_{2}^2,\\sqrt{2} p_1 p_2)\\n\\phi (q) = (q_{1}^2,q_{2}^2,\\sqrt{2} q_1 q_2)\n```\n\n----------------------------------------\n\nTITLE: Initial Configuring and Printing Options with CMake\nDESCRIPTION: The code demonstrates how to configure OpenCV initially and print all available options using CMake. It includes options to display help messages and advanced settings. The commands require pre-installed CMake and access to the OpenCV source folder.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# initial configuration\ncmake ../opencv\n\n# print all options\ncmake -L\n\n# print all options with help message\ncmake -LH\n\n# print all options including advanced\ncmake -LA\n```\n\n----------------------------------------\n\nTITLE: Defining the FLANN Module for OpenCV with Python Bindings - CMake\nDESCRIPTION: Declares and configures the FLANN module within the OpenCV CMake build system, specifying a dependency on opencv_core and enabling Python wrapper generation with the WRAP python option. Requires the ocv_define_module CMake macro, typically provided by OpenCV's build infrastructure. Accepts the module name, required dependencies, and options for bindings. The input is a set of identifiers; the output is the configuration of the FLANN module within the OpenCV build tree. Applicable only within OpenCV CMake-based builds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/flann/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nocv_define_module(flann opencv_core WRAP python)\n```\n\n----------------------------------------\n\nTITLE: Declaring Variables (Parsing Arguments) for Laplacian Demo in Python\nDESCRIPTION: Initializes Python variables for the Laplacian demo by parsing command-line arguments using argparse to get the input image path. Sets default values for kernel size, scale, delta, and output depth (ddepth). Requires argparse and cv2.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#! [variables]\n# [variables]\n# Declare the variables we are going to use\nkernel_size = 3\nscale = 1\ndelta = 0\nddepth = cv.CV_16S\n# [variables]\n# ! [variables]\n```\n\n----------------------------------------\n\nTITLE: Decompressing Partial Scanlines in JPEG Images (C)\nDESCRIPTION: This function allows decompression of only a portion of each row in a JPEG image. It must be called after jpeg_start_decompress() and before any calls to jpeg*_read_scanlines() or jpeg*_skip_scanlines(). The function adjusts xoffset to fall on an iMCU boundary and updates width accordingly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_19\n\nLANGUAGE: C\nCODE:\n```\njpeg_crop_scanline (j_decompress_ptr cinfo, JDIMENSION *xoffset,\n                            JDIMENSION *width)\n                /* Use jpeg12_crop_scanline() for 12-bit data precision. */\n```\n\n----------------------------------------\n\nTITLE: Displaying Help for setup_winrt.bat\nDESCRIPTION: Executes the `setup_winrt.bat` script with the `-h` flag to display its available command-line options and usage instructions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_3\n\nLANGUAGE: batch\nCODE:\n```\nsetup_winrt.bat -h\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Conditional Logic in C++\nDESCRIPTION: This code snippet is part of the FAST corner detection algorithm implementation. It uses nested if-else statements to compare pixel values at various offsets to determine if a point is a corner. The algorithm checks surrounding pixels against a threshold and makes decisions based on these comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  if(ptr[offset2] < c_b)\n    if(ptr[offset9] > cb)\n      if(ptr[offset5] > cb)\n        if(ptr[offset1] < c_b)\n          if(ptr[offset4] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset11] > cb)\n                      if(ptr[offset10] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset7] > cb)\n                if(ptr[offset8] > cb)\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset4] > cb)\n                      goto is_a_corner;\n                    else\n                      if(ptr[offset11] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset3] > cb)\n                      if(ptr[offset4] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset6] > cb)\n            if(ptr[offset7] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset3] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset10] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset3] < c_b)\n          if(ptr[offset4] < c_b)\n            if(ptr[offset5] < c_b)\n              if(ptr[offset1] < c_b)\n                if(ptr[offset6] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset11] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset7] < c_b)\n                    if(ptr[offset8] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset1] < c_b)\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      if(ptr[offset9] < c_b)\n        if(ptr[offset5] < c_b)\n          if(ptr[offset1] < c_b)\n            if(ptr[offset6] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset4] < c_b)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      if(ptr[offset4] < c_b)\n                        if(ptr[offset7] < c_b)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset11] < c_b)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset10] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset8] < c_b)\n                    if(ptr[offset10] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset7] < c_b)\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset4] < c_b)\n                    if(ptr[offset3] < c_b)\n                      goto is_a_corner;\n                    else\n                      if(ptr[offset10] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset10] < c_b)\n                      if(ptr[offset11] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset10] < c_b)\n```\n\n----------------------------------------\n\nTITLE: CMake Macro for Copying Common Java Test Files\nDESCRIPTION: Defines a CMake macro `copy_common_tests` to recursively copy files from a source location (`_src_location`) matching `res/*` or `src/*` patterns to a destination location (`_dst_location`). It uses `add_custom_command` to perform the copy operation only if the source file is newer or doesn't exist at the destination. It also appends both the source and destination file paths to a dependency list variable (`_deps`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n# UTILITY: copy common java test files and add them to _deps\n# copy_common_tests(<source-folder> <destination-folder> <variable-to-store-deps>)\nmacro(copy_common_tests _src_location _dst_location _deps)\n  set(_src ${_src_location})\n  set(_dst ${_dst_location})\n  file(GLOB_RECURSE _files RELATIVE \"${_src}\" \"${_src}/res/*\" \"${_src}/src/*\")\n  foreach(f ${_files})\n    add_custom_command(\n        OUTPUT \"${_dst}/${f}\"\n        COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${_src}/${f}\" \"${_dst}/${f}\"\n        MAIN_DEPENDENCY \"${_src}/${f}\"\n        COMMENT \"Copying ${f}\")\n    list(APPEND ${_deps} \"${_src}/${f}\" \"${_dst}/${f}\")\n  endforeach()\n  unset(_files)\n  unset(_src)\n  unset(_dst)\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Adding Text to Images with OpenCV in Python\nDESCRIPTION: Demonstrates how to add text to an image using the cv.putText() function, specifying font type, scale, color, and other parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_drawing_functions/py_drawing_functions.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfont = cv.FONT_HERSHEY_SIMPLEX\ncv.putText(img,'OpenCV',(10,500), font, 4,(255,255,255),2,cv.LINE_AA)\n```\n\n----------------------------------------\n\nTITLE: Defining Advanced Camera Calibration Parameters in OpenCV - XML\nDESCRIPTION: This snippet defines an XML configuration structure for advanced camera calibration parameters in OpenCV. It details settings necessary for chAruco pattern generation, frame filtering, solver behavior, and capture constraints. The file should be named (for example) defaultConfig.xml and is intended to be read by the calibration application at startup to override defaults. Users must fill in appropriate values matching their calibration board setup and hardware, and ensure the XML complies with the OpenCV file storage format.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/interactive_calibration/interactive_calibration.markdown#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\\\"1.0\\\"?>\\n<opencv_storage>\\n<charuco_dict>0</charuco_dict>\\n<charuco_square_length>200</charuco_square_length>\\n<charuco_marker_size>100</charuco_marker_size>\\n<calibration_step>1</calibration_step>\\n<max_frames_num>30</max_frames_num>\\n<min_frames_num>10</min_frames_num>\\n<solver_eps>1e-7</solver_eps>\\n<solver_max_iters>30</solver_max_iters>\\n<fast_solver>0</fast_solver>\\n<frame_filter_conv_param>0.1</frame_filter_conv_param>\\n<camera_resolution>1280 720</camera_resolution>\\n</opencv_storage>\n```\n\n----------------------------------------\n\nTITLE: Importing OpenCV Framework in Swift\nDESCRIPTION: This snippet demonstrates how to import the OpenCV framework module into a Swift source file to enable usage of OpenCV features. Before use, ensure that the OpenCV framework has been added to your Xcode project and configured correctly for Swift interoperability. Input: none; Output: the OpenCV module is made accessible in the current scope.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/objc/doc/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Swift\nCODE:\n```\nimport OpenCV\n```\n\n----------------------------------------\n\nTITLE: Classification Test Configuration Class\nDESCRIPTION: DataClass defining test configuration parameters including batch size, image dimensions, and data paths for classification evaluation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass TestClsConfig:\n    batch_size: int = 50\n    frame_size: int = 224\n    img_root_dir: str = \"./ILSVRC2012_img_val\"\n    # location of image-class matching\n    img_cls_file: str = \"./val.txt\"\n    bgr_to_rgb: bool = True\n```\n\n----------------------------------------\n\nTITLE: Template Matching Formula: TM_SQDIFF (LaTeX)\nDESCRIPTION: Mathematical formula for the Squared Difference (TM_SQDIFF) template matching method used in OpenCV's `matchTemplate` function. R(x,y) is the result, T is the template, and I is the image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n\\f[R(x,y)= \\sum _{x',y'} (T(x',y')-I(x+x',y+y'))^2\\f]\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Target for Android Build in CMake\nDESCRIPTION: This CMake command defines a custom target named `${the_module}_android`. This target is added to the `ALL` target, meaning it will be built by default. It depends on the completion of the custom command that builds the AAR, signified by the existence of the `${OPENCV_DEPHELPER}/${the_module}_android` file. It also lists the Android manifest file as a source file associated with this target.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/android_sdk/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(${the_module}_android ALL DEPENDS \"${OPENCV_DEPHELPER}/${the_module}_android\" SOURCES \"${__base_dir}/${ANDROID_MANIFEST_FILE}\")\n```\n\n----------------------------------------\n\nTITLE: Checking Object Detection Help Documentation\nDESCRIPTION: Command to display help information for the object detection script.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython object_detection.py opencv_fd -h\n```\n\n----------------------------------------\n\nTITLE: Creating cv.Mat from ImageData in JavaScript\nDESCRIPTION: Illustrates constructing a cv.Mat object from an ImageData object for further processing with OpenCV.js. This operation assumes the canvas contains 8-bit RGBA images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_image_display/js_image_display.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet src = cv.matFromImageData(imgData);\n```\n\n----------------------------------------\n\nTITLE: Splitting Image into BGR Planes in C++\nDESCRIPTION: C++ snippet using OpenCV's `split` function to separate a 3-channel source image (assumed to be in BGR format) into three individual single-channel Mats, stored in a `std::vector<Mat>`. The input is the source image, and the output is the vector of planes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/calcHist_Demo.cpp Separate the image in 3 places ( B, G and R )\n```\n\n----------------------------------------\n\nTITLE: Converting Color Image to Grayscale using OpenCV in Objective-C\nDESCRIPTION: This code snippet shows how to convert a color image (inputMat) to grayscale using OpenCV's cvtColor function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/image_manipulation/image_manipulation.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Objective-C\nCODE:\n```\ncv::Mat greyMat;\ncv::cvtColor(inputMat, greyMat, COLOR_BGR2GRAY);\n```\n\n----------------------------------------\n\nTITLE: Implementing Structured Pixel Comparison for Feature Detection in C++\nDESCRIPTION: This code snippet implements a structured approach to pixel comparison for feature detection. It uses nested if-else statements to compare pixel values at various offsets against thresholds, determining whether to proceed to a 'success_structured' or 'structured' state.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_23\n\nLANGUAGE: C++\nCODE:\n```\nx++;\nif(x > xsizeB)\n    break;\nelse\n{\n    const unsigned char* const ptr = img.ptr() + y*width + x;\n    const int cb = *ptr + threshold;\n    const int c_b = *ptr - threshold;\n    if(ptr[offset0] > cb)\n      if(ptr[offset2] > cb)\n        if(ptr[offset5] > cb)\n          if(ptr[offset9] > cb)\n            if(ptr[offset7] > cb)\n              if(ptr[offset1] > cb)\n                if(ptr[offset6] > cb)\n                  if(ptr[offset3] > cb)\n                    if(ptr[offset4] > cb)\n                      goto success_structured;\n                    else\n                      if(ptr[offset10] > cb)\n                        if(ptr[offset11] > cb)\n                          goto success_structured;\n                        else\n                          goto structured;\n                      else\n                        goto structured;\n                  else\n                    if(ptr[offset8] > cb)\n                      if(ptr[offset10] > cb)\n                        if(ptr[offset4] > cb)\n                          goto success_structured;\n                        else\n                          if(ptr[offset11] > cb)\n                            goto success_structured;\n                          else\n                            goto structured;\n                      else\n                        goto structured;\n                    else\n                      goto structured;\n                else\n                  if(ptr[offset11] > cb)\n                    if(ptr[offset3] > cb)\n                      if(ptr[offset4] > cb)\n                        goto success_structured;\n                      else\n                        if(ptr[offset10] > cb)\n                          goto success_structured;\n                        else\n                          goto structured;\n                    else\n                      if(ptr[offset8] > cb)\n                        if(ptr[offset10] > cb)\n                          goto success_structured;\n                        else\n                          goto structured;\n                      else\n                        goto structured;\n                  else\n                    goto structured;\n              else\n                if(ptr[offset6] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset4] > cb)\n                      if(ptr[offset3] > cb)\n                        goto success_structured;\n                      else\n                        if(ptr[offset10] > cb)\n                          goto success_structured;\n                        else\n                          goto structured;\n                    else\n                      if(ptr[offset10] > cb)\n                        if(ptr[offset11] > cb)\n                          goto success_structured;\n                        else\n                          goto structured;\n                      else\n                        goto structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset1] > cb)\n                if(ptr[offset11] > cb)\n                  if(ptr[offset3] > cb)\n                    if(ptr[offset4] > cb)\n                      goto success_structured;\n                    else\n                      if(ptr[offset10] > cb)\n                        goto success_structured;\n                      else\n                        goto structured;\n                  else\n                    if(ptr[offset8] > cb)\n                      if(ptr[offset10] > cb)\n                        goto success_structured;\n                      else\n                        goto structured;\n                    else\n                      goto structured;\n                else\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset3] > cb)\n                      if(ptr[offset4] > cb)\n```\n\n----------------------------------------\n\nTITLE: Main Function of Motion Deblur Filter in C++\nDESCRIPTION: Contains the core algorithm for recovering a motion-blurred image using PSF generation, Wiener filter, and frequency domain filtering. It premises the use of OpenCV with robust frequency domain techniques for image restoration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/motion_deblur_filter/motion_deblur_filter.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n@snippet samples/cpp/tutorial_code/ImgProc/motion_deblur_filter/motion_deblur_filter.cpp main\n```\n\n----------------------------------------\n\nTITLE: C++11 Lambda for Parallel Convolution in OpenCV\nDESCRIPTION: This snippet demonstrates simplifying the parallel convolution implementation using C++11 lambdas. By eliminating the necessity for a separate class, the lambda expression provides a concise approach for leveraging OpenCV's parallel_for_ framework.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_new/how_to_use_OpenCV_parallel_for_new.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_use_OpenCV_parallel_for_new.cpp convolution-parallel-cxx11\n```\n\n----------------------------------------\n\nTITLE: Optimized GPU Arithmetic using Explicit Functions in C++\nDESCRIPTION: Demonstrates the optimized way to perform the arithmetic operation `t1 = 2 * mu1_mu2 + C1` on the GPU. Instead of using overloaded operators, it explicitly calls `gpu::multiply` and `gpu::add`. This avoids the creation of temporary intermediate `GpuMat` objects, reducing memory overhead and potentially improving performance by utilizing in-place operations where possible.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\ngpu::multiply(b.mu1_mu2, 2, b.t1); //b.t1 = 2 * b.mu1_mu2 + C1;\ngpu::add(b.t1, C1, b.t1);\n```\n\n----------------------------------------\n\nTITLE: Importing OpenCV in Python\nDESCRIPTION: This snippet shows how to import the OpenCV library in Python and assign it the alias 'cv' for easy access. The alias helps keep the code concise and readable, especially when calling OpenCV's functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/display_image/display_image.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Serialization for File I/O (Outside Class) - OpenCV C++\nDESCRIPTION: This C++ snippet defines global read() and write() methods outside the custom class to support OpenCV FileStorage serialization for user-defined types. It sets default values if a node is missing and allows using << and >> operators for file I/O. All members must be assigned by reading the correct node elements.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\nvoid write(FileStorage& fs, const MyData& data)\\n{\\n    data.write(fs);\\n}\\nvoid read(const FileNode& node, MyData& data, const MyData& defaultData = MyData())\\n{\\n    if(node.empty())\\n        data = defaultData;\\n    else\\n        data.read(node);\\n}\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Plugins (Shell)\nDESCRIPTION: This multi-line shell script shows the process to configure and build OpenCV plugins (such as TBB parallel backends) using CMake in a plugin subdirectory. It requires compatible versions of the TBB library and the OpenCV build, CMake, and a compiler matching the main build. Environment variables such as TBB_DIR and specific CMake define options (OPENCV_PLUGIN_NAME, OPENCV_PLUGIN_DESTINATION, CMAKE_BUILD_TYPE) must be set accordingly. The <suffix>, <dest-folder>, <config>, and source directory path need to be adjusted for your environment. Plugin binaries will be generated in the current directory and can be loaded dynamically by OpenCV if named correctly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/general_install/general_install.markdown#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# set-up environment for TBB detection, for example:\\n#   export TBB_DIR=<dir-with-tbb-cmake-config>\\ncmake -G<generator> \\\\n    -DOPENCV_PLUGIN_NAME=opencv_core_tbb_<suffix> \\\\n    -DOPENCV_PLUGIN_DESTINATION=<dest-folder> \\\\n    -DCMAKE_BUILD_TYPE=<config> \\\\n    <opencv>/modules/core/misc/plugins/parallel_tbb\\ncmake --build . --config <config>\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Java JAR Build Environment\nDESCRIPTION: Sets up directories, file paths, and build variables for the OpenCV Java JAR. Configures source file copying and prepares the build environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jar/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(${the_module}_jar)\n\nset(OPENCV_JAVA_DIR \"${CMAKE_CURRENT_BINARY_DIR}/opencv\" CACHE INTERNAL \"\")\n\nfile(REMOVE_RECURSE \"${OPENCV_JAVA_DIR}\")\nfile(REMOVE \"${OPENCV_DEPHELPER}/${the_module}_jar_source_copy\")\n\nset(java_src_dir \"${OPENCV_JAVA_DIR}/java\")\nfile(MAKE_DIRECTORY \"${java_src_dir}\")\n\nset(JAR_NAME_WE opencv-${OPENCV_JAVA_LIB_NAME_SUFFIX})\nset(JAR_NAME ${JAR_NAME_WE}.jar)\nset(OPENCV_JAR_DIR \"${OpenCV_BINARY_DIR}/bin/\" CACHE INTERNAL \"\")\nset(OPENCV_JAR_FILE \"${OPENCV_JAR_DIR}${JAR_NAME}\" CACHE INTERNAL \"\")\n\nocv_copyfiles_append_dir(JAVA_SRC_COPY \"${OPENCV_JAVA_BINDINGS_DIR}/gen/java\" \"${java_src_dir}\")\n\nset(depends gen_opencv_java_source \"${OPENCV_DEPHELPER}/gen_opencv_java_source\")\nocv_copyfiles_add_target(${the_module}_jar_source_copy JAVA_SRC_COPY \"Copy Java(JAR) source files\" ${depends})\nset(depends ${the_module}_jar_source_copy \"${OPENCV_DEPHELPER}/${the_module}_jar_source_copy\")\n```\n\n----------------------------------------\n\nTITLE: Setting Description for OpenCV Module - CMake\nDESCRIPTION: Assigns a human-readable description to the OpenCV FLANN module for documentation and configuration purposes. This variable is used internally by the OpenCV CMake build system to provide information about the module to users and maintainers. There are no parameters other than the string; no explicit dependencies beyond standard CMake usage.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/flann/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(the_description \"Clustering and Search in Multi-Dimensional Spaces\")\n```\n\n----------------------------------------\n\nTITLE: Configuring TBB Include Directories and Source Files\nDESCRIPTION: Sets up the include directories for TBB and collects the source files for compilation. Filters out specific files that have external dependencies to simplify the build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/tbb/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nocv_include_directories(\"${tbb_src_dir}/include\"\n                        \"${tbb_src_dir}/src/\"\n                        \"${tbb_src_dir}/src/rml/include\"\n                        \"${CMAKE_CURRENT_SOURCE_DIR}\")\n\nfile(GLOB lib_srcs \"${tbb_src_dir}/src/tbb/*.cpp\")\nfile(GLOB lib_hdrs \"${tbb_src_dir}/src/tbb/*.h\")\nocv_list_filterout(lib_srcs \"${tbb_src_dir}/src/tbb/tbbbind.cpp\")  # hwloc.h requirement\nocv_list_filterout(lib_srcs \"${tbb_src_dir}/src/tbb/tbb_bind.cpp\")  # hwloc.h requirement 2020.1+\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV with OpenNI2 Support using CMake\nDESCRIPTION: Creates a build directory, navigates into it, and runs CMake to configure the OpenCV build. The '-DWITH_OPENNI2=ON' flag explicitly enables OpenNI2 support, instructing CMake to find the necessary OpenNI libraries and headers (using the environment variables set previously) and include OpenNI-related functionality in the build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ mkdir build\n$ cd build\n$ cmake -DWITH_OPENNI2=ON ..\n```\n\n----------------------------------------\n\nTITLE: Building a Minimal OpenCV Debugging Application in C++\nDESCRIPTION: This C++ snippet demonstrates a simple command-line program that loads an image and applies the Canny edge detector. It illustrates the use of standard OpenCV headers and objects (cv::Mat), and is meant to be built and debugged with Visual Studio to showcase the Image Watch visual debugger extension. Users must supply an input image filename as a command-line argument, and the program requires an OpenCV installation configured with Visual Studio. The code produces no output files and is intended for in-memory debugging and visualization only.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_visual_studio_image_watch/windows_visual_studio_image_watch.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n// Test application for the Visual Studio Image Watch Debugger extension\n\n#include <iostream>                        // std::cout\n#include <opencv2/core/core.hpp>           // cv::Mat\n#include <opencv2/imgcodecs/imgcodecs.hpp>     // cv::imread()\n#include <opencv2/imgproc/imgproc.hpp>     // cv::Canny()\n\nusing namespace std;\nusing namespace cv;\n\nvoid help()\n{\n    cout\n        << \"----------------------------------------------------\" << endl\n        << \"This is a test program for the Image Watch Debugger \" << endl\n        << \"plug-in for Visual Studio. The program loads an     \" << endl\n        << \"image from a file and runs the Canny edge detector. \" << endl\n        << \"No output is displayed or written to disk.\"\n        << endl\n        << \"Usage:\"                                               << endl\n        << \"image-watch-demo inputimage\"                          << endl\n        << \"----------------------------------------------------\" << endl\n        << endl;\n}\n\nint main(int argc, char *argv[])\n{\n    help();\n\n    if (argc != 2)\n    {\n        cout << \"Wrong number of parameters\" << endl;\n        return -1;\n    }\n\n    cout << \"Loading input image: \" << argv[1] << endl;\n    Mat input;\n    input = imread(argv[1], IMREAD_COLOR);\n\n    cout << \"Detecting edges in input image\" << endl;\n    Mat edges;\n    Canny(input, edges, 10, 100);\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring PNG Support in CMake\nDESCRIPTION: CMake configuration options for enabling PNG support in OpenCV imgcodecs module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/readme.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nWITH_PNG=ON # Enable libpng support\nWITH_SPNG=ON # Enable libspng support\n```\n\n----------------------------------------\n\nTITLE: Estimating Pose and Drawing Axes on Multiple Images with OpenCV (Python)\nDESCRIPTION: Processes a set of chessboard images, locates the chessboard corners, refines them, estimates the object's pose using cv.solvePnP, and projects 3D axis points onto the image plane. It then uses the draw function to overlay axes, displaying each result. Depends on camera calibration data, axis/object points, and previously defined draw function. Inputs are image files matching a pattern, and successful cases may result in saved annotated images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_pose/py_pose.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor fname in glob.glob('left*.jpg'):\\n    img = cv.imread(fname)\\n    gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\n    ret, corners = cv.findChessboardCorners(gray, (7,6),None)\\n\\n    if ret == True:\\n        corners2 = cv.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)\\n\\n        # Find the rotation and translation vectors.\\n        ret,rvecs, tvecs = cv.solvePnP(objp, corners2, mtx, dist)\\n\\n        # project 3D points to image plane\\n        imgpts, jac = cv.projectPoints(axis, rvecs, tvecs, mtx, dist)\\n\\n        img = draw(img,corners2,imgpts)\\n        cv.imshow('img',img)\\n        k = cv.waitKey(0) & 0xFF\\n        if k == ord('s'):\\n            cv.imwrite(fname[:6]+'.png', img)\\n\\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Implementing a Kernel for the OpenCV CPU Backend (OpenCV G-API, C++)\nDESCRIPTION: Illustrates a backend-specific implementation of a declared kernel interface for the CPU (OpenCV) plugin. The run() method adapts the interface to work with real data objects (cv::Mat), providing actual functional behavior. This implementation is packaged as part of a kernel package and is invoked during graph execution. Requires prior kernel interface declaration and OpenCV includes; implementation signature must follow backend conventions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/doc/20-kernel-api.markdown#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// Example kernel implementation for the CPU backend (OpenCV)\nstruct GCPUFilter2D\n{\n    static void run(const cv::Mat &in, const cv::Mat &kernel, cv::Point anchor, cv::Mat &out)\n    {\n        cv::filter2D(in, out, -1, kernel, anchor, 0, cv::BORDER_DEFAULT);\n    }\n};\n// Used when compiling with cv::gapi::kernels<...>()\n```\n\n----------------------------------------\n\nTITLE: Outputting std::vector of Points in OpenCV C++\nDESCRIPTION: Demonstrates how to output a vector of points directly using the << operator. OpenCV provides built-in support for common container combinations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\nvector<Point2f> vPoints(20);\nfor (size_t i = 0; i < vPoints.size(); ++i)\n    vPoints[i] = Point2f((float)(i * 5), (float)(i % 7));\ncout << \"A vector of 2D Points = \" << vPoints << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Configuring Leiningen Profile with Localrepo Plugin\nDESCRIPTION: This Clojure snippet shows how to configure the Leiningen user profile to include the lein-localrepo plugin, allowing installation of any JAR as a Maven artifact locally. The profiles.clj file is updated to ensure the plugin is accessible to any CLJ project. No additional installation steps are needed as the plugin is downloaded automatically when first used.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_1\n\nLANGUAGE: clojure\nCODE:\n```\n{:user {:plugins [[lein-localrepo \"0.5.2\"]]}}\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Decision Tree in C++\nDESCRIPTION: This code snippet shows part of a decision tree implementation for the FAST corner detection algorithm. It compares pixel values at various offsets around a central point to determine if the point represents a corner in an image. The algorithm uses goto statements to branch to either 'is_a_corner' or 'is_not_a_corner' labels based on intensity comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_25\n\nLANGUAGE: C++\nCODE:\n```\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                  else\n                    if(ptr[offset9] < c_b)\n                      if(ptr[offset7] > cb)\n                        if(ptr[offset1] < c_b)\n                          goto is_not_a_corner;\n                        else\n                          if(ptr[offset1] > cb)\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset3] > cb)\n                                if(ptr[offset4] > cb)\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] < c_b)\n                                if(ptr[offset3] > cb)\n                                  if(ptr[offset4] > cb)\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                if(ptr[offset3] > cb)\n                                  if(ptr[offset4] > cb)\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset1] < c_b)\n                          goto is_not_a_corner;\n                        else\n                          if(ptr[offset1] > cb)\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset3] > cb)\n                                if(ptr[offset4] > cb)\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] < c_b)\n                                if(ptr[offset3] > cb)\n                                  if(ptr[offset4] > cb)\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                if(ptr[offset3] > cb)\n                                  if(ptr[offset4] > cb)\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                    else\n                      if(ptr[offset7] > cb)\n                        if(ptr[offset9] > cb)\n                          if(ptr[offset1] < c_b)\n                            if(ptr[offset6] < c_b)\n                              goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset8] > cb)\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            if(ptr[offset1] > cb)\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset10] > cb)\n                                  if(ptr[offset11] > cb)\n                                    if(ptr[offset3] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      if(ptr[offset8] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                if(ptr[offset6] < c_b)\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      if(ptr[offset3] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        if(ptr[offset8] > cb)\n                                          goto is_a_corner;\n                                        else\n                                          goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      if(ptr[offset3] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        if(ptr[offset8] > cb)\n                                          goto is_a_corner;\n                                        else\n                                          goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] < c_b)\n                                goto is_not_a_corner;\n                              else\n                                if(ptr[offset6] > cb)\n                                  if(ptr[offset8] > cb)\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                        else\n                          if(ptr[offset1] < c_b)\n                            goto is_not_a_corner;\n                          else\n                            if(ptr[offset1] > cb)\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset3] > cb)\n```\n\n----------------------------------------\n\nTITLE: ARM Architecture Optimization Configuration\nDESCRIPTION: Configures NEON optimization settings for ARM architectures, including source files and compiler definitions based on the selected optimization level.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libpng/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(TARGET_ARCH MATCHES \"^(ARM|arm|aarch)\")\n  if(TARGET_ARCH MATCHES \"^(ARM64|arm64|aarch64)\")\n    set(PNG_ARM_NEON_POSSIBLE_VALUES on off)\n    set(PNG_ARM_NEON \"on\"\n        CACHE STRING \"Enable ARM NEON optimizations: on|off; on is default\")\n  else()\n    set(PNG_ARM_NEON_POSSIBLE_VALUES check on off)\n    set(PNG_ARM_NEON \"off\"\n        CACHE STRING \"Enable ARM NEON optimizations: check|on|off; off is default\")\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Setting an Environment Variable in Linux Shell Script\nDESCRIPTION: This shell script snippet sets 'MY_ENV_VARIABLE' for the current shell session or process. Useful for applications like OpenCV that check environment variables at runtime. Dependencies: Bash or POSIX-compliant shell. Parameters: Variable name and value. Limitations: Only persists for the current process hierarchy, not globally.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/env_reference/env_reference.markdown#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport MY_ENV_VARIABLE=true\\n./my_app\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV.js on Windows with Docker - Bash/PowerShell\nDESCRIPTION: This command runs a Docker container for the Emscripten SDK on Windows, mapping the current PowerShell location into the container and running the build script using emcmake and Python3. It preserves Windows pathing and permissions conventions. Outputs are OpenCV.js build artifacts. Requires Docker installed and PowerShell as shell.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm --workdir /src -v \"$(get-location):/src\" \"emscripten/emsdk\" emcmake python3 ./platforms/js/build_js.py build_js\n```\n\n----------------------------------------\n\nTITLE: Listing G-API Source Files in CMake\nDESCRIPTION: Defines a CMake variable `gapi_srcs` containing an explicit list of all C++ source files (`.cpp`) required to build the core G-API functionality, including frontend API, compiler, executor, various backends (CPU, Fluid, OAK, OCL, IE, OV, ONNX, Render, PlaidML), common backend code, serialization, streaming, Python bindings, and utility code.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset(gapi_srcs\n    # Front-end part\n    src/api/grunarg.cpp\n    src/api/gorigin.cpp\n    src/api/gmat.cpp\n    src/api/garray.cpp\n    src/api/gopaque.cpp\n    src/api/gscalar.cpp\n    src/api/gframe.cpp\n    src/api/gkernel.cpp\n    src/api/gbackend.cpp\n    src/api/gcommon.cpp\n    src/api/gproto.cpp\n    src/api/gnode.cpp\n    src/api/gcall.cpp\n    src/api/gcomputation.cpp\n    src/api/operators.cpp\n    src/api/kernels_core.cpp\n    src/api/kernels_imgproc.cpp\n    src/api/kernels_video.cpp\n    src/api/kernels_nnparsers.cpp\n    src/api/kernels_ot.cpp\n    src/api/kernels_streaming.cpp\n    src/api/kernels_stereo.cpp\n    src/api/render.cpp\n    src/api/render_ocv.cpp\n    src/api/ginfer.cpp\n    src/api/media.cpp\n    src/api/rmat.cpp\n\n    # Compiler part\n    src/compiler/gmodel.cpp\n    src/compiler/gmodelbuilder.cpp\n    src/compiler/gislandmodel.cpp\n    src/compiler/gcompiler.cpp\n    src/compiler/gcompiled.cpp\n    src/compiler/gstreaming.cpp\n    src/compiler/passes/helpers.cpp\n    src/compiler/passes/dump_dot.cpp\n    src/compiler/passes/islands.cpp\n    src/compiler/passes/meta.cpp\n    src/compiler/passes/kernels.cpp\n    src/compiler/passes/exec.cpp\n    src/compiler/passes/transformations.cpp\n    src/compiler/passes/pattern_matching.cpp\n    src/compiler/passes/perform_substitution.cpp\n    src/compiler/passes/streaming.cpp\n    src/compiler/passes/intrin.cpp\n\n    # Executor\n    src/executor/gabstractexecutor.cpp\n    src/executor/gabstractstreamingexecutor.cpp\n    src/executor/gexecutor.cpp\n    src/executor/gtbbexecutor.cpp\n    src/executor/gthreadedexecutor.cpp\n    src/executor/gstreamingexecutor.cpp\n    src/executor/gasync.cpp\n    src/executor/thread_pool.cpp\n\n    # CPU Backend (currently built-in)\n    src/backends/cpu/gcpubackend.cpp\n    src/backends/cpu/gcpukernel.cpp\n    src/backends/cpu/gcpuimgproc.cpp\n    src/backends/cpu/gcpustereo.cpp\n    src/backends/cpu/gcpuvideo.cpp\n    src/backends/cpu/gcpucore.cpp\n    src/backends/cpu/gcpuot.cpp\n    src/backends/cpu/gnnparsers.cpp\n\n    # Fluid Backend (also built-in, FIXME:move away)\n    src/backends/fluid/gfluidbuffer.cpp\n    src/backends/fluid/gfluidbackend.cpp\n    src/backends/fluid/gfluidimgproc.cpp\n    src/backends/fluid/gfluidimgproc_func.dispatch.cpp\n    src/backends/fluid/gfluidcore.cpp\n    src/backends/fluid/gfluidcore_func.dispatch.cpp\n\n    # OAK Backend (optional)\n    src/backends/oak/goak.cpp\n    src/backends/oak/goakbackend.cpp\n    src/backends/oak/goak_memory_adapters.cpp\n\n    # OCL Backend (currently built-in)\n    src/backends/ocl/goclbackend.cpp\n    src/backends/ocl/goclkernel.cpp\n    src/backends/ocl/goclimgproc.cpp\n    src/backends/ocl/goclcore.cpp\n\n    # IE Backend. FIXME: should be included by CMake\n    # if and only if IE support is enabled\n    src/backends/ie/giebackend.cpp\n    src/backends/ie/giebackend/giewrapper.cpp\n\n    # OV Backend. FIXME: should be included by CMake\n    # if and only if OV support is enabled\n    src/backends/ov/govbackend.cpp\n\n    # ONNX backend\n    src/backends/onnx/gonnxbackend.cpp\n    src/backends/onnx/dml_ep.cpp\n    src/backends/onnx/coreml_ep.cpp\n\n    # Render backend\n    src/backends/render/grenderocv.cpp\n    src/backends/render/ft_render.cpp\n\n    # PlaidML Backend\n    src/backends/plaidml/gplaidmlcore.cpp\n    src/backends/plaidml/gplaidmlbackend.cpp\n\n    # Common backend code\n    src/backends/common/gmetabackend.cpp\n    src/backends/common/gcompoundbackend.cpp\n    src/backends/common/gcompoundkernel.cpp\n\n    # Serialization API and routines\n    src/api/s11n.cpp\n    src/backends/common/serialization.cpp\n\n    # Streaming backend\n    src/backends/streaming/gstreamingbackend.cpp\n\n    # Python bridge\n    src/backends/ie/bindings_ie.cpp\n    src/backends/onnx/bindings_onnx.cpp\n    src/backends/ov/bindings_ov.cpp\n    src/backends/python/gpythonbackend.cpp\n\n    # Queue Streaming source\n    src/streaming/queue_source.cpp\n\n    # OpenVPL Streaming source\n    src/streaming/onevpl/source.cpp\n    src/streaming/onevpl/source_priv.cpp\n    src/streaming/onevpl/file_data_provider.cpp\n    src/streaming/onevpl/cfg_params.cpp\n    src/streaming/onevpl/cfg_params_parser.cpp\n    src/streaming/onevpl/utils.cpp\n    src/streaming/onevpl/default.cpp\n    src/streaming/onevpl/data_provider_interface_exception.cpp\n    src/streaming/onevpl/accelerators/surface/base_frame_adapter.cpp\n    src/streaming/onevpl/accelerators/surface/cpu_frame_adapter.cpp\n    src/streaming/onevpl/accelerators/surface/dx11_frame_adapter.cpp\n    src/streaming/onevpl/accelerators/surface/surface.cpp\n    src/streaming/onevpl/accelerators/surface/surface_pool.cpp\n    src/streaming/onevpl/accelerators/utils/shared_lock.cpp\n    src/streaming/onevpl/accelerators/accel_policy_cpu.cpp\n    src/streaming/onevpl/accelerators/accel_policy_dx11.cpp\n    src/streaming/onevpl/accelerators/accel_policy_va_api.cpp\n    src/streaming/onevpl/accelerators/dx11_alloc_resource.cpp\n    src/streaming/onevpl/engine/engine_session.cpp\n    src/streaming/onevpl/engine/processing_engine_base.cpp\n    src/streaming/onevpl/engine/decode/decode_engine_legacy.cpp\n    src/streaming/onevpl/engine/decode/decode_session.cpp\n    src/streaming/onevpl/engine/transcode/transcode_engine_legacy.cpp\n    src/streaming/onevpl/engine/transcode/transcode_session.cpp\n    src/streaming/onevpl/engine/preproc/preproc_engine.cpp\n    src/streaming/onevpl/engine/preproc/preproc_session.cpp\n    src/streaming/onevpl/engine/preproc/preproc_dispatcher.cpp\n    src/streaming/onevpl/engine/preproc_engine_interface.cpp\n    src/streaming/onevpl/demux/async_mfp_demux_data_provider.cpp\n    src/streaming/onevpl/data_provider_dispatcher.cpp\n\n    src/streaming/onevpl/cfg_param_device_selector.cpp\n    src/streaming/onevpl/device_selector_interface.cpp\n\n    # GStreamer Streaming source\n    src/streaming/gstreamer/gstreamer_pipeline_facade.cpp\n    src/streaming/gstreamer/gstreamerpipeline.cpp\n    src/streaming/gstreamer/gstreamersource.cpp\n    src/streaming/gstreamer/gstreamer_buffer_utils.cpp\n    src/streaming/gstreamer/gstreamer_media_adapter.cpp\n    src/streaming/gstreamer/gstreamerenv.cpp\n\n    # Utils (ITT tracing)\n    src/utils/itt.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV Installation Script in Git Bash\nDESCRIPTION: Command to execute the OpenCV installation script in Git Bash. This starts the build process for OpenCV and OpenCV contrib modules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_install/windows_install.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./installOCV.sh\n```\n\n----------------------------------------\n\nTITLE: Setting JPEG Compression Quality (C)\nDESCRIPTION: This function constructs JPEG quantization tables for the specified quality setting (0-100 scale). The force_baseline parameter constrains table entries to 1-255 for full JPEG baseline compatibility.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_23\n\nLANGUAGE: C\nCODE:\n```\njpeg_set_quality (j_compress_ptr cinfo, int quality, boolean force_baseline)\n```\n\n----------------------------------------\n\nTITLE: SIFT Feature Matching with Ratio Test in Python\nDESCRIPTION: Implements feature matching using SIFT descriptors and Brute-Force matcher with k-nearest neighbors. Applies Lowe's ratio test for better match filtering.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nimport matplotlib.pyplot as plt\n\nimg1 = cv.imread('box.png',cv.IMREAD_GRAYSCALE)          # queryImage\nimg2 = cv.imread('box_in_scene.png',cv.IMREAD_GRAYSCALE) # trainImage\n\n# Initiate SIFT detector\nsift = cv.SIFT_create()\n\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute(img1,None)\nkp2, des2 = sift.detectAndCompute(img2,None)\n\n# BFMatcher with default params\nbf = cv.BFMatcher()\nmatches = bf.knnMatch(des1,des2,k=2)\n\n# Apply ratio test\ngood = []\nfor m,n in matches:\n    if m.distance < 0.75*n.distance:\n        good.append([m])\n\n# cv.drawMatchesKnn expects list of lists as matches.\nimg3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n\nplt.imshow(img3),plt.show()\n```\n\n----------------------------------------\n\nTITLE: Creating and Linking the libtiff Library with CMake\nDESCRIPTION: This snippet defines the creation of the libtiff static library with its source files and links it against the required libraries. It configures library properties such as output name and debugging settings, ensuring proper linkage and debugging symbols are available for development.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libtiff/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${TIFF_LIBRARY} STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${lib_srcs})\ntarget_link_libraries(${TIFF_LIBRARY} ${ZLIB_LIBRARIES})\n\nset_target_properties(${TIFF_LIBRARY}\n    PROPERTIES\n    OUTPUT_NAME \"${TIFF_LIBRARY}\"\n    DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n    COMPILE_PDB_NAME ${TIFF_LIBRARY}\n    COMPILE_PDB_NAME_DEBUG \"${TIFF_LIBRARY}${OPENCV_DEBUG_POSTFIX}\"\n    ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring libjpeg Library Build with CMake - CMake\nDESCRIPTION: This snippet sets up the libjpeg static library build in the OpenCV project using CMake. It includes directives to gather all source and header files, apply source filtering based on the platform, configure compilation flags, disable selected warnings, define target properties, and handle installation. Dependencies include OpenCV-specific macros and variables, and a working CMake environment. Inputs are controlled by the OpenCV and CMake build configuration (such as target platform and enabled folders), with outputs being the built static library and its debug symbols. The setup requires that OpenCV and CMake variables (e.g., JPEG_LIBRARY, CMAKE_CURRENT_SOURCE_DIR) are predefined and expects standard CMake toolchain support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# ----------------------------------------------------------------------------\\n#  CMake file for libjpeg. See root CMakeLists.txt\\n#\\n# ----------------------------------------------------------------------------\\nproject(${JPEG_LIBRARY})\\nocv_include_directories(${CMAKE_CURRENT_SOURCE_DIR})\\nfile(GLOB lib_srcs *.c)\\nfile(GLOB lib_hdrs *.h)\\nif(ANDROID OR IOS OR APPLE)\\n  ocv_list_filterout(lib_srcs jmemansi.c)\\nelse()\\n  ocv_list_filterout(lib_srcs jmemnobs.c)\\nendif()\\n# ----------------------------------------------------------------------------------\\n#         Define the library target:\\n# ----------------------------------------------------------------------------------\\nadd_library(${JPEG_LIBRARY} STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${lib_srcs} ${lib_hdrs})\\nif(CV_GCC OR CV_CLANG)\\n  set_source_files_properties(jcdctmgr.c PROPERTIES COMPILE_FLAGS \"-O1\")\\nendif()\\nocv_warnings_disable(CMAKE_C_FLAGS -Wcast-align -Wshadow -Wunused -Wshift-negative-value -Wimplicit-fallthrough)\\nocv_warnings_disable(CMAKE_C_FLAGS -Wunused-parameter) # clang\\nocv_warnings_disable(CMAKE_C_FLAGS /wd4013 /wd4244 /wd4267) # vs2005\\nset_target_properties(${JPEG_LIBRARY}\\n  PROPERTIES OUTPUT_NAME ${JPEG_LIBRARY}\\n  DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\\n  COMPILE_PDB_NAME ${JPEG_LIBRARY}\\n  COMPILE_PDB_NAME_DEBUG \"${JPEG_LIBRARY}${OPENCV_DEBUG_POSTFIX}\"\\n  ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\\n  )\\nif(ENABLE_SOLUTION_FOLDERS)\\n  set_target_properties(${JPEG_LIBRARY} PROPERTIES FOLDER \"3rdparty\")\\nendif()\\nif(NOT BUILD_SHARED_LIBS)\\n  ocv_install_target(${JPEG_LIBRARY} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev OPTIONAL)\\nendif()\\nocv_install_3rdparty_licenses(libjpeg README)\n```\n\n----------------------------------------\n\nTITLE: Finding All C++ Sample Files in CMake\nDESCRIPTION: Uses the `file(GLOB ...)` command to find all files ending with `.cpp` in the current source directory (`CMAKE_CURRENT_SOURCE_DIR`). It stores the list of relative file paths in the `all_samples` variable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB all_samples RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} *.cpp)\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Algorithm in C++\nDESCRIPTION: This code snippet is part of the FAST corner detection algorithm. It compares pixel values at various offsets to determine if a pixel is a corner. The algorithm uses a series of nested conditional statements and goto labels for efficient branching.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_20\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  goto homogeneous;\nelse\n  if(ptr[offset8] < c_b)\n    if(ptr[offset10] < c_b)\n      goto success_homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  if(ptr[offset6] < c_b)\n    if(ptr[offset3] < c_b)\n      if(ptr[offset4] < c_b)\n        goto success_homogeneous;\n      else\n        goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  if(ptr[offset10] < c_b)\n    if(ptr[offset11] < c_b)\n      if(ptr[offset7] < c_b)\n        if(ptr[offset1] < c_b)\n          if(ptr[offset3] < c_b)\n            goto success_homogeneous;\n          else\n            if(ptr[offset8] < c_b)\n              goto success_homogeneous;\n            else\n              goto homogeneous;\n        else\n          if(ptr[offset6] < c_b)\n            if(ptr[offset8] < c_b)\n              goto success_homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n      else\n        if(ptr[offset1] < c_b)\n          if(ptr[offset3] < c_b)\n            goto success_homogeneous;\n          else\n            if(ptr[offset8] < c_b)\n              goto success_homogeneous;\n            else\n              goto homogeneous;\n        else\n          goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  if(ptr[offset9] > cb)\n    if(ptr[offset5] > cb)\n      if(ptr[offset7] > cb)\n        if(ptr[offset1] < c_b)\n          if(ptr[offset4] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto success_structured;\n                else\n                  goto structured;\n              else\n                goto homogeneous;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset8] > cb)\n                  if(ptr[offset11] > cb)\n                    if(ptr[offset10] > cb)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset4] > cb)\n                    goto success_structured;\n                  else\n                    if(ptr[offset11] > cb)\n                      goto success_structured;\n                    else\n                      goto structured;\n                else\n                  if(ptr[offset3] > cb)\n                    if(ptr[offset4] > cb)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n        else\n          if(ptr[offset6] > cb)\n            if(ptr[offset8] > cb)\n              if(ptr[offset4] > cb)\n                if(ptr[offset3] > cb)\n                  goto success_structured;\n                else\n                  if(ptr[offset10] > cb)\n                    goto success_structured;\n                  else\n                    goto homogeneous;\n              else\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    goto success_structured;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n      else\n        if(ptr[offset1] < c_b)\n          if(ptr[offset3] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto success_structured;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n        else\n          goto homogeneous;\n    else\n      if(ptr[offset3] < c_b)\n        if(ptr[offset4] < c_b)\n          if(ptr[offset5] < c_b)\n            if(ptr[offset7] < c_b)\n              if(ptr[offset1] < c_b)\n                if(ptr[offset6] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset11] < c_b)\n                    goto success_structured;\n                  else\n                    goto homogeneous;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset8] < c_b)\n                    goto success_structured;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n            else\n              if(ptr[offset1] < c_b)\n                if(ptr[offset6] < c_b)\n                  goto success_homogeneous;\n                else\n                  if(ptr[offset11] < c_b)\n                    goto success_homogeneous;\n                  else\n                    goto homogeneous;\n              else\n                goto homogeneous;\n          else\n            if(ptr[offset1] < c_b)\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n        else\n          goto homogeneous;\n      else\n        goto homogeneous;\n  else\n    if(ptr[offset3] < c_b)\n      if(ptr[offset4] < c_b)\n        if(ptr[offset5] < c_b)\n```\n\n----------------------------------------\n\nTITLE: Verifying Pkg-config for ARMv7 (Bash)\nDESCRIPTION: Tests the `pkg-config` setup for the `armhf` (ARMv7) architecture. Similar to the aarch64 check, it sets environment variables to direct `pkg-config` to the appropriate paths for `armhf` libraries and lists all available packages.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nPKG_CONFIG_PATH=/usr/lib/arm-linux-gnueabihf/pkgconfig:/usr/share/pkgconfig \\\n  PKG_CONFIG_LIBDIR=/usr/lib/arm-linux-gnueabihf \\\n  PKG_CONFIG_SYSROOT_DIR=/ \\\n      pkg-config --list-all\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Android Camera Calibration Example\nDESCRIPTION: CMake configuration that sets up an Android camera calibration example project. It defines the project name, adds Android-specific build settings, sets OpenCV library dependencies, and specifies minimum SDK target of 11.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/camera-calibration/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(sample example-camera-calibration)\n\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\" LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\" SDK_TARGET 11 \"${ANDROID_SDK_TARGET}\")\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Processing Camera Frames with Pure OpenCL\nDESCRIPTION: C++ code showing how to process camera frames using direct OpenCL calls. This approach uses cl::ImageGL to wrap OpenGL textures and process them without copying data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n// process_pure_opencl\n```\n\n----------------------------------------\n\nTITLE: Starting JPEG Compression Cycle in C\nDESCRIPTION: This snippet demonstrates how to begin a JPEG compression cycle using 'jpeg_start_compress'. This function initializes internal state, allocates working storage, and starts the JPEG datastream header. Passing 'TRUE' ensures a complete JPEG interchange datastream is produced.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_6\n\nLANGUAGE: C\nCODE:\n```\njpeg_start_compress(&cinfo, TRUE);\n```\n\n----------------------------------------\n\nTITLE: Creating Display Window\nDESCRIPTION: Creates a window to display the remapping results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/remap/remap.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nnamedWindow( \"Remap demo\", WINDOW_AUTOSIZE );\n```\n\nLANGUAGE: Java\nCODE:\n```\nHighGui.namedWindow(\"Remap demo\", HighGui.WINDOW_AUTOSIZE);\n```\n\nLANGUAGE: Python\nCODE:\n```\ncv.namedWindow(\"Remap demo\", cv.WINDOW_AUTOSIZE)\n```\n\n----------------------------------------\n\nTITLE: Adding Actions Runner Service\nDESCRIPTION: Commands to copy and reload the actions-runner service configuration in systemd.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/arch/s390/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo cp self-hosted-builder/actions-runner.service /etc/systemd/system/\nsudo systemctl daemon-reload\n```\n\n----------------------------------------\n\nTITLE: Process Contours and Draw Shapes in OpenCV Java\nDESCRIPTION: Applies polygonal approximation for contours and draws bounding rectangles and circles using OpenCV in Java.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\nfor (int i = 0; i < contours.size(); i++) {\n    MatOfPoint2f contour2f = new MatOfPoint2f(contours.get(i).toArray());\n    MatOfPoint2f approxCurve = new MatOfPoint2f();\n    Imgproc.approxPolyDP(contour2f, approxCurve, 3, true);\n    Rect rect = Imgproc.boundingRect(new MatOfPoint(approxCurve.toArray()));\n    Point center = new Point();\n    float[] radius = new float[1];\n    Imgproc.minEnclosingCircle(approxCurve, center, radius);\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing cv::Mat Objects with Specific Types and Sizes (C++)\nDESCRIPTION: Provides C++ examples for creating OpenCV `cv::Mat` objects. It demonstrates initializing matrices with specified dimensions (rows, columns, or `cv::Size`) and data types using the predefined constants (e.g., `CV_32F`, `CV_64FC2`, `CV_8UC3`). It also shows how to create a single-channel matrix (`grayscale`) with the same size and element depth as an existing multi-channel image (`img`) using `img.size()` and `CV_MAKETYPE`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/doc/intro.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n    Mat mtx(3, 3, CV_32F); // make a 3x3 floating-point matrix\n    Mat cmtx(10, 1, CV_64FC2); // make a 10x1 2-channel floating-point\n                               // matrix (10-element complex vector)\n    Mat img(Size(1920, 1080), CV_8UC3); // make a 3-channel (color) image\n                                        // of 1920 columns and 1080 rows.\n    Mat grayscale(img.size(), CV_MAKETYPE(img.depth(), 1)); // make a 1-channel image of\n                                                            // the same size and same\n                                                            // channel type as img\n```\n\n----------------------------------------\n\nTITLE: Generating Configuration Header for OpenCV Data Directories in CMake\nDESCRIPTION: This CMake script generates a configuration header file for OpenCV data directories. It checks the provided CMake variables such as CMAKE_INSTALL_PREFIX and OPENCV_OTHER_INSTALL_PATH to create the necessary directories as define statements. The script further handles platform-specific path adjustments and avoids unnecessary file writes by comparing existing content.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# generate data (samples data) config file\nset(OPENCV_DATA_CONFIG_FILE \"${OPENCV_CONFIG_FILE_INCLUDE_DIR}/opencv_data_config.hpp\")\nset(OPENCV_DATA_CONFIG_STR \"\")\n\nif(CMAKE_INSTALL_PREFIX)\n  set(OPENCV_DATA_CONFIG_STR \"${OPENCV_DATA_CONFIG_STR}\\n#define OPENCV_INSTALL_PREFIX \\\"${CMAKE_INSTALL_PREFIX}\\\"\\n\")\nendif()\nif(OPENCV_OTHER_INSTALL_PATH)\n  set(OPENCV_DATA_CONFIG_STR \"${OPENCV_DATA_CONFIG_STR}\\n#define OPENCV_DATA_INSTALL_PATH \\\"${OPENCV_OTHER_INSTALL_PATH}\\\"\\n\")\nendif()\n\nset(OPENCV_DATA_CONFIG_STR \"${OPENCV_DATA_CONFIG_STR}\\n#define OPENCV_BUILD_DIR \\\"${CMAKE_BINARY_DIR}\\\"\\n\")\n\nfile(RELATIVE_PATH SOURCE_DIR_RELATIVE ${CMAKE_BINARY_DIR} ${CMAKE_SOURCE_DIR})\nset(OPENCV_DATA_CONFIG_STR \"${OPENCV_DATA_CONFIG_STR}\\n#define OPENCV_DATA_BUILD_DIR_SEARCH_PATHS \\\\\\\"${SOURCE_DIR_RELATIVE}/\\\"\\n\")\n\nif(WIN32)\n  file(RELATIVE_PATH INSTALL_DATA_DIR_RELATIVE \"${CMAKE_INSTALL_PREFIX}/${OPENCV_BIN_INSTALL_PATH}\" \"${CMAKE_INSTALL_PREFIX}/${OPENCV_OTHER_INSTALL_PATH}\")\nelse()\n  file(RELATIVE_PATH INSTALL_DATA_DIR_RELATIVE \"${CMAKE_INSTALL_PREFIX}/${OPENCV_LIB_INSTALL_PATH}\" \"${CMAKE_INSTALL_PREFIX}/${OPENCV_OTHER_INSTALL_PATH}\")\nendif()\nlist(APPEND OPENCV_INSTALL_DATA_DIR_RELATIVE \"${INSTALL_DATA_DIR_RELATIVE}\")\nstring(REPLACE \";\" \",\\\\n    \\\"\" OPENCV_INSTALL_DATA_DIR_RELATIVE_STR \"\\\"${OPENCV_INSTALL_DATA_DIR_RELATIVE}\\\"\")\nset(OPENCV_DATA_CONFIG_STR \"${OPENCV_DATA_CONFIG_STR}\\n#define OPENCV_INSTALL_DATA_DIR_RELATIVE ${OPENCV_INSTALL_DATA_DIR_RELATIVE_STR}\\n\")\n\nif(EXISTS \"${OPENCV_DATA_CONFIG_FILE}\")\n  file(READ \"${OPENCV_DATA_CONFIG_FILE}\" __content)\nendif()\nif(NOT OPENCV_DATA_CONFIG_STR STREQUAL \"${__content}\")\n  file(WRITE \"${OPENCV_DATA_CONFIG_FILE}\" \"${OPENCV_DATA_CONFIG_STR}\")\nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Highgui Plugins Configuration Options in OpenCV\nDESCRIPTION: Defines options available since OpenCV 4.5.3 to control building GTK backend as a dynamically loaded plugin for the highgui module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_19\n\nLANGUAGE: markdown\nCODE:\n```\n| Option | Default | Description |\n| --------| ------ | ------- |\n| `HIGHGUI_ENABLE_PLUGINS` | _ON_ | Enable or disable plugins completely. |\n| `HIGHGUI_PLUGIN_LIST` | _empty_ | Comma- or semicolon-separated list of backend names to be compiled as plugins. Supported names are _gtk_, _gtk2_, _gtk3_, and _all_. |\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic x86 Features for ZLIB in CMake\nDESCRIPTION: Sets up basic configurations for the x86 architecture (BASEARCH_X86_FOUND). It adds the DX86_FEATURES definition and appends common x86 header and source files (for runtime CPU detection if enabled). It also includes a fallback header for MSVC compilers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_24\n\nLANGUAGE: cmake\nCODE:\n```\n    elseif(BASEARCH_X86_FOUND)\n        add_definitions(-DX86_FEATURES)\n        list(APPEND ZLIB_ARCH_HDRS ${ARCHDIR}/x86_functions.h)\n        if(WITH_RUNTIME_CPU_DETECTION)\n            list(APPEND ZLIB_ARCH_HDRS ${ARCHDIR}/x86_features.h)\n            list(APPEND ZLIB_ARCH_SRCS ${ARCHDIR}/x86_features.c)\n        endif()\n        if(MSVC)\n            list(APPEND ZLIB_ARCH_HDRS fallback_builtins.h)\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Backprojecting 3D Points to 2D in PnPProblem Using C++\nDESCRIPTION: Contains the backproject3DPoint function of the PnPProblem class, projecting 3D world coordinates to 2D image points using the estimated rotation and translation. It outputs a 2D point in the image frame.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\n// Backproject a 3D point to 2D using the estimated pose parameters\n\ncv::Point2f PnPProblem::backproject3DPoint(const cv::Point3f &point3d)\n{\n    // 3D point vector [x y z 1]'\n    cv::Mat point3d_vec = cv::Mat(4, 1, CV_64FC1);\n    point3d_vec.at<double>(0) = point3d.x;\n    point3d_vec.at<double>(1) = point3d.y;\n    point3d_vec.at<double>(2) = point3d.z;\n    point3d_vec.at<double>(3) = 1;\n\n    // 2D point vector [u v 1]'\n    cv::Mat point2d_vec = cv::Mat(4, 1, CV_64FC1);\n    point2d_vec = _A_matrix * _P_matrix * point3d_vec;\n\n    // Normalization of [u v]'\n    cv::Point2f point2d;\n    point2d.x = point2d_vec.at<double>(0) / point2d_vec.at<double>(2);\n    point2d.y = point2d_vec.at<double>(1) / point2d_vec.at<double>(2);\n\n    return point2d;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ZLIB-NG Support in CMake\nDESCRIPTION: CMake configuration option for using zlib-ng as the zlib implementation in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/readme.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nWITH_ZLIB_NG=ON # Use zlib-ng implementation\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV.js Loader with Loader Option - Bash\nDESCRIPTION: Builds OpenCV.js along with its optional loader by adding the --build_loader flag. The loader enables WebAssembly feature detection and automated selection of the appropriate OpenCV.js build at runtime. Requires emcmake, Python, and the optional dependency wasm-feature-detect for web usage. Outputs loader.js for web applications.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nemcmake python ./opencv/platforms/js/build_js.py build_js --build_loader\n```\n\n----------------------------------------\n\nTITLE: Creating a New Clojure Project with Leiningen Bash\nDESCRIPTION: This Bash command sequence illustrates creating a new Clojure project using Leiningen, with added dependencies for OpenCV. The project.clj file is modified to include OpenCV JARs. Expected output is a new project directory structure with specified dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nlein new simple-sample\\ncd simple-sample\n```\n\n----------------------------------------\n\nTITLE: Trackbar Callback in OpenCV Python\nDESCRIPTION: Sets a trackbar on an OpenCV window and binds a callback function in Python, reacting to trackbar updates. Requires OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ncv2.createTrackbar(\"Trackbar\", \"Source\", sliderValue, maxValue, on_trackbar)\n```\n\n----------------------------------------\n\nTITLE: Specifying JPEG Color Space and Components in libjpeg (C)\nDESCRIPTION: These C fields within the compression parameters structure (`cinfo`) define the color space used within the JPEG file (`jpeg_color_space`, type `J_COLOR_SPACE`) and the corresponding number of color components (`num_components`, type `int`). It is generally recommended to use the `jpeg_set_color_space()` function to set these values rather than modifying them directly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_33\n\nLANGUAGE: C\nCODE:\n```\nJ_COLOR_SPACE jpeg_color_space\n```\n\nLANGUAGE: C\nCODE:\n```\nint num_components\n```\n\n----------------------------------------\n\nTITLE: Configuring Module Builds in OpenCV\nDESCRIPTION: Shows how to enable or disable specific OpenCV modules using `cmake` options. This includes disabling a module or building specified modules with automatic dependency inclusion.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DBUILD_opencv_calib3d=OFF ../opencv\n```\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DBUILD_LIST=calib3d,videoio,ts ../opencv\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV.js with Docker Using Emscripten Version 2.0.10 - Bash\nDESCRIPTION: This command runs the OpenCV.js build process inside a Docker container using the emscripten/emsdk:2.0.10 image to ensure compatibility. It mounts the current directory and sets correct user IDs for file permission matching. Recommended if the latest version of emscripten fails to build successfully. Outputs are OpenCV.js build files generated by Python3 and emcmake.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -v $(pwd):/src -u $(id -u):$(id -g) emscripten/emsdk:2.0.10 emcmake python3 ./platforms/js/build_js.py build_js\n```\n\n----------------------------------------\n\nTITLE: Alternative OpenNI SDK Environment Initialization Script (>= v2.3.0.86)\nDESCRIPTION: A shell script provided as an alternative to `install.sh` for Orbbec OpenNI SDK versions 2.3.0.86 and newer. It checks for root privileges, determines the script's path, installs udev rules for the USB device (if not on Darwin/macOS), and creates an 'OpenNIDevEnvironment' file that exports the OPENNI2_INCLUDE and OPENNI2_REDIST environment variables pointing to the SDK location. This script needs to be run with sudo.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Check if user is root/running with sudo\nif [ `whoami` != root ]; then\n    echo Please run this script with sudo\n    exit\nfi\n\nORIG_PATH=`pwd`\ncd `dirname $0`\nSCRIPT_PATH=`pwd`\ncd $ORIG_PATH\n\nif [ \"`uname -s`\" != \"Darwin\" ]; then\n    # Install UDEV rules for USB device\n    cp ${SCRIPT_PATH}/orbbec-usb.rules /etc/udev/rules.d/558-orbbec-usb.rules\n    echo \"usb rules file install at /etc/udev/rules.d/558-orbbec-usb.rules\"\nfi\n\nOUT_FILE=\"$SCRIPT_PATH/OpenNIDevEnvironment\"\necho \"export OPENNI2_INCLUDE=$SCRIPT_PATH/../sdk/Include\" > $OUT_FILE\necho \"export OPENNI2_REDIST=$SCRIPT_PATH/../sdk/libs\" >> $OUT_FILE\nchmod a+r $OUT_FILE\necho \"exit\"\n```\n\n----------------------------------------\n\nTITLE: Setting Image Parameters for JPEG Compression with libjpeg in C\nDESCRIPTION: This snippet illustrates how to configure the key image parameters needed before compressing an image with the libjpeg library in C. It sets the image width, height, number of input components (color channels), and is intended for use with a 24-bit RGB image. The example assumes that variables Width and Height are already defined. It is necessary to assign these values to the jpeg_compress_struct before starting the compression process. This configuration determines how the JPEG library interprets the source image data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_4\n\nLANGUAGE: C\nCODE:\n```\ncinfo.image_width = Width;      /* image width and height, in pixels */\ncinfo.image_height = Height;\ncinfo.input_components = 3;     /* # of color components per pixel */\n```\n\n----------------------------------------\n\nTITLE: Storing SIMD Register Data into Memory using OpenCV Intrinsics (C++)\nDESCRIPTION: This snippet illustrates how to store the contents of a SIMD register into a memory buffer using OpenCV intrinsics. The v_store function writes the register data (e.g., the first 128 bits / 4 floats) into the destination pointer. The type of 'ptr' must match the register's element type to avoid misinterpretation of stored values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/univ_intrin/univ_intrin.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nfloat ptr[4];\nv_store(ptr, reg); // store the first 128 bits(interpreted as 4x32-bit floats) of reg into ptr.\n```\n\n----------------------------------------\n\nTITLE: Reading Input Image\nDESCRIPTION: Shows how to read the input image for ChArUco detection\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_detection/charuco_detection.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\ncv::Mat image;\nimage = cv::imread(parser.get<cv::String>(\"image\"));\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV with MSBuild (Windows) (Batch)\nDESCRIPTION: This Batch command uses MSBuild to compile the OpenCV library based on the Visual Studio solution generated by CMake. It specifies the solution file ('OpenCV.sln'), the build target ('Build'), the configuration ('Release'), and verbosity ('m' for minimal). The '/m' flag enables parallel builds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_5\n\nLANGUAGE: bat\nCODE:\n```\nmsbuild /m OpenCV.sln /t:Build /p:Configuration=Release /v:m\n```\n\n----------------------------------------\n\nTITLE: Image Stitching Using OpenCV in Python\nDESCRIPTION: This Python snippet executes image stitching using a homography matrix calculated from two rotating images via OpenCV. Dependencies are OpenCV and NumPy. Input expects image pairs and outputs a panoramic image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\nimport numpy as np\n\ndef stitch_images(image1, image2, homography):\n    # Code to stitch images\n    # ...\n\n```\n\n----------------------------------------\n\nTITLE: Installing Ninja Build System on Linux\nDESCRIPTION: Command to install Ninja build system on Linux using the package manager.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install ninja-build\n```\n\n----------------------------------------\n\nTITLE: Sequential Convolution Implementation using OpenCV\nDESCRIPTION: This snippet demonstrates a sequential implementation of image convolution using OpenCV. It involves iterating over the image pixels and applying the kernel to compute the convolution. The implementation handles edge cases by adding borders to the source image prior to computation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_new/how_to_use_OpenCV_parallel_for_new.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_use_OpenCV_parallel_for_new.cpp convolution-sequential\n```\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_use_OpenCV_parallel_for_new.cpp convolution-make-borders\n```\n\nLANGUAGE: C++\nCODE:\n```\n@snippet how_to_use_OpenCV_parallel_for_new.cpp convolution-kernel-loop\n```\n\n----------------------------------------\n\nTITLE: Configuring and Populating the OpenCV World Project (CMake)\nDESCRIPTION: This block executes after the initial CMake pass (`NOT OPENCV_INITIAL_PASS`). It disables precompiled headers for the world build, declares the `opencv_world` project, and applies a workaround for a linker issue in MSVC 2015 (VC14) by setting the `/INCREMENTAL:NO` flag. It then iterates through the modules listed in `OPENCV_MODULES_BUILD`, checks if each module is part of the world (`OPENCV_MODULE_${m}_IS_PART_OF_WORLD`), and includes its build configuration using the `include_one_module` function. Status messages indicate the progress. Finally, it sets the `CMAKE_CURRENT_SOURCE_DIR` for the `opencv_world` module itself.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/world/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT OPENCV_INITIAL_PASS)\n  set(ENABLE_PRECOMPILED_HEADERS OFF CACHE INTERNAL \"\" FORCE)\n  project(opencv_world)\n\n  # MSVS 2014 (vc14): LINK : fatal error LNK1210: exceeded internal ILK size limit; link with /INCREMENTAL:NO\n  if(MSVC AND MSVC_VERSION EQUAL 1900)\n    foreach(flag_var\n            CMAKE_EXE_LINKER_FLAGS_DEBUG\n            CMAKE_EXE_LINKER_FLAGS_RELWITHDEBINFO\n            CMAKE_MODULE_LINKER_FLAGS_DEBUG\n            CMAKE_MODULE_LINKER_FLAGS_RELWITHDEBINFO\n            CMAKE_SHARED_LINKER_FLAGS_DEBUG\n            CMAKE_SHARED_LINKER_FLAGS_RELWITHDEBINFO\n    )\n      if(${flag_var} MATCHES \"/INCREMENTAL\")\n        string(REGEX REPLACE \"/INCREMENTAL[^ ]*\" \"/INCREMENTAL:NO\" ${flag_var} \"${${flag_var}}\")\n      else()\n        set(${flag_var} \"${${flag_var}} /INCREMENTAL:NO*\")\n      endif()\n    endforeach(flag_var)\n  endif()\n\n  message(STATUS \"Processing WORLD modules...\")\n  foreach(m ${OPENCV_MODULES_BUILD})\n    set(the_module ${m})\n    if(OPENCV_MODULE_${m}_IS_PART_OF_WORLD)\n      message(STATUS \"    module ${m}...\")\n      set(CMAKE_CURRENT_SOURCE_DIR \"${OPENCV_MODULE_${m}_LOCATION}\")\n      #add_subdirectory(\"${OPENCV_MODULE_${m}_LOCATION}\" ${CMAKE_CURRENT_BINARY_DIR}/${m})\n      include_one_module(${m})\n    endif()\n  endforeach()\n  message(STATUS \"Processing WORLD modules... DONE\")\n  set(CMAKE_CURRENT_SOURCE_DIR \"${OPENCV_MODULE_opencv_world_LOCATION}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing Pixel Comparison Logic for Feature Detection in C++\nDESCRIPTION: This snippet contains a highly nested conditional structure that compares pixel values at different offsets. It's used to detect specific patterns or features in an image, likely part of a corner or edge detection algorithm in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_37\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset4] < c_b)\n  if(ptr[offset5] > cb)\n    if(ptr[offset12] > cb)\n      if(ptr[offset7] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset9] > cb)\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                if(ptr[offset13] > cb)\n                  if(ptr[offset6] > cb)\n                    {} // goto success_homogeneous;\n                  else\n                    if(ptr[offset14] > cb)\n                      if(ptr[offset15] > cb)\n                        {} // goto success_homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n    if(ptr[offset12] < c_b)\n      if(ptr[offset13] < c_b)\n        if(ptr[offset14] < c_b)\n          if(ptr[offset15] < c_b)\n            if(ptr[offset1] < c_b)\n              if(ptr[offset3] < c_b)\n                {} // goto success_homogeneous;\n              else\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n            else\n              if(ptr[offset8] < c_b)\n                if(ptr[offset9] < c_b)\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset7] < c_b)\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset9] < c_b)\n                    if(ptr[offset10] < c_b)\n                      if(ptr[offset11] < c_b)\n                        {} // goto success_homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n  if(ptr[offset5] < c_b)\n    if(ptr[offset7] > cb)\n      if(ptr[offset14] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset9] > cb)\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                if(ptr[offset12] > cb)\n                  if(ptr[offset13] > cb)\n                    if(ptr[offset6] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      if(ptr[offset15] > cb)\n                        {} // goto success_homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n      if(ptr[offset14] < c_b)\n        if(ptr[offset15] < c_b)\n          if(ptr[offset1] < c_b)\n            if(ptr[offset3] < c_b)\n              if(ptr[offset6] < c_b)\n                {} // goto success_homogeneous;\n              else\n                if(ptr[offset13] < c_b)\n                  {} // goto success_homogeneous;\n                else\n                  continue; // goto homogeneous;\n            else\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  if(ptr[offset12] < c_b)\n                    if(ptr[offset13] < c_b)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset8] < c_b)\n              if(ptr[offset9] < c_b)\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    if(ptr[offset12] < c_b)\n                      if(ptr[offset13] < c_b)\n                        {} // goto success_homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n    if(ptr[offset7] < c_b)\n      if(ptr[offset3] < c_b)\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] < c_b)\n            if(ptr[offset8] < c_b)\n              {} // goto success_homogeneous;\n            else\n              if(ptr[offset15] < c_b)\n                {} // goto success_homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset13] < c_b)\n              if(ptr[offset14] < c_b)\n                if(ptr[offset15] < c_b)\n```\n\n----------------------------------------\n\nTITLE: Documenting Functions in C++ with Doxygen\nDESCRIPTION: Provides an example of documenting a C++ function using Doxygen syntax. This includes brief descriptions, parameter documentation, and equations formatted with TeX, assuming familiarity with Doxygen comment conventions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n/** @brief Calculates the exponent of every array element.\n\nThe function exp calculates the exponent of every element of the input array: \n\\f[ \\texttt{dst} [I] = e^{ src(I) } \\f]\n\nThe maximum relative error is about 7e-6 for single-precision input and less than 1e-10 for\ndouble-precision input. Currently, the function converts denormalized values to zeros on output.\nSpecial values (NaN, Inf) are not handled.\n\n@param src input array.\n@param dst output array of the same size and type as src.\n\n@sa log , cartToPolar , polarToCart , phase , pow , sqrt , magnitude\n*/\nCV_EXPORTS_W void exp(InputArray src, OutputArray dst);\n```\n\n----------------------------------------\n\nTITLE: Customizing Source File Compile Definitions and API Header Inclusion - Platform-Specific Logic - CMake\nDESCRIPTION: Applies a compile definition DEBUG_POSTFIX to the backend_plugin.cpp file if the OPENCV_DEBUG_POSTFIX variable is defined. Removes WinRT and iOS-specific API headers from the list of public headers unless explicitly enabled, and adjusts C++ build flags for certain WinRT modes. This platform-specific logic ensures correct compilation and API exposure depending on build settings and target environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/videoio/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(OPENCV_DEBUG_POSTFIX)\n  ocv_append_source_file_compile_definitions(\"${CMAKE_CURRENT_LIST_DIR}/src/backend_plugin.cpp\" \"DEBUG_POSTFIX=${OPENCV_DEBUG_POSTFIX}\")\nendif()\n\n# Removing WinRT API headers by default\nlist(REMOVE_ITEM videoio_ext_hdrs \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/cap_winrt.hpp\")\n\n# Remove iOS API header by default\nlist(REMOVE_ITEM videoio_ext_hdrs \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/cap_ios.h\")\n\nif(DEFINED WINRT AND NOT DEFINED ENABLE_WINRT_MODE_NATIVE)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /ZW\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Win32UI Backend for HighGUI in CMake\nDESCRIPTION: Sets up the Win32UI backend for OpenCV HighGUI on Windows platforms. Handles both plugin and built-in configurations, and adds OpenGL support if available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nif(TARGET ocv.3rdparty.win32ui)\n  if(\"win32ui\" IN_LIST HIGHGUI_PLUGIN_LIST OR HIGHGUI_PLUGIN_LIST STREQUAL \"all\")\n    ocv_create_builtin_highgui_plugin(opencv_highgui_win32 ocv.3rdparty.win32ui \"window_w32.cpp\")\n  elseif(NOT OPENCV_HIGHGUI_BUILTIN_BACKEND)\n    set(OPENCV_HIGHGUI_BUILTIN_BACKEND \"WIN32UI\")\n    list(APPEND highgui_srcs ${CMAKE_CURRENT_LIST_DIR}/src/window_w32.cpp)\n    list(APPEND tgts ocv.3rdparty.win32ui)\n    if(HAVE_OPENGL AND OPENGL_LIBRARIES)\n      list(APPEND tgts \"${OPENGL_LIBRARIES}\")\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Example Directory Structure for -info File Method (Text)\nDESCRIPTION: Shows the recommended directory structure when using the `-info <collection_file_name>` argument with `opencv_createsamples`. It demonstrates placing the image files (e.g., `img1.jpg`, `img2.jpg`) within a subdirectory (`img`) relative to the description file (`info.dat`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/traincascade.markdown#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n/img\n  img1.jpg\n  img2.jpg\ninfo.dat\n```\n\n----------------------------------------\n\nTITLE: Drawing a Filled Circle in Python\nDESCRIPTION: Implementation of the MyFilledCircle function that draws a filled circle in OpenCV Python. The function takes the image and center point, and uses the circle() function with a negative thickness value to create a solid filled circle.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef MyFilledCircle(img, center):\n    cv.circle(img,\n               center,\n               w//32,\n               (0, 0, 255),\n               -1,\n               cv.LINE_8)\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV from Source on macOS\nDESCRIPTION: This command builds OpenCV using make, utilizing all available CPU cores for faster compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/macos_install/macos_install.markdown#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake -j$(sysctl -n hw.ncpu)\n```\n\n----------------------------------------\n\nTITLE: Configuring Android Project in CMake\nDESCRIPTION: Sets up an Android project for the MobileNet object detection example. Configures project dependencies and SDK target version, and adds it to OpenCV Android examples if the target is successfully created.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/mobilenet-objdetect/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\" LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\" SDK_TARGET 11 \"${ANDROID_SDK_TARGET}\")\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Clang Compiler on Linux\nDESCRIPTION: Command to install the Clang compiler on Linux using the package manager.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install clang\n```\n\n----------------------------------------\n\nTITLE: Configuring Doxyfile for OpenCV Tag Linking - Doxygen Configuration\nDESCRIPTION: This configuration snippet sets the Doxygen TAGFILES variable to link the local OpenCV tag file to the remote documentation. It enables automatic hyperlinking from entities like cv::Mat referenced in your project to their definitions in the OpenCV documentation. You need to have the opencv.tag file downloaded locally in docs/doxygen-tags/ and Doxygen installed. 'TAGFILES' accepts path and URL pairs; output is extended references in HTML docs. Constraints: the local and URL paths must be accurate.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/cross_referencing/tutorial_cross_referencing.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Doxygen\nCODE:\n```\nTAGFILES = ./docs/doxygen-tags/opencv.tag=http://docs.opencv.org/4.11.0\n```\n\n----------------------------------------\n\nTITLE: Enabling libjpeg v7 API/ABI Emulation via CMake Option\nDESCRIPTION: Provides the CMake command-line argument `-DWITH_JPEG7=1` used to configure the libjpeg-turbo build process. Setting this option enables emulation of the libjpeg v7 API and ABI, allowing applications compiled against libjpeg v7 to run with libjpeg-turbo without recompilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/README.md#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n-DWITH_JPEG7=1\n```\n\n----------------------------------------\n\nTITLE: Setting Linear JPEG Compression Quality (C)\nDESCRIPTION: Similar to jpeg_set_quality(), but uses sample tables from the JPEG standard multiplied by a scale factor. Larger scale factors result in lower quality. Useful for conforming to Adobe PostScript DCT conventions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_24\n\nLANGUAGE: C\nCODE:\n```\njpeg_set_linear_quality (j_compress_ptr cinfo, int scale_factor,\n                         boolean force_baseline)\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Decision Tree in C++\nDESCRIPTION: This code snippet is part of the FAST corner detection algorithm that evaluates whether a pixel is a corner based on comparisons with its surrounding pixels. It uses conditional branching with goto statements to efficiently navigate the decision tree of pixel intensity comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_28\n\nLANGUAGE: C++\nCODE:\n```\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            if(ptr[offset8] < c_b)\n                              if(ptr[offset10] < c_b)\n                                if(ptr[offset11] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset11] < c_b)\n                            if(ptr[offset3] < c_b)\n                              if(ptr[offset4] < c_b)\n                                goto is_a_corner;\n                              else\n                                if(ptr[offset10] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                            else\n                              if(ptr[offset8] < c_b)\n                                if(ptr[offset10] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset7] < c_b)\n                if(ptr[offset2] > cb)\n                  if(ptr[offset1] > cb)\n                    if(ptr[offset6] > cb)\n                      goto is_not_a_corner;\n                    else\n                      if(ptr[offset6] < c_b)\n                        if(ptr[offset8] < c_b)\n                          if(ptr[offset4] < c_b)\n                            if(ptr[offset3] < c_b)\n                              goto is_a_corner;\n                            else\n                              if(ptr[offset10] < c_b)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            if(ptr[offset10] < c_b)\n                              if(ptr[offset11] < c_b)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset1] < c_b)\n                      if(ptr[offset6] > cb)\n                        if(ptr[offset8] < c_b)\n                          if(ptr[offset10] < c_b)\n                            if(ptr[offset11] < c_b)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        if(ptr[offset6] < c_b)\n                          if(ptr[offset8] < c_b)\n                            if(ptr[offset4] < c_b)\n                              if(ptr[offset3] < c_b)\n                                goto is_a_corner;\n                              else\n                                if(ptr[offset10] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                            else\n                              if(ptr[offset10] < c_b)\n                                if(ptr[offset11] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          if(ptr[offset8] < c_b)\n                            if(ptr[offset10] < c_b)\n                              if(ptr[offset11] < c_b)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                    else\n                      if(ptr[offset6] > cb)\n                        goto is_not_a_corner;\n                      else\n                        if(ptr[offset6] < c_b)\n                          if(ptr[offset8] < c_b)\n                            if(ptr[offset4] < c_b)\n                              if(ptr[offset3] < c_b)\n                                goto is_a_corner;\n                              else\n                                if(ptr[offset10] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                            else\n                              if(ptr[offset10] < c_b)\n                                if(ptr[offset11] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                else\n                  if(ptr[offset2] < c_b)\n                    if(ptr[offset1] > cb)\n                      if(ptr[offset6] > cb)\n                        goto is_not_a_corner;\n                      else\n                        if(ptr[offset6] < c_b)\n                          if(ptr[offset8] < c_b)\n                            if(ptr[offset4] < c_b)\n                              if(ptr[offset3] < c_b)\n                                goto is_a_corner;\n                              else\n                                if(ptr[offset10] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                            else\n                              if(ptr[offset10] < c_b)\n                                if(ptr[offset11] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      if(ptr[offset1] < c_b)\n                        if(ptr[offset6] > cb)\n                          if(ptr[offset11] < c_b)\n                            if(ptr[offset3] < c_b)\n                              if(ptr[offset4] < c_b)\n                                goto is_a_corner;\n                              else\n                                if(ptr[offset10] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                            else\n                              if(ptr[offset8] < c_b)\n                                if(ptr[offset10] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          if(ptr[offset6] < c_b)\n                            if(ptr[offset3] < c_b)\n                              if(ptr[offset4] < c_b)\n                                goto is_a_corner;\n                              else\n                                if(ptr[offset10] < c_b)\n                                  if(ptr[offset11] < c_b)\n                                    goto is_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                            else\n                              if(ptr[offset8] < c_b)\n```\n\n----------------------------------------\n\nTITLE: FFmpeg Configuration Option in OpenCV\nDESCRIPTION: Defines the WITH_FFMPEG build option for integrating with the FFmpeg library for video decoding and encoding. This enables support for various video formats through required components including avcodec, avformat, avutil, and swscale.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_11\n\nLANGUAGE: markdown\nCODE:\n```\n`WITH_FFMPEG` (default: _ON_)\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV Semihosting Examples with QEMU\nDESCRIPTION: These commands demonstrate how to execute the compiled OpenCV semihosting example applications (`example_semihosting_histogram` and `example_semihosting_norm`) using the QEMU AArch64 emulator (`qemu-aarch64`). This allows testing the semihosting binaries in a Linux userspace environment. Assumes the binaries are located in the `./bin/` directory relative to where the command is run and that `qemu-aarch64` is installed and available in the system's PATH.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/semihosting/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n    qemu-aarch64 ./bin/example_semihosting_histogram\n    qemu-aarch64 ./bin/example_semihosting_norm\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for OpenCV HAL in CMake\nDESCRIPTION: This snippet sets up a CMake project to build a shared library for the OpenCV HAL using the CMake build system. It sets the CMake minimum version, project name, and library details. The snippet also configures include directories and copies specific header files and configuration files to the build directory. Dependencies include OpenCV source directory and standard CMake utilities. Key outputs are the configuration file and library artifacts.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/hal/c_hal/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5 FATAL_ERROR)\n\nset(PROJECT_NAME \"c_hal\")\nset(HAL_LIB_NAME \"c_hal\")\n\nadd_library(${HAL_LIB_NAME} impl.c)\nset_target_properties(${HAL_LIB_NAME} PROPERTIES POSITION_INDEPENDENT_CODE TRUE)\nset(OPENCV_SRC_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../..\")\ntarget_include_directories(${HAL_LIB_NAME} PUBLIC ${CMAKE_CURRENT_SOURCE_DIR} ${OPENCV_SRC_DIR}/modules/core/include)\n\nset(OpenCV_HAL_FOUND TRUE)\nset(OpenCV_HAL_VERSION 0.0.1)\nset(OpenCV_HAL_LIBRARIES ${CMAKE_CURRENT_BINARY_DIR}/lib${HAL_LIB_NAME}.a)\nset(OpenCV_HAL_HEADERS \"impl.h\")\nset(OpenCV_HAL_INCLUDE_DIRS ${CMAKE_CURRENT_LIST_DIR})\n\nconfigure_file(\"impl.h\" \"${CMAKE_BINARY_DIR}/impl.h\" COPYONLY)\nconfigure_file(\"config.cmake\" \"${CMAKE_BINARY_DIR}/OpenCV_HALConfig.cmake\")\n```\n\n----------------------------------------\n\nTITLE: Sourcing OpenNI Environment Variables\nDESCRIPTION: Executes the 'OpenNIDevEnvironment' script provided by the Orbbec OpenNI SDK. This script sets environment variables (like OPENNI2_INCLUDE and OPENNI2_REDIST) necessary for development tools and OpenCV's build system to find the OpenNI headers and libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ source OpenNIDevEnvironment\n```\n\n----------------------------------------\n\nTITLE: Setting up zlib Source Files and Build Targets\nDESCRIPTION: Configures source files, headers, and build targets for zlib library. Supports both static and shared library builds with conditional compilation of additional features like GZFILE operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_36\n\nLANGUAGE: cmake\nCODE:\n```\nset(ZLIB_PUBLIC_HDRS\n    ${CMAKE_CURRENT_BINARY_DIR}/zconf${SUFFIX}.h\n    ${CMAKE_CURRENT_BINARY_DIR}/zlib_name_mangling${SUFFIX}.h\n    ${CMAKE_CURRENT_BINARY_DIR}/zlib${SUFFIX}.h\n)\nset(ZLIB_PRIVATE_HDRS\n    adler32_p.h\n    chunkset_tpl.h\n    compare256_rle.h\n    arch_functions.h\n    crc32_braid_p.h\n    crc32_braid_comb_p.h\n    crc32_braid_tbl.h\n    deflate.h\n    deflate_p.h\n    functable.h\n    inffast_tpl.h\n    inffixed_tbl.h\n    inflate.h\n    inflate_p.h\n    inftrees.h\n    insert_string_tpl.h\n    match_tpl.h\n    trees.h\n    trees_emit.h\n    trees_tbl.h\n    zbuild.h\n    zendian.h\n    zutil.h\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Colored Segmentation Mask from Predictions\nDESCRIPTION: This Python snippet post-processes the raw segmentation prediction (a 2D array of class IDs, `segm_mask`) to create a visually interpretable colored mask. It uses a predefined list or dictionary `colors` (mapping class IDs to BGR or RGB color tuples, likely loaded from PASCAL VOC info) to map each predicted class ID to its corresponding color. The resulting flat list of colors is reshaped into a 3-channel image (`processed_mask`) matching the mask's height and width. This mask is then resized using nearest-neighbor interpolation (`cv2.INTER_NEAREST`) to match the original input image dimensions (`img_width`, `img_height`) and converted to `uint8` format. Finally, `cv2.cvtColor` might be used to ensure the color format (e.g., BGR to RGB) is consistent, for instance, with PASCAL VOC color definitions. Requires `numpy` and `cv2` (OpenCV).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# convert mask values into PASCAL VOC colors\nprocessed_mask = np.stack([colors[color_id] for color_id in segm_mask.flatten()])\n\n# reshape mask into 3-channel image\nprocessed_mask = processed_mask.reshape(mask_height, mask_width, 3)\nprocessed_mask = cv2.resize(processed_mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST).astype(\n    np.uint8)\n\n# convert colored mask from BGR to RGB for compatibility with PASCAL VOC colors\nprocessed_mask = cv2.cvtColor(processed_mask, cv2.COLOR_BGR2RGB)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenJPEG Threading Support\nDESCRIPTION: Sets up threading configuration for OpenJPEG, including platform-specific mutex implementations and thread library detection. Handles Windows and POSIX thread support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/openjp2/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\noption(OPJ_USE_THREAD \"Build with thread/mutex support \" ON)\nif(NOT OPJ_USE_THREAD)\n  add_definitions(-DMUTEX_stub)\nendif()\n\nfind_package(Threads QUIET)\n\nif(OPJ_USE_THREAD AND WIN32 AND NOT Threads_FOUND )\n  add_definitions(-DMUTEX_win32)\n  set(Threads_FOUND YES)\nendif()\n\nif(OPJ_USE_THREAD AND Threads_FOUND AND CMAKE_USE_WIN32_THREADS_INIT)\n  add_definitions(-DMUTEX_win32)\nendif()\n\nif(OPJ_USE_THREAD AND Threads_FOUND AND CMAKE_USE_PTHREADS_INIT )\n  add_definitions(-DMUTEX_pthread)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Checking Python Version in gdb\nDESCRIPTION: This snippet checks the Python version bundled with gdb. It requires gdb to be active and a shell session open within gdb. The script prints the Python version info, which is crucial for compatibility checks.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_gdb_pretty_printer/linux_gdb_pretty_printer.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npython\nimport sys\nprint(sys.version_info)\nend\n```\n\n----------------------------------------\n\nTITLE: Adjusting MSVC Compiler Flags for Standalone Build in CMake\nDESCRIPTION: Modifies C/C++ compiler and linker flags specifically for the MSVC compiler when building samples standalone. If build hardening isn't enabled, it suppresses CRT secure warnings. If linking against a static OpenCV library (`NOT OpenCV_SHARED`), it changes the C runtime library linkage from dynamic (/MD, /MDd) to static (/MT, /MTd) in standard CMake flag variables and adjusts linker flags to ignore conflicting default libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC)\n  if(NOT ENABLE_BUILD_HARDENING)\n    add_definitions(-D_CRT_SECURE_NO_WARNINGS)\n  endif()\n\n  if(NOT OpenCV_SHARED)\n    foreach(flag_var\n            CMAKE_C_FLAGS CMAKE_C_FLAGS_DEBUG CMAKE_C_FLAGS_RELEASE\n            CMAKE_C_FLAGS_MINSIZEREL CMAKE_C_FLAGS_RELWITHDEBINFO\n            CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_DEBUG CMAKE_CXX_FLAGS_RELEASE\n            CMAKE_CXX_FLAGS_MINSIZEREL CMAKE_CXX_FLAGS_RELWITHDEBINFO)\n      if(${flag_var} MATCHES \"/MD\")\n        string(REGEX REPLACE \"/MD\" \"/MT\" ${flag_var} \"${${flag_var}}\")\n      endif()\n      if(${flag_var} MATCHES \"/MDd\")\n        string(REGEX REPLACE \"/MDd\" \"/MTd\" ${flag_var} \"${${flag_var}}\")\n      endif()\n    endforeach(flag_var)\n\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} /NODEFAULTLIB:atlthunk.lib /NODEFAULTLIB:msvcrt.lib /NODEFAULTLIB:msvcrtd.lib\")\n    if(NOT BUILD_WITH_STATIC_CRT)\n      set(CMAKE_EXE_LINKER_FLAGS_DEBUG \"${CMAKE_EXE_LINKER_FLAGS_DEBUG} /NODEFAULTLIB:libcmt.lib\")\n      set(CMAKE_EXE_LINKER_FLAGS_RELEASE \"${CMAKE_EXE_LINKER_FLAGS_RELEASE} /NODEFAULTLIB:libcmtd.lib\")\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating Asymmetric Circle Grid Pattern using Python Script (Shell)\nDESCRIPTION: Uses the 'gen_pattern.py' script to create an asymmetric circle grid pattern ('acircles') saved as 'acircleboard.svg'. It specifies 7 rows, 5 columns, a square size of 10mm, and a radius rate of 2 (meaning radius = square_size / 2 * radius_rate = 10mm), resulting in less spacing between circles compared to the symmetric type. Requires Python and the 'gen_pattern.py' script.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_pattern/camera_calibration_pattern.markdown#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython gen_pattern.py -o acircleboard.svg --rows 7 --columns 5 --type acircles --square_size 10 --radius_rate 2\n```\n\n----------------------------------------\n\nTITLE: Downloading OpenCV Source Using wget\nDESCRIPTION: Commands to download and extract OpenCV source code using wget and unzip.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nwget -O opencv.zip https://github.com/opencv/opencv/archive/4.x.zip\nunzip opencv.zip\n```\n\n----------------------------------------\n\nTITLE: Creating Destination Images and Mapping Matrices\nDESCRIPTION: Initializes destination image and mapping matrices for x and y coordinates that will be used in the remapping process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/remap/remap.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nMat dst, map_x, map_y;\ndst.create( src.size(), src.type() );\nmap_x.create( src.size(), CV_32FC1 );\nmap_y.create( src.size(), CV_32FC1 );\n```\n\nLANGUAGE: Java\nCODE:\n```\nMat dst = new Mat();\nMat map_x = new Mat();\nMat map_y = new Mat();\ndst.create(src.size(), src.type());\nmap_x.create(src.size(), CvType.CV_32FC1);\nmap_y.create(src.size(), CvType.CV_32FC1);\n```\n\nLANGUAGE: Python\nCODE:\n```\ndst = np.zeros(src.shape, dtype=src.dtype)\nmap_x = np.zeros((src.shape[0], src.shape[1]), dtype=np.float32)\nmap_y = np.zeros((src.shape[0], src.shape[1]), dtype=np.float32)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV.js Loader in CMake\nDESCRIPTION: Sets up a custom command to copy the loader.js file to the output directory, which is used for loading OpenCV.js in web environments.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/js/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(opencv_loader_js_bin_dir \"${EXECUTABLE_OUTPUT_PATH}\")\nset(loader_dir ${CMAKE_CURRENT_SOURCE_DIR}/src)\n\nset(opencv_loader_js_file_deps \"\")\n\nfile(MAKE_DIRECTORY \"${opencv_loader_js_bin_dir}\")\n\nadd_custom_command(\n        TARGET ${PROJECT_NAME} POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy\n                ${loader_dir}/loader.js\n                ${opencv_loader_js_bin_dir}/loader.js)\nlist(APPEND opencv_loader_js_file_deps \"${loader_dir}/loader.js\" \"${opencv_loader_js_bin_dir}/loader.js\")\n\nadd_custom_target(${PROJECT_NAME}_loader ALL\n                  DEPENDS ${OCV_JS_PATH} ${opencv_loader_js_file_deps})\n\nadd_custom_target(opencv_test_js ALL DEPENDS opencv_js_test opencv_js_perf opencv_js_loader)\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Logic in C++\nDESCRIPTION: This code snippet implements the core logic of the FAST corner detection algorithm. It compares pixel values at various offsets to determine if a point is a corner. The algorithm uses goto statements to efficiently navigate through different conditions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_31\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  if(ptr[offset1] < c_b)\n    if(ptr[offset6] > cb)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset10] < c_b)\n          if(ptr[offset11] < c_b)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      if(ptr[offset6] < c_b)\n        if(ptr[offset8] < c_b)\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        if(ptr[offset8] < c_b)\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n  else\n    if(ptr[offset6] > cb)\n      goto is_not_a_corner;\n    else\n      if(ptr[offset6] < c_b)\n        if(ptr[offset8] < c_b)\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset7] > cb)\n    if(ptr[offset9] < c_b)\n      if(ptr[offset1] > cb)\n        if(ptr[offset6] < c_b)\n          goto is_not_a_corner;\n        else\n          if(ptr[offset6] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset4] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] < c_b)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset8] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset6] < c_b)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset8] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n    else\n      if(ptr[offset9] > cb)\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] < c_b)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset3] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset10] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset1] > cb)\n            if(ptr[offset6] < c_b)\n              goto is_not_a_corner;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset3] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset8] > cb)\n                      if(ptr[offset10] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset10] > cb)\n                      if(ptr[offset11] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              goto is_not_a_corner;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset8] > cb)\n                  if(ptr[offset4] > cb)\n                    if(ptr[offset3] > cb)\n                      goto is_a_corner;\n                    else\n                      if(ptr[offset10] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset10] > cb)\n                      if(ptr[offset11] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n      else\n        if(ptr[offset1] > cb)\n          if(ptr[offset6] < c_b)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset1] < c_b)\n            if(ptr[offset6] < c_b)\n              goto is_not_a_corner;\n            else\n```\n\n----------------------------------------\n\nTITLE: Setting Default JPEG Compression Parameters (C)\nDESCRIPTION: This function sets all JPEG parameters to reasonable defaults based on the input image's color space. It's recommended to call this function before setting any specific parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_20\n\nLANGUAGE: C\nCODE:\n```\njpeg_set_defaults (j_compress_ptr cinfo)\n```\n\n----------------------------------------\n\nTITLE: Listing OpenCV GUI Tutorials in Markdown\nDESCRIPTION: This snippet lists various GUI-related tutorials for OpenCV using Markdown syntax. Each list item includes a reference to a specific tutorial page and a brief description of its content.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_table_of_contents_gui.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Markdown\nCODE:\n```\n-   @ref tutorial_display_image\n\n    Learn to load an\n    image, display it, and save it back\n\n-   @subpage tutorial_py_video_display\n\n    Learn to play videos,\n    capture videos from a camera, and write videos\n\n-   @subpage tutorial_py_drawing_functions\n\n    Learn to draw lines,\n    rectangles, ellipses, circles, etc with OpenCV\n\n-   @subpage tutorial_py_mouse_handling\n\n    Draw stuff with your\n    mouse\n\n-   @subpage tutorial_py_trackbar\n\n    Create trackbar to\n    control certain parameters\n```\n\n----------------------------------------\n\nTITLE: Conditionally Including cudacodec Module in CMake\nDESCRIPTION: Checks if the `opencv_cudacodec` module is available (`HAVE_opencv_cudacodec` is true). If it is, it uses `ocv_include_modules_recurse` to include its headers and settings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_opencv_cudacodec)\n  ocv_include_modules_recurse(opencv_cudacodec)\nendif()\n```\n\n----------------------------------------\n\nTITLE: XML Configuration for Camera Calibration Input Images\nDESCRIPTION: This XML snippet shows the structure of a configuration file used to specify input images for camera calibration in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_12\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\"?>\n<opencv_storage>\n<images>\nimages/CameraCalibration/VID5/xx1.jpg\nimages/CameraCalibration/VID5/xx2.jpg\nimages/CameraCalibration/VID5/xx3.jpg\nimages/CameraCalibration/VID5/xx4.jpg\nimages/CameraCalibration/VID5/xx5.jpg\nimages/CameraCalibration/VID5/xx6.jpg\nimages/CameraCalibration/VID5/xx7.jpg\nimages/CameraCalibration/VID5/xx8.jpg\n</images>\n</opencv_storage>\n```\n\n----------------------------------------\n\nTITLE: Loading Camera Calibration Results with OpenCV (Python)\nDESCRIPTION: This snippet loads previously computed camera calibration data from an .npz file, retrieving the camera matrix and distortion coefficients needed for subsequent pose estimation calculations. It uses NumPy to load data, expects 'mtx' and 'dist' arrays to be present, and is required as a dependency for all the following pose estimation computations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_pose/py_pose.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\\nimport cv2 as cv\\nglob\\n\\n# Load previously saved data\\nwith np.load('B.npz') as X:\\n    mtx, dist, _, _ = [X[i] for i in ('mtx','dist','rvecs','tvecs')]\n```\n\n----------------------------------------\n\nTITLE: Finding Shortest Distance with Point Polygon Test in OpenCV in JavaScript\nDESCRIPTION: This JavaScript code snippet uses the OpenCV function cv.pointPolygonTest to determine the shortest distance from a given point to the closest edge of a contour. It accepts a contour, a point, and a boolean to determine if the distance should be measured. If measureDist is true, the function returns the signed distance, while measureDist false only checks point-in-contour status.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contours_more_functions/js_contours_more_functions.markdown#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet dist = cv.pointPolygonTest(cnt, new cv.Point(50, 50), true);\n```\n\n----------------------------------------\n\nTITLE: Conditionally Setting WinRT Compilation Flags in CMake\nDESCRIPTION: Checks if the `WINRT` variable is defined and `ENABLE_WINRT_MODE_NATIVE` is not defined. If these conditions are met (indicating a non-native WinRT build, likely C++/CX), it appends the `/ZW` flag to the C++ compiler flags (`CMAKE_CXX_FLAGS`) to enable Windows Runtime language extensions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(DEFINED WINRT AND NOT DEFINED ENABLE_WINRT_MODE_NATIVE)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /ZW\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Prerequisites\nDESCRIPTION: Command to install Podman container management tool using DNF package manager.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/arch/s390/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo dnf install podman\n```\n\n----------------------------------------\n\nTITLE: Splitting and Merging Matrix Channels in OpenCV JavaScript\nDESCRIPTION: These snippets show cv.split, which decomposes a multi-channel array into its constituent channels (arrays), and cv.merge, which combines multiple arrays into a single multi-channel matrix. They facilitate manipulating individual frequency or spatial channels during image processing. Inputs include the source matrix/vector and output destination(s). Dependencies: OpenCV.js; arrays involved must have compatible size and depth.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_transforms/js_fourier_transform/js_fourier_transform.markdown#2025-04-22_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.split (m, mv)\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.merge (mv, dst)\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Framework Excluding Specific Module in Bash\nDESCRIPTION: Command to build the OpenCV framework for iOS with extra modules, excluding a specific module (e.g., optflow).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/ios_install/ios_install.markdown#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/<my_working_directory>\npython opencv/platforms/ios/build_framework.py ios --contrib opencv_contrib --without optflow\n```\n\n----------------------------------------\n\nTITLE: Example CMake Output Verifying OpenNI2 Support\nDESCRIPTION: Shows a snippet of typical CMake configuration output for OpenCV. The line 'OpenNI2:                     YES (2.3.0)' confirms that CMake successfully detected the OpenNI2 SDK (version 2.3.0 in this example) and that OpenCV will be built with support for it.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n--   Video I/O:\n--     DC1394:                      YES (2.2.6)\n--     FFMPEG:                      YES\n--       avcodec:                   YES (58.91.100)\n--       avformat:                  YES (58.45.100)\n--       avutil:                    YES (56.51.100)\n--       swscale:                   YES (5.7.100)\n--       avresample:                NO\n--     GStreamer:                   YES (1.18.1)\n--     OpenNI2:                     YES (2.3.0)\n--     v4l/v4l2:                    YES (linux/videodev2.h)\n```\n\n----------------------------------------\n\nTITLE: Adding Dispatched Source Files for Architecture Optimization - CMake\nDESCRIPTION: This snippet triggers inclusion of the \\\"undistort\\\" source with architecture-specific optimizations (SSE2, AVX2) in the build. It relies on OpenCV's custom macro \\\"ocv_add_dispatched_file\\\" and aids in conditional compilation for performance. The function expects source file and one or more target architectures as parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/calib3d/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nocv_add_dispatched_file(undistort SSE2 AVX2)\n```\n\n----------------------------------------\n\nTITLE: Configuring Installation Rules for libjasper in CMake\nDESCRIPTION: This snippet sets up installation rules for the libjasper library and its associated license files within the OpenCV project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjasper/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(${JASPER_LIBRARY} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev OPTIONAL)\nendif()\n\nocv_install_3rdparty_licenses(jasper LICENSE README copyright)\n```\n\n----------------------------------------\n\nTITLE: Generating Doxygen Configuration Files from Templates in CMake\nDESCRIPTION: Uses the `configure_file` command to generate the final `DoxygenLayout.xml`, `Doxyfile`, and `root.markdown` files in the build directory (`CMAKE_CURRENT_BINARY_DIR`). It uses the template files specified by `OPENCV_DOCS_DOXYGEN_LAYOUT`, `OPENCV_DOCS_DOXYFILE_IN`, and `root.markdown.in`, substituting variables marked with `@VAR@` (due to `@ONLY`) with their current CMake values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\n# writing file\nconfigure_file(\"${OPENCV_DOCS_DOXYGEN_LAYOUT}\" DoxygenLayout.xml @ONLY)\nconfigure_file(\"${OPENCV_DOCS_DOXYFILE_IN}\" ${doxyfile} @ONLY)\nconfigure_file(root.markdown.in ${rootfile} @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV JavaScript Module in CMake\nDESCRIPTION: Sets up the OpenCV JavaScript module, including dependencies, compiler flags, and build targets. It checks for Emscripten and Python, configures the module, and sets up compilation and linking flags for Emscripten.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/js/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(OPENCV_INITIAL_PASS)\n  add_subdirectory(generator)\nendif()\n\nif(NOT BUILD_opencv_js)\n  return()\nendif()\n\nset(the_description \"The JavaScript(JS) bindings\")\n\nset(OPENCV_JS \"opencv.js\")\nset(JS_HELPER \"${CMAKE_CURRENT_SOURCE_DIR}/src/helpers.js\")\n\nfind_path(EMSCRIPTEN_INCLUDE_DIR\n          emscripten/bind.h\n          PATHS\n            ENV EMSCRIPTEN_ROOT\n          PATH_SUFFIXES system/include include\n          DOC \"Location of Emscripten SDK\")\n\nif(NOT EMSCRIPTEN_INCLUDE_DIR OR NOT PYTHON_DEFAULT_AVAILABLE)\n  set(DISABLE_MSG \"Module 'js' disabled because the following dependencies are not found:\")\n  if(NOT EMSCRIPTEN_INCLUDE_DIR)\n    set(DISABLE_MSG \"${DISABLE_MSG} Emscripten\")\n  endif()\n  if(NOT PYTHON_DEFAULT_AVAILABLE)\n    set(DISABLE_MSG \"${DISABLE_MSG} Python\")\n  endif()\n  message(STATUS ${DISABLE_MSG})\n  ocv_module_disable(js)\nendif()\n\nocv_add_module(js BINDINGS PRIVATE_REQUIRED opencv_js_bindings_generator)\n\nocv_module_include_directories(${EMSCRIPTEN_INCLUDE_DIR})\n\nadd_definitions(\"-std=c++11\")\n\nset(deps ${OPENCV_MODULE_${the_module}_DEPS})\nlist(REMOVE_ITEM deps opencv_js_bindings_generator)\nlink_libraries(${deps})\n\nset(bindings_cpp \"${OPENCV_JS_BINDINGS_DIR}/gen/bindings.cpp\")\nset_source_files_properties(${bindings_cpp} PROPERTIES GENERATED TRUE)\n\nOCV_OPTION(BUILD_WASM_INTRIN_TESTS \"Build WASM intrin tests\" OFF )\nif(BUILD_WASM_INTRIN_TESTS)\n  add_definitions(-DTEST_WASM_INTRIN)\n  ocv_module_include_directories(\"${CMAKE_CURRENT_SOURCE_DIR}/../ts/include\")\n  ocv_module_include_directories(\"${CMAKE_CURRENT_SOURCE_DIR}/../imgcodecs/include\")\n  ocv_module_include_directories(\"${CMAKE_CURRENT_SOURCE_DIR}/../videoio/include\")\n  ocv_module_include_directories(\"${CMAKE_CURRENT_SOURCE_DIR}/../highgui/include\")\n  ocv_add_executable(${the_module} ${bindings_cpp} \"${CMAKE_CURRENT_SOURCE_DIR}/../ts/src/ts_gtest.cpp\")\nelse()\n  ocv_add_executable(${the_module} ${bindings_cpp})\nendif()\n\nadd_dependencies(${the_module} gen_opencv_js_source)\n\nset(COMPILE_FLAGS \"\")\nif(NOT CMAKE_CXX_COMPILER_ID MATCHES \"MSVC\")\n    set(COMPILE_FLAGS \"${COMPILE_FLAGS} -Wno-missing-prototypes\")\nendif()\nif(COMPILE_FLAGS)\n    set_target_properties(${the_module} PROPERTIES COMPILE_FLAGS ${COMPILE_FLAGS})\nendif()\n\nset(EMSCRIPTEN_LINK_FLAGS \"${EMSCRIPTEN_LINK_FLAGS} -s TOTAL_MEMORY=128MB -s WASM_MEM_MAX=1GB -s ALLOW_MEMORY_GROWTH=1\")\nset(EMSCRIPTEN_LINK_FLAGS \"${EMSCRIPTEN_LINK_FLAGS} -s MODULARIZE=1\")\nset(EMSCRIPTEN_LINK_FLAGS \"${EMSCRIPTEN_LINK_FLAGS} -s EXPORT_NAME=\\\"'cv'\\\" -s DEMANGLE_SUPPORT=1\")\nset(EMSCRIPTEN_LINK_FLAGS \"${EMSCRIPTEN_LINK_FLAGS} -s FORCE_FILESYSTEM=1 --use-preload-plugins --bind --post-js ${JS_HELPER} ${COMPILE_FLAGS}\")\nset_target_properties(${the_module} PROPERTIES LINK_FLAGS \"${EMSCRIPTEN_LINK_FLAGS}\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Dependencies for OpenCV\nDESCRIPTION: This snippet configures automatic downloading of dependencies during the OpenCV build, offering alternatives for setting cache locations and proxy settings. It also highlights the use of helper scripts for failed downloads.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nexport OPENCV_DOWNLOAD_PATH=/tmp/opencv-cache\ncmake ../opencv\n# or\ncmake -DOPENCV_DOWNLOAD_PATH=/tmp/opencv-cache ../opencv\n```\n\nLANGUAGE: sh\nCODE:\n```\nexport http_proxy=<proxy-host>:<port>\nexport https_proxy=<proxy-host>:<port>\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Project with CMake\nDESCRIPTION: Sets up a CMake project for OpenCV, including package finding, version checking, and linking configuration. Defines an executable target and links it with OpenCV libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/cpp/example_cmake/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n# cmake needs this line\ncmake_minimum_required(VERSION 3.5)\n\n# Define project name\nproject(opencv_example_project)\n\n# Find OpenCV, you may need to set OpenCV_DIR variable\n# to the absolute path to the directory containing OpenCVConfig.cmake file\n# via the command line or GUI\nfind_package(OpenCV REQUIRED)\n\n# If the package has been found, several variables will\n# be set, you can find the full list with descriptions\n# in the OpenCVConfig.cmake file.\n# Print some message showing some of them\nmessage(STATUS \"OpenCV library status:\")\nmessage(STATUS \"    config: ${OpenCV_DIR}\")\nmessage(STATUS \"    version: ${OpenCV_VERSION}\")\nmessage(STATUS \"    libraries: ${OpenCV_LIBS}\")\nmessage(STATUS \"    include path: ${OpenCV_INCLUDE_DIRS}\")\n\n# Declare the executable target built from your sources\nadd_executable(opencv_example example.cpp)\n\n# Link your application with OpenCV libraries\ntarget_link_libraries(opencv_example PRIVATE ${OpenCV_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Configuring x86 AVX2 Optimizations for ZLIB in CMake\nDESCRIPTION: Checks if AVX2 optimization is enabled (WITH_AVX2), if intrinsics are available (HAVE_AVX2_INTRIN), and if SSE4.2 is also enabled. If supported, it adds the DX86_AVX2 definition, appends multiple AVX2-specific source files (for slide_hash, chunkset, compare256, adler32), adds feature information for each, and sets compile flags (AVX2FLAG, NOLTOFLAG). Otherwise, it disables AVX2 support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_30\n\nLANGUAGE: cmake\nCODE:\n```\n        if(WITH_AVX2)\n            check_avx2_intrinsics()\n            if(HAVE_AVX2_INTRIN AND WITH_SSE42)\n                add_definitions(-DX86_AVX2)\n                set(AVX2_SRCS ${ARCHDIR}/slide_hash_avx2.c)\n                add_feature_info(AVX2_SLIDEHASH 1 \"Support AVX2 optimized slide_hash, using \\\"${AVX2FLAG}\\\"\")\n                list(APPEND AVX2_SRCS ${ARCHDIR}/chunkset_avx2.c)\n                add_feature_info(AVX2_CHUNKSET 1 \"Support AVX2 optimized chunkset, using \\\"${AVX2FLAG}\\\"\")\n                list(APPEND AVX2_SRCS ${ARCHDIR}/compare256_avx2.c)\n                add_feature_info(AVX2_COMPARE256 1 \"Support AVX2 optimized compare256, using \\\"${AVX2FLAG}\\\"\")\n                list(APPEND AVX2_SRCS ${ARCHDIR}/adler32_avx2.c)\n                add_feature_info(AVX2_ADLER32 1 \"Support AVX2-accelerated adler32, using \\\"${AVX2FLAG}\\\"\")\n                list(APPEND ZLIB_ARCH_SRCS ${AVX2_SRCS})\n                set_property(SOURCE ${AVX2_SRCS} PROPERTY COMPILE_FLAGS \"${AVX2FLAG} ${NOLTOFLAG}\")\n            else()\n                set(WITH_AVX2 OFF)\n            endif()\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Drawing Probabilistic Hough Line Segments in OpenCV (Java)\nDESCRIPTION: This Java code draws line segments from HoughLinesP on the result image. For each row in the Mat, it plots a line between the segment endpoints using Imgproc.line. Needs a non-empty linesP Mat and output Mat. Requires Java OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\nfor (int i = 0; i < linesP.rows(); i++) {\\n    double[] l = linesP.get(i, 0);\\n    Imgproc.line(color_dst, new Point(l[0], l[1]), new Point(l[2], l[3]), new Scalar(0,255,0), 3, Imgproc.LINE_AA);\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV CMake Build for NVIDIA Drive PX 2\nDESCRIPTION: CMake configuration options for building OpenCV with CUDA support on the NVIDIA Drive PX 2 platform. Enables Python 2 bindings, CUDA 8.0, and other optimizations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n$ cmake \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=/usr \\\n    -DBUILD_PNG=OFF \\\n    -DBUILD_TIFF=OFF \\\n    -DBUILD_TBB=OFF \\\n    -DBUILD_JPEG=OFF \\\n    -DBUILD_JASPER=OFF \\\n    -DBUILD_ZLIB=OFF \\\n    -DBUILD_EXAMPLES=ON \\\n    -DBUILD_JAVA=OFF \\\n    -DBUILD_opencv_python2=ON \\\n    -DBUILD_opencv_python3=OFF \\\n    -DENABLE_NEON=ON \\\n    -DWITH_OPENCL=OFF \\\n    -DWITH_OPENMP=OFF \\\n    -DWITH_FFMPEG=ON \\\n    -DWITH_GSTREAMER=OFF \\\n    -DWITH_GSTREAMER_0_10=OFF \\\n    -DWITH_CUDA=ON \\\n    -DWITH_GTK=ON \\\n    -DWITH_VTK=OFF \\\n    -DWITH_TBB=ON \\\n    -DWITH_1394=OFF \\\n    -DWITH_OPENEXR=OFF \\\n    -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-8.0 \\\n    -DCUDA_ARCH_BIN=6.2 \\\n    -DCUDA_ARCH_PTX=\"\" \\\n    -DINSTALL_C_EXAMPLES=ON \\\n    -DINSTALL_TESTS=OFF \\\n    -DOPENCV_TEST_DATA_PATH=../opencv_extra/testdata \\\n    ../opencv\n```\n\n----------------------------------------\n\nTITLE: Testing Python Code with Pylint using Make in Shell\nDESCRIPTION: This shell command runs pylint to perform a static analysis on Python code within OpenCV. Requires pylint to be installed through the package manager or via pip.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nmake check_pylint\n```\n\nLANGUAGE: sh\nCODE:\n```\npip install pylint\n```\n\n----------------------------------------\n\nTITLE: Creating Image for Histogram Display in Java\nDESCRIPTION: Java snippet creating a blank image (`histImage`) for histogram visualization. It defines width (`histW`) and height (`histH`), calculates the width for each bin (`binW`), and initializes a 3-channel (`CvType.CV_8UC3`) Mat of the specified size with all pixels set to zero (black).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_22\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/Histograms_Matching/histogram_calculation/CalcHistDemo.java Draw the histograms for B, G and R\n```\n\n----------------------------------------\n\nTITLE: Setting OpenCV Library Directory in Visual Studio\nDESCRIPTION: Adds the OpenCV library directory to the project's additional library directories using an environment variable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_visual_studio_opencv/windows_visual_studio_opencv.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$(OPENCV_DIR)\\lib\n```\n\n----------------------------------------\n\nTITLE: Reproducing OpenCV Model Conversion with Python\nDESCRIPTION: This command executes the model conversion pipeline in test mode with default image preprocessing using OpenCV and the DNN model runner.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.segmentation.py_to_py_segm --model_name fcnresnet50 --test True --default_img_preprocess True --evaluate False\n```\n\n----------------------------------------\n\nTITLE: Processing Source Lists and Setting Include Directories for OpenCV World (CMake)\nDESCRIPTION: Calls two custom OpenCV CMake functions. `ocv_glob_module_sources` takes the previously aggregated `headers_list` and `sources_list` as input, likely performing final processing, globbing, or validation on these file lists. `ocv_module_include_directories` sets up the necessary include paths required to compile the `opencv_world` module, ensuring headers from all included sub-modules are findable by the compiler.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/world/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nocv_glob_module_sources(HEADERS ${headers_list} SOURCES ${sources_list})\n\nocv_module_include_directories()\n```\n\n----------------------------------------\n\nTITLE: Setting Halide as Preferable Backend in OpenCV DNN\nDESCRIPTION: This snippet shows the command to set Halide as the preferred backend for the OpenCV DNN module. This step is critical for integrating Halide in the deep learning processing pipeline.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_halide/dnn_halide.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nnet.setPreferableBackend(DNN_BACKEND_HALIDE);\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV.js Test Files in CMake\nDESCRIPTION: Sets up custom commands and targets for copying and preparing test files for OpenCV.js, including test data and performance test files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/js/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(opencv_test_js_bin_dir \"${EXECUTABLE_OUTPUT_PATH}\")\nset(test_dir ${CMAKE_CURRENT_SOURCE_DIR}/test)\n\nset(opencv_test_js_file_deps \"\")\n\nfile(MAKE_DIRECTORY \"${opencv_test_js_bin_dir}\")\n\nfile(GLOB_RECURSE test_files RELATIVE \"${test_dir}\" \"${test_dir}/*\")\nforeach(f ${test_files})\n  add_custom_command(OUTPUT \"${opencv_test_js_bin_dir}/${f}\"\n                     COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${test_dir}/${f}\" \"${opencv_test_js_bin_dir}/${f}\"\n                     DEPENDS \"${test_dir}/${f}\"\n                     COMMENT \"Copying ${f}\"\n                    )\n  list(APPEND opencv_test_js_file_deps \"${test_dir}/${f}\" \"${opencv_test_js_bin_dir}/${f}\")\nendforeach()\n\nset(test_data \"haarcascade_frontalface_default.xml\")\nset(test_data_path \"${PROJECT_SOURCE_DIR}/../../data/haarcascades/${test_data}\")\n\nadd_custom_command(OUTPUT \"${opencv_test_js_bin_dir}/${test_data}\"\n                   COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${test_data_path}\" \"${opencv_test_js_bin_dir}/${test_data}\"\n                   DEPENDS \"${test_data_path}\"\n                   COMMENT \"Copying ${test_data}\"\n                  )\nlist(APPEND opencv_test_js_file_deps \"${test_data_path}\" \"${opencv_test_js_bin_dir}/${test_data}\")\n\nadd_custom_target(${PROJECT_NAME}_test\n                  DEPENDS ${OCV_JS_PATH} ${opencv_test_js_file_deps})\n```\n\n----------------------------------------\n\nTITLE: Implementing Corner Detection Logic in C++\nDESCRIPTION: This snippet contains a series of nested conditional statements that compare pixel values at different offsets to determine if a point is a corner. It uses goto statements for efficient branching and appears to be part of a larger corner detection algorithm.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_22\n\nLANGUAGE: C++\nCODE:\n```\ngoto is_not_a_corner;\nelse\n  if(ptr[offset6] < c_b)\n    if(ptr[offset10] > cb)\n      if(ptr[offset11] > cb)\n        if(ptr[offset3] > cb)\n          goto is_a_corner;\n        else\n          if(ptr[offset8] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    if(ptr[offset10] > cb)\n      if(ptr[offset11] > cb)\n        if(ptr[offset3] > cb)\n          goto is_a_corner;\n        else\n          if(ptr[offset8] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  if(ptr[offset6] < c_b)\n    goto is_not_a_corner;\n  else\n    if(ptr[offset6] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset11] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  if(ptr[offset1] < c_b)\n    if(ptr[offset6] < c_b)\n      goto is_not_a_corner;\n    else\n      if(ptr[offset6] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    if(ptr[offset1] > cb)\n      if(ptr[offset6] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset8] > cb)\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          if(ptr[offset8] > cb)\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n    else\n      if(ptr[offset6] < c_b)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset6] > cb)\n          if(ptr[offset8] > cb)\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\nelse\n  if(ptr[offset2] < c_b)\n    goto is_not_a_corner;\n  else\n    if(ptr[offset2] > cb)\n      if(ptr[offset1] < c_b)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset1] > cb)\n          if(ptr[offset6] > cb)\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                if(ptr[offset3] > cb)\n                  goto is_a_corner;\n                else\n                  if(ptr[offset8] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  if(ptr[offset3] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset8] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  if(ptr[offset3] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset8] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  if(ptr[offset2] < c_b)\n    if(ptr[offset7] > cb)\n      goto is_not_a_corner;\n    else\n      if(ptr[offset7] < c_b)\n        if(ptr[offset1] < c_b)\n          if(ptr[offset6] > cb)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset4] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset1] > cb)\n            if(ptr[offset6] > cb)\n              goto is_not_a_corner;\n            else\n              if(ptr[offset6] < c_b)\n```\n\n----------------------------------------\n\nTITLE: OpenVX C++ Image Class Interface Example\nDESCRIPTION: Shows the interface definition for the Image class in the OpenVX C++ wrapper, demonstrating the static factory methods for creating different types of image objects.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openvx/README.md#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nclass Image\n{\n    static Image create(vx_context context, vx_uint32 width, vx_uint32 height, vx_df_image format);\n    static Image createVirtual(vx_graph graph, vx_uint32 width = 0, vx_uint32 height = 0, vx_df_image format = VX_DF_IMAGE_VIRT);\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV with Contrib Repository in Shell\nDESCRIPTION: Command line option to include the OpenCV contrib repository when building OpenCV using CMake. The contrib repository contains experimental and non-free algorithms that are not part of the main OpenCV repository.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/transition_guide/transition_guide.markdown#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n-DOPENCV_EXTRA_MODULES_PATH=<path-to-opencv_contrib>/modules\n```\n\n----------------------------------------\n\nTITLE: Creating OpenCV Java Library Target in CMake\nDESCRIPTION: Adds the OpenCV Java library target with appropriate sources and dependencies. Handles different configurations for fat Java library builds and platform-specific settings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jni/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(__type MODULE)\nif(BUILD_FAT_JAVA_LIB)\n  set(__type SHARED) # samples link to libopencv_java\nelseif(APPLE)\n  set(CMAKE_SHARED_MODULE_SUFFIX \".dylib\")  # Java is not able to load .so files\nendif()\nocv_add_library(${the_module} ${__type}\n    ${handwritten_h_sources} ${handwritten_cpp_sources} ${generated_cpp_sources}\n    ${copied_files}\n)\nadd_dependencies(${the_module} gen_opencv_java_source)\n\nocv_target_include_directories(${the_module} \"${CMAKE_CURRENT_SOURCE_DIR}/../generator/src/cpp\")\nocv_target_include_directories(${the_module} \"${OPENCV_JAVA_BINDINGS_DIR}/gen/cpp\")\nocv_target_include_modules(${the_module} ${OPENCV_MODULE_${the_module}_DEPS})\nif(NOT ANDROID)\n  ocv_target_include_directories(${the_module} SYSTEM ${JNI_INCLUDE_DIRS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Histogram Value Range in Java\nDESCRIPTION: Java snippet defining the range of pixel values for the histogram. A `MatOfFloat` object named `histRange` is created and initialized with the values 0f and 256f, specifying the lower (inclusive) and upper (exclusive) bounds for the histogram bins.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/Histograms_Matching/histogram_calculation/CalcHistDemo.java Set the ranges ( for B,G,R) )\n```\n\n----------------------------------------\n\nTITLE: Blending Two Images with OpenCV.js Based on Range Input - JavaScript\nDESCRIPTION: This code blends two images using OpenCV.js based on the value of a trackbar (input range) and displays the result in a canvas. It retrieves the current value, computes blending weights (alpha and beta), reads images from canvases, blends them using cv.addWeighted, and displays the result. All created Mat objects are properly deleted to avoid memory leaks. Dependencies: OpenCV.js, HTML canvas elements with specific IDs, input range and text elements. Parameters: trackbar and weightValue element IDs. Outputs: Displays blended image on a canvas.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_trackbar/js_trackbar.markdown#2025-04-22_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet weightValue = document.getElementById('weightValue');\nlet trackbar = document.getElementById('trackbar');\nweightValue.setAttribute('value', trackbar.value);\nlet alpha = trackbar.value/trackbar.max;\nlet beta = ( 1.0 - alpha );\nlet src1 = cv.imread('canvasInput1');\nlet src2 = cv.imread('canvasInput2');\nlet dst = new cv.Mat();\ncv.addWeighted( src1, alpha, src2, beta, 0.0, dst, -1);\ncv.imshow('canvasOutput', dst);\ndst.delete();\nsrc1.delete();\nsrc2.delete();\n```\n\n----------------------------------------\n\nTITLE: Loading an Image using OpenCV in C++\nDESCRIPTION: Loads an image from a file specified by a command-line argument using the `imread` function. It checks if the image was loaded successfully and exits if it fails.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/pyramids/pyramids.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n/**\n * @function main\n */\nint main( int argc, char** argv )\n{\n    /// General instructions\n    printf( \"\\n Zoom In-Out demo \\n \" );\n    printf( \"------------------\\n\" );\n    printf( \" * [i] -> Zoom in \\n\" );\n    printf( \" * [o] -> Zoom out \\n\" );\n    printf( \" * [ESC] -> Close program \\n\\n\" );\n\n    //![load]\n    CommandLineParser parser( argc, argv, \"{@input | ../data/chicky_512.png | input image}\" );\n    Mat src = imread( samples::findFile( parser.get<String>( \"@input\" ) ) );\n\n    // Check if image is loaded fine\n    if( src.empty() ){\n        printf(\" Error opening image\\n\");\n        printf(\" Program Arguments: [image_name -- default ../data/chicky_512.png] \\n\");\n        return EXIT_FAILURE;\n    }\n    //![load]\n```\n\n----------------------------------------\n\nTITLE: Initializing Output Images in OpenCV (C++/Java/Python)\nDESCRIPTION: Creates empty `Mat` objects (or NumPy arrays in Python) with the same size and type as the binary image (`bw`). These will store the results of the morphological operations for horizontal and vertical line detection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection.md#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n//![init]\n// Create the images that will use to extract the horizontal and vertical lines\nMat horizontal = bw.clone();\nMat vertical = bw.clone();\n//![init]\n```\n\nLANGUAGE: java\nCODE:\n```\n//![init]\n// Create the images that will use to extract the horizontal and vertical lines\nMat horizontal = bw.clone();\nMat vertical = bw.clone();\n//![init]\n```\n\nLANGUAGE: python\nCODE:\n```\n#![init]\n# Create the images that will use to extract the horizontal and vertical lines\nhorizontal = np.copy(bw)\nvertical = np.copy(bw)\n#![init]\n```\n\n----------------------------------------\n\nTITLE: Calculating Back Projection with OpenCV in C++\nDESCRIPTION: The C++ sample demonstrates loading an image, converting it to HSV, extracting the Hue channel, allowing user selection of bin sizes, and calculating both the histogram and its back projection using OpenCV. Dependencies include OpenCV C++, and the code requires user-provided images as input. Outputs include displayed windows for the original image, histogram, and back projection; key adjustable parameters are the number of bins for the histogram. The code uses cv::mixChannels, cv::calcHist, and cv::calcBackProject, with a trackbar to trigger updates.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/back_projection/back_projection.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/imgproc.hpp>\\n#include <opencv2/highgui.hpp>\\n\\nusing namespace cv;\\n\\nint main(int argc, char** argv)\\n{\\n    CommandLineParser parser(argc, argv, \\n        \\\"{@input | | input image}\\\");\\n    Mat src = imread(samples::findFile(parser.get<String>(\\\"@input\\\")), IMREAD_COLOR);\\n    if (src.empty())\\n    {\\n        printf(\\\"Could not open or find the image\\n\\\");\\n        return -1;\\n    }\\n    // ... rest of code\\n}\\n\n```\n\nLANGUAGE: C++\nCODE:\n```\n// Convert to HSV\\nMat hsv;\\ncvtColor(src, hsv, COLOR_BGR2HSV);\\n\n```\n\nLANGUAGE: C++\nCODE:\n```\n// Extract only the Hue channel\\nMat hue;\\nint ch[] = {0, 0};\\nhue.create(hsv.size(), hsv.depth());\\nmixChannels(&hsv, 1, &hue, 1, ch, 1);\\n\n```\n\nLANGUAGE: C++\nCODE:\n```\n// Create Trackbar for bins\\nint bins = 30;\\ncreateTrackbar(\\\"Histogram Bins\\\", \\\"Source image\\\", &bins, 180, Hist_and_Backproj);\\n\n```\n\nLANGUAGE: C++\nCODE:\n```\n// Show the image and wait for exit\\nimshow(\\\"Source image\\\", src);\\nwaitKey();\\n\n```\n\nLANGUAGE: C++\nCODE:\n```\n// Initialize histogram parameters in callback\\nvoid Hist_and_Backproj(int, void*)\\n{\\n    int histSize = MAX(bins, 2);\\n    float hue_range[] = {0, 180};\\n    const float* ranges = {hue_range};\\n    // ... hist and backproj\\n}\\n\n```\n\nLANGUAGE: C++\nCODE:\n```\n// Calculate histogram and normalize\\ncalcHist(&hue, 1, 0, Mat(), hist, 1, &histSize, &ranges, true, false);\\nnormalize(hist, hist, 0, 255, NORM_MINMAX, -1, Mat());\\n\n```\n\nLANGUAGE: C++\nCODE:\n```\n// Backprojection\\ncalcBackProject(&hue, 1, 0, hist, backproj, &ranges, 1, true);\\n\n```\n\n----------------------------------------\n\nTITLE: Drawing Ellipse with OpenCV in Python\nDESCRIPTION: Draws a half ellipse at the center of the image using the cv.ellipse() function, demonstrating various parameters like axes lengths and angles.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_drawing_functions/py_drawing_functions.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ncv.ellipse(img,(256,256),(100,50),0,0,180,255,-1)\n```\n\n----------------------------------------\n\nTITLE: Creating Display Window in Java\nDESCRIPTION: This Java snippet uses `HighGui.namedWindow` to create a display window titled with the value of `WINDOW_NAME`. The trackbar will be added to this window.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n//![window]\n// Create window\nHighGui.namedWindow(WINDOW_NAME, HighGui.WINDOW_AUTOSIZE);\n//![window]\n```\n\n----------------------------------------\n\nTITLE: Calculating Convexity Defects with OpenCV in JavaScript\nDESCRIPTION: The purpose of this OpenCV function is to detect convexity defects in a given contour relative to its convex hull. The function cv.convexityDefects requires the indices of contour points that form the convex hull, and it outputs a vector describing the defects. A typical defect is provided as a four-element tuple denoting start index, end index, the farthest point from the hull, and depth. Prerequisite: convex hull must be computed with returnPoints set to False.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contours_more_functions/js_contours_more_functions.markdown#2025-04-22_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Checking and Modifying OpenCV Optimizations in Python\nDESCRIPTION: Explains how to check whether OpenCV is using optimized code (e.g., with SSE2, AVX) via cv.useOptimized(), and how to enable or disable these optimizations using cv.setUseOptimized(). Shows the effect of this on the performance of cv.medianBlur using IPython's %timeit after enabling/disabling optimization. Dependencies are cv2, IPython (for %timeit), and an image. Inputs: OpenCV image array. Outputs: performance difference between optimized and unoptimized computation. IPython shell required for magic commands.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_optimization/py_optimization.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# check if optimization is enabled\nIn [5]: cv.useOptimized()\nOut[5]: True\n\nIn [6]: %timeit res = cv.medianBlur(img,49)\n10 loops, best of 3: 34.9 ms per loop\n\n# Disable it\nIn [7]: cv.setUseOptimized(False)\n\nIn [8]: cv.useOptimized()\nOut[8]: False\n\nIn [9]: %timeit res = cv.medianBlur(img,49)\n10 loops, best of 3: 64.1 ms per loop\n```\n\n----------------------------------------\n\nTITLE: Calculating PSNR on GPU (Basic) in C++\nDESCRIPTION: Defines a C++ function `getPSNR_GPU` that calculates the PSNR between two images using basic OpenCV GPU functions. It uploads the input `Mat` objects (`I1`, `I2`) to `GpuMat` objects (`gI1`, `gI2`) and then calls a GPU-accelerated `psnr` function. This represents a direct port from CPU to GPU without specific optimizations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-basics-similarity/gpu_basics_similarity.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n//![getpsnrcuda]\ndouble getPSNR_GPU(const Mat& I1, const Mat& I2)\n{\n    gpu::GpuMat gI1, gI2;\n    gI1.upload(I1);\n    gI2.upload(I2);\n\n    return psnr(gI1, gI2);\n}\n//![getpsnrcuda]\n```\n\n----------------------------------------\n\nTITLE: Drawing a Filled Circle in C++\nDESCRIPTION: Implementation of the MyFilledCircle function that draws a filled circle in OpenCV C++. The function takes the image and center point, and uses the circle() function with a negative thickness value to create a solid filled circle.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\nvoid MyFilledCircle( Mat img, Point center )\n{\n  circle( img,\n      center,\n      w/32,\n      Scalar( 0, 0, 255 ),\n      FILLED,\n      LINE_8 );\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering and Copying Values Greater Than Zero using Streams\nDESCRIPTION: Demonstrates using Thrust's count_if and copy_if with a stream to find and copy only positive values from a source matrix to a destination matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-thrust-interop/gpu_thrust_interop.markdown#2025-04-22_snippet_8\n\nLANGUAGE: CUDA\nCODE:\n```\n// Count number of elements greater than 0\nint num_greater = thrust::count_if(\n    thrust::system::cuda::par.on(StreamAccessor::getStream(stream)),\n    d_random_ptr,\n    d_random_ptr + d_random.rows * d_random.cols,\n    is_greater()\n);\n\n// Allocate space for the results\ncv::cuda::GpuMat d_result(num_greater, 1, CV_32FC1);\nthrust::device_ptr<float> d_result_ptr((float*)d_result.data);\n\n// Copy values greater than 0 to result\nthrust::copy_if(\n    thrust::system::cuda::par.on(StreamAccessor::getStream(stream)),\n    d_random_ptr,\n    d_random_ptr + d_random.rows * d_random.cols,\n    d_result_ptr,\n    is_greater()\n);\n\n// Download results to CPU\ncv::Mat h_result(d_result);\nstd::cout << \"Found \" << num_greater << \" values greater than 0\" << std::endl;\nstd::cout << \"First 5 values: \" << std::endl;\nfor(int i = 0; i < std::min(5, num_greater); i++)\n{\n    std::cout << h_result.at<float>(i) << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Conditional Logic in C++\nDESCRIPTION: Complex nested conditional structure that evaluates pixel values using pointer arithmetic to determine corner points. The code compares intensity values at different offsets against threshold values 'cb' and 'c_b' to classify pixels as corners or non-corners.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\ngoto is_not_a_corner;\nelse\n  if(ptr[offset1] > cb)\n    if(ptr[offset6] > cb)\n      if(ptr[offset10] > cb)\n        if(ptr[offset11] > cb)\n          if(ptr[offset3] > cb)\n            goto is_a_corner;\n          else\n            if(ptr[offset8] > cb)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      if(ptr[offset6] < c_b)\n        if(ptr[offset10] > cb)\n          if(ptr[offset11] > cb)\n            if(ptr[offset3] > cb)\n              goto is_a_corner;\n            else\n              if(ptr[offset8] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        if(ptr[offset10] > cb)\n          if(ptr[offset11] > cb)\n            if(ptr[offset3] > cb)\n              goto is_a_corner;\n            else\n              if(ptr[offset8] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset2] < c_b)\n    if(ptr[offset1] < c_b)\n      if(ptr[offset6] > cb)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset3] < c_b)\n            if(ptr[offset4] < c_b)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Configuring Wayland Backend for HighGUI in CMake\nDESCRIPTION: Configures the Wayland backend for OpenCV HighGUI when WITH_WAYLAND is enabled. Sets up Wayland protocol generation, includes necessary libraries and dependencies, and adds EGL support if available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nif(WITH_WAYLAND AND HAVE_WAYLAND)\n  set(OPENCV_HIGHGUI_BUILTIN_BACKEND \"Wayland\")\n  add_definitions(-DHAVE_WAYLAND)\n\n  set(CMAKE_INCLUDE_CURRENT_DIR ON)\n\n  if (HAVE_WAYLAND_PROTOCOLS)\n      ocv_wayland_generate(\n            ${WAYLAND_PROTOCOLS_BASE}/stable/xdg-shell/xdg-shell.xml\n            xdg-shell-client-protocol)\n  endif()\n\n  list(APPEND highgui_srcs\n    ${CMAKE_CURRENT_LIST_DIR}/src/window_wayland.cpp\n    ${WAYLAND_PROTOCOL_SOURCES}\n  )\n  list(APPEND HIGHGUI_LIBRARIES \"${WAYLAND_CLIENT_LINK_LIBRARIES};${WAYLAND_CURSOR_LINK_LIBRARIES};${XKBCOMMON_LINK_LIBRARIES}\")\n\n  if(HAVE_WAYLAND_EGL)\n    if(WAYLAND_EGL_LIBRARIES)\n      list(APPEND HIGHGUI_LIBRARIES \"${WAYLAND_EGL_LIBRARIES}\")\n    endif()\n  endif()\n\n  ocv_module_include_directories(${WAYLAND_CLIENT_INCLUDE_DIRS} ${XKBCOMMON_INCLUDE_DIRS})\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variable for WebNN\nDESCRIPTION: This code sets the environment variable `WEBNN_NATIVE_DIR` to the output directory of the WebNN build. This configuration is necessary to enable the native DNN backend in OpenCV for WebNN integration. PATH_TO_WebNN should be replaced with the actual path to the output directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/src/webnn/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport WEBNN_NATIVE_DIR=${PATH_TO_WebNN}\n```\n\n----------------------------------------\n\nTITLE: Configuring Additional HTML Files for Doxygen in CMake\nDESCRIPTION: Appends paths to specific files (icons, images, JavaScript utility files) to the `CMAKE_DOXYGEN_HTML_FILES` list. These files will be copied by Doxygen to the HTML output directory. Finally, it formats this list into a newline-separated string suitable for the Doxyfile configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND CMAKE_DOXYGEN_HTML_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/opencv.ico\")\nlist(APPEND CMAKE_DOXYGEN_HTML_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/pattern.png\")\nlist(APPEND CMAKE_DOXYGEN_HTML_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/acircles_pattern.png\")\nlist(APPEND CMAKE_DOXYGEN_HTML_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/bodybg.png\")\n# list(APPEND CMAKE_DOXYGEN_HTML_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/mymath.sty\")\nlist(APPEND CMAKE_DOXYGEN_HTML_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/tutorial-utils.js\")\nstring(REPLACE \";\" \" \\\\\\n\" CMAKE_DOXYGEN_HTML_FILES \"${CMAKE_DOXYGEN_HTML_FILES}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Deep Neural Network Module Description - CMake\nDESCRIPTION: This statement sets the project-level description for the DNN module. Useful for documentation and for integration with CMake system features. The variable 'the_description' is set to a string describing the module's functionality. No dependencies or parameters are required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(the_description \"Deep neural network module. It allows to load models from different frameworks and to make forward pass\")\n```\n\n----------------------------------------\n\nTITLE: Generating OpenCV for Windows Store 8.0 ARM using CMake\nDESCRIPTION: Invokes CMake directly using the Visual Studio 2013 ARM generator to create project files for OpenCV targeting Windows Store 8.0 on the ARM architecture. Specifies the system name (WindowsStore) and older system version (8.0).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Visual Studio 12 2013 ARM\" -DCMAKE_SYSTEM_NAME=WindowsStore -DCMAKE_SYSTEM_VERSION=8.0 <path-to-source>\n```\n\n----------------------------------------\n\nTITLE: Configuring JNI OpenCV Build Environment - CMake\nDESCRIPTION: This CMake script snippet sets up the project for building a JNI native library with OpenCV dependencies for Android. It handles selecting OpenCV components based on whether the SDK or AAR (Maven) source is used, and sets appropriate CMake cache variables and project names. Prerequisites include a properly installed OpenCV package, and optionally, defined OPENCV_FROM_SDK and OPENCV_VERSION_MAJOR variables. Inputs: environment variables; Outputs: target configuration and selected OpenCV components.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-4-opencl/jni/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.6)\n\nset(target JNIpart)\nproject(${target} CXX)\n\nif (OPENCV_FROM_SDK)\n  message(STATUS \"Using OpenCV from local SDK\")\n  set(ANDROID_OPENCV_COMPONENTS \"opencv_java\" CACHE STRING \"\")\nelse()\n  message(STATUS \"Using OpenCV from AAR (Maven repo)\")\n  set(ANDROID_OPENCV_COMPONENTS \"OpenCV::opencv_java${OPENCV_VERSION_MAJOR}\" CACHE STRING \"\")\nendif()\n\nmessage(STATUS \"ANDROID_ABI=${ANDROID_ABI}\")\nfind_package(OpenCV REQUIRED COMPONENTS ${ANDROID_OPENCV_COMPONENTS})\nfind_package(OpenCL QUIET)\n\n```\n\n----------------------------------------\n\nTITLE: Building and Installing OpenCV with CMake\nDESCRIPTION: Build OpenCV using CMake within the specified build directory and install it by copying compiled artifacts to the install folder. Requires prior configuration with CMake and the Ninja build system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncmake --build   build4-full_arm64\nsudo cmake --install build4-full_arm64\n```\n\n----------------------------------------\n\nTITLE: Defining OpenCV Stitching Module in CMake\nDESCRIPTION: Defines the stitching module with its required and optional dependencies. This includes core OpenCV modules, CUDA-related modules, and additional dependencies. It also specifies Python wrapping.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/stitching/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nocv_define_module(stitching opencv_imgproc opencv_features2d opencv_calib3d opencv_flann\n                  OPTIONAL opencv_cudaarithm opencv_cudawarping opencv_cudafeatures2d opencv_cudalegacy opencv_cudaimgproc ${STITCHING_CONTRIB_DEPS}\n                  WRAP python)\n```\n\n----------------------------------------\n\nTITLE: Checking for unistd.h Header in Zlib CMake Configuration\nDESCRIPTION: Checks for the presence of the unistd.h header file on non-MSVC platforms. This is used to determine system capabilities for Zlib compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT MSVC)\n  check_include_file(unistd.h Z_HAVE_UNISTD_H)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating zconf.h Configuration File\nDESCRIPTION: Macro to generate zconf.h configuration file from template, replacing specific preprocessor directives with CMake variables for platform-specific configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_35\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(generate_cmakein input output)\n    file(REMOVE ${output})\n    file(STRINGS ${input} _lines)\n    foreach(_line IN LISTS _lines)\n        string(REGEX REPLACE \"#ifdef HAVE_UNISTD_H.*\" \"@ZCONF_UNISTD_LINE@\" _line \"${_line}\")\n        string(REGEX REPLACE \"#ifdef NEED_PTRDIFF_T.*\" \"@ZCONF_PTRDIFF_LINE@\" _line \"${_line}\")\n        if(NEED_PTRDIFF_T)\n            string(REGEX REPLACE \"typedef PTRDIFF_TYPE\" \"typedef @PTRDIFF_TYPE@\" _line \"${_line}\")\n        endif()\n        file(APPEND ${output} \"${_line}\\n\")\n    endforeach()\nendmacro(generate_cmakein)\n```\n\n----------------------------------------\n\nTITLE: OpenCV Apps Registration\nDESCRIPTION: Registers specific OpenCV applications to be built using the ocv_add_app macro.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nocv_add_app(annotation)\nocv_add_app(visualisation)\nocv_add_app(interactive-calibration)\nocv_add_app(version)\nocv_add_app(model-diagnostics)\n```\n\n----------------------------------------\n\nTITLE: Adding OpenCV Python Path to PYTHONPATH Environment Variable (Shell)\nDESCRIPTION: Command to append the directory containing the installed OpenCV Python module (`/usr/local/lib/python2.7/site-packages`) to the `PYTHONPATH` environment variable. This allows the Python interpreter to find the `cv2` module. It's recommended to add this line to a shell configuration file like `~/.bashrc` for persistence across sessions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nexport PYTHONPATH=$PYTHONPATH:/usr/local/lib/python2.7/site-packages\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Assets for OpenCV\nDESCRIPTION: Configures JavaScript assets by either using built opencv.js or a specified location, and sets up Haar cascade files for face and eye detection tutorials.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_opencv_js)\n    set(ocv_js_dir \"${CMAKE_BINARY_DIR}/bin\")\n    set(ocv_js \"opencv.js\")\n    list(APPEND js_assets \"${ocv_js_dir}/${ocv_js}\")\n  elseif(DEFINED OPENCV_JS_LOCATION)\n    list(APPEND js_assets \"${OPENCV_JS_LOCATION}\")\n  endif()\n\n  # copy haar cascade files\n  set(haar_cascade_files \"\")\n  set(data_harrcascades_path \"${OpenCV_SOURCE_DIR}/data/haarcascades/\")\n  list(APPEND js_tutorials_assets_deps \"${data_harrcascades_path}/haarcascade_frontalface_default.xml\" \"${data_harrcascades_path}/haarcascade_eye.xml\")\n  list(APPEND js_assets \"${data_harrcascades_path}/haarcascade_frontalface_default.xml\" \"${data_harrcascades_path}/haarcascade_eye.xml\")\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum Inliers via Command Line Argument (Bash/Shell)\nDESCRIPTION: Provides an example command-line usage for running the C++ tutorial executable (`cpp-tutorial-pnp_detection`). It demonstrates how to override the default minimum number of inliers required for the Kalman Filter update step by passing the `--inliers=20` argument. This value would likely be parsed by the C++ application and used to initialize or update the `minInliersKalman` variable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n./cpp-tutorial-pnp_detection --inliers=20\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Framework for iOS in Bash\nDESCRIPTION: Command to build the OpenCV framework for iOS using the provided Python script.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/ios_install/ios_install.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/<my_working_directory>\npython opencv/platforms/ios/build_framework.py ios\n```\n\n----------------------------------------\n\nTITLE: Adjusting CPU Optimization Levels in OpenCV\nDESCRIPTION: Enables configuring CPU optimization levels and instruction set selection. Provides options for SSE3 by default, with advanced options for enabling, disabling, or adjusting various optimization settings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCPU_BASELINE=AVX2 ../opencv\n```\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCPU_DISPATCH=AVX,AVX2 ../opencv\n```\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCPU_DISPATCH= ../opencv\n```\n\nLANGUAGE: sh\nCODE:\n```\n# disable universal intrinsics\ncmake -DCV_ENABLE_INTRINSICS=OFF ../opencv\n# disable all possible built-in optimizations\ncmake -DCV_DISABLE_OPTIMIZATION=ON ../opencv\n```\n\n----------------------------------------\n\nTITLE: Defining Source and Header File Collections for DNN Module - CMake\nDESCRIPTION: Uses CMake 'file(GLOB_RECURSE ...)' to gather all source files (*.cpp), internal headers (*.hpp, *.h), and sets up the plugin source list. Applies filters to exclude backend and importer source code not intended for the default build. Inputs: Directory paths and file patterns; outputs: file lists for DNN module compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_21\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE dnn_srcs\n     \"${CMAKE_CURRENT_LIST_DIR}/src/*.cpp\"\n)\nfile(GLOB_RECURSE dnn_int_hdrs\n     \"${CMAKE_CURRENT_LIST_DIR}/src/*.hpp\"\n     \"${CMAKE_CURRENT_LIST_DIR}/src/*.h\"\n)\nset(dnn_plugin_srcs ${dnn_srcs} ${dnn_int_hdrs})\nocv_list_filterout_ex(dnn_plugin_srcs\n    \"/src/dnn.cpp$|/src/dnn_utils.cpp$|/src/dnn_read.cpp$|/src/registry.cpp$|/src/backend.cpp$\"\n    # importers\n    \"/src/(caffe|darknet|onnx|tensorflow|torch)/\"\n    # executors\n    \"/src/(cuda|cuda4dnn|ocl4dnn|vkcom|webnn)/\"\n)\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Conditional Logic Implementation in C++\nDESCRIPTION: Complex nested conditional logic for checking pixel values against thresholds in different offsets. Uses goto statements for control flow between different detection scenarios (structured and homogeneous).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_28\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset2] < c_b)\n  if(ptr[offset3] < c_b)\n    if(ptr[offset4] < c_b)\n      if(ptr[offset1] < c_b)\n        if(ptr[offset6] < c_b)\n          goto success_structured;\n        else\n          goto structured;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset8] < c_b)\n            goto success_structured;\n          else\n            goto structured;\n        else\n          goto structured;\n    else\n      goto structured;\n  else\n    goto structured;\nelse\n  goto structured;\n```\n\n----------------------------------------\n\nTITLE: Defining and Using a Macro to Check for Header Files in CMake\nDESCRIPTION: Includes the `CheckIncludeFile` CMake module and defines a macro `ensure_file_include` to check for the existence of specified C header files. The macro takes the filename, a variable to store the result (e.g., `HAVE_STRING_H`), and a mandatory status (YES/NO). It prints informative messages based on whether the file is found and if it's mandatory. Subsequently, it uses this macro to check for several standard C headers required by OpenJPEG.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\n#-----------------------------------------------------------------------------\n# opj_config.h generation (1/2)\n\n# Check if some include files are provided by the system\n# These files are mandatory, so if they are not provided OpenJPEG library can't be built\ninclude(CheckIncludeFile)\nmacro(ensure_file_include INCLUDE_FILENAME VARIABLE_NAME MANDATORY_STATUS)\n  check_include_file(${INCLUDE_FILENAME} ${VARIABLE_NAME})\n  if(NOT ${VARIABLE_NAME})\n    if(${MANDATORY_STATUS})\n      message(STATUS \"The file '${INCLUDE_FILENAME}' is mandatory for OpenJPEG build, but not found on your system\")\n      return()\n    else()\n      message(STATUS \"The file '${INCLUDE_FILENAME}' is optional for OpenJPEG build and not found on your system.\" \n              \" Internal implementation will be used.\")\n    endif()\n  endif()\nendmacro()\n\nensure_file_include(\"string.h\"   HAVE_STRING_H YES)\nensure_file_include(\"memory.h\"   HAVE_MEMORY_H YES)\nensure_file_include(\"stdlib.h\"   HAVE_STDLIB_H YES)\nensure_file_include(\"stdio.h\"    HAVE_STDIO_H  YES)\nensure_file_include(\"math.h\"     HAVE_MATH_H   YES)\nensure_file_include(\"float.h\"    HAVE_FLOAT_H  YES)\nensure_file_include(\"time.h\"     HAVE_TIME_H   YES)\nensure_file_include(\"stdarg.h\"   HAVE_STDARG_H YES)\nensure_file_include(\"ctype.h\"    HAVE_CTYPE_H  YES)\nensure_file_include(\"assert.h\"   HAVE_ASSERT_H YES)\n\n# For the following files, we provide an alternative, they are not mandatory\nensure_file_include(\"stdint.h\"   OPJ_HAVE_STDINT_H   NO)\nensure_file_include(\"inttypes.h\" OPJ_HAVE_INTTYPES_H NO)\n```\n\n----------------------------------------\n\nTITLE: Creating Build Directory for OpenCV\nDESCRIPTION: Commands to create and enter the build directory for OpenCV compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p build && cd build\n```\n\n----------------------------------------\n\nTITLE: Creating Project Directory for SBT Sample (Bash)\nDESCRIPTION: This sequence of Bash commands navigates to a chosen location outside the OpenCV source directory and creates a new directory named 'JavaSample' to house the SBT-based Java project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncd <somewhere outside opencv>\nmkdir JavaSample\n```\n\n----------------------------------------\n\nTITLE: Formatting Image-Annotation Pair List (Plaintext)\nDESCRIPTION: Illustrates the required format for a text file listing pairs of image file paths and their corresponding annotation XML file paths. This file serves as input for Caffe's data creation scripts (`create_data.sh`, `create_annoset.py`) to build the LMDB database.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/face_detector/how_to_train_face_detector.txt#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nimages_val/0.jpg annotations_val/0.jpg.xml\n```\n\n----------------------------------------\n\nTITLE: Declaring an Empty cv::Mat Variable for Edge Detection in C++\nDESCRIPTION: This snippet shows the declaration of an empty cv::Mat object intended to store the results of Canny edge detection. It highlights where to set a Visual Studio breakpoint for using the Image Watch extension and serves as a key step in the image processing workflow. No input parameters are needed for this statement, but the variable is later populated using an OpenCV function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_visual_studio_image_watch/windows_visual_studio_image_watch.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nMat edges;\n```\n\n----------------------------------------\n\nTITLE: Estimating Homography using OpenCV's findHomography - Java\nDESCRIPTION: The code demonstrates calling OpenCV Java’s findHomography method to estimate a projective transformation between two chessboard images. Both input and output are Mat objects containing the matched points and resulting 3x3 matrix, respectively. RANSAC or other methods may be selected for robustness.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/features2D/Homography/PerspectiveCorrection.java estimate-homography\n```\n\n----------------------------------------\n\nTITLE: Installing Zlib Library for Static Builds in CMake\nDESCRIPTION: Installs the Zlib library for static builds as part of the OpenCV modules. This ensures the static library is available to users of the OpenCV project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(${ZLIB_LIBRARY} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Draw Final Matches and Output in OpenCV using Python\nDESCRIPTION: Python snippet to visualize and save the result of matched keypoints using OpenCV, including outputting match statistics.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nsamples/python/tutorial_code/features2D/akaze_matching/AKAZE_match.py draw final matches\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Structured Pattern Checking in C++\nDESCRIPTION: Implementation of the structured pattern checking in FAST corner detection algorithm. This section increments the x-coordinate and tests pixel values against threshold values to determine if the current pixel is a corner point by examining its surrounding pixels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nstructured:\n{\n  x++;\n  if(x > xsizeB)\n      break;\n  else\n  {\n      const unsigned char* const ptr = img.ptr() + y*width + x;\n      const int cb = *ptr + threshold;\n      const int c_b = *ptr - threshold;\n      if(ptr[offset0] > cb)\n        if(ptr[offset5] > cb)\n          if(ptr[offset2] > cb)\n            if(ptr[offset9] > cb)\n              if(ptr[offset1] > cb)\n                if(ptr[offset6] > cb)\n                  if(ptr[offset3] > cb)\n                    if(ptr[offset4] > cb)\n                      goto success_structured;\n                    else\n                      if(ptr[offset10] > cb)\n                        if(ptr[offset11] > cb)\n                          goto success_structured;\n                        else\n                          goto structured;\n                      else\n                        goto structured;\n                  else\n                    if(ptr[offset8] > cb)\n                      if(ptr[offset10] > cb)\n                        if(ptr[offset11] > cb)\n                          goto success_structured;\n                        else\n                          if(ptr[offset4] > cb)\n                            if(ptr[offset7] > cb)\n                              goto success_structured;\n                            else\n                              goto structured;\n                          else\n                            goto structured;\n                      else\n                        goto structured;\n                    else\n                      goto structured;\n                else\n                  if(ptr[offset11] > cb)\n                    if(ptr[offset3] > cb)\n                      if(ptr[offset4] > cb)\n                        goto success_structured;\n                      else\n                        if(ptr[offset10] > cb)\n                          goto success_structured;\n                        else\n                          goto structured;\n                    else\n                      if(ptr[offset8] > cb)\n                        if(ptr[offset10] > cb)\n                          goto success_structured;\n                        else\n                          goto structured;\n                      else\n                        goto structured;\n                  else\n                    goto structured;\n              else\n                if(ptr[offset6] > cb)\n                  if(ptr[offset7] > cb)\n                    if(ptr[offset8] > cb)\n                      if(ptr[offset4] > cb)\n                        if(ptr[offset3] > cb)\n                          goto success_structured;\n                        else\n                          if(ptr[offset10] > cb)\n                            goto success_structured;\n                          else\n                            goto structured;\n                      else\n                        if(ptr[offset10] > cb)\n                          if(ptr[offset11] > cb)\n                            goto success_structured;\n                          else\n                            goto structured;\n                        else\n                          goto structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset1] > cb)\n                    if(ptr[offset6] > cb)\n                      goto success_structured;\n                    else\n                      if(ptr[offset11] > cb)\n                        goto success_structured;\n                      else\n                        goto structured;\n                  else\n                    if(ptr[offset6] > cb)\n                      if(ptr[offset7] > cb)\n                        if(ptr[offset8] > cb)\n                          goto success_structured;\n                        else\n                          goto structured;\n                      else\n                        goto structured;\n                    else\n                      goto structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n          else\n            if(ptr[offset9] > cb)\n              if(ptr[offset7] > cb)\n                if(ptr[offset8] > cb)\n                  if(ptr[offset1] > cb)\n                    if(ptr[offset10] > cb)\n                      if(ptr[offset11] > cb)\n                        goto success_structured;\n                      else\n                        if(ptr[offset6] > cb)\n                          if(ptr[offset4] > cb)\n                            goto success_structured;\n                          else\n                            goto structured;\n                        else\n                          goto structured;\n                    else\n                      if(ptr[offset6] > cb)\n                        if(ptr[offset3] > cb)\n                          if(ptr[offset4] > cb)\n                            goto success_structured;\n                          else\n                            goto structured;\n                        else\n```\n\n----------------------------------------\n\nTITLE: Configuring libjasper Project in CMake for OpenCV\nDESCRIPTION: This snippet sets up the project for libjasper library and adds definitions to exclude support for various image formats.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjasper/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(${JASPER_LIBRARY})\n\nadd_definitions(-DEXCLUDE_MIF_SUPPORT -DEXCLUDE_PNM_SUPPORT -DEXCLUDE_BMP_SUPPORT -DEXCLUDE_RAS_SUPPORT  -DEXCLUDE_JPG_SUPPORT -DEXCLUDE_PGX_SUPPORT)\n\nocv_include_directories(${CMAKE_CURRENT_SOURCE_DIR})\n\nfile(GLOB lib_srcs *.c)\nfile(GLOB lib_hdrs *.h)\nfile(GLOB lib_ext_hdrs jasper/*.h)\n```\n\n----------------------------------------\n\nTITLE: Marking and Including Code Snippets from Files - Doxygen Snippet\nDESCRIPTION: Explains marking code regions in a source file for later inclusion in documentation with @snippet. Code is bracketed by '//! [label]' and later referenced by filename and label. This allows targeted inclusion of only relevant portions. Inputs are code region delimiters and snippet command; output is extracted code region in documentation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n//! [var_init]\nint a = 0;\n//! [var_init]\n```\n\nLANGUAGE: markdown\nCODE:\n```\n@snippet samples/cpp/test.cpp var_init\n```\n\n----------------------------------------\n\nTITLE: GCC Compiler-Specific Configuration\nDESCRIPTION: Configures GCC-specific compiler flags for performance optimization, including visibility settings and inline parameters. Includes version-specific compiler flag adjustments.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/carotene/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCC)\n    set(CMAKE_CXX_FLAGS \"-fvisibility=hidden ${CMAKE_CXX_FLAGS}\")\n\n    # allow more inlines - these parameters improve performance for:\n    # - matchTemplate about 5-10%\n    # - goodFeaturesToTrack 10-20%\n    # - cornerHarris 30% for some cases\n    if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS \"10.0.0\")\n        set_source_files_properties(${carotene_sources} COMPILE_FLAGS \"--param ipcp-unit-growth=100000 --param inline-unit-growth=100000 --param large-stack-frame-growth=5000\")\n    else()\n        set_source_files_properties(${carotene_sources} COMPILE_FLAGS \"--param ipa-cp-unit-growth=100000 --param inline-unit-growth=100000 --param large-stack-frame-growth=5000\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties for the OpenJPEG Library in CMake\nDESCRIPTION: Configures properties for the OpenJPEG library target (identified by `${OPENJPEG_LIBRARY_NAME}`, which is `libopenjp2`). It sets the output name, applies the standard OpenCV debug postfix (`OPENCV_DEBUG_POSTFIX`) to debug builds, defines PDB (Program Database) names for debugging symbols on Windows, and specifies the output directory for the built library artifacts using the `3P_LIBRARY_OUTPUT_PATH` variable (likely defined in the parent OpenCV CMake configuration).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${OPENJPEG_LIBRARY_NAME}\n  PROPERTIES\n    OUTPUT_NAME              ${OPENJPEG_LIBRARY_NAME}\n    DEBUG_POSTFIX            \"${OPENCV_DEBUG_POSTFIX}\"\n    COMPILE_PDB_NAME         ${OPENJPEG_LIBRARY_NAME}\n    COMPILE_PDB_NAME_DEBUG   \"${OPENJPEG_LIBRARY_NAME}${OPENCV_DEBUG_POSTFIX}\"\n    ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n)\n```\n\n----------------------------------------\n\nTITLE: Including CUDA Include Directories in CMake\nDESCRIPTION: Conditionally adds the CUDA include directories specified by the `CUDA_INCLUDE_DIRS` variable to the build configuration using `ocv_include_directories` if CUDA support is available (`HAVE_CUDA` is true).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_CUDA)\n  ocv_include_directories(${CUDA_INCLUDE_DIRS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenCV Include Directory in Visual Studio\nDESCRIPTION: Adds the OpenCV include directory to the project's additional include directories using an environment variable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_visual_studio_opencv/windows_visual_studio_opencv.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$(OPENCV_DIR)\\..\\..\\include\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific C++ Warnings in CMake\nDESCRIPTION: Checks if the custom `ocv_warnings_disable` command exists. If it does, it calls the command to disable specific C++ warnings (`-Wsuggest-override`, `-Winconsistent-missing-override`) for the C++ compiler flags (`CMAKE_CXX_FLAGS`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(COMMAND ocv_warnings_disable)\n  ocv_warnings_disable(CMAKE_CXX_FLAGS -Wsuggest-override -Winconsistent-missing-override)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Project and Recursively Including Dependencies for TAPI Samples - CMake\nDESCRIPTION: Configures the project under the name tapi_samples and uses ocv_include_modules_recurse to include all modules needed by the TAPI samples. It also dynamically discovers all .cpp files in the current directory and prepares them for sample build. The configuration relies on the earlier definition of OPENCV_TAPI_SAMPLES_REQUIRED_DEPS to include appropriate modules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/tapi/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nproject(tapi_samples)\nocv_include_modules_recurse(${OPENCV_TAPI_SAMPLES_REQUIRED_DEPS})\nfile(GLOB all_samples RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} *.cpp)\n```\n\n----------------------------------------\n\nTITLE: Configuration options for building OpenCV as shared libraries\nDESCRIPTION: Additional CMake configuration parameters needed to build OpenCV as shared libraries instead of static libraries. Enables shared libraries and zlib dependency.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/wince/readme.md#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n-DBUILD_SHARED_LIBS=ON `\n-DBUILD_ZLIB=ON\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific TBB Library Configuration\nDESCRIPTION: Configures platform-specific settings for the TBB library. For Windows, this creates the DLL export definition file, while for Unix platforms it links against system libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/tbb/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif (WIN32)\n  if (ARM)\n    set(platform_macro /D_M_ARM=1)\n  endif()\n\n  add_custom_command(TARGET tbb\n                     PRE_BUILD\n                     COMMAND ${CMAKE_C_COMPILER} /nologo /TC /EP ${tbb_src_dir}\\\\src\\\\tbb\\\\win32-tbb-export.def /DTBB_NO_LEGACY=1 /D_CRT_SECURE_NO_DEPRECATE /D__TBB_BUILD=1 ${platform_macro} /I${tbb_src_dir}\\\\src /I${tbb_src_dir}\\\\include > \"${tbb_src_dir}\\\\src\\\\tbb\\\\tbb.def\"\n                     WORKING_DIRECTORY ${tbb_src_dir}\\\\src\\\\tbb\n                     COMMENT \"Generating tbb.def file\" VERBATIM\n                    )\n\n  set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} /DEF:${tbb_src_dir}/src/tbb/tbb.def /DLL /MAP /fixed:no /INCREMENTAL:NO\")\nelse()\n  target_link_libraries(tbb c m dl)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Disabling RVV Support in OpenCV RISC-V Build\nDESCRIPTION: This CMake option disables RVV (RISC-V Vector Extension) support when building OpenCV for RISC-V targets that do not support it.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/arch/riscv/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n-D WITH_RVV=OFF\n```\n\n----------------------------------------\n\nTITLE: Allocating/Resizing Output Mat using create() in OpenCV in Java\nDESCRIPTION: Allocates or reinitializes a Mat in Java with create, matching size and type of input Mat. Ensures that output Mat is ready for operation results. Input is the output Mat, target rows, cols, and type.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_26\n\nLANGUAGE: Java\nCODE:\n```\noutput.create(input.rows(), input.cols(), input.type());\n```\n\n----------------------------------------\n\nTITLE: Waiting for User Input to Exit OpenCV Application\nDESCRIPTION: Code to keep the application window open until the user presses a key, allowing them to view the images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_equalization/histogram_equalization.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nwaitKey(0);\n```\n\nLANGUAGE: Java\nCODE:\n```\nwaitKey(0);\n```\n\nLANGUAGE: Python\nCODE:\n```\ncv.waitKey(0)\n```\n\n----------------------------------------\n\nTITLE: Generating Linearly Separable Training Data for SVM (C++)\nDESCRIPTION: Generates training data that is linearly separable using random points from two classes with different probability density functions. This creates the first part of the dataset needed for demonstrating SVMs.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n// Set up the linearly separable part of the training data\nint labels[2] = {1, -1};\nMat trainData(2 * NTRAINING_SAMPLES, 2, CV_32FC1);\nMat trainLabels(2 * NTRAINING_SAMPLES, 1, CV_32SC1);\nRNG rng(100); // Random value generation class\n\n// Generate training samples\nfor(int i = 0; i < 2; ++i)\n{\n    Mat trainClass = trainData.rowRange(i * NTRAINING_SAMPLES, (i + 1) * NTRAINING_SAMPLES);\n    // The x coordinate of the points is in [0, 0.4 * WIDTH] or [0.6 * WIDTH, WIDTH]\n    // The y coordinate of the points is in [0, 0.4 * HEIGHT] or [0.6 * HEIGHT, HEIGHT]\n    Mat center = trainClass.colRange(0, 1);\n    if (i == 0)\n    {\n        center = center * 0.2f + 0.2f;\n    }\n    else\n    {\n        center = center * 0.2f + 0.6f;\n    }\n    rng.fill(trainClass.colRange(1, 2), RNG::UNIFORM, Scalar(0), Scalar(1));\n\n    // Set the labels of the classes. They are integers from the set {1, -1}\n    trainLabels.rowRange(i * NTRAINING_SAMPLES, (i + 1) * NTRAINING_SAMPLES)\n        .setTo(Scalar(labels[i]));\n}\n```\n\n----------------------------------------\n\nTITLE: AVFoundation Configuration Option in OpenCV\nDESCRIPTION: Defines the WITH_AVFOUNDATION build option for Apple platforms to enable the AVFoundation framework for camera frame capture and video encoding/decoding.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_15\n\nLANGUAGE: markdown\nCODE:\n```\n`WITH_AVFOUNDATION` (Apple; default: _ON_)\n```\n\n----------------------------------------\n\nTITLE: Building Halide on Windows using CMake and MSBuild\nDESCRIPTION: This snippet provides instructions for building the Halide project on Windows, using CMake and MSBuild in a Visual Studio environment. This process excludes building tests, apps, and tutorials.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_halide/dnn_halide.markdown#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ncd halide_root\nmkdir build && cd build\ncmake.exe -DLLVM_DIR=\\\\path-to-llvm-install\\\\lib\\\\cmake\\\\llvm -DLLVM_VERSION=40 -DWITH_TESTS=OFF -DWITH_APPS=OFF -DWITH_TUTORIALS=OFF -DCMAKE_BUILD_TYPE=Release -G \"Visual Studio 14 Win64\" ..\nMSBuild.exe /m:4 /t:Build /p:Configuration=Release .\\\\ALL_BUILD.vcxproj\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading OpenCV Mat Objects to XML/YAML/JSON - OpenCV C++\nDESCRIPTION: These snippets show how to write and read Mat objects (matrices/images) to XML/YAML/JSON files using OpenCV's FileStorage in C++. Writing is performed with the << operator and reading with the >> operator or [] operator. Requires OpenCV and a properly created cv::Mat object. This can handle matrices of any supported type.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nMat mat = Mat::eye(3, 3, CV_64F);\\nfs << \"matrix\" << mat;\\n\\nfs[\"matrix\"] >> mat;\n```\n\n----------------------------------------\n\nTITLE: Gathering Source Files and Metadata for Doxygen from OpenCV Modules in CMake\nDESCRIPTION: Initializes lists to store paths for include files, documentation, BibTeX files, samples, tutorials, and HAL interfaces. It iterates through main and extra OpenCV modules, checks against a blacklist, and appends relevant file/directory paths to the corresponding lists. It conditionally excludes CUDA-related headers, gathers sample and tutorial files, generates a markdown file listing contrib tutorials, collects HAL replacement headers and BibTeX files, and builds lists of main and extra module references for Doxygen.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\n# gathering headers\nset(paths_include)\nset(paths_doc)\nset(paths_bib)\nset(paths_sample)\nset(paths_tutorial)\nset(paths_hal_interface)\nset(refs_main)\nset(refs_extra)\nset(deps)\nforeach(m ${OPENCV_MODULES_MAIN} ${OPENCV_MODULES_EXTRA})\n  set(the_module \"${m}\")\n  if(NOT the_module MATCHES \"^opencv_\")\n    set(the_module \"opencv_${m}\")\n  endif()\n  list(FIND blacklist ${m} _pos)\n  if(NOT EXISTS \"${OPENCV_MODULE_${the_module}_LOCATION}/include\"\n      AND NOT EXISTS \"${OPENCV_MODULE_${the_module}_LOCATION}/doc\"\n  )\n    set(_pos -2)  # blacklist\n  endif()\n  if(${_pos} EQUAL -1)\n    list(APPEND CMAKE_DOXYGEN_ENABLED_SECTIONS \"HAVE_opencv_${m}\")\n    # include folder\n    set(header_dir \"${OPENCV_MODULE_opencv_${m}_LOCATION}/include\")\n    if(EXISTS \"${header_dir}\")\n      list(APPEND paths_include \"${header_dir}\")\n      list(APPEND deps ${header_dir})\n      if(OPENCV_DOCS_EXCLUDE_CUDA)\n        if(EXISTS \"${OPENCV_MODULE_opencv_${m}_LOCATION}/include/opencv2/${m}/cuda\")\n          list(APPEND CMAKE_DOXYGEN_EXCLUDE_LIST \"${OPENCV_MODULE_opencv_${m}_LOCATION}/include/opencv2/${m}/cuda\")\n        endif()\n        file(GLOB list_cuda_files \"${OPENCV_MODULE_opencv_${m}_LOCATION}/include/opencv2/${m}/*cuda*.hpp\")\n        if(list_cuda_files)\n          list(APPEND CMAKE_DOXYGEN_EXCLUDE_LIST ${list_cuda_files})\n        endif()\n      endif()\n    endif()\n    # doc folder\n    set(docs_dir \"${OPENCV_MODULE_opencv_${m}_LOCATION}/doc\")\n    if(EXISTS \"${docs_dir}\")\n      list(APPEND paths_doc \"${docs_dir}\")\n      list(APPEND deps ${docs_dir})\n    endif()\n    # sample folder\n    set(sample_dir \"${OPENCV_MODULE_opencv_${m}_LOCATION}/samples\")\n    if(EXISTS \"${sample_dir}\")\n      list(APPEND paths_sample \"${sample_dir}\")\n      list(APPEND deps ${sample_dir})\n    endif()\n    # tutorial folder\n    set(tutorial_dir \"${OPENCV_MODULE_opencv_${m}_LOCATION}/tutorials\")\n    if(EXISTS \"${tutorial_dir}\")\n      list(APPEND paths_tutorial \"${tutorial_dir}\")\n      list(APPEND deps ${tutorial_dir})\n\n      # tutorial reference entry\n      file(GLOB tutorials RELATIVE \"${OPENCV_MODULE_opencv_${m}_LOCATION}\" \"${tutorial_dir}/*.markdown\")\n      foreach (t ${tutorials})\n        if (NOT DEFINED CMAKE_DOXYGEN_TUTORIAL_CONTRIB_ROOT)\n          set(CMAKE_DOXYGEN_TUTORIAL_CONTRIB_ROOT \"- @ref tutorial_contrib_root\")\n          set(tutorial_contrib_root \"${CMAKE_CURRENT_BINARY_DIR}/contrib_tutorials.markdown\")\n          file(WRITE \"${tutorial_contrib_root}\"\n            \"Tutorials for contrib modules {#tutorial_contrib_root}\\n\"\n            \"=============================\\n\")\n        endif()\n        file(STRINGS \"${OPENCV_MODULE_opencv_${m}_LOCATION}/${t}\" tutorial_id LIMIT_COUNT 1 REGEX \".*{#[^}]+}\")\n        string(REGEX REPLACE \".*{#([^}]+)}\" \"\\\\1\" tutorial_id \"${tutorial_id}\")\n        file(APPEND \"${tutorial_contrib_root}\" \"- ${m}. @subpage ${tutorial_id}\\n\")\n      endforeach()\n    endif()\n    # HAL replacement file\n    set(replacement_header \"${OPENCV_MODULE_opencv_${m}_LOCATION}/src/hal_replacement.hpp\")\n    if(EXISTS \"${replacement_header}\")\n      list(APPEND paths_hal_interface \"${replacement_header}\")\n    endif()\n\n    # BiBTeX file\n    set(bib_file \"${docs_dir}/${m}.bib\")\n    if(EXISTS \"${bib_file}\")\n      set(paths_bib \"${paths_bib} ${bib_file}\")\n      list(APPEND deps ${bib_file})\n    endif()\n    # Reference entry\n    set(one_ref \"\\t- ${m}. @ref ${m}\\n\")\n    list(FIND OPENCV_MODULES_EXTRA ${m} _pos)\n    if(${_pos} EQUAL -1)\n      set(refs_main \"${refs_main}${one_ref}\")\n    else()\n      set(refs_extra \"${refs_extra}${one_ref}\")\n    endif()\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions and Module-Specific Configuration (CMake)\nDESCRIPTION: Adds a private compile definition `OPENCV_MODULE_IS_PART_OF_WORLD=1` to the `opencv_world` target using `ocv_target_compile_definitions`. This definition allows source code to conditionally compile sections specific to the world build. It then conditionally calls `ocv_imgcodecs_configure_target()` and `ocv_highgui_configure_target()` if the respective modules (`opencv_imgcodecs`, `opencv_highgui`) are being built and included in the `opencv_world` target, enabling module-specific post-creation configuration steps.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/world/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nocv_target_compile_definitions(${the_module} PRIVATE OPENCV_MODULE_IS_PART_OF_WORLD=1)\n\nif(BUILD_opencv_imgcodecs AND OPENCV_MODULE_opencv_imgcodecs_IS_PART_OF_WORLD)\n  ocv_imgcodecs_configure_target()\nendif()\nif(BUILD_opencv_highgui AND OPENCV_MODULE_opencv_highgui_IS_PART_OF_WORLD)\n  ocv_highgui_configure_target()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Displaying OpenCL, ONNX, and CANN Integration Status in OpenCV Build\nDESCRIPTION: Checks and displays the status of OpenCL, ONNX, and CANN integrations, showing specific features, include paths, and library information when available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_27\n\nLANGUAGE: cmake\nCODE:\n```\nif(WITH_OPENCL OR HAVE_OPENCL)\n  ocv_build_features_string(opencl_features\n    IF HAVE_OPENCL_SVM THEN \"SVM\"\n    IF HAVE_CLAMDFFT THEN \"AMDFFT\"\n    IF HAVE_CLAMDBLAS THEN \"AMDBLAS\"\n    IF HAVE_OPENCL_D3D11_NV THEN \"NVD3D11\"\n    IF HAVE_VA_INTEL THEN \"INTELVA\"\n    ELSE \"no extra features\")\n  status(\"\")\n  status(\"  OpenCL:\"     HAVE_OPENCL   THEN   \"YES (${opencl_features})\" ELSE \"NO\")\n  if(HAVE_OPENCL)\n    status(\"    Include path:\"  OPENCL_INCLUDE_DIRS THEN \"${OPENCL_INCLUDE_DIRS}\" ELSE \"NO\")\n    status(\"    Link libraries:\"       OPENCL_LIBRARIES THEN \"${OPENCL_LIBRARIES}\" ELSE \"Dynamic load\")\n  endif()\nendif()\n\nif(WITH_ONNX OR HAVE_ONNX)\n  status(\"\")\n  status(\"  ONNX:\"     HAVE_ONNX THEN \"YES\" ELSE \"NO\")\n  if(HAVE_ONNX)\n    status(\"    Include path:\"  ONNX_INCLUDE_DIR THEN \"${ONNX_INCLUDE_DIR}\" ELSE \"NO\")\n    status(\"    Link libraries:\" ONNX_LIBRARIES THEN \"${ONNX_LIBRARIES}\" ELSE \"NO\")\n  endif()\nendif()\n\nif(WITH_CANN)\n  status(\"\")\n  status(\"  CANN:\"    HAVE_CANN THEN \"YES\" ELSE \"NO\")\n  if(HAVE_CANN)\n    status(\"    Include path\"     CANN_INCLUDE_DIRS THEN \"${CANN_INCLUDE_DIRS}\" ELSE \"NO\")\n    status(\"    Link libraries:\"  CANN_LIBRARIES    THEN \"${CANN_LIBRARIES}\"    ELSE \"NO\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: CMake Hook for OpenCV Compiler and Linker Options\nDESCRIPTION: This snippet configures post-build and post-compiler hooks. It includes configurations for Python and CUDA support and sets compiler and linker options based on the environment, such as not using certain features on iOS or XROS.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nocv_cmake_hook(POST_CMAKE_BUILD_OPTIONS)\n\n# --- Python Support ---\nif(NOT IOS AND NOT XROS)\n  include(cmake/OpenCVDetectPython.cmake)\nendif()\n\ninclude(cmake/OpenCVCompilerOptions.cmake)\n\nocv_cmake_hook(POST_COMPILER_OPTIONS)\n\n# --- CUDA Support ---\nif(ENABLE_CUDA_FIRST_CLASS_LANGUAGE)\n  if(CMAKE_VERSION VERSION_LESS 3.18)\n    message(WARNING \\\"CUDA: First class language only supported for CMake versions >= 3.18, falling back to FindCUDA!\\\")\n    set(ENABLE_CUDA_FIRST_CLASS_LANGUAGE OFF CACHE BOOL \\\"Enable CUDA as a first class language, if enabled dependant projects will need to use CMake >= 3.18\\\" FORCE)\n  else()\n\n    # Check CUDA_PATH if supplied\n    if(UNIX AND CUDA_PATH AND NOT ENV{CUDA_PATH})\n      set(ENV{CUDA_PATH} ${CUDA_PATH})\n    elseif(WIN32 AND CUDA_PATH)\n      set(ENV{PATH} \\\"${CUDA_PATH}\\\\bin\\;$ENV{PATH}\\\")\n    endif()\n    include(CheckLanguage)\n    check_language(CUDA)\n\n    # Fallback to checking default locations\n    if(NOT CMAKE_CUDA_COMPILER)\n      # Checking windows default search...[truncated]\n```\n\n----------------------------------------\n\nTITLE: Configuration options for headless WEC2013 build\nDESCRIPTION: Comprehensive set of CMake configuration options for building OpenCV for headless WEC2013 environments. Disables various modules and features to reduce dependencies and resource requirements.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/wince/readme.md#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\n-DBUILD_EXAMPLES=OFF `\n-DBUILD_opencv_apps=OFF `\n-DBUILD_opencv_calib3d=OFF `\n-DBUILD_opencv_highgui=OFF `\n-DBUILD_opencv_features2d=OFF `\n-DBUILD_opencv_flann=OFF `\n-DBUILD_opencv_ml=OFF `\n-DBUILD_opencv_objdetect=OFF `\n-DBUILD_opencv_photo=OFF `\n-DBUILD_opencv_shape=OFF `\n-DBUILD_opencv_stitching=OFF `\n-DBUILD_opencv_superres=OFF `\n-DBUILD_opencv_ts=OFF `\n-DBUILD_opencv_video=OFF `\n-DBUILD_opencv_videoio=OFF `\n-DBUILD_opencv_videostab=OFF `\n-DBUILD_opencv_dnn=OFF `\n-DBUILD_opencv_java=OFF `\n-DBUILD_opencv_python2=OFF `\n-DBUILD_opencv_python3=OFF `\n-DBUILD_opencv_java_bindings_generator=OFF `\n-DBUILD_opencv_python_bindings_generator=OFF `\n-DBUILD_TIFF=OFF `\n-DCV_TRACE=OFF `\n-DWITH_OPENCL=OFF `\n-DHAVE_OPENCL=OFF `\n-DWITH_QT=OFF `\n-DWITH_GTK=OFF `\n-DWITH_QUIRC=OFF `\n-DWITH_JASPER=OFF `\n-DWITH_WEBP=OFF `\n-DWITH_PROTOBUF=OFF `\n-DBUILD_SHARED_LIBS=OFF `\n-DWITH_OPENEXR=OFF `\n-DWITH_TIFF=OFF `\n```\n\n----------------------------------------\n\nTITLE: Invoking Error Handlers in JPEG Library (C Macros)\nDESCRIPTION: Describes the C macros used within the JPEG library or application code to trigger the error handling mechanism. `ERREXITn(...)` is used for fatal errors, `WARNMSn(...)` for warnings about corrupt data, and `TRACEMSn(...)` for trace/informational messages. These macros store the message code and any printf-style parameters into the `cinfo->err` struct before calling `error_exit` or `emit_message`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_48\n\nLANGUAGE: c\nCODE:\n```\nERREXITn(...)\nWARNMSn(...)\nTRACEMSn(...)\n```\n\n----------------------------------------\n\nTITLE: Enabling Soft-Float Compilation Flag with CMake - Bash\nDESCRIPTION: Configures the OpenCV cross-compilation process with the SOFTFP flag enabled, which instructs the compiler to use software floating-point. Requires all CMake configuration prerequisites, and is intended for platforms where hardware floating-point is not supported. Adjust optional parameters as needed to suit platform constraints.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/arm_crosscompile_with_cmake.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncmake [<some optional parameters>] -DSOFTFP=ON -DCMAKE_TOOLCHAIN_FILE=<path to the OpenCV source directory>/platforms/linux/arm-gnueabi.toolchain.cmake <path to the OpenCV source directory>\n```\n\n----------------------------------------\n\nTITLE: Defining the Progress Monitoring Structure in libjpeg (C)\nDESCRIPTION: Defines the `jpeg_progress_mgr` structure in C, used to manage progress reporting in libjpeg. The library updates the fields (`pass_counter`, `pass_limit`, `completed_passes`, `total_passes`) which can be accessed within a user-defined callback function pointed to by `cinfo->progress->progress_monitor`. This allows applications to track the progress of compression or decompression operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_65\n\nLANGUAGE: c\nCODE:\n```\nstruct jpeg_progress_mgr {\n  void (*progress_monitor) (j_common_ptr cinfo);\n\n  long pass_counter;      /* work units completed in this pass */\n  long pass_limit;        /* total number of work units in this pass */\n  int completed_passes;   /* passes completed so far */\n  int total_passes;       /* total number of passes expected */\n};\n```\n\n----------------------------------------\n\nTITLE: Referencing Entities with @ref - Doxygen/Markdown\nDESCRIPTION: Shows explicit, named, implicit, and disabled references using the doxygen @ref command. Proper usage generates documentation links to classes, pages, or anchors. No dependencies except doxygen processing. Input is entity name or alias; output is hyperlink in documentation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\nExplicit reference: @ref MyClass\nExplicit named reference: @ref example_page \"Example page\"\nImplicit reference: cv::abc::MyClass1 or just MyClass1\nDisable implicit reference: %MyClass1\n```\n\n----------------------------------------\n\nTITLE: Library Target Configuration\nDESCRIPTION: Defines the library targets and their compilation settings, including platform-specific definitions and NEON support configuration. Creates both object and static library targets.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/carotene/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(carotene_objs OBJECT EXCLUDE_FROM_ALL\n  ${carotene_headers}\n  ${carotene_sources}\n)\n\nif(NOT CAROTENE_NS STREQUAL \"carotene\")\n    target_compile_definitions(carotene_objs PUBLIC \"-DCAROTENE_NS=${CAROTENE_NS}\")\nendif()\n\nif(WITH_NEON)\n    target_compile_definitions(carotene_objs PRIVATE \"-DWITH_NEON\")\nendif()\n\nif(MINGW)\n    target_compile_definitions(carotene_objs PRIVATE \"-D_USE_MATH_DEFINES=1\")\nendif()\n\n# we add dummy file to fix XCode build\nadd_library(carotene STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} \"$<TARGET_OBJECTS:carotene_objs>\" \"${CAROTENE_SOURCE_DIR}/dummy.cpp\")\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific Compiler Warnings in CMake for OpenJPEG\nDESCRIPTION: Uses the OpenCV macro `ocv_warnings_disable` to suppress specific compiler warnings (related to implicit conversions, unused variables, missing prototypes/declarations, and documentation) that might arise during the compilation of the OpenJPEG C code, ensuring a cleaner build output within the OpenCV environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nocv_warnings_disable(CMAKE_C_FLAGS\n    -Wimplicit-const-int-float-conversion  # clang\n    -Wunused-but-set-variable # clang15\n    -Wmissing-prototypes # clang, function opj_t1_ht_decode_cblk\n    -Wmissing-declarations # gcc, function opj_t1_ht_decode_cblk\n    -Wdocumentation # clang\n)\n```\n\n----------------------------------------\n\nTITLE: Adding CUDA Preprocessor Definition in CMake\nDESCRIPTION: Conditionally adds the preprocessor definition `-DHAVE_CUDA=1` to the compiler flags if either the `HAVE_CUDA` or `CUDA_FOUND` CMake variable is true, indicating that CUDA support is available and configured.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_CUDA OR CUDA_FOUND)\n  add_definitions(-DHAVE_CUDA=1)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV Repository Using Git\nDESCRIPTION: Command to clone the OpenCV repository using Git, including all branches and commit history.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/opencv/opencv.git\n```\n\n----------------------------------------\n\nTITLE: Adding UMD Wrapper for OpenCV.js in CMake\nDESCRIPTION: Sets up a custom command to create a UMD (Universal Module Definition) wrapper for the OpenCV.js file, allowing it to be used in various JavaScript environments.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/js/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(MODULE_JS_PATH \"${OpenCV_BINARY_DIR}/bin/${the_module}.js\")\nset(OCV_JS_PATH \"${OpenCV_BINARY_DIR}/bin/${OPENCV_JS}\")\n\nadd_custom_command(\n   OUTPUT ${OCV_JS_PATH}\n   COMMAND ${PYTHON_DEFAULT_EXECUTABLE} \"${CMAKE_CURRENT_SOURCE_DIR}/src/make_umd.py\" ${MODULE_JS_PATH} \"${OCV_JS_PATH}\"\n   DEPENDS ${the_module}\n   DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/src/make_umd.py\")\n\nadd_custom_target(${OPENCV_JS} ALL\n                  DEPENDS ${OCV_JS_PATH}\n                  DEPENDS ${the_module})\n```\n\n----------------------------------------\n\nTITLE: Setting Default Backend and Finalizing OpenCV HighGUI Module in CMake\nDESCRIPTION: Sets a default backend if none was selected, displays status information, handles include and library directories, and finalizes the module configuration with necessary target properties, compiler options, and test setup.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT OPENCV_HIGHGUI_BUILTIN_BACKEND)\n  set(OPENCV_HIGHGUI_BUILTIN_BACKEND \"NONE\")\nendif()\nmessage(STATUS \"highgui: using builtin backend: ${OPENCV_HIGHGUI_BUILTIN_BACKEND}\")\nset(OPENCV_HIGHGUI_BUILTIN_BACKEND \"${OPENCV_HIGHGUI_BUILTIN_BACKEND}\" PARENT_SCOPE)  # informational\n\nif(TRUE)\n  # these variables are set by 'ocv_append_build_options(HIGHGUI ...)'\n  foreach(P ${HIGHGUI_INCLUDE_DIRS})\n    ocv_include_directories(${P})\n  endforeach()\n\n  foreach(P ${HIGHGUI_LIBRARY_DIRS})\n    link_directories(${P})\n  endforeach()\nendif()\n\nif(tgts STREQUAL \"PRIVATE\")\n  set(tgts \"\")\nendif()\n\nocv_install_used_external_targets(${tgts})\n\nsource_group(\"Src\" FILES ${highgui_srcs} ${highgui_hdrs})\nsource_group(\"Include\" FILES ${highgui_ext_hdrs})\nocv_set_module_sources(HEADERS ${highgui_ext_hdrs} SOURCES ${highgui_srcs} ${highgui_hdrs})\nocv_module_include_directories()\n\nocv_create_module(${HIGHGUI_LIBRARIES})\n\nmacro(ocv_highgui_configure_target)\nif(APPLE)\n  add_apple_compiler_options(${the_module})\nendif()\n\nif(MSVC AND NOT BUILD_SHARED_LIBS AND BUILD_WITH_STATIC_CRT)\n  set_target_properties(${the_module} PROPERTIES LINK_FLAGS \"/NODEFAULTLIB:atlthunk.lib /NODEFAULTLIB:atlsd.lib /NODEFAULTLIB:libcmt.lib /DEBUG\")\nendif()\n\nocv_warnings_disable(CMAKE_CXX_FLAGS -Wno-deprecated-declarations)\nendmacro()\n\nif(NOT BUILD_opencv_world)\n  ocv_highgui_configure_target()\nendif()\n\nocv_add_accuracy_tests(${tgts})\n#ocv_add_perf_tests(${tgts})\n\nif(HIGHGUI_ENABLE_PLUGINS)\n```\n\n----------------------------------------\n\nTITLE: Disabling Unused Function Warning for GCC/Clang in CMake\nDESCRIPTION: Checks if the compiler is GCC (`CV_GCC`) or Clang (`CV_CLANG`) and if noisy warnings are not enabled (`ENABLE_NOISY_WARNINGS` is false). If both conditions are true, it appends the `-Wno-unused-function` flag to the C compiler flags (`CMAKE_C_FLAGS`) to suppress warnings about unused functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nif((CV_GCC OR CV_CLANG) AND NOT ENABLE_NOISY_WARNINGS)\n  set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wno-unused-function\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Downloading and Validating TBB Source Code\nDESCRIPTION: Downloads the TBB source code from GitHub using the ocv_download function. Verifies the download with MD5 hash check and determines the source directory structure.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/tbb/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(tbb_src_dir \"${OpenCV_BINARY_DIR}/3rdparty/tbb\")\nocv_download(FILENAME ${OPENCV_TBB_FILENAME}\n             HASH ${OPENCV_TBB_RELEASE_MD5}\n             URL\n               \"${OPENCV_TBB_URL}\"\n               \"$ENV{OPENCV_TBB_URL}\"\n               \"https://github.com/oneapi-src/oneTBB/archive/refs/tags/\"\n             DESTINATION_DIR ${tbb_src_dir}\n             ID TBB\n             STATUS res\n             UNPACK RELATIVE_URL)\nif(NOT res)\n  return()\nendif()\nif(OPENCV_TBB_SUBDIR)\n  # use current value\n  ocv_assert(EXISTS \"${tbb_src_dir}/${OPENCV_TBB_SUBDIR}\")\nelseif(EXISTS \"${tbb_src_dir}/oneTBB-${OPENCV_TBB_RELEASE_}\")\n  set(OPENCV_TBB_SUBDIR \"oneTBB-${OPENCV_TBB_RELEASE_}\")\nelseif(EXISTS \"${tbb_src_dir}/tbb-${OPENCV_TBB_RELEASE_}\")\n  set(OPENCV_TBB_SUBDIR \"oneTBB-${OPENCV_TBB_RELEASE_}\")\nelse()\n  message(FATAL_ERROR \"TBB: Can't configure TBB. Specify OPENCV_TBB_SUBDIR through command-line.\")\nendif()\nset(tbb_src_dir \"${tbb_src_dir}/${OPENCV_TBB_SUBDIR}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Source Files and Conditional Compilation for libtiff\nDESCRIPTION: This snippet lists the source and header files for the libtiff library and conditionally includes platform-specific files. On Windows platforms, it includes tif_win32.c; otherwise, it includes tif_unix.c. This ensures that the build process includes the correct implementation based on the operating system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libtiff/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(lib_srcs\n    tif_aux.c\n    tif_close.c\n    tif_codec.c\n    tif_color.c\n    tif_compress.c\n    tif_dir.c\n    tif_dirinfo.c\n    tif_dirread.c\n    tif_dirwrite.c\n    tif_dumpmode.c\n    tif_error.c\n    tif_extension.c\n    tif_fax3.c\n    tif_fax3sm.c\n    tif_flush.c\n    tif_getimage.c\n    tif_hash_set.c\n    tif_jbig.c\n    tif_jpeg_12.c\n    tif_jpeg.c\n    tif_luv.c\n    tif_lzma.c\n    tif_lzw.c\n    tif_next.c\n    tif_ojpeg.c\n    tif_open.c\n    tif_packbits.c\n    tif_pixarlog.c\n    tif_predict.c\n    tif_print.c\n    tif_read.c\n    tif_strip.c\n    tif_swab.c\n    tif_thunder.c\n    tif_tile.c\n    tif_version.c\n    tif_warning.c\n    tif_webp.c\n    tif_write.c\n    tif_zip.c\n    tif_zstd.c\n    tif_stream.cxx\n    t4.h\n    tif_dir.h\n    tif_fax3.h\n    tif_hash_set.h\n    tif_predict.h\n    tiff.h\n    tiffio.h\n    tiffiop.h\n    \"${CMAKE_CURRENT_BINARY_DIR}/tiffvers.h\"\n    uvcode.h\n    tiffio.hxx\n    \"${CMAKE_CURRENT_BINARY_DIR}/tif_config.h\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/tiffconf.h\"\n    )\n\nif(WIN32 AND NOT WINRT)\n  list(APPEND lib_srcs tif_win32.c)\nelse()\n  list(APPEND lib_srcs tif_unix.c)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Opening Video Files with OpenCV VideoCapture in C++\nDESCRIPTION: Shows how to initialize `cv::VideoCapture` objects using command-line arguments (`argv`) for video file paths. It demonstrates two methods: passing the path directly to the constructor and using the `open()` method after construction. Requires `<string>` and `<opencv2/videoio.hpp>` headers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_input_psnr_ssim.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nconst string sourceReference = argv[1],sourceCompareWith = argv[2];\n\nVideoCapture captRefrnc(sourceReference);\n// or\nVideoCapture captUndTst;\ncaptUndTst.open(sourceCompareWith);\n```\n\n----------------------------------------\n\nTITLE: Running ResNet-50 Evaluation Example\nDESCRIPTION: Specific command example for evaluating PyTorch ResNet-50 model using the DNN model runner.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_10\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name resnet50\n```\n\n----------------------------------------\n\nTITLE: Outputting std::vector via cv::Mat in OpenCV C++\nDESCRIPTION: Shows how to convert and output a std::vector through cv::Mat. This demonstrates the interoperability between STL containers and OpenCV data structures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_17\n\nLANGUAGE: C++\nCODE:\n```\nvector<float> v;\nv.push_back((float)CV_PI);\nv.push_back(2);\nv.push_back(3.01f);\ncout << \"Vector of floats via Mat = \" << Mat(v) << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: OpenGL Configuration Option in OpenCV\nDESCRIPTION: Defines the WITH_OPENGL build option which enables OpenGL integration for hardware-accelerated window drawing with GTK, WIN32 and Qt backends, plus basic OpenGL interoperability.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_18\n\nLANGUAGE: markdown\nCODE:\n```\n`WITH_OPENGL` (default: _OFF_)\n```\n\n----------------------------------------\n\nTITLE: Executing PaddlePaddle ResNet50 Demo Script (Shell)\nDESCRIPTION: Runs the Python script `paddle_resnet50.py`. This script is expected to handle exporting the PaddlePaddle ResNet50 model to ONNX, loading it using `cv2.dnn.readNetFromONNX`, preprocessing an image, and performing inference.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython paddle_resnet50.py\n```\n\n----------------------------------------\n\nTITLE: Verifying System Libraries and Options in CMake for OpenCV\nDESCRIPTION: This section of the CMake configuration manages detection and verification of system libraries and options, such as pkg-config availability and certain symbolic checks, ensuring dependencies are correctly configured for OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nif(UNIX OR MINGW)\n  if(NOT APPLE_FRAMEWORK OR OPENCV_ENABLE_PKG_CONFIG)\n    if(CMAKE_CROSSCOMPILING AND NOT DEFINED ENV{PKG_CONFIG_LIBDIR} AND NOT DEFINED ENV{PKG_CONFIG_SYSROOT_DIR}\n...\n  # Ensure that libpthread is not listed as one of the libraries to pass to the linker.\n  if (OPENCV_DISABLE_THREAD_SUPPORT)\n    list(REMOVE_ITEM OPENCV_LINKER_LIBS pthread)\n  endif()\n\n  if(OPENCV_ENABLE_MEMALIGN)\n    CHECK_SYMBOL_EXISTS(posix_memalign stdlib.h HAVE_POSIX_MEMALIGN)\n    CHECK_INCLUDE_FILE(malloc.h HAVE_MALLOC_H)\n    if(HAVE_MALLOC_H)\n      CHECK_SYMBOL_EXISTS(memalign malloc.h HAVE_MEMALIGN)\n    endif()\n    # TODO:\n    # - std::aligned_alloc() C++17 / C11\n  endif()\n\n  CHECK_SYMBOL_EXISTS(getauxval sys/auxv.h HAVE_GETAUXVAL)\n  CHECK_SYMBOL_EXISTS(elf_aux_info sys/auxv.h HAVE_ELF_AUX_INFO)\nel... [truncated]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVX HAL Static Library\nDESCRIPTION: Creates and configures the OpenVX HAL static library with source files, include directories, and link dependencies. Sets output directory and optional installation rules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openvx/hal/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(openvx_hal STATIC openvx_hal.cpp openvx_hal.hpp ${OPENCV_3P_OPENVX_DIR}/include/ivx.hpp ${OPENCV_3P_OPENVX_DIR}/include/ivx_lib_debug.hpp)\ntarget_include_directories(openvx_hal PUBLIC\n  ${CMAKE_CURRENT_SOURCE_DIR}\n  ${OPENCV_3P_OPENVX_DIR}/include\n  ${CMAKE_SOURCE_DIR}/modules/core/include\n  ${CMAKE_SOURCE_DIR}/modules/imgproc/include\n  ${CMAKE_SOURCE_DIR}/modules/features2d/include\n  ${OPENVX_INCLUDE_DIR})\ntarget_link_libraries(openvx_hal PUBLIC ${OPENVX_LIBRARIES})\nset_target_properties(openvx_hal PROPERTIES ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH})\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(openvx_hal EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev)\nendif()\n```\n\n----------------------------------------\n\nTITLE: OpenCV App Addition Macro\nDESCRIPTION: Defines a macro for conditionally adding OpenCV application subdirectories based on BUILD_APPS_LIST configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(ocv_add_app directory)\n  if(DEFINED BUILD_APPS_LIST)\n    list(FIND BUILD_APPS_LIST ${directory} _index)\n    if (${_index} GREATER -1)\n      add_subdirectory(${directory})\n    else()\n      message(STATUS \"Skip OpenCV app: ${directory}\")\n    endif()\n  else()\n    add_subdirectory(${directory})\n  endif()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Creating Directory Structure in Git Bash for OpenCV Build\nDESCRIPTION: Commands to create a directory structure in Git Bash for building OpenCV. This snippet creates a directory at C:/lib and navigates to it.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_install/windows_install.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir /c/lib\ncd /c/lib\n```\n\n----------------------------------------\n\nTITLE: Configuring and Exporting slow_hal Module - CMake\nDESCRIPTION: This CMake script initializes the build environment for the 'slow_hal' library within the larger OpenCV project. It sets up the project and library names, creates a static or shared library from source, configures position-independent code, and specifies the necessary include directories (including core OpenCV headers). Various variables are exported so other modules can detect and use the HAL implementation, and 'configure_file' ensures key headers and configuration scripts are copied to the appropriate binary locations for consumption by other components. Prerequisites include CMake 3.5+ and the expected directory structure under the OpenCV source tree.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/hal/slow_hal/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5 FATAL_ERROR)\n\nset(PROJECT_NAME \"slow_hal\")\nset(HAL_LIB_NAME \"slow_hal\")\n\nadd_library(${HAL_LIB_NAME} impl.cpp)\nset_target_properties(${HAL_LIB_NAME} PROPERTIES POSITION_INDEPENDENT_CODE TRUE)\nset(OPENCV_SRC_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../..\")\ntarget_include_directories(${HAL_LIB_NAME} PUBLIC ${CMAKE_CURRENT_SOURCE_DIR} ${OPENCV_SRC_DIR}/modules/core/include)\n\nset(OpenCV_HAL_FOUND TRUE)\nset(OpenCV_HAL_VERSION 0.0.1)\nset(OpenCV_HAL_LIBRARIES ${CMAKE_CURRENT_BINARY_DIR}/lib${HAL_LIB_NAME}.a)\nset(OpenCV_HAL_HEADERS \"impl.hpp\")\nset(OpenCV_HAL_INCLUDE_DIRS ${CMAKE_CURRENT_LIST_DIR})\n\nconfigure_file(\"impl.hpp\" \"${CMAKE_BINARY_DIR}/impl.hpp\" COPYONLY)\nconfigure_file(\"config.cmake\" \"${CMAKE_BINARY_DIR}/OpenCV_HALConfig.cmake\")\n\n```\n\n----------------------------------------\n\nTITLE: Loading a Fixed AC Huffman Table in C using libjpeg\nDESCRIPTION: Shows how to load a predefined AC Huffman table into a specific slot (`n`) within the libjpeg decompression context (`cinfo`). It allocates memory using `jpeg_alloc_huff_table` if needed, then populates the `bits` array (code counts per length) and the `huffval` array (symbols in code-length order) from user-provided `counts` and `symbols` arrays. This is necessary when decoding abbreviated JPEGs lacking embedded Huffman tables.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_60\n\nLANGUAGE: c\nCODE:\n```\nif (cinfo.ac_huff_tbl_ptrs[n] == NULL)\n  cinfo.ac_huff_tbl_ptrs[n] = jpeg_alloc_huff_table((j_common_ptr) &cinfo);\nhuff_ptr = cinfo.ac_huff_tbl_ptrs[n];       /* huff_ptr is JHUFF_TBL* */\nfor (i = 1; i <= 16; i++) {\n  /* counts[i] is number of Huffman codes of length i bits, i=1..16 */\n  huff_ptr->bits[i] = counts[i];\n}\nfor (i = 0; i < 256; i++) {\n  /* symbols[] is the list of Huffman symbols, in code-length order */\n  huff_ptr->huffval[i] = symbols[i];\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Point Polygon Test with OpenCV in C++\nDESCRIPTION: This C++ code demonstrates how to use the cv::pointPolygonTest function in OpenCV to test the presence of a point relative to a polygon. Dependencies include OpenCV 3.0 or above. The input consists of a contour (polygon) and a test point; the function returns a value indicating whether the point is inside, outside, or on the contour. The code handles visualization using OpenCV windows and requires linking against the OpenCV library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/point_polygon_test/point_polygon_test.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <opencv2/opencv.hpp>\\n\\nusing namespace cv;\\nusing namespace std;\\n\\nint main(int, char**)\\n{\\n    // Define a polygon (contour)\\n    vector<Point> contour;\\n    contour.push_back(Point(100, 100));\\n    contour.push_back(Point(200, 100));\\n    contour.push_back(Point(200, 200));\\n    contour.push_back(Point(100, 200));\\n\\n    // Define test points\\n    Point testPoint1(150, 150); // Should be inside\\n    Point testPoint2(250, 150); // Should be outside\\n\\n    // Test pointPolygonTest\\n    double result1 = pointPolygonTest(contour, testPoint1, false);\\n    double result2 = pointPolygonTest(contour, testPoint2, false);\\n\\n    cout << \"Test Point 1 (\" << testPoint1 << \"): \" << result1 << endl;\\n    cout << \"Test Point 2 (\" << testPoint2 << \"): \" << result2 << endl;\\n\\n    // Visualization: draw contour and points\\n    Mat img = Mat::zeros(300, 300, CV_8UC3);\\n    polylines(img, contour, true, Scalar(255,255,255), 2);\\n    circle(img, testPoint1, 5, Scalar(0,255,0), -1);\\n    circle(img, testPoint2, 5, Scalar(0,0,255), -1);\\n    imshow(\"Point Polygon Test\", img);\\n    waitKey(0);\\n    return 0;\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: JPEG Processing Function Declarations\nDESCRIPTION: Core JPEG processing function declarations used for managing buffered-image mode operations and scan processing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_57\n\nLANGUAGE: c\nCODE:\n```\njpeg_finish_output()\njpeg_start_output()\njpeg_consume_input()\njpeg_start_decompress()\njpeg_finish_decompress()\njpeg_new_colormap()\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Conditional Logic in OpenCV\nDESCRIPTION: Complex nested conditional statements that compare pixel values against thresholds (c_b and cb) to determine if a point qualifies as a corner. Uses multiple offset positions to analyze surrounding pixels and jumps to either 'is_a_corner' or 'is_not_a_corner' based on the comparison results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset2] < c_b)\n    goto is_not_a_corner;\nelse\n    if(ptr[offset2] > cb)\n      if(ptr[offset1] < c_b)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset1] > cb)\n          if(ptr[offset6] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset4] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Versioning and Configuration Files Generation\nDESCRIPTION: This snippet sets the version number for libtiff and processes configuration templates using CMake's configure_file command to generate header files required for the build. The tif_config.h, tiffconf.h, and tiffvers.h files are crucial for defining platform-specific and version-specific configurations during the build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libtiff/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(LIBTIFF_MAJOR_VERSION \"4\")\nset(LIBTIFF_MINOR_VERSION \"6\")\nset(LIBTIFF_MICRO_VERSION \"0\")\nset(LIBTIFF_VERSION \"${LIBTIFF_MAJOR_VERSION}.${LIBTIFF_MINOR_VERSION}.${LIBTIFF_MICRO_VERSION}\")\nfile(READ \"RELEASE-DATE\" LIBTIFF_RELEASE_DATE content)\n\nset(TIFF_MAX_DIR_COUNT \"1048576\")\n\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/tif_config.h.cmake.in\"\n               \"${CMAKE_CURRENT_BINARY_DIR}/tif_config.h\"\n               @ONLY)\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/tiffconf.h.cmake.in\"\n               \"${CMAKE_CURRENT_BINARY_DIR}/tiffconf.h\"\n               @ONLY)\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/tiffvers.h.cmake.in\"\n               \"${CMAKE_CURRENT_BINARY_DIR}/tiffvers.h\"\n               @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Configuring Android Project Build Capability in OpenCV CMake\nDESCRIPTION: Checks if the necessary tools and versions are available to build Android projects. Sets a flag indicating whether Android projects can be built.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif(ANDROID AND ANDROID_EXECUTABLE AND ANT_EXECUTABLE AND (ANT_VERSION VERSION_GREATER 1.7) AND (ANDROID_TOOLS_Pkg_Revision GREATER 13))\n  SET(CAN_BUILD_ANDROID_PROJECTS TRUE)\nelse()\n  SET(CAN_BUILD_ANDROID_PROJECTS FALSE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Serialization Methods for File I/O (Inside Class) - OpenCV C++\nDESCRIPTION: This snippet adds methods for custom serialization within a C++ class to enable seamless integration with OpenCV's FileStorage. Implementations include writing and reading each member variable. These methods must match the serialization protocol of OpenCV and handle default values for missing fields.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\nvoid write(FileStorage& fs) const\\n{\\n    fs << \"{\" << \"A\" << A << \"X\" << X << \"id\" << id << \"}\";\\n}\\nvoid read(const FileNode& node)\\n{\\n    A = (int)node[\"A\"];\\n    X = (double)node[\"X\"];\\n    id = (string)node[\"id\"];\\n}\n```\n\n----------------------------------------\n\nTITLE: Conditional Debug Module and Module Definition in CMake\nDESCRIPTION: This snippet includes a conditional definition for including a debug module when DEBUG_opencv_features2d is set. It defines the features2d module with dependencies on opencv_imgproc and optional dependencies like opencv_flann. The module is set up to be capable of being wrapped for various languages such as Java, Objective-C, Python, and JavaScript.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(debug_modules \\\"\\\")\\nif(DEBUG_opencv_features2d)\\n  list(APPEND debug_modules opencv_highgui)\\nendif()\\nocv_define_module(features2d opencv_imgproc ${debug_modules} OPTIONAL opencv_flann WRAP java objc python js)\n```\n\n----------------------------------------\n\nTITLE: Initializing JPEG Decompression Object in C\nDESCRIPTION: This snippet focuses on creating and initializing a JPEG decompression object. It involves setting up error handling and is similar to initializing a compression object. The key function is 'jpeg_create_decompress' for decompression processes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_11\n\nLANGUAGE: C\nCODE:\n```\nstruct jpeg_decompress_struct cinfo;\nstruct jpeg_error_mgr jerr;\ncinfo.err = jpeg_std_error(&jerr);\njpeg_create_decompress(&cinfo);\n```\n\n----------------------------------------\n\nTITLE: Setting OpenJPEG Shared Library SOVERSION in CMake\nDESCRIPTION: Determines the shared object version (SOVERSION) for the OpenJPEG library. It defaults to 7 based on the OpenJPEG team's versioning scheme but allows overriding via the `OPENJPEG_SOVERSION` CMake variable during configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n# Because autotools does not support X.Y notation for SOVERSION, we have to use\n# two numbering, one for the openjpeg version and one for openjpeg soversion\n# version | soversion\n#   1.0   |  0\n#   1.1   |  1\n#   1.2   |  2\n#   1.3   |  3\n#   1.4   |  4\n#   1.5   |  5\n#   1.5.1 |  5\n#   2.0   |  6\n#   2.0.1 |  6\n#   2.1   |  7\n#   2.1.1 |  7\n#   2.1.2 |  7\n#   2.2.0 |  7\n#   2.3.0 |  7\n#   2.3.1 |  7\n#   2.4.0 |  7\n#   2.5.0 |  7\n# above is the recommendation by the OPJ team. If you really need to override this default,\n# you can specify your own OPENJPEG_SOVERSION at cmake configuration time:\n# cmake -DOPENJPEG_SOVERSION:STRING=42 /path/to/openjpeg\nif(NOT OPENJPEG_SOVERSION)\n  set(OPENJPEG_SOVERSION 7)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Threading and Compiler-Specific Options for TBB\nDESCRIPTION: Sets up threading and compiler-specific options for TBB based on the build environment. Includes special handling for GCC, Clang, and pthread support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/tbb/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(HAVE_PTHREAD)\n  add_definitions(-DUSE_PTHREAD) #required for Unix\nendif()\n\nif(CV_GCC)\n  add_definitions(-DTBB_USE_GCC_BUILTINS=1) #required for ARM GCC\n  if(NOT CMAKE_CXX_COMPILER_VERSION LESS 6.0)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -flifetime-dse=1\") # workaround for GCC 6.x\n  endif()\nendif()\n\nif(ANDROID_COMPILER_IS_CLANG)\n  add_definitions(-D__TBB_GCC_BUILTIN_ATOMICS_PRESENT=1)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV 2.4 Build for DRIVE PX 2 using CMake (Shell)\nDESCRIPTION: Runs CMake to configure the OpenCV 2.4 build specifically for the NVIDIA DRIVE PX 2 platform (Vibrante V4L). It sets build type to Release, enables CUDA 8.0 for architecture 6.2, enables Python 2 bindings (`BUILD_opencv_python`), TBB, and FFMPEG, while disabling several other optional dependencies (PNG, TIFF, Jasper, ZLIB, Java, nonfree modules, OpenCL, OpenMP, GStreamer, VTK, 1394, OpenEXR) and specifying the test data path. Assumes execution from a build directory sibling to `opencv` and `opencv_extra`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\n$ cmake \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=/usr \\\n    -DBUILD_PNG=OFF \\\n    -DBUILD_TIFF=OFF \\\n    -DBUILD_TBB=OFF \\\n    -DBUILD_JPEG=OFF \\\n    -DBUILD_JASPER=OFF \\\n    -DBUILD_ZLIB=OFF \\\n    -DBUILD_EXAMPLES=ON \\\n    -DBUILD_JAVA=OFF \\\n    -DBUILD_opencv_nonfree=OFF \\\n    -DBUILD_opencv_python=ON \\\n    -DENABLE_NEON=ON \\\n    -DWITH_OPENCL=OFF \\\n    -DWITH_OPENMP=OFF \\\n    -DWITH_FFMPEG=ON \\\n    -DWITH_GSTREAMER=OFF \\\n    -DWITH_GSTREAMER_0_10=OFF \\\n    -DWITH_CUDA=ON \\\n    -DWITH_GTK=ON \\\n    -DWITH_VTK=OFF \\\n    -DWITH_TBB=ON \\\n    -DWITH_1394=OFF \\\n    -DWITH_OPENEXR=OFF \\\n    -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-8.0 \\\n    -DCUDA_ARCH_BIN=6.2 \\\n    -DCUDA_ARCH_PTX=\"\" \\\n    -DINSTALL_C_EXAMPLES=ON \\\n    -DINSTALL_TESTS=ON \\\n    -DOPENCV_TEST_DATA_PATH=../opencv_extra/testdata \\\n    ../opencv\n```\n\n----------------------------------------\n\nTITLE: Setting Solution Folder for Zlib in CMake\nDESCRIPTION: Places the Zlib library in the \"3rdparty\" solution folder if solution folders are enabled. This helps organize the project structure in IDEs.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(${ZLIB_LIBRARY} PROPERTIES FOLDER \"3rdparty\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditionally Defining Highgui Module with Dependencies and Wrappers in CMake\nDESCRIPTION: Defines the 'highgui' module using the custom `ocv_add_module` function. It specifies `opencv_imgproc` as a required dependency and `opencv_imgcodecs` and `opencv_videoio` as optional dependencies. Crucially, it conditionally includes Java wrappers (`WRAP java`) only when not building for Android. Python wrappers (`WRAP python`) are included for both Android and other platforms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ANDROID)\n  ocv_add_module(highgui opencv_imgproc OPTIONAL opencv_imgcodecs opencv_videoio WRAP python)\nelse()\n  ocv_add_module(highgui opencv_imgproc OPTIONAL opencv_imgcodecs opencv_videoio WRAP python java)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing AArch64 Python 3 Development Library (Bash)\nDESCRIPTION: Installs the Python 3 development library (`libpython3-dev`) specifically for the `arm64` architecture onto the host system. This is required if building the OpenCV Python 3 wrapper for the `arm64` target.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y \\\n    libpython3-dev:arm64\n```\n\n----------------------------------------\n\nTITLE: Configuring JPEG Support in CMake\nDESCRIPTION: CMake configuration options for enabling JPEG support in OpenCV imgcodecs module. Includes options for choosing between libjpeg-turbo and standard libjpeg, as well as SIMD optimization controls.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/readme.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nWITH_JPEG=ON # Enable JPEG support\nBUILD_JPEG=ON # Use libjpeg-turbo by default\nBUILD_JPEG_TURBO_DISABLE=ON # Force using libjpeg instead\nENABLE_LIBJPEG_TURBO_SIMD=ON # Control SIMD instructions\n```\n\n----------------------------------------\n\nTITLE: Configuring RVV HAL Internal Variables with CMake - CMake\nDESCRIPTION: This snippet initializes and caches variables that control integration of the RVV (RISC-V Vector) Hardware Abstraction Layer into OpenCV. It requires ${MIN_VER_CMAKE} to be defined and the build to use CMake, but has no external dependencies. Key parameters set include minimum CMake version, the HAL library name, version string, library targets, header file, and include directories. No parameters are externally exposed; all values are set as internal cache entries to facilitate subsequent build steps within the CMake system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/hal_rvv/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION ${MIN_VER_CMAKE} FATAL_ERROR)\n\nset(HAL_LIB_NAME \"\")\n\nset(RVV_HAL_FOUND TRUE CACHE INTERNAL \"\")\nset(RVV_HAL_VERSION \"0.0.1\" CACHE INTERNAL \"\")\nset(RVV_HAL_LIBRARIES ${HAL_LIB_NAME} CACHE INTERNAL \"\")\nset(RVV_HAL_HEADERS \"hal_rvv.hpp\" CACHE INTERNAL \"\")\nset(RVV_HAL_INCLUDE_DIRS \"${CMAKE_CURRENT_SOURCE_DIR}\" \"${CMAKE_SOURCE_DIR}/modules/imgproc/include\" CACHE INTERNAL \"\")\n```\n\n----------------------------------------\n\nTITLE: Camera Frame Transformation Matrix Equation\nDESCRIPTION: Mathematical equation showing how 3D points are transformed from world coordinates to camera coordinates using rotation and translation components.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/calib3d/doc/solvePnP.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Mathematical Notation\nCODE:\n```\n\\begin{bmatrix}\nX_c \\\\\nY_c \\\\\nZ_c \\\\\n1\n\\end{bmatrix} =\n\\hspace{0.2em} ^{c}\\bf{T}_w\n\\begin{bmatrix}\nX_{w} \\\\\nY_{w} \\\\\nZ_{w} \\\\\n1\n\\end{bmatrix}\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenCL Compiler Definitions if Available - CMake\nDESCRIPTION: If both OPENCV_DNN_OPENCL and HAVE_OPENCL are true, this sets a compile definition (CV_OCL4DNN=1) for the DNN module, enabling code paths specific to OpenCL. The snippet checks variables and applies definitions to target. No parameters beyond variable checks and setting compile definitions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(OPENCV_DNN_OPENCL AND HAVE_OPENCL)\n  ocv_target_compile_definitions(${the_module} PRIVATE \"CV_OCL4DNN=1\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Ensuring Doxygen HTML Output Directory Exists in CMake\nDESCRIPTION: Uses the `file(MAKE_DIRECTORY ...)` command to create the specified HTML output directory (`${CMAKE_CURRENT_BINARY_DIR}/doxygen/html`) if it does not already exist. This prevents errors later when Doxygen tries to write output files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\n# make sure the build directory exists\nfile(MAKE_DIRECTORY \"${opencv_tutorial_html_dir}\")\n```\n\n----------------------------------------\n\nTITLE: Verifying Pkg-config for AArch64 Freetype/Harfbuzz (Bash)\nDESCRIPTION: Uses `pkg-config` with the appropriate environment variables set for `aarch64` to retrieve the compiler and linker flags needed for using the Freetype2 and HarfBuzz libraries installed for the `arm64` architecture. Successful output confirms `pkg-config` can find these libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nPKG_CONFIG_PATH=/usr/lib/aarch64-linux-gnu/pkgconfig:/usr/share/pkgconfig \\\n    PKG_CONFIG_LIBDIR=/usr/lib/aarch64-linux-gnu \\\n    PKG_CONFIG_SYSROOT_DIR=/ \\\n       pkg-config freetype2 harfbuzz --cflags --libs\n-I/usr/include/freetype2 -I/usr/include/libpng16 -I/usr/include/harfbuzz -I/usr/include/glib-2.0 -I/usr/lib/aarch64-linux-gnu/glib-2.0/include -L/usr/lib/aarch64-linux-gnu -lfreetype -lharfbuzz\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV.js Performance Tests with Node.js - Shell\nDESCRIPTION: This shell script sequence changes to the 'bin/perf' directory, installs dependencies with npm, and invokes the performance test JavaScript script for 'threshold' with a custom test parameter filter. It requires a built OpenCV.js with performance tests and Node.js/npm. The '--test_param_filter' argument controls which test is run. Outputs are benchmark results displayed in the terminal.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\ncd bin/perf\\nnpm install\\nnode perf_threshold.js --test_param_filter=\"(1920x1080, CV_8UC1, THRESH_BINARY)\"\n```\n\n----------------------------------------\n\nTITLE: Text Recognition on Public Datasets Using Bash\nDESCRIPTION: These bash commands are used to perform text recognition on publicly available datasets as test cases. It involves models stored in ONNX format and requires the OpenCV executables to be pre-built. The commands take paths to models and evaluation data as inputs, along with flags for execution options. Results appearing on the connected display or adjustments in verbosity level might be considered constraints.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_text_spotting/dnn_text_spotting.markdown#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexample_dnn_scene_text_recognition -mp=path/to/crnn.onnx -e=true -edp=path/to/evaluation_data_rec -vp=/path/to/alphabet_36.txt -rgb=0\nexample_dnn_scene_text_recognition -mp=path/to/crnn_cs.onnx -e=true -edp=path/to/evaluation_data_rec -vp=/path/to/alphabet_94.txt -rgb=1\n```\n\n----------------------------------------\n\nTITLE: Displaying Intel IPP Status in OpenCV Build\nDESCRIPTION: Checks and displays the status of Intel Integrated Performance Primitives (IPP) integration, including version information, root directory, linking method, and the presence of IPP Image Wrappers (IW).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_22\n\nLANGUAGE: cmake\nCODE:\n```\nif(WITH_IPP AND HAVE_IPP)\n  status(\"    Intel IPP:\" \"${IPP_VERSION_STR} [${IPP_VERSION_MAJOR}.${IPP_VERSION_MINOR}.${IPP_VERSION_BUILD}]\")\n  status(\"           at:\" \"${IPP_ROOT_DIR}\")\n  if(NOT HAVE_IPP_ICV)\n    status(\"       linked:\" BUILD_WITH_DYNAMIC_IPP THEN \"dynamic\" ELSE \"static\")\n  endif()\n  if(HAVE_IPP_IW)\n    if(BUILD_IPP_IW)\n      status(\"    Intel IPP IW:\" \"sources (${IW_VERSION_MAJOR}.${IW_VERSION_MINOR}.${IW_VERSION_UPDATE})\")\n    else()\n      status(\"    Intel IPP IW:\" \"binaries (${IW_VERSION_MAJOR}.${IW_VERSION_MINOR}.${IW_VERSION_UPDATE})\")\n    endif()\n    status(\"              at:\" \"${IPP_IW_PATH}\")\n  else()\n    status(\"    Intel IPP IW:\"   NO)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring s390 CRC32-VX Optimization for ZLIB in CMake\nDESCRIPTION: Checks if CRC32 Vector Extension (VX) optimization is enabled (WITH_CRC32_VX) for s390 and verifies VGFMA intrinsics support (HAVE_VGFMA_INTRIN). If supported, it adds the S390_CRC32_VX definition, appends the `crc32-vx.c` source file, and sets specific compile flags (VGFMAFLAG, NOLTOFLAG) for that file. Otherwise, it disables the feature.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_23\n\nLANGUAGE: cmake\nCODE:\n```\n        if(WITH_CRC32_VX)\n            check_vgfma_intrinsics()\n            if(HAVE_VGFMA_INTRIN)\n                add_definitions(-DS390_CRC32_VX)\n                set(CRC32_VX_SRCS ${ARCHDIR}/crc32-vx.c)\n                list(APPEND ZLIB_ARCH_SRCS ${CRC32_VX_SRCS})\n                set_property(SOURCE ${CRC32_VX_SRCS} PROPERTY COMPILE_FLAGS \"${VGFMAFLAG} ${NOLTOFLAG}\")\n            else()\n                set(WITH_CRC32_VX OFF)\n            endif()\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Doxygen Documentation Generation Setup\nDESCRIPTION: Configures Doxygen documentation generation target with dependencies on JavaScript assets and other required files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_21\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(\n    doxygen_cpp\n    COMMAND ${DOXYGEN_EXECUTABLE} ${doxyfile}\n    DEPENDS ${doxyfile} ${rootfile} ${bibfile} ${deps} ${js_tutorials_assets_deps}\n    COMMENT \"Generate Doxygen documentation\"\n  )\n```\n\n----------------------------------------\n\nTITLE: Displaying OpenVINO and Inference Engine Status in OpenCV Build\nDESCRIPTION: Complex conditional section that checks for OpenVINO or Inference Engine integration, displaying version information, library paths, and include directories when available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_24\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_OPENVINO\n    OR (WITH_OPENVINO AND NOT WITH_INF_ENGINE AND NOT INF_ENGINE_TARGET)\n)\n  status(\"    OpenVINO:\" TARGET openvino::runtime THEN \"YES (${OpenVINO_VERSION})\" ELSE \"NO\")\nelse()\n  if(WITH_INF_ENGINE OR INF_ENGINE_TARGET)\n    if(INF_ENGINE_TARGET)\n      list(GET INF_ENGINE_TARGET 0 ie_target)\n      set(__msg \"YES (${INF_ENGINE_RELEASE} / ${INF_ENGINE_VERSION})\")\n      ocv_get_imported_target(ie_target \"${ie_target}\")\n      get_target_property(_lib ${ie_target} IMPORTED_LOCATION)\n      get_target_property(_lib_imp_rel ${ie_target} IMPORTED_IMPLIB_RELEASE)\n      get_target_property(_lib_imp_dbg ${ie_target} IMPORTED_IMPLIB_DEBUG)\n      get_target_property(_lib_rel ${ie_target} IMPORTED_LOCATION_RELEASE)\n      get_target_property(_lib_dbg ${ie_target} IMPORTED_LOCATION_DEBUG)\n      ocv_build_features_string(_lib\n        IF _lib THEN \"${_lib}\"\n        IF _lib_imp_rel AND _lib_imp_dbg THEN \"${_lib_imp_rel} / ${_lib_imp_dbg}\"\n        IF _lib_rel AND _lib_dbg THEN \"${_lib_rel} / ${_lib_dbg}\"\n        IF _lib_rel  THEN \"${_lib_rel}\"\n        IF _lib_dbg  THEN \"${_lib_dbg}\"\n        ELSE \"unknown\"\n      )\n      get_target_property(_inc ${ie_target} INTERFACE_INCLUDE_DIRECTORIES)\n      status(\"    Inference Engine:\" \"${__msg}\")\n      status(\"        * libs:\" \"${_lib}\")\n      status(\"        * includes:\" \"${_inc}\")\n    else()\n      status(\"    Inference Engine:\"     \"NO\")\n    endif()\n  endif()\n  if(WITH_NGRAPH OR HAVE_NGRAPH)\n    if(HAVE_NGRAPH)\n      ocv_get_imported_target(__target ngraph::ngraph)\n      set(__msg \"YES (${ngraph_VERSION})\")\n      get_target_property(_lib ${__target} IMPORTED_LOCATION)\n      get_target_property(_lib_imp_rel ${__target} IMPORTED_IMPLIB_RELEASE)\n      get_target_property(_lib_imp_dbg ${__target} IMPORTED_IMPLIB_DEBUG)\n      get_target_property(_lib_rel ${__target} IMPORTED_LOCATION_RELEASE)\n      get_target_property(_lib_dbg ${__target} IMPORTED_LOCATION_DEBUG)\n      ocv_build_features_string(_lib\n        IF _lib THEN \"${_lib}\"\n        IF _lib_imp_rel AND _lib_imp_dbg THEN \"${_lib_imp_rel} / ${_lib_imp_dbg}\"\n        IF _lib_rel AND _lib_dbg THEN \"${_lib_rel} / ${_lib_dbg}\"\n        IF _lib_rel  THEN \"${_lib_rel}\"\n        IF _lib_dbg  THEN \"${_lib_dbg}\"\n        ELSE \"unknown\"\n      )\n      get_target_property(_inc ${__target} INTERFACE_INCLUDE_DIRECTORIES)\n      status(\"    nGraph:\" \"${__msg}\")\n      status(\"        * libs:\" \"${_lib}\")\n      status(\"        * includes:\" \"${_inc}\")\n    else()\n      status(\"    nGraph:\"     \"NO\")\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Location of Existing FastCV Libraries in eSDK Sysroot (Path)\nDESCRIPTION: This file path points to the directory within the Qualcomm eSDK sysroot where pre-existing FastCV libraries are typically located. If the FastCV library is updated (e.g., downloaded or built by the OpenCV build process), the new libraries found in the build output directory should replace the ones in this location to ensure the target system uses the updated version.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_fastcv/building_fastcv.markdown#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n<ESDK_PATH>\\qcom-wayland_sdk\\tmp\\sysroots\\qcs6490-rb3gen2-vision-kit\\usr\\lib\n```\n\n----------------------------------------\n\nTITLE: Applying Input Image Smoothing in libjpeg (C)\nDESCRIPTION: A C integer field within the compression parameters structure (`cinfo`). If set to a non-zero value (1 to 100), the input image data is smoothed before compression. A value of 1 corresponds to minimal smoothing, while 100 represents maximum smoothing. The default value is zero, meaning no smoothing is applied.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_37\n\nLANGUAGE: C\nCODE:\n```\nint smoothing_factor\n```\n\n----------------------------------------\n\nTITLE: Bitwise Operations with Vector Registers in C++\nDESCRIPTION: Shows bitwise shift and AND operations on vector registers containing integer values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/univ_intrin/univ_intrin.markdown#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nv_int32 as;                              // {a1, ..., an}\nv_int32 al = as << 2;                    // {a1 << 2, ..., an << 2}\nv_int32 bl = as >> 2;                    // {a1 >> 2, ..., an >> 2}\n\nv_int32 a, b;\nv_int32 a_and_b = a & b;                 // {a1 & b1, ..., an & bn}\n```\n\n----------------------------------------\n\nTITLE: Tonemapping HDR Image using OpenCV Python\nDESCRIPTION: Maps a 32-bit float HDR image to the range [0..1] using a gamma value. This requires clipping out-of-bound values to prevent overflow when processing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_photo/py_hdr/py_hdr.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Tonemap HDR image\ntonemap1 = cv.createTonemap(gamma=2.2)\nres_debevec = tonemap1.process(hdr_debevec.copy())\n```\n\n----------------------------------------\n\nTITLE: Accessing Image Data Type using OpenCV in Python\nDESCRIPTION: Demonstrates how to retrieve the data type of the image elements (e.g., `uint8` for standard 8-bit images) using the `img.dtype` attribute. Checking the `dtype` is crucial for debugging as many OpenCV-Python errors stem from invalid data types.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_core/py_basic_ops/py_basic_ops.markdown#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> print( img.dtype )\nuint8\n```\n\n----------------------------------------\n\nTITLE: Android GLSurfaceView Implementation for Camera Preview\nDESCRIPTION: Java code for GLSurfaceView implementation including surface callbacks and camera handling. This class handles the camera preview rendering using OpenGL ES.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n    @Override\n    public void onSurfaceChanged(GL10 gl, int surfaceWidth, int surfaceHeight) {\n        Log.i(LOGTAG, \"onSurfaceChanged(\"+surfaceWidth+\"x\"+surfaceHeight+\")\");\n        NativeGLRenderer.changeSize(surfaceWidth, surfaceHeight);\n        setCameraPreviewSize(surfaceWidth, surfaceHeight);\n    }\n\n    @Override\n    public void onSurfaceCreated(GL10 gl, EGLConfig config) {\n        Log.i(LOGTAG, \"onSurfaceCreated\");\n        String strGLVersion = GLES20.glGetString(GLES20.GL_VERSION);\n        if (strGLVersion != null)\n            Log.i(LOGTAG, \"OpenGL ES version: \" + strGLVersion);\n\n        int hTex = NativeGLRenderer.initGL();\n        mSTex = new SurfaceTexture(hTex);\n        mSTex.setOnFrameAvailableListener(this);\n        openCamera();\n        mGLInit = true;\n    }\n```\n\n----------------------------------------\n\nTITLE: Generating SSD MobileNetV1 Text Graph (.pbtxt) with Script - Console\nDESCRIPTION: Executes the 'tf_text_graph_ssd.py' script to create a .pbtxt text graph from the TensorFlow frozen graph and the model config. Essential for enabling OpenCV to interpret TensorFlow SSD detection models. Accepts input, config, and output paths as parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: console\nCODE:\n```\npython tf_text_graph_ssd.py --input ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb --config ssd_mobilenet_v1_coco_2017_11_17/ssd_mobilenet_v1_coco.config --output ssd_mobilenet_v1_coco_2017_11_17.pbtxt\n```\n\n----------------------------------------\n\nTITLE: Displaying 32F Image by Conversion to 8U for imshow with OpenCV in Java\nDESCRIPTION: In Java OpenCV, converts a float Mat to 8-bit before display, using convertTo. Native imshow not available; display via workaround. Automatic scaling may be required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_44\n\nLANGUAGE: Java\nCODE:\n```\nMat img8u = new Mat();\\nimg.convertTo(img8u, CvType.CV_8U, 255.0);\\n// display img8u using Java GUI or save to file\n```\n\n----------------------------------------\n\nTITLE: Output Shape Computation for Custom Layer OpenCV C++\nDESCRIPTION: Determine output blob shapes based on input forms, allowing additional memory requests using \\'internals\\'.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/custom_layers.hpp MyLayer::getMemoryShapes\n```\n\n----------------------------------------\n\nTITLE: Getting Back Projection in Java with OpenCV\nDESCRIPTION: This snippet demonstrates how to calculate the back projection of an image using OpenCV in Java. It uses the calcBackProject method with previously calculated histogram and image data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/back_projection/back_projection.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nImgproc.calcBackProject(Arrays.asList(hue), channels, hist, backproj, ranges, 1);\n```\n\n----------------------------------------\n\nTITLE: Custom Operation for Converting Face Detection to ROI Array\nDESCRIPTION: Declaration of a custom G-API operation that processes the face detection network output into an array of face rectangles (ROIs).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// ROI type definition\nusing Contour = std::vector<cv::Point>;\n\n// Custom operation to parse SSD detection results\nG_API_OP(ParseSSD, <cv::GArray<cv::Rect>(cv::GMat, cv::Size, float, bool)>, \"custom.fd_postproc\") {\n    static cv::GArrayDesc outMeta(const cv::GMatDesc&, const cv::Size&, float, bool) {\n        return cv::empty_array_desc();\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Frozen Graph Reading\nDESCRIPTION: Demonstrates loading a TensorFlow frozen model graph into a tf.Graph object which can then be used for inference. It requires the TensorFlow library and an available graph file. Inputs include the path to the frozen graph file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# init deeplab model graph\nmodel_graph = tf.Graph()\n\n# obtain\nwith tf.io.gfile.GFile(frozen_graph_path, 'rb') as graph_file:\n    tf_model_graph = GraphDef()\ntf_model_graph.ParseFromString(graph_file.read())\n\nwith model_graph.as_default():\n    tf.import_graph_def(tf_model_graph, name='')\n```\n\n----------------------------------------\n\nTITLE: Generating Non-Linearly Separable Training Data for SVM (C++)\nDESCRIPTION: Creates overlapping training data that is non-linearly separable by introducing random points that cross class boundaries. This second part of the dataset demonstrates the need for non-linear SVM techniques.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/non_linear_svms.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n// Set up the non-linearly separable part of the training data\nfloat FRAC_LINEAR_SEP = 0.9f; // Fraction of the training samples which will be linearly separable\nint trainingSamplesToAdd = 2 * NTRAINING_SAMPLES * (1 - FRAC_LINEAR_SEP);\nMat extraTrainData(trainingSamplesToAdd, 2, CV_32FC1);\nMat extraTrainLabels(trainingSamplesToAdd, 1, CV_32SC1);\nRNG rng2(100);\n\n// Generate extra non-linearly separable points\nfor(int i = 0; i < trainingSamplesToAdd; i++)\n{\n    int clsIdx = i % 2;\n    float x = rng2.uniform(0.0f, 1.0f);\n    float y = rng2.uniform(0.0f, 1.0f);\n    extraTrainData.at<float>(i, 0) = x;\n    extraTrainData.at<float>(i, 1) = y;\n    extraTrainLabels.at<int>(i, 0) = labels[clsIdx];\n}\n\n// Merge all the training data\nMat completeTrainData;\nvconcat(trainData, extraTrainData, completeTrainData);\nMat completeTrainLabels;\nvconcat(trainLabels, extraTrainLabels, completeTrainLabels);\n```\n\n----------------------------------------\n\nTITLE: Configuring DNN Module Build and Tests\nDESCRIPTION: Main CMake configuration for OpenCV DNN module including source compilation, test configuration, and optional backend support. Sets up module sources, dependencies, and testing parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_24\n\nLANGUAGE: cmake\nCODE:\n```\nocv_append_source_file_compile_definitions(\"${CMAKE_CURRENT_LIST_DIR}/src/dnn_params.cpp\" \"OPENCV_DNN_BACKEND_DEFAULT=${OPENCV_DNN_BACKEND_DEFAULT}\")\n\nocv_install_used_external_targets(${libs} ${dnn_runtime_libs})\n\nocv_glob_module_sources(${sources_options} SOURCES ${fw_srcs} ${webnn_srcs})\nocv_create_module(${libs} ${dnn_runtime_libs})\nocv_add_samples()\nocv_add_accuracy_tests(${dnn_runtime_libs})\n\nif(NOT BUILD_PROTOBUF)\n  if(TARGET opencv_test_dnn)\n    ocv_target_compile_definitions(opencv_test_dnn PRIVATE \"OPENCV_DNN_EXTERNAL_PROTOBUF=1\")\n  endif()\nendif()\n\nset(perf_path \"${CMAKE_CURRENT_LIST_DIR}/perf\")\nfile(GLOB_RECURSE perf_srcs \"${perf_path}/*.cpp\")\nfile(GLOB_RECURSE perf_hdrs \"${perf_path}/*.hpp\" \"${perf_path}/*.h\")\nocv_add_perf_tests(${dnn_runtime_libs}\n    FILES test_common \"${CMAKE_CURRENT_LIST_DIR}/test/test_common.hpp\" \"${CMAKE_CURRENT_LIST_DIR}/test/test_common.impl.hpp\"\n    FILES Src ${perf_srcs}\n    FILES Include ${perf_hdrs}\n)\n\nocv_option(OPENCV_DNN_PERF_CAFFE \"Add performance tests of Caffe framework\" OFF)\nocv_option(OPENCV_DNN_PERF_CLCAFFE \"Add performance tests of clCaffe framework\" OFF)\nif(BUILD_PERF_TESTS)\n  if (OPENCV_DNN_PERF_CAFFE\n      OR ${the_module}_PERF_CAFFE  # compatibility for deprecated option\n  )\n    find_package(Caffe QUIET)\n    if (Caffe_FOUND)\n      ocv_target_compile_definitions(opencv_perf_dnn PRIVATE \"HAVE_CAFFE=1\")\n      ocv_target_link_libraries(opencv_perf_dnn caffe)\n    endif()\n  elseif(OPENCV_DNN_PERF_CLCAFFE\n         OR ${the_module}_PERF_CAFFE  # compatibility for deprecated option\n  )\n    find_package(Caffe QUIET)\n    if (Caffe_FOUND)\n      ocv_target_compile_definitions(opencv_perf_dnn PRIVATE \"HAVE_CLCAFFE=1\")\n      ocv_target_link_libraries(opencv_perf_dnn caffe)\n    endif()\n  endif()\nendif()\n\nif(DNN_ENABLE_PLUGINS)\n  ocv_target_compile_definitions(${the_module} PRIVATE ENABLE_PLUGINS)\n  if(TARGET opencv_test_dnn)\n    ocv_target_compile_definitions(opencv_test_dnn PRIVATE ENABLE_PLUGINS)\n  endif()\n  if(OPENCV_DEBUG_POSTFIX)\n    ocv_append_source_file_compile_definitions(\"${CMAKE_CURRENT_LIST_DIR}/src/backend.cpp\" \"DEBUG_POSTFIX=${OPENCV_DEBUG_POSTFIX}\")\n  endif()\nendif()\n\nocv_option(OPENCV_TEST_DNN_OPENVINO \"Build test with OpenVINO code\" (TARGET ocv.3rdparty.openvino))\nif(TARGET ocv.3rdparty.openvino AND OPENCV_TEST_DNN_OPENVINO)\n  if(TARGET opencv_test_dnn)\n    ocv_target_link_libraries(opencv_test_dnn ocv.3rdparty.openvino)\n  endif()\nendif()\n\nocv_option(OPENCV_TEST_DNN_CANN \"Build test with CANN\" (TARGET ocv.3rdparty.cann))\nif(TARGET ocv.3rdparty.cann AND OPENCV_TEST_DNN_CANN)\n  if(TARGET opencv_test_dnn)\n    ocv_target_link_libraries(opencv_test_dnn ocv.3rdparty.cann)\n  endif()\nendif()\n\nocv_option(OPENCV_TEST_DNN_TIMVX \"Build test with TIM-VX\" (HAVE_TIMVX))\nif(OPENCV_TEST_DNN_TIMVX)\n  if(TARGET opencv_test_dnn)\n    ocv_target_compile_definitions(opencv_test_dnn PRIVATE \"HAVE_TIMVX=1\")\n  endif()\nendif()\n\nocv_option(OPENCV_TEST_DNN_TFLITE \"Build test with TFLite\" (OPENCV_DNN_TFLITE))\nif(OPENCV_TEST_DNN_TFLITE)\n  if(TARGET opencv_test_dnn)\n    ocv_target_compile_definitions(opencv_test_dnn PRIVATE \"OPENCV_TEST_DNN_TFLITE=1\")\n  endif()\n  if(TARGET opencv_perf_dnn)\n    ocv_target_compile_definitions(opencv_perf_dnn PRIVATE \"OPENCV_TEST_DNN_TFLITE=1\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Application Build with CMake\nDESCRIPTION: This code snippet uses CMake commands to specify the source files and OpenCV modules required for building an application named \\'opencv_createsamples\\'. The \\'file(GLOB SRCS *.cpp)\\' command collects all C++ source files in the directory, and \\'ocv_add_application\\' links necessary OpenCV modules such as opencv_core, opencv_imgproc, opencv_objdetect, etc. Prerequisites include having CMake and OpenCV installed. Inputs include the source files, and outputs involve a configured build setup.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/createsamples/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRCS *.cpp)\nocv_add_application(opencv_createsamples\n    MODULES opencv_core opencv_imgproc opencv_objdetect opencv_imgcodecs opencv_highgui opencv_calib3d opencv_features2d opencv_videoio\n    SRCS ${SRCS})\n```\n\n----------------------------------------\n\nTITLE: Handling Non-Existent Files in Python with OpenCV\nDESCRIPTION: Shows how to handle attempts to read from non-existent files in Python using OpenCV, which initializes the data structure with default values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\npython/tutorial_code/core/file_input_output/file_input_output.py nonexist\n```\n\n----------------------------------------\n\nTITLE: Terminating Custom Destination Manager in JPEG Library (C)\nDESCRIPTION: Defines the `term_destination` function signature, a required method for a custom `jpeg_destination_mgr`. This function is called by `jpeg_finish_compress()` after all compressed data has been generated. Its responsibility is to flush any remaining data from the output buffer. The amount of data remaining can be determined from `next_output_byte` and the known buffer start/size, or by calculating `total_buffer_size - free_in_buffer`. This method is *not* called by `jpeg_abort()` or `jpeg_destroy()`; cleanup during aborts must be handled separately.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_52\n\nLANGUAGE: c\nCODE:\n```\nterm_destination (j_compress_ptr cinfo)\n```\n\n----------------------------------------\n\nTITLE: Configuring libspng CMake Project\nDESCRIPTION: Core CMake configuration for building libspng library. Sets up project variables, includes directories, and defines source files. Configures the library with ZLIB dependency and appropriate compiler definitions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libspng/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nproject(${SPNG_LIBRARY})\n\nset(CURR_INCLUDE_DIR \"${CMAKE_CURRENT_LIST_DIR}\")\nset_property(GLOBAL PROPERTY SPNG_INCLUDE_DIR ${CURR_INCLUDE_DIR})\nocv_include_directories(${ZLIB_INCLUDE_DIRS})\n\nfile(GLOB_RECURSE spng_headers RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\" \"*.h\")\nfile(GLOB_RECURSE spng_sources RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\" \"*.c\")\n\nmessage(STATUS \"libspng will be used as PNG codec\")\n```\n\n----------------------------------------\n\nTITLE: Building Qt for OpenCV\nDESCRIPTION: Command to build Qt using nmake after configuration\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_install/windows_install.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnmake\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Build Target for Android Examples in CMake\nDESCRIPTION: Creates a custom target named `opencv_android_examples`. This target acts as an aggregate or placeholder, likely used to group or trigger the build of all the included Android sample projects. It doesn't produce an output file itself but can depend on other targets defined within the subdirectories.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(opencv_android_examples)\n```\n\n----------------------------------------\n\nTITLE: Creating the G-API Module Target and Linking Dependencies in CMake\nDESCRIPTION: Finalizes the G-API module definition using `ocv_create_module()`, which creates the actual build target (library). It then links the created module target (`${the_module}`, typically `opencv_gapi`) against its essential private dependency 'ade'. It conditionally links against the OpenVINO target (`ocv.3rdparty.openvino`) if available and enabled via `OPENCV_GAPI_WITH_OPENVINO`, and against TBB (`tbb`) if `HAVE_TBB` is true.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nocv_create_module()\n\nocv_target_link_libraries(${the_module} PRIVATE ade)\n\nif(TARGET ocv.3rdparty.openvino AND OPENCV_GAPI_WITH_OPENVINO)\n  ocv_target_link_libraries(${the_module} PRIVATE ocv.3rdparty.openvino)\n  ocv_install_used_external_targets(ocv.3rdparty.openvino)\nendif()\n\nif(HAVE_TBB)\n  ocv_target_link_libraries(${the_module} PRIVATE tbb)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Executing PaddleSeg Portrait Segmentation Demo Script (Shell)\nDESCRIPTION: Runs the Python script `paddle_humanseg.py`. This script loads the previously converted ONNX model (`humanseg_hrnet18_tiny.onnx`) using `cv2.dnn.readNetFromONNX`, preprocesses an input image, performs inference for segmentation, postprocesses the output, and saves the visualized result.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython paddle_humanseg.py\n```\n\n----------------------------------------\n\nTITLE: Including Common Python Configuration in CMake\nDESCRIPTION: Includes a shared CMake file (common.cmake) with common configuration settings for Python bindings. After including the shared configuration, it unsets module-specific variables to avoid affecting other modules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/python/python3/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(../common.cmake)\n\nunset(MODULE_NAME)\nunset(MODULE_INSTALL_SUBDIR)\n```\n\n----------------------------------------\n\nTITLE: Configuring KleidiCV HAL Build Settings\nDESCRIPTION: Sets up the KleidiCV HAL project with specific compiler options and includes required dependencies. Handles SME2 option configuration and suppresses specific compiler warnings for unused functions and old-style casts.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/kleidicv/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nproject(kleidicv_hal)\n\nif(HAVE_KLEIDICV)\n  option(KLEIDICV_ENABLE_SME2 \"\" OFF) # not compatible with some CLang versions in NDK\n  include(\"${KLEIDICV_SOURCE_PATH}/adapters/opencv/CMakeLists.txt\")\n  # HACK to suppress adapters/opencv/kleidicv_hal.cpp:343:12: warning: unused function 'from_opencv' [-Wunused-function]\n  target_compile_options( kleidicv_hal PRIVATE\n      $<TARGET_PROPERTY:kleidicv,COMPILE_OPTIONS>\n      \"-Wno-old-style-cast\" \"-Wno-unused-function\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Drawing a Line in Python\nDESCRIPTION: Implementation of the MyLine function that draws a line between two points in OpenCV Python. The function takes the image, start and end points, and uses the line() function with specified color, thickness, and line type.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef MyLine(img, start, end):\n    thickness = 2\n    lineType = cv.LINE_8\n\n    cv.line(img,\n             start,\n             end,\n             (0, 0, 0),\n             thickness,\n             lineType)\n```\n\n----------------------------------------\n\nTITLE: Initializing Variables for JavaScript Tutorial Assets in CMake\nDESCRIPTION: Sets CMake variables defining the target HTML directory for Doxygen output (`opencv_tutorial_html_dir`), the source directory containing JavaScript tutorial assets (`js_tutorials_assets_dir`), and initializes an empty list for potential dependencies (`js_tutorials_assets_deps`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\n# js tutorial assets\nset(opencv_tutorial_html_dir \"${CMAKE_CURRENT_BINARY_DIR}/doxygen/html\")\nset(js_tutorials_assets_dir \"${CMAKE_CURRENT_SOURCE_DIR}/js_tutorials/js_assets\")\nset(js_tutorials_assets_deps \"\")\n```\n\n----------------------------------------\n\nTITLE: Installation Paths Configuration for OpenNI and PrimeSensor\nDESCRIPTION: Default installation paths for OpenNI libraries and PrimeSensor Module on different operating systems.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/kinect_openni.markdown#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nOpenNI:\n    Linux & MacOSX:\n        Libs into: /usr/lib\n        Includes into: /usr/include/ni\n    Windows:\n        Libs into: c:/Program Files/OpenNI/Lib\n        Includes into: c:/Program Files/OpenNI/Include\nPrimeSensor Module:\n    Linux & MacOSX:\n        Bins into: /usr/bin\n    Windows:\n        Bins into: c:/Program Files/Prime Sense/Sensor/Bin\n```\n\n----------------------------------------\n\nTITLE: Installing DNN Sample Source Files in CMake\nDESCRIPTION: Installs DNN example source files including CPP, HPP, and CMakeLists files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nocv_install_example_src(dnn *.cpp *.hpp CMakeLists.txt)\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Homogeneous Pattern Checking in C++\nDESCRIPTION: Implementation of the homogeneous pattern checking portion of the FAST corner detection algorithm. The code uses nested conditionals to compare pixel values at various offsets against a threshold to determine if a pixel is a corner point.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  if(ptr[offset5] < c_b)\n    if(ptr[offset9] < c_b)\n      if(ptr[offset6] < c_b)\n        if(ptr[offset7] < c_b)\n          if(ptr[offset4] < c_b)\n            if(ptr[offset3] < c_b)\n              if(ptr[offset8] < c_b)\n                goto success_homogeneous;\n              else\n                if(ptr[offset1] < c_b)\n                  if(ptr[offset2] < c_b)\n                    goto success_homogeneous;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n            else\n              if(ptr[offset8] < c_b)\n                if(ptr[offset10] < c_b)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n          else\n            if(ptr[offset11] < c_b)\n              if(ptr[offset8] < c_b)\n                if(ptr[offset10] < c_b)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n        else\n          goto homogeneous;\n      else\n        goto homogeneous;\n    else\n      if(ptr[offset2] < c_b)\n        if(ptr[offset3] < c_b)\n          if(ptr[offset4] < c_b)\n            if(ptr[offset7] < c_b)\n              if(ptr[offset1] < c_b)\n                if(ptr[offset6] < c_b)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset8] < c_b)\n                    goto success_homogeneous;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n        else\n          goto homogeneous;\n      else\n        goto homogeneous;\n  else\n    goto homogeneous;\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Required Dependencies for CUDA Samples in CMake\nDESCRIPTION: Sets a CMake variable `OPENCV_CUDA_SAMPLES_REQUIRED_DEPS` containing a list of OpenCV modules required to build the CUDA samples. This variable is used later for dependency checking and linking.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENCV_CUDA_SAMPLES_REQUIRED_DEPS\n  opencv_core\n  opencv_flann\n  opencv_imgproc\n  opencv_imgcodecs\n  opencv_videoio\n  opencv_highgui\n  opencv_ml\n  opencv_video\n  opencv_objdetect\n  opencv_features2d\n  opencv_calib3d\n  opencv_superres\n  opencv_cudaarithm\n  opencv_cudafilters\n  opencv_cudawarping\n  opencv_cudaimgproc\n  opencv_cudafeatures2d\n  opencv_cudaoptflow\n  opencv_cudabgsegm\n  opencv_cudastereo\n  opencv_cudaobjdetect)\n```\n\n----------------------------------------\n\nTITLE: Installing Third-party Licenses in CMake\nDESCRIPTION: This snippet installs third-party licenses associated with the module. It specifically mentions the MSCR component and its corresponding license file path. This ensures compliance with third-party software licenses in the OpenCV project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nocv_install_3rdparty_licenses(mscr \\\"${CMAKE_CURRENT_SOURCE_DIR}/3rdparty/mscr/chi_table_LICENSE.txt\\\")\n```\n\n----------------------------------------\n\nTITLE: Installing Build Tools and Cross-Compilers (Bash)\nDESCRIPTION: Updates the package list and installs essential packages required for building software and cross-compiling. This includes `git`, `cmake`, `pkgconf`, `build-essential`, the optional `ninja-build` for faster compilation, and toolchains for armhf (ARMv7) and arm64 (AArch64) targets.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update -y\nsudo apt install -y \\\n    git \\\n    cmake \\\n    pkgconf \\\n    build-essential \\\n    ninja-build \\\n    crossbuild-essential-armhf \\\n    crossbuild-essential-arm64\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows RT Features in CMake for OpenCV\nDESCRIPTION: This snippet handles Windows RT-specific configurations for OpenCV. It checks for Windows RT support and reports relevant information about the build environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32)\nstatus(\"\")\nstatus(\"  Windows RT support:\" WINRT THEN YES ELSE NO)\n  if(WINRT)\n    status(\"    Building for Microsoft platform: \" ${CMAKE_SYSTEM_NAME})\n    status(\"    Building for architectures: \" ${CMAKE_VS_EFFECTIVE_PLATFORMS})\n    status(\"    Building for version: \" ${CMAKE_SYSTEM_VERSION})\n    if (DEFINED ENABLE_WINRT_MODE_NATIVE)\n      status(\"    Building for C++ without CX extensions\")\n    endif()\n  endif()\nendif(WIN32)\n```\n\n----------------------------------------\n\nTITLE: Adjusting Compiler Flags for TBB Compatibility\nDESCRIPTION: Filters out compiler flags that are not handled properly by the TBB code. Removes visibility settings and certain warning flags that could cause build issues.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/tbb/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\n# filter out flags that are not handled well by the TBB code\nforeach(var CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_RELEASE CMAKE_CXX_FLAGS_DEBUG)\n  string(REPLACE \"-Werror=non-virtual-dtor\" \"\" ${var} \"${${var}}\")\n  string(REPLACE \"-fvisibility=hidden\" \"\" ${var} \"${${var}}\")\n  string(REPLACE \"-fvisibility-inlines-hidden\" \"\" ${var} \"${${var}}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Conditional Logic in C++\nDESCRIPTION: Implements pixel intensity comparison logic for corner detection using multiple offsets. The code evaluates various pixel positions relative to a central point to determine if the location qualifies as a corner based on intensity thresholds cb and c_b.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_34\n\nLANGUAGE: c++\nCODE:\n```\nif(ptr[offset6] > cb)\n  if(ptr[offset3] < c_b)\n    if(ptr[offset4] < c_b)\n      if(ptr[offset10] < c_b)\n        if(ptr[offset11] < c_b)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset6] < c_b)\n    if(ptr[offset3] < c_b)\n      if(ptr[offset4] < c_b)\n        if(ptr[offset10] < c_b)\n          if(ptr[offset11] < c_b)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    if(ptr[offset3] < c_b)\n      if(ptr[offset4] < c_b)\n        if(ptr[offset10] < c_b)\n          if(ptr[offset11] < c_b)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Adding Toggle Buttons for Multi-Language Examples - Doxygen Toggle Markup\nDESCRIPTION: Shows the pattern for adding toggle buttons in doxygen-based documentation for displaying alternative code or text blocks (e.g., for C++, Java, Python). Inputs are labeled toggle blocks wrapping content; outputs are tabbed interface in rendered documentation. No dependencies beyond doxygen configuration. Key parameters include button type/name.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_13\n\nLANGUAGE: markdown\nCODE:\n```\n@add_toggle{Button Name}\n\n  text / code / doxygen commands\n\n@end_toggle\n```\n\n----------------------------------------\n\nTITLE: Defining a Typed Kernel Interface with G_TYPED_KERNEL (OpenCV G-API, C++)\nDESCRIPTION: Declares a new G-API kernel interface using the G_TYPED_KERNEL macro, specifying the kernel name, function signature, and a string identifier. This macro generates a type for graph pipeline construction and facilitates future backend-specific implementations. Requires OpenCV G-API, and must include a definition of outMeta() to propagate metadata within the pipeline. Inputs and outputs are specified using G-API dynamic types and standard C++ types, as required by the macro signature.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/doc/20-kernel-api.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#define G_TYPED_KERNEL(KernelName, Signature, Id)\n// Macro expands to kernel interface type declaration\n// Usage example in doc:\nG_TYPED_KERNEL(Filter2D, <cv::GMat(cv::GMat, cv::Mat, cv::Point)>, \"org.opencv.filter2d\");\n// where\n//   - Filter2D        : Kernel interface name\n//   - cv::GMat        : Return type (G-API dynamic type)\n//   - cv::GMat, ...   : Argument types\n//   - \"org.opencv.filter2d\" : String ID for internal use\n```\n\n----------------------------------------\n\nTITLE: Estimating Diamond Marker Pose as ChArUco Board (C++)\nDESCRIPTION: This snippet highlights an alternative approach to pose estimation, treating the detected diamond marker as a small ChArUco board using OpenCV's routines. With the four corners and known square size, the diamond is processed as a mini board for pose computation. This approach makes use of the aruco/charuco pose estimation functions, leveraging existing ChArUco infrastructure. Requires OpenCV aruco & charuco modules, and accurate detection/calibration data. The output is the 3D pose of the diamond marker relative to the camera.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_diamond_detection/charuco_diamond_detection.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n// Pose estimation as ChArUco board\ncv::Ptr<cv::aruco::CharucoBoard> diamondBoard = cv::aruco::CharucoBoard::create(/*specify board parameters*/);\ncv::Vec3d rvec, tvec;\nbool valid = cv::aruco::estimatePoseCharucoBoard(\n    diamondCorners[0], // or proper detected corners\n    diamondIds[0],\n    diamondBoard,\n    cameraMatrix,\n    distCoeffs,\n    rvec,\n    tvec\n);\nif (valid) {\n    cv::aruco::drawAxis(image, cameraMatrix, distCoeffs, rvec, tvec, squareLength * 0.5);\n}\n```\n\n----------------------------------------\n\nTITLE: Linking regex Library for QNX in OpenCV ts Module\nDESCRIPTION: Checks for the availability of the regex library on QNX systems and links it to the ts module if found.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/ts/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_SYSTEM_NAME STREQUAL \"QNX\")\n  include(CheckLibraryExists)\n  CHECK_LIBRARY_EXISTS(regex regexec \"\" HAVE_REGEX_LIBRARY)\n  if(HAVE_REGEX_LIBRARY)\n    ocv_target_link_libraries(${the_module} PUBLIC regex)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Check Compiler Support for Built-in Functions in CMake\nDESCRIPTION: These code snippets test whether the compiler supports various __builtin functions like __builtin_ctz and __builtin_assume_aligned. When available, these are defined as macros for further use during optimized code generation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# Check for __attribute__((aligned(x))) support in the compiler\n#\ncheck_c_source_compiles(\n    \"int main(void) {\n        __attribute__((aligned(8))) int test = 0;\n        (void)test;\n        return 0;\n    }\"\n    HAVE_ATTRIBUTE_ALIGNED FAIL_REGEX \"aligned\")\nif(HAVE_ATTRIBUTE_ALIGNED)\n    add_definitions(-DHAVE_ATTRIBUTE_ALIGNED)\nendif()\n\n#\n# Check for __builtin_assume_aligned(x,n) support in the compiler\n#\ncheck_c_source_compiles(\n    \"char *test(char *buffer) {\n        char *abuffer = __builtin_assume_aligned(buffer,64);\n        return abuffer;\n    }\n    int main() {\n        return 0;\n    }\"\n    HAVE_BUILTIN_ASSUME_ALIGNED)\nif(HAVE_BUILTIN_ASSUME_ALIGNED)\n    add_definitions(-DHAVE_BUILTIN_ASSUME_ALIGNED)\nendif()\n\n#\n# check for __builtin_ctz() support in the compiler\n#\ncheck_c_source_compiles(\n    \"int main(void) {\n        unsigned int zero = 0;\n        long test = __builtin_ctz(zero);\n        (void)test;\n        return 0;\n    }\"\n    HAVE_BUILTIN_CTZ\n)\nif(HAVE_BUILTIN_CTZ)\n    add_definitions(-DHAVE_BUILTIN_CTZ)\nendif()\n\n#\n# check for __builtin_ctzll() support in the compiler\n#\ncheck_c_source_compiles(\n    \"int main(void) {\n        unsigned int zero = 0;\n        long test = __builtin_ctzll(zero);\n        (void)test;\n        return 0;\n    }\"\n    HAVE_BUILTIN_CTZLL\n)\nif(HAVE_BUILTIN_CTZLL)\n    add_definitions(-DHAVE_BUILTIN_CTZLL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Including Common CMake Configuration File\nDESCRIPTION: This line includes another CMake file named `common.cmake` located in the same directory as the current `CMakeLists.txt` file (`CMAKE_CURRENT_SOURCE_DIR`). This is used to import shared settings, functions, or macros common to different parts of the Java module build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(${CMAKE_CURRENT_SOURCE_DIR}/common.cmake)\n```\n\n----------------------------------------\n\nTITLE: Installing Media and GUI Dependencies\nDESCRIPTION: Commands to install dependencies for GUI features, camera support, and media handling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_ubuntu/py_setup_in_ubuntu.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install libavcodec-dev libavformat-dev libswscale-dev\nsudo apt-get install libgstreamer-plugins-base1.0-dev libgstreamer1.0-dev\n```\n\n----------------------------------------\n\nTITLE: Loading a Fixed Quantization Table in C using libjpeg\nDESCRIPTION: Demonstrates how to load a predefined quantization table into a specific slot (`n`) within the libjpeg decompression context (`cinfo`). It allocates memory for the table if necessary using `jpeg_alloc_quant_table` and then copies the 64 quantization values from a source array (`Qtable`) into the `quantval` member of the `JQUANT_TBL` structure. This is typically used when preparing to decode an abbreviated JPEG image that relies on external tables.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_59\n\nLANGUAGE: c\nCODE:\n```\nif (cinfo.quant_tbl_ptrs[n] == NULL)\n  cinfo.quant_tbl_ptrs[n] = jpeg_alloc_quant_table((j_common_ptr) &cinfo);\nquant_ptr = cinfo.quant_tbl_ptrs[n];        /* quant_ptr is JQUANT_TBL* */\nfor (i = 0; i < 64; i++) {\n  /* Qtable[] is desired quantization table, in natural array order */\n  quant_ptr->quantval[i] = Qtable[i];\n}\n```\n\n----------------------------------------\n\nTITLE: Finding Highgui External Header Files using Glob in CMake\nDESCRIPTION: Uses the `file(GLOB ...)` command to find all files matching the patterns `*.hpp` and `*.h` within specific include directories related to the highgui module (`include/opencv2/`, `include/opencv2/${name}/`). The results are stored in the `highgui_ext_hdrs` variable, likely used for installation or packaging.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB highgui_ext_hdrs\n     \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/*.hpp\"\n     \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/*.hpp\"\n     \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/*.h\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Matrix for Blurred Image with OpenCV in Clojure\nDESCRIPTION: Creates a new Mat object with the same dimensions and element type as the original image. This will hold the result after applying image processing functions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_23\n\nLANGUAGE: Clojure\nCODE:\n```\nuser=> (def blurred (Mat. 512 512 CvType/CV_8UC3))\n#'user/blurred\nuser=>\n```\n\n----------------------------------------\n\nTITLE: Configuring x86 SSSE3 Optimizations for ZLIB in CMake\nDESCRIPTION: Checks if SSSE3 optimization is enabled (WITH_SSSE3), if intrinsics are available (HAVE_SSSE3_INTRIN), and if SSE2 is also enabled. If all conditions are met, it adds the DX86_SSSE3 definition, appends SSSE3-specific source files (for adler32 and chunkset) to ZLIB_ARCH_SRCS, adds feature information, and sets specific compile flags (SSSE3FLAG, NOLTOFLAG) for these files. Otherwise, it disables SSSE3 support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_27\n\nLANGUAGE: cmake\nCODE:\n```\n        if(WITH_SSSE3)\n            check_ssse3_intrinsics()\n            if(HAVE_SSSE3_INTRIN AND WITH_SSE2)\n                add_definitions(-DX86_SSSE3)\n                set(SSSE3_SRCS ${ARCHDIR}/adler32_ssse3.c ${ARCHDIR}/chunkset_ssse3.c)\n                add_feature_info(SSSE3_ADLER32 1 \"Support SSSE3-accelerated adler32, using \\\"${SSSE3FLAG}\\\"\")\n                list(APPEND ZLIB_ARCH_SRCS ${SSSE3_SRCS})\n                set_property(SOURCE ${SSSE3_SRCS} PROPERTY COMPILE_FLAGS \"${SSSE3FLAG} ${NOLTOFLAG}\")\n            else()\n                set(WITH_SSSE3 OFF)\n            endif()\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Helper Function for File Existence Checking\nDESCRIPTION: Implements a utility function that appends files to a list only if they exist on the filesystem. Provides warnings for missing files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/protobuf/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(append_if_exist OUTPUT_LIST)\n    set(${OUTPUT_LIST})\n    foreach(fil ${ARGN})\n        if(EXISTS ${fil})\n            list(APPEND ${OUTPUT_LIST} \"${fil}\")\n        else()\n            message(WARNING \"file missing: ${fil}\")\n        endif()\n    endforeach()\n    set(${OUTPUT_LIST} ${${OUTPUT_LIST}} PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Including Sample Utilities for Standalone Build in CMake\nDESCRIPTION: Includes the `samples_utils.cmake` file within the standalone build context. This script might contain different logic or rely on the found OpenCV package compared to the version used during the integrated build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(\"${CMAKE_CURRENT_LIST_DIR}/samples_utils.cmake\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Configuration of Python Modules in OpenCV\nDESCRIPTION: This CMake script configures the Python support modules during the OpenCV build process. It checks for specific conditions such as the build being for Android or a particular framework and accordingly enables or disables Python 2 and 3 modules. The script requires CMake version 3.5 or higher and manages dependencies by including directories for bindings and tests.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/python/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# ----------------------------------------------------------------------------\n#  CMake file for python support\n# ----------------------------------------------------------------------------\nif(DEFINED OPENCV_INITIAL_PASS)  # OpenCV build\n\nif(ANDROID OR APPLE_FRAMEWORK OR WINRT)\n  ocv_module_disable_(python2)\n  ocv_module_disable_(python3)\n  return()\nelseif(BUILD_opencv_world OR (WIN32 AND CMAKE_BUILD_TYPE STREQUAL \"Debug\"))\n  if(NOT DEFINED BUILD_opencv_python2)\n    set(__disable_python2 ON)\n  endif()\n  if(NOT DEFINED BUILD_opencv_python3)\n    set(__disable_python3 ON)\n  endif()\nendif()\n\nadd_subdirectory(bindings)\n\nadd_subdirectory(test)\n\nif(__disable_python2)\n  ocv_module_disable_(python2)\nendif()\nif(__disable_python3)\n  ocv_module_disable_(python3)\nendif()\nif(__disable_python2 AND __disable_python3)\n  return()\nendif()\n\nadd_subdirectory(python2)\nadd_subdirectory(python3)\n\nelse()  # standalone build\n\ncmake_minimum_required(VERSION 3.5)\nproject(OpenCVPython CXX C)\ninclude(\"./standalone.cmake\")\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Carotene CMake Project\nDESCRIPTION: Initializes the CMake project for Carotene and configures basic project variables and directories. Sets up namespace configuration and includes required directories.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/carotene/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION ${MIN_VER_CMAKE} FATAL_ERROR)\n\nproject(Carotene)\n\nset(CAROTENE_NS \"carotene\" CACHE STRING \"Namespace for Carotene definitions\")\n\nset(CAROTENE_INCLUDE_DIR include)\nset(CAROTENE_SOURCE_DIR src)\n\nfile(GLOB_RECURSE carotene_headers RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\" \"${CAROTENE_INCLUDE_DIR}/*.hpp\")\nfile(GLOB_RECURSE carotene_sources RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\" \"${CAROTENE_SOURCE_DIR}/*.cpp\"\n                                                                        \"${CAROTENE_SOURCE_DIR}/*.hpp\")\n\ninclude_directories(${CAROTENE_INCLUDE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Testing Python 3 OpenCV Wrapper\nDESCRIPTION: Run a Python script to check if the OpenCV Python 3 wrapper is correctly installed by printing OpenCV's build information.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\npython3 -c \"import cv2; print(cv2.getBuildInformation())\"\n```\n\n----------------------------------------\n\nTITLE: Pixel Corner Detection Logic in C++\nDESCRIPTION: Complex nested conditional logic for determining if a pixel represents a corner by comparing intensity values with neighboring pixels using pointer arithmetic. The code uses goto statements to branch between 'is_a_corner' and 'is_not_a_corner' labels based on multiple threshold comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_35\n\nLANGUAGE: C++\nCODE:\n```\n                                    goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                else\n                  if(ptr[offset7] > cb)\n                    if(ptr[offset9] < c_b)\n                      goto is_not_a_corner;\n                    else\n                      if(ptr[offset9] > cb)\n                        if(ptr[offset1] > cb)\n                          if(ptr[offset6] < c_b)\n                            goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset8] > cb)\n                                if(ptr[offset4] > cb)\n                                  if(ptr[offset3] > cb)\n                                    goto is_a_corner;\n                                  else\n                                    if(ptr[offset10] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                else\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset1] < c_b)\n                            if(ptr[offset6] < c_b)\n                              goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset8] > cb)\n                                  if(ptr[offset4] > cb)\n                                    if(ptr[offset3] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      if(ptr[offset10] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                  else\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] < c_b)\n                              goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset8] > cb)\n                                  if(ptr[offset4] > cb)\n                                    if(ptr[offset3] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      if(ptr[offset10] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                  else\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset9] < c_b)\n                      if(ptr[offset7] < c_b)\n                        if(ptr[offset1] > cb)\n                          if(ptr[offset6] > cb)\n                            goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] < c_b)\n                              if(ptr[offset8] < c_b)\n                                if(ptr[offset10] < c_b)\n                                  if(ptr[offset11] < c_b)\n                                    goto is_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset1] < c_b)\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset8] < c_b)\n                                if(ptr[offset10] < c_b)\n                                  if(ptr[offset11] < c_b)\n                                    goto is_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] < c_b)\n                                if(ptr[offset8] < c_b)\n                                  if(ptr[offset10] < c_b)\n                                    if(ptr[offset11] < c_b)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                if(ptr[offset8] < c_b)\n                                  if(ptr[offset10] < c_b)\n                                    if(ptr[offset11] < c_b)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] > cb)\n                              goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] < c_b)\n                                if(ptr[offset8] < c_b)\n                                  if(ptr[offset10] < c_b)\n                                    if(ptr[offset11] < c_b)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n            else\n              if(ptr[offset2] > cb)\n                if(ptr[offset7] < c_b)\n                  if(ptr[offset9] > cb)\n                    goto is_not_a_corner;\n                  else\n                    if(ptr[offset9] < c_b)\n                      if(ptr[offset1] > cb)\n                        if(ptr[offset6] > cb)\n                          goto is_not_a_corner;\n                        else\n                          if(ptr[offset6] < c_b)\n                            if(ptr[offset8] < c_b)\n                              if(ptr[offset10] < c_b)\n                                if(ptr[offset11] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset1] < c_b)\n                          if(ptr[offset6] > cb)\n                            if(ptr[offset8] < c_b)\n                              if(ptr[offset10] < c_b)\n                                if(ptr[offset11] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] < c_b)\n                              if(ptr[offset8] < c_b)\n                                if(ptr[offset10] < c_b)\n                                  if(ptr[offset11] < c_b)\n                                    goto is_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Compiler Optimization Flags Configuration\nDESCRIPTION: Sets up various compiler optimization flags based on the platform and compiler version, particularly for GCC and Clang.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/carotene/hal/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CV_GCC OR CV_CLANG)\n  if(X86 OR ARMEABI_V6 OR (MIPS AND ANDROID_COMPILER_VERSION VERSION_LESS \"4.6\"))\n    list(APPEND TEGRA_COMPILER_FLAGS -fweb -fwrapv -frename-registers -fsched-stalled-insns-dep=100 -fsched-stalled-insns=2)\n  elseif(CV_CLANG)\n    list(APPEND TEGRA_COMPILER_FLAGS -fwrapv)\n  else()\n    list(APPEND TEGRA_COMPILER_FLAGS -fweb -fwrapv -frename-registers -fsched2-use-superblocks -fsched2-use-traces\n                                     -fsched-stalled-insns-dep=100 -fsched-stalled-insns=2)\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Build Options in CMake\nDESCRIPTION: This snippet defines various CMake options for fine-tuning the OpenCV build process. It includes options for enabling ccache, precompiled headers, profiling, and other compiler-specific optimizations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(ENABLE_CCACHE              \"Use ccache\"                                               (UNIX AND (CMAKE_GENERATOR MATCHES \"Makefile\" OR CMAKE_GENERATOR MATCHES \"Ninja\" OR CMAKE_GENERATOR MATCHES \"Xcode\")) )\nOCV_OPTION(ENABLE_PRECOMPILED_HEADERS \"Use precompiled headers\"                                  MSVC IF (MSVC OR (NOT IOS AND NOT XROS AND NOT CMAKE_CROSSCOMPILING) ) )\nOCV_OPTION(ENABLE_DELAYLOAD           \"Enable delayed loading of OpenCV DLLs\"                    OFF VISIBLE_IF MSVC AND BUILD_SHARED_LIBS)\nOCV_OPTION(ENABLE_SOLUTION_FOLDERS    \"Solution folder in Visual Studio or in other IDEs\"        (MSVC_IDE OR CMAKE_GENERATOR MATCHES Xcode) )\nOCV_OPTION(ENABLE_PROFILING           \"Enable profiling in the GCC compiler (Add flags: -g -pg)\" OFF  IF CV_GCC )\nOCV_OPTION(ENABLE_COVERAGE            \"Enable coverage collection with  GCov\"                    OFF  IF CV_GCC )\nOCV_OPTION(OPENCV_ENABLE_MEMORY_SANITIZER \"Better support for memory/address sanitizers\"         OFF)\nOCV_OPTION(ENABLE_OMIT_FRAME_POINTER  \"Enable -fomit-frame-pointer for GCC\"                      ON   IF CV_GCC )\nOCV_OPTION(ENABLE_POWERPC             \"Enable PowerPC for GCC\"                                   ON   IF (CV_GCC AND CMAKE_SYSTEM_PROCESSOR MATCHES powerpc.*) )\nOCV_OPTION(ENABLE_FAST_MATH           \"Enable compiler options for fast math optimizations on FP computations (not recommended)\" OFF)\nOCV_OPTION(ENABLE_NOISY_WARNINGS      \"Show all warnings even if they are too noisy\"             OFF )\nOCV_OPTION(OPENCV_WARNINGS_ARE_ERRORS \"Treat warnings as errors\"                                 OFF )\nOCV_OPTION(ANDROID_EXAMPLES_WITH_LIBS \"Build binaries of Android examples with native libraries\" OFF  IF ANDROID )\nOCV_OPTION(ENABLE_IMPL_COLLECTION     \"Collect implementation data on function call\"             OFF )\nOCV_OPTION(ENABLE_INSTRUMENTATION     \"Instrument functions to collect calls trace and performance\" OFF )\nOCV_OPTION(ENABLE_GNU_STL_DEBUG       \"Enable GNU STL Debug mode (defines _GLIBCXX_DEBUG)\"       OFF IF CV_GCC )\nOCV_OPTION(ENABLE_BUILD_HARDENING     \"Enable hardening of the resulting binaries (against security attacks, detects memory corruption, etc)\" OFF)\nOCV_OPTION(ENABLE_LTO                 \"Enable Link Time Optimization\" OFF IF CV_GCC OR MSVC)\nOCV_OPTION(ENABLE_THIN_LTO            \"Enable Thin LTO\" OFF IF CV_CLANG)\nOCV_OPTION(GENERATE_ABI_DESCRIPTOR    \"Generate XML file for abi_compliance_checker tool\" OFF IF UNIX)\nOCV_OPTION(OPENCV_GENERATE_PKGCONFIG  \"Generate .pc file for pkg-config build tool (deprecated)\" OFF)\nOCV_OPTION(CV_ENABLE_INTRINSICS       \"Use intrinsic-based optimized code\" ON )\nOCV_OPTION(CV_DISABLE_OPTIMIZATION    \"Disable explicit optimized code (dispatched code/intrinsics/loop unrolling/etc)\" OFF )\nOCV_OPTION(CV_TRACE                   \"Enable OpenCV code trace\" ON)\nOCV_OPTION(OPENCV_GENERATE_SETUPVARS  \"Generate setup_vars* scripts\" ON IF (NOT ANDROID AND NOT APPLE_FRAMEWORK) )\nOCV_OPTION(ENABLE_CONFIG_VERIFICATION \"Fail build if actual configuration doesn't match requested (WITH_XXX != HAVE_XXX)\" OFF)\nOCV_OPTION(OPENCV_ENABLE_MEMALIGN     \"Enable posix_memalign or memalign usage\" ON)\nOCV_OPTION(OPENCV_DISABLE_FILESYSTEM_SUPPORT \"Disable filesystem support\" OFF)\nOCV_OPTION(OPENCV_DISABLE_THREAD_SUPPORT \"Build the library without multi-threaded code.\" OFF)\nOCV_OPTION(OPENCV_DISABLE_ENV_SUPPORT \"Disable environment variables access (getenv)\" (CMAKE_SYSTEM_NAME MATCHES \"Windows(CE|Phone|Store)\"))\nOCV_OPTION(OPENCV_SEMIHOSTING         \"Build the library for semihosting target (Arm). See https://developer.arm.com/documentation/100863/latest.\" OFF)\nOCV_OPTION(ENABLE_CUDA_FIRST_CLASS_LANGUAGE \"Enable CUDA as a first class language, if enabled dependant projects will need to use CMake >= 3.18\" OFF\n  VISIBLE_IF (WITH_CUDA AND NOT CMAKE_VERSION VERSION_LESS 3.18)\n  VERIFY HAVE_CUDA)\n\nOCV_OPTION(ENABLE_PYLINT              \"Add target with Pylint checks\"                            (BUILD_DOCS OR BUILD_EXAMPLES) IF (NOT CMAKE_CROSSCOMPILING AND NOT APPLE_FRAMEWORK) )\nOCV_OPTION(ENABLE_FLAKE8              \"Add target with Python flake8 checker\"                    (BUILD_DOCS OR BUILD_EXAMPLES) IF (NOT CMAKE_CROSSCOMPILING AND NOT APPLE_FRAMEWORK) )\n```\n\n----------------------------------------\n\nTITLE: Disabling Python3 Module When Dependencies Are Missing in CMake\nDESCRIPTION: Checks if required Python3 include path and NumPy include directories are available. If not, it disables the Python3 module from being built with OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/python/python3/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT PYTHON3_INCLUDE_PATH OR NOT PYTHON3_NUMPY_INCLUDE_DIRS)\n  ocv_module_disable(python3)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV Repository\nDESCRIPTION: Installs Git and clones the OpenCV repository from GitHub.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nyum install git\ngit clone https://github.com/opencv/opencv.git\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for OpenCV.js Documentation - Bash\nDESCRIPTION: This bash command builds a Docker image named \"opencv-js-doc\" from the local Dockerfile, which includes Doxygen and is based on emscripten/emsdk:2.0.10. Requires the Dockerfile present in the current directory. Produces a reusable Docker image for subsequent documentation builds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . -t opencv-js-doc\n```\n\n----------------------------------------\n\nTITLE: Defining and Checking Required Dependencies for TAPI Samples - CMake\nDESCRIPTION: Defines the required OpenCV modules for TAPI samples in a variable and then checks their availability using ocv_check_dependencies. Ensures each sample is only built if all required modules (like opencv_core, opencv_imgproc, etc.) are present. Key parameter is OPENCV_TAPI_SAMPLES_REQUIRED_DEPS, which must contain all module names needed for TAPI sample functionality.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/tapi/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(OPENCV_TAPI_SAMPLES_REQUIRED_DEPS\n  opencv_core\n  opencv_imgproc\n  opencv_video\n  opencv_imgcodecs\n  opencv_videoio\n  opencv_highgui\n  opencv_objdetect\n  opencv_features2d\n  opencv_calib3d\n  opencv_flann)\nocv_check_dependencies(${OPENCV_TAPI_SAMPLES_REQUIRED_DEPS})\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVX Directory and Including HAL Subdirectory in CMake\nDESCRIPTION: This CMake code snippet executes if the preceding check determines that OpenVX is available (`HAVE_OPENVX` is true). It sets the CMake variable `OPENCV_3P_OPENVX_DIR` to the path of the current source directory (`CMAKE_CURRENT_SOURCE_DIR`). Then, it uses `add_subdirectory(hal)` to include the `hal` subdirectory, instructing CMake to process the `CMakeLists.txt` within that directory, which is necessary for building the OpenVX Hardware Abstraction Layer.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openvx/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENCV_3P_OPENVX_DIR ${CMAKE_CURRENT_SOURCE_DIR})\nadd_subdirectory(hal)\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration Output for OpenCV Cross-Compilation\nDESCRIPTION: This snippet shows the CMake configuration output for cross-compiling OpenCV. It confirms the host system as Linux x86_64, the target system as Linux arm, and shows that FFmpeg is available. The output also includes version information and enabled features.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_30\n\nLANGUAGE: cmake\nCODE:\n```\n-- General configuration for OpenCV 4.8.0-dev =====================================\n--   Version control:               408730b\n--\n--   Extra modules:\n--     Location (extra):            /home/kmtr/work/opencv_contrib/modules\n--     Version control (extra):     faa5468\n--\n--   Platform:\n--     Timestamp:                   2023-12-02T03:39:58Z\n--     Host:                        Linux 6.5.0-13-generic x86_64\n--     Target:                      Linux 1 arm\n--     CMake:                       3.27.4\n--     CMake generator:             Ninja\n--     CMake build tool:            /usr/bin/ninja\n--     Configuration:               Release\n--\n--   CPU/HW features:\n--     Baseline:                    NEON\n--       requested:                 DETECT\n--       required:                  NEON\n--       disabled:                  VFPV3\n--\n--   C/C++:\n--     Built as dynamic libs?:      YES\n--     C++ standard:                11\n--     C++ Compiler:                /usr/bin/arm-linux-gnueabihf-g++  (ver 13.2.0)\n\n:\n:\n\n--\n--   Video I/O:\n--     DC1394:                      NO\n--     FFMPEG:                      YES\n--       avcodec:                   YES (60.3.100)\n--       avformat:                  YES (60.3.100)\n--       avutil:                    YES (58.2.100)\n--       swscale:                   YES (7.1.100)\n--       avresample:                NO\n--     GStreamer:                   NO\n--     v4l/v4l2:                    YES (linux/videodev2.h)\n--\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Target for OpenCV Java Bindings Generation in CMake\nDESCRIPTION: Creates a CMake custom target named `gen_opencv_java_source`. This target serves as a build system entity that depends on the output files generated by the previously defined custom command (`java_generated_files`). It lists the main Python generator script (`gen_java.py`) and the JSON configuration file (`gen_java.json`) as sources, which helps integrate the generation process into IDEs and build system dependency tracking.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/generator/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(gen_opencv_java_source DEPENDS ${java_generated_files}\n    SOURCES \"${JAVA_SOURCE_DIR}/generator/gen_java.py\"\n            \"${CMAKE_CURRENT_BINARY_DIR}/gen_java.json\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Interactive SVG Option for Doxygen in CMake\nDESCRIPTION: Sets a CMake cache variable `OPENCV_DOCS_INTERACTIVE_SVG` (defaulting to 'NO') to control the `INTERACTIVE_SVG` option in Doxygen, specifically addressing compatibility issues with certain Doxygen versions. It also sets the corresponding internal variable `CMAKECONFIG_INTERACTIVE_SVG` for the Doxyfile template.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\n# Doxygen 1.8.16 fix: https://github.com/doxygen/doxygen/pull/6870\n# NO is needed here: https://github.com/opencv/opencv/pull/16039\nset(OPENCV_DOCS_INTERACTIVE_SVG \"NO\" CACHE BOOL \"Doxygen/INTERACTIVE_SVG value\")\nset(CMAKECONFIG_INTERACTIVE_SVG \"${OPENCV_DOCS_INTERACTIVE_SVG}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing AGAST Pixel Offsets in OpenCV (C++)\nDESCRIPTION: Defines the makeAgastOffsets function, which sets up offset tables for different AGAST detector variants (OAST_9_16, AGAST_7_12d, AGAST_7_12s, and AGAST_5_8). The function takes a destination pixel array, row stride, and detector type, filling the offsets for use in AGAST corner evaluation. This depends on OpenCV infrastructure (constants/enumerations) and ensures that only supported types are processed; CV_Assert is used for sanity checks. The output is a 16-length pixel array with relative offsets for neighborhood sampling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include \"agast_score.hpp\"\n\n#ifdef _MSC_VER\n#pragma warning( disable : 4127 )\n#endif\n\nnamespace cv\n{\n\nvoid makeAgastOffsets(int pixel[16], int rowStride, int type)\n{\n    static const int offsets16[][2] =\n    {\n        {-3,  0}, {-3, -1}, {-2, -2}, {-1, -3}, {0, -3}, { 1, -3}, { 2, -2}, { 3, -1},\n        { 3,  0}, { 3,  1}, { 2,  2}, { 1,  3}, {0,  3}, {-1,  3}, {-2,  2}, {-3,  1}\n    };\n\n    static const int offsets12d[][2] =\n    {\n        {-3,  0}, {-2, -1}, {-1, -2}, {0, -3}, { 1, -2}, { 2, -1},\n        { 3,  0}, { 2,  1}, { 1,  2}, {0,  3}, {-1,  2}, {-2,  1}\n    };\n\n    static const int offsets12s[][2] =\n    {\n        {-2,  0}, {-2, -1}, {-1, -2}, {0, -2}, { 1, -2}, { 2, -1},\n        { 2,  0}, { 2,  1}, { 1,  2}, {0,  2}, {-1,  2}, {-2,  1}\n    };\n\n    static const int offsets8[][2] =\n    {\n        {-1,  0}, {-1, -1}, {0, -1}, { 1, -1},\n        { 1,  0}, { 1,  1}, {0,  1}, {-1,  1}\n    };\n\n    const int (*offsets)[2] = type == AgastFeatureDetector::OAST_9_16 ? offsets16 :\n                              type == AgastFeatureDetector::AGAST_7_12d ? offsets12d :\n                              type == AgastFeatureDetector::AGAST_7_12s ? offsets12s :\n                              type == AgastFeatureDetector::AGAST_5_8 ? offsets8  : 0;\n\n    CV_Assert(pixel && offsets);\n\n    int k = 0;\n    for( ; k < 16; k++ )\n        pixel[k] = offsets[k][0] + offsets[k][1] * rowStride;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV Tests\nDESCRIPTION: Command to run the basic OpenCV tests using make.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ make test\n```\n\n----------------------------------------\n\nTITLE: Visualizing Raw and Estimated Poses in C++ (OpenCV)\nDESCRIPTION: This C++ snippet demonstrates drawing the results of pose estimation onto an image (`frame_vis`) using OpenCV (Step X, optional visualization). It calls a helper function `drawObjectMesh` twice: once to render a 3D mesh based on the raw PnP result (`pnp_detection`, drawn in green) and once for the Kalman-filtered estimate (`pnp_detection_est`, drawn in yellow). It then computes the 2D image projections of 3D coordinate axes endpoints using `backproject3DPoint` based on the estimated pose and draws these axes using `draw3DCoordinateAxes`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_27\n\nLANGUAGE: cpp\nCODE:\n```\n// -- Step X: Draw pose\n\ndrawObjectMesh(frame_vis, &mesh, &pnp_detection, green);                // draw current pose\ndrawObjectMesh(frame_vis, &mesh, &pnp_detection_est, yellow);           // draw estimated pose\n\ndouble l = 5;\nstd::vector<cv::Point2f> pose_points2d;\npose_points2d.push_back(pnp_detection_est.backproject3DPoint(cv::Point3f(0,0,0)));    // axis center\npose_points2d.push_back(pnp_detection_est.backproject3DPoint(cv::Point3f(l,0,0)));    // axis x\npose_points2d.push_back(pnp_detection_est.backproject3DPoint(cv::Point3f(0,l,0)));    // axis y\npose_points2d.push_back(pnp_detection_est.backproject3DPoint(cv::Point3f(0,0,l)));    // axis z\ndraw3DCoordinateAxes(frame_vis, pose_points2d);                                       // draw axes\n```\n\n----------------------------------------\n\nTITLE: Implementing CvVideoCameraDelegate Protocol (Objective-C++)\nDESCRIPTION: This code demonstrates how to implement the CvVideoCameraDelegate protocol, which is necessary for processing video frames. It includes a basic image processing example that inverts the colors of the frame.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/video_processing/video_processing.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Objective-C++\nCODE:\n```\n#pragma mark - Protocol CvVideoCameraDelegate\n\n#ifdef __cplusplus\n- (void)processImage:(Mat&)image;\n{\n    // Do some OpenCV stuff with the image\n    Mat image_copy;\n    cvtColor(image, image_copy, COLOR_BGR2GRAY);\n\n    // invert image\n    bitwise_not(image_copy, image_copy);\n\n    //Convert BGR to BGRA (three channel to four channel)\n    Mat bgr;\n    cvtColor(image_copy, bgr, COLOR_GRAY2BGR);\n\n    cvtColor(bgr, image, COLOR_BGR2BGRA);\n}\n#endif\n```\n\n----------------------------------------\n\nTITLE: Executing Custom OCR Model using OpenCV DNN from the Command Line - Bash\nDESCRIPTION: This snippet demonstrates how to start the OpenCV-based text detection and recognition pipeline using command-line arguments. The text_detection tool requires specifying the path to both the text detection and text recognition models (typically in ONNX format). The '-m' flag denotes the detection model, while '-ocr' indicates the OCR (recognition) model. No installation is documented here, but it presumes 'text_detection' is either in your PATH or runnable from the current directory, and compatible model files are available. The expected input is a live or provided image stream, and the output will be recognized text printed or displayed depending on the implementation. Ensure OpenCV's DNN module and any dependencies required by the tool are installed. This command may require additional arguments for input/output configuration depending on your use case.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_OCR/dnn_OCR.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ text_detection -m=[path_to_text_detect_model] -ocr=[path_to_text_recognition_model]\n```\n\n----------------------------------------\n\nTITLE: Checking for Aligned Memory Allocation Functions in CMake\nDESCRIPTION: Checks for the availability of various functions used for allocating aligned memory blocks. It first checks for `malloc.h` using `check_include_files`. Then, using `check_symbol_exists`, it checks for `_aligned_malloc` (MSVC), `posix_memalign` (POSIX), and `memalign` (obsolete) within their respective headers. Notably, the check for `posix_memalign` temporarily sets `CMAKE_REQUIRED_DEFINITIONS` to ensure the correct POSIX feature test macros are defined during the check.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\n# Allocating Aligned Memory Blocks\ninclude(CheckIncludeFiles)\ncheck_include_files(malloc.h OPJ_HAVE_MALLOC_H)\ninclude(CheckSymbolExists)\n# _aligned_alloc https://msdn.microsoft.com/en-us/library/8z34s9c6.aspx\ncheck_symbol_exists(_aligned_malloc malloc.h OPJ_HAVE__ALIGNED_MALLOC)\n# posix_memalign (needs _POSIX_C_SOURCE >= 200112L on Linux)\nset(_prev_CMAKE_REQUIRED_DEFINITIONS ${CMAKE_REQUIRED_DEFINITIONS})\nset(CMAKE_REQUIRED_DEFINITIONS -D_POSIX_C_SOURCE=200112L)\ncheck_symbol_exists(posix_memalign stdlib.h OPJ_HAVE_POSIX_MEMALIGN)\nset(CMAKE_REQUIRED_DEFINITIONS ${_prev_CMAKE_REQUIRED_DEFINITIONS})\nunset(_prev_CMAKE_REQUIRED_DEFINITIONS)\n# memalign (obsolete)\ncheck_symbol_exists(memalign malloc.h OPJ_HAVE_MEMALIGN)\n```\n\n----------------------------------------\n\nTITLE: Declaring and Dispatching Optimized Source Files in OpenCV with CMake\nDESCRIPTION: These CMake commands register a set of source files (such as 'accum', 'bilateral_filter', etc.) to be compiled with support for specific CPU instruction set extensions (e.g., SSE2, SSE4_1, AVX2, AVX512_SKX). This pattern enables the OpenCV imgproc module to use the most optimized code path available for an end-user's hardware, providing performance improvements at runtime. Each call to 'ocv_add_dispatched_file' maps a logical operation module to its corresponding platform-specific implementations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/imgproc/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nocv_add_dispatched_file(accum SSE4_1 AVX AVX2)\nocv_add_dispatched_file(bilateral_filter SSE2 AVX2)\nocv_add_dispatched_file(box_filter SSE2 SSE4_1 AVX2)\nocv_add_dispatched_file(filter SSE2 SSE4_1 AVX2)\nocv_add_dispatched_file(color_hsv SSE2 SSE4_1 AVX2)\nocv_add_dispatched_file(color_rgb SSE2 SSE4_1 AVX2)\nocv_add_dispatched_file(color_yuv SSE2 SSE4_1 AVX2)\nocv_add_dispatched_file(median_blur SSE2 SSE4_1 AVX2)\nocv_add_dispatched_file(morph SSE2 SSE4_1 AVX2)\nocv_add_dispatched_file(smooth SSE2 SSE4_1 AVX2)\nocv_add_dispatched_file(sumpixels SSE2 AVX2 AVX512_SKX)\n```\n\n----------------------------------------\n\nTITLE: Creating Build Directory\nDESCRIPTION: Commands to create and navigate to build directory for OpenCV compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_ubuntu/py_setup_in_ubuntu.markdown#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build\ncd build\n```\n\n----------------------------------------\n\nTITLE: Documentation Dependencies and Installation Setup\nDESCRIPTION: Sets up dependencies between different documentation targets and configures installation of the generated documentation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_24\n\nLANGUAGE: cmake\nCODE:\n```\nadd_dependencies(doxygen doxygen_cpp)\n\n  if(TARGET doxygen_python)\n    add_dependencies(doxygen doxygen_python)\n  endif()\n\n  if(TARGET doxygen_javadoc)\n    add_dependencies(doxygen_cpp doxygen_javadoc)\n  endif()\n\n  add_dependencies(opencv_docs doxygen)\n\n  install(DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/doxygen/html\n    DESTINATION \"${OPENCV_DOC_INSTALL_PATH}\"\n    COMPONENT \"docs\" OPTIONAL\n    ${compatible_MESSAGE_NEVER}\n  )\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: Commands to install Python-specific dependencies for both Python 2 and 3.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_ubuntu/py_setup_in_ubuntu.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install python-dev python-numpy\nsudo apt-get install python3-dev python3-numpy\n```\n\n----------------------------------------\n\nTITLE: Configuring libtiff Build and Feature Detection - CMake - CMake\nDESCRIPTION: This code configures the libtiff build for the OpenCV project via CMake. It sets up checks for availability of header files, function existence, type sizes, library linkage, and endianness detection to ensure portability across platforms. Key options for enabling or disabling algorithm support, dependency integration (e.g., JPEG, ZLIB), and platform-specific build settings are provided, with resulting flags and preprocessor definitions passed down to the compilation stage. Typical inputs involve system and toolchain characteristics; outputs are cached build options, optionally including detected header/functions, enabled features, and compiler definitions. Prerequisites: a working CMake installation and any libraries to be linked (JPEG, ZLIB, etc.).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libtiff/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# ----------------------------------------------------------------------------\\n#  CMake file for libtiff. See root CMakeLists.txt\\n#\\n# ----------------------------------------------------------------------------\\nproject(${TIFF_LIBRARY})\\n\\ninclude(CheckCSourceCompiles)\\ninclude(CheckFunctionExists)\\ninclude(CheckIncludeFile)\\ninclude(CheckTypeSize)\\n\\n\\n# Find libm, if available\\nfind_library(M_LIBRARY m)\\n\\ncheck_include_file(assert.h    HAVE_ASSERT_H)\\nif(NOT MSVC)\\n  check_include_file(dlfcn.h     HAVE_DLFCN_H)\\nendif()\\ncheck_include_file(fcntl.h     HAVE_FCNTL_H)\\ncheck_include_file(inttypes.h  HAVE_INTTYPES_H)\\ncheck_include_file(io.h        HAVE_IO_H)\\ncheck_include_file(limits.h    HAVE_LIMITS_H)\\ncheck_include_file(malloc.h    HAVE_MALLOC_H)\\ncheck_include_file(memory.h    HAVE_MEMORY_H)\\ncheck_include_file(search.h    HAVE_SEARCH_H)\\ncheck_include_file(stdint.h    HAVE_STDINT_H)\\ncheck_include_file(string.h    HAVE_STRING_H)\\nif(NOT MSVC)\\n  check_include_file(strings.h   HAVE_STRINGS_H)\\n  check_include_file(sys/time.h  HAVE_SYS_TIME_H)\\nendif()\\ncheck_include_file(sys/types.h HAVE_SYS_TYPES_H)\\nif(NOT MSVC)\\n  check_include_file(unistd.h    HAVE_UNISTD_H)\\nendif()\\n\\n# Inspired from /usr/share/autoconf/autoconf/c.m4\\nforeach(inline_keyword \"inline\" \"__inline__\" \"__inline\")\\n  if(NOT DEFINED C_INLINE)\\n    set(CMAKE_REQUIRED_DEFINITIONS_SAVE ${CMAKE_REQUIRED_DEFINITIONS})\\n    set(CMAKE_REQUIRED_DEFINITIONS ${CMAKE_REQUIRED_DEFINITIONS}\\n        \"-Dinline=${inline_keyword}\")\\n    check_c_source_compiles(\"\\n        typedef int foo_t;\\n        static inline foo_t static_foo() {return 0;}\\n        foo_t foo(){return 0;}\\n        int main(int argc, char *argv[]) {return 0;}\"\\n      C_HAS_${inline_keyword})\\n    set(CMAKE_REQUIRED_DEFINITIONS ${CMAKE_REQUIRED_DEFINITIONS_SAVE})\\n    if(C_HAS_${inline_keyword})\\n      set(C_INLINE TRUE)\\n      set(INLINE_KEYWORD \"${inline_keyword}\")\\n    endif()\\n endif()\\nendforeach()\\nif(NOT DEFINED C_INLINE)\\n  set(INLINE_KEYWORD)\\nendif()\\n\\n\\n# Check type sizes\\n# NOTE: Could be replaced with C99 <stdint.h>\\ncheck_type_size(\"signed short\" SIZEOF_SIGNED_SHORT)\\ncheck_type_size(\"unsigned short\" SIZEOF_UNSIGNED_SHORT)\\ncheck_type_size(\"signed int\" SIZEOF_SIGNED_INT)\\ncheck_type_size(\"unsigned int\" SIZEOF_UNSIGNED_INT)\\ncheck_type_size(\"signed long\" SIZEOF_SIGNED_LONG)\\ncheck_type_size(\"unsigned long\" SIZEOF_UNSIGNED_LONG)\\ncheck_type_size(\"signed long long\" SIZEOF_SIGNED_LONG_LONG)\\ncheck_type_size(\"unsigned long long\" SIZEOF_UNSIGNED_LONG_LONG)\\ncheck_type_size(\"unsigned char *\" SIZEOF_UNSIGNED_CHAR_P)\\n\\nset(CMAKE_EXTRA_INCLUDE_FILES_SAVE ${CMAKE_EXTRA_INCLUDE_FILES})\\nset(CMAKE_EXTRA_INCLUDE_FILES ${CMAKE_EXTRA_INCLUDE_FILES} \"stddef.h\")\\ncheck_type_size(\"size_t\" SIZEOF_SIZE_T)\\ncheck_type_size(\"ptrdiff_t\" SIZEOF_PTRDIFF_T)\\nset(CMAKE_EXTRA_INCLUDE_FILES ${CMAKE_EXTRA_INCLUDE_FILES_SAVE})\\n\\nset(TIFF_INT8_T \"signed char\")\\nset(TIFF_UINT8_T \"unsigned char\")\\n\\nset(TIFF_INT16_T \"signed short\")\\nset(TIFF_UINT16_T \"unsigned short\")\\n\\nif(SIZEOF_SIGNED_INT EQUAL 4)\\n  set(TIFF_INT32_T \"signed int\")\\n  set(TIFF_INT32_FORMAT \"%d\")\\nelseif(SIZEOF_SIGNED_LONG EQUAL 4)\\n  set(TIFF_INT32_T \"signed long\")\\n  set(TIFF_INT32_FORMAT \"%ld\")\\nendif()\\n\\nif(SIZEOF_UNSIGNED_INT EQUAL 4)\\n  set(TIFF_UINT32_T \"unsigned int\")\\n  set(TIFF_UINT32_FORMAT \"%u\")\\nelseif(SIZEOF_UNSIGNED_LONG EQUAL 4)\\n  set(TIFF_UINT32_T \"unsigned long\")\\n  set(TIFF_UINT32_FORMAT \"%lu\")\\nendif()\\n\\nif(SIZEOF_SIGNED_LONG EQUAL 8)\\n  set(TIFF_INT64_T \"signed long\")\\n  set(TIFF_INT64_FORMAT \"%ld\")\\nelseif(SIZEOF_SIGNED_LONG_LONG EQUAL 8)\\n  set(TIFF_INT64_T \"signed long long\")\\n  if(MINGW)\\n    set(TIFF_INT64_FORMAT \"%I64d\")\\n  else()\\n    set(TIFF_INT64_FORMAT \"%lld\")\\n  endif()\\nendif()\\n\\nif(SIZEOF_UNSIGNED_LONG EQUAL 8)\\n  set(TIFF_UINT64_T \"unsigned long\")\\n  set(TIFF_UINT64_FORMAT \"%lu\")\\nelseif(SIZEOF_UNSIGNED_LONG_LONG EQUAL 8)\\n  set(TIFF_UINT64_T \"unsigned long long\")\\n  if(MINGW)\\n    set(TIFF_UINT64_FORMAT \"%I64u\")\\n  else()\\n    set(TIFF_UINT64_FORMAT \"%llu\")\\n  endif()\\nendif()\\n\\nif(SIZEOF_UNSIGNED_INT EQUAL SIZEOF_SIZE_T)\\n  set(TIFF_SIZE_T \"uint32_t\")\\n  set(TIFF_SIZE_FORMAT \"%u\")\\n  set(TIFF_SSIZE_T \"int32_t\")\\n  set(TIFF_SSIZE_FORMAT \"%d\")\\nelseif(SIZEOF_UNSIGNED_LONG EQUAL SIZEOF_SIZE_T)\\n  set(TIFF_SIZE_T \"uint64_t\")\\n  set(TIFF_SIZE_FORMAT \"%lu\")\\n  set(TIFF_SSIZE_T \"int64_t\")\\n  set(TIFF_SSIZE_FORMAT \"%ld\")\\nelseif(SIZEOF_UNSIGNED_LONG_LONG EQUAL SIZEOF_SIZE_T)\\n  set(TIFF_SIZE_T \"uint64_t\")\\n  if(MINGW)\\n    set(TIFF_SIZE_FORMAT \"%I64u\")\\n    set(TIFF_SSIZE_FORMAT \"%I64d\")\\n  else()\\n    set(TIFF_SIZE_FORMAT \"%llu\")\\n    set(TIFF_SSIZE_FORMAT \"%lld\")\\n  endif()\\nendif()\\n\\nif(SIZEOF_SIGNED_INT EQUAL SIZEOF_UNSIGNED_CHAR_P)\\nelseif(SIZEOF_SIGNED_LONG EQUAL SIZEOF_UNSIGNED_CHAR_P)\\nelseif(SIZEOF_SIGNED_LONG_LONG EQUAL SIZEOF_UNSIGNED_CHAR_P)\\n  set(TIFF_SSIZE_T \"signed long long\")\\n  if(MINGW)\\n  else()\\n  endif()\\nendif()\\n\\nif(NOT SIZEOF_PTRDIFF_T)\\n  set(TIFF_PTRDIFF_T \"${TIFF_SSIZE_T}\")\\n  set(TIFF_PTRDIFF_FORMAT \"${SSIZE_FORMAT}\")\\nelse()\\n  set(TIFF_PTRDIFF_T \"ptrdiff_t\")\\n  set(TIFF_PTRDIFF_FORMAT \"%ld\")\\nendif()\\n\\n# Nonstandard int types\\nif(NOT MSVC)\\n  check_type_size(INT8 int8)\\n  set(HAVE_INT8 ${INT8})\\n  check_type_size(INT16 int16)\\n  set(HAVE_INT16 ${INT16})\\n  check_type_size(INT32 int32)\\n  set(HAVE_INT32 ${INT32})\\nendif()\\n\\n# Check functions\\nif(NOT MSVC)\\n   set(CMAKE_REQUIRED_LIBRARIES_SAVE ${CMAKE_REQUIRED_LIBRARIES})\\n  set(CMAKE_REQUIRED_LIBRARIES ${CMAKE_REQUIRED_LIBRARIES} ${M_LIBRARY})\\n  check_function_exists(floor HAVE_FLOOR)\\n  check_function_exists(pow   HAVE_POW)\\n  check_function_exists(sqrt  HAVE_SQRT)\\n  set(CMAKE_REQUIRED_LIBRARIES ${CMAKE_REQUIRED_LIBRARIES_SAVE})\\nendif()\\n\\nif(NOT MSVC)\\n  check_function_exists(isascii    HAVE_ISASCII)\\n  check_function_exists(memset     HAVE_MEMSET)\\n  check_function_exists(mmap       HAVE_MMAP)\\n  check_function_exists(getopt     HAVE_GETOPT)\\nendif()\\ncheck_function_exists(memmove    HAVE_MEMMOVE)\\ncheck_function_exists(setmode    HAVE_SETMODE)\\ncheck_function_exists(strcasecmp HAVE_STRCASECMP)\\ncheck_function_exists(strchr     HAVE_STRCHR)\\ncheck_function_exists(strrchr    HAVE_STRRCHR)\\ncheck_function_exists(strstr     HAVE_STRSTR)\\ncheck_function_exists(strtol     HAVE_STRTOL)\\ncheck_function_exists(strtol     HAVE_STRTOUL)\\ncheck_function_exists(strtoull   HAVE_STRTOULL)\\ncheck_function_exists(lfind      HAVE_LFIND)\\n\\n# CPU bit order\\nset(fillorder FILLORDER_MSB2LSB)\\nif(CMAKE_HOST_SYSTEM_PROCESSOR MATCHES \"i.*86.*\" OR\\n   CMAKE_HOST_SYSTEM_PROCESSOR MATCHES \"amd64.*\" OR\\n   # AMD64 on Windows\\n   CMAKE_HOST_SYSTEM_PROCESSOR MATCHES \"AMD64\" OR\\n   CMAKE_HOST_SYSTEM_PROCESSOR MATCHES \"x86_64.*\")\\n  set(fillorder FILLORDER_LSB2MSB)\\nendif()\\nset(HOST_FILLORDER ${fillorder} CACHE STRING \"Native CPU bit order\")\\nmark_as_advanced(HOST_FILLORDER)\\n\\n# CPU endianness\\ninclude(TestBigEndian)\\ntest_big_endian(bigendian)\\nif(bigendian)\\n  set(bigendian ON)\\nelse()\\n  set(bigendian OFF)\\nendif()\\nset(HOST_BIG_ENDIAN ${bigendian} CACHE STRING \"Native CPU bit order\")\\nmark_as_advanced(HOST_BIG_ENDIAN)\\nif(HOST_BIG_ENDIAN)\\n  set(HOST_BIG_ENDIAN 1)\\nelse()\\n  set(HOST_BIG_ENDIAN 0)\\nendif()\\nif(HOST_BIG_ENDIAN)\\n  add_definitions(-DWORDS_BIGENDIAN)\\nendif()\\n\\n# IEEE floating point\\nset(HAVE_IEEEFP 1 CACHE STRING \"IEEE floating point is available\")\\nmark_as_advanced(HAVE_IEEEFP)\\n\\n# Large file support\\nif(UNIX OR MINGW)\\n  if(ANDROID AND (ANDROID_NATIVE_API_LEVEL LESS 21) AND CV_GCC)\\n    # Android NDK build problem: 'mmap' issue with GCC and API<21\\n  else()\\n    # This might not catch every possibility catered for by\\n    # AC_SYS_LARGEFILE.\\n    add_definitions(-D_FILE_OFFSET_BITS=64)\\n    set(FILE_OFFSET_BITS 64)\\n  endif()\\nendif()\\n\\n# Documentation install directory (default to cmake project docdir)\\nset(LIBTIFF_DOCDIR \"${CMAKE_INSTALL_FULL_DOCDIR}\")\\n\\n# Options to enable and disable internal codecs\\n\\noption(ccitt \"support for CCITT Group 3 & 4 algorithms\" ON)\\nset(CCITT_SUPPORT ${ccitt})\\n\\noption(packbits \"support for Macintosh PackBits algorithm\" ON)\\nset(PACKBITS_SUPPORT ${packbits})\\n\\noption(lzw \"support for LZW algorithm\" ON)\\nset(LZW_SUPPORT ${lzw})\\n\\noption(thunder \"support for ThunderScan 4-bit RLE algorithm\" ON)\\nset(THUNDER_SUPPORT ${thunder})\\n\\noption(next \"support for NeXT 2-bit RLE algorithm\" ON)\\nset(NEXT_SUPPORT ${next})\\n\\noption(logluv \"support for LogLuv high dynamic range algorithm\" ON)\\nset(LOGLUV_SUPPORT ${logluv})\\n\\n# Option for Microsoft Document Imaging\\noption(mdi \"support for Microsoft Document Imaging\" ON)\\nset(MDI_SUPPORT ${mdi})\\n\\n# ZLIB\\nset(ZLIB_SUPPORT 0)\\nif(ZLIB_LIBRARY)\\n  set(ZLIB_SUPPORT 1)\\nendif()\\nset(ZIP_SUPPORT ${ZLIB_SUPPORT})\\n\\nset(PIXARLOG_SUPPORT FALSE)\\n\\n# JPEG\\nset(JPEG_SUPPORT FALSE)\\nif(HAVE_JPEG)\\n  set(JPEG_SUPPORT TRUE)\\n  if(TARGET ${JPEG_LIBRARY} AND DEFINED ${JPEG_LIBRARY}_BINARY_DIR)\\n    include_directories(\"${${JPEG_LIBRARY}_BINARY_DIR}\")\\n  endif()\\n  include_directories(${JPEG_INCLUDE_DIR})\\nendif()\\n\\noption(old-jpeg \"support for Old JPEG compression (read-only)\" OFF)  # OpenCV: changed to OFF\\nset(OJPEG_SUPPORT FALSE)\\nif(JPEG_SUPPORT AND old-jpeg)\\n  set(OJPEG_SUPPORT TRUE)\\nendif()\\n\\n# OpenCV: turned off\\nset(JBIG_SUPPORT 0)\\nset(LZMA_SUPPORT 0)  # OpenCV: turned off\\nset(JPEG12_FOUND FALSE)  # OpenCV: turned off\\nset(STRIPCHOP_DEFAULT)\\nset(STRIP_SIZE_DEFAULT 8192)\\n\\n# Win32 IO\\nset(win32_io FALSE)\\nif(WIN32 AND NOT WINRT)\\n  set(win32_io TRUE)\\nendif()\\nset(USE_WIN32_FILEIO ${win32_io} CACHE BOOL \"Use win32 IO system (Microsoft Windows only)\")\\nif(USE_WIN32_FILEIO)\\n  set(USE_WIN32_FILEIO TRUE)\\nelse()\\n  set(USE_WIN32_FILEIO FALSE)\\nendif()\\n\\n# Orthogonal features\\n\\n# OpenCV: turned ON\\nset(SUBIFD_SUPPORT 1)\\nset(DEFAULT_EXTRASAMPLE_AS_ALPHA 1)\\nset(CHECK_JPEG_YCBCR_SUBSAMPLING 1)\\n\\nif(JPEG_INCLUDE_DIR)\\n  list(APPEND TIFF_INCLUDES ${JPEG_INCLUDE_DIR})\\nendif()\\nif(JPEG12_INCLUDE_DIR)\\n  list(APPEND TIFF_INCLUDES ${JPEG12_INCLUDE_DIR})\\nendif()\\nif(JBIG_INCLUDE_DIR)\\n  list(APPEND TIFF_INCLUDES ${JBIG_INCLUDE_DIR})\\nendif()\\nif(LIBLZMA_INCLUDE_DIRS)\\n  list(APPEND TIFF_INCLUDES ${LIBLZMA_INCLUDE_DIRS})\\nendif()\\n\n```\n\n----------------------------------------\n\nTITLE: Defining Macro for Objective-C Bindings Generation Target in CMake\nDESCRIPTION: Creates a macro that defines custom commands and targets for generating Objective-C bindings for a specific platform. This macro sets up the directories, Python command arguments, and dependencies for the generator script execution.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/objc/generator/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(objc_generated_targets \"\")\n\nmacro(ocv_add_objc_generated_target TARGET)\n  set(objc_${TARGET}_generated_output_dependecy \"${OPENCV_DEPHELPER}/gen_opencv_objc_source_${TARGET}\")\n  file(MAKE_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}/${TARGET}\")\n  add_custom_command(\n      OUTPUT ${objc_generated_files} \"${objc_${TARGET}_generated_output_dependecy}\"\n      COMMAND ${PYTHON_DEFAULT_EXECUTABLE} \"${OBJC_SOURCE_DIR}/generator/gen_objc.py\"\n              -p \"${OBJC_SOURCE_DIR}/../python/src2/gen2.py\"\n              -c \"${CONFIG_FILE}\"\n              -t \"${TARGET}\"\n              -f \"${OPENCV_OBJC_FRAMEWORK_NAME}\"\n      COMMAND ${CMAKE_COMMAND} -E touch \"${objc_${TARGET}_generated_output_dependecy}\"\n      WORKING_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}/${TARGET}\"\n      DEPENDS \"${OpenCV_SOURCE_DIR}/modules/objc/generator/gen_objc.py\"\n              \"${OpenCV_SOURCE_DIR}/modules/python/src2/gen2.py\"\n              \"${OpenCV_SOURCE_DIR}/modules/python/src2/hdr_parser.py\"\n              # don't, result of file(WRITE): \"${CMAKE_CURRENT_BINARY_DIR}/gen_objc.json\"\n              ${deps}\n              # not allowed (file(WRITE) result): \"${CONFIG_FILE}\"\n      COMMENT \"Generate files for Objective-C bindings (${TARGET})\"\n  )\n  add_custom_target(gen_opencv_objc_source_${TARGET}\n      # excluded from all: ALL\n      DEPENDS ${objc_generated_files} ${objc_${TARGET}_generated_output_dependecy}\n      SOURCES \"${OBJC_SOURCE_DIR}/generator/gen_objc.py\"\n              \"${OBJC_SOURCE_DIR}/generator/templates/cmakelists.template\"\n              \"${CMAKE_CURRENT_BINARY_DIR}/gen_objc.json\"\n  )\n  list(APPEND objc_generated_targets gen_opencv_objc_source_${TARGET})\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation using Make in Shell\nDESCRIPTION: This shell command compiles the Doxygen documentation for OpenCV. It assumes a certain directory structure and successful CMake configuration. Produces documentation files viewable in a web browser.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nmake doxygen\n```\n\n----------------------------------------\n\nTITLE: Declaring VideoIO Module and Source/Header Files - OpenCV CMake Macros - CMake\nDESCRIPTION: Adds and configures the videoio module within OpenCV, specifying dependencies (imgproc, imgcodecs) and language wrappers (java, objc, python). Lists main source and header files for compilation, and gathers additional headers matching patterns for inclusion. Notably uses ocv_add_module macro and CMake file globbing, relying on OpenCV CMake utilities.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/videoio/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nocv_add_module(videoio opencv_imgproc opencv_imgcodecs WRAP java objc python)\n\nset(videoio_hdrs ${CMAKE_CURRENT_LIST_DIR}/src/precomp.hpp)\n\nset(videoio_srcs\n  \"${CMAKE_CURRENT_LIST_DIR}/src/videoio_registry.cpp\"\n  \"${CMAKE_CURRENT_LIST_DIR}/src/videoio_c.cpp\"\n  \"${CMAKE_CURRENT_LIST_DIR}/src/cap.cpp\"\n  \"${CMAKE_CURRENT_LIST_DIR}/src/cap_images.cpp\"\n  \"${CMAKE_CURRENT_LIST_DIR}/src/cap_mjpeg_encoder.cpp\"\n  \"${CMAKE_CURRENT_LIST_DIR}/src/cap_mjpeg_decoder.cpp\"\n  \"${CMAKE_CURRENT_LIST_DIR}/src/backend_plugin.cpp\"\n  \"${CMAKE_CURRENT_LIST_DIR}/src/backend_static.cpp\"\n  \"${CMAKE_CURRENT_LIST_DIR}/src/container_avi.cpp\")\n\nfile(GLOB videoio_ext_hdrs\n  \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/*.hpp\"\n  \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/*.hpp\"\n  \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/*.h\"\n  \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/legacy/*.h\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel ITT API Library for OpenCV in CMake\nDESCRIPTION: Complete CMake configuration for building the Intel ITT API library within OpenCV. It sets up the library name, source files, include paths, platform-specific dependencies, and installation rules. The configuration also extracts the API version and disables specific compiler warnings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ittnotify/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT ITT_LIBRARY)\n  set(ITT_LIBRARY \"ittnotify\")\nendif()\nproject(${ITT_LIBRARY} C)\n\nif(NOT WIN32)\n  include(CheckLibraryExists)\n  if(COMMAND CHECK_LIBRARY_EXISTS)\n    CHECK_LIBRARY_EXISTS(dl dlerror \"\" HAVE_DL_LIBRARY)\n  endif()\nendif()\n\nocv_warnings_disable(CMAKE_C_FLAGS -Wimplicit-fallthrough)\n\nocv_include_directories(\"${CMAKE_CURRENT_SOURCE_DIR}/include\")\nset(ITT_INCLUDE_DIRECTORY \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n\nset(ITT_PUBLIC_HDRS\n    include/ittnotify.h\n    include/jitprofiling.h\n    include/libittnotify.h\n)\nset(ITT_PRIVATE_HDRS\n    src/ittnotify/disable_warnings.h\n    src/ittnotify/ittnotify_config.h\n    src/ittnotify/ittnotify_static.h\n    src/ittnotify/ittnotify_types.h\n)\nset(ITT_SRCS\n    src/ittnotify/ittnotify_static.c\n    src/ittnotify/jitprofiling.c\n)\n\nadd_library(${ITT_LIBRARY} STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${ITT_SRCS} ${ITT_PUBLIC_HDRS} ${ITT_PRIVATE_HDRS})\n\nfile(STRINGS \"src/ittnotify/ittnotify_config.h\" API_VERSION_NUM REGEX \"#define\\[ \\t\\]+API_VERSION_NUM[ \\t]+([0-9\\.]+)\")\nif(API_VERSION_NUM MATCHES \"#define\\[ \\t\\]+API_VERSION_NUM[ \\t]+([0-9\\.]*)\")\n  set(ITTNOTIFY_VERSION \"${CMAKE_MATCH_1}\"  CACHE INTERNAL \"\" FORCE)\nendif()\n\nif(NOT WIN32)\n  if(HAVE_DL_LIBRARY)\n    target_link_libraries(${ITT_LIBRARY} dl)\n  endif()\nendif()\n\nset_target_properties(${ITT_LIBRARY} PROPERTIES\n        OUTPUT_NAME ${ITT_LIBRARY}\n        DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n        COMPILE_PDB_NAME ${ITT_LIBRARY}\n        COMPILE_PDB_NAME_DEBUG \"${ITT_LIBRARY}${OPENCV_DEBUG_POSTFIX}\"\n        ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n    )\n\nocv_warnings_disable(CMAKE_C_FLAGS -Wundef -Wsign-compare)\nocv_warnings_disable(CMAKE_C_FLAGS -Wstrict-prototypes) # clang15\n\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(${ITT_LIBRARY} PROPERTIES FOLDER \"3rdparty\")\nendif()\n\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(${ITT_LIBRARY} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev OPTIONAL)\nendif()\n\nocv_install_3rdparty_licenses(ittnotify src/ittnotify/BSD-3-Clause.txt src/ittnotify/GPL-2.0-only.txt)\n```\n\n----------------------------------------\n\nTITLE: Accessing Image Properties with OpenCV.js - JavaScript\nDESCRIPTION: Demonstrates loading an image into a cv.Mat and printing its width, height, size, depth, channel count, and type. Requires OpenCV.js to be loaded and an input element (e.g., canvas with id \\\"canvasInput\\\"). Useful for debugging and validation of image data before further manipulation. Outputs image properties to the console.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_basic_ops/js_basic_ops.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet src = cv.imread(\"canvasInput\");\nconsole.log('image width: ' + src.cols + '\\n' +\n            'image height: ' + src.rows + '\\n' +\n            'image size: ' + src.size().width + '*' + src.size().height + '\\n' +\n            'image depth: ' + src.depth() + '\\n' +\n            'image channels ' + src.channels() + '\\n' +\n            'image type: ' + src.type() + '\\n');\n```\n\n----------------------------------------\n\nTITLE: Setting OpenEXR and IlmBase Version Information\nDESCRIPTION: This snippet defines version information for both OpenEXR and IlmBase libraries. It sets major, minor, and patch versions as well as version strings and API version identifiers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openexr/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(ILMBASE_VERSION_MAJOR \"2\")\nset(ILMBASE_VERSION_MINOR \"3\")\nset(ILMBASE_VERSION_PATCH \"0\")\n\nset(ILMBASE_VERSION \"${ILMBASE_VERSION_MAJOR}.${ILMBASE_VERSION_MINOR}.${ILMBASE_VERSION_PATCH}\")\nset(ILMBASE_VERSION_API ${ILMBASE_VERSION_MAJOR}_${ILMBASE_VERSION_MINOR})\n\nset(OPENEXR_VERSION_MAJOR \"2\")\nset(OPENEXR_VERSION_MINOR \"3\")\nset(OPENEXR_VERSION_PATCH \"0\")\n\nset(OPENEXR_VERSION \"${OPENEXR_VERSION_MAJOR}.${OPENEXR_VERSION_MINOR}.${OPENEXR_VERSION_PATCH}\")\nset(OPENEXR_VERSION_API ${OPENEXR_VERSION_MAJOR}_${OPENEXR_VERSION_MINOR})\n\nset(OPENEXR_VERSION \"${OPENEXR_VERSION}\" PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Assigning Entities to a Documentation Group - Doxygen Group Markup\nDESCRIPTION: Demonstrates assigning functions, classes, or whole blocks to doxygen-defined groups using @ingroup or @addtogroup, which facilitates structured module documentation. Inputs are entity or block and group marker; output is those entities listed in the specified group in documentation navigation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n/** @brief Example function\n    @ingroup mymodule\n*/\nor\n/**\n@addtogroup mymodule_experimental\n@{\n*/\n... several functions, classes or enumerations here\n/**\n@}\n*/\n```\n\n----------------------------------------\n\nTITLE: Defining HAVE_THREADS Macro based on System Support (OpenCV Build) in CMake\nDESCRIPTION: Checks if threading support is available (via `Threads::Threads` target, `HAVE_PTHREAD`, MSVC, or Apple) and if example threads are not disabled (`OPENCV_EXAMPLES_DISABLE_THREADS`). If conditions are met, it adds the `-DHAVE_THREADS=1` preprocessor definition for use in C/C++ source files. This applies when building as part of OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif((TARGET Threads::Threads OR HAVE_PTHREAD OR MSVC OR APPLE) AND NOT OPENCV_EXAMPLES_DISABLE_THREADS)\n  add_definitions(-DHAVE_THREADS=1)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building OpenCV OpenVX C++ Sample Targets using CMake\nDESCRIPTION: This CMake code block defines the project name as 'openvx_samples'. It includes required OpenCV modules recursively using `ocv_include_modules_recurse` based on the previously defined dependency list. It adds preprocessor definitions `-DIVX_USE_OPENCV` and `-DIVX_HIDE_INFO_WARNINGS` for compiling the samples. It finds all `.cpp` files recursively using `file(GLOB_RECURSE)` and stores them in `cpp_samples`. Finally, it iterates through each found sample file, defines a build target for it using `ocv_define_sample` (associating it with 'openvx'), and links the target against necessary OpenCV libraries (`OPENCV_LINKER_LIBS`) and the required modules using `ocv_target_link_libraries`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/openvx/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nproject(openvx_samples)\nocv_include_modules_recurse(${OPENCV_OPENVX_SAMPLE_REQUIRED_DEPS})\nadd_definitions(-DIVX_USE_OPENCV)\nadd_definitions(-DIVX_HIDE_INFO_WARNINGS)\nfile(GLOB_RECURSE cpp_samples RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} *.cpp)\nforeach(sample_filename ${cpp_samples})\n  ocv_define_sample(tgt ${sample_filename} openvx)\n  ocv_target_link_libraries(${tgt} PRIVATE ${OPENCV_LINKER_LIBS} ${OPENCV_OPENVX_SAMPLE_REQUIRED_DEPS})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Cloning and Preparing OpenCV Repositories (Git, Shell)\nDESCRIPTION: This shell script demonstrates how to clone the OpenCV, opencv_contrib, and opencv_extra repositories from GitHub and optionally check out specific version tags for compatibility. It requires Git to be installed and available in the command line environment. The script handles acquiring main and optional repositories, ensuring that correct tags are used for all components. Parameters such as <some-tag> and <same-tag-as-opencv> must be set to the desired release tag values. Output will be local directories containing the repository code at the specified tags.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/general_install/general_install.markdown#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/opencv/opencv\\ngit -C opencv checkout <some-tag>\\n\\n# optionally\\ngit clone https://github.com/opencv/opencv_contrib\\ngit -C opencv_contrib checkout <same-tag-as-opencv>\\n\\n# optionally\\ngit clone https://github.com/opencv/opencv_extra\\ngit -C opencv_extra checkout <same-tag-as-opencv>\n```\n\n----------------------------------------\n\nTITLE: Gathering and Filtering JavaScript Tutorial Assets in CMake\nDESCRIPTION: Recursively finds all files within the JavaScript tutorial assets directory (`js_tutorials_assets_dir`) using `file(GLOB_RECURSE ...)` and stores them in the `js_assets` list. It then filters out any ESLint configuration files (`.eslintrc.json`) using a custom function `ocv_list_filterout`. Finally, it appends the path to a specific MP4 video file required for a calibration tutorial.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_18\n\nLANGUAGE: cmake\nCODE:\n```\n# gather and copy specific files for js tutorials\nfile(GLOB_RECURSE js_assets \"${js_tutorials_assets_dir}/*\")\nocv_list_filterout(js_assets \"\\\\.eslintrc.json\")\nlist(APPEND js_assets \"${OpenCV_SOURCE_DIR}/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/Data/box.mp4\")\n```\n\n----------------------------------------\n\nTITLE: ARM CPU Feature Detection\nDESCRIPTION: Checks for ARM-specific CPU features like CRC32 and NEON using compiler tests\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_18\n\nLANGUAGE: c\nCODE:\n```\n#include <sys/auxv.h>\nint main() {\n    return (getauxval(AT_HWCAP) & HWCAP_CRC32);\n}\n```\n\nLANGUAGE: c\nCODE:\n```\n#include <sys/auxv.h>\nint main() {\n    return (getauxval(AT_HWCAP2) & HWCAP2_CRC32);\n}\n```\n\nLANGUAGE: c\nCODE:\n```\n#include <sys/auxv.h>\n#include <asm/hwcap.h>\nint main() {\n    return (getauxval(AT_HWCAP2) & HWCAP2_CRC32);\n}\n```\n\n----------------------------------------\n\nTITLE: Closing Namespace in C++\nDESCRIPTION: This code snippet closes the CV namespace, indicating the end of the OpenCV-related code block.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_48\n\nLANGUAGE: C++\nCODE:\n```\n} // END NAMESPACE CV\n\n```\n\n----------------------------------------\n\nTITLE: Validating Matches and Estimating Homography with OpenCV (Python)\nDESCRIPTION: This snippet checks if enough good matches exist (with a minimum count). If so, it extracts matched point coordinates from each image, computes the perspective homography using RANSAC to handle outliers, transforms the reference object's corners, and draws the detected region on the scene image. If not enough matches exist, it reports a failure. Dependencies: Output from previous SIFT/FLANN matching, OpenCV, and NumPy. Inputs are the good match list, keypoints, and images. Outputs either the drawn polygon or a missing match message. At least four inlier matches are required to estimate homography reliably.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nif len(good)>MIN_MATCH_COUNT:\\n    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\\n    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\\n\\n    M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0)\\n    matchesMask = mask.ravel().tolist()\\n\\n    h,w = img1.shape\\n    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\\n    dst = cv.perspectiveTransform(pts,M)\\n\\n    img2 = cv.polylines(img2,[np.int32(dst)],True,255,3, cv.LINE_AA)\\n\\nelse:\\n    print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\\n    matchesMask = None\n```\n\n----------------------------------------\n\nTITLE: Automatic Memory Management in OpenCV C++\nDESCRIPTION: This snippet illustrates the automatic memory management in OpenCV using `cv::Mat`. It demonstrates reference counting, where data is shared between structures like `Mat`, and the use of `clone` for creating separate copies. Includes the creation and release of matrices and changes to reference counters when copying or releasing data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/doc/intro.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n    // create a big 8Mb matrix\n    Mat A(1000, 1000, CV_64F);\n\n    // create another header for the same matrix;\n    // this is an instant operation, regardless of the matrix size.\n    Mat B = A;\n    // create another header for the 3-rd row of A; no data is copied either\n    Mat C = B.row(3);\n    // now create a separate copy of the matrix\n    Mat D = B.clone();\n    // copy the 5-th row of B to C, that is, copy the 5-th row of A\n    // to the 3-rd row of A.\n    B.row(5).copyTo(C);\n    // now let A and D share the data; after that the modified version\n    // of A is still referenced by B and C.\n    A = D;\n    // now make B an empty matrix (which references no memory buffers),\n    // but the modified version of A will still be referenced by C,\n    // despite that C is just a single row of the original A\n    B.release();\n\n    // finally, make a full copy of C. As a result, the big modified\n    // matrix will be deallocated, since it is not referenced by anyone\n    C = C.clone();\n```\n\n----------------------------------------\n\nTITLE: Defining JPEG DCT Coefficient Data Structures in C\nDESCRIPTION: Defines C typedefs for handling Discrete Cosine Transform (DCT) coefficients within the libjpeg library. `JCOEF` represents a single 16-bit signed integer coefficient. `JBLOCK` defines an 8x8 block (DCTSIZE2 = 64) of coefficients. `JBLOCKROW` is a pointer to a row of 8x8 blocks. `JBLOCKARRAY` points to an array of block rows. `JBLOCKIMAGE` points to an array of component block arrays. Coefficients are typically stored quantized and in natural order within these structures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/structure.txt#2025-04-22_snippet_3\n\nLANGUAGE: c\nCODE:\n```\n    typedef short JCOEF;                a 16-bit signed integer\n    typedef JCOEF JBLOCK[DCTSIZE2];     an 8x8 block of coefficients\n    typedef JBLOCK *JBLOCKROW;          ptr to one horizontal row of 8x8 blocks\n    typedef JBLOCKROW *JBLOCKARRAY;     ptr to a list of such rows\n    typedef JBLOCKARRAY *JBLOCKIMAGE;   ptr to a list of color component arrays\n```\n\n----------------------------------------\n\nTITLE: Applying Morphological Transformations (Opening, Closing, etc.) - OpenCV.js - JavaScript\nDESCRIPTION: This snippet introduces the cv.morphologyEx() function in OpenCV.js, which generalizes morphological operations such as opening, closing, gradient, top hat, and black hat using a specified operation code (op). Key parameters are src (input image), dst (output image), op (operation type from cv.MorphTypes), kernel, anchor, iterations, borderType, and borderValue. Images may have variable depth and channel counts. The function allows succinct execution of combined morphological operations as needed for different applications.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_morphological_ops/js_morphological_ops.markdown#2025-04-22_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.morphologyEx(src, dst, op, kernel, anchor = new cv.Point(-1, -1), iterations = 1, borderType = cv.BORDER_CONSTANT, borderValue = cv.morphologyDefaultBorderValue())\n```\n\n----------------------------------------\n\nTITLE: Image Smoothing Using Gaussian Filter in C++\nDESCRIPTION: Shows the application of Gaussian blurring with cv.GaussianBlur(), which smooths images using a Gaussian kernel. It's effective for reducing image noise and detail. Requires OpenCV with parameters for the source image, destination, kernel size, and sigma values for the kernel. Suitable for various image depths.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_filtering/js_filtering.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ncv::GaussianBlur(src, dst, cv::Size(ksize, ksize), sigmaX, sigmaY, cv::BORDER_DEFAULT);\n```\n\n----------------------------------------\n\nTITLE: Parsing and Validating Plugin List with Conditional Warnings - VideoIO Plugin System - CMake\nDESCRIPTION: Parses the VIDEOIO_PLUGIN_LIST to support both comma- and semicolon-separated values, lowercases the plugin names, and disables the list if plugin usage is globally disabled. Issues a warning if the list is set while plugins are disabled, ensuring that configuration inconsistencies are caught early in the build. No external dependencies are required; key inputs include user- or preset variables VIDEOIO_PLUGIN_LIST and VIDEOIO_ENABLE_PLUGINS.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/videoio/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nstring(REPLACE \",\" \";\" VIDEOIO_PLUGIN_LIST \"${VIDEOIO_PLUGIN_LIST}\")  # support comma-separated list (,) too\nstring(TOLOWER \"${VIDEOIO_PLUGIN_LIST}\" VIDEOIO_PLUGIN_LIST)\nif(NOT VIDEOIO_ENABLE_PLUGINS)\n  if(VIDEOIO_PLUGIN_LIST)\n    message(WARNING \"VideoIO: plugins are disabled through VIDEOIO_ENABLE_PLUGINS, so VIDEOIO_PLUGIN_LIST='${VIDEOIO_PLUGIN_LIST}' is ignored\")\n    set(VIDEOIO_PLUGIN_LIST \"\")\n  endif()\nelse()\n  # Make virtual opencv_videoio_plugins target\n  if(NOT TARGET opencv_videoio_plugins)\n    add_custom_target(opencv_videoio_plugins ALL)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading ICC Profiles with libjpeg - C\nDESCRIPTION: Provides the function signatures for writing and reading ICC device profiles in JPEG images using libjpeg. These functions are agnostic to ICC data internals, and focus only on embedding or extracting binary profile data at the correct position within the JPEG marker stream. Usage requires a properly initialized compressor or decompressor context, and functions must be called in strict sequence during JPEG write/read workflows.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_62\n\nLANGUAGE: c\nCODE:\n```\nvoid jpeg_write_icc_profile (j_compress_ptr cinfo,\n                                     const JOCTET *icc_data_ptr,\n                                     unsigned int icc_data_len);\nboolean jpeg_read_icc_profile (j_decompress_ptr cinfo,\n                               JOCTET **icc_data_ptr,\n                               unsigned int *icc_data_len);\n```\n\n----------------------------------------\n\nTITLE: Including Android Sample Project Subdirectories in CMake\nDESCRIPTION: Adds multiple subdirectories to the CMake build process using the `add_subdirectory` command. Each listed directory (e.g., '15-puzzle', 'face-detection', 'tutorial-1-camerapreview') is expected to contain its own `CMakeLists.txt` file defining an individual Android sample project. CMake processes the `CMakeLists.txt` file in each subdirectory, integrating its targets and build rules into the main project build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(15-puzzle)\nadd_subdirectory(face-detection)\nadd_subdirectory(qr-detection)\nadd_subdirectory(image-manipulations)\nadd_subdirectory(camera-calibration)\nadd_subdirectory(color-blob-detection)\nadd_subdirectory(mobilenet-objdetect)\nadd_subdirectory(video-recorder)\nadd_subdirectory(tutorial-1-camerapreview)\nadd_subdirectory(tutorial-2-mixedprocessing)\nadd_subdirectory(tutorial-3-cameracontrol)\nadd_subdirectory(tutorial-4-opencl)\n```\n\n----------------------------------------\n\nTITLE: Calculating Equivalent Diameter using OpenCV.js\nDESCRIPTION: Calculates the equivalent diameter of a contour, which is the diameter of a circle having the same area as the contour. Requires a contour (`cnt`). Uses `cv.contourArea` to get the area and applies the formula sqrt(4 * area / PI).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contour_properties/js_contour_properties.markdown#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nlet area = cv.contourArea(cnt, false);\nlet equiDiameter = Math.sqrt(4 * area / Math.PI);\n```\n\n----------------------------------------\n\nTITLE: Waiting for User Exit in C++ OpenCV Program\nDESCRIPTION: Code snippet demonstrating how to wait for a user to press a key to exit an OpenCV application in C++. This is typically used at the end of image processing applications to keep the result window open until the user is ready to close it.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\n// Wait until user exit program by pressing a key\nwaitKey(0);\n```\n\n----------------------------------------\n\nTITLE: Initializing TBB Project and Version Configuration in CMake\nDESCRIPTION: Sets up the TBB project, defines version information and download paths. Includes a check to prevent using BUILD_TBB on non-ARM Windows platforms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/tbb/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(tbb CXX)\n\nif (WIN32 AND NOT ARM)\n  message(FATAL_ERROR \"BUILD_TBB option supports Windows on ARM only!\\nUse regular official TBB build instead of the BUILD_TBB option!\")\nendif()\n\nocv_update(OPENCV_TBB_RELEASE \"v2022.1.0\")\nocv_update(OPENCV_TBB_RELEASE_MD5 \"cce28e6cb1ceae14a93848990c98cb6b\")\nocv_update(OPENCV_TBB_FILENAME \"${OPENCV_TBB_RELEASE}.tar.gz\")\nstring(REGEX REPLACE \"^v\" \"\" OPENCV_TBB_RELEASE_ \"${OPENCV_TBB_RELEASE}\")\n#ocv_update(OPENCV_TBB_SUBDIR ...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Compiler Definitions and Warnings for Protobuf\nDESCRIPTION: Sets up compiler-specific definitions and disables various warnings based on the compiler being used (MSVC, GCC, ICC). Includes special handling for threading and platform-specific issues.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/protobuf/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(HAVE_PTHREAD)\n  add_definitions(-DHAVE_PTHREAD=1)\nendif()\n\nif(MSVC)\n  add_definitions( -D_CRT_SECURE_NO_WARNINGS=1 )\n  ocv_warnings_disable(CMAKE_CXX_FLAGS /wd4244 /wd4267 /wd4018 /wd4355 /wd4800 /wd4251 /wd4996 /wd4146)\nelse()\n  ocv_warnings_disable(CMAKE_CXX_FLAGS -Wno-deprecated -Wmissing-prototypes -Wmissing-declarations -Wshadow)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining calib3d Module with Dependencies and Language Wrappers - CMake\nDESCRIPTION: This snippet defines the calib3d module using OpenCV's ocv_define_module macro. It specifies dependencies such as opencv_imgproc, opencv_features2d, opencv_flann, and conditionally includes debug modules. It also enables wrappers for Java, Objective-C, Python, and JavaScript. Prerequisite: Supporting CMake macros must be included.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/calib3d/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nocv_define_module(calib3d opencv_imgproc opencv_features2d opencv_flann ${debug_modules}\n    WRAP java objc python js\n)\n```\n\n----------------------------------------\n\nTITLE: Updating and Including OpenCV Configurations in CMake\nDESCRIPTION: This snippet sets the OPENCV_CONFIG_FILE_INCLUDE_DIR to store platform-dependent cvconfig.h files and includes these directories in the CMake processing path. It is crucial for CMake to locate platform-specific configuration headers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nocv_update(OPENCV_CONFIG_FILE_INCLUDE_DIR \\\"${CMAKE_BINARY_DIR}/\\\" CACHE PATH \\\"Where to create the platform-dependant cvconfig.h\\\")\nocv_include_directories(${OPENCV_CONFIG_FILE_INCLUDE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Brute-Force Matching with OpenCV in Java\nDESCRIPTION: Java snippet using OpenCV's brute-force matcher to match descriptors between images and implementing 2-nearest neighbor matching.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nsamples/java/tutorial_code/features2D/akaze_matching/AKAZEMatchDemo.java 2-nn matching\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Conditional Checks for Image Feature Detection in C++\nDESCRIPTION: This code snippet contains a series of nested if-else statements comparing pixel values at different offsets. It's part of a larger algorithm, likely for corner or feature detection in images. The code uses goto-like structures through comments for control flow.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_34\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset9] < c_b)\n  if(ptr[offset7] < c_b)\n    if(ptr[offset8] < c_b)\n      if(ptr[offset6] < c_b)\n        if(ptr[offset5] < c_b)\n          if(ptr[offset4] < c_b)\n            if(ptr[offset3] < c_b)\n              if(ptr[offset1] < c_b)\n                {} // goto success_homogeneous;\n              else\n                if(ptr[offset10] < c_b)\n                  {} // goto success_homogeneous;\n                else\n                  continue; // goto homogeneous;\n            else\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  if(ptr[offset12] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                if(ptr[offset12] < c_b)\n                  if(ptr[offset13] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n        else\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              if(ptr[offset12] < c_b)\n                if(ptr[offset13] < c_b)\n                  if(ptr[offset14] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n      else\n        if(ptr[offset10] < c_b)\n          if(ptr[offset11] < c_b)\n            if(ptr[offset12] < c_b)\n              if(ptr[offset13] < c_b)\n                if(ptr[offset14] < c_b)\n                  if(ptr[offset15] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  if(ptr[offset9] > cb)\n    if(ptr[offset10] > cb)\n      if(ptr[offset11] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset12] > cb)\n            if(ptr[offset13] > cb)\n              if(ptr[offset14] > cb)\n                if(ptr[offset15] > cb)\n                  {} // goto success_homogeneous;\n                else\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset7] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n              else\n                if(ptr[offset5] > cb)\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset7] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n            else\n              if(ptr[offset4] > cb)\n                if(ptr[offset5] > cb)\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset7] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset3] > cb)\n              if(ptr[offset4] > cb)\n                if(ptr[offset5] > cb)\n                  if(ptr[offset6] > cb)\n                    if(ptr[offset7] > cb)\n\n```\n\n----------------------------------------\n\nTITLE: Pixel Comparison Decision Tree for Feature Detection in C++\nDESCRIPTION: A decision tree implementation that compares pixel values at various offsets against threshold values (cb and c_b) to determine if a pixel represents a corner or feature point. The algorithm uses goto statements to branch between 'homogeneous', 'structured', and 'success' states based on pixel intensity patterns.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\nif(ptr[offset11] > cb)\n  goto success_homogeneous;\nelse\n  goto homogeneous;\nelse\n  if(ptr[offset6] > cb)\n    if(ptr[offset8] > cb)\n      goto success_homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  if(ptr[offset1] > cb)\n    if(ptr[offset6] > cb)\n      goto success_homogeneous;\n    else\n      if(ptr[offset11] > cb)\n        goto success_homogeneous;\n      else\n        goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  if(ptr[offset9] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset5] < c_b)\n        if(ptr[offset1] > cb)\n          if(ptr[offset4] > cb)\n            if(ptr[offset10] > cb)\n              if(ptr[offset3] > cb)\n                if(ptr[offset11] > cb)\n                  goto success_structured;\n                else\n                  goto structured;\n              else\n                goto homogeneous;\n            else\n              if(ptr[offset6] < c_b)\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset11] < c_b)\n                    if(ptr[offset10] < c_b)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset8] < c_b)\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset4] < c_b)\n                    goto success_structured;\n                  else\n                    if(ptr[offset11] < c_b)\n                      goto success_structured;\n                    else\n                      goto structured;\n                else\n                  if(ptr[offset3] < c_b)\n                    if(ptr[offset4] < c_b)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n        else\n          if(ptr[offset6] < c_b)\n            if(ptr[offset8] < c_b)\n              if(ptr[offset4] < c_b)\n                if(ptr[offset3] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto homogeneous;\n              else\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto success_structured;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n      else\n        if(ptr[offset1] > cb)\n          if(ptr[offset3] > cb)\n            if(ptr[offset4] > cb)\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  goto success_structured;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n        else\n          goto homogeneous;\n    else\n      if(ptr[offset1] > cb)\n        if(ptr[offset3] > cb)\n          if(ptr[offset4] > cb)\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                goto success_homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n        else\n          goto homogeneous;\n      else\n        goto homogeneous;\n  else\n    if(ptr[offset10] > cb)\n      if(ptr[offset11] > cb)\n        if(ptr[offset9] > cb)\n          if(ptr[offset7] > cb)\n            if(ptr[offset1] > cb)\n              if(ptr[offset3] > cb)\n                goto success_homogeneous;\n              else\n                if(ptr[offset8] > cb)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset8] > cb)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n          else\n            if(ptr[offset1] > cb)\n              if(ptr[offset3] > cb)\n                goto success_homogeneous;\n              else\n                if(ptr[offset8] > cb)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n            else\n              goto homogeneous;\n        else\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset4] > cb)\n                goto success_homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n      else\n        goto homogeneous;\n    else\n      goto homogeneous;\nelse\n  if(ptr[offset7] > cb)\n    if(ptr[offset9] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset5] > cb)\n          if(ptr[offset1] > cb)\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                goto success_homogeneous;\n              else\n                if(ptr[offset6] > cb)\n                  if(ptr[offset4] > cb)\n                    goto success_structured;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset3] > cb)\n```\n\n----------------------------------------\n\nTITLE: Initializing Android Activity with OpenGL-enabled Camera Preview - Java\nDESCRIPTION: This Java class defines a minimal Android Activity to set up a camera preview using an OpenGL surface view. It initializes the interface in landscape mode, disables the title bar, sets the view to full screen, and manages the surface view's lifecycle in onCreate, onPause, and onResume. Prerequisites are basic Android development setup and a corresponding GLSurfaceView implementation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\npublic class Tutorial4Activity extends Activity {\\n\\n    private MyGLSurfaceView mView;\\n\\n    @Override\\n    public void onCreate(Bundle savedInstanceState) {\\n        super.onCreate(savedInstanceState);\\n        requestWindowFeature(Window.FEATURE_NO_TITLE);\\n        getWindow().setFlags(WindowManager.LayoutParams.FLAG_FULLSCREEN,\\n                WindowManager.LayoutParams.FLAG_FULLSCREEN);\\n        getWindow().setFlags(WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON,\\n                WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);\\n        setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_LANDSCAPE);\\n\\n        mView = new MyGLSurfaceView(this);\\n        setContentView(mView);\\n    }\\n\\n    @Override\\n    protected void onPause() {\\n        mView.onPause();\\n        super.onPause();\\n    }\\n\\n    @Override\\n    protected void onResume() {\\n        super.onResume();\\n        mView.onResume();\\n    }\\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Lossless JPEG Compression with libjpeg in C\nDESCRIPTION: This C function enables lossless JPEG compression mode within the libjpeg library. It requires the compression structure pointer `cinfo`, a predictor selection value (integer 1-7) which determines the DPCM predictor used, and a point transform value (integer 0 to {precision}-1). A point transform of 0 ensures mathematically lossless compression, while non-zero values introduce a form of lossy quantization. Lossless mode disables many standard JPEG features like DCT, quality selection, and color space conversion.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_29\n\nLANGUAGE: C\nCODE:\n```\njpeg_enable_lossless (j_compress_ptr cinfo, int predictor_selection_value,\n                      int point_transform)\n```\n\n----------------------------------------\n\nTITLE: Configuring Android-specific Settings in CMake for OpenCV\nDESCRIPTION: This snippet handles Android-specific configurations for OpenCV, including NDK, SDK, and toolchain settings. It checks for Android build environment and reports relevant information.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\nif(ANDROID)\n  status(\"\")\n  if(DEFINED ANDROID_NDK_REVISION)\n    set(__msg \"${ANDROID_NDK} (ver ${ANDROID_NDK_REVISION})\")\n  else()\n    set(__msg \"location: ${ANDROID_NDK}\")\n  endif()\n  status(\"  Android NDK: \" ${__msg})\n  status(\"    Android ABI:\" ${ANDROID_ABI})\n  if(BUILD_WITH_STANDALONE_TOOLCHAIN)\n    status(\"    NDK toolchain:\" \"standalone: ${ANDROID_STANDALONE_TOOLCHAIN}\")\n  elseif(BUILD_WITH_ANDROID_NDK OR DEFINED ANDROID_TOOLCHAIN_NAME)\n    status(\"    NDK toolchain:\" \"${ANDROID_TOOLCHAIN_NAME}\")\n  endif()\n  status(\"    STL type:\" ${ANDROID_STL})\n  status(\"    Native API level:\" ${ANDROID_NATIVE_API_LEVEL})\n\n  if(BUILD_ANDROID_PROJECTS)\n    status(\"  Android SDK: \" \"${ANDROID_SDK} (tools: ${ANDROID_SDK_TOOLS_VERSION} build tools: ${ANDROID_SDK_BUILD_TOOLS_VERSION})\")\n    if(ANDROID_EXECUTABLE)\n      status(\"    android tool:\"  \"${ANDROID_EXECUTABLE}\")\n    endif()\n  else()\n    status(\"  Android SDK: \" \"not used, projects are not built\")\n  endif()\n  if(DEFINED ANDROID_SDK_COMPATIBLE_TARGET)\n    status(\"    SDK target:\" \"${ANDROID_SDK_COMPATIBLE_TARGET}\")\n  endif()\n  if(DEFINED ANDROID_PROJECTS_BUILD_TYPE)\n    if(ANDROID_PROJECTS_BUILD_TYPE STREQUAL \"ANT\")\n      status(\"    Projects build scripts:\" \"Ant/Eclipse compatible\")\n    elseif(ANDROID_PROJECTS_BUILD_TYPE STREQUAL \"ANT\")\n      status(\"    Projects build scripts:\" \"Gradle\")\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Camera Calibration Matrix Values\nDESCRIPTION: 3x3 camera calibration matrix containing focal lengths (fx, fy), principal point (cx, cy) and skew. Used for mapping 3D world points to 2D image points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/data/essential_mat_data.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n651.4462353114224 0 376.27522319223914\n0 653.7348054191838 280.1106539526218\n0 0 1\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV using Make\nDESCRIPTION: Executes the 'make' command within the OpenCV build directory. This command starts the compilation and linking process based on the configuration generated by CMake in the previous step, ultimately building the OpenCV libraries with the enabled OpenNI2 support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ make\n```\n\n----------------------------------------\n\nTITLE: Configuring Whitelist for JavaScript Bindings Generation in CMake\nDESCRIPTION: Sets up the whitelist file for JavaScript bindings generation, either from an environment variable or by generating it from module-specific whitelist files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/js/generator/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(DEFINED ENV{OPENCV_JS_WHITELIST})\n  set(OPENCV_JS_WHITELIST_FILE \"$ENV{OPENCV_JS_WHITELIST}\")\n  message(STATUS \"Use white list from environment ${OPENCV_JS_WHITELIST_FILE}\")\nelse()\n  #generate white list from modules/<module_name>/misc/js/whitelist.json\n  set(OPENCV_JS_WHITELIST_FILE \"${CMAKE_CURRENT_BINARY_DIR}/whitelist.json\")\n  foreach(m in ${OPENCV_JS_MODULES})\n    set(js_whitelist \"${OPENCV_MODULE_${m}_LOCATION}/misc/js/gen_dict.json\")\n    if (EXISTS \"${js_whitelist}\")\n      file(READ \"${js_whitelist}\" whitelist_content)\n      list(APPEND OPENCV_JS_WHITELIST_CONTENT  \"\\\"${m}\\\": ${whitelist_content}\")\n    endif()\n  endforeach(m)\n  string(REPLACE \";\" \", \\n\" OPENCV_JS_WHITELIST_CONTENT_STRING \"${OPENCV_JS_WHITELIST_CONTENT}\")\n  set(OPENCV_JS_WHITELIST_CONTENT_STRING \"{\\n${OPENCV_JS_WHITELIST_CONTENT_STRING}}\\n\")\n  ocv_update_file(\"${OPENCV_JS_WHITELIST_FILE}\" \"${OPENCV_JS_WHITELIST_CONTENT_STRING}\")\n  message(STATUS \"Use autogenerated whitelist ${OPENCV_JS_WHITELIST_FILE}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing libspng Library\nDESCRIPTION: Configures installation settings for the libspng library. Handles solution folder organization, library installation paths, and license file installation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libspng/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_SOLUTION_FOLDERS)\n    set_target_properties(${SPNG_LIBRARY} PROPERTIES FOLDER \"3rdparty\")\nendif()\n\nif(NOT BUILD_SHARED_LIBS)\n    ocv_install_target(${SPNG_LIBRARY} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev OPTIONAL)\nendif()\n\nocv_install_3rdparty_licenses(${SPNG_LIBRARY} LICENSE)\n```\n\n----------------------------------------\n\nTITLE: Implementing Keypoint Detection and Storage Logic in AGAST Algorithm\nDESCRIPTION: This code snippet handles keypoint memory allocation and storage after corner detection. It dynamically adjusts the reserved memory based on the number of expected corners and creates new KeyPoint objects for detected corners.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_41\n\nLANGUAGE: C++\nCODE:\n```\nif(total == nExpectedCorners)\n{\n    if(nExpectedCorners == 0)\n    {\n        nExpectedCorners = 512;\n        keypoints.reserve(nExpectedCorners);\n    }\n    else\n    {\n        nExpectedCorners *= 2;\n        keypoints.reserve(nExpectedCorners);\n    }\n}\nkeypoints.push_back(KeyPoint(Point2f((float)x, (float)y), 1.0f));\ntotal++;\n```\n\n----------------------------------------\n\nTITLE: Specifying Code Block Language with @code{.xml} - Doxygen XML Markup\nDESCRIPTION: Shows how to mark a code block as XML in documentation using the @code{.xml} directive, enabling tailored syntax highlighting. No additional dependencies. Input is XML snippet; output is syntax-highlighted code.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_10\n\nLANGUAGE: xml\nCODE:\n```\n@code{.xml}\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Computing Projective Homography Matrix from Euclidean Homography - C++\nDESCRIPTION: This snippet constructs a projective homography matrix from the plane-to-plane Euclidean homography and camera intrinsics using matrix multiplication. Requires the previously computed homography and calibrated camera intrinsic matrix. Output is a projective matrix used for image warping across views.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_24\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet homography_from_camera_displacement.cpp compute-homography\n```\n\n----------------------------------------\n\nTITLE: Listing Android Devices with adb - Command Line (bash)\nDESCRIPTION: This sequence demonstrates the use of the 'adb devices' command to enumerate connected Android devices from the command line. It is useful for verifying whether a device is properly recognized by the development environment and ready for debugging, deployment, or further interaction. No additional dependencies are required except the Android SDK platform-tools package.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_dev_intro.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsavuor@rostislav-laptop:~/Android/Sdk/platform-tools$ ./adb devices\nList of devices attached\nR58MB40Q3VP     device\n\nsavuor@rostislav-laptop:~/Android/Sdk/platform-tools$\n```\n\n----------------------------------------\n\nTITLE: Executing Caffe Training Process via Shell\nDESCRIPTION: Provides the shell commands to initiate the Caffe SSD model training. It first creates directories for snapshots and logs, then executes the Caffe training tool, specifying the solver configuration file (`solver.prototxt`), the GPU to use (GPU 0), and redirecting both standard output and error streams to a log file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/face_detector/how_to_train_face_detector.txt#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmkdir -p snapshot\nmkdir -p log\n/path_for_caffe_build_dir/tools/caffe train -solver=\"solver.prototxt\" -gpu 0  2>&1 | tee -a log/log.log\n```\n\n----------------------------------------\n\nTITLE: Handling Negative Slopes in Image Gradients with OpenCV Python\nDESCRIPTION: This snippet demonstrates the importance of data type handling when working with gradient operators. It shows how using cv.CV_8U directly can result in losing edge information (negative transitions), while using cv.CV_64F with absolute value conversion preserves all edges.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_gradients/py_gradients.markdown#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('box.png', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n\n# Output dtype = cv.CV_8U\nsobelx8u = cv.Sobel(img,cv.CV_8U,1,0,ksize=5)\n\n# Output dtype = cv.CV_64F. Then take its absolute and convert to cv.CV_8U\nsobelx64f = cv.Sobel(img,cv.CV_64F,1,0,ksize=5)\nabs_sobel64f = np.absolute(sobelx64f)\nsobel_8u = np.uint8(abs_sobel64f)\n\nplt.subplot(1,3,1),plt.imshow(img,cmap = 'gray')\nplt.title('Original'), plt.xticks([]), plt.yticks([])\nplt.subplot(1,3,2),plt.imshow(sobelx8u,cmap = 'gray')\nplt.title('Sobel CV_8U'), plt.xticks([]), plt.yticks([])\nplt.subplot(1,3,3),plt.imshow(sobel_8u,cmap = 'gray')\nplt.title('Sobel abs(CV_64F)'), plt.xticks([]), plt.yticks([])\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Manipulating Matrices in Clojure Using OpenCV\nDESCRIPTION: Closely follows the OpenCV Java tutorial, demonstrating matrix creation, element setting, and data output in Clojure. This showcases translating typical Java code constructs into Clojure.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_18\n\nLANGUAGE: clojure\nCODE:\n```\nuser=> (def m (Mat. 5 10 CvType/CV_8UC1 (Scalar. 0 0)))\n#'user/m\nuser=> (def mr1 (.row m 1))\n#'user/mr1\nuser=> (.setTo mr1 (Scalar. 1 0))\n#<Mat Mat [ 1*10*CV_8UC1, isCont=true, isSubmat=true, nativeObj=0x7fc9dac49880, dataAddr=0x7fc9d9c98d5a ]>\nuser=> (def mc5 (.col m 5))\n#'user/mc5\nuser=> (.setTo mc5 (Scalar. 5 0))\n#<Mat Mat [ 5*1*CV_8UC1, isCont=false, isSubmat=true, nativeObj=0x7fc9d9c995a0, dataAddr=0x7fc9d9c98d55 ]>\nuser=> (println (.dump m))\n[0, 0, 0, 0, 0, 5, 0, 0, 0, 0;\n  1, 1, 1, 1, 1, 5, 1, 1, 1, 1;\n  0, 0, 0, 0, 0, 5, 0, 0, 0, 0;\n  0, 0, 0, 0, 0, 5, 0, 0, 0, 0;\n  0, 0, 0, 0, 0, 5, 0, 0, 0, 0]\nnil\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Structure with CMake for OpenCV - CMake\nDESCRIPTION: This CMake script sets project build variables (such as a semihosting suffix and a raw pixel include path) and organizes the build process by including subdirectories that correspond to different OpenCV modules (headers, histogram, and normalization). It requires CMake and expects an existing directory structure for the specified subdirectories. Inputs include project-specific paths and variable definitions, and the outputs are the configured CMake build directories with their respective CMakeLists. Limitations include being meaningful only within the context of the OpenCV build system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/semihosting/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# This file is part of OpenCV project.\n# It is subject to the license terms in the LICENSE file found in the top-level directory\n# of this distribution and at http://opencv.org/license.html\n\nset(SEMIHOSTING_SUFFIX semihosting)\n\nadd_subdirectory(include)\nset(RAW_PIXEL_INCLUDE ${CMAKE_CURRENT_BINARY_DIR}/include)\nadd_subdirectory(histogram)\nadd_subdirectory(norm)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCL Support Option for DNN Module - CMake\nDESCRIPTION: Defines the CMake option OPENCV_DNN_OPENCL to enable or disable OpenCL support for accelerated computing in the DNN module. It is set based on whether HAVE_OPENCL is available and if the platform is not Apple. Key parameters: option name, help text, and logical condition. No immediate outputs, but downstream build logic will check this variable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nocv_option(OPENCV_DNN_OPENCL \"Build with OpenCL support\" HAVE_OPENCL AND NOT APPLE)\n```\n\n----------------------------------------\n\nTITLE: Test Module Configuration Class\nDESCRIPTION: DataClass defining test module configuration including paths, image preprocessing parameters, and model settings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass TestClsModuleConfig:\n    cls_test_data_dir: str = \"../data\"\n    test_module_name: str = \"classification\"\n    test_module_path: str = \"classification.py\"\n    input_img: str = os.path.join(cls_test_data_dir, \"squirrel_cls.jpg\")\n    model: str = \"\"\n\n    frame_height: str = str(TestClsConfig.frame_size)\n    frame_width: str = str(TestClsConfig.frame_size)\n    scale: str = \"1.0\"\n    mean: List[str] = field(default_factory=lambda: [\"0.0\", \"0.0\", \"0.0\"])\n    std: List[str] = field(default_factory=list)\n    crop: str = \"False\"\n    rgb: str = \"True\"\n    rsz_height: str = \"\"\n    rsz_width: str = \"\"\n    classes: str = os.path.join(cls_test_data_dir, \"dnn\", \"classification_classes_ILSVRC2012.txt\")\n```\n\n----------------------------------------\n\nTITLE: Including Optional Tutorial Subdirectories\nDESCRIPTION: This part of the script adds tutorial and example subdirectories conditionally. It ensures that specific version requirements are met before including subdirectories for parallel backend examples or individual CMake examples. This modular approach helps manage optional content in the OpenCV sample setup.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/cpp/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(\"tutorial_code/calib3d/real_time_pose_estimation/CMakeLists.txt\" OPTIONAL)\n\n# Standalone samples only\nif(OpenCV_FOUND AND NOT CMAKE_VERSION VERSION_LESS \"3.1\")\n  add_subdirectory(\"example_cmake\")\nendif()\nif(OpenCV_FOUND AND NOT CMAKE_VERSION VERSION_LESS \"3.9\"\n    AND NOT OPENCV_EXAMPLES_SKIP_PARALLEL_BACKEND\n)\n  add_subdirectory(\"tutorial_code/core/parallel_backend\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Color/Intensity within Contour Mask in OpenCV Python\nDESCRIPTION: This snippet calculates the mean color (for a color image) or mean intensity (for a grayscale image) within the region defined by a mask. It utilizes the `cv.mean` function, applying it to the input image (`im`) and considering only the pixels specified by the `mask`. Requires the input image `im` and a pre-computed `mask` image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_properties/py_contour_properties.markdown#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmean_val = cv.mean(im,mask = mask)\n```\n\n----------------------------------------\n\nTITLE: JPEG Compression Operation Structure in C\nDESCRIPTION: Basic outline of a JPEG compression operation showing the typical sequence of function calls required for compressing an image using the IJG JPEG library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_0\n\nLANGUAGE: C\nCODE:\n```\nAllocate and initialize a JPEG compression object\nSpecify the destination for the compressed data (eg, a file)\nSet parameters for compression, including image size & colorspace\njpeg_start_compress(...);\nwhile (scan lines remain to be written)\n        jpeg_write_scanlines(...);  /* Use jpeg12_write_scanlines() for\n                                       9-bit through 12-bit data\n                                       precision and\n                                       jpeg16_write_scanlines() for\n                                       13-bit through 16-bit data\n                                       precision. */\njpeg_finish_compress(...);\nRelease the JPEG compression object\n```\n\n----------------------------------------\n\nTITLE: Configuring x86 AVX512VNNI Optimizations for ZLIB in CMake\nDESCRIPTION: Checks if AVX512VNNI optimization is enabled (WITH_AVX512VNNI), if intrinsics are available (HAVE_AVX512VNNI_INTRIN), and if AVX2 is also enabled. If supported, it adds the DX86_AVX512VNNI definition, adds feature information for AVX512VNNI adler32 support, appends the specific source file, and sets compile flags (AVX512VNNIFLAG, NOLTOFLAG). Otherwise, it disables AVX512VNNI support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_32\n\nLANGUAGE: cmake\nCODE:\n```\n        if(WITH_AVX512VNNI)\n            check_avx512vnni_intrinsics()\n            if(HAVE_AVX512VNNI_INTRIN AND WITH_AVX2)\n                add_definitions(-DX86_AVX512VNNI)\n                add_feature_info(AVX512VNNI_ADLER32 1 \"Support AVX512VNNI adler32, using \\\"${AVX512VNNIFLAG}\\\"\")\n                list(APPEND AVX512VNNI_SRCS ${ARCHDIR}/adler32_avx512_vnni.c)\n                list(APPEND ZLIB_ARCH_SRCS ${AVX512VNNI_SRCS})\n                set_property(SOURCE ${AVX512VNNI_SRCS} PROPERTY COMPILE_FLAGS \"${AVX512VNNIFLAG} ${NOLTOFLAG}\")\n            else()\n                set(WITH_AVX512VNNI OFF)\n            endif()\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Android CPU Features Library Build in CMake\nDESCRIPTION: Complete CMake configuration for building the Android CPU Features library. Sets up include directories, creates static library target, configures output paths and installation rules. Uses conditional logic to handle Android-specific setup and shared library builds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/cpufeatures/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT ANDROID)\n  message(\"cpufeatures is ANDROID project\")\nendif()\n\nocv_update(OPENCV_CPUFEATURES_TARGET_NAME libcpufeatures)\n\nset(CPUFEATURES_ROOT \"${CMAKE_CURRENT_SOURCE_DIR}\" CACHE PATH \"Android cpufeatures project sources (for example, <android-ndk>/sources/android/cpufeatures)\")\n\nset(CPUFEATURES_INCLUDE_DIRS ${CPUFEATURES_ROOT} CACHE INTERNAL \"\")\nset(CPUFEATURES_LIBRARIES \"${OPENCV_CPUFEATURES_TARGET_NAME}\" CACHE INTERNAL \"\")\n\nif(NOT DEFINED CPUFEATURES_SOURCES)\n  set(CPUFEATURES_SOURCES ${CPUFEATURES_ROOT}/cpu-features.c ${CPUFEATURES_ROOT}/cpu-features.h)\nendif()\n\ninclude_directories(${CPUFEATURES_INCLUDE_DIRS})\nadd_library(${OPENCV_CPUFEATURES_TARGET_NAME} STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${CPUFEATURES_SOURCES})\n\nset_target_properties(${OPENCV_CPUFEATURES_TARGET_NAME}\n  PROPERTIES OUTPUT_NAME cpufeatures\n  DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n  COMPILE_PDB_NAME cpufeatures\n  COMPILE_PDB_NAME_DEBUG \"cpufeatures${OPENCV_DEBUG_POSTFIX}\"\n  ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n)\n\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(${OPENCV_CPUFEATURES_TARGET_NAME} PROPERTIES FOLDER \"3rdparty\")\nendif()\n\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(${OPENCV_CPUFEATURES_TARGET_NAME} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev OPTIONAL)\nendif()\n\nocv_install_3rdparty_licenses(cpufeatures LICENSE README.md)\n```\n\n----------------------------------------\n\nTITLE: Automated Sample Target Definition and Linking for OpenCV TAPI - CMake\nDESCRIPTION: Iterates over all TAPI sample source files, defining build targets via ocv_define_sample and linking required OpenCV libraries. Ensures each sample is set up as a separate build target and properly linked with both OpenCV's global linker libraries and the specific dependencies defined for TAPI. This step automates boilerplate target setup for each example in the samples directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/tapi/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(sample_filename ${all_samples})\n  ocv_define_sample(tgt ${sample_filename} tapi)\n  ocv_target_link_libraries(${tgt} PRIVATE ${OPENCV_LINKER_LIBS} ${OPENCV_TAPI_SAMPLES_REQUIRED_DEPS})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Third-Party Build Configuration Options\nDESCRIPTION: Defines build options for various third-party libraries including zlib, libtiff, OpenJPEG, and others. Configures conditional builds based on platform and user preferences.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nOCV_OPTION(OPENCV_FORCE_3RDPARTY_BUILD   \"Force using 3rdparty code from source\" OFF)\nOCV_OPTION(BUILD_ZLIB               \"Build zlib from source\"             (WIN32 OR APPLE OR OPENCV_FORCE_3RDPARTY_BUILD) )\nOCV_OPTION(BUILD_TIFF               \"Build libtiff from source\"          (WIN32 OR ANDROID OR APPLE OR OPENCV_FORCE_3RDPARTY_BUILD) )\nOCV_OPTION(BUILD_OPENJPEG           \"Build OpenJPEG from source\"         (WIN32 OR ANDROID OR APPLE OR OPENCV_FORCE_3RDPARTY_BUILD) )\nOCV_OPTION(BUILD_JASPER             \"Build libjasper from source\"        (WIN32 OR ANDROID OR APPLE OR OPENCV_FORCE_3RDPARTY_BUILD) )\nOCV_OPTION(BUILD_JPEG               \"Build libjpeg from source\"          (WIN32 OR ANDROID OR APPLE OR OPENCV_FORCE_3RDPARTY_BUILD) )\nOCV_OPTION(BUILD_PNG                \"Build libpng from source\"           (WIN32 OR ANDROID OR APPLE OR OPENCV_FORCE_3RDPARTY_BUILD) )\n```\n\n----------------------------------------\n\nTITLE: Default Input Preprocessing Configuration for MobileNet (Python)\nDESCRIPTION: Provides an example dictionary configuring default image preprocessing for MobileNet in the test pipeline, specifying mean subtraction, scale normalization, crop options, and color format. This block is typically found in default_preprocess_config.py and is a required dependency for standardized testing. Key-value pairs in tf_input_blob are referenced by the pipeline during preprocessing. Inputs must match preprocessing needs of the model; outputs affect how images are prepared for inference.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntf_input_blob = {\n    \"mean\": [\"127.5\", \"127.5\", \"127.5\"],\n    \"scale\": str(1 / 127.5),\n    \"std\": [],\n    \"crop\": \"True\",\n    \"rgb\": \"True\"\n}\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Decision Tree in C++\nDESCRIPTION: Complex nested conditional structure for comparing pixel intensities against threshold values in the FAST corner detection algorithm. Uses goto statements for control flow between success and structured cases based on pixel intensity comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_24\n\nLANGUAGE: C++\nCODE:\n```\n                                    goto success_structured;\n                                  else\n                                    goto structured;\n                                else\n                                  goto structured;\n                              else\n                                goto structured;\n                          else\n                            goto structured;\n                      else\n                        if(ptr[offset3] > cb)\n                          if(ptr[offset4] > cb)\n                            if(ptr[offset7] > cb)\n                              if(ptr[offset1] > cb)\n                                if(ptr[offset6] > cb)\n                                  goto success_structured;\n                                else\n                                  if(ptr[offset11] > cb)\n                                    goto success_structured;\n                                  else\n                                    goto structured;\n                              else\n                                if(ptr[offset6] > cb)\n                                  if(ptr[offset8] > cb)\n                                    goto success_structured;\n                                  else\n                                    goto structured;\n                                else\n                                  goto structured;\n                            else\n                              if(ptr[offset1] > cb)\n                                if(ptr[offset6] > cb)\n                                  goto success_structured;\n                                else\n                                  if(ptr[offset11] > cb)\n                                    goto success_structured;\n                                  else\n                                    goto structured;\n                              else\n                                goto structured;\n                          else\n                            goto structured;\n                        else\n                          goto structured;\n                    else\n                      if(ptr[offset7] < c_b)\n                        if(ptr[offset9] < c_b)\n                          if(ptr[offset5] < c_b)\n                            if(ptr[offset1] > cb)\n                              if(ptr[offset4] > cb)\n                                if(ptr[offset10] > cb)\n                                  if(ptr[offset3] > cb)\n                                    if(ptr[offset11] > cb)\n                                      goto success_structured;\n                                    else\n                                      goto structured;\n                                  else\n                                    goto structured;\n                                else\n                                  if(ptr[offset6] < c_b)\n                                    if(ptr[offset8] < c_b)\n                                      if(ptr[offset11] < c_b)\n                                        if(ptr[offset10] < c_b)\n                                          goto success_structured;\n                                        else\n                                          goto structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto structured;\n                                  else\n                                    goto structured;\n                              else\n                                if(ptr[offset6] < c_b)\n                                  if(ptr[offset8] < c_b)\n                                    if(ptr[offset10] < c_b)\n                                      if(ptr[offset4] < c_b)\n                                        goto success_structured;\n                                      else\n                                        if(ptr[offset11] < c_b)\n                                          goto success_structured;\n                                        else\n                                          goto structured;\n                                    else\n                                      if(ptr[offset3] < c_b)\n                                        if(ptr[offset4] < c_b)\n                                          goto success_structured;\n                                        else\n                                          goto structured;\n                                      else\n                                        goto structured;\n                                  else\n                                    goto structured;\n                                else\n                                  goto structured;\n```\n\n----------------------------------------\n\nTITLE: Initializing Custom Destination Manager in JPEG Library (C)\nDESCRIPTION: Defines the `init_destination` function signature, a required method for a custom `jpeg_destination_mgr`. This function is called by `jpeg_start_compress()` before any compressed data is written. Its purpose is to perform any necessary setup for the destination manager and initialize the `next_output_byte` and `free_in_buffer` fields. `free_in_buffer` must be set to a positive value.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_50\n\nLANGUAGE: c\nCODE:\n```\ninit_destination (j_compress_ptr cinfo)\n```\n\n----------------------------------------\n\nTITLE: Configuring IPP IW Library Build in CMake\nDESCRIPTION: Sets up the IPP IW library project, including source files, include directories, and compiler definitions. It also configures platform-specific compiler flags and defines the library target with appropriate properties.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ippicv/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(${IPP_IW_LIBRARY})\n\nocv_include_directories(${IPP_INCLUDE_DIRS} ${IPP_IW_PATH}/include)\nadd_definitions(-DIW_BUILD)\nif(HAVE_IPP_ICV)\n  add_definitions(-DICV_BASE)\nendif()\n\nfile(GLOB lib_srcs ${IPP_IW_PATH}/src/*.c ${IPP_IW_PATH}/src/*.cpp)\nfile(GLOB lib_hdrs ${IPP_IW_PATH}/include/*.h ${IPP_IW_PATH}/include/iw/*.h ${IPP_IW_PATH}/include/iw++/*.hpp)\n\nadd_library(${IPP_IW_LIBRARY} STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${lib_srcs} ${lib_hdrs})\n\nif(UNIX)\n  if(CV_GCC OR CV_CLANG OR CV_ICC)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wno-unused-function -Wno-missing-braces -Wno-missing-field-initializers\")\n  endif()\n  if(CV_CLANG)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wno-self-assign -Wno-strict-prototypes\")\n  endif()\nendif()\n\nset_target_properties(${IPP_IW_LIBRARY}\n  PROPERTIES OUTPUT_NAME ${IPP_IW_LIBRARY}\n  DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n  COMPILE_PDB_NAME ${IPP_IW_LIBRARY}\n  COMPILE_PDB_NAME_DEBUG \"${IPP_IW_LIBRARY}${OPENCV_DEBUG_POSTFIX}\"\n  ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n  )\n\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(${IPP_IW_LIBRARY} PROPERTIES FOLDER \"3rdparty\")\nendif()\n\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(${IPP_IW_LIBRARY} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev OPTIONAL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining 16-Bit JPEG Pixel Sample Data Structures in C\nDESCRIPTION: Defines C typedefs for handling 16-bit pixel sample data within the libjpeg library. `J16SAMPLE` represents a single unsigned 16-bit pixel component (0-65535), typically implemented as `unsigned short`. `J16SAMPROW` is a pointer to a row of `J16SAMPLE` values. `J16SAMPARRAY` points to an array of rows. `J16SAMPIMAGE` points to an array of component arrays. This maintains the consistent pointer-per-row design for different bit depths.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/structure.txt#2025-04-22_snippet_2\n\nLANGUAGE: c\nCODE:\n```\n    typedef something J16SAMPLE;        a pixel component value, 0..MAXJ16SAMPLE\n    typedef J16SAMPLE *J16SAMPROW;      ptr to a row of samples\n    typedef J16SAMPROW *J16SAMPARRAY;   ptr to a list of rows\n    typedef J16SAMPARRAY *J16SAMPIMAGE; ptr to a list of color-component arrays\n```\n\n----------------------------------------\n\nTITLE: GStreamer Configuration Option in OpenCV\nDESCRIPTION: Defines the WITH_GSTREAMER build option which enables integration with the GStreamer library for decoding and encoding video files, capturing frames from cameras and network streams.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_12\n\nLANGUAGE: markdown\nCODE:\n```\n`WITH_GSTREAMER` (default: _ON_)\n```\n\n----------------------------------------\n\nTITLE: Displaying ZLIB Architecture-Specific Sources in CMake\nDESCRIPTION: This CMake command prints a status message listing the architecture-specific source files that have been added to the `ZLIB_ARCH_SRCS` variable based on the detected CPU features and enabled optimizations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_34\n\nLANGUAGE: cmake\nCODE:\n```\nmessage(STATUS \"Architecture-specific source files: ${ZLIB_ARCH_SRCS}\")\n```\n\n----------------------------------------\n\nTITLE: Exiting the Clojure REPL Session\nDESCRIPTION: Simple command to exit the active Clojure REPL session, marking the end of an interactive programming session within the Clojure environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_19\n\nLANGUAGE: clojure\nCODE:\n```\nuser=> (exit)\nBye for now!\n```\n\n----------------------------------------\n\nTITLE: AGAST Function with Default Corner Pattern\nDESCRIPTION: Simplified AGAST function that automatically uses the OAST_9_16 pattern. It serves as a convenience wrapper for the main AGAST implementation, providing a simpler interface for basic feature detection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_42\n\nLANGUAGE: C++\nCODE:\n```\nvoid AGAST(InputArray _img, std::vector<KeyPoint>& keypoints, int threshold, bool nonmax_suppression)\n{\n    AGAST(_img, keypoints, threshold, nonmax_suppression, AgastFeatureDetector::OAST_9_16);\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring Doxygen Warning Documentation Comment - C++/Doxygen\nDESCRIPTION: This snippet demonstrates how to use a Doxygen-style documentation comment with an @warning tag in C++. This allows custom warning messages to be included in the generated documentation, such as notifying users that the function returns a cv::Mat object. No external dependencies are required except Doxygen. To use, place the comment above your function or class; Doxygen will extract it and display the warning in generated docs. Input is the comment itself and output is the formatted warning in docs.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/cross_referencing/tutorial_cross_referencing.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n/**\n * @warning This functions returns a cv::Mat.\n */\n```\n\n----------------------------------------\n\nTITLE: Creating OpenCV Mat with the create() Function in C++\nDESCRIPTION: Demonstrates using the Mat::create() function to allocate memory for a matrix. This method only reallocates if the new size doesn't fit in the old memory allocation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n// create is another way, but don't use this if you want to initialize with values\nM.create(4,4, CV_8UC(2));\ncout << \"M = \" << endl << \" \" << M << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Drawing Detected ChArUco Diamond Markers with OpenCV ArUco (C++)\nDESCRIPTION: This snippet shows how to overlay the results of ChArUco diamond marker detection on an image using cv::aruco::drawDetectedDiamonds(). It takes the original image, the detected diamond corners, and their associated IDs as inputs and visually highlights the diamonds by drawing their outlines and marker IDs on the image. This step is typically performed after successful detection and helps visualize recognition results for validation or debugging. The function requires dependencies on OpenCV's aruco module and expects the outputs from detectDiamonds().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_diamond_detection/charuco_diamond_detection.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n// Draw detected diamonds on the image\ncv::Mat imageWithDiamonds = image.clone();\ncv::aruco::drawDetectedDiamonds(imageWithDiamonds, diamondCorners, diamondIds);\ncv::imshow(\"Detected Diamonds\", imageWithDiamonds);\ncv::waitKey(0);\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Conditional Logic in C++\nDESCRIPTION: Complex nested conditional logic implementing corner detection by comparing pixel values at different offsets against threshold values. Uses goto statements to branch to 'is_a_corner' or 'is_not_a_corner' based on comparison results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\ngoto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                else\n                                  if(ptr[offset8] > cb)\n                                    if(ptr[offset10] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset3] > cb)\n                                  if(ptr[offset4] > cb)\n                                    goto is_a_corner;\n                                  else\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                else\n                                  if(ptr[offset8] > cb)\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                              else\n                                if(ptr[offset11] > cb)\n                                  if(ptr[offset3] > cb)\n                                    if(ptr[offset4] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      if(ptr[offset10] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                  else\n                                    if(ptr[offset8] > cb)\n                                      if(ptr[offset10] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Setting Stitching Module Description in CMake\nDESCRIPTION: Sets the description for the stitching module in OpenCV. This description is used in the build system to identify the purpose of the module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/stitching/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(the_description \"Images stitching\")\n```\n\n----------------------------------------\n\nTITLE: Architecture Detection and Configuration\nDESCRIPTION: Detects CPU architecture and sets appropriate directory paths for architecture-specific optimizations\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\nset(GENERIC_ARCHDIR \"arch/generic\")\n\nset(ZLIB_ARCH_SRCS)\nset(ZLIB_ARCH_HDRS ${GENERIC_ARCHDIR}/generic_functions.h)\n\nif(BASEARCH_ARM_FOUND)\n    set(ARCHDIR \"arch/arm\")\nelseif(BASEARCH_PPC_FOUND)\n    set(ARCHDIR \"arch/power\")\nelseif(BASEARCH_RISCV_FOUND)\n    set(ARCHDIR \"arch/riscv\")\nelseif(BASEARCH_S360_FOUND)\n    set(ARCHDIR \"arch/s390\")\nelseif(BASEARCH_X86_FOUND)\n    set(ARCHDIR \"arch/x86\")\n    if(NOT ${ARCH} MATCHES \"x86_64\")\n        add_feature_info(SSE2 1 \"Support the SSE2 instruction set, using \\\"${SSE2FLAG}\\\"\")\n    endif()\nelse()\n    set(ARCHDIR ${GENERIC_ARCHDIR})\n    message(STATUS \"No optimized architecture: using ${ARCHDIR}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Displaying Frame Number on Video Output\nDESCRIPTION: Extracts the current frame number from VideoCapture and displays it in the top-left corner of the frame. A white rectangle is drawn behind the text to improve visibility.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/background_subtraction.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n//get the frame number and write it on the current frame\nrectangle(frame, cv::Point(10, 2), cv::Point(100,20),\n          cv::Scalar(255,255,255), -1);\nstringstream ss;\nss << capture.get(CAP_PROP_POS_FRAMES);\nstring frameNumberString = ss.str();\nputText(frame, frameNumberString.c_str(), cv::Point(15, 15),\n        FONT_HERSHEY_SIMPLEX, 0.5 , cv::Scalar(0,0,0));\n```\n\nLANGUAGE: Java\nCODE:\n```\n// get the frame number and write it on the current frame\nImgproc.rectangle(frame, new Point(10, 2), new Point(100,20), new Scalar(255,255,255), -1);\nString frameNumberString = String.format(\"%d\", (int)capture.get(Videoio.CAP_PROP_POS_FRAMES));\nImgproc.putText(frame, frameNumberString, new Point(15, 15), Imgproc.FONT_HERSHEY_SIMPLEX, 0.5, new Scalar(0,0,0));\n```\n\nLANGUAGE: Python\nCODE:\n```\n# get the frame number and write it on the current frame\ncv.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\nframe_number = str(int(cap.get(cv.CAP_PROP_POS_FRAMES)))\ncv.putText(frame, frame_number, (15, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0))\n```\n\n----------------------------------------\n\nTITLE: Executing Pixel Comparison Decision Tree for Feature Classification in C++\nDESCRIPTION: This C++ code snippet executes a pre-defined decision tree by comparing pixel intensity values, accessed via `ptr` and pre-calculated `offset` variables, against thresholds `cb` and `c_b`. The nested `if` conditions and `goto` statements implement a highly optimized check, likely for feature detection (e.g., FAST corners), classifying the central pixel and jumping to specific labels (`success_homogeneous`, `homogeneous`, `success_structured`, `structured`) based on the comparison outcomes. It requires `ptr`, `offset` variables, `cb`, `c_b`, and the target labels to be defined in the surrounding context.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n                                    goto success_homogeneous;\n                                  else\n                                    goto homogeneous;\n                              else\n                                if(ptr[offset6] > cb)\n                                  if(ptr[offset7] > cb)\n                                    if(ptr[offset8] > cb)\n                                      goto success_homogeneous;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                            else\n                              if(ptr[offset1] > cb)\n                                if(ptr[offset3] > cb)\n                                  if(ptr[offset4] > cb)\n                                    goto success_homogeneous;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            goto homogeneous;\n                        else\n                          goto homogeneous;\n                      else\n                        if(ptr[offset9] > cb)\n                          if(ptr[offset7] > cb)\n                            if(ptr[offset8] > cb)\n                              if(ptr[offset10] > cb)\n                                if(ptr[offset11] > cb)\n                                  if(ptr[offset1] > cb)\n                                    goto success_homogeneous;\n                                  else\n                                    if(ptr[offset6] > cb)\n                                      goto success_homogeneous;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                          else\n                            goto homogeneous;\n                        else\n                          goto homogeneous;\n                else\n                if(ptr[offset0] < c_b)\n                  if(ptr[offset2] > cb)\n                    if(ptr[offset5] > cb)\n                      if(ptr[offset7] > cb)\n                        if(ptr[offset6] > cb)\n                          if(ptr[offset4] > cb)\n                            if(ptr[offset3] > cb)\n                              if(ptr[offset1] > cb)\n                                goto success_homogeneous;\n                              else\n                                if(ptr[offset8] > cb)\n                                  goto success_homogeneous;\n                                else\n                                  goto homogeneous;\n                            else\n                              if(ptr[offset9] > cb)\n                                if(ptr[offset8] > cb)\n                                  if(ptr[offset10] > cb)\n                                    goto success_structured;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            if(ptr[offset9] > cb)\n                              if(ptr[offset8] > cb)\n                                if(ptr[offset10] > cb)\n                                  if(ptr[offset11] > cb)\n                                    goto success_structured;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          goto homogeneous;\n                      else\n                        if(ptr[offset9] < c_b)\n                          if(ptr[offset8] < c_b)\n                            if(ptr[offset10] < c_b)\n                              if(ptr[offset11] < c_b)\n                                if(ptr[offset7] < c_b)\n                                  if(ptr[offset1] < c_b)\n                                    goto success_structured;\n                                  else\n                                    if(ptr[offset6] < c_b)\n                                      goto success_structured;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                          else\n                            goto homogeneous;\n                        else\n                          goto homogeneous;\n                    else\n                      if(ptr[offset9] < c_b)\n                        if(ptr[offset7] < c_b)\n                          if(ptr[offset8] < c_b)\n                            if(ptr[offset5] < c_b)\n                              if(ptr[offset1] < c_b)\n                                if(ptr[offset10] < c_b)\n                                  if(ptr[offset11] < c_b)\n                                    goto success_structured;\n                                  else\n                                    if(ptr[offset6] < c_b)\n                                      if(ptr[offset4] < c_b)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  if(ptr[offset6] < c_b)\n                                    if(ptr[offset3] < c_b)\n                                      if(ptr[offset4] < c_b)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                              else\n                                if(ptr[offset6] < c_b)\n                                  if(ptr[offset4] < c_b)\n                                    if(ptr[offset3] < c_b)\n                                      goto success_structured;\n                                    else\n                                      if(ptr[offset10] < c_b)\n                                        goto success_structured;\n                                      else\n                                        goto homogeneous;\n                                  else\n                                    if(ptr[offset10] < c_b)\n                                      if(ptr[offset11] < c_b)\n                                        goto success_structured;\n                                      else\n                                        goto homogeneous;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  goto homogeneous;\n                            else\n                              if(ptr[offset10] < c_b)\n                                if(ptr[offset11] < c_b)\n                                  if(ptr[offset1] < c_b)\n                                    goto success_homogeneous;\n                                  else\n                                    if(ptr[offset6] < c_b)\n                                      goto success_homogeneous;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            goto homogeneous;\n                        else\n                          goto homogeneous;\n                      else\n                        goto homogeneous;\n                  else\n                    if(ptr[offset2] < c_b)\n                      if(ptr[offset9] > cb)\n                        if(ptr[offset5] > cb)\n                          if(ptr[offset1] < c_b)\n                            if(ptr[offset4] < c_b)\n                              if(ptr[offset10] < c_b)\n                                if(ptr[offset3] < c_b)\n                                  if(ptr[offset11] < c_b)\n                                    goto success_structured;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                if(ptr[offset6] > cb)\n                                  if(ptr[offset7] > cb)\n                                    if(ptr[offset8] > cb)\n                                      if(ptr[offset11] > cb)\n                                        if(ptr[offset10] > cb)\n                                          goto success_structured;\n                                        else\n                                          goto structured;\n```\n\n----------------------------------------\n\nTITLE: Configuring s390 Intrinsics and Features for ZLIB in CMake\nDESCRIPTION: Checks for s390 architecture (BASEARCH_S360_FOUND) and available intrinsics (HAVE_S390_INTRIN). If intrinsics are found, it adds definitions and appends s390-specific header and source files to ZLIB build lists, enabling runtime CPU detection if configured (WITH_RUNTIME_CPU_DETECTION).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_20\n\nLANGUAGE: cmake\nCODE:\n```\n    elseif(BASEARCH_S360_FOUND)\n        check_s390_intrinsics()\n        if(HAVE_S390_INTRIN)\n            add_definitions(-DS390_FEATURES)\n            list(APPEND ZLIB_ARCH_HDRS ${ARCHDIR}/s390_functions.h)\n            if(WITH_RUNTIME_CPU_DETECTION)\n                list(APPEND ZLIB_ARCH_HDRS ${ARCHDIR}/s390_features.h)\n                list(APPEND ZLIB_ARCH_SRCS ${ARCHDIR}/s390_features.c)\n            endif()\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV.js with Performance Tests - Bash\nDESCRIPTION: This bash command builds OpenCV.js with performance tests enabled via the '--build_perf' switch. It is run using emcmake, Python, and the OpenCV JS build script. The resulting build includes performance testing suites such as 'cvtColor', 'resize', and 'threshold'. Outputs are performance test binaries suitable for benchmarking using either a local HTTP server in browsers or Node.js.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nemcmake python ./opencv/platforms/js/build_js.py build_js --build_perf\n```\n\n----------------------------------------\n\nTITLE: Configuring and Validating Dependencies for OpenCV SYCL Samples - CMake\nDESCRIPTION: This CMake snippet checks for build-related variables and required dependencies before configuring the OpenCV SYCL sample projects. It ensures that the necessary modules (opencv_core, opencv_imgproc, etc.) are present and only proceeds if conditions like minimum CMake version and build flags are satisfied. It demonstrates conditional flow, dependency registration, and version checks required to safely prepare a heterogeneous build environment for SYCL samples.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/sycl/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(OPENCV_SKIP_SAMPLES_SYCL)\n  return()\nendif()\n\nocv_install_example_src(sycl *.cpp *.hpp CMakeLists.txt)\n\nset(OPENCV_SYCL_SAMPLES_REQUIRED_DEPS\n  opencv_core\n  opencv_imgproc\n  opencv_imgcodecs\n  opencv_videoio\n  opencv_highgui)\nocv_check_dependencies(${OPENCV_SYCL_SAMPLES_REQUIRED_DEPS})\n\nif(NOT BUILD_EXAMPLES OR NOT OCV_DEPENDENCIES_FOUND OR OPENCV_SKIP_SAMPLES_BUILD_SYCL)\n  return()\nendif()\n\nif(CMAKE_VERSION VERSION_LESS \"3.5\")\n  message(STATUS \"SYCL samples require CMake 3.5+\")\n  return()\nendif()\n\ncmake_policy(VERSION 3.5)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Image Paths for Doxygen Configuration in CMake\nDESCRIPTION: Collects various directory paths containing images required for the Doxygen documentation into a single CMake list variable `doxygen_image_path`. This includes common images, module-specific doc folders, tutorial paths (core, Python, JavaScript, module-specific), the main source directory, module directories, contrib module paths, and any custom user-defined paths.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nset(doxygen_image_path\n    ${CMAKE_CURRENT_SOURCE_DIR}/images\n    ${paths_doc}\n    ${tutorial_path}\n    ${tutorial_py_path}\n    ${tutorial_js_path}\n    ${paths_tutorial}\n    #${OpenCV_SOURCE_DIR}/samples/data         # TODO: need to resolve ambiguous conflicts first\n    ${OpenCV_SOURCE_DIR}\n    ${OpenCV_SOURCE_DIR}/modules               # <opencv>/modules\n    ${OPENCV_EXTRA_MODULES_PATH}               # <opencv_contrib>/modules\n    ${OPENCV_DOCS_EXTRA_IMAGE_PATH}            # custom variable for user modules\n)\n```\n\n----------------------------------------\n\nTITLE: Setting an Environment Variable for a Single Linux Command\nDESCRIPTION: This one-liner shell snippet sets the 'MY_ENV_VARIABLE' environment variable only for the execution scope of './my_app'. Dependencies: POSIX-compatible shell. No global state is changed. Useful for short-lived overrides.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/env_reference/env_reference.markdown#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nMY_ENV_VARIABLE=true ./my_app\n```\n\n----------------------------------------\n\nTITLE: Disabling DNN Module for Windows RT - CMake\nDESCRIPTION: This snippet conditionally disables the dnn module if WINRT is set, preventing builds on Windows RT platforms. This is controlled by checking the 'WINRT' CMake variable and calling 'ocv_module_disable'. No parameters beyond WINRT are required. Intended for platform-specific source exclusion within the build system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(WINRT)\n  ocv_module_disable(dnn)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Nested Conditional Pixel Comparison for FAST Corner Detection in OpenCV\nDESCRIPTION: This snippet shows a portion of the FAST corner detection algorithm which compares pixel values against thresholds. The code uses conditional statements to check if pixels at various offsets are above or below thresholds (cb and c_b), determining potential corner points in an image. The algorithm continues or proceeds to a success state based on these comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_35\n\nLANGUAGE: C++\nCODE:\n```\n{} // goto success_homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  if(ptr[offset1] > cb)\n    if(ptr[offset12] > cb)\n      if(ptr[offset13] > cb)\n        if(ptr[offset14] > cb)\n          if(ptr[offset15] > cb)\n            {} // goto success_homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\nif(ptr[offset9] < c_b)\n  if(ptr[offset7] < c_b)\n    if(ptr[offset8] < c_b)\n      if(ptr[offset10] < c_b)\n        if(ptr[offset11] < c_b)\n          if(ptr[offset6] < c_b)\n            if(ptr[offset5] < c_b)\n              if(ptr[offset4] < c_b)\n                if(ptr[offset3] < c_b)\n                  {} // goto success_homogeneous;\n                else\n                  if(ptr[offset12] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n              else\n                if(ptr[offset12] < c_b)\n                  if(ptr[offset13] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n            else\n              if(ptr[offset12] < c_b)\n                if(ptr[offset13] < c_b)\n                  if(ptr[offset14] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset12] < c_b)\n              if(ptr[offset13] < c_b)\n                if(ptr[offset14] < c_b)\n                  if(ptr[offset15] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\nif(ptr[offset0] < c_b)\n  if(ptr[offset2] > cb)\n    if(ptr[offset9] > cb)\n      if(ptr[offset7] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset6] > cb)\n            if(ptr[offset5] > cb)\n              if(ptr[offset4] > cb)\n                if(ptr[offset3] > cb)\n                  if(ptr[offset1] > cb)\n                    {} // goto success_homogeneous;\n                  else\n                    if(ptr[offset10] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                else\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      if(ptr[offset12] > cb)\n                        {} // goto success_homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n              else\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    if(ptr[offset12] > cb)\n                      if(ptr[offset13] > cb)\n                        {} // goto success_homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n            else\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  if(ptr[offset12] > cb)\n                    if(ptr[offset13] > cb)\n                      if(ptr[offset14] > cb)\n                        {} // goto success_homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                if(ptr[offset12] > cb)\n                  if(ptr[offset13] > cb)\n                    if(ptr[offset14] > cb)\n                      if(ptr[offset15] > cb)\n                        {} // goto success_homogeneous;\n                      else\n                        continue; // goto homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n    if(ptr[offset9] < c_b)\n      if(ptr[offset10] < c_b)\n        if(ptr[offset11] < c_b)\n          if(ptr[offset8] < c_b)\n            if(ptr[offset12] < c_b)\n              if(ptr[offset13] < c_b)\n                if(ptr[offset14] < c_b)\n                  if(ptr[offset15] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    if(ptr[offset6] < c_b)\n                      if(ptr[offset7] < c_b)\n                        {} // goto success_homogeneous;\n                      else\n```\n\n----------------------------------------\n\nTITLE: Destroying JPEG Compression Object in C\nDESCRIPTION: This snippet illustrates how to destroy a JPEG compression object using 'jpeg_destroy_compress'. This frees all associated resources. Developers must manually free any additional allocations such as structures created with malloc().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_9\n\nLANGUAGE: C\nCODE:\n```\njpeg_destroy_compress(&cinfo);\n```\n\n----------------------------------------\n\nTITLE: Configuring x86 PCLMULQDQ Optimization for ZLIB in CMake\nDESCRIPTION: Checks if PCLMULQDQ optimization is enabled (WITH_PCLMULQDQ), if intrinsics are available (HAVE_PCLMULQDQ_INTRIN), and if SSE4.2 is also enabled. If supported, it adds the DX86_PCLMULQDQ_CRC definition for CRC32 calculation, appends the specific source file (`crc32_pclmulqdq.c`), adds feature information, and sets compile flags (SSE42FLAG, PCLMULFLAG, NOLTOFLAG). Otherwise, it disables PCLMULQDQ support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_29\n\nLANGUAGE: cmake\nCODE:\n```\n        if(WITH_PCLMULQDQ)\n            check_pclmulqdq_intrinsics()\n            if(HAVE_PCLMULQDQ_INTRIN AND WITH_SSE42)\n                add_definitions(-DX86_PCLMULQDQ_CRC)\n                set(PCLMULQDQ_SRCS ${ARCHDIR}/crc32_pclmulqdq.c)\n                add_feature_info(PCLMUL_CRC 1 \"Support CRC hash generation using PCLMULQDQ, using \\\"${SSE42FLAG} ${PCLMULFLAG}\\\"\")\n                list(APPEND ZLIB_ARCH_SRCS ${PCLMULQDQ_SRCS})\n                set_property(SOURCE ${PCLMULQDQ_SRCS} PROPERTY COMPILE_FLAGS \"${SSE42FLAG} ${PCLMULFLAG} ${NOLTOFLAG}\")\n            else()\n                set(WITH_PCLMULQDQ OFF)\n            endif()\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Modules and Source Files with CMake\nDESCRIPTION: This CMake script sets up the dependencies for an OpenCV application by specifying required OpenCV modules (such as opencv_core, opencv_imgproc, and others) and gathers C++ source files with a glob pattern. It then registers the application 'opencv_interactive-calibration' using these modules and source files, simplifying the build system setup. Dependencies include an installed version of OpenCV with the listed modules available, and the expected input consists of C++ source files in the current directory. The setup is constrained to OpenCV and CMake environments.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/interactive-calibration/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(DEPS opencv_core opencv_imgproc opencv_features2d opencv_highgui opencv_calib3d opencv_videoio opencv_objdetect)\nfile(GLOB SRCS *.cpp)\nocv_add_application(opencv_interactive-calibration MODULES ${DEPS} SRCS ${SRCS})\n```\n\n----------------------------------------\n\nTITLE: Defining 8-Bit JPEG Pixel Sample Data Structures in C\nDESCRIPTION: Defines C typedefs for handling 8-bit pixel sample data within the libjpeg library. `JSAMPLE` represents a single unsigned 8-bit pixel component (0-255). `JSAMPROW` is a pointer to a row of `JSAMPLE` values. `JSAMPARRAY` is a pointer to an array of `JSAMPROW` (representing a 2D image plane or component). `JSAMPIMAGE` is a pointer to an array of `JSAMPARRAY` (representing multiple color components). This structure uses pointers per row for flexibility and potential performance benefits.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/structure.txt#2025-04-22_snippet_0\n\nLANGUAGE: c\nCODE:\n```\n    typedef something JSAMPLE;          a pixel component value, 0..MAXJSAMPLE\n    typedef JSAMPLE *JSAMPROW;          ptr to a row of samples\n    typedef JSAMPROW *JSAMPARRAY;       ptr to a list of rows\n    typedef JSAMPARRAY *JSAMPIMAGE;     ptr to a list of color-component arrays\n```\n\n----------------------------------------\n\nTITLE: Configuring Installation Root with CMake - Shell\nDESCRIPTION: Configures the installation root directory for OpenCV build artifacts by specifying the CMAKE_INSTALL_PREFIX variable to a custom absolute path. Requires 'cmake' to be installed and assumes the source code is in '../opencv'. No build is performed; this step only generates configuration for subsequent build steps.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\ncmake -DCMAKE_INSTALL_PREFIX=/opt/opencv ../opencv\n```\n\n----------------------------------------\n\nTITLE: Defining Example Source Installation Function (OpenCV Build) in CMake\nDESCRIPTION: Defines a CMake function `ocv_install_example_src` used when building as part of OpenCV. It takes a relative path `relpath` and a list of files (`ARGN`). If the `INSTALL_C_EXAMPLES` CMake variable is ON, it installs the specified files to the `${OPENCV_SAMPLES_SRC_INSTALL_PATH}/${relpath}` directory as part of the `samples` component.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ocv_install_example_src relpath)\n  if(INSTALL_C_EXAMPLES)\n    file(GLOB files ${ARGN})\n    install(FILES ${files}\n            DESTINATION \"${OPENCV_SAMPLES_SRC_INSTALL_PATH}/${relpath}\"\n            COMPONENT samples)\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring Arithmetic/Huffman Coding in libjpeg (C)\nDESCRIPTION: A C boolean field within the compression parameters structure (`cinfo`). Set to TRUE to enable arithmetic coding for entropy coding, or FALSE (the default) to use Huffman coding. Arithmetic coding can sometimes offer slightly better compression ratios but may be slower or less widely supported.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_30\n\nLANGUAGE: C\nCODE:\n```\nboolean arith_code\n```\n\n----------------------------------------\n\nTITLE: Adding Vendor AI Accelerator Support (TimVX, CANN, WebNN) to DNN Module - CMake\nDESCRIPTION: If TimVX, CANN, or WebNN support is enabled, appends their respective include directories and links libraries with whole-archive flags for linker. This enables hardware acceleration for compatible platforms. Inputs: HAVE_TIMVX, HAVE_CANN, HAVE_WEBNN; outputs: extra include directories and linker flags for specified libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\nif(HAVE_TIMVX)\n    list(APPEND include_dirs ${TIMVX_INCLUDE_DIR})\n    list(APPEND libs -Wl,--whole-archive ${TIMVX_LIBRARY} -Wl,--no-whole-archive)\nendif()\n\nif(HAVE_CANN)\n  list(APPEND include_dirs ${CANN_INCLUDE_DIRS})\n  list(APPEND libs -Wl,--whole-archive ${CANN_LIBRARIES} -Wl,--no-whole-archive)\nendif()\n\nset(webnn_srcs \"\")\nif(NOT EMSCRIPTEN)\n  if(HAVE_WEBNN)\n    list(APPEND include_dirs ${WEBNN_HEADER_DIRS})\n    list(APPEND include_dirs ${WEBNN_INCLUDE_DIRS})\n    list(APPEND libs -Wl,--whole-archive ${WEBNN_LIBRARIES} -Wl,--no-whole-archive)\n    list(APPEND webnn_srcs $ENV{WEBNN_NATIVE_DIR}/gen/src/webnn/webnn_cpp.cpp)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Simple OpenCV C++ Program\nDESCRIPTION: A C++ program that prints OpenCV build information. Demonstrates basic use of OpenCV library in C++ on the target platform.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_26\n\nLANGUAGE: cpp\nCODE:\n```\n#include <iostream>\n#include <opencv2/core.hpp>\nint main(void)\n{\n  std::cout << cv::getBuildInformation() << std::endl;\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Drawing a Line in Java\nDESCRIPTION: Implementation of the MyLine function that draws a line between two points in OpenCV Java. The function takes the image, start and end points, and uses the line() function with specified thickness and line type.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_16\n\nLANGUAGE: java\nCODE:\n```\nprivate static void MyLine(Mat img, Point start, Point end) {\n    int thickness = 2;\n    int lineType = Core.LINE_8;\n\n    Imgproc.line(img,\n            start,\n            end,\n            new Scalar(0, 0, 0),\n            thickness,\n            lineType);\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Laplacian Operator Full Example in Java\nDESCRIPTION: Complete Java code demonstrating loading an image, applying Gaussian blur, converting to grayscale, using the Laplacian operator for edge detection with Imgproc.Laplacian, converting the result to CV_8U, and displaying it using Swing. This code relies on the OpenCV Java bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n// The tutorial code's is shown lines below. You can also download it from\n// [here](https://raw.githubusercontent.com/opencv/opencv/4.x/samples/java/tutorial_code/ImgTrans/LaPlace/LaplaceDemo.java)\n@include samples/java/tutorial_code/ImgTrans/LaPlace/LaplaceDemo.java\n```\n\n----------------------------------------\n\nTITLE: Generating Integer FourCC Code using CV_FOURCC Macro (C++)\nDESCRIPTION: Demonstrates using the OpenCV `CV_FOURCC` macro to generate the integer representation of a FourCC codec identifier directly from four character literals (e.g., 'P','I','M','1' for the MPEG1 codec). This is useful when the desired codec is known beforehand.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_write.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nCV_FOURCC('P','I','M,'1') // this is an MPEG1 codec from the characters to integer\n```\n\n----------------------------------------\n\nTITLE: Loading Camera Intrinsic Parameters in OpenCV C++\nDESCRIPTION: Code to load camera intrinsic parameters from a file. These parameters include the camera matrix and distortion coefficients needed for undistorting image points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/homography/homography.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat cameraMatrix, distCoeffs;\n// Load camera calibration parameters\ncv::FileStorage fs(intrinsicPath, cv::FileStorage::READ);\nfs[\"camera_matrix\"] >> cameraMatrix;\nfs[\"distortion_coefficients\"] >> distCoeffs;\n```\n\n----------------------------------------\n\nTITLE: Trackbar Callback Function in Python\nDESCRIPTION: This Python snippet defines the `on_trackbar` callback function. It receives the current trackbar value (`val`), calculates `alpha` and `beta`, performs image blending using `cv.addWeighted`, and updates the 'Linear Blend' window display using `cv.imshow`. It relies on `src1`, `src2`, `alpha_slider_max` being accessible from the enclosing scope.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/trackbar.markdown#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#![on_trackbar]\ndef on_trackbar(val):\n    alpha = val / alpha_slider_max\n    beta = ( 1.0 - alpha )\n    dst = cv.addWeighted(src1, alpha, src2, beta, 0.0)\n    cv.imshow('Linear Blend', dst)\n#![on_trackbar]\n```\n\n----------------------------------------\n\nTITLE: Defining PASCAL VOC Annotation Format in XML\nDESCRIPTION: Specifies the XML structure required for annotation files in the PASCAL VOC format. Each XML file corresponds to an image and lists object bounding boxes, including coordinates (`xmin`, `ymin`, `xmax`, `ymax`) and the class name ('face'). This format is necessary for the Caffe SSD training data preparation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/face_detector/how_to_train_face_detector.txt#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<annotation>\n  <size>\n    <width>300</width>\n    <height>300</height>\n  </size>\n  <object>\n    <name>face</name>\n    <difficult>0</difficult>\n    <bndbox>\n      <xmin>100</xmin>\n      <ymin>100</ymin>\n      <xmax>200</xmax>\n      <ymax>200</ymax>\n    </bndbox>\n  </object>\n  <object>\n    <name>face</name>\n    <difficult>0</difficult>\n    <bndbox>\n      <xmin>0</xmin>\n      <ymin>0</ymin>\n      <xmax>100</xmax>\n      <ymax>100</ymax>\n    </bndbox>\n  </object>\n</annotation>\n```\n\n----------------------------------------\n\nTITLE: Drawing an Atom in Python\nDESCRIPTION: Drawing an atom using ellipses and circles in OpenCV Python. The atom is represented by ellipses for the electron orbits and a filled circle for the nucleus.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# 1. Draw a simple atom:\n# -----------------------\n\n# 1.a. Creating ellipses\nMyEllipse(atom_image, 90)\nMyEllipse(atom_image, 0)\nMyEllipse(atom_image, 45)\nMyEllipse(atom_image, -45)\n\n# 1.b. Creating circles\nMyFilledCircle(atom_image, (w//2, w//2))\n```\n\n----------------------------------------\n\nTITLE: Running G-API Graph in Main Function\nDESCRIPTION: Demonstrates how to construct and run a G-API graph using cv::GComputation, including input processing and output generation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/anisotropic_segmentation/porting_anisotropic_segmentation.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nint main(int argc, char** argv)\n{\n    // ... (omitted code for brevity)\n\n    cv::GMat in;\n    cv::GMat gst = calcGST(in, w);\n    cv::GMat normalized;\n    cv::GMat out = cv::gapi::addWeighted(in, 0.8, normalized, 0.2, 0);\n    cv::GComputation computation(cv::GIn(in), cv::GOut(out, gst));\n\n    cv::Mat in_mat = cv::imread(argv[1], cv::IMREAD_GRAYSCALE);\n    cv::Mat out_mat, gst_mat;\n    computation.apply(cv::gin(in_mat), cv::gout(out_mat, gst_mat));\n\n    // ... (omitted code for brevity)\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Regions of Interest (ROI) with OpenCV Mat in C++\nDESCRIPTION: Illustrates how to create new Mat headers (`D` and `E`) that refer to a sub-region (Region of Interest - ROI) of an existing Mat object (`A`). Method 1 uses a `cv::Rect` to define the rectangular region. Method 2 uses `cv::Range` to specify row and column boundaries. Both resulting Mat objects share the underlying data with the original matrix `A`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nMat D (A, Rect(10, 10, 100, 100) ); // using a rectangle\nMat E = A(Range::all(), Range(1,3)); // using row and column boundaries\n```\n\n----------------------------------------\n\nTITLE: Visualizing PCA Axis in C++ using OpenCV\nDESCRIPTION: Defines a utility function `drawAxis` to draw a line representing a principal component (axis) on an image. It takes the image, center point, endpoint of the axis (scaled eigenvector), color, and line thickness as input. Uses `line` and `circle` from OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n//! [visualization]\n// Function to draw the axes of the object detected by PCA\nstatic void drawAxis(Mat &img, Point p, Point q, Scalar colour, const float scale = 0.2)\n{\n    double angle = atan2( (double) p.y - q.y, (double) p.x - q.x ); // angle in radians\n    double hypotenuse = sqrt( (double) (p.y - q.y) * (p.y - q.y) + (p.x - q.x) * (p.x - q.x));\n\n    // Here we lengthen the arrow by a factor of scale\n    q.x = (int) (p.x - scale * hypotenuse * cos(angle));\n    q.y = (int) (p.y - scale * hypotenuse * sin(angle));\n    line(img, p, q, colour, 1, LINE_AA);\n\n    // create the arrow hooks\n    p.x = (int) (q.x + 9 * cos(angle + CV_PI / 4));\n    p.y = (int) (q.y + 9 * sin(angle + CV_PI / 4));\n    line(img, p, q, colour, 1, LINE_AA);\n\n    p.x = (int) (q.x + 9 * cos(angle - CV_PI / 4));\n    p.y = (int) (q.y + 9 * sin(angle - CV_PI / 4));\n    line(img, p, q, colour, 1, LINE_AA);\n}\n//! [visualization]\n```\n\n----------------------------------------\n\nTITLE: Splitting Image into BGR Planes in Java\nDESCRIPTION: Java snippet using OpenCV's `Core.split` function to separate a 3-channel source image (assumed to be in BGR format) into three individual single-channel Mats, stored in a `List<Mat>`. The input is the source image Mat, and the output is the List of plane Mats.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/Histograms_Matching/histogram_calculation/CalcHistDemo.java Separate the image in 3 places ( B, G and R )\n```\n\n----------------------------------------\n\nTITLE: Declaring Global Variables for Template Matching (Java)\nDESCRIPTION: Declares class member variables used in the Java template matching demo, including `Mat` objects for the input image, template, and result map. Also includes variables for window names and the selected matching method integer.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/ImgProc/tutorial_template_matching/MatchTemplateDemo.java declare\n```\n\n----------------------------------------\n\nTITLE: Drawing an Atom in Java\nDESCRIPTION: Drawing an atom using ellipses and circles in OpenCV Java. The atom is represented by ellipses for electron orbits and a filled circle for the nucleus.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n// 1. Draw a simple atom:\n// -----------------------\n\n// 1.a. Creating ellipses\nMyEllipse(atom_image, 90);\nMyEllipse(atom_image, 0);\nMyEllipse(atom_image, 45);\nMyEllipse(atom_image, -45);\n\n// 1.b. Creating circles\nMyFilledCircle(atom_image, new Point(w/2, w/2));\n```\n\n----------------------------------------\n\nTITLE: Customizing Toolchain and Dependency Options for OpenCV (CMake)\nDESCRIPTION: These options allow specifying an external toolchain file or enabling/disabling various hardware and software dependencies such as Carotene, Eigen, OpenVX, LAPACK, IPP, and others. Set corresponding WITH_* or BUILD_* options via CMake. Prerequisites vary based on the individual library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_32\n\nLANGUAGE: cmake\nCODE:\n```\nCMAKE_TOOLCHAIN_FILE\nWITH_CAROTENE\nWITH_KLEIDICV\nWITH_CPUFEATURES\nWITH_EIGEN\nWITH_OPENVX\nWITH_DIRECTX\nWITH_VA\nWITH_LAPACK\nWITH_QUIRC\nBUILD_ZLIB\nBUILD_ITT\nWITH_IPP\nBUILD_IPP_IW\n```\n\n----------------------------------------\n\nTITLE: Checking OpenVX Availability in CMake\nDESCRIPTION: This CMake code block checks if the `HAVE_OPENVX` variable is false (meaning OpenVX is not available). If the condition is true, it prints a status message to the console indicating that OpenVX-related HAL features will be disabled and stops further processing within the current CMake script file using the `return()` command.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openvx/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT HAVE_OPENVX)\n  message(STATUS \"OpenVX is not available, disabling openvx-related HAL and stuff\")\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Pixel Comparison Branching Logic in C++\nDESCRIPTION: Implements a series of nested pixel value comparisons using pointer offsets. Uses conditional branching with goto statements to handle different pixel intensity scenarios for image feature detection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nif(ptr[offset1] > cb)\n  goto success_structured;\nelse\n  if(ptr[offset6] > cb)\n    goto success_structured;\n  else\n    goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\nif(ptr[offset0] < c_b)\n  if(ptr[offset2] > cb)\n    if(ptr[offset5] > cb)\n      if(ptr[offset7] > cb)\n        if(ptr[offset6] > cb)\n          if(ptr[offset4] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset1] > cb)\n                goto success_structured;\n              else\n                if(ptr[offset8] > cb)\n                  goto success_structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset9] > cb)\n                if(ptr[offset8] > cb)\n                  if(ptr[offset10] > cb)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n          else\n            if(ptr[offset9] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n            else\n              goto structured;\n        else\n          goto structured;\n      else\n        if(ptr[offset9] < c_b)\n          if(ptr[offset8] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                if(ptr[offset7] < c_b)\n                  if(ptr[offset1] < c_b)\n                    goto success_structured;\n                  else\n                    if(ptr[offset6] < c_b)\n                      goto success_structured;\n                    else\n                      goto structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n            else\n              goto structured;\n          else\n            goto structured;\n        else\n          goto structured;\n```\n\n----------------------------------------\n\nTITLE: Including Packaging and Dumping Variables in CMake\nDESCRIPTION: This CMake snippet includes the `OpenCVPackaging.cmake` file to incorporate CPack configurations for creating installers or packages. Subsequently, it utilizes the custom function `ocv_cmake_dump_vars` to serialize the current state of all CMake variables into a file named `CMakeVars.txt` for debugging. An optional debug evaluation step (`ocv_cmake_eval`) might be triggered.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_33\n\nLANGUAGE: cmake\nCODE:\n```\n# ----------------------------------------------------------------------------\n# CPack stuff\n# ----------------------------------------------------------------------------\n\ninclude(cmake/OpenCVPackaging.cmake)\n\n# This should be the last command\nocv_cmake_dump_vars(\"\" TOFILE \"CMakeVars.txt\")\nocv_cmake_eval(DEBUG_POST ONCE)\n```\n\n----------------------------------------\n\nTITLE: Template Matching Code Formula\nDESCRIPTION: Mathematical representation in code showing how template matching stores metric values in result matrix R based on template position (x,y)\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nR(x,y) = f(I(x,y), T(x,y))\n```\n\n----------------------------------------\n\nTITLE: Manipulating OpenCV Objects in Clojure\nDESCRIPTION: Shows how to call methods and modify member fields on objects in Clojure using OpenCV, demonstrating operations such as area calculation and object field mutation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_11\n\nLANGUAGE: clojure\nCODE:\n```\nuser=> (.area r1)\n10000.0\nuser=> (.area sq-100)\n10000.0\nuser=> (set! (.x p1) 10)\n10\nuser=> p1\n#<Point {10.0, 0.0}>\nuser=> (set! (.width sq-100) 10)\n10\nuser=> (set! (.height sq-100) 10)\n10\nuser=> (.area sq-100)\n100.0\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Decision Tree Fragment in C++\nDESCRIPTION: This C++ code snippet represents a portion of a decision tree used for FAST corner detection. It performs a series of nested comparisons between pixel intensity values ('ptr[offsetN]') at specific offsets around a central pixel and pre-defined upper ('cb') and lower ('c_b') thresholds. Based on the outcome of these comparisons, it uses 'goto' statements to jump to labels 'is_a_corner' or 'is_not_a_corner', rapidly classifying the central pixel. This highly optimized, albeit complex, structure minimizes computational overhead for feature detection.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset1] < c_b)\n                      if(ptr[offset12] < c_b)\n                        if(ptr[offset13] < c_b)\n                          if(ptr[offset14] < c_b)\n                            if(ptr[offset15] < c_b)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n          if(ptr[offset2] < c_b)\n            if(ptr[offset4] > cb)\n              if(ptr[offset11] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset9] > cb)\n                      if(ptr[offset10] > cb)\n                        if(ptr[offset6] > cb)\n                          if(ptr[offset5] > cb)\n                            if(ptr[offset3] > cb)\n                              goto is_a_corner;\n                            else\n                              if(ptr[offset12] > cb)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            if(ptr[offset12] > cb)\n                              if(ptr[offset13] > cb)\n                                if(ptr[offset14] > cb)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset12] > cb)\n                            if(ptr[offset13] > cb)\n                              if(ptr[offset14] > cb)\n                                if(ptr[offset15] > cb)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n              if(ptr[offset11] < c_b)\n                if(ptr[offset12] < c_b)\n                  if(ptr[offset13] < c_b)\n                    if(ptr[offset10] < c_b)\n                      if(ptr[offset14] < c_b)\n                        if(ptr[offset15] < c_b)\n                          if(ptr[offset1] < c_b)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset8] < c_b)\n                              if(ptr[offset9] < c_b)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset6] < c_b)\n                            if(ptr[offset7] < c_b)\n                              if(ptr[offset8] < c_b)\n                                if(ptr[offset9] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset5] < c_b)\n                          if(ptr[offset6] < c_b)\n                            if(ptr[offset7] < c_b)\n                              if(ptr[offset8] < c_b)\n                                if(ptr[offset9] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      if(ptr[offset1] < c_b)\n                        if(ptr[offset3] < c_b)\n                          if(ptr[offset14] < c_b)\n                            if(ptr[offset15] < c_b)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n            if(ptr[offset4] < c_b)\n              if(ptr[offset5] > cb)\n                if(ptr[offset12] > cb)\n                  if(ptr[offset7] > cb)\n                    if(ptr[offset8] > cb)\n                      if(ptr[offset9] > cb)\n                        if(ptr[offset10] > cb)\n                          if(ptr[offset11] > cb)\n                            if(ptr[offset13] > cb)\n                              if(ptr[offset6] > cb)\n                                goto is_a_corner;\n                              else\n                                if(ptr[offset14] > cb)\n                                  if(ptr[offset15] > cb)\n                                    goto is_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                if(ptr[offset12] < c_b)\n                  if(ptr[offset13] < c_b)\n                    if(ptr[offset14] < c_b)\n                      if(ptr[offset15] < c_b)\n                        if(ptr[offset1] < c_b)\n                          if(ptr[offset3] < c_b)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset10] < c_b)\n                              if(ptr[offset11] < c_b)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset8] < c_b)\n                            if(ptr[offset9] < c_b)\n                              if(ptr[offset10] < c_b)\n                                if(ptr[offset11] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset6] < c_b)\n                          if(ptr[offset7] < c_b)\n                            if(ptr[offset8] < c_b)\n                              if(ptr[offset9] < c_b)\n                                if(ptr[offset10] < c_b)\n                                  if(ptr[offset11] < c_b)\n                                    goto is_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n```\n\n----------------------------------------\n\nTITLE: Disabling Compiler Warnings for OpenEXR\nDESCRIPTION: This code disables various compiler warnings for OpenEXR across different compilers (GCC, Clang, MSVC). It suppresses warnings that are not relevant for the library's functionality to keep the build output clean.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openexr/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nocv_warnings_disable(CMAKE_CXX_FLAGS -Wshadow -Wunused -Wsign-compare -Wundef -Wmissing-declarations -Wuninitialized -Wswitch -Wparentheses -Warray-bounds -Wextra\n                                     -Wdeprecated-declarations -Wmisleading-indentation -Wdeprecated\n                                     -Wsuggest-override -Winconsistent-missing-override\n                                     -Wimplicit-fallthrough\n                                     -Wtautological-compare  # clang\n                                     -Wmissing-prototypes  # gcc/clang\n                                     -Wreorder\n                                     -Wunused-result\n                                     -Wimplicit-const-int-float-conversion  # clang\n)\nif(CV_GCC AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 8.0)\n  ocv_warnings_disable(CMAKE_CXX_FLAGS -Wclass-memaccess)\nendif()\n\nocv_warnings_disable(CMAKE_CXX_FLAGS /wd4018 /wd4099 /wd4100 /wd4101 /wd4127 /wd4189 /wd4245 /wd4305 /wd4389 /wd4512 /wd4701 /wd4702 /wd4706 /wd4800) # vs2005\nocv_warnings_disable(CMAKE_CXX_FLAGS /wd4334) # vs2005 Win64\nocv_warnings_disable(CMAKE_CXX_FLAGS /wd4244) # vs2008\nocv_warnings_disable(CMAKE_CXX_FLAGS /wd4267) # vs2008 Win64\nocv_warnings_disable(CMAKE_CXX_FLAGS /wd4456) # vs2015\nocv_warnings_disable(CMAKE_CXX_FLAGS /wd4819) # vs2019 Win64\n\nif(MSVC AND CV_ICC)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /Qrestrict\")\n  set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} /Qrestrict\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Check Compiler Flags and Attributes in CMake\nDESCRIPTION: This snippet checks the support for specific compiler flags and attributes, including -fno-semantic-interposition, and attribute visibility (hidden, internal) using check_compiler_flag and check_c_source_compiles commands. It ensures linker and compiler optimizations by defining macros.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# Check whether compiler supports -fno-semantic-interposition parameter\n#\ncheck_c_compiler_flag(-fno-semantic-interposition HAVE_NO_INTERPOSITION)\n\n#\n# Check if we can hide zlib internal symbols that are linked between separate source files using hidden\n#\ncheck_c_source_compiles(\n    \"#define Z_INTERNAL __attribute__((visibility (\\\"hidden\\\")))\n    int Z_INTERNAL foo;\n    int main() {\n        return 0;\n    }\"\n    HAVE_ATTRIBUTE_VISIBILITY_HIDDEN FAIL_REGEX \"visibility\")\nif(HAVE_ATTRIBUTE_VISIBILITY_HIDDEN)\n    add_definitions(-DHAVE_VISIBILITY_HIDDEN)\nendif()\n\n#\n# Check if we can hide zlib internal symbols that are linked between separate source files using internal\n#\ncheck_c_source_compiles(\n    \"#define Z_INTERNAL __attribute__((visibility (\\\"internal\\\")))\n    int Z_INTERNAL foo;\n    int main() {\n        return 0;\n    }\"\n    HAVE_ATTRIBUTE_VISIBILITY_INTERNAL FAIL_REGEX \"visibility\")\nif(HAVE_ATTRIBUTE_VISIBILITY_INTERNAL)\n    add_definitions(-DHAVE_VISIBILITY_INTERNAL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Controlling JFIF Header Emission in libjpeg (C)\nDESCRIPTION: A C boolean field within the compression parameters structure (`cinfo`). If TRUE, a standard JFIF (JPEG File Interchange Format) APP0 marker is included in the output file. Functions like `jpeg_set_defaults()` typically set this based on the selected color space (TRUE for YCbCr or grayscale).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_38\n\nLANGUAGE: C\nCODE:\n```\nboolean write_JFIF_header\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV Repository\nDESCRIPTION: Commands to create a working directory and clone the OpenCV repository with minimal history.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/highgui_wayland_ubuntu.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir work\ncd work\ngit clone --depth=1 https://github.com/opencv/opencv.git\n```\n\n----------------------------------------\n\nTITLE: Using CMake to List Available OpenCV Configuration Options\nDESCRIPTION: Command to display all available configuration parameters for OpenCV using CMake's list help option.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\ncmake -LH ../opencv\n```\n\n----------------------------------------\n\nTITLE: Overriding Warning/Trace Message Emission in JPEG Library (C)\nDESCRIPTION: Defines the `emit_message` function signature, a method within the `jpeg_error_mgr` struct. This function decides whether to output a non-fatal warning (`msg_level` = -1) or a trace message (`msg_level` >= 0) by calling `output_message`. It receives the JPEG object pointer and the message level. Override this method primarily to change the handling of warnings, such as choosing to abort processing when a warning occurs.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_47\n\nLANGUAGE: c\nCODE:\n```\nemit_message (j_common_ptr cinfo, int msg_level)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV HAL (Hardware Abstraction Layer) in CMake\nDESCRIPTION: Sets up the OpenCV Hardware Abstraction Layer (HAL) by registering various acceleration libraries and configuring their inclusion based on build options and platform support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nset(_hal_includes \"\")\nmacro(ocv_hal_register HAL_LIBRARIES_VAR HAL_HEADERS_VAR HAL_INCLUDE_DIRS_VAR)\n  # 1. libraries\n  foreach (l ${${HAL_LIBRARIES_VAR}})\n    if(NOT TARGET ${l})\n      get_filename_component(l \"${l}\" ABSOLUTE)\n    endif()\n    list(APPEND OPENCV_HAL_LINKER_LIBS ${l})\n  endforeach()\n  # 2. headers\n  foreach (h ${${HAL_HEADERS_VAR}})\n    set(_hal_includes \"${_hal_includes}\\n#include \\\"${h}\\\"\")\n  endforeach()\n  # 3. include paths\n  ocv_include_directories(${${HAL_INCLUDE_DIRS_VAR}})\nendmacro()\n\nif(NOT DEFINED OpenCV_HAL)\n  set(OpenCV_HAL \"OpenCV_HAL\")\nendif()\n\n# ... (HAL configuration for various accelerations) ...\n\nforeach(hal ${OpenCV_HAL})\n  if(hal STREQUAL \"carotene\")\n    if(\";${CPU_BASELINE_FINAL};\" MATCHES \";NEON;\")\n      add_subdirectory(3rdparty/carotene/hal)\n      ocv_hal_register(CAROTENE_HAL_LIBRARIES CAROTENE_HAL_HEADERS CAROTENE_HAL_INCLUDE_DIRS)\n      list(APPEND OpenCV_USED_HAL \"carotene (ver ${CAROTENE_HAL_VERSION})\")\n    else()\n      message(STATUS \"Carotene: NEON is not available, disabling carotene...\")\n    endif()\n  elseif(hal STREQUAL \"fastcv\")\n    # ... (similar configuration for other HALs) ...\n  endif()\nendforeach()\nconfigure_file(\"${OpenCV_SOURCE_DIR}/cmake/templates/custom_hal.hpp.in\" \"${OPENCV_CONFIG_FILE_INCLUDE_DIR}/custom_hal.hpp\" @ONLY)\nunset(_hal_includes)\n```\n\n----------------------------------------\n\nTITLE: Detecting Line Segments using Probabilistic Hough Transform in OpenCV.js\nDESCRIPTION: Uses the Probabilistic Hough Transform, an optimized version, to detect line segments in a binary image. It returns lines represented by their endpoints (x1, y1, x2, y2). Requires a single-channel 8-bit binary input image. Key parameters include `rho`, `theta`, `threshold`, `minLineLength` (minimum segment length), and `maxLineGap` (maximum gap between points on the same line). Depends on the OpenCV.js library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_houghlines/js_houghlines.markdown#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ncv.HoughLinesP (image, lines, rho, theta, threshold, minLineLength = 0, maxLineGap = 0)\n\n@param image          8-bit, single-channel binary source image. The image may be modified by the function.\n@param lines          output vector of lines(cv.32SC4 type). Each line is represented by a 4-element vector (x1,y1,x2,y2) ,where (x1,y1) and (x2,y2) are the ending points of each detected line segment.\n@param rho            distance resolution of the accumulator in pixels.\n@param theta          angle resolution of the accumulator in radians.\n@param threshold      accumulator threshold parameter. Only those lines are returned that get enough votes\n@param minLineLength  minimum line length. Line segments shorter than that are rejected.\n@param maxLineGap     maximum allowed gap between points on the same line to link them.\n```\n\n----------------------------------------\n\nTITLE: Estimating Camera Response Function for HDR Processing in OpenCV\nDESCRIPTION: This code estimates the camera response function (CRF) which is necessary for HDR construction algorithms. It uses a calibration algorithm to calculate the inverse CRF for all 256 possible pixel values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/hdr_imaging.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nMat response;\nPtr<CalibrateDebevec> calibrate = createCalibrateDebevec();\ncalibrate->process(images, response, times);\n```\n\nLANGUAGE: java\nCODE:\n```\nMat response = new Mat();\nCalibrateDebevec calibrate = Photo.createCalibrateDebevec();\ncalibrate.process(images, response, times);\n```\n\nLANGUAGE: python\nCODE:\n```\ncalibrate = cv.createCalibrateDebevec()\nresponse = calibrate.process(images, times)\n```\n\n----------------------------------------\n\nTITLE: Handling Full Output Buffer in JPEG Destination Manager (C)\nDESCRIPTION: Defines the `empty_output_buffer` function signature, a required method for a custom `jpeg_destination_mgr`. This function is called by the library whenever the output buffer fills up (`free_in_buffer` reaches zero). The implementation should process the entire buffer content (e.g., write it to a file or network stream), reset `next_output_byte` to the start of the buffer, reset `free_in_buffer` to the total buffer size (must be positive), and return `TRUE`. Returning `FALSE` is used for I/O suspension.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_51\n\nLANGUAGE: c\nCODE:\n```\nempty_output_buffer (j_compress_ptr cinfo)\n```\n\n----------------------------------------\n\nTITLE: Defining a Point in Java\nDESCRIPTION: Two different ways to define a 2D point using the Point class in Java. A Point represents a 2D coordinate with x and y values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nPoint pt = new Point();\npt.x = 10;\npt.y = 8;\n```\n\nLANGUAGE: java\nCODE:\n```\nPoint pt = new Point(10, 8);\n```\n\n----------------------------------------\n\nTITLE: Fallback Case for Non-Maximum Suppression in C++\nDESCRIPTION: This code snippet handles the fallback case where non-maximum suppression is not applied. In this case, all original keypoints are retained without filtering.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_47\n\nLANGUAGE: C++\nCODE:\n```\n} else\n{\n  keypoints = kpts;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries in OpenCV Module\nDESCRIPTION: Links private targets to the specified OpenCV module, facilitating the integration of dependencies required for module functionality. The 'tgts' parameter comprises the names of the targets to be linked. This snippet expects the module and its dependencies to be defined in the build system context.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\nocv_target_link_libraries(${the_module} LINK_PRIVATE ${tgts})\n```\n\n----------------------------------------\n\nTITLE: Building Android Projects and Examples with OpenCV (CMake/Environment)\nDESCRIPTION: These options control building Android-specific projects and examples. Requires Android SDK and NDK to be installed. Set BUILD_ANDROID_PROJECTS and BUILD_ANDROID_EXAMPLES to ON in the CMake command, and define Android toolchain and SDK/NDK paths using ANDROID_HOME, ANDROID_SDK, ANDROID_NDK, or ANDROID_SDK_ROOT.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_31\n\nLANGUAGE: cmake\nCODE:\n```\nBUILD_ANDROID_PROJECTS\nBUILD_ANDROID_EXAMPLES\nANDROID_HOME\nANDROID_SDK\nANDROID_NDK\nANDROID_SDK_ROOT\n```\n\n----------------------------------------\n\nTITLE: Accessing an HTML Range Input by ID - JavaScript\nDESCRIPTION: This snippet retrieves a previously defined HTML input element of type 'range' using its ID via document.getElementById. This is necessary to read or set the trackbar's value or attach event handlers. The variable 'x' will reference the input range element with ID 'myRange'. No specific dependencies other than a matching DOM element.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_gui/js_trackbar/js_trackbar.markdown#2025-04-22_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet x = document.getElementById('myRange');\n```\n\n----------------------------------------\n\nTITLE: Example Output of pkg-config for OpenCV Libraries\nDESCRIPTION: This illustrates the typical output of the `pkg-config --libs opencv` command. It includes the library search path (`-L/usr/local/lib`) and a list of OpenCV module libraries (`-lopencv_core`, `-lopencv_imgproc`, etc.) needed for linking. This information is used to configure the linker settings in Eclipse.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_eclipse/linux_eclipse.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n-L/usr/local/lib -lopencv_core -lopencv_imgproc -lopencv_highgui -lopencv_ml -lopencv_video -lopencv_features2d -lopencv_calib3d -lopencv_objdetect -lopencv_videoio -lopencv_imgcodecs -lopencv_flann\n```\n\n----------------------------------------\n\nTITLE: Including ChArUco Header\nDESCRIPTION: Shows how to include the necessary header file for ChArUco functionality\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_detection/charuco_detection.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <opencv2/aruco/charuco.hpp>\n```\n\n----------------------------------------\n\nTITLE: Configuring IPP Library Build Settings\nDESCRIPTION: Sets up compiler definitions, include directories, linked libraries and output settings for the IPP HAL library. Includes conditional compilation for IPP ICV and IW components.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ipphal/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_IPP_ICV)\n  target_compile_definitions(ipphal PRIVATE HAVE_IPP_ICV)\nendif()\n\nif(HAVE_IPP_IW)\n  target_compile_definitions(ipphal PRIVATE HAVE_IPP_IW)\nendif()\n\ntarget_include_directories(ipphal PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\nocv_warnings_disable(CMAKE_CXX_FLAGS -Wno-suggest-override)\n\ntarget_include_directories(ipphal PRIVATE\n  \"${CMAKE_CURRENT_SOURCE_DIR}/src\"\n  ${CMAKE_SOURCE_DIR}/modules/core/include\n  ${IPP_INCLUDE_DIRS}\n)\n\ntarget_link_libraries(ipphal PUBLIC ${IPP_IW_LIBRARY} ${IPP_LIBRARIES})\n\nset_target_properties(ipphal PROPERTIES ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH})\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories and Compiler Options Based on Toolchain - CMake\nDESCRIPTION: Applies collected include directories to the build and selectively disables compiler warnings depending on whether GNU or Clang compiler is used. Ensures source compatibility with warning-free builds, especially for Protobuf-generated and third-party code. Inputs: include_dirs, compiler detection; outputs: build flags and warning suppression.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\nocv_module_include_directories(${include_dirs})\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n  ocv_append_source_files_cxx_compiler_options(fw_srcs \"-Wno-suggest-override\")  # GCC\n  ocv_append_source_files_cxx_compiler_options(fw_srcs \"-Wno-array-bounds\")  # GCC 9.3.0 (Ubuntu 20.04)\nelif(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n  ocv_append_source_files_cxx_compiler_options(fw_srcs \"-Wno-inconsistent-missing-override\")  # Clang\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Quantization Tables in libjpeg (C)\nDESCRIPTION: A C field within the compression parameters structure (`cinfo`), representing an array of pointers (`JQUANT_TBL *`) to quantization tables. Each element corresponds to a table slot (indexed 0 to NUM_QUANT_TBLS-1). A NULL pointer indicates that no custom table is defined for that slot, causing libjpeg to use default tables. This allows specifying custom quantization matrices for lossy compression.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_42\n\nLANGUAGE: C\nCODE:\n```\nJQUANT_TBL *quant_tbl_ptrs[NUM_QUANT_TBLS]\n```\n\n----------------------------------------\n\nTITLE: Example of Running the Ant Build Script (Windows) (Batch)\nDESCRIPTION: This Batch command provides a concrete example of executing the Ant build script on a Windows system. It passes example paths for the OpenCV JAR directory (`X:\\opencv-2.4.4\\bin`) and the native library directory (`X:\\opencv-2.4.4\\bin\\Release`) using the `-D` flag.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_9\n\nLANGUAGE: bat\nCODE:\n```\nant -DocvJarDir=X:\\opencv-2.4.4\\bin -DocvLibDir=X:\\opencv-2.4.4\\bin\\Release\n```\n\n----------------------------------------\n\nTITLE: Configuring Android Test Source Directories in CMake\nDESCRIPTION: Defines the source directories for Android tests, including the current source directory, common test directory, and generated test directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/android_test/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(ANDROID_TESTS_SRC_DIRS\n\"'${CMAKE_CURRENT_SOURCE_DIR}/src', \\\n'${OpenCV_SOURCE_DIR}/modules/java/test/common_test/src', \\\n'${CMAKE_BINARY_DIR}/modules/java_bindings_generator/gen/test'\" CACHE INTERNAL \"\")\n```\n\n----------------------------------------\n\nTITLE: Installing ARM Cross-Compilation Toolchain - Bash\nDESCRIPTION: Installs the gcc-arm-linux-gnueabi toolchain required for cross-compiling software from a Linux host for ARM targets using the gnueabi ABI. Dependencies include apt-get and network access to Ubuntu repositories. The primary parameter is the package name, and the command should be run with sudo privileges. This command is specific to distributions using the apt package manager.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/arm_crosscompile_with_cmake.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install gcc-arm-linux-gnueabi\n```\n\n----------------------------------------\n\nTITLE: Drawing Rectangle with OpenCV in Python\nDESCRIPTION: Draws a green rectangle at the top-right corner of the image using the cv.rectangle() function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_drawing_functions/py_drawing_functions.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncv.rectangle(img,(384,0),(510,128),(0,255,0),3)\n```\n\n----------------------------------------\n\nTITLE: Running the SSD MobileNetV1 Model Retrieval Script - Console\nDESCRIPTION: Runs the sample Python module that retrieves the SSD MobileNetV1 TensorFlow model and extracts the frozen graph. The command expects the appropriate module path and is executed within the activated environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.tf.detection.py_to_py_ssd_mobilenet\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Conditional Logic\nDESCRIPTION: Complex nested conditional logic for comparing pixel intensity values at different offsets around a candidate corner point. The code uses goto statements to branch between success_structured, structured, and homogeneous cases based on pixel value comparisons against threshold values c_b and cb.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_26\n\nLANGUAGE: cpp\nCODE:\n```\ngoto success_structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  if(ptr[offset1] < c_b)\n    goto success_structured;\n  else\n    goto structured;\nelse\n  if(ptr[offset1] < c_b)\n    if(ptr[offset4] < c_b)\n      goto success_structured;\n    else\n      goto structured;\n  else\n    goto structured;\nelse\n  if(ptr[offset7] > cb)\n    if(ptr[offset9] > cb)\n      if(ptr[offset5] > cb)\n        if(ptr[offset4] > cb)\n          if(ptr[offset6] > cb)\n            if(ptr[offset8] > cb)\n              if(ptr[offset10] > cb)\n                goto success_structured;\n              else\n                goto structured;\n            else\n              goto structured;\n          else\n            goto structured;\n        else\n          goto structured;\n      else\n        goto structured;\n    else\n      goto structured;\n  else\n    goto structured;\n```\n\n----------------------------------------\n\nTITLE: Configuring Restart Markers in libjpeg (C)\nDESCRIPTION: These C integer fields within the compression parameters structure (`cinfo`) control the insertion of restart markers in the JPEG data stream. Setting `restart_interval` specifies the interval in Minimum Coded Units (MCUs), while `restart_in_rows` sets the interval based on MCU rows. Non-zero values enable restart markers, which can improve robustness against data corruption. Defaults are zero (no restarts).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_35\n\nLANGUAGE: C\nCODE:\n```\nunsigned int restart_interval\n```\n\nLANGUAGE: C\nCODE:\n```\nint restart_in_rows\n```\n\n----------------------------------------\n\nTITLE: Calculating Contour Area in Python with OpenCV\nDESCRIPTION: This snippet demonstrates how to calculate the area of a contour using the cv.contourArea() function in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\narea = cv.contourArea(cnt)\n```\n\n----------------------------------------\n\nTITLE: Configuring Random Data Array for CMake Project\nDESCRIPTION: This snippet uses CMake to generate random hexadecimal values and populates them into a C array. It relies on CMake's built-in functions like set, string, and math to manipulate and generate data. The process starts by defining the array size and header paths, followed by generating random numbers and formatting them appropriately. The final step uses configure_file to apply the generated data to a header file template. The snippet is specific to CMake's functionalities for managing build properties and files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/semihosting/include/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(RAW_PIXELS_SIZE 102400)\nset(RAW_PIXELS_HEADER ${CMAKE_CURRENT_BINARY_DIR}/raw_pixels.hpp)\nset(RAW_PIXELS_HEADER_IN ${CMAKE_CURRENT_SOURCE_DIR}/raw_pixels.hpp.in)\n\nset(RAW_PIXEL_VALUES \"\")\n# Seed the random number generator.\nstring(RANDOM LENGTH 8 ALPHABET 0123456789abcdf RANDOM_SEED 314 number)\nmath(EXPR LOOP_RANGE \"${RAW_PIXELS_SIZE} - 1\")\n\nforeach(i RANGE ${LOOP_RANGE})\n  string(RANDOM LENGTH 8 ALPHABET 0123456789abcdf number)\n  string(CONCAT RAW_PIXEL_VALUES ${RAW_PIXEL_VALUES} \"0x${number}, \\\\\\n\")\nendforeach()\n\nconfigure_file(${RAW_PIXELS_HEADER_IN} ${RAW_PIXELS_HEADER})\n```\n\n----------------------------------------\n\nTITLE: Displaying 32F Image by Conversion to 8U for imshow with OpenCV in C++\nDESCRIPTION: Shows how to visualize a 32-bit float image by converting to unsigned 8-bit for imshow. Uses convertTo to cast and possibly normalize. Ensures imshow can display image properly. Be aware of dynamic range mapping.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_43\n\nLANGUAGE: C++\nCODE:\n```\ncv::Mat img8u;\\nimg.convertTo(img8u, CV_8U, 255.0);\\ncv::imshow(\"Window\", img8u);\\ncv::waitKey(0);\n```\n\n----------------------------------------\n\nTITLE: Embedding External HTML Content with iframe in HTML\nDESCRIPTION: This HTML snippet uses an <iframe> tag to embed the content of `js_imgproc_camera.html` within the current page. The `onload` attribute contains JavaScript code to dynamically adjust the iframe's height to fit the loaded content's height, ensuring the embedded page is fully visible without scrolling within the frame itself. This technique is commonly used in documentation or tutorials to include interactive examples or separate HTML documents.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_imgproc_camera/js_imgproc_camera.markdown#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe src=\"../../js_imgproc_camera.html\" width=\"100%\"\n        onload=\"this.style.height=this.contentDocument.body.scrollHeight +'px';\">\n</iframe>\n```\n\n----------------------------------------\n\nTITLE: JPEG Decompression Operation Structure in C\nDESCRIPTION: Basic outline of a JPEG decompression operation showing the sequence of function calls needed to decompress a JPEG image using the IJG JPEG library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_1\n\nLANGUAGE: C\nCODE:\n```\nAllocate and initialize a JPEG decompression object\nSpecify the source of the compressed data (eg, a file)\nCall jpeg_read_header() to obtain image info\nSet parameters for decompression\njpeg_start_decompress(...);\nwhile (scan lines remain to be read)\n        jpeg_read_scanlines(...);  /* Use jpeg12_read_scanlines() for\n                                      9-bit through 12-bit data\n```\n\n----------------------------------------\n\nTITLE: Configuring Compiler Warnings for libjasper in CMake\nDESCRIPTION: This snippet disables specific compiler warnings for the libjasper library build across different compilers and platforms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjasper/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nocv_warnings_disable(CMAKE_C_FLAGS -Wno-implicit-function-declaration -Wno-uninitialized -Wmissing-prototypes\n                                   -Wno-unused-but-set-parameter -Wmissing-declarations -Wunused -Wshadow\n                                   -Wsign-compare -Wstrict-overflow -Wpointer-compare\n                                   -Wabsolute-value  # clang on Linux\n                                   -Wimplicit-fallthrough\n)\nocv_warnings_disable(CMAKE_C_FLAGS -Wunused-parameter -Wstrict-prototypes) # clang\nocv_warnings_disable(CMAKE_C_FLAGS /wd4013 /wd4018 /wd4101 /wd4244 /wd4267 /wd4715) # vs2005\n```\n\n----------------------------------------\n\nTITLE: Aggregating Include Directories and Library Dependencies for DNN Module - CMake\nDESCRIPTION: Appends framework include directories and library dependencies to the DNN build based on earlier configuration. Handles special case if protobuf is not internally built, then includes system protobuf includes as well. Input variables derived from previous code (fw_inc, fw_hdrs, etc.). Output aggregates required dependencies for compilation and linking.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND include_dirs ${fw_inc})\nlist(APPEND libs ${Protobuf_LIBRARIES})\nif(NOT BUILD_PROTOBUF)\n  list(APPEND include_dirs ${Protobuf_INCLUDE_DIRS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Template for Apache License 2.0 Notice (Text)\nDESCRIPTION: This is a standard boilerplate notice template recommended by the Apache License 2.0 appendix. It should be included in source files, typically within comments. The placeholders `[yyyy]` (year) and `[name of copyright owner]` must be replaced with the specific project details. It asserts copyright and states the work is licensed under Apache 2.0, providing a link to the full license.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/flatbuffers/LICENSE.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Generating Eclipse Project Files with SBT (Bash)\nDESCRIPTION: This Bash snippet shows how to use SBT to generate Eclipse project files. First, `sbt` starts the SBT interactive console. Then, typing `eclipse` within the console invokes the command provided by the `sbteclipse-plugin` (added previously) to create the necessary `.project` and `.classpath` files for importing the project into Eclipse.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nsbt # Starts the sbt console\neclipse # Running \"eclipse\" from within the sbt console\n```\n\n----------------------------------------\n\nTITLE: Copying Test Project Files in CMake\nDESCRIPTION: Copies essential test project files to the Android test directory. It iterates through a list of files and copies each one.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/android_test/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND TEST_PROJECT_FILES \"CMakeLists.txt\" \"gradle.properties\" \"settings.gradle\")\nforeach(TEST_PROJECT_FILE ${TEST_PROJECT_FILES})\n    file(COPY \"${CMAKE_CURRENT_SOURCE_DIR}/${TEST_PROJECT_FILE}\" DESTINATION \"${OPENCV_ANDROID_TEST_DIR}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Calculating Object Orientation via PCA in C++ using OpenCV\nDESCRIPTION: Defines a function `getOrientation` that takes a vector of points (a contour) and calculates its orientation using PCA. It formats the contour points into a Mat, performs PCA using `cv::PCA`, and extracts the mean (center), eigenvectors, and eigenvalues. Requires OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/introduction_to_pca.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n//! [pca]\n// Perform PCA analysis\nstatic double getOrientation(const vector<Point> &pts, Mat &img)\n{\n    //Construct a buffer used by the PCA analysis\n    int sz = static_cast<int>(pts.size());\n    Mat data_pts = Mat(sz, 2, CV_64F);\n    for (int i = 0; i < data_pts.rows; ++i)\n    {\n        data_pts.at<double>(i, 0) = pts[i].x;\n        data_pts.at<double>(i, 1) = pts[i].y;\n    }\n\n    //Perform PCA analysis\n    PCA pca_analysis(data_pts, Mat(), PCA::DATA_AS_ROW);\n\n    //Store the center of the object\n    Point cntr = Point(static_cast<int>(pca_analysis.mean.at<double>(0, 0)),\n                      static_cast<int>(pca_analysis.mean.at<double>(0, 1)));\n\n    //Store the eigenvalues and eigenvectors\n    vector<Point2d> eigen_vecs(2);\n    vector<double> eigen_val(2);\n    for (int i = 0; i < 2; ++i)\n    {\n        eigen_vecs[i] = Point2d(pca_analysis.eigenvectors.at<double>(i, 0),\n                                pca_analysis.eigenvectors.at<double>(i, 1));\n\n        eigen_val[i] = pca_analysis.eigenvalues.at<double>(i);\n    }\n\n    // Draw the principal components\n    circle(img, cntr, 3, Scalar(255, 0, 255), 2);\n    Point p1 = cntr + 0.02 * Point(static_cast<int>(eigen_vecs[0].x * eigen_val[0]), static_cast<int>(eigen_vecs[0].y * eigen_val[0]));\n    Point p2 = cntr - 0.02 * Point(static_cast<int>(eigen_vecs[1].x * eigen_val[1]), static_cast<int>(eigen_vecs[1].y * eigen_val[1]));\n    drawAxis(img, cntr, p1, Scalar(0, 255, 0), 1);\n    drawAxis(img, cntr, p2, Scalar(255, 255, 0), 5);\n\n    double angle = atan2(eigen_vecs[0].y, eigen_vecs[0].x); // orientation in radians\n\n    return angle;\n}\n//! [pca]\n```\n\n----------------------------------------\n\nTITLE: Creating Basic HTML Structure for Image Upload (HTML/JavaScript)\nDESCRIPTION: Defines the basic HTML structure for a web page titled 'Hello OpenCV.js'. It includes an `<img>` tag to display an image, a file input element (`<input type='file'>`) for uploading images, and embedded JavaScript to handle the file selection. When a file is selected, the JavaScript updates the `src` attribute of the `<img>` tag to display the chosen image using `URL.createObjectURL`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_usage/js_usage.markdown#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html>\n<head>\n<meta charset=\"utf-8\">\n<title>Hello OpenCV.js</title>\n</head>\n<body>\n<h2>Hello OpenCV.js</h2>\n<div>\n  <div class=\"inputoutput\">\n    <img id=\"imageSrc\" alt=\"No Image\" />\n    <div class=\"caption\">imageSrc <input type=\"file\" id=\"fileInput\" name=\"file\" /></div>\n  </div>\n</div>\n<script type=\"text/javascript\">\nlet imgElement = document.getElementById(\"imageSrc\")\nlet inputElement = document.getElementById(\"fileInput\");\ninputElement.addEventListener(\"change\", (e) => {\n  imgElement.src = URL.createObjectURL(e.target.files[0]);\n}, false);\n</script>\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: JPEG Configuration Structures\nDESCRIPTION: Configuration structure members for controlling JPEG decoding behavior and color quantization settings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_58\n\nLANGUAGE: c\nCODE:\n```\ncinfo.two_pass_quantize\ncinfo.colormap\ncinfo.actual_number_of_colors\ncinfo.enable_1pass_quant\ncinfo.enable_external_quant\ncinfo.enable_2pass_quant\n```\n\n----------------------------------------\n\nTITLE: Cross-Compiling OpenCV for RISC-V using CMake\nDESCRIPTION: This CMake command configures the build environment for cross-compiling OpenCV for RISC-V. It specifies the toolchain file, installation prefix, and paths to the RISC-V toolchain and QEMU.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/arch/riscv/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncmake -G Ninja -B ./build-riscv \\\n  -D CMAKE_TOOLCHAIN_FILE=./cmake/toolchain-riscv.cmake \\\n  -D CMAKE_INSTALL_PREFIX=./build-riscv/install \\\n  -D TOOLCHAIN_PATH={TOOLCHAIN_PATH} \\\n  -D QEMU_PATH={QEMU_PATH} \\\n  .\n\ncmake --build ./build-riscv\n```\n\n----------------------------------------\n\nTITLE: Setting Uniform and Accumulate Flags in Python\nDESCRIPTION: Python snippet implicitly sets histogram parameters. While `uniform` and `accumulate` flags exist in the C++ `calcHist` function, they are often handled implicitly or through default arguments in the Python `cv.calcHist` wrapper. The standard usage shown typically implies uniform bins and non-accumulating calculation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@snippet samples/python/tutorial_code/Histograms_Matching/histogram_calculation/calcHist_Demo.py Set histogram param\n```\n\n----------------------------------------\n\nTITLE: Defining Extended Colorspace Constants in C for libjpeg-turbo\nDESCRIPTION: Defines C constants representing extended colorspaces (like RGBX, BGR, ABGR, etc.) supported by libjpeg-turbo. These constants are used with `cinfo.in_color_space` (compression) or `cinfo.out_color_space` (decompression) to handle different pixel buffer orderings directly, improving performance by avoiding manual conversion.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/README.md#2025-04-22_snippet_0\n\nLANGUAGE: c\nCODE:\n```\nJCS_EXT_RGB   /* red/green/blue */\nJCS_EXT_RGBX  /* red/green/blue/x */\nJCS_EXT_BGR   /* blue/green/red */\nJCS_EXT_BGRX  /* blue/green/red/x */\nJCS_EXT_XBGR  /* x/blue/green/red */\nJCS_EXT_XRGB  /* x/red/green/blue */\nJCS_EXT_RGBA  /* red/green/blue/alpha */\nJCS_EXT_BGRA  /* blue/green/red/alpha */\nJCS_EXT_ABGR  /* alpha/blue/green/red */\nJCS_EXT_ARGB  /* alpha/red/green/blue */\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Decision Tree in C++\nDESCRIPTION: A decision tree implementation for the FAST corner detection algorithm. The code compares pixel values at various offsets against threshold values (cb and c_b) to determine if a point is a corner. The algorithm uses goto statements to branch to either 'is_a_corner' or 'is_not_a_corner' based on the evaluation results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset4] > cb)\n  goto is_a_corner;\nelse\n  if(ptr[offset10] > cb)\n    goto is_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset8] > cb)\n    if(ptr[offset10] > cb)\n      goto is_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset6] > cb)\n    if(ptr[offset3] > cb)\n      if(ptr[offset4] > cb)\n        goto is_a_corner;\n      else\n        if(ptr[offset10] > cb)\n          if(ptr[offset11] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      if(ptr[offset8] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset11] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    if(ptr[offset11] > cb)\n      if(ptr[offset3] > cb)\n        if(ptr[offset4] > cb)\n          goto is_a_corner;\n        else\n          if(ptr[offset10] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset8] > cb)\n          if(ptr[offset10] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset1] < c_b)\n    goto is_not_a_corner;\n  else\n    if(ptr[offset1] > cb)\n      if(ptr[offset6] > cb)\n        if(ptr[offset3] > cb)\n          if(ptr[offset4] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        if(ptr[offset6] < c_b)\n          if(ptr[offset3] > cb)\n            if(ptr[offset4] > cb)\n              if(ptr[offset11] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          if(ptr[offset3] > cb)\n            if(ptr[offset4] > cb)\n              if(ptr[offset11] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  if(ptr[offset9] < c_b)\n    if(ptr[offset7] > cb)\n      if(ptr[offset1] < c_b)\n        if(ptr[offset6] < c_b)\n          goto is_not_a_corner;\n        else\n          if(ptr[offset6] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset4] > cb)\n                if(ptr[offset8] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset1] > cb)\n          if(ptr[offset6] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset4] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n        else\n          if(ptr[offset6] < c_b)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset8] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n    else\n      if(ptr[offset1] < c_b)\n        goto is_not_a_corner;\n      else\n        if(ptr[offset1] > cb)\n          if(ptr[offset6] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset4] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n  else\n    if(ptr[offset7] > cb)\n      if(ptr[offset9] > cb)\n        if(ptr[offset1] < c_b)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building libjpeg-turbo - CMake\nDESCRIPTION: This CMake code snippet configures compilation and building flags for libjpeg-turbo within OpenCV, setting project variables, compiler options, include files, and versions. It provides macros and options for feature toggling (arithmetic coding support, SIMD), performs feature checks, assigns source files according to build configuration and hardware platform, and defines static library targets. Dependencies include OpenCV's CMake helpers, platform compiler specifics (such as GCC/Clang/MSVC), and relevant source files in the 'src' directory. The input and output revolve around build options, environment detection, and artifacts such as generated header files and static libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(${JPEG_LIBRARY} C)\n\nmacro(boolean_number var)\n  if(${var})\n    set(${var} 1 ${ARGN})\n  else()\n    set(${var} 0 ${ARGN})\n  endif()\nendmacro()\n\nocv_warnings_disable(CMAKE_C_FLAGS -Wunused-parameter -Wsign-compare -Wshorten-64-to-32 -Wimplicit-fallthrough)\nif(APPLE)\n  ocv_warnings_disable(CMAKE_C_FLAGS -Wunused-variable) # NEON flags are not used on Mac\nendif()\n\nif(CV_GCC AND NOT CMAKE_CXX_COMPILER_VERSION VERSION_LESS 13)\n  # src/jchuff.c:1042:22: warning: writing 1 byte into a region of size 0 [-Wstringop-overflow=]\n  ocv_warnings_disable(CMAKE_C_FLAGS -Wstringop-overflow)\nendif()\n\nset(VERSION 3.1.0)\nset(COPYRIGHT_YEAR \"1991-2024\")\nstring(REPLACE \".\" \";\" VERSION_TRIPLET ${VERSION})\nlist(GET VERSION_TRIPLET 0 VERSION_MAJOR)\nlist(GET VERSION_TRIPLET 1 VERSION_MINOR)\nlist(GET VERSION_TRIPLET 2 VERSION_REVISION)\nfunction(pad_number NUMBER OUTPUT_LEN)\n  string(LENGTH \"${${NUMBER}}\" INPUT_LEN)\n  if(INPUT_LEN LESS OUTPUT_LEN)\n    math(EXPR ZEROES \"${OUTPUT_LEN} - ${INPUT_LEN} - 1\")\n    set(NUM ${${NUMBER}})\n    foreach(C RANGE ${ZEROES})\n      set(NUM \"0${NUM}\")\n    endforeach()\n    set(${NUMBER} ${NUM} PARENT_SCOPE)\n  endif()\nendfunction()\npad_number(VERSION_MINOR 3)\npad_number(VERSION_REVISION 3)\nset(LIBJPEG_TURBO_VERSION_NUMBER ${VERSION_MAJOR}${VERSION_MINOR}${VERSION_REVISION})\n\nstring(TIMESTAMP BUILD \"opencv-${OPENCV_VERSION}-libjpeg-turbo\")\nif(CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n  set(BUILD \"${BUILD}-debug\")\nendif()\n\nmessage(STATUS \"libjpeg-turbo: VERSION = ${VERSION}, BUILD = ${BUILD}\")\n\nmath(EXPR BITS \"${CMAKE_SIZEOF_VOID_P} * 8\")\nstring(TOLOWER \"${CMAKE_SYSTEM_PROCESSOR}\" CMAKE_SYSTEM_PROCESSOR_LC)\n\nif(CMAKE_SYSTEM_PROCESSOR_LC MATCHES \"x86_64\" OR\n  CMAKE_SYSTEM_PROCESSOR_LC MATCHES \"amd64\" OR\n  CMAKE_SYSTEM_PROCESSOR_LC MATCHES \"i[0-9]86\" OR\n  CMAKE_SYSTEM_PROCESSOR_LC MATCHES \"x86\" OR\n  CMAKE_SYSTEM_PROCESSOR_LC MATCHES \"ia32\")\n  if(BITS EQUAL 64 OR CMAKE_C_COMPILER_ABI MATCHES \"ELF X32\")\n    set(CPU_TYPE x86_64)\n  else()\n    set(CPU_TYPE i386)\n  endif()\n  if(NOT CMAKE_SYSTEM_PROCESSOR STREQUAL ${CPU_TYPE})\n    set(CMAKE_SYSTEM_PROCESSOR ${CPU_TYPE})\n  endif()\nelseif(CMAKE_SYSTEM_PROCESSOR_LC STREQUAL \"aarch64\" OR\n  CMAKE_SYSTEM_PROCESSOR_LC MATCHES \"^arm\")\n  if(BITS EQUAL 64)\n    set(CPU_TYPE arm64)\n  else()\n    set(CPU_TYPE arm)\n  endif()\nelseif(CMAKE_SYSTEM_PROCESSOR_LC MATCHES \"^ppc\" OR\n  CMAKE_SYSTEM_PROCESSOR_LC MATCHES \"^powerpc\")\n  set(CPU_TYPE powerpc)\nelse()\n  set(CPU_TYPE ${CMAKE_SYSTEM_PROCESSOR_LC})\nendif()\nif(CMAKE_OSX_ARCHITECTURES MATCHES \"x86_64\" OR\n  CMAKE_OSX_ARCHITECTURES MATCHES \"arm64\" OR\n  CMAKE_OSX_ARCHITECTURES MATCHES \"i386\")\n  set(CPU_TYPE ${CMAKE_OSX_ARCHITECTURES})\nendif()\nif(CMAKE_OSX_ARCHITECTURES MATCHES \"ppc\")\n  set(CPU_TYPE powerpc)\nendif()\nif(MSVC_IDE AND CMAKE_GENERATOR_PLATFORM MATCHES \"arm64\")\n  set(CPU_TYPE arm64)\nendif()\n\nOCV_OPTION(ENABLE_LIBJPEG_TURBO_SIMD \"Include SIMD extensions for libjpeg-turbo, if available for this platform\" (NOT CV_DISABLE_OPTIMIZATION))\noption(WITH_ARITH_ENC \"Include arithmetic encoding support when emulating the libjpeg v6b API/ABI\" TRUE)\noption(WITH_ARITH_DEC \"Include arithmetic decoding support when emulating the libjpeg v6b API/ABI\" TRUE)\nset(WITH_SIMD 1)\nset(HAVE_LIBJPEG_TURBO_SIMD 0 PARENT_SCOPE)\n\ninclude(CheckCSourceCompiles)\ninclude(CheckIncludeFiles)\ninclude(CheckTypeSize)\n\ncheck_type_size(\"size_t\" SIZE_T)\ncheck_type_size(\"unsigned long\" UNSIGNED_LONG)\n\nif(SIZEOF_SIZE_T EQUAL SIZEOF_UNSIGNED_LONG)\n  check_c_source_compiles(\"int main(int argc, char **argv) { unsigned long a = argc;  return __builtin_ctzl(a); }\"\n    HAVE_BUILTIN_CTZL)\nendif()\nif(MSVC)\n  check_include_files(\"intrin.h\" HAVE_INTRIN_H)\nendif()\n\nif(UNIX)\n  # Check for headers\n  check_include_files(locale.h HAVE_LOCALE_H)\n  check_include_files(stddef.h HAVE_STDDEF_H)\n  check_include_files(stdlib.h HAVE_STDLIB_H)\n  check_include_files(sys/types.h NEED_SYS_TYPES_H)\n\n  # Other predefines\n  # undef NEED_BSD_STRINGS\n  ocv_update(HAVE_UNSIGNED_CHAR 1)\n  ocv_update(HAVE_UNSIGNED_SHORT 1)\n  # undef INCOMPLETE_TYPES_BROKEN\n  ocv_update(RIGHT_SHIFT_IS_UNSIGNED 0)\nendif()\n\n\nset(BITS_IN_JSAMPLE 8)\n\nif(WITH_ARITH_ENC)\n  set(C_ARITH_CODING_SUPPORTED 1)\nendif()\n\nif(WITH_ARITH_DEC)\n  set(D_ARITH_CODING_SUPPORTED 1)\nendif()\n\nset(JPEG_LIB_VERSION 70)\n\n# OpenCV\nset(JPEG_LIB_VERSION \"${VERSION}-${JPEG_LIB_VERSION}\" PARENT_SCOPE)\n\nset(THREAD_LOCAL \"\")  # WITH_TURBOJPEG is not used\n\nadd_definitions(-DNO_GETENV -DNO_PUTENV)\n\nif(MSVC)\n  add_definitions(-W3 -wd4996 -wd4018)\nendif()\n\ninclude_directories(${CMAKE_CURRENT_BINARY_DIR} ${CMAKE_CURRENT_SOURCE_DIR}/src)\n\nset(JPEG16_SOURCES jcapistd.c jccolor.c jcdiffct.c jclossls.c jcmainct.c\n    jcprepct.c jcsample.c jdapistd.c jdcolor.c jddiffct.c jdlossls.c jdmainct.c\n    jdpostct.c jdsample.c jutils.c)\n\nset(JPEG12_SOURCES ${JPEG16_SOURCES} jccoefct.c jcdctmgr.c jdcoefct.c\n    jddctmgr.c jdmerge.c jfdctfst.c jfdctint.c jidctflt.c jidctfst.c jidctint.c\n    jidctred.c jquant1.c jquant2.c)\n\nset(JPEG_SOURCES ${JPEG12_SOURCES} jcapimin.c jchuff.c jcicc.c jcinit.c\n    jclhuff.c jcmarker.c jcmaster.c jcomapi.c jcparam.c jcphuff.c jctrans.c\n    jdapimin.c jdatadst.c jdatasrc.c jdhuff.c jdicc.c jdinput.c jdlhuff.c\n    jdmarker.c jdmaster.c jdphuff.c jdtrans.c jerror.c jfdctflt.c jmemmgr.c\n    jmemnobs.c jpeg_nbits.c)\n\nif(WITH_ARITH_ENC OR WITH_ARITH_DEC)\n  set(JPEG_SOURCES ${JPEG_SOURCES} jaricom.c)\nendif()\n\nif(WITH_ARITH_ENC)\n  set(JPEG_SOURCES ${JPEG_SOURCES} jcarith.c)\nendif()\n\nif(WITH_ARITH_DEC)\n  set(JPEG_SOURCES ${JPEG_SOURCES} jdarith.c)\nendif()\n\nif(CMAKE_COMPILER_IS_GNUCC OR CMAKE_C_COMPILER_ID MATCHES \"Clang\")\n  # Use the maximum optimization level for release builds\n  foreach(var CMAKE_C_FLAGS_RELEASE CMAKE_C_FLAGS_RELWITHDEBINFO)\n    if(${var} MATCHES \"-O2\")\n      string(REGEX REPLACE \"-O2\" \"-O3\" ${var} \"${${var}}\")\n    endif()\n  endforeach()\nendif()\n\nif(CMAKE_SYSTEM_NAME STREQUAL \"SunOS\")\n  if(CMAKE_C_COMPILER_ID MATCHES \"SunPro\")\n    # Use the maximum optimization level for release builds\n    foreach(var CMAKE_C_FLAGS_RELEASE CMAKE_C_FLAGS_RELWITHDEBINFO)\n      if(${var} MATCHES \"-xO3\")\n        string(REGEX REPLACE \"-xO3\" \"-xO5\" ${var} \"${${var}}\")\n      endif()\n      if(${var} MATCHES \"-xO2\")\n        string(REGEX REPLACE \"-xO2\" \"-xO5\" ${var} \"${${var}}\")\n      endif()\n    endforeach()\n  endif()\nendif()\n\ninclude(CheckTypeSize)\ncheck_type_size(\"size_t\" SIZE_T)\ncheck_type_size(\"unsigned long\" UNSIGNED_LONG)\n\nif(ENABLE_LIBJPEG_TURBO_SIMD)\n  add_subdirectory(simd)\n  if(NEON_INTRINSICS)\n    add_definitions(-DNEON_INTRINSICS)\n  endif()\nelse()\n  set(WITH_SIMD 0)\nendif()\n\nif(WITH_SIMD)\n  message(STATUS \"SIMD extensions: ${CPU_TYPE} (WITH_SIMD = ${WITH_SIMD})\")\n  set(HAVE_LIBJPEG_TURBO_SIMD 1 PARENT_SCOPE)\n  if(MSVC_IDE OR XCODE)\n    set_source_files_properties(${SIMD_OBJS} PROPERTIES GENERATED 1)\n  endif()\n  set(SIMD_TARGET_OBJECTS $<TARGET_OBJECTS:simd>)\nendif()\n\nconfigure_file(jversion.h.in jversion.h)\nconfigure_file(jconfig.h.in jconfig.h)\nconfigure_file(jconfigint.h.in jconfigint.h)\n\nocv_list_add_prefix(JPEG16_SOURCES src/)\nocv_list_add_prefix(JPEG12_SOURCES src/)\nocv_list_add_prefix(JPEG_SOURCES src/)\n\nset(JPEG_SOURCES ${JPEG_SOURCES} ${SIMD_OBJS})\n\nadd_library(jpeg12-static OBJECT ${JPEG12_SOURCES})\nset_property(TARGET jpeg12-static PROPERTY COMPILE_FLAGS\n  \"-DBITS_IN_JSAMPLE=12\")\nadd_library(jpeg16-static OBJECT ${JPEG16_SOURCES})\nset_property(TARGET jpeg16-static PROPERTY COMPILE_FLAGS\n  \"-DBITS_IN_JSAMPLE=16\")\nadd_library(${JPEG_LIBRARY} STATIC ${JPEG_SOURCES} ${SIMD_TARGET_OBJECTS}\n  ${SIMD_OBJS} $<TARGET_OBJECTS:jpeg12-static>\n  $<TARGET_OBJECTS:jpeg16-static>)\n\nset_target_properties(${JPEG_LIBRARY}\n  PROPERTIES OUTPUT_NAME ${JPEG_LIBRARY}\n  DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n  COMPILE_PDB_NAME ${JPEG_LIBRARY}\n  COMPILE_PDB_NAME_DEBUG \"${JPEG_LIBRARY}${OPENCV_DEBUG_POSTFIX}\"\n  ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n  )\n\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(${JPEG_LIBRARY} PROPERTIES FOLDER \"3rdparty/jpeg\")\n  set_target_properties(jpeg12-static PROPERTIES FOLDER \"3rdparty/jpeg\")\n  set_target_properties(jpeg16-static PROPERTIES FOLDER \"3rdparty/jpeg\")\nendif()\n\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(${JPEG_LIBRARY} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev OPTIONAL)\nendif()\n\nocv_install_3rdparty_licenses(libjpeg-turbo README.md LICENSE.md README.ijg)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring zlib-ng Build with SystemZ Hardware Acceleration (Shell)\nDESCRIPTION: These commands show how to configure and build zlib-ng with SystemZ deflate hardware acceleration support using either configure or cmake.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/arch/s390/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ ./configure --with-dfltcc-deflate --with-dfltcc-inflate\n$ make\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ cmake -DWITH_DFLTCC_DEFLATE=1 -DWITH_DFLTCC_INFLATE=1 .\n$ make\n```\n\n----------------------------------------\n\nTITLE: Defining SBT Project Build Settings (Scala)\nDESCRIPTION: This Scala code defines the basic build settings for the SBT project in a file named `project/build.scala`. It sets the Scala version, defines common Scala compiler options (`scalacOptions`), aggregates default settings, and defines the root project named 'JavaSample' located in the current directory (`.`) with the combined settings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\nimport sbt._\nimport Keys._\n\nobject JavaSampleBuild extends Build {\n  def scalaSettings = Seq(\n    scalaVersion := \"2.10.0\",\n    scalacOptions ++= Seq(\n      \"-optimize\",\n      \"-unchecked\",\n      \"-deprecation\"\n    )\n  )\n\n  def buildSettings =\n    Project.defaultSettings ++\n    scalaSettings\n\n  lazy val root = {\n    val settings = buildSettings ++ Seq(name := \"JavaSample\")\n    Project(id = \"JavaSample\", base = file(\".\"), settings = settings)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Copying Pure Java Test Files in CMake for OpenCV\nDESCRIPTION: Gathers and copies specific files for pure Java tests. It uses file(GLOB_RECURSE) to find test files and add_custom_command to copy each file to the test directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/pure_test/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE test_files RELATIVE \"${test_dir}\" \"${test_dir}/src/*\")\nfile(GLOB_RECURSE test_lib_files RELATIVE \"${test_dir}\" \"${test_dir}/lib/*.jar\")\nforeach(f ${test_files} ${test_lib_files})\n  add_custom_command(OUTPUT \"${OPENCV_JAVA_TEST_DIR}/${f}\"\n      COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${test_dir}/${f}\" \"${OPENCV_JAVA_TEST_DIR}/${f}\"\n      DEPENDS \"${test_dir}/${f}\"\n      COMMENT \"Copying ${f}\"\n  )\n  list(APPEND depends \"${test_dir}/${f}\" \"${OPENCV_JAVA_TEST_DIR}/${f}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Setting up MobileNet SSD Model Downloads in CMake\nDESCRIPTION: Downloads the MobileNet SSD caffemodel weights file and prototxt configuration from specified URLs. Includes hash verification and multiple fallback URLs. Files are saved to the res/raw directory of the project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/mobilenet-objdetect/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(sample example-mobilenet-objdetect)\n\nocv_download(FILENAME \"mobilenet_iter_73000.caffemodel\"\n             HASH \"bbcb3b6a0afe1ec89e1288096b5b8c66\"\n             URL\n               \"${OPENCV_MOBILENET_SSD_WEIGHTS_URL}\"\n               \"$ENV{OPENCV_MOBILENET_SSD_WEIGHTS_URL}\"\n               \"https://raw.githubusercontent.com/chuanqi305/MobileNet-SSD/97406996b1eee2d40eb0a00ae567cf41e23369f9/mobilenet_iter_73000.caffemodel\"\n             DESTINATION_DIR \"${CMAKE_CURRENT_LIST_DIR}/res/raw\"\n             ID OPENCV_MOBILENET_SSD_WEIGHTS\n             STATUS res)\n\nocv_download(FILENAME \"deploy.prototxt\"\n             HASH \"f1978dc4fe20c680e850ce99830c5945\"\n             URL\n               \"${OPENCV_MOBILENET_SSD_CONFIG_URL}\"\n               \"$ENV{OPENCV_MOBILENET_SSD_CONFIG_URL}\"\n               \"https://raw.githubusercontent.com/chuanqi305/MobileNet-SSD/97406996b1eee2d40eb0a00ae567cf41e23369f9/deploy.prototxt\"\n             DESTINATION_DIR \"${CMAKE_CURRENT_LIST_DIR}/res/raw\"\n             ID OPENCV_MOBILENET_SSD_CONFIG\n             STATUS res)\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV Unit Tests on RISC-V QEMU\nDESCRIPTION: This command runs the OpenCV unit tests in the RISC-V build directory using QEMU user mode emulation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/arch/riscv/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd ./build-riscv && ctest --verbose\n```\n\n----------------------------------------\n\nTITLE: Suppressing Warnings in Autogenerated Protocol Buffer Files - CMake\nDESCRIPTION: Disables a wide range of compiler warnings, particularly for compiler-generated files such as 'caffe.pb.*'. This is to prevent build interruption from non-critical warnings when building code generated by Protobuf or similar tools. No parameters or outputs beyond altering compiler warning options in the build environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nocv_warnings_disable(CMAKE_CXX_FLAGS\n    /wd4125 /wd4267 /wd4127 /wd4244 /wd4512 /wd4702\n    /wd4456 /wd4510 /wd4610 /wd4800\n    /wd4701 /wd4703                    # potentially uninitialized local/pointer variable 'value' used\n    /wd4505                            # unreferenced local function has been removed\n    /wd4458                            # declaration of 'x' hides class member. GCC still works, MSVC bug is here: https://developercommunity.visualstudio.com/content/problem/219311/c-c4458-declaration-hides-class-member-warning-iss.html\n    -wd858 -wd2196\n    -Winvalid-offsetof                 # Apple Clang (attr_value.pb.cc)\n)\n```\n\n----------------------------------------\n\nTITLE: Single OpenCV World Library\nDESCRIPTION: Shows how to link against a single OpenCV world library instead of individual module libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_visual_studio_opencv/windows_visual_studio_opencv.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nopencv_world330.lib\n```\n\n----------------------------------------\n\nTITLE: Location of Newly Built/Downloaded FastCV Libraries (Path)\nDESCRIPTION: This path indicates the directory within the OpenCV build folder (`build/3rdparty/fastcv/libs`) where the FastCV libraries, potentially downloaded or updated during the CMake configuration or build process, are placed. These libraries should be used to replace any older versions residing in the eSDK sysroot path if an update is required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_fastcv/building_fastcv.markdown#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nbuild\\3rdparty\\fastcv\\libs\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenCV Sample Project with CMake\nDESCRIPTION: This CMake script sets up a sample project named 'norm' within the OpenCV framework. It checks for necessary dependencies such as opencv_core and includes OpenCV modules using ocv utility functions. The script defines the sample, resolves include directories, and links the required libraries for successful compilation. The setup assumes prior configuration of OpenCV in the environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/semihosting/norm/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(PROJECT_NAME norm)\nproject(${PROJECT_NAME})\n\nocv_install_example_src(norm *.cpp *.hpp CMakeLists.txt)\n\nset(LOCAL_DEPS\n  opencv_core\n  ${OPENCV_MODULES_PUBLIC}\n  ${OpenCV_LIB_COMPONENTS})\nocv_check_dependencies(${LOCAL_DEPS})\n\nif(NOT OCV_DEPENDENCIES_FOUND)\n  return()\nendif()\n\nocv_define_sample(norm norm.cpp ${SEMIHOSTING_SUFFIX})\nocv_include_modules_recurse(${LOCAL_DEPS})\ntarget_include_directories(${norm} PRIVATE ${CMAKE_CURRENT_BINARY_DIR})\ntarget_include_directories(${norm} PRIVATE ${RAW_PIXEL_INCLUDE})\nocv_target_link_libraries(${norm} PRIVATE ${OPENCV_LINKER_LIBS}\n  ${LOCAL_DEPS})\n```\n\n----------------------------------------\n\nTITLE: Detailed Image Stitching with Configurable Parameters Python\nDESCRIPTION: This Python script offers customizable image stitching by adjusting parameters such as the feature type, matcher, and warp strategies. It is powered by OpenCV and is configured through command-line arguments to tailor the stitching process. Inputs involve image files, and it outputs a comprehensive panorama.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/stitcher.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\nimport sys\n\nimg_names = ['boat1.jpg', 'boat2.jpg', ...]\nimgs = [cv2.imread(name) for name in img_names]\nstitcher = cv2.createStitcher(False)\nstitcher.setFeaturesFinder(cv2.ORB_create())\nstitched, result = stitcher.stitch(imgs)\nif stitched == cv2.Stitcher_OK:\n    cv2.imwrite('result_detailed_python.jpg', result)\n```\n\n----------------------------------------\n\nTITLE: OpenCV Application Creation Function\nDESCRIPTION: Defines a function for creating OpenCV applications with proper linking, dependencies, and installation settings. Handles module inclusion, target properties, and installation paths.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ocv_add_application the_target)\n  cmake_parse_arguments(APP \"\" \"\" \"MODULES;SRCS\" ${ARGN})\n  ocv_check_dependencies(${APP_MODULES})\n  if(NOT OCV_DEPENDENCIES_FOUND)\n     return()\n  endif()\n\n  project(${the_target})\n  ocv_target_include_modules_recurse(${the_target} ${APP_MODULES})\n  ocv_target_include_directories(${the_target} PRIVATE \"${OpenCV_SOURCE_DIR}/include/opencv\")\n  ocv_add_executable(${the_target} ${APP_SRCS})\n  ocv_target_link_libraries(${the_target} ${APP_MODULES})\n  set_target_properties(${the_target} PROPERTIES\n                        DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n                        ARCHIVE_OUTPUT_DIRECTORY ${LIBRARY_OUTPUT_PATH}\n                        RUNTIME_OUTPUT_DIRECTORY ${EXECUTABLE_OUTPUT_PATH}\n                        OUTPUT_NAME \"${the_target}\")\n\n  if(ENABLE_SOLUTION_FOLDERS)\n    set_target_properties(${the_target} PROPERTIES FOLDER \"applications\")\n  endif()\n\n  if(NOT INSTALL_CREATE_DISTRIB\n      OR (OPENCV_INSTALL_APPS_LIST STREQUAL \"all\" OR \";${OPENCV_INSTALL_APPS_LIST};\" MATCHES \";${the_target};\")\n  )\n    install(TARGETS ${the_target} RUNTIME DESTINATION ${OPENCV_BIN_INSTALL_PATH} COMPONENT dev)\n  elseif(INSTALL_CREATE_DISTRIB)\n    if(BUILD_SHARED_LIBS)\n      install(TARGETS ${the_target} RUNTIME DESTINATION ${OPENCV_BIN_INSTALL_PATH} CONFIGURATIONS Release COMPONENT dev)\n    endif()\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Handling eSDK Environment Misconfiguration Warning (Shell Output)\nDESCRIPTION: This text shows a potential warning message displayed when sourcing the eSDK environment setup script. It indicates that the `LD_LIBRARY_PATH` environment variable is currently set, which might interfere with the correct operation of the SDK's tools and libraries. The recommended action is to unset this variable using `unset LD_LIBRARY_PATH` after verifying it's safe to do so within the user's environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_fastcv/building_fastcv.markdown#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nYour environment is misconfigured, you probably need to 'unset LD_LIBRARY_PATH'\nbut please check why this was set in the first place and that it's safe to unset.\nThe SDK will not operate correctly in most cases when LD_LIBRARY_PATH is set.\n```\n\n----------------------------------------\n\nTITLE: Setting JPEG Data Precision in libjpeg (C)\nDESCRIPTION: A C integer field within the compression parameters structure (`cinfo`). This parameter specifies the number of bits per sample (valid range 2 to 16) for the JPEG output. Specific functions (`jpeg12_write_scanlines`, `jpeg16_write_scanlines`) are required for 9-12 bit and 13-16 bit precision respectively. Data precisions other than 8-bit or 12-bit necessitate the use of lossless compression mode.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_31\n\nLANGUAGE: C\nCODE:\n```\nint data_precision\n```\n\n----------------------------------------\n\nTITLE: Displaying Python Integration Status in OpenCV Build\nDESCRIPTION: Checks and displays information about Python 2 and Python 3 integration for OpenCV, including interpreter and library versions, NumPy support, and installation paths.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_28\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_opencv_python2)\n  status(\"\")\n  status(\"  Python 2:\")\n  status(\"    Interpreter:\"     PYTHON2INTERP_FOUND  THEN \"${PYTHON2_EXECUTABLE} (ver ${PYTHON2_VERSION_STRING})\"       ELSE NO)\n  if(PYTHON2LIBS_VERSION_STRING)\n    status(\"    Libraries:\"   HAVE_opencv_python2  THEN  \"${PYTHON2_LIBRARIES} (ver ${PYTHON2LIBS_VERSION_STRING})\"   ELSE NO)\n  else()\n    status(\"    Libraries:\"   HAVE_opencv_python2  THEN  \"${PYTHON2_LIBRARIES}\"                                      ELSE NO)\n  endif()\n  status(\"    numpy:\"         PYTHON2_NUMPY_INCLUDE_DIRS THEN \"${PYTHON2_NUMPY_INCLUDE_DIRS} (ver ${PYTHON2_NUMPY_VERSION})\" ELSE \"NO (Python wrappers can not be generated)\")\n  status(\"    install path:\"  HAVE_opencv_python2  THEN \"${__INSTALL_PATH_PYTHON2}\"                            ELSE \"-\")\nendif()\n\nif(BUILD_opencv_python3)\n  status(\"\")\n  status(\"  Python 3:\")\n  status(\"    Interpreter:\"     PYTHON3INTERP_FOUND  THEN \"${PYTHON3_EXECUTABLE} (ver ${PYTHON3_VERSION_STRING})\"       ELSE NO)\n  if(PYTHON3LIBS_VERSION_STRING)\n    status(\"    Libraries:\"   HAVE_opencv_python3  THEN  \"${PYTHON3_LIBRARIES} (ver ${PYTHON3LIBS_VERSION_STRING})\"   ELSE NO)\n  else()\n    status(\"    Libraries:\"   HAVE_opencv_python3  THEN  \"${PYTHON3_LIBRARIES}\"                                      ELSE NO)\n  endif()\n  status(\"    Limited API:\" PYTHON3_LIMITED_API THEN \"YES (ver ${PYTHON3_LIMITED_API_VERSION})\"                    ELSE NO)\n  status(\"    numpy:\"         PYTHON3_NUMPY_INCLUDE_DIRS THEN \"${PYTHON3_NUMPY_INCLUDE_DIRS} (ver ${PYTHON3_NUMPY_VERSION})\" ELSE \"NO (Python3 wrappers can not be generated)\")\n  status(\"    install path:\"  HAVE_opencv_python3  THEN \"${__INSTALL_PATH_PYTHON3}\"                            ELSE \"-\")\nendif()\n\nstatus(\"\")\nstatus(\"  Python (for build):\"  PYTHON_DEFAULT_AVAILABLE THEN \"${PYTHON_DEFAULT_EXECUTABLE}\" ELSE NO)\n```\n\n----------------------------------------\n\nTITLE: Command Line Navigation and Execution in Windows\nDESCRIPTION: Example showing how to navigate directories and execute an OpenCV program with command line arguments in Windows Command Prompt.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_visual_studio_opencv/windows_visual_studio_opencv.markdown#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nD:\\nCD OpenCV\\MySolutionName\\Release\\nMySolutionName.exe exampleImage.jpg\n```\n\n----------------------------------------\n\nTITLE: Configuring Installation and Solution Settings\nDESCRIPTION: Sets up installation targets for static builds and configures solution folder organization when enabled.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ipphal/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(ipphal EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev)\nendif()\n\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(ipphal PROPERTIES FOLDER \"3rdparty\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV CMake Build for NVIDIA Jetson TX1\nDESCRIPTION: CMake configuration options for building OpenCV with CUDA support on the NVIDIA Jetson TX1 platform. Uses CUDA 8.0 and disables precompiled headers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n$ cmake \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=/usr \\\n    -DBUILD_PNG=OFF \\\n    -DBUILD_TIFF=OFF \\\n    -DBUILD_TBB=OFF \\\n    -DBUILD_JPEG=OFF \\\n    -DBUILD_JASPER=OFF \\\n    -DBUILD_ZLIB=OFF \\\n    -DBUILD_EXAMPLES=ON \\\n    -DBUILD_JAVA=OFF \\\n    -DBUILD_opencv_python2=ON \\\n    -DBUILD_opencv_python3=OFF \\\n    -DENABLE_PRECOMPILED_HEADERS=OFF \\\n    -DWITH_OPENCL=OFF \\\n    -DWITH_OPENMP=OFF \\\n    -DWITH_FFMPEG=ON \\\n    -DWITH_GSTREAMER=OFF \\\n    -DWITH_GSTREAMER_0_10=OFF \\\n    -DWITH_CUDA=ON \\\n    -DWITH_GTK=ON \\\n    -DWITH_VTK=OFF \\\n    -DWITH_TBB=ON \\\n    -DWITH_1394=OFF \\\n    -DWITH_OPENEXR=OFF \\\n    -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-8.0 \\\n    -DCUDA_ARCH_BIN=5.3 \\\n    -DCUDA_ARCH_PTX=\"\" \\\n    -DINSTALL_C_EXAMPLES=ON \\\n    -DINSTALL_TESTS=OFF \\\n    -DOPENCV_TEST_DATA_PATH=../opencv_extra/testdata \\\n    ../opencv\n```\n\n----------------------------------------\n\nTITLE: Using a Kernel Wrapper Function in a Pipeline (OpenCV G-API, C++)\nDESCRIPTION: Applies the C++ kernel wrapper function inside a graph construction to simplify kernel use with optional or default arguments. This enhances readability and usability by hiding verbose kernel interface calls. Relies on previous definition of the wrapper function and G-API context.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/doc/20-kernel-api.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// Usage of kernel wrapper function for concise graph construction\nauto out = myFilter2D(in, kernel);\n// 'anchor' argument uses default value as defined in the wrapper\n```\n\n----------------------------------------\n\nTITLE: Displaying Results with OpenCV imshow in Java\nDESCRIPTION: This Java snippet provides a sample placeholder for displaying images using Java Swing or a helper class, as OpenCV Java does not have a direct imshow function. It typically requires a helper function to convert and show Mat images. Outputs are shown in individual windows.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\n// Display the images (implementation for showing Mats in Java required)\\nHighGui.imshow(\"Source\", src);\\nHighGui.imshow(\"Detected Lines (in red) - Standard Hough Line Transform\", color_dst);\\nHighGui.imshow(\"Detected Lines (in green) - Probabilistic Line Transform\", color_dstP);\\nHighGui.waitKey();\\n\n```\n\n----------------------------------------\n\nTITLE: Drawing 1-D Hue Histogram in Java with OpenCV\nDESCRIPTION: This snippet demonstrates how to draw a 1-D Hue histogram of an image using OpenCV in Java. It creates a histogram image and uses the line function to draw the histogram bars.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/back_projection/back_projection.markdown#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nint w = 400, h = 400;\nint bin_w = (int) Math.round((double) w / histSize);\nMat histImg = Mat.zeros(h, w, CvType.CV_8UC3);\nfor (int i = 0; i < histSize; i++) {\n    Imgproc.line(histImg, new Point(bin_w * (i), h),\n            new Point(bin_w * (i), h - Math.round(hist.get(i, 0)[0] * h / 255.0)),\n            new Scalar(0, 0, 255), 2, 8, 0);\n}\nHighGui.imshow(\"Histogram\", histImg);\n```\n\n----------------------------------------\n\nTITLE: Default Preprocessing Configuration for PyTorch Input Blob\nDESCRIPTION: This Python dictionary defines default parameters for preprocessing images before being passed to a PyTorch input blob, including mean, scale, and standard deviation values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npytorch_segm_input_blob = {\n    \"mean\": [\"123.675\", \"116.28\", \"103.53\"],\n    \"scale\": str(1 / 255.0),\n    \"std\": [\"0.229\", \"0.224\", \"0.225\"],\n    \"crop\": \"False\",\n    \"rgb\": \"True\"\n}\n```\n\n----------------------------------------\n\nTITLE: Multiple Data Stream Capture with OpenNI\nDESCRIPTION: Example demonstrating how to capture both depth map and BGR image simultaneously using grab() and retrieve() methods.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/kinect_openni.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nVideoCapture capture(0); // or CAP_OPENNI\nfor(;;)\n{\n    Mat depthMap;\n    Mat bgrImage;\n\n    capture.grab();\n\n    capture.retrieve( depthMap, CAP_OPENNI_DEPTH_MAP );\n    capture.retrieve( bgrImage, CAP_OPENNI_BGR_IMAGE );\n\n    if( waitKey( 30 ) >= 0 )\n        break;\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Data into SIMD Registers using OpenCV Intrinsics (C++)\nDESCRIPTION: This collection of snippets demonstrates how to load floating point data into variable-sized and constant-sized SIMD registers using OpenCV intrinsics. It covers construction from pointers, direct argument initialization (for constant registers), and using load functions (vx_load, v_load, v256_load, v512_load). The example assumes 'float ptr[32]', and illustrates how slicing the pointer inputs loads contiguous segments respecting the register's native lane width. Correct pointer alignment may require using vx_load_aligned.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/univ_intrin/univ_intrin.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nfloat ptr[32] = {1, 2, 3 ..., 32};   // ptr is a pointer to a contiguous memory block of 32 floats\n\n// Variable Sized Registers //\nint x = v_float32().nlanes;          // set x as the number of values the register can hold\n\nv_float32 reg1(ptr);                 // reg1 stores first x values according to the maximum register size available.\nv_float32 reg2(ptr + x);             // reg stores the next x values\n\n// Constant Sized Registers //\nv_float32x4 reg1(ptr);               // reg1 stores the first 4 floats (1, 2, 3, 4)\nv_float32x4 reg2(ptr + 4);           // reg2 stores the next 4 floats (5, 6, 7, 8)\n\n// Or we can explicitly write down the values.\nv_float32x4(1, 2, 3, 4);\n```\n\nLANGUAGE: cpp\nCODE:\n```\nfloat ptr[32] = {1, 2, 3, ..., 32};\nv_float32 reg_var;\nreg_var = vx_load(ptr);              // loads values from ptr[0] upto ptr[reg_var.nlanes - 1]\n\nv_float32x4 reg_128;\nreg_128 = v_load(ptr);               // loads values from ptr[0] upto ptr[3]\n\nv_float32x8 reg_256;\nreg_256 = v256_load(ptr);            // loads values from ptr[0] upto ptr[7]\n\nv_float32x16 reg_512;\nreg_512 = v512_load(ptr);            // loads values from ptr[0] upto ptr[15]\n```\n\n----------------------------------------\n\nTITLE: Executing DNN Conversion Script for PyTorch FCN ResNet-50\nDESCRIPTION: This command executes a Python script located within the `dnn_model_runner` module. The script `py_to_py_fcnresnet50.py` presumably handles the conversion of a PyTorch FCN ResNet-50 segmentation model to ONNX format and potentially runs inference or comparison.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.pytorch.segmentation.py_to_py_fcnresnet50\n```\n\n----------------------------------------\n\nTITLE: Displaying Java Build Status in CMake\nDESCRIPTION: This CMake snippet checks if Java support (`BUILD_JAVA`) is enabled. If so, it uses the `status` command to print configuration details about the Java environment (ANT, Java version, JNI headers), Java wrappers, and Java tests, depending on the values of various CMake variables set earlier in the build process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_29\n\nLANGUAGE: cmake\nCODE:\n```\n# ========================== java ==========================\nif(BUILD_JAVA)\n  status(\"\")\n  status(\"  Java:\"            BUILD_FAT_JAVA_LIB  THEN \"export all functions\"                                      ELSE \"\")\n  status(\"    ant:\"           ANT_EXECUTABLE      THEN \"${ANT_EXECUTABLE} (ver ${ANT_VERSION})\"                    ELSE NO)\n  if(NOT ANDROID)\n    status(\"    Java:\"        Java_FOUND     THEN \"YES (ver ${Java_VERSION})\"                                      ELSE NO)\n    status(\"    JNI:\"         JNI_INCLUDE_DIRS    THEN \"${JNI_INCLUDE_DIRS}\"                                       ELSE NO)\n  endif()\n  status(\"    Java wrappers:\" HAVE_opencv_java                                                            THEN \"YES (${OPENCV_JAVA_SDK_BUILD_TYPE})\" ELSE NO)\n  status(\"    Java tests:\"    BUILD_TESTS AND (opencv_test_java_BINARY_DIR OR opencv_test_android_BINARY_DIR) THEN YES ELSE NO)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating a CLAHE Object in JavaScript\nDESCRIPTION: Shows how to create an instance of the `cv.CLAHE` class for Contrast Limited Adaptive Histogram Equalization in OpenCV.js. CLAHE works by dividing the image into small tiles (defined by `tileGridSize`, default 8x8) and applying histogram equalization to each tile, limiting contrast amplification using `clipLimit` (default 40) to reduce noise. Bilinear interpolation is used to smooth tile borders. The created CLAHE object needs to be deleted after use.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_histograms/js_histogram_equalization/js_histogram_equalization.markdown#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ncv.CLAHE (clipLimit = 40, tileGridSize = new cv.Size(8, 8))\n```\n\n----------------------------------------\n\nTITLE: Displaying a Simple Alert View in an iOS ViewController\nDESCRIPTION: This Objective-C code snippet, intended for the `viewDidLoad` method within a `ViewController.m` (or `.mm`) file, creates and displays a basic `UIAlertView`. It initializes the alert with a title 'Hello!', a message 'Welcome to OpenCV', sets the current view controller as the delegate, and provides a 'Continue' button. Calling `[alert show]` presents the alert to the user, confirming the application setup.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/hello/hello.markdown#2025-04-22_snippet_1\n\nLANGUAGE: m\nCODE:\n```\nUIAlertView * alert = [[UIAlertView alloc] initWithTitle:@\"Hello!\" message:@\"Welcome to OpenCV\" delegate:self cancelButtonTitle:@\"Continue\" otherButtonTitles:nil];\n[alert show];\n```\n\n----------------------------------------\n\nTITLE: Check for Large File Support and System Functions in CMake\nDESCRIPTION: This block sets definitions for large file support and checks for optional system functions like fseeko and strerror, adding necessary macros if functions are unavailable. It highlights compiler feature checks using the check_function_exists command.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# Check to see if we have large file support\n#\nset(CMAKE_REQUIRED_DEFINITIONS -D_LARGEFILE64_SOURCE=1 -D__USE_LARGEFILE64)\ncheck_type_size(off64_t OFF64_T)\nif(HAVE_OFF64_T)\n    add_definitions(-D_LARGEFILE64_SOURCE=1 -D__USE_LARGEFILE64)\nelse()\n    check_type_size(_off64_t _OFF64_T)\n    if(HAVE__OFF64_T)\n        add_definitions(-D_LARGEFILE64_SOURCE=1 -D__USE_LARGEFILE64)\n    else()\n        check_type_size(__off64_t __OFF64_T)\n    endif()\nendif()\nset(CMAKE_REQUIRED_DEFINITIONS) # clear variable\n\n#\n# Check for fseeko and other optional functions\n#\ncheck_function_exists(fseeko HAVE_FSEEKO)\nif(NOT HAVE_FSEEKO)\n    add_definitions(-DNO_FSEEKO)\nendif()\n\ncheck_function_exists(strerror HAVE_STRERROR)\nif(NOT HAVE_STRERROR)\n    add_definitions(-DNO_STRERROR)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Structuring Elements - OpenCV.js - JavaScript\nDESCRIPTION: This snippet demonstrates the use of cv.getStructuringElement() in OpenCV.js for generating structuring elements (kernels) of defined shapes and sizes, such as rectangular, elliptical, or cross-shaped. Parameters include shape (from cv.MorphShapes), ksize (size as [width, height]), and anchor (defaults to center). The return value is a cv.Mat kernel usable in other morphological operations. Only cross-shaped elements depend on anchor values; for other shapes the anchor adjusts outcome alignment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_morphological_ops/js_morphological_ops.markdown#2025-04-22_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.getStructuringElement(shape, ksize, anchor = new cv.Point(-1, -1))\n```\n\n----------------------------------------\n\nTITLE: Sample Employer Copyright Disclaimer\nDESCRIPTION: This is a sample text for a copyright disclaimer that an employer or educational institution might sign. It explicitly disclaims copyright interest in a specific program written by an employee or student, allowing the actual author to license it freely (e.g., under the GPL). Placeholders for the company name, program name, author name, signatory details, and date need to be replaced.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ittnotify/src/ittnotify/GPL-2.0-only.txt#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nYoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker.\n```\n\n----------------------------------------\n\nTITLE: Setting Android Linker Flags for 16k Page Size Support in CMake\nDESCRIPTION: Checks if `ANDROID_SUPPORT_FLEXIBLE_PAGE_SIZES` is enabled. If so, and if the target `ANDROID_ABI` is either `arm64-v8a` or `x86_64`, it appends the `-Wl,-z,max-page-size=16384` flag to `CMAKE_SHARED_LINKER_FLAGS`. This ensures compatibility with 16k page sizes on relevant architectures, particularly with older NDKs (prior to 27).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-2-mixedprocessing/jni/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n# For 16k pages support with NDK prior 27\n# Details: https://developer.android.com/guide/practices/page-sizes?hl=en\nif(ANDROID_SUPPORT_FLEXIBLE_PAGE_SIZES)\n  if(ANDROID_ABI STREQUAL arm64-v8a OR ANDROID_ABI STREQUAL x86_64)\n    set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -Wl,-z,max-page-size=16384\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Displaying Vulkan, WebNN, and Tim-VX Integration Status in OpenCV Build\nDESCRIPTION: Checks and displays the status of Vulkan, WebNN, and Tim-VX integrations, showing include paths and library information when available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_26\n\nLANGUAGE: cmake\nCODE:\n```\nif(WITH_VULKAN OR HAVE_VULKAN)\n  status(\"\")\n  status(\"  Vulkan:\"     HAVE_VULKAN THEN \"YES\" ELSE \"NO\")\n  if(HAVE_VULKAN)\n    status(\"    Include path:\"  VULKAN_INCLUDE_DIRS THEN \"${VULKAN_INCLUDE_DIRS}\" ELSE \"NO\")\n    status(\"    Link libraries:\" VULKAN_LIBRARIES THEN \"${VULKAN_LIBRARIES}\" ELSE \"Dynamic load\")\n  endif()\nendif()\n\nif(WITH_WEBNN OR HAVE_WEBNN)\n  status(\"\")\n  status(\"  WebNN:\"     HAVE_WEBNN THEN \"YES\" ELSE \"NO\")\n  if(HAVE_WEBNN AND NOT EMSCRIPTEN)\n    status(\"    Include path:\"  WEBNN_HEADER_DIRS THEN \"${WEBNN_HEADER_DIRS}\" ELSE \"NO\")\n    status(\"    Link libraries:\" WEBNN_LIBRARIES THEN \"${WEBNN_LIBRARIES}\" ELSE \"NO\")\n  endif()\nendif()\n\nif(WITH_TIMVX)\n  status(\"\")\n  status(\"  Tim-VX:\"     HAVE_TIMVX THEN \"YES\" ELSE \"NO\")\n  if(HAVE_TIMVX)\n    status(\"    Include path\"  TIMVX_INCLUDE_DIR THEN \"${TIMVX_INCLUDE_DIR}\" ELSE \"NO\")\n    status(\"    Link libraries:\" TIMVX_LIBRARY THEN \"${TIMVX_LIBRARY}\" ELSE \"NO\")\n    status(\"    VIVANTE SDK path\" VIVANTE_SDK_DIR THEN \"${VIVANTE_SDK_DIR}\" ELSE \"NO\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Using a Kernel Interface in a Pipeline with ::on() (OpenCV G-API, C++)\nDESCRIPTION: Demonstrates instantiating a kernel node in a computational graph using the kernel's ::on() method, matching the signature defined in the kernel interface. This allows the kernel interface to be used directly in graph construction, ensuring parameters conform to the declared interface. Requires kernel type to be defined previously via G_TYPED_KERNEL, and all G-API context prerequisites to be met.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/doc/20-kernel-api.markdown#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// Example of using the kernel interface in a pipeline\nauto out = Filter2D::on(in, kernel, anchor);\n// 'in'      : input cv::GMat node\n// 'kernel'  : input cv::Mat parameter for kernel\n// 'anchor'  : input cv::Point for anchor location\n// Returns a cv::GMat node as declared\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV.js with Threads Optimization - Bash\nDESCRIPTION: This bash command builds OpenCV.js with WebAssembly and threads optimization enabled using Emscripten's emcmake, Python, and the OpenCV JS build script. Requires Emscripten toolchain and Python installed. The '--threads' option activates threading support; number of threads defaults to the device's logical core count. Outputs 'opencv.js' with threading support for use in browsers that enable WebAssembly threads. This optimization is not supported in Node.js environments.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nemcmake python ./opencv/platforms/js/build_js.py build_js --build_wasm --threads\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for libwebp in CMake\nDESCRIPTION: This snippet sets up the project and includes necessary directories depending on the target platform. It utilizes CMake functions to gather all source and header files relevant to libwebp. It handles conditional compilation for ARM architectures without NEON support and sets source file properties accordingly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libwebp/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(${WEBP_LIBRARY})\n\nocv_include_directories(${CMAKE_CURRENT_SOURCE_DIR})\nif(ANDROID)\n  ocv_include_directories(${CPUFEATURES_INCLUDE_DIRS})\nendif()\n\nfile(GLOB lib_srcs sharpyuv/*.c src/dec/*.c src/demux/*.c src/dsp/*.c src/enc/*.c src/mux/*.c src/utils/*.c src/webp/*.c)\nfile(GLOB lib_hdrs sharpyuv/*.h src/dec/*.h src/demux/*.h src/dsp/*.h src/enc/*.h src/mux/*.h src/utils/*.h src/webp/*.h)\n\n# FIXIT\nif(ANDROID AND ARMEABI_V7A AND NOT NEON)\n  foreach(file ${lib_srcs})\n    if(\"${file}\" MATCHES \"_neon.c\")\n      set_source_files_properties(\"${file}\" COMPILE_FLAGS \"-mfpu=neon\")\n    endif()\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Displaying Various Library Integrations in OpenCV Build\nDESCRIPTION: Checks and displays the status of multiple library integrations including VA, LAPACK, and Halide, showing whether they're enabled and their path information when available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_23\n\nLANGUAGE: cmake\nCODE:\n```\nif(WITH_VA OR HAVE_VA)\n  status(\"    VA:\"            HAVE_VA          THEN \"YES\" ELSE NO)\nendif()\n\nif(WITH_LAPACK OR HAVE_LAPACK)\n  status(\"    Lapack:\"      HAVE_LAPACK     THEN \"YES (${LAPACK_LIBRARIES})\" ELSE NO)\nendif()\n\nif(WITH_HALIDE OR HAVE_HALIDE)\n  status(\"    Halide:\"     HAVE_HALIDE      THEN \"YES (${HALIDE_LIBRARIES} ${HALIDE_INCLUDE_DIRS})\" ELSE NO)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Manual CMake Command Line Build Configuration for Windows Store 8.1 x64\nDESCRIPTION: Provides an example of manually invoking CMake from the command line to configure a build for Windows Store 8.1 x64. It specifies the Visual Studio 2013 Win64 generator, system name, system version, effective platform (x64), and a custom installation prefix path relative to the build directory. Assumes execution within a 'bin' subdirectory of the source.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Visual Studio 12 2013 Win64\" -DCMAKE_SYSTEM_NAME:String=WindowsStore -DCMAKE_SYSTEM_VERSION:String=8.1 -DCMAKE_VS_EFFECTIVE_PLATFORMS:String=x64 -DCMAKE_INSTALL_PREFIX:PATH=.\\install\\WS\\8.1\\x64\\ ..\n```\n\n----------------------------------------\n\nTITLE: Generating OpenCV for Windows Phone 8.0 x86 using CMake\nDESCRIPTION: Invokes CMake directly using the Visual Studio 2013 generator to create project files for OpenCV targeting Windows Phone 8.0 on the x86 architecture. Specifies the system name and older system version (8.0).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Visual Studio 12 2013\" -DCMAKE_SYSTEM_NAME=WindowsPhone -DCMAKE_SYSTEM_VERSION=8.0 <path-to-source>\n```\n\n----------------------------------------\n\nTITLE: Installing libtiff Artifacts with CMake\nDESCRIPTION: This section of the script handles the installation directives for the libtiff library. It installs the library archive to a specified path and manages third-party licenses, ensuring the build output is complete with necessary documentation and binaries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libtiff/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(${TIFF_LIBRARY} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev OPTIONAL)\nendif()\n\nocv_install_3rdparty_licenses(libtiff COPYRIGHT)\n```\n\n----------------------------------------\n\nTITLE: Rebuilding Container Process\nDESCRIPTION: Complete sequence of commands to rebuild the gaplib-actions-runner container, including stopping service, removing old container and image, building new image, and restarting service.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/arch/s390/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Stop actions-runner service\nsudo systemctl stop actions-runner\n\n# Delete old container\nsudo podman container rm gaplib-actions-runner\n\n# Delete old image\nsudo podman image rm localhost/zlib-ng/actions-runner\n\n# Build image\nsudo podman build --squash -f Dockerfile.zlib-ng --tag zlib-ng/actions-runner --build-arg .\n\n# Build container\nsudo podman create --name=gaplib-actions-runner --env-file=/etc/actions-runner --init --interactive --volume=actions-runner-temp:/home/actions-runner zlib-ng/actions-runner\n\n# Start actions-runner service\nsudo systemctl start actions-runner\n```\n\n----------------------------------------\n\nTITLE: Accessing Pixel Rows/Columns with Ptr Methods (ucharPtr) in OpenCV.js - JavaScript\nDESCRIPTION: Uses ucharPtr to get a Uint8Array for a specific pixel's data, making it possible to efficiently extract or work with all channels at a given coordinate. Optimized for random-access to pixel data and generally faster than repeated calls to at-like methods. Usage depends on matrix type and expected in-place read/write operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_basic_ops/js_basic_ops.markdown#2025-04-22_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet row = 3, col = 4;\nlet src = cv.imread(\"canvasInput\");\nlet pixel = src.ucharPtr(row, col);\nlet R = pixel[0];\nlet G = pixel[1];\nlet B = pixel[2];\nlet A = pixel[3];\n```\n\n----------------------------------------\n\nTITLE: Constructing Size Structures in OpenCV.js (JavaScript)\nDESCRIPTION: Details two options for creating a Size structure: with the cv.Size constructor or using an object literal with width and height properties. This defines the dimensions of shapes or images. It requires OpenCV.js and numerical width and height values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_some_data_structures/js_image_arithmetics.markdown#2025-04-22_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n// The first way\nlet size = new cv.Size(width, height);\n// The second way\nlet size = {width : width, height : height};\n```\n\n----------------------------------------\n\nTITLE: Defining UMat Class with Python Bindings in C++\nDESCRIPTION: Demonstrates the definition of OpenCV's UMat class with various Python binding annotations. Includes examples of mappable types (CV_WRAP_MAPPABLE), phantom methods (CV_WRAP_PHANTOM), and method wrapping with default arguments (CV_WRAP_AS, CV_WRAP_DEFAULT).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_bindings/py_bindings_basics/py_bindings_basics.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nclass CV_EXPORTS_W UMat\n{\npublic:\n    //! Mat is mappable to UMat.\n    // You would need to provide `static bool cv_mappable_to(const Ptr<Mat>& src, Ptr<UMat>& dst)`\n    CV_WRAP_MAPPABLE(Ptr<Mat>);\n\n    /! returns the OpenCL queue used by OpenCV UMat.\n    // You would need to provide the method body in the binder code\n    CV_WRAP_PHANTOM(static void* queue());\n\n    //! returns the OpenCL context used by OpenCV UMat\n    // You would need to provide the method body in the binder code\n    CV_WRAP_PHANTOM(static void* context());\n\n    //! The wrapped method become equivalent to `get(int flags = ACCESS_RW)`\n    CV_WRAP_AS(get) Mat getMat(int flags CV_WRAP_DEFAULT(ACCESS_RW)) const;\n};\n```\n\n----------------------------------------\n\nTITLE: Applying 2D Convolution with OpenCV in C++\nDESCRIPTION: Demonstrates the use of cv.filter2D() in OpenCV to apply a 2D convolution over an image with a kernel. This method is used to blur images or find edges through different filter kernels like low-pass or high-pass filters. Dependencies include OpenCV. Key parameters are the source image, destination image, kernel, and border type. The snippet highlights an example of a 5x5 averaging filter kernel.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_filtering/js_filtering.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ncv::filter2D(src, dst, ddepth, kernel, cv::Point(-1, -1), 0, cv::BORDER_DEFAULT);\n```\n\n----------------------------------------\n\nTITLE: Drawing Face Masks in C++\nDESCRIPTION: This snippet handles drawing masks by calculating and manipulating several layers of masks ('sharp', 'bilateral', and 'background') using G-API's threshold and bitwise operations to prepare them for use in face beautification.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/face_beautification/face_beautification.cpp msk_ppline\n```\n\n----------------------------------------\n\nTITLE: Checking C++11 Support for OpenEXR in CMake\nDESCRIPTION: This code checks if the compiler supports C++11, which is required for OpenEXR. If C++11 is not supported, it disables the OpenEXR build with a warning message.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openexr/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT HAVE_CXX11)\n  ocv_check_compiler_flag(CXX \"-std=c++11\" HAVE_STD_CXX11 \"${OpenCV_SOURCE_DIR}/cmake/checks/cxx11.cpp\")\n  if(HAVE_STD_CXX11)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\n  else()\n    if(BUILD_OPENEXR)\n      message(WARNING \"OpenCV: builtin OpenEXR requires C++11 support. OpenEXR is disabled.\")\n    endif()\n    return()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Basic CMakeLists.txt for an OpenCV Project\nDESCRIPTION: This snippet shows the content of a simple `CMakeLists.txt` file used to build an OpenCV application with CMake, suitable for generating an Eclipse project. It defines the project name (`helloworld_proj`), finds the required OpenCV package using `FIND_PACKAGE`, specifies the executable name and source file (`ADD_EXECUTABLE`), and links the executable against the necessary OpenCV libraries found by CMake (`TARGET_LINK_LIBRARIES`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_eclipse/linux_eclipse.markdown#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nPROJECT( helloworld_proj )\nFIND_PACKAGE( OpenCV REQUIRED )\nADD_EXECUTABLE( helloworld helloworld.cxx )\nTARGET_LINK_LIBRARIES( helloworld \\f${OpenCV_LIBS} )\n```\n\n----------------------------------------\n\nTITLE: Conditionally Including Android SDK or JAR Subdirectory in CMake\nDESCRIPTION: This snippet conditionally includes either the `android_sdk` or the `jar` subdirectory based on the value of the `ANDROID` CMake variable. If building for Android (`ANDROID` is true), it includes `android_sdk` to generate the `${the_module}_android` target. Otherwise, it includes `jar` to generate the `${the_module}_jar` target for standard Java environments.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(ANDROID)\n  add_subdirectory(android_sdk)  # generates ${the_module}_android target\nelse()\n  add_subdirectory(jar)  # generates ${the_module}_jar target\nendif()\n```\n\n----------------------------------------\n\nTITLE: Handling Standalone Build Initial Pass in CMake\nDESCRIPTION: Checks if the CMake script is being run in a standalone build context (not as part of the main OpenCV build). If `OPENCV_INITIAL_PASS` is not defined, it configures a minimal project named 'gapi_standalone', includes a specific CMake file for standalone configuration, and exits the current script processing to prevent further execution in this context.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n# FIXME: Rework standalone build in more generic maner\n# (Restructure directories, add common pass, etc)\nif(NOT DEFINED OPENCV_INITIAL_PASS)\n    cmake_minimum_required(VERSION 3.3)\n    project(gapi_standalone)\n    include(\"cmake/standalone.cmake\")\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Custom Implementation of Otsu's Algorithm in Python\nDESCRIPTION: Shows a manual implementation of Otsu's thresholding algorithm to demonstrate how it calculates the optimal threshold value by minimizing weighted within-class variance.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_thresholding/py_thresholding.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimg = cv.imread('noisy2.png', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nblur = cv.GaussianBlur(img,(5,5),0)\n\n# find normalized_histogram, and its cumulative distribution function\nhist = cv.calcHist([blur],[0],None,[256],[0,256])\nhist_norm = hist.ravel()/hist.sum()\nQ = hist_norm.cumsum()\n\nbins = np.arange(256)\n\nfn_min = np.inf\nthresh = -1\n\nfor i in range(1,256):\n    p1,p2 = np.hsplit(hist_norm,[i]) # probabilities\n    q1,q2 = Q[i],Q[255]-Q[i] # cum sum of classes\n    if q1 < 1.e-6 or q2 < 1.e-6:\n        continue\n    b1,b2 = np.hsplit(bins,[i]) # weights\n\n    # finding means and variances\n    m1,m2 = np.sum(p1*b1)/q1, np.sum(p2*b2)/q2\n    v1,v2 = np.sum(((b1-m1)**2)*p1)/q1,np.sum(((b2-m2)**2)*p2)/q2\n\n    # calculates the minimization function\n    fn = v1*q1 + v2*q2\n    if fn < fn_min:\n        fn_min = fn\n        thresh = i\n\n# find otsu's threshold value with OpenCV function\nret, otsu = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\nprint( \"{} {}\".format(thresh,ret) )\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection Model with OpenCV\nDESCRIPTION: Command line example for running object detection using OpenCV face detection model with custom model and config paths.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython object_detection.py opencv_fd --model /path/to/caffemodel --config /path/to/prototxt\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV Binaries using CMake (Shell)\nDESCRIPTION: This shell command installs OpenCV or related build products from the build directory to the system or user-selected install prefix. Prerequisites include a successful build step and sufficient permissions for the install path (run with superuser privileges if needed). <build-directory> is where the compiled binaries reside. The --target install argument triggers the install process, and <other-options> may include configuration like release/debug mode. The installed files will go to the default or specified installation location.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/general_install/general_install.markdown#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncmake --build <build-directory> --target install <other-options>\n```\n\n----------------------------------------\n\nTITLE: Custom Serialization Usage with FileStorage for Custom Classes - OpenCV Python\nDESCRIPTION: This Python snippet illustrates using the custom class's write and read methods to serialize and deserialize an instance using OpenCV's FileStorage. The class methods must handle all attributes. Requires that the class and its serialization methods are properly implemented.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ndata = MyData()\\ndata.write(fs)\\ndata2 = MyData()\\ndata2.read(node)\n```\n\n----------------------------------------\n\nTITLE: Loading Animation Frames in OpenCV\nDESCRIPTION: Shows how to use cv::imreadanimation to load frames from an animated WebP image file. The function loads all frames into the animation structure for processing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/animations.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ncv::imreadanimation(\"animated.webp\", animation);\n```\n\nLANGUAGE: Python\nCODE:\n```\nanimation = cv.imreadanimation(\"animated.webp\")\n```\n\n----------------------------------------\n\nTITLE: Comparing PyTorch Preprocessing Order\nDESCRIPTION: This Python-like pseudocode illustrates the standard image preprocessing steps often used with PyTorch models trained on ImageNet. It involves scaling pixel values to the [0, 1] range, subtracting the per-channel mean, and then dividing by the per-channel standard deviation. This clarifies why the `mean` values in the `cv2.dnn.blobFromImage` example were multiplied by 255.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimg /= 255.0\nimg -= [0.485, 0.456, 0.406]\nimg /= [0.229, 0.224, 0.225]\n```\n\n----------------------------------------\n\nTITLE: Drawing an Atom in C++\nDESCRIPTION: Drawing an atom using ellipses and circles in OpenCV C++. The atom is represented by ellipses for orbits and filled circles for electrons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n/// 1. Draw a simple atom:\n/// -----------------------\n\n/// 1.a. Creating ellipses\nMyEllipse( atom_image, 90 );\nMyEllipse( atom_image, 0 );\nMyEllipse( atom_image, 45 );\nMyEllipse( atom_image, -45 );\n\n/// 1.b. Creating circles\nMyFilledCircle( atom_image, Point( w/2, w/2) );\n\n```\n\n----------------------------------------\n\nTITLE: Accessing 1D Histogram Bin Value in C++\nDESCRIPTION: C++ code example demonstrating how to access the value of a specific bin in a 1D histogram stored in a `cv::Mat`. It uses the `.at<float>(i)` method, where `i` is the index of the desired bin.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_30\n\nLANGUAGE: cpp\nCODE:\n```\nb_hist.at<float>(i)\n```\n\n----------------------------------------\n\nTITLE: Executing Text Recognition Using Bash\nDESCRIPTION: These bash examples illustrate how to execute text recognition using pre-trained models available in the OpenCV repository. The commands run binary executable files with specific model paths and image paths as parameters. It requires OpenCV compiled binaries and ONNX or protobuf model files. Inputs include paths to models, images, and an alphabet file for recognition, with options like '-rgb' flag for preprocessing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_text_spotting/dnn_text_spotting.markdown#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexample_dnn_scene_text_recognition -mp=path/to/crnn_cs.onnx -i=path/to/an/image -rgb=1 -vp=/path/to/alphabet_94.txt\nexample_dnn_scene_text_detection -mp=path/to/DB_TD500_resnet50.onnx -i=path/to/an/image -ih=736 -iw=736\nexample_dnn_scene_text_spotting -dmp=path/to/DB_IC15_resnet50.onnx -rmp=path/to/crnn_cs.onnx -i=path/to/an/image -iw=1280 -ih=736 -rgb=1 -vp=/path/to/alphabet_94.txt\nexample_dnn_text_detection -dmp=path/to/EAST.pb -rmp=path/to/crnn_cs.onnx -i=path/to/an/image -rgb=1 -vp=path/to/alphabet_94.txt\n```\n\n----------------------------------------\n\nTITLE: Segmentation Test Module Configuration\nDESCRIPTION: Defines the configuration for the segmentation test module, including paths to test data, input image, and model parameters such as frame dimensions, preprocessing options for scaling, mean subtraction, and normalization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass TestSegmModuleConfig:\n    segm_test_data_dir: str = \"test_data/sem_segm\"\n    test_module_name: str = \"segmentation\"\n    test_module_path: str = \"segmentation.py\"\n    input_img: str = os.path.join(segm_test_data_dir, \"2007_000033.jpg\")\n    model: str = \"\"\n\n    frame_height: str = str(TestSegmConfig.frame_size)\n    frame_width: str = str(TestSegmConfig.frame_size)\n    scale: float = 1.0\n    mean: List[float] = field(default_factory=lambda: [0.0, 0.0, 0.0])\n    std: List[float] = field(default_factory=list)\n    crop: bool = False\n    rgb: bool = True\n    classes: str = os.path.join(segm_test_data_dir, \"pascal-classes.txt\")\n```\n\n----------------------------------------\n\nTITLE: Setting eSDK Root Environment Variable for Qualcomm Linux Build (Shell)\nDESCRIPTION: This shell command sets the `ESDK_ROOT` environment variable to the installation path of the Qualcomm Embedded Software Development Kit (eSDK). This variable is essential for the subsequent build steps, as it allows the build system and environment setup scripts to locate the necessary tools and libraries within the eSDK. Replace `<eSDK install location>` with the actual directory where the eSDK is installed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_fastcv/building_fastcv.markdown#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport ESDK_ROOT=<eSDK install location>\n```\n\n----------------------------------------\n\nTITLE: Text Detection on Public Datasets Using Bash\nDESCRIPTION: These bash examples are for detecting text on public datasets using models specified in ONNX format. They demonstrate the execution of text detection with adjustable input dimensions and evaluation paths. Required dependencies include pre-compiled OpenCV and model files along with the evaluation dataset. Users should adjust '-ih' and '-iw' options based on the input image resolution constraints.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_text_spotting/dnn_text_spotting.markdown#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexample_dnn_scene_text_detection -mp=path/to/DB_TD500_resnet50.onnx -e=true -edp=path/to/evaluation_data_det/TD500 -ih=736 -iw=736\nexample_dnn_scene_text_detection -mp=path/to/DB_IC15_resnet50.onnx -e=true -edp=path/to/evaluation_data_det/IC15 -ih=736 -iw=1280\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV.js on Windows with Docker Using Emscripten Version 2.0.10 - Bash/PowerShell\nDESCRIPTION: This command runs the OpenCV.js build process from within a Docker container on Windows using a specific tested tag of emscripten/emsdk (2.0.10). It ensures compatibility and uses PowerShell pathing for mounting the source directory. Outputs are OpenCV.js builds. Requires Docker, PowerShell, and correct permissions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm --workdir /src -v \"$(get-location):/src\" \"emscripten/emsdk:2.0.10\" emcmake python3 ./platforms/js/build_js.py build_js\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV.js with emcmake and Build Script - Bash\nDESCRIPTION: Invokes the build script for OpenCV.js using Python with emcmake to set up the Emscripten toolchain. It targets a directory named build_js for output. Prerequisites: emcmake, Python, and CMake are installed. Parameters: paths to the build script and build directory. Output is a bundled opencv.js file with WebAssembly embedded or separate, depending on flags. For production, additional options can be passed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nemcmake python ./opencv/platforms/js/build_js.py build_js\n```\n\n----------------------------------------\n\nTITLE: Markdown Tutorial Navigation Structure for SVM\nDESCRIPTION: Markdown structure defining the navigation hierarchy for SVM tutorials, including links to basic concepts and OpenCV implementation pages.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_svm/py_svm_index.markdown#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nSupport Vector Machines (SVM) {#tutorial_py_svm_index}\n=============================\n\n-   @subpage tutorial_py_svm_basics\n\n    Get a basic understanding of what SVM is\n\n-   @subpage tutorial_py_svm_opencv\n\n    Let's use SVM functionalities in OpenCV\n```\n\n----------------------------------------\n\nTITLE: Library Target Definition\nDESCRIPTION: Defines the PNG library target with its source files, linking dependencies, and output properties.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libpng/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${PNG_LIBRARY} STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${lib_srcs} ${lib_hdrs})\ntarget_link_libraries(${PNG_LIBRARY} ${ZLIB_LIBRARIES})\n\nset_target_properties(${PNG_LIBRARY}\n  PROPERTIES OUTPUT_NAME ${PNG_LIBRARY}\n  DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n  COMPILE_PDB_NAME ${PNG_LIBRARY}\n  COMPILE_PDB_NAME_DEBUG \"${PNG_LIBRARY}${OPENCV_DEBUG_POSTFIX}\"\n  ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n  )\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform-Specific Settings for OpenEXR\nDESCRIPTION: This code detects the platform (Windows, Apple, or UNIX) and sets appropriate platform-specific configuration flags for OpenEXR. It handles special cases like semaphore support on different platforms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openexr/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32)\n  set(HAVE_COMPLETE_IOMANIP 1)\n  set(OPENEXR_IMF_HAVE_COMPLETE_IOMANIP 1)\n  set(PLATFORM_WINDOWS 1)\nelseif(APPLE)\n  set(HAVE_POSIX_SEMAPHORES 0)  # Unnamed semaphores are not supported: https://github.com/opencv/opencv/issues/9361\n  if(DARWIN)\n    set(OPENEXR_IMF_HAVE_DARWIN 1)\n  endif()\nelseif(UNIX)\n  include(CheckIncludeFile)\n  check_include_file(semaphore.h HAVE_POSIX_SEMAPHORES)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Displaying an Image using OpenCV in Python\nDESCRIPTION: This snippet shows how to display an image in Python using OpenCV's cv.imshow. It creates a window with the specified title and shows the image array until a key is pressed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/display_image/display_image.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ncv.imshow('Display window', image)\ncv.waitKey(0)\n```\n\n----------------------------------------\n\nTITLE: Template Matching Formula: TM_CCOEFF (LaTeX)\nDESCRIPTION: Mathematical formula for the Correlation Coefficient (TM_CCOEFF) template matching method used in OpenCV's `matchTemplate` function. It uses mean-subtracted template (T') and image patches (I'). R(x,y) is the result, T is the template, I is the image, w is width, and h is height.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_5\n\nLANGUAGE: latex\nCODE:\n```\n\\f[R(x,y)= \\sum _{x',y'} (T'(x',y')  \\cdot I'(x+x',y+y'))\\f]\n\nwhere\n\n\\f[\\begin{array}{l} T'(x',y')=T(x',y') - 1/(w  \\cdot h)  \\cdot \\sum _{x'',y''} T(x'',y'') \\\\ I'(x+x',y+y')=I(x+x',y+y') - 1/(w  \\cdot h)  \\cdot \\sum _{x'',y''} I(x+x'',y+y'') \\end{array}\\f]\n```\n\n----------------------------------------\n\nTITLE: Loading Images with OpenCV in Java\nDESCRIPTION: This snippet loads an image using OpenCV's Imgcodecs module in Java. It uses Imgcodecs.imread and checks for null or empty image matrices. The required parameter is the file path; the output is a Mat object. OpenCV Java bindings must be correctly configured.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\nimport org.opencv.core.*;\\nimport org.opencv.imgcodecs.Imgcodecs;\\n\\nMat src = Imgcodecs.imread(\"path_to_image\", Imgcodecs.IMREAD_COLOR);\\nif(src.empty()) {\\n    System.out.println(\"Could not open or find the image!\");\\n    return;\\n}\\n\n```\n\n----------------------------------------\n\nTITLE: Feature Matching with FLANN Matcher - Python\nDESCRIPTION: This snippet uses the FLANN-based matcher in OpenCV to find matching features between two sets of SIFT descriptors. It requires a previous initialization of descriptors with SIFT and outputs matching points with a ratio test to filter the best matches.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_epipolar_geometry/py_epipolar_geometry.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# FLANN parameters\nFLANN_INDEX_KDTREE = 1\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\nsearch_params = dict(checks=50)\n\nflann = cv.FlannBasedMatcher(index_params,search_params)\nmatches = flann.knnMatch(des1,des2,k=2)\n\npts1 = []\npts2 = []\n\n# ratio test as per Lowe's paper\nfor i,(m,n) in enumerate(matches):\n    if m.distance < 0.8*n.distance:\n        pts2.append(kp2[m.trainIdx].pt)\n        pts1.append(kp1[m.queryIdx].pt)\n```\n\n----------------------------------------\n\nTITLE: Pixel Intensity Comparison for Corner Detection in C++\nDESCRIPTION: This code snippet implements a corner detection algorithm by comparing pixel intensities at different offsets. It uses nested if-else statements to navigate through various conditions, ultimately determining whether a point is a corner or not.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_23\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset3] < c_b)\n  if(ptr[offset4] < c_b)\n    if(ptr[offset8] < c_b)\n      goto is_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\n\n// ... (more nested conditions)\n\nif(ptr[offset9] > cb)\n  if(ptr[offset1] < c_b)\n    if(ptr[offset6] < c_b)\n      goto is_not_a_corner;\n    else\n      if(ptr[offset6] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              goto is_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Markdown Header for VideoIO Module Documentation\nDESCRIPTION: Markdown section header defining the documentation title for OpenCV's videoio module with an HTML anchor tag reference.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/_old/table_of_content_videoio.markdown#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nVideo Input and Output (videoio module) {#tutorial_table_of_content_videoio}\n=========================================\n\nContent has been moved to this page: @ref tutorial_table_of_content_app\n```\n\n----------------------------------------\n\nTITLE: Defining and Exporting the OpenCV imgproc Module with Language Bindings in CMake\nDESCRIPTION: This CMake snippet defines the 'imgproc' module and declares its dependency on 'opencv_core'. Additionally, it specifies that wrappers should be generated for Java, Objective-C, Python, and JavaScript, enabling cross-language bindings of this module's functionality. Dependencies must be available and WRAP flags ensure the proper bindings generation for the specified languages.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/imgproc/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nocv_define_module(imgproc opencv_core WRAP java objc python js)\n```\n\n----------------------------------------\n\nTITLE: Querying Color Conversion Flags with OpenCV in Python\nDESCRIPTION: This snippet illustrates querying all available color conversion flags in OpenCV by listing any attribute starting with 'COLOR_' from the cv2 module. Useful for checking supported color-space conversions at runtime. Requires OpenCV (cv2) to be installed. No inputs other than the Python interpreter; output is a printed list of conversion flags.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import cv2 as cv\\n>>> flags = [i for i in dir(cv) if i.startswith('COLOR_')]\\n>>> print( flags )\n```\n\n----------------------------------------\n\nTITLE: Formatting Module Reference Lists for Doxygen in CMake\nDESCRIPTION: Formats the previously gathered main and extra module references (`refs_main`, `refs_extra`) into markdown lists suitable for inclusion in the Doxygen documentation root file. Each list is prefixed with a header ('- Main modules:' or '- Extra modules:').\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n# fix references\n# set(ref_header \"Module name | Folder\\n----------- | ------\")\n# if(refs_main)\n#    set(refs_main \"### Main modules\\n${ref_header}\\n${refs_main}\")\n# endif()\n# if(refs_extra)\n#   set(refs_extra \"### Extra modules\\n${ref_header}\\n${refs_extra}\")\n# endif()\nif(refs_main)\n  set(refs_main \"- Main modules:\\n${refs_main}\")\nendif()\nif(refs_extra)\n  set(refs_extra \"- Extra modules:\\n${refs_extra}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enabling ICC Profile Extraction via Marker Saving - libjpeg - C\nDESCRIPTION: Illustrates the use of jpeg_save_markers to enable extraction of embedded ICC profiles from JPEG markers. This call configures the decompression context to save all APP2 (ICC) markers for subsequent retrieval by jpeg_read_icc_profile. Must be called before reading JPEG header in decompression; third parameter indicates maximum marker length to save.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_63\n\nLANGUAGE: c\nCODE:\n```\njpeg_save_markers(cinfo, JPEG_APP0 + 2, 0xFFFF);\n```\n\n----------------------------------------\n\nTITLE: Applying Laplacian Operator in Python\nDESCRIPTION: Applies the Laplacian operator to the grayscale image (`src_gray`) using cv2.Laplacian. The output depth (`ddepth`) is specified as cv2.CV_16S to handle potential negative values from the second derivative and prevent overflow. Kernel size, scale, delta, and border type use predefined values. Requires cv2.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#! [laplacian]\n# [laplacian]\n# Apply Laplace function\ndst = cv.Laplacian( src_gray, ddepth, ksize=kernel_size )\n# [laplacian]\n# ! [laplacian]\n```\n\n----------------------------------------\n\nTITLE: Setting RANSAC Parameters for Pose Estimation in C++\nDESCRIPTION: Defines RANSAC parameters for iterative pose estimation in OpenCV, detailing iteration counts, reprojection errors, and confidence levels in a C++ application.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n// RANSAC parameters\n\nint iterationsCount = 500;        // number of Ransac iterations.\nfloat reprojectionError = 2.0;    // maximum allowed distance to consider it an inlier.\nfloat confidence = 0.95;          // RANSAC successful confidence.\n```\n\n----------------------------------------\n\nTITLE: Processing OpenCV Install Apps List\nDESCRIPTION: Converts comma-separated list of apps to semicolon-separated for CMake compatibility.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nstring(REPLACE \",\" \";\" OPENCV_INSTALL_APPS_LIST \"${OPENCV_INSTALL_APPS_LIST}\")\n```\n\n----------------------------------------\n\nTITLE: Build Options Configuration\nDESCRIPTION: Defines various build options for controlling features, compatibility, testing, optimizations, and runtime behavior of the library\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\noption(WITH_GZFILEOP \"Compile with support for gzFile related functions\" ON)\noption(ZLIB_COMPAT \"Compile with zlib compatible API\" ON)\noption(ZLIB_ENABLE_TESTS \"Build test binaries\" OFF)\noption(ZLIBNG_ENABLE_TESTS \"Test zlib-ng specific API\" OFF)\noption(WITH_GTEST \"Build gtest_zlib\" OFF)\noption(WITH_FUZZERS \"Build test/fuzz\" OFF)\noption(WITH_BENCHMARKS \"Build test/benchmarks\" OFF)\noption(WITH_BENCHMARK_APPS \"Build application benchmarks\" OFF)\noption(WITH_OPTIM \"Build with optimisation\" ON)\noption(WITH_REDUCED_MEM \"Reduced memory usage for special cases (reduces performance)\" OFF)\noption(WITH_NEW_STRATEGIES \"Use new strategies\" ON)\noption(WITH_NATIVE_INSTRUCTIONS\n    \"Instruct the compiler to use the full instruction set on this host (gcc/clang -march=native)\" OFF)\noption(WITH_RUNTIME_CPU_DETECTION \"Build with runtime detection of CPU architecture\" ON)\noption(WITH_MAINTAINER_WARNINGS \"Build with project maintainer warnings\" OFF)\noption(WITH_CODE_COVERAGE \"Enable code coverage reporting\" OFF)\noption(WITH_INFLATE_STRICT \"Build with strict inflate distance checking\" OFF)\noption(WITH_INFLATE_ALLOW_INVALID_DIST \"Build with zero fill for inflate invalid distances\" OFF)\noption(WITH_UNALIGNED \"Support unaligned reads on platforms that support it\" ON)\n```\n\n----------------------------------------\n\nTITLE: Preparing TBB Version Information for Build\nDESCRIPTION: Sets up the TBB source files list and configures the version string file that will be embedded in the library. This ensures the built library contains proper version information.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/tbb/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nset(TBB_SOURCE_FILES ${lib_srcs} ${lib_hdrs})\n\nset(tbb_version_file \"version_string.ver\")\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/${tbb_version_file}.cmakein\" \"${CMAKE_CURRENT_BINARY_DIR}/${tbb_version_file}\" @ONLY)\nlist(APPEND TBB_SOURCE_FILES \"${CMAKE_CURRENT_BINARY_DIR}/${tbb_version_file}\")\n```\n\n----------------------------------------\n\nTITLE: Running the Ant Build Script (Bash)\nDESCRIPTION: This Bash command executes the Ant build process defined in `build.xml`. It uses the '-D' flag to pass Java system properties required by the build script: `ocvJarDir` specifies the directory containing the OpenCV JAR file (e.g., `opencv-xxx.jar`), and `ocvLibDir` specifies the directory containing the native OpenCV Java library (e.g., `libopencv_javaxxx.so` or `opencv_javaxxx.dll`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nant -DocvJarDir=path/to/dir/containing/opencv-244.jar -DocvLibDir=path/to/dir/containing/opencv_java244/native/library\n```\n\n----------------------------------------\n\nTITLE: Displaying Animation Frames with OpenCV\nDESCRIPTION: Illustrates how to iterate through and display each frame of the animation with a delay to simulate animated playback using OpenCV windows.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/animations.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nfor (size_t i = 0; i < animation.frames.size(); i++) {\n    cv::imshow(\"Animation\", animation.frames[i]);\n    cv::waitKey(animation.delays[i]);\n}\n```\n\nLANGUAGE: Python\nCODE:\n```\nfor frame, delay in zip(animation[\"frames\"], animation[\"delays\"]):\n    cv.imshow(\"Animation\", frame)\n    cv.waitKey(delay)\n```\n\n----------------------------------------\n\nTITLE: Creating and Plotting One-Dimensional Test Data for K-Means Clustering in Python\nDESCRIPTION: This code generates random one-dimensional test data, reshapes it into a column vector format for K-means, and plots a histogram of the data. The data consists of 50 values divided into two groups (25 values in range 25-100 and 25 values in range 175-255).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\n\nx = np.random.randint(25,100,25)\ny = np.random.randint(175,255,25)\nz = np.hstack((x,y))\nz = z.reshape((50,1))\nz = np.float32(z)\nplt.hist(z,256,[0,256]),plt.show()\n```\n\n----------------------------------------\n\nTITLE: Applying Top Hat Transform with OpenCV Python\nDESCRIPTION: Shows how to perform top hat transformation which finds the difference between input image and its opening.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.markdown#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ntophat = cv.morphologyEx(img, cv.MORPH_TOPHAT, kernel)\n```\n\n----------------------------------------\n\nTITLE: Loading Images for Histogram Comparison in OpenCV\nDESCRIPTION: Code for loading three test images with different environment settings to be used for histogram comparison testing.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_comparison/histogram_comparison.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nMat src_base = imread( argv[1], IMREAD_COLOR );\nMat src_test1 = imread( argv[2], IMREAD_COLOR );\nMat src_test2 = imread( argv[3], IMREAD_COLOR );\n```\n\nLANGUAGE: java\nCODE:\n```\nMat src_base = Imgcodecs.imread( samples.findFile(args[0]) );\nMat src_test1 = Imgcodecs.imread( samples.findFile(args[1]) );\nMat src_test2 = Imgcodecs.imread( samples.findFile(args[2]) );\n```\n\nLANGUAGE: python\nCODE:\n```\nsrc_base = cv.imread(samples.findFile(argv[1]))\nsrc_test1 = cv.imread(samples.findFile(argv[2]))\nsrc_test2 = cv.imread(samples.findFile(argv[3]))\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenMP Backend with CMake\nDESCRIPTION: This snippet checks and configures OpenMP as a parallel backend for an OpenCV example. It finds OpenMP and sets up a project if OpenMP is available. The `example-openmp.cpp` is added as an executable, and necessary libraries are linked. This configuration requires OpenCV's core module and the OpenMP C++ libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/cpp/tutorial_code/core/parallel_backend/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT OPENCV_EXAMPLES_SKIP_PARALLEL_BACKEND_OPENMP\n    AND NOT OPENCV_EXAMPLES_SKIP_OPENMP\n)\n  project(opencv_example_openmp_backend)\n  find_package(OpenMP)\n  if(OpenMP_FOUND)\n    add_executable(opencv_example_openmp_backend example-openmp.cpp)\n    target_link_libraries(opencv_example_openmp_backend PRIVATE\n        opencv_core\n        OpenMP::OpenMP_CXX\n    )\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Moving OpenCV Python Module to System Site-Packages (Shell)\nDESCRIPTION: Uses the `mv` command as root (`su`) to relocate the compiled OpenCV Python wrapper (`cv2.so`) from its installation directory (`/usr/local/lib/python2.7/site-packages/`) to a standard Python site-packages location (`/usr/lib/python2.7/site-packages`). This makes the module discoverable by the Python interpreter without modifying environment variables, but may need repeating after updates.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nsu mv /usr/local/lib/python2.7/site-packages/cv2.so /usr/lib/python2.7/site-packages\n```\n\n----------------------------------------\n\nTITLE: Adaptive Thresholding Implementation in OpenCV Python\nDESCRIPTION: Shows implementation of adaptive thresholding using both mean and Gaussian methods. Compares global thresholding with adaptive thresholding approaches for images with varying illumination.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_thresholding/py_thresholding.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimg = cv.imread('sudoku.png', cv.IMREAD_GRAYSCALE)\nassert img is not None, \"file could not be read, check with os.path.exists()\"\nimg = cv.medianBlur(img,5)\n\nret,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\nth2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C,\\\n            cv.THRESH_BINARY,11,2)\nth3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n            cv.THRESH_BINARY,11,2)\n\ntitles = ['Original Image', 'Global Thresholding (v = 127)',\n            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\nimages = [img, th1, th2, th3]\n\nfor i in range(4):\n    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n    plt.title(titles[i])\n    plt.xticks([]),plt.yticks([])\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Performing Meanshift in OpenCV.js\nDESCRIPTION: This snippet demonstrates the use of cv.meanShift to track objects using their histogram backprojection in OpenCV.js. Required dependencies include OpenCV.js library. Key parameters are probImage (back projection of the object histogram), window (initial search window), and criteria (stop criteria). Outputs the number of iterations and the new location of the window.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_video/js_meanshift/js_meanshift.markdown#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\ncv.meanShift(probImage, window, criteria);\n```\n\n----------------------------------------\n\nTITLE: Enabling Noisy Compiler Warnings in OpenCV via CMake (CMake)\nDESCRIPTION: This CMake option enables additional compiler warnings considered less critical (noisy warnings). It can be set by using -DENABLE_NOISY_WARNINGS=ON in the CMake command. There are no dependencies, but enabling it will result in more detailed output from the compiler. Used to help developers identify potential issues even if not all are critical.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_25\n\nLANGUAGE: cmake\nCODE:\n```\nENABLE_NOISY_WARNINGS\n```\n\n----------------------------------------\n\nTITLE: Setting Up Working Directory and Cloning Repositories (Bash)\nDESCRIPTION: Navigates to the home directory, creates a 'work' directory, changes into it, and then clones the main OpenCV repository and the OpenCV Contrib repository from GitHub. The `--depth=1` flag performs a shallow clone, downloading only the latest revision to save time and space.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd ~\nmkdir work\ncd work\ngit clone --depth=1 https://github.com/opencv/opencv.git\ngit clone --depth=1 https://github.com/opencv/opencv_contrib.git\n```\n\n----------------------------------------\n\nTITLE: OpenCV Wayland Sample Application\nDESCRIPTION: Example C++ application demonstrating basic Wayland window creation and image display using OpenCV. Shows how to create a window, load an image, and handle basic window events.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/highgui_wayland_ubuntu.markdown#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// g++ main.cpp -o a.out -I /usr/local/include/opencv4 -lopencv_core -lopencv_highgui -lopencv_imgcodecs\n#include <opencv2/core.hpp>\n#include <opencv2/highgui.hpp>\n#include <opencv2/imgcodecs.hpp>\n#include <iostream>\n#include <string>\n\nint main(void)\n{\n  std::cout << \"cv::currentUIFramework() returns \" << cv::currentUIFramework() << std::endl;\n\n  cv::Mat src;\n  src = cv::imread(\"opencv-logo.png\");\n\n  cv::namedWindow(\"src\");\n\n  int key = 0;\n  do\n  {\n      cv::imshow(\"src\", src );\n      key = cv::waitKey(50);\n  } while( key != 'q' );\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Computing Histograms for BGR Channels in Java\nDESCRIPTION: Java snippet calculating histograms for each B, G, and R plane using `Imgproc.calcHist`. It takes the list of planes (`bgrPlanes`), specifies the channel index (0), uses no mask (`new Mat()`), defines output Mats (`bHist`, `gHist`, `rHist`), and provides parameters (`histSize`, `histRange`, `histUniform`, `accumulate`).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\n@snippet samples/java/tutorial_code/Histograms_Matching/histogram_calculation/CalcHistDemo.java Compute the histograms\n```\n\n----------------------------------------\n\nTITLE: Defining a Helper Function for Random Color Generation in OpenCV C++\nDESCRIPTION: Defines a static helper function `randomColor` that takes a reference to a cv::RNG object. It generates a random unsigned integer from the RNG and then extracts byte-sized components using bitwise operations to create a random BGR color represented as a cv::Scalar object.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/random_generator_and_text/random_generator_and_text.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n@code{.cpp}\nstatic Scalar randomColor( RNG& rng )\n  {\n  int icolor = (unsigned) rng;\n  return Scalar( icolor&255, (icolor>>8)&255, (icolor>>16)&255 );\n  }\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Drawing Probabilistic Hough Line Segments in OpenCV (Python)\nDESCRIPTION: This Python snippet draws the line segments detected by HoughLinesP onto the image. For every segment, it draws a green line between the two endpoints using cv2.line. Expects output from HoughLinesP and OpenCV/numpy imports.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nif linesP is not None:\\n    for i in range(0, len(linesP)):\\n        l = linesP[i][0]\\n        cv2.line(img, (l[0], l[1]), (l[2], l[3]), (0,255,0), 3, cv2.LINE_AA)\\n\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenCV Contrib Modules in Build - Bash\nDESCRIPTION: Passes a CMake option to the build_js.py script for including modules from the opencv_contrib repository. Dependency: emcmake, Python, and opencv_contrib modules available in the specified location. Parameter: --cmake_option specifying path to contrib modules. The output is an OpenCV.js build with extra features enabled.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nemcmake python ./platforms/js/build_js.py build_js --cmake_option=\"-DOPENCV_EXTRA_MODULES_PATH=opencv_contrib/modules\"\n```\n\n----------------------------------------\n\nTITLE: Setting Module Description in CMakeLists - CMake\nDESCRIPTION: This snippet sets a descriptive string for the module using the CMake set command. The variable the_description is assigned the value \\\"Machine Learning\\\" for potential use in documentation generation, build introspection, or module metadata. There are no required dependencies or special inputs, and this value is not programmatically enforced beyond assignment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/ml/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(the_description \"Machine Learning\")\n```\n\n----------------------------------------\n\nTITLE: Converting Laplacian Output to CV_8U in Python\nDESCRIPTION: Converts the Laplacian output (`dst`), which has a depth of CV_16S, to an 8-bit unsigned integer image (`abs_dst`) using cv2.convertScaleAbs. This calculates the absolute value and scales the pixel values to fit the 0-255 range, preparing the image for display. Requires cv2.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n#! [convert]\n# [convert]\n# converting back to CV_8U\nabs_dst = cv.convertScaleAbs(dst)\n# [convert]\n# ! [convert]\n```\n\n----------------------------------------\n\nTITLE: Version Extraction from Header\nDESCRIPTION: Extracts version information from zlib.h.in header file using regex patterns to parse both ZLIB and ZLIBNG version strings\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(READ ${CMAKE_CURRENT_SOURCE_DIR}/zlib.h.in _zlib_h_contents)\nstring(REGEX REPLACE \".*#define[ \\t]+ZLIB_VERSION[ \\t]+\\\"([0-9]+.[0-9]+.[0-9]+).*\\\".*\"\n        \"\\\\1\" ZLIB_HEADER_VERSION ${_zlib_h_contents})\nstring(REGEX REPLACE \".*#define[ \\t]+ZLIBNG_VERSION[ \\t]+\\\"([-0-9A-Za-z.]+)\\\".*\"\n        \"\\\\1\" ZLIBNG_HEADER_VERSION ${_zlib_h_contents})\nmessage(STATUS \"ZLIB_HEADER_VERSION: ${ZLIB_HEADER_VERSION}\")\nmessage(STATUS \"ZLIBNG_HEADER_VERSION: ${ZLIBNG_HEADER_VERSION}\")\n```\n\n----------------------------------------\n\nTITLE: Optimizing TensorFlow Graph with TransformGraph\nDESCRIPTION: This Python function uses TensorFlow's TransformGraph to optimize the frozen model graph. It requires the TF graph, input and output layer names, and a set of transformation rules. It is crucial for enhancing the model's performance by reducing graph complexity.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef optimize_tf_graph(\n        in_graph,\n        out_graph=DEFAULT_OPT_GRAPH_NAME,\n        inputs=DEFAULT_INPUTS,\n        outputs=DEFAULT_OUTPUTS,\n        transforms=DEFAULT_TRANSFORMS,\n        is_manual=True,\n        was_optimized=True\n):\n    # ...\n\n    tf_opt_graph = TransformGraph(\n        tf_graph,\n        inputs,\n        outputs,\n        transforms\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring x86 SSE2 Optimizations for ZLIB in CMake\nDESCRIPTION: Checks if SSE2 optimization is enabled (WITH_SSE2) and if intrinsics are available (HAVE_SSE2_INTRIN). If so, it adds the DX86_SSE2 definition, appends SSE2-specific source files to ZLIB_ARCH_SRCS, and sets compile flags (SSE2FLAG, NOLTOFLAG) specifically for non-x86_64 architectures. It also handles the FORCE_SSE2 option to assume SSE2 capability without runtime checks. Otherwise, it disables SSE2 support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_26\n\nLANGUAGE: cmake\nCODE:\n```\n        if(WITH_SSE2)\n            check_sse2_intrinsics()\n            if(HAVE_SSE2_INTRIN)\n                add_definitions(-DX86_SSE2)\n                set(SSE2_SRCS ${ARCHDIR}/chunkset_sse2.c ${ARCHDIR}/compare256_sse2.c ${ARCHDIR}/slide_hash_sse2.c)\n                list(APPEND ZLIB_ARCH_SRCS ${SSE2_SRCS})\n                if(NOT ${ARCH} MATCHES \"x86_64\")\n                    set_property(SOURCE ${SSE2_SRCS} PROPERTY COMPILE_FLAGS \"${SSE2FLAG} ${NOLTOFLAG}\")\n                    add_feature_info(FORCE_SSE2 FORCE_SSE2 \"Assume CPU is SSE2 capable\")\n                    if(FORCE_SSE2)\n                        add_definitions(-DX86_NOCHECK_SSE2)\n                    endif()\n                endif()\n            else()\n                set(WITH_SSE2 OFF)\n            endif()\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Displaying Background Subtraction Results\nDESCRIPTION: Shows the original frame and the resulting foreground mask in separate windows using the imshow function. This allows real-time visualization of the background subtraction process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/background_subtraction.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n//show the current frame and the fg masks\nimshow(\"Frame\", frame);\nimshow(\"FG Mask\", fgMask);\n```\n\nLANGUAGE: Java\nCODE:\n```\n// show the current frame and the fg masks\nImgCodecs.imencode(\".png\", frame, buffer);\nImage frameImg = ImageIO.read(new ByteArrayInputStream(buffer.toArray()));\nImgCodecs.imencode(\".png\", fgMask, buffer);\nImage fgMaskImg = ImageIO.read(new ByteArrayInputStream(buffer.toArray()));\nimageFrame.setImage(frameImg);\nfgMaskFrame.setImage(fgMaskImg);\n```\n\nLANGUAGE: Python\nCODE:\n```\n# show the current frame and the fg masks\ncv.imshow('Frame', frame)\ncv.imshow('FG Mask', fgMask)\n```\n\n----------------------------------------\n\nTITLE: Pixel Comparison Logic for FAST Corner Detection in C++\nDESCRIPTION: This code implements the decision tree for FAST corner detection algorithm. It compares pixel values against threshold values (cb and c_b) in specific offsets around a central pixel to determine if the current point is a corner or not.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\ngoto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset12] > cb)\n    if(ptr[offset7] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset9] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              if(ptr[offset13] > cb)\n                if(ptr[offset14] > cb)\n                  if(ptr[offset6] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset15] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n  if(ptr[offset12] < c_b)\n    if(ptr[offset13] < c_b)\n      if(ptr[offset14] < c_b)\n        if(ptr[offset15] < c_b)\n          if(ptr[offset1] < c_b)\n            if(ptr[offset3] < c_b)\n              goto is_a_corner;\n            else\n              if(ptr[offset10] < c_b)\n                if(ptr[offset11] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset8] < c_b)\n              if(ptr[offset9] < c_b)\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset6] < c_b)\n            if(ptr[offset7] < c_b)\n              if(ptr[offset8] < c_b)\n                if(ptr[offset9] < c_b)\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset11] > cb)\n    if(ptr[offset7] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset9] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset12] > cb)\n              if(ptr[offset13] > cb)\n                if(ptr[offset6] > cb)\n                  if(ptr[offset5] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset14] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset14] > cb)\n                    if(ptr[offset15] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n  if(ptr[offset11] < c_b)\n    if(ptr[offset12] < c_b)\n      if(ptr[offset13] < c_b)\n        if(ptr[offset10] < c_b)\n          if(ptr[offset14] < c_b)\n            if(ptr[offset15] < c_b)\n              if(ptr[offset1] < c_b)\n                goto is_a_corner;\n              else\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset9] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset6] < c_b)\n                if(ptr[offset7] < c_b)\n                  if(ptr[offset8] < c_b)\n                    if(ptr[offset9] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset5] < c_b)\n              if(ptr[offset6] < c_b)\n                if(ptr[offset7] < c_b)\n                  if(ptr[offset8] < c_b)\n                    if(ptr[offset9] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset1] < c_b)\n            if(ptr[offset3] < c_b)\n              if(ptr[offset14] < c_b)\n                if(ptr[offset15] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset9] > cb)\n    if(ptr[offset7] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset11] > cb)\n            if(ptr[offset6] > cb)\n              if(ptr[offset5] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset3] > cb)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset12] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset12] > cb)\n                    if(ptr[offset13] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset12] > cb)\n                  if(ptr[offset13] > cb)\n                    if(ptr[offset14] > cb)\n                      goto is_a_corner;\n```\n\n----------------------------------------\n\nTITLE: OAST_9_16 Feature Detection Function Implementation\nDESCRIPTION: Main implementation of the OAST_9_16 feature detection algorithm. Initializes necessary variables and offsets for the detection process and sets up the scanning loop structure.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_30\n\nLANGUAGE: C++\nCODE:\n```\nstatic void OAST_9_16(InputArray _img, std::vector<KeyPoint>& keypoints, int threshold)\n{\n    cv::Mat img;\n    if(!_img.getMat().isContinuous())\n      img = _img.getMat().clone();\n    else\n      img = _img.getMat();\n\n    size_t total = 0;\n    int xsize = img.cols;\n    int ysize = img.rows;\n    size_t nExpectedCorners = keypoints.capacity();\n    int x, y;\n    int xsizeB=xsize - 4;\n    int ysizeB=ysize - 3;\n    int width;\n\n    keypoints.resize(0);\n\n    int pixel_9_16_[16];\n    makeAgastOffsets(pixel_9_16_, (int)img.step, AgastFeatureDetector::OAST_9_16);\n```\n\n----------------------------------------\n\nTITLE: Transforming TensorFlow Segmentation Mask to Colored Visualization\nDESCRIPTION: Converts a segmentation mask from TensorFlow model predictions into a colored visualization using PASCAL VOC class colors. The mask is resized to match the original image dimensions and converted from BGR to RGB color space.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncolors = np.array(colors)\nprocessed_mask = colors[segm_mask[0]]\n\nimg_height = original_img_shape[0]\nimg_width = original_img_shape[1]\n\nprocessed_mask = cv2.resize(processed_mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST).astype(\n    np.uint8)\n\n# convert colored mask from BGR to RGB for compatibility with PASCAL VOC colors\nprocessed_mask = cv2.cvtColor(processed_mask, cv2.COLOR_BGR2RGB)\n```\n\n----------------------------------------\n\nTITLE: Playing Video from File in Python with OpenCV\nDESCRIPTION: This code snippet shows how to play a video from a file using OpenCV. It uses cv.VideoCapture() with a file name, reads frames in a loop, converts them to grayscale, and displays them until 'q' is pressed or the video ends.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_video_display/py_video_display.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\ncap = cv.VideoCapture('vtest.avi')\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n\n    # if frame is read correctly ret is True\n    if not ret:\n        print(\"Can't receive frame (stream end?). Exiting ...\")\n        break\n    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n\n    cv.imshow('frame', gray)\n    if cv.waitKey(1) == ord('q'):\n        break\n\ncap.release()\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Sourcing eSDK Environment Setup Script for Qualcomm Linux (Shell)\nDESCRIPTION: This shell command executes the eSDK environment setup script (`environment-setup-armv8-2a-qcom-linux`) provided by the Qualcomm Linux SDK. Sourcing this script configures the current shell session with the appropriate cross-compilation toolchain paths, library paths, and other environment variables required to build OpenCV specifically for the target ARMv8.2-A Qualcomm Linux platform.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_fastcv/building_fastcv.markdown#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsource environment-setup-armv8-2a-qcom-linux\n```\n\n----------------------------------------\n\nTITLE: Visualizing K-Means Clustering Results for One-Dimensional Data\nDESCRIPTION: This code plots the clustering results by showing histograms of the two separate clusters in different colors (red and blue) along with their centroids in yellow. This visualization helps to see how the algorithm has separated the data.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.markdown#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Now plot 'A' in red, 'B' in blue, 'centers' in yellow\nplt.hist(A,256,[0,256],color = 'r')\nplt.hist(B,256,[0,256],color = 'b')\nplt.hist(centers,32,[0,256],color = 'y')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV for ARM with Make - Bash\nDESCRIPTION: Invokes 'make' in the build directory to compile OpenCV for ARM using previously configured build files. Requires prior execution of CMake with the proper toolchain. All targets configured via CMake will be built. The command must be run in the appropriate build directory, and may take significant time depending on hardware.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/arm_crosscompile_with_cmake.markdown#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmake\n```\n\n----------------------------------------\n\nTITLE: Defining Doxygen Citelist Warning Filter Pattern (Plaintext)\nDESCRIPTION: This pattern identifies lines starting with 'citelist :' followed by any characters and the specific warning text 'Unexpected new line character.'. It is likely used within a build script or configuration file to suppress or handle this particular Doxygen warning during the documentation generation for the OpenCV project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/disabled_doc_warnings.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncitelist : .*Unexpected new line character.*\n```\n\n----------------------------------------\n\nTITLE: Creating Platform-Specific and Combined Objective-C Generation Targets in CMake\nDESCRIPTION: Sets up build targets for each platform (iOS, macOS, visionOS) based on configuration, and creates a combined target that depends on all platform-specific targets. This allows selective generation of Objective-C bindings for specific platforms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/objc/generator/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(OPENCV_OBJC_TARGET)\n  ocv_add_objc_generated_target(${OPENCV_OBJC_TARGET})\nelse()\n  ocv_add_objc_generated_target(osx)\n  ocv_add_objc_generated_target(ios)\n  ocv_add_objc_generated_target(visionos)\nendif()\n\nadd_custom_target(gen_opencv_objc_source\n    # excluded from all: ALL\n    DEPENDS ${objc_generated_targets}\n)\n```\n\n----------------------------------------\n\nTITLE: OpenCL Implementation in OpenCV 2.x vs 3.x\nDESCRIPTION: Compares OpenCL-aware code between OpenCV 2.x and 3.x versions, showing how the explicit ocl namespace and classes have been replaced with the transparent API using UMat.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/transition_guide/transition_guide.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n// initialization\nVideoCapture vcap(...);\nocl::OclCascadeClassifier fd(\"haar_ff.xml\");\nocl::oclMat frame, frameGray;\nMat frameCpu;\nvector<Rect> faces;\nfor(;;){\n    // processing loop\n    vcap >> frameCpu;\n    frame = frameCpu;\n    ocl::cvtColor(frame, frameGray, BGR2GRAY);\n    ocl::equalizeHist(frameGray, frameGray);\n    fd.detectMultiScale(frameGray, faces, ...);\n    // draw rectangles …\n    // show image …\n}\n```\n\nLANGUAGE: cpp\nCODE:\n```\n// initialization\nVideoCapture vcap(...);\nCascadeClassifier fd(\"haar_ff.xml\");\nUMat frame, frameGray; // the only change from plain CPU version\nvector<Rect> faces;\nfor(;;){\n    // processing loop\n    vcap >> frame;\n    cvtColor(frame, frameGray, BGR2GRAY);\n    equalizeHist(frameGray, frameGray);\n    fd.detectMultiScale(frameGray, faces, ...);\n    // draw rectangles …\n    // show image …\n}\n```\n\n----------------------------------------\n\nTITLE: Asset Copy Configuration for Tutorials\nDESCRIPTION: Sets up custom commands to copy JavaScript assets and Haar cascade files to the tutorial directory, creating necessary dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_20\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(f ${js_assets})\n    get_filename_component(fname \"${f}\" NAME)\n    add_custom_command(OUTPUT \"${opencv_tutorial_html_dir}/${fname}\"\n                       COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${f}\" \"${opencv_tutorial_html_dir}/${fname}\"\n                       DEPENDS \"${f}\"\n                       COMMENT \"Copying ${fname}\"\n    )\n    list(APPEND js_tutorials_assets_deps \"${f}\" \"${opencv_tutorial_html_dir}/${fname}\")\n  endforeach()\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Emscripten Version (2.0.10) - Bash\nDESCRIPTION: Updates the Emsdk, installs version 2.0.10 of Emscripten, and activates that specific version for future builds. Use this for compatibility and webassembly feature verification. Dependency: Emscripten SDK. No parameters needed. Outputs: Emscripten 2.0.10 environment setup.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./emsdk update\\n./emsdk install 2.0.10\\n./emsdk activate 2.0.10\n```\n\n----------------------------------------\n\nTITLE: Disabling Compiler Warnings for Zlib in CMake\nDESCRIPTION: Disables specific compiler warnings that are triggered by Zlib code but can be safely ignored. This improves build output clarity by reducing unnecessary warnings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nocv_warnings_disable(CMAKE_C_FLAGS -Wshorten-64-to-32 -Wattributes -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wshift-negative-value\n    -Wundef  # _LFS64_LARGEFILE is not defined\n    /wd4267  # MSVS 2015 (x64) + zlib 1.2.11\n    -Wimplicit-fallthrough\n    /wd4244  # MSVS + zlib 1.2.12: warning C4244: '=': conversion from 'ush' to 'uchf', possible loss of data\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Display Window for Threshold Output (C++)\nDESCRIPTION: This C++ snippet creates an OpenCV highgui window with namedWindow for displaying the output image after thresholding. It sets up the GUI environment required for user interaction. Requires OpenCV highgui module. No parameters; creates a window titled 'Threshold Demo'.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n// [window]\\nnamedWindow( window_name, WINDOW_AUTOSIZE );\\n// [window]\n```\n\n----------------------------------------\n\nTITLE: Setting Up EMSDK Environment Variables - Bash\nDESCRIPTION: Sources the EMSDK environment script to set environment variables and then echoes the EMSDK path. This step ensures that Emscripten tools are accessible in the current shell session. Dependency: Emscripten SDK must be installed. Parameters: none. Prints the EMSDK environment variable as confirmation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsource ./emsdk_env.sh\\necho ${EMSDK}\n```\n\n----------------------------------------\n\nTITLE: Applying Sobel Operator in Java\nDESCRIPTION: This Java snippet provides an implementation of the Sobel operator using OpenCV to detect edges in an input image. It involves loading the image, applying Gaussian blur for noise reduction, converting to grayscale, and then computing the gradient using Sobel. The snippet outputs an edge-highlighted image and requires OpenCV and a sample image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/sobel_derivatives/sobel_derivatives.markdown#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nMat image = Imgcodecs.imread(\"lena.jpg\"); // Load source image\n```\n\nLANGUAGE: java\nCODE:\n```\nImgproc.GaussianBlur(src, src, new Size(3, 3), 0, 0, Core.BORDER_DEFAULT); // Reduce noise\n```\n\nLANGUAGE: java\nCODE:\n```\nImgproc.cvtColor(src, src_gray, Imgproc.COLOR_BGR2GRAY); // Convert to grayscale\n```\n\nLANGUAGE: java\nCODE:\n```\nImgproc.Sobel(src_gray, grad_x, CvType.CV_16S, 1, 0, 3, scale, delta, Core.BORDER_DEFAULT); // Sobel for x-gradient\nImgproc.Sobel(src_gray, grad_y, CvType.CV_16S, 0, 1, 3, scale, delta, Core.BORDER_DEFAULT); // Sobel for y-gradient\n```\n\nLANGUAGE: java\nCODE:\n```\nCore.convertScaleAbs(grad_x, abs_grad_x); \nCore.convertScaleAbs(grad_y, abs_grad_y);\n```\n\nLANGUAGE: java\nCODE:\n```\nCore.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0, grad); // Approximate the gradient\n```\n\nLANGUAGE: java\nCODE:\n```\nHighGui.imshow(\"Sobel Demo\", grad); // Display results\nHighGui.waitKey();\n```\n\n----------------------------------------\n\nTITLE: Using Median Blurring for Noise Reduction in C++\nDESCRIPTION: Details the use of cv.medianBlur() to remove noise from images, effective against salt-and-pepper noise. The median filter replaces the center pixel with the median from the kernel area. Requires OpenCV and parameters such as source image and kernel size, which must be a positive odd integer.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_filtering/js_filtering.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\ncv::medianBlur(src, dst, ksize);\n```\n\n----------------------------------------\n\nTITLE: Calibrating the Camera with OpenCV in Python\nDESCRIPTION: This snippet demonstrates how to calibrate a camera using the collected object and image points using OpenCV. The cv.calibrateCamera() function computes the camera matrix, distortion coefficients, and others elements needed. Input consists of object and image points; output includes values like mtx and dist.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_calib3d/py_calibration/py_calibration.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n```\n\n----------------------------------------\n\nTITLE: Customizing CMake Hook Scripts Directory for OpenCV (CMake)\nDESCRIPTION: This setting allows specifying a directory containing custom CMake hook scripts. OpenCV will include scripts from this directory at key stages during the configuration process. Files should follow naming conventions like CMAKE_INIT.cmake, PRE_CMAKE_BOOTSTRAP.cmake, etc. Set using -DOPENCV_CMAKE_HOOKS_DIR=your/hooks/dir.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_28\n\nLANGUAGE: cmake\nCODE:\n```\nOPENCV_CMAKE_HOOKS_DIR\n```\n\n----------------------------------------\n\nTITLE: Copying Mat Data Using clone() in OpenCV in Java\nDESCRIPTION: Shows how to create a deep copy of an image Mat in Java using clone(). The new Mat owns its own storage and modifications do not affect the original. Always use for value semantics or if the original may be deallocated.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\nMat copy = img.clone();\n```\n\n----------------------------------------\n\nTITLE: Loading and Setting Up for Generalized Hough Transform in C++\nDESCRIPTION: This code snippet initializes the variables and loads the image and template necessary for running the Generalized Hough Transform using OpenCV. The position vectors store the detected matches with four key parameters related to object location, scale, and orientation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/generalized_hough_ballard_guil/generalized_hough_ballard_guil.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n# Load image, template and setup variables\n@snippet samples/cpp/tutorial_code/ImgTrans/generalizedHoughTransform.cpp generalized-hough-transform-load-and-setup\n```\n\n----------------------------------------\n\nTITLE: Creating Display Window for Threshold Output (Python)\nDESCRIPTION: This Python snippet creates a named window using cv.namedWindow to display thresholded images via OpenCV's GUI utilities. Needs to be called prior to setting up trackbars and display. Requires cv2. No parameters.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# [window]\\ncv.namedWindow(window_name)\\n# [window]\n```\n\n----------------------------------------\n\nTITLE: Memory Configuration Block\nDESCRIPTION: Configures reduced memory settings by defining buffer sizes and memory limitations\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\nif(WITH_REDUCED_MEM)\n    add_definitions(-DHASH_SIZE=32768u -DGZBUFSIZE=8192 -DNO_LIT_MEM)\n    message(STATUS \"Configured for reduced memory environment\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Placeholder Installation Function (Standalone Build) in CMake\nDESCRIPTION: Defines an empty `ocv_install_example_src` function within the standalone build context. This serves as a no-op, effectively disabling the source installation behavior defined for the integrated build, as installation is not typically managed by the standalone sample build script itself.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ocv_install_example_src)\n  # not used in this branch\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Setting JPEG Colorspace and Related Parameters (C)\nDESCRIPTION: This function sets the JPEG file's colorspace and other color-space-dependent parameters. It should be called before setting individual parameters if you plan to change the colorspace.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_21\n\nLANGUAGE: C\nCODE:\n```\njpeg_set_colorspace (j_compress_ptr cinfo, J_COLOR_SPACE colorspace)\n```\n\n----------------------------------------\n\nTITLE: Adding DNN Module and Language Bindings - CMake\nDESCRIPTION: This adds the DNN module to the OpenCV build and enables bindings for Python, Java, Objective-C, and JavaScript. 'ocv_add_module' specifies the module dependencies (opencv_core, opencv_imgproc) and sets up language wrappers for cross-language support. Parameters list module dependencies and languages to generate bindings for.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nocv_add_module(dnn opencv_core opencv_imgproc WRAP python java objc js)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Layer Interface OpenCV C++\nDESCRIPTION: This snippet shows the interface for defining a custom layer in OpenCV by deriving from cv::dnn::Layer.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers.md#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n@snippet dnn/custom_layers.hpp A custom layer interface\n```\n\n----------------------------------------\n\nTITLE: Overloading and Renaming Functions for Python Bindings with OpenCV Macros in C++\nDESCRIPTION: This set of function declarations demonstrates how to export overloaded functions to Python, each with a unique Python-exposed name using CV_EXPORTS_W and CV_EXPORTS_AS. The functions perform variations on integral image computations, with each overload exposed under a distinct name. Inputs include images and optional output arrays; the macros ensure each function is accessible by name in Python. The snippet shows how argument defaults are handled.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_bindings/py_bindings_basics/py_bindings_basics.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n//! computes the integral image\nCV_EXPORTS_W void integral( InputArray src, OutputArray sum, int sdepth = -1 );\n\n//! computes the integral image and integral for the squared image\nCV_EXPORTS_AS(integral2) void integral( InputArray src, OutputArray sum,\n                                        OutputArray sqsum, int sdepth = -1, int sqdepth = -1 );\n\n//! computes the integral image, integral for the squared image and the tilted integral image\nCV_EXPORTS_AS(integral3) void integral( InputArray src, OutputArray sum,\n                                        OutputArray sqsum, OutputArray tilted,\n                                        int sdepth = -1, int sqdepth = -1 );\n```\n\n----------------------------------------\n\nTITLE: Predicate for Filtering Values Greater Than Zero\nDESCRIPTION: A simple predicate functor that returns true for values greater than zero, used for filtering operations with Thrust algorithms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-thrust-interop/gpu_thrust_interop.markdown#2025-04-22_snippet_6\n\nLANGUAGE: CUDA\nCODE:\n```\nstruct is_greater : public thrust::unary_function<float, bool>\n{\n    __host__ __device__\n    bool operator()(const float &x) const\n    {\n        return x > 0;\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Saving HDR and LDR Results in OpenCV\nDESCRIPTION: This code saves the results of the HDR processing. The HDR image is saved in Radiance format (.hdr), while the tonemapped LDR and fusion results are saved as standard 8-bit images.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/hdr_imaging.markdown#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nimwrite(\"ldr.jpg\", ldr_8bit);\nimwrite(\"fusion.jpg\", fusion_8bit);\nimwrite(\"hdr.hdr\", hdr);\n```\n\nLANGUAGE: java\nCODE:\n```\nImgcodecs.imwrite(\"ldr.jpg\", ldr8bit);\nImgcodecs.imwrite(\"fusion.jpg\", fusion8bit);\nImgcodecs.imwrite(\"hdr.hdr\", hdr);\n```\n\nLANGUAGE: python\nCODE:\n```\ncv.imwrite('ldr.jpg', ldr_8bit)\ncv.imwrite('fusion.jpg', fusion_8bit)\ncv.imwrite('hdr.hdr', hdr)\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenCV Installation on macOS using Python\nDESCRIPTION: This Python command imports OpenCV and prints its version to verify the installation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/macos_install/macos_install.markdown#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython3 -c \"import cv2; print(cv2.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV.js with WebAssembly SIMD Optimization - Bash\nDESCRIPTION: This bash command builds the WebAssembly version of OpenCV.js with SIMD optimization enabled. It requires Emscripten's LLVM upstream backend, Python, and the OpenCV JS build script. '--simd' enables SIMD instructions for better performance, but this feature is experimental and requires enabling SIMD support flags in browsers or Node.js. Outputs are optimized builds that may not work on all environments; using the latest browser or Node.js is recommended.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nemcmake python ./opencv/platforms/js/build_js.py build_js --build_wasm --simd\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Build with CMake for Ninja\nDESCRIPTION: Command to configure OpenCV build using CMake, generating Ninja build files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncmake -G Ninja ../opencv\n```\n\n----------------------------------------\n\nTITLE: Invoking detect_board_charuco.cpp with Command Line Parameters - OpenCV C++\nDESCRIPTION: This snippet shows example command line parameters for running the 'detect_board_charuco.cpp' sample provided in OpenCV. It uses the CommandLineParser utility to specify board dimensions, marker sizes, dictionary, and optionally the location of an input image and a camera calibration file. This enables flexible execution for various board and camera configurations. All parameters are passed as command-line options and must be formatted as shown.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_detection/charuco_detection.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n-w=5 -h=7 -sl=0.04 -ml=0.02 -d=10 -v=/path_to_opencv/opencv/doc/tutorials/objdetect/charuco_detection/images/choriginal.jpg\n```\n\nLANGUAGE: cpp\nCODE:\n```\n-w=5 -h=7 -sl=0.04 -ml=0.02 -d=10\n-v=/path_to_opencv/opencv/doc/tutorials/objdetect/charuco_detection/images/choriginal.jpg\n-c=/path_to_opencv/opencv/samples/cpp/tutorial_code/objectDetection/tutorial_camera_charuco.yml\n```\n\n----------------------------------------\n\nTITLE: Configuring Face Detection Android Example with CMake\nDESCRIPTION: Configures an Android face detection example project by downloading the required ONNX model and setting up the Android project with OpenCV dependencies. The script downloads a YuNet face detection model and configures the Android project with appropriate SDK targets and library dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/face-detection/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(sample example-face-detection)\n\nocv_download(FILENAME \"face_detection_yunet_2023mar.onnx\"\n             HASH \"4ae92eeb150c82ce15ac80738b3b8167\"\n             URL\n               \"${OPENCV_FACE_DETECT_YN_URL}\"\n               \"$ENV{OPENCV_FACE_DETECT_YN_URL}\"\n               \"https://media.githubusercontent.com/media/opencv/opencv_zoo/main/models/face_detection_yunet/face_detection_yunet_2023mar.onnx\"\n             DESTINATION_DIR \"${CMAKE_CURRENT_LIST_DIR}/res/raw\"\n             ID OPENCV_FACE_DETECT_YN\n             STATUS res)\n\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\" LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\" SDK_TARGET 11 \"${ANDROID_SDK_TARGET}\")\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Filling OpenCV Mat with Random Values in C++\nDESCRIPTION: Shows how to fill a matrix with random values using the randu() function. Requires specifying lower and upper limits for the random values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n// fill with random values\nMat R = Mat(3, 2, CV_8UC3);\nrandu(R, Scalar::all(0), Scalar::all(255));\n```\n\n----------------------------------------\n\nTITLE: Importing OpenCV in Prefix Header (Objective-C)\nDESCRIPTION: This snippet shows how to import the OpenCV library in the project's prefix header file. It's important to include OpenCV before UIKit and Foundation to avoid macro conflicts.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/video_processing/video_processing.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Objective-C\nCODE:\n```\n#import <Availability.h>\n\n#ifndef __IPHONE_4_0\n#warning \"This project uses features only available in iOS SDK 4.0 and later.\"\n#endif\n\n#ifdef __cplusplus\n#import <opencv2/opencv.hpp>\n#endif\n\n#ifdef __OBJC__\n    #import <UIKit/UIKit.h>\n    #import <Foundation/Foundation.h>\n#endif\n```\n\n----------------------------------------\n\nTITLE: Running Segmentation Evaluation for DeepLab MobileNet model\nDESCRIPTION: Command to run the evaluation pipeline on the PASCAL VOC dataset for the DeepLab MobileNet segmentation model. The evaluation compares TensorFlow and OpenCV DNN implementations, measuring pixel accuracy, mean IoU, and inference time.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.tf.segmentation.py_to_py_segm\n```\n\n----------------------------------------\n\nTITLE: Displaying Help for gen_pattern.py Script (Shell)\nDESCRIPTION: This shell command executes the Python script `gen_pattern.py` using the `python` interpreter and passes the `--help` argument. This is a standard way to request usage instructions and available command-line options for a script, in this case, for generating various SVG calibration patterns.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/pattern_tools/README.txt#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython gen_pattern.py --help\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Documentation, Tests, and Examples Build using CMake (Shell)\nDESCRIPTION: Instructs CMake to build the OpenCV documentation (`BUILD_DOCS=ON`) while disabling the compilation of tests (`BUILD_TESTS=OFF`), performance tests (`BUILD_PERF_TESTS=OFF`), and examples (`BUILD_EXAMPLES=OFF`). This can save build time if these components are not needed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\ncmake -D BUILD_DOCS=ON -D BUILD_TESTS=OFF -D BUILD_PERF_TESTS=OFF -D BUILD_EXAMPLES=OFF ..\n```\n\n----------------------------------------\n\nTITLE: Initializing ORB Feature Detector and Descriptor Extractor in C++\nDESCRIPTION: Instantiates and configures the RobustMatcher with ORB feature detector and descriptor extractor for efficient feature extraction. Describes creating detectors and extractors, setting them in a RobustMatcher instance.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nRobustMatcher rmatcher;                                                          // instantiate RobustMatcher\n\ncv::FeatureDetector * detector = new cv::OrbFeatureDetector(numKeyPoints);       // instantiate ORB feature detector\ncv::DescriptorExtractor * extractor = new cv::OrbDescriptorExtractor();          // instantiate ORB descriptor extractor\n\nrmatcher.setFeatureDetector(detector);                                           // set feature detector\nrmatcher.setDescriptorExtractor(extractor);                                      // set descriptor extractor\n```\n\n----------------------------------------\n\nTITLE: PyTorch Model Inference\nDESCRIPTION: This Python snippet demonstrates inference with the original PyTorch ResNet-50 model. It prepares the preprocessed image, invokes the model for prediction, and extracts information like class ID and prediction confidence.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\noriginal_net.eval()\npreproc_img = torch.FloatTensor(preproc_img)\n\nout = original_net(preproc_img)\nprint(\"\\nPyTorch model prediction: \\n\")\nprint(\"* shape: \", out.shape)\nimagenet_class_id = torch.argmax(out, axis=1).item()\nprint(\"* class ID: {}, label: {}\".format(imagenet_class_id, imagenet_labels[imagenet_class_id]))\nconfidence = out[0][imagenet_class_id]\nprint(\"* confidence: {:.4f}\".format(confidence.item()))\n```\n\n----------------------------------------\n\nTITLE: Defining Highgui Source Files in CMake\nDESCRIPTION: Sets the CMake variable `highgui_srcs` to a list containing the paths to the core source files (`backend.cpp`, `window.cpp`, `roiSelector.cpp`) for the highgui module. This variable will be used when defining the module's target to specify its source files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(highgui_srcs\n    ${CMAKE_CURRENT_LIST_DIR}/src/backend.cpp\n    ${CMAKE_CURRENT_LIST_DIR}/src/window.cpp\n    ${CMAKE_CURRENT_LIST_DIR}/src/roiSelector.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading Source Image in Python\nDESCRIPTION: Loads the source image using cv2.imread based on the path obtained from command-line arguments. Includes error handling to check if the image loading was successful.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#! [load]\n# [load]\nparser = argparse.ArgumentParser(description='Code for Laplace Operator tutorial.')\nparser.add_argument('--input', help='Path to input image.', default='lena.jpg')\nargs = parser.parse_args()\n\nsrc = cv.imread(cv.samples.findFile(args.input), cv.IMREAD_COLOR)\n\n# Check if image is loaded fine\nif src is None:\n    print ('Error opening image')\n    print ('Program Arguments: [image_name -- default lena.jpg]')\n    exit()\n# [load]\n# ! [load]\n```\n\n----------------------------------------\n\nTITLE: Setting OpenJPEG Build Identifier and Status Message in CMake\nDESCRIPTION: Constructs a build identifier string `OPENJPEG_BUILD` incorporating OpenCV and OpenJPEG versions. It appends '-debug' if the build type is Debug and prints a status message indicating the detected OpenJPEG version and the generated build identifier.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENJPEG_BUILD \"opencv-${OPENCV_VERSION}-openjp2-${OPENJPEG_VERSION}\")\nif(CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n  set(OPENJPEG_BUILD \"${OPENJPEG_BUILD}-debug\")\nendif()\n\nmessage(STATUS \"OpenJPEG: VERSION = ${OPENJPEG_VERSION}, BUILD = ${OPENJPEG_BUILD}\")\n```\n\n----------------------------------------\n\nTITLE: Merging Exposures into HDR Image with OpenCV Python\nDESCRIPTION: Merges loaded exposure images into a single HDR image using Debevec and Robertson methods available in OpenCV. The resulting HDR images have a float32 type to retain full dynamic range.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_photo/py_hdr/py_hdr.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Merge exposures to HDR image\nmerge_debevec = cv.createMergeDebevec()\nhdr_debevec = merge_debevec.process(img_list, times=exposure_times.copy())\nmerge_robertson = cv.createMergeRobertson()\nhdr_robertson = merge_robertson.process(img_list, times=exposure_times.copy())\n```\n\n----------------------------------------\n\nTITLE: Loading an Image from File in Grayscale with OpenCV in Java\nDESCRIPTION: Shows loading an image as grayscale with OpenCV's Java interface by supplying Imgcodecs.IMREAD_GRAYSCALE flag. Needs OpenCV Java bindings. Image at input path is loaded as a single-channel Mat. Only one channel (intensity) is present in the result.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nMat img_gray = Imgcodecs.imread(\"my_image.jpg\", Imgcodecs.IMREAD_GRAYSCALE);\n```\n\n----------------------------------------\n\nTITLE: Detecting SIFT Keypoints in OpenCV Python\nDESCRIPTION: This code demonstrates how to create a SIFT object, detect keypoints in a grayscale image, and draw those keypoints on the image. It uses the SIFT_create() method and detect() function to find keypoints.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2 as cv\n\nimg = cv.imread('home.jpg')\ngray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n\nsift = cv.SIFT_create()\nkp = sift.detect(gray,None)\n\nimg=cv.drawKeypoints(gray,kp,img)\n\ncv.imwrite('sift_keypoints.jpg',img)\n```\n\n----------------------------------------\n\nTITLE: Fitting a Line to Contours in Python with OpenCV\nDESCRIPTION: This code shows how to fit a line to a set of points in a contour using cv.fitLine() and then draw the line using cv.line(). It calculates the line parameters and endpoints to draw across the image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nrows,cols = img.shape[:2]\n[vx,vy,x,y] = cv.fitLine(cnt, cv.DIST_L2,0,0.01,0.01)\nlefty = int((-x*vy/vx) + y)\nrighty = int(((cols-x)*vy/vx)+y)\ncv.line(img,(cols-1,righty),(0,lefty),(0,255,0),2)\n```\n\n----------------------------------------\n\nTITLE: Configuring IPP Optimizations\nDESCRIPTION: Sets up Intel IPP (Integrated Performance Primitives) optimization options for specific operations like mean, minmax, and sum calculations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_IPP)\n  OCV_OPTION(OPENCV_IPP_ENABLE_ALL \"Enable all OPENCV_IPP_ options at once\" OFF)\n  OCV_OPTION(OPENCV_IPP_MEAN \"Enable IPP optimizations for mean (+200Kb in binary size)\" OPENCV_IPP_ENABLE_ALL)\n  OCV_OPTION(OPENCV_IPP_MINMAX \"Enable IPP optimizations for minMaxLoc/minMaxIdx (+200Kb in binary size)\" OPENCV_IPP_ENABLE_ALL)\n  OCV_OPTION(OPENCV_IPP_SUM \"Enable IPP optimizations for sum (+100Kb in binary size)\" OPENCV_IPP_ENABLE_ALL)\n```\n\n----------------------------------------\n\nTITLE: Compiling and Running Streaming Pipelines in C++\nDESCRIPTION: This snippet compiles the G-API graph for streaming mode, optimizing the execution for video streams. It involves defining the input source, utilizing start() method to fetch pipeline results, and managing GUI event handling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/face_beautification/face_beautification.cpp str_comp\n```\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/face_beautification/face_beautification.cpp str_src\n```\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/face_beautification/face_beautification.cpp str_loop\n```\n\n----------------------------------------\n\nTITLE: Translating Images using OpenCV\nDESCRIPTION: Shows the translation of images using the cv.warpAffine function by creating a transformation matrix for shifting in the (x,y) direction. It ensures the output image has the same type as the input, allowing pixel position adjustments via the transformation matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_geometric_transformations/js_geometric_transformations.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ncv.warpAffine(src, dst, M, dsize, flags = cv.INTER_LINEAR, borderMode = cv.BORDER_CONSTANT, borderValue = new cv.Scalar())\n```\n\n----------------------------------------\n\nTITLE: Generating OpenCV for Windows Store 8.0 x86 using CMake\nDESCRIPTION: Invokes CMake directly using the Visual Studio 2013 generator to create project files for OpenCV targeting Windows Store 8.0 on the x86 architecture. Specifies the system name (WindowsStore) and older system version (8.0).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Visual Studio 12 2013\" -DCMAKE_SYSTEM_NAME=WindowsStore -DCMAKE_SYSTEM_VERSION=8.0 <path-to-source>\n```\n\n----------------------------------------\n\nTITLE: Setting Default JPEG Quantization Tables (C)\nDESCRIPTION: This function sets default quantization tables with linear q_scale_factor[] values. It's only available in libjpeg v7+ API/ABI emulation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_26\n\nLANGUAGE: C\nCODE:\n```\njpeg_default_qtables (j_compress_ptr cinfo, boolean force_baseline)\n```\n\n----------------------------------------\n\nTITLE: Configuring Protobuf Library Build\nDESCRIPTION: Sets up the static library build configuration for Protobuf, including source files, include directories, and build properties. Also handles platform-specific linking requirements.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/protobuf/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(libprotobuf STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${Protobuf_SRCS})\ntarget_include_directories(libprotobuf SYSTEM PUBLIC $<BUILD_INTERFACE:${PROTOBUF_ROOT}/src>)\nset_target_properties(libprotobuf\n    PROPERTIES\n    FOLDER \"3rdparty\"\n    OUTPUT_NAME libprotobuf\n    DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n    COMPILE_PDB_NAME libprotobuf\n    COMPILE_PDB_NAME_DEBUG \"libprotobuf${OPENCV_DEBUG_POSTFIX}\"\n    ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting up Platform-Specific Target Configuration for Objective-C Bindings\nDESCRIPTION: Determines the target platform (iOS, visionOS, or macOS) based on the build environment and sets the appropriate framework name. This configures the generator to produce platform-specific bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/objc/generator/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(objc_generated_files\n    # \"${OPENCV_OBJC_SIGNATURES_FILE}\"\n)\n\nstring(REPLACE \"opencv_\" \"\" MODULES \"${OPENCV_OBJC_MODULES}\")\n\nif(NOT DEFINED OPENCV_OBJC_TARGET AND APPLE_FRAMEWORK)\n  if(IOS)\n    set(OPENCV_OBJC_TARGET \"ios\")\n  elseif(XROS)\n    set(OPENCV_OBJC_TARGET \"visionos\")\n  else()\n    set(OPENCV_OBJC_TARGET \"osx\")\n  endif()\nendif()\n\nif(NOT DEFINED OPENCV_OBJC_FRAMEWORK_NAME)\n  if(DEFINED FRAMEWORK_NAME)\n    set(OPENCV_OBJC_FRAMEWORK_NAME \"${FRAMEWORK_NAME}\")\n  else()\n    set(OPENCV_OBJC_FRAMEWORK_NAME \"opencv2\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Copying Module-Specific Test Files in CMake for OpenCV Java\nDESCRIPTION: Copies test files specific to each OpenCV module to the Java test directory. It uses ocv_copyfiles_append_dir and ocv_copyfiles_add_target functions to manage the copy process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/pure_test/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nocv_copyfiles_append_dir(JAVA_TEST_SRC_COPY \"${OPENCV_JAVA_BINDINGS_DIR}/gen/test\" \"${OPENCV_JAVA_TEST_DIR}/src\")\n\nlist(APPEND depends gen_opencv_java_source \"${OPENCV_DEPHELPER}/gen_opencv_java_source\")\nocv_copyfiles_add_target(${the_module}_test_source_copy JAVA_TEST_SRC_COPY \"Copy Java(Test) source files\" ${depends})\nset(depends ${the_module}_test_source_copy \"${OPENCV_DEPHELPER}/${the_module}_test_source_copy\")\n```\n\n----------------------------------------\n\nTITLE: Waiting for User Exit in Java OpenCV Program\nDESCRIPTION: Code snippet showing how to wait for a user to press a key to exit an OpenCV application in Java. This keeps the result window visible until the user decides to close the application.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_22\n\nLANGUAGE: java\nCODE:\n```\n// Wait until user exit program by pressing a key\nHighGui.waitKey(0);\n```\n\n----------------------------------------\n\nTITLE: Appending CMake Hooks to Module Initialization - CMake\nDESCRIPTION: Appends a module-specific CMake script hook (INIT_MODULE_SOURCES_opencv_dnn.cmake) to perform custom source initialization for the DNN module. Hook file is specified relative to the current directory. Used to extend or override default module initialization behavior.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nocv_cmake_hook_append(INIT_MODULE_SOURCES_opencv_dnn \"${CMAKE_CURRENT_LIST_DIR}/cmake/hooks/INIT_MODULE_SOURCES_opencv_dnn.cmake\")\n```\n\n----------------------------------------\n\nTITLE: Finishing JPEG Compression Cycle in C\nDESCRIPTION: This snippet shows how to complete a JPEG compression cycle using 'jpeg_finish_compress'. It ensures the final data buffer is written and releases memory. This is critical for ensuring that all data is successfully saved and to free resources. This step follows the writing of scanlines.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_8\n\nLANGUAGE: C\nCODE:\n```\njpeg_finish_compress(&cinfo);\n```\n\n----------------------------------------\n\nTITLE: Main Loop for Advanced Shape Drawing in OpenCV with Python\nDESCRIPTION: This snippet shows the main loop for the advanced drawing application. It sets up the window, binds the mouse callback function, and handles keyboard input to toggle between drawing modes.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_mouse_handling/py_mouse_handling.markdown#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimg = np.zeros((512,512,3), np.uint8)\ncv.namedWindow('image')\ncv.setMouseCallback('image',draw_circle)\n\nwhile(1):\n    cv.imshow('image',img)\n    k = cv.waitKey(1) & 0xFF\n    if k == ord('m'):\n        mode = not mode\n    elif k == 27:\n        break\n\ncv.destroyAllWindows()\n```\n\n----------------------------------------\n\nTITLE: Extracting Vertices from RotatedRect in OpenCV.js (JavaScript)\nDESCRIPTION: Illustrates how to extract the four vertices of a RotatedRect using the cv.RotatedRect.points function. The input is a RotatedRect structure; the output is an array of Point objects representing the corners. Requires OpenCV.js and a valid RotatedRect.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_some_data_structures/js_image_arithmetics.markdown#2025-04-22_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet vertices = cv.RotatedRect.points(rotatedRect);\nlet point1 = vertices[0];\nlet point2 = vertices[1];\nlet point3 = vertices[2];\nlet point4 = vertices[3];\n```\n\n----------------------------------------\n\nTITLE: Generating OpenCV for Windows Phone 8.0 ARM using CMake\nDESCRIPTION: Invokes CMake directly using the Visual Studio 2013 ARM generator to create project files for OpenCV targeting Windows Phone 8.0 on the ARM architecture. Specifies the system name and older system version (8.0).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Visual Studio 12 2013 ARM\" -DCMAKE_SYSTEM_NAME=WindowsPhone -DCMAKE_SYSTEM_VERSION=8.0 <path-to-source>\n```\n\n----------------------------------------\n\nTITLE: Initializing SURF and Detecting Keypoints in OpenCV Python\nDESCRIPTION: This snippet demonstrates how to create a SURF object, set the Hessian threshold, and detect keypoints and descriptors in an image using OpenCV's SURF implementation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> img = cv.imread('fly.png', cv.IMREAD_GRAYSCALE)\n\n# Create SURF object. You can specify params here or later.\n# Here I set Hessian Threshold to 400\n>>> surf = cv.xfeatures2d.SURF_create(400)\n\n# Find keypoints and descriptors directly\n>>> kp, des = surf.detectAndCompute(img,None)\n\n>>> len(kp)\n 699\n```\n\n----------------------------------------\n\nTITLE: Setting Image to Black with OpenCV in C++\nDESCRIPTION: Shows how to set all pixels to zero (black) for a cv::Mat in C++. cv::Mat::setTo fills the entire image. Input is a Mat instance; after operation, all channels are 0.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_28\n\nLANGUAGE: C++\nCODE:\n```\nimg.setTo(cv::Scalar(0));\n```\n\n----------------------------------------\n\nTITLE: Checking ADE Dependency and Disabling G-API Module in CMake\nDESCRIPTION: Verifies if the 'ade' (Adequacy) target, a prerequisite for G-API, exists. If the 'ade' target is not found, it disables the G-API module using `ocv_module_disable` and stops further processing for this module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT TARGET ade)\n  # can't build G-API because of the above reasons\n  ocv_module_disable(gapi)\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Computing Bounding Rectangle from RotatedRect in OpenCV.js (JavaScript)\nDESCRIPTION: Shows how to compute the axis-aligned bounding rectangle for a given RotatedRect using cv.RotatedRect.boundingRect. Accepts a RotatedRect and returns a Rect that fully contains it. The input must be a valid RotatedRect; OpenCV.js is required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_core/js_some_data_structures/js_image_arithmetics.markdown#2025-04-22_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\nlet boundingRect = cv.RotatedRect.boundingRect(rotatedRect);\n```\n\n----------------------------------------\n\nTITLE: Loading Source Image in Java\nDESCRIPTION: Loads the source image from the path provided as a command-line argument using Imgcodecs.imread. Includes basic error handling if the image fails to load.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n//! [load]\nString imagePath = (args.length > 0) ? args[0] : \"../../data/lena.jpg\";\nsrc = Imgcodecs.imread(imagePath, Imgcodecs.IMREAD_COLOR);\nif (src.empty()) {\n    System.out.println(\"Error opening image\");\n    System.out.println(\"Program Arguments: [image_path -- default ../../data/lena.jpg]\");\n    System.exit(-1);\n}\n//! [load]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Build Type and Installation Prefix using CMake (Shell)\nDESCRIPTION: Sets the CMake build type to 'RELEASE' for optimization and specifies '/usr/local' as the installation directory using the `CMAKE_BUILD_TYPE` and `CMAKE_INSTALL_PREFIX` flags. The '..' indicates the source directory is one level up from the current build directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ncmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local ..\n```\n\n----------------------------------------\n\nTITLE: Running Robust Descriptor Matcher with Parameters in Command Line\nDESCRIPTION: Executes a C++ tutorial program with specific parameters for descriptor matching, including ratio test and fast matcher options, exemplifying command-line control over the matching process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/real_time_pose/real_time_pose.markdown#2025-04-22_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n./cpp-tutorial-pnp_detection --ratio=0.8 --keypoints=1000 --fast=false\n```\n\n----------------------------------------\n\nTITLE: Perspective Transformation with OpenCV\nDESCRIPTION: Shows perspective transformation using cv.getPerspectiveTransform to find a 3x3 matrix with four points mapping between the input and output image. The cv.warpPerspective function applies the transformation, maintaining straight lines.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_geometric_transformations/js_geometric_transformations.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\ncv.warpPerspective(src, dst, M, dsize, flags = cv.INTER_LINEAR, borderMode = cv.BORDER_CONSTANT, borderValue = new cv.Scalar())\n```\n\nLANGUAGE: C++\nCODE:\n```\ncv.getPerspectiveTransform(src, dst)\n```\n\n----------------------------------------\n\nTITLE: Verifying EMSCRIPTEN Path with emcmake - Bash\nDESCRIPTION: Runs a subshell using emcmake to echo the EMSCRIPTEN environment variable, verifying Emscripten setup. Dependency: emcmake launcher must be available in the path. Parameters: none. Output: prints location of the EMSCRIPTEN installation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nemcmake sh -c 'echo ${EMSCRIPTEN}'\n```\n\n----------------------------------------\n\nTITLE: Creating Thrust Begin Iterator for GpuMat\nDESCRIPTION: Defines a function to create a Thrust iterator that points to the beginning of a GpuMat. This iterator properly handles the pitched memory layout of the matrix.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gpu/gpu-thrust-interop/gpu_thrust_interop.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ntemplate<typename T>\nthrust::transform_iterator<step_functor<T>, thrust::counting_iterator<int> > begin_itr(const cv::cuda::GpuMat& mat)\n{\n    return thrust::make_transform_iterator(\n        thrust::counting_iterator<int>(0),\n        step_functor<T>(mat.cols, mat.rows, mat.step / sizeof(T))\n    );\n}\n```\n\n----------------------------------------\n\nTITLE: Rotating Images with OpenCV\nDESCRIPTION: Demonstrates image rotation using OpenCV's cv.getRotationMatrix2D function, which provides rotated matrices that support arbitrary angles and scales. It allows focusing on a specified center for rotation, with matrices adjusted for scaling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_geometric_transformations/js_geometric_transformations.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ncv.getRotationMatrix2D(center, angle, scale)\n```\n\n----------------------------------------\n\nTITLE: Reducing Noise with Gaussian Blur in Python\nDESCRIPTION: Applies a 3x3 Gaussian blur to the source image using cv2.GaussianBlur to reduce noise before edge detection. Requires cv2 (OpenCV Python bindings).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#! [reduce_noise]\n# [reduce_noise]\n# Reduce noise by blurring with a Gaussian filter ( kernel size = 3 )\nsrc = cv.GaussianBlur( src, (3, 3), 0, 0, cv.BORDER_DEFAULT )\n# [reduce_noise]\n# ! [reduce_noise]\n```\n\n----------------------------------------\n\nTITLE: Using Memory Manager for Automatic Structure Allocation in libjpeg-turbo - C\nDESCRIPTION: This snippet demonstrates how to use the memory manager in libjpeg-turbo to allocate structures that will automatically be freed during various JPEG processing stages. To allocate memory, the alloc_small method of the memory manager is called, which is particularly handy for structures smaller than a few kilobytes. Larger structures can use alloc_large. Prerequisites include initializing the JPEG compression or decompression object (cinfo).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_66\n\nLANGUAGE: C\nCODE:\n```\nptr = (*cinfo->mem->alloc_small) ((j_common_ptr)cinfo, JPOOL_IMAGE, size);\n```\n\n----------------------------------------\n\nTITLE: Saving an Image to File with OpenCV in Java\nDESCRIPTION: Demonstrates how to export a Mat to an image file in Java using Imgcodecs.imwrite. OpenCV Java bindings required. Input is filename and image Mat; output is boolean for success. Format is auto-detected from extension.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_operations.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nImgcodecs.imwrite(\"my_image_copy.png\", img);\n```\n\n----------------------------------------\n\nTITLE: Copying OpenCV JAR and Build Files in CMake for Java Tests\nDESCRIPTION: Copies the OpenCV JAR file and build.xml to the test directory. It uses add_custom_command to ensure the JAR is copied after it's generated and to copy the build.xml file.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/pure_test/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_command(OUTPUT \"${OPENCV_JAVA_TEST_DIR}/bin/${JAR_NAME}\"\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${OPENCV_JAR_FILE}\" \"${OPENCV_JAVA_TEST_DIR}/bin/${JAR_NAME}\"\n    DEPENDS \"${OPENCV_JAR_FILE}\" \"${OPENCV_DEPHELPER}/${the_module}_jar\"\n    COMMENT \"Copying the OpenCV jar\"\n)\n\nadd_custom_command(OUTPUT \"${OPENCV_JAVA_TEST_DIR}/build.xml\"\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/build.xml\" \"${OPENCV_JAVA_TEST_DIR}/build.xml\"\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/build.xml\"\n    COMMENT \"Copying build.xml\"\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Bilateral Filter with OpenCV in Python\nDESCRIPTION: In this Python snippet, the bilateralFilter() function from OpenCV applies a bilateral filter for smoothing while maintaining edges. Dependencies include OpenCV, and parameters include diameter, and standard deviations for color and space.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.markdown#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n@snippet samples/python/tutorial_code/imgProc/Smoothing/smoothing.py bilateralfilter\n```\n\n----------------------------------------\n\nTITLE: Declaring Global Variables for Template Matching (C++)\nDESCRIPTION: Declares global variables used in the C++ template matching demo, including `Mat` objects for the input image, template, result map, and display image. Also includes variables for window names and the selected matching method.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp declare\n```\n\n----------------------------------------\n\nTITLE: Calculating Extent using OpenCV.js\nDESCRIPTION: Computes the extent of a contour, which is the ratio of the contour's area to the area of its bounding rectangle. Requires a contour (`cnt`). Uses `cv.contourArea` for the contour area and `cv.boundingRect` to calculate the bounding rectangle area.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contour_properties/js_contour_properties.markdown#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nlet area = cv.contourArea(cnt, false);\nlet rect = cv.boundingRect(cnt));\nlet rectArea = rect.width * rect.height;\nlet extent = area / rectArea;\n```\n\n----------------------------------------\n\nTITLE: Displaying a cv.Mat on an HTML Canvas (JavaScript)\nDESCRIPTION: Demonstrates how to use the `cv.imshow()` function from OpenCV.js to display the contents of a `cv.Mat` object (`mat`) onto an HTML `<canvas>` element. The canvas is identified by its ID, 'outputCanvas'.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_usage/js_usage.markdown#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\ncv.imshow(\"outputCanvas\", mat);\n```\n\n----------------------------------------\n\nTITLE: Default Preprocessing Configuration\nDESCRIPTION: Configuration constants for default image preprocessing parameters including scaling, dimensions, and normalization values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nBASE_IMG_SCALE_FACTOR = 1 / 255.0\nPYTORCH_RSZ_HEIGHT = 256\nPYTORCH_RSZ_WIDTH = 256\n\npytorch_resize_input_blob = {\n    \"mean\": [\"123.675\", \"116.28\", \"103.53\"],\n    \"scale\": str(BASE_IMG_SCALE_FACTOR),\n    \"std\": [\"0.229\", \"0.224\", \"0.225\"],\n    \"crop\": \"True\",\n    \"rgb\": \"True\",\n    \"rsz_height\": str(PYTORCH_RSZ_HEIGHT),\n    \"rsz_width\": str(PYTORCH_RSZ_WIDTH)\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Module Sources and Include Directories\nDESCRIPTION: Configures source files, headers, and include directories for the core module, including CUDA, OpenCL, and parallel processing components.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB lib_cuda_hdrs\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/cuda/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/cuda/*.h\")\nfile(GLOB lib_cuda_hdrs_detail\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/cuda/detail/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/cuda/detail/*.h\")\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV Source Repository - Bash\nDESCRIPTION: Uses git to clone the official OpenCV repository. Requires Git to be installed and network access. Only parameter is the repository URL. The output is a local directory containing the OpenCV source code.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/opencv/opencv.git\n```\n\n----------------------------------------\n\nTITLE: Point Polygon Testing in OpenCV Python\nDESCRIPTION: Shows how to find the shortest distance between a point and a contour using cv.pointPolygonTest(). The function returns negative values for points outside the contour, positive for inside, and zero for points on the contour.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contours_more_functions/py_contours_more_functions.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndist = cv.pointPolygonTest(cnt,(50,50),True)\n```\n\n----------------------------------------\n\nTITLE: Applying Histogram Equalization with the Look-up Table\nDESCRIPTION: This single line of code applies the histogram equalization transform to the original image using the previously calculated look-up table. It uses the pixel values of the original image as indices into the CDF array to obtain the equalized values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimg2 = cdf[img]\n```\n\n----------------------------------------\n\nTITLE: Running Complete OpenCV Segmentation Workflow\nDESCRIPTION: Command to reproduce the full OpenCV segmentation workflow using the dnn_model_runner with default image preprocessing settings. This executes the steps described in the 'Model Conversion Pipeline' section.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython -m dnn_model_runner.dnn_conversion.tf.segmentation.py_to_py_segm --test True --default_img_preprocess True --evaluate False\n```\n\n----------------------------------------\n\nTITLE: Enabling Hardware Compression for Levels 1-6 (C)\nDESCRIPTION: This code snippet shows how to enable hardware compression for levels 1-6 by adding a CFLAGS option when building zlib-ng.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/arch/s390/README.md#2025-04-22_snippet_1\n\nLANGUAGE: c\nCODE:\n```\n-DDFLTCC_LEVEL_MASK=0x7e\n```\n\n----------------------------------------\n\nTITLE: Applying Gaussian Blur in OpenCV Python\nDESCRIPTION: This code snippet demonstrates how to apply Gaussian blurring to an image using cv.GaussianBlur(). It uses a 5x5 kernel with automatic sigma calculation, which is effective for removing Gaussian noise.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_filtering/py_filtering.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nblur = cv.GaussianBlur(img,(5,5),0)\n```\n\n----------------------------------------\n\nTITLE: Displaying Objective-C Wrappers Status in CMake\nDESCRIPTION: This CMake snippet checks if Objective-C support (`BUILD_OBJC`) is enabled. If true, it prints a status message using the `status` command, indicating whether Objective-C wrappers (`HAVE_opencv_objc`) were successfully configured based on the value of the `HAVE_opencv_objc` variable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_30\n\nLANGUAGE: cmake\nCODE:\n```\n# ========================== Objective-C =======================\nif(BUILD_OBJC)\n  status(\"\")\n  status(\"  Objective-C wrappers:\" HAVE_opencv_objc                                                       THEN YES ELSE NO)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating Checkerboard Pattern using Python Script (Shell)\nDESCRIPTION: Executes the 'gen_pattern.py' script to create a checkerboard pattern SVG file named 'chessboard.svg'. The pattern will have 9 rows, 6 columns, and each square will have a size of 20mm. Requires Python and the 'gen_pattern.py' script.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration_pattern/camera_calibration_pattern.markdown#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython gen_pattern.py -o chessboard.svg --rows 9 --columns 6 --type checkerboard --square_size 20\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for OpenCL Samples in OpenCV\nDESCRIPTION: This CMake script checks for the necessary version of CMake and required OpenCV and OpenCL dependencies, and sets up project configuration for building OpenCL samples. Dependencies include opencv_core, opencv_imgproc, and others. The script uses find_package to locate OpenCL and configures the environment to compile and link OpenCL sample projects. If dependencies are not met, it exits early.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/opencl/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nocv_install_example_src(opencl *.cpp *.hpp CMakeLists.txt)\n\n# cmake 3.1 needed for find_package(OpenCL)\nif(CMAKE_VERSION VERSION_LESS \"3.1\")\n  message(STATUS \"OpenCL samples require CMakes 3.1+\")\n  return()\nendif()\n\nset(OPENCV_OPENCL_SAMPLES_REQUIRED_DEPS\n  opencv_core\n  opencv_imgproc\n  opencv_video\n  opencv_imgcodecs\n  opencv_videoio\n  opencv_highgui)\nocv_check_dependencies(${OPENCV_OPENCL_SAMPLES_REQUIRED_DEPS})\n\nif(NOT BUILD_EXAMPLES OR NOT OCV_DEPENDENCIES_FOUND)\n  return()\nendif()\n\nfind_package(OpenCL 1.2 QUIET)\nif(NOT OpenCL_FOUND)\n  message(STATUS \"OpenCL samples are skipped: OpenCL SDK is required\")\n  return()\nendif()\n\nproject(opencl_samples)\nocv_include_modules_recurse(${OPENCV_OPENCL_SAMPLES_REQUIRED_DEPS})\nocv_include_directories(${OpenCL_INCLUDE_DIR})\nfile(GLOB all_samples RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} *.cpp)\nforeach(sample_filename ${all_samples})\n  ocv_define_sample(tgt ${sample_filename} opencl)\n  ocv_target_link_libraries(${tgt} PRIVATE\n    ${OPENCV_LINKER_LIBS}\n    ${OPENCV_OPENCL_SAMPLES_REQUIRED_DEPS}\n    ${OpenCL_LIBRARY})\nendforeach()\n\n```\n\n----------------------------------------\n\nTITLE: Trackbar Callback in OpenCV Java\nDESCRIPTION: Creates a trackbar for an OpenCV window and associates a callback function to it in Java, responding to state changes. Requires OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\nImgproc.createTrackbar(\"Trackbar\", \"Source\", sliderValue, maxValue, on_trackbar);\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for WINCE versions earlier than 8.0\nDESCRIPTION: Example configuration parameters for targeting WINCE versions earlier than 8.0. Specifies system version, toolset, and processor architecture for the build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/wince/readme.md#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n-DCMAKE_SYSTEM_VERSION=7.0 -DCMAKE_GENERATOR_TOOLSET=CE700 -DCMAKE_SYSTEM_PROCESSOR=arm-v4\n```\n\n----------------------------------------\n\nTITLE: Displaying Results in C++\nDESCRIPTION: C++ snippet displaying the original source image and the generated histogram image in separate windows using `cv::imshow`. It then waits indefinitely for a key press using `cv::waitKey` before exiting.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.markdown#2025-04-22_snippet_32\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet samples/cpp/tutorial_code/Histograms_Matching/calcHist_Demo.cpp Display\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Stereo Block Matching Algorithm Steps\nDESCRIPTION: Pseudo-algorithm showing the steps for implementing stereo block matching across multiple GPUs, demonstrating how to split processing across devices for improved performance.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/doc/cuda.markdown#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n1. Split each image of the stereo pair into two horizontal overlapping stripes.\n2. Process each pair of stripes (from the left and right images) on a separate Fermi* GPU.\n3. Merge the results into a single disparity map.\n```\n\n----------------------------------------\n\nTITLE: Simple Progressive Decoding Loop in C\nDESCRIPTION: Alternative loop structure for progressive JPEG decoding without special final pass processing. Continues until all input is processed and output is complete.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_55\n\nLANGUAGE: c\nCODE:\n```\nfor (;;) {\n    absorb any waiting input by calling jpeg_consume_input()\n    jpeg_start_output(&cinfo, cinfo.input_scan_number);\n    ...\n    jpeg_finish_output()\n    if (jpeg_input_complete(&cinfo) &&\n        cinfo.input_scan_number == cinfo.output_scan_number)\n      break;\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Centroid of Contours in Python with OpenCV\nDESCRIPTION: This code snippet shows how to calculate the centroid of a contour using the moments obtained from cv.moments(). It uses the formulas Cx = M10/M00 and Cy = M01/M00.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.markdown#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncx = int(M['m10']/M['m00'])\ncy = int(M['m01']/M['m00'])\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Color or Intensity using OpenCV.js\nDESCRIPTION: Computes the average color (for multi-channel images) or mean intensity (for grayscale images) of an array (`src`), optionally considering only the region specified by a `mask`. Uses the `cv.mean` function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_contours/js_contour_properties/js_contour_properties.markdown#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nlet average = cv.mean(src, mask);\n```\n\n----------------------------------------\n\nTITLE: Example Interactive Program Startup Notice (GPLv2)\nDESCRIPTION: This is an example of a short notice that an interactive program licensed under the GPLv2 should display upon startup. It includes the program name, version, copyright holder, a clear statement of ABSOLUTELY NO WARRANTY, and mentions that the software is free and redistributable under certain conditions, prompting the user to type specific commands (`show w` and `show c`) for more details.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ittnotify/src/ittnotify/GPL-2.0-only.txt#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nGnomovision version 69, Copyright (C) year name of author Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details.\n```\n\n----------------------------------------\n\nTITLE: Including JNI Subdirectory in CMake\nDESCRIPTION: This command includes the `jni` subdirectory into the CMake build process. This subdirectory is responsible for building the Java Native Interface (JNI) components required for the OpenCV Java bindings. The comment notes that this generates the `${the_module}` target directly.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(jni)  # generates ${the_module} target (${the_module}_jni doesn't work properly with Android non-gradle samples)\n```\n\n----------------------------------------\n\nTITLE: Default Formatting for OpenCV Mat Output in C++\nDESCRIPTION: Demonstrates the default formatting option for printing OpenCV Mat objects to the console using the << operator.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\ncout << \"R (default) = \" << endl << R << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Configuring Library Installation Paths\nDESCRIPTION: Sets up installation directory paths for pkg-config, handling both absolute and relative paths for includes and libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_37\n\nLANGUAGE: cmake\nCODE:\n```\nif(IS_ABSOLUTE \"${CMAKE_INSTALL_INCLUDEDIR}\")\n    set(PC_INC_INSTALL_DIR \"${CMAKE_INSTALL_INCLUDEDIR}\")\nelse()\n    set(PC_INC_INSTALL_DIR \"\\${prefix}/${CMAKE_INSTALL_INCLUDEDIR}\")\nendif()\n\nif(IS_ABSOLUTE \"${CMAKE_INSTALL_LIBDIR}\")\n    set(PC_LIB_INSTALL_DIR \"${CMAKE_INSTALL_LIBDIR}\")\nelse()\n    set(PC_LIB_INSTALL_DIR \"\\${exec_prefix}/${CMAKE_INSTALL_LIBDIR}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: ARM NEON Compatibility Check\nDESCRIPTION: Tests for ARM NEON compatibility and required floating-point ABI settings on 32-bit systems\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/simd/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: c\nCODE:\n```\n#if defined(__ARM_NEON__) || (!defined(__linux__) && !defined(ANDROID) && !defined(__ANDROID__))\n#error \"Neon run-time auto-detection will not be used\"\n#endif\n#if __ARM_PCS_VFP == 1\n#error \"float ABI = hard\"\n#endif\n#if __SOFTFP__ != 1\n#error \"float ABI = softfp\"\n#endif\nint main(void) { return 0; }\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenCV Mat using C/C++ Arrays in C++\nDESCRIPTION: Creates matrices of different dimensions using C/C++ arrays with the Mat constructor. Demonstrates both 2D and 3D matrix initialization with data values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n// Creates a matrix with custom data type and initial values\nint sz[3] = {2,2,2};\nMat L(3,sz, CV_8UC(1), Scalar::all(0));\n```\n\n----------------------------------------\n\nTITLE: Listing Available Mouse Events in OpenCV with Python\nDESCRIPTION: This snippet demonstrates how to list all available mouse events in OpenCV using Python. It uses a list comprehension to filter directory contents of cv module for event-related items.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_mouse_handling/py_mouse_handling.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2 as cv\nevents = [i for i in dir(cv) if 'EVENT' in i]\nprint( events )\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenCV Installation in Python\nDESCRIPTION: The Python snippet is used to verify whether OpenCV has been correctly installed by importing the cv2 library and printing its version. This requires Python and OpenCV to be installed beforehand.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_windows/py_setup_in_windows.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> import cv2 as cv\n>>> print(cv.__version__)\n```\n\n----------------------------------------\n\nTITLE: Command Line Parameters for ArUco Marker Creation\nDESCRIPTION: Code snippet showing the command line parameters for the create_marker.cpp sample. This demonstrates how to specify the output file, dictionary type, and marker ID.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/aruco_detection/aruco_detection.markdown#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n\"marker23.png\" -d=10 -id=23\n```\n\n----------------------------------------\n\nTITLE: Building with Extra Modules in OpenCV\nDESCRIPTION: This snippet illustrates adding extra modules to the OpenCV build using the `OPENCV_EXTRA_MODULES_PATH` option. The modules must be organized with compatible CMakeLists.txt files. Note that only 0- and 1-level deep locations are supported.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# build with all modules in opencv_contrib\ncmake -DOPENCV_EXTRA_MODULES_PATH=../opencv_contrib/modules ../opencv\n\n# build with one of opencv_contrib modules\ncmake -DOPENCV_EXTRA_MODULES_PATH=../opencv_contrib/modules/bgsegm ../opencv\n\n# build with two custom modules (semicolon must be escaped in bash)\ncmake -DOPENCV_EXTRA_MODULES_PATH=../my_mod1\\;../my_mod2 ../opencv\n```\n\n----------------------------------------\n\nTITLE: Installing Valgrind on GNU/Linux\nDESCRIPTION: This command installs Valgrind and Massif Visualizer tools on a Debian/Ubuntu system. These tools are necessary to profile application memory usage.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/anisotropic_segmentation/porting_anisotropic_segmentation.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo apt-get install valgrind massif-visualizer\n```\n\n----------------------------------------\n\nTITLE: Executing Video Similarity Check via Command Line\nDESCRIPTION: Demonstrates how to run the described C++ application from the command line, providing the reference video file, the comparison video file, and potentially other numerical arguments. This example uses relative paths for the video files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_input_psnr_ssim.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nvideo/Megamind.avi video/Megamind_bug.avi  35 10\n```\n\n----------------------------------------\n\nTITLE: Accessing Algorithm Properties in OpenCV 3.0\nDESCRIPTION: Shows how to access algorithm properties using virtual methods instead of the deprecated generic get/set methods.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/transition_guide/transition_guide.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// good way\ndouble clipLimit = clahe->getClipLimit();\nclahe->setClipLimit(clipLimit);\n// bad way\ndouble clipLimit = clahe->getDouble(\"clipLimit\");\nclahe->set(\"clipLimit\", clipLimit);\nclahe->setDouble(\"clipLimit\", clipLimit);\n```\n\n----------------------------------------\n\nTITLE: Hypothetical Commands for License Details\nDESCRIPTION: These are hypothetical command examples (`show w` for warranty details, `show c` for redistribution conditions) mentioned in the context of the interactive program startup notice. They represent placeholders for actual commands, menu items, or other UI elements a program might implement to display the relevant sections of the GPL.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ittnotify/src/ittnotify/GPL-2.0-only.txt#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n`show w`\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n`show c`\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenCV Android Test Project in CMake\nDESCRIPTION: Sets up the project name and configures the Android test directory. It also removes any existing test directory to ensure a clean setup.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/android_test/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(opencv_test_android)\n\nset(OPENCV_ANDROID_TEST_DIR \"${OpenCV_BINARY_DIR}/android_test\" CACHE INTERNAL \"\")\nfile(REMOVE_RECURSE \"${OPENCV_ANDROID_TEST_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Waiting for User Exit in Python OpenCV Program\nDESCRIPTION: Code snippet illustrating how to wait for a user keystroke to exit an OpenCV application in Python. This ensures the result windows remain open until the user chooses to close the program.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.markdown#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Wait until user exit program by pressing a key\ncv.waitKey(0)\n```\n\n----------------------------------------\n\nTITLE: Declaring Deep Learning Networks with G-API\nDESCRIPTION: Declaration of two neural networks used in face beautification: a face detector and a facial landmarks detector using G_API_NET macro.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// G-API network declaration\nG_API_NET(FaceDetector, <cv::GMat(cv::GMat)>, \"face-detector\");\nG_API_NET(LandmarksDetector, <cv::GMat(cv::GMat)>, \"landmarks-detector\");\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Build for ARM Cross Compilation - Bash\nDESCRIPTION: Runs CMake with a toolchain file targeting ARM gnueabi on Linux, configuring OpenCV for cross-compilation. Requires CMake 2.6 or higher and the relevant toolchain file from the OpenCV source tree. Optional parameters can be added to customize the build. Replace placeholder paths as appropriate. This command does not initiate the build process but prepares the configuration.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/arm_crosscompile_with_cmake.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncmake [<some optional parameters>] -DCMAKE_TOOLCHAIN_FILE=<path to the OpenCV source directory>/platforms/linux/arm-gnueabi.toolchain.cmake <path to the OpenCV source directory>\n```\n\n----------------------------------------\n\nTITLE: Embedding OpenCV.js Pose Estimation Demo with HTML Iframe\nDESCRIPTION: This HTML snippet utilizes an `<iframe>` tag to embed an external HTML page (`../../js_pose_estimation.html`), which is expected to contain the OpenCV.js pose estimation demonstration. The `onload` JavaScript event handler dynamically adjusts the iframe's height to match the height of the loaded content's body, ensuring the entire embedded page is visible without internal scrollbars. The width is set to 100% to occupy the full width of its container.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_dnn/js_pose_estimation/js_pose_estimation.markdown#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe src=\"../../js_pose_estimation.html\" width=\"100%\"\n        onload=\"this.style.height=this.contentDocument.body.scrollHeight +'px';\">\n</iframe>\n```\n\n----------------------------------------\n\nTITLE: Outputting 2D Points in OpenCV C++\nDESCRIPTION: Shows how to output 2D point objects using the << operator. OpenCV supports direct streaming of various data structures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\nPoint2f P(5, 1);\ncout << \"Point (2D) = \" << P << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Building and Installing OpenCV using Make (Shell)\nDESCRIPTION: Commands to compile the configured OpenCV source code using `make` within the build directory, switch to the root user using `su` (required for installation to system directories like /usr/local), and then install the built library files using `make install`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\nmake\nsu\nmake install\n```\n\n----------------------------------------\n\nTITLE: Approximating Forehead Contour using C++\nDESCRIPTION: The code snippet approximates the forehead contour using detected facial landmarks of the jaw. It creates a half-ellipse based on three jaw points, assuming the jaw width is equivalent to the forehead width. The forehead height is estimated based on jaw height without direct y-axis points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/face_beautification/face_beautification.markdown#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n@snippet cpp/tutorial_code/gapi/face_beautification/face_beautification.cpp ld_pp_fhd\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV Repository\nDESCRIPTION: Commands to install Git and clone the OpenCV source code repository.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_ubuntu/py_setup_in_ubuntu.markdown#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install git\ngit clone https://github.com/opencv/opencv.git\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV.js with WebAssembly Intrinsics Tests - Bash\nDESCRIPTION: This bash command builds OpenCV.js with both SIMD and intrinsic tests enabled, requiring Emscripten, Python, and the platform's build script. The options '--simd' and '--build_wasm_intrin_test' are used for adding SIMD and intrinsic tests, respectively, while '--build_wasm' targets WebAssembly output. Output includes test-enabled versions of OpenCV.js for use in browser and/or Node.js.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nemcmake python ./opencv/platforms/js/build_js.py build_js --build_wasm --simd --build_wasm_intrin_test\n```\n\n----------------------------------------\n\nTITLE: Setting OpenCV Build Type and Install Path via CMake (Shell)\nDESCRIPTION: Specifies the CMake build type as 'RELEASE' and sets the installation prefix to '/usr/local'. This command ensures the final build is optimized and installed in the standard local directory. It is shown as a separate step for clarity, although flags can be combined.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\ncmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local ..\n```\n\n----------------------------------------\n\nTITLE: Starting the Camera (Objective-C)\nDESCRIPTION: This snippet shows how to start the camera when a button is pressed. It assumes that the UI is properly connected to the ViewController.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/video_processing/video_processing.markdown#2025-04-22_snippet_4\n\nLANGUAGE: Objective-C\nCODE:\n```\n#pragma mark - UI Actions\n\n- (IBAction)actionStart:(id)sender;\n{\n    [self.videoCamera start];\n}\n```\n\n----------------------------------------\n\nTITLE: Draw Final Matches and Output in OpenCV using Java\nDESCRIPTION: Java code that draws the final matches of keypoints and outputs them, with additional statistics provided about the matching process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/akaze_matching/akaze_matching.markdown#2025-04-22_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\nsamples/java/tutorial_code/features2D/akaze_matching/AKAZEMatchDemo.java draw final matches\n```\n\n----------------------------------------\n\nTITLE: Preventing In-Source Builds in CMake\nDESCRIPTION: Checks and prevents in-source builds by comparing source and binary directories. Displays an error message if build is attempted in source directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(\" ${CMAKE_SOURCE_DIR}\" STREQUAL \" ${CMAKE_BINARY_DIR}\")\n  message(FATAL_ERROR \"\nFATAL: In-source builds are not allowed.\n       You should create a separate directory for build files.\n\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Python-Style Formatting for OpenCV Mat Output in C++\nDESCRIPTION: Shows how to format OpenCV Mat output to resemble Python syntax using the format() method with FormatPython flag.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.markdown#2025-04-22_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\ncout << \"R (python) = \" << endl << format(R, Formatter::FMT_PYTHON) << endl << endl;\n```\n\n----------------------------------------\n\nTITLE: Closing the Clojure REPL\nDESCRIPTION: Illustrates how to gracefully exit from the Clojure REPL to wrap up a session. Useful for beginners to understand how to end interactive sessions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_14\n\nLANGUAGE: clojure\nCODE:\n```\nuser=> (exit)\nBye for now!\n```\n\n----------------------------------------\n\nTITLE: Declaring Variables for Image Border Processing\nDESCRIPTION: Declaration of variables including source image, window name, and random number generator for border color generation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/copyMakeBorder/copyMakeBorder.markdown#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nMat src, dst;\nchar window_name[] = \"copyMakeBorder Demo\";\nRNG rng(12345);\n```\n\nLANGUAGE: Java\nCODE:\n```\nMat src = new Mat(), dst = new Mat();\nString window_name = \"copyMakeBorder Demo\";\nRandom rng = new Random(12345);\n```\n\nLANGUAGE: Python\nCODE:\n```\nwindow_name = \"copyMakeBorder Demo\"\n```\n\n----------------------------------------\n\nTITLE: Computing Grayscale Value from Iteration Number for Mandelbrot Set - C++\nDESCRIPTION: This function computes the pixel grayscale value for a Mandelbrot image, setting it to black if the iteration count reaches the max (inside the set), or scaling the escaped iteration to [0,255]. Depends on info from iteration algorithm, accepts current iteration and maximum as input, outputs uchar grayscale value.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/how_to_use_OpenCV_parallel_for_/how_to_use_OpenCV_parallel_for_.markdown#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n// Grayscale value assignment for Mandelbrot image\nuchar grayscaleValue(int iter, int maxIter)\n{\n    if (iter == maxIter)\n        return 0; // black\n    else\n        return (uchar)(255.0 * iter / maxIter);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Detect Edges with Canny in OpenCV Python\nDESCRIPTION: Uses the OpenCV Canny function for edge detection in an image using Python. Requires OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nedges = cv2.Canny(gray, lowThreshold, highThreshold)\n```\n\n----------------------------------------\n\nTITLE: Find Contours in OpenCV Python\nDESCRIPTION: Finds contours in an image and saves them to lists using OpenCV in Python. Dependencies include OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ncontours, hierarchy = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n```\n\n----------------------------------------\n\nTITLE: Create and Display Windows with OpenCV Java\nDESCRIPTION: Creates a window and displays an image within it using OpenCV in Java. Requires OpenCV library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.markdown#2025-04-22_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nHighGui.namedWindow(\"Source\", HighGui.WINDOW_AUTOSIZE);\nHighGui.imshow(\"Source\", src);\n```\n\n----------------------------------------\n\nTITLE: Drawing Back Projection in Java with OpenCV\nDESCRIPTION: This code demonstrates how to display the back projection result using OpenCV in Java. It creates a window and shows the back projection image.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/back_projection/back_projection.markdown#2025-04-22_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\nHighGui.imshow(\"BackProj\", backproj);\n```\n\n----------------------------------------\n\nTITLE: Installing Image Format Dependencies\nDESCRIPTION: Installs development libraries for various image formats like PNG, JPEG, TIFF, etc.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nyum install libpng-devel\nyum install libjpeg-turbo-devel\nyum install jasper-devel\nyum install openexr-devel\nyum install libtiff-devel\nyum install libwebp-devel\n```\n\n----------------------------------------\n\nTITLE: Installing Required Build Dependencies\nDESCRIPTION: Commands to install essential build dependencies including CMake, GCC, and Python development packages.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_ubuntu/py_setup_in_ubuntu.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install cmake\nsudo apt-get install gcc g++\n```\n\n----------------------------------------\n\nTITLE: Defining Segmentation Test Configuration in Python\nDESCRIPTION: This Python data class defines configuration parameters necessary for segmentation model evaluation with defaults set for directories and frame size.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass TestSegmConfig:\n    frame_size: int = 500\n    img_root_dir: str = \"./VOC2012\"\n    img_dir: str = os.path.join(img_root_dir, \"JPEGImages/\")\n    img_segm_gt_dir: str = os.path.join(img_root_dir, \"SegmentationClass/\")\n    segm_val_file: str = os.path.join(img_root_dir, \"ImageSets/Segmentation/seg11valid.txt\")\n    colour_file_cls: str = os.path.join(img_root_dir, \"ImageSets/Segmentation/pascal-classes.txt\")\n```\n\n----------------------------------------\n\nTITLE: Handling Non-Existent Files in C++ with OpenCV\nDESCRIPTION: Demonstrates how to handle attempts to read from non-existent files in C++ using OpenCV, which initializes the data structure with default values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.markdown#2025-04-22_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\ncpp/tutorial_code/core/file_input_output/file_input_output.cpp nonexist\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV-Python via Package Manager\nDESCRIPTION: Installs OpenCV-Python and NumPy using Fedora's package manager yum.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ yum install numpy opencv*\n```\n\n----------------------------------------\n\nTITLE: Running G-API Pipeline in Serial Mode\nDESCRIPTION: Shows the reference implementation of running the pipeline in serial (non-streaming) mode for benchmarking. Processes frames one at a time without pipelining optimization.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/interactive_face_detection/interactive_face_detection.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\ncv::VideoCapture cap;\nif (!cmd.get<std::string>(\"input\").empty()) cap.open(cmd.get<std::string>(\"input\"));\nelse cap.open(0);\n\ncv::TickMeter tm;\ntm.start();\n\ncv::Mat in_frame;\nsize_t frames = 0;\nwhile (keep_running) {\n    if (!cap.read(in_frame)) break;\n    cv::Mat out;\n    std::vector<cv::Rect> faces;\n    std::vector<cv::Str> age_strings, emo_strings;\n\n    auto out_vector = pipeline(cv::gin(in_frame),\n                              cv::gout(out, faces, age_strings, emo_strings));\n    frames++;\n    if (!cmd.get<bool>(\"pure\")) {\n        cv::imshow(\"Out\", out);\n        const int key = cv::waitKey(1);\n        if (key == 27) break;\n    }\n}\ntm.stop();\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Framework with Extra Modules for iOS in Bash\nDESCRIPTION: Command to build the OpenCV framework for iOS including extra modules from opencv_contrib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/ios_install/ios_install.markdown#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/<my_working_directory>\npython opencv/platforms/ios/build_framework.py ios --contrib opencv_contrib\n```\n\n----------------------------------------\n\nTITLE: Installing Xcode Command Line Tools in Bash\nDESCRIPTION: Command to install Xcode command line tools, which are required for building OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/ios_install/ios_install.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nxcode-select --install\n```\n\n----------------------------------------\n\nTITLE: Drawing Detected Corners\nDESCRIPTION: Demonstrates how to visualize detected ChArUco corners on the image\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/objdetect/charuco_detection/charuco_detection.markdown#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\ncv::Mat imageCopy;\nimage.copyTo(imageCopy);\ncv::aruco::drawDetectedCornersCharuco(imageCopy, charucoCorners, charucoIds);\n```\n\n----------------------------------------\n\nTITLE: Importing and Utilizing OpenCV Classes in Clojure\nDESCRIPTION: Illustrates using import in Clojure to bring OpenCV classes into scope, facilitating the creation of objects like Point, Rect, and Size for further operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_10\n\nLANGUAGE: clojure\nCODE:\n```\nuser=> (import '[org.opencv.core Point Rect Size])\norg.opencv.core.Size\nuser=> (def r1 (Rect. p1 p2))\n#'user/r1\nuser=> r1\n#<Rect {0, 0, 100x100}>\nuser=> (class r1)\norg.opencv.core.Rect\nuser=> (instance? org.opencv.core.Rect r1)\ntrue\nuser=> (Size. 100 100)\n#<Size 100x100>\nuser=> (def sq-100 (Size. 100 100))\n#'user/sq-100\nuser=> (class sq-100)\norg.opencv.core.Size\nuser=> (instance? org.opencv.core.Size sq-100)\ntrue\n```\n\n----------------------------------------\n\nTITLE: Installing Git on Linux\nDESCRIPTION: Command to install Git version control system on Linux using the package manager.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install git\n```\n\n----------------------------------------\n\nTITLE: Setting OpenCV Photo Module Description\nDESCRIPTION: Defines the description for the OpenCV photo module as 'Computational Photography'.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/photo/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(the_description \"Computational Photography\")\n```\n\n----------------------------------------\n\nTITLE: Adding OpenCV Binaries to Windows PATH using Batch Variable Expansion\nDESCRIPTION: This snippet demonstrates how to add the OpenCV binaries directory to the system PATH variable using environment variable expansion (%OPENCV_DIR%\\bin). This ensures dynamic link libraries are discoverable at runtime. The approach presumes OPENCV_DIR is previously set, and adding this path is essential for loading DLLs without manually copying them near each executable. After changing PATH, applications relying on OpenCV can locate required binaries when executed from any location.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/windows_install/windows_install.markdown#2025-04-22_snippet_7\n\nLANGUAGE: batch\nCODE:\n```\n%OPENCV_DIR%\\bin\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenCV Directory in CMake Projects on macOS\nDESCRIPTION: This command shows how to specify the OpenCV_DIR variable in CMake projects to find the installed OpenCV package.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/macos_install/macos_install.markdown#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncmake -DOpenCV_DIR=~/build_opencv ..\n```\n\n----------------------------------------\n\nTITLE: Creating Display Window for Threshold Output (Java)\nDESCRIPTION: This Java snippet uses HighGui.namedWindow to open a window named 'Threshold Demo', providing the display surface for thresholded images. Must be called before showing results. Requires OpenCV Java highgui bindings. No input, just creates the window.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/threshold/threshold.markdown#2025-04-22_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\n// [window]\\nHighGui.namedWindow(window_name);\\n// [window]\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Decision Tree Implementation in C++\nDESCRIPTION: A portion of the FAST corner detection algorithm that uses a decision tree of pixel comparisons against brightness thresholds (cb and c_b). The algorithm evaluates surrounding pixels at various offsets to determine if the current point is a corner by branching to either 'is_a_corner' or 'is_not_a_corner'.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_24\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset1] > cb)\n    if(ptr[offset6] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset10] > cb)\n          if(ptr[offset11] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      if(ptr[offset6] < c_b)\n        if(ptr[offset8] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        if(ptr[offset8] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n  else\n    if(ptr[offset6] < c_b)\n      goto is_not_a_corner;\n    else\n      if(ptr[offset6] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset2] > cb)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset9] < c_b)\n        if(ptr[offset1] < c_b)\n          goto is_not_a_corner;\n        else\n          if(ptr[offset1] > cb)\n            if(ptr[offset6] > cb)\n              if(ptr[offset3] > cb)\n                if(ptr[offset4] > cb)\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset6] < c_b)\n                if(ptr[offset3] > cb)\n                  if(ptr[offset4] > cb)\n                    if(ptr[offset10] > cb)\n                      if(ptr[offset11] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                if(ptr[offset3] > cb)\n                  if(ptr[offset4] > cb)\n                    if(ptr[offset10] > cb)\n                      if(ptr[offset11] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset9] > cb)\n          if(ptr[offset1] < c_b)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset1] > cb)\n              if(ptr[offset6] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    if(ptr[offset3] > cb)\n                      goto is_a_corner;\n                    else\n                      if(ptr[offset8] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      if(ptr[offset3] > cb)\n                        goto is_a_corner;\n                      else\n                        if(ptr[offset8] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  if(ptr[offset10] > cb)\n                    if(ptr[offset11] > cb)\n                      if(ptr[offset3] > cb)\n                        goto is_a_corner;\n                      else\n                        if(ptr[offset8] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset1] < c_b)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset1] > cb)\n              if(ptr[offset6] > cb)\n                if(ptr[offset3] > cb)\n                  if(ptr[offset4] > cb)\n                    if(ptr[offset10] > cb)\n                      if(ptr[offset11] > cb)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset3] > cb)\n                    if(ptr[offset4] > cb)\n                      if(ptr[offset10] > cb)\n                        if(ptr[offset11] > cb)\n                          goto is_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  if(ptr[offset3] > cb)\n                    if(ptr[offset4] > cb)\n                      if(ptr[offset10] > cb)\n                        if(ptr[offset11] > cb)\n                          goto is_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Logic in C++\nDESCRIPTION: This snippet shows a portion of the FAST corner detection algorithm. It compares pixel intensities at different offsets to determine if a point is a corner. The code uses goto statements for control flow and implements a decision tree structure.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset10] < c_b)\n  if(ptr[offset11] < c_b)\n    goto success_structured;\n  else\n    goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  if(ptr[offset9] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset8] < c_b)\n        if(ptr[offset5] < c_b)\n          if(ptr[offset1] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                goto success_structured;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset4] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset6] < c_b)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset4] < c_b)\n                if(ptr[offset3] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n              else\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              goto structured;\n        else\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              if(ptr[offset1] < c_b)\n                goto success_structured;\n              else\n                if(ptr[offset6] < c_b)\n                  goto success_structured;\n                else\n                  goto structured;\n            else\n              goto structured;\n          else\n            goto structured;\n      else\n        goto structured;\n    else\n      goto structured;\n  else\n    if(ptr[offset5] > cb)\n      if(ptr[offset9] > cb)\n        if(ptr[offset6] > cb)\n          if(ptr[offset7] > cb)\n            if(ptr[offset8] > cb)\n              if(ptr[offset4] > cb)\n                if(ptr[offset3] > cb)\n                  goto success_structured;\n                else\n                  if(ptr[offset10] > cb)\n                    goto success_structured;\n                  else\n                    goto structured;\n              else\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              goto structured;\n          else\n            goto structured;\n        else\n          goto structured;\n      else\n        goto homogeneous;\n    else\n      goto structured;\nelse\n  if(ptr[offset5] > cb)\n    if(ptr[offset9] > cb)\n      if(ptr[offset6] > cb)\n        if(ptr[offset7] > cb)\n          if(ptr[offset4] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset8] > cb)\n                goto success_structured;\n              else\n                if(ptr[offset1] > cb)\n                  if(ptr[offset2] > cb)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset8] > cb)\n                if(ptr[offset10] > cb)\n                  goto success_structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n          else\n            if(ptr[offset11] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset10] > cb)\n                  goto success_structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n            else\n              goto structured;\n        else\n          goto structured;\n      else\n        goto structured;\n    else\n      if(ptr[offset2] > cb)\n        if(ptr[offset3] > cb)\n          if(ptr[offset4] > cb)\n            if(ptr[offset7] > cb)\n              if(ptr[offset1] > cb)\n                if(ptr[offset6] > cb)\n                  goto success_structured;\n                else\n                  goto structured;\n              else\n                if(ptr[offset6] > cb)\n                  if(ptr[offset8] > cb)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              goto structured;\n          else\n            goto structured;\n        else\n          goto structured;\n      else\n        goto structured;\n  else\n    if(ptr[offset5] < c_b)\n      if(ptr[offset9] < c_b)\n        if(ptr[offset6] < c_b)\n          if(ptr[offset7] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset8] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset1] < c_b)\n                    if(ptr[offset2] < c_b)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n              else\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset11] < c_b)\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building OpenCV\nDESCRIPTION: Commands to configure OpenCV build with CMake and compile/install the library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_ubuntu/py_setup_in_ubuntu.markdown#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncmake ../\nmake\nsudo make install\n```\n\n----------------------------------------\n\nTITLE: Configuring Android OpenCV Example Project with CMake\nDESCRIPTION: Sets up an Android project build configuration for a 15-puzzle example application using OpenCV. Specifies the project name, dependencies, and minimum SDK target version of 11.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/15-puzzle/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(sample example-15-puzzle)\n\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\" LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\" SDK_TARGET 11 \"${ANDROID_SDK_TARGET}\")\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV and Building with Docker for Emscripten - Bash\nDESCRIPTION: This snippet clones the OpenCV GitHub repository, changes into its root directory, then uses Docker to run an emscripten/emsdk container that builds OpenCV.js. It maps the local directory for persistence, sets user permissions, and uses emcmake and Python3 to invoke the build. Requires Docker and Git. Works on Linux/MacOS systems that support vanilla shell scripting.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/opencv/opencv.git\\ncd opencv\\ndocker run --rm -v $(pwd):/src -u $(id -u):$(id -g) emscripten/emsdk emcmake python3 ./platforms/js/build_js.py build_js\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Pixel Comparison Logic\nDESCRIPTION: Complex nested conditional logic for comparing pixel values against brightness thresholds (c_b and cb) using pointer offsets to determine corner features. The code implements part of the FAST corner detection algorithm's decision tree for classifying pixels as corners based on their surrounding intensity values.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_21\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset7] < c_b)\n  if(ptr[offset1] < c_b)\n    if(ptr[offset6] < c_b)\n      goto success_homogeneous;\n    else\n      if(ptr[offset11] < c_b)\n        goto success_homogeneous;\n      else\n        goto homogeneous;\n  else\n    if(ptr[offset6] < c_b)\n      if(ptr[offset8] < c_b)\n        goto success_homogeneous;\n      else\n        goto homogeneous;\n    else\n      goto homogeneous;\nelse\n  if(ptr[offset1] < c_b)\n    if(ptr[offset6] < c_b)\n      goto success_homogeneous;\n    else\n      if(ptr[offset11] < c_b)\n        goto success_homogeneous;\n      else\n        goto homogeneous;\n  else\n    goto homogeneous;\n```\n\n----------------------------------------\n\nTITLE: Implementing Pixel Comparison Logic for Feature Detection in C++\nDESCRIPTION: This code snippet implements a complex decision tree for comparing pixel values against thresholds. It uses nested if-else statements to navigate through different pixel offsets, determining whether to proceed to a 'homogeneous' or 'success_homogeneous' state based on comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_22\n\nLANGUAGE: C++\nCODE:\n```\ngoto homogeneous;\nelse\n  if(ptr[offset11] > cb)\n    if(ptr[offset8] > cb)\n      if(ptr[offset10] > cb)\n        goto success_homogeneous;\n      else\n        goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  if(ptr[offset2] > cb)\n    if(ptr[offset3] > cb)\n      if(ptr[offset4] > cb)\n        if(ptr[offset1] > cb)\n          if(ptr[offset6] > cb)\n            goto success_homogeneous;\n          else\n            goto homogeneous;\n        else\n          if(ptr[offset6] > cb)\n            if(ptr[offset8] > cb)\n              goto success_homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n      else\n        goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  if(ptr[offset5] < c_b)\n    if(ptr[offset7] < c_b)\n      if(ptr[offset9] < c_b)\n        if(ptr[offset6] < c_b)\n          if(ptr[offset4] < c_b)\n            if(ptr[offset3] < c_b)\n              if(ptr[offset8] < c_b)\n                goto success_homogeneous;\n              else\n                if(ptr[offset1] < c_b)\n                  if(ptr[offset2] < c_b)\n                    goto success_homogeneous;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n            else\n              if(ptr[offset8] < c_b)\n                if(ptr[offset10] < c_b)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n          else\n            if(ptr[offset11] < c_b)\n              if(ptr[offset8] < c_b)\n                if(ptr[offset10] < c_b)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n        else\n          goto homogeneous;\n      else\n        if(ptr[offset2] < c_b)\n          if(ptr[offset3] < c_b)\n            if(ptr[offset4] < c_b)\n              if(ptr[offset1] < c_b)\n                if(ptr[offset6] < c_b)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset8] < c_b)\n                    goto success_homogeneous;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n        else\n          goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\n```\n\n----------------------------------------\n\nTITLE: Building Android AAR Package using Gradle via CMake\nDESCRIPTION: This CMake snippet defines a custom command to build the OpenCV Android library project as an AAR file. It specifies the output AAR file path and a dependency helper file. The command executes `./gradlew opencv:assemble` in the `ANDROID_BUILD_BASE_DIR`, potentially with verbose options. After the Gradle command completes, it touches the dependency helper file to signal completion. The command depends on the targets/files listed in the `depends` variable (set previously) and the `${the_module}` target itself. A `file(REMOVE)` command ensures the build is triggered after CMake re-runs.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/android_sdk/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\n# build jar\nset(AAR_FILE \"${OPENCV_JAVA_DIR}/build/outputs/aar/opencv-release.aar\")\nadd_custom_command(\n    OUTPUT \"${AAR_FILE}\" \"${OPENCV_DEPHELPER}/${the_module}_android\"\n    COMMAND ./gradlew ${OPENCV_GRADLE_VERBOSE_OPTIONS} \"opencv:assemble\"\n    COMMAND ${CMAKE_COMMAND} -E touch \"${OPENCV_DEPHELPER}/${the_module}_android\"\n    WORKING_DIRECTORY \"${ANDROID_BUILD_BASE_DIR}\"\n    DEPENDS ${depends} ${the_module}\n    COMMENT \"Building OpenCV Android library project\"\n)\nfile(REMOVE \"${OPENCV_DEPHELPER}/${the_module}_android\")  # force rebuild after CMake run\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Dependencies\nDESCRIPTION: This console snippet installs dependencies specified in a requirements.txt file into the activated virtual environment. It ensures all necessary libraries are available for running the model conversion pipeline with OpenCV and PyTorch.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing with Stripped Binaries to Reduce Size - Shell\nDESCRIPTION: Runs the CMake 'install/strip' target to install binaries with symbol information removed, reducing the resulting binary size by 10-15%. This step should be used on platforms (typically Linux) when release builds are prioritized and debugging information is not required. Caution: Once stripped, debugging the binaries will be limited.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\ncmake --build . --target install/strip\n```\n\n----------------------------------------\n\nTITLE: Toggle Example for C++, Java, and Python - Doxygen with @snippet\nDESCRIPTION: Provides a multi-language toggle button example, each embedding a code snippet by referencing its source file and section. Inputs are language toggle directives and @snippet commands; outputs are code blocks displayed according to selected language. Used to demonstrate the same logic across C++, Java, and Python snippets in documentation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_14\n\nLANGUAGE: markdown\nCODE:\n```\n@add_toggle_cpp\n\n   Text for C++ button\n   @snippet samples/cpp/tutorial_code/introduction/documentation/documentation.cpp hello_world\n\n@end_toggle\n\n@add_toggle_java\n\n   Text for Java button\n   @snippet samples/java/tutorial_code/introduction/documentation/Documentation.java  hello_world\n\n@end_toggle\n\n@add_toggle_python\n\n   Text for Python button\n   @snippet samples/python/tutorial_code/introduction/documentation/documentation.py hello_world\n\n@end_toggle\n```\n\n----------------------------------------\n\nTITLE: Installing Maven and JDK Dependencies via APT for OpenCV Build\nDESCRIPTION: Command to install Maven and the default JDK using the aptitude package manager, which are prerequisites for the OpenCV Maven build process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/maven/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo aptitude install maven default-jdk\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Video Module Dependencies\nDESCRIPTION: Defines the OpenCV video module with its required and optional dependencies. Includes support for multiple programming language wrappers including Java, Objective-C, Python, and JavaScript.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/video/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(the_description \"Video Analysis\")\nocv_define_module(video\n    opencv_imgproc\n    OPTIONAL\n      opencv_calib3d\n      opencv_dnn\n    WRAP\n      java\n      objc\n      python\n      js\n)\n```\n\n----------------------------------------\n\nTITLE: Setting an Environment Variable in Python Using the os Module\nDESCRIPTION: This Python snippet assigns a string value to 'MY_ENV_VARIABLE' using 'os.environ', and shows an example of importing OpenCV ('cv2') which may read such variables. Dependencies: Python's 'os' and 'cv2' (OpenCV) modules. Limitations: On some platforms or Python distributions (notably Windows), this may not affect native libraries like OpenCV, depending on process model.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/env_reference/env_reference.markdown#2025-04-22_snippet_3\n\nLANGUAGE: py\nCODE:\n```\nimport os\\nos.environ[\"MY_ENV_VARIABLE\"] = \"True\" # value must be a string\\nimport cv2 # variables set after this may not have effect\n```\n\n----------------------------------------\n\nTITLE: Configuring TIFF Support in CMake\nDESCRIPTION: CMake configuration option for enabling TIFF and zlib support in OpenCV imgcodecs module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/readme.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nWITH_TIFF=ON # Enable TIFF and zlib support\n```\n\n----------------------------------------\n\nTITLE: Declaring Variables for Laplacian Demo in C++\nDESCRIPTION: Declares necessary C++ variables for the Laplacian demo, including Mat objects to store the source, grayscale, and destination images, kernel size, scale, delta, desired output depth (ddepth), and window name. Requires OpenCV headers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.markdown#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n//! [variables]\n// Declare the variables we are going to use\nMat src, src_gray, dst;\nint kernel_size = 3;\nint scale = 1;\nint delta = 0;\nint ddepth = CV_16S;\nconst char* window_name = \"Laplace Demo\";\n//! [variables]\n```\n\n----------------------------------------\n\nTITLE: Setup OpenCV Environment with Leiningen\nDESCRIPTION: Demonstrates setting up a Leiningen environment that automatically takes care of OpenCV library initialization, simplifying the process to start the project with necessary dependencies pre-loaded.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nlein repl\nnREPL server started on port 51645 on host 127.0.0.1\nREPL-y 0.3.0\nClojure 1.5.1\n    Docs: (doc function-name-here)\n          (find-doc \"part-of-name-here\")\n  Source: (source function-name-here)\n Javadoc: (javadoc java-object-or-class-here)\n    Exit: Control+D or (exit) or (quit)\n Results: Stored in vars *1, *2, *3, an exception in *e\n\nuser=>\n```\n\n----------------------------------------\n\nTITLE: Adding CUDA Support Option and Validating Dependencies for DNN Module - CMake\nDESCRIPTION: Defines and validates variables for enabling CUDA support (OPENCV_DNN_CUDA) in the DNN module. Checks that dependencies (CUDA Toolkit, cuBLAS, cuDNN) are available; emits error messages if not. Enables CUDA-specific code compilation if all requirements are present. Inputs include HAVE_CUDA, HAVE_CUBLAS, and HAVE_CUDNN; outputs are compile definitions or fatal error messages.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nocv_option(OPENCV_DNN_CUDA \"Build with CUDA support\"\n    HAVE_CUDA\n    AND HAVE_CUBLAS\n    AND HAVE_CUDNN\n)\n\nif(OPENCV_DNN_CUDA)\n  if(HAVE_CUDA AND HAVE_CUBLAS AND HAVE_CUDNN)\n    ocv_target_compile_definitions(${the_module} PRIVATE \"CV_CUDA4DNN=1\")\n  else()\n    if(NOT HAVE_CUDA)\n      message(SEND_ERROR \"DNN: CUDA backend requires CUDA Toolkit. Please resolve dependency or disable OPENCV_DNN_CUDA=OFF\")\n    elseif(NOT HAVE_CUBLAS)\n      message(SEND_ERROR \"DNN: CUDA backend requires cuBLAS. Please resolve dependency or disable OPENCV_DNN_CUDA=OFF\")\n    elseif(NOT HAVE_CUDNN)\n      message(SEND_ERROR \"DNN: CUDA backend requires cuDNN. Please resolve dependency or disable OPENCV_DNN_CUDA=OFF\")\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing calcGST() Function Header in G-API\nDESCRIPTION: Shows the implementation of the calcGST() function header using G-API operations, demonstrating syntax differences from traditional OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/gapi/anisotropic_segmentation/porting_anisotropic_segmentation.markdown#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ncv::GMat calcGST(const cv::GMat& inputImage, const int w)\n{\n    auto img = cv::gapi::convertTo(inputImage, CV_32F);\n    auto imgDiffX = cv::gapi::Sobel(img, CV_32F, 1, 0, 3);\n    auto imgDiffY = cv::gapi::Sobel(img, CV_32F, 0, 1, 3);\n    auto imgDiffXY = cv::gapi::mul(imgDiffX, imgDiffY);\n    auto imgDiffXX = cv::gapi::mul(imgDiffX, imgDiffX);\n    auto imgDiffYY = cv::gapi::mul(imgDiffY, imgDiffY);\n```\n\n----------------------------------------\n\nTITLE: Checking OpenCV Build Results\nDESCRIPTION: Commands to check the presence of built OpenCV libraries and executables.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nls lib/*\nls bin/*\n```\n\n----------------------------------------\n\nTITLE: Running build_xcframework.py to Build XCFramework - Python/Bash\nDESCRIPTION: This example shows how to execute the build_xcframework.py script from the command line to create an OpenCV xcframework. It requires Python 3.6+, CMake 3.18.5+, and Xcode 12.2+ with command line tools. The command builds OpenCV for multiple Apple architectures and outputs results to the specified directory. Expected output is an opencv2.xcframework. Ensure the script and Python/CMake dependencies are present in your environment.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/apple/readme.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/<my_working_directory>\\npython opencv/platforms/apple/build_xcframework.py --out ./build_xcframework\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV with Make\nDESCRIPTION: Command to build OpenCV using Make, utilizing multiple CPU cores for faster compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_install/linux_install.markdown#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nmake -j4\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV Tests with Custom Arguments\nDESCRIPTION: Command to run OpenCV tests with custom arguments passed to ctest, such as verbose output and parallel test execution.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ make test ARGS=\"--verbose --parallel 3\"\n```\n\n----------------------------------------\n\nTITLE: Camera API Abstract Methods Definition\nDESCRIPTION: Abstract methods that need to be implemented by camera API inheritors. These methods handle camera operations for both Camera and Camera2 APIs.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_ocl_intro.markdown#2025-04-22_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\n    protected abstract void openCamera();\n    protected abstract void closeCamera();\n    protected abstract void setCameraPreviewSize(int width, int height);\n```\n\n----------------------------------------\n\nTITLE: Forcing Compilation of Fast GEMM Kernels for Various Architectures - CMake\nDESCRIPTION: This registers the 'fast_gemm_kernels' implementation for a range of architectures using 'ocv_add_dispatched_file_force_all'. This optimizes matrix multiplication performance in DNN layers. Source file and architecture flags are required as input. Output is build configuration including the optimized kernels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nocv_add_dispatched_file_force_all(\"layers/cpu_kernels/fast_gemm_kernels\" AVX AVX2 NEON LASX)\n```\n\n----------------------------------------\n\nTITLE: Creating OpenCV World Target and Configuring VTK Integration (CMake)\nDESCRIPTION: Uses the custom `ocv_create_module` function to define the build target for the `opencv_world` library, passing the aggregated link dependencies (`link_deps`). This function utilizes the previously collected sources, headers, and include directories. Additionally, it checks if the `opencv_viz` module is being built, is part of the world build, and if the VTK version is 8.90.0 or newer. If all conditions are met, it calls `vtk_module_autoinit` to handle VTK module initialization for the `opencv_world` target, linking necessary VTK libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/world/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\n#message(STATUS \"${OPENCV_MODULE_${the_module}_HEADERS}\")\n#message(STATUS \"${OPENCV_MODULE_${the_module}_SOURCES}\")\nocv_create_module(${link_deps})\n\nif(\";${OPENCV_MODULES_BUILD};\" MATCHES \";opencv_viz;\" AND OPENCV_MODULE_opencv_viz_IS_PART_OF_WORLD AND NOT (VTK_VERSION VERSION_LESS \"8.90.0\"))\n  vtk_module_autoinit(TARGETS opencv_world MODULES ${VTK_LIBRARIES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Architecture Binary Support in CMake\nDESCRIPTION: CMake configuration example showing how to specify CUDA architecture binary support for different compute capabilities. This setting controls which GPU architectures the compiled code will support natively.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/doc/cuda.markdown#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nCUDA_ARCH_BIN=\"1.0 1.3 2.0\"\n```\n\n----------------------------------------\n\nTITLE: OpenCV Linker Configuration\nDESCRIPTION: Links OpenCV libraries to the project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/apps/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nlink_libraries(${OPENCV_LINKER_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Cross-Compiling OpenCV for ARMv7 using CMake\nDESCRIPTION: This bash script installs necessary dependencies for cross-compiling OpenCV for ARMv7 architecture. It then uses CMake to configure the build with NEON optimization and a custom toolchain file. Finally, it builds and installs the project, creating a compressed archive of the installation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y \\\n    linux-libc-dev:armhf \\\n    libavcodec-dev:armhf \\\n    libavformat-dev:armhf \\\n    libavutil-dev:armhf \\\n    libswscale-dev:armhf \\\n    libfreetype-dev:armhf \\\n    libharfbuzz-dev:armhf\n\nPKG_CONFIG_PATH=/usr/lib/arm-linux-gnueabihf/pkgconfig:/usr/share/pkgconfig \\\n    PKG_CONFIG_LIBDIR=/usr/lib/arm-linux-gnueabihf \\\n    PKG_CONFIG_SYSROOT_DIR=/ \\\n        cmake -S opencv \\\n              -B build4-full_armhf \\\n              -DENABLE_NEON=ON \\\n              -DCMAKE_TOOLCHAIN_FILE=/home/kmtr/work/opencv/platforms/linux/arm-gnueabi.toolchain.cmake \\\n              -DOPENCV_EXTRA_MODULES_PATH=opencv_contrib/modules \\\n              -GNinja\n\ncmake      --build   build4-full_armhf\nsudo cmake --install build4-full_armhf\ntar czvf opencv_armhf.tgz -C build4-full_armhf/install .\n```\n\n----------------------------------------\n\nTITLE: Installing Optional Dependencies\nDESCRIPTION: Installs TBB, Eigen, and documentation tools for enhanced functionality.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nyum install tbb-devel\nyum install eigen3-devel\nyum install doxygen\n```\n\n----------------------------------------\n\nTITLE: Executing FAST Segment Test Logic with Nested Conditionals and Goto in C++\nDESCRIPTION: This C++ code implements a segment of a decision tree, likely for FAST corner detection. It compares pixel intensities at various offsets (`offset1` to `offset11`) around a central point (`ptr`) against upper (`cb`) and lower (`c_b`) thresholds. The deeply nested `if-else` structure, combined with `goto` statements, rapidly determines if the pixel pattern matches criteria for a feature, jumping to predefined labels like `success_structured`, `success_homogeneous`, `structured`, or `homogeneous` accordingly. This structure is designed for performance in image processing loops. Dependencies include the `ptr` to pixel data, pre-calculated integer `offset` values, threshold variables `cb` and `c_b`, and the existence of the specified `goto` labels elsewhere in the code.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            goto homogeneous;\n                        else\n                          goto homogeneous;\n                    else\n                      if(ptr[offset9] > cb)\n                        if(ptr[offset7] > cb)\n                          if(ptr[offset8] > cb)\n                            if(ptr[offset1] > cb)\n                              if(ptr[offset10] > cb)\n                                if(ptr[offset11] > cb)\n                                  goto success_homogeneous;\n                                else\n                                  if(ptr[offset6] > cb)\n                                    if(ptr[offset4] > cb)\n                                      goto success_structured;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                              else\n                                if(ptr[offset6] > cb)\n                                  if(ptr[offset3] > cb)\n                                    if(ptr[offset4] > cb)\n                                      goto success_structured;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  goto homogeneous;\n                            else\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset4] > cb)\n                                  if(ptr[offset3] > cb)\n                                    goto success_homogeneous;\n                                  else\n                                    if(ptr[offset10] > cb)\n                                      goto success_homogeneous;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      goto success_homogeneous;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            goto homogeneous;\n                        else\n                          goto homogeneous;\n                      else\n                        goto homogeneous;\n                  else\n                    if(ptr[offset5] < c_b)\n                      if(ptr[offset9] > cb)\n                        if(ptr[offset3] < c_b)\n                          if(ptr[offset4] < c_b)\n                            if(ptr[offset11] > cb)\n                              if(ptr[offset1] > cb)\n                                if(ptr[offset8] > cb)\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset2] > cb)\n                                      goto success_structured;\n                                    else\n                                      if(ptr[offset7] > cb)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  if(ptr[offset6] < c_b)\n                                    if(ptr[offset2] < c_b)\n                                      if(ptr[offset7] < c_b)\n                                        if(ptr[offset8] < c_b)\n                                          goto success_structured;\n                                        else\n                                          goto structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                              else\n                                if(ptr[offset6] > cb)\n                                  if(ptr[offset7] > cb)\n                                    if(ptr[offset8] > cb)\n                                      if(ptr[offset10] > cb)\n                                        goto success_structured;\n                                      else\n                                        goto structured;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                                else\n                                  if(ptr[offset6] < c_b)\n                                    if(ptr[offset2] < c_b)\n                                      if(ptr[offset7] < c_b)\n                                        if(ptr[offset1] < c_b)\n                                          goto success_structured;\n                                        else\n                                          if(ptr[offset8] < c_b)\n                                            goto success_structured;\n                                          else\n                                            goto structured;\n                                      else\n                                        goto homogeneous;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                            else\n                              if(ptr[offset2] < c_b)\n                                if(ptr[offset7] < c_b)\n                                  if(ptr[offset1] < c_b)\n                                    if(ptr[offset6] < c_b)\n                                      goto success_structured;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    if(ptr[offset6] < c_b)\n                                      if(ptr[offset8] < c_b)\n                                        goto success_structured;\n                                      else\n                                        goto homogeneous;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                          else\n                            if(ptr[offset11] > cb)\n                              if(ptr[offset8] > cb)\n                                if(ptr[offset10] > cb)\n                                  if(ptr[offset1] > cb)\n                                    if(ptr[offset2] > cb)\n                                      goto success_structured;\n                                    else\n                                      if(ptr[offset7] > cb)\n                                        goto success_structured;\n                                      else\n                                        goto homogeneous;\n                                  else\n                                    if(ptr[offset6] > cb)\n                                      if(ptr[offset7] > cb)\n                                        goto success_structured;\n                                      else\n                                        goto homogeneous;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  goto homogeneous;\n                              else\n                                goto homogeneous;\n                            else\n                              goto homogeneous;\n                        else\n                          if(ptr[offset11] > cb)\n                            if(ptr[offset10] > cb)\n                              if(ptr[offset3] > cb)\n                                if(ptr[offset1] > cb)\n                                  if(ptr[offset2] > cb)\n                                    goto success_homogeneous;\n                                  else\n                                    if(ptr[offset7] > cb)\n                                      if(ptr[offset8] > cb)\n                                        goto success_structured;\n                                      else\n                                        goto homogeneous;\n                                    else\n                                      goto homogeneous;\n                                else\n                                  if(ptr[offset6] > cb)\n                                    if(ptr[offset7] > cb)\n                                      if(ptr[offset8] > cb)\n                                        goto success_structured;\n                                      else\n                                        goto homogeneous;\n                                    else\n                                      goto homogeneous;\n                                  else\n                                    goto homogeneous;\n                              else\n                                if(ptr[offset8] > cb)\n                                  if(ptr[offset1] > cb)\n                                    if(ptr[offset2] > cb)\n```\n\n----------------------------------------\n\nTITLE: Starting Clojure REPL with OpenCV\nDESCRIPTION: This snippet shows how to start a Clojure REPL environment using Leiningen, preparing for OpenCV operations by importing necessary classes such as Mat, Imgcodecs, and Imgproc.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_21\n\nLANGUAGE: Clojure\nCODE:\n```\nlein repl\nnREPL server started on port 50624 on host 127.0.0.1\nREPL-y 0.3.0\nClojure 1.5.1\n    Docs: (doc function-name-here)\n          (find-doc \"part-of-name-here\")\n  Source: (source function-name-here)\n Javadoc: (javadoc java-object-or-class-here)\n    Exit: Control+D or (exit) or (quit)\n Results: Stored in vars *1, *2, *3, an exception in *e\n\nuser=> (import '[org.opencv.core Mat Size CvType]\n               '[org.opencv.imgcodecs Imgcodecs]\n               '[org.opencv.imgproc Imgproc])\norg.opencv.imgproc.Imgproc\n```\n\n----------------------------------------\n\nTITLE: Installing GPU Example Source Files in CMake\nDESCRIPTION: Uses the custom `ocv_install_example_src` CMake function to install the source files (*.cpp, *.hpp) and the CMakeLists.txt file associated with the 'gpu' examples.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nocv_install_example_src(gpu *.cpp *.hpp CMakeLists.txt)\n```\n\n----------------------------------------\n\nTITLE: Including Code in Documentation with @code Command - Doxygen Markup\nDESCRIPTION: Demonstrates using @code and @endcode to embed code blocks in documentation. Syntax highlighting is automatic based on file type, or can be overridden manually (e.g., with {.xml}). No external dependencies. Inputs are code text; output is formatted code in rendered documentation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n@code\nfloat val = img.at<float>(borderInterpolate(100, img.rows, cv::BORDER_REFLECT_101),\n                          borderInterpolate(-5, img.cols, cv::BORDER_WRAP));\n@endcode\n```\n\n----------------------------------------\n\nTITLE: Configuring udev Rules for Android Device Access - Linux (guess)\nDESCRIPTION: This snippet sets the necessary udev permissions to allow a Linux system to detect and grant appropriate access to an Android device via USB using its vendor ID. Place this line into /etc/udev/rules.d/51-android.rules, adjusting 'idVendor' as needed for your specific device, and assign the correct user group to permit device communication using adb. This configuration is required for device enumeration and debugging over USB with Android tools on Linux systems.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/android_binary_package/android_dev_intro.markdown#2025-04-22_snippet_0\n\nLANGUAGE: guess\nCODE:\n```\nSUBSYSTEM==\"usb\", ATTR{idVendor}==\"1004\",  MODE=\"0666\", GROUP=\"plugdev\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Corner Detection Logic in C++ for OpenCV\nDESCRIPTION: This snippet contains a series of nested if-else statements that compare pixel values at various offsets to determine if a point is a corner. It uses goto statements to branch to 'is_a_corner' or 'is_not_a_corner' based on the conditions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset11] < c_b)\n  if(ptr[offset1] < c_b)\n    if(ptr[offset3] < c_b)\n      goto is_a_corner;\n    else\n      if(ptr[offset8] < c_b)\n        goto is_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    if(ptr[offset6] < c_b)\n      if(ptr[offset7] < c_b)\n        if(ptr[offset8] < c_b)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\n\n// ... (additional nested conditions)\n\nif(ptr[offset5] > cb)\n  if(ptr[offset9] > cb)\n    if(ptr[offset6] > cb)\n      if(ptr[offset7] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset4] > cb)\n            if(ptr[offset3] > cb)\n              goto is_a_corner;\n            else\n              if(ptr[offset10] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset10] > cb)\n              if(ptr[offset11] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for OpenCV with OpenVX Support\nDESCRIPTION: CMake command line options to enable OpenVX support in OpenCV. The configuration requires specifying the path to prebuilt OpenVX and explicitly enabling the OpenVX module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openvx/hal/README.md#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n-DOPENVX_ROOT=/path/to/prebuilt/openvx -DWITH_OPENVX=YES\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example for Training Data\nDESCRIPTION: Shows the basic directory structure for organizing negative samples used in cascade classifier training.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/traincascade.markdown#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n/img\n  img1.jpg\n  img2.jpg\nbg.txt\n```\n\n----------------------------------------\n\nTITLE: Defining OpenCV ml Module with Bindings - CMake\nDESCRIPTION: This snippet defines the OpenCV Machine Learning (ml) module and its dependencies using the ocv_define_module command in CMake. The ml module depends on opencv_core and is enabled for Java, Objective-C, and Python wrappers. This setup allows the module to be built with support for these external interfaces, requiring the relevant language toolchains and wrapper generators to be available during the configuration process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/ml/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nocv_define_module(ml opencv_core WRAP java objc python)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV\nDESCRIPTION: Command to install the built OpenCV libraries and headers. May require root privileges depending on the installation location.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ make install\n```\n\n----------------------------------------\n\nTITLE: Defining and Configuring the OpenCV ObjDetect Module (CMake)\nDESCRIPTION: This CMake script defines the 'objdetect' module in the OpenCV build system, establishing its description and specifying dependencies such as core, imgproc, and calib3d modules. It also marks the DNN module as optional and enables wrapping for Python, Java, Objective-C, and JavaScript. The script further includes conditional statements to include QUIRC if present in the environment, configuring include directories and linking the appropriate library. Required dependencies include OpenCV's core modules and potentially QUIRC, which must be available for its code path to activate.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/objdetect/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(the_description \"Object Detection\")\nocv_define_module(objdetect\n    opencv_core\n    opencv_imgproc\n    opencv_calib3d\n    OPTIONAL\n        opencv_dnn\n    WRAP\n        python\n        java\n        objc\n        js\n)\n\nif(HAVE_QUIRC)\n    get_property(QUIRC_INCLUDE GLOBAL PROPERTY QUIRC_INCLUDE_DIR)\n    ocv_include_directories(${QUIRC_INCLUDE})\n    ocv_target_link_libraries(${the_module} quirc)\nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Pixel Comparisons in C++\nDESCRIPTION: This code snippet is part of the FAST corner detection algorithm. It performs pixel intensity comparisons at various offsets around a central pixel to determine if it's a corner. The algorithm uses goto statements for efficient branching based on comparison results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset4] > cb)\n  goto success_structured;\nelse\n  goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  if(ptr[offset6] > cb)\n    if(ptr[offset4] > cb)\n      if(ptr[offset3] > cb)\n        goto success_homogeneous;\n      else\n        if(ptr[offset10] > cb)\n          goto success_homogeneous;\n        else\n          goto homogeneous;\n    else\n      if(ptr[offset10] > cb)\n        if(ptr[offset11] > cb)\n          goto success_homogeneous;\n        else\n          goto homogeneous;\n      else\n        goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  if(ptr[offset10] > cb)\n    if(ptr[offset11] > cb)\n      if(ptr[offset1] > cb)\n        goto success_homogeneous;\n      else\n        if(ptr[offset6] > cb)\n          goto success_homogeneous;\n        else\n          goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  goto homogeneous;\nelse\n  if(ptr[offset7] < c_b)\n    if(ptr[offset5] < c_b)\n      if(ptr[offset2] < c_b)\n        if(ptr[offset6] < c_b)\n          if(ptr[offset4] < c_b)\n            if(ptr[offset3] < c_b)\n              if(ptr[offset1] < c_b)\n                goto success_homogeneous;\n              else\n                if(ptr[offset8] < c_b)\n                  goto success_homogeneous;\n                else\n                  goto homogeneous;\n            else\n              if(ptr[offset9] < c_b)\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n          else\n            if(ptr[offset9] < c_b)\n              if(ptr[offset8] < c_b)\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto success_structured;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n        else\n          goto homogeneous;\n      else\n        if(ptr[offset9] < c_b)\n          if(ptr[offset6] < c_b)\n            if(ptr[offset8] < c_b)\n              if(ptr[offset4] < c_b)\n                if(ptr[offset3] < c_b)\n                  goto success_homogeneous;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto success_homogeneous;\n                  else\n                    goto homogeneous;\n              else\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto success_homogeneous;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n            else\n              goto homogeneous;\n          else\n            goto homogeneous;\n        else\n          goto homogeneous;\n    else\n      goto homogeneous;\n  else\n    goto homogeneous;\nelse\nif(ptr[offset0] < c_b)\n  if(ptr[offset2] < c_b)\n    if(ptr[offset9] < c_b)\n      if(ptr[offset5] < c_b)\n        if(ptr[offset7] < c_b)\n          if(ptr[offset1] < c_b)\n            if(ptr[offset6] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset4] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto homogeneous;\n              else\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset4] < c_b)\n                      goto success_structured;\n                    else\n                      if(ptr[offset11] < c_b)\n                        goto success_structured;\n                      else\n                        goto structured;\n                  else\n                    goto homogeneous;\n                else\n                  goto homogeneous;\n            else\n              if(ptr[offset11] < c_b)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    goto success_structured;\n                  else\n                    if(ptr[offset10] < c_b)\n                      goto success_structured;\n                    else\n                      goto homogeneous;\n                else\n                  if(ptr[offset8] < c_b)\n                    if(ptr[offset10] < c_b)\n                      goto success_structured;\n                    else\n                      goto homogeneous;\n                  else\n                    goto homogeneous;\n              else\n                goto homogeneous;\n          else\n            if(ptr[offset6] < c_b)\n              if(ptr[offset8] < c_b)\n                if(ptr[offset4] < c_b)\n                  if(ptr[offset3] < c_b)\n                    goto success_structured;\n                  else\n                    if(ptr[offset10] < c_b)\n                      goto success_structured;\n                    else\n                      goto homogeneous;\n                else\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto success_structured;\n                    else\n                      goto homogeneous;\n                  else\n                    goto homogeneous;\n              else\n                goto homogeneous;\n            else\n              goto homogeneous;\n        else\n          if(ptr[offset1] < c_b)\n            if(ptr[offset11] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset4] < c_b)\n                  goto success_homogeneous;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto success_homogeneous;\n```\n\n----------------------------------------\n\nTITLE: Starting Clojure REPL with Leiningen\nDESCRIPTION: This snippet demonstrates how to start a Clojure REPL using Leiningen. It sets up a REPL environment necessary for Clojure development, enabling evaluation of Clojure expressions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd simple-sample\nlein repl\n...\n...\nnREPL server started on port 50907 on host 127.0.0.1\nREPL-y 0.3.0\nClojure 1.5.1\n    Docs: (doc function-name-here)\n          (find-doc \"part-of-name-here\")\n  Source: (source function-name-here)\n Javadoc: (javadoc java-object-or-class-here)\n    Exit: Control+D or (exit) or (quit)\n Results: Stored in vars *1, *2, *3, an exception in *e\n\nuser=>\n```\n\n----------------------------------------\n\nTITLE: AGAST Corner Detection Function in C++\nDESCRIPTION: Implementation of AGAST_7_12s corner detection algorithm that processes image data to detect corner points. The function handles image continuity checks, manages keypoint storage, and implements the core detection logic using pixel offset comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\nstatic void AGAST_7_12s(InputArray _img, std::vector<KeyPoint>& keypoints, int threshold)\n{\n    cv::Mat img;\n    if(!_img.getMat().isContinuous())\n      img = _img.getMat().clone();\n    else\n      img = _img.getMat();\n\n    size_t total = 0;\n    int xsize = img.cols;\n    int ysize = img.rows;\n    size_t nExpectedCorners = keypoints.capacity();\n    int x, y;\n    int xsizeB=xsize - 3;\n    int ysizeB=ysize - 2;\n    int width;\n\n    keypoints.resize(0);\n\n    int pixel_7_12s_[16];\n    makeAgastOffsets(pixel_7_12s_, (int)img.step, AgastFeatureDetector::AGAST_7_12s);\n\n    short offset0 = (short) pixel_7_12s_[0];\n    short offset1 = (short) pixel_7_12s_[1];\n    short offset2 = (short) pixel_7_12s_[2];\n    short offset3 = (short) pixel_7_12s_[3];\n    short offset4 = (short) pixel_7_12s_[4];\n    short offset5 = (short) pixel_7_12s_[5];\n    short offset6 = (short) pixel_7_12s_[6];\n    short offset7 = (short) pixel_7_12s_[7];\n    short offset8 = (short) pixel_7_12s_[8];\n    short offset9 = (short) pixel_7_12s_[9];\n    short offset10 = (short) pixel_7_12s_[10];\n    short offset11 = (short) pixel_7_12s_[11];\n\n    width = xsize;\n```\n\n----------------------------------------\n\nTITLE: Creating AgastFeatureDetector Instances with Factory Method\nDESCRIPTION: Factory method for creating AgastFeatureDetector instances. Uses the Implementation class and smart pointers to handle memory management for the detector objects.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_44\n\nLANGUAGE: C++\nCODE:\n```\nPtr<AgastFeatureDetector> AgastFeatureDetector::create( int threshold, bool nonmaxSuppression, int type )\n{\n    return makePtr<AgastFeatureDetector_Impl>(threshold, nonmaxSuppression, type);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Python3 Module Properties in CMake\nDESCRIPTION: Configures the Python3 module properties including description, module name, and installation subdirectory. These properties are used during the build process to properly generate and install the Python3 bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/python/python3/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(the_description \"The python3 bindings\")\nset(MODULE_NAME python3)\nset(MODULE_INSTALL_SUBDIR python3)\n\nset(PYTHON PYTHON3)\n```\n\n----------------------------------------\n\nTITLE: Injecting OpenCV Libraries in Leiningen Project\nDESCRIPTION: Modifies the project.clj file to automate loading of OpenCV native libraries in Clojure, removing the need for manual loading every time the REPL starts. Enhances productivity.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_15\n\nLANGUAGE: clojure\nCODE:\n```\n(defproject simple-sample \"0.1.0-SNAPSHOT\"\n  ...\ninjections [(clojure.lang.RT/loadLibrary org.opencv.core.Core/NATIVE_LIBRARY_NAME)])\n```\n\n----------------------------------------\n\nTITLE: Installing Compulsory Dependencies\nDESCRIPTION: Installs required dependencies including CMake, Python development tools, and compilation tools.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nyum install cmake\nyum install python-devel numpy\nyum install gcc gcc-c++\n```\n\n----------------------------------------\n\nTITLE: Two-class Discrete AdaBoost Algorithm in Mathematical Notation\nDESCRIPTION: Formal mathematical description of the AdaBoost algorithm implemented in OpenCV, including initialization, iteration steps, and classification formula. The algorithm combines weak classifiers to create a strong classifier for binary classification tasks.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/ml/doc/ml_intro.markdown#2025-04-22_snippet_1\n\nLANGUAGE: LaTeX\nCODE:\n```\n- Set \\f$N\\f$ examples \\f${(x_i,y_i)}1N\\f$ with \\f$x_i \\in{R^K}, y_i \\in{-1, +1}\\f$ .\n\n- Assign weights as \\f$w_i = 1/N, i = 1,...,N\\f$ .\n\n- Repeat for \\f$m = 1,2,...,M\\f$ :\n\n    - Fit the classifier \\f$f_m(x) \\in{-1,1}\\f$, using weights \\f$w_i\\f$ on the training data.\n\n    - Compute \\f$err_m = E_w [1_{(y \\neq f_m(x))}], c_m = log((1 - err_m)/err_m)\\f$ .\n\n    - Set \\f$w_i \\Leftarrow w_i exp[c_m 1_{(y_i \\neq f_m(x_i))}], i = 1,2,...,N,\\f$ and\n        renormalize so that \\f$\\Sigma i w_i = 1\\f$ .\n\n- Classify new samples _x_ using the formula: \\f$\\textrm{sign} (\\Sigma m = 1M c_m f_m(x))\\f$ .\n```\n\n----------------------------------------\n\nTITLE: Creating Generator Configuration File in CMake\nDESCRIPTION: Generates a JSON configuration file that includes the root directory, build directory, and modules information. The file is only updated if the content has changed to avoid unnecessary rebuilds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/objc/generator/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_opencv_objc)\n  set(__objc_build_dir \"\\\"objc_build_dir\\\": \\\"${CMAKE_CURRENT_BINARY_DIR}/../objc\\\",\")\nendif()\n\nset(CONFIG_FILE \"${CMAKE_CURRENT_BINARY_DIR}/gen_objc.json\")\nset(__config_str\n\"{\n  \\\"rootdir\\\": \\\"${OpenCV_SOURCE_DIR}\\\",\n  ${__objc_build_dir}\n  \\\"modules\\\": [\n${__modules_config}\n  ]\n}\n\")\n#TODO: ocv_update_file(\"${CONFIG_FILE}\" \"${__config_str}\" ON_CHANGE_REMOVE \"${OPENCV_DEPHELPER}/gen_opencv_objc_source\")\nif(EXISTS \"${CONFIG_FILE}\")\n  file(READ \"${CONFIG_FILE}\" __content)\nelse()\n  set(__content \"\")\nendif()\nif(NOT \"${__content}\" STREQUAL \"${__config_str}\")\n  file(WRITE \"${CONFIG_FILE}\" \"${__config_str}\")\n  file(REMOVE \"${OPENCV_DEPHELPER}/gen_opencv_objc_source\")\nendif()\nunset(__config_str)\n```\n\n----------------------------------------\n\nTITLE: Defining a Point in Python\nDESCRIPTION: How to define a 2D point using a tuple in Python for OpenCV functions. The tuple contains x and y coordinates.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/basic_geometric_drawing/basic_geometric_drawing.markdown#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npt = (10, 0) # x = 10, y = 0\n```\n\n----------------------------------------\n\nTITLE: Example Output of pkg-config for OpenCV Include Paths\nDESCRIPTION: This shows example output from the `pkg-config --cflags opencv` command. It lists the directories (`-I/usr/local/include/opencv` and `-I/usr/local/include`) where the compiler should search for OpenCV header files. This output can be used to configure the include paths in Eclipse.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/linux_eclipse/linux_eclipse.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n-I/usr/local/include/opencv -I/usr/local/include\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Android Camera Tutorial Build\nDESCRIPTION: CMake configuration for building an OpenCV Android camera control tutorial example. Sets the project name, adds Android build target with OpenCV library dependencies, and specifies minimum SDK version 11.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-3-cameracontrol/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(sample example-tutorial-3-cameracontrol)\n\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\" LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\" SDK_TARGET 11 \"${ANDROID_SDK_TARGET}\")\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Scoring AGAST Corners with OAST_9_16 in OpenCV (C++)\nDESCRIPTION: Implements a template specialization for agast_cornerScore with OAST_9_16, which calculates the AGAST score for a given image pixel. The function uses precomputed neighborhood offsets and a binary search approach to threshold testing for rapid corner classification. Depends on OpenCV data types and assumes ptr, pixel, and threshold are valid. Input is a pointer to the source pixel, a list of pixel offsets, and a detection threshold; computations involve intensity comparisons and early exits based on the AGAST algorithm. Output is an integer corner score, and the code is heavily optimized for branching.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n// 16 pixel mask\ntemplate<>\nint agast_cornerScore<AgastFeatureDetector::OAST_9_16>(const uchar* ptr, const int pixel[], int threshold)\n{\n    int bmin = threshold;\n    int bmax = 255;\n    int b_test = (bmax + bmin) / 2;\n\n    short offset0 = (short) pixel[0];\n    short offset1 = (short) pixel[1];\n    short offset2 = (short) pixel[2];\n    short offset3 = (short) pixel[3];\n    short offset4 = (short) pixel[4];\n    short offset5 = (short) pixel[5];\n    short offset6 = (short) pixel[6];\n    short offset7 = (short) pixel[7];\n    short offset8 = (short) pixel[8];\n    short offset9 = (short) pixel[9];\n    short offset10 = (short) pixel[10];\n    short offset11 = (short) pixel[11];\n    short offset12 = (short) pixel[12];\n    short offset13 = (short) pixel[13];\n    short offset14 = (short) pixel[14];\n    short offset15 = (short) pixel[15];\n\n    while(true)\n    {\n        const int cb = *ptr + b_test;\n        const int c_b = *ptr - b_test;\n        if(ptr[offset0] > cb)\n          if(ptr[offset2] > cb)\n            if(ptr[offset4] > cb)\n              if(ptr[offset5] > cb)\n                if(ptr[offset7] > cb)\n                  if(ptr[offset3] > cb)\n                    if(ptr[offset1] > cb)\n                      if(ptr[offset6] > cb)\n                        if(ptr[offset8] > cb)\n                          goto is_a_corner;\n                        else\n                          if(ptr[offset15] > cb)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset13] > cb)\n                          if(ptr[offset14] > cb)\n                            if(ptr[offset15] > cb)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      if(ptr[offset8] > cb)\n                        if(ptr[offset9] > cb)\n                          if(ptr[offset10] > cb)\n                            if(ptr[offset6] > cb)\n                              goto is_a_corner;\n                            else\n                              if(ptr[offset11] > cb)\n                                if(ptr[offset12] > cb)\n                                  if(ptr[offset13] > cb)\n                                    if(ptr[offset14] > cb)\n                                      if(ptr[offset15] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset10] > cb)\n                      if(ptr[offset11] > cb)\n                        if(ptr[offset12] > cb)\n                          if(ptr[offset8] > cb)\n                            if(ptr[offset9] > cb)\n                              if(ptr[offset6] > cb)\n                                goto is_a_corner;\n                              else\n                                if(ptr[offset13] > cb)\n                                  if(ptr[offset14] > cb)\n                                    if(ptr[offset15] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                            else\n                              if(ptr[offset1] > cb)\n                                if(ptr[offset13] > cb)\n                                  if(ptr[offset14] > cb)\n                                    if(ptr[offset15] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            if(ptr[offset1] > cb)\n                              if(ptr[offset13] > cb)\n                                if(ptr[offset14] > cb)\n                                  if(ptr[offset15] > cb)\n                                    goto is_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                if(ptr[offset7] < c_b)\n                  if(ptr[offset14] > cb)\n                    if(ptr[offset15] > cb)\n                      if(ptr[offset1] > cb)\n                        if(ptr[offset3] > cb)\n                          if(ptr[offset6] > cb)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset13] > cb)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset10] > cb)\n                            if(ptr[offset11] > cb)\n                              if(ptr[offset12] > cb)\n                                if(ptr[offset13] > cb)\n\n```\n\n----------------------------------------\n\nTITLE: Setting JPEG Quality Levels in C\nDESCRIPTION: Demonstrates how to set separate quality levels for luminance and chrominance in JPEG compression using libjpeg. It also shows how to disable chrominance subsampling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_43\n\nLANGUAGE: C\nCODE:\n```\njpeg_set_defaults(cinfo);\n\n/* Set luminance quality 90. */\ncinfo->q_scale_factor[0] = jpeg_quality_scaling(90);\n/* Set chrominance quality 70. */\ncinfo->q_scale_factor[1] = jpeg_quality_scaling(70);\n\njpeg_default_qtables(cinfo, force_baseline);\n\n/* Disable chrominance subsampling */\ncinfo->comp_info[0].v_samp_factor = 1;\ncinfo->comp_info[0].h_samp_factor = 1;\n```\n\n----------------------------------------\n\nTITLE: Configuring APT Sources for MultiArch on Ubuntu 23.04 (Unparsed)\nDESCRIPTION: Example content to be added to `/etc/apt/sources.list` (or a file in `/etc/apt/sources.list.d/`) using `sudo apt edit-sources`. These lines configure APT to find packages for `arm64` and `armhf` architectures in the Ubuntu 23.04 (Lunar) repositories.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_4\n\nLANGUAGE: unparsed\nCODE:\n```\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports lunar main restricted\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports lunar-updates main restricted\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports lunar universe\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports lunar-updates universe\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports lunar multiverse\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports lunar-updates multiverse\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports lunar-backports main restricted universe multiverse\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports lunar-security main restricted\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports lunar-security universe\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports lunar-security multiverse\n```\n\n----------------------------------------\n\nTITLE: Implementing Pixel Comparison Logic for Feature Detection in OpenCV C++\nDESCRIPTION: This code snippet implements a decision tree algorithm for feature detection in images. It compares pixel values at various offsets against thresholds ('cb' and 'c_b') and uses control flow with goto statements to efficiently handle different detection cases. This is likely part of a FAST or similar corner detection algorithm in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_25\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset8] > cb)\n  if(ptr[offset5] > cb)\n    if(ptr[offset1] > cb)\n      if(ptr[offset10] > cb)\n        if(ptr[offset11] > cb)\n          goto success_structured;\n        else\n          if(ptr[offset6] > cb)\n            if(ptr[offset4] > cb)\n              goto success_structured;\n            else\n              goto structured;\n          else\n            goto structured;\n      else\n        if(ptr[offset6] > cb)\n          if(ptr[offset3] > cb)\n            if(ptr[offset4] > cb)\n              goto success_structured;\n            else\n              goto structured;\n          else\n            goto structured;\n        else\n          goto structured;\n    else\n      if(ptr[offset6] > cb)\n        if(ptr[offset4] > cb)\n          if(ptr[offset3] > cb)\n            goto success_structured;\n          else\n            if(ptr[offset10] > cb)\n              goto success_structured;\n            else\n              goto structured;\n        else\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              goto success_structured;\n            else\n              goto structured;\n          else\n            goto structured;\n      else\n        goto structured;\n  else\n    if(ptr[offset10] > cb)\n      if(ptr[offset11] > cb)\n        if(ptr[offset1] > cb)\n          goto success_structured;\n        else\n          if(ptr[offset6] > cb)\n            goto success_structured;\n          else\n            goto structured;\n      else\n        goto structured;\n    else\n      goto structured;\nelse\n  goto structured;\nelse\n  goto structured;\nelse\n  if(ptr[offset7] < c_b)\n    if(ptr[offset5] < c_b)\n      if(ptr[offset2] < c_b)\n        if(ptr[offset6] < c_b)\n          if(ptr[offset4] < c_b)\n            if(ptr[offset3] < c_b)\n              if(ptr[offset1] < c_b)\n                goto success_structured;\n              else\n                if(ptr[offset8] < c_b)\n                  goto success_structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset9] < c_b)\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n          else\n            if(ptr[offset9] < c_b)\n              if(ptr[offset8] < c_b)\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n              else\n                goto structured;\n            else\n              goto structured;\n        else\n          goto structured;\n      else\n        if(ptr[offset9] < c_b)\n          if(ptr[offset6] < c_b)\n            if(ptr[offset8] < c_b)\n              if(ptr[offset4] < c_b)\n                if(ptr[offset3] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n              else\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              goto structured;\n          else\n            goto structured;\n        else\n          goto structured;\n    else\n      goto structured;\n  else\n    goto structured;\nelse\nif(ptr[offset0] < c_b)\n  if(ptr[offset2] < c_b)\n    if(ptr[offset11] < c_b)\n      if(ptr[offset3] < c_b)\n        if(ptr[offset5] < c_b)\n          if(ptr[offset9] < c_b)\n            if(ptr[offset7] < c_b)\n              if(ptr[offset1] < c_b)\n                if(ptr[offset4] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset8] < c_b)\n                    if(ptr[offset4] < c_b)\n                      goto success_structured;\n                    else\n                      if(ptr[offset10] < c_b)\n                        goto success_structured;\n                      else\n                        goto structured;\n                  else\n                    goto structured;\n                else\n                  goto structured;\n            else\n              if(ptr[offset1] < c_b)\n                if(ptr[offset4] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset10] < c_b)\n                    goto success_structured;\n                  else\n                    goto structured;\n              else\n                goto structured;\n          else\n            if(ptr[offset4] < c_b)\n              if(ptr[offset7] < c_b)\n                if(ptr[offset1] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset8] < c_b)\n                      goto success_structured;\n                    else\n                      goto structured;\n                  else\n                    goto structured;\n              else\n                if(ptr[offset1] < c_b)\n                  goto success_structured;\n                else\n                  goto structured;\n            else\n              goto structured;\n        else\n          if(ptr[offset10] < c_b)\n            if(ptr[offset9] < c_b)\n              if(ptr[offset7] < c_b)\n                if(ptr[offset1] < c_b)\n                  goto success_structured;\n                else\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset8] < c_b)\n```\n\n----------------------------------------\n\nTITLE: AGAST 7-12s Corner Score Template Implementation in C++\nDESCRIPTION: Template specialization implementing the AGAST corner scoring algorithm with a 7-12 square pixel mask pattern. The function performs pixel comparisons using a decision tree to determine if a point is a corner based on intensity differences with neighboring pixels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\ntemplate<>\nint agast_cornerScore<AgastFeatureDetector::AGAST_7_12s>(const uchar* ptr, const int pixel[], int threshold)\n{\n    int bmin = threshold;\n    int bmax = 255;\n    int b_test = (bmax + bmin)/2;\n\n    short offset0 = (short) pixel[0];\n    short offset1 = (short) pixel[1];\n    short offset2 = (short) pixel[2];\n    short offset3 = (short) pixel[3];\n    short offset4 = (short) pixel[4];\n    short offset5 = (short) pixel[5];\n    short offset6 = (short) pixel[6];\n    short offset7 = (short) pixel[7];\n    short offset8 = (short) pixel[8];\n    short offset9 = (short) pixel[9];\n    short offset10 = (short) pixel[10];\n    short offset11 = (short) pixel[11];\n\n    while(true)\n    {\n        const int cb = *ptr + b_test;\n        const int c_b = *ptr - b_test;\n        if(ptr[offset0] > cb)\n          if(ptr[offset5] > cb)\n            if(ptr[offset2] < c_b)\n              if(ptr[offset7] > cb)\n                if(ptr[offset9] < c_b)\n                  goto is_not_a_corner;\n                else\n                  if(ptr[offset9] > cb)\n                    if(ptr[offset1] < c_b)\n                      if(ptr[offset6] < c_b)\n                        goto is_not_a_corner;\n                      else\n                        if(ptr[offset6] > cb)\n                          if(ptr[offset8] > cb)\n                            if(ptr[offset4] > cb)\n                              if(ptr[offset3] > cb)\n                                goto is_a_corner;\n                              else\n                                if(ptr[offset10] > cb)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                            else\n                              if(ptr[offset10] > cb)\n                                if(ptr[offset11] > cb)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Pixel Comparison Logic in C++\nDESCRIPTION: Nested conditional logic for comparing pixel intensities against threshold values in FAST corner detection. The code checks surrounding pixels against center brightness (cb) and darker threshold (c_b) values to determine corner points.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_40\n\nLANGUAGE: cpp\nCODE:\n```\ncontinue; // goto homogeneous;\nelse\n  if(ptr[offset9] < c_b)\n    if(ptr[offset10] < c_b)\n      if(ptr[offset11] < c_b)\n        if(ptr[offset8] < c_b)\n          if(ptr[offset12] < c_b)\n            if(ptr[offset13] < c_b)\n              if(ptr[offset14] < c_b)\n                if(ptr[offset15] < c_b)\n                  {} // goto success_homogeneous;\n                else\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset7] < c_b)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Binary Search Implementation\nDESCRIPTION: A binary search implementation for corner detection that compares pixel values at different offsets to determine if a point is a corner. Uses goto statements for control flow and maintains binary search bounds (bmin, bmax) to converge on the optimal threshold value.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_44\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset3] < c_b)\n    if(ptr[offset5] < c_b)\n      if(ptr[offset1] < c_b)\n        if(ptr[offset4] < c_b)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        if(ptr[offset4] < c_b)\n          if(ptr[offset6] < c_b)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      if(ptr[offset1] < c_b)\n        if(ptr[offset4] < c_b)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    if(ptr[offset5] > cb)\n      if(ptr[offset3] > cb)\n        if(ptr[offset4] > cb)\n          if(ptr[offset6] > cb)\n            goto is_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n\nis_a_corner:\n    bmin=b_test;\n    goto end;\n\nis_not_a_corner:\n    bmax=b_test;\n    goto end;\n\nend:\n\nif(bmin == bmax - 1 || bmin == bmax)\n    return bmin;\nb_test = (bmin + bmax) / 2;\n}\n\n} // namespace cv\n```\n\n----------------------------------------\n\nTITLE: Declaring Small Value-Passed Structs with OpenCV Macros in C++\nDESCRIPTION: This snippet defines the DMatch struct for matching features in OpenCV, using CV_EXPORTS_W_SIMPLE to mark it for Python export. Constructors and fields are annotated with CV_WRAP and CV_PROP_RW, respectively, indicating which methods and data members should be accessible in Python. Required dependencies include OpenCV headers. Instances can be directly manipulated in both C++ and Python, with fields controlling indices and distances.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_bindings/py_bindings_basics/py_bindings_basics.markdown#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nclass CV_EXPORTS_W_SIMPLE DMatch\n{\npublic:\n    CV_WRAP DMatch();\n    CV_WRAP DMatch(int _queryIdx, int _trainIdx, float _distance);\n    CV_WRAP DMatch(int _queryIdx, int _trainIdx, int _imgIdx, float _distance);\n\n    CV_PROP_RW int queryIdx; // query descriptor index\n    CV_PROP_RW int trainIdx; // train descriptor index\n    CV_PROP_RW int imgIdx;   // train image index\n\n    CV_PROP_RW float distance;\n};\n```\n\n----------------------------------------\n\nTITLE: Linking OpenCV Java Library in CMake\nDESCRIPTION: Configures linking for the OpenCV Java library, handling different scenarios for fat Java library builds and platform-specific linking rules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jni/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(__deps ${OPENCV_MODULE_${the_module}_DEPS})\nlist(REMOVE_ITEM __deps opencv_java_bindings_generator) # don't add dummy module to target_link_libraries list\n\nif(BUILD_FAT_JAVA_LIB)\n  ocv_list_unique(__deps)\n  set(__extradeps ${__deps})\n  ocv_list_filterout(__extradeps \"^opencv_\")\n  if(__extradeps)\n    list(REMOVE_ITEM __deps ${__extradeps})\n  endif()\n  if(APPLE)\n    foreach(_dep ${__deps})\n      ocv_target_link_libraries(${the_module} PRIVATE -Wl,-force_load \"${_dep}\")\n    endforeach()\n  elseif(((CV_GCC OR CV_CLANG OR UNIX) OR (OPENCV_FORCE_FAT_JAVA_LIB_LD_RULES)) AND (NOT OPENCV_SKIP_FAT_JAVA_LIB_LD_RULES))\n    ocv_target_link_libraries(${the_module} PRIVATE -Wl,-whole-archive ${__deps} -Wl,-no-whole-archive)\n  else()\n    ocv_target_link_libraries(${the_module} PRIVATE ${__deps})\n  endif()\n  ocv_target_link_libraries(${the_module} PRIVATE ${__extradeps} ${OPENCV_LINKER_LIBS})\nelse()\n  ocv_target_link_libraries(${the_module} PRIVATE ${__deps} ${OPENCV_LINKER_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Build Options in CMake\nDESCRIPTION: This snippet defines various CMake options for configuring the OpenCV build. It includes options for building shared libraries, examples, documentation, and platform-specific features.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nOCV_OPTION(BUILD_SHARED_LIBS        \"Build shared libraries (.dll/.so) instead of static ones (.lib/.a)\" NOT (ANDROID OR APPLE_FRAMEWORK) )\nOCV_OPTION(BUILD_opencv_apps        \"Build utility applications (used for example to train classifiers)\" (NOT ANDROID AND NOT WINRT) IF (NOT APPLE_FRAMEWORK) )\nOCV_OPTION(BUILD_opencv_js          \"Build JavaScript bindings by Emscripten\" OFF )\nOCV_OPTION(BUILD_ANDROID_PROJECTS   \"Build Android projects providing .apk files\" ON  IF ANDROID )\nOCV_OPTION(BUILD_ANDROID_EXAMPLES   \"Build examples for Android platform\"         ON  IF ANDROID )\nOCV_OPTION(BUILD_DOCS               \"Create build rules for OpenCV Documentation\" OFF  IF (NOT WINRT AND NOT APPLE_FRAMEWORK))\nOCV_OPTION(BUILD_EXAMPLES           \"Build all examples\"                          OFF )\nOCV_OPTION(BUILD_PACKAGE            \"Enables 'make package_source' command\"       ON  IF NOT WINRT)\nOCV_OPTION(BUILD_PERF_TESTS         \"Build performance tests\"                     NOT INSTALL_CREATE_DISTRIB  IF (NOT APPLE_FRAMEWORK) )\nOCV_OPTION(BUILD_TESTS              \"Build accuracy & regression tests\"           NOT INSTALL_CREATE_DISTRIB  IF (NOT APPLE_FRAMEWORK) )\nOCV_OPTION(BUILD_WITH_DEBUG_INFO    \"Include debug info into release binaries ('OFF' means default settings)\" OFF )\nOCV_OPTION(BUILD_WITH_STATIC_CRT    \"Enables use of statically linked CRT for statically linked OpenCV\" ON IF MSVC )\nOCV_OPTION(BUILD_WITH_DYNAMIC_IPP   \"Enables dynamic linking of IPP (only for standalone IPP)\" OFF )\nOCV_OPTION(BUILD_FAT_JAVA_LIB       \"Create Java wrapper exporting all functions of OpenCV library (requires static build of OpenCV modules)\" ANDROID IF NOT BUILD_SHARED_LIBS)\nOCV_OPTION(BUILD_ANDROID_SERVICE    \"Build OpenCV Manager for Google Play\" OFF IF ANDROID )\nOCV_OPTION(BUILD_CUDA_STUBS         \"Build CUDA modules stubs when no CUDA SDK\" OFF  IF (NOT APPLE_FRAMEWORK) )\nOCV_OPTION(BUILD_JAVA               \"Enable Java support\"                         (ANDROID OR NOT CMAKE_CROSSCOMPILING)  IF (ANDROID OR (NOT APPLE_FRAMEWORK AND NOT WINRT)) )\nOCV_OPTION(BUILD_OBJC               \"Enable Objective-C support\"                  ON  IF APPLE_FRAMEWORK )\nOCV_OPTION(BUILD_KOTLIN_EXTENSIONS  \"Build Kotlin extensions (Android)\"           ON  IF ANDROID )\n```\n\n----------------------------------------\n\nTITLE: Generating Configuration JSON (`gen_java.json`) for Java Bindings in CMake\nDESCRIPTION: Constructs the content for the `gen_java.json` file. This JSON file provides configuration to the Python generator script, including the OpenCV source root directory, a list of modules to process (using `__modules_config` generated previously), and a list of remapped template files (using `__remap_config`). It checks if the existing `gen_java.json` file content differs from the newly generated content; if so, it overwrites the file and removes a dependency helper file (`gen_opencv_java_source`) to ensure the generator script is re-run.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/generator/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(CONFIG_FILE \"${CMAKE_CURRENT_BINARY_DIR}/gen_java.json\")\nset(__config_str\n\"{\n  \\\"rootdir\\\": \\\"${OpenCV_SOURCE_DIR}\\\",\n  \\\"modules\\\": [\n${__modules_config}\n  ],\n  \\\"files_remap\\\": [\n${__remap_config}\n  ]\n}\n\")\nif(EXISTS \"${CONFIG_FILE}\")\n  file(READ \"${CONFIG_FILE}\" __content)\nelse()\n  set(__content \"\")\nendif()\nif(NOT \"${__content}\" STREQUAL \"${__config_str}\")\n  file(WRITE \"${CONFIG_FILE}\" \"${__config_str}\")\n  file(REMOVE \"${OPENCV_DEPHELPER}/gen_opencv_java_source\")\nendif()\nunset(__config_str)\n```\n\n----------------------------------------\n\nTITLE: Adding Dispatched Source File for Winograd Convolution Kernel - CMake\nDESCRIPTION: This snippet registers the Winograd convolution kernel implementation ('conv_winograd_f63') for multiple architectures via 'ocv_add_dispatched_file'. It allows these kernels to be built for the specified hardware. Parameters define which CPU instruction sets to use. No external dependencies are required beyond supported toolchains.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nocv_add_dispatched_file(\"layers/cpu_kernels/conv_winograd_f63\" AVX AVX2 NEON NEON_FP16)\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for Python Bindings\nDESCRIPTION: Configures Python bindings generation in OpenCV using CMake. Includes setting module names, directories, and list of Python modules. It uses conditions and commands to manage module wrapping, dependencies, and custom targets. Key operations include directory configuration, list building, file writing, and command creation. Dependencies include various Python scripts and OpenCV module headers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/python/bindings/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(MODULE_NAME \"python_bindings_generator\")\nset(OPENCV_MODULE_IS_PART_OF_WORLD FALSE)\nocv_add_module(${MODULE_NAME} INTERNAL)\n\nset(OPENCV_PYTHON_SIGNATURES_FILE \"${CMAKE_CURRENT_BINARY_DIR}/pyopencv_signatures.json\" CACHE INTERNAL \"\")\nset(OPENCV_PYTHON_BINDINGS_DIR \"${CMAKE_CURRENT_BINARY_DIR}\" CACHE INTERNAL \"\")\n\n# This file is included from a subdirectory\nset(PYTHON_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../\")\n\nif(NOT OPENCV_SKIP_PYTHON_LOADER)\n  include(\"${PYTHON_SOURCE_DIR}/python_loader.cmake\")\nendif()\n\n# get list of modules to wrap\nset(OPENCV_PYTHON_MODULES)\nforeach(m ${OPENCV_MODULES_BUILD})\n  if (\";${OPENCV_MODULE_${m}_WRAPPERS};\" MATCHES \";python;\" AND HAVE_${m})\n    list(APPEND OPENCV_PYTHON_MODULES ${m})\n    #message(STATUS \"\\t${m}\")\n  endif()\nendforeach()\n\nset(opencv_hdrs \"\")\nset(opencv_userdef_hdrs \"\")\nforeach(m ${OPENCV_PYTHON_MODULES})\n  foreach (hdr ${OPENCV_MODULE_${m}_HEADERS})\n    ocv_is_subdir(is_sub \"${OPENCV_MODULE_${m}_LOCATION}/include\" \"${hdr}\")\n    if(is_sub)\n      list(APPEND opencv_hdrs \"${hdr}\")\n    endif()\n  endforeach()\n\n  # both wrapping and C++ implementation\n  file(GLOB hdr2 ${OPENCV_MODULE_${m}_LOCATION}/misc/python/python_*.hpp)\n  list(SORT hdr2)\n  list(APPEND opencv_hdrs ${hdr2})\n  list(APPEND opencv_userdef_hdrs ${hdr2})\n\n  file(GLOB hdr ${OPENCV_MODULE_${m}_LOCATION}/misc/python/shadow*.hpp)\n  list(SORT hdr)\n  list(APPEND opencv_hdrs ${hdr})\n  file(GLOB userdef_hdrs ${OPENCV_MODULE_${m}_LOCATION}/misc/python/pyopencv*.hpp)\n  list(SORT userdef_hdrs)\n  list(APPEND opencv_userdef_hdrs ${userdef_hdrs})\nendforeach(m)\n\n# header blacklist\nocv_list_filterout(opencv_hdrs \"modules/.*\\\\.h$\")\nocv_list_filterout(opencv_hdrs \"modules/core/.*/cuda/\")\nocv_list_filterout(opencv_hdrs \"modules/core/.*/hal/\")\nocv_list_filterout(opencv_hdrs \"modules/core/.*/opencl/\")\nocv_list_filterout(opencv_hdrs \"modules/.+/utils/.*\")\nocv_list_filterout(opencv_hdrs \"modules/.*\\\\.inl\\\\.h*\")\nocv_list_filterout(opencv_hdrs \"modules/.*_inl\\\\.h*\")\nocv_list_filterout(opencv_hdrs \"modules/.*\\\\.details\\\\.h*\")\nocv_list_filterout(opencv_hdrs \"modules/.*\\\\.private\\\\.h*\")\nocv_list_filterout(opencv_hdrs \"modules/.*/private\\\\.h*\")\nocv_list_filterout(opencv_hdrs \"modules/.*/legacy/.*\")\nocv_list_filterout(opencv_hdrs \"modules/.*/detection_based_tracker\\\\.hpp\") # Conditional compilation\nif(NOT HAVE_CUDA)\n  ocv_list_filterout(opencv_hdrs \"modules/cuda.*\")\n  ocv_list_filterout(opencv_hdrs \"modules/cudev\")\nendif()\n\nset(cv2_generated_files\n    \"${CMAKE_CURRENT_BINARY_DIR}/pyopencv_generated_enums.h\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/pyopencv_generated_funcs.h\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/pyopencv_generated_include.h\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/pyopencv_generated_modules.h\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/pyopencv_generated_modules_content.h\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/pyopencv_generated_types.h\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/pyopencv_generated_types_content.h\"\n    \"${OPENCV_PYTHON_SIGNATURES_FILE}\"\n)\n\nstring(REPLACE \";\" \"\\n\" opencv_hdrs_ \"${opencv_hdrs}\")\nfile(WRITE \"${CMAKE_CURRENT_BINARY_DIR}/headers.txt\" \"${opencv_hdrs_}\")\nfile(GLOB_RECURSE typing_stubs_generation_files \"${PYTHON_SOURCE_DIR}/src2/typing_stubs_generation/*.py\")\nadd_custom_command(\n    OUTPUT ${cv2_generated_files}\n    COMMAND \"${PYTHON_DEFAULT_EXECUTABLE}\" \"${PYTHON_SOURCE_DIR}/src2/gen2.py\" \"${CMAKE_CURRENT_BINARY_DIR}\" \"${CMAKE_CURRENT_BINARY_DIR}/headers.txt\"\n    DEPENDS \"${PYTHON_SOURCE_DIR}/src2/gen2.py\"\n            \"${PYTHON_SOURCE_DIR}/src2/hdr_parser.py\"\n            \"${typing_stubs_generation_files}\"\n            \"${PYTHON_SOURCE_DIR}/src2/typing_stubs_generator.py\"\n            # not a real build dependency (file(WRITE) result): ${CMAKE_CURRENT_BINARY_DIR}/headers.txt\n            ${opencv_hdrs}\n    COMMENT \"Generate files for Python bindings and documentation\"\n)\n\nadd_custom_target(gen_opencv_python_source DEPENDS ${cv2_generated_files})\n\nif(TARGET copy_opencv_typing_stubs)\n  add_dependencies(copy_opencv_typing_stubs gen_opencv_python_source)\nendif()\n\nset(cv2_custom_hdr \"${CMAKE_CURRENT_BINARY_DIR}/pyopencv_custom_headers.h\")\nset(cv2_custom_hdr_str \"//user-defined headers\\n\")\nforeach(uh ${opencv_userdef_hdrs})\n    set(cv2_custom_hdr_str \"${cv2_custom_hdr_str}#include \\\"${uh}\\\"\\n\")\nendforeach(uh)\nif(EXISTS \"${cv2_custom_hdr}\")\n  file(READ \"${cv2_custom_hdr}\" __content)\nelse()\n  set(__content \"\")\nendif()\nif(\"${__content}\" STREQUAL \"${cv2_custom_hdr_str}\")\n  # Up-to-date\nelse()\n  file(WRITE \"${cv2_custom_hdr}\" \"${cv2_custom_hdr_str}\")\nendif()\nunset(__content)\n\n\n#\n# Configuration for standalone build of Python bindings\n#\nset(PYTHON_CONFIG_SCRIPT \"\")\nocv_cmake_script_append_var(PYTHON_CONFIG_SCRIPT\n    CMAKE_BUILD_TYPE\n    BUILD_SHARED_LIBS\n\n    CMAKE_C_FLAGS CMAKE_C_FLAGS_DEBUG CMAKE_C_FLAGS_RELEASE\n    CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_DEBUG CMAKE_CXX_FLAGS_RELEASE\n\n    CV_GCC CV_CLANG ENABLE_NOISY_WARNINGS\n\n    CMAKE_MODULE_LINKER_FLAGS\n    CMAKE_INSTALL_PREFIX\n    OPENCV_PYTHON_INSTALL_PATH\n\n    OpenCV_SOURCE_DIR\n\n    OPENCV_FORCE_PYTHON_LIBS\n    OPENCV_PYTHON_SKIP_LINKER_EXCLUDE_LIBS\n\n    OPENCV_PYTHON_BINDINGS_DIR\n    cv2_custom_hdr\n    cv2_generated_files\n)\nset(CMAKE_HELPER_SCRIPT \"${CMAKE_BINARY_DIR}/opencv_python_config.cmake\")\nfile(GENERATE OUTPUT \"${CMAKE_HELPER_SCRIPT}\" CONTENT \"${PYTHON_CONFIG_SCRIPT}\")\n```\n\n----------------------------------------\n\nTITLE: Running OpenCV.js Node.js Tests - Shell\nDESCRIPTION: This shell script initializes the Node.js environment for OpenCV.js by changing to the build output directory, installing dependencies using npm, and running the JavaScript test suite. Dependencies include Node.js (ideally version 8.x) and npm. Input consists of local build artifacts in the 'build_js/bin' directory, and output is test results printed to the console. Ensure Node.js is properly installed and that all necessary permissions are granted.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\ncd build_js/bin\\nnpm install\\nnode tests.js\n```\n\n----------------------------------------\n\nTITLE: Mimicking OpenCV Java Examples in Clojure\nDESCRIPTION: Translates an OpenCV Java example into Clojure using REPL, illustrating how to perform matrix modifications and data display by intermingling Java interop in Clojure.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\nimport org.opencv.core.Mat;\nimport org.opencv.core.CvType;\nimport org.opencv.core.Scalar;\n\nclass SimpleSample {\n\n  static{ System.loadLibrary(\"opencv_java244\"); }\n\n  public static void main(String[] args) {\n    Mat m = new Mat(5, 10, CvType.CV_8UC1, new Scalar(0));\n    System.out.println(\"OpenCV Mat: \" + m);\n    Mat mr1 = m.row(1);\n    mr1.setTo(new Scalar(1));\n    Mat mc5 = m.col(5);\n    mc5.setTo(new Scalar(5));\n    System.out.println(\"OpenCV Mat data:\\n\" + m.dump());\n  }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Suppressing Deprecated C Warnings on Windows (MSVC) in CMake\nDESCRIPTION: Adds specific preprocessor definitions when compiling with MSVC on Windows (excluding Borland, Cygwin, MinGW) to suppress warnings about deprecated C standard library functions, unless explicitly enabled via `ITK_ENABLE_VISUAL_STUDIO_DEPRECATED_C_WARNINGS`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\n# --------------------------------------------------------------------------\n# On Visual Studio 8 MS deprecated C. This removes all 1.276E1265 security\n# warnings\nif(WIN32)\n  if(NOT BORLAND)\n    if(NOT CYGWIN)\n      if(NOT MINGW)\n        if(NOT ITK_ENABLE_VISUAL_STUDIO_DEPRECATED_C_WARNINGS)\n          add_definitions(\n            -D_CRT_FAR_MAPPINGS_NO_DEPRECATE\n            -D_CRT_IS_WCTYPE_NO_DEPRECATE\n            -D_CRT_MANAGED_FP_NO_DEPRECATE\n            -D_CRT_NONSTDC_NO_DEPRECATE\n            -D_CRT_SECURE_NO_DEPRECATE\n            -D_CRT_SECURE_NO_DEPRECATE_GLOBALS\n            -D_CRT_SETERRORMODE_BEEP_SLEEP_NO_DEPRECATE\n            -D_CRT_TIME_FUNCTIONS_NO_DEPRECATE\n            -D_CRT_VCCLRIT_NO_DEPRECATE\n            -D_SCL_SECURE_NO_DEPRECATE\n            )\n        endif()\n      endif()\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Linking TBB and Freetype with OpenCV GAPI Tests\nDESCRIPTION: Adds Freetype and TBB support to the OpenCV test module opencv_test_gapi by defining necessary preprocessor flags and linking with respective libraries. Ensures that TBB and Freetype's functionality are accessible to GAPI tests.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nif(HAVE_TBB AND TARGET opencv_test_gapi)\n  ocv_target_link_libraries(opencv_test_gapi PRIVATE tbb)\nendif()\n```\n\nLANGUAGE: CMake\nCODE:\n```\nif(HAVE_FREETYPE)\n  ocv_target_compile_definitions(${the_module} PRIVATE -DHAVE_FREETYPE)\n  if(TARGET opencv_test_gapi)\n    ocv_target_compile_definitions(opencv_test_gapi PRIVATE -DHAVE_FREETYPE)\n  endif()\n  ocv_target_link_libraries(${the_module} PRIVATE ${FREETYPE_LIBRARIES})\n  ocv_target_include_directories(${the_module} PRIVATE ${FREETYPE_INCLUDE_DIRS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Linkage Property in CMake\nDESCRIPTION: Sets the CMake variable `tgts` to the string \"PRIVATE\". This variable is likely used later in `target_link_libraries` or similar commands to specify that subsequent libraries should be linked privately to the highgui target.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(tgts \"PRIVATE\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Corner Detection in FAST Algorithm using Pixel Comparisons in C++\nDESCRIPTION: Part of the FAST corner detection algorithm that uses a decision tree structure to compare pixel values at various offsets against brightness thresholds. The code examines a pattern of pixels around a center point to determine if the point qualifies as a corner based on continuous segments that are brighter or darker than the center pixel.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nif(ptr[offset8] < c_b)\n  if(ptr[offset4] < c_b)\n    if(ptr[offset3] < c_b)\n      goto is_a_corner;\n    else\n      if(ptr[offset10] < c_b)\n        goto is_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    if(ptr[offset10] < c_b)\n      if(ptr[offset11] < c_b)\n        goto is_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  if(ptr[offset2] < c_b)\n    if(ptr[offset1] < c_b)\n      if(ptr[offset3] < c_b)\n        if(ptr[offset4] < c_b)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset2] > cb)\n    if(ptr[offset1] > cb)\n      if(ptr[offset3] > cb)\n        if(ptr[offset4] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              goto is_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    if(ptr[offset2] < c_b)\n      if(ptr[offset3] < c_b)\n        if(ptr[offset4] < c_b)\n          if(ptr[offset7] < c_b)\n            if(ptr[offset1] < c_b)\n              if(ptr[offset6] < c_b)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset6] < c_b)\n                if(ptr[offset8] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  if(ptr[offset2] > cb)\n    if(ptr[offset10] > cb)\n      if(ptr[offset11] > cb)\n        if(ptr[offset9] > cb)\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              goto is_a_corner;\n            else\n              if(ptr[offset8] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset6] > cb)\n              if(ptr[offset7] > cb)\n                if(ptr[offset8] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset1] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset4] > cb)\n                goto is_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    if(ptr[offset9] > cb)\n      if(ptr[offset7] > cb)\n        if(ptr[offset8] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              if(ptr[offset1] > cb)\n                goto is_a_corner;\n              else\n                if(ptr[offset6] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\nif(ptr[offset0] < c_b)\n  if(ptr[offset2] > cb)\n    if(ptr[offset5] > cb)\n      if(ptr[offset7] > cb)\n        if(ptr[offset6] > cb)\n          if(ptr[offset4] > cb)\n            if(ptr[offset3] > cb)\n              if(ptr[offset1] > cb)\n                goto is_a_corner;\n              else\n                if(ptr[offset8] > cb)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n              if(ptr[offset9] > cb)\n                if(ptr[offset8] > cb)\n                  if(ptr[offset10] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset9] > cb)\n              if(ptr[offset8] > cb)\n                if(ptr[offset10] > cb)\n                  if(ptr[offset11] > cb)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        if(ptr[offset9] < c_b)\n          if(ptr[offset8] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                if(ptr[offset7] < c_b)\n                  if(ptr[offset1] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset6] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      if(ptr[offset9] < c_b)\n        if(ptr[offset7] < c_b)\n          if(ptr[offset8] < c_b)\n            if(ptr[offset5] < c_b)\n              if(ptr[offset1] < c_b)\n                if(ptr[offset10] < c_b)\n                  if(ptr[offset11] < c_b)\n                    goto is_a_corner;\n                  else\n                    if(ptr[offset6] < c_b)\n                      if(ptr[offset4] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset3] < c_b)\n                      if(ptr[offset4] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset4] < c_b)\n                    if(ptr[offset3] < c_b)\n                      goto is_a_corner;\n                    else\n                      if(ptr[offset10] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Detecting DOT Executable for Doxygen Graph Generation in CMake\nDESCRIPTION: Checks if the DOT executable (part of Graphviz), required for generating graphs in Doxygen, is found (`DOXYGEN_DOT_EXECUTABLE` is set). If found, it reports the path via a status message and sets initial internal variables (`init_dot_path`, `init_dot_mode`) accordingly. If not found, it sets these variables to indicate DOT is unavailable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif (DOXYGEN_DOT_EXECUTABLE)\n  message(STATUS \"Found DOT executable: ${DOXYGEN_DOT_EXECUTABLE}\")\n  set(init_dot_path \"${DOXYGEN_DOT_EXECUTABLE}\")\n  set(init_dot_mode \"YES\")\nelse()\n  set(init_dot_path \"\")\n  set(init_dot_mode \"NO\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring GTK Backend for HighGUI in CMake\nDESCRIPTION: Sets up the GTK backend (GTK2 or GTK3) for OpenCV HighGUI. Handles both plugin and built-in configurations, and adds OpenGL support through GtkGLExt if available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nif(TARGET ocv.3rdparty.gtk3 OR TARGET ocv.3rdparty.gtk2)\n  if(TARGET ocv.3rdparty.gtk3 AND NOT WITH_GTK_2_X)\n    set(__gtk_dependency \"ocv.3rdparty.gtk3\")\n  else()\n    set(__gtk_dependency \"ocv.3rdparty.gtk2\")\n  endif()\n  if(\n    NOT HIGHGUI_PLUGIN_LIST STREQUAL \"all\"\n    AND NOT \"gtk\" IN_LIST HIGHGUI_PLUGIN_LIST\n    AND NOT \"gtk2\" IN_LIST HIGHGUI_PLUGIN_LIST\n    AND NOT \"gtk3\" IN_LIST HIGHGUI_PLUGIN_LIST\n    AND NOT OPENCV_HIGHGUI_BUILTIN_BACKEND\n  )\n    if(__gtk_dependency STREQUAL \"ocv.3rdparty.gtk3\")\n      set(OPENCV_HIGHGUI_BUILTIN_BACKEND \"GTK3\")\n      if(OPENGL_LIBRARIES)\n        list(APPEND HIGHGUI_LIBRARIES \"${OPENGL_LIBRARIES}\")\n      endif()\n    elseif(__gtk_dependency STREQUAL \"ocv.3rdparty.gtk2\")\n      set(OPENCV_HIGHGUI_BUILTIN_BACKEND \"GTK2\")\n    else()\n      set(OPENCV_HIGHGUI_BUILTIN_BACKEND \"GTK\")\n    endif()\n    list(APPEND highgui_srcs ${CMAKE_CURRENT_LIST_DIR}/src/window_gtk.cpp)\n    list(APPEND tgts ${__gtk_dependency})\n    if(TARGET ocv.3rdparty.gtkglext\n        AND __gtk_dependency STREQUAL \"ocv.3rdparty.gtk2\"\n        AND NOT OPENCV_GTK_DISABLE_GTKGLEXT\n    )\n      list(APPEND tgts ocv.3rdparty.gtkglext)\n      if(TARGET ocv.3rdparty.gtk_opengl\n          AND __gtk_dependency STREQUAL \"ocv.3rdparty.gtk2\"\n          AND NOT OPENCV_GTK_DISABLE_OPENGL\n      )\n        list(APPEND tgts ocv.3rdparty.gtk_opengl)\n      endif()\n    endif()\n  elseif(\"gtk\" IN_LIST HIGHGUI_PLUGIN_LIST)\n    ocv_create_builtin_highgui_plugin(opencv_highgui_gtk ${__gtk_dependency} \"window_gtk.cpp\")\n    if(TARGET ocv.3rdparty.gtkglext)\n      ocv_target_link_libraries(opencv_highgui_gtk ocv.3rdparty.gtkglext)\n    endif()\n  else()\n    if(TARGET ocv.3rdparty.gtk3 AND (\"gtk3\" IN_LIST HIGHGUI_PLUGIN_LIST OR HIGHGUI_PLUGIN_LIST STREQUAL \"all\"))\n      ocv_create_builtin_highgui_plugin(opencv_highgui_gtk3 ocv.3rdparty.gtk3 \"window_gtk.cpp\")\n      if(TARGET ocv.3rdparty.gtkglext)\n        ocv_target_link_libraries(opencv_highgui_gtk3 ocv.3rdparty.gtkglext)\n      endif()\n    endif()\n    if(TARGET ocv.3rdparty.gtk2 AND (\"gtk2\" IN_LIST HIGHGUI_PLUGIN_LIST OR HIGHGUI_PLUGIN_LIST STREQUAL \"all\"))\n      ocv_create_builtin_highgui_plugin(opencv_highgui_gtk2 ocv.3rdparty.gtk2 \"window_gtk.cpp\")\n      if(TARGET ocv.3rdparty.gtkglext)\n        ocv_target_link_libraries(opencv_highgui_gtk2 ocv.3rdparty.gtkglext)\n      endif()\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Python Example Files with CMake - CMake\nDESCRIPTION: This CMake snippet checks if the INSTALL_PYTHON_EXAMPLES variable is set, then gathers all .py files in the current directory using file(GLOB) and prepares them for installation with the install() command. The files are placed in the directory specified by OPENCV_SAMPLES_SRC_INSTALL_PATH under a 'python' subfolder, with read-only permissions for owner, group, and world. Users must ensure that OPENCV_SAMPLES_SRC_INSTALL_PATH is set in the parent CMake configuration and that CMake has access to the correct permissions and paths. The snippet is intended to be included in a larger CMake build script and assumes that Python example scripts are available in the current directory. If INSTALL_PYTHON_EXAMPLES is not set, no action is taken.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/python/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(INSTALL_PYTHON_EXAMPLES)\n  file(GLOB install_list *.py )\n  install(FILES ${install_list}\n          DESTINATION ${OPENCV_SAMPLES_SRC_INSTALL_PATH}/python\n          PERMISSIONS OWNER_READ GROUP_READ WORLD_READ COMPONENT samples)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Initializing CMake Project for OpenCV JNI Shared Library - CMake\nDESCRIPTION: This snippet configures a CMake project intended to package an OpenCV JNI shared library by setting a minimum CMake version, declaring the project name, and creating a dummy static library target. It relies on 'dummy.cpp' as a placeholder source file to ensure the build integrates the required 'libc++_shared.so' dependency into the final package. No external dependencies are needed beyond a compatible CMake installation and the presence of 'dummy.cpp' in the project's root directory. Output is a dummy static library target for dependency inclusion.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/android_sdk/libcxx_helper/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.6)\n\nproject(opencv_jni_shared)\n\n# dummy target to bring libc++_shared.so into packages\nadd_library(opencv_jni_shared STATIC dummy.cpp)\n```\n\n----------------------------------------\n\nTITLE: Enabling FastCV in OpenCV Android Build Configuration (Python)\nDESCRIPTION: This Python configuration snippet demonstrates how to enable FastCV support within the OpenCV Android build system. It modifies the `opencv/platforms/android/default.config.py` file by adding the `WITH_FASTCV='ON'` flag to the `cmake_vars` dictionary for the `arm64-v8a` Application Binary Interface (ABI), targeting API level 24. This step ensures that the CMake build process includes the FastCV Hardware Abstraction Layer (HAL) and/or extensions when compiling OpenCV for compatible Android devices.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_fastcv/building_fastcv.markdown#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n ABI(\"3\", \"arm64-v8a\", None, 24, cmake_vars=dict(WITH_FASTCV='ON')),\n```\n\n----------------------------------------\n\nTITLE: Setting Cache and Configuration Variables for DOT Usage in CMake\nDESCRIPTION: Sets CMake cache variables (`OPENCV_DOCS_DOT_PATH`, `OPENCV_DOCS_HAVE_DOT`) based on the detected DOT executable status, allowing users to override the path or enable/disable DOT usage. It also sets corresponding internal CMake variables (`CMAKECONFIG_DOT_PATH`, `CMAKECONFIG_HAVE_DOT`) used for configuring the Doxyfile template.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENCV_DOCS_DOT_PATH \"${init_dot_path}\" CACHE PATH \"Doxygen/DOT_PATH value\")\nset(OPENCV_DOCS_HAVE_DOT \"${init_dot_mode}\" CACHE BOOL \"Doxygen: build extra diagrams\")\nset(CMAKECONFIG_DOT_PATH \"${OPENCV_DOCS_DOT_PATH}\")\nset(CMAKECONFIG_HAVE_DOT \"${OPENCV_DOCS_HAVE_DOT}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Framework Dependencies and Protocol Buffer Handling - CMake\nDESCRIPTION: Sets up detection and code generation for Protobuf-based frameworks (Caffe, TensorFlow, ONNX). Conditionally generates protocol buffer source/header files or uses pre-generated files depending on the PROTOBUF_UPDATE_FILES variable. Also sets include directories for each framework. Requires Protobuf, its discovery, and available source files. Outputs are lists of generated or prebuilt source/header files and include paths.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif(HAVE_PROTOBUF)\n  ocv_target_compile_definitions(${the_module} PRIVATE \"HAVE_PROTOBUF=1\")\n\n  if(PROTOBUF_UPDATE_FILES)\n    file(GLOB proto_files \"${CMAKE_CURRENT_LIST_DIR}/src/tensorflow/*.proto\" \"${CMAKE_CURRENT_LIST_DIR}/src/caffe/opencv-caffe.proto\" \"${CMAKE_CURRENT_LIST_DIR}/src/onnx/opencv-onnx.proto\")\n    set(PROTOBUF_GENERATE_CPP_APPEND_PATH ON) # required for tensorflow\n    protobuf_generate_cpp(fw_srcs fw_hdrs ${proto_files})\n  else()\n    file(GLOB fw_srcs \"${CMAKE_CURRENT_LIST_DIR}/misc/tensorflow/*.cc\" \"${CMAKE_CURRENT_LIST_DIR}/misc/caffe/opencv-caffe.pb.cc\" \"${CMAKE_CURRENT_LIST_DIR}/misc/onnx/opencv-onnx.pb.cc\")\n    file(GLOB fw_hdrs \"${CMAKE_CURRENT_LIST_DIR}/misc/tensorflow/*.h\" \"${CMAKE_CURRENT_LIST_DIR}/misc/caffe/opencv-caffe.pb.h\" \"${CMAKE_CURRENT_LIST_DIR}/misc/onnx/opencv-onnx.pb.h\")\n    set(fw_inc \"${CMAKE_CURRENT_LIST_DIR}/misc/caffe\" \"${CMAKE_CURRENT_LIST_DIR}/misc/tensorflow\" \"${CMAKE_CURRENT_LIST_DIR}/misc/onnx\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV with Verbose Output\nDESCRIPTION: Command to build OpenCV using make with verbose output, showing detailed compilation steps for each unit.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ make -j6 VERBOSE=1\n```\n\n----------------------------------------\n\nTITLE: Initializing JPEG Compression Object with libjpeg in C\nDESCRIPTION: This C code demonstrates how to set up and initialize the JPEG compression object (struct jpeg_compress_struct) and associated error handler (struct jpeg_error_mgr) using the libjpeg library. The error manager is initialized first and then linked with the compression object, followed by a call to jpeg_create_compress() to allocate necessary structures. Dependencies include the libjpeg headers and linking against the libjpeg library. Key parameters include the pointer relationships between the compression info structure and the error handler. This initialization is required before performing any compression operations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_2\n\nLANGUAGE: C\nCODE:\n```\nstruct jpeg_compress_struct cinfo;\nstruct jpeg_error_mgr jerr;\n...\ncinfo.err = jpeg_std_error(&jerr);\njpeg_create_compress(&cinfo);\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV.js Performance Tests in CMake\nDESCRIPTION: Sets up custom commands and targets for copying and preparing performance test files for OpenCV.js.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/js/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(opencv_perf_js_bin_dir \"${EXECUTABLE_OUTPUT_PATH}/perf\")\nset(perf_dir ${CMAKE_CURRENT_SOURCE_DIR}/perf)\n\nset(opencv_perf_js_file_deps \"\")\n\nfile(MAKE_DIRECTORY \"${opencv_perf_js_bin_dir}\")\n\nfile(GLOB_RECURSE perf_files RELATIVE \"${perf_dir}\" \"${perf_dir}/*\")\nforeach(f ${perf_files})\n  add_custom_command(OUTPUT \"${opencv_perf_js_bin_dir}/${f}\"\n                     COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${perf_dir}/${f}\" \"${opencv_perf_js_bin_dir}/${f}\"\n                     DEPENDS \"${perf_dir}/${f}\"\n                     COMMENT \"Copying ${f}\"\n                    )\n  list(APPEND opencv_perf_js_file_deps \"${perf_dir}/${f}\" \"${opencv_perf_js_bin_dir}/${f}\")\nendforeach()\n\nadd_custom_target(${PROJECT_NAME}_perf\n                  DEPENDS ${OCV_JS_PATH} ${opencv_perf_js_file_deps})\n```\n\n----------------------------------------\n\nTITLE: Conditionally Disabling OpenCV Java Module in CMake\nDESCRIPTION: This CMake code block defines conditions under which the `opencv_java` module should be disabled. It checks for specific target platforms (Apple Framework, WinRT), the absence of default Python, missing build tools (Ant, Java for non-Gradle Android), unavailable JNI support (or old Android NDK), or if the unified `opencv_world` module is being built. If any of these conditions are true, the `ocv_module_disable(java)` command prevents the Java module from being built.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(APPLE_FRAMEWORK OR WINRT\n    OR NOT PYTHON_DEFAULT_AVAILABLE\n    OR NOT (ANT_EXECUTABLE OR Java_FOUND OR ANDROID_PROJECTS_BUILD_TYPE STREQUAL \"GRADLE\")\n    OR NOT (JNI_FOUND OR (ANDROID AND (NOT DEFINED ANDROID_NATIVE_API_LEVEL OR ANDROID_NATIVE_API_LEVEL GREATER 7)))\n    OR BUILD_opencv_world\n    )\n  ocv_module_disable(java)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Skipping Scanlines During JPEG Decompression in C using libjpeg\nDESCRIPTION: Shows the function signature for `jpeg_skip_scanlines` (and its 12-bit variant `jpeg12_skip_scanlines`) used for partial image decompression. This function allows skipping a specified number of rows (`num_lines`) to efficiently jump to a vertical offset in the image. It has limitations: it does not support suspending data sources or two-pass color quantization. It is most effective when skipping large chunks of rows.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_18\n\nLANGUAGE: C\nCODE:\n```\njpeg_skip_scanlines(j_decompress_ptr cinfo, JDIMENSION num_lines);\n        /* Use jpeg12_skip_scanlines() for 12-bit data precision. */\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug Modules and Conditional Dependencies - CMake\nDESCRIPTION: This block defines an initially empty debug_modules variable and conditionally appends opencv_highgui to it if debugging is enabled for the calib3d module. It uses a CMake if/list block and evaluates the presence of the DEBUG_opencv_calib3d variable. Key parameter: DEBUG_opencv_calib3d.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/calib3d/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(debug_modules \"\")\nif(DEBUG_opencv_calib3d)\n  list(APPEND debug_modules opencv_highgui)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Conditional Logic in OpenCV\nDESCRIPTION: Nested conditional structure comparing pixel values against threshold values (c_b and cb) to determine corner points. Uses pointer arithmetic and offset values to check surrounding pixels and branches to either 'is_a_corner' or 'is_not_a_corner' based on the comparison results.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_39\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset5] < c_b)\n    if(ptr[offset7] > cb)\n      goto is_not_a_corner;\n    else\n      if(ptr[offset7] < c_b)\n        if(ptr[offset2] > cb)\n          if(ptr[offset9] > cb)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset9] < c_b)\n              if(ptr[offset1] > cb)\n                if(ptr[offset6] > cb)\n                  goto is_not_a_corner;\n                else\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset8] < c_b)\n                      if(ptr[offset4] < c_b)\n                        if(ptr[offset3] < c_b)\n                          goto is_a_corner;\n                        else\n                          if(ptr[offset10] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset10] < c_b)\n                          if(ptr[offset11] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset1] < c_b)\n                  if(ptr[offset6] > cb)\n                    goto is_not_a_corner;\n                  else\n                    if(ptr[offset6] < c_b)\n                      if(ptr[offset8] < c_b)\n                        if(ptr[offset4] < c_b)\n                          if(ptr[offset3] < c_b)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset10] < c_b)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset10] < c_b)\n                            if(ptr[offset11] < c_b)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset6] > cb)\n                    goto is_not_a_corner;\n                  else\n                    if(ptr[offset6] < c_b)\n                      if(ptr[offset8] < c_b)\n                        if(ptr[offset4] < c_b)\n                          if(ptr[offset3] < c_b)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset10] < c_b)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset10] < c_b)\n                            if(ptr[offset11] < c_b)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n        else\n          if(ptr[offset2] < c_b)\n            if(ptr[offset9] > cb)\n              if(ptr[offset1] < c_b)\n                if(ptr[offset6] > cb)\n                  goto is_not_a_corner;\n                else\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset3] < c_b)\n                      if(ptr[offset4] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n              else\n                if(ptr[offset1] > cb)\n                  if(ptr[offset6] > cb)\n                    goto is_not_a_corner;\n                  else\n                    if(ptr[offset6] < c_b)\n                      if(ptr[offset3] < c_b)\n                        if(ptr[offset4] < c_b)\n                          if(ptr[offset8] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset6] > cb)\n                    goto is_not_a_corner;\n                  else\n                    if(ptr[offset6] < c_b)\n                      if(ptr[offset3] < c_b)\n                        if(ptr[offset4] < c_b)\n                          if(ptr[offset8] < c_b)\n                            goto is_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n            else\n              if(ptr[offset9] < c_b)\n                if(ptr[offset1] > cb)\n                  if(ptr[offset6] > cb)\n                    goto is_not_a_corner;\n                  else\n                    if(ptr[offset6] < c_b)\n                      if(ptr[offset8] < c_b)\n                        if(ptr[offset4] < c_b)\n                          if(ptr[offset3] < c_b)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset10] < c_b)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset10] < c_b)\n                            if(ptr[offset11] < c_b)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset1] < c_b)\n                    if(ptr[offset6] > cb)\n                      goto is_not_a_corner;\n                    else\n                      if(ptr[offset6] < c_b)\n                        if(ptr[offset4] < c_b)\n                          if(ptr[offset3] < c_b)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset8] < c_b)\n                              if(ptr[offset10] < c_b)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset8] < c_b)\n                            if(ptr[offset10] < c_b)\n                              if(ptr[offset11] < c_b)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset6] > cb)\n                      goto is_not_a_corner;\n                    else\n                      if(ptr[offset6] < c_b)\n                        if(ptr[offset8] < c_b)\n                          if(ptr[offset4] < c_b)\n                            if(ptr[offset3] < c_b)\n```\n\n----------------------------------------\n\nTITLE: Copying Common Test Files in CMake for OpenCV Java Tests\nDESCRIPTION: Copies common test files including resources and utilities to the Java test directory. It uses a custom function 'copy_common_tests' to perform the copy operation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/pure_test/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncopy_common_tests(\"${CMAKE_CURRENT_SOURCE_DIR}/../common_test\" \"${OPENCV_JAVA_TEST_DIR}\" depends)\n```\n\n----------------------------------------\n\nTITLE: Including OpenCV DNN Module Directory in C++\nDESCRIPTION: This snippet specifies a directory inclusion with CMake to access necessary headers for the OpenCV DNN module. It ensures that the path to 'layers/layers_common.simd_declarations.hpp' is included during the build process. The inclusion is essential for accessing SIMD declarations which are used for optimizing performance in the DNN module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/misc/plugin/openvino/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include_directories(\"${OPENCV_MODULE_opencv_dnn_BINARY_DIR}\")  // Cannot open include file: 'layers/layers_common.simd_declarations.hpp'\n```\n\n----------------------------------------\n\nTITLE: Adding Preprocessor Definition for WebP Support in CMake\nDESCRIPTION: Checks if the `HAVE_WEBP` variable is defined (indicating that the WebP library was found and enabled during configuration). If true, it adds the preprocessor definition `-DHAVE_WEBP` using `add_definitions`. This allows conditional compilation of code within highgui that depends on WebP functionality.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_WEBP)\n  add_definitions(-DHAVE_WEBP)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing GUI and Media Dependencies\nDESCRIPTION: Installs GTK support, camera support, and media dependencies for OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nyum install gtk2-devel\nyum install libdc1394-devel\nyum install ffmpeg-devel\nyum install gstreamer-plugins-base-devel\n```\n\n----------------------------------------\n\nTITLE: Configuring JNI Output Path in CMake\nDESCRIPTION: Sets the JNI output path based on whether the target platform is Android or not.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jni/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ANDROID)\n  ocv_update(JNI_OUTPUT_PATH  \"${OpenCV_BINARY_DIR}/jni/${ANDROID_NDK_ABI_NAME}\")\nelse()\n  ocv_update(JNI_OUTPUT_PATH  \"${LIBRARY_OUTPUT_PATH}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining WebNN and Other Hardware Backends for DNN Module - CMake\nDESCRIPTION: These snippets configure compile definitions to enable WebNN, TimVX, and CANN support if the corresponding options are enabled and available. Each hardware/backend feature is conditionally checked and, if present, a respective compiler definition is set. Inputs are HAVE_WEBNN, HAVE_TIMVX, and HAVE_CANN variables; output is enabling backend-specific code compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif(WITH_WEBNN AND HAVE_WEBNN)\n  ocv_target_compile_definitions(${the_module} PRIVATE \"HAVE_WEBNN=1\")\nendif()\n\nif(HAVE_TIMVX)\n  ocv_target_compile_definitions(${the_module} PRIVATE \"HAVE_TIMVX=1\")\nendif()\n\nif(HAVE_CANN)\n  ocv_target_compile_definitions(${the_module} PRIVATE \"HAVE_CANN=1\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Python 3 Development Packages (Bash)\nDESCRIPTION: Installs Python 3 minimal package and the NumPy library. These are prerequisites if the Python 3 wrapper for OpenCV needs to be enabled during the cross-compilation process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y \\\n    python3-minimal \\\n    python3-numpy\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Compiler Warnings\nDESCRIPTION: Disables specific compiler warnings (-Wundef, -Wmissing-declarations, -Wshadow) when CUDA support is enabled.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/photo/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(HAVE_CUDA)\n  ocv_warnings_disable(CMAKE_CXX_FLAGS -Wundef -Wmissing-declarations -Wshadow)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Dispatched CPU Implementations for Fluid Backend in CMake\nDESCRIPTION: Uses the `ocv_add_dispatched_file` function to register CPU-optimized implementations (using SSE4.1 and AVX2 instruction sets) for specific functions within the G-API Fluid backend (`gfluidimgproc_func` and `gfluidcore_func`). This enables runtime dispatch to the most efficient implementation available on the target CPU.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nocv_add_dispatched_file(backends/fluid/gfluidimgproc_func SSE4_1 AVX2)\nocv_add_dispatched_file(backends/fluid/gfluidcore_func SSE4_1 AVX2)\n```\n\n----------------------------------------\n\nTITLE: Threading Plugins Configuration Option in OpenCV\nDESCRIPTION: Defines the PARALLEL_ENABLE_PLUGINS option available since OpenCV 4.5.2 to support dynamically loaded threading backends, requiring separate compilation process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_17\n\nLANGUAGE: markdown\nCODE:\n```\n| Option | Default | Description |\n| ------ | ------- | ----------- |\n| PARALLEL_ENABLE_PLUGINS | ON | Enable plugin support, if this option is disabled OpenCV will not try to load anything |\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version and Checking OpenCV Dependencies\nDESCRIPTION: This CMake snippet sets the minimum required CMake version to 3.5. It defines a list variable `OPENCV_OPENVX_SAMPLE_REQUIRED_DEPS` containing necessary OpenCV modules (core, imgproc, imgcodecs, videoio, highgui) for the OpenVX samples. It then uses the OpenCV-specific function `ocv_check_dependencies` to verify if these modules are available. If the `BUILD_EXAMPLES` option is disabled or if dependencies are not found (`OCV_DEPENDENCIES_FOUND` is false), the script execution returns early.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/openvx/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5)\n\nset(OPENCV_OPENVX_SAMPLE_REQUIRED_DEPS\n  opencv_core\n  opencv_imgproc\n  opencv_imgcodecs\n  opencv_videoio\n  opencv_highgui)\nocv_check_dependencies(${OPENCV_OPENVX_SAMPLE_REQUIRED_DEPS})\n\nif(NOT BUILD_EXAMPLES OR NOT OCV_DEPENDENCIES_FOUND)\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Slow HAL Replacement Library with CMake\nDESCRIPTION: This snippet outlines the steps to build a custom HAL replacement library 'slow_hal' using CMake. It guides on creating a build directory and executing the CMake command to compile the library, resulting in a static library 'libslow_hal.a'. The slow_hal library showcases naive C++ implementations for logical operations to intentionally reduce performance, serving as a study for function replacements.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/hal/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncmake <opencv-src>/samples/hal/slow_hal\n```\n\nLANGUAGE: shell\nCODE:\n```\nmake\n```\n\n----------------------------------------\n\nTITLE: Selecting DCT Algorithm in libjpeg (C)\nDESCRIPTION: A C field of type `J_DCT_METHOD` within the compression parameters structure (`cinfo`). This parameter selects the algorithm used for the Discrete Cosine Transform step in lossy compression. Options include `JDCT_ISLOW` (accurate integer), `JDCT_IFAST` (faster, less accurate integer), `JDCT_FLOAT` (floating-point, legacy), `JDCT_DEFAULT`, and `JDCT_FASTEST`. The choice impacts compression speed and quality, particularly at high quality settings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_32\n\nLANGUAGE: C\nCODE:\n```\nJ_DCT_METHOD dct_method\n```\n\n----------------------------------------\n\nTITLE: Setting YCbCr Component Sampling Factors for Raw Data - libjpeg - C\nDESCRIPTION: Demonstrates how to set the horizontal and vertical sampling factors for each color component in YCbCr JPEG encoding when supplying raw (downsampled) image data. Proper values ensure compatibility between input buffers and JPEG encoding pipeline, and are required for applications bypassing automatic downsampling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_64\n\nLANGUAGE: c\nCODE:\n```\ncinfo->comp_info[0].h_samp_factor = 2;           // for Y\ncinfo->comp_info[0].v_samp_factor = 2;\ncinfo->comp_info[1].h_samp_factor = 1;           // for Cb\ncinfo->comp_info[1].v_samp_factor = 1;\ncinfo->comp_info[2].h_samp_factor = 1;           // for Cr\ncinfo->comp_info[2].v_samp_factor = 1;\n```\n\n----------------------------------------\n\nTITLE: Creating Built-in DNN Plugin with OpenCV in C++\nDESCRIPTION: This snippet creates a built-in DNN plugin using OpenCV's custom plugin system. It integrates OpenVINO with the OpenCV DNN module, allowing enhanced inference capabilities. The function 'ocv_create_builtin_dnn_plugin' requires OpenCV DNN and OpenVINO to be linked as dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/misc/plugin/openvino/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nocv_create_builtin_dnn_plugin(opencv_dnn_openvino ocv.3rdparty.openvino ${dnn_plugin_srcs})\n```\n\n----------------------------------------\n\nTITLE: Running All WebAssembly Intrinsic Tests in OpenCV.js - JavaScript\nDESCRIPTION: This JavaScript snippet runs all WebAssembly intrinsic (HAL) tests in the OpenCV.js environment. Depends on 'cv' being loaded and initialized in context. Outputs test results to the debug console. Use to comprehensively test all supported WebAssembly intrinsics.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_19\n\nLANGUAGE: js\nCODE:\n```\ncv.test_hal_intrin_all()\n```\n\n----------------------------------------\n\nTITLE: Disabling CUDA-related Warnings in CMake\nDESCRIPTION: Disables specific compiler warnings related to CUDA if CUDA support is enabled. This helps to reduce noise during the build process for CUDA-enabled builds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/stitching/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(HAVE_CUDA)\n  ocv_warnings_disable(CMAKE_CXX_FLAGS -Wundef -Wmissing-declarations -Wshadow -Wstrict-aliasing)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Intel Samples in OpenCV with CMake\nDESCRIPTION: The CMake script starts by configuring an example project using the `ocv_install_example_src` function to specify the source files. It ensures required dependencies (such as `opencv_core`, `opencv_imgproc`, etc.) are found before defining the project `va_intel_samples`. It includes necessary modules and sets up sample targets, linking them with specified libraries. The script uses conditional logic to exit early if examples or dependencies are not available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/va_intel/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nocv_install_example_src(opencl *.cpp *.inc CMakeLists.txt)\n\nset(OPENCV_VA_INTEL_SAMPLES_REQUIRED_DEPS\n  opencv_core\n  opencv_imgproc\n  opencv_imgcodecs\n  opencv_videoio\n  opencv_highgui)\nocv_check_dependencies(${OPENCV_VA_INTEL_SAMPLES_REQUIRED_DEPS})\n\nif(NOT BUILD_EXAMPLES OR NOT OCV_DEPENDENCIES_FOUND)\n  return()\nendif()\n\nproject(va_intel_samples)\nocv_include_modules_recurse(${OPENCV_VA_INTEL_SAMPLES_REQUIRED_DEPS})\nfile(GLOB all_samples RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} *.cpp)\nforeach(sample_filename ${all_samples})\n  ocv_define_sample(tgt ${sample_filename} va_intel)\n  ocv_target_link_libraries(${tgt} PRIVATE ${OPENCV_LINKER_LIBS} ${OPENCV_VA_INTEL_SAMPLES_REQUIRED_DEPS} ${VA_LIBRARIES})\nendforeach()\n\n```\n\n----------------------------------------\n\nTITLE: Linking LAPACK Libraries to calib3d Target - CMake\nDESCRIPTION: This snippet links LAPACK libraries to the module target, using variables to represent the module and LAPACK dependency list. Proper use requires the_module and LAPACK_LIBRARIES to be defined in the build context. This ensures the calib3d module has access to linear algebra and numerical computation routines.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/calib3d/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nocv_target_link_libraries(${the_module} ${LAPACK_LIBRARIES})\n```\n\n----------------------------------------\n\nTITLE: Generating OpenCV for Windows Store 8.1 ARM using CMake\nDESCRIPTION: Invokes CMake directly to generate Visual Studio 2013 project files for OpenCV targeting Windows Store 8.1 on the ARM architecture. Specifies the ARM-specific generator, system name (WindowsStore), and system version.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Visual Studio 12 2013 ARM\" -DCMAKE_SYSTEM_NAME=WindowsStore -DCMAKE_SYSTEM_VERSION=8.1 <path-to-source>\n```\n\n----------------------------------------\n\nTITLE: Generating Windows Store 8.1 x64 Configuration for Testing\nDESCRIPTION: Executes the `setup_winrt.bat` script specifically to generate the Visual Studio project files needed for building and testing OpenCV for Windows Store 8.1 on the x64 architecture. This is presented as a step in the test setup process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_14\n\nLANGUAGE: batch\nCODE:\n```\nsetup_winrt.bat \"WS\" \"8.1\" \"x64\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Android Camera Preview Example\nDESCRIPTION: CMake configuration that sets up an Android camera preview example project. It creates a project variable, adds an Android project with OpenCV library dependencies, and establishes build dependencies if the target exists.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-1-camerapreview/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(sample example-tutorial-1-camerapreview)\n\nadd_android_project(${sample} \"${CMAKE_CURRENT_SOURCE_DIR}\" LIBRARY_DEPS \"${OPENCV_ANDROID_LIB_DIR}\" SDK_TARGET 11 \"${ANDROID_SDK_TARGET}\")\nif(TARGET ${sample})\n  add_dependencies(opencv_android_examples ${sample})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Executing the OpenCV Video Writing Example (Bash)\nDESCRIPTION: Shows an example command line execution of the video writing program compiled from the C++ source. It specifies the input video file (`video/Megamind.avi`), the channel to extract ('R' for Red), and indicates ('Y') that the user should be prompted to select a video codec at runtime.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/video_write.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nvideo-write.exe video/Megamind.avi R Y\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building OpenCV GStreamer VideoIO Plugin (CMake)\nDESCRIPTION: This CMake script sets the minimum required version (3.5), locates the main OpenCV source directory relative to the current file, and includes a common OpenCV plugin helper CMake file (`OpenCVPluginStandalone.cmake`). It explicitly enables GStreamer support, includes video I/O initialization logic, defines the plugin's dependencies on OpenCV modules (core, imgproc, imgcodecs), and uses the custom `ocv_create_plugin` macro to define the `opencv_videoio_gstreamer` plugin based on the source file `src/cap_gstreamer.cpp`. Finally, it prints a status message indicating the detected GStreamer version being used.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/videoio/misc/plugin_gstreamer/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5)\n\nget_filename_component(OpenCV_SOURCE_DIR \"${CMAKE_CURRENT_LIST_DIR}/../../../..\" ABSOLUTE)\ninclude(\"${OpenCV_SOURCE_DIR}/cmake/OpenCVPluginStandalone.cmake\")\n\n# scan dependencies\nset(WITH_GSTREAMER ON)\ninclude(\"${OpenCV_SOURCE_DIR}/modules/videoio/cmake/init.cmake\")\n\nset(OPENCV_PLUGIN_DEPS core imgproc imgcodecs)\nocv_create_plugin(videoio \"opencv_videoio_gstreamer\" \"ocv.3rdparty.gstreamer\" \"GStreamer\" \"src/cap_gstreamer.cpp\")\n\nmessage(STATUS \"Using GStreamer: ${GSTREAMER_VERSION}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Android Test Module Files in CMake\nDESCRIPTION: Copies and configures specific files for the Android test module, including the AndroidManifest.xml and build.gradle files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/android_test/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nfile(COPY \"${CMAKE_CURRENT_SOURCE_DIR}/tests_module/AndroidManifest.xml\" DESTINATION \"${OPENCV_ANDROID_TEST_DIR}/tests_module\")\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/tests_module/build.gradle.in\" \"${OPENCV_ANDROID_TEST_DIR}/tests_module/build.gradle\" @ONLY)\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/build.gradle.in\" \"${OPENCV_ANDROID_TEST_DIR}/build.gradle\" @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Architecture Detection for macOS\nDESCRIPTION: Determines the target architecture on macOS systems by checking CMAKE_OSX_ARCHITECTURES or falling back to CMAKE_SYSTEM_PROCESSOR.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libpng/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(APPLE AND CMAKE_OSX_ARCHITECTURES)\n  set(TARGET_ARCH ${CMAKE_OSX_ARCHITECTURES})\nelse()\n  set(TARGET_ARCH ${CMAKE_SYSTEM_PROCESSOR})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Modules Configuration JSON for Objective-C Bindings in CMake\nDESCRIPTION: Creates a JSON configuration with information about OpenCV modules to be wrapped in Objective-C. This includes each module's name and relative location in the source tree, which is used by the generator script.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/objc/generator/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\n# This file is included from a subdirectory\nset(OBJC_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/..\")\ninclude(${OBJC_SOURCE_DIR}/common.cmake)  # fill OPENCV_OBJC_MODULES\n\n# common files\nfile(GLOB_RECURSE deps \"${CMAKE_CURRENT_SOURCE_DIR}/templates/*\")\n\nset(__modules_config \"\") # list of OpenCV modules\nforeach(m ${OPENCV_OBJC_MODULES})\n  set(module_objc_dir \"${OPENCV_MODULE_${m}_LOCATION}/misc/objc\")\n  list(APPEND deps ${OPENCV_MODULE_${m}_HEADERS})\n  file(GLOB_RECURSE misc_files \"${module_objc_dir}/*\")\n  list(APPEND deps ${misc_files})\n\n  string(REGEX REPLACE \"^opencv_\" \"\" m_ \"${m}\")\n  if(__modules_config)\n    set(__modules_config \"${__modules_config},\\n\")\n  endif()\n  file(RELATIVE_PATH rel_path \"${OpenCV_SOURCE_DIR}\" \"${OPENCV_MODULE_${m}_LOCATION}\")\n  set(__modules_config \"${__modules_config}    { \\\"name\\\": \\\"${m_}\\\", \\\"location\\\": \\\"${rel_path}\\\" }\")\nendforeach(m)\n```\n\n----------------------------------------\n\nTITLE: Selecting Default JPEG Colorspace (C)\nDESCRIPTION: This function selects an appropriate JPEG colorspace based on the input color space and calls jpeg_set_colorspace(). It's a subroutine of jpeg_set_defaults().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_22\n\nLANGUAGE: C\nCODE:\n```\njpeg_default_colorspace (j_compress_ptr cinfo)\n```\n\n----------------------------------------\n\nTITLE: Conditionally Including JNI Generator Subdirectory in CMake\nDESCRIPTION: This snippet checks if CMake is performing its initial configuration pass using the `OPENCV_INITIAL_PASS` variable. If it is the initial pass, it includes the `generator` subdirectory, which is responsible for generating JNI/JAR source code and documentation signatures for the OpenCV Java bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(OPENCV_INITIAL_PASS)\n  # generator for JNI/JAR source code and documentation signatures\n  add_subdirectory(generator)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Overriding Message Output in JPEG Library (C)\nDESCRIPTION: Defines the `output_message` function signature, a method within the `jpeg_error_mgr` struct. This function handles the actual output of any JPEG message (fatal errors, warnings, traces). It receives a pointer to the JPEG object (`j_common_ptr`). Override this method to redirect messages from the default `stderr` to a custom destination like a log file or UI element. This function only handles *sending* the message, not formatting it.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_45\n\nLANGUAGE: c\nCODE:\n```\noutput_message (j_common_ptr cinfo)\n```\n\n----------------------------------------\n\nTITLE: Checking OpenCV Version with CMake\nDESCRIPTION: The code snippet demonstrates how to use the OpenCV_VERSION variable in CMake to differentiate configurations for OpenCV version 2.4 and 3.x. If the version is less than 3.0, the script configures the project to use version 2.4 modules; otherwise, it configures the project to use version 3.x modules. This setup requires CMake and an environment where OpenCV_VERSION is defined.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/transition_guide/transition_guide.markdown#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(OpenCV_VERSION VERSION_LESS \"3.0\")\n# use 2.4 modules\nelse()\n# use 3.x modules\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Emscripten SDK - Bash\nDESCRIPTION: Installs and activates the latest Emscripten SDK in a Unix-like environment. These commands update the SDK, install the most recent version, and set it as active. Dependency: The Emscripten SDK (downloaded from emscripten.org) must be present. No parameters are needed. Outputs the installed SDK to be used for compiling to WebAssembly. Ensure you run these commands inside the EMSDK root directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./emsdk update\\n./emsdk install latest\\n./emsdk activate latest\n```\n\n----------------------------------------\n\nTITLE: Initializing a C++ CMake Project\nDESCRIPTION: Sets the minimum required CMake version to 3.6. Defines a variable `target` with the value `mixed_sample` and initializes a C++ project (`CXX`) with this name.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/android/tutorial-2-mixedprocessing/jni/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.6)\n\nset(target mixed_sample)\nproject(${target} CXX)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Virtual Environment\nDESCRIPTION: This console code snippet demonstrates setting up a Python 3.7+ virtual environment using virtualenv. It is essential for installing and managing dependencies for the OpenCV and PyTorch integration tutorial.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nvirtualenv -p /usr/bin/python3.7 <env_dir_path>\nsource <env_dir_path>/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Corner Detection Conditional Logic - C++\nDESCRIPTION: Complex nested conditional structure for determining corner pixels by comparing neighboring pixel values using pointer arithmetic. The code uses goto statements to jump to either 'is_a_corner' or 'is_not_a_corner' based on various threshold comparisons.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_36\n\nLANGUAGE: cpp\nCODE:\n```\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset2] < c_b)\n    if(ptr[offset7] > cb)\n      if(ptr[offset9] > cb)\n        if(ptr[offset1] > cb)\n          goto is_not_a_corner;\n        else\n          if(ptr[offset1] < c_b)\n            if(ptr[offset6] > cb)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset4] < c_b)\n                  if(ptr[offset10] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n```\n\n----------------------------------------\n\nTITLE: Configuring Android Library Build System using CMake\nDESCRIPTION: This snippet sets up the necessary environment and configuration for building an OpenCV library for Android. It manages file removal and creation, establishes source directories, and applies Gradle configurations if not using an Android executable. The setup includes flexibly handling SDK versioning and configurations for publishing through Maven.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/android_sdk/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(${the_module}_android)\n\nif(ANDROID_EXECUTABLE)\n  set(OPENCV_JAVA_DIR \"${OpenCV_BINARY_DIR}/android_sdk\" CACHE INTERNAL \"\")\nelse()  # gradle\n  set(OPENCV_JAVA_DIR \"${ANDROID_BUILD_BASE_DIR}/opencv\" CACHE INTERNAL \"\")\nendif()\nset(OPENCV_ANDROID_LIB_DIR \"${OPENCV_JAVA_DIR}\" CACHE INTERNAL \"\")  # for OpenCV samples\n\nfile(REMOVE_RECURSE \"${OPENCV_JAVA_DIR}\")\nfile(MAKE_DIRECTORY \"${OPENCV_JAVA_DIR}/bin\")\nset(java_src_dir \"${OPENCV_JAVA_DIR}/src\")\nfile(MAKE_DIRECTORY \"${java_src_dir}\")\n\nocv_copyfiles_append_dir(JAVA_SRC_COPY \"${OPENCV_JAVA_BINDINGS_DIR}/gen/java\" \"${java_src_dir}\")\n\nset(SOURSE_SETS_JNI_LIBS_SRC_DIRS \"'native/libs'\")\nset(SOURSE_SETS_JAVA_SRC_DIRS \"'java/src'\")\nset(SOURSE_SETS_RES_SRC_DIRS \"'java/res'\")\nset(SOURSE_SETS_MANIFEST_SRC_FILE \"'java/AndroidManifest.xml'\")\nset(BUILD_GRADLE_COMPILE_OPTIONS \"\n    android {\n        buildFeatures {\n            buildConfig true\n        }\n    }\n    compileOptions {\n        sourceCompatibility JavaVersion.VERSION_${ANDROID_GRADLE_JAVA_VERSION_INIT}\n        targetCompatibility JavaVersion.VERSION_${ANDROID_GRADLE_JAVA_VERSION_INIT}\n    }\n\")\nset(MAVEN_PUBLISH_PLUGIN_DECLARATION \"apply plugin: 'maven-publish'\")\nset(BUILD_GRADLE_ANDROID_PUBLISHING_CONFIG \"\n    buildFeatures {\n        prefabPublishing true\n        buildConfig true\n    }\n\n    prefab {\n        opencv_jni_shared {\n            headers 'native/jni/include'\n        }\n    }\n\n    publishing {\n        singleVariant('release') {\n            withSourcesJar()\n            withJavadocJar()\n        }\n    }\n\")\n\nset(BUILD_GRADLE_PUBLISHING_CONFIG \"\npublishing {\n    publications {\n        release(MavenPublication) {\n            groupId = 'org.opencv'\n            artifactId = 'opencv'\n            version = '${OPENCV_VERSION_PLAIN}'\n\n            afterEvaluate {\n               from components.release\n           }\n        }\n    }\n    repositories {\n        maven {\n            name = 'myrepo'\n            url = \"\\${project.buildDir}/repo\"\n        }\n    }\n}\n\")\n\nif(ANDROID_EXECUTABLE)\n\nocv_assert(ANDROID_TOOLS_Pkg_Revision GREATER 13)\n\nocv_copyfiles_append_dir(JAVA_SRC_COPY \"${OPENCV_JAVA_BINDINGS_DIR}/gen/android/java\" \"${java_src_dir}\")\n\n# calc default SDK Target\nandroid_get_compatible_target(ANDROID_SDK_COMPATIBLE_TARGET ${ANDROID_NATIVE_API_LEVEL} ${ANDROID_SDK_TARGET} 14)\nif(ANDROID_SDK_COMPATIBLE_TARGET)\n  set(ANDROID_SDK_COMPATIBLE_TARGET \"${ANDROID_SDK_COMPATIBLE_TARGET}\" CACHE INTERNAL \"\")\nendif()\nstring(REGEX REPLACE \"android-\" \"\" android_sdk_target_num ${ANDROID_SDK_COMPATIBLE_TARGET})\n\nif( (ANDROID_SDK_TARGET AND ANDROID_SDK_TARGET LESS 21) OR (android_sdk_target_num LESS 21) )\n  message(STATUS \"[OpenCV for Android SDK]: A new OpenGL Camera Bridge (CameraGLSurfaceView, CameraGLRendererBase, CameraRenderer, Camera2Renderer) is disabled, because ANDROID_SDK_TARGET (${android_sdk_target_num}) < 21\")\nelse()\n  ocv_copyfiles_append_dir(JAVA_SRC_COPY \"${OPENCV_JAVA_BINDINGS_DIR}/gen/android-21/java\" \"${java_src_dir}\")\nendif()\n\nif( (ANDROID_SDK_TARGET AND ANDROID_SDK_TARGET LESS 24) OR (android_sdk_target_num LESS 24) )\n  message(STATUS \"[OpenCV for Android SDK]: An experiemntal Native Camera is disabled, because ANDROID_SDK_TARGET (${android_sdk_target_num}) < 24\")\nelse()\n  ocv_copyfiles_append_dir(JAVA_SRC_COPY \"${OPENCV_JAVA_BINDINGS_DIR}/gen/android-24/java\" \"${java_src_dir}\")\nendif()\n\n# copy boilerplate\nfile(GLOB_RECURSE seed_project_files_rel RELATIVE \"${CMAKE_CURRENT_SOURCE_DIR}/android_lib/\" \"${CMAKE_CURRENT_SOURCE_DIR}/android_lib/*\")\nlist(REMOVE_ITEM seed_project_files_rel \"${ANDROID_MANIFEST_FILE}\")\nforeach(file ${seed_project_files_rel})\n  configure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/android_lib/${file}\" \"${OPENCV_JAVA_DIR}/${file}\" @ONLY)\n  list(APPEND depends \"${CMAKE_CURRENT_SOURCE_DIR}/android_lib/${file}\")\n  get_filename_component(install_subdir \"${file}\" PATH)\n  install(FILES \"${OPENCV_JAVA_DIR}/${file}\" DESTINATION \"${JAVA_INSTALL_ROOT}/${install_subdir}\" COMPONENT java)\nendforeach()\n\nlist(APPEND depends gen_opencv_java_source \"${OPENCV_DEPHELPER}/gen_opencv_java_source\")\nocv_copyfiles_add_target(${the_module}_android_source_copy JAVA_SRC_COPY \"Copy Java(Android SDK) source files\" ${depends})\nfile(REMOVE \"${OPENCV_DEPHELPER}/${the_module}_android_source_copy\")  # force rebuild after CMake run\n\nset(depends ${the_module}_android_source_copy \"${OPENCV_DEPHELPER}/${the_module}_android_source_copy\")\n\n# generate Android library project\nset(android_sdk_project_files ${ANDROID_LIB_PROJECT_FILES})  # build.xml;local.properties;proguard-project.txt;project.properties\nocv_list_add_prefix(android_sdk_project_files \"${OPENCV_JAVA_DIR}/\")\n\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/android_lib/${ANDROID_MANIFEST_FILE}\" \"${CMAKE_CURRENT_BINARY_DIR}/${ANDROID_MANIFEST_FILE}\" @ONLY)\n\nadd_custom_command(\n    OUTPUT ${android_sdk_project_files} \"${OPENCV_JAVA_DIR}/${ANDROID_MANIFEST_FILE}\"\n    COMMAND ${CMAKE_COMMAND} -E remove ${android_sdk_project_files}\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_BINARY_DIR}/${ANDROID_MANIFEST_FILE}\" \"${OPENCV_JAVA_DIR}/${ANDROID_MANIFEST_FILE}\"\n    COMMAND ${ANDROID_EXECUTABLE} --silent create lib-project --path \"${OPENCV_JAVA_DIR}\" --target \"${ANDROID_SDK_COMPATIBLE_TARGET}\" --name OpenCV --package org.opencv 2>\"${CMAKE_CURRENT_BINARY_DIR}/create_lib_project.log\"\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_BINARY_DIR}/${ANDROID_MANIFEST_FILE}\" \"${OPENCV_JAVA_DIR}/${ANDROID_MANIFEST_FILE}\"\n    WORKING_DIRECTORY \"${OPENCV_JAVA_DIR}\"\n    MAIN_DEPENDENCY \"${CMAKE_CURRENT_BINARY_DIR}/${ANDROID_MANIFEST_FILE}\"\n    DEPENDS ${depends}\n    COMMENT \"Generating OpenCV Android library project. SDK target: ${lib_target_sdk_target}\"\n)\nlist(APPEND depends ${android_sdk_project_files} \"${OPENCV_JAVA_DIR}/${ANDROID_MANIFEST_FILE}\")\n\ninstall(DIRECTORY \"${OPENCV_JAVA_DIR}/src\" DESTINATION \"${JAVA_INSTALL_ROOT}\" COMPONENT java)\ninstall(FILES \"${OPENCV_JAVA_DIR}/${ANDROID_PROJECT_PROPERTIES_FILE}\" DESTINATION ${JAVA_INSTALL_ROOT} COMPONENT java)\ninstall(FILES \"${OPENCV_JAVA_DIR}/${ANDROID_MANIFEST_FILE}\" DESTINATION ${JAVA_INSTALL_ROOT} COMPONENT java)\n\n# build jar\nset(JAR_FILE \"${OpenCV_BINARY_DIR}/bin/classes.jar\")\n# build the library project\n# normally we should do this after a native part, but for a library project we can build the java part first\nadd_custom_command(\n    OUTPUT \"${JAR_FILE}\" \"${OPENCV_DEPHELPER}/${the_module}_android\"\n    COMMAND ${ANT_EXECUTABLE} -q -noinput -k debug -Djava.target=1.6 -Djava.source=1.6\n    COMMAND ${CMAKE_COMMAND} -E touch \"${OPENCV_DEPHELPER}/${the_module}_android\"\n    WORKING_DIRECTORY \"${OPENCV_JAVA_DIR}\"\n    DEPENDS ${depends}\n    COMMENT \"Building OpenCV Android library project\"\n)\n\nadd_custom_target(${the_module}_android DEPENDS \"${OPENCV_DEPHELPER}/${the_module}_android\" SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/android_lib/${ANDROID_MANIFEST_FILE}\")\nadd_dependencies(${the_module} ${the_module}_android)\n\n# empty 'gen' and 'res' folders\ninstall(CODE \"\nFILE(MAKE_DIRECTORY \\\"\\$ENV{DESTDIR}\\${CMAKE_INSTALL_PREFIX}/${JAVA_INSTALL_ROOT}/gen\\\")\nFILE(MAKE_DIRECTORY \\\"\\$ENV{DESTDIR}\\${CMAKE_INSTALL_PREFIX}/${JAVA_INSTALL_ROOT}/res\\\")\n\" COMPONENT java)\n\nocv_update(ANDROID_COMPILE_SDK_VERSION \"27\")\nocv_update(ANDROID_MIN_SDK_VERSION \"14\")\nif(ANDROID_NATIVE_API_LEVEL GREATER 21)\n  ocv_update(ANDROID_TARGET_SDK_VERSION \"${ANDROID_NATIVE_API_LEVEL}\")\nelse()\n  ocv_update(ANDROID_TARGET_SDK_VERSION \"21\")\nendif()\n\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/build.gradle.in\" \"${CMAKE_CURRENT_BINARY_DIR}/build.gradle\" @ONLY)\ninstall(FILES \"${CMAKE_CURRENT_BINARY_DIR}/build.gradle\" DESTINATION ${JAVA_INSTALL_ROOT}/.. COMPONENT java)\n\nelse()  # gradle build\n#\n# Android Gradle-based project\n#\n\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/build.gradle.in\" \"${ANDROID_TMP_INSTALL_BASE_DIR}/opencv/build.gradle\" @ONLY)\n\n#TODO: INSTALL ONLY\nocv_copyfiles_append_dir(JAVA_SRC_COPY \"${OPENCV_JAVA_BINDINGS_DIR}/gen/android/java\" \"${java_src_dir}\")\nocv_copyfiles_append_dir(JAVA_SRC_COPY \"${OPENCV_JAVA_BINDINGS_DIR}/gen/android-21/java\" \"${java_src_dir}\")\nocv_copyfiles_append_dir(JAVA_SRC_COPY \"${OPENCV_JAVA_BINDINGS_DIR}/gen/android-24/java\" \"${java_src_dir}\")\n\n# copy boilerplate\nset(SOURSE_SETS_JNI_LIBS_SRC_DIRS \"'../../jni'\")\nset(SOURSE_SETS_JAVA_SRC_DIRS \"'src'\")\nset(SOURSE_SETS_RES_SRC_DIRS \"'${OpenCV_SOURCE_DIR}/modules/java/android_sdk/android_gradle_lib/res'\")\nset(SOURSE_SETS_MANIFEST_SRC_FILE \"'AndroidManifest.xml'\")\nset(MAVEN_PUBLISH_PLUGIN_DECLARATION \"\")\nset(BUILD_GRADLE_ANDROID_PUBLISHING_CONFIG \"\")\nset(BUILD_GRADLE_PUBLISHING_CONFIG \"\")\n\nset(__base_dir \"${CMAKE_CURRENT_SOURCE_DIR}/android_gradle_lib/\")\nfile(GLOB_RECURSE seed_project_files_rel RELATIVE \"${__base_dir}/\" \"${__base_dir}/*\")\nlist(REMOVE_ITEM seed_project_files_rel \"${ANDROID_MANIFEST_FILE}\")\nforeach(file ${seed_project_files_rel})\n  configure_file(\"${__base_dir}/${file}\" \"${OPENCV_JAVA_DIR}/${file}\" @ONLY)\n  list(APPEND depends \"${__base_dir}/${file}\")\n  get_filename_component(install_subdir \"${file}\" PATH)\n  if(NOT file STREQUAL \"build.gradle\")\n    install(FILES \"${OPENCV_JAVA_DIR}/${file}\" DESTINATION \"${JAVA_INSTALL_ROOT}/${install_subdir}\" COMPONENT java)\n  endif()\nendforeach()\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/build.gradle.in\" \"${OPENCV_JAVA_DIR}/build.gradle\" @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Including Required OpenCV Modules Recursively in CMake\nDESCRIPTION: Uses the custom `ocv_include_modules_recurse` CMake function to include necessary headers and potentially other build settings for all modules listed in the `OPENCV_CUDA_SAMPLES_REQUIRED_DEPS` variable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nocv_include_modules_recurse(${OPENCV_CUDA_SAMPLES_REQUIRED_DEPS})\n```\n\n----------------------------------------\n\nTITLE: Setting JFIF Resolution Information in libjpeg (C)\nDESCRIPTION: C fields within the compression parameters structure (`cinfo`) used to specify pixel density information in the JFIF marker. `density_unit` indicates the units (0=unknown, 1=dots/inch, 2=dots/cm), while `X_density` and `Y_density` (both `UINT16`) provide the horizontal and vertical densities. Default values are 0, 1, 1, signifying square pixels of unknown size.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_40\n\nLANGUAGE: C\nCODE:\n```\nUINT8 density_unit\n```\n\nLANGUAGE: C\nCODE:\n```\nUINT16 X_density\n```\n\nLANGUAGE: C\nCODE:\n```\nUINT16 Y_density\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building an OpenCV Highgui GTK Plugin - CMake\nDESCRIPTION: This CMake snippet configures the build process for the opencv_highgui_gtk plugin, scanning for GTK dependencies, verifying their availability, and setting up conditional OpenGL integration. It uses custom OpenCV CMake includes and macros to manage features, handle versioned GTK dependencies, and ensure that the correct components are included for successful compilation. Dependencies include OpenCV modules (core, imgproc, imgcodecs) and third-party GTK libraries, with input parameters such as HAVE_GTK, WITH_GTK, and WITH_OPENGL determining build features. Output is the plugin target; missing dependencies or incorrect setup will terminate the build with informative error messages.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/misc/plugins/plugin_gtk/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5)\nproject(opencv_highgui_gtk)\n\nget_filename_component(OpenCV_SOURCE_DIR \"${CMAKE_CURRENT_LIST_DIR}/../../../../..\" ABSOLUTE)\ninclude(\"${OpenCV_SOURCE_DIR}/cmake/OpenCVPluginStandalone.cmake\")\n\n# scan dependencies\nset(WITH_GTK ON)\ninclude(\"${OpenCV_SOURCE_DIR}/modules/highgui/cmake/init.cmake\")\n\nif(NOT HAVE_GTK)\n  message(FATAL_ERROR \"GTK: NO\")\nendif()\n\nocv_warnings_disable(CMAKE_CXX_FLAGS -Wno-deprecated-declarations)\n\nset(OPENCV_PLUGIN_DEPS core imgproc imgcodecs)\nif(TARGET ocv.3rdparty.gtk3)\n  set(__deps ocv.3rdparty.gtk3)\nelif(TARGET ocv.3rdparty.gtk2)\n  set(__deps ocv.3rdparty.gtk2)\nelif(TARGET ocv.3rdparty.gtk)\n  set(__deps ocv.3rdparty.gtk)\nelse()\n  message(FATAL_ERROR \"Missing dependency target for GTK libraries\")\nendif()\nocv_create_plugin(highgui \"opencv_highgui_gtk\" \"${__deps}\" \"GTK\" \"src/window_gtk.cpp\")\nif(WITH_OPENGL)\n  if(HAVE_GTK2\n      AND TARGET ocv.3rdparty.gtkglext\n      AND TARGET ocv.3rdparty.gtk_opengl\n      AND NOT OPENCV_GTK_DISABLE_GTKGLEXT\n      AND NOT OPENCV_GTK_DISABLE_OPENGL\n  )\n    message(STATUS \"OpenGL: YES\")\n    target_link_libraries(${OPENCV_PLUGIN_NAME} PRIVATE\n        ocv.3rdparty.gtkglext ocv.3rdparty.gtk_opengl\n    )\n  else()\n    message(WARNING \"OpenGL dependencies are not available!\")\n  endif()\nendif()\n\nif(HAVE_GTK3)\n  message(STATUS \"GTK3+: ver ${GTK3_VERSION}\")\nelif(HAVE_GTK3)\n  message(STATUS \"GTK2+: ver ${GTK2_VERSION}\")\nelif(DEFINED GTK_VERSION)\n  message(STATUS \"GTK+: ver ${GTK_VERSION}\")\nelse()\n  message(STATUS \"GTK+: YES\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Handling OpenCL Source Exclusion and Compiler Flags for the DNN Module - CMake\nDESCRIPTION: Checks OpenCL and related options for the DNN module, adding required include directories if OpenCL is enabled; otherwise, excludes OpenCL-specific source files. Modifies the sources_options variable accordingly. Inputs are OPENCV_DNN_OPENCL and HAVE_OPENCL variables, output modifies include_dirs and sources_options.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/dnn/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nif(OPENCV_DNN_OPENCL AND HAVE_OPENCL)\n  list(APPEND include_dirs ${OPENCL_INCLUDE_DIRS})\nelse()\n  set(sources_options EXCLUDE_OPENCL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Type and Flags for OpenCV World Module (CMake)\nDESCRIPTION: Initializes CMake variables `the_description`, `OPENCV_MODULE_IS_PART_OF_WORLD`, and `BUILD_opencv_world_INIT`. It then conditionally sets `OPENCV_MODULE_TYPE` to `STATIC` or leaves it unset (implicitly shared) and `OPENCV_WORLD_FLAGS_PROPERTY` to `STATIC_LIBRARY_FLAGS` or `LINK_FLAGS` based on the value of the `BUILD_SHARED_LIBS` variable. This prepares the environment for building the aggregated `opencv_world` library.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/world/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(the_description \"All OpenCV modules\")\nset(OPENCV_MODULE_IS_PART_OF_WORLD FALSE)\nset(BUILD_opencv_world_INIT OFF)\n\nif(NOT BUILD_SHARED_LIBS)\n  set(OPENCV_MODULE_TYPE STATIC)\n  set(OPENCV_WORLD_FLAGS_PROPERTY STATIC_LIBRARY_FLAGS)\nelse()\n  set(OPENCV_WORLD_FLAGS_PROPERTY LINK_FLAGS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Java Tests JAR using Ant in CMake\nDESCRIPTION: Configures the build process for the OpenCV Java tests JAR using Ant. It sets up the build command and specifies dependencies for the build process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/pure_test/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_command(OUTPUT \"${OPENCV_JAVA_TEST_DIR}/build/jar/opencv-test.jar\"\n    COMMAND \"${ANT_EXECUTABLE}\" -noinput -k build\n    WORKING_DIRECTORY \"${OPENCV_JAVA_TEST_DIR}\"\n    DEPENDS ${depends} \"${OPENCV_JAVA_TEST_DIR}/build.xml\" \"${CMAKE_CURRENT_SOURCE_DIR}/build.xml\" \"${OPENCV_JAR_FILE}\" \"${OPENCV_JAVA_TEST_DIR}/bin/${JAR_NAME}\"\n    COMMENT \"Build Java tests\"\n)\n\nfile(GENERATE OUTPUT \"${OPENCV_JAVA_TEST_DIR}/ant-$<CONFIGURATION>.properties\" CONTENT \"opencv.lib.path=$<TARGET_FILE_DIR:${the_module}>\")\n```\n\n----------------------------------------\n\nTITLE: Configuring WinRT Video Capture Support\nDESCRIPTION: Conditionally adds WinRT-specific video capture implementation files when building for Windows Runtime 8.1+. Includes headers and source files for WinRT capture, bridge, and media streaming components.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/videoio/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(DEFINED WINRT AND NOT DEFINED WINRT_8_0 AND NOT DEFINED ENABLE_WINRT_MODE_NATIVE)\n    message(STATUS \"  ${name}: WinRT detected. Adding WinRT API header\")\n    list(APPEND videoio_ext_hdrs \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/cap_winrt.hpp\")\n\n    list(APPEND videoio_srcs\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt_capture.cpp\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt_bridge.cpp\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt_video.cpp\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt/CaptureFrameGrabber.cpp\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt/MediaStreamSink.cpp)\n    list(APPEND videoio_hdrs\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt_capture.hpp\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt_bridge.hpp\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt_video.hpp\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt/MFIncludes.hpp\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt/CaptureFrameGrabber.hpp\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt/MediaSink.hpp\n        ${CMAKE_CURRENT_LIST_DIR}/src/cap_winrt/MediaStreamSink.hpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Acceleration Libraries for OpenCV Tests\nDESCRIPTION: Checks for available acceleration libraries such as OpenVINO and ADE, and configures the OpenCV test modules accordingly. It links appropriate libraries and includes directories to use these accelerations, ensuring optimal performance of tests linked directly to these symbols.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nset(__test_extra_deps \"\")\nif(TARGET ocv.3rdparty.openvino AND OPENCV_GAPI_WITH_OPENVINO)\n  list(APPEND __test_extra_deps ocv.3rdparty.openvino)\nendif()\nocv_add_accuracy_tests(${__test_extra_deps})\n```\n\nLANGUAGE: CMake\nCODE:\n```\nif(TARGET opencv_test_gapi)\n  target_include_directories(opencv_test_gapi PRIVATE \"${CMAKE_CURRENT_LIST_DIR}/src\")\n  target_link_libraries(opencv_test_gapi PRIVATE ade)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Writing Inline and Block Formulas - Doxygen Markup\nDESCRIPTION: Presents usage of Doxygen's inline (\\f$ ... \\f$) and block (\\f[ ... \\f]) formula tags for mathematical expressions. No dependencies aside from doxygen itself. Inputs are LaTeX-formatted formulas, outputs are rendered math in documentation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n\\f$ ... \\f$\n```\n\nLANGUAGE: markdown\nCODE:\n```\n\\f[ ... \\f]\n```\n\n----------------------------------------\n\nTITLE: Configuring Relative Paths and Dependency Checks in CMake\nDESCRIPTION: This CMake snippet appends the relative path of pattern tools to the OpenCV configuration if not cross-compiling. It also checks the BUILD_DOCS flag to determine whether to return early or continue to process documentation dependencies. Essential dependencies include doxygen for documentation generation and optional tools such as javadoc and python bindings.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT CMAKE_CROSSCOMPILING)\\n  file(RELATIVE_PATH __loc_relative \\\"${OpenCV_BINARY_DIR}\\\" \\\"${CMAKE_CURRENT_LIST_DIR}/pattern_tools\\n\\\")\\n  file(APPEND \\\"${OpenCV_BINARY_DIR}/opencv_apps_python_tests.cfg\\\" \\\"${__loc_relative}\\\")\\nendif()\\n\\nif(NOT BUILD_DOCS)\\n  return()\\nendif()\\n\\n# Dependencies scheme (* - optional):\\n#\\n# javadoc* -> doxygen_javadoc* -> doxygen_cpp ---------> doxygen -> opencv_docs\\n#    \\                               \\                     /        /\\n#     \\                               -> doxygen_python* ->        /\n```\n\n----------------------------------------\n\nTITLE: Enabling CMake Download in OpenCV Maven Build\nDESCRIPTION: Maven command-line option to download the latest CMake binary instead of using the native package. This is an optional configuration for x86 processors.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/maven/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n-Ddownload.cmake=true\n```\n\n----------------------------------------\n\nTITLE: Adding Example Subdirectories for Standalone Build in CMake\nDESCRIPTION: Includes a specific subset of example subdirectories (cpp, directx (if Win32), dnn, opencl, sycl, tapi) when building the samples standalone. Some directories included in the integrated build (like gpu, opengl, openvx) are commented out or omitted, likely reflecting dependencies or typical usage scenarios for standalone builds against a pre-installed OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(cpp)\nif(WIN32)\n  add_subdirectory(directx)\nendif()\nadd_subdirectory(dnn)\n# add_subdirectory(gpu)\nadd_subdirectory(opencl)\nadd_subdirectory(sycl)\n# add_subdirectory(opengl)\n# add_subdirectory(openvx)\nadd_subdirectory(tapi)\n# add_subdirectory(va_intel)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dispatched Files for CPU Optimizations\nDESCRIPTION: Adds CPU architecture-specific dispatched files for core mathematical and matrix operations using SSE2, AVX, AVX2, and LASX optimizations.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/core/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nocv_add_dispatched_file(mathfuncs_core SSE2 AVX AVX2 LASX)\nocv_add_dispatched_file(stat SSE4_2 AVX2 LASX)\nocv_add_dispatched_file(arithm SSE2 SSE4_1 AVX2 VSX3 LASX)\n```\n\n----------------------------------------\n\nTITLE: Running Individual WebAssembly Intrinsic Type Tests in OpenCV.js - JavaScript\nDESCRIPTION: These JavaScript snippets allow for targeting WebAssembly intrinsic (HAL) tests for specific data types (uint8, int8, uint16, etc.) within OpenCV.js. Requires 'cv' to be loaded. Each function call tests the intrinsics for one particular data type, with results reported in the debug console. Useful for isolating and diagnosing issues with specific SIMD types.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_20\n\nLANGUAGE: js\nCODE:\n```\ncv.test_hal_intrin_uint8()\\ncv.test_hal_intrin_int8()\\ncv.test_hal_intrin_uint16()\\ncv.test_hal_intrin_int16()\\ncv.test_hal_intrin_uint32()\\ncv.test_hal_intrin_int32()\\ncv.test_hal_intrin_uint64()\\ncv.test_hal_intrin_int64()\\ncv.test_hal_intrin_float32()\\ncv.test_hal_intrin_float64()\n```\n\n----------------------------------------\n\nTITLE: Carotene Library Integration\nDESCRIPTION: Configures and compiles the Carotene library component, setting up NEON support when available in the CPU baseline.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/carotene/hal/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(compile_carotene)\n  if(\";${CPU_BASELINE_FINAL};\" MATCHES \";NEON;\")\n    set(WITH_NEON ON)\n  endif()\n\n  add_subdirectory(\"${CAROTENE_DIR}\" \"${CMAKE_CURRENT_BINARY_DIR}/carotene\")\nendfunction()\n\ncompile_carotene()\n```\n\n----------------------------------------\n\nTITLE: Enabling Foreign Architectures in DPKG (Bash)\nDESCRIPTION: Configures the `dpkg` package manager to recognize and allow the installation of packages built for the `arm64` and `armhf` architectures alongside the native host architecture. This is a core step in enabling MultiArch functionality.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo dpkg --add-architecture arm64\nsudo dpkg --add-architecture armhf\n```\n\n----------------------------------------\n\nTITLE: Updating Python Tests Configuration File in CMake for OpenCV\nDESCRIPTION: Converts the list of test locations to a newline-separated string and updates the configuration file with this content using the ocv_update_file function.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/python/test/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nstring(REPLACE \";\" \"\\n\" opencv_tests_locations_ \"${opencv_tests_locations}\")\nocv_update_file(\"${OPENCV_PYTHON_TESTS_CONFIG_FILE}\" \"${opencv_tests_locations_}\")\n```\n\n----------------------------------------\n\nTITLE: General CMake Command Line Syntax for OpenCV\nDESCRIPTION: Shows the basic syntax for invoking CMake from the command line to configure an OpenCV build. Requires specifying options and the path to the OpenCV source directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ncmake [options] <path-to-source>\n```\n\n----------------------------------------\n\nTITLE: MIPS DSPr2 Capability Test\nDESCRIPTION: Checks for MIPS DSPr2 instruction support on MIPS32r2 platforms\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/simd/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: c\nCODE:\n```\n#if !(defined(__mips__) && __mips_isa_rev >= 2)\n#error MIPS DSPr2 is currently only available on MIPS32r2 platforms.\n#endif\nint main(void) {\n  int c = 0, a = 0, b = 0;\n  __asm__ __volatile__ (\n    \"precr.qb.ph %[c], %[a], %[b]\"\n    : [c] \"=r\" (c)\n    : [a] \"r\" (a), [b] \"r\" (b)\n  );\n  return c;\n}\n```\n\n----------------------------------------\n\nTITLE: CMake Build Configuration Output\nDESCRIPTION: Example output showing successful Wayland configuration during CMake setup, displaying version information for various dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/highgui_wayland_ubuntu.markdown#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n--\n--   GUI:                           Wayland\n--     Wayland:                     (Experimental) YES\n--       Wayland Client:            YES (ver 1.22.0)\n--       Wayland Cursor:            YES (ver 1.22.0)\n--       Wayland Protocols:         YES (ver 1.34)\n--       Xkbcommon:                 YES (ver 1.6.0)\n--       Wayland EGL(Option):       YES (ver 18.1.0)\n--     GTK+:                        NO\n--     VTK support:                 NO\n```\n\n----------------------------------------\n\nTITLE: Configuring Namespace Settings for IlmBase and OpenEXR\nDESCRIPTION: This snippet configures namespace settings for IlmBase and OpenEXR components. It sets up internal namespaces with version suffixes to avoid conflicts when embedded in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openexr/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(ILMBASE_VERSION_API \"opencv\")\nset(ILMBASE_INTERNAL_NAMESPACE_CUSTOM 1)\nset(IMATH_INTERNAL_NAMESPACE \"Imath_${ILMBASE_VERSION_API}\")\nset(IEX_INTERNAL_NAMESPACE \"Iex_${ILMBASE_VERSION_API}\")\nset(ILMTHREAD_INTERNAL_NAMESPACE \"IlmThread_${ILMBASE_VERSION_API}\")\n\nset(ILMBASE_NAMESPACE_CUSTOM 0)\nset(IMATH_NAMESPACE \"Imath\")\nset(IEX_NAMESPACE \"Iex\")\nset(ILMTHREAD_NAMESPACE \"IlmThread\")\nset(ILMBASE_VERSION_STRING \"\\\"${ILMBASE_VERSION}\\\"\" )\nset(ILMBASE_PACKAGE_STRING \"\\\"IlmBase ${ILMBASE_VERSION}\\\"\" )\n\n\nset(OPENEXR_VERSION_API \"opencv\")\nset(OPENEXR_IMF_INTERNAL_NAMESPACE_CUSTOM 1)\nset(OPENEXR_IMF_INTERNAL_NAMESPACE \"Imf_${ILMBASE_VERSION_API}\")\nset(OPENEXR_IMF_NAMESPACE_CUSTOM 0)\nset(OPENEXR_IMF_NAMESPACE \"Imf\")\n\nset(OPENEXR_VERSION_STRING \"\\\"${OPENEXR_VERSION}\\\"\" )\nset(OPENEXR_PACKAGE_STRING \"\\\"OpenEXR ${OPENEXR_VERSION}\\\"\" )\n```\n\n----------------------------------------\n\nTITLE: Python Documentation Generation Setup\nDESCRIPTION: Configures Python documentation generation target with signature injection when all required dependencies are available.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_23\n\nLANGUAGE: cmake\nCODE:\n```\nif(PYTHON_DEFAULT_EXECUTABLE\n      AND HAVE_PYTHON_BS4\n      AND OPENCV_PYTHON_SIGNATURES_FILE\n      AND TARGET gen_opencv_python_source)\n    add_custom_target(doxygen_python\n      COMMAND ${PYTHON_DEFAULT_EXECUTABLE} \"${CMAKE_CURRENT_SOURCE_DIR}/tools/add_signatures.py\" \"${CMAKE_CURRENT_BINARY_DIR}/doxygen/html/\" \"${OPENCV_PYTHON_SIGNATURES_FILE}\" \"python\"\n      DEPENDS doxygen_cpp gen_opencv_python_source\n      COMMENT \"Inject Python signatures into documentation\"\n    )\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Collecting G-API 3rd Party Source Files (VASOT) in CMake\nDESCRIPTION: Uses `file(GLOB_RECURSE ...)` to find and collect all C++ source files (`.cpp`) from the VAS Object Tracking (vasot) third-party library located within the G-API module's source directory. The results are stored in the `gapi_3rdparty_srcs` variable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE gapi_3rdparty_srcs\n    \"${CMAKE_CURRENT_LIST_DIR}/src/3rdparty/vasot/src/*.cpp\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining libjasper Library Target in CMake\nDESCRIPTION: This snippet defines the libjasper library target as a static library and sets up compiler definitions for Windows builds.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjasper/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${JASPER_LIBRARY} STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${lib_srcs} ${lib_hdrs} ${lib_ext_hdrs})\n\nif(WIN32 AND NOT MINGW)\n  add_definitions(-DJAS_WIN_MSVC_BUILD)\nendif(WIN32 AND NOT MINGW)\n```\n\n----------------------------------------\n\nTITLE: Converting JPEG Quality Scale to Linear Scaling Percentage (C)\nDESCRIPTION: This function converts a value on the IJG-recommended quality scale to a linear scaling percentage. Note that this function may change in future releases.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_25\n\nLANGUAGE: C\nCODE:\n```\nint jpeg_quality_scaling (int quality)\n```\n\n----------------------------------------\n\nTITLE: Check for Memory Allocation Functions in CMake\nDESCRIPTION: This block of code verifies the presence of memory allocation functions like posix_memalign and aligned_alloc, setting the appropriate macros for conditional compilation. This ensures compatibility for dynamic memory handling.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_REQUIRED_DEFINITIONS -D_POSIX_C_SOURCE=200112L)\ncheck_symbol_exists(posix_memalign stdlib.h HAVE_POSIX_MEMALIGN)\nif(HAVE_POSIX_MEMALIGN)\n    add_definitions(-DHAVE_POSIX_MEMALIGN)\nendif()\nset(CMAKE_REQUIRED_DEFINITIONS)\n\nset(CMAKE_REQUIRED_DEFINITIONS -D_ISOC11_SOURCE=1)\ncheck_symbol_exists(aligned_alloc stdlib.h HAVE_ALIGNED_ALLOC)\nif(HAVE_ALIGNED_ALLOC)\n    add_definitions(-DHAVE_ALIGNED_ALLOC)\nendif()\nset(CMAKE_REQUIRED_DEFINITIONS)\n```\n\n----------------------------------------\n\nTITLE: Adaptive Progressive Decoding Loop in C\nDESCRIPTION: Shows the recommended loop structure for handling progressive JPEG decoding with final pass processing. Automatically adapts to incoming data speed and ensures full quality display.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_54\n\nLANGUAGE: c\nCODE:\n```\ndo {\n    absorb any waiting input by calling jpeg_consume_input()\n    final_pass = jpeg_input_complete(&cinfo);\n    adjust output decompression parameters if required\n    jpeg_start_output(&cinfo, cinfo.input_scan_number);\n    ...\n    jpeg_finish_output()\n} while (!final_pass);\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV.js Tests - Bash\nDESCRIPTION: Adds the --build_test option to the OpenCV.js build script to include test source code in the build. Dependency: emcmake, Python. Output is test files placed alongside opencv.js for testing functionality either in browser or automated environments.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nemcmake python ./opencv/platforms/js/build_js.py build_js --build_test\n```\n\n----------------------------------------\n\nTITLE: Location of libwebp Library in eSDK Sysroot (Path)\nDESCRIPTION: This file path shows the location of the `libwebp.so.7` shared library within the Qualcomm eSDK sysroot. This library might be a runtime dependency for some OpenCV tests or applications. If tests fail on the target device due to this library being missing, it should be located at this path within the eSDK and manually copied (pushed) to the appropriate library directory on the target device.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_fastcv/building_fastcv.markdown#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n<ESDK_PATH>\\qcom-wayland_sdk\\tmp\\sysroots\\qcs6490-rb3gen2-vision-kit\\usr\\lib\\libwebp.so.7\n```\n\n----------------------------------------\n\nTITLE: Copying and Configuring libcxx_helper Files in CMake\nDESCRIPTION: This CMake snippet recursively finds all files within the `libcxx_helper` directory relative to the current source directory. It then iterates through these files, configures each one (replacing `@VAR@` placeholders) and copies it to the `OPENCV_JAVA_DIR`. Each original file is added to the `depends` list, and an installation rule is created to install the configured file into the appropriate subdirectory under `${JAVA_INSTALL_ROOT}/..` as part of the 'java' component.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/android_sdk/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# copy libcxx_helper\nset(__base_dir \"${CMAKE_CURRENT_SOURCE_DIR}/\")\nfile(GLOB_RECURSE __files_rel RELATIVE \"${__base_dir}/\" \"${__base_dir}/libcxx_helper/*\")\nforeach(file ${__files_rel})\n  configure_file(\"${__base_dir}/${file}\" \"${OPENCV_JAVA_DIR}/${file}\" @ONLY)\n  list(APPEND depends \"${__base_dir}/${file}\")\n  get_filename_component(install_subdir \"${file}\" PATH)\n  install(FILES \"${OPENCV_JAVA_DIR}/${file}\" DESTINATION \"${JAVA_INSTALL_ROOT}/../${install_subdir}\" COMPONENT java)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Collecting G-API Header Files using CMake\nDESCRIPTION: Uses the `file(GLOB ...)` command to find and collect all header files (`.hpp`, `.h`) associated with the G-API module from various subdirectories within the module's include path. The collected list of headers is stored in the `gapi_ext_hdrs` variable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB gapi_ext_hdrs\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/*.h\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/cpu/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/fluid/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/gpu/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/infer/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/oak/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/ocl/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/own/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/plaidml/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/python/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/render/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/s11n/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/streaming/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/streaming/gstreamer/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/streaming/onevpl/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/plaidml/*.hpp\"\n    \"${CMAKE_CURRENT_LIST_DIR}/include/opencv2/${name}/util/*.hpp\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining OpenJPEG Version Variables in CMake\nDESCRIPTION: Sets CMake variables for the OpenJPEG library version components (MAJOR, MINOR, BUILD) and constructs a full version string. It also sets the standard `PACKAGE_VERSION` variable, which is often used by packaging tools.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\n#-----------------------------------------------------------------------------\n# OPENJPEG version number, useful for packaging and doxygen doc:\nset(OPENJPEG_VERSION_MAJOR 2)\nset(OPENJPEG_VERSION_MINOR 5)\nset(OPENJPEG_VERSION_BUILD 3)\nset(OPENJPEG_VERSION\n  \"${OPENJPEG_VERSION_MAJOR}.${OPENJPEG_VERSION_MINOR}.${OPENJPEG_VERSION_BUILD}\")\nset(PACKAGE_VERSION\n  \"${OPENJPEG_VERSION_MAJOR}.${OPENJPEG_VERSION_MINOR}.${OPENJPEG_VERSION_BUILD}\")\n```\n\n----------------------------------------\n\nTITLE: Finding and Verifying Doxygen in CMake\nDESCRIPTION: Locates the Doxygen package using `find_package`. If found, it checks if the Doxygen version is less than 1.12 and issues a warning if it is. Finally, it adds a custom target named `doxygen` to trigger documentation generation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(Doxygen)\nif(DOXYGEN_FOUND)\n  if (DOXYGEN_VERSION VERSION_LESS 1.12)\n    message(WARNING \"Found doxygen ${DOXYGEN_VERSION}, version 1.12 is used for testing, there is \"\n                    \"a chance your documentation will look different or have some limitations.\")\n  endif()\n  add_custom_target(doxygen)\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenNI Environment Variables\nDESCRIPTION: Uses the 'echo' command to display the values of the OPENNI2_INCLUDE and OPENNI2_REDIST environment variables. This step verifies that the 'OpenNIDevEnvironment' script was sourced correctly and the variables are set to the expected paths for the SDK's include and redistributable directories.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ echo $OPENNI2_INCLUDE\n/home/user/OpenNI_2.3.0.63/Linux/OpenNI-Linux-x64-2.3.0.63/Include\n$ echo $OPENNI2_REDIST\n/home/user/OpenNI_2.3.0.63/Linux/OpenNI-Linux-x64-2.3.0.63/Redist\n```\n\n----------------------------------------\n\nTITLE: Generating OpenCV Module Configuration\nDESCRIPTION: Creates an auto-generated configuration file with definitions for the built-in backend of OpenCV's HighGUI module. It modifies the configuration string based on the back-end configuration and updates a header file accordingly. The 'OPENCV_HIGHGUI_BUILTIN_BACKEND' and 'CMAKE_CURRENT_BINARY_DIR' environment variables must be correctly set prior to invocation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\nset(CONFIG_STR \"// Auto-generated file\n#define OPENCV_HIGHGUI_BUILTIN_BACKEND_STR \\\"${OPENCV_HIGHGUI_BUILTIN_BACKEND}\\\"\n\")\nif(OPENCV_HIGHGUI_BUILTIN_BACKEND STREQUAL \"NONE\")\nset(CONFIG_STR \"${CONFIG_STR}\n#define OPENCV_HIGHGUI_WITHOUT_BUILTIN_BACKEND 1\n\")\nendif()\n\nocv_update_file(\"${CMAKE_CURRENT_BINARY_DIR}/opencv_highgui_config.hpp\" \"${CONFIG_STR}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Module Description and Dispatch Files in CMake\nDESCRIPTION: This snippet sets the description for a 2D Features Framework and adds a dispatched file for the SIFT algorithm with support for various SIMD instruction sets such as SSE4.1, AVX2, and AVX512_SKX. These settings are specific to the CMake build system used in OpenCV projects.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(the_description \\\"2D Features Framework\\\")\\nocv_add_dispatched_file(sift SSE4_1 AVX2 AVX512_SKX)\n```\n\n----------------------------------------\n\nTITLE: Selecting Output File for JPEG Compression with jpeg_stdio_dest in C\nDESCRIPTION: This code snippet shows how to specify the destination for compressed JPEG data in C using libjpeg. It opens a file in binary write mode and attaches it to the jpeg_compress_struct via jpeg_stdio_dest. The snippet includes typical error handling for file I/O and demonstrates the use of fopen with the 'wb' mode to ensure binary output. The dependency is standard C I/O and libjpeg’s destination management. Parameters include the output filename, the C file pointer, and the JPEG compression structure. This step is necessary to prevent corruption of binary data during image compression.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_3\n\nLANGUAGE: C\nCODE:\n```\nFILE *outfile;\n...\nif ((outfile = fopen(filename, \"wb\")) == NULL) {\n    fprintf(stderr, \"can't open %s\\n\", filename);\n    exit(1);\n}\njpeg_stdio_dest(&cinfo, outfile);\n```\n\n----------------------------------------\n\nTITLE: Conditionally Exclude C API and Enable IPP Gaussian Blur in OpenCV imgproc Module (CMake)\nDESCRIPTION: This CMake code conditionally compiles the imgproc module with the C API excluded if 'OPENCV_CORE_EXCLUDE_C_API' is set, and sets 'OPENCV_EXCLUDE_C_API=1' as a compile definition. If IPP (Intel Performance Primitives) is available and the respective option is enabled, it adds an additional compile definition to enable IPP-optimized Gaussian blur. Dependencies include pre-defined variables like OPENCV_CORE_EXCLUDE_C_API, HAVE_IPP, and CMAKE_CURRENT_SOURCE_DIR. Intended inputs are variable settings at CMake configuration time, and outputs are per-configuration compilation flags for the OpenCV imgproc module.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/imgproc/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(OPENCV_CORE_EXCLUDE_C_API)\n  ocv_target_compile_definitions(${the_module} PRIVATE \"OPENCV_EXCLUDE_C_API=1\")\nendif()\n\nif(HAVE_IPP)\n  # OPENCV_IPP_ENABLE_ALL is defined in modules/core/CMakeList.txt\n  OCV_OPTION(OPENCV_IPP_GAUSSIAN_BLUR \"Enable IPP optimizations for GaussianBlur (+8Mb in binary size)\" OPENCV_IPP_ENABLE_ALL)\n  if(OPENCV_IPP_GAUSSIAN_BLUR)\n    ocv_append_source_file_compile_definitions(${CMAKE_CURRENT_SOURCE_DIR}/src/smooth.dispatch.cpp \"ENABLE_IPP_GAUSSIAN_BLUR=1\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Platform-Specific Compiler Definitions for TBB\nDESCRIPTION: Configures platform-specific compiler definitions for Windows (including ARM-specific settings) and Unix platforms. These definitions control TBB's behavior and feature set.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/tbb/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (WIN32)\n  add_definitions(/D__TBB_DYNAMIC_LOAD_ENABLED=0\n                  /D__TBB_BUILD=1\n                  /DTBB_NO_LEGACY=1\n                  /D_UNICODE\n                  /DUNICODE\n                  /DWINAPI_FAMILY=WINAPI_FAMILY_APP\n                  /DDO_ITT_NOTIFY=0\n                  /DUSE_WINTHREAD\n               ) # defines were copied from windows.cl.inc\n\n  if (ARM)\n    add_definitions(/D_WIN32_WINNT=0x0602\n                    /D__TBB_WIN32_USE_CL_BUILTINS\n                   )\n  endif()\n\nset(CMAKE_LINKER_FLAGS \"${CMAKE_LINKER_FLAGS} /APPCONTAINER\")\nelse()\n  add_definitions(-D__TBB_DYNAMIC_LOAD_ENABLED=0         #required\n                  -D__TBB_WEAK_SYMBOLS_PRESENT=0         #required for 4.3\n                  -D__TBB_BUILD=1                        #required\n                  -D__TBB_SURVIVE_THREAD_SWITCH=0        #no cilk support\n                  -DTBB_USE_DEBUG=0                      #just to be sure\n                  -DTBB_NO_LEGACY=1                      #don't need backward compatibility\n                  -DDO_ITT_NOTIFY=0                      #it seems that we don't need these notifications\n                 )\nendif()\n```\n\n----------------------------------------\n\nTITLE: NEON Intrinsics Basic Test\nDESCRIPTION: Verifies basic NEON intrinsics functionality using vector operations\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/simd/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: c\nCODE:\n```\n#include <arm_neon.h>\nint main(int argc, char **argv) {\n  uint16x8_t input = vdupq_n_u16((uint16_t)argc);\n  uint8x8_t output = vmovn_u16(input);\n  return (int)output[0];\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building NDSRVP HAL with CMake - CMake\nDESCRIPTION: This CMake script initializes logging, sets minimum required CMake version, defines source/include directories, discovers sources and headers, and configures the NDSRVP HAL static library for an OpenCV project. Dependencies include OpenCV's core, imgproc, and features2d modules. It sets output locations, manages installation (differentiating static and shared builds), adds internal build metadata, and explicitly marks HAL components as found and versioned. Key variables such as NDSRVP_INCLUDE_DIR and NDSRVP_SOURCE_DIR must be valid relative paths, and this script is intended to be included as part of the overall OpenCV CMake build process.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ndsrvp/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nmessage(STATUS \"##########\")\nmessage(STATUS \"# NDSRVP #\")\nmessage(STATUS \"##########\")\n\ncmake_minimum_required(VERSION ${MIN_VER_CMAKE} FATAL_ERROR)\n\n# project setup\n\nset(NDSRVP_INCLUDE_DIR include)\nset(NDSRVP_SOURCE_DIR src)\n\nfile(GLOB ndsrvp_headers RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\" \"${NDSRVP_INCLUDE_DIR}/*.hpp\")\nfile(GLOB ndsrvp_sources RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\" \"${NDSRVP_SOURCE_DIR}/*.cpp\")\n\nadd_library(ndsrvp_hal STATIC)\ntarget_sources(ndsrvp_hal PRIVATE ${ndsrvp_headers} ${ndsrvp_sources})\n\nset_target_properties(ndsrvp_hal PROPERTIES ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH})\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(ndsrvp_hal EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev)\nendif()\ntarget_include_directories(ndsrvp_hal PRIVATE\n  ${CMAKE_CURRENT_SOURCE_DIR}\n  ${CMAKE_SOURCE_DIR}/modules/core/include\n  ${CMAKE_SOURCE_DIR}/modules/imgproc/include\n  ${CMAKE_SOURCE_DIR}/modules/features2d/include)\n\n# project info\n\nset(NDSRVP_HAL_FOUND TRUE CACHE INTERNAL \"\")\nset(NDSRVP_HAL_VERSION \"0.0.1\" CACHE INTERNAL \"\")\nset(NDSRVP_HAL_LIBRARIES \"ndsrvp_hal\" CACHE INTERNAL \"\")\nset(NDSRVP_HAL_HEADERS \"ndsrvp_hal.hpp\" CACHE INTERNAL \"\")\nset(NDSRVP_HAL_INCLUDE_DIRS \"${CMAKE_CURRENT_SOURCE_DIR}\" CACHE INTERNAL \"\")\n```\n\n----------------------------------------\n\nTITLE: Configuring GUI Options in CMake for OpenCV\nDESCRIPTION: This snippet configures and reports the status of various GUI options for OpenCV, including Wayland, QT, Win32 UI, Cocoa, GTK, Framebuffer, OpenGL, and VTK support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_21\n\nLANGUAGE: CMake\nCODE:\n```\nstatus(\"\")\nstatus(\"  GUI: \" \"${OPENCV_HIGHGUI_BUILTIN_BACKEND}\")\n\nif(WITH_WAYLAND OR HAVE_WAYLAND)\n  status(\"    Wayland:\" HAVE_WAYLAND THEN \"(Experimental) YES\" ELSE \"NO\")\n  status(\"      Wayland Client:\" HAVE_WAYLAND_CLIENT THEN \"YES (ver ${WAYLAND_CLIENT_VERSION})\" ELSE \"NO\")\n  status(\"      Wayland Cursor:\" HAVE_WAYLAND_CURSOR THEN \"YES (ver ${WAYLAND_CURSOR_VERSION})\" ELSE \"NO\")\n  status(\"      Wayland Protocols:\" HAVE_WAYLAND_PROTOCOLS THEN \"YES (ver ${WAYLAND_PROTOCOLS_VERSION})\" ELSE \"NO\")\n  status(\"      Xkbcommon:\" HAVE_XKBCOMMON THEN \"YES (ver ${XKBCOMMON_VERSION})\" ELSE \"NO\")\n  status(\"      Wayland EGL(Option):\" HAVE_WAYLAND_EGL THEN \"YES (ver ${WAYLAND_EGL_VERSION})\" ELSE \"NO\")\nendif()\n\nif(WITH_QT OR HAVE_QT)\n  if(HAVE_QT)\n    status(\"    QT:\" \"YES (ver ${QT_VERSION_MAJOR}.${QT_VERSION_MINOR}.${QT_VERSION_PATCH} ${QT_EDITION})\")\n    if(HAVE_QT_OPENGL)\n      if(Qt${QT_VERSION_MAJOR}OpenGL_LIBRARIES)\n        status(\"      QT OpenGL support:\" HAVE_QT_OPENGL THEN \"YES (${Qt${QT_VERSION_MAJOR}OpenGL_LIBRARIES} ${Qt${QT_VERSION_MAJOR}OpenGL_VERSION_STRING})\" ELSE NO)\n      else()\n        status(\"      QT OpenGL support:\" HAVE_QT_OPENGL THEN \"YES (${QT_QTOPENGL_LIBRARY})\" ELSE NO)\n      endif()\n    else()\n      status(\"      QT OpenGL support:\" \"NO\")\n    endif()\n  else()\n    status(\"    QT:\" \"NO\")\n  endif()\nendif()\n\nif(WITH_WIN32UI)\n  status(\"    Win32 UI:\" HAVE_WIN32UI THEN YES ELSE NO)\nendif()\n\nif(HAVE_COCOA)  # APPLE\n  status(\"    Cocoa:\"  YES)\nendif()\n\nif(WITH_GTK OR HAVE_GTK)\n  if(HAVE_GTK3)\n    status(\"    GTK+:\" \"YES (ver ${GTK3_VERSION})\")\n  elseif(HAVE_GTK)\n    status(\"    GTK+:\" \"YES (ver ${GTK2_VERSION})\")\n    status(\"      GtkGlExt:\" HAVE_GTKGLEXT THEN \"YES (ver ${GTKGLEXT_VERSION})\" ELSE NO)\n  else()\n    status(\"    GTK+:\" \"NO\")\n  endif()\nendif()\n\nif(WITH_FRAMEBUFFER OR HAVE_FRAMEBUFFER)\n  status(\"    Framebuffer UI:\" HAVE_FRAMEBUFFER THEN YES ELSE NO)\n  if(WITH_FRAMEBUFFER_XVFB OR HAVE_FRAMEBUFFER_XVFB)\n    status(\"    Virtual framebuffer UI:\" HAVE_FRAMEBUFFER_XVFB THEN YES ELSE NO)\n  endif()\nendif()\n\nif(WITH_OPENGL OR HAVE_OPENGL)\n  status(\"    OpenGL support:\" HAVE_OPENGL THEN \"YES (${OPENGL_LIBRARIES})\" ELSE NO)\nendif()\n\nif(WITH_VTK OR HAVE_VTK)\n  status(\"    VTK support:\" HAVE_VTK THEN \"YES (ver ${VTK_VERSION})\" ELSE NO)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Code Quality Checks in OpenCV CMake\nDESCRIPTION: Configures Python code quality checks using Pylint and Flake8 if enabled and Python is available. It sets up custom targets for running these tools on the OpenCV codebase.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_PYLINT AND PYTHON_DEFAULT_AVAILABLE)\n  include(cmake/OpenCVPylint.cmake)\nendif()\nif(ENABLE_FLAKE8 AND PYTHON_DEFAULT_AVAILABLE)\n  find_package(Flake8 QUIET)\n  if(NOT FLAKE8_FOUND OR NOT FLAKE8_EXECUTABLE)\n    include(\"${CMAKE_CURRENT_LIST_DIR}/cmake/FindFlake8.cmake\")\n  endif()\n  if(FLAKE8_FOUND)\n    list(APPEND OPENCV_FLAKE8_EXCLUDES \".git\" \"__pycache__\" \"config.py\" \"*.config.py\" \"config-*.py\")\n    list(APPEND OPENCV_FLAKE8_EXCLUDES \"svgfig.py\")  # 3rdparty\n    if(NOT PYTHON3_VERSION_STRING VERSION_GREATER 3.6)\n      # Python 3.6+ (PEP 526): variable annotations (type hints)\n      list(APPEND OPENCV_FLAKE8_EXCLUDES \"samples/dnn/dnn_model_runner/dnn_conversion/common/test/configs\")\n    endif()\n    string(REPLACE \";\" \",\" OPENCV_FLAKE8_EXCLUDES_STR \"${OPENCV_FLAKE8_EXCLUDES}\")\n    add_custom_target(check_flake8\n        COMMAND \"${FLAKE8_EXECUTABLE}\" . --count --select=E9,E901,E999,F821,F822,F823 --show-source --statistics --exclude='${OPENCV_FLAKE8_EXCLUDES_STR}'\n        WORKING_DIRECTORY \"${OpenCV_SOURCE_DIR}\"\n        COMMENT \"Running flake8\"\n    )\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Embedding External Content with HTML iframe\nDESCRIPTION: This HTML snippet uses an iframe to embed the content of 'js_semantic_segmentation.html' located two directories up. The iframe spans the full width of its container. Upon loading ('onload' event), a JavaScript snippet adjusts the iframe's height to match the height of the loaded content's body, preventing internal scrollbars.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_dnn/js_semantic_segmentation/js_semantic_segmentation.markdown#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe src=\"../../js_semantic_segmentation.html\" width=\"100%\"\n        onload=\"this.style.height=this.contentDocument.body.scrollHeight +'px';\">\n</iframe>\n```\n\n----------------------------------------\n\nTITLE: Reading Decompressed Scanlines in C using libjpeg\nDESCRIPTION: Illustrates the function call `jpeg_read_scanlines` (or its variants `jpeg12_read_scanlines` for 9-12 bit precision, `jpeg16_read_scanlines` for 13-16 bit precision) used within a loop to read decompressed image data. This function is called repeatedly (step 6) until all scanlines specified by `cinfo.output_height` are read. It fills the provided buffer(s) with scanline data and returns the number of lines actually read in that call. The specific function variant depends on the required data precision.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_15\n\nLANGUAGE: C\nCODE:\n```\njpeg_read_scanlines(...);  /* Use jpeg12_read_scanlines() for 9-bit\n                                  through 12-bit data precision and\n                                  jpeg16_read_scanlines() for 13-bit\n                                  through 16-bit data precision. */\n```\n\n----------------------------------------\n\nTITLE: Disabling Compiler Warnings for TBB Build\nDESCRIPTION: Disables specific compiler warnings that would otherwise be triggered by the TBB source code. This is necessary to build TBB cleanly across different compilers and platforms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/tbb/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nocv_warnings_disable(CMAKE_CXX_FLAGS\n    /wd4702\n    -Wshadow\n    -Wunused-parameter\n    -Wclass-memaccess                  # TBB 2018 under GCC 8+\n    -Wimplicit-fallthrough             # TBB 2018 under GCC 7+\n    -Wmissing-prototypes               # MacOSX, Android/Clang\n    -Wundef -Wmissing-declarations     # TBB 2019\n    -Wnon-virtual-dtor                 # oneTBB-2020.2 Android\n    -Wunused-but-set-variable          # oneTBB-2020.2 Android\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying CUDA Integration Status in OpenCV Build\nDESCRIPTION: Checks and displays NVIDIA CUDA integration status, including version, supported features (CUFFT, CUBLAS, NVCUVID, etc.), and configured GPU architectures.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_25\n\nLANGUAGE: cmake\nCODE:\n```\nif(WITH_CUDA OR HAVE_CUDA)\n  ocv_build_features_string(cuda_features\n    IF HAVE_CUFFT THEN \"CUFFT\"\n    IF HAVE_CUBLAS THEN \"CUBLAS\"\n    IF HAVE_NVCUVID THEN \"NVCUVID\"\n    IF HAVE_NVCUVENC THEN \"NVCUVENC\"\n    IF CUDA_FAST_MATH THEN \"FAST_MATH\"\n    ELSE \"no extra features\")\n  status(\"\")\n  status(\"  NVIDIA CUDA:\" HAVE_CUDA THEN \"YES (ver ${CUDA_VERSION_STRING}, ${cuda_features})\" ELSE NO)\n  if(HAVE_CUDA)\n    status(\"    NVIDIA GPU arch:\"      ${OPENCV_CUDA_ARCH_BIN})\n    status(\"    NVIDIA PTX archs:\"     ${OPENCV_CUDA_ARCH_PTX})\n  endif()\n endif()\n```\n\n----------------------------------------\n\nTITLE: SIMD Failure Handling Macro in CMake\nDESCRIPTION: Defines a macro that handles SIMD compilation failures based on whether SIMD is required. If SIMD is required, it fails with an error; otherwise, it continues with reduced performance.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/simd/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(simd_fail message)\n  if(REQUIRE_SIMD)\n    message(FATAL_ERROR \"${message}.\")\n  else()\n    message(STATUS \"${message}.  Performance will suffer.\")\n    set(WITH_SIMD 0 PARENT_SCOPE)\n  endif()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Template License Notice for Source Files (GPLv2)\nDESCRIPTION: This is a template comment block recommended for inclusion at the beginning of source files. It declares the program's name and purpose, asserts copyright, states that the program is free software distributable under the terms of the GNU GPL version 2 (or optionally, later versions), explicitly disclaims any warranty (including merchantability and fitness for a particular purpose), and directs the user to the full GPL text for details. It also includes a placeholder for obtaining a copy of the license.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ittnotify/src/ittnotify/GPL-2.0-only.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<one line to give the program's name and an idea of what it does.>\nCopyright (C) < yyyy> <name of author>\n\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.\n```\n\n----------------------------------------\n\nTITLE: Creating IPP HAL Static Library\nDESCRIPTION: Defines the static library target 'ipphal' with its source files for various IPP operations including mean, minmax, norm, cartesian/polar conversions, and transforms.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ipphal/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(ipphal STATIC\n    \"${CMAKE_CURRENT_SOURCE_DIR}/src/mean_ipp.cpp\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/src/minmax_ipp.cpp\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/src/norm_ipp.cpp\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/src/cart_polar_ipp.cpp\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/src/transforms_ipp.cpp\"\n)\n```\n\n----------------------------------------\n\nTITLE: Overriding Fatal Error Handling in JPEG Library (C)\nDESCRIPTION: Defines the `error_exit` function signature, a method within the `jpeg_error_mgr` struct. This function is called for fatal errors. It receives a pointer to the JPEG object (`j_common_ptr`). The implementation must not return control to the caller; it should typically call `output_message` to display the error stored in `cinfo->err` and then terminate (e.g., via `longjmp` or `exit`). Overriding this is common to replace the default `exit()` behavior and allow application-level cleanup using `jpeg_abort` or `jpeg_destroy`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_44\n\nLANGUAGE: c\nCODE:\n```\nerror_exit (j_common_ptr cinfo)\n```\n\n----------------------------------------\n\nTITLE: Setting JFIF Version Number in libjpeg (C)\nDESCRIPTION: C fields (`UINT8`) within the compression parameters structure (`cinfo`) specifying the major (`JFIF_major_version`) and minor (`JFIF_minor_version`) version numbers to be written into the JFIF marker if `write_JFIF_header` is TRUE. The default set by `jpeg_set_defaults()` is 1.01. This should be updated to 1.02 if using JFIF 1.02 extension markers.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_39\n\nLANGUAGE: C\nCODE:\n```\nUINT8 JFIF_major_version\n```\n\nLANGUAGE: C\nCODE:\n```\nUINT8 JFIF_minor_version\n```\n\n----------------------------------------\n\nTITLE: Generating Test Locations List in CMake for OpenCV Python Tests\nDESCRIPTION: Creates a list of relative paths to Python test locations for each module. It includes the main test directory and any module-specific test directories.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/python/test/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nfile(RELATIVE_PATH __loc_relative \"${OPENCV_PYTHON_TESTS_CONFIG_FILE_DIR}\" \"${CMAKE_CURRENT_LIST_DIR}\")\nset(opencv_tests_locations \"${__loc_relative}\")\nforeach(m ${OPENCV_PYTHON_MODULES})\n  set(__loc \"${OPENCV_MODULE_${m}_LOCATION}/misc/python/test\")\n  if(EXISTS \"${__loc}\")\n    file(RELATIVE_PATH __loc_relative \"${OPENCV_PYTHON_TESTS_CONFIG_FILE_DIR}\" \"${__loc}\")\n    list(APPEND opencv_tests_locations \"${__loc_relative}\")\n  endif()\nendforeach(m)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV 2.4 Build for Jetson TX1 using CMake (Shell)\nDESCRIPTION: Runs CMake to configure the OpenCV 2.4 build specifically for the NVIDIA Jetson TX1 platform (L4T). It sets build type to Release, enables CUDA 8.0 for architecture 5.3, enables Python 2 bindings (`BUILD_opencv_python`), TBB, and FFMPEG, disables precompiled headers, disables several other optional dependencies (PNG, TIFF, Jasper, ZLIB, Java, nonfree modules, OpenCL, OpenMP, GStreamer, VTK, 1394, OpenEXR), and specifies the test data path. Assumes execution from a build directory sibling to `opencv` and `opencv_extra`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_16\n\nLANGUAGE: Shell\nCODE:\n```\n$ cmake \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=/usr \\\n    -DBUILD_PNG=OFF \\\n    -DBUILD_TIFF=OFF \\\n    -DBUILD_TBB=OFF \\\n    -DBUILD_JPEG=OFF \\\n    -DBUILD_JASPER=OFF \\\n    -DBUILD_ZLIB=OFF \\\n    -DBUILD_EXAMPLES=ON \\\n    -DBUILD_JAVA=OFF \\\n    -DBUILD_opencv_nonfree=OFF \\\n    -DBUILD_opencv_python=ON \\\n    -DENABLE_PRECOMPILED_HEADERS=OFF \\\n    -DWITH_OPENCL=OFF \\\n    -DWITH_OPENMP=OFF \\\n    -DWITH_FFMPEG=ON \\\n    -DWITH_GSTREAMER=OFF \\\n    -DWITH_GSTREAMER_0_10=OFF \\\n    -DWITH_CUDA=ON \\\n    -DWITH_GTK=ON \\\n    -DWITH_VTK=OFF \\\n    -DWITH_TBB=ON \\\n    -DWITH_1394=OFF \\\n    -DWITH_OPENEXR=OFF \\\n    -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-8.0 \\\n    -DCUDA_ARCH_BIN=5.3 \\\n    -DCUDA_ARCH_PTX=\"\" \\\n    -DINSTALL_C_EXAMPLES=ON \\\n    -DINSTALL_TESTS=ON \\\n    -DOPENCV_TEST_DATA_PATH=../opencv_extra/testdata \\\n    ../opencv\n```\n\n----------------------------------------\n\nTITLE: Check for System and Linux Include Files in CMake\nDESCRIPTION: These snippets check the presence of various system and Linux-specific include files to determine available features using CMake commands like check_include_file. Detected features are then defined as macros for conditional compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# Check for standard/system includes\n#\ncheck_include_file(arm_acle.h  HAVE_ARM_ACLE_H)\nif(HAVE_ARM_ACLE_H)\n    add_definitions(-DHAVE_ARM_ACLE_H)\nendif()\ncheck_include_file(sys/auxv.h  HAVE_SYS_AUXV_H)\nif(HAVE_SYS_AUXV_H)\n    add_definitions(-DHAVE_SYS_AUXV_H)\nendif()\ncheck_include_file(sys/sdt.h   HAVE_SYS_SDT_H)\nif(HAVE_SYS_SDT_H)\n    add_definitions(-DHAVE_SYS_SDT_H)\nendif()\ncheck_include_file(unistd.h    HAVE_UNISTD_H)\n\n#\n# Check for Linux includes\n#\ncheck_include_file(linux/auxvec.h HAVE_LINUX_AUXVEC_H)\nif(HAVE_LINUX_AUXVEC_H)\n    add_definitions(-DHAVE_LINUX_AUXVEC_H)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Declaring CMake Project for GPU Samples\nDESCRIPTION: Declares a CMake project named 'gpu_samples'. This sets up the context for subsequent build commands related to the GPU examples.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nproject(gpu_samples)\n```\n\n----------------------------------------\n\nTITLE: Installing Sample Data Directory (OpenCV Build) in CMake\nDESCRIPTION: Conditionally installs the `data` directory (likely containing media files used by examples) into the samples source installation path (`${OPENCV_SAMPLES_SRC_INSTALL_PATH}`) as part of the `samples_data` component. This installation only occurs if `INSTALL_C_EXAMPLES` is enabled during the OpenCV build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(INSTALL_C_EXAMPLES)\n  install(DIRECTORY data DESTINATION \"${OPENCV_SAMPLES_SRC_INSTALL_PATH}\" COMPONENT samples_data)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding CoinOR CLP Linear Programming Support for OpenCV Contrib (CMake)\nDESCRIPTION: This build option adds coinor-clp support to the videostab contrib module in OpenCV. The user must install the development libraries for coinor-clp beforehand. Use -DWITH_CLP=ON to enable, required for certain stabilization features in contrib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_30\n\nLANGUAGE: cmake\nCODE:\n```\nWITH_CLP\n```\n\n----------------------------------------\n\nTITLE: Creating Build Directory\nDESCRIPTION: Creates and navigates to a build directory for compiling OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nmkdir build\ncd build\n```\n\n----------------------------------------\n\nTITLE: Generating OpenCV Tests Configuration File in CMake\nDESCRIPTION: Creates a configuration file for OpenCV tests, including installation prefix and test data path if available. The file is only written if the content has changed.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/ts/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(OPENCV_TESTS_CONFIG_FILE \"${CMAKE_BINARY_DIR}/opencv_tests_config.hpp\")\nset(OPENCV_TESTS_CONFIG_STR \"\")\nif(CMAKE_INSTALL_PREFIX)\n  set(OPENCV_TESTS_CONFIG_STR \"${OPENCV_TESTS_CONFIG_STR}\n#define OPENCV_INSTALL_PREFIX \\\"${CMAKE_INSTALL_PREFIX}\\\"\n\")\nendif()\nif(OPENCV_TEST_DATA_INSTALL_PATH)\n  set(OPENCV_TESTS_CONFIG_STR \"${OPENCV_TESTS_CONFIG_STR}\n#define OPENCV_TEST_DATA_INSTALL_PATH \\\"${OPENCV_TEST_DATA_INSTALL_PATH}\\\"\n\")\nendif()\nif(EXISTS \"${OPENCV_TESTS_CONFIG_FILE}\")\n  file(READ \"${OPENCV_TESTS_CONFIG_FILE}\" __content)\nendif()\nif(NOT OPENCV_TESTS_CONFIG_STR STREQUAL \"${__content}\")\n  file(WRITE \"${OPENCV_TESTS_CONFIG_FILE}\" \"${OPENCV_TESTS_CONFIG_STR}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Including Directories for libtiff\nDESCRIPTION: This part of the CMake script sets include directories for the libtiff library build process. The source and binary directories, along with any required zlib include directories, are included to ensure the compiler can access all necessary headers for the libtiff compilation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libtiff/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nocv_include_directories(\"${CMAKE_CURRENT_SOURCE_DIR}\" \"${CMAKE_CURRENT_BINARY_DIR}\" ${ZLIB_INCLUDE_DIRS})\n```\n\n----------------------------------------\n\nTITLE: Adding MSVC-specific Definitions for Zlib in CMake\nDESCRIPTION: Adds Microsoft Visual C++ specific preprocessor definitions to suppress deprecation warnings when building Zlib with MSVC.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n  add_definitions(-D_CRT_SECURE_NO_DEPRECATE)\n  add_definitions(-D_CRT_NONSTDC_NO_DEPRECATE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring IPP HAL Project Settings\nDESCRIPTION: Initializes core project variables including version, library names, and header file locations for the IPP HAL component.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ipphal/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nproject(ipphal)\n\nset(IPP_HAL_VERSION 0.0.1 CACHE INTERNAL \"\")\nset(IPP_HAL_LIBRARIES \"ipphal\" CACHE INTERNAL \"\")\nset(IPP_HAL_INCLUDE_DIRS \"${CMAKE_CURRENT_SOURCE_DIR}/include\" CACHE INTERNAL \"\")\nset(IPP_HAL_HEADERS\n  \"${CMAKE_CURRENT_SOURCE_DIR}/include/ipp_hal_core.hpp\"\n  CACHE INTERNAL \"\")\n```\n\n----------------------------------------\n\nTITLE: Generating Configuration Headers for OpenEXR and IlmBase\nDESCRIPTION: This code generates configuration header files for OpenEXR and IlmBase by substituting CMake variables into template files. It also sets up include paths for the OpenEXR components.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openexr/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/IlmBaseConfig.h.cmakein\"\n               \"${CMAKE_CURRENT_BINARY_DIR}/IlmBaseConfig.h\" @ONLY)\nconfigure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/OpenEXRConfig.h.cmakein\"\n               \"${CMAKE_CURRENT_BINARY_DIR}/OpenEXRConfig.h\" @ONLY)\n\nset(OPENEXR_INCLUDE_PATHS \"${CMAKE_CURRENT_SOURCE_DIR}/Half\"\n                          \"${CMAKE_CURRENT_SOURCE_DIR}/Iex\"\n                          \"${CMAKE_CURRENT_SOURCE_DIR}/IlmThread\"\n                          \"${CMAKE_CURRENT_SOURCE_DIR}/Imath\"\n                          \"${CMAKE_CURRENT_SOURCE_DIR}/IlmImf\"\n                          \"${CMAKE_CURRENT_BINARY_DIR}\")\n\nocv_include_directories(\"${CMAKE_CURRENT_BINARY_DIR}\" ${ZLIB_INCLUDE_DIRS} ${OPENEXR_INCLUDE_PATHS})\n```\n\n----------------------------------------\n\nTITLE: Implementing FAST Corner Detection Decision Tree in C++\nDESCRIPTION: This snippet shows part of the decision tree structure used in the FAST corner detection algorithm. The code compares pixel values at different offsets against brightness thresholds (c_b and cb) to determine if a point is a corner. Success branches lead to corner detection while continue statements skip to the next candidate point.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast.txt#2025-04-22_snippet_38\n\nLANGUAGE: C++\nCODE:\n```\n{} // goto success_homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  continue; // goto homogeneous;\nelse\n  if(ptr[offset8] < c_b)\n    if(ptr[offset9] < c_b)\n      if(ptr[offset10] < c_b)\n        if(ptr[offset6] < c_b)\n          {} // goto success_homogeneous;\n        else\n          if(ptr[offset11] < c_b)\n            if(ptr[offset12] < c_b)\n              if(ptr[offset13] < c_b)\n                if(ptr[offset14] < c_b)\n                  if(ptr[offset15] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  if(ptr[offset10] < c_b)\n    if(ptr[offset11] < c_b)\n      if(ptr[offset12] < c_b)\n        if(ptr[offset8] < c_b)\n          if(ptr[offset9] < c_b)\n            if(ptr[offset6] < c_b)\n              {} // goto success_homogeneous;\n            else\n              if(ptr[offset13] < c_b)\n                if(ptr[offset14] < c_b)\n                  if(ptr[offset15] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n          else\n            if(ptr[offset1] < c_b)\n              if(ptr[offset13] < c_b)\n                if(ptr[offset14] < c_b)\n                  if(ptr[offset15] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n        else\n          if(ptr[offset1] < c_b)\n            if(ptr[offset13] < c_b)\n              if(ptr[offset14] < c_b)\n                if(ptr[offset15] < c_b)\n                  {} // goto success_homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  if(ptr[offset14] < c_b)\n    if(ptr[offset15] < c_b)\n      if(ptr[offset1] < c_b)\n        if(ptr[offset3] < c_b)\n          if(ptr[offset6] < c_b)\n            {} // goto success_homogeneous;\n          else\n            if(ptr[offset13] < c_b)\n              {} // goto success_homogeneous;\n            else\n              continue; // goto homogeneous;\n        else\n          if(ptr[offset10] < c_b)\n            if(ptr[offset11] < c_b)\n              if(ptr[offset12] < c_b)\n                if(ptr[offset13] < c_b)\n                  {} // goto success_homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n      else\n        if(ptr[offset8] < c_b)\n          if(ptr[offset9] < c_b)\n            if(ptr[offset10] < c_b)\n              if(ptr[offset11] < c_b)\n                if(ptr[offset12] < c_b)\n                  if(ptr[offset13] < c_b)\n                    {} // goto success_homogeneous;\n                  else\n                    continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n    continue; // goto homogeneous;\nelse\n  if(ptr[offset12] > cb)\n    if(ptr[offset7] > cb)\n      if(ptr[offset8] > cb)\n        if(ptr[offset9] > cb)\n          if(ptr[offset10] > cb)\n            if(ptr[offset11] > cb)\n              if(ptr[offset13] > cb)\n                if(ptr[offset14] > cb)\n                  if(ptr[offset6] > cb)\n                    {} // goto success_homogeneous;\n                  else\n                    if(ptr[offset15] > cb)\n                      {} // goto success_homogeneous;\n                    else\n                      continue; // goto homogeneous;\n                else\n                  continue; // goto homogeneous;\n              else\n                continue; // goto homogeneous;\n            else\n              continue; // goto homogeneous;\n          else\n            continue; // goto homogeneous;\n        else\n          continue; // goto homogeneous;\n      else\n        continue; // goto homogeneous;\n    else\n      continue; // goto homogeneous;\n  else\n  if(ptr[offset12] < c_b)\n    if(ptr[offset13] < c_b)\n      if(ptr[offset14] < c_b)\n        if(ptr[offset15] < c_b)\n          if(ptr[offset1] < c_b)\n            if(ptr[offset3] < c_b)\n              {} // goto success_homogeneous;\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Architecture Build Options and Compiler Flags - CMake - CMake\nDESCRIPTION: This CMake snippet manages feature detection, variable settings, option toggling, and compiler flags for various architectures such as ARM, PowerPC, RISC-V, IBM Z, and x86. It employs conditional logic to enable or disable SIMD and intrinsics optimizations, adjust compiler warning levels, and manage build-time options for compatibility with compilers like GCC, Clang, Intel, and MSVC. Dependencies include the presence of specific CMake modules (e.g., cmake_dependent_option), CMake version compatibility, and support for platform-specific compiler flags and options. Key parameters define architecture detection flags (e.g., BASEARCH_ARM_FOUND), feature toggles (e.g., WITH_NEON), advanced installation settings, and mechanisms for embedding version-specific build properties. Expected inputs are CMake variables preset by the host environment or toolchain, and outputs are build system configurations optimized for the target architecture. Some features (like AVX, LTO, VFPv4) are auto-detected, and unsupported environments (e.g., old MSVC or unknown architecture) are handled with error or status messages.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n# Add multi-choice option\\nset(WITH_SANITIZER AUTO CACHE STRING \\\"Enable sanitizer support\\\")\\nset_property(CACHE WITH_SANITIZER PROPERTY STRINGS \\\"Memory\\\" \\\"Address\\\" \\\"Undefined\\\" \\\"Thread\\\")\\n\\nif(BASEARCH_ARM_FOUND)\\n    option(WITH_ACLE \\\"Build with ACLE\\\" ON)\\n    option(WITH_NEON \\\"Build with NEON intrinsics\\\" ON)\\n    cmake_dependent_option(WITH_ARMV6 \\\"Build with ARMv6 SIMD\\\" ON \\\"NOT ARCH STREQUAL \\\\\\\"aarch64\\\\\\\"\\\" OFF)\\nelif(BASEARCH_PPC_FOUND)\\n    option(WITH_ALTIVEC \\\"Build with AltiVec (VMX) optimisations for PowerPC\\\" ON)\\n    option(WITH_POWER8 \\\"Build with optimisations for POWER8\\\" ON)\\n    option(WITH_POWER9 \\\"Build with optimisations for POWER9\\\" ON)\\nelif(BASEARCH_RISCV_FOUND)\\n    option(WITH_RVV \\\"Build with RVV intrinsics\\\" ON)\\nelif(BASEARCH_S360_FOUND)\\n    option(WITH_DFLTCC_DEFLATE \\\"Build with DFLTCC intrinsics for compression on IBM Z\\\" OFF)\\n    option(WITH_DFLTCC_INFLATE \\\"Build with DFLTCC intrinsics for decompression on IBM Z\\\" OFF)\\n    option(WITH_CRC32_VX \\\"Build with vectorized CRC32 on IBM Z\\\" ON)\\nelif(BASEARCH_X86_FOUND)\\n    option(WITH_SSE2 \\\"Build with SSE2\\\" ON)\\n    cmake_dependent_option(WITH_SSSE3 \\\"Build with SSSE3\\\" ON \\\"WITH_SSE2\\\" OFF)\\n    cmake_dependent_option(WITH_SSE42 \\\"Build with SSE42\\\" ON \\\"WITH_SSSE3\\\" OFF)\\n    cmake_dependent_option(WITH_PCLMULQDQ \\\"Build with PCLMULQDQ\\\" ON \\\"WITH_SSE42\\\" OFF)\\n    cmake_dependent_option(WITH_AVX2 \\\"Build with AVX2\\\" ON \\\"WITH_SSE42\\\" OFF)\\n    cmake_dependent_option(WITH_AVX512 \\\"Build with AVX512\\\" ON \\\"WITH_AVX2\\\" OFF)\\n    cmake_dependent_option(WITH_AVX512VNNI \\\"Build with AVX512 VNNI extensions\\\" ON \\\"WITH_AVX512\\\" OFF)\\n    cmake_dependent_option(WITH_VPCLMULQDQ \\\"Build with VPCLMULQDQ\\\" ON \\\"WITH_PCLMULQDQ;WITH_AVX512\\\" OFF)\\nendif()\\n\\noption(INSTALL_UTILS \\\"Copy minigzip and minideflate during install\\\" OFF)\\n\\nset(ZLIB_BUILD_SHARED_LIBS OFF)\\nset(SKIP_INSTALL_ALL ON)\\nocv_warnings_disable(CMAKE_C_FLAGS -Wmissing-prototypes -Wmissing-declarations -Wundef -Wstrict-prototypes -Wtype-limits)\\nocv_warnings_disable(CMAKE_C_FLAGS /wd4819 /wd4244 /wd4334)\\n\\nmark_as_advanced(FORCE\\n    ZLIB_SYMBOL_PREFIX\\n    WITH_REDUCED_MEM\\n    WITH_ACLE WITH_NEON\\n    WITH_ARMV6\\n    WITH_DFLTCC_DEFLATE\\n    WITH_DFLTCC_INFLATE\\n    WITH_CRC32_VX\\n    WITH_AVX2 WITH_SSE2\\n    WITH_SSSE3 WITH_SSE42\\n    WITH_PCLMULQDQ\\n    WITH_ALTIVEC\\n    WITH_POWER8\\n    WITH_POWER9\\n    WITH_RVV\\n    WITH_INFLATE_STRICT\\n    WITH_INFLATE_ALLOW_INVALID_DIST\\n    WITH_UNALIGNED\\n    INSTALL_UTILS\\n    )\\n\\nif(ZLIB_COMPAT)\\n    add_definitions(-DZLIB_COMPAT)\\n    set(WITH_GZFILEOP ON)\\n    set(SUFFIX \\\"\\\")\\n    set(ZLIB_FULL_VERSION ${ZLIB_HEADER_VERSION}.zlib-ng)\\n    set(EXPORT_NAME ZLIB)\\nelse()\\n    set(SUFFIX \\\"-ng\\\")\\n    set(ZLIB_FULL_VERSION ${ZLIBNG_HEADER_VERSION})\\n    set(EXPORT_NAME zlib-ng)\\nendif()\\n\\nif(WITH_GZFILEOP)\\n    add_definitions(-DWITH_GZFILEOP)\\nendif()\\n\\nif(CMAKE_C_COMPILER_ID MATCHES \\\"^Intel\\\")\\n    if(CMAKE_HOST_UNIX)\\n        set(WARNFLAGS -Wall)\\n        set(WARNFLAGS_MAINTAINER -Wall -Wcheck -Wremarks)\\n        set(WARNFLAGS_DISABLE)\\n    else()\\n        set(WARNFLAGS /Wall)\\n        set(WARNFLAGS_MAINTAINER /W5)\\n        set(WARNFLAGS_DISABLE)\\n    endif()\\n    check_c_compiler_flag(-diag-disable=10441 HAVE_DIAG_10441)\\n    if(HAVE_DIAG_10441)\\n        list(APPEND WARNFLAGS_DISABLE \\\"-diag-disable=10441\\\")\\n        set(CMAKE_EXE_LINKER_FLAGS \\\"${CMAKE_EXE_LINKER_FLAGS} -diag-disable=10441\\\")\\n        set(CMAKE_SHARED_LINKER_FLAGS \\\"${CMAKE_SHARED_LINKER_FLAGS} -diag-disable=10441\\\")\\n    endif()\\nelif(MSVC)\\n    # Minimum supported MSVC version is 1800 = Visual Studio 12.0/2013\\n    # See also https://cmake.org/cmake/help/latest/variable/MSVC_VERSION.html\\n    if(MSVC_VERSION VERSION_LESS 1800)\\n        message(SEND_ERROR \\\"Unsupported Visual Studio compiler version (requires 2013 or later).\\\")\\n    endif()\\n    # TODO. ICC can be used through MSVC. I'm not sure if we'd ever see that combination\\n    # (who'd use cmake from an IDE...) but checking for ICC before checking for MSVC should\\n    # avoid mistakes.\\n    # /Oi ?\\n    set(WARNFLAGS /W3)\\n    set(WARNFLAGS_MAINTAINER /W4)\\n    set(WARNFLAGS_DISABLE)\\n    if(BASEARCH_ARM_FOUND)\\n        add_definitions(-D_ARM_WINAPI_PARTITION_DESKTOP_SDK_AVAILABLE)\\n        if(NOT \\\"${ARCH}\\\" MATCHES \\\"aarch64\\\")\\n            set(NEONFLAG \\\"/arch:VFPv4\\\")\\n        endif()\\n    endif()\\nelif(CMAKE_C_COMPILER_ID MATCHES \\\"GNU\\\" OR CMAKE_C_COMPILER_ID MATCHES \\\"Clang\\\")\\n    # Enable warnings in GCC and Clang\\n    set(WARNFLAGS -Wall)\\n    set(WARNFLAGS_MAINTAINER -Wextra)\\n    set(WARNFLAGS_DISABLE)\\n    # Check whether -fno-lto is available\\n    set(CMAKE_REQUIRED_FLAGS \\\"-fno-lto\\\")\\n    check_c_source_compiles(\\n        \\\"int main() { return 0; }\\\"\\n        FNO_LTO_AVAILABLE FAIL_REGEX \\\"not supported\\\")\\n    set(CMAKE_REQUIRED_FLAGS)\\n    if(FNO_LTO_AVAILABLE)\\n        set(ZNOLTOFLAG \\\"-fno-lto\\\")\\n    endif()\\n    if(NOT WITH_NATIVE_INSTRUCTIONS)\\n        if(BASEARCH_ARM_FOUND)\\n            if(\\\"${ARCH}\\\" MATCHES \\\"arm\\\" AND NOT CMAKE_C_FLAGS MATCHES \\\"-mfloat-abi\\\")\\n                # Auto-detect support for ARM floating point ABI\\n                check_include_file(features.h HAVE_FEATURES_H)\\n                if(HAVE_FEATURES_H)\\n                    set(CMAKE_REQUIRED_FLAGS -mfloat-abi=softfp)\\n                    check_c_source_compiles(\\n                        \\\"#include <features.h>\\n                        int main() { return 0; }\\\"\\n                        HAVE_FLOATABI_SOFTFP)\\n                    if(HAVE_FLOATABI_SOFTFP)\\n                        set(FLOATABI -mfloat-abi=softfp)\\n                    else()\\n                        set(CMAKE_REQUIRED_FLAGS -mfloat-abi=hard)\\n                        check_c_source_compiles(\\n                            \\\"#include <features.h>\\n                            int main() { return 0; }\\\"\\n                            HAVE_FLOATABI_HARD)\\n                        if(HAVE_FLOATABI_HARD)\\n                            set(FLOATABI -mfloat-abi=hard)\\n                        endif()\\n                    endif()\\n                    set(CMAKE_REQUIRED_FLAGS)\\n                endif()\\n                if(FLOATABI)\\n                    message(STATUS \\\"ARM floating point arch: ${FLOATABI}\\\")\\n                    add_compile_options(${FLOATABI})\\n                else()\\n                    message(STATUS \\\"ARM floating point arch not auto-detected\\\")\\n                endif()\\n            endif()\\n        endif()\\n        # Disable LTO unless Native Instructions are enabled\\n        if(FNO_LTO_AVAILABLE)\\n            set(NOLTOFLAG ${ZNOLTOFLAG})\\n        endif()\\n    endif()\\n    if(MINGW)\\n        # Add `-Wno-pedantic-ms-format` only if the toolchain supports it\\n        check_c_compiler_flag(-Wno-pedantic-ms-format HAVE_NO_PEDANTIC_MS_FORMAT)\\n        if(HAVE_NO_PEDANTIC_MS_FORMAT)\\n            list(APPEND WARNFLAGS_DISABLE -Wno-pedantic-ms-format)\\n        endif()\\n    endif()\\nendif()\\n\\n# Set native march/mcpu\\nif(WITH_NATIVE_INSTRUCTIONS)\\n    if(NATIVE_ARCH_OVERRIDE)\\n        message(STATUS \\\"WARNING: WITH_NATIVE_INSTRUCTIONS enabled, but running with NATIVE_ARCH_OVERRIDE: ${NATIVE_ARCH_OVERRIDE}\\\")\\n        set(NATIVEFLAG \\\"${NATIVE_ARCH_OVERRIDE}\\\")\\n    else()\\n        if(CMAKE_C_COMPILER_ID MATCHES \\\"GNU\\\" OR CMAKE_C_COMPILER_ID MATCHES \\\"Clang\\\")\\n            check_c_compiler_flag(-march=native HAVE_MARCH_NATIVE)\\n            if(HAVE_MARCH_NATIVE)\\n                set(NATIVEFLAG \\\"-march=native\\\")\\n            else()\\n                check_c_compiler_flag(-mcpu=native HAVE_MCPU_NATIVE)\\n                if(HAVE_MCPU_NATIVE)\\n                    set(NATIVEFLAG \\\"-mcpu=native\\\")\\n                endif()\\n            endif()\\n            # Fall through\\n        endif()\\n    endif()\\n    if(NATIVEFLAG)\\n        # Apply flags to all source files and compilation checks\\n        if(WIN32)\\n            separate_arguments(NATIVEOPTIONS WINDOWS_COMMAND \\\"${NATIVEFLAG}\\\")\\n        else()\\n            separate_arguments(NATIVEOPTIONS UNIX_COMMAND \\\"${NATIVEFLAG}\\\")\\n        endif()\\n        add_compile_options(${NATIVEOPTIONS})\\n        set(WITH_RUNTIME_CPU_DETECTION OFF)\\n    else()\\n        message(STATUS \\\"Ignoring WITH_NATIVE_INSTRUCTIONS; not implemented yet on this configuration\\\")\\n        set(WITH_NATIVE_INSTRUCTIONS OFF)\\n    endif()\\nendif()\\n\\n# Compile without functable or CPU detection\\nif(NOT WITH_RUNTIME_CPU_DETECTION)\\n    if(MSVC AND BASEARCH_X86_FOUND)\\n        message(STATUS \\\"WARNING: Microsoft Visual Studio does not support compile time detection of CPU features for \\\"/arch\\\" before \\\"AVX\\\"\\\")\\n        # Workaround for MSVC. By default MSVC does not define the __SSE*__ macros.\\n        # Fix it if AVX is enabled.\\n        set(CMAKE_REQUIRED_FLAGS \\\"${NATIVEFLAG}\\\")\\n        check_c_source_compiles(\\n            \\\"#ifndef __AVX__\\n            #  error \\\\\\\"AVX is not enabled.\\\\\\\"\\n            #endif\\n            int main(void) { return 0; }\\\"\\n            MSVC_IS_ENABLED_AVX\\n        )\\n        set(CMAKE_REQUIRED_FLAGS)\\n        if(MSVC_IS_ENABLED_AVX)\\n            add_definitions(\\n                -D__SSE__=1\\n                -D__SSE2__=1\\n                -D__SSE3__=1\\n                -D__SSSE3__=1\\n                -D__SSE4_1__=1\\n                -D__SSE4_2__=1\\n                -D__PCLMUL__=1\\n            )\\n        endif()\\n    endif()\\n    add_definitions(-DDISABLE_RUNTIME_CPU_DETECTION)\\nendif()\\n\\n# Force disable LTO if WITH_NATIVE_INSTRUCTIONS is not active\\nif(NOT WITH_NATIVE_INSTRUCTIONS)\\n    set(CMAKE_INTERPROCEDURAL_OPTIMIZATION OFF)\\n    foreach(_cfg_name IN LISTS CMAKE_CONFIGURATION_TYPES)\\n        string(TOUPPER \\\"${_cfg_name}\\\" _cfg_name_uc)\\n        set(CMAKE_INTERPROCEDURAL_OPTIMIZATION_${_cfg_name_uc} OFF)\\n    endforeach()\\nendif()\\n\\n# Set architecture alignment requirements\\nif(NOT WITH_UNALIGNED)\\n    add_definitions(-DNO_UNALIGNED)\\n    message(STATUS \\\"Unaligned reads manually disabled\\\")\\nendif()\\n\\n# Apply warning compiler flags\\nif(WITH_MAINTAINER_WARNINGS)\\n    add_compile_options(${WARNFLAGS} ${WARNFLAGS_MAINTAINER} ${WARNFLAGS_DISABLE})\\nelse()\\n    add_compile_options(${WARNFLAGS} ${WARNFLAGS_DISABLE})\\nendif()\\n\\n# Set code coverage compiler flags\\nif(WITH_CODE_COVERAGE)\\n    add_code_coverage()\\nendif()\\n\n```\n\n----------------------------------------\n\nTITLE: Checking for Large File Support in Zlib CMake Configuration\nDESCRIPTION: Checks if the system supports off64_t type for large file operations and enables large file support if available. This allows Zlib to handle files larger than 2GB.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncheck_type_size(off64_t OFF64_T)\nif(HAVE_OFF64_T)\n  add_definitions(-D_LARGEFILE64_SOURCE=1)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Android Test Resource Directory in CMake\nDESCRIPTION: Specifies the resource directory for Android tests.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/android_test/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(ANDROID_TESTS_RES_DIR \"'${OpenCV_SOURCE_DIR}/modules/java/test/common_test/res'\" CACHE INTERNAL \"\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Android-Specific Settings for OpenCV Java Library in CMake\nDESCRIPTION: Sets up Android-specific linking and post-build commands for the OpenCV Java library, including stripping debug information.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jni/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(ANDROID)\n  ocv_target_link_libraries(${the_module} PUBLIC    jnigraphics  # for Mat <=> Bitmap converters\n                                          INTERFACE jnigraphics\n  )\n  ocv_target_link_libraries(${the_module} PUBLIC    log dl z\n                                          INTERFACE log dl z\n  )\n\n  # force strip library after the build command\n  # because samples and tests will make a copy of the library before install\n  if(NOT BUILD_WITH_DEBUG_INFO AND NOT CMAKE_BUILD_TYPE MATCHES \"Debug\")\n    add_custom_command(TARGET ${the_module} POST_BUILD COMMAND ${CMAKE_STRIP} --strip-unneeded \"$<TARGET_FILE:${the_module}>\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Previous Objective-C Bindings Artifacts in CMake\nDESCRIPTION: Removes any previously generated Objective-C bindings for macOS, iOS, and visionOS platforms to ensure a clean build. Forces re-running the generator by removing dependency helper files.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/objc/generator/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(REMOVE_RECURSE \"${OPENCV_OBJC_BINDINGS_DIR}/osx\")\nfile(REMOVE \"${OPENCV_DEPHELPER}/gen_opencv_objc_source_osx\")  # force re-run after CMake\nfile(REMOVE_RECURSE \"${OPENCV_OBJC_BINDINGS_DIR}/ios\")\nfile(REMOVE \"${OPENCV_DEPHELPER}/gen_opencv_objc_source_ios\")  # force re-run after CMake\nfile(REMOVE_RECURSE \"${OPENCV_OBJC_BINDINGS_DIR}/visionos\")\nfile(REMOVE \"${OPENCV_DEPHELPER}/gen_opencv_objc_source_visionos\")  # force re-run after CMake\n```\n\n----------------------------------------\n\nTITLE: Defining Library Target for libwebp in CMake\nDESCRIPTION: This snippet defines the library target within CMake and adjusts its configuration based on threading support and platform-specific linker requirements. It also sets various properties related to warnings, output directories, and installation paths. The snippet uses OpenCV-specific utility functions for warning management and installation tasks.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libwebp/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT OPENCV_DISABLE_THREAD_SUPPORT)\n  add_definitions(-DWEBP_USE_THREAD)\nendif()\n\nadd_library(${WEBP_LIBRARY} STATIC ${OPENCV_3RDPARTY_EXCLUDE_FROM_ALL} ${lib_srcs} ${lib_hdrs})\nif(ANDROID)\n  target_link_libraries(${WEBP_LIBRARY} ${CPUFEATURES_LIBRARIES})\nendif()\n\nocv_warnings_disable(CMAKE_C_FLAGS -Wunused-variable -Wunused-function -Wshadow -Wmaybe-uninitialized\n    -Wmissing-prototypes  # clang\n    -Wmissing-declarations # gcc\n    -Wimplicit-fallthrough\n    -Wunused-but-set-variable # clang15\n)\nocv_warnings_disable(CMAKE_C_FLAGS /wd4244 /wd4267) # vs2005\n\nset_target_properties(${WEBP_LIBRARY}\n  PROPERTIES OUTPUT_NAME ${WEBP_LIBRARY}\n  DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n  COMPILE_PDB_NAME ${WEBP_LIBRARY}\n  COMPILE_PDB_NAME_DEBUG \"${WEBP_LIBRARY}${OPENCV_DEBUG_POSTFIX}\"\n  ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n  )\n\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(${WEBP_LIBRARY} PROPERTIES FOLDER \"3rdparty\")\nendif()\n\nif(NOT BUILD_SHARED_LIBS)\n  ocv_install_target(${WEBP_LIBRARY} EXPORT OpenCVModules ARCHIVE DESTINATION ${OPENCV_3P_LIB_INSTALL_PATH} COMPONENT dev OPTIONAL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Initializing Doxygen Blacklist and Unsetting Variables in CMake\nDESCRIPTION: Sets a CMake variable `blacklist` using the value of `DOXYGEN_BLACKLIST` and appends specific modules ('ts', 'world') that should be excluded from documentation. It also unsets CMake variables related to tutorial roots to ensure they are initialized correctly later.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\n# not documented modules list\nset(blacklist \"${DOXYGEN_BLACKLIST}\")\nlist(APPEND blacklist \"ts\" \"world\")\nunset(CMAKE_DOXYGEN_TUTORIAL_CONTRIB_ROOT)\nunset(CMAKE_DOXYGEN_TUTORIAL_JS_ROOT)\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Decision Tree in C++\nDESCRIPTION: A portion of the FAST corner detection algorithm that evaluates whether a pixel is a corner based on intensity comparisons with neighboring pixels. The code uses an efficient nested if-else structure with goto statements to branch to 'is_a_corner' or 'is_not_a_corner' labels based on intensity threshold conditions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_41\n\nLANGUAGE: C++\nCODE:\n```\n                                  goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset6] < c_b)\n                            goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset8] > cb)\n                                if(ptr[offset4] > cb)\n                                  if(ptr[offset3] > cb)\n                                    goto is_a_corner;\n                                  else\n                                    if(ptr[offset10] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                else\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  if(ptr[offset2] > cb)\n                    if(ptr[offset9] < c_b)\n                      if(ptr[offset1] > cb)\n                        if(ptr[offset6] < c_b)\n                          goto is_not_a_corner;\n                        else\n                          if(ptr[offset6] > cb)\n                            if(ptr[offset3] > cb)\n                              if(ptr[offset4] > cb)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset1] < c_b)\n                          if(ptr[offset6] < c_b)\n                            goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset3] > cb)\n                                if(ptr[offset4] > cb)\n                                  if(ptr[offset8] > cb)\n                                    goto is_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset6] < c_b)\n                            goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset3] > cb)\n                                if(ptr[offset4] > cb)\n                                  if(ptr[offset8] > cb)\n                                    goto is_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                    else\n                      if(ptr[offset9] > cb)\n                        if(ptr[offset1] < c_b)\n                          if(ptr[offset6] < c_b)\n                            goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset8] > cb)\n                                if(ptr[offset4] > cb)\n                                  if(ptr[offset3] > cb)\n                                    goto is_a_corner;\n                                  else\n                                    if(ptr[offset10] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                else\n                                  if(ptr[offset10] > cb)\n                                    if(ptr[offset11] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset1] > cb)\n                            if(ptr[offset6] < c_b)\n                              goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset4] > cb)\n                                  if(ptr[offset3] > cb)\n                                    goto is_a_corner;\n                                  else\n                                    if(ptr[offset8] > cb)\n                                      if(ptr[offset10] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                else\n                                  if(ptr[offset8] > cb)\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] < c_b)\n                              goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset8] > cb)\n                                  if(ptr[offset4] > cb)\n                                    if(ptr[offset3] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      if(ptr[offset10] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                  else\n                                    if(ptr[offset10] > cb)\n                                      if(ptr[offset11] > cb)\n                                        goto is_a_corner;\n                                      else\n                                        goto is_not_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                      else\n                        if(ptr[offset1] > cb)\n                          if(ptr[offset6] < c_b)\n                            goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset3] > cb)\n                                if(ptr[offset4] > cb)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset1] < c_b)\n                            if(ptr[offset6] < c_b)\n                              goto is_not_a_corner;\n                            else\n                              if(ptr[offset6] > cb)\n                                if(ptr[offset3] > cb)\n                                  if(ptr[offset4] > cb)\n                                    if(ptr[offset8] > cb)\n                                      goto is_a_corner;\n                                    else\n                                      goto is_not_a_corner;\n                                  else\n                                    goto is_not_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            if(ptr[offset6] < c_b)\n                              goto is_not_a_corner;\n                            else\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific MSVC Compiler Warnings in CMake\nDESCRIPTION: Conditionally disables specific Microsoft Visual C++ (MSVC) compiler warnings based on the MSVC version and target architecture. It disables C4503 (decorated name length exceeded) and C4996 (deprecated code) for older MSVC versions, and C4702 (unreachable code) for MSVC 2015/2017 or when targeting ARM/AARCH64.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC)\n  if(MSVC_VERSION LESS 1910)\n    # Disable obsolete warning C4503 popping up on MSVC << 15 2017\n    # https://docs.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warning-level-1-c4503?view=vs-2019\n    # and IE deprecated code warning C4996\n    ocv_warnings_disable(CMAKE_CXX_FLAGS /wd4503 /wd4996)\n  endif()\n  if((MSVC_VERSION LESS 1920) OR ARM OR AARCH64) # MSVS 2015/2017 on x86 and ARM\n    ocv_warnings_disable(CMAKE_CXX_FLAGS /wd4702)  # 'unreachable code'\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Performing Post-Status Verification and Cleanup in CMake\nDESCRIPTION: This CMake code conditionally runs configuration verification using the custom function `ocv_verify_config` if the `ENABLE_CONFIG_VERIFICATION` variable is set. It also executes a specific CUDA clean target (`CUDA_BUILD_CLEAN_TARGET`) if CUDA (`HAVE_CUDA`) is enabled and the target command exists. Finally, it runs a `POST_FINALIZE` hook using `ocv_cmake_hook` for any necessary post-finalization actions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_32\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_CONFIG_VERIFICATION)\n  ocv_verify_config()\nendif()\n\nif(HAVE_CUDA AND COMMAND CUDA_BUILD_CLEAN_TARGET)\n  CUDA_BUILD_CLEAN_TARGET()\nendif()\n\nocv_cmake_hook(POST_FINALIZE)\n```\n\n----------------------------------------\n\nTITLE: Implementing Corner Detection Logic in C++\nDESCRIPTION: This snippet implements the core logic for corner detection using pixel value comparisons. It uses nested if-else statements to check pixel values at different offsets and determine if a point is a corner.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_30\n\nLANGUAGE: C++\nCODE:\n```\nelse\n  if(ptr[offset8] < c_b)\n    if(ptr[offset10] < c_b)\n      if(ptr[offset11] < c_b)\n        goto is_a_corner;\n      else\n        goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  if(ptr[offset11] < c_b)\n    if(ptr[offset3] < c_b)\n      if(ptr[offset4] < c_b)\n        goto is_a_corner;\n      else\n        if(ptr[offset10] < c_b)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n    else\n      if(ptr[offset8] < c_b)\n        if(ptr[offset10] < c_b)\n          goto is_a_corner;\n        else\n          goto is_not_a_corner;\n      else\n        goto is_not_a_corner;\n  else\n    goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  goto is_not_a_corner;\nelse\n  if(ptr[offset2] > cb)\n    goto is_not_a_corner;\n  else\n    if(ptr[offset2] < c_b)\n      if(ptr[offset7] > cb)\n        if(ptr[offset1] > cb)\n          goto is_not_a_corner;\n        else\n          if(ptr[offset1] < c_b)\n            if(ptr[offset6] < c_b)\n              if(ptr[offset3] < c_b)\n                if(ptr[offset4] < c_b)\n                  goto is_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              if(ptr[offset6] > cb)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    if(ptr[offset11] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n          else\n            goto is_not_a_corner;\n      else\n        if(ptr[offset7] < c_b)\n          if(ptr[offset1] > cb)\n            if(ptr[offset6] > cb)\n              goto is_not_a_corner;\n            else\n              if(ptr[offset6] < c_b)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    if(ptr[offset8] < c_b)\n                      goto is_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n            if(ptr[offset1] < c_b)\n              if(ptr[offset6] < c_b)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                if(ptr[offset6] > cb)\n                  if(ptr[offset3] < c_b)\n                    if(ptr[offset4] < c_b)\n                      if(ptr[offset11] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  if(ptr[offset3] < c_b)\n                    if(ptr[offset4] < c_b)\n                      if(ptr[offset11] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n            else\n              if(ptr[offset6] > cb)\n                goto is_not_a_corner;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset3] < c_b)\n                    if(ptr[offset4] < c_b)\n                      if(ptr[offset8] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n        else\n          if(ptr[offset1] > cb)\n            goto is_not_a_corner;\n          else\n            if(ptr[offset1] < c_b)\n              if(ptr[offset6] < c_b)\n                if(ptr[offset3] < c_b)\n                  if(ptr[offset4] < c_b)\n                    goto is_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                if(ptr[offset6] > cb)\n                  if(ptr[offset3] < c_b)\n                    if(ptr[offset4] < c_b)\n                      if(ptr[offset11] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  if(ptr[offset3] < c_b)\n                    if(ptr[offset4] < c_b)\n                      if(ptr[offset11] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n    else\n      goto is_not_a_corner;\nelse\n  if(ptr[offset5] > cb)\n    if(ptr[offset2] > cb)\n      if(ptr[offset7] < c_b)\n        if(ptr[offset9] > cb)\n          goto is_not_a_corner;\n        else\n          if(ptr[offset9] < c_b)\n            if(ptr[offset1] > cb)\n              if(ptr[offset6] > cb)\n                goto is_not_a_corner;\n              else\n                if(ptr[offset6] < c_b)\n                  if(ptr[offset8] < c_b)\n                    if(ptr[offset10] < c_b)\n                      if(ptr[offset11] < c_b)\n                        goto is_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n            else\n```\n\n----------------------------------------\n\nTITLE: Configuring x86 SSE4.2 Optimizations for ZLIB in CMake\nDESCRIPTION: Checks if SSE4.2 optimization is enabled (WITH_SSE42), if intrinsics are available (HAVE_SSE42_INTRIN), and if SSSE3 is also enabled. If conditions are met, it adds the DX86_SSE42 definition, appends the SSE4.2-specific adler32 source file to ZLIB_ARCH_SRCS, adds feature information, and sets compile flags (SSE42FLAG, NOLTOFLAG). Otherwise, it disables SSE4.2 support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_28\n\nLANGUAGE: cmake\nCODE:\n```\n        if(WITH_SSE42)\n            check_sse42_intrinsics()\n            if(HAVE_SSE42_INTRIN AND WITH_SSSE3)\n                add_definitions(-DX86_SSE42)\n                set(SSE42_SRCS ${ARCHDIR}/adler32_sse42.c)\n                add_feature_info(SSE42_CRC 1 \"Support SSE4.2 optimized adler32 hash generation, using \\\"${SSE42FLAG}\\\"\")\n                list(APPEND ZLIB_ARCH_SRCS ${SSE42_SRCS})\n                set_property(SOURCE ${SSE42_SRCS} PROPERTY COMPILE_FLAGS \"${SSE42FLAG} ${NOLTOFLAG}\")\n            else()\n                set(WITH_SSE42 OFF)\n            endif()\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Virtual Environment for OpenCV DNN - Console\nDESCRIPTION: This snippet creates and activates a Python 3.7+ virtual environment required for OpenCV DNN experiments. The first command makes a new environment, and the second one activates it. Ensure that 'virtualenv' is installed beforehand.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nvirtualenv -p /usr/bin/python3.7 <env_dir_path>\\nsource <env_dir_path>/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV Java JAR and Configuring Documentation\nDESCRIPTION: Sets up installation of the OpenCV Java JAR file and configures the documentation generation process based on the build type (Ant or Java).\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/jar/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(FILES ${OPENCV_JAR_FILE} OPTIONAL DESTINATION ${OPENCV_JAR_INSTALL_PATH} COMPONENT java)\n\nadd_dependencies(${the_module} ${the_module}_jar)\n\nif(BUILD_DOCS)\n  if(OPENCV_JAVA_SDK_BUILD_TYPE STREQUAL \"ANT\")\n    add_custom_command(OUTPUT \"${OPENCV_DEPHELPER}/${the_module}doc\"\n      COMMAND ${ANT_EXECUTABLE} -noinput -k javadoc\n      COMMAND ${CMAKE_COMMAND} -E touch \"${OPENCV_DEPHELPER}/${the_module}doc\"\n      WORKING_DIRECTORY \"${OPENCV_JAVA_DIR}\"\n      DEPENDS ${depends}\n      COMMENT \"Generating Javadoc\"\n    )\n    add_custom_target(${the_module}doc DEPENDS \"${OPENCV_DEPHELPER}/${the_module}doc\")\n\n    install(DIRECTORY ${OpenCV_BINARY_DIR}/doc/doxygen/html/javadoc\n      DESTINATION \"${OPENCV_DOC_INSTALL_PATH}/html\"\n      COMPONENT \"docs\" OPTIONAL\n      ${compatible_MESSAGE_NEVER}\n    )\n  elseif(OPENCV_JAVA_SDK_BUILD_TYPE STREQUAL \"JAVA\")\n    set(Java_JAVADOC_EXECUTABLE ${Java_JAVADOC_EXECUTABLE} -encoding utf-8)\n\n    create_javadoc(${the_module}\n      FILES \"@${OPENCV_JAVA_DIR}/java_sources\"\n      SOURCEPATH \"${OPENCV_JAVA_DIR}/java\"\n      INSTALLPATH \"${OPENCV_JAVADOC_DESTINATION}\"\n      WINDOWTITLE \"OpenCV ${OPENCV_VERSION_PLAIN} Java documentation\"\n      DOCTITLE \"OpenCV Java documentation (${OPENCV_VERSION})\"\n      VERSION TRUE\n    )\n    add_dependencies(${the_module}_javadoc ${the_module}_jar_sources)\n    add_custom_target(${the_module}doc DEPENDS ${the_module}_javadoc)\n\n    install(DIRECTORY ${OpenCV_BINARY_DIR}/doc/doxygen/html/javadoc/${the_module}/\n      DESTINATION \"${OPENCV_DOC_INSTALL_PATH}/html/javadoc\"\n      COMPONENT \"docs\" OPTIONAL\n      ${compatible_MESSAGE_NEVER}\n    )\n  else()\n    ocv_assert(0)\n  endif()\n\n  set(CMAKE_DOXYGEN_JAVADOC_NODE\n    \"<tab type=\\\"user\\\" url=\\\"./javadoc/index.html\\\" title=\\\"Java documentation\\\"/>\"\n    CACHE INTERNAL \"Link to the Java documentation\") # set to the cache to make it global\n  add_custom_target(doxygen_javadoc DEPENDS ${the_module}doc)\n  add_dependencies(opencv_docs ${the_module}doc)\nelse()\n  unset(CMAKE_DOXYGEN_JAVADOC_NODE CACHE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configure Debugging and Optimization Options in CMake\nDESCRIPTION: The snippet configures compile options based on the build type and compiler (e.g., adding ZLIB_DEBUG for debug builds). It also distinguishes settings for MSVC and x86 architecture, adding specific options to enforce SSE2 support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nadd_compile_options($<$<CONFIG:Debug>:-DZLIB_DEBUG>)\n\nif(MSVC)\n    set(CMAKE_DEBUG_POSTFIX \"d\")\n    add_definitions(-D_CRT_SECURE_NO_DEPRECATE)\n    add_definitions(-D_CRT_NONSTDC_NO_DEPRECATE)\nendif()\n\nif(BASEARCH_X86_FOUND)\n    # FORCE_SSE2 option will only be shown if HAVE_SSE2_INTRIN is true\n    if(\"${ARCH}\" MATCHES \"i[3-6]86\")\n        cmake_dependent_option(FORCE_SSE2 \"Always assume CPU is SSE2 capable\" OFF \"HAVE_SSE2_INTRIN\" OFF)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding ZLIB Dependency for Apple Platforms in CMake\nDESCRIPTION: Checks if the target platform is Apple (`APPLE` variable is true). If so, it adds the ZLIB include directories (obtained from the `ZLIB_INCLUDE_DIRS` variable) using `ocv_include_directories` and appends the ZLIB libraries (from `ZLIB_LIBRARIES`) to the `HIGHGUI_LIBRARIES` list. This ensures ZLIB is correctly linked when building highgui on macOS or iOS.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(APPLE)\n  ocv_include_directories(${ZLIB_INCLUDE_DIRS})\n  list(APPEND HIGHGUI_LIBRARIES ${ZLIB_LIBRARIES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: XML Output of Camera Calibration Results\nDESCRIPTION: This XML snippet shows the structure of the output file containing camera matrix and distortion coefficients after calibration in OpenCV.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/calib3d/camera_calibration/camera_calibration.markdown#2025-04-22_snippet_13\n\nLANGUAGE: xml\nCODE:\n```\n<camera_matrix type_id=\"opencv-matrix\">\n<rows>3</rows>\n<cols>3</cols>\n<dt>d</dt>\n<data>\n 6.5746697944293521e+002 0. 3.1950000000000000e+002 0.\n 6.5746697944293521e+002 2.3950000000000000e+002 0. 0. 1.</data></camera_matrix>\n<distortion_coefficients type_id=\"opencv-matrix\">\n<rows>5</rows>\n<cols>1</cols>\n<dt>d</dt>\n<data>\n -4.1802327176423804e-001 5.0715244063187526e-001 0. 0.\n -5.7843597214487474e-001</data></distortion_coefficients>\n```\n\n----------------------------------------\n\nTITLE: Generating OpenCV for Windows Store 8.1 x86 using CMake\nDESCRIPTION: Invokes CMake directly to generate Visual Studio 2013 project files for OpenCV targeting Windows Store 8.1 on the x86 architecture. Specifies the generator, system name (WindowsStore), and system version.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Visual Studio 12 2013\" -DCMAKE_SYSTEM_NAME=WindowsStore -DCMAKE_SYSTEM_VERSION=8.1 <path-to-source>\n```\n\n----------------------------------------\n\nTITLE: FAST Corner Detection Decision Tree Implementation in C++\nDESCRIPTION: This snippet is part of a decision tree that examines pixel values (ptr[offset]) against brightness thresholds (c_b and cb) to determine if a point in an image is a corner. The algorithm branches extensively based on comparisons of pixel intensities at various offsets, leading to either 'is_a_corner' or 'is_not_a_corner' labels.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/features2d/doc/agast_score.txt#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n              if(ptr[offset11] < c_b)\n                if(ptr[offset7] < c_b)\n                  if(ptr[offset8] < c_b)\n                    if(ptr[offset9] < c_b)\n                      if(ptr[offset10] < c_b)\n                        if(ptr[offset12] < c_b)\n                          if(ptr[offset13] < c_b)\n                            if(ptr[offset6] < c_b)\n                              if(ptr[offset5] < c_b)\n                                goto is_a_corner;\n                              else\n                                if(ptr[offset14] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                            else\n                              if(ptr[offset14] < c_b)\n                                if(ptr[offset15] < c_b)\n                                  goto is_a_corner;\n                                else\n                                  goto is_not_a_corner;\n                              else\n                                goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                  else\n                    goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n          else\n          if(ptr[offset2] < c_b)\n            if(ptr[offset9] > cb)\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset12] > cb)\n                      if(ptr[offset13] > cb)\n                        if(ptr[offset14] > cb)\n                          if(ptr[offset15] > cb)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset7] > cb)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset5] > cb)\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset7] > cb)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset4] > cb)\n                          if(ptr[offset5] > cb)\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset7] > cb)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      if(ptr[offset3] > cb)\n                        if(ptr[offset4] > cb)\n                          if(ptr[offset5] > cb)\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset7] > cb)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset1] > cb)\n                      if(ptr[offset12] > cb)\n                        if(ptr[offset13] > cb)\n                          if(ptr[offset14] > cb)\n                            if(ptr[offset15] > cb)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n            if(ptr[offset9] < c_b)\n              if(ptr[offset7] < c_b)\n                if(ptr[offset8] < c_b)\n                  if(ptr[offset6] < c_b)\n                    if(ptr[offset5] < c_b)\n                      if(ptr[offset4] < c_b)\n                        if(ptr[offset3] < c_b)\n                          if(ptr[offset1] < c_b)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset10] < c_b)\n                              goto is_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset10] < c_b)\n                            if(ptr[offset11] < c_b)\n                              if(ptr[offset12] < c_b)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset10] < c_b)\n                          if(ptr[offset11] < c_b)\n                            if(ptr[offset12] < c_b)\n                              if(ptr[offset13] < c_b)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                    else\n                      if(ptr[offset10] < c_b)\n                        if(ptr[offset11] < c_b)\n                          if(ptr[offset12] < c_b)\n                            if(ptr[offset13] < c_b)\n                              if(ptr[offset14] < c_b)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                  else\n                    if(ptr[offset10] < c_b)\n                      if(ptr[offset11] < c_b)\n                        if(ptr[offset12] < c_b)\n                          if(ptr[offset13] < c_b)\n                            if(ptr[offset14] < c_b)\n                              if(ptr[offset15] < c_b)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                        else\n                          goto is_not_a_corner;\n                      else\n                        goto is_not_a_corner;\n                    else\n                      goto is_not_a_corner;\n                else\n                  goto is_not_a_corner;\n              else\n                goto is_not_a_corner;\n            else\n              goto is_not_a_corner;\n          else\n            if(ptr[offset9] > cb)\n              if(ptr[offset10] > cb)\n                if(ptr[offset11] > cb)\n                  if(ptr[offset8] > cb)\n                    if(ptr[offset12] > cb)\n                      if(ptr[offset13] > cb)\n                        if(ptr[offset14] > cb)\n                          if(ptr[offset15] > cb)\n                            goto is_a_corner;\n                          else\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset7] > cb)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                        else\n                          if(ptr[offset5] > cb)\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset7] > cb)\n                                goto is_a_corner;\n                              else\n                                goto is_not_a_corner;\n                            else\n                              goto is_not_a_corner;\n                          else\n                            goto is_not_a_corner;\n                      else\n                        if(ptr[offset4] > cb)\n                          if(ptr[offset5] > cb)\n                            if(ptr[offset6] > cb)\n                              if(ptr[offset7] > cb)\n```\n\n----------------------------------------\n\nTITLE: Build Configuration and Options\nDESCRIPTION: Defines build type settings and numerous compilation options for customizing the build process with various features and optimizations\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nget_property(GENERATOR_IS_MULTI_CONFIG GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG)\nif(NOT GENERATOR_IS_MULTI_CONFIG)\n    if(NOT CMAKE_BUILD_TYPE)\n        set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING\n            \"Choose the type of build, standard options are: Debug Release RelWithDebInfo MinSizeRel.\"\n            FORCE)\n        add_feature_info(CMAKE_BUILD_TYPE 1 \"Build type: ${CMAKE_BUILD_TYPE} (default)\")\n    else()\n        add_feature_info(CMAKE_BUILD_TYPE 1 \"Build type: ${CMAKE_BUILD_TYPE} (selected)\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Checking Host Architecture with DPKG (Bash)\nDESCRIPTION: Displays the native architecture of the host system as recognized by `dpkg`. The example output `amd64` indicates a standard 64-bit x86 system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nsudo dpkg --print-architecture\namd64\n```\n\n----------------------------------------\n\nTITLE: Configuring x86 VPCLMULQDQ Optimization for ZLIB in CMake\nDESCRIPTION: Checks if VPCLMULQDQ optimization is enabled (WITH_VPCLMULQDQ), if intrinsics are available (HAVE_VPCLMULQDQ_INTRIN), and if PCLMULQDQ and AVX512 are also enabled. If supported, it adds the DX86_VPCLMULQDQ_CRC definition for CRC32 calculation, appends the specific source file (`crc32_vpclmulqdq.c`), adds feature information, and sets compile flags (PCLMULFLAG, VPCLMULFLAG, AVX512FLAG, NOLTOFLAG). Otherwise, it disables VPCLMULQDQ support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_33\n\nLANGUAGE: cmake\nCODE:\n```\n        if(WITH_VPCLMULQDQ)\n            check_vpclmulqdq_intrinsics()\n            if(HAVE_VPCLMULQDQ_INTRIN AND WITH_PCLMULQDQ AND WITH_AVX512)\n                add_definitions(-DX86_VPCLMULQDQ_CRC)\n                set(VPCLMULQDQ_SRCS ${ARCHDIR}/crc32_vpclmulqdq.c)\n                add_feature_info(VPCLMUL_CRC 1 \"Support CRC hash generation using VPCLMULQDQ, using \\\"${PCLMULFLAG} ${VPCLMULFLAG} ${AVX512FLAG}\\\"\")\n                list(APPEND ZLIB_ARCH_SRCS ${VPCLMULQDQ_SRCS})\n                set_property(SOURCE ${VPCLMULQDQ_SRCS} PROPERTY COMPILE_FLAGS \"${PCLMULFLAG} ${VPCLMULFLAG} ${AVX512FLAG} ${NOLTOFLAG}\")\n            else()\n                set(WITH_VPCLMULQDQ OFF)\n            endif()\n        endif()\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Including Plugin CMake Script\nDESCRIPTION: Includes another CMake script located at `cmake/plugin.cmake` relative to the current script's directory (`CMAKE_CURRENT_LIST_DIR`). This likely incorporates common logic or configurations related to plugin handling within the OpenCV build system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(${CMAKE_CURRENT_LIST_DIR}/cmake/plugin.cmake)\n```\n\n----------------------------------------\n\nTITLE: Setting JAVA_HOME Before Running CMake (Bash)\nDESCRIPTION: This Bash snippet demonstrates how to set the JAVA_HOME environment variable before running CMake. This is necessary if CMake cannot automatically locate the Java Development Kit (JDK) installation required for building the Java bindings. Replace the path with the actual JDK installation directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/desktop_java/java_dev_intro.markdown#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport JAVA_HOME=/usr/lib/jvm/java-6-oracle\ncmake -DBUILD_SHARED_LIBS=OFF ..\n```\n\n----------------------------------------\n\nTITLE: DirectShow Configuration Option in OpenCV\nDESCRIPTION: Defines the WITH_DSHOW build option for Windows platforms to enable the older DirectShow framework for camera frame capture. This option is deprecated in favor of MSMF.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/config_reference/config_reference.markdown#2025-04-22_snippet_14\n\nLANGUAGE: markdown\nCODE:\n```\n`WITH_DSHOW` (Windows; default: _ON_)\n```\n\n----------------------------------------\n\nTITLE: Configuring Thread Support for OpenCV ts Module in CMake\nDESCRIPTION: Disables thread support in the ts module if OPENCV_DISABLE_THREAD_SUPPORT is set, by defining GTEST_HAS_PTHREAD=0.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/ts/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(OPENCV_DISABLE_THREAD_SUPPORT)\n  # This is required to disable threads in the ts module, as\n  # described in `ts_gtest.h`.\n  ocv_target_compile_definitions(${the_module} PUBLIC GTEST_HAS_PTHREAD=0)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV 2.4 Build for Jetson TK1 using CMake (Shell)\nDESCRIPTION: Runs CMake to configure the OpenCV 2.4 build specifically for the NVIDIA Jetson TK1 platform (L4T). It sets build type to Release, enables CUDA 6.5 for architecture 3.2, enables Python 2 bindings (`BUILD_opencv_python`), NEON, TBB, and FFMPEG, while disabling several other optional dependencies (PNG, TIFF, Jasper, ZLIB, Java, nonfree modules, OpenCL, OpenMP, GStreamer, VTK, 1394, OpenEXR) and specifying the test data path. Assumes execution from a build directory sibling to `opencv` and `opencv_extra`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/building_tegra_cuda/building_tegra_cuda.markdown#2025-04-22_snippet_15\n\nLANGUAGE: Shell\nCODE:\n```\n$ cmake \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_INSTALL_PREFIX=/usr \\\n    -DBUILD_PNG=OFF \\\n    -DBUILD_TIFF=OFF \\\n    -DBUILD_TBB=OFF \\\n    -DBUILD_JPEG=OFF \\\n    -DBUILD_JASPER=OFF \\\n    -DBUILD_ZLIB=OFF \\\n    -DBUILD_EXAMPLES=ON \\\n    -DBUILD_JAVA=OFF \\\n    -DBUILD_opencv_nonfree=OFF \\\n    -DBUILD_opencv_python=ON \\\n    -DENABLE_NEON=ON \\\n    -DWITH_OPENCL=OFF \\\n    -DWITH_OPENMP=OFF \\\n    -DWITH_FFMPEG=ON \\\n    -DWITH_GSTREAMER=OFF \\\n    -DWITH_GSTREAMER_0_10=OFF \\\n    -DWITH_CUDA=ON \\\n    -DWITH_GTK=ON \\\n    -DWITH_VTK=OFF \\\n    -DWITH_TBB=ON \\\n    -DWITH_1394=OFF \\\n    -DWITH_OPENEXR=OFF \\\n    -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-6.5 \\\n    -DCUDA_ARCH_BIN=3.2 \\\n    -DCUDA_ARCH_PTX=\"\" \\\n    -DINSTALL_C_EXAMPLES=ON \\\n    -DINSTALL_TESTS=ON \\\n    -DOPENCV_TEST_DATA_PATH=../opencv_extra/testdata \\\n    ../opencv\n```\n\n----------------------------------------\n\nTITLE: Configuring x86 AVX512 Optimizations for ZLIB in CMake\nDESCRIPTION: Checks if AVX512 optimization is enabled (WITH_AVX512), if intrinsics are available (HAVE_AVX512_INTRIN), and if AVX2 is also enabled. If supported, it adds the DX86_AVX512 definition, appends the AVX512 adler32 source and header files, adds feature information, and sets compile flags (AVX512FLAG, NOLTOFLAG). Otherwise, it disables AVX512 support.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_31\n\nLANGUAGE: cmake\nCODE:\n```\n        if(WITH_AVX512)\n            check_avx512_intrinsics()\n            if(HAVE_AVX512_INTRIN AND WITH_AVX2)\n                add_definitions(-DX86_AVX512)\n                list(APPEND AVX512_SRCS ${ARCHDIR}/adler32_avx512.c)\n                add_feature_info(AVX512_ADLER32 1 \"Support AVX512-accelerated adler32, using \\\"${AVX512FLAG}\\\"\")\n                list(APPEND ZLIB_ARCH_SRCS ${AVX512_SRCS})\n                list(APPEND ZLIB_ARCH_HDRS ${ARCHDIR}/adler32_avx512_p.h)\n                set_property(SOURCE ${AVX512_SRCS} PROPERTY COMPILE_FLAGS \"${AVX512FLAG} ${NOLTOFLAG}\")\n            else()\n                set(WITH_AVX512 OFF)\n            endif()\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Conditionally Exiting Build Script in CMake\nDESCRIPTION: Checks if the `BUILD_EXAMPLES` variable is false or if the `OCV_DEPENDENCIES_FOUND` variable (set by `ocv_check_dependencies`) is false. If either condition is true, it stops processing the current CMake script using `return()`, effectively skipping the build of GPU samples.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/gpu/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_EXAMPLES OR NOT OCV_DEPENDENCIES_FOUND)\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring x86 XSAVE Intrinsics Support for ZLIB in CMake\nDESCRIPTION: Checks for x86 XSAVE intrinsics support (HAVE_XSAVE_INTRIN). If available, it adds feature information, sets compile flags for the x86 features source file (if runtime detection is enabled), and adds the DX86_HAVE_XSAVE_INTRIN definition unless using GCC versions older than 8.2.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_25\n\nLANGUAGE: cmake\nCODE:\n```\n        check_xsave_intrinsics()\n        if(HAVE_XSAVE_INTRIN)\n            add_feature_info(XSAVE 1 \"Support XSAVE intrinsics using \\\"${XSAVEFLAG}\\\"\")\n            if(WITH_RUNTIME_CPU_DETECTION)\n                set_property(SOURCE ${ARCHDIR}/x86_features.c PROPERTY COMPILE_FLAGS \"${XSAVEFLAG}\")\n            endif()\n            if(NOT (CMAKE_C_COMPILER_ID MATCHES \"GNU\" AND CMAKE_C_COMPILER_VERSION VERSION_LESS 8.2))\n                add_definitions(-DX86_HAVE_XSAVE_INTRIN)\n            endif()\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Checking for Additional System Header Files in CMake\nDESCRIPTION: Uses the standard `CHECK_INCLUDE_FILE` CMake command directly (without the custom macro) to check for the presence of several POSIX/Unix-specific header files (`strings.h`, `sys/stat.h`, `sys/types.h`, `unistd.h`). The results are stored in corresponding `HAVE_*` variables.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\n# why check this one ? for openjpip ?\nCHECK_INCLUDE_FILE(\"strings.h\"      HAVE_STRINGS_H)\nCHECK_INCLUDE_FILE(\"sys/stat.h\"     HAVE_SYS_STAT_H)\nCHECK_INCLUDE_FILE(\"sys/types.h\"    HAVE_SYS_TYPES_H)\nCHECK_INCLUDE_FILE(\"unistd.h\"       HAVE_UNISTD_H)\n```\n\n----------------------------------------\n\nTITLE: Iterating Through OpenCV Modules for Java Bindings Configuration in CMake\nDESCRIPTION: This loop iterates through the list of OpenCV modules specified in the `OPENCV_JAVA_MODULES` variable. For each module, it appends the module's header files and any files found in its `misc/java` directory to the `deps` list. It also constructs a JSON fragment (`__modules_config`) containing the module's sanitized name and relative location. Finally, it calls `ocv_remap_files` to process any template files found within the module's `misc/java` directory.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/generator/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(__modules_config \"\") # list of OpenCV modules\nforeach(m ${OPENCV_JAVA_MODULES})\n  set(module_java_dir \"${OPENCV_MODULE_${m}_LOCATION}/misc/java\")\n  list(APPEND deps ${OPENCV_MODULE_${m}_HEADERS})\n  file(GLOB_RECURSE misc_files \"${module_java_dir}/*\")\n  list(APPEND deps ${misc_files})\n\n  string(REGEX REPLACE \"^opencv_\" \"\" m_ \"${m}\")\n  if(__modules_config)\n    set(__modules_config \"${__modules_config},\\\\n\")\n  endif()\n  file(RELATIVE_PATH rel_path \"${OpenCV_SOURCE_DIR}\" \"${OPENCV_MODULE_${m}_LOCATION}\")\n  set(__modules_config \"${__modules_config}    { \\\"name\\\": \\\"${m_}\\\", \\\"location\\\": \\\"${rel_path}\\\" }\")\n\n  ocv_remap_files(misc_files)\nendforeach(m)\n```\n\n----------------------------------------\n\nTITLE: Organizing CMake Project Structure with Solution Folders\nDESCRIPTION: This snippet assigns the built libtiff library to a solution folder named '3rdparty' for better project organization when using supported IDEs. This structural organization aids in managing large projects by grouping related components.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libtiff/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(${TIFF_LIBRARY} PROPERTIES FOLDER \"3rdparty\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building OpenCV Documentation using Make Doxygen (Shell)\nDESCRIPTION: Executes the `doxygen` target using `make` within the build directory. This command processes the source code comments and configuration files to generate HTML documentation for the OpenCV library, typically outputting to `build/doc/doxygen/html/index.html`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora.markdown#2025-04-22_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\nmake doxygen\n```\n\n----------------------------------------\n\nTITLE: Checking for fseeko Function in Zlib CMake Configuration\nDESCRIPTION: Checks if the fseeko function exists in the system and adds a definition to disable it if not available. This is important for handling large files in Zlib.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncheck_function_exists(fseeko HAVE_FSEEKO)\nif(NOT HAVE_FSEEKO)\n  add_definitions(-DNO_FSEEKO)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating OpenCV Java Test Target in CMake\nDESCRIPTION: Creates the main target for building OpenCV Java tests. It sets up dependencies and configures project properties for solution folders if enabled.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/java/test/pure_test/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(${PROJECT_NAME} ALL\n    DEPENDS ${the_module} \"${OPENCV_JAVA_TEST_DIR}/build/jar/opencv-test.jar\"\n    SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/build.xml\"\n)\n\nif(ENABLE_SOLUTION_FOLDERS)\n  set_target_properties(${PROJECT_NAME} PROPERTIES FOLDER \"tests accuracy\")\nendif()\n\nadd_dependencies(opencv_tests ${PROJECT_NAME})\n```\n\n----------------------------------------\n\nTITLE: NASM Assembly Configuration for x86/x86-64\nDESCRIPTION: Configures NASM assembly compilation for x86 and x86-64 architectures, including object format detection, debug flags, and platform-specific settings\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/simd/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CPU_TYPE STREQUAL \"x86_64\" OR CPU_TYPE STREQUAL \"i386\")\n\nset(CMAKE_ASM_NASM_FLAGS_DEBUG_INIT \"-g\")\nset(CMAKE_ASM_NASM_FLAGS_RELWITHDEBINFO_INIT \"-g\")\n\nif(NOT DEFINED CMAKE_ASM_NASM_COMPILER AND DEFINED ENV{ASM_NASM})\n  set(CMAKE_ASM_NASM_COMPILER $ENV{ASM_NASM})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Copying and Packaging OpenCV Libraries as JARs Bash\nDESCRIPTION: This series of Bash commands demonstrates how to organize and package OpenCV libraries as JAR files for use in the local Maven repository. The instructions guide users to organize native libraries according to OS and architecture before packaging them with the jar command. Output is a directory layout for the libraries.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/clojure_dev_intro/clojure_dev_intro.markdown#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/opt\\nmkdir clj-opencv\\ncd clj-opencv\\ncp ~/opt/opencv/build/bin/opencv-247.jar .\n```\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p native/macosx/x86_64\\ncp ~/opt/opencv/build/lib/libopencv_java247.dylib native/macosx/x86_64/\n```\n\nLANGUAGE: bash\nCODE:\n```\njar -cMf opencv-native-247.jar native\n```\n\n----------------------------------------\n\nTITLE: C Language Standard Configuration\nDESCRIPTION: Sets up C language standards and requirements, including C11 as default standard, requiring standard compliance, and disabling compiler extensions\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT CMAKE_C_STANDARD)\n    set(CMAKE_C_STANDARD 11)          # The C standard whose features are requested to build this target\nendif()\nif(NOT CMAKE_C_STANDARD_REQUIRED)\n    set(CMAKE_C_STANDARD_REQUIRED ON) # Boolean describing whether the value of C_STANDARD is a requirement\nendif()\nif(NOT CMAKE_C_EXTENSIONS)\n    set(CMAKE_C_EXTENSIONS OFF)       # Boolean specifying whether compiler specific extensions are requested\nendif()\nset(VALID_C_STANDARDS \"99\" \"11\")\nif(NOT CMAKE_C_STANDARD IN_LIST VALID_C_STANDARDS)\n    MESSAGE(FATAL_ERROR \"CMAKE_C_STANDARD:STRING=${CMAKE_C_STANDARD} not in known standards list\\n ${VALID_C_STANDARDS}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Zlib Library Output Properties in CMake\nDESCRIPTION: Configures output properties for the Zlib library including naming, debug postfix, and output directory. This ensures consistent library naming and placement.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(${ZLIB_LIBRARY} PROPERTIES\n    OUTPUT_NAME ${ZLIB_LIBRARY}\n    DEBUG_POSTFIX \"${OPENCV_DEBUG_POSTFIX}\"\n    COMPILE_PDB_NAME ${ZLIB_LIBRARY}\n    COMPILE_PDB_NAME_DEBUG \"${ZLIB_LIBRARY}${OPENCV_DEBUG_POSTFIX}\"\n    ARCHIVE_OUTPUT_DIRECTORY ${3P_LIBRARY_OUTPUT_PATH}\n)\n```\n\n----------------------------------------\n\nTITLE: Defining OpenJPEG Source Files in CMake\nDESCRIPTION: Sets up the list of source files that comprise the OpenJPEG library. Includes core functionality files like thread handling, image processing, and memory management.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/openjpeg/openjp2/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENJPEG_SRCS\n  ${CMAKE_CURRENT_SOURCE_DIR}/thread.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/bio.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/cio.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/dwt.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/event.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/ht_dec.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/image.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/invert.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/j2k.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/jp2.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/mct.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/mqc.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/openjpeg.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/opj_clock.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/pi.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/t1.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/t2.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/tcd.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/tgt.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/function_list.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/opj_malloc.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/sparse_array.c\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring VideoIO Plugin Enabling and Plugin Backend List - CMake - CMake\nDESCRIPTION: Configures whether to enable the videoio plugin system and sets the backend list for plugin compilation. These CMake commands establish cache variables for build configuration, default plugin enabling based on platform, and manage advanced options presentation. Key parameters include VIDEOIO_ENABLE_PLUGINS_DEFAULT, VIDEOIO_PLUGIN_LIST, and VIDEOIO_ENABLE_PLUGINS, which control plugin building and usage for the VideoIO module. Inputs are provided by CMake variable values or the cache; output is configuration for downstream build logic.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/videoio/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(VIDEOIO_ENABLE_PLUGINS_DEFAULT ON)\nif(EMSCRIPTEN OR IOS OR XROS OR WINRT)\n  set(VIDEOIO_ENABLE_PLUGINS_DEFAULT OFF)\nendif()\n\nset(VIDEOIO_PLUGIN_LIST \"\" CACHE STRING \"List of videoio backends to be compiled as plugins (ffmpeg, gstreamer, mfx, msmf or special value 'all')\")\nset(VIDEOIO_ENABLE_PLUGINS \"${VIDEOIO_ENABLE_PLUGINS_DEFAULT}\" CACHE BOOL \"Allow building and using of videoio plugins\")\nmark_as_advanced(VIDEOIO_PLUGIN_LIST VIDEOIO_ENABLE_PLUGINS)\n```\n\n----------------------------------------\n\nTITLE: Sample Signature for Copyright Disclaimer\nDESCRIPTION: This is a placeholder signature block for the sample employer copyright disclaimer. It indicates where the authorized representative's signature, name, title, and the date should go.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ittnotify/src/ittnotify/GPL-2.0-only.txt#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n<signature of Ty Coon>, 1 April 1989 Ty Coon, President of Vice\n```\n\n----------------------------------------\n\nTITLE: PowerPC Altivec Support Test\nDESCRIPTION: Validates PowerPC Altivec vector operations support\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/simd/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: c\nCODE:\n```\n#include <altivec.h>\nint main(void) {\n  __vector int vi = { 0, 0, 0, 0 };\n  int i[4];\n  vec_st(vi, 0, i);\n  return i[0];\n}\n```\n\n----------------------------------------\n\nTITLE: Checking JPEG Decoding Status\nDESCRIPTION: Code symbols for checking JPEG decoding status through return codes from jpeg_consume_input() function and testing jpeg_input_complete().\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_56\n\nLANGUAGE: cpp\nCODE:\n```\nJPEG_REACHED_EOI\nJPEG_REACHED_SOS\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for 2D Features Framework in Markdown\nDESCRIPTION: This snippet defines a table of contents for tutorials related to OpenCV's 2D Features framework using Markdown syntax. It includes links to various tutorials covering different aspects of feature detection, description, and matching.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/features2d/table_of_content_features2d.markdown#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n2D Features framework (feature2d module) {#tutorial_table_of_content_features2d}\n=========================================\n\n-   @subpage tutorial_harris_detector\n-   @subpage tutorial_good_features_to_track\n-   @subpage tutorial_generic_corner_detector\n-   @subpage tutorial_corner_subpixels\n-   @subpage tutorial_feature_detection\n-   @subpage tutorial_feature_description\n-   @subpage tutorial_feature_flann_matcher\n-   @subpage tutorial_feature_homography\n-   @subpage tutorial_detection_of_planar_objects\n-   @subpage tutorial_akaze_matching\n-   @subpage tutorial_akaze_tracking\n-   @subpage tutorial_homography\n```\n\n----------------------------------------\n\nTITLE: Installing Orbbec OpenNI SDK on Linux (v2.3.0.63)\nDESCRIPTION: Navigates into the extracted Orbbec OpenNI SDK directory for Linux 64-bit (version 2.3.0.63) and executes the installation script with root privileges. This installs the necessary drivers and libraries for the Orbbec Astra camera.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/app/orbbec_astra_openni.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cd Linux/OpenNI-Linux-x64-2.3.0.63/\n$ sudo ./install.sh\n```\n\n----------------------------------------\n\nTITLE: Template Matching Formula: TM_CCOEFF_NORMED (LaTeX)\nDESCRIPTION: Mathematical formula for the Normalized Correlation Coefficient (TM_CCOEFF_NORMED) template matching method used in OpenCV's `matchTemplate` function. It normalizes the correlation coefficient result, making it robust to brightness and contrast changes. R(x,y) is the result, T' is the mean-subtracted template, and I' is the mean-subtracted image patch.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/imgproc/histograms/template_matching/template_matching.markdown#2025-04-22_snippet_6\n\nLANGUAGE: latex\nCODE:\n```\n\\f[R(x,y)= \\frac{ \\sum_{x',y'} (T'(x',y') \\cdot I'(x+x',y+y')) }{ \\sqrt{\\sum_{x',y'}T'(x',y')^2 \\cdot \\sum_{x',y'} I'(x+x',y+y')^2} }\\f]\n```\n\n----------------------------------------\n\nTITLE: Generating OpenCV for Windows Phone 8.1 x86 using CMake\nDESCRIPTION: Invokes CMake directly to generate Visual Studio 2013 project files for OpenCV targeting Windows Phone 8.1 on the x86 architecture. It specifies the generator, system name, and system version.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Visual Studio 12 2013\" -DCMAKE_SYSTEM_NAME=WindowsPhone -DCMAKE_SYSTEM_VERSION=8.1 <path-to-source>\n```\n\n----------------------------------------\n\nTITLE: Defining OpenCV GUI Features Tutorial Section in Markdown\nDESCRIPTION: This snippet defines the main heading for the GUI Features tutorial section in OpenCV documentation using Markdown syntax. It includes a reference tag for linking within the documentation system.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_gui/py_table_of_contents_gui.markdown#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\nGui Features in OpenCV {#tutorial_py_table_of_contents_gui}\n======================\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV ts Module in CMake\nDESCRIPTION: Sets up the OpenCV ts module as a static library, configures build options, and includes necessary dependencies. It also handles platform-specific settings for WINRT and QNX.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/ts/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(the_description \"The ts module\")\n\nif(NOT BUILD_opencv_ts AND NOT BUILD_TESTS AND NOT BUILD_PERF_TESTS)\n  ocv_module_disable(ts)\nendif()\n\nset(OPENCV_MODULE_TYPE STATIC)\nset(OPENCV_MODULE_IS_PART_OF_WORLD FALSE)\n\nif(WINRT)\n  # WINRT doesn't have access to environment variables\n  # so adding corresponding macros during CMake run\n  add_env_definitions(OPENCV_TEST_DATA_PATH)\n  add_env_definitions(OPENCV_PERF_VALIDATION_DIR)\nendif()\n\nocv_warnings_disable(CMAKE_CXX_FLAGS -Wundef)\n\nocv_add_module(ts INTERNAL opencv_core opencv_imgproc opencv_imgcodecs opencv_videoio opencv_highgui)\n\nocv_glob_module_sources()\nocv_module_include_directories()\nocv_create_module()\n```\n\n----------------------------------------\n\nTITLE: Defining Compile Options for OpenCV Module\nDESCRIPTION: Sets compile definitions for an OpenCV module and an optional test target to enable plugins. It checks if the 'opencv_test_highgui' target is present and adds the 'ENABLE_PLUGINS' definition. The 'the_module' parameter represents the name of the module being configured. There are no additional inputs or outputs, but the target should exist within the build context.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/highgui/CMakeLists.txt#2025-04-22_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\nocv_target_compile_definitions(${the_module} PRIVATE ENABLE_PLUGINS)\nif(TARGET opencv_test_highgui)\n  ocv_target_compile_definitions(opencv_test_highgui PRIVATE ENABLE_PLUGINS)\nendif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring G-API Module Sources and Include Paths in CMake\nDESCRIPTION: Prepares the source file list, organizes files for IDEs, sets the final header and source lists for the module, and adds necessary include directories. `ocv_list_add_prefix` ensures source paths are relative to the current CMake list directory. `ocv_source_group` improves project organization in IDEs. `ocv_set_module_sources` registers the collected headers and sources. `ocv_module_include_directories` adds the module's 'src' directory and the VASOT include directory to the build's include path.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nocv_list_add_prefix(gapi_srcs \"${CMAKE_CURRENT_LIST_DIR}/\")\n\n# For IDE users\nocv_source_group(\"Src\"     FILES ${gapi_srcs} ${gapi_3rdparty_srcs})\nocv_source_group(\"Include\" FILES ${gapi_ext_hdrs})\n\nocv_set_module_sources(HEADERS ${gapi_ext_hdrs} SOURCES ${gapi_srcs} ${gapi_3rdparty_srcs})\nocv_module_include_directories(\"${CMAKE_CURRENT_LIST_DIR}/src\")\n\n# VAS Object Tracking includes\nocv_module_include_directories(${CMAKE_CURRENT_LIST_DIR}/src/3rdparty/vasot/include)\n```\n\n----------------------------------------\n\nTITLE: Defining Zlib Source Files in CMake\nDESCRIPTION: Lists all the public headers, private headers, and source files needed to build the Zlib library. This comprehensive listing ensures all necessary files are included in the build.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(ZLIB_PUBLIC_HDRS\n    ${CMAKE_CURRENT_BINARY_DIR}/zconf.h\n    zlib.h\n)\nset(ZLIB_PRIVATE_HDRS\n    crc32.h\n    deflate.h\n    gzguts.h\n    inffast.h\n    inffixed.h\n    inflate.h\n    inftrees.h\n    trees.h\n    zutil.h\n)\nset(ZLIB_SRCS\n    adler32.c\n    compress.c\n    crc32.c\n    deflate.c\n    gzclose.c\n    gzlib.c\n    gzread.c\n    gzwrite.c\n    inflate.c\n    infback.c\n    inftrees.c\n    inffast.c\n    trees.c\n    uncompr.c\n    zutil.c\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Download Path Environment Variable\nDESCRIPTION: Bash commands to set the download path environment variable and execute the download script.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/samples/dnn/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENCV_DOWNLOAD_DATA_PATH=download_folder\npython your_script.py\n```\n\n----------------------------------------\n\nTITLE: MultiArch Library and Pkg-config File Structure (Unparsed)\nDESCRIPTION: Describes the typical directory layout where shared libraries and `pkg-config` (.pc) files are stored for different architectures when using MultiArch. Libraries and specific .pc files reside in architecture-specific subdirectories under `/usr/lib`, while common header-related .pc files might be in `/usr/share/pkgconfig`.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_10\n\nLANGUAGE: unparsed\nCODE:\n```\n/usr\n  + lib\n    + aarch64-linux-gnu   - shared libraries for arm64\n      + pkgconfig         - pkg-config files for arm64 libraries\n    + arm-linux-gnueabihf - shared libraries for armhf\n      + pkgconfig         - pkg-config files for armhf libraries\n  + share\n    + pkgconfig         - pkg-config files(for header files)\n```\n\n----------------------------------------\n\nTITLE: Setting up the Objective-C Bindings Generator as an OpenCV Module in CMake\nDESCRIPTION: Initializes the Objective-C bindings generator as an internal OpenCV module with dependencies on opencv_core and opencv_imgproc. Sets up the necessary directories for binding generation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/objc/generator/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(MODULE_NAME \"objc_bindings_generator\")\nset(OPENCV_MODULE_IS_PART_OF_WORLD FALSE)\nocv_add_module(${MODULE_NAME} INTERNAL opencv_core opencv_imgproc)\n\n#set(OPENCV_OBJC_SIGNATURES_FILE \"${CMAKE_CURRENT_BINARY_DIR}/opencv_objc_signatures.json\" CACHE INTERNAL \"\")\nset(OPENCV_OBJC_BINDINGS_DIR \"${CMAKE_CURRENT_BINARY_DIR}\" CACHE INTERNAL \"\")\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Command for JavaScript Bindings Generation in CMake\nDESCRIPTION: Defines a custom command to generate JavaScript bindings using Python scripts, specifying input files, output files, and dependencies.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/js/generator/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_command(\n  OUTPUT ${bindings_cpp} \"${OPENCV_DEPHELPER}/gen_opencv_js_source\"\n  COMMAND\n      ${PYTHON_DEFAULT_EXECUTABLE}\n      \"${CMAKE_CURRENT_SOURCE_DIR}/embindgen.py\"\n      \"${scripts_hdr_parser}\"\n      \"${bindings_cpp}\"\n      \"${CMAKE_CURRENT_BINARY_DIR}/headers.txt\"\n      \"${JS_SOURCE_DIR}/src/core_bindings.cpp\"\n      \"${OPENCV_JS_WHITELIST_FILE}\"\n  COMMAND\n      ${CMAKE_COMMAND} -E touch \"${OPENCV_DEPHELPER}/gen_opencv_js_source\"\n  WORKING_DIRECTORY\n      \"${CMAKE_CURRENT_BINARY_DIR}/gen\"\n  DEPENDS\n      ${JS_SOURCE_DIR}/src/core_bindings.cpp\n      ${CMAKE_CURRENT_SOURCE_DIR}/embindgen.py\n      ${CMAKE_CURRENT_SOURCE_DIR}/templates.py\n      \"${OPENCV_JS_WHITELIST_FILE}\"\n      ${scripts_hdr_parser}\n      ${opencv_hdrs}\n  COMMENT \"Generate source files for JavaScript bindings\"\n)\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenCV Repository in Bash\nDESCRIPTION: Commands to clone the OpenCV repository from GitHub using Git in the Terminal on macOS.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/ios/ios_install/ios_install.markdown#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/<my_working _directory>\ngit clone https://github.com/opencv/opencv.git\n```\n\n----------------------------------------\n\nTITLE: Configuring APT Sources for MultiArch on Ubuntu 23.10 (Unparsed)\nDESCRIPTION: Example content to be added to `/etc/apt/sources.list` (or a file in `/etc/apt/sources.list.d/`) using `sudo apt edit-sources`. These lines configure APT to find packages for `arm64` and `armhf` architectures in the Ubuntu 23.10 (Mantic) repositories.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/crosscompilation/crosscompile_with_multiarch.markdown#2025-04-22_snippet_5\n\nLANGUAGE: unparsed\nCODE:\n```\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports mantic main restricted\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports mantic-updates main restricted\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports mantic universe\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports mantic-updates universe\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports mantic multiverse\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports mantic-updates multiverse\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports mantic-backports main restricted universe multiverse\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports mantic-security main restricted\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports mantic-security universe\ndeb [arch=arm64,armhf] http://ports.ubuntu.com/ubuntu-ports mantic-security multiverse\n```\n\n----------------------------------------\n\nTITLE: Defining and Grouping OpenCV Modules - Doxygen Group Markup\nDESCRIPTION: Shows how to declare doxygen groups using @defgroup and nest subgroups for modular organization of code documentation. Classes and functions can be assigned to these groups via @ingroup or @addtogroup. Inputs are group and module names, outputs are grouped documentation pages. Used for clear module hierarchy in OpenCV documentation.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/documenting_opencv/documentation_tutorial.markdown#2025-04-22_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\n/**\n@defgroup mymodule My great module\n    optional description\n@{\n    @defgroup mymodule_basic Basic operations\n        optional description\n    @defgroup mymodule_experimental Experimental operations\n        optional description\n@}\n*/\n```\n\n----------------------------------------\n\nTITLE: Enabling s390 DFLTCC Deflate Optimization for ZLIB in CMake\nDESCRIPTION: Checks if the DFLTCC deflate optimization is enabled (WITH_DFLTCC_DEFLATE) for the s390 architecture. If enabled, it adds the S390_DFLTCC_DEFLATE definition and appends the corresponding source file (`dfltcc_deflate.c`) to the ZLIB architecture-specific source list.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/CMakeLists.txt#2025-04-22_snippet_21\n\nLANGUAGE: cmake\nCODE:\n```\n        if(WITH_DFLTCC_DEFLATE)\n            add_definitions(-DS390_DFLTCC_DEFLATE)\n            list(APPEND ZLIB_ARCH_SRCS ${ARCHDIR}/dfltcc_deflate.c)\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Documentation Build with Doxygen - Dockerfile\nDESCRIPTION: This Dockerfile sets up a container based on emscripten/emsdk:2.0.10 and installs the 'doxygen' package to enable documentation generation. Suitable for reproducible documentation builds alongside OpenCV.js source builds. Requires Docker build context. No direct input or output data managed by this Dockerfile.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_setup/js_setup/js_setup.markdown#2025-04-22_snippet_27\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM emscripten/emsdk:2.0.10\\n\\nRUN apt-get update \\\\n  && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends doxygen \\\\n  && rm -rf /var/lib/apt/lists/*\n```\n\n----------------------------------------\n\nTITLE: Generating and Building All WinRT Configurations using setup_winrt.bat\nDESCRIPTION: Navigates to the WinRT platform directory within the OpenCV source and executes the `setup_winrt.bat` script to generate Visual Studio projects for all specified Windows Phone (WP) and Windows Store (WS) versions (8.0, 8.1) and architectures (x86, ARM). The `-b` flag automatically builds the generated solutions in both Debug and Release configurations and runs the INSTALL project.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/platforms/winrt/readme.txt#2025-04-22_snippet_0\n\nLANGUAGE: batch\nCODE:\n```\ncd opencv/platforms/winrt\nsetup_winrt.bat \"WP,WS\" \"8.0,8.1\" \"x86,ARM\" -b\n```\n\n----------------------------------------\n\nTITLE: Failing CMake Build for Unsupported CPU for SIMD\nDESCRIPTION: This CMake code is executed within an `else()` block, indicating that the preceding `if()` condition (likely checking for specific CPU types supporting SIMD) evaluated to false. It calls the `simd_fail` function (presumably a custom macro or function defined elsewhere in the project) to terminate the CMake configuration process with a fatal error. The error message explicitly states that SIMD extensions are unavailable for the detected CPU, referencing the `CMAKE_SYSTEM_PROCESSOR` variable.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/simd/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nelse()\n\nsimd_fail(\"SIMD extensions not available for this CPU (${CMAKE_SYSTEM_PROCESSOR})\")\n\nendif() # CPU_TYPE\n```\n\n----------------------------------------\n\nTITLE: Displaying Installation Path and Finalizing Status in CMake\nDESCRIPTION: This CMake snippet first calls a custom hook `ocv_cmake_hook` for potentially adding extra status information via `STATUS_DUMP_EXTRA`. It then prints the configured installation directory (`CMAKE_INSTALL_PREFIX`), adds visual separators, and uses the custom function `ocv_finalize_status` to complete the status summary output.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/CMakeLists.txt#2025-04-22_snippet_31\n\nLANGUAGE: cmake\nCODE:\n```\nocv_cmake_hook(STATUS_DUMP_EXTRA)\n\n# ========================== auxiliary ==========================\nstatus(\"\")\nstatus(\"  Install to:\" \"${CMAKE_INSTALL_PREFIX}\")\nstatus(\"-----------------------------------------------------------------\")\nstatus(\"\")\n\n\nocv_finalize_status()\n```\n\n----------------------------------------\n\nTITLE: Setting Project Description - CMake\nDESCRIPTION: This code snippet sets a CMake variable describing the module as related to camera calibration and 3D reconstruction. It is used for documentation and module metadata within CMake build processes. No dependencies or parameters are required.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/calib3d/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(the_description \"Camera Calibration and 3D Reconstruction\")\n```\n\n----------------------------------------\n\nTITLE: Downloading and Preparing LaTeX Theme - Shell Script\nDESCRIPTION: This snippet demonstrates invoking a shell script (get_sty.sh) to download and set up the required Metropolis Beamer theme for the LaTeX presentation. It assumes a Unix-like environment with execute permissions for the script, and requires wget for fetching files. Outputs are the downloaded theme files placed appropriately for later LaTeX compilation. The snippet requires all prerequisite software mentioned in the guide.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/modules/gapi/doc/slides/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ ./get_sty.sh\n```\n\n----------------------------------------\n\nTITLE: Enabling Actions Runner Service\nDESCRIPTION: Command to enable and start the actions-runner service using systemctl.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/zlib-ng/arch/s390/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl enable --now actions-runner\n```\n\n----------------------------------------\n\nTITLE: Configuring DOT Image Format for Doxygen in CMake\nDESCRIPTION: Sets a CMake cache variable `OPENCV_DOCS_DOT_IMAGE_FORMAT` to control the image format (defaulting to 'svg') generated by DOT for Doxygen diagrams. It also sets the corresponding internal variable `CMAKECONFIG_DOT_IMAGE_FORMAT` used in the Doxyfile template.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\n# 'png' is good enough for compatibility (but requires +50% storage space)\nset(OPENCV_DOCS_DOT_IMAGE_FORMAT \"svg\" CACHE STRING \"Doxygen/DOT_IMAGE_FORMAT value\")\nset(CMAKECONFIG_DOT_IMAGE_FORMAT \"${OPENCV_DOCS_DOT_IMAGE_FORMAT}\")\n```\n\n----------------------------------------\n\nTITLE: OpenCV Tutorial Navigation Structure in Markdown\nDESCRIPTION: Markdown structure defining the navigation hierarchy and organization of OpenCV tutorials. Includes conditional compilation for CUDA modules.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/tutorials.markdown#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nOpenCV Tutorials {#tutorial_root}\n================\n\n- @subpage tutorial_table_of_content_introduction - build and install OpenCV on your computer\n- @subpage tutorial_table_of_content_core - basic building blocks of the library\n- @subpage tutorial_table_of_content_imgproc - image processing functions\n- @subpage tutorial_table_of_content_app - application utils (GUI, image/video input/output)\n- @subpage tutorial_table_of_content_calib3d - extract 3D world information from 2D images\n- @subpage tutorial_table_of_content_objdetect - INSERT OBJDETECT MODULE INFO\n- @subpage tutorial_table_of_content_features2d - feature detectors, descriptors and matching framework\n- @subpage tutorial_table_of_content_dnn - infer neural networks using built-in _dnn_ module\n- @subpage tutorial_table_of_content_gapi - graph-based approach to computer vision algorithms building\n- @subpage tutorial_table_of_content_other - other modules (ml, objdetect, stitching, video, photo)\n- @subpage tutorial_table_of_content_ios - running OpenCV on an iDevice\n@cond CUDA_MODULES\n- @subpage tutorial_table_of_content_gpu - utilizing power of video card to run CV algorithms\n@endcond\n```\n\n----------------------------------------\n\nTITLE: SVM Kernel Function Calculation\nDESCRIPTION: Derivation of kernel function showing how dot product in higher dimensional space can be computed in lower dimensional space.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_ml/py_svm/py_svm_basics/py_svm_basics.markdown#2025-04-22_snippet_2\n\nLANGUAGE: latex\nCODE:\n```\n\\begin{aligned}\\nK(p,q)  = \\phi(p).\\phi(q) &= \\phi(p)^T \\phi(q) \\\\\\n                          &= (p_{1}^2,p_{2}^2,\\sqrt{2} p_1 p_2).(q_{1}^2,q_{2}^2,\\sqrt{2} q_1 q_2) \\\\\\n                          &= p_{1}^2 q_{1}^2 + p_{2}^2 q_{2}^2 + 2 p_1 q_1 p_2 q_2 \\\\\\n                          &= (p_1 q_1 + p_2 q_2)^2 \\\\\\n          \\phi(p).\\phi(q) &= (p.q)^2\\n\\end{aligned}\n```\n\n----------------------------------------\n\nTITLE: Controlling Adobe Marker Emission in libjpeg (C)\nDESCRIPTION: A C boolean field within the compression parameters structure (`cinfo`). If TRUE, an Adobe-specific APP14 marker is included in the output file, often used to indicate color transforms (e.g., for RGB, CMYK color spaces). `jpeg_set_defaults()` typically sets this based on the color space. It is generally advised not to enable both `write_JFIF_header` and `write_Adobe_marker` simultaneously.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_41\n\nLANGUAGE: C\nCODE:\n```\nboolean write_Adobe_marker\n```\n\n----------------------------------------\n\nTITLE: OpenCV Security Team PGP Public Key\nDESCRIPTION: PGP public key for encrypting sensitive security vulnerability reports sent to the OpenCV security team. Key was updated on April 19, 2023.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/SECURITY.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\ntVpvxypkp4FAHRP7cIS8ODU=\n=g4fo\n-----END PGP PUBLIC KEY BLOCK-----\n```\n\n----------------------------------------\n\nTITLE: Aborting JPEG Compression Cycle in C\nDESCRIPTION: This snippet describes how to abort a JPEG compression cycle using 'jpeg_abort_compress' or 'jpeg_abort'. These functions reset the JPEG object to an idle state, releasing memory. This approach is useful for recycling JPEG objects for another image or when an error occurs.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/libjpeg-turbo/src/libjpeg.txt#2025-04-22_snippet_10\n\nLANGUAGE: C\nCODE:\n```\njpeg_destroy_compress(&cinfo);\n```\n\n----------------------------------------\n\nTITLE: Markdown Tutorial Links for OpenCV Modules\nDESCRIPTION: Structured markdown document containing tutorial links organized by OpenCV module categories including photo, stitching, video, object detection and machine learning topics.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/tutorials/others/table_of_content_other.markdown#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nOther tutorials (ml, objdetect, photo, stitching, video) {#tutorial_table_of_content_other}\n========================================================\n\n-   photo. @subpage tutorial_hdr_imaging\n-   stitching. @subpage tutorial_stitcher\n-   video. @subpage tutorial_background_subtraction\n-   video. @subpage tutorial_meanshift\n-   video. @subpage tutorial_optical_flow\n-   objdetect. @subpage tutorial_cascade_classifier\n-   objdetect. @subpage tutorial_traincascade\n-   objdetect. @subpage tutorial_barcode_detect_and_decode\n-   ml. @subpage tutorial_introduction_to_svm\n-   ml. @subpage tutorial_non_linear_svms\n-   ml. @subpage tutorial_introduction_to_pca\n```\n\n----------------------------------------\n\nTITLE: Markdown Redirect Stub for OpenCV Object Detection Tutorial\nDESCRIPTION: A markdown stub document that redirects readers to the new location of the object detection tutorial content.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/py_tutorials/py_objdetect/py_table_of_contents_objdetect.markdown#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nObject Detection {#tutorial_py_table_of_contents_objdetect}\n================\n\nContent has been moved: @ref tutorial_table_of_content_objdetect\n```\n\n----------------------------------------\n\nTITLE: Basic LGPL License Notice Template\nDESCRIPTION: Standard template for applying LGPL license notice to source files, including copyright statement, license terms, and warranty disclaimer.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ffmpeg/license.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n<one line to give the library's name and a brief idea of what it does.>\nCopyright (C) <year>  <name of author>\n\nThis library is free software; you can redistribute it and/or\nmodify it under the terms of the GNU Lesser General Public\nLicense as published by the Free Software Foundation; either\nversion 2.1 of the License, or (at your option) any later version.\n\nThis library is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nLesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public\nLicense along with this library; if not, write to the Free Software\nFoundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA\n```\n\n----------------------------------------\n\nTITLE: Embedding an Interactive OpenCV.js Template Matching Example\nDESCRIPTION: This HTML snippet utilizes an `<iframe>` to embed an external HTML page ('js_template_matching_matchTemplate.html'). This page likely provides an interactive JavaScript example demonstrating the OpenCV.js template matching described in the tutorial. The `onload` attribute dynamically adjusts the iframe height based on the loaded content's height.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/doc/js_tutorials/js_imgproc/js_template_matching/js_template_matching.markdown#2025-04-22_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<iframe src=\"../../js_template_matching_matchTemplate.html\" width=\"100%\"\n        onload=\"this.style.height=this.contentDocument.body.scrollHeight +'px';\">\n</iframe>\n```\n\n----------------------------------------\n\nTITLE: Copyright Disclaimer Template\nDESCRIPTION: Sample template for copyright disclaimer from employer or organization for library contributions.\nSOURCE: https://github.com/opencv/opencv/blob/4.x/3rdparty/ffmpeg/license.txt#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nYoyodyne, Inc., hereby disclaims all copyright interest in the\nlibrary `Frob' (a library for tweaking knobs) written by James Random Hacker.\n\n<signature of Ty Coon>, 1 April 1990\nTy Coon, President of Vice\n```"
  }
]
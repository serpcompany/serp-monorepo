[
  {
    "owner": "openvinotoolkit",
    "repo": "openvino.genai",
    "content": "TITLE: Custom Streamer Class with LLMPipeline in OpenVINO GenAI\nDESCRIPTION: This C++ code shows how to create a custom streamer class for advanced processing of output tokens using OpenVINO GenAI's LLMPipeline. The `CustomStreamer` class inherits from `ov::genai::StreamerBase` and overrides the `write` and `end` methods for custom functionality. It requires including necessary headers for StreamerBase, LLMPipeline, and I/O operations.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/streaming.mdx#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/streamer_base.hpp\"\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\n// highlight-start\n// Create custom streamer class\nclass CustomStreamer: public ov::genai::StreamerBase {\npublic:\n    ov::genai::StreamingStatus write(int64_t token) {\n        // Custom processing logic for new decoded token.\n\n        // Return flag corresponds whether generation should be stopped.\n        return ov::genai::StreamingStatus::RUNNING;\n    };\n\n    ov::genai::StreamingStatus write(const std::vector<int64_t>& tokens) {\n        // Custom processing logic for new vector of decoded tokens.\n\n        // Return flag corresponds whether generation should be stopped.\n        return ov::genai::StreamingStatus::RUNNING;\n    };\n\n    void end() {\n        // Custom finalization logic.\n    };\n};\n// highlight-end\n\nint main(int argc, char* argv[]) {\n    std::string prompt;\n    // highlight-next-line\n    std::shared_ptr<CustomStreamer> custom_streamer;\n\n    std::string model_path = argv[1];\n    ov::genai::LLMPipeline pipe(model_path, \"CPU\");\n\n    // highlight-next-line\n    pipe.start_chat();\n    std::cout << \"question:\\n\";\n    while (std::getline(std::cin, prompt)) {\n        // highlight-next-line\n        pipe.generate(prompt, ov::genai::streamer(custom_streamer), ov::genai::max_new_tokens(100));\n        std::cout << \"\\n----------\\n\"\n                    \"question:\\n\";\n    }\n    // highlight-next-line\n    pipe.finish_chat();\n}\n```\n\n----------------------------------------\n\nTITLE: Simple Chat Application in Python\nDESCRIPTION: This Python code implements a simple chat application using the LLMPipeline. It initializes the pipeline, sets a generation configuration, and then enters a loop to continuously prompt the user for input and generate responses until the user enters 'Stop!'.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(models_path)\n\nconfig = {'max_new_tokens': 100, 'num_beam_groups': 3, 'num_beams': 15, 'diversity_penalty': 1.5}\npipe.set_generation_config(config)\n\npipe.start_chat()\nwhile True:\n\tprint('question:')\n\tprompt = input()\n    if prompt == 'Stop!':\n\t\tbreak\n\tprint(pipe(prompt, max_new_tokens=200))\npipe.finish_chat()\n```\n\n----------------------------------------\n\nTITLE: Initialize and Use LoRA Adapters in LLMPipeline - Python\nDESCRIPTION: This Python snippet shows how to initialize an LLMPipeline with LoRA adapters using the openvino_genai library. It demonstrates adding multiple adapters with different weights, generating text, and switching to a different adapter configuration. The code requires the openvino_genai library to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/lora-adapters.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\n\n# Initialize pipeline with adapters\nadapter_config = ov_genai.AdapterConfig()\n\n# Add multiple adapters with different weights\nadapter1 = ov_genai.Adapter(\"path/to/lora1.safetensors\")\nadapter2 = ov_genai.Adapter(\"path/to/lora2.safetensors\")\n\nadapter_config.add(adapter1, alpha=0.5)\nadapter_config.add(adapter2, alpha=0.5)\n\npipe = ov_genai.LLMPipeline(\n    model_path,\n    \"CPU\",\n    adapters=adapter_config\n)\n\n# Generate with current adapters\noutput1 = pipe.generate(\"Generate story about\", max_new_tokens=100)\n\n# Switch to different adapter configuration\nnew_config = ov_genai.AdapterConfig()\nnew_config.add(adapter1, alpha=1.0)\noutput2 = pipe.generate(\n    \"Generate story about\",\n    max_new_tokens=100,\n    adapters=new_config\n)\n```\n\n----------------------------------------\n\nTITLE: Speech-to-Text with WhisperPipeline C++\nDESCRIPTION: This C++ code snippet demonstrates how to perform speech-to-text transcription using the OpenVINO genai WhisperPipeline. It takes paths to the models directory and a WAV file as command-line arguments, reads the audio data, initializes the WhisperPipeline with the specified device, and generates the transcription using the pipeline. The generated text is then printed to the console. Requires OpenVINO genai library and audio_utils.hpp.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_run_model/_code_example_cpp.mdx#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/whisper_pipeline.hpp\"\n#include \"audio_utils.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::filesystem::path models_path = argv[1];\n    std::string wav_file_path = argv[2];\n\n    ov::genai::RawSpeechInput raw_speech = utils::audio::read_wav(wav_file_path);\n\n    ov::genai::WhisperPipeline pipe(models_path, \"${props.device || 'CPU'}\");\n    auto result = pipe.generate(raw_speech, ov::genai::max_new_tokens(100));\n    std::cout << result << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Text Generation with LLMPipeline in Python\nDESCRIPTION: This Python code snippet uses the openvino-genai library to perform text generation using the LLMPipeline API. It initializes the pipeline with a specified model path and device (CPU in this example) and then generates text based on a given prompt, limiting the output to a maximum of 100 new tokens.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai\n#Will run model on CPU, GPU or NPU are possible options\npipe = openvino_genai.LLMPipeline(\"./TinyLlama-1.1B-Chat-v1.0/\", \"CPU\")\nprint(pipe.generate(\"The Sun is yellow because\", max_new_tokens=100))\n```\n\n----------------------------------------\n\nTITLE: Text Generation with OpenVINO GenAI in Python\nDESCRIPTION: This code snippet demonstrates how to use the `openvino_genai` library to load an LLM pipeline and generate text. It initializes an `LLMPipeline` with a model path and device (CPU by default) and then uses the `generate` method to produce text based on a given prompt. The `max_new_tokens` parameter limits the length of the generated text.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/text-generation/_sections/_run_model/_code_example_python.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\n\npipe = ov_genai.LLMPipeline(model_path, \"${props.device || 'CPU'}\")\nprint(pipe.generate(\"What is OpenVINO?\", max_new_tokens=100))\n```\n\n----------------------------------------\n\nTITLE: Simple Chat Application in C++\nDESCRIPTION: This C++ code implements a simple chat application using the LLMPipeline.  It continuously prompts the user for input, generates responses using grouped beam search, and prints the output to the console until the user enters 'Stop!'.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string prompt;\n\n    std::string models_path = argv[1];\n    ov::genai::LLMPipeline pipe(models_path, \"CPU\");\n\n    ov::genai::GenerationConfig config;\n    config.max_new_tokens = 100;\n    config.num_beam_groups = 3;\n    config.num_beams = 15;\n    config.diversity_penalty = 1.0f;\n\n    pipe.start_chat();\n    for (;;;) {\n        std::cout << \"question:\\n\";\n        std::getline(std::cin, prompt);\n        if (prompt == \"Stop!\")\n            break;\n\n        std::cout << \"answer:\\n\";\n        auto answer = pipe(prompt, config);\n        std::cout << answer << std::endl;\n    }\n    pipe.finish_chat();\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Output with Lambda in C++\nDESCRIPTION: This C++ code demonstrates how to stream the output of the LLMPipeline using a lambda function.  The lambda function receives each word as it's generated and prints it to the console.  The function also returns a `StreamingStatus` indicating whether the generation should continue.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string models_path = argv[1];\n    ov::genai::LLMPipeline pipe(models_path, \"CPU\");\n\n    auto streamer = [](std::string word) {\n        std::cout << word << std::flush;\n        // Return flag corresponds whether generation should be stopped.\n        return ov::genai::StreamingStatus::RUNNING;\n    };\n    std::cout << pipe.generate(\"The Sun is yellow because\", ov::genai::streamer(streamer), ov::genai::max_new_tokens(200));\n}\n```\n\n----------------------------------------\n\nTITLE: Using LLMPipeline with genai-node\nDESCRIPTION: Demonstrates the usage of the LLMPipeline class from the genai-node package to generate text. It imports the LLMPipeline, initializes it with a model path and device, starts a chat session, generates text based on the input and configuration, finishes the chat session, and prints the result. The `streamer` function is used to stream the output subwords to the console.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/README.md#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { LLMPipeline } from 'genai-node';\n\nconst pipe = await LLMPipeline(MODEL_PATH, device);\n\nconst input = 'What is the meaning of life?';\nconst config = { 'max_new_tokens': 100 };\n\nawait pipe.startChat();\nconst result = await pipe.generate(input, config, streamer);\nawait pipe.finishChat();\n\n// Output all generation result\nconsole.log(result);\n\nfunction streamer(subword) {\n  process.stdout.write(subword);\n}\n```\n\n----------------------------------------\n\nTITLE: Setup Python Environment for LLM Benchmarking (Bash)\nDESCRIPTION: This code snippet sets up a Python virtual environment, activates it, installs pip, clones the openvino.genai repository, navigates to the llm_bench directory, and installs the required dependencies from the requirements.txt file. It prepares the environment for running LLM benchmarks.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv ov-llm-bench-env\nsource ov-llm-bench-env/bin/activate\npip install --upgrade pip\n\ngit clone  https://github.com/openvinotoolkit/openvino.genai.git\ncd openvino.genai/tools/llm_bench\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initialize and Use LoRA Adapters in LLMPipeline - C++\nDESCRIPTION: This C++ snippet demonstrates how to initialize an LLMPipeline with LoRA adapters using the OpenVINO GenAI library. It shows how to add multiple adapters with different weights, generate text, and switch to a different adapter configuration. The code requires the OpenVINO GenAI library and appropriate header files.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/lora-adapters.mdx#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n\nint main() {\n    ov::genai::AdapterConfig adapter_config;\n\n    // Add multiple adapters with different weights\n    ov::genai::Adapter adapter1(\"path/to/lora1.safetensors\");\n    ov::genai::Adapter adapter2(\"path/to/lora2.safetensors\");\n\n    adapter_config.add(adapter1, 0.5f);\n    adapter_config.add(adapter2, 0.5f);\n\n    ov::genai::LLMPipeline pipe(\n        model_path,\n        \"CPU\",\n        ov::genai::adapters(adapter_config)\n    );\n\n    // Generate with current adapters\n    auto output1 = pipe.generate(\"Generate story about\", ov::genai::max_new_tokens(100));\n\n    // Switch to different adapter configuration\n    ov::genai::AdapterConfig new_config;\n    new_config.add(adapter1, 1.0f);\n    auto output2 = pipe.generate(\n        \"Generate story about\",\n        ov::genai::adapters(new_config),\n        ov::genai::max_new_tokens(100)\n    );\n}\n```\n\n----------------------------------------\n\nTITLE: Chat Session Management in C++\nDESCRIPTION: This C++ snippet demonstrates how to manage a chat session using the OpenVINO GenAI library. It initializes the `LLMPipeline`, sets the generation configuration, and uses `start_chat()` and `finish_chat()` to manage the KV-cache.  The example takes user input, generates an answer, and prints the result until the program is terminated.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/chat-scenario.mdx#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string prompt;\n\n    std::string model_path = argv[1];\n    ov::genai::LLMPipeline pipe(model_path, \"CPU\");\n\n    ov::genai::GenerationConfig config;\n    config.max_new_tokens = 100;\n    config.num_beam_groups = 3;\n    config.num_beams = 15;\n    config.diversity_penalty = 1.0f;\n\n    // highlight-next-line\n    pipe.start_chat();\n    std::cout << \"question:\\n\";\n    while (std::getline(std::cin, prompt)) {\n        std::cout << \"answer:\\n\";\n        auto answer = pipe.generate(prompt, config);\n        std::cout << answer << std::endl;\n        std::cout << \"\\n----------\\n\"\n            \"question:\\n\";\n    }\n    // highlight-next-line\n    pipe.finish_chat();\n}\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with OpenVINO GenAI in Python\nDESCRIPTION: This snippet demonstrates how to generate an image from a text prompt using the OpenVINO GenAI library. It initializes a `Text2ImagePipeline` with a specified model path and device. The `generate` method is then called with the prompt, producing an image tensor which is converted to a PIL Image and saved as a BMP file. The `${props.device || 'CPU'}` allows users to specify the device (CPU, GPU, etc.) for the pipeline, defaulting to CPU if not provided.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-generation/_sections/_run_model/_text2image_python.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\nfrom PIL import Image\n\npipe = ov_genai.Text2ImagePipeline(model_path, \"${props.device || 'CPU'}\")\nimage_tensor = pipe.generate(prompt)\n\nimage = Image.fromarray(image_tensor.data[0])\nimage.save(\"image.bmp\")\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Pipeline Callback Example (Python)\nDESCRIPTION: This code snippet demonstrates how to use a callback function within the `Text2ImagePipeline` to monitor and potentially interrupt the image generation process. The callback receives the current step, total steps, and latent representation, allowing for custom logic to be applied during generation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/image_generation/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npipe = openvino_genai.Text2ImagePipeline(model_dir, device)\n\ndef callback(step, num_steps, latent):\n   print(f\"Image generation step: {step + 1} / {num_steps}\")\n   image_tensor = pipe.decode(latent) # get intermediate image tensor\n   if your_condition: # return True if you want to interrupt image generation\n      return True\n   return False\n\nimage = pipe.generate(\n   ...\n   callback = callback\n)\n```\n\n----------------------------------------\n\nTITLE: Running OpenVINO GenAI Model on GPU (Python)\nDESCRIPTION: This snippet demonstrates how to run an OpenVINO GenAI model on the GPU using Python. It utilizes the `LLMPipeline` object and requires the converted model to be available in a folder. The `device` parameter is set to \"GPU\".\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/text-generation/_sections/_run_model/index.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n<CodeExamplePython device=\"GPU\" />\n```\n\n----------------------------------------\n\nTITLE: Benchmark LLM Model (Python/Bash)\nDESCRIPTION: This command runs the `benchmark.py` script to benchmark the performance of an LLM. It requires specifying the path to the model, and it accepts optional parameters for the inference device, report file, framework, prompt text, and number of iterations. It measures the performance of the LLM during inference.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark.py -m <model> -d <device> -r <report_csv> -f <framework> -p <prompt text> -n <num_iters>\n# e.g.\npython benchmark.py -m models/llama-2-7b-chat/ -n 2\npython benchmark.py -m models/llama-2-7b-chat/ -p \"What is openvino?\" -n 2\npython benchmark.py -m models/llama-2-7b-chat/ -pf prompts/llama-2-7b-chat_l.jsonl -n 2\n```\n\n----------------------------------------\n\nTITLE: Grouped Beam Search with LLMPipeline in Python\nDESCRIPTION: This Python code demonstrates text generation with grouped beam search using the LLMPipeline. It initializes the pipeline and then calls the `generate` function with parameters controlling the number of beam groups, beams, and the diversity penalty.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\n\nresult = pipe.generate(\"The Sun is yellow because\", max_new_tokens=100, num_beam_groups=3, num_beams=15, diversity_penalty=1.5)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Running OpenVINO GenAI Model on CPU (C++)\nDESCRIPTION: This snippet demonstrates how to run an OpenVINO GenAI model on the CPU using C++. It assumes the availability of a converted model within a specified folder and utilizes the `LLMPipeline` object. The `device` parameter is set to \"CPU\".\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/text-generation/_sections/_run_model/index.mdx#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n<CodeExampleCPP device=\"CPU\" />\n```\n\n----------------------------------------\n\nTITLE: Image Generation with Text2ImagePipeline in C++\nDESCRIPTION: This C++ code uses the openvino-genai library to generate an image from text using the Text2ImagePipeline API. It initializes the pipeline with a model path and device (CPU in this example) and generates the image from a text prompt. The generated image is saved as a BMP file. Requires C++ compatible package installation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/image_generation/text2image_pipeline.hpp\"\n#include \"imwrite.hpp\"\n\nint main(int argc, char* argv[]) {\n   const std::string models_path = argv[1], prompt = argv[2];\n   const std::string device = \"CPU\";  // GPU can be used as well\n\n   ov::genai::Text2ImagePipeline pipe(models_path, device);\n   ov::Tensor image = pipe.generate(prompt);\n\n   imwrite(\"image.bmp\", image, true);\n}\n```\n\n----------------------------------------\n\nTITLE: Convert Whisper Model to OpenVINO Format\nDESCRIPTION: This snippet demonstrates how to use the Optimum CLI to download and convert a Whisper model from Hugging Face to the OpenVINO format. The `trustRemoteCode` flag allows execution of custom code potentially packaged with the model.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/index.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n<OptimumCLI model='openai/whisper-base' outputDir='whisper_ov' trustRemoteCode />\n```\n\n----------------------------------------\n\nTITLE: Image2Image Pipeline with OpenVINO in C++\nDESCRIPTION: This C++ code snippet utilizes the OpenVINO genai library to implement an image-to-image pipeline. It takes a model path, a prompt, and an input image path as command-line arguments. The code loads the input image, initializes the Image2ImagePipeline, generates a new image based on the prompt and input image, and then saves the generated image as a BMP file. The pipeline uses the specified device (defaulting to CPU) and a strength parameter of 0.8.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-generation/_sections/_run_model/_image2image_cpp.mdx#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/image_generation/image2image_pipeline.hpp\"\n#include \"load_image.hpp\"\n#include \"imwrite.hpp\"\n\nint main(int argc, char* argv[]) {\n    const std::string models_path = argv[1], prompt = argv[2], image_path = argv[3];\n\n    ov::Tensor input_image = utils::load_image(image_path);\n\n    ov::genai::Image2ImagePipeline pipe(models_path, \"${props.device || 'CPU'}\");\n    ov::Tensor generated_image = pipe.generate(prompt, input_image, ov::genai::strength(0.8f));\n\n    imwrite(\"image.bmp\", generated_image, true);\n}\n```\n\n----------------------------------------\n\nTITLE: Modify Generation Config (Basic) in C++\nDESCRIPTION: Demonstrates how to modify basic generation parameters like max_new_tokens, temperature, top_k, top_p, and repetition_penalty in OpenVINO GenAI using C++. It retrieves the default configuration, adjusts parameters, and generates text with the modified configuration. Requires the `ov::genai::LLMPipeline` class.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/text-generation/_sections/_usage_options/_generation_parameters.mdx#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nint main() {\n    ov::genai::LLMPipeline pipe(model_path, \"CPU\");\n\n    // Get default configuration\n    auto config = pipe.get_generation_config();\n\n    // Modify parameters\n    config.max_new_tokens = 100;\n    config.temperature = 0.7f;\n    config.top_k = 50;\n    config.top_p = 0.9f;\n    config.repetition_penalty = 1.2f;\n\n    // Generate text with custom configuration\n    auto output = pipe.generate(prompt, config);\n}\n```\n\n----------------------------------------\n\nTITLE: Converting LLM to OpenVINO using OptimumCLI\nDESCRIPTION: This snippet demonstrates how to download and convert a Large Language Model (LLM) from Hugging Face to the OpenVINO format using the OptimumCLI tool. It requires the `model` (Hugging Face model name), `outputDir` (directory to store the converted model), `weightFormat` (quantization format), and `trustRemoteCode` (flag to trust remote code).\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/text-generation/index.mdx#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n<OptimumCLI model='TinyLlama/TinyLlama-1.1B-Chat-v1.0' outputDir='TinyLlama_1_1b_v1_ov' weightFormat='int4' trustRemoteCode />\n```\n\n----------------------------------------\n\nTITLE: Configure Generation Parameters with Whisper Pipeline in C++\nDESCRIPTION: This snippet shows how to configure generation parameters such as max_new_tokens, temperature, top_k, top_p, and repetition_penalty for the WhisperPipeline in C++. It gets the default configuration, modifies the parameters, and generates text using the custom configuration. The input is raw audio data, and the output is the generated text.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nint main() {\n    ov::genai::WhisperPipeline pipe(model_path, \"CPU\");\n\n    // Get default configuration\n    auto config = pipe.get_generation_config();\n\n    // Modify parameters\n    config.max_new_tokens = 100;\n    config.temperature = 0.7f;\n    config.top_k = 50;\n    config.top_p = 0.9f;\n    config.repetition_penalty = 1.2f;\n\n    // Generate text with custom configuration\n    auto result = pipe.generate(raw_speech, config);\n}\n```\n\n----------------------------------------\n\nTITLE: Continuous Batching with LLMPipeline in C++\nDESCRIPTION: This C++ code snippet demonstrates how to enable continuous batching with LLMPipeline by providing a SchedulerConfig object when initializing the pipeline. The `cache_size` parameter in the configuration determines the size of the KV cache in GB.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n\nint main(int argc, char* argv[]) {\n    ov::genai::SchedulerConfig scheduler_config;\n    // fill other fields in scheduler_config with custom data if required\n    scheduler_config.cache_size = 1;    // minimal possible KV cache size in GB, adjust as required\n\n    ov::genai::LLMPipeline pipe(models_path, \"CPU\", ov::genai::scheduler_config(scheduler_config));\n}\n```\n\n----------------------------------------\n\nTITLE: Modify Generation Config (Basic) in Python\nDESCRIPTION: Demonstrates how to modify basic generation parameters like max_new_tokens, temperature, top_k, top_p, and repetition_penalty in OpenVINO GenAI using Python.  It gets the default generation config from the LLMPipeline, modifies the parameters, and then generates text using the custom config. Requires the `openvino_genai` library.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/text-generation/_sections/_usage_options/_generation_parameters.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\n\npipe = ov_genai.LLMPipeline(model_path, \"CPU\")\n\n# Get default configuration\nconfig = pipe.get_generation_config()\n\n# Modify parameters\nconfig.max_new_tokens = 100\nconfig.temperature = 0.7\nconfig.top_k = 50\nconfig.top_p = 0.9\nconfig.repetition_penalty = 1.2\n\n# Generate text with custom configuration\noutput = pipe.generate(prompt, config)\n```\n\n----------------------------------------\n\nTITLE: Image Inpainting with OpenVINO GenAI in Python\nDESCRIPTION: This code snippet demonstrates how to perform image inpainting using the OpenVINO GenAI library. It reads an input image and mask image using PIL, initializes an InpaintingPipeline with a model path and device, and then uses the pipeline to generate an inpainted image based on a prompt. The generated image is then saved to a BMP file. Requires the openvino_genai, openvino, PIL (Pillow) and numpy libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-generation/_sections/_run_model/_inpainting_python.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\nimport openvino as ov\nfrom PIL import Image\nimport numpy as np\n\ndef read_image(path: str) -> ov.Tensor:\n    pic = Image.open(path).convert(\"RGB\")\n    image_data = np.array(pic)[None]\n    return ov.Tensor(image_data)\n\ninput_image_data = read_image(\"input_image.jpg\")\nmask_image = read_image(\"mask.jpg\")\n\npipe = ov_genai.InpaintingPipeline(model_path, \"${props.device || 'CPU'}\")\nimage_tensor = pipe.generate(prompt, image=input_image_data, mask_image=mask_image)\n\nimage = Image.fromarray(image_tensor.data[0])\nimage.save(\"image.bmp\")\n\n```\n\n----------------------------------------\n\nTITLE: Streaming Output with Custom Class in Python\nDESCRIPTION: This Python code demonstrates how to stream the output of the LLMPipeline using a custom class that inherits from `openvino_genai.StreamerBase`. The `put` method is called for each token, and the `end` method is called when generation completes. The `put` method should return a boolean indicating whether generation should stop.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\n\nclass CustomStreamer(ov_genai.StreamerBase):\n    def __init__(self):\n        super().__init__()\n        # Initialization logic.\n\n    def put(self, token_id) -> bool:\n        # Custom decoding/tokens processing logic.\n\n        # Returns a flag whether generation should be stopped, if true generation stops.\n        return False\n\n    def end(self):\n        # Custom finalization logic.\n\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\ncustom_streamer = CustomStreamer()\n\npipe.generate(\"The Sun is yellow because\", max_new_tokens=15, streamer=custom_streamer)\n```\n\n----------------------------------------\n\nTITLE: Run Speculative Decoding LM\nDESCRIPTION: This command runs the `speculative_decoding_lm.py` script, demonstrating speculative decoding for faster text generation. The script requires the main model directory, draft model directory, and a prompt as input.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython speculative_decoding_lm.py model_dir draft_model_dir prompt\n```\n\n----------------------------------------\n\nTITLE: Convert model to OpenVINO format\nDESCRIPTION: This command converts a model to the OpenVINO Intermediate Representation (IR) format using the `optimum-cli export openvino` command.  It requires specifying the input model and the desired output folder. This conversion is necessary to leverage OpenVINO's inference capabilities.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --model <model> <output_folder>\n```\n\n----------------------------------------\n\nTITLE: OpenVINO LLM Pipeline Generation in C++\nDESCRIPTION: This code snippet demonstrates how to initialize and use an OpenVINO LLM pipeline in C++ to generate text based on a given prompt.  It requires the openvino and openvino.genai libraries. It takes the model path as a command line argument and initializes the pipeline with the specified device (defaulting to CPU).  The pipeline is then used to generate text with a maximum token limit.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/text-generation/_sections/_run_model/_code_example_cpp.mdx#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string models_path = argv[1];\n    ov::genai::LLMPipeline pipe(model_path, \"${props.device || 'CPU'}\");\n    std::cout << pipe.generate(\"What is OpenVINO?\", ov::genai::max_new_tokens(100)) << '\\n';\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Streamer Class with LLMPipeline in OpenVINO GenAI\nDESCRIPTION: This code demonstrates creating a custom streamer class in Python for more sophisticated processing of output tokens with OpenVINO GenAI's LLMPipeline. The `CustomStreamer` class inherits from `ov_genai.StreamerBase` and overrides the `write` and `end` methods to implement custom logic. It requires the `openvino_genai` package.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/streaming.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\n\npipe = ov_genai.LLMPipeline(model_path, \"CPU\")\n\n# highlight-start\n# Create custom streamer class\nclass CustomStreamer(ov_genai.StreamerBase):\n    def __init__(self):\n        super().__init__()\n        # Initialization logic.\n\n    def write(self, token: int | list[int]) -> ov_genai.StreamingStatus:\n        # Custom processing logic for new decoded token(s).\n\n        # Return flag corresponds whether generation should be stopped.\n        return ov_genai.StreamingStatus.RUNNING\n\n    def end(self):\n        # Custom finalization logic.\n        pass\n# highlight-end\n\n# highlight-next-line\npipe.start_chat()\nwhile True:\n    try:\n        prompt = input('question:\\n')\n    except EOFError:\n        break\n    # highlight-next-line\n    pipe.generate(prompt, streamer=CustomStreamer(), max_new_tokens=100)\n    print('\\n----------\\n')\n# highlight-next-line\npipe.finish_chat()\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO GenAI with pip\nDESCRIPTION: This code snippet demonstrates how to install the OpenVINO GenAI library and the optimum-intel library using pip. optimum-intel is used for downloading, converting, and optimizing LLMs from Hugging Face. It is not required to run the models, only to convert and compress them.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n    # Installing OpenVINO GenAI via pip\n    pip install openvino-genai\n\n    # Install optimum-intel to be able to download, convert and optimize LLMs from Hugging Face\n    # Optimum is not required to run models, only to convert and compress\n    pip install optimum-intel@git+https://github.com/huggingface/optimum-intel.git\n\n    # (Optional) Install (TBD) to be able to download models from Model Scope\n```\n\n----------------------------------------\n\nTITLE: Configuring Image Generation Parameters in Python\nDESCRIPTION: This Python code snippet demonstrates how to use the `Text2ImagePipeline` in the `openvino_genai` library to generate an image and configure parameters like width, height, number of images per prompt, number of inference steps, and guidance scale.  It requires the `openvino_genai` and `PIL` libraries. The generated image is saved as a BMP file.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-generation/_sections/_usage_options/index.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\nfrom PIL import Image\n\npipe = ov_genai.Text2ImagePipeline(model_path, \"CPU\")\nimage_tensor = pipe.generate(\n  prompt,\n  # highlight-start\n  width=512,\n  height=512,\n  num_images_per_prompt=1,\n  num_inference_steps=30,\n  guidance_scale=7.5\n  # highlight-end\n)\n\nimage = Image.fromarray(image_tensor.data[0])\nimage.save(\"image.bmp\")\n```\n\n----------------------------------------\n\nTITLE: VLMPipeline Image and Text Generation with OpenVINO\nDESCRIPTION: This code snippet demonstrates how to perform visual language model (VLM) inference using the OpenVINO GenAI library. It reads images from a specified path, initializes the VLMPipeline with a given model and device, and generates text based on a prompt and the input images.  It requires the openvino_genai, openvino, Pillow (PIL), numpy, and pathlib libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-processing/_sections/_run_model/_code_example_python.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\nimport openvino as ov\nfrom PIL import Image\nimport numpy as np\nfrom pathlib import Path\n\ndef read_image(path: str) -> ov.Tensor:\n    pic = Image.open(path).convert(\"RGB\")\n    image_data = np.array(pic)[None]\n    return ov.Tensor(image_data)\n\ndef read_images(path: str) -> list[ov.Tensor]:\n    entry = Path(path)\n    if entry.is_dir():\n        return [read_image(str(file)) for file in sorted(entry.iterdir())]\n    return [read_image(path)]\n\nimages = read_images(\"./images\")\n\npipe = ov_genai.VLMPipeline(model_path, \"${props.device || 'CPU'}\")\nresult = pipe.generate(prompt, images=images, max_new_tokens=100)\nprint(result.texts[0])\n\n```\n\n----------------------------------------\n\nTITLE: Generating Text and Printing Performance Metrics in C++\nDESCRIPTION: This C++ code snippet demonstrates generating text using the LLMPipeline and accessing performance metrics. It initializes the pipeline, generates text, retrieves the associated performance metrics, and then prints the mean values of generate duration, TTFT, TPOT and throughput to the console. It requires the openvino/genai/llm_pipeline.hpp header.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string models_path = argv[1];\n    ov::genai::LLMPipeline pipe(models_path, \"CPU\");\n    auto result = pipe.generate(\"The Sun is yellow because\", ov::genai::max_new_tokens(20));\n    auto perf_metrics = result.perf_metrics;\n\n    std::cout << std::fixed << std::setprecision(2);\n    std::cout << \"Generate duration: \" << perf_metrics.get_generate_duration().mean << \" ms\" << std::endl;\n    std::cout << \"TTFT: \" << perf_metrics.get_ttft().mean  << \" ms\" << std::endl;\n    std::cout << \"TPOT: \" << perf_metrics.get_tpot().mean  << \" ms/token \" << std::endl;\n    std::cout << \"Throughput: \" << perf_metrics.get_throughput().mean  << \" tokens/s\" << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Whisper Pipeline Usage (Python)\nDESCRIPTION: This Python code demonstrates how to use the `openvino_genai.WhisperPipeline` for speech recognition. It reads a WAV file using `librosa`, initializes the pipeline with a specified model directory and device, and then generates text from the audio. The pipeline expects normalized audio with a sample rate of 16kHz.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/whisper_speech_recognition/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai\nimport librosa\n\ndef read_wav(filepath):\n    raw_speech, samplerate = librosa.load(filepath, sr=16000)\n    return raw_speech.tolist()\n\npipe = openvino_genai.WhisperPipeline(model_dir, \"CPU\")\n# Pipeline expects normalized audio with Sample Rate of 16kHz\nraw_speech = read_wav('how_are_you_doing_today.wav')\nresult = pipe.generate(raw_speech)\n#  How are you doing today?\n```\n\n----------------------------------------\n\nTITLE: Timestamp Prediction with Whisper Pipeline in C++\nDESCRIPTION: This snippet shows how to enable timestamp prediction for each segment of speech using the WhisperPipeline in C++. It sets the return_timestamps parameter to True and then iterates through the chunks to print the timestamps and text segments. The input is raw audio data, and the output includes the text and the start/end timestamps for each segment.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nint main() {\n    ov::genai::WhisperPipeline pipe(model_path, \"CPU\");\n\n    // Enable timestamp prediction\n    result = pipe.generate(raw_speech, ov::genai::return_timestamps(true));\n\n    // Print timestamps and text segments\n    for (auto& chunk : *result.chunks) {\n        std::cout << \"timestamps: [\" << chunk.start_ts << \", \" << chunk.end_ts\n                  << \"] text: \" << chunk.text << \"\\n\";\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Image2Image generation in Python\nDESCRIPTION: This code snippet demonstrates how to use the Image2Image pipeline in Python to generate a new image based on a text prompt and an input image. It loads a pre-trained model, initializes the pipeline, processes the image, and saves the generated image.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nfrom PIL import Image\nimport openvino_genai\nimport openvino as ov\n\ndevice = 'CPU'  # GPU can be used as well\npipe = openvino_genai.Image2ImagePipeline(\"./dreamlike_anime_1_0_ov/INT8\", device)\n\nimage = Image.open(\"small_city.jpg\")\nimage_data = np.array(image)[None]\nimage_data = ov.Tensor(image_data)\n\nimage_tensor = pipe.generate(\n    \"cyberpunk cityscape like Tokyo New York with tall buildings at dusk golden hour cinematic lighting\",\n    image=image_data,\n    strength=0.8\n)\n\nimage = Image.fromarray(image_tensor.data[0])\nimage.save(\"image.bmp\")\n```\n\n----------------------------------------\n\nTITLE: Installing requirements and exporting model to OpenVINO format\nDESCRIPTION: This command installs the required packages from `export-requirements.txt` and then uses the `optimum-cli` to export the specified model to the OpenVINO IR format.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/c/text_generation/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install --upgrade-strategy eager -r ../../export-requirements.txt\noptimum-cli export openvino --model <model> <output_folder>\n```\n\n----------------------------------------\n\nTITLE: Run Prompt Lookup Decoding LM Sample (bash)\nDESCRIPTION: Executes the `prompt_lookup_decoding_lm` application. It requires the model directory and a prompt as input.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n./prompt_lookup_decoding_lm <MODEL_DIR> \"<PROMPT>\"\n```\n\n----------------------------------------\n\nTITLE: Creating Benchmark Image Gen Executable with CMake\nDESCRIPTION: This snippet creates an executable named 'benchmark_image_gen' from 'benchmark_image_gen.cpp', 'load_image.cpp', and 'imwrite.cpp'. It sets include directories and links the executable against 'openvino::genai', 'cxxopts::cxxopts', and 'indicators::indicators'.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(benchmark_image_gen benchmark_image_gen.cpp load_image.cpp imwrite.cpp)\ntarget_include_directories(benchmark_image_gen PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}\" \"${CMAKE_BINARY_DIR}\")\ntarget_link_libraries(benchmark_image_gen PRIVATE openvino::genai cxxopts::cxxopts indicators::indicators)\nset_target_properties(benchmark_image_gen PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n\ninstall(TARGETS benchmark_image_gen\n        RUNTIME DESTINATION samples_bin/\n        COMPONENT samples_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Convert Model to OpenVINO IR Format Example (Bash)\nDESCRIPTION: This is an example of converting the Llama-2-7b-chat-hf model to OpenVINO IR format with fp16 precision.  The resulting OpenVINO model will be stored in the `models/llama-2-7b-chat` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --model meta-llama/Llama-2-7b-chat-hf --weight-format fp16 models/llama-2-7b-chat\n```\n\n----------------------------------------\n\nTITLE: Timestamp Prediction with Whisper Pipeline in Python\nDESCRIPTION: This snippet shows how to enable timestamp prediction for each segment of speech using the WhisperPipeline in Python. It sets the return_timestamps parameter to True and then iterates through the chunks to print the timestamps and text segments. The input is raw audio data, and the output includes the text and the start/end timestamps for each segment.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npipe = ov_genai.WhisperPipeline(model_path, \"CPU\")\n\n# Enable timestamp prediction\nresult = pipe.generate(raw_speech, return_timestamps=True)\n\n# Print timestamps and text segments\nfor chunk in result.chunks:\n    print(f\"timestamps: [{chunk.start_ts:.2f}, {chunk.end_ts:.2f}] text: {chunk.text}\")\n```\n\n----------------------------------------\n\nTITLE: Accumulating Performance Metrics in C++\nDESCRIPTION: This C++ snippet demonstrates accumulating performance metrics from multiple generate calls. The perf_metrics objects from each generation are added together to combine the results. The models_path is passed as a command line argument.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/performance-metrics.mdx#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string models_path = argv[1];\n    ov::genai::LLMPipeline pipe(models_path, \"CPU\");\n    auto result_1 = pipe.generate(\"The Sun is yellow because\", ov::genai::max_new_tokens(20));\n    auto result_2 = pipe.generate(\"The Sun is yellow because\", ov::genai::max_new_tokens(20));\n    auto perf_metrics = result_1.perf_metrics + result_2.perf_metrics\n\n    std::cout << std::fixed << std::setprecision(2);\n    std::cout << \"Generate duration: \" << perf_metrics.get_generate_duration().mean << \" ms\" << std::endl;\n    std::cout << \"TTFT: \" << metrics.get_ttft().mean  << \" ms\" << std::endl;\n    std::cout << \"TPOT: \" << metrics.get_tpot().mean  << \" ms/token \" << std::endl;\n    std::cout << \"Throughput: \" << metrics.get_throughput().mean  << \" tokens/s\" << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Export OpenVINO IR Model using optimum-cli\nDESCRIPTION: This command exports an OpenVINO IR model from the TinyLlama/TinyLlama-1.1B-Chat-v1.0 Hugging Face model using the optimum-cli tool. The --trust-remote-code flag is used to allow execution of custom code within the model.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\noptimum-cli export openvino --model \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" --trust-remote-code \"TinyLlama-1.1B-Chat-v1.0\"\n```\n\n----------------------------------------\n\nTITLE: Modify Generation Config (Beam Search) in Python\nDESCRIPTION: Demonstrates how to modify beam search generation parameters like max_new_tokens, num_beams, num_beam_groups, and diversity_penalty in OpenVINO GenAI using Python. It obtains the default configuration, modifies parameters specific to beam search, and generates text. Requires the `openvino_genai` library.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/text-generation/_sections/_usage_options/_generation_parameters.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\n\npipe = ov_genai.LLMPipeline(model_path, \"CPU\")\n\n# Get default generation config\nconfig = pipe.get_generation_config()\n\n# Modify parameters\nconfig.max_new_tokens = 256\nconfig.num_beams = 15\nconfig.num_beam_groups = 3\nconfig.diversity_penalty = 1.0\n\n# Generate text with custom configuration\noutput = pipe.generate(prompt, config)\n```\n\n----------------------------------------\n\nTITLE: Visual Language Generation with VLMPipeline in Python\nDESCRIPTION: This Python code uses the openvino-genai library to perform visual language generation using the VLMPipeline API. It loads an image, converts it to an OpenVINO tensor, and then generates text describing the image based on a given prompt.  Device is set to CPU.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport openvino as ov\nimport openvino_genai\nfrom PIL import Image\n\n# Choose GPU instead of CPU in the line below to run the model on Intel integrated or discrete GPU\npipe = openvino_genai.VLMPipeline(\"./InternVL2-1B\", \"CPU\")\n\nimage = Image.open(\"dog.jpg\")\nimage_data = np.array(image)\nimage_data = ov.Tensor(image_data)\n\nprompt = \"Can you describe the image?\"\nresult = pipe.generate(prompt, image=image_data, max_new_tokens=100)\nprint(result.texts[0])\n```\n\n----------------------------------------\n\nTITLE: Text Generation with LLMPipeline in C++\nDESCRIPTION: This C++ code demonstrates text generation using the LLMPipeline API from the openvino-genai library. It initializes the pipeline with a model path and device (CPU in this example), and then generates text from a prompt, limiting the output to a maximum of 100 new tokens. Requires C++ compatible package installation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string models_path = argv[1];\n    ov::genai::LLMPipeline pipe(models_path, \"CPU\");\n    std::cout << pipe.generate(\"The Sun is yellow because\", ov::genai::max_new_tokens(100)) << '\\n';\n}\n```\n\n----------------------------------------\n\nTITLE: Download model from Hugging Face\nDESCRIPTION: This command downloads a pre-converted OpenVINO model from the Hugging Face Model Hub using the `huggingface-cli download` command. The specified model is downloaded to the provided output folder.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli download <model> --local-dir <output_folder>\n```\n\n----------------------------------------\n\nTITLE: Convert Model to Optimum-Intel OpenVINO\nDESCRIPTION: This command converts a Hugging Face Transformer model to the OpenVINO format using Optimum-Intel. It quantizes the model to 8-bit by default.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\noptimum-cli export openvino -m microsoft/Phi-3-mini-4k-instruct phi-3-openvino\n```\n\n----------------------------------------\n\nTITLE: Grouped Beam Search with LLMPipeline in C++\nDESCRIPTION: This C++ snippet demonstrates text generation with grouped beam search using the LLMPipeline. It initializes the pipeline, creates a GenerationConfig object, sets parameters for beam groups, beams, and diversity penalty, and then calls the generate function with the configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string models_path = argv[1];\n    ov::genai::LLMPipeline pipe(models_path, \"CPU\");\n\n    ov::genai::GenerationConfig config;\n    config.max_new_tokens = 256;\n    config.num_beam_groups = 3;\n    config.num_beams = 15;\n    config.diversity_penalty = 1.0f;\n\n    std::cout << pipe.generate(\"The Sun is yellow because\", config);\n}\n```\n\n----------------------------------------\n\nTITLE: Text Generation with LLMPipeline in C++\nDESCRIPTION: This C++ code snippet demonstrates text generation using the LLMPipeline class from the openvino_genai library. It initializes the pipeline with a model path and device (CPU), then generates text based on a prompt with a maximum number of new tokens specified using `ov::genai::max_new_tokens`.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string models_path = argv[1];\n    ov::genai::LLMPipeline pipe(models_path, \"CPU\");\n    std::cout << pipe.generate(\"The Sun is yellow because\", ov::genai::max_new_tokens(256));\n}\n```\n\n----------------------------------------\n\nTITLE: Benchmark Original PyTorch Model (Bash)\nDESCRIPTION: This code downloads the PyTorch model and then benchmarks it using the `benchmark.py` script, specifying the PyTorch framework with the `-f pt` option.  The script measures performance with the original PyTorch model.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Download PyTorch Model\nhuggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir models/llama-2-7b-chat/pytorch\n# Benchmark with PyTorch Framework\npython benchmark.py -m models/llama-2-7b-chat/pytorch -n 2 -f pt\n```\n\n----------------------------------------\n\nTITLE: Timestamps Prediction (Python)\nDESCRIPTION: This Python code shows how to predict timestamps for the transcribed text. By passing `return_timestamps=True` to the `generate` method, the model returns timestamps for each chunk of text. The code then iterates through the chunks and prints the start and end timestamps along with the corresponding text.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/whisper_speech_recognition/README.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nraw_speech = read_wav(\"how_are_you_doing_today.wav\")\nresult = pipe.generate(raw_speech, return_timestamps=True)\n\nfor chunk in result.chunks:\n    print(f\"timestamps: [{chunk.start_ts:.2f}, {chunk.end_ts:.2f}] text: {chunk.text}\")\n# timestamps: [0.00, 2.00] text:  How are you doing today?\n```\n\n----------------------------------------\n\nTITLE: Chat Session Management in Python\nDESCRIPTION: This Python snippet demonstrates how to manage a chat session using `openvino_genai`. It initializes the `LLMPipeline`, sets the generation configuration, and uses `start_chat()` and `finish_chat()` to maintain conversation context and improve performance. The example takes user input as a prompt and generates an answer using the pipeline.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/chat-scenario.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(model_path, 'CPU')\n\nconfig = {'max_new_tokens': 100, 'num_beam_groups': 3, 'num_beams': 15, 'diversity_penalty': 1.5}\npipe.set_generation_config(config)\n\n# highlight-next-line\npipe.start_chat()\nwhile True:\n    try:\n        prompt = input('question:\\n')\n    except EOFError:\n        break\n    answer = pipe.generate(prompt)\n    print('answer:\\n')\n    print(answer)\n    print('\\n----------\\n')\n# highlight-next-line\npipe.finish_chat()\n```\n\n----------------------------------------\n\nTITLE: Controlling Padding in Python\nDESCRIPTION: Examples of how to control padding during the encoding process in Python. It shows how to pad to the longest sequence or to a specified max length. It includes equivalent HuggingFace tokenizer calls in the comments, and prints the resulting shape of the `input_ids` tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\n\ntokenizer = ov_genai.Tokenizer(models_path)\nprompts = [\"The Sun is yellow because\", \"The\"]\n\n# Since prompt is defenitely shorter than maximal length (which is taken from IR) will not affect shape.\n# Resulting shape is defined by length of the longest tokens sequence.\n# Equivalent of HuggingFace hf_tokenizer.encode(prompt, padding=\"longest\", truncation=True)\ntokens = tokenizer.encode([\"The Sun is yellow because\", \"The\"])\n# or is equivalent to\ntokens = tokenizer.encode([\"The Sun is yellow because\", \"The\"], pad_to_max_length=False)\nprint(tokens.input_ids.shape)\n# out_shape: [2, 6]\n\n# Resulting tokens tensor will be padded to 1024, sequences which exceed this length will be truncated.\n# Equivalent of HuggingFace hf_tokenizer.encode(prompt, padding=\"max_length\", truncation=True, max_length=1024)\ntokens = tokenizer.encode([\"The Sun is yellow because\", \n                           \"The\"\n                           \"The longest string ever\" * 2000], pad_to_max_length=True, max_length=1024)\nprint(tokens.input_ids.shape)\n# out_shape: [3, 1024]\n\n# For single string prompts truncation and padding are also applied.\ntokens = tokenizer.encode(\"The Sun is yellow because\", pad_to_max_length=True, max_length=128)\nprint(tokens.input_ids.shape)\n# out_shape: [1, 128]\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Generation Parameters in VLM Pipeline (Python)\nDESCRIPTION: This code snippet demonstrates how to configure generation parameters for a VLMPipeline in Python using the OpenVINO GenAI library. It initializes the pipeline, retrieves the default configuration, modifies parameters such as max_new_tokens, temperature, top_k, top_p, and repetition_penalty, and then generates text with the custom configuration. The required dependency is the openvino_genai library.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-processing/_sections/_usage_options/index.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\n\npipe = ov_genai.VLMPipeline(model_path, \"CPU\")\n\n# Get default configuration\nconfig = pipe.get_generation_config()\n\n# Modify parameters\nconfig.max_new_tokens = 100\nconfig.temperature = 0.7\nconfig.top_k = 50\nconfig.top_p = 0.9\nconfig.repetition_penalty = 1.2\n\n# Generate text with custom configuration\noutput = pipe.generate(prompt, images, config)\n```\n\n----------------------------------------\n\nTITLE: Streaming Function with LLMPipeline in OpenVINO GenAI\nDESCRIPTION: This code snippet demonstrates how to use a streaming function with OpenVINO GenAI's LLMPipeline to output words to the console immediately upon generation. The `streamer` function takes a subword as input, prints it to the console, and returns a flag indicating whether generation should continue. It requires the `openvino_genai` package.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/streaming.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\n\npipe = ov_genai.LLMPipeline(model_path, \"CPU\")\n\n# highlight-start\n# Create a streamer function\ndef streamer(subword):\n    print(subword, end='', flush=True)\n    # Return flag corresponds whether generation should be stopped.\n    return ov_genai.StreamingStatus.RUNNING\n# highlight-end\n\n# highlight-next-line\npipe.start_chat()\nwhile True:\n    try:\n        prompt = input('question:\\n')\n    except EOFError:\n        break\n    # highlight-next-line\n    pipe.generate(prompt, streamer=streamer, max_new_tokens=100)\n    print('\\n----------\\n')\n# highlight-next-line\npipe.finish_chat()\n```\n\n----------------------------------------\n\nTITLE: Timestamps Prediction (C++)\nDESCRIPTION: This C++ code demonstrates how to predict timestamps for the transcribed text using the Whisper pipeline. The `ov::genai::return_timestamps(true)` argument enables timestamps prediction, and the code iterates through the chunks of the result to print the start and end timestamps along with the corresponding text.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/README.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nov::genai::RawSpeechInput raw_speech = read_wav(\"how_are_you_doing_today.wav\");\nauto result = pipeline.generate(raw_speech, ov::genai::return_timestamps(true));\n\nstd::cout << std::setprecision(2);\nfor (auto& chunk : *result.chunks) {\n    std::cout << \"timestamps: [\" << chunk.start_ts << \", \" << chunk.end_ts << \"] text: \" << chunk.text << \"\\n\";\n}\n// timestamps: [0, 2] text:  How are you doing today?\n```\n\n----------------------------------------\n\nTITLE: Convert Stable Diffusion XL Model to OpenVINO\nDESCRIPTION: This code snippet shows how to download and convert the Stable Diffusion XL base 1.0 model from Hugging Face to the OpenVINO format using the Optimum CLI. It specifies the model name, output directory, weight format (int4), and enables trust remote code.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-generation/index.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n<OptimumCLI model='stabilityai/stable-diffusion-xl-base-1.0' outputDir='stable_diffusion_xl_base_1_0_ov' weightFormat='int4' trustRemoteCode />\n```\n\n----------------------------------------\n\nTITLE: Calculating Pure Inference Time in Python\nDESCRIPTION: This code snippet demonstrates how to calculate pure inference time, excluding tokenization and detokenization durations, using raw performance metrics in Python. It retrieves the generate duration, tokenization durations and detokenization durations from the raw metrics to compute pure inference duration.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\nimport numpy as np\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\nresult = pipe.generate([\"The Sun is yellow because\"], max_new_tokens=20)\nperf_metrics = result.perf_metrics\nprint(f'Generate duration: {perf_metrics.get_generate_duration().mean:.2f} ms')\n\nraw_metrics = perf_metrics.raw_metrics\ngenerate_duration = np.array(raw_metrics.generate_durations)\ntok_detok_duration = np.array(raw_metrics.tokenization_durations) - np.array(raw_metrics.detokenization_durations)\npure_inference_duration = np.sum(generate_duration - tok_detok_duration) / 1000 # in milliseconds\nprint(f'Pure Inference duration: {pure_inference_duration:.2f} ms')\n```\n\n----------------------------------------\n\nTITLE: Configuring Image Generation Parameters in C++\nDESCRIPTION: This C++ code snippet shows how to use the `Text2ImagePipeline` in the `openvino::genai` library to generate an image and configure parameters like width, height, number of images per prompt, number of inference steps, and guidance scale. It includes the `openvino/genai/image_generation/text2image_pipeline.hpp` header and a custom `imwrite.hpp` for saving the image. The generated image is saved as a BMP file.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-generation/_sections/_usage_options/index.mdx#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include \"openvino/genai/image_generation/text2image_pipeline.hpp\"\n#include \"imwrite.hpp\"\n\nint main(int argc, char* argv[]) {\n    const std::string models_path = argv[1], prompt = argv[2];\n\n    ov::genai::Text2ImagePipeline pipe(models_path, \"CPU\");\n    ov::Tensor image = pipe.generate(\n        prompt,\n        // highlight-start\n        ov::genai::width(512),\n        ov::genai::height(512),\n        ov::genai::num_images_per_prompt(1),\n        ov::genai::num_inference_steps(30),\n        ov::genai::guidance_scale(7.5f)\n        // highlight-end\n    );\n\n    imwrite(\"image.bmp\", image, true);\n}\n```\n\n----------------------------------------\n\nTITLE: Collect Ground Truth from Hugging Face Model\nDESCRIPTION: This command uses WWB to collect ground truth data from a baseline Hugging Face Transformer model. It specifies the base model, output file, and model type.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nwwb --base-model microsoft/Phi-3-mini-4k-instruct --gt-data gt.csv --model-type text --hf\n```\n\n----------------------------------------\n\nTITLE: Streaming Function with LLMPipeline in OpenVINO GenAI\nDESCRIPTION: This code snippet demonstrates how to use a streaming function in C++ with OpenVINO GenAI's LLMPipeline to output words immediately upon generation. The `streamer` lambda function takes a word as input, prints it to the console, and returns a flag indicating whether generation should continue. It includes necessary headers for LLMPipeline and I/O operations.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/streaming.mdx#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string prompt;\n\n    std::string model_path = argv[1];\n    ov::genai::LLMPipeline pipe(model_path, \"CPU\");\n\n    // highlight-start\n    // Create a streamer function\n    auto streamer = [](std::string word) {\n        std::cout << word << std::flush;\n        // Return flag corresponds whether generation should be stopped.\n        return ov::genai::StreamingStatus::RUNNING;\n    };\n    // highlight-end\n\n    // highlight-next-line\n    pipe.start_chat();\n    std::cout << \"question:\\n\";\n    while (std::getline(std::cin, prompt)) {\n        // highlight-next-line\n        pipe.generate(prompt, ov::genai::streamer(streamer), ov::genai::max_new_tokens(100));\n        std::cout << \"\\n----------\\n\"\n                    \"question:\\n\";\n    }\n    // highlight-next-line\n    pipe.finish_chat();\n}\n```\n\n----------------------------------------\n\nTITLE: Accumulating Performance Metrics in Python\nDESCRIPTION: This code demonstrates accumulating performance metrics by adding two `perf_metrics` objects in Python. The code initializes the LLMPipeline, generates text twice with different prompts, retrieves the performance metrics from each generation, adds them together, and then prints the combined performance metrics.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\nres_1 = pipe.generate([\"The Sun is yellow because\"], max_new_tokens=20)\nres_2 = pipe.generate([\"Why Sky is blue because\"], max_new_tokens=20)\nperf_metrics = res_1.perf_metrics + res_2.perf_metrics\n\nprint(f'Generate duration: {perf_metrics.get_generate_duration().mean:.2f}')\nprint(f'TTFT: {perf_metrics.get_ttft().mean:.2f} ms')\nprint(f'TPOT: {perf_metrics.get_tpot().mean:.2f} ms/token')\nprint(f'Throughput: {perf_metrics.get_throughput().mean:.2f} tokens/s')\n```\n\n----------------------------------------\n\nTITLE: Download PyTorch Model for torch.compile() (Bash)\nDESCRIPTION: This command downloads the PyTorch model locally, required before running the benchmarking script with `torch.compile()`.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir models/llama-2-7b-chat/pytorch\n```\n\n----------------------------------------\n\nTITLE: Running OpenVINO GenAI Model on CPU (Python)\nDESCRIPTION: This snippet demonstrates how to run an OpenVINO GenAI model on the CPU using Python. It relies on the `LLMPipeline` object and requires the converted model to be available in a folder. The `device` parameter is set to \"CPU\".\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/text-generation/_sections/_run_model/index.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n<CodeExamplePython device=\"CPU\" />\n```\n\n----------------------------------------\n\nTITLE: Transcription with Specified Language (C++)\nDESCRIPTION: This C++ code shows how to specify the language of the input audio for transcription using the Whisper pipeline. The `ov::genai::language` parameter allows to set the input language.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/README.md#_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\nov::genai::RawSpeechInput raw_speech = read_wav(\"how_are_you_doing_today.wav\");\nauto result = pipeline.generate(raw_speech, ov::genai::language(\"<|en|>\"));\n//  How are you doing today?\n\nraw_speech = read_wav(\"fr_sample.wav\");\nresult = pipeline.generate(raw_speech, ov::genai::language(\"<|fr|>\"));\n//  Il s'agit d'une entit trs complexe qui consiste...\n```\n\n----------------------------------------\n\nTITLE: Generating Text and Printing Performance Metrics in Python\nDESCRIPTION: This code snippet demonstrates how to generate text using the LLMPipeline in OpenVINO GenAI and access performance metrics like generate duration, TTFT, TPOT, and throughput. It initializes the pipeline, generates text, retrieves performance metrics, and prints the mean values of the metrics.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\nresult = pipe.generate([\"The Sun is yellow because\"], max_new_tokens=20)\nperf_metrics = result.perf_metrics\n\nprint(f'Generate duration: {perf_metrics.get_generate_duration().mean:.2f}')\nprint(f'TTFT: {perf_metrics.get_ttft().mean:.2f} ms')\nprint(f'TPOT: {perf_metrics.get_tpot().mean:.2f} ms/token')\nprint(f'Throughput: {perf_metrics.get_throughput().mean:.2f} tokens/s')\n```\n\n----------------------------------------\n\nTITLE: Run Greedy Causal LM\nDESCRIPTION: This command runs the `greedy_causal_lm.py` script, demonstrating basic text generation using a causal language model.  It requires the model directory and a prompt as input.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython greedy_causal_lm.py [-h] model_dir prompt\n```\n\n----------------------------------------\n\nTITLE: Text Generation with LLMPipeline in Python\nDESCRIPTION: This Python snippet demonstrates text generation using the LLMPipeline class from the openvino_genai library. It initializes an LLMPipeline with a model path and device (CPU), then generates text based on a prompt and a specified maximum number of new tokens.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\nprint(pipe.generate(\"The Sun is yellow because\", max_new_tokens=100))\n```\n\n----------------------------------------\n\nTITLE: Initial Prompt Usage with Whisper Pipeline in C++\nDESCRIPTION: This snippet demonstrates how to use an initial prompt with the WhisperPipeline in C++ to guide the model's output. It provides an initial prompt that influences the transcription result. The input is raw audio data, and the output is the transcribed text influenced by the prompt.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nint main() {\n    ov::genai::WhisperPipeline pipe(model_path, \"CPU\");\n\n    auto result = pipeline.generate(raw_speech);\n    // He has gone and gone for good answered Paul Icrom who...\n\n    result = pipeline.generate(raw_speech, ov::genai::initial_prompt(\"Polychrome\"));\n    // He has gone and gone for good answered Polychrome who...\n}\n```\n\n----------------------------------------\n\nTITLE: Build OpenVINO GenAI wheel\nDESCRIPTION: Builds a wheel package for OpenVINO GenAI using pip. The wheel is created in the `dist` directory. The `--extra-index-url` options specify additional package repositories to search for dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_24\n\nLANGUAGE: sh\nCODE:\n```\npython -m pip wheel . -w dist/ --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/pre-release --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n```\n\n----------------------------------------\n\nTITLE: Compare with Compressed Model on SQuAD Dataset\nDESCRIPTION: This command compares a compressed model against ground truth data generated from the SQuAD dataset. It specifies the target model, ground truth data, dataset, split, and dataset field.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nwwb --target-model /home/user/models/Llama_2_7b_chat_hf_int8 --gt-data llama_2_7b_squad_gt.csv --dataset squad --split validation[:32] --dataset-field question\n```\n\n----------------------------------------\n\nTITLE: Running the Image Generation Benchmark Script\nDESCRIPTION: This command executes the `benchmark_image_gen.py` script with various options. The options include specifying the pipeline type, model path, number of iterations, and device to run the pipeline on. The script then performs benchmarking and outputs performance metrics.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/image_generation/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark_image_gen.py [OPTIONS]\n```\n\n----------------------------------------\n\nTITLE: Loading Encrypted Model from Memory Buffer C++\nDESCRIPTION: This code snippet demonstrates how to load an LLM model and tokenizer directly from memory buffers, which is useful when the model is stored in an encrypted format and decrypted on the fly.  It uses the `ov::genai::Tokenizer` and `ov::genai::LLMPipeline` classes. It assumes the existence of a `decrypt_model` function to decrypt the model files.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\nauto [model_str, weights_tensor] = decrypt_model(models_path + \"/openvino_model.xml\", models_path + \"/openvino_model.bin\");\nov::genai::Tokenizer tokenizer(models_path);\nov::genai::LLMPipeline pipe(model_str, weights_tensor, tokenizer, device);\n```\n\n----------------------------------------\n\nTITLE: Run Beam Search Causal LM\nDESCRIPTION: This command executes the `beam_search_causal_lm.py` script, which uses beam search for text generation. It takes the model directory and one or more prompts as arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython beam_search_causal_lm.py model_dir prompt [prompts ...]\n```\n\n----------------------------------------\n\nTITLE: Text to Image Pipeline Callback in C++\nDESCRIPTION: This C++ code snippet demonstrates how to add a callback function to the `Text2ImagePipeline` to monitor and potentially interrupt the image generation process. The callback provides access to the intermediate latent tensor and the current step number.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/README.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::genai::Text2ImagePipeline pipe(models_path, device);\n\nauto callback = [&](size_t step, size_t num_steps, ov::Tensor& latent) -> bool {\n   std::cout << \"Image generation step: \" << step + 1 << \" / \" << num_steps << std::endl;\n   ov::Tensor img = pipe.decode(latent); // get intermediate image tensor\n   if (your_condition) // return true if you want to interrupt image generation\n      return true;\n   return false;\n};\n\nov::Tensor image = pipe.generate(prompt,\n   /* other generation properties */\n   ov::genai::callback(callback)\n);\n```\n\n----------------------------------------\n\nTITLE: Run WhisperPipeline on CPU with C++\nDESCRIPTION: This snippet uses the OpenVINO GenAI WhisperPipeline to perform speech recognition on the CPU. It relies on the `CodeExampleCPP` component, passing 'CPU' as the device parameter. It assumes that the component contains the necessary C++ code to construct the pipeline, load the model, and perform inference.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_run_model/index.mdx#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n<CodeExampleCPP device=\"CPU\" />\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements and Converting VLM Model with Optimum-cli\nDESCRIPTION: This shell script installs the necessary dependencies using pip and then uses the `optimum-cli` tool to export the openbmb/MiniCPM-V-2_6 model to the OpenVINO format. The `--upgrade-strategy eager` option ensures that `optimum-intel` is upgraded to the latest version, and `--trust-remote-code` is required for the specific model.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/visual_language_chat/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install --upgrade-strategy eager -r ../../export-requirements.txt\noptimum-cli export openvino --model openbmb/MiniCPM-V-2_6 --trust-remote-code MiniCPM-V-2_6\n```\n\n----------------------------------------\n\nTITLE: Accessing Raw Performance Metrics (Timestamps) in Python\nDESCRIPTION: This code snippet demonstrates accessing raw performance metrics, specifically timestamps of token generation, in Python. It initializes the LLMPipeline, generates text, retrieves performance metrics, accesses the `raw_metrics` field, and prints the generate duration, throughput and timestamps.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\nresult = pipe.generate([\"The Sun is yellow because\"], max_new_tokens=20)\nperf_metrics = result.perf_metrics\nraw_metrics = perf_metrics.raw_metrics\n\nprint(f'Generate duration: {perf_metrics.get_generate_duration().mean:.2f}')\nprint(f'Throughput: {perf_metrics.get_throughput().mean:.2f} tokens/s')\nprint(f'Timestamps: {\" ms, \".join(f\"{i:.2f}\" for i in raw_metrics.m_new_token_times)}')\n```\n\n----------------------------------------\n\nTITLE: Run LoRA Greedy Causal LM Sample (bash)\nDESCRIPTION: Executes the `lora_greedy_causal_lm` application, demonstrating greedy decoding with LoRA fine-tuned models. It requires the model directory, the adapter safetensors file, and a prompt as input.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n./lora_greedy_causal_lm <MODEL_DIR> <ADAPTER_SAFETENSORS_FILE> \"<PROMPT>\"\n```\n\n----------------------------------------\n\nTITLE: Whisper Pipeline Translation (Python)\nDESCRIPTION: This Python snippet demonstrates how to perform speech translation using the Whisper pipeline. By setting the `task` parameter to \"translate\" in the `generate` method, the model translates the source audio into English.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/whisper_speech_recognition/README.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nraw_speech = read_wav(\"fr_sample.wav\")\nresult = pipe.generate(raw_speech, task=\"translate\")\n# It is a very complex entity that consists...\n```\n\n----------------------------------------\n\nTITLE: Running Docker Image and Building Project\nDESCRIPTION: This snippet runs the openvino_llm:latest Docker image, mounts the current working directory, navigates to the project directory inside the container, and builds the project using CMake. It assumes Docker is installed and the openvino_llm:latest image is available.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ncd ../../../..\ndocker run -it -v `pwd`:/workspace/openvino.genai/ openvino_llm:latest\ncd /workspace/openvino.genai/\ncmake -DCMAKE_BUILD_TYPE=Release -S ./ -B ./build/ && cmake --build ./build/ -j\n```\n\n----------------------------------------\n\nTITLE: Run Multinomial Causal LM\nDESCRIPTION: This command runs the `multinomial_causal_lm.py` script, demonstrating text generation with multinomial sampling. The script takes the model directory and a prompt as command-line arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython multinomial_causal_lm.py model_dir prompt\n```\n\n----------------------------------------\n\nTITLE: Run Chat Sample in JavaScript\nDESCRIPTION: This command executes the `chat_sample.js` script using Node.js.  It requires a model directory as an argument, which specifies the location of the converted OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/js/text_generation/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nnode chat_sample.js model_dir\n```\n\n----------------------------------------\n\nTITLE: Running LLMs Benchmark Script with Options (bash)\nDESCRIPTION: This command executes the `benchmark_genai.py` script to benchmark an LLM model. It specifies options for the model path, prompt, number of warm-up iterations, maximum number of new tokens, number of iterations, and the device to run the model on. The script calculates and reports performance metrics.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark_genai.py [-m MODEL] [-p PROMPT] [-nw NUM_WARMUP] [-n NUM_ITER] [-mt MAX_NEW_TOKENS] [-d DEVICE]\n```\n\n----------------------------------------\n\nTITLE: Accumulating Performance Metrics in Python\nDESCRIPTION: This snippet shows how to accumulate performance metrics from multiple generate calls in Python by adding the perf_metrics objects together. This allows for calculating statistics across several generations using the openvino_genai library.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/performance-metrics.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\nres_1 = pipe.generate([\"The Sun is yellow because\"], max_new_tokens=20)\nres_2 = pipe.generate([\"Why Sky is blue because\"], max_new_tokens=20)\nperf_metrics = res_1.perf_metrics + res_2.perf_metrics\n\nprint(f'Generate duration: {perf_metrics.get_generate_duration().mean:.2f}')\nprint(f'TTFT: {perf_metrics.get_ttft().mean:.2f} ms')\nprint(f'TPOT: {perf_metrics.get_tpot().mean:.2f} ms/token')\nprint(f'Throughput: {perf_metrics.get_throughput().mean:.2f} tokens/s')\n```\n\n----------------------------------------\n\nTITLE: Run Chat Sample\nDESCRIPTION: This command executes the `chat_sample.py` script, which provides an interactive chat interface powered by OpenVINO. The script takes the model directory as a command-line argument.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython chat_sample.py model_dir\n```\n\n----------------------------------------\n\nTITLE: Initializing Tokenizer in C++\nDESCRIPTION: This snippet shows how to initialize the `ov::genai::Tokenizer` class in C++, similar to the Python example. It demonstrates initializing the tokenizer directly from a model path and retrieving it from an `LLMPipeline`. It requires including the `openvino/genai/llm_pipeline.hpp` header file. The `models_path` variable should contain the path to the tokenizer's model files.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/tokenization.mdx#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n\n// Initialize from the path\nauto tokenizer = ov::genai::Tokenizer(models_path);\n\n// Or get tokenizer instance from LLMPipeline\nauto pipe = ov::genai::LLMPipeline pipe(models_path, \"CPU\");\nauto tokenzier = pipe.get_tokenizer();\n```\n\n----------------------------------------\n\nTITLE: Getting Tokenizer Instance from LLMPipeline in C++\nDESCRIPTION: This C++ code snippet shows how to obtain an instance of the Tokenizer class from the LLMPipeline. The code initializes an LLMPipeline object and then retrieves the Tokenizer instance associated with it.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_20\n\nLANGUAGE: C++\nCODE:\n```\n// Get instance of Tokenizer from LLMPipeline.\nauto pipe = ov::genai::LLMPipeline pipe(models_path, \"CPU\");\nauto tokenzier = pipe.get_tokenizer();\n```\n\n----------------------------------------\n\nTITLE: Encoding without Special Tokens in C++\nDESCRIPTION: This snippet shows how to use the `encode()` method in C++ to tokenize a string without adding special tokens. It uses `ov::genai::add_special_tokens(false)` as an argument to the `encode()` function to achieve this.  The result is a tokenized sequence without any special tokens.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/tokenization.mdx#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nauto tokens = tokenizer.encode(\"The Sun is yellow because\", ov::genai::add_special_tokens(false));\n```\n\n----------------------------------------\n\nTITLE: Accessing Performance Metrics after Generation in Python\nDESCRIPTION: This snippet demonstrates how to generate text using the LLMPipeline and then access and print performance metrics such as generate duration, TTFT, TPOT, and throughput using the perf_metrics object. It requires the openvino_genai library.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/performance-metrics.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\nresult = pipe.generate([\"The Sun is yellow because\"], max_new_tokens=20)\nperf_metrics = result.perf_metrics\n\nprint(f'Generate duration: {perf_metrics.get_generate_duration().mean:.2f}')\nprint(f'TTFT: {perf_metrics.get_ttft().mean:.2f} ms')\nprint(f'TPOT: {perf_metrics.get_tpot().mean:.2f} ms/token')\nprint(f'Throughput: {perf_metrics.get_throughput().mean:.2f} tokens/s')\n```\n\n----------------------------------------\n\nTITLE: Run WhisperPipeline on GPU with C++\nDESCRIPTION: This snippet uses the OpenVINO GenAI WhisperPipeline to perform speech recognition on the GPU. It relies on the `CodeExampleCPP` component, passing 'GPU' as the device parameter. It assumes that the component contains the necessary C++ code to construct the pipeline, load the model, and perform inference.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_run_model/index.mdx#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n<CodeExampleCPP device=\"GPU\" />\n```\n\n----------------------------------------\n\nTITLE: Configure Generation Parameters with Whisper Pipeline in Python\nDESCRIPTION: This snippet shows how to configure generation parameters such as max_new_tokens, temperature, top_k, top_p, and repetition_penalty for the WhisperPipeline in Python. It gets the default configuration, modifies the parameters, and generates text using the custom configuration. The input is raw audio data, and the output is the generated text.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\n\npipe = ov_genai.WhisperPipeline(model_path, \"CPU\")\n\n# Get default configuration\nconfig = pipe.get_generation_config()\n\n# Modify parameters\nconfig.max_new_tokens = 100\nconfig.temperature = 0.7\nconfig.top_k = 50\nconfig.top_p = 0.9\nconfig.repetition_penalty = 1.2\n\n# Generate text with custom configuration\nresult = pipe.generate(raw_speech, config)\n```\n\n----------------------------------------\n\nTITLE: Install modelscope and optimum-intel\nDESCRIPTION: Installs the `modelscope` package and the `optimum-intel` package from GitHub. `modelscope` is needed to download models from ModelScope, while `optimum-intel` facilitates the conversion and optimization for OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/model-preparation/convert-to-openvino.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install modelscope\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install optimum-intel@git+https://github.com/huggingface/optimum-intel.git\n```\n\n----------------------------------------\n\nTITLE: Command Line Options for Image Generation Benchmarking\nDESCRIPTION: These are the command-line options for `benchmark_image_gen`. They allow users to specify parameters for benchmarking different image generation pipelines like text-to-image, image-to-image, and inpainting, control model paths, prompts, the number of iterations, target device, dimensions, steps, and more.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./benchmark_image_gen [OPTIONS]\n```\n\n----------------------------------------\n\nTITLE: Inpainting Pipeline with OpenVINO GenAI (C++)\nDESCRIPTION: This C++ code snippet demonstrates the usage of the `InpaintingPipeline` class from the OpenVINO GenAI library. It initializes the pipeline with paths to the required models, takes a prompt, an input image, and a mask image as input, generates an image using the pipeline, and saves the generated image to a BMP file.  The `load_image` function is expected to load images into `ov::Tensor` objects. Dependencies include the OpenVINO GenAI library, OpenCV for image loading/saving, and custom `load_image` and `imwrite` utilities.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-generation/_sections/_run_model/_inpainting_cpp.mdx#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/image_generation/inpainting_pipeline.hpp\"\n#include \"load_image.hpp\"\n#include \"imwrite.hpp\"\n\nint main(int argc, char* argv[]) {\n    const std::string models_path = argv[1], prompt = argv[2];\n\n    ov::Tensor input_image = utils::load_image(argv[3]);\n    ov::Tensor mask_image = utils::load_image(argv[4]);\n\n    ov::genai::InpaintingPipeline pipe(models_path, \"${props.device || 'CPU'}\");\n    ov::Tensor generated_image = pipe.generate(prompt, input_image, mask_image);\n\n    imwrite(\"image.bmp\", generated_image, true);\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Median Token-to-Token Duration in Python\nDESCRIPTION: This code snippet demonstrates how to calculate the median duration between generating each token using raw performance metrics in Python. It gets the timestamps for each new token generated, computes the differences between consecutive timestamps to get durations, and then calculates the median of those durations.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\nimport numpy as np\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\nresult = pipe.generate([\"The Sun is yellow because\"], max_new_tokens=20)\nperf_metrics = result.perf_metrics\nraw_metrics = perf_metrics.raw_metrics\n\nprint(f'Generate duration: {perf_metrics.get_generate_duration().mean:.2f}')\nprint(f'Throughput: {perf_metrics.get_throughput().mean:.2f} tokens/s')\ndurations = np.array(raw_metrics.m_new_token_times[1:]) - np.array(raw_metrics.m_new_token_times[:-1])\nprint(f'Median from token to token duration: {np.median(durations):.2f} ms')\n```\n\n----------------------------------------\n\nTITLE: Download OpenVINO Optimized LLM from Hugging Face\nDESCRIPTION: This command downloads a pre-converted OpenVINO IR model from a Hugging Face collection. It uses the `huggingface-cli` tool to download the specified model to a local directory.  Requires the `huggingface-hub` package.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/js/text_generation/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install huggingface-hub\nhuggingface-cli download <model> --local-dir <output_folder>\n```\n\n----------------------------------------\n\nTITLE: Whisper Pipeline Initialization and Inference (C++)\nDESCRIPTION: This C++ code initializes a WhisperPipeline with a specified model directory and device (CPU) and performs speech recognition on a WAV audio file. It reads the audio, normalizes it, and then runs inference using the `generate` method. The result is the transcribed text.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/README.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n#include \"openvino/genai/whisper_pipeline.hpp\"\n\nov::genai::WhisperPipeline pipeline(model_dir, \"CPU\");\n// Pipeline expects normalized audio with Sample Rate of 16kHz\nov::genai::RawSpeechInput raw_speech = read_wav(\"how_are_you_doing_today.wav\");\nauto result = pipeline.generate(raw_speech);\n//  How are you doing today?\n```\n\n----------------------------------------\n\nTITLE: Running VLM Benchmark\nDESCRIPTION: This shell script demonstrates how to run the `benchmark_vlm` application to measure the performance of a visual language model.  It showcases the command-line options for specifying the model path, image path, prompt, and iteration parameters. The output provides performance metrics such as load time, generate time, tokenization time, and throughput.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/visual_language_chat/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_vlm [OPTIONS]\n```\n\n----------------------------------------\n\nTITLE: Export Text-to-Image Model to OpenVINO (Quantized)\nDESCRIPTION: This command exports a text-to-image model to the OpenVINO format with 8-bit quantized weights using Optimum-Intel.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\noptimum-cli export openvino -m SimianLuo/LCM_Dreamshaper_v7 --weight-format int8 sd-lcm-int8\n```\n\n----------------------------------------\n\nTITLE: Whisper Pipeline Transcription (Python)\nDESCRIPTION: This Python code demonstrates using the Whisper pipeline for transcription. It reads audio from a WAV file and generates the corresponding text. The pipeline automatically detects the language of the source audio.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/whisper_speech_recognition/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nraw_speech = read_wav('how_are_you_doing_today.wav')\nresult = pipe.generate(raw_speech)\n#  How are you doing today?\n\nraw_speech = read_wav('fr_sample.wav')\nresult = pipe.generate(raw_speech)\n#  Il s'agit d'une entit trs complexe qui consiste...\n```\n\n----------------------------------------\n\nTITLE: Set OpenVINO environment variables manually (Linux)\nDESCRIPTION: Sets the required environment variables for OpenVINO manually on Linux systems. `OpenVINO_DIR` points to the runtime directory, `PYTHONPATH` includes the OpenVINO Python bindings, and `LD_LIBRARY_PATH` includes the necessary shared libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nexport OpenVINO_DIR=<INSTALL_DIR>/runtime\nexport PYTHONPATH=<INSTALL_DIR>/python:./build/:$PYTHONPATH\nexport LD_LIBRARY_PATH=<INSTALL_DIR>/runtime/lib/intel64:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Transcription with Automatic/Explicit Language Detection in Python\nDESCRIPTION: This snippet shows how to perform transcription with the WhisperPipeline in Python, using both automatic language detection and explicitly specifying the language. It reads audio files and generates text, either automatically detecting the language or using a provided language code. The input is raw audio data, and the output is the transcribed text.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipe = ov_genai.WhisperPipeline(model_path, \"CPU\")\n\n# Automatic language detection\nraw_speech = read_wav(\"speech_sample.wav\")\nresult = pipe.generate(raw_speech)\n\n# Explicitly specify language (English)\nresult = pipe.generate(raw_speech, language=\"<|en|>\")\n\n# French speech sample\nraw_speech = read_wav(\"french_sample.wav\")\nresult = pipe.generate(raw_speech, language=\"<|fr|>\")\n```\n\n----------------------------------------\n\nTITLE: Converting and quantizing Whisper model\nDESCRIPTION: This script demonstrates how to convert and quantize the Whisper model from the Hugging Face library using optimum-cli. It exports the model to OpenVINO format and applies int8 static quantization.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\n#Download and convert to OpenVINO whisper-base model\noptimum-cli export openvino --model openai/whisper-base whisper-base\n\n#Download, convert and apply int8 static quantization to whisper-base model\noptimum-cli export openvino --model openai/whisper-base \\\n--quant-mode int8 --dataset librispeech --num-samples 32 whisper-base-int8\n```\n\n----------------------------------------\n\nTITLE: Inpainting generation in C++\nDESCRIPTION: This code snippet demonstrates how to use the Inpainting pipeline in C++ to fill in missing parts of an image based on a text prompt, an input image, and a mask image. It loads a pre-trained model, initializes the pipeline, processes the images using custom load_image and imwrite functions, and saves the generated image. The code requires the installation of a C++ compatible OpenVINO GenAI package.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/image_generation/inpainting_pipeline.hpp\"\n#include \"load_image.hpp\"\n#include \"imwrite.hpp\"\n\nint main(int argc, char* argv[]) {\n   const std::string models_path = argv[1], prompt = argv[2];\n   const std::string device = \"CPU\";  // GPU can be used as well\n\n   ov::Tensor image = utils::load_image(argv[3]);\n   ov::Tensor mask_image = utils::load_image(argv[4]);\n\n   ov::genai::InpaintingPipeline pipe(models_path, device);\n   ov::Tensor generated_image = pipe.generate(prompt, image, mask_image);\n\n   imwrite(\"image.bmp\", generated_image, true);\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Special Tokens During Encoding in Python\nDESCRIPTION: Demonstrates how to disable adding special tokens during the encoding process using the `add_special_tokens` option in Python.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntokens = tokenizer.encode(\"The Sun is yellow because\", add_special_tokens=False)\n```\n\n----------------------------------------\n\nTITLE: Install Optimum-Intel and Export OpenVINO Model (Shell)\nDESCRIPTION: This shell command installs `optimum-intel` and exports an OpenVINO model from the `openai/whisper-base` model. The `--upgrade-strategy eager` option ensures that `optimum-intel` is upgraded to the latest version. The `--trust-remote-code` option is needed because the model is loaded from a remote repository.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/whisper_speech_recognition/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install --upgrade-strategy eager -r ../../export-requirements.txt\noptimum-cli export openvino --trust-remote-code --model openai/whisper-base whisper-base\n```\n\n----------------------------------------\n\nTITLE: Run Encrypted Model Causal LM\nDESCRIPTION: This command executes the `encrypted_model_causal_lm.py` script, which demonstrates loading a model from an encrypted memory buffer. The script takes the model directory and a prompt as arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython encrypted_model_causal_lm.py model_dir prompt\n```\n\n----------------------------------------\n\nTITLE: Initializing LLMPipeline and Tokenizer in Python\nDESCRIPTION: Initializes the LLMPipeline and retrieves the tokenizer. The tokenizer is then used to encode text prompts.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\ntokenizer = pipe.get_tokenizer()\n```\n\n----------------------------------------\n\nTITLE: Enabling Unicode UTF-8 Support on Windows\nDESCRIPTION: This describes the steps to enable Unicode UTF-8 support for Windows cmd to resolve `UnicodeEncodeError`. It involves changing system locale settings and rebooting the system.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n1. Enable Unicode characters for Windows cmd - open `Region` settings from `Control panel`. `Administrative`->`Change system locale`->`Beta: Use Unicode UTF-8 for worldwide language support`->`OK`. Reboot.\n2. Enable UTF-8 mode by setting environment variable `PYTHONIOENCODING=\"utf8\"`.\n```\n\n----------------------------------------\n\nTITLE: Controlling Padding in Python\nDESCRIPTION: This Python snippet demonstrates how to control padding during tokenization using the `pad_to_max_length` and `max_length` parameters of the `encode()` method. It shows how to pad to the longest sequence in a batch or to a specified `max_length`. It also illustrates the effect of these parameters on the shape of the resulting `input_ids` tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/tokenization.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\n\ntokenizer = ov_genai.Tokenizer(models_path)\nprompts = [\"The Sun is yellow because\", \"The\"]\n\n# Since prompt is defenitely shorter than maximal length (which is taken from IR) will not affect shape.\n# Resulting shape is defined by length of the longest tokens sequence.\n# Equivalent of HuggingFace hf_tokenizer.encode(prompt, padding=\"longest\", truncation=True)\ntokens = tokenizer.encode([\"The Sun is yellow because\", \"The\"])\n# or is equivalent to\ntokens = tokenizer.encode([\"The Sun is yellow because\", \"The\"], pad_to_max_length=False)\nprint(tokens.input_ids.shape)\n# out_shape: [2, 6]\n\n# Resulting tokens tensor will be padded to 1024, sequences which exceed this length will be truncated.\n# Equivalent of HuggingFace hf_tokenizer.encode(prompt, padding=\"max_length\", truncation=True, max_length=1024)\ntokens = tokenizer.encode([\"The Sun is yellow because\",\n                                  \"The\"\n                                  \"The longest string ever\" * 2000], pad_to_max_length=True, max_length=1024)\nprint(tokens.input_ids.shape)\n# out_shape: [3, 1024]\n\n# For single string prompts truncation and padding are also applied.\ntokens = tokenizer.encode(\"The Sun is yellow because\", pad_to_max_length=True, max_length=128)\nprint(tokens.input_ids.shape)\n# out_shape: [1, 128]\n```\n\n----------------------------------------\n\nTITLE: TextEvaluator with Custom Prompts\nDESCRIPTION: This code snippet shows how to use a custom list of prompts (e.g., from a dataset) with the `TextEvaluator` to compare language models.  It loads a dataset using the `datasets` library and passes the text data to the `evaluator.score` method.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nval = load_dataset(\"lambada\", split=\"validation[20:40]\")\nprompts = val[\"text\"]\n...\nmetrics_per_prompt, metrics = evaluator.score(optimized_model, test_data=prompts)\n```\n\n----------------------------------------\n\nTITLE: Disabling CUDA for Qwen-7B-Chat-Int4 Conversion (Python)\nDESCRIPTION: This snippet addresses the AssertionError: \"Torch not compiled with CUDA enabled\" when converting Qwen-7B-Chat-Int4 to OpenVINO IR files. The solution involves setting SUPPORT_CUDA to False in modeling_qwen.py, which effectively disables CUDA support during the conversion process and allows it to proceed on a CPU-only environment.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/doc/NOTES.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n-SUPPORT_CUDA = torch.cuda.is_available()\n+SUPPORT_CUDA = False\n SUPPORT_BF16 = SUPPORT_CUDA and torch.cuda.is_bf16_supported()\n```\n\n----------------------------------------\n\nTITLE: Converting Image Generation Model with Optimum CLI\nDESCRIPTION: These commands use the Optimum CLI to convert the dreamlike-anime-1.0 model to OpenVINO format. The first command converts the model to FP16 precision. The second command uses INT8 hybrid quantization with `conceptual_captions` dataset to further optimize and reduce inference latency.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n#Download and convert to OpenVINO dreamlike-anime-1.0 model\noptimum-cli export openvino --model dreamlike-art/dreamlike-anime-1.0 --weight-format fp16 dreamlike_anime_1_0_ov/FP16\n\n#You can also use INT8 hybrid quantization to further optimize the model and reduce inference latency\noptimum-cli export openvino --model dreamlike-art/dreamlike-anime-1.0 --weight-format int8 --dataset conceptual_captions dreamlike_anime_1_0_ov/INT8\n```\n\n----------------------------------------\n\nTITLE: Inpainting generation in Python\nDESCRIPTION: This code snippet demonstrates how to use the Inpainting pipeline in Python to fill in missing parts of an image based on a text prompt, an input image, and a mask image. It loads a pre-trained model, initializes the pipeline, processes the images, and saves the generated image.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nfrom PIL import Image\nimport openvino_genai\nimport openvino as ov\n\ndef read_image(path: str) -> openvino.Tensor:\n    pic = Image.open(path).convert(\"RGB\")\n    image_data = np.array(pic)[None]\n    return openvino.Tensor(image_data)\n\ndevice = 'CPU'  # GPU can be used as well\npipe = openvino_genai.InpaintingPipeline(args.model_dir, device)\n\nimage = read_image(\"image.jpg\")\nmask_image = read_image(\"mask.jpg\")\n\nimage_tensor = pipe.generate(\n    \"Face of a yellow cat, high resolution, sitting on a park bench\",\n    image=image,\n    mask_image=mask_image\n)\n\nimage = Image.fromarray(image_tensor.data[0])\nimage.save(\"image.bmp\")\n```\n\n----------------------------------------\n\nTITLE: Set OpenVINO Log Level on Linux/macOS\nDESCRIPTION: This snippet demonstrates how to set the `OPENVINO_LOG_LEVEL` environment variable to 3 (INFO) on Linux and macOS systems. This will enable INFO level logging, along with ERR and WARNING levels.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/debug-logging.mdx#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport OPENVINO_LOG_LEVEL=3\n```\n\n----------------------------------------\n\nTITLE: Convert Model to OpenVINO IR Format (Bash)\nDESCRIPTION: This command uses the `optimum-cli` tool to export a Hugging Face model to the OpenVINO IR format. It requires specifying the model ID, weight format (precision), and output directory. The tool converts the PyTorch model to an OpenVINO compatible format for optimized inference.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\noptimum-cli export openvino --model <MODEL_ID> --weight-format <PRECISION> <OUTPUT_DIR>\n\noptimum-cli export openvino -h # For detailed information\n```\n\n----------------------------------------\n\nTITLE: Running LLMs benchmarking sample\nDESCRIPTION: This command executes the benchmark sample, allowing for customization of the model path, prompt, number of warmup iterations, maximum new tokens, number of iterations, and the device for execution.  The `benchmark_gena_c` executable is expected to be in the current working directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/c/text_generation/README.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n./benchmark_gena_c  [-m MODEL] [-p PROMPT] [-nw NUM_WARMUP] [-n NUM_ITER] [-mt MAX_NEW_TOKENS] [-d DEVICE]\n```\n\n----------------------------------------\n\nTITLE: VLM Benchmark Usage Example\nDESCRIPTION: This example shows how to use the `benchmark_vlm` tool with specific parameters to benchmark a VLM model. The `-m` option specifies the model directory, `-i` the image path, and `-n` the number of iterations.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/visual_language_chat/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_vlm -m miniCPM-V-2_6 -i 319483352-d5fbbd1a-d484-415c-88cb-9986625b7b11.jpg -n 3\n```\n\n----------------------------------------\n\nTITLE: Modify Generation Config (Beam Search) in C++\nDESCRIPTION: Demonstrates how to modify beam search generation parameters like max_new_tokens, num_beams, num_beam_groups, and diversity_penalty in OpenVINO GenAI using C++. The example gets the default configuration, modifies beam search specific parameters, and then generates text with those configurations. Requires the `ov::genai::LLMPipeline` class and accepts model_path as command line argument.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/text-generation/_sections/_usage_options/_generation_parameters.mdx#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nint main(int argc, char* argv[]) {\n    std::string model_path = argv[1];\n    ov::genai::LLMPipeline pipe(model_path, \"CPU\");\n\n    // Get default generation config\n    ov::genai::GenerationConfig config = pipe.get_generation_config();\n\n    // Modify parameters\n    config.max_new_tokens = 256;\n    config.num_beams = 15;\n    config.num_beam_groups = 3;\n    config.diversity_penalty = 1.0f;\n\n    // Generate text with custom configuration\n    auto output = pipe.generate(prompt, config);\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Tokenizer in Python\nDESCRIPTION: This snippet demonstrates how to initialize the `ov_genai.Tokenizer` class in Python, both from a model path and by retrieving it from an `LLMPipeline` object. The `models_path` variable should point to the directory containing the tokenizer's model files. The `LLMPipeline` is initialized with the model path and the device to run on (e.g., 'CPU').\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/tokenization.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\n\n# Initialize from the path\ntokenizer = ov_genai.Tokenizer(models_path)\n\n# Or get tokenizer instance from LLMPipeline\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\ntokenizer = pipe.get_tokenizer()\n```\n\n----------------------------------------\n\nTITLE: Overriding eos_token_id for phi3_v models in Python\nDESCRIPTION: This code snippet demonstrates how to override the default `eos_token_id` with the one from a tokenizer for phi3_v models, which might be necessary due to inconsistent model configurations. It uses the `generation_config.set_eos_token_id()` method to set the end-of-sequence token ID to the value returned by `pipe.get_tokenizer().get_eos_token_id()`. This ensures correct sequence termination during generation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/supported-models/index.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ngeneration_config.set_eos_token_id(pipe.get_tokenizer().get_eos_token_id())\n```\n\n----------------------------------------\n\nTITLE: Run Whisper Samples Tests with pytest\nDESCRIPTION: This command executes the Whisper samples tests using `pytest`. The `-m whisper` flag specifies that only tests marked with the `whisper` marker should be executed.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\npython -m pytest tests/python_tests/samples -m whisper\n```\n\n----------------------------------------\n\nTITLE: Create Python Module using pybind11 in CMake\nDESCRIPTION: This snippet compiles the C++ source files into a Python module using the `pybind11_add_module` function. It links the module against the `openvino::genai` library and sets include directories for necessary headers.  It also configures output directories and copies necessary __init__.py files for Python package creation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/python/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB python_sources \"${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\")\n\nset(TARGET_NAME py_openvino_genai)\npybind11_add_module(${TARGET_NAME} ${python_sources})\n\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::genai)\ntarget_include_directories(${TARGET_NAME} PRIVATE \"${OpenVINOGenAI_SOURCE_DIR}/src/cpp/src\") # for tokenizers_path.hpp\nset_target_properties(${TARGET_NAME} PROPERTIES\n    ARCHIVE_OUTPUT_DIRECTORY \"$<1:${CMAKE_BINARY_DIR}/openvino_genai/>\"\n    LIBRARY_OUTPUT_DIRECTORY \"$<1:${CMAKE_BINARY_DIR}/openvino_genai/>\"\n)\nfile(COPY \"${CMAKE_CURRENT_SOURCE_DIR}/openvino_genai/__init__.py\"\n          \"${CMAKE_CURRENT_SOURCE_DIR}/openvino_genai/__init__.pyi\"\n          \"${CMAKE_CURRENT_SOURCE_DIR}/openvino_genai/py_openvino_genai.pyi\"\n     DESTINATION \"${CMAKE_BINARY_DIR}/openvino_genai/\")\n```\n\n----------------------------------------\n\nTITLE: Transcription with Automatic/Explicit Language Detection in C++\nDESCRIPTION: This snippet shows how to perform transcription with the WhisperPipeline in C++, using both automatic language detection and explicitly specifying the language. It reads audio files and generates text, either automatically detecting the language or using a provided language code.  The input is raw audio data, and the output is the transcribed text.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nint main() {\n    ov::genai::WhisperPipeline pipe(model_path, \"CPU\");\n\n    // Automatic language detection\n    auto result = pipe.generate(raw_speech);\n\n    // Explicitly specify language (English)\n    result = pipe.generate(raw_speech, ov::genai::language(\"<|en|>\"));\n\n    // French speech sample\n    raw_speech = utils::audio::read_wav(\"french_sample.wav\");\n    result = pipe.generate(raw_speech, ov::genai::language(\"<|fr|>\"));\n}\n```\n\n----------------------------------------\n\nTITLE: Link Libraries\nDESCRIPTION: Links the 'openvino_genai_c' library with other OpenVINO components. It publicly links to 'openvino::runtime::c' and privately links to 'openvino::genai'.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/c/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino::runtime::c PRIVATE openvino::genai)\n```\n\n----------------------------------------\n\nTITLE: Measure Similarity for Optimum-OpenVINO Inference\nDESCRIPTION: This command uses WWB to measure the similarity metric for an Optimum-OpenVINO inference backend.  It specifies the target model, ground truth data, and model type.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nwwb --target-model phi-3-openvino --gt-data gt.csv --model-type text\n```\n\n----------------------------------------\n\nTITLE: Creating LoRA Text2Image Executable with CMake\nDESCRIPTION: This snippet creates an executable named 'lora_text2image' from the source files 'lora_text2image.cpp' and 'imwrite.cpp'. It sets include directories and links the executable against 'openvino::genai' and 'indicators::indicators'.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(lora_text2image lora_text2image.cpp imwrite.cpp)\n\ntarget_include_directories(lora_text2image PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}\")\ntarget_link_libraries(lora_text2image PRIVATE openvino::genai indicators::indicators)\n\nset_target_properties(lora_text2image PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n\ninstall(TARGETS lora_text2image\n        RUNTIME DESTINATION samples_bin/\n        COMPONENT samples_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Calculating Median Token Duration in Python\nDESCRIPTION: This Python snippet calculates the median duration between generated tokens using the raw timestamps available in the raw_metrics object. It imports the openvino_genai and numpy libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/performance-metrics.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\nimport numpy as np\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\nresult = pipe.generate([\"The Sun is yellow because\"], max_new_tokens=20)\nperf_metrics = result.perf_metrics\nraw_metrics = perf_metrics.raw_metrics\n\nprint(f'Generate duration: {perf_metrics.get_generate_duration().mean:.2f}')\nprint(f'Throughput: {perf_metrics.get_throughput().mean:.2f} tokens/s')\ndurations = np.array(raw_metrics.m_new_token_times[1:]) - np.array(raw_metrics.m_new_token_times[:-1])\nprint(f'Median from token to token duration: {np.median(durations):.2f} ms')\n```\n\n----------------------------------------\n\nTITLE: Run Recorder Script (Shell)\nDESCRIPTION: This shell command executes the `recorder.py` script. The script records 5 seconds of audio from the microphone. `PyAudio` dependency must be installed before running.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/whisper_speech_recognition/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npython recorder.py\n```\n\n----------------------------------------\n\nTITLE: Converting VLM to OpenVINO Format\nDESCRIPTION: This snippet demonstrates how to download and convert a VLM from Hugging Face to the OpenVINO format using Optimum CLI. The model 'openbmb/MiniCPM-V-2_6' is converted to OpenVINO format with int4 weights and saved to the 'MiniCPM_V_2_6_ov' directory.  `trustRemoteCode` is enabled to allow execution of custom code within the model.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-processing/index.mdx#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\noptimum-cli export openvino --model openbmb/MiniCPM-V-2_6 --output MiniCPM_V_2_6_ov --weight-format int4 --trust-remote-code\n```\n\n----------------------------------------\n\nTITLE: Speech-to-Text with OpenVINO GenAI Whisper Pipeline (Python)\nDESCRIPTION: This code snippet demonstrates how to perform speech-to-text using the OpenVINO GenAI library's WhisperPipeline. It reads a WAV file using librosa, initializes the pipeline with a model path and device, and then generates text from the audio input. The device can be specified via the props object, defaulting to CPU if not provided.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_run_model/_code_example_python.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\nimport librosa\n\ndef read_wav(filepath):\n    raw_speech, samplerate = librosa.load(filepath, sr=16000)\n    return raw_speech.tolist()\n\nraw_speech = read_wav('sample.wav')\n\npipe = ov_genai.WhisperPipeline(model_path, \"${props.device || 'CPU'}\")\nresult = pipe.generate(raw_speech, max_new_tokens=100)\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Collect Visual Language References\nDESCRIPTION: This command collects reference images and saves the mapping in a CSV file using WWB for a visual language model. It specifies the base model, ground truth data, model type, and the `--hf` flag.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nwwb --base-model llava-hf/llava-v1.6-mistral-7b-hf --gt-data llava_test/gt.csv --model-type visual-text --hf\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties\nDESCRIPTION: This snippet sets the `INSTALL_RPATH_USE_LINK_PATH` property to `ON` for the target. This ensures that the runtime path (RPATH) is correctly set on macOS systems, especially when System Integrity Protection (SIP) is enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/continuous_batching/benchmark/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n```\n\n----------------------------------------\n\nTITLE: Fetch pybind11 Dependency in CMake\nDESCRIPTION: This snippet uses CMake's FetchContent module to download and include the pybind11 library, which is used for creating Python bindings for C++ code. The URL and SHA256 hash are specified to ensure the correct version is downloaded.  It also sets PYBIND11_FINDPYTHON to ON to use FindPython3.cmake\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/python/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(FetchContent)\n\nFetchContent_Declare(\n    pybind11\n    URL https://github.com/pybind/pybind11/archive/refs/tags/v2.13.5.tar.gz\n    URL_HASH SHA256=b1e209c42b3a9ed74da3e0b25a4f4cd478d89d5efbb48f04b277df427faf6252\n)\nFetchContent_GetProperties(pybind11)\n# search for FindPython3.cmake instead of legacy modules\nset(PYBIND11_FINDPYTHON ON)\n\nif(NOT pybind11_POPULATED)\n    FetchContent_Populate(pybind11)\n    add_subdirectory(${pybind11_SOURCE_DIR} ${pybind11_BINARY_DIR})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Transcription with Specified Language (Python)\nDESCRIPTION: This Python code showcases transcription with a specified language. The `language` parameter is passed to the `generate` method to explicitly set the language of the source audio. This ensures that the model correctly transcribes the audio based on the provided language.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/whisper_speech_recognition/README.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nraw_speech = read_wav(\"how_are_you_doing_today.wav\")\nresult = pipe.generate(raw_speech, language=\"<|en|>\")\n#  How are you doing today?\n\nraw_speech = read_wav(\"fr_sample.wav\")\nresult = pipe.generate(raw_speech, language=\"<|fr|>\")\n#  Il s'agit d'une entit trs complexe qui consiste...\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion Prompt Example\nDESCRIPTION: This JSON snippet illustrates a prompt file for stable diffusion image generation.  It includes parameters like 'steps', 'width', 'height', 'guidance_scale', and 'prompt' to control the image generation process. These parameters define the image resolution, inference steps, and content of the generated image.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/doc/PROMPT.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"steps\":\"10\", \"width\":\"256\", \"height\":\"256\", \"guidance_scale\":\"1.0\", \"prompt\": \"side profile centered painted portrait, Gandhi rolling a blunt, Gloomhaven, matte painting concept art, art nouveau, 8K HD Resolution, beautifully background\"}\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Configuration JSON (JSON)\nDESCRIPTION: This JSON snippet represents a configuration file for OpenVINO. It sets the `INFERENCE_NUM_THREADS` property to control the number of threads used during inference.  The placeholder `<NUMBER>` should be replaced with the total number of physical cores in 2 sockets for multi-socket platforms.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/README.md#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"INFERENCE_NUM_THREADS\": <NUMBER>\n} \n```\n\n----------------------------------------\n\nTITLE: Building Benchmark Executable in CMake\nDESCRIPTION: This snippet creates an executable named `benchmark_genai` from `benchmark_genai.cpp`, links it against `openvino::genai` and `cxxopts::cxxopts` libraries, sets the install RPATH property for macOS, and defines installation rules. It installs the executable into the `samples_bin` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(benchmark_genai benchmark_genai.cpp)\ntarget_link_libraries(benchmark_genai PRIVATE openvino::genai cxxopts::cxxopts)\nset_target_properties(benchmark_genai PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n\ninstall(TARGETS benchmark_genai\n        RUNTIME DESTINATION samples_bin/\n        COMPONENT samples_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with OpenVINO C++\nDESCRIPTION: This C++ code snippet demonstrates how to use the OpenVINO GenAI Text2ImagePipeline to generate an image from a text prompt. It initializes the pipeline with the specified model path and device (CPU by default if no device is given through props). The generated image, represented as an ov::Tensor, is then saved to a BMP file using the imwrite function. The code expects the models path and prompt as command line arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-generation/_sections/_run_model/_text2image_cpp.mdx#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/image_generation/text2image_pipeline.hpp\"\n#include \"imwrite.hpp\"\n\nint main(int argc, char* argv[]) {\n    const std::string models_path = argv[1], prompt = argv[2];\n\n    ov::genai::Text2ImagePipeline pipe(models_path, \"${props.device || 'CPU'}\");\n    ov::Tensor image = pipe.generate(prompt);\n\n    imwrite(\"image.bmp\", image, true);\n}\n```\n\n----------------------------------------\n\nTITLE: Phi3VForCausalLM Configuration Override\nDESCRIPTION: This snippet demonstrates how to override the default `eos_token_id` for Phi3VForCausalLM models, as their configurations are inconsistent. It retrieves the `eos_token_id` from the tokenizer and sets it in the generation configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/SUPPORTED_MODELS.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ngeneration_config.set_eos_token_id(pipe.get_tokenizer().get_eos_token_id())\n```\n\n----------------------------------------\n\nTITLE: Run Beam Search Causal LM Sample in JavaScript\nDESCRIPTION: This command executes the `beam_search_causal_lm.js` script using Node.js. It takes a model directory and one or more prompts as arguments, utilizing beam search for text generation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/js/text_generation/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nnode beam_search_causal_lm.js model_dir prompt [prompts ...]\n```\n\n----------------------------------------\n\nTITLE: Speech Translation (C++)\nDESCRIPTION: This C++ code translates speech from a source language (French in this example) to English using the Whisper pipeline. The `ov::genai::task(\"translate\")` parameter configures the pipeline for translation mode.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/README.md#_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\nov::genai::RawSpeechInput raw_speech = read_wav(\"fr_sample.wav\");\nauto result = pipeline.generate(raw_speech, ov::genai::task(\"translate\"));\n//  It is a very complex entity that consists...\n```\n\n----------------------------------------\n\nTITLE: Initializing Tokenizer from Path in C++\nDESCRIPTION: This C++ code snippet demonstrates initializing the Tokenizer class from a path. It creates an instance of the Tokenizer using the models_path.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\n// Initialize from the path\n#include \"openvino/genai/llm_pipeline.hpp\"\nauto tokenizer = ov::genai::Tokenizer(models_path);\n```\n\n----------------------------------------\n\nTITLE: Installing Hugging Face Hub\nDESCRIPTION: This command installs the `huggingface_hub` package, which is required to download models from Hugging Face.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/model-preparation/download-openvino-models.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install huggingface_hub\n```\n\n----------------------------------------\n\nTITLE: Disabling Special Tokens During Encoding in C++\nDESCRIPTION: Demonstrates how to disable adding special tokens during the encoding process using the `add_special_tokens` option in C++.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\nauto tokens = tokenizer.encode(\"The Sun is yellow because\", ov::genai::add_special_tokens(false));\n```\n\n----------------------------------------\n\nTITLE: TextEvaluator API Example\nDESCRIPTION: This code demonstrates how to use the `TextEvaluator` class from the `whowhatbench` library to score an optimized language model against a baseline model. It loads models and tokenizers, calculates metrics, and identifies worst examples.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport whowhatbench\n\nmodel_id = \"facebook/opt-1.3b\"\nbase_small = AutoModelForCausalLM.from_pretrained(model_id)\noptimized_model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nevaluator = whowhatbench.TextEvaluator(base_model=base_small, tokenizer=tokenizer)\nmetrics_per_prompt, metrics = evaluator.score(optimized_model)\n\nmetric_of_interest = \"similarity\"\nprint(metric_of_interest, \": \", metrics[\"similarity\"][0])\n\nworst_examples = evaluator.worst_examples(top_k=5, metric=metric_of_interest)\nprint(\"Metric: \", metric_of_interest)\nfor e in worst_examples:\n    print(\"\\t=========================\")\n    print(\"\\tPrompt: \", e[\"prompt\"])\n    print(\"\\tBaseline Model:\\n \", \"\\t\" + e[\"source_model\"])\n    print(\"\\tOptimized Model:\\n \", \"\\t\" + e[\"optimized_model\"])\n```\n\n----------------------------------------\n\nTITLE: Building jinja2cpp Dependency using FetchContent\nDESCRIPTION: This function uses CMake's FetchContent module to download, configure and build the jinja2cpp library. It sets specific build options, such as disabling shared libraries and setting the C++ standard. It defines compilation flags and adds the jinja2cpp library as a subdirectory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/cpp/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ov_genai_build_jinja2cpp)\n    FetchContent_Declare(jinja2cpp\n        URL https://github.com/jinja2cpp/Jinja2Cpp/archive/86dfb939b5c2beb7fabddae2df386be4e7fb9507.tar.gz\n        URL_HASH SHA256=c222e2407316a573561bd74dfd1cd7b34b359f5481ac59529b3de355162ef9f7)\n\n    FetchContent_GetProperties(jinja2cpp)\n    if(NOT jinja2cpp_POPULATED)\n        FetchContent_Populate(jinja2cpp)\n\n        set(BUILD_SHARED_LIBS OFF)\n        set(JINJA2CPP_INSTALL OFF CACHE BOOL \"\")\n        set(JINJA2CPP_CXX_STANDARD 17 CACHE STRING \"\")\n        set(JINJA2CPP_BUILD_SHARED OFF CACHE BOOL \"\")\n        set(JINJA2CPP_USE_REGEX \"std\" CACHE STRING \"\")\n        set(JINJA2CPP_WITH_JSON_BINDINGS \"none\" CACHE STRING \"\")\n        set(JINJA2CPP_STRICT_WARNINGS OFF CACHE BOOL \"\")\n        set(JINJA2CPP_PIC ON CACHE BOOL \"\")\n\n        # options for Jinja2Cpp dependencies\n        option(RAPIDJSON_BUILD_DOC \"Build rapidjson documentation.\" OFF)\n\n        if(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG OR (OV_COMPILER_IS_INTEL_LLVM AND UNIX))\n            set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-undef\")\n        endif()\n        if(SUGGEST_OVERRIDE_SUPPORTED)\n            set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-suggest-override\")\n        endif()\n\n        add_subdirectory(\"${jinja2cpp_SOURCE_DIR}\" \"${jinja2cpp_BINARY_DIR}\" EXCLUDE_FROM_ALL)\n\n        target_compile_definitions(jinja2cpp PUBLIC JINJA2CPP_LINK_AS_SHARED=0)\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Add Shared Library\nDESCRIPTION: Creates a shared library target named by TARGET_NAME, using the source files collected in the SOURCE_FILES variable. Also creates an alias for the target.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/c/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} SHARED ${SOURCE_FILES})\nadd_library(openvino::genai::c ALIAS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Installing and Exporting VLM with Optimum-Intel\nDESCRIPTION: This shell script installs the necessary requirements using pip and then exports the specified visual language model (VLM) using the `optimum-cli` tool. The `--upgrade-strategy eager` option ensures `optimum-intel` is upgraded, and `--trust-remote-code` is used to allow execution of code from the Hugging Face model repository.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/visual_language_chat/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install --upgrade-strategy eager -r ../../requirements.txt\noptimum-cli export openvino --model openbmb/MiniCPM-V-2_6 --trust-remote-code MiniCPM-V-2_6\n```\n\n----------------------------------------\n\nTITLE: Transcription with Automatic Language Detection (C++)\nDESCRIPTION: This C++ code demonstrates speech transcription with automatic language detection using the Whisper pipeline.  It reads audio files and the pipeline automatically predicts the language of the source audio.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/README.md#_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\nov::genai::RawSpeechInput raw_speech = read_wav(\"how_are_you_doing_today.wav\");\nauto result = pipeline.generate(raw_speech);\n//  How are you doing today?\n\nraw_speech = read_wav(\"fr_sample.wav\");\nresult = pipeline.generate(raw_speech);\n//  Il s'agit d'une entit trs complexe qui consiste...\n```\n\n----------------------------------------\n\nTITLE: Benchmark LLM with torch.compile() (Python/Bash)\nDESCRIPTION: This command runs the benchmarking script with `torch.compile()`, using the `--torch_compile_backend` option to specify the backend (`pytorch` or `openvino`).  Before running this command, the Pytorch model must be downloaded.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython ./benchmark.py -m models/llama-2-7b-chat/pytorch -d CPU --torch_compile_backend openvino\n```\n\n----------------------------------------\n\nTITLE: Beam Search Generation with Whisper Pipeline in C++\nDESCRIPTION: This snippet demonstrates how to use beam search generation with the WhisperPipeline in C++. It configures parameters like max_new_tokens, num_beams, num_beam_groups, and diversity_penalty. The input is raw audio data, and the output is the generated text using beam search.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nint main() {\n    ov::genai::WhisperPipeline pipe(model_path, \"CPU\");\n\n    // Get default generation config\n    ov::genai::GenerationConfig config = pipe.get_generation_config();\n\n    // Modify parameters\n    config.max_new_tokens = 256;\n    config.num_beams = 15;\n    config.num_beam_groups = 3;\n    config.diversity_penalty = 1.0f;\n\n    // Generate text with custom configuration\n    auto result = pipe.generate(raw_speech, config);\n}\n```\n\n----------------------------------------\n\nTITLE: Initial Prompt and Hotwords (C++)\nDESCRIPTION: This C++ code illustrates how to use `initial_prompt` to influence the output text in the Whisper pipeline. `initial_prompt` steers the model to use particular spellings or styles.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/README.md#_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\nauto result = pipeline.generate(raw_speech);\n//  He has gone and gone for good answered Paul Icrom who...\n\nresult = pipeline.generate(raw_speech, ov::genai::initial_prompt(\"Polychrome\"));\n//  He has gone and gone for good answered Polychrome who...\n```\n\n----------------------------------------\n\nTITLE: Chat Template Configuration for LLMPipeline\nDESCRIPTION: This JSON snippet demonstrates a default chat template that can be manually added to the `tokenizer_config.json` file of a Large Language Model (LLM). This is useful when the model wasn't originally tuned for chat and lacks a chat template. It defines how user and assistant messages are formatted for the model.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/js/text_generation/README.md#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n\"chat_template\": \"{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n<|im_start|>assistant\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|im_end|>\\n'}}{% endif %}{% endfor %}\"\n```\n\n----------------------------------------\n\nTITLE: UnicodeEncodeError Example (text)\nDESCRIPTION: This is an example of the `UnicodeEncodeError` that can occur when the sample script attempts to print Unicode characters to the Windows console without proper encoding configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nUnicodeEncodeError: 'charmap' codec can't encode character '\\u25aa' in position 0: character maps to <undefined>\n```\n\n----------------------------------------\n\nTITLE: Download ModelScope model\nDESCRIPTION: Downloads a specified model from ModelScope to a local directory.  The `<model_path>` placeholder needs to be replaced with the actual path where the model should be saved. Example model: `Qwen/Qwen2-7b`.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/model-preparation/convert-to-openvino.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmodelscope download --model 'Qwen/Qwen2-7b' --local_dir <model_path>\n```\n\n----------------------------------------\n\nTITLE: Deploy Website using SSH\nDESCRIPTION: This command deploys the website using SSH. The `USE_SSH=true` environment variable enables SSH-based deployment, likely configured within the `deploy` script in `package.json`. This typically involves pushing the built website to a remote server via SSH.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/README.md#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n$ USE_SSH=true npm run deploy\n```\n\n----------------------------------------\n\nTITLE: Run Greedy Causal LM Sample in JavaScript\nDESCRIPTION: This command executes the `greedy_causal_lm.js` script using Node.js. It requires a model directory and a prompt as arguments, specifying the model location and initial text prompt for generation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/js/text_generation/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nnode greedy_causal_lm.js model_dir prompt\n```\n\n----------------------------------------\n\nTITLE: Setting up environment and building samples on Linux and macOS\nDESCRIPTION: This script sets up the environment by sourcing the `setupvars.sh` file and then builds the C samples using the `build_samples.sh` script.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/c/text_generation/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nsource <INSTALL_DIR>/setupvars.sh\n./<INSTALL_DIR>/samples/c/build_samples.sh\n```\n\n----------------------------------------\n\nTITLE: Convert and Download Whisper Model with Optimum CLI\nDESCRIPTION: This shell script converts and downloads a Whisper model using the Optimum CLI. The `--upgrade-strategy eager` option ensures that optimum-intel is upgraded to the latest version.  The `--trust-remote-code` option is also used. An additional option `--disable-stateful` is required, if NPU is the inference device.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install --upgrade-strategy eager -r ../../requirements.txt\noptimum-cli export openvino --trust-remote-code --model openai/whisper-base whisper-base\n```\n\n----------------------------------------\n\nTITLE: Creating Inpainting Executable with CMake\nDESCRIPTION: This snippet creates an executable named 'inpainting' from the source files 'inpainting.cpp', 'load_image.cpp', and 'imwrite.cpp'.  It configures include directories and links the executable with the OpenVINO GenAI and indicators libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(inpainting inpainting.cpp load_image.cpp imwrite.cpp)\n\ntarget_include_directories(inpainting PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}\" \"${CMAKE_BINARY_DIR}\")\ntarget_link_libraries(inpainting PRIVATE openvino::genai indicators::indicators)\n\nset_target_properties(inpainting PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n\ninstall(TARGETS inpainting\n        RUNTIME DESTINATION samples_bin/\n        COMPONENT samples_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Initial Prompt Usage with Whisper Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to use an initial prompt with the WhisperPipeline in Python to guide the model's output. It provides an initial prompt that influences the transcription result.  The input is raw audio data, and the output is the transcribed text influenced by the prompt.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npipe = ov_genai.WhisperPipeline(model_path, \"CPU\")\n\nresult = pipe.generate(raw_speech)\n# He has gone and gone for good answered Paul Icrom who...\n\nresult = pipe.generate(raw_speech, initial_prompt=\"Polychrome\")\n# He has gone and gone for good answered Polychrome who...\n```\n\n----------------------------------------\n\nTITLE: Run Beam Search Causal LM Sample (bash)\nDESCRIPTION: Executes the `beam_search_causal_lm` application, utilizing beam search for text generation. It requires the model directory and one or more prompts as arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./beam_search_causal_lm <MODEL_DIR> \"<PROMPT 1>\" [\"<PROMPT 2>\" ...]\n```\n\n----------------------------------------\n\nTITLE: Accessing Performance Metrics after Generation in C++\nDESCRIPTION: This snippet demonstrates how to generate text using the LLMPipeline in C++ and access performance metrics.  It requires the openvino/genai/llm_pipeline.hpp header. The models_path is passed as a command line argument.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/performance-metrics.mdx#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string models_path = argv[1];\n    ov::genai::LLMPipeline pipe(models_path, \"CPU\");\n    auto result = pipe.generate(\"The Sun is yellow because\", ov::genai::max_new_tokens(20));\n    auto perf_metrics = result.perf_metrics;\n\n    std::cout << std::fixed << std::setprecision(2);\n    std::cout << \"Generate duration: \" << perf_metrics.get_generate_duration().mean << \" ms\" << std::endl;\n    std::cout << \"TTFT: \" << metrics.get_ttft().mean  << \" ms\" << std::endl;\n    std::cout << \"TPOT: \" << metrics.get_tpot().mean  << \" ms/token \" << std::endl;\n    std::cout << \"Throughput: \" << metrics.get_throughput().mean  << \" tokens/s\" << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Log Level (Linux)\nDESCRIPTION: This command sets the OpenVINO log level to 3 (INFO) in a Linux environment. This will print information, warnings, and errors related to the OpenVINO execution.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DEBUG_LOG.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport OPENVINO_LOG_LEVEL=3\n```\n\n----------------------------------------\n\nTITLE: Compute Visual Language Metric\nDESCRIPTION: This command computes the metric for a visual language model using WWB with the OpenVINO GenAI backend. It specifies the target model, ground truth data, model type, and the `--genai` flag.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\nwwb --target-model llava-int8 --gt-data llava_test/gt.csv --model-type visual-text --genai\n```\n\n----------------------------------------\n\nTITLE: Controlling Padding in C++\nDESCRIPTION: Examples of how to control padding during the encoding process in C++.  It shows how to pad to the longest sequence or to a specified max length.  It includes equivalent HuggingFace tokenizer calls in the comments.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_25\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\nauto tokenizer = ov::genai::Tokenizer(models_path);\nstd::vector<std::string> prompts = {\"The Sun is yellow because\", \"The\"};\n\n// Since prompt is defenitely shorter than maximal length (which is taken from IR) will not affect shape.\n// Resulting shape is defined by length of the longest tokens sequence.\n// Equivalent of HuggingFace hf_tokenizer.encode(prompt, padding=\"longest\", truncation=True)\ntokens = tokenizer.encode({\"The Sun is yellow because\", \"The\"})\n// or is equivalent to\ntokens = tokenizer.encode({\"The Sun is yellow because\", \"The\"}, ov::genai::pad_to_max_length(False))\n// out_shape: [2, 6]\n\n// Resulting tokens tensor will be padded to 1024.\n// Equivalent of HuggingFace hf_tokenizer.encode(prompt, padding=\"max_length\", truncation=True, max_length=1024)\ntokens = tokenizer.encode({\"The Sun is yellow because\", \n                           \"The\",\n                           std::string(2000, 'n')}, ov::genai::pad_to_max_length(True), ov::genai::max_length(1024))\n// out_shape: [3, 1024]\n\n// For single string prompts truncation and padding are also applied.\ntokens = tokenizer.encode({\"The Sun is yellow because\"}, ov::genai::pad_to_max_length(True), ov::genai::max_length(1024))\n// out_shape: [1, 128]\n```\n\n----------------------------------------\n\nTITLE: Compute Text-to-Image Metric\nDESCRIPTION: This command computes the metric for a text-to-image model using WWB with the OpenVINO GenAI backend. It specifies the target model, ground truth data, model type, and the `--genai` flag.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nwwb --target-model sd-lcm-int8 --gt-data lcm_test/gt.csv --model-type text-to-image --genai\n```\n\n----------------------------------------\n\nTITLE: Setting Library Destinations for NPM Package in CMake\nDESCRIPTION: This snippet configures the library destinations specifically for NPM package generation. It sets the LIBRARY_DESTINATION, ARCHIVE_DESTINATION, and RUNTIME_DESTINATION to the top level directory ('.'). It also defines RPATH/LC_RPATH entries for Linux and macOS to locate shared libraries during runtime when installed from npm or built from source.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(CPACK_GENERATOR STREQUAL \"NPM\")\n    set(LIBRARY_DESTINATION .)\n    set(ARCHIVE_DESTINATION .)\n    set(RUNTIME_DESTINATION .)\n\n    # setting RPATH / LC_RPATH depending on platform\n    if(LINUX)\n        # to find libopenvino_genai.so in the same folder\n        set(rpaths \"$ORIGIN\")\n        # to find libopenvino.so when installing from npm\n        list(APPEND rpaths \"$ORIGIN/../../openvino-node/bin\")\n        # to find libopenvino.so when installing from source\n        list(APPEND rpaths \"$ORIGIN/../node_modules/openvino-node/bin\")\n    elseif(APPLE)\n        # to find libopenvino_genai.dylib in the same folder\n        set(rpaths \"@loader_path\")\n        # to find libopenvino.dylib when installing from npm\n        list(APPEND rpaths \"@loader_path/../../openvino-node/bin\")\n        # to find libopenvino.dylib when installing from source\n        list(APPEND rpaths \"@loader_path/../node_modules/openvino-node/bin\")\n    endif()\nelse()\n    set(LIBRARY_DESTINATION runtime/lib/${ARCH_DIR})\n    set(ARCHIVE_DESTINATION runtime/lib/${ARCH_DIR})\n    set(RUNTIME_DESTINATION runtime/bin/${ARCH_DIR})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Run Multinomial Causal LM Sample (bash)\nDESCRIPTION: Executes the `multinomial_causal_lm` application, which uses multinomial sampling for diverse text generation. It takes the model directory and a prompt as input.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n./multinomial_causal_lm <MODEL_DIR> \"<PROMPT>\"\n```\n\n----------------------------------------\n\nTITLE: Install WWB with Nightly OpenVINO Builds\nDESCRIPTION: This command installs WWB along with nightly builds of openvino, openvino-tokenizers, and openvino-genai. It sets environment variables to specify the extra index URL for pip.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nPIP_PRE=1 \\\nPIP_EXTRA_INDEX_URL=https://storage.openvinotoolkit.org/simple/wheels/nightly \\\npip install .\n```\n\n----------------------------------------\n\nTITLE: Configuring Generation Parameters in VLM Pipeline (C++)\nDESCRIPTION: This code snippet demonstrates how to configure generation parameters for a VLMPipeline in C++ using the OpenVINO GenAI library. It initializes the pipeline, retrieves the default configuration, modifies parameters such as max_new_tokens, temperature, top_k, top_p, and repetition_penalty, and then generates text with the custom configuration.  It requires the OpenVINO GenAI C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-processing/_sections/_usage_options/index.mdx#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nint main() {\n    ov::genai::VLMPipeline pipe(model_path, \"CPU\");\n\n    // Get default configuration\n    auto config = pipe.get_generation_config();\n\n    // Modify parameters\n    config.max_new_tokens = 100;\n    config.temperature = 0.7f;\n    config.top_k = 50;\n    config.top_p = 0.9f;\n    config.repetition_penalty = 1.2f;\n\n    // Generate text with custom configuration\n    auto output = pipe.generate(prompt, images, config);\n}\n```\n\n----------------------------------------\n\nTITLE: Running OpenVINO GenAI Model on GPU (C++)\nDESCRIPTION: This snippet demonstrates how to run an OpenVINO GenAI model on the GPU using C++. It assumes the availability of a converted model within a specified folder and utilizes the `LLMPipeline` object. The `device` parameter is set to \"GPU\".\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/text-generation/_sections/_run_model/index.mdx#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n<CodeExampleCPP device=\"GPU\" />\n```\n\n----------------------------------------\n\nTITLE: Measure Similarity for OpenVINO GenAI Inference\nDESCRIPTION: This command measures the similarity metric for an OpenVINO GenAI inference backend using WWB. It specifies the target model, ground truth data, model type, and the `--genai` flag to use the GenAI backend.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nwwb --target-model phi-3-openvino --gt-data gt.csv --model-type text --genai\n```\n\n----------------------------------------\n\nTITLE: Run Multinomial Causal LM Sample in JavaScript\nDESCRIPTION: This command executes the `multinomial_causal_lm.js` script using Node.js.  It requires a model directory and a prompt as arguments, employing multinomial sampling for diverse text generation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/js/text_generation/README.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nnode multinomial_causal_lm.js model_dir prompt\n```\n\n----------------------------------------\n\nTITLE: Running VLM Benchmark Script in Python\nDESCRIPTION: This shell command shows how to run the `benchmark_vlm.py` script to evaluate the performance of the VLM.  It takes optional arguments to configure the model path, prompt, image, number of warmup iterations, maximum new tokens, number of iterations, and device.  Example usage is provided.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/visual_language_chat/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npython benchmark_vlm.py [OPTIONS]\n```\n\n----------------------------------------\n\nTITLE: Configure CMake for OpenVINO with extra modules\nDESCRIPTION: Configures CMake to build OpenVINO with OpenVINO GenAI as an extra module.  The `CPACK_ARCHIVE_COMPONENT_INSTALL=OFF` option is set to disable component-based installation in the archive. `-S` specifies the source directory and `-B` specifies the build directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DOPENVINO_EXTRA_MODULES=./openvino.genai -DCPACK_ARCHIVE_COMPONENT_INSTALL=OFF -S ./openvino -B ./build\n```\n\n----------------------------------------\n\nTITLE: Running Chat Sample C\nDESCRIPTION: This command runs the chat sample, requiring the path to the model directory as an argument. The `chat_sample_c` executable is assumed to be in the current working directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/c/text_generation/README.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n./chat_sample_c model_dir\n```\n\n----------------------------------------\n\nTITLE: Running Throughput Benchmark Application\nDESCRIPTION: This snippet runs the throughput benchmark application for causal language models. It requires a built executable at ./build/text_generation/causal_lm/cpp/continuous_batching/apps/throughput_benchmark, an OpenVINO model located at /workspace/openvino.genai/text_generation/causal_lm/cpp/continuous_batching/ov_model, and a dataset at /workspace/ShareGPT_V3_unfiltered_cleaned_split.json.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ncd /workspace/openvino.genai/\n./build/text_generation/causal_lm/cpp/continuous_batching/apps/throughput_benchmark --model /workspace/openvino.genai/text_generation/causal_lm/cpp/continuous_batching/ov_model --dataset /workspace/ShareGPT_V3_unfiltered_cleaned_split.json --dynamic_split_fuse --num_prompts 100 --device CPU --plugin_config {/\"ENABLE_PROFILING/\":true}\n```\n\n----------------------------------------\n\nTITLE: Running VLM Chat Sample in Python\nDESCRIPTION: This command executes the `visual_language_chat.py` script with the specified model directory and image file as arguments. It demonstrates how to use the VLM pipeline for basic visual-language chat functionality. Requires the `deployment-requirements.txt` to be installed first.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/visual_language_chat/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython visual_language_chat.py ./miniCPM-V-2_6/ 319483352-d5fbbd1a-d484-415c-88cb-9986625b7b11.jpg\n```\n\n----------------------------------------\n\nTITLE: Image to Image Generation with OpenVINO GenAI in Python\nDESCRIPTION: This code snippet demonstrates how to use the OpenVINO GenAI library to perform image-to-image generation. It reads an image, initializes the Image2ImagePipeline with a specified model path and device (defaulting to CPU), generates a new image based on a prompt and input image, and saves the output image. The 'strength' parameter controls the influence of the prompt on the output image. The pipeline requires the openvino_genai and openvino packages, as well as the PIL library for image handling and numpy.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-generation/_sections/_run_model/_image2image_python.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\nimport openvino as ov\nfrom PIL import Image\nimport numpy as np\n\ndef read_image(path: str) -> ov.Tensor:\n    pic = Image.open(path).convert(\"RGB\")\n    image_data = np.array(pic)[None]\n    return ov.Tensor(image_data)\n\ninput_image_data = read_image(\"input_image.jpg\")\n\npipe = ov_genai.Image2ImagePipeline(model_path, \"${props.device || 'CPU'}\")\nimage_tensor = pipe.generate(prompt, image=input_image_data, strength=0.8)\n\nimage = Image.fromarray(image_tensor.data[0])\nimage.save(\"image.bmp\")\n\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Script Execution\nDESCRIPTION: This command executes the `benchmark_image_gen` script, providing options to specify the pipeline type, model directory, and number of iterations to perform on the CPU device.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./benchmark_image_gen -t text2image -m dreamlike_anime_1_0_ov/FP16 -n 10 -d CPU\n```\n\n----------------------------------------\n\nTITLE: Example Benchmark Command\nDESCRIPTION: This is an example command to benchmark a text-to-image pipeline using the `dreamlike_anime_1_0_ov/FP16` model with 10 iterations on the CPU device. This command demonstrates how to run the benchmark script with specific configurations for pipeline type, model path, number of iterations, and device.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/image_generation/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n`python benchmark_image_gen.py -t text2image -m dreamlike_anime_1_0_ov/FP16 -n 10 -d CPU`\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties for Object Library\nDESCRIPTION: This snippet configures the include directories, link libraries, compile features, and compile definitions for the object library `openvino_genai_obj`. It sets public and private include paths, links against `openvino::runtime`, `openvino::threading`, `nlohmann_json::nlohmann_json` and `jinja2cpp`, defines the C++ standard, sets compile definitions, and enables position independent code.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/cpp/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME_OBJ}\n    PUBLIC \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\"\n           \"$<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}>\"\n    PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}/src\")\n\nif(ENABLE_GGUF)\n    target_link_libraries(${TARGET_NAME_OBJ} PRIVATE gguflib)\n    target_compile_definitions(${TARGET_NAME_OBJ} PRIVATE ENABLE_GGUF)\nendif()\n\ntarget_include_directories(${TARGET_NAME_OBJ} SYSTEM PRIVATE \"${safetensors.h_SOURCE_DIR}\")\n\ntarget_link_libraries(${TARGET_NAME_OBJ} PRIVATE openvino::runtime openvino::threading nlohmann_json::nlohmann_json jinja2cpp)\n\ntarget_compile_features(${TARGET_NAME_OBJ} PRIVATE cxx_std_17)\n\ntarget_compile_definitions(${TARGET_NAME_OBJ} PRIVATE openvino_genai_EXPORTS)\n\nset_target_properties(${TARGET_NAME_OBJ} PROPERTIES POSITION_INDEPENDENT_CODE ON)\n```\n\n----------------------------------------\n\nTITLE: Download Pre-Converted Model from Hugging Face Hub (sh)\nDESCRIPTION: Downloads a pre-converted OpenVINO IR model from the Hugging Face Hub using the `huggingface-cli` tool. This requires the `huggingface-hub` package to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install huggingface-hub\nhuggingface-cli download <model> --local-dir <output_folder>\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO Tokenizers to Virtual Environment\nDESCRIPTION: This snippet installs the OpenVINO tokenizers into the created virtual environment, assuming the tokenizers have been built. It also defines the OpenVINO_DIR environment variable to point to the built OpenVINO directory before installing using pip. This step requires the proper path to openvino.genai to be substituted.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\ncd /path/to/openvino.genai/thirdparty/openvino_tokenizers\nexport OpenVINO_DIR=/path/to/openvino/build\npip install --no-deps .\n```\n\n----------------------------------------\n\nTITLE: Export Visual Language Model to OpenVINO (Quantized)\nDESCRIPTION: This command exports a FP16 visual language model to the OpenVINO format with int8 quantization using Optimum-Intel.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\noptimum-cli export openvino -m llava-hf/llava-v1.6-mistral-7b-hf  --weight-format int8 llava-int8\n```\n\n----------------------------------------\n\nTITLE: Collect Text-to-Image References\nDESCRIPTION: This command collects reference images and saves the mapping in a CSV file using WWB for a text-to-image model. It specifies the base model, ground truth data, model type, and the `--hf` flag.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nwwb --base-model SimianLuo/LCM_Dreamshaper_v7--gt-data lcm_test/gt.csv --model-type text-to-image --hf\n```\n\n----------------------------------------\n\nTITLE: Convert LLM for OpenVINO using Optimum Intel\nDESCRIPTION: This command converts a Large Language Model (LLM) to the OpenVINO IR format using the `optimum-intel` library.  It requires the `optimum-intel` package and exports the specified model. The `--trust-remote-code` option is needed when the model requires execution of custom code.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/js/text_generation/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install --upgrade-strategy eager -r ../../export-requirements.txt\noptimum-cli export openvino --trust-remote-code --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 TinyLlama-1.1B-Chat-v1.0\n```\n\n----------------------------------------\n\nTITLE: Initial Prompt and Hotwords (Python)\nDESCRIPTION: This Python code demonstrates the usage of `initial_prompt` in the Whisper pipeline's `generate` method.  The `initial_prompt` can be used to influence the output of the model, guiding it to use specific words or styles. The prompt is prepended to the transcription after the `<|startofprev|>` token.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/whisper_speech_recognition/README.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nresult = pipe.generate(raw_speech)\n#  He has gone and gone for good answered Paul Icrom who...\n\nresult = pipe.generate(raw_speech, initial_prompt=\"Polychrome\")\n#  He has gone and gone for good answered Polychrome who...\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face (Bash)\nDESCRIPTION: This command allows you to log in to Hugging Face using the Hugging Face CLI. This is necessary for accessing and using non-public models from the Hugging Face Hub.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Verify OpenVINO GenAI installation\nDESCRIPTION: Verifies the installation of the OpenVINO GenAI package by importing it and printing its version. This ensures that the package is installed correctly and can be used in Python scripts.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_26\n\nLANGUAGE: sh\nCODE:\n```\npython -c \"import openvino_genai; print(openvino_genai.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies using pip (optional)\nDESCRIPTION: This script clones the OpenVINO GenAI repository (if it doesn't already exist), changes directory into it, then installs python dependencies using pip. It installs the openvino_tokenizers package from a nightly build index URL, and then installs other requirements specified in ./samples/requirements.txt.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# (Optional) Clone OpenVINO GenAI repository if it does not exist\ngit clone --recursive https://github.com/openvinotoolkit/openvino.genai.git\ncd openvino.genai\n# Install python dependencies\npython -m pip install ./thirdparty/openvino_tokenizers/[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\npython -m pip install --upgrade-strategy eager -r ./samples/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Install Specific OpenVINO Version (Bash)\nDESCRIPTION: These commands install a specific version of the OpenVINO package using pip.  The second block shows how to install a nightly version of OpenVINO. Note that using nightly builds is intended for development and testing purposes as the releases are not validated.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# e.g. \npip install openvino==2024.4.0\n# Optional, install the openvino nightly package if needed.\n# OpenVINO nightly is pre-release software and has not undergone full release validation or qualification. \npip uninstall openvino\npip install --upgrade --pre openvino openvino-tokenizers --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n```\n\n----------------------------------------\n\nTITLE: CPack Configuration\nDESCRIPTION: Configures CPack, CMake's packaging tool. This snippet sets whether to include the top-level directory in archives, lists all components, and appends additional components based on whether Python and JS are enabled. It also sets a default CPack generator for Windows and adjusts it if it's set to NPM.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED CPACK_ARCHIVE_COMPONENT_INSTALL)\n    set(CPACK_ARCHIVE_COMPONENT_INSTALL ON)\nendif()\nset(CPACK_INCLUDE_TOPLEVEL_DIRECTORY OFF)\n# Workaround https://gitlab.kitware.com/cmake/cmake/-/issues/2614\nset(CPACK_COMPONENTS_ALL core_genai core_genai_dev core_c_genai core_c_genai_dev cpp_samples_genai licensing_genai openvino_tokenizers openvino_tokenizers_docs)\nif(ENABLE_PYTHON)\n    list(APPEND CPACK_COMPONENTS_ALL pygenai_${Python3_VERSION_MAJOR}_${Python3_VERSION_MINOR})\nendif()\nif(ENABLE_JS)\n    list(APPEND CPACK_COMPONENTS_ALL genai_node_addon)\nendif()\nif(WIN32 AND NOT DEFINED CPACK_GENERATOR)\n    set(CPACK_GENERATOR \"ZIP\")\nendif()\nif(CPACK_GENERATOR STREQUAL \"NPM\")\n    set(CPACK_GENERATOR \"TGZ\")\nendif()\ninclude(CPack)\n```\n\n----------------------------------------\n\nTITLE: Run LoRA Greedy Causal LM\nDESCRIPTION: This command runs the `lora_greedy_causal_lm.py` script, demonstrating greedy decoding with LoRA fine-tuned models. It requires the model directory, the adapter file, and a prompt as input.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython lora_greedy_causal_lm.py model_dir adapter_safetensors_file prompt\n```\n\n----------------------------------------\n\nTITLE: Running Visual Language Pipeline in C++\nDESCRIPTION: This code snippet shows how to use the OpenVINO GenAI Visual Language Pipeline in C++. It initializes the pipeline with a model path and device, loads images using a utility function, generates text based on a prompt and loaded images, and prints the resulting text. The snippet depends on the `openvino/genai/visual_language/pipeline.hpp` and `load_image.hpp` headers, along with the OpenVINO library.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/image-processing/_sections/_run_model/_code_example_cpp.mdx#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/visual_language/pipeline.hpp\"\n#include \"load_image.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string models_path = argv[1], images_path = argv[2];;\n    std::vector<ov::Tensor> images = utils::load_images(images_path);\n\n    ov::genai::VLMPipeline pipe(models_path, \"${props.device || 'CPU'}\");\n    ov::genai::VLMDecodedResults result = pipe.generate(\n        prompt,\n        ov::genai::images(images),\n        ov::genai::max_new_tokens(100)\n    );\n    std::cout << result.texts[0] << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Export Model to OpenVINO IR Format (sh)\nDESCRIPTION: Exports a model to OpenVINO Intermediate Representation (IR) format using the `optimum-cli` tool. This command takes the model name and output folder as arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\noptimim-cli export openvino --model <model> <output_folder>\n```\n\n----------------------------------------\n\nTITLE: Encoding without Special Tokens in Python\nDESCRIPTION: This snippet demonstrates how to use the `encode()` method of the `Tokenizer` class in Python to tokenize a string while disabling the addition of special tokens. The `add_special_tokens=False` argument ensures that the tokenizer does not add any special tokens (e.g., BOS, EOS) to the tokenized sequence.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/tokenization.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntokens = tokenizer.encode(\"The Sun is yellow because\", add_special_tokens=False)\n```\n\n----------------------------------------\n\nTITLE: Installing Target and Exporting for Package Configuration\nDESCRIPTION: This snippet installs the target, exports the target for use in package configurations, and configures package configuration files. It specifies installation directories for libraries, archives, and runtime components.  It also creates and installs the CMake package configuration files.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/cpp/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME} EXPORT OpenVINOGenAITargets\n        LIBRARY DESTINATION ${LIBRARY_DESTINATION} COMPONENT core_genai\n            NAMELINK_COMPONENT core_genai_dev\n        ARCHIVE DESTINATION ${ARCHIVE_DESTINATION} COMPONENT core_genai_dev\n        RUNTIME DESTINATION ${RUNTIME_DESTINATION} COMPONENT core_genai\n        INCLUDES DESTINATION runtime/include)\n\n# development files do not need to be built for NPM package\nif(CPACK_GENERATOR STREQUAL \"NPM\")\n    return()\nendif()\n\ninstall(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/include/\n                  ${OpenVINOGenAI_SOURCE_DIR}/src/c/include/\n        DESTINATION runtime/include COMPONENT core_genai_dev)\ninstall(FILES ${CMAKE_CURRENT_BINARY_DIR}/openvino/genai/version.hpp\n        DESTINATION runtime/include/openvino/genai COMPONENT core_genai_dev)\n\ninstall(EXPORT OpenVINOGenAITargets FILE OpenVINOGenAITargets.cmake\n        NAMESPACE openvino:: DESTINATION runtime/cmake\n        COMPONENT core_genai_dev)\n\ninclude(CMakePackageConfigHelpers)\nconfigure_package_config_file(\"${OpenVINOGenAI_SOURCE_DIR}/cmake/templates/OpenVINOGenAIConfig.cmake.in\"\n                              \"${CMAKE_BINARY_DIR}/OpenVINOGenAIConfig.cmake\" INSTALL_DESTINATION runtime/cmake)\nwrite_basic_package_version_file(\"${CMAKE_BINARY_DIR}/OpenVINOGenAIConfigVersion.cmake\"\n                                 VERSION ${OpenVINOGenAI_VERSION} COMPATIBILITY AnyNewerVersion)\ninstall(FILES \"${CMAKE_BINARY_DIR}/OpenVINOGenAIConfig.cmake\" \"${CMAKE_BINARY_DIR}/OpenVINOGenAIConfigVersion.cmake\"\n        DESTINATION runtime/cmake COMPONENT core_genai_dev)\nexport(EXPORT OpenVINOGenAITargets FILE \"${CMAKE_BINARY_DIR}/OpenVINOGenAITargets.cmake\" NAMESPACE openvino::)\n```\n\n----------------------------------------\n\nTITLE: Run OpenVINO GenAI Tests with pytest\nDESCRIPTION: This command executes the OpenVINO GenAI tests using `pytest`. The `-m precommit` flag specifies that only tests marked with the `precommit` marker should be executed. It assumes that the OpenVINO GenAI library is installed or accessible via `PYTHONPATH`.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython -m pytest tests/python_tests/ -m precommit\n```\n\n----------------------------------------\n\nTITLE: Run Speculative Decoding LM Sample (bash)\nDESCRIPTION: Executes the `speculative_decoding_lm` application, speeding up token generation using a draft model alongside the main model. It takes the main model directory, the draft model directory, and a prompt as input.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n./speculative_decoding_lm <MODEL_DIR> <DRAFT_MODEL_DIR> \"<PROMPT>\"\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINOGenAI Package with CMake\nDESCRIPTION: This snippet uses the `find_package` command to locate the OpenVINOGenAI package. It specifies potential search paths, including the build directory and the OpenVINO installation directory. The `REQUIRED` keyword ensures that the build fails if the package is not found.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenVINOGenAI REQUIRED\n    PATHS\n        \"${CMAKE_BINARY_DIR}\"  # Reuse the package from the build.\n        ${OpenVINO_DIR}  # GenAI may be installed alogside OpenVINO.\n    NO_CMAKE_FIND_ROOT_PATH\n)\n```\n\n----------------------------------------\n\nTITLE: Whisper Prompt Example\nDESCRIPTION: This JSON snippet presents a prompt file for audio transcription using the Whisper model.  It includes parameters like 'media' (audio file path), 'language', and 'timestamp'. These settings specify the input audio file, the language of the audio, and whether to include timestamps in the transcription.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/doc/PROMPT.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"media\": \"./audio/intel_ad_90s_128kbps.mp3\", \"language\": \"<|en|>\", \"timestamp\":false}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"media\": \"./audio/intel_ad_120s_128kbps.mp3\", \"language\": \"<|en|>\", \"timestamp\":true}\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library with add_library\nDESCRIPTION: This snippet creates an object library named `openvino_genai_obj` from the source files. It includes recursive globbing to find source files and appends the generated version file to the list of source files. GGUF sources are conditionally removed from the list depending on the ENABLE_GGUF flag.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/cpp/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SOURCE_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp\" \"${CMAKE_CURRENT_SOURCE_DIR}/src/*.c\")\nlist(APPEND SOURCE_FILES \"${CMAKE_CURRENT_BINARY_DIR}/version.cpp\")\n\nif(NOT ENABLE_GGUF)\n    set(GGUF_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/src/gguf_utils/gguf.cpp\n                    ${CMAKE_CURRENT_SOURCE_DIR}/src/gguf_utils/gguf_quants.cpp\n                    ${CMAKE_CURRENT_SOURCE_DIR}/src/gguf_utils/gguf_modeling.cpp\n                    ${CMAKE_CURRENT_SOURCE_DIR}/src/gguf_utils/building_blocks.cpp)\n    list(REMOVE_ITEM SOURCE_FILES ${GGUF_SOURCES})\nendif()\n\nset(TARGET_NAME openvino_genai)\nset(TARGET_NAME_OBJ ${TARGET_NAME}_obj)\n\nadd_library(${TARGET_NAME_OBJ} OBJECT ${SOURCE_FILES})\n```\n\n----------------------------------------\n\nTITLE: Image2Image generation in C++\nDESCRIPTION: This code snippet demonstrates how to use the Image2Image pipeline in C++ to generate a new image based on a text prompt and an input image. It loads a pre-trained model, initializes the pipeline, processes the image using custom load_image and imwrite functions, and saves the generated image. The code requires the installation of a C++ compatible OpenVINO GenAI package.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/image_generation/image2image_pipeline.hpp\"\n#include \"load_image.hpp\"\n#include \"imwrite.hpp\"\n\nint main(int argc, char* argv[]) {\n   const std::string models_path = argv[1], prompt = argv[2], image_path = argv[3];\n   const std::string device = \"CPU\";  // GPU can be used as well\n\n   ov::Tensor image = utils::load_image(image_path);\n\n   ov::genai::Image2ImagePipeline pipe(models_path, device);\n   ov::Tensor generated_image = pipe.generate(prompt, image, ov::genai::strength(0.8f));\n\n   imwrite(\"image.bmp\", generated_image, true);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Visual Language Chat Executable CMake\nDESCRIPTION: This snippet defines an executable target named 'visual_language_chat'. It specifies the source files, include directories, and linked libraries. It also sets the INSTALL_RPATH_USE_LINK_PATH property for macOS compatibility. Finally, it defines the install target to copy the executable into the samples_bin directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/visual_language_chat/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(visual_language_chat visual_language_chat.cpp load_image.cpp)\ntarget_include_directories(visual_language_chat PRIVATE \"${CMAKE_CURRENT_SOUCE_DIR}\" \"${CMAKE_BINARY_DIR}\")\ntarget_link_libraries(visual_language_chat PRIVATE openvino::genai)\n\nset_target_properties(visual_language_chat PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n\ninstall(TARGETS visual_language_chat\n        RUNTIME DESTINATION samples_bin/\n        COMPONENT samples_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Controlling Padding in C++\nDESCRIPTION: This C++ snippet demonstrates how to control padding and truncation during tokenization using the `ov::genai::pad_to_max_length` and `ov::genai::max_length` parameters with the `encode()` method. It shows how to pad to the longest sequence or to a specified `max_length`, affecting the shape of the resulting tensor. The behavior mirrors the Python example.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/tokenization.mdx#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n\nauto tokenizer = ov::genai::Tokenizer(models_path);\nstd::vector<std::string> prompts = {\"The Sun is yellow because\", \"The\"};\n\n// Since prompt is defenitely shorter than maximal length (which is taken from IR) will not affect shape.\n// Resulting shape is defined by length of the longest tokens sequence.\n// Equivalent of HuggingFace hf_tokenizer.encode(prompt, padding=\"longest\", truncation=True)\ntokens = tokenizer.encode({\"The Sun is yellow because\", \"The\"})\n// or is equivalent to\ntokens = tokenizer.encode({\"The Sun is yellow because\", \"The\"}, ov::genai::pad_to_max_length(False))\n// out_shape: [2, 6]\n\n// Resulting tokens tensor will be padded to 1024.\n// Equivalent of HuggingFace hf_tokenizer.encode(prompt, padding=\"max_length\", truncation=True, max_length=1024)\ntokens = tokenizer.encode({\"The Sun is yellow because\",\n                                  \"The\",\n                                  std::string(2000, 'n')}, ov::genai::pad_to_max_length(True), ov::genai::max_length(1024))\n// out_shape: [3, 1024]\n\n// For single string prompts truncation and padding are also applied.\ntokens = tokenizer.encode({\"The Sun is yellow because\"}, ov::genai::pad_to_max_length(True), ov::genai::max_length(1024))\n// out_shape: [1, 128]\n```\n\n----------------------------------------\n\nTITLE: Creating Text2Image Executable with CMake\nDESCRIPTION: This snippet creates an executable named 'text2image' from the source files 'text2image.cpp' and 'imwrite.cpp'. It then sets the include directories to include the current source directory and the binary directory. Finally, it links the executable with the 'openvino::genai' and 'indicators::indicators' libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(text2image text2image.cpp imwrite.cpp)\n\ntarget_include_directories(text2image PRIVATE ${CMAKE_BINARY_DIR} \"${CMAKE_CURRENT_SOURCE_DIR}\")\ntarget_link_libraries(text2image PRIVATE openvino::genai indicators::indicators)\n\nset_target_properties(text2image PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n\ninstall(TARGETS text2image\n        RUNTIME DESTINATION samples_bin/\n        COMPONENT samples_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Log Level (Windows)\nDESCRIPTION: This command sets the OpenVINO log level to 3 (INFO) in a Windows environment. This will print information, warnings, and errors related to the OpenVINO execution.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DEBUG_LOG.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nWindows - set OPENVINO_LOG_LEVEL=3\n```\n\n----------------------------------------\n\nTITLE: Accumulating Performance Metrics in C++\nDESCRIPTION: This C++ code demonstrates accumulating performance metrics from multiple generate calls by adding the `perf_metrics` objects.  It initializes the pipeline, performs two generate calls, adds the resulting `perf_metrics`, and prints the combined performance metrics.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string models_path = argv[1];\n    ov::genai::LLMPipeline pipe(models_path, \"CPU\");\n    auto result_1 = pipe.generate(\"The Sun is yellow because\", ov::genai::max_new_tokens(20));\n    auto result_2 = pipe.generate(\"The Sun is yellow because\", ov::genai::max_new_tokens(20));\n    auto perf_metrics = result_1.perf_metrics + result_2.perf_metrics\n\n    std::cout << std::fixed << std::setprecision(2);\n    std::cout << \"Generate duration: \" << perf_metrics.get_generate_duration().mean << \" ms\" << std::endl;\n    std::cout << \"TTFT: \" << perf_metrics.get_ttft().mean  << \" ms\" << std::endl;\n    std::cout << \"TPOT: \" << perf_metrics.get_tpot().mean  << \" ms/token \" << std::endl;\n    std::cout << \"Throughput: \" << perf_metrics.get_throughput().mean  << \" tokens/s\" << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Run WhisperPipeline on CPU with Python\nDESCRIPTION: This snippet uses the OpenVINO GenAI WhisperPipeline to perform speech recognition on the CPU.  It relies on the `CodeExamplePython` component, passing 'CPU' as the device parameter. It assumes that the component contains the necessary Python code to construct the pipeline, load the model, and perform inference.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_run_model/index.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n<CodeExamplePython device=\"CPU\" />\n```\n\n----------------------------------------\n\nTITLE: Run Whisper Speech Recognition (Shell)\nDESCRIPTION: This command executes the `whisper_speech_recognition.py` script, passing the model name `whisper-base` and the audio file `how_are_you_doing_today.wav` as arguments. This script uses the Whisper model for speech recognition on the given audio file.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/whisper_speech_recognition/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npython whisper_speech_recognition.py whisper-base how_are_you_doing_today.wav\n```\n\n----------------------------------------\n\nTITLE: Streaming Output with Custom Class in C++\nDESCRIPTION: This C++ code demonstrates how to stream the output of the LLMPipeline using a custom class that inherits from `ov::genai::StreamerBase`.  The `put` method in the class is called for each generated token, and the `end` method is called when generation is complete.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/streamer_base.hpp\"\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nclass CustomStreamer: public ov::genai::StreamerBase {\npublic:\n    bool put(int64_t token) {\n        // Custom decoding/tokens processing logic.\n\n        // Returns a flag whether generation should be stopped, if true generation stops.\n        return false;\n    };\n\n    void end() {\n        // Custom finalization logic.\n    };\n};\n\nint main(int argc, char* argv[]) {\n    CustomStreamer custom_streamer;\n\n    std::string models_path = argv[1];\n    ov::genai::LLMPipeline pipe(models_path, \"CPU\");\n    std::cout << pipe.generate(\"The Sun is yellow because\", ov::genai::max_new_tokens(15), ov::genai::streamer(custom_streamer));\n}\n```\n\n----------------------------------------\n\nTITLE: Set RPATH for Python Module in CMake\nDESCRIPTION: This snippet sets the RPATH or LC_RPATH for the Python module, allowing it to find the shared libraries at runtime. The path depends on the operating system and whether a pip package is being built.  If building a pip package on macOS, it appends the path to `libopenvino.dylib` within the `openvino` package.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/python/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(LINUX)\n    # to find libopenvino_genai.so in the same folder\n    set(py_rpaths \"$ORIGIN\")\nelif(APPLE)\n    # to find libopenvino_genai.dylib in the same folder\n    set(py_rpaths \"@loader_path\")\n    if(DEFINED PY_BUILD_CMAKE_PACKAGE_NAME)\n        # in case we build pip package, we need to refer to libopenvino.dylib from 'openvino' package\n        list(APPEND py_rpaths \"@loader_path/../openvino/libs\")\n    endif()\nendif()\n\nif(py_rpaths)\n    set_target_properties(${TARGET_NAME} PROPERTIES INSTALL_RPATH \"${py_rpaths}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Model Properties Output\nDESCRIPTION: This snippet displays the expected output when `OPENVINO_LOG_LEVEL` is set to a value greater than `ov::log::Level::WARNING`. It provides information about the compiled model, including its name, optimal number of inference requests, number of streams, inference precision, and other execution-related parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/debug-logging.mdx#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nNETWORK_NAME: Model0\nOPTIMAL_NUMBER_OF_INFER_REQUESTS: 1\nNUM_STREAMS: 1\nINFERENCE_NUM_THREADS: 48\nPERF_COUNT: NO\nINFERENCE_PRECISION_HINT: bf16\nPERFORMANCE_HINT: LATENCY\nEXECUTION_MODE_HINT: PERFORMANCE\nPERFORMANCE_HINT_NUM_REQUESTS: 0\nENABLE_CPU_PINNING: YES\nSCHEDULING_CORE_TYPE: ANY_CORE\nMODEL_DISTRIBUTION_POLICY:\nENABLE_HYPER_THREADING: NO\nEXECUTION_DEVICES: CPU\nCPU_DENORMALS_OPTIMIZATION: NO\nLOG_LEVEL: LOG_NONE\nCPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1\nDYNAMIC_QUANTIZATION_GROUP_SIZE: 32\nKV_CACHE_PRECISION: f16\nAFFINITY: CORE\nEXECUTION_DEVICES:\nCPU: Intel(R) Xeon(R) Platinum 8468\n```\n\n----------------------------------------\n\nTITLE: Build OpenVINO GenAI project\nDESCRIPTION: Builds the OpenVINO GenAI project using CMake. `--config Release` specifies a release build configuration, and `-j` enables parallel compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build ./build/ --config Release -j\n```\n\n----------------------------------------\n\nTITLE: Set OpenVINO environment variables manually (Windows CMD)\nDESCRIPTION: Sets the required environment variables for OpenVINO manually in the Windows Command Prompt.  This includes `OpenVINO_DIR`, `PYTHONPATH`, `OPENVINO_LIB_PATHS`, and `PATH`.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_11\n\nLANGUAGE: cmd\nCODE:\n```\nset OpenVINO_DIR=<INSTALL_DIR>\\runtime\nset PYTHONPATH=<INSTALL_DIR>\\python;%CD%\\build;%PYTHONPATH%\nset OPENVINO_LIB_PATHS=<INSTALL_DIR>\\bin\\intel64\\Release;%OPENVINO_LIB_PATHS%\nset PATH=%OPENVINO_LIB_PATHS%;%PATH%\n```\n\n----------------------------------------\n\nTITLE: Install export requirements\nDESCRIPTION: This command installs the necessary Python packages specified in the export-requirements.txt file. The `--upgrade-strategy eager` flag ensures that optimum-intel is upgraded to the latest version, which is often required for model conversion tasks.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade-strategy eager -r ../../export-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO with Python Bindings\nDESCRIPTION: This snippet describes the process of building OpenVINO with Python bindings. It involves creating a build directory, navigating into it, and then running CMake and Make commands. The `{ov_build_type}` variable should be replaced with the desired build type (e.g., Release, Debug).\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\ncd /path/to/openvino\nmkdir build\ncd build\ncmake -DCMAKE_BUILD_TYPE={ov_build_type} ..\nmake -j24\n```\n\n----------------------------------------\n\nTITLE: Setting Architecture Directory in CMake\nDESCRIPTION: This snippet determines the architecture directory based on the CMAKE_SYSTEM_PROCESSOR variable. It normalizes architecture names and handles platform-specific variations for Windows, macOS, and Linux.  It accounts for x86_64 and arm64 architectures and handles multi-configuration generators.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nstring(TOLOWER \"${CMAKE_SYSTEM_PROCESSOR}\" ARCH_DIR)\nif(ARCH_DIR MATCHES \"amd64.*|x86_64.*|AMD64.*\")\n    set(ARCH_DIR intel64)\nelif(ARCH_DIR MATCHES \"^(arm64.*|aarch64.*|AARCH64.*|ARM64.*)\")\n    if(APPLE)\n        set(ARCH_DIR \"arm64\")\n    else()\n        set(ARCH_DIR \"aarch64\")\n    endif()\nelif(ARCH_DIR STREQUAL \"x86_64\" OR ARCH_DIR STREQUAL \"amd64\"  # Windows detects Intel's 64-bit CPU as AMD64\n        OR CMAKE_OSX_ARCHITECTURES STREQUAL \"x86_64\")\n    set(ARCH_DIR intel64)\nendif()\n\nif(WIN32 OR APPLE)\n    if(GENERATOR_IS_MULTI_CONFIG_VAR)\n        set(ARCH_DIR ${ARCH_DIR}/$<CONFIG>)\n    else()\n        set(ARCH_DIR ${ARCH_DIR}/${CMAKE_BUILD_TYPE})\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Default Chat Template\nDESCRIPTION: This is a default chat template in JSON format. It is used to format the input and output of the chat model. This template may need to be adjusted based on the specific model being used.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n\"chat_template\": \"{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n<|im_start|>assistant\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|im_end|>\\n'}}{% endif %}{% endfor %}\"\n```\n\n----------------------------------------\n\nTITLE: Declaring Python Package Dependencies\nDESCRIPTION: This snippet declares Python package dependencies for the `openvino.genai` project. It includes direct package names along with version constraints for `transformers` and `diffusers`. Additionally, it specifies `--extra-index-url` options to include custom package repositories, particularly for PyTorch and OpenVINO builds.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/requirements.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cpu\nnumpy\n--extra-index-url https://storage.openvinotoolkit.org/simple/wheels/pre-release\n--extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\nopenvino\nopenvino-tokenizers\nopenvino_genai\npillow\ntorch\ntransformers>=4.40.0\ndiffusers>=0.22.0\n#optimum is in dependency list of optimum-intel \ngit+https://github.com/huggingface/optimum-intel.git@main#egg=optimum-intel\ngit+https://github.com/openvinotoolkit/nncf.git@develop#egg=nncf\npackaging\npsutil\ntimm\ntiktoken\nlibrosa # For Whisper\nmatplotlib\n```\n\n----------------------------------------\n\nTITLE: Installing License Files\nDESCRIPTION: Installs the LICENSE and third-party-programs.txt files to the docs/licensing directory in the installation.  These files are renamed during installation. `COMPONENT licensing_genai` associates these files with a specific component for packaging purposes.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES LICENSE DESTINATION docs/licensing COMPONENT licensing_genai RENAME LICENSE-GENAI)\ninstall(FILES third-party-programs.txt DESTINATION docs/licensing COMPONENT licensing_genai RENAME third-party-programs-genai.txt)\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO GenAI from source using pip\nDESCRIPTION: Installs the OpenVINO GenAI package directly from the source code using pip. This command installs the package in editable mode.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_25\n\nLANGUAGE: sh\nCODE:\n```\npython -m pip install .\n```\n\n----------------------------------------\n\nTITLE: Fetching and Configuring gguf-tools Dependency using FetchContent\nDESCRIPTION: This snippet fetches and configures the gguf-tools library using CMake's FetchContent module, conditionally enabled by the `ENABLE_GGUF` flag. It declares the dependency, makes it available, and sets the target properties.  It also adds a static library target for gguf-tools and sets compiler options.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/cpp/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_GGUF)\n    FetchContent_Declare(\n      gguflib\n      URL https://github.com/Lourdle/gguf-tools/archive/bac796ada809ac293e685db59b075971181cb008.zip\n      URL_HASH SHA256=4d6eab5055468d222833f3f83fe2f7909ccd06114278c2c0b468570ef002c22d)\n    FetchContent_MakeAvailable(gguflib)\n    set_target_properties(gguf-tools PROPERTIES EXCLUDE_FROM_ALL ON)\n\n    add_library(gguflib STATIC ${gguflib_SOURCE_DIR}/fp16.c ${gguflib_SOURCE_DIR}/gguflib.c)\n    set_target_properties(gguflib PROPERTIES POSITION_INDEPENDENT_CODE ON)\n    target_include_directories(gguflib PUBLIC \"${gguflib_SOURCE_DIR}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO GenAI using CMake\nDESCRIPTION: Installs the built OpenVINO GenAI artifacts using CMake. `--prefix <INSTALL_DIR>` specifies the installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\ncmake --install ./build/ --config Release --prefix <INSTALL_DIR>\n```\n\n----------------------------------------\n\nTITLE: Source OpenVINO setupvars script (Linux/macOS)\nDESCRIPTION: Sources the `setupvars.sh` script from the OpenVINO installation directory. This script sets up the environment variables required for using OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nsource <INSTALL_DIR>/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Files in CMake\nDESCRIPTION: This CMake snippet installs required text files into the specified destination directory. It copies `deployment-requirements.txt`, `export-requirements.txt`, and `requirements.txt` to the `samples` directory and tags them as part of the `cpp_samples_genai` component for later uninstallation or packaging.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(FILES\n        deployment-requirements.txt\n        export-requirements.txt\n        requirements.txt\n    DESTINATION samples\n    COMPONENT cpp_samples_genai)\n```\n\n----------------------------------------\n\nTITLE: Converting and compressing text generation model using Optimum CLI\nDESCRIPTION: These commands use the Optimum CLI tool to export and convert a text generation model (TinyLlama-Chat-v1.0) to the OpenVINO format. The first command converts the model to FP16 precision, while the second converts and compresses it to INT4 for improved performance.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n#(Basic) download and convert to OpenVINO TinyLlama-Chat-v1.0 model\noptimum-cli export openvino --model \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" --weight-format fp16 --trust-remote-code \"TinyLlama-1.1B-Chat-v1.0\"\n\n#(Recommended) download, convert to OpenVINO and compress to int4 TinyLlama-Chat-v1.0 model\noptimum-cli export openvino --model \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" --weight-format int4 --trust-remote-code \"TinyLlama-1.1B-Chat-v1.0\"\n```\n\n----------------------------------------\n\nTITLE: Chat Template Example\nDESCRIPTION: Example chat template for use with LLMs, formatted as a JSON string.  This template structures the input and output for chat models.  It iterates through messages, distinguishing between user and assistant roles, and adds appropriate start and end tokens.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n\"chat_template\": \"{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n<|im_start|>assistant\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|im_end|>\\n'}}{% endif %}{% endfor %}\"\n```\n\n----------------------------------------\n\nTITLE: Defining a CMake Function to Add Executable\nDESCRIPTION: This CMake function `add_sample_executable` creates an executable target, links it against the `openvino::genai` library, sets properties for runtime paths on macOS, and defines installation rules.  The function takes the target name as input, builds the executable from `${target_name}.cpp`, and installs it into the `samples_bin` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(add_sample_executable target_name)\n    add_executable(${target_name} ${target_name}.cpp)\n    target_link_libraries(${target_name} PRIVATE openvino::genai)\n    set_target_properties(${target_name} PROPERTIES\n        # Ensure out-of-box LC_RPATH on macOS with SIP\n        INSTALL_RPATH_USE_LINK_PATH ON)\n    install(TARGETS ${target_name}\n            RUNTIME DESTINATION samples_bin/\n            COMPONENT samples_bin\n            EXCLUDE_FROM_ALL)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Control Number of Samples\nDESCRIPTION: This command uses the `--num-samples` parameter to control the number of samples used for ground truth generation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\nwwb --base-model meta-llama/Llama-2-7b-chat-hf --gt-data llama_2_7b_wwb_gt.csv --num-samples 10\n```\n\n----------------------------------------\n\nTITLE: Build OpenVINO archive with GenAI\nDESCRIPTION: Builds the OpenVINO archive with the GenAI module. The `-j` flag allows for parallel compilation, speeding up the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build ./build --target package -j\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenVINO\nDESCRIPTION: This snippet sets the PYTHONPATH, LD_LIBRARY_PATH, and OpenVINO_DIR environment variables. These variables are crucial for the proper functioning of OpenVINO and its related tools. The `{ov_build_type}` variable should be replaced with the same value used in the OpenVINO build process (e.g., Release, Debug).\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/path/to/openvino/bin/intel64/{ov_build_type}\nexport PYTHONPATH=${PYTHONPATH}:/path/to/openvino/bin/intel64/Release/python:/path/to/openvino/tools/ovc\nexport OpenVINO_DIR=/path/to/openvino/{ov_build_type}\n```\n\n----------------------------------------\n\nTITLE: Set OpenVINO environment variables manually (macOS)\nDESCRIPTION: Sets the required environment variables for OpenVINO manually on macOS systems. `OpenVINO_DIR` points to the runtime directory, `PYTHONPATH` includes the OpenVINO Python bindings, and `DYLD_LIBRARY_PATH` includes the necessary shared libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nexport OpenVINO_DIR=<INSTALL_DIR>/runtime\nexport PYTHONPATH=<INSTALL_DIR>/python:./build/:$PYTHONPATH\nexport DYLD_LIBRARY_PATH=<INSTALL_DIR>/runtime/lib/intel64:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables (Linux)\nDESCRIPTION: Sets environment variables required for OpenVINO GenAI development on Linux. It defines `OpenVINO_DIR`, `PYTHONPATH`, and `LD_LIBRARY_PATH`, pointing to the OpenVINO runtime, Python bindings, and shared libraries, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/BUILD.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nexport OpenVINO_DIR=<INSTALL_DIR>/runtime\nexport PYTHONPATH=<INSTALL_DIR>/python:./build/:$PYTHONPATH\nexport LD_LIBRARY_PATH=<INSTALL_DIR>/runtime/lib/intel64:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Clone OpenVINO GenAI repository\nDESCRIPTION: Clones the OpenVINO GenAI repository from GitHub recursively, ensuring all submodules are initialized. Then changes the current directory to the cloned repository.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/openvinotoolkit/openvino.genai.git\ncd openvino.genai\n```\n\n----------------------------------------\n\nTITLE: Generating CMake Project for Continuous Batching\nDESCRIPTION: This snippet generates the CMake project for the continuous batching component. It configures the build type and specifies the OpenVINO directory.  The build type is set to Debug and the OpenVINO_DIR variable points to the OpenVINO build directory. The correct path for openvino.genai must be substituted.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\ncd build\ncmake -DCMAKE_BUILD_TYPE=Debug -DOpenVINO_DIR=/path/to/openvino/build ..\n```\n\n----------------------------------------\n\nTITLE: Visual Language Generation with VLMPipeline in C++\nDESCRIPTION: This C++ code snippet demonstrates visual language generation using the VLMPipeline API. It loads an image, creates an OpenVINO tensor from it, and then generates text based on a prompt, incorporating the image as context.  Requires C++ compatible package installation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/visual_language/pipeline.hpp\"\n#include \"load_image.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n    std::string models_path = argv[1];\n    ov::genai::VLMPipeline pipe(models_path, \"CPU\");\n    ov::Tensor rgb = utils::load_image(argv[2]);\n    std::cout << pipe.generate(\n        prompt,\n        ov::genai::image(rgb),\n        ov::genai::max_new_tokens(100)\n    ) << '\\n';\n}\n```\n\n----------------------------------------\n\nTITLE: Fixing AttributeError in chatglm2-6b Tokenizer (Python)\nDESCRIPTION: This snippet resolves an AttributeError: can't set attribute error encountered when running chatglm2-6b models. The fix involves modifying the tokenization_chatglm.py file by adding code to prevent certain tokens from being passed as keyword arguments to the SPTokenizer constructor. This resolves the attribute error during model initialization.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/doc/NOTES.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n          self.vocab_file = vocab_file\n          self.tokenizer = SPTokenizer(vocab_file)\n +        kwargs.pop(\"eos_token\", None)\n +        kwargs.pop(\"pad_token\", None)\n +        kwargs.pop(\"unk_token\", None)\n          self.special_tokens = {\n              \"<bos>\": self.tokenizer.bos_id,\n              \"<eos>\": self.tokenizer.eos_id,\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Performance Metrics Output\nDESCRIPTION: This snippet shows the performance metrics output when executing the Speculative Decoding or Prompt Lookup pipeline.  The metrics include total duration, draft/main model duration, acceptance rate, tokens per second, and accepted tokens.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/debug-logging.mdx#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n===============================\nTotal duration, sec: 26.6217\nDraft model duration, sec: 1.60329\nMain model duration, sec: 25.0184\nDraft model duration, %: 6.02248\nMain model duration, %: 93.9775\nAVG acceptance rate, %: 21.6809\n===============================\nREQUEST_ID: 0\nMain model iterations: 47\nToken per sec: 3.75633\nAVG acceptance rate, %: 21.6809\nAccepted tokens by draft model: 51\nGenerated tokens: 100\nAccepted token rate, %: 51\n===============================\nRequest_id: 0 ||| 40 0 40 20 0 0 40 40 0 20 20 20 0 40 0 0 20 80 0 80 20 0 0 0 40 80 0 40 60 40 80 0 0 0 0 40 20 20 0 40 20 40 0 20 0 0 0\n```\n\n----------------------------------------\n\nTITLE: Iterating over Sample List and Creating Executables\nDESCRIPTION: This CMake code iterates over a list of sample names defined in `SAMPLE_LIST`. For each sample name, it calls the `add_sample_executable` function to create the corresponding executable, link it against the `openvino::genai::c` library, and configure installation properties as defined in that function. The list contains the names of the C files used to create the executable samples.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/c/text_generation/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset (SAMPLE_LIST\n    greedy_causal_lm_c\n    chat_sample_c\n    benchmark_genai_c)\n\nforeach(sample IN LISTS SAMPLE_LIST)\n    add_sample_executable(${sample})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Image Generation with Text2ImagePipeline in Python\nDESCRIPTION: This Python code snippet uses the openvino-genai library to perform image generation using the Text2ImagePipeline API. It initializes the pipeline with a specified model path and device (CPU in this example), and then generates an image based on a given text prompt. The generated image is saved to a BMP file.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nfrom PIL import Image\nimport openvino_genai\n\ndevice = 'CPU'  # GPU can be used as well\npipe = openvino_genai.Text2ImagePipeline(\"./dreamlike_anime_1_0_ov/INT8\", device)\nimage_tensor = pipe.generate(\"cyberpunk cityscape like Tokyo New York with tall buildings at dusk golden hour cinematic lighting\")\n\nimage = Image.fromarray(image_tensor.data[0])\nimage.save(\"image.bmp\")\n```\n\n----------------------------------------\n\nTITLE: Creating Heterogeneous Stable Diffusion Executable with CMake\nDESCRIPTION: This snippet creates an executable named 'heterogeneous_stable_diffusion' from 'heterogeneous_stable_diffusion.cpp' and 'imwrite.cpp'.  It specifies the include directories and links the executable with the 'openvino::genai' and 'indicators::indicators' libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(heterogeneous_stable_diffusion\n    heterogeneous_stable_diffusion.cpp\n    imwrite.cpp)\n\ntarget_include_directories(heterogeneous_stable_diffusion PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}\")\ntarget_link_libraries(heterogeneous_stable_diffusion PRIVATE openvino::genai indicators::indicators)\n\nset_target_properties(heterogeneous_stable_diffusion PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n\ninstall(TARGETS heterogeneous_stable_diffusion\n        RUNTIME DESTINATION samples_bin/\n        COMPONENT samples_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Configure CMake for OpenVINO GenAI build\nDESCRIPTION: Configures CMake to build the OpenVINO GenAI project.  `-DCMAKE_BUILD_TYPE=Release` specifies a release build. `-S` specifies the source directory and `-B` specifies the build directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_BUILD_TYPE=Release -S ./ -B ./build/\n```\n\n----------------------------------------\n\nTITLE: Include Directories\nDESCRIPTION: Specifies the include directories for the target.  This sets the include directories for the project, allowing the compiler to find the necessary header files.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\n# Include directories\ntarget_include_directories(${PROJECT_NAME} PRIVATE\n  \"${node-api-headers_SOURCE_DIR}/include\"\n  \"${node-addon-api_SOURCE_DIR}\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Install macOS dependencies using brew\nDESCRIPTION: Installs coreutils and scons using the brew package manager on macOS. These are additional dependencies required for building OpenVINO GenAI on macOS.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbrew install coreutils scons\n```\n\n----------------------------------------\n\nTITLE: Set OpenVINO Log Level on Windows\nDESCRIPTION: This snippet shows how to set the `OPENVINO_LOG_LEVEL` environment variable to 3 (INFO) on Windows systems using the `set` command. This enables INFO level logging, as well as ERR and WARNING levels.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/debug-logging.mdx#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nset OPENVINO_LOG_LEVEL=3\n```\n\n----------------------------------------\n\nTITLE: Installing C++ Directories in CMake\nDESCRIPTION: This CMake snippet installs the C++ sample directories into the `samples/cpp` destination. It includes the directories for text generation, image generation, visual language chat, and whisper speech recognition, and designates them as part of the `cpp_samples_genai` component.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(DIRECTORY\n            cpp/text_generation\n            cpp/image_generation\n            cpp/visual_language_chat\n            cpp/whisper_speech_recognition\n        DESTINATION samples/cpp COMPONENT cpp_samples_genai)\n```\n\n----------------------------------------\n\nTITLE: Whisper pipeline generation in C++\nDESCRIPTION: This C++ code demonstrates using the Whisper pipeline for speech-to-text. It reads a WAV file, initializes the pipeline, and generates text from the audio input. Requires custom audio_utils functions for reading the audio file.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\n#include <iostream>\n\n#include \"audio_utils.hpp\"\n#include \"openvino/genai/whisper_pipeline.hpp\"\n\nint main(int argc, char* argv[]) {\n    std::filesystem::path models_path = argv[1];\n    std::string wav_file_path = argv[2];\n    std::string device = \"CPU\"; // GPU can be used as well\n\n    ov::genai::WhisperPipeline pipeline(models_path, device);\n\n    ov::genai::RawSpeechInput raw_speech = utils::audio::read_wav(wav_file_path);\n\n    std::cout << pipeline.generate(raw_speech, ov::genai::max_new_tokens(100)) << '\\n';\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading LLM Models with Optimum CLI\nDESCRIPTION: This snippet uses the Optimum CLI to export an OpenVINO model.  It downloads the facebook/opt-125m model and saves it to the ./ov_model directory. Requires the optimum-cli tool to be installed and configured.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ncd /workspace/openvino.genai/text_generation/causal_lm/cpp/continuous_batching/\noptimum-cli export openvino --model facebook/opt-125m ./ov_model\n```\n\n----------------------------------------\n\nTITLE: Downloading stb_image Library CMake\nDESCRIPTION: This snippet downloads the stb_image.h header file from a given URL and stores it in the CMAKE_BINARY_DIR. The EXPECTED_HASH argument ensures the integrity of the downloaded file by verifying its MD5 hash. This header is a dependency for image loading in the sample applications.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/visual_language_chat/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(DOWNLOAD\n    https://raw.githubusercontent.com/nothings/stb/f75e8d1cad7d90d72ef7a4661f1b994ef78b4e31/stb_image.h\n    ${CMAKE_BINARY_DIR}/stb_image.h\n    EXPECTED_HASH MD5=27932e6fb3a2f26aee2fc33f2cb4e696)\n```\n\n----------------------------------------\n\nTITLE: Declaring and Making Available cxxopts Dependency\nDESCRIPTION: This snippet uses `FetchContent_Declare` to declare the 'cxxopts' library as an external dependency, specifying its URL and SHA256 hash. `FetchContent_MakeAvailable` then handles downloading and making the library available for use.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(FetchContent)\n\nif(POLICY CMP0135)\n    cmake_policy(SET CMP0135 NEW)\nendif()\n\nFetchContent_Declare(cxxopts\n    URL https://github.com/jarro2783/cxxopts/archive/refs/tags/v3.1.1.tar.gz\n    URL_HASH SHA256=523175f792eb0ff04f9e653c90746c12655f10cb70f1d5e6d6d9491420298a08)\nFetchContent_MakeAvailable(cxxopts)\n```\n\n----------------------------------------\n\nTITLE: Install huggingface-hub\nDESCRIPTION: Installs the huggingface-hub package, required for downloading models and related files from the Hugging Face Model Hub.  This is a prerequisite for downloading pre-converted OpenVINO models.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install huggingface-hub\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties based on Package Configuration\nDESCRIPTION: This snippet conditionally sets the suffix property for the shared library based on whether it is being built as part of a Python package for Linux or Apple. It appends version information to the suffix.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/cpp/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(DEFINED PY_BUILD_CMAKE_PACKAGE_NAME AND LINUX)\n    # Don't pack symlinks but append version hash to the name for wheel\n    set_target_properties(${TARGET_NAME} PROPERTIES\n        SUFFIX ${CMAKE_SHARED_LIBRARY_SUFFIX}.${MAJOR_SUFFIX}${OpenVINOGenAI_VERSION_MINOR}${OpenVINOGenAI_VERSION_PATCH})\nelseif(DEFINED PY_BUILD_CMAKE_PACKAGE_NAME AND APPLE)\n    set_target_properties(${TARGET_NAME} PROPERTIES\n        SUFFIX .${MAJOR_SUFFIX}${OpenVINOGenAI_VERSION_MINOR}${OpenVINOGenAI_VERSION_PATCH}${CMAKE_SHARED_LIBRARY_SUFFIX})\nelse()\n    set_target_properties(${TARGET_NAME} PROPERTIES\n        VERSION ${OpenVINOGenAI_VERSION}\n        SOVERSION ${MAJOR_SUFFIX}${OpenVINOGenAI_VERSION_MINOR}${OpenVINOGenAI_VERSION_PATCH})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up environment and building samples in Windows Command Prompt\nDESCRIPTION: These commands sets up the environment by running `setupvars.bat` and then builds the C samples using the `build_samples_msvc.bat` script in Windows Command Prompt.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/c/text_generation/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n<INSTALL_DIR>\\setupvars.bat\n<INSTALL_DIR>\\samples\\c\\build_samples_msvc.bat\n```\n\n----------------------------------------\n\nTITLE: Set PYTHONPATH and DYLD_LIBRARY_PATH for GenAI (macOS)\nDESCRIPTION: Sets the `PYTHONPATH` and `DYLD_LIBRARY_PATH` environment variables manually for OpenVINO GenAI on macOS. These variables are necessary for the system to find the Python modules and shared libraries of GenAI.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nexport PYTHONPATH=<GENAI_ROOT_DIR>/build:$PYTHONPATH\nexport DYLD_LIBRARY_PATH=<GENAI_ROOT_DIR>/build/openvino_genai:$DYLD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment and Installing Requirements\nDESCRIPTION: This snippet creates a Python virtual environment, activates it, and installs the necessary dependencies. It assumes that Python 3 and pip are installed. It also references an important note to comment out `openvino` and `openvino_tokenizers` if they're present in the requirements file. The correct path for openvino.genai must be substituted.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\ncd /path/to/openvino.genai/text_generation/causal_lm/cpp/continuous_batching\npython3 -m venv .env\nsource .env/bin/activate\npip3 install -r python/tests/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories\nDESCRIPTION: Adds subdirectories to the build.  These subdirectories contain the source code, samples, and tests for the project. The addition of samples and continuous batching tools is conditional based on the existence of their directories and the value of ENABLE_SAMPLES.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(thirdparty)\nadd_subdirectory(src)\nif(EXISTS \"${OpenVINOGenAI_SOURCE_DIR}/samples\" AND ENABLE_SAMPLES)\n    add_subdirectory(samples)\nendif()\nif(EXISTS \"${OpenVINOGenAI_SOURCE_DIR}/tools/continuous_batching\")\n    add_subdirectory(tools/continuous_batching)\nendif()\nif(EXISTS \"${OpenVINOGenAI_SOURCE_DIR}/tests/cpp\")\n    add_subdirectory(tests/cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Whisper pipeline generation in Python\nDESCRIPTION: This code snippet shows how to use the Whisper pipeline in Python to perform speech-to-text recognition. It loads an audio file, initializes the pipeline, processes the audio data, and prints the generated text. Requires librosa for audio processing.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai\nimport librosa\n\ndef read_wav(filepath):\n    raw_speech, samplerate = librosa.load(filepath, sr=16000)\n    return raw_speech.tolist()\n\ndevice = \"CPU\" # GPU can be used as well\npipe = openvino_genai.WhisperPipeline(\"whisper-base\", device)\nraw_speech = read_wav(\"sample.wav\")\nprint(pipe.generate(raw_speech))\n```\n\n----------------------------------------\n\nTITLE: Listing Samples and Adding Executables in CMake\nDESCRIPTION: This snippet defines a list of sample names and then iterates through the list, calling the `add_sample_executable` function for each sample. The `add_sample_executable` function creates the necessary executables.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset (SAMPLE_LIST\n    greedy_causal_lm\n    encrypted_model_causal_lm\n    beam_search_causal_lm\n    chat_sample\n    lora_greedy_causal_lm\n    multinomial_causal_lm\n    prompt_lookup_decoding_lm\n    speculative_decoding_lm)\n\nforeach(sample IN LISTS SAMPLE_LIST)\n    add_sample_executable(${sample})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Calling OpenVINO setupvars script (Windows PowerShell)\nDESCRIPTION: Sets up the environment by calling the OpenVINO `setupvars.ps1` script. This script configures environment variables required for OpenVINO development, such as paths to libraries and binaries. `<INSTALL_DIR>` should be replaced with the actual installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/BUILD.md#_snippet_3\n\nLANGUAGE: cmd\nCODE:\n```\n. <INSTALL_DIR>/setupvars.ps1\n```\n\n----------------------------------------\n\nTITLE: Extending PYTHONPATH for Continuous Batching\nDESCRIPTION: This snippet extends the PYTHONPATH environment variable to include the Python bindings generated by the continuous batching build process. This allows Python to find and import the generated modules.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\nexport PYTHONPATH=${PYTHONPATH}:/path/to/openvino.genai/text_generation/causal_lm/cpp/continuous_batching/build/python\n```\n\n----------------------------------------\n\nTITLE: Declaring and Making Available indicators Dependency\nDESCRIPTION: This snippet uses `FetchContent_Declare` to declare an external dependency, the 'indicators' library, specifying its URL and SHA256 hash for verification. `FetchContent_MakeAvailable` then makes the library available for use in the project, handling the download and build process.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(FetchContent)\n\nif(POLICY CMP0135)\n    cmake_policy(SET CMP0135 NEW)\nendif()\n\nFetchContent_Declare(indicators\n    URL https://github.com/p-ranav/indicators/archive/refs/tags/v2.3.tar.gz\n    URL_HASH SHA256=70da7a693ff7a6a283850ab6d62acf628eea17d386488af8918576d0760aef7b)\nFetchContent_MakeAvailable(indicators)\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables (Windows PowerShell)\nDESCRIPTION: Sets environment variables required for OpenVINO GenAI development in Windows PowerShell. Defines `$env:OpenVINO_DIR`, `$env:PYTHONPATH`, `$env:OPENVINO_LIB_PATHS` and `$env:PATH`, pointing to the OpenVINO runtime, Python bindings and binaries, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/BUILD.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n$env:OpenVINO_DIR = \"<INSTALL_DIR>\\runtime\"\n$env:PYTHONPATH = \"<INSTALL_DIR>\\python;$PWD\\build;$env:PYTHONPATH\"\n$env:OPENVINO_LIB_PATHS = \"<INSTALL_DIR>\\bin\\intel64\\Release;$env:OPENVINO_LIB_PATHS\"\n$env:PATH = \"$env:OPENVINO_LIB_PATHS;$env:PATH\"\n```\n\n----------------------------------------\n\nTITLE: Fetching cxxopts Library in CMake\nDESCRIPTION: This snippet uses `FetchContent` to download and make available the `cxxopts` library, which is used for command-line option parsing. It specifies the URL and SHA256 hash for verification.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(FetchContent)\n\nif(POLICY CMP0135)\n    cmake_policy(SET CMP0135 NEW)\nendif()\n\nFetchContent_Declare(cxxopts\n    URL https://github.com/jarro2783/cxxopts/archive/refs/tags/v3.1.1.tar.gz\n    URL_HASH SHA256=523175f792eb0ff04f9e653c90746c12655f10cb70f1d5e6d6d9491420298a08)\nFetchContent_MakeAvailable(cxxopts)\n```\n\n----------------------------------------\n\nTITLE: Instantiate Model from Hugging Face\nDESCRIPTION: This command uses the `--hf` flag to instantiate the model from a Hugging Face model ID or folder using `AutoModelForCausalLM`.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\nwwb --base-model meta-llama/Llama-2-7b-chat-hf --gt-data llama_2_7b_wwb_gt.csv --hf\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables (Windows Command Prompt)\nDESCRIPTION: Sets environment variables required for OpenVINO GenAI development in Windows Command Prompt. Defines `OpenVINO_DIR`, `PYTHONPATH`, `OPENVINO_LIB_PATHS` and `PATH`, pointing to the OpenVINO runtime, Python bindings and binaries, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/BUILD.md#_snippet_6\n\nLANGUAGE: cmd\nCODE:\n```\nset OpenVINO_DIR=<INSTALL_DIR>\\runtime\nset PYTHONPATH=<INSTALL_DIR>\\python;%CD%\\build;%PYTHONPATH%\nset OPENVINO_LIB_PATHS=<INSTALL_DIR>\\bin\\intel64\\Release;%OPENVINO_LIB_PATHS%\nset PATH=%OPENVINO_LIB_PATHS%;%PATH%\n```\n\n----------------------------------------\n\nTITLE: Translation with Whisper Pipeline in C++\nDESCRIPTION: This snippet demonstrates how to translate non-English speech to English using the WhisperPipeline in C++. It sets the task parameter to \"translate\". The input is raw audio data, and the output is the translated text.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nint main() {\n    ov::genai::WhisperPipeline pipe(model_path, \"CPU\");\n\n    // Translate French audio to English\n    raw_speech = utils::audio::read_wav(\"french_sample.wav\");\n    result = pipe.generate(raw_speech, ov::genai::task(\"translate\"));\n}\n```\n\n----------------------------------------\n\nTITLE: GPTQ Dependency Specification\nDESCRIPTION: This line specifies a dependency on the `auto-gptq` library with a minimum version of 0.5.1. This is likely a requirement to enable GPTQ functionality within the `openvino.genai` project. GPTQ is a post-training quantization technique that reduces the size and increases the inference speed of large language models (LLMs).\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/requirements/requirements_gptq.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nauto-gptq>=0.5.1 # for gptq\n```\n\n----------------------------------------\n\nTITLE: Installing Python Directories in CMake\nDESCRIPTION: This CMake snippet installs the Python sample directories into the `samples/python` destination. It includes the directories for text generation, image generation, visual language chat, and whisper speech recognition. The `USE_SOURCE_PERMISSIONS` argument ensures that the installed files retain their source permissions.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(DIRECTORY\n            python/text_generation\n            python/image_generation\n            python/visual_language_chat\n            python/whisper_speech_recognition\n        DESTINATION samples/python COMPONENT cpp_samples_genai\n        USE_SOURCE_PERMISSIONS)\n```\n\n----------------------------------------\n\nTITLE: Adding Sample Executable with CMake Function\nDESCRIPTION: This CMake function `add_sample_executable` defines a reusable block of commands to create an executable from a C source file, link it against the `openvino::genai::c` library, and configure installation properties. The function takes the target name as an argument and sets properties to ensure proper runtime path handling on macOS.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/c/text_generation/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(add_sample_executable target_name)\n    add_executable(${target_name} ${target_name}.c)\n    # Specifies that the source file should be compiled as a C source file\n    set_source_files_properties(${target_name}.c PROPERTIES LANGUAGE C)\n    target_link_libraries(${target_name} PRIVATE openvino::genai::c)\n    set_target_properties(${target_name} PROPERTIES\n        # Ensure out-of-box LC_RPATH on macOS with SIP\n        INSTALL_RPATH_USE_LINK_PATH ON)\n    install(TARGETS ${target_name}\n            RUNTIME DESTINATION samples_bin/\n            COMPONENT samples_bin\n            EXCLUDE_FROM_ALL)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Fetching External Content with FetchContent\nDESCRIPTION: This snippet uses the `FetchContent` module to download and make available the `dr_libs` library if it hasn't already been included as a target. It specifies the URL and SHA256 hash for verification. `FetchContent_MakeAvailable` makes the library available for linking.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(FetchContent)\n\nif(NOT TARGET dr_libs)\n    FetchContent_Declare(dr_libs\n        URL https://github.com/mackron/dr_libs/archive/da35f9d6c7374a95353fd1df1d394d44ab66cf01.tar.gz\n        URL_HASH SHA256=2704d347f480ca1bc92233fb01747e4550cc8031735b6ea62ca9990ebb8851ae)\n    FetchContent_MakeAvailable(dr_libs)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Continuous Batching Project\nDESCRIPTION: This snippet builds the continuous batching project using the generated CMake project. The `-j24` flag specifies that the build process should use 24 parallel jobs to speed up compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\nmake -j24\n```\n\n----------------------------------------\n\nTITLE: Install Python Module Files with CMake\nDESCRIPTION: This snippet installs the Python module files, including the `__init__.py` files and the compiled module itself, to the appropriate location within the installation directory. It also installs license and related files. The component `pygenai_${Python3_VERSION_MAJOR}_${Python3_VERSION_MINOR}` is used to group the files for a specific Python version.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/python/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES \"${CMAKE_CURRENT_SOURCE_DIR}/openvino_genai/__init__.py\"\n              \"${CMAKE_CURRENT_SOURCE_DIR}/openvino_genai/__init__.pyi\"\n              \"${CMAKE_CURRENT_SOURCE_DIR}/openvino_genai/py_openvino_genai.pyi\"\n        DESTINATION python/openvino_genai\n        COMPONENT pygenai_${Python3_VERSION_MAJOR}_${Python3_VERSION_MINOR})\ninstall(TARGETS ${TARGET_NAME}\n        LIBRARY DESTINATION python/openvino_genai\n        COMPONENT pygenai_${Python3_VERSION_MAJOR}_${Python3_VERSION_MINOR})\n\ninstall(FILES \"${OpenVINOGenAI_SOURCE_DIR}/LICENSE\"\n              \"${OpenVINOGenAI_SOURCE_DIR}/third-party-programs.txt\"\n              \"${OpenVINOGenAI_SOURCE_DIR}/SECURITY.md\"\n        DESTINATION \"${PY_BUILD_CMAKE_PACKAGE_NAME}-${PY_BUILD_CMAKE_PACKAGE_VERSION}.dist-info\"\n        COMPONENT wheel_genai\n        EXCLUDE_FROM_ALL)\n\n# wheel_genai component is used for wheel generation in pyproject.toml.\n# Exclude wheel_genai from normal packaging because there's pygenai_X_Y component for that.\ninstall(TARGETS openvino_genai ${TARGET_NAME}\n        LIBRARY DESTINATION openvino_genai COMPONENT wheel_genai EXCLUDE_FROM_ALL\n        RUNTIME DESTINATION openvino_genai COMPONENT wheel_genai EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Translation with Whisper Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to translate non-English speech to English using the WhisperPipeline in Python. It sets the task parameter to \"translate\". The input is raw audio data, and the output is the translated text.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipe = ov_genai.WhisperPipeline(model_path, \"CPU\")\n\n# Translate French audio to English\nraw_speech = read_wav(\"french_sample.wav\")\nresult = pipe.generate(raw_speech, task=\"translate\")\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINO GenAI Package CMake\nDESCRIPTION: This snippet searches for the OpenVINOGenAI package in specified paths. It first looks within the build directory, then in the OpenVINO directory, ensuring the package is found for building the samples.  The REQUIRED keyword ensures that the build fails if the package is not found.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/visual_language_chat/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(OpenVINOGenAI REQUIRED\n    PATHS\n        \"${CMAKE_BINARY_DIR}\"  # Reuse the package from the build.\n        ${OpenVINO_DIR}  # GenAI may be installed alogside OpenVINO.\n    NO_CMAKE_FIND_ROOT_PATH\n)\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO GenAI Node.js bindings\nDESCRIPTION: Configures and builds the OpenVINO GenAI Node.js bindings using CMake.  It sets the build type to Release, enables JavaScript bindings, specifies the NPM packager, defines source and build directories, and then builds and installs the bindings to the ./src/js/bin directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/BUILD.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_BUILD_TYPE=Release \\\n        -DENABLE_JS=ON -DCPACK_GENERATOR=NPM \\\n        -S . -B ./build\ncmake --build ./build --config Release -j\ncmake --install ./build/ --config Release --prefix ./src/js/bin\n```\n\n----------------------------------------\n\nTITLE: Install Optimum Intel with Eager Upgrade Strategy (Bash)\nDESCRIPTION: This command installs the `optimum-intel` package with the `--upgrade-strategy eager` option, ensuring it's upgraded to the latest version. This is required for proper model conversion.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/image_generation/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npip install --upgrade-strategy eager -r ../../export-requirements.txt\noptimum-cli export openvino --model dreamlike-art/dreamlike-anime-1.0 --task stable-diffusion --weight-format fp16 dreamlike_anime_1_0_ov/FP16\n```\n\n----------------------------------------\n\nTITLE: Defining and Linking the Executable\nDESCRIPTION: This snippet defines the target executable named `continuous_batching_benchmark` from the source file `continuous_batching_benchmark.cpp`. It then links the executable against the `openvino::genai`, `nlohmann_json::nlohmann_json`, `cxxopts::cxxopts`, and `Threads::Threads` libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/continuous_batching/benchmark/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME continuous_batching_benchmark)\nadd_executable(${TARGET_NAME} ${TARGET_NAME}.cpp)\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::genai nlohmann_json::nlohmann_json cxxopts::cxxopts Threads::Threads)\n```\n\n----------------------------------------\n\nTITLE: Generate Ground Truth from Internal Questions\nDESCRIPTION: This command generates ground truth data for a baseline model using an internal set of questions, saving the output to a CSV file. It specifies the base model and output file.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\nwwb --base-model meta-llama/Llama-2-7b-chat-hf --gt-data llama_2_7b_wwb_gt.csv\n```\n\n----------------------------------------\n\nTITLE: Installation Configuration in CMake\nDESCRIPTION: This CMake snippet defines the installation rules for the test executable. It specifies the destination directory (`tests/`) and the component to which the executable belongs (`tests`).  The `EXCLUDE_FROM_ALL` option ensures that the test executable is not built as part of the default build process.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/cpp/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TEST_TARGET_NAME}\n        RUNTIME DESTINATION tests/\n        COMPONENT tests\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Run WhisperPipeline on GPU with Python\nDESCRIPTION: This snippet uses the OpenVINO GenAI WhisperPipeline to perform speech recognition on the GPU. It relies on the `CodeExamplePython` component, passing 'GPU' as the device parameter. It assumes that the component contains the necessary Python code to construct the pipeline, load the model, and perform inference.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_run_model/index.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n<CodeExamplePython device=\"GPU\" />\n```\n\n----------------------------------------\n\nTITLE: Start Local Development Server\nDESCRIPTION: This command starts a local development server for the Docusaurus website.  It uses the `start` script defined in `package.json` which typically runs the Docusaurus development server.  Changes made to the source code are usually reflected live in the browser without a full restart.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/README.md#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n$ npm run start\n```\n\n----------------------------------------\n\nTITLE: Set PYTHONPATH and PATH for GenAI (Windows CMD)\nDESCRIPTION: Sets the `PYTHONPATH` and `PATH` environment variables manually for OpenVINO GenAI in the Windows Command Prompt. These variables are necessary for the system to find the Python modules and shared libraries of GenAI.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_18\n\nLANGUAGE: cmd\nCODE:\n```\nset PYTHONPATH=<GENAI_ROOT_DIR>\\build;%PYTHONPATH%\nset PATH=<GENAI_ROOT_DIR>\\build\\openvino_genai;%PATH%\n```\n\n----------------------------------------\n\nTITLE: Creating and Linking Executable\nDESCRIPTION: This snippet creates an executable named `whisper_speech_recognition` from the provided source files. It then links the executable against the `openvino::genai` library and specifies the include directories for the `dr_libs` library.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(whisper_speech_recognition whisper_speech_recognition.cpp audio_utils.cpp)\ntarget_link_libraries(whisper_speech_recognition PRIVATE openvino::genai)\ntarget_include_directories(whisper_speech_recognition PRIVATE \"$<BUILD_INTERFACE:${dr_libs_SOURCE_DIR}>\")\nset_target_properties(whisper_speech_recognition PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\ntarget_compile_features(whisper_speech_recognition PRIVATE cxx_std_11)\n```\n\n----------------------------------------\n\nTITLE: Example OpenVINO Model Properties Log Output\nDESCRIPTION: This example shows the structure and content of the model properties printed when the log level is set high enough. It includes properties such as NETWORK_NAME, OPTIMAL_NUMBER_OF_INFER_REQUESTS, INFERENCE_PRECISION_HINT, and EXECUTION_DEVICES.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DEBUG_LOG.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nNETWORK_NAME: Model0\nOPTIMAL_NUMBER_OF_INFER_REQUESTS: 1\nNUM_STREAMS: 1\nINFERENCE_NUM_THREADS: 48\nPERF_COUNT: NO\nINFERENCE_PRECISION_HINT: bf16\nPERFORMANCE_HINT: LATENCY\nEXECUTION_MODE_HINT: PERFORMANCE\nPERFORMANCE_HINT_NUM_REQUESTS: 0\nENABLE_CPU_PINNING: YES\nSCHEDULING_CORE_TYPE: ANY_CORE\nMODEL_DISTRIBUTION_POLICY:\nENABLE_HYPER_THREADING: NO\nEXECUTION_DEVICES: CPU\nCPU_DENORMALS_OPTIMIZATION: NO\nLOG_LEVEL: LOG_NONE\nCPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1\nDYNAMIC_QUANTIZATION_GROUP_SIZE: 32\nKV_CACHE_PRECISION: f16\nAFFINITY: CORE\nEXECUTION_DEVICES:\nCPU: Intel(R) Xeon(R) Platinum 8468\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINO Package\nDESCRIPTION: Finds the OpenVINO Developer Package with specified version and components (Runtime, Threading). If the developer package is not found, it falls back to finding the standard OpenVINO package.  The paths are specified to help CMake find the package. This uses the path found from python if it was found. This allows compilation with SDL flags if the OpenVINO developer package is found.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenVINODeveloperPackage ${OpenVINOGenAI_VERSION} QUIET\n             COMPONENTS Runtime Threading\n             PATHS \"${OpenVINO_DIR}\")\nif(NOT OpenVINODeveloperPackage_FOUND)\n    find_package(OpenVINO ${OpenVINOGenAI_VERSION} REQUIRED\n                 COMPONENTS Runtime Threading\n                 PATHS \"${OpenVINO_DIR_PY}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Example VLM Benchmark Run\nDESCRIPTION: This shell command executes the `benchmark_vlm.py` script with specific model and image.  The model is 'miniCPM-V-2_6', the image is '319483352-d5fbbd1a-d484-415c-88cb-9986625b7b11.jpg' and the number of iterations is 3.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/visual_language_chat/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython benchmark_vlm.py -m miniCPM-V-2_6 -i 319483352-d5fbbd1a-d484-415c-88cb-9986625b7b11.jpg -n 3\n```\n\n----------------------------------------\n\nTITLE: Fixing AttributeError in Baichuan2-7B-Chat Tokenizer (Python)\nDESCRIPTION: This snippet resolves the AttributeError: 'BaichuanTokenizer' object has no attribute 'sp_model' error when converting Baichuan2-7B-Chat models. The fix requires rearranging the initialization of the tokenizer in `tokenization_baichuan.py`. The `self.sp_model` and related attributes must be initialized before calling the parent class's `__init__` method.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/doc/NOTES.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n         eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n         unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n         pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n+        self.vocab_file = vocab_file\n+        self.add_bos_token = add_bos_token\n+        self.add_eos_token = add_eos_token\n+        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n+        self.sp_model.Load(vocab_file)\n         super().__init__(\n             bos_token=bos_token,\n             eos_token=eos_token,\n             clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n-        self.vocab_file = vocab_file\n-        self.add_bos_token = add_bos_token\n-        self.add_eos_token = add_eos_token\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(vocab_file)\n```\n\n----------------------------------------\n\nTITLE: Running encrypted_model_causal_lm sample Bash\nDESCRIPTION: This command executes the `encrypted_model_causal_lm` sample, providing the model directory and a prompt as arguments. The sample demonstrates loading an encrypted LLM directly from memory and enabling user-defined encryption for the plugin cache.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n./encrypted_model_causal_lm <MODEL_DIR> \"<PROMPT>\"\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories with CMake\nDESCRIPTION: This snippet uses the `add_subdirectory` command in CMake to include the 'accuracy' and 'benchmark' subdirectories in the build process. This command searches for a CMakeLists.txt file in each specified subdirectory and processes it, adding the targets and variables defined there to the current build.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/continuous_batching/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(accuracy)\nadd_subdirectory(benchmark)\n```\n\n----------------------------------------\n\nTITLE: Run Tests for Specific Models\nDESCRIPTION: This command executes tests selectively for specified Hugging Face models. The `--model_ids` argument is used to provide a space-separated list of model IDs. Only tests related to these models will be executed.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npython -m pytest tests/python_tests/ -m nightly -k \"test_multibatch\" --model_ids \"TinyLlama/TinyLlama-1.1B-Chat-v1.0 Qwen/Qwen2-0.5B-Instruct\"\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Inclusion in CMake\nDESCRIPTION: This snippet demonstrates conditional inclusion of subdirectories based on defined variables. It includes cpp, c, python, and js subdirectories based on the ENABLE_PYTHON, ENABLE_JS, and PY_BUILD_CMAKE_PACKAGE_NAME flags. This allows for modular building and only including relevant components.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(cpp)\n\nif(NOT DEFINED PY_BUILD_CMAKE_PACKAGE_NAME)\n    add_subdirectory(c)\nendif()\n\nif(ENABLE_PYTHON)\n    add_subdirectory(python)\nendif()\n\nif(ENABLE_JS)\n    add_subdirectory(js)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO LLM GenAI Docker Image\nDESCRIPTION: This snippet clones the openvino.genai repository, updates submodules, navigates to the continuous batching directory, and builds the project using make. It assumes git and make are installed.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone --branch ct-beam-search https://github.com/ilya-lavrenov/openvino.genai.git\ngit submodule update --remote --init\ncd text_generation/causal_lm/cpp/continuous_batching/\nmake\n```\n\n----------------------------------------\n\nTITLE: Project Declaration\nDESCRIPTION: Declares the OpenVINOGenAI project with its version, description, homepage URL, and supported languages (C++ and C). This is a standard CMake command that defines the project and its basic attributes.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nproject(OpenVINOGenAI\n        VERSION 2025.2.0.0\n        DESCRIPTION \"OpenVINO GenAI\"\n        HOMEPAGE_URL \"https://github.com/openvinotoolkit/openvino.genai\"\n        LANGUAGES CXX C)\n```\n\n----------------------------------------\n\nTITLE: Call OpenVINO setupvars batch script from Archive (Windows CMD)\nDESCRIPTION: Calls the `setupvars.bat` script from the OpenVINO installation directory within the extracted archive. This sets up environment variables for OpenVINO in the Windows Command Prompt and builds samples.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_21\n\nLANGUAGE: cmd\nCODE:\n```\n<INSTALL_DIR>\\setupvars.bat\n<INSTALL_DIR>\\samples\\cpp\\build_samples_msvc.bat\n```\n\n----------------------------------------\n\nTITLE: Set PYTHONPATH and LD_LIBRARY_PATH for GenAI (Linux)\nDESCRIPTION: Sets the `PYTHONPATH` and `LD_LIBRARY_PATH` environment variables manually for OpenVINO GenAI on Linux. These variables are necessary for the system to find the Python modules and shared libraries of GenAI.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nexport PYTHONPATH=<GENAI_ROOT_DIR>/build/:$PYTHONPATH\nexport LD_LIBRARY_PATH=<GENAI_ROOT_DIR>/build/openvino_genai/:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies using npm\nDESCRIPTION: This command installs the project dependencies using npm (Node Package Manager).  It reads the `package.json` file to determine the necessary packages and installs them into the `node_modules` directory. This is a prerequisite for running any other commands.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/README.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n$ npm i\n```\n\n----------------------------------------\n\nTITLE: Installing the Executable\nDESCRIPTION: This snippet installs the `whisper_speech_recognition` executable to the `samples_bin/` directory under the installation prefix. It sets the component to `samples_bin` and excludes it from the default `all` target.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS whisper_speech_recognition\n        RUNTIME DESTINATION samples_bin/\n        COMPONENT samples_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Set OpenVINO environment variables manually (Windows PowerShell)\nDESCRIPTION: Sets the required environment variables for OpenVINO manually in Windows PowerShell. This includes `OpenVINO_DIR`, `PYTHONPATH`, `OPENVINO_LIB_PATHS`, and `PATH`.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\n$env:OpenVINO_DIR = \"<INSTALL_DIR>\\runtime\"\n$env:PYTHONPATH = \"<INSTALL_DIR>\\python;$PWD\\build;$env:PYTHONPATH\"\n$env:OPENVINO_LIB_PATHS = \"<INSTALL_DIR>\\bin\\intel64\\Release;$env:OPENVINO_LIB_PATHS\"\n$env:PATH = \"$env:OPENVINO_LIB_PATHS;$env:PATH\"\n```\n\n----------------------------------------\n\nTITLE: Run Samples Tests with Environment Variables\nDESCRIPTION: This command runs the OpenVINO GenAI samples tests. It sets the `SAMPLES_PY_DIR` and `SAMPLES_CPP_DIR` environment variables to point to the Python and C++ samples directories, respectively. It executes the samples tests using pytest and the `-m samples` marker.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nSAMPLES_PY_DIR=openvino.genai/samples/python SAMPLES_CPP_DIR=openvino.genai/samples_bin python -m pytest tests/python_tests -m samples\n```\n\n----------------------------------------\n\nTITLE: Set Model Path and Run Tests\nDESCRIPTION: This command sets the `GENAI_MODELS_PATH_PREFIX` environment variable to specify the directory where downloaded Hugging Face models will be saved. The `pytest` command then executes the tests, using this specified path for models.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nGENAI_MODELS_PATH_PREFIX=$HOME/test_models python -m pytest tests/python_tests/ -m precommit\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO Tokenizers\nDESCRIPTION: This snippet describes how to build the OpenVINO tokenizers. It involves creating a build directory, running CMake, and using Make. The `{ov_build_type}` variable needs to be replaced with build type such as `Release` or `Debug`.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\ncd /path/to/openvino.genai/thirdparty/openvino_tokenizers\nmkdir build\ncd build\ncmake -DCMAKE_BUILD_TYPE={ov_build_type} ..\nmake -j24\n```\n\n----------------------------------------\n\nTITLE: Creating Shared Library with add_library\nDESCRIPTION: This snippet creates a shared library named `openvino_genai` and sets its properties. It links the object library `openvino_genai_obj` and configures include directories, link libraries, and compile features. It also sets the export name and output directories.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/cpp/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${TARGET_NAME} SHARED $<TARGET_OBJECTS:${TARGET_NAME_OBJ}>)\nadd_library(openvino::genai ALIAS ${TARGET_NAME})\n\ntarget_include_directories(${TARGET_NAME} INTERFACE \"$<INSTALL_INTERFACE:runtime/include>\"\n                                                    \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\"\n                                                    \"$<BUILD_INTERFACE:${OpenVINOGenAI_SOURCE_DIR}/src/cpp/src/gguf_utils>\"\n                                                    \"$<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}>\")\n\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino::runtime PRIVATE openvino::threading nlohmann_json::nlohmann_json jinja2cpp ${CMAKE_DL_LIBS})\n\nif(ENABLE_GGUF):\n  target_link_libraries(${TARGET_NAME} PRIVATE gguflib)\nendif()\n\ntarget_compile_features(${TARGET_NAME} INTERFACE cxx_std_17)\n\nif(TARGET openvino_tokenizers)\n    add_dependencies(${TARGET_NAME} openvino_tokenizers)\nendif()\n\nset_target_properties(${TARGET_NAME} PROPERTIES\n    EXPORT_NAME genai\n    INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO}\n    ARCHIVE_OUTPUT_DIRECTORY \"$<1:${CMAKE_BINARY_DIR}/openvino_genai/>\"\n    LIBRARY_OUTPUT_DIRECTORY \"$<1:${CMAKE_BINARY_DIR}/openvino_genai/>\"\n    RUNTIME_OUTPUT_DIRECTORY \"$<1:${CMAKE_BINARY_DIR}/openvino_genai/>\"\n)\n```\n\n----------------------------------------\n\nTITLE: Install Requirements for Model Conversion (sh)\nDESCRIPTION: Installs the required Python packages for converting models to OpenVINO format using `pip`. The `--upgrade-strategy eager` option ensures that `optimum-intel` is upgraded to the latest version.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install --upgrade-strategy eager -r ../../export-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Extracting Version Suffix in CMake\nDESCRIPTION: This snippet extracts the last two digits from the OpenVINOGenAI_VERSION_MAJOR variable and stores it in MAJOR_SUFFIX. This is done to ensure the SOVERSION variable remains within the maximum length of 4 symbols.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nstring(REGEX MATCH [=[[0-9][0-9]$]=] MAJOR_SUFFIX ${OpenVINOGenAI_VERSION_MAJOR})\n```\n\n----------------------------------------\n\nTITLE: Install Deployment Requirements (sh)\nDESCRIPTION: Installs the required Python packages for deploying and running the GenAI samples using `pip`.  The `--upgrade-strategy eager` option ensures that packages are updated to their latest versions.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install --upgrade-strategy eager -r ../../deployment-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINO GenAI Package using CMake\nDESCRIPTION: This snippet uses the `find_package` command to locate the OpenVINOGenAI library. It specifies potential search paths, including the build directory and a location alongside OpenVINO. The `REQUIRED` keyword ensures that the configuration fails if the package is not found.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(OpenVINOGenAI REQUIRED\n    PATHS\n        \"${CMAKE_BINARY_DIR}\"  # Reuse the package from the build.\n        ${OpenVINO_DIR}  # GenAI may be installed alogside OpenVINO.\n    NO_CMAKE_FIND_ROOT_PATH\n)\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Minimum Version\nDESCRIPTION: Specifies the minimum CMake version required for the project.  This ensures that the CMake features used are supported by the CMake version available on the system. The requirement comes from Jinja2Cpp.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.23.0)\n```\n\n----------------------------------------\n\nTITLE: Build Static Website Content\nDESCRIPTION: This command builds the static content for the Docusaurus website. It uses the `build` script defined in `package.json` which compiles the source code and generates optimized static HTML, CSS, and JavaScript files. The output is placed in the `build` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/README.md#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n$ npm run build\n```\n\n----------------------------------------\n\nTITLE: Run LLM Samples Tests with pytest\nDESCRIPTION: This command executes the LLM samples tests using `pytest`. The `-m llm` flag specifies that only tests marked with the `llm` marker should be executed.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\npython -m pytest tests/python_tests/samples -m llm\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenVINO GenAI Node.js installation\nDESCRIPTION: Verifies the successful installation of the OpenVINO GenAI Node.js package by requiring the `openvino-genai-node` module and printing the `LLMPipeline` object to the console. A successful execution indicates that the module is properly installed and accessible.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/BUILD.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nnode -e \"const { LLMPipeline } = require('openvino-genai-node'); console.log(LLMPipeline);\"\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO GenAI Dependencies\nDESCRIPTION: These lines specify the dependencies for the OpenVINO GenAI project, including version constraints and custom index URLs for accessing pre-release and nightly builds. The `extra-index-url` options configure pip to search for packages in the specified locations. The `openvino_genai`, `librosa`, and `pillow` packages are listed with specific version requirements.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/deployment-requirements.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n--extra-index-url https://storage.openvinotoolkit.org/simple/wheels/pre-release\n--extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\nopenvino_genai~=2025.2.0.0.dev\nlibrosa==0.10.2.post1  # For Whisper\npillow==11.2.1  # Image processing for VLMs\n```\n\n----------------------------------------\n\nTITLE: Defining Linux Platform\nDESCRIPTION: This snippet defines the LINUX variable as ON if the operating system is Unix-like (excluding macOS, Android, and Cygwin). This is used to conditionally include platform-specific configurations or code.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(UNIX AND NOT (APPLE OR ANDROID OR CYGWIN))\n    set(LINUX ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Install and Export Stable Diffusion Model\nDESCRIPTION: This shell script installs the necessary dependencies and exports a Stable Diffusion model to the OpenVINO format using the optimum-cli tool.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install --upgrade-strategy eager -r ../../requirements.txt\noptimum-cli export openvino --model dreamlike-art/dreamlike-anime-1.0 --task stable-diffusion --weight-format fp16 dreamlike_anime_1_0_ov/FP16\n```\n\n----------------------------------------\n\nTITLE: Running Greedy Causal LM\nDESCRIPTION: This command runs the greedy causal language model sample, taking the model directory and prompt as command-line arguments. Assumes that `greedy_causal_lm_c` executable is in the current working directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/c/text_generation/README.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n./greedy_causal_lm_c  model_dir prompt\n```\n\n----------------------------------------\n\nTITLE: Install deployment requirements\nDESCRIPTION: This command installs the Python packages specified in the deployment-requirements.txt file.  The `--upgrade-strategy eager` flag ensures the dependencies required for running the samples are up-to-date.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade-strategy eager -r ../../deployment-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Render SamplesList Component (JSX)\nDESCRIPTION: This JSX snippet renders the `SamplesList` component. This component is responsible for displaying the available OpenVINO GenAI samples on the page. It is likely a React component that fetches or contains data about the samples.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/samples/index.mdx#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<SamplesList />\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINO Path via Python\nDESCRIPTION: If not cross-compiling, attempts to find the OpenVINO directory by executing a Python script that uses the `openvino.utils` module. The output of the script, which should be the OpenVINO directory path, is stored in the `OpenVINO_DIR_PY` variable. The `ERROR_QUIET` argument suppresses error messages if the command fails.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT CMAKE_CROSSCOMPILING)\n    find_package(Python3 QUIET COMPONENTS Interpreter)\n    if(Python3_Interpreter_FOUND)\n        execute_process(\n            COMMAND ${Python3_EXECUTABLE} -c \"from openvino.utils import get_cmake_path; print(get_cmake_path(), end='')\"\n            OUTPUT_VARIABLE OpenVINO_DIR_PY\n            ERROR_QUIET\n        )\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Python Virtual Environment Preference\nDESCRIPTION: Configures the Python3_FIND_VIRTUALENV variable to FIRST if it is not already defined.  This tells CMake to prefer finding Python environments within a virtual environment.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED Python3_FIND_VIRTUALENV)\n    set(Python3_FIND_VIRTUALENV FIRST)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties\nDESCRIPTION: This snippet sets the `INSTALL_RPATH_USE_LINK_PATH` property for both executable targets to `ON`. This ensures that the runtime path (RPATH) is correctly configured on macOS systems with System Integrity Protection (SIP), allowing the executables to find their dependencies at runtime.  It modifies install settings related to the binary's location.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/continuous_batching/accuracy/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} ${TARGET_NAME_CB} PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n```\n\n----------------------------------------\n\nTITLE: Installing genai-node package\nDESCRIPTION: Installs the genai-node package using npm. This is a prerequisite for using the OpenVINO GenAI Node.js bindings.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nnpm install genai-node\n```\n\n----------------------------------------\n\nTITLE: Set PYTHONPATH and Run Tests\nDESCRIPTION: This command sets the `PYTHONPATH` environment variable to include the path to the built OpenVINO GenAI library, allowing `pytest` to find the library during test execution. This is used when the library is built manually instead of using a wheel.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nPYTHONPATH=$PYTHONPATH:.../openvino.genai/build-Release/ python -m pytest tests/python_tests/ -m precommit\n```\n\n----------------------------------------\n\nTITLE: Google Test Dependency Management CMake\nDESCRIPTION: This snippet uses CMake's FetchContent module to manage the Google Test dependency.  It defines the location of the Google Test repository and ensures it's available for the project. The BUILD_SHARED_LIBS variable is set to OFF, enforcing static linking of gtest libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/cpp/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT TARGET gtest)\n  set(INSTALL_GTEST OFF CACHE BOOL \"\")\n  set(BUILD_SHARED_LIBS OFF)\n  \n  FetchContent_Declare(\n    googletest\n    URL       https://github.com/google/googletest/archive/03597a01ee50ed33e9dfd640b249b4be3799d395.zip\n    URL_HASH SHA256=edd885a1ab32b6999515a880f669efadb80b3f880215f315985fa3f6eca7c4d3\n  )\n  FetchContent_MakeAvailable(googletest)\n\n  foreach(gtarget IN ITEMS gtest gmock gtest_main gmock_main)\n    if(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG OR (OV_COMPILER_IS_INTEL_LLVM AND UNIX))\n      target_compile_options(${gtarget} PUBLIC -Wno-undef)\n    endif()\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Create Python Virtual Environment\nDESCRIPTION: This code snippet demonstrates how to create and activate a Python virtual environment. It is recommended to set up a virtual environment before installing WWB to isolate its dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython -m venv eval_env\nsource eval_env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Run Greedy Causal LM Sample (bash)\nDESCRIPTION: Executes the `greedy_causal_lm` application for basic text generation using a causal language model. It requires the model directory and a prompt as input.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./greedy_causal_lm <MODEL_DIR> \"<PROMPT>\"\n```\n\n----------------------------------------\n\nTITLE: Run Prompt Lookup Decoding LM\nDESCRIPTION: This command executes the `prompt_lookup_decoding_lm.py` script, which uses prompt lookup decoding for text generation. The script takes the model directory and prompt as input.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/text_generation/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython prompt_lookup_decoding_lm.py model_dir prompt\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenVINO GenAI repository\nDESCRIPTION: Clones the OpenVINO GenAI repository and navigates into it. The `--recursive` flag initializes submodules, ensuring all dependencies are included.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/BUILD.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/openvinotoolkit/openvino.genai.git\ncd openvino.genai\n```\n\n----------------------------------------\n\nTITLE: Handling Multi-Config Generators\nDESCRIPTION: This snippet handles the configuration of build types for multi-config generators (like Visual Studio) and single-config generators (like Ninja).  It sets a default build type if one is not already defined, issuing a status message and setting the CMAKE_BUILD_TYPE cache variable. The if statement checks whether the current generator is a multi-config generator or not.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nget_property(GENERATOR_IS_MULTI_CONFIG_VAR GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG)\nif(CMAKE_GENERATOR STREQUAL \"Ninja Multi-Config\")\n    # 'Ninja Multi-Config' specific, see:\n    # https://cmake.org/cmake/help/latest/variable/CMAKE_DEFAULT_BUILD_TYPE.html\n    set(CMAKE_DEFAULT_BUILD_TYPE \"Release\" CACHE STRING \"CMake default build type\")\nelif(NOT GENERATOR_IS_MULTI_CONFIG_VAR AND NOT DEFINED CMAKE_BUILD_TYPE)\n    message(STATUS \"CMAKE_BUILD_TYPE is not defined, 'Release' will be used\")\n    # Setting CMAKE_BUILD_TYPE as CACHE must go before project(). Otherwise project() sets its value and set() doesn't take an effect\n    set(CMAKE_BUILD_TYPE Release CACHE STRING \"Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel ...\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Python Tests\nDESCRIPTION: This snippet runs the Python tests for the continuous batching component using the pytest framework. It assumes that pytest is installed and configured correctly.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_13\n\nLANGUAGE: Bash\nCODE:\n```\ncd python/tests\npytest .\n```\n\n----------------------------------------\n\nTITLE: Installing timm and einops for InternVL2 Conversion\nDESCRIPTION: This command installs the `timm` and `einops` libraries, which are required to convert InternVL2 models for use with OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/supported-models/index.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install timm einops\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Policies\nDESCRIPTION: This snippet sets CMake policies for compatibility. It checks if specific policies (CMP0135 and CMP0169) are defined and sets them to specific behaviors (`NEW` and `OLD` respectively) to ensure consistent behavior across CMake versions.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/whisper_speech_recognition/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(POLICY CMP0135)\n    cmake_policy(SET CMP0135 NEW)\nendif()\n\nif(POLICY CMP0169)\n    cmake_policy(SET CMP0169 OLD)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Debug Postfix for Windows/macOS\nDESCRIPTION: Sets the `CMAKE_DEBUG_POSTFIX` variable to \"d\" on Windows and macOS. This is used to append a \"d\" to the name of debug builds of libraries or executables, differentiating them from release builds.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32 OR APPLE)\n  set(CMAKE_DEBUG_POSTFIX \"d\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Control Language of Prompts\nDESCRIPTION: This command uses the `--language` parameter to control the language of the prompts used for ground truth generation.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\nwwb --base-model meta-llama/Llama-2-7b-chat-hf --gt-data llama_2_7b_wwb_gt.csv --hf\n```\n\n----------------------------------------\n\nTITLE: Fetching nlohmann_json Dependency using FetchContent\nDESCRIPTION: This snippet uses CMake's FetchContent module to download and make available the nlohmann_json library if it's not already a target. It specifies the URL and SHA256 hash for verification.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/cpp/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT TARGET nlohmann_json)\n    FetchContent_Declare(nlohmann_json\n        URL https://github.com/nlohmann/json/archive/refs/tags/v3.11.3.tar.gz\n        URL_HASH SHA256=0d8ef5af7f9794e3263480193c491549b2ba6cc74bb018906202ada498a79406)\n    FetchContent_MakeAvailable(nlohmann_json)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Downloading Model from Hugging Face\nDESCRIPTION: This command downloads a pre-converted OpenVINO model from Hugging Face using the `huggingface-cli` tool.  It requires the model ID and the desired output directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/model-preparation/download-openvino-models.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli download \"OpenVINO/phi-2-fp16-ov\" --local-dir model_path\n```\n\n----------------------------------------\n\nTITLE: Running benchmark_genai sample Bash\nDESCRIPTION: This command executes the `benchmark_genai` sample for benchmarking LLMs in OpenVINO GenAI.  It uses command-line options to specify the model path, prompt, number of warmup iterations, maximum number of new tokens, number of iterations, and the device to run the model on.  This sample provides performance metrics.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n./benchmark_genai [OPTIONS]\n```\n\n----------------------------------------\n\nTITLE: Run Chat Sample (bash)\nDESCRIPTION: Executes the `chat_sample` application, which provides an interactive chat interface powered by OpenVINO. It takes the model directory as an argument.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./chat_sample <MODEL_DIR>\n```\n\n----------------------------------------\n\nTITLE: Source OpenVINO setupvars PowerShell script (Windows)\nDESCRIPTION: Runs the `setupvars.ps1` script from the OpenVINO installation directory in PowerShell. This script configures the environment variables necessary for OpenVINO development in PowerShell.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_8\n\nLANGUAGE: cmd\nCODE:\n```\n. <INSTALL_DIR>/setupvars.ps1\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard\nDESCRIPTION: Sets the C++ standard to C++17 for the project. This ensures that the code is compiled using the specified C++ standard, enabling the use of C++17 features.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# Set C++ standard\nset(CMAKE_CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: Run Specific Tests with pytest\nDESCRIPTION: This command executes specific OpenVINO GenAI tests using `pytest`, filtering by markers and keywords. The `-m nightly` flag selects tests marked with the `nightly` marker, and the `-k` option further filters the tests to only include those with \"test_multibatch and test_chat\" in their name or description.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython -m pytest tests/python_tests/ -m nightly -k \"test_multibatch and test_chat\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Tokenizer in Python\nDESCRIPTION: This Python code snippet demonstrates how to initialize the Tokenizer class from a path.  It creates an instance of the Tokenizer using the models_path.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\ntokenizer = ov_genai.Tokenizer(models_path)\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Policies\nDESCRIPTION: Sets CMake policies to specific behaviors. These policies control how CMake handles certain situations and ensure consistent behavior across different CMake versions. CMP0135 and CMP0169 are being set to NEW and OLD respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(POLICY CMP0135)\n    cmake_policy(SET CMP0135 NEW)\nendif()\n\nif(POLICY CMP0169)\n    cmake_policy(SET CMP0169 OLD)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating Version Files with configure_file\nDESCRIPTION: This snippet uses the `configure_file` command in CMake to generate version files (`version.hpp` and `version.cpp`) from template files. These files likely contain version information about the OpenVINO GenAI library and are placed in the build directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/cpp/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nconfigure_file(\"${OpenVINOGenAI_SOURCE_DIR}/cmake/templates/version.hpp.in\"\n               \"${CMAKE_CURRENT_BINARY_DIR}/openvino/genai/version.hpp\" @ONLY)\n\nconfigure_file(\"${OpenVINOGenAI_SOURCE_DIR}/cmake/templates/version.cpp.in\"\n               \"${CMAKE_CURRENT_BINARY_DIR}/version.cpp\" @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Creating Build Directory for Continuous Batching Project\nDESCRIPTION: This snippet creates the build directory for the continuous batching project within the openvino.genai repository. It prepares the directory structure for subsequent CMake and Make commands.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DOCKER.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nmkdir /path/to/openvino.genai/text_generation/causal_lm/cpp/continuous_batching/build\n```\n\n----------------------------------------\n\nTITLE: Setting Library Properties\nDESCRIPTION: Sets properties of the target, such as the prefix and suffix of the library. This ensures that the generated library has the correct file extension (.node).\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\n# Set library properties\nset_target_properties(${PROJECT_NAME} PROPERTIES\n    PREFIX \"\"\n    SUFFIX \".node\"\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading stb_image.h using CMake\nDESCRIPTION: This snippet uses the `file(DOWNLOAD)` command to download the stb_image.h header file from a remote URL.  It specifies the expected MD5 hash to ensure the integrity of the downloaded file. The file is downloaded to the build directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(DOWNLOAD https://raw.githubusercontent.com/nothings/stb/f75e8d1cad7d90d72ef7a4661f1b994ef78b4e31/stb_image.h ${CMAKE_BINARY_DIR}/stb_image.h\n     EXPECTED_HASH MD5=27932e6fb3a2f26aee2fc33f2cb4e696)\n```\n\n----------------------------------------\n\nTITLE: Image to Image Generation Prompt Example\nDESCRIPTION: This JSON snippet provides an example for Image to Image Generation tasks.  It includes parameters like 'steps', 'width', 'height', 'guidance_scale', 'strength', 'prompt', and 'media' to control the image generation process. These parameters define the image resolution, inference steps, input prompt and input image.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/doc/PROMPT.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"steps\":\"10\", \"width\":\"256\", \"height\":\"256\", \"guidance_scale\":\"0.8\", \"prompt\": \"side profile centered painted portrait, Gandhi rolling a blunt, Gloomhaven, matte painting concept art, art nouveau, 8K HD Resolution, beautifully background\", \"media\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\"}\n```\n\n----------------------------------------\n\nTITLE: Set Include Directories\nDESCRIPTION: Specifies the include directories required to compile the library.  It sets public include directories for runtime and build interfaces, including the current source directory's include folder and any interface include directories defined in the openvino::genai target.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/c/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC \"$<INSTALL_INTERFACE:runtime/include>\"\n                                                 \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\"\n                                                 $<BUILD_INTERFACE:$<TARGET_PROPERTY:openvino::genai,INTERFACE_INCLUDE_DIRECTORIES>>)\n```\n\n----------------------------------------\n\nTITLE: Finding Python for Python Bindings\nDESCRIPTION: Conditionally finds Python if `ENABLE_PYTHON` is set. It uses custom `ov_find_python3` and `ov_detect_python_module_extension` functions if the OpenVINODeveloperPackage is found. Otherwise, it uses the standard `find_package(Python3)` command to find Python with Interpreter and Development components. The `Development.Module` component is used when CMake version is 3.18 or greater.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_PYTHON)\n    # the following two calls are required for cross-compilation\n    if(OpenVINODeveloperPackage_FOUND)\n        ov_find_python3(REQUIRED)\n        ov_detect_python_module_extension()\n    else()\n        if(CMAKE_VERSION VERSION_GREATER_EQUAL 3.18)\n            find_package(Python3 REQUIRED COMPONENTS Interpreter Development.Module)\n        else()\n            find_package(Python3 REQUIRED COMPONENTS Interpreter Development)\n        endif()\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Fetching safetensors.h Dependency using FetchContent\nDESCRIPTION: This snippet uses CMake's FetchContent to fetch the safetensors.h library. It declares the dependency and then makes it available for use. URL and SHA256 hash are provided for verification.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/cpp/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nFetchContent_Declare(safetensors.h\n    URL https://github.com/hsnyder/safetensors.h/archive/974a85d7dfd6e010558353226638bb26d6b9d756.tar.gz\n    URL_HASH SHA256=9aaf5961609601cf9aaa96582a207bce7c6e5fbf57ed2cc669bb7bde6a937d4b)\nFetchContent_MakeAvailable(safetensors.h)\n```\n\n----------------------------------------\n\nTITLE: Install Target\nDESCRIPTION: Configures the installation process for the library.  It specifies the destination directories for the library, archive (static library), runtime components, and includes, and associates them with specific components for packaging.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/c/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME} EXPORT OpenVINOGenAITargets\n        LIBRARY DESTINATION ${LIBRARY_DESTINATION} COMPONENT core_c_genai\n            NAMELINK_COMPONENT core_c_genai_dev\n        ARCHIVE DESTINATION ${ARCHIVE_DESTINATION} COMPONENT core_c_genai_dev\n        RUNTIME DESTINATION ${RUNTIME_DESTINATION} COMPONENT core_c_genai\n        INCLUDES DESTINATION runtime/include)\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINOGenAI Package in CMake\nDESCRIPTION: This snippet uses `find_package` to locate the OpenVINOGenAI package. It specifies required components and search paths including the build directory and a potential installation alongside OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/text_generation/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(OpenVINOGenAI REQUIRED\n    PATHS\n        \"${CMAKE_BINARY_DIR}\"  # Reuse the package from the build.\n        ${OpenVINO_DIR}  # GenAI may be installed alogside OpenVINO.\n    NO_CMAKE_FIND_ROOT_PATH\n)\n```\n\n----------------------------------------\n\nTITLE: Setting RPATH/LC_RPATH\nDESCRIPTION: Sets the RPATH or LC_RPATH depending on the platform (Linux or Apple). This allows the library to find its dependencies at runtime. It constructs paths relative to the library and npm install location.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\n# setting RPATH / LC_RPATH depending on platform\nif(LINUX)\n    # to find libopenvino_genai.so in the same folder\n    set(rpaths \"$ORIGIN\")\n    # to find libopenvino.so when installing from npm\n    list(APPEND rpaths \"$ORIGIN/../../openvino-node/bin\")\n    # to find libopenvino.so when installing from source\n    list(APPEND rpaths \"$ORIGIN/../node_modules/openvino-node/bin\")\nelif(APPLE)\n    # to find libopenvino_genai.dylib in the same folder\n    set(rpaths \"@loader_path\")\n    # to find libopenvino.dylib when installing from npm\n    list(APPEND rpaths \"@loader_path/../../openvino-node/bin\")\n    # to find libopenvino.dylib when installing from source\n    list(APPEND rpaths \"@loader_path/../node_modules/openvino-node/bin\")\nendif()\n\nif(rpaths)\n    set_target_properties(${PROJECT_NAME} PROPERTIES INSTALL_RPATH \"${rpaths}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables (macOS)\nDESCRIPTION: Sets environment variables required for OpenVINO GenAI development on macOS. It defines `OpenVINO_DIR`, `PYTHONPATH`, and `DYLD_LIBRARY_PATH`, pointing to the OpenVINO runtime, Python bindings, and shared libraries, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/BUILD.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nexport OpenVINO_DIR=<INSTALL_DIR>/runtime\nexport PYTHONPATH=<INSTALL_DIR>/python:./build/:$PYTHONPATH\nexport DYLD_LIBRARY_PATH=<INSTALL_DIR>/runtime/lib/intel64:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Creating Image2Image Executable with CMake\nDESCRIPTION: This snippet creates an executable named 'image2image' from the source files 'image2image.cpp', 'load_image.cpp', and 'imwrite.cpp'. It sets the include directories and links the executable with 'openvino::genai' and 'indicators::indicators'.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/image_generation/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(image2image image2image.cpp load_image.cpp imwrite.cpp)\n\ntarget_include_directories(image2image PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}\" \"${CMAKE_BINARY_DIR}\")\ntarget_link_libraries(image2image PRIVATE openvino::genai indicators::indicators)\n\nset_target_properties(image2image PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n\ninstall(TARGETS image2image\n        RUNTIME DESTINATION samples_bin/\n        COMPONENT samples_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Finding Required Packages\nDESCRIPTION: This snippet uses the `find_package` command to locate the OpenVINO (with the Runtime component) and Threads packages. These packages are required for building the benchmark application.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/continuous_batching/benchmark/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(OpenVINO REQUIRED COMPONENTS Runtime)\nfind_package(Threads REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Glob Source Files\nDESCRIPTION: Collects all C++ source files (.cpp) from the 'src' directory and header files (.hpp) from the 'include' directory within the current source directory into the SOURCE_FILES variable.  These files will be used to build the library.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/c/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCE_FILES \"${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp\" \"${CMAKE_CURRENT_SOURCE_DIR}/include/*.hpp\")\n```\n\n----------------------------------------\n\nTITLE: Exclude Tests using pytest\nDESCRIPTION: This command runs all tests except the beam search tests. The `-m precommit` flag specifies the `precommit` marker and `-k \"not test_beam_search\"` excludes the beam search tests using a negative keyword.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython -m pytest tests/python_tests/ -m precommit -k \"not test_beam_search\"\n```\n\n----------------------------------------\n\nTITLE: Source OpenVINO setupvars PowerShell script from Archive (Windows)\nDESCRIPTION: Runs the `setupvars.ps1` script from the OpenVINO installation directory in PowerShell and builds samples. This script configures the environment variables necessary for OpenVINO development in PowerShell.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_22\n\nLANGUAGE: cmd\nCODE:\n```\n.<INSTALL_DIR>\\setupvars.ps1\n.<INSTALL_DIR>\\samples\\cpp\\build_samples.ps1\n```\n\n----------------------------------------\n\nTITLE: Set Target Properties\nDESCRIPTION: Sets various properties for the library, including the export name, interprocedural optimization (LTO), version, and SOVERSION.  The version and SOVERSION properties ensure proper versioning and compatibility for the shared library.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/c/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES\n    EXPORT_NAME genai::c\n    INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO}\n    VERSION ${OpenVINOGenAI_VERSION}\n    SOVERSION ${MAJOR_SUFFIX}${OpenVINOGenAI_VERSION_MINOR}${OpenVINOGenAI_VERSION_PATCH})\n```\n\n----------------------------------------\n\nTITLE: LDM Super-Resolution Prompt Example\nDESCRIPTION: This JSON snippet demonstrates a prompt file for LDM super-resolution. It includes parameters such as 'steps', 'width', 'height', and 'prompt' (image path). These parameters determine the upscaling steps, target resolution, and the input image path for the super-resolution process.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/doc/PROMPT.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"steps\": \"20\", \"width\": \"256\", \"height\": \"256\", \"prompt\": \"./image_256x256_size/4.png\"}\n```\n\n----------------------------------------\n\nTITLE: Creating and Linking Executables\nDESCRIPTION: This snippet defines two executable targets (`continuous_batching_accuracy` and `continuous_batching_speculative_decoding`) and links them against the `openvino::genai` and `cxxopts::cxxopts` libraries. The `add_executable` command creates the executables, and `target_link_libraries` specifies the dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/continuous_batching/accuracy/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME continuous_batching_accuracy)\nadd_executable(${TARGET_NAME} ${TARGET_NAME}.cpp)\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::genai cxxopts::cxxopts)\n\nset(TARGET_NAME_CB continuous_batching_speculative_decoding)\nadd_executable(${TARGET_NAME_CB} ${TARGET_NAME_CB}.cpp)\ntarget_link_libraries(${TARGET_NAME_CB} PRIVATE openvino::genai cxxopts::cxxopts)\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINOGenAI Package with CMake\nDESCRIPTION: This CMake code snippet attempts to locate the OpenVINOGenAI package using the `find_package` command. It searches in the current binary directory (where the build is taking place) and in the OpenVINO installation directory, if specified. The `REQUIRED` keyword makes the package mandatory for the build to succeed. `NO_CMAKE_FIND_ROOT_PATH` prevents searching in the root paths that might be set by the toolchain.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/c/text_generation/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenVINOGenAI REQUIRED\n    PATHS\n        \"${CMAKE_BINARY_DIR}\"  # Reuse the package from the build.\n        ${OpenVINO_DIR}  # GenAI may be installed alogside OpenVINO.\n    NO_CMAKE_FIND_ROOT_PATH\n)\n```\n\n----------------------------------------\n\nTITLE: Clone OpenVINO and OpenVINO GenAI repositories\nDESCRIPTION: Clones the OpenVINO and OpenVINO GenAI repositories from GitHub recursively. This ensures that all submodules are also cloned, which are required for building.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/openvinotoolkit/openvino.git\ngit clone --recursive https://github.com/openvinotoolkit/openvino.genai.git\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries\nDESCRIPTION: Links the necessary libraries to the target.  It links the openvino::genai, delayimp.lib and CMAKE_JS_LIB libraries to the target.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${PROJECT_NAME} PRIVATE openvino::genai ${DELAYIMP_LIB} ${CMAKE_JS_LIB})\n```\n\n----------------------------------------\n\nTITLE: Calling OpenVINO setupvars script (Windows Command Prompt)\nDESCRIPTION: Sets up the environment by calling the OpenVINO `setupvars.bat` script.  This script configures environment variables required for OpenVINO development, such as paths to libraries and binaries. `<INSTALL_DIR>` should be replaced with the actual installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/BUILD.md#_snippet_2\n\nLANGUAGE: cmd\nCODE:\n```\ncall <INSTALL_DIR>\\setupvars.bat\n```\n\n----------------------------------------\n\nTITLE: Creating Benchmark VLM Executable CMake\nDESCRIPTION: This snippet defines an executable target named 'benchmark_vlm'. It specifies the source files, include directories, and linked libraries. It links to both 'openvino::genai' and 'cxxopts::cxxopts'. It also sets the INSTALL_RPATH_USE_LINK_PATH property for macOS compatibility. Finally, it defines the install target to copy the executable into the samples_bin directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/visual_language_chat/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(benchmark_vlm benchmark_vlm.cpp load_image.cpp)\ntarget_include_directories(benchmark_vlm PRIVATE \"${CMAKE_CURRENT_SOUCE_DIR}\" \"${CMAKE_BINARY_DIR}\")\ntarget_link_libraries(benchmark_vlm PRIVATE openvino::genai cxxopts::cxxopts)\nset_target_properties(benchmark_vlm PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n\ninstall(TARGETS benchmark_vlm\n        RUNTIME DESTINATION samples_bin/\n        COMPONENT samples_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Installing Executables\nDESCRIPTION: This snippet installs the specified executable targets (`${TARGET_NAME}` and `${TARGET_NAME_CB}`) to the `samples_bin/` directory under the `tools_bin` component during the installation process.  The `EXCLUDE_FROM_ALL` option prevents these targets from being built when the `ALL` target is built.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/continuous_batching/accuracy/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME} ${TARGET_NAME_CB} \n        RUNTIME DESTINATION samples_bin/\n        COMPONENT tools_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Calculating Pure Inference Time in Python\nDESCRIPTION: This snippet calculates the pure inference time by subtracting tokenization and detokenization durations from the overall generate duration using the raw metrics available in the perf_metrics object. It requires the openvino_genai and numpy libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/performance-metrics.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\nimport numpy as np\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\nresult = pipe.generate([\"The Sun is yellow because\"], max_new_tokens=20)\nperf_metrics = result.perf_metrics\nprint(f'Generate duration: {perf_metrics.get_generate_duration().mean:.2f} ms')\n\nraw_metrics = perf_metrics.raw_metrics\ngenerate_duration = np.array(raw_metrics.generate_durations)\ntok_detok_duration = np.array(raw_metrics.tokenization_durations) - np.array(raw_metrics.detokenization_durations)\npure_inference_duration = np.sum(generate_duration - tok_detok_duration) / 1000 # in milliseconds\nprint(f'Pure Inference duration: {pure_inference_duration:.2f} ms')\n```\n\n----------------------------------------\n\nTITLE: Install WWB from Source\nDESCRIPTION: This command installs WWB from the current source directory using pip. It assumes that the user is in the root directory of the WWB project.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install .\n```\n\n----------------------------------------\n\nTITLE: Disabling Constexpr Mutex Constructor for MSVC\nDESCRIPTION: This snippet provides a workaround for an MSVC compiler issue in some versions of Visual Studio 2022. It disables the constexpr mutex constructor by adding a compile definition.  This is a specific workaround related to potential null dereference with mutexes.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\n# Workaround for an MSVC compiler issue in some versions of Visual Studio 2022.\n# The issue involves a null dereference to a mutex. For details, refer to link https://github.com/microsoft/STL/wiki/Changelog#vs-2022-1710\nif(MSVC AND MSVC_VERSION GREATER_EQUAL 1930 AND MSVC_VERSION LESS 1941)\n    add_compile_definitions(_DISABLE_CONSTEXPR_MUTEX_CONSTRUCTOR)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Executable Target Definition in CMake\nDESCRIPTION: This CMake snippet defines the executable target for the tests.  It specifies the source files (`tests_src`) and links the target with the `openvino_genai_obj` object library. It also defines include directories required for compilation using `target_include_directories`. The `tests_src` variable should be defined before this snippet.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/cpp/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB tests_src \"*.cpp\")\n\nset(TEST_TARGET_NAME \"tests_continuous_batching\")\n\nadd_executable(${TEST_TARGET_NAME} ${tests_src} $<TARGET_OBJECTS:openvino_genai_obj>)\n\ntarget_link_libraries(${TEST_TARGET_NAME} PRIVATE $<TARGET_PROPERTY:openvino::genai,LINK_LIBRARIES> gtest_main gmock_main)\ntarget_include_directories(${TEST_TARGET_NAME} PRIVATE \"${OpenVINOGenAI_SOURCE_DIR}/src/cpp/src\"\n                                                       $<TARGET_PROPERTY:openvino::genai,INTERFACE_INCLUDE_DIRECTORIES>)\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version\nDESCRIPTION: Specifies the minimum required version of CMake for the build. This ensures that the CMake features used in the script are supported by the CMake version available on the system.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18)\n```\n\n----------------------------------------\n\nTITLE: Beam Search Generation with Whisper Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to use beam search generation with the WhisperPipeline in Python. It configures parameters like max_new_tokens, num_beams, num_beam_groups, and diversity_penalty. The input is raw audio data, and the output is the generated text using beam search.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/use-cases/speech-recognition/_sections/_usage_options/index.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\n\npipe = ov_genai.WhisperPipeline(model_path, \"CPU\")\n\n# Get default generation config\nconfig = pipe.get_generation_config()\n\n# Modify parameters\nconfig.max_new_tokens = 256\nconfig.num_beams = 15\nconfig.num_beam_groups = 3\nconfig.diversity_penalty = 1.0\n\n# Generate text with custom configuration\nresult = pipe.generate(raw_speech, config)\n```\n\n----------------------------------------\n\nTITLE: Setting Dist Folder\nDESCRIPTION: Sets the distribution folder where the build artifacts will be placed. This variable defines the output directory for the compiled library.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(dist_folder \"${CMAKE_SOURCE_DIR}/bin/\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Raw Token Timestamps in Python\nDESCRIPTION: This Python code retrieves the raw timestamps for each generated token using the raw_metrics field of the perf_metrics object. The timestamps are then formatted and printed. It uses the openvino_genai and numpy libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/performance-metrics.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(models_path, \"CPU\")\nresult = pipe.generate([\"The Sun is yellow because\"], max_new_tokens=20)\nperf_metrics = result.perf_metrics\nraw_metrics = perf_metrics.raw_metrics\n\nprint(f'Generate duration: {perf_metrics.get_generate_duration().mean:.2f}')\nprint(f'Throughput: {perf_metrics.get_throughput().mean:.2f} tokens/s')\nprint(f'Timestamps: {\" ms, \".join(f\"{i:.2f}\" for i in raw_metrics.m_new_token_times)}')\n```\n\n----------------------------------------\n\nTITLE: Linking Options (Apple/AARCH64/ARM)\nDESCRIPTION: Sets specific linker options for Apple (macOS) and AARCH64/ARM architectures. For Apple, it suppresses undefined symbols, and for AARCH64/ARM, it ignores unresolved symbols.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nif(APPLE)\n    target_link_options(${PROJECT_NAME} PRIVATE -Wl,-undefined,suppress,-flat_namespace)\nelif(AARCH64 OR ARM)\n    target_link_options(${PROJECT_NAME} PRIVATE -Wl,--unresolved-symbols=ignore-all)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Addition in CMake\nDESCRIPTION: This snippet conditionally adds the `openvino_tokenizers` subdirectory to the build process based on the value of `BUILD_TOKENIZERS`. If `BUILD_TOKENIZERS` is enabled, the tokenizer project is included. This allows the user to build OpenVINO GenAI without the tokenizers if desired.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/thirdparty/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_TOKENIZERS)\n    add_subdirectory(./openvino_tokenizers/ \"${CMAKE_BINARY_DIR}/openvino_tokenizers/\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Fetching node-api-headers\nDESCRIPTION: Fetches the node-api-headers library using FetchContent. This downloads the specified version of node-api-headers from GitHub and makes it available for use in the project. The URL_HASH ensures the integrity of the downloaded content.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(FetchContent)\n\nFetchContent_Declare(\n    node-api-headers\n    URL      https://github.com/nodejs/node-api-headers/archive/refs/tags/v1.1.0.tar.gz\n    URL_HASH SHA256=70608bc1e6dddce280285f3462f18a106f687c0720a4b90893e1ecd86e5a8bbf\n)\nFetchContent_MakeAvailable(node-api-headers)\n```\n\n----------------------------------------\n\nTITLE: Deploy Website to GitHub Pages\nDESCRIPTION: This command deploys the website to GitHub Pages. Setting `GIT_USER` specifies the GitHub username, which is then used by the `deploy` script in `package.json` to push the built website to the `gh-pages` branch of the repository.  This branch is then served as the website.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/README.md#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n$ GIT_USER=<Your GitHub username> npm run deploy\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet adds subdirectories to the current CMake project. Each subdirectory represents a different module within the OpenVINO GenAI project, such as text generation, image generation, visual language chat, and whisper speech recognition, implemented in both C++ and C.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(cpp/text_generation)\nadd_subdirectory(cpp/image_generation)\nadd_subdirectory(cpp/visual_language_chat)\nadd_subdirectory(cpp/whisper_speech_recognition)\nadd_subdirectory(c/text_generation)\n```\n\n----------------------------------------\n\nTITLE: Enable Verbose Mode\nDESCRIPTION: This command enables verbose mode using the `-v` flag to display more detailed output during the comparison process.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_21\n\nLANGUAGE: sh\nCODE:\n```\nwwb --target-model /home/user/models/Llama_2_7b_chat_hf_int8 --gt-data llama_2_7b_wwb_gt.csv  --num-samples 10 -v\n```\n\n----------------------------------------\n\nTITLE: Install Test Dependencies using pip\nDESCRIPTION: This command installs the necessary Python dependencies for running the OpenVINO GenAI tests. It utilizes the `pip` package installer and reads the requirements from the `tests/python_tests/requirements.txt` file.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -r tests/python_tests/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Dependency Management with FetchContent\nDESCRIPTION: This snippet uses CMake's FetchContent module to download and make available the cxxopts and nlohmann_json libraries.  It specifies the URL and SHA256 hash for each dependency to ensure reproducibility and integrity. It also checks for the existence of the nlohmann_json target before attempting to fetch it.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/continuous_batching/accuracy/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(FetchContent)\n\nif(POLICY CMP0135)\n    cmake_policy(SET CMP0135 NEW)\nendif()\n\nFetchContent_Declare(cxxopts\n    URL https://github.com/jarro2783/cxxopts/archive/refs/tags/v3.1.1.tar.gz\n    URL_HASH SHA256=523175f792eb0ff04f9e653c90746c12655f10cb70f1d5e6d6d9491420298a08)\nFetchContent_MakeAvailable(cxxopts)\n\nif(NOT TARGET nlohmann_json)\n    FetchContent_Declare(nlohmann_json\n        URL https://github.com/nlohmann/json/archive/refs/tags/v3.11.3.tar.gz\n        URL_HASH SHA256=0d8ef5af7f9794e3263480193c491549b2ba6cc74bb018906202ada498a79406)\n    FetchContent_MakeAvailable(nlohmann_json)\nendif()\n\nfind_package(OpenVINO REQUIRED COMPONENTS Runtime)\n```\n\n----------------------------------------\n\nTITLE: Windows Specific Settings\nDESCRIPTION: Configures Windows-specific settings, including linker flags, library paths, and source files.  This section handles delay-loading of node.exe and sets up the necessary libraries and source files for Windows builds. It also includes a definition file used for generating a node.lib file.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n    set(CMAKE_SHARED_LINKER_FLAGS /DELAYLOAD:NODE.EXE)\n    set(CMAKE_JS_LIB ${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/node.lib)\n    set(CMAKE_JS_SRC ${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/win_delay_load_hook.cc)\n\n    set(CMAKE_JS_NODELIB_DEF ${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/node-lib.def)\n    set(CMAKE_JS_NODELIB_TARGET ${CMAKE_JS_LIB})\n    set(DELAYIMP_LIB delayimp.lib)\nendif()\n```\n\n----------------------------------------\n\nTITLE: WWB Help Command\nDESCRIPTION: This command displays the help information for the WWB command-line tool, showing available options and arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\nwwb --help\n```\n\n----------------------------------------\n\nTITLE: Downloading Model from ModelScope\nDESCRIPTION: This command downloads a pre-converted OpenVINO model from ModelScope using the `modelscope` CLI.  It requires the model ID and the desired output directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/model-preparation/download-openvino-models.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmodelscope download --model \"OpenVINO/phi-2-fp16-ov\" --local_dir model_path\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties for Tokenizers in CMake\nDESCRIPTION: This snippet configures the output directories for the `openvino_tokenizers` target. It uses generator expressions to ensure that all generated binaries (archives, libraries, and executables) are placed into a single directory, `${CMAKE_BINARY_DIR}/openvino_genai/`, regardless of the build configuration (e.g., Release or Debug).\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/thirdparty/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(openvino_tokenizers PROPERTIES\n        # Generator expressions to disable appending a per-configuration subdirectory (Release, Debug).\n        # ARCHIVE_OUTPUT is irrelevant. It's here just to keep all the artifacts in one place.\n        ARCHIVE_OUTPUT_DIRECTORY \"$<1:${CMAKE_BINARY_DIR}/openvino_genai/>\"\n        LIBRARY_OUTPUT_DIRECTORY \"$<1:${CMAKE_BINARY_DIR}/openvino_genai/>\"\n        RUNTIME_OUTPUT_DIRECTORY \"$<1:${CMAKE_BINARY_DIR}/openvino_genai/>\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Upgrade pip\nDESCRIPTION: Upgrades the pip package installer to the latest version. This ensures that you have the most recent features and bug fixes for installing Python packages.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\npython -m pip install --upgrade pip\n```\n\n----------------------------------------\n\nTITLE: Set Target Name\nDESCRIPTION: Sets the name of the target library to 'openvino_genai_c'. This name is used in subsequent CMake commands to refer to the library.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/c/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME openvino_genai_c)\n```\n\n----------------------------------------\n\nTITLE: Generating node.lib (MSVC)\nDESCRIPTION: Generates the node.lib file on Windows using the provided definition file.  This part is Windows-specific and generates the node.lib file from the provided definition file using the archiver tool.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC AND CMAKE_JS_NODELIB_DEF AND CMAKE_JS_NODELIB_TARGET) # Generate node.lib\n  execute_process(COMMAND ${CMAKE_AR} /def:${CMAKE_JS_NODELIB_DEF} /out:${CMAKE_JS_NODELIB_TARGET} ${CMAKE_STATIC_LINKER_FLAGS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: MSVC Linker Options for OpenVINO GenAI Tests\nDESCRIPTION: This CMake snippet configures linker options specifically for the MSVC compiler. It uses `target_link_options` to suppress specific warning messages (4207 and 4286) that may arise during the linking process when using the MSVC compiler. This is done to reduce noise in the build output.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/cpp/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n  target_link_options(${TEST_TARGET_NAME} PRIVATE /IGNORE:4207,4286)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining NAPI Version\nDESCRIPTION: Specifies the N-API version to use. This ensures compatibility with the target Node.js versions. It uses preprocessor definitions to set the NAPI version.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n# Specify NAPI version 8\n# supports v12.22.0+, v14.17.0+, v15.12.0+, 16.0.0 and all later Node.js versions\nadd_definitions(-DNAPI_VERSION=8)\n```\n\n----------------------------------------\n\nTITLE: Installing ModelScope\nDESCRIPTION: This command installs the `modelscope` package, which is required to download models from ModelScope.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/model-preparation/download-openvino-models.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install modelscope\n```\n\n----------------------------------------\n\nTITLE: Defining Build Option for Tokenizers in CMake\nDESCRIPTION: This snippet defines a CMake option `BUILD_TOKENIZERS` which controls whether the OpenVINO Tokenizers are built along with OpenVINO GenAI.  The default value is set to ON, meaning the tokenizers are built by default. This option can be toggled during CMake configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/thirdparty/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\noption(BUILD_TOKENIZERS \"Build OpenVINO Tokenizers together with OpenVINO GenAI\" ON)\n```\n\n----------------------------------------\n\nTITLE: Source OpenVINO setupvars script from Archive (Linux/macOS)\nDESCRIPTION: Sources the `setupvars.sh` script from the OpenVINO installation directory within the extracted archive. This script sets up the environment variables required for using OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\nsource <INSTALL_DIR>/setupvars.sh\n<INSTALL_DIR>/samples/cpp/build_samples.sh\n```\n\n----------------------------------------\n\nTITLE: Install Xcode command line tools on macOS\nDESCRIPTION: Installs the Clang compiler and other command line tools from Xcode on macOS. These tools are essential for compiling and building software from source.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nxcode-select --install\n```\n\n----------------------------------------\n\nTITLE: Project Definition\nDESCRIPTION: Defines the project name.  The `project()` command sets the name of the CMake project to `genai_node_addon`.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nproject(genai_node_addon)\n```\n\n----------------------------------------\n\nTITLE: Including CMake Feature and Version Files\nDESCRIPTION: Includes the cmake/features.cmake and cmake/version.cmake files. These files likely contain custom CMake functions or variables for configuring features and managing project versions.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(cmake/features.cmake)\ninclude(cmake/version.cmake)\n```\n\n----------------------------------------\n\nTITLE: Fetching External Dependencies with FetchContent\nDESCRIPTION: This snippet uses CMake's FetchContent module to download and make available the cxxopts and nlohmann_json libraries. It specifies the URL and SHA256 hash for verification. The CMP0135 policy is also set to NEW if it exists.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/continuous_batching/benchmark/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(FetchContent)\n\nif(POLICY CMP0135)\n    cmake_policy(SET CMP0135 NEW)\nendif()\n\nFetchContent_Declare(cxxopts\n    URL https://github.com/jarro2783/cxxopts/archive/refs/tags/v3.1.1.tar.gz\n    URL_HASH SHA256=523175f792eb0ff04f9e653c90746c12655f10cb70f1d5e6d6d9491420298a08)\nFetchContent_MakeAvailable(cxxopts)\n\nif(NOT TARGET nlohmann_json)\n    FetchContent_Declare(nlohmann_json\n        URL https://github.com/nlohmann/json/archive/refs/tags/v3.11.3.tar.gz\n        URL_HASH SHA256=0d8ef5af7f9794e3263480193c491549b2ba6cc74bb018906202ada498a79406)\n    FetchContent_MakeAvailable(nlohmann_json)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Install optimum-intel package\nDESCRIPTION: Installs the `optimum-intel` package from GitHub, which is required to download, convert, and optimize models for OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/model-preparation/convert-to-openvino.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install optimum-intel@git+https://github.com/huggingface/optimum-intel.git\n```\n\n----------------------------------------\n\nTITLE: Generate or Validate Python Stub Files with pybind11-stubgen\nDESCRIPTION: This snippet handles the generation or validation of Python stub files (.pyi) using `pybind11-stubgen`.  It checks for the availability of `pybind11-stubgen`, determines whether to run validation or copy generated files, and adds a custom command to execute the stub generation process.  It takes into account CI environments, cross-compilation scenarios, and OpenVINO installation status.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/python/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\n# Generate or check .pyi stub files generated by pybind11-stubgen\n\nset(pyproject_toml \"${OpenVINOGenAI_SOURCE_DIR}/pyproject.toml\")\nfile(STRINGS ${pyproject_toml} pybind11_stubgen_dep REGEX \"pybind11-stubgen\")\n\nif(pybind11_stubgen_dep MATCHES \"pybind11-stubgen==[0-9\\.]+\")\n    set(pybind11_stubgen_dep \"${CMAKE_MATCH_0}\")\nelse()\n    message(FATAL_ERROR \"Internal error: failed to parse pybind11-stubgen version from from '${pyproject_toml}'\")\nendif()\n\nif(OpenVINODeveloperPackage_FOUND)\n    ov_check_pip_package(REQUIREMENT ${pybind11_stubgen_dep}\n                         RESULT_VAR pybind11_stubgen_AVAILABLE\n                         WARNING_MESSAGE \"Please, install ${pybind11_stubgen_dep} if you plan to develop Python OpenVINO GenAI API\"\n                         MESSAGE_MODE WARNING)\nelif(DEFINED PY_BUILD_CMAKE_PACKAGE_NAME AND NOT WIN32)\n    # in case of wheel build, pybind11-stubgen is always available via pyproject.toml's build-system\n    # except Win32 where we have issues with pybind11_stubgen executable which cannot import its own module\n    set(pybind11_stubgen_AVAILABLE ON)\n\n    # by default, wheel build is performed with build-isolation, which means that some variables like PYTHONPATH\n    # are not available. But if user called setupvars.sh, then OpenVINO dir is available, while PYTHONPATH - no.\n    # In this case, we will have mismatch on Linux when OpenVINO can point on build dir / install dir, while\n    # PYTHONPATH points out to locally installed tmp OpenVINO wheel (build against wheel).\n    # Ways to handle it:\n    # - setting PYTHONPATH to $ENV{INTEL_OPENVINO_DIR}/python if INTEL_OPENVINO_DIR is defined. It means we are building against\n    #   OpenVINO archive or installation tree\n    # - if it's not defined, we cannot do any guesses and hence, disable pybind11-stubgen usage\n    if(DEFINED ENV{INTEL_OPENVINO_DIR})\n        set(openvino_pythonpath \"$ENV{INTEL_OPENVINO_DIR}/python\")\n    elseif(LINUX AND NOT OpenVINO_DIR STREQUAL OpenVINO_DIR_PY)\n        # here we imply that OpenVINO_DIR_PY points to manylinux, while OpenVINO_DIR point to Ubuntu binaries\n        set(pybind11_stubgen_AVAILABLE OFF)\n    endif()\nendif()\n\n# but we also need to check whether OpenVINO is installed\nif(CMAKE_CROSSCOMPILING)\n    # we cannot check OpenVINO during cross-compile\n    set(pybind11_stubgen_AVAILABLE OFF)\nelse()\n    execute_process(\n        COMMAND ${Python3_EXECUTABLE} -c \"import openvino\"\n        RESULT_VARIABLE EXIT_CODE\n        OUTPUT_VARIABLE OUTPUT_TEXT\n        ERROR_VARIABLE ERROR_TEXT)\n\n    # OpenVINO is not available because of import error\n    if(NOT EXIT_CODE EQUAL 0)\n        set(pybind11_stubgen_AVAILABLE OFF)\n    endif()\nendif()\n\nif(pybind11_stubgen_AVAILABLE)\n    if(DEFINED ENV{CI} OR DEFINED ENV{TF_BUILD} OR DEFINED ENV{JENKINS_URL})\n        set(ci_run ON)\n    endif()\n\n    set(stub_files_location \"${OpenVINOGenAI_BINARY_DIR}/src/python\")\n    set(init_pyi_file \"${stub_files_location}/openvino_genai/__init__.pyi\")\n    set(generated_files ${init_pyi_file}\n                        ${stub_files_location}/openvino_genai/py_openvino_genai.pyi)\n    set_source_files_properties(${generated_files} PROPERTIES GENERATED ON)\n\n    if(COMMAND find_host_program)\n        find_host_program(pybind11_stubgen NAMES pybind11-stubgen NO_CACHE REQUIRED)\n    else()\n        find_program(pybind11_stubgen NAMES pybind11-stubgen NO_CACHE REQUIRED)\n    endif()\n\n    if(ci_run)\n        set(validation_command\n            COMMAND \"${CMAKE_COMMAND}\"\n                -D generated_pyi_files_location=${stub_files_location}\n                -D source_pyi_files_location=${CMAKE_CURRENT_SOURCE_DIR}\n                -P \"${CMAKE_CURRENT_SOURCE_DIR}/compare_pyi.cmake\")\n        set(validation_dependencies\n            \"${CMAKE_CURRENT_SOURCE_DIR}/openvino_genai/__init__.pyi\"\n            \"${CMAKE_CURRENT_SOURCE_DIR}/openvino_genai/py_openvino_genai.pyi\")\n    else()\n        set(copy_to_source_command\n            COMMAND \"${CMAKE_COMMAND}\" -E copy ${generated_files} \"${CMAKE_CURRENT_SOURCE_DIR}/openvino_genai/\")\n    endif()\n\n    set(output_file pybind11_stub_gen_completed.txt)\n    add_custom_command(OUTPUT ${output_file}\n        COMMAND \"${CMAKE_COMMAND}\" -E rm -f \"${CMAKE_BINARY_DIR}/openvino_genai/__init__.pyi\"\n                                            \"${CMAKE_BINARY_DIR}/openvino_genai/py_openvino_genai.pyi\"\n        COMMAND \"${CMAKE_COMMAND}\" -E env PYTHONPATH=${CMAKE_BINARY_DIR}:${openvino_pythonpath}:$ENV{PYTHONPATH}\n                ${pybind11_stubgen} --output-dir ${stub_files_location} openvino_genai\n        COMMAND \"${CMAKE_COMMAND}\"\n                -D init_pyi_file=${init_pyi_file}\n                -P \"${CMAKE_CURRENT_SOURCE_DIR}/clean_version.cmake\"\n        ${validation_command}\n        ${copy_to_source_command}\n        COMMAND \"${CMAKE_COMMAND}\" -E copy ${generated_files} \"${CMAKE_BINARY_DIR}/openvino_genai/\"\n        COMMAND \"${CMAKE_COMMAND}\" -E touch ${output_file}\n        DEPENDS\n            ${python_sources}\n            ${validation_dependencies}\n            \"${CMAKE_CURRENT_SOURCE_DIR}/openvino_genai/__init__.py\"\n            \"${CMAKE_CURRENT_SOURCE_DIR}/clean_version.cmake\"\n            \"${CMAKE_CURRENT_SOURCE_DIR}/compare_pyi.cmake\"\n        COMMENT \"[${pybind11_stubgen_dep}] Generate .pyi files\"\n        VERBATIM)\n\n    add_custom_target(${TARGET_NAME}_stub ALL DEPENDS ${output_file})\nelif(OpenVINODeveloperPackage_FOUND)\n    # Produce warning message at build time as well\n    add_custom_command(OUTPUT pybind11_stub_gen_not_found.txt\n        COMMAND ${CMAKE_COMMAND}\n            -E cmake_echo_color --red \"Warning: Please, install ${pybind11_stubgen_dep}\")\n    add_custom_target(${TARGET_NAME}_stub ALL DEPENDS pybind11_stub_gen_not_found.txt)\nelse()\n    add_custom_target(${TARGET_NAME}_stub ALL)\nendif()\n\nadd_dependencies(${TARGET_NAME}_stub ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Text Generation Prompt Example\nDESCRIPTION: This JSON snippet provides an example prompt file for text generation tasks. It includes the 'prompt' parameter, which specifies the input text for the model. The llm_bench tool utilizes this prompt to generate text.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/doc/PROMPT.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"prompt\": \"what is openvino?\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"prompt\": \"A chat between a curious user and an artificial intelligence assistant.\"}\n```\n\n----------------------------------------\n\nTITLE: Import SamplesList Component (JSX)\nDESCRIPTION: This line imports the `SamplesList` component from the relative path `./_components/samples-list`. This component is expected to render a list of available GenAI samples. The component is likely a React component defined in the specified file.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/samples/index.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport SamplesList from './_components/samples-list';\n```\n\n----------------------------------------\n\nTITLE: Call OpenVINO setupvars batch script (Windows CMD)\nDESCRIPTION: Calls the `setupvars.bat` script from the OpenVINO installation directory.  This sets up environment variables for OpenVINO in the Windows Command Prompt.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_7\n\nLANGUAGE: cmd\nCODE:\n```\ncall <INSTALL_DIR>\\setupvars.bat\n```\n\n----------------------------------------\n\nTITLE: Fetching node-addon-api\nDESCRIPTION: Fetches the node-addon-api library using FetchContent. This downloads the specified version of node-addon-api from GitHub and makes it available for use in the project.  The URL_HASH ensures the integrity of the downloaded content.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nFetchContent_Declare(\n    node-addon-api\n    URL      https://github.com/nodejs/node-addon-api/archive/refs/tags/v8.0.0.tar.gz\n    URL_HASH SHA256=42424c5206b9d67b41af4fcff5d6e3cb22074168035a03b8467852938a281d47\n)\nFetchContent_MakeAvailable(node-addon-api)\n```\n\n----------------------------------------\n\nTITLE: Install additional dependencies\nDESCRIPTION: Installs the additional dependencies required to convert certain models with `optimum-cli` and to run the examples. It uses the `requirements.txt` file located in the `./samples` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/site/docs/guides/model-preparation/convert-to-openvino.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade-strategy eager -r ./samples/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installation Configuration\nDESCRIPTION: Configures the installation of the target. This specifies where the library and runtime components should be installed during the installation process.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${PROJECT_NAME}\n    LIBRARY DESTINATION . COMPONENT ${PROJECT_NAME}\n    RUNTIME DESTINATION . COMPONENT ${PROJECT_NAME}\n)\n```\n\n----------------------------------------\n\nTITLE: Compare with Compressed Model on Internal Questions\nDESCRIPTION: This command compares a compressed model against ground truth data generated from an internal set of questions. It specifies the target model and ground truth data.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\nwwb --target-model /home/user/models/Llama_2_7b_chat_hf_int8 --gt-data llama_2_7b_wwb_gt.csv\n```\n\n----------------------------------------\n\nTITLE: Run Tests with OV_CACHE Environment Variable\nDESCRIPTION: This command executes tests with the `OV_CACHE` environment variable set. This variable specifies a directory where downloaded and converted models are cached, allowing them to be reused between runs. The `-m samples` flag specifies that the samples tests should be run.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nOV_CACHE=$HOME/ov_cache python -m pytest tests/python_tests -m samples\n```\n\n----------------------------------------\n\nTITLE: Installing the Target\nDESCRIPTION: This snippet installs the built executable to the `samples_bin` directory under the runtime destination. It also designates it as a `tools_bin` component and excludes it from the `ALL` target, meaning it won't be installed by default unless explicitly requested.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/continuous_batching/benchmark/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME} \n        RUNTIME DESTINATION samples_bin/\n        COMPONENT tools_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Setting up environment and building samples in Windows PowerShell\nDESCRIPTION: These commands sets up the environment by running `setupvars.ps1` and then builds the C samples using the `build_samples.ps1` script in Windows PowerShell.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/c/text_generation/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n.<INSTALL_DIR>\\setupvars.ps1\n.<INSTALL_DIR>\\samples\\c\\build_samples.ps1\n```\n\n----------------------------------------\n\nTITLE: Creating the Library\nDESCRIPTION: Creates a shared library named ${PROJECT_NAME} and specifies the source files to be included in the library.  It creates a shared library from the provided source files.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\n# Create a library\nadd_library(${PROJECT_NAME} SHARED\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/addon.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/llm_pipeline/llm_pipeline_wrapper.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/llm_pipeline/finish_chat_worker.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/llm_pipeline/start_chat_worker.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/llm_pipeline/init_worker.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/helper.cpp\n\n    ${CMAKE_JS_SRC}\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies\nDESCRIPTION: This snippet displays a list of Python packages and their corresponding versions or git repository URLs required for the OpenVINO GenAI project. The dependencies ensure that the project can function correctly, covering areas such as neural network frameworks, optimization tools, data processing libraries, and testing frameworks.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cpu\ndiffusers==0.33.1\noptimum-intel @ git+https://github.com/huggingface/optimum-intel.git@main\nnumpy<2.0.0; platform_system == \"Darwin\" and platform_machine == \"x86_64\"\nonnx==1.17.0\npytest\npytest-html\nhf_transfer\ngguf>=0.10.0\n\n# requirements for specific models\n# - hf-tiny-model-private/tiny-random-RoFormerForCausalLM\nrjieba\n# - baichuan-inc/Baichuan2-7B-Chat\nbitsandbytes\n# - nomic-ai/gpt4all-falcon\n# - Qwen/Qwen-7B\n# - Qwen/Qwen-7B-Chat\n# - mosaicml/mpt-7b\n# - internlm/internlm2-7b\neinops\n# - Qwen/Qwen-7B\n# - Qwen/Qwen-7B-Chat\ntransformers_stream_generator\n# - openbmb/MiniCPM-V-2\ntorchvision\n# - openbmb/MiniCPM-V-2\ntimm\n# - Qwen/Qwen-7B\n# - Qwen/Qwen-7B-Chat\n# - Salesforce/xgen-7b-8k-base\ntiktoken\n# - microsoft/biogpt\nsacremoses\n# - openai/whisper-base\nlibrosa\nsoundfile\ndatasets\nrouge\n```\n\n----------------------------------------\n\nTITLE: Run Tests with CLEANUP_CACHE Environment Variable\nDESCRIPTION: This command executes tests with the `CLEANUP_CACHE` environment variable set to 1. This indicates that all downloaded and converted models should be removed after the tests have finished using them. The HuggingFace cache is not affected. The `-m samples` flag specifies that the samples tests should be run.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tests/python_tests/README.md#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nCLEANUP_CACHE=1 python -m pytest tests/python_tests -m samples\n```\n\n----------------------------------------\n\nTITLE: Creating Encrypted Model VLM Executable CMake\nDESCRIPTION: This snippet defines an executable target named 'encrypted_model_vlm'. It specifies the source files, include directories, and linked libraries. It also sets the INSTALL_RPATH_USE_LINK_PATH property for macOS compatibility. Finally, it defines the install target to copy the executable into the samples_bin directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/cpp/visual_language_chat/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(encrypted_model_vlm encrypted_model_vlm.cpp load_image.cpp)\ntarget_include_directories(encrypted_model_vlm PRIVATE \"${CMAKE_CURRENT_SOUCE_DIR}\" \"${CMAKE_BINARY_DIR}\")\ntarget_link_libraries(encrypted_model_vlm PRIVATE openvino::genai)\n\nset_target_properties(encrypted_model_vlm PROPERTIES\n    # Ensure out of box LC_RPATH on macOS with SIP\n    INSTALL_RPATH_USE_LINK_PATH ON)\n\ninstall(TARGETS encrypted_model_vlm\n        RUNTIME DESTINATION samples_bin/\n        COMPONENT samples_bin\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Downloading pre-converted model from Hugging Face\nDESCRIPTION: This command installs the `huggingface-hub` library and downloads a pre-converted OpenVINO IR model from the specified Hugging Face model repository to a local directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/c/text_generation/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npip install huggingface-hub\nhuggingface-cli download <model> --local-dir <output_folder>\n```\n\n----------------------------------------\n\nTITLE: Installing C Directories in CMake\nDESCRIPTION: This CMake snippet installs the C sample directories into the `samples/c` destination.  It installs the `c/text_generation` directory and tags it as part of the `cpp_samples_genai` component.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(DIRECTORY\n        c/text_generation\n        DESTINATION samples/c COMPONENT cpp_samples_genai)\n```\n\n----------------------------------------\n\nTITLE: Add API Validator (Conditional)\nDESCRIPTION: Conditionally adds a post-build step to validate the API if the OpenVINODeveloperPackage is found. This helps ensure API stability and correctness.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/c/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(OpenVINODeveloperPackage_FOUND)\n    # must be called after all target_link_libraries\n    ov_add_api_validator_post_build_step(TARGET ${TARGET_NAME})\n\n    ov_ncc_naming_style(FOR_TARGET ${TARGET_NAME}\n                        SOURCE_DIRECTORIES \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Example OpenVINO Speculative Decoding Performance Metrics\nDESCRIPTION: This example shows the performance metrics outputted when speculative decoding or prompt lookup pipeline is executed. Includes metrics such as Total duration, Draft model duration, Main model duration, AVG acceptance rate, Token per sec, Accepted tokens by draft model, and Generated tokens.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/DEBUG_LOG.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n===============================\nTotal duration, sec: 26.6217\nDraft model duration, sec: 1.60329\nMain model duration, sec: 25.0184\nDraft model duration, %: 6.02248\nMain model duration, %: 93.9775\nAVG acceptance rate, %: 21.6809\n===============================\nREQUEST_ID: 0\nMain model iterations: 47\nToken per sec: 3.75633\nAVG acceptance rate, %: 21.6809\nAccepted tokens by draft model: 51\nGenerated tokens: 100\nAccepted token rate, %: 51\n===============================\nRequest_id: 0 ||| 40 0 40 20 0 0 40 40 0 20 20 20 0 40 0 0 20 80 0 80 20 0 0 0 40 80 0 40 60 40 80 0 0 0 0 40 20 20 0 40 20 40 0 20 0 0 0\n```\n\n----------------------------------------\n\nTITLE: Inpainting Image Generation Prompt Example\nDESCRIPTION: This JSON snippet provides an example for Inpainting Image Generation tasks.  It includes parameters like 'steps', 'width', 'height', 'guidance_scale', 'strength', 'prompt', 'media' and 'mask_image' to control the inpainting generation process. These parameters define the image resolution, inference steps, input prompt, input image and mask image.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/llm_bench/doc/PROMPT.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\"steps\":\"10\", \"width\":\"256\", \"height\":\"256\", \"guidance_scale\":\"0.8\", \"prompt\": \"side profile centered painted portrait, Gandhi rolling a blunt, Gloomhaven, matte painting concept art, art nouveau, 8K HD Resolution, beautifully background\", \"media\": \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\", \"mask_image\": \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"}\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Image using curl\nDESCRIPTION: Downloads a sample image (dog.jpg) from a specified URL using the `curl` command. This image is used as input for the visual language model.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ncurl -O \"https://storage.openvinotoolkit.org/test_data/images/dog.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Generate Ground Truth from SQuAD Dataset\nDESCRIPTION: This command generates ground truth data for a baseline model using the SQuAD dataset, saving the output to a CSV file. It specifies the base model, output file, dataset, split, and dataset field.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/tools/who_what_benchmark/README.md#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nwwb --base-model meta-llama/Llama-2-7b-chat-hf --gt-data llama_2_7b_squad_gt.csv --dataset squad --split validation[:32] --dataset-field question\n```\n\n----------------------------------------\n\nTITLE: Set PYTHONPATH and PATH for GenAI (Windows PowerShell)\nDESCRIPTION: Sets the `PYTHONPATH` and `PATH` environment variables manually for OpenVINO GenAI in Windows PowerShell. These variables are necessary for the system to find the Python modules and shared libraries of GenAI.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/docs/BUILD.md#_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\n$env:PYTHONPATH = \"<GENAI_ROOT_DIR>\\build;$env:PYTHONPATH\"\n$env:PATH = \"<GENAI_ROOT_DIR>\\build\\openvino_genai;$env:PATH\"\n```\n\n----------------------------------------\n\nTITLE: Sourcing OpenVINO setupvars script (Linux/macOS)\nDESCRIPTION: Sets up the environment by sourcing the OpenVINO `setupvars.sh` script. This script configures environment variables required for OpenVINO development, such as paths to libraries and binaries. `<INSTALL_DIR>` should be replaced with the actual installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/src/js/BUILD.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsource <INSTALL_DIR>/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Converting Visual Language Model with Optimum CLI\nDESCRIPTION: This command uses the Optimum CLI to download and convert the OpenGVLab/InternVL2-1B model to OpenVINO format with int4 weight compression for the language model and int8 for other components.  `timm` and `einops` libraries are required before running this command.\nSOURCE: https://github.com/openvinotoolkit/openvino.genai/blob/master/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# Download and convert the OpenGVLab/InternVL2-1B model to OpenVINO with int4 weight-compression for the language model\n# Other components are compressed to int8\noptimum-cli export openvino -m OpenGVLab/InternVL2-1B --trust-remote-code --weight-format int4 InternVL2-1B\n```"
  }
]
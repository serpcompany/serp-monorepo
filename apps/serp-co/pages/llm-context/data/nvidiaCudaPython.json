[
  {
    "owner": "nvidia",
    "repo": "cuda-python",
    "content": "TITLE: Compare Kernel and Application Performance Metrics\nDESCRIPTION: This table summarizes the kernel and application execution times for C++ and Python implementations, illustrating nearly identical performance profiles. It aids in benchmarking and verifying that Python CUDA code achieves comparable efficiency to C++.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_14\n\n\n\n----------------------------------------\n\nTITLE: Preparing Kernel Arguments Using NumPy for CUDA Kernel Launch\nDESCRIPTION: This detailed guide explains how to prepare kernel arguments using NumPy arrays, including creating custom data types, allocating device memory, and constructing the `kernelParams` array. It ensures the kernel receives properly formatted pointers in contiguous memory, compatible with the CUDA launch API. It also covers the usage of ctypes as an alternative to prepare parameters with proper memory layout.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ntestStruct = np.dtype([(\"value\", np.int32)], align=True)\n\npInt = checkCudaErrors(cudart.cudaMalloc(np.dtype(np.int32).itemsize))\npFloat = checkCudaErrors(cudart.cudaMalloc(np.dtype(np.float32).itemsize))\npStruct = checkCudaErrors(cudart.cudaMalloc(testStruct.itemsize))\n\nkernelValues = (\n    np.array(1, dtype=np.uint32),\n    np.array([pInt], dtype=np.intp),\n    np.array(123.456, dtype=np.float32),\n    np.array([pFloat], dtype=np.intp),\n    np.array([5], testStruct),\n    np.array([pStruct], dtype=np.intp),\n)\n\nkernelParams = np.array([arg.ctypes.data for arg in kernelValues], dtype=np.intp)\n\ncheckCudaErrors(cuda.cuLaunchKernel(\n    kernel,\n    1, 1, 1,\n    1, 1, 1,\n    0, stream,\n    kernelParams=kernelParams,\n    extra=0,\n))\n```\n\n----------------------------------------\n\nTITLE: Preparing Kernel Argument List in Python\nDESCRIPTION: Constructs the argument list required by `cuLaunchKernel`. It creates a Python list containing the host scalar NumPy array (`a`) and the NumPy arrays holding the device pointers (`dX`, `dY`, `dOut`, `n`). This list is then converted into a NumPy array (`args`) containing the `ctypes.data` pointers of each element, ensuring a contiguous array of pointers (`void**`) as expected by the launch API.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nargs = [a, dX, dY, dOut, n]\nargs = np.array([arg.ctypes.data for arg in args], dtype=np.uint64)\n```\n\n----------------------------------------\n\nTITLE: Performance Profiling with NVIDIA Nsight Systems and CUDA Events\nDESCRIPTION: This section discusses profiling CUDA application performance using NVIDIA Nsight Systems and CUDA Events. It shows how to compare kernel and application execution times between C++ and Python implementations, emphasizing minimal differences and profiling commands. It also introduces Nsight Compute for detailed kernel analysis.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_12\n\n\n\n----------------------------------------\n\nTITLE: Launching CUDA Kernel and Retrieving Results in Python\nDESCRIPTION: Launches the compiled SAXPY kernel (`kernel`) using `cuLaunchKernel` with the specified grid/block dimensions (`NUM_BLOCKS`, `NUM_THREADS`), the CUDA stream (`stream`), and the prepared argument list pointer (`args.ctypes.data`). It then asynchronously copies the results from the device output buffer (`dOutclass`) back to the host array (`hOut`) using `cuMemcpyDtoHAsync`. Finally, `cuStreamSynchronize` blocks CPU execution until all operations enqueued in the stream (data copies and kernel launch) are complete.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ncheckCudaErrors(driver.cuLaunchKernel(\n   kernel,\n   NUM_BLOCKS,  # grid x dim\n   1,  # grid y dim\n   1,  # grid z dim\n   NUM_THREADS,  # block x dim\n   1,  # block y dim\n   1,  # block z dim\n   0,  # dynamic shared memory\n   stream,  # stream\n   args.ctypes.data,  # kernel arguments\n   0,  # extra (ignore)\n))\n\ncheckCudaErrors(driver.cuMemcpyDtoHAsync(\n   hOut.ctypes.data, dOutclass, bufferSize, stream\n))\ncheckCudaErrors(driver.cuStreamSynchronize(stream))\n```\n\n----------------------------------------\n\nTITLE: Memory Management Functions for CUDA Runtime in C\nDESCRIPTION: Functions to allocate, free, and manage memory resources on the host and device, including unified memory, pinned memory, 3D arrays, and mipmapped arrays. Dependencies include CUDA runtime; these functions are essential for flexible and efficient memory handling in CUDA applications.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_8\n\nLANGUAGE: C\nCODE:\n```\ncuda.bindings.runtime.cudaMallocManaged\ncuda.bindings.runtime.cudaMalloc\ncuda.bindings.runtime.cudaMallocHost\ncuda.bindings.runtime.cudaMallocPitch\ncuda.bindings.runtime.cudaMallocArray\ncuda.bindings.runtime.cudaFree\ncuda.bindings.runtime.cudaFreeHost\ncuda.bindings.runtime.cudaFreeArray\ncuda.bindings.runtime.cudaFreeMipmappedArray\ncuda.bindings.runtime.cudaHostAlloc\ncuda.bindings.runtime.cudaHostRegister\ncuda.bindings.runtime.cudaHostUnregister\ncuda.bindings.runtime.cudaHostGetDevicePointer\ncuda.bindings.runtime.cudaHostGetFlags\ncuda.bindings.runtime.cudaMalloc3D\ncuda.bindings.runtime.cudaMalloc3DArray\ncuda.bindings.runtime.cudaMallocMipmappedArray\ncuda.bindings.runtime.cudaGetMipmappedArrayLevel\ncuda.bindings.runtime.cudaMemcpy3D\ncuda.bindings.runtime.cudaMemcpy3DPeer\ncuda.bindings.runtime.cudaMemcpy3DAsync\ncuda.bindings.runtime.cudaMemcpy3DPeerAsync\ncuda.bindings.runtime.cudaMemGetInfo\ncuda.bindings.runtime.cudaArrayGetInfo\ncuda.bindings.runtime.cudaArrayGetPlane\ncuda.bindings.runtime.cudaArrayGetMemoryRequirements\ncuda.bindings.runtime.cudaMipmappedArrayGetMemoryRequirements\ncuda.bindings.runtime.cudaArrayGetSparseProperties\ncuda.bindings.runtime.cudaMipmappedArrayGetSparseProperties\ncuda.bindings.runtime.cudaMemcpy\ncuda.bindings.runtime.cudaMemcpyPeer\ncuda.bindings.runtime.cudaMemcpy2D\ncuda.bindings.runtime.cudaMemcpy2DToArray\ncuda.bindings.runtime.cudaMemcpy2DFromArray\ncuda.bindings.runtime.cudaMemcpy2DArrayToArray\n```\n\n----------------------------------------\n\nTITLE: Profile CUDA Application with Nsight Systems Shell Command\nDESCRIPTION: This shell command profiles the CUDA application execution using Nsight Systems, enabling detailed performance analysis and statistics collection for CUDA kernels and applications.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nnsys profile -s none -t cuda --stats=true <executable>\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_MEMORY_POOLS_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports using the `cuMemAllocAsync` and `cuMemPool` family of APIs. Memory pools provide a mechanism for more efficient memory management and allocation.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_81\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_MEMORY_POOLS_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Loading PTX and Getting Kernel Function in Python\nDESCRIPTION: Loads the compiled PTX code (generated previously by NVRTC and stored in the `ptx` variable) into a CUDA module using `cuModuleLoadData`. Then, it retrieves a handle (`kernel`) to the specific 'saxpy' kernel function within that module using `cuModuleGetFunction`. The PTX data is passed via its `ctypes.data` pointer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Load PTX as module data and retrieve function\nptx = np.char.array(ptx)\n# Note: Incompatible --gpu-architecture would be detected here\nmodule = checkCudaErrors(driver.cuModuleLoadData(ptx.ctypes.data))\nkernel = checkCudaErrors(driver.cuModuleGetFunction(module, b\"saxpy\"))\n```\n\n----------------------------------------\n\nTITLE: Retrieving Device Memory Pointers in Python\nDESCRIPTION: Obtains the integer representation of the device memory pointers allocated previously (`dXclass`, `dYclass`, `dOutclass`) by casting the `CUdeviceptr` objects returned by `cuMemAlloc` to Python `int`. These integer pointers are then wrapped in NumPy arrays (`dX`, `dY`, `dOut`) of type `uint64` for easier handling when constructing the kernel argument list.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndX = np.array([int(dXclass)], dtype=np.uint64)\ndY = np.array([int(dYclass)], dtype=np.uint64)\ndOut = np.array([int(dOutclass)], dtype=np.uint64)\n```\n\n----------------------------------------\n\nTITLE: Cleanup CUDA Resources in Python\nDESCRIPTION: This snippet performs memory deallocation and resource cleanup for CUDA contexts, modules, streams, and device memory in Python. It ensures that all allocated resources are properly released to prevent memory leaks and dangling references after kernel execution.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ncheckCudaErrors(driver.cuStreamDestroy(stream))\ncheckCudaErrors(driver.cuMemFree(dXclass))\ncheckCudaErrors(driver.cuMemFree(dYclass))\ncheckCudaErrors(driver.cuMemFree(dOutclass))\ncheckCudaErrors(driver.cuModuleUnload(module))\ncheckCudaErrors(driver.cuCtxDestroy(context))\n```\n\n----------------------------------------\n\nTITLE: Event Management Functions for CUDA Runtime in C\nDESCRIPTION: Functions to create, record, query, synchronize, and destroy CUDA events, essential for timing and synchronization. Dependencies include CUDA runtime; functions facilitate precise event tracking and measurement of GPU tasks, with key functions like cudaEventCreate, cudaEventRecord, cudaEventQuery, and cudaEventDestroy.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_4\n\nLANGUAGE: C\nCODE:\n```\ncuda.bindings.runtime.cudaEventCreate\ncuda.bindings.runtime.cudaEventCreateWithFlags\ncuda.bindings.runtime.cudaEventRecord\ncuda.bindings.runtime.cudaEventRecordWithFlags\ncuda.bindings.runtime.cudaEventQuery\ncuda.bindings.runtime.cudaEventSynchronize\ncuda.bindings.runtime.cudaEventDestroy\ncuda.bindings.runtime.cudaEventElapsedTime\ncuda.bindings.runtime.cudaEventElapsedTime_v2\n```\n\n----------------------------------------\n\nTITLE: Handling CUDA Object Types in Kernel Arguments\nDESCRIPTION: This snippet covers passing NVIDIA CUDA objects, like `cudaTextureObject_t`, into kernels. It shows how to convert such objects to integer pointers using `__int__()` for NumPy arrays, and how to prepare `kernelValues` and `kernelTypes` with special handling via `getPtr()` or `__int__()` methods for ctypes. This allows passing GPU resource handles as kernel parameters.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nsimpleCubemapTexture = \"\"\"\\\nextern \"C\"\n__global__ void transformKernel(float *g_odata, int width, cudaTextureObject_t tex)\n{\n    ...\n}\n\"\"\"\n\ndef main():\n    ...\n    d_data = checkCudaErrors(cudart.cudaMalloc(size))\n    width = 64\n    tex = checkCudaErrors(cudart.cudaCreateTextureObject(texRes, texDescr, None))\n    ...\n    kernelValues = (\n        np.array([d_data], dtype=np.intp),\n        np.array(width, dtype=np.uint32),\n        np.array([int(tex)], dtype=np.intp),\n    )\n    kernelArgs = np.array([arg.ctypes.data for arg in kernelValues], dtype=np.intp)\n\n# For ctypes approach\n    kernelValues = (\n        d_data,\n        width,\n        tex,\n    )\n    kernelTypes = (\n        ctypes.c_void_p,\n        ctypes.c_int,\n        None,\n    )\n    kernelArgs = (kernelValues, kernelTypes)\n\n```\n\n----------------------------------------\n\nTITLE: Building CUDA Python Documentation Using Shell Script\nDESCRIPTION: This snippet shows how to build the documentation by executing the provided shell script `./build_docs.sh`, ensuring the version is listed in `versions.json`. It also explains how to generate all docs at once with `./build_all_docs.sh`. The snippets include commands for local server setup for previewing documentation by setting the `CUDA_PYTHON_DOMAIN` environment variable and starting a Python HTTP server, as well as instructions for establishing an SSH tunnel for remote access. These commands facilitate local testing and remote browsing of generated docs, supporting version management and deployment workflows.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_python/docs/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Build the documentation\n1. Ensure the version is included in the [`versions.json`](./versions.json).\n2. Build the docs with `./build_docs.sh`.\n3. The html artifacts should be available under both `./build/html/latest` and `./build/html/<version>`.\n\nAlternatively, we can build all the docs at once by running [`./build_all_docs.sh`](./build_all_docs.sh).\n\nWhen building the docs, some (but not all) of the urls can be rendered/examined locally by setting the environment\nvariable `CUDA_PYTHON_DOMAIN` as follows:\n```shell\nCUDA_PYTHON_DOMAIN=\"http://localhost:1234/\" ./build_all_docs.sh\npython -m http.server -d build/html 1234\n```\nIf the docs are built on a remote machine, you can set up the ssh tunnel in a separate terminal session\nvia\n```shell\nssh -L 1234:localhost:1234 username@hostname\n```\nThen browse the built docs by visiting `http://localhost:1234/` on a local machine.\n\nTo publish the docs with the built version, it is important to note that the html files of older versions\nshould be kept intact, in order for the version selection (through `versions.json`) to work.\n\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_READ_ONLY_HOST_REGISTER_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports using the `cuMemHostRegister` flag `CU_MEMHOSTERGISTER_READ_ONLY` to register memory that must be mapped as read-only to the GPU. This improves memory safety and performance in certain cases.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_79\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_READ_ONLY_HOST_REGISTER_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Documenting CUDA Memory Pool Attributes\nDESCRIPTION: This code snippet documents the attributes related to CUDA memory pools. These attributes control memory reuse behavior, such as whether to allow reusing memory freed in other streams, to what extent to reuse released memory when there is no dependency, and how much memory to reserve before releasing it to the OS. This affects memory management and performance within the CUDA environment.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_52\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaMemPoolAttr\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaMemPoolAttr.cudaMemPoolReuseFollowEventDependencies\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaMemPoolAttr.cudaMemPoolReuseAllowOpportunistic\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaMemPoolAttr.cudaMemPoolReuseAllowInternalDependencies\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaMemPoolAttr.cudaMemPoolAttrReleaseThreshold\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Source Build (Shell)\nDESCRIPTION: Example commands for setting the `CUDA_HOME` and `LIBRARY_PATH` environment variables on Linux. This is necessary when building `cuda-python` from source to specify the location of the required CUDA Toolkit headers and libraries.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/install.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ export CUDA_HOME=/usr/local/cuda\n$ export LIBRARY_PATH=$CUDA_HOME/lib64:$LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Installing cuda-python via PyPI (Shell)\nDESCRIPTION: Installs or upgrades the `cuda-python` package using the pip package manager from the Python Package Index (PyPI). This is the standard way to install the package.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/install.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ pip install -U cuda-python\n```\n\n----------------------------------------\n\nTITLE: Error Handling Functions for CUDA Runtime in C\nDESCRIPTION: Provides mechanisms to retrieve the last error, peek error status, and convert error codes to human-readable names and strings. These functions facilitate debugging and error management in CUDA applications; dependencies are CUDA runtime. Key functions include cudaGetLastError, cudaPeekAtLastError, cudaGetErrorName, and cudaGetErrorString.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_2\n\nLANGUAGE: C\nCODE:\n```\ncuda.bindings.runtime.cudaGetLastError\ncuda.bindings.runtime.cudaPeekAtLastError\ncuda.bindings.runtime.cudaGetErrorName\ncuda.bindings.runtime.cudaGetErrorString\n```\n\n----------------------------------------\n\nTITLE: Defining the SAXPY CUDA C++ Kernel as a String\nDESCRIPTION: Defines the SAXPY (Single-precision A*X Plus Y) CUDA C++ kernel as a multiline Python string. This kernel computes `out = a * x + y` for each element in the input arrays `x` and `y`, using standard CUDA thread indexing.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nsaxpy = \"\"\"\\\nextern \\\"C\\\" __global__\nvoid saxpy(float a, float *x, float *y, float *out, size_t n)\n{\n size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n if (tid < n) {\n   out[tid] = a * x[tid] + y[tid];\n }\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing cuda-core from PyPI\nDESCRIPTION: Installs the `cuda.core` package with CUDA 12 support using pip. This requires `cuda.bindings` (part of `cuda-python`) 12. The `[cu12]` extra specifies the CUDA version.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/docs/source/install.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ pip install cuda-core[cu12]\n```\n\n----------------------------------------\n\nTITLE: Implementing __cuda_stream__ protocol in Python\nDESCRIPTION: This code snippet shows how to implement the ``__cuda_stream__`` protocol in a Python class representing a CUDA stream.  The ``__cuda_stream__`` method returns a tuple containing the version number (0) and the address of the underlying ``cudaStream_t`` as an integer.  This enables ``cuda.core`` to recognize and use the stream object seamlessly.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/docs/source/interoperability.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MyStream:\n\n    def __cuda_stream__(self):\n        return (0, self.ptr)\n\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing CUDA Core Python Package with pip\nDESCRIPTION: Command to install the cuda-core Python package with CUDA 12.x compatibility using pip. The package provides Pythonic access to CUDA core functionalities.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/DESCRIPTION.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install cuda-core[cu12]\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_FABRIC_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports exporting memory to a fabric handle using `cuMemExportToShareableHandle()` or if requested using `cuMemCreate()`\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_94\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_FABRIC_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Device Management Functions for CUDA Runtime in C\nDESCRIPTION: A set of functions to manage and query CUDA-capable devices, such as resetting devices, synchronizing, setting limits, retrieving properties, and managing memory pools. Dependencies include CUDA runtime; these functions enable fine-grained control and information retrieval about GPU devices.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_1\n\nLANGUAGE: C\nCODE:\n```\ncuda.bindings.runtime.cudaDeviceReset\ncuda.bindings.runtime.cudaDeviceSynchronize\ncuda.bindings.runtime.cudaDeviceSetLimit\ncuda.bindings.runtime.cudaDeviceGetLimit\ncuda.bindings.runtime.cudaDeviceGetTexture1DLinearMaxWidth\ncuda.bindings.runtime.cudaDeviceGetCacheConfig\ncuda.bindings.runtime.cudaDeviceGetStreamPriorityRange\ncuda.bindings.runtime.cudaDeviceSetCacheConfig\ncuda.bindings.runtime.cudaDeviceGetByPCIBusId\ncuda.bindings.runtime.cudaDeviceGetPCIBusId\ncuda.bindings.runtime.cudaIpcGetEventHandle\ncuda.bindings.runtime.cudaIpcOpenEventHandle\ncuda.bindings.runtime.cudaIpcGetMemHandle\ncuda.bindings.runtime.cudaIpcOpenMemHandle\ncuda.bindings.runtime.cudaIpcCloseMemHandle\ncuda.bindings.runtime.cudaDeviceFlushGPUDirectRDMAWrites\ncuda.bindings.runtime.cudaDeviceRegisterAsyncNotification\ncuda.bindings.runtime.cudaDeviceUnregisterAsyncNotification\ncuda.bindings.runtime.cudaGetDeviceCount\ncuda.bindings.runtime.cudaGetDeviceProperties\ncuda.bindings.runtime.cudaDeviceGetAttribute\ncuda.bindings.runtime.cudaDeviceGetDefaultMemPool\ncuda.bindings.runtime.cudaDeviceSetMemPool\ncuda.bindings.runtime.cudaDeviceGetMemPool\ncuda.bindings.runtime.cudaDeviceGetNvSciSyncAttributes\ncuda.bindings.runtime.cudaDeviceGetP2PAttribute\ncuda.bindings.runtime.cudaChooseDevice\ncuda.bindings.runtime.cudaInitDevice\ncuda.bindings.runtime.cudaSetDevice\ncuda.bindings.runtime.cudaGetDevice\ncuda.bindings.runtime.cudaSetDeviceFlags\ncuda.bindings.runtime.cudaGetDeviceFlags\n```\n\n----------------------------------------\n\nTITLE: CUDA Stream and Event Classes\nDESCRIPTION: Includes classes for CUDA stream and event management, supporting default, non-blocking, legacy, per-thread streams, and various event flags for synchronization, timing, and interprocess sharing.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_67\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaStream_t\n.. autoclass:: cuda.bindings.runtime.cudaEvent_t\n.. autoattribute:: cuda.bindings.runtime.cudaStreamDefault\n\n    Default stream flag\n\n.. autoattribute:: cuda.bindings.runtime.cudaStreamNonBlocking\n\n    Stream does not synchronize with stream 0 (the NULL stream)\n\n.. autoattribute:: cuda.bindings.runtime.cudaStreamLegacy\n\n    Legacy stream handle that can be passed as a cudaStream_t to use an implicit stream with legacy synchronization behavior.\n\n.. autoattribute:: cuda.bindings.runtime.cudaStreamPerThread\n\n    Per-thread stream handle that can be used for thread-specific operations.\n\n.. autoattribute:: cuda.bindings.runtime.cudaEventDefault\n\n    Default event flag\n\n.. autoattribute:: cuda.bindings.runtime.cudaEventBlockingSync\n\n    Event uses blocking synchronization\n\n.. autoattribute:: cuda.bindings.runtime.cudaEventDisableTiming\n\n    Event will not record timing data\n\n.. autoattribute:: cuda.bindings.runtime.cudaEventInterprocess\n\n    Event is suitable for interprocess use; cudaEventDisableTiming must be set\n\n.. autoattribute:: cuda.bindings.runtime.cudaEventRecordDefault\n\n    Default event record flag\n\n.. autoattribute:: cuda.bindings.runtime.cudaEventRecordExternal\n\n    Event is captured in the graph as an external event node when performing stream capture\n\n.. autoattribute:: cuda.bindings.runtime.cudaEventWaitDefault\n\n    Default event wait flag\n\n.. autoattribute:: cuda.bindings.runtime.cudaEventWaitExternal\n\n    Event is captured in the graph as an external event node when performing stream capture\n```\n\n----------------------------------------\n\nTITLE: Setting Attributes for CUDA Extension Types in Python\nDESCRIPTION: This snippet demonstrates proper initialization and attribute setting for CUDA extension type instances using NVIDIA's cuda-python low-level bindings. It emphasizes the difference between using setter methods (which propagate changes to the underlying C object) and in-place modification (which may not synchronize as expected). This approach relies on the driver and extension types provided by cuda-python, particularly 'CUlaunchConfig' and 'CUlaunchAttribute'. Users should ensure that the CUDA Python bindings are installed and accessible. The code initializes a launch configuration, updates an attribute count, creates a new attribute, and safely sets it using the attribute setter with a list assignment, as direct append to the attribute list does not synchronize with the underlying CUDA object.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/tips_and_tricks.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncfg = cuda.CUlaunchConfig()\n\ncfg.numAttrs += 1\nattr = cuda.CUlaunchAttribute()\n\n...\n\n# This works. We are passing the new attribute to the setter\ndrv_cfg.attrs = [attr]\n\n# This does not work. We are only modifying the returned attribute in place\ndrv_cfg.attrs.append(attr)\n```\n\n----------------------------------------\n\nTITLE: Installing cuda-python with Optional Dependencies via PyPI (Shell)\nDESCRIPTION: Installs or upgrades `cuda-python` along with all its optional dependencies (nvidia-cuda-nvrtc-cu12, nvidia-nvjitlink-cu12, nvidia-cuda-nvcc-cu12) using pip. These dependencies provide NVRTC, nvJitLink, and NVVM shared libraries respectively.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/install.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install -U cuda-python[all]\n```\n\n----------------------------------------\n\nTITLE: CUDA Memory Type Enumeration\nDESCRIPTION: This class defines different types of CUDA memory, including unregistered, host, device, and managed memory. It is used to specify and manage memory allocation types explicitly within CUDA Python programs.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaMemoryType\n\n    .. autoattribute:: cuda.bindings.runtime.cudaMemoryType.cudaMemoryTypeUnregistered\n\n        Unregistered memory\n\n    .. autoattribute:: cuda.bindings.runtime.cudaMemoryType.cudaMemoryTypeHost\n\n        Host memory\n\n    .. autoattribute:: cuda.bindings.runtime.cudaMemoryType.cudaMemoryTypeDevice\n\n        Device memory\n\n    .. autoattribute:: cuda.bindings.runtime.cudaMemoryType.cudaMemoryTypeManaged\n\n        Managed memory\n\n```\n\n----------------------------------------\n\nTITLE: Installing Optional Dependencies (cuda-python) - Shell\nDESCRIPTION: This command demonstrates how to install the `cuda-python` package along with its optional dependencies (like `nvidia-cuda-nvrtc-cu12` and `nvidia-nvjitlink-cu12`) using pip. The `[all]` extra includes these optional packages, which provide additional CUDA toolkit features.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/release/12.8.0-notes.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install cuda-python[all]\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_advise Options\nDESCRIPTION: Provides advice to the CUDA runtime about memory access patterns.  These options, such as `CU_MEM_ADVISE_UNSET_READ_MOSTLY`, provide hints to optimize memory usage. These advise calls can improve memory management.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_126\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_advise.CU_MEM_ADVISE_UNSET_READ_MOSTLY\n```\n\n----------------------------------------\n\nTITLE: Allocating Device Memory and Copying Data in Python\nDESCRIPTION: Allocates memory buffers (`dXclass`, `dYclass`, `dOutclass`) on the GPU using `cuMemAlloc` based on the calculated `bufferSize`. Creates a CUDA stream (`stream`) for asynchronous operations. Asynchronously copies the host input arrays (`hX`, `hY`) to the corresponding device buffers using `cuMemcpyHtoDAsync`, passing the host data via their `ctypes.data` pointers.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndXclass = checkCudaErrors(driver.cuMemAlloc(bufferSize))\ndYclass = checkCudaErrors(driver.cuMemAlloc(bufferSize))\ndOutclass = checkCudaErrors(driver.cuMemAlloc(bufferSize))\n\nstream = checkCudaErrors(driver.cuStreamCreate(0))\n\ncheckCudaErrors(driver.cuMemcpyHtoDAsync(\n   dXclass, hX.ctypes.data, bufferSize, stream\n))\ncheckCudaErrors(driver.cuMemcpyHtoDAsync(\n   dYclass, hY.ctypes.data, bufferSize, stream\n))\n```\n\n----------------------------------------\n\nTITLE: Running pre-commit (shell)\nDESCRIPTION: Executes pre-commit to check code formatting and ensure it adheres to the requirements defined in `pyproject.toml`. It displays diffs on failure. Requires pre-commit to be installed.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npre-commit run -a --show-diff-on-failure\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN\nDESCRIPTION: This attribute represents the maximum opt-in shared memory per block supported by the device. Applications can configure the amount of shared memory used by each block.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_62\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN\n```\n\n----------------------------------------\n\nTITLE: Profiler Control Functions for CUDA Runtime in C\nDESCRIPTION: Functions to start and stop the CUDA profiler, enabling performance analysis and profiling of CUDA applications. Dependencies include CUDA runtime library; key functions include cudaProfilerStart and cudaProfilerStop, which control profiling sessions.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_0\n\nLANGUAGE: C\nCODE:\n```\ncuda.bindings.runtime.cudaProfilerStart\ncuda.bindings.runtime.cudaProfilerStop\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_CAN_USE_64_BIT_STREAM_MEM_OPS\nDESCRIPTION: This attribute indicates whether 64-bit operations are supported in `cuStreamBatchMemOp` and related MemOp APIs.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_88\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_CAN_USE_64_BIT_STREAM_MEM_OPS\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_HOST_REGISTER_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports host memory registration via `cudaHostRegister`. Host memory registration allows for more efficient data transfer between the host and the device.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_64\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_HOST_REGISTER_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Installing Optional Dependencies for cuda-python and cuda-bindings using pip\nDESCRIPTION: This shell command installs the cuda-python package with all optional CUDA-related dependencies, including cuda-bindings and CUDA Toolkit variants like nvidia-cuda-nvrtc-cu12. It ensures that all relevant CUDA libraries are installed for comprehensive functionality.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/release/11.8.6-notes.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install cuda-python[all]\n```\n\n----------------------------------------\n\nTITLE: CUDA Filter Modes\nDESCRIPTION: Defines and documents CUDA filter modes. These constants specify the filtering method used when sampling textures. Point filtering returns the value of the closest texel, while linear filtering interpolates between texels. The filter mode is used during texture sampling operations.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_56\n\n\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports exporting memory to a POSIX file descriptor using `cuMemExportToShareableHandle`, if requested through `cuMemCreate`. This enables interoperability with other processes and systems.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_69\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_TIMELINE_SEMAPHORE_INTEROP_SUPPORTED\nDESCRIPTION: This attribute indicates if external timeline semaphore interoperation is supported on the device. Timeline semaphores allow for synchronization between CUDA streams and external entities.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_80\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_TIMELINE_SEMAPHORE_INTEROP_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: CUgraphInstantiate Flags Definition - CUDA Python\nDESCRIPTION: Defines flags for instantiating CUDA graphs. These flags control the instantiation behavior, such as automatically freeing memory on launch, uploading the graph, launching from the device, and using node priority attributes.  They influence how the graph is prepared and executed.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_163\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUgraphInstantiate_flags.CUDA_GRAPH_INSTANTIATE_FLAG_AUTO_FREE_ON_LAUNCH\n\n\n    Automatically free memory allocated in a graph before relaunching.\n\n\n.. autoattribute:: cuda.bindings.driver.CUgraphInstantiate_flags.CUDA_GRAPH_INSTANTIATE_FLAG_UPLOAD\n\n\n    Automatically upload the graph after instantiation. Only supported by :py:obj:`~.cuGraphInstantiateWithParams`. The upload will be performed using the stream provided in `instantiateParams`.\n\n\n.. autoattribute:: cuda.bindings.driver.CUgraphInstantiate_flags.CUDA_GRAPH_INSTANTIATE_FLAG_DEVICE_LAUNCH\n\n\n    Instantiate the graph to be launchable from the device. This flag can only be used on platforms which support unified addressing. This flag cannot be used in conjunction with CUDA_GRAPH_INSTANTIATE_FLAG_AUTO_FREE_ON_LAUNCH.\n\n\n.. autoattribute:: cuda.bindings.driver.CUgraphInstantiate_flags.CUDA_GRAPH_INSTANTIATE_FLAG_USE_NODE_PRIORITY\n\n\n    Run the graph using the per-node priority attributes rather than the priority of the stream it is launched into.\n```\n\n----------------------------------------\n\nTITLE: cudaErrorInsufficientDriver Definition\nDESCRIPTION: This defines the `cudaErrorInsufficientDriver` error. It signals that the installed NVIDIA CUDA driver is older than the CUDA runtime library. Users should update the driver to resolve this.  It points towards a driver compatibility issue.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorInsufficientDriver\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_range_attribute\nDESCRIPTION: Defines attributes that can be queried for a memory range. These attributes, such as `CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION`, provides details about how the memory range is being used and preferred location. It is typically used with CUmemGetInfo to obtain memory range information.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_132\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_range_attribute.CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_advise Options\nDESCRIPTION: Provides advice to the CUDA runtime about memory access patterns.  These options, such as `CU_MEM_ADVISE_SET_PREFERRED_LOCATION`, provide hints to optimize memory usage. These advise calls can improve memory management.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_127\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_advise.CU_MEM_ADVISE_SET_PREFERRED_LOCATION\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports compression of memory. Memory compression can reduce memory footprint and improve performance.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_73\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: CUDA Stream Capture Mode Options\nDESCRIPTION: This class enumerates modes for CUDA stream capture, including global, thread-local, and relaxed modes, allowing developers to control capture behavior for performance optimization.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_48\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaStreamCaptureMode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaStreamCaptureMode.cudaStreamCaptureModeGlobal\n\n    .. autoattribute:: cuda.bindings.runtime.cudaStreamCaptureMode.cudaStreamCaptureModeThreadLocal\n\n    .. autoattribute:: cuda.bindings.runtime.cudaStreamCaptureMode.cudaStreamCaptureModeRelaxed\n\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_DEFERRED_MAPPING_CUDA_ARRAY_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports deferred mapping CUDA arrays and CUDA mipmapped arrays.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_87\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_DEFERRED_MAPPING_CUDA_ARRAY_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Preparing Kernel Arguments Using ctypes for CUDA Kernel Launch\nDESCRIPTION: This section demonstrates how to prepare kernel arguments using ctypes structures and primitive types in Python. It explains how to allocate device memory, package input values with correct data types, and pass them as contiguous parameters to `cuLaunchKernel`. It also discusses handling CUDA objects requiring special pointer handling via methods like `getPtr()`.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nclass testStruct(ctypes.Structure):\n    _fields_ = [(\"value\", ctypes.c_int)]\n\npInt = checkCudaErrors(cudart.cudaMalloc(ctypes.sizeof(ctypes.c_int)))\npFloat = checkCudaErrors(cudart.cudaMalloc(ctypes.sizeof(ctypes.c_float)))\npStruct = checkCudaErrors(cudart.cudaMalloc(ctypes.sizeof(testStruct)))\n\nkernelValues = (\n    1,\n    pInt,\n    123.456,\n    pFloat,\n    testStruct(5),\n    pStruct,\n)\n\nkernelTypes = (\n    ctypes.c_int,\n    ctypes.c_void_p,\n    ctypes.c_float,\n    ctypes.c_void_p,\n    None,\n    ctypes.c_void_p,\n)\n\ncheckCudaErrors(cuda.cuLaunchKernel(\n    kernel,\n    1, 1, 1,\n    1, 1, 1,\n    0, stream,\n    kernelParams=(kernelValues, kernelTypes),\n    extra=0,\n))\n```\n\n----------------------------------------\n\nTITLE: CUDA Memory Copy Kind Enum Documentation\nDESCRIPTION: This enumeration specifies the direction of memory copy operations in CUDA, including host-to-host, host-to-device, device-to-host, device-to-device, and default inference based on pointers, essential for efficient data transfer management.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_45\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaMemcpyKind\n\n    .. autoattribute:: cuda.bindings.runtime.cudaMemcpyKind.cudaMemcpyHostToHost\n\n        Host -> Host\n\n    .. autoattribute:: cuda.bindings.runtime.cudaMemcpyKind.cudaMemcpyHostToDevice\n\n        Host -> Device\n\n    .. autoattribute:: cuda.bindings.runtime.cudaMemcpyKind.cudaMemcpyDeviceToHost\n\n        Device -> Host\n\n    .. autoattribute:: cuda.bindings.runtime.cudaMemcpyKind.cudaMemcpyDeviceToDevice\n\n        Device -> Device\n\n    .. autoattribute:: cuda.bindings.runtime.cudaMemcpyKind.cudaMemcpyDefault\n\n        Direction of the transfer is inferred from the pointer values. Requires unified virtual addressing\n\n```\n\n----------------------------------------\n\nTITLE: Defining CUmemorytype\nDESCRIPTION: Describes the type of memory being used, such as host memory, device memory, array memory, or unified memory. These types guide memory allocation and access patterns within the CUDA system.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_118\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmemorytype.CU_MEMORYTYPE_HOST\n```\n\n----------------------------------------\n\nTITLE: CUDA Access Property Enum Explanation\nDESCRIPTION: This class catalogs cache persistence properties for CUDA memory access, including normal, streaming, and persisting modes, affecting data caching strategies for performance tuning.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_46\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaAccessProperty\n\n    .. autoattribute:: cuda.bindings.runtime.cudaAccessProperty.cudaAccessPropertyNormal\n\n        Normal cache persistence.\n\n    .. autoattribute:: cuda.bindings.runtime.cudaAccessProperty.cudaAccessPropertyStreaming\n\n        Streaming access is less likely to persist from cache.\n\n    .. autoattribute:: cuda.bindings.runtime.cudaAccessProperty.cudaAccessPropertyPersisting\n\n        Persisting access is more likely to persist in cache.\n\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_D3D12_CIG_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports CIG (CUDA Interop with Graphics) with D3D12.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_101\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_D3D12_CIG_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Defining CUshared_carveout\nDESCRIPTION: Defines preferences for shared memory carveout.  These settings control the balance between shared memory and L1 cache. Examples include preferring maximum shared memory or maximum L1 cache.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_117\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUshared_carveout.CU_SHAREDMEM_CARVEOUT_MAX_L1\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_CAN_FLUSH_REMOTE_WRITES\nDESCRIPTION: This attribute indicates if the device supports flushing remote writes using the `CU_STREAM_WAIT_VALUE_FLUSH` flag and the `CU_STREAM_MEM_OP_FLUSH_REMOTE_WRITES` MemOp. This is related to stream memory operations.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_63\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_CAN_FLUSH_REMOTE_WRITES\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_DIRECT_MANAGED_MEM_ACCESS_FROM_HOST\nDESCRIPTION: This attribute indicates if the host can directly access managed memory on the device without migration. This simplifies memory management by allowing the host to directly read/write memory managed by the device.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_66\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_DIRECT_MANAGED_MEM_ACCESS_FROM_HOST\n```\n\n----------------------------------------\n\nTITLE: Defining CUfunc_cache Preferences\nDESCRIPTION: Describes various preferences for shared memory and L1 cache configurations.  These preferences are used to tune memory performance. Each attribute, such as `CU_FUNC_CACHE_PREFER_SHARED`, indicates a specific memory configuration preference.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_109\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUfunc_cache.CU_FUNC_CACHE_PREFER_SHARED\n```\n\n----------------------------------------\n\nTITLE: Defining CUfunc_cache Preferences\nDESCRIPTION: Describes various preferences for shared memory and L1 cache configurations.  These preferences are used to tune memory performance. Each attribute, such as `CU_FUNC_CACHE_PREFER_L1`, indicates a specific memory configuration preference.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_110\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUfunc_cache.CU_FUNC_CACHE_PREFER_L1\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_HOST_NUMA_ID\nDESCRIPTION: This attribute indicates the NUMA ID of the host node closest to the device. It returns -1 if the system does not support NUMA.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_100\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_HOST_NUMA_ID\n```\n\n----------------------------------------\n\nTITLE: Shared Memory Carveout Attribute Description\nDESCRIPTION: Details the 'sharedMemCarveout' attribute for kernel launches, which specifies the preferred percentage of shared memory to allocate for a kernel, especially when L1 cache and shared memory are hardware resources sharing. This attribute influences kernel launch configuration but is only a hint; the driver may override it based on the launch requirements.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_58\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributePreferredSharedMemoryCarveout\n\n Valid for launches. On devices where the L1 cache and shared memory use the same hardware resources, setting :py:obj:`~.cudaLaunchAttributeValue.sharedMemCarveout` to a percentage between 0-100 signals sets the shared memory carveout preference in percent of the total shared memory for that kernel launch. This attribute takes precedence over :py:obj:`~.cudaFuncAttributePreferredSharedMemoryCarveout`. This is only a hint, and the driver can choose a different configuration if required for the launch.\n```\n\n----------------------------------------\n\nTITLE: Enable Automatic PCH Option\nDESCRIPTION: This option enables automatic Precompiled Header (PCH) processing.  This is used to speed up compilation. The dependency is CUDA 12.8 or later.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n --pch ( -pch )\n```\n\n----------------------------------------\n\nTITLE: Defining CUsharedconfig\nDESCRIPTION: Defines the configuration of shared memory. Options include setting the default bank size, the four-byte bank size, or eight-byte bank size.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_112\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUsharedconfig.CU_SHARED_MEM_CONFIG_DEFAULT_BANK_SIZE\n```\n\n----------------------------------------\n\nTITLE: CUDA Array and Memory Handle Classes\nDESCRIPTION: Defines classes representing CUDA array objects, external memory, IPC handles, and similar constructs used for memory management and interprocess communication within CUDA applications.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_66\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaEglPlaneDesc\n.. autoclass:: cuda.bindings.runtime.cudaEglFrame\n.. autoclass:: cuda.bindings.runtime.cudaEglStreamConnection\n.. autoclass:: cuda.bindings.runtime.cudaArray_t\n.. autoclass:: cuda.bindings.runtime.cudaArray_const_t\n.. autoclass:: cuda.bindings.runtime.cudaMipmappedArray_t\n.. autoclass:: cuda.bindings.runtime.cudaMipmappedArray_const_t\n.. autoclass:: cuda.bindings.runtime.cudaHostFn_t\n.. autoclass:: cuda.bindings.runtime.CUuuid\n.. autoclass:: cuda.bindings.runtime.cudaUUID_t\n.. autoclass:: cuda.bindings.runtime.cudaIpcEventHandle_t\n.. autoclass:: cuda.bindings.runtime.cudaIpcMemHandle_t\n.. autoclass:: cuda.bindings.runtime.cudaMemFabricHandle_t\n```\n\n----------------------------------------\n\nTITLE: cudaErrorNoDevice Definition\nDESCRIPTION: This defines the `cudaErrorNoDevice` error, which signifies that no CUDA-capable devices were detected by the installed CUDA driver. This usually indicates a hardware or driver issue.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorNoDevice\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_range_attribute\nDESCRIPTION: Defines attributes that can be queried for a memory range. These attributes, such as `CU_MEM_RANGE_ATTRIBUTE_READ_MOSTLY`, provides details about how the memory range is being used and preferred location. It is typically used with CUmemGetInfo to obtain memory range information.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_131\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_range_attribute.CU_MEM_RANGE_ATTRIBUTE_READ_MOSTLY\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_range_attribute\nDESCRIPTION: Defines attributes that can be queried for a memory range. These attributes, such as `CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION_ID`, provides details about how the memory range is being used and preferred location. It is typically used with CUmemGetInfo to obtain memory range information.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_136\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_range_attribute.CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION_ID\n```\n\n----------------------------------------\n\nTITLE: cudaErrorDevicesUnavailable Definition\nDESCRIPTION: This describes the `cudaErrorDevicesUnavailable` error, signaling that all CUDA devices are busy or unavailable. This can be due to exclusive compute modes, blocking GPU usage, or memory constraints. It points towards a hardware availability problem.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorDevicesUnavailable\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_range_attribute\nDESCRIPTION: Defines attributes that can be queried for a memory range. These attributes, such as `CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION_ID`, provides details about how the memory range is being used and preferred location. It is typically used with CUmemGetInfo to obtain memory range information.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_138\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_range_attribute.CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION_ID\n```\n\n----------------------------------------\n\nTITLE: cudaErrorCallRequiresNewerDriver Definition\nDESCRIPTION: This documents the `cudaErrorCallRequiresNewerDriver` error. It signifies that the API call requires a newer CUDA driver than the one currently installed. Users should install an updated NVIDIA CUDA driver to use the API call. Driver version incompatibilities are a main reason.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorCallRequiresNewerDriver\n```\n\n----------------------------------------\n\nTITLE: Optimization Info Option\nDESCRIPTION: This option provides optimization reports for the specified kind of optimization. Requires the CUDA compiler. The supported kind tags must be known.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\n --optimization-info=<kind> ( -opt-info )\n```\n\n----------------------------------------\n\nTITLE: cudaErrorInvalidDevice Definition\nDESCRIPTION: This describes the `cudaErrorInvalidDevice` error. It occurs when the device ordinal supplied doesn't correspond to a valid CUDA device, or that the action requested is invalid for the specified device. This often means the device ID passed isn't correct or valid.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorInvalidDevice\n```\n\n----------------------------------------\n\nTITLE: cudaErrorArrayIsMapped Definition\nDESCRIPTION: This describes the `cudaErrorArrayIsMapped` error. It indicates that the specified array is currently mapped and thus cannot be destroyed. This shows issues when freeing up allocated device memory.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorArrayIsMapped\n```\n\n----------------------------------------\n\nTITLE: cudaErrorMapBufferObjectFailed Definition\nDESCRIPTION: This describes the `cudaErrorMapBufferObjectFailed` error, which indicates that the buffer object could not be mapped. This usually refers to an issue when allocating or accessing buffer objects.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorMapBufferObjectFailed\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_IPC_EVENT_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports IPC Events (Inter-Process Communication Events).\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_91\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_IPC_EVENT_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_MAX_BLOCKS_PER_MULTIPROCESSOR\nDESCRIPTION: This attribute specifies the maximum number of blocks per multiprocessor that the device supports. This limits the concurrency within a single multiprocessor.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_72\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_MAX_BLOCKS_PER_MULTIPROCESSOR\n```\n\n----------------------------------------\n\nTITLE: Defining CUcomputemode\nDESCRIPTION: Defines compute modes for a device. These modes control how contexts are created and used on a device.  Compute modes include default, prohibited, and exclusive process modes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_122\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUcomputemode.CU_COMPUTEMODE_DEFAULT\n```\n\n----------------------------------------\n\nTITLE: CUDA Driver Proc Address Flags\nDESCRIPTION: Defines flags for selecting procedures' address retrieval strategies within the CUDA driver, including default, legacy stream, and per-thread default stream options. These flags influence how driver symbols are searched and resolved in application code.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_154\n\nLANGUAGE: Python\nCODE:\n```\n# No specific code snippets provided; enumerates options for driver symbol address search.\n```\n\n----------------------------------------\n\nTITLE: Allow __int128 Type Option\nDESCRIPTION: This option allows the `__int128` type in device code and defines the macro `CUDACC_RTC_INT128`. Requires the CUDA compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\n --device-int128 ( -device-int128 )\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA Error Checking Helpers in Python\nDESCRIPTION: Defines two helper functions: `_cudaGetErrorEnum` to get the string representation of CUDA Driver or NVRTC errors, and `checkCudaErrors` to validate API call results and raise runtime exceptions if an error occurs. This pattern is used throughout the subsequent snippets.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef _cudaGetErrorEnum(error):\n    if isinstance(error, driver.CUresult):\n        err, name = driver.cuGetErrorName(error)\n        return name if err == driver.CUresult.CUDA_SUCCESS else \"<unknown>\"\n    elif isinstance(error, nvrtc.nvrtcResult):\n        return nvrtc.nvrtcGetErrorString(error)[1]\n    else:\n        raise RuntimeError('Unknown error type: {}'.format(error))\n\ndef checkCudaErrors(result):\n    if result[0].value:\n        raise RuntimeError(\"CUDA error code={}({})\".format(result[0].value, _cudaGetErrorEnum(result[0])))\n    if len(result) == 1:\n        return None\n    elif len(result) == 2:\n        return result[1]\n    else:\n        return result[1:]\n```\n\n----------------------------------------\n\nTITLE: Installing cuda-python via Conda (Shell)\nDESCRIPTION: Installs the `cuda-python` package using the Conda package manager from the `conda-forge` channel. This is an alternative installation method often used in Conda environments.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/install.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ conda install -c conda-forge cuda-python\n```\n\n----------------------------------------\n\nTITLE: CUstreamBatchMemOpType: CU_STREAM_MEM_OP_FLUSH_REMOTE_WRITES\nDESCRIPTION: This has the same effect as :py:obj:`~.CU_STREAM_WAIT_VALUE_FLUSH`, but as a standalone operation.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamBatchMemOpType.CU_STREAM_MEM_OP_FLUSH_REMOTE_WRITES\n```\n\n----------------------------------------\n\nTITLE: Defining CUmemorytype\nDESCRIPTION: Describes the type of memory being used, such as host memory, device memory, array memory, or unified memory. These types guide memory allocation and access patterns within the CUDA system.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_121\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmemorytype.CU_MEMORYTYPE_UNIFIED\n```\n\n----------------------------------------\n\nTITLE: Defining CUmemorytype\nDESCRIPTION: Describes the type of memory being used, such as host memory, device memory, array memory, or unified memory. These types guide memory allocation and access patterns within the CUDA system.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_119\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmemorytype.CU_MEMORYTYPE_DEVICE\n```\n\n----------------------------------------\n\nTITLE: CUstreamWaitValue_flags: CU_STREAM_WAIT_VALUE_GEQ\nDESCRIPTION: Defines the CU_STREAM_WAIT_VALUE_GEQ flag. This flag makes the stream wait until (int32_t)(*addr - value) >= 0 (or int64_t for 64 bit values), using a cyclic comparison that ignores wraparound.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamWaitValue_flags.CU_STREAM_WAIT_VALUE_GEQ\n```\n\n----------------------------------------\n\nTITLE: CUstreamWaitValue_flags: CU_STREAM_WAIT_VALUE_AND\nDESCRIPTION: Defines the CU_STREAM_WAIT_VALUE_AND flag. This flag makes the stream wait until (*addr & value) != 0.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamWaitValue_flags.CU_STREAM_WAIT_VALUE_AND\n```\n\n----------------------------------------\n\nTITLE: Defining CUjit_option\nDESCRIPTION: Defines options for the CUDA JIT compiler. These options control various aspects of compilation and linking, such as the maximum number of registers a thread may use,  the optimization level, and buffer sizes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_139\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUjit_option.CU_JIT_MAX_REGISTERS\n```\n\n----------------------------------------\n\nTITLE: CUstreamUpdateCaptureDependencies_flags: CU_STREAM_ADD_CAPTURE_DEPENDENCIES\nDESCRIPTION: Defines the CU_STREAM_ADD_CAPTURE_DEPENDENCIES flag to add new nodes to the dependency set.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamUpdateCaptureDependencies_flags.CU_STREAM_ADD_CAPTURE_DEPENDENCIES\n```\n\n----------------------------------------\n\nTITLE: CUstreamWriteValue_flags: CU_STREAM_WRITE_VALUE_NO_MEMORY_BARRIER\nDESCRIPTION: Defines the CU_STREAM_WRITE_VALUE_NO_MEMORY_BARRIER flag.  This flag permits the write to be reordered with writes issued before it, as a performance optimization. It's not supported in the v2 API.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamWriteValue_flags.CU_STREAM_WRITE_VALUE_NO_MEMORY_BARRIER\n```\n\n----------------------------------------\n\nTITLE: Defining CUjit_option\nDESCRIPTION: Defines options for the CUDA JIT compiler. These options control various aspects of compilation and linking, such as minimum number of threads per block,  the optimization level, and buffer sizes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_140\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUjit_option.CU_JIT_THREADS_PER_BLOCK\n```\n\n----------------------------------------\n\nTITLE: Defining CUjit_option\nDESCRIPTION: Defines options for the CUDA JIT compiler. These options control various aspects of compilation and linking, such as the optimization level.  The level ranges from 0-4 with 4 being the highest level of optimization.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_146\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUjit_option.CU_JIT_OPTIMIZATION_LEVEL\n```\n\n----------------------------------------\n\nTITLE: CUstreamBatchMemOpType: CU_STREAM_MEM_OP_WRITE_VALUE_32\nDESCRIPTION: Represents a :py:obj:`~.cuStreamWriteValue32` operation within CUstreamBatchMemOpType.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamBatchMemOpType.CU_STREAM_MEM_OP_WRITE_VALUE_32\n```\n\n----------------------------------------\n\nTITLE: CUstreamWaitValue_flags: CU_STREAM_WAIT_VALUE_NOR\nDESCRIPTION: Defines the CU_STREAM_WAIT_VALUE_NOR flag. This flag makes the stream wait until ~(*addr | value) != 0. Support for this operation can be queried with :py:obj:`~.cuDeviceGetAttribute()` and :py:obj:`~.CU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_WAIT_VALUE_NOR`.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamWaitValue_flags.CU_STREAM_WAIT_VALUE_NOR\n```\n\n----------------------------------------\n\nTITLE: CUstreamBatchMemOpType: CU_STREAM_MEM_OP_WRITE_VALUE_64\nDESCRIPTION: Represents a :py:obj:`~.cuStreamWriteValue64` operation within CUstreamBatchMemOpType.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamBatchMemOpType.CU_STREAM_MEM_OP_WRITE_VALUE_64\n```\n\n----------------------------------------\n\nTITLE: Defining CUjit_option\nDESCRIPTION: Defines options for the CUDA JIT compiler. These options control various aspects of compilation and linking, such as wall clock time spent,  the optimization level, and buffer sizes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_141\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUjit_option.CU_JIT_WALL_TIME\n```\n\n----------------------------------------\n\nTITLE: Defining CUjit_option\nDESCRIPTION: Defines options for the CUDA JIT compiler. These options control various aspects of compilation and linking, such as the target.  The target is chosen based on supplied `CUjit_target`.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_148\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUjit_option.CU_JIT_TARGET\n```\n\n----------------------------------------\n\nTITLE: CUoccupancy_flags: CU_OCCUPANCY_DISABLE_CACHING_OVERRIDE\nDESCRIPTION: Defines the CU_OCCUPANCY_DISABLE_CACHING_OVERRIDE flag, assuming global caching is enabled and cannot be automatically turned off.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUoccupancy_flags.CU_OCCUPANCY_DISABLE_CACHING_OVERRIDE\n```\n\n----------------------------------------\n\nTITLE: Defining CUjit_option\nDESCRIPTION: Defines options for the CUDA JIT compiler. These options control various aspects of compilation and linking, such as log buffer sizes,  the optimization level, and buffer sizes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_143\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUjit_option.CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_NUMA_ID\nDESCRIPTION: This attribute indicates the NUMA node ID of the GPU memory.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_97\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_NUMA_ID\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES\nDESCRIPTION: This attribute indicates whether the device accesses pageable memory through the host's page tables. This is important for memory management and address translation.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_65\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES\n```\n\n----------------------------------------\n\nTITLE: Defining CUjit_option\nDESCRIPTION: Defines options for the CUDA JIT compiler. These options control various aspects of compilation and linking, such as the buffer size of error logs,  the optimization level, and buffer sizes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_145\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUjit_option.CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES\n```\n\n----------------------------------------\n\nTITLE: Descriptive of launch completion event and device-updatable kernel node attributes\nDESCRIPTION: This snippet explains the behavior and usage of launch completion events in CUDA, including constraints on event visibility, timing, and interprocess restrictions. It also details the device-updatable kernel node attribute for graph nodes, indicating whether kernels can be updated on the device post-launch, with associated restrictions and procedures for updates. The attribute must be set before node creation and cannot be unset afterward, impacting graph management and kernel parameter updates.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_150\n\nLANGUAGE: Python\nCODE:\n```\n# No specific code snippets provided; informational documentation about event behavior and node attributes.\n```\n\n----------------------------------------\n\nTITLE: CUDA Library Option Flags for Loading\nDESCRIPTION: Specifies options for loading CUDA libraries, including 'HOST_UNIVERSAL_FUNCTION_AND_DATA_TABLE' and 'BINARY_IS_PRESERVED'. These options influence how the CUDA driver manages library loading, data preservation, and resource sharing, optimizing for memory usage or reusability.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_158\n\nLANGUAGE: Python\nCODE:\n```\n# No specific code snippets provided; options for CUDA library management.\n```\n\n----------------------------------------\n\nTITLE: Defining Graphics Interoperability Functions (CUDA Python/Sphinx)\nDESCRIPTION: Uses Sphinx `autofunction` directives to create documentation for CUDA Driver API graphics interoperability functions within the `cuda.bindings.driver` module, focusing on resource management (register/unregister, map/unmap) and retrieving mapped pointers or arrays.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_173\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autofunction:: cuda.bindings.driver.cuGraphicsUnregisterResource\n.. autofunction:: cuda.bindings.driver.cuGraphicsSubResourceGetMappedArray\n.. autofunction:: cuda.bindings.driver.cuGraphicsResourceGetMappedMipmappedArray\n.. autofunction:: cuda.bindings.driver.cuGraphicsResourceGetMappedPointer\n.. autofunction:: cuda.bindings.driver.cuGraphicsResourceSetMapFlags\n.. autofunction:: cuda.bindings.driver.cuGraphicsMapResources\n.. autofunction:: cuda.bindings.driver.cuGraphicsUnmapResources\n```\n\n----------------------------------------\n\nTITLE: CUmemcpy3DOperandType Flags Definition - CUDA Python\nDESCRIPTION: Defines flags indicating the operand type used in 3D memory copy operations. Options include a valid pointer or a CUarray. These flags specify the data structure used as source or destination in the 3D memory copy.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_168\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmemcpy3DOperandType.CU_MEMCPY_OPERAND_TYPE_POINTER\n\n\n    Memcpy operand is a valid pointer.\n\n\n.. autoattribute:: cuda.bindings.driver.CUmemcpy3DOperandType.CU_MEMCPY_OPERAND_TYPE_ARRAY\n\n\n    Memcpy operand is a CUarray.\n\n\n.. autoattribute:: cuda.bindings.driver.CUmemcpy3DOperandType.CU_MEMCPY_OPERAND_TYPE_MAX\n```\n\n----------------------------------------\n\nTITLE: CUeglFrameType Flags Definition - CUDA Python\nDESCRIPTION: Defines flags indicating the frame type for EGL (Embedded-System Graphics Library) interoperability. Options include `CU_EGL_FRAME_TYPE_ARRAY` for CUDA arrays and `CU_EGL_FRAME_TYPE_PITCH` for pitch pointers. These flags define the memory layout and access method for EGL frames used with CUDA.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_169\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUeglFrameType.CU_EGL_FRAME_TYPE_ARRAY\n\n\n    Frame type CUDA array\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglFrameType.CU_EGL_FRAME_TYPE_PITCH\n\n\n    Frame type pointer\n```\n\n----------------------------------------\n\nTITLE: Defining CUfunction_attribute\nDESCRIPTION: Defines a block scheduling policy for a function.  This policy, specified by `CUclusterSchedulingPolicy`, influences how blocks are scheduled. It uses  `CUfuncSetAttribute` and `CUKernelSetAttribute` functions for setting.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_106\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUfunction_attribute.CU_FUNC_ATTRIBUTE_CLUSTER_SCHEDULING_POLICY_PREFERENCE\n```\n\n----------------------------------------\n\nTITLE: Defining CUsharedconfig\nDESCRIPTION: Defines the configuration of shared memory. Options include setting the default bank size, the four-byte bank size, or eight-byte bank size.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_114\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUsharedconfig.CU_SHARED_MEM_CONFIG_EIGHT_BYTE_BANK_SIZE\n```\n\n----------------------------------------\n\nTITLE: Defining Driver Entry Point Access Function (CUDA Python/Sphinx)\nDESCRIPTION: Specifies the Sphinx `autofunction` directive to generate documentation for the `cuGetProcAddress` function from the `cuda.bindings.driver` module, used for accessing CUDA driver entry points.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_174\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autofunction:: cuda.bindings.driver.cuGetProcAddress\n```\n\n----------------------------------------\n\nTITLE: Defining EGL Interoperability Functions (CUDA Python/Sphinx)\nDESCRIPTION: Uses Sphinx `autofunction` directives to document EGL interoperability functions from the `cuda.bindings.driver` module. This includes functions for registering EGL images, managing EGL stream connections (connect/disconnect, acquire/release frames), presenting/returning frames, mapping EGL frames, and creating CUDA events from EGL sync objects.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_178\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autofunction:: cuda.bindings.driver.cuGraphicsEGLRegisterImage\n.. autofunction:: cuda.bindings.driver.cuEGLStreamConsumerConnect\n.. autofunction:: cuda.bindings.driver.cuEGLStreamConsumerConnectWithFlags\n.. autofunction:: cuda.bindings.driver.cuEGLStreamConsumerDisconnect\n.. autofunction:: cuda.bindings.driver.cuEGLStreamConsumerAcquireFrame\n.. autofunction:: cuda.bindings.driver.cuEGLStreamConsumerReleaseFrame\n.. autofunction:: cuda.bindings.driver.cuEGLStreamProducerConnect\n.. autofunction:: cuda.bindings.driver.cuEGLStreamProducerDisconnect\n.. autofunction:: cuda.bindings.driver.cuEGLStreamProducerPresentFrame\n.. autofunction:: cuda.bindings.driver.cuEGLStreamProducerReturnFrame\n.. autofunction:: cuda.bindings.driver.cuGraphicsResourceGetMappedEglFrame\n.. autofunction:: cuda.bindings.driver.cuEventCreateFromEGLSync\n```\n\n----------------------------------------\n\nTITLE: Defining Green Contexts API Elements (CUDA Python/Sphinx)\nDESCRIPTION: Uses Sphinx directives (`autoclass`, `autoattribute`, `autofunction`) to document the Green Contexts API from `cuda.bindings.driver`. This covers related classes (`CUdevSmResource_st`, `CUdevResource_st`, etc.), flags (`CUgreenCtxCreate_flags`, `CUdevSmResourceSplit_flags`), resource types (`CUdevResourceType`), and functions for creating/managing green contexts and their associated resources and streams.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_176\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: cuda.bindings.driver.CUdevSmResource_st\n.. autoclass:: cuda.bindings.driver.CUdevResource_st\n.. autoclass:: cuda.bindings.driver.CUdevSmResource\n.. autoclass:: cuda.bindings.driver.CUdevResource\n.. autoclass:: cuda.bindings.driver.CUgreenCtxCreate_flags\n\n    .. autoattribute:: cuda.bindings.driver.CUgreenCtxCreate_flags.CU_GREEN_CTX_DEFAULT_STREAM\n\n\n        Required. Creates a default stream to use inside the green context\n\n.. autoclass:: cuda.bindings.driver.CUdevSmResourceSplit_flags\n\n    .. autoattribute:: cuda.bindings.driver.CUdevSmResourceSplit_flags.CU_DEV_SM_RESOURCE_SPLIT_IGNORE_SM_COSCHEDULING\n\n\n    .. autoattribute:: cuda.bindings.driver.CUdevSmResourceSplit_flags.CU_DEV_SM_RESOURCE_SPLIT_MAX_POTENTIAL_CLUSTER_SIZE\n\n.. autoclass:: cuda.bindings.driver.CUdevResourceType\n\n    .. autoattribute:: cuda.bindings.driver.CUdevResourceType.CU_DEV_RESOURCE_TYPE_INVALID\n\n\n    .. autoattribute:: cuda.bindings.driver.CUdevResourceType.CU_DEV_RESOURCE_TYPE_SM\n\n\n        Streaming multiprocessors related information\n\n.. autoclass:: cuda.bindings.driver.CUdevResourceDesc\n.. autoclass:: cuda.bindings.driver.CUdevSmResource\n.. autofunction:: cuda.bindings.driver._CONCAT_OUTER\n.. autofunction:: cuda.bindings.driver.cuGreenCtxCreate\n.. autofunction:: cuda.bindings.driver.cuGreenCtxDestroy\n.. autofunction:: cuda.bindings.driver.cuCtxFromGreenCtx\n.. autofunction:: cuda.bindings.driver.cuDeviceGetDevResource\n.. autofunction:: cuda.bindings.driver.cuCtxGetDevResource\n.. autofunction:: cuda.bindings.driver.cuGreenCtxGetDevResource\n.. autofunction:: cuda.bindings.driver.cuDevSmResourceSplitByCount\n.. autofunction:: cuda.bindings.driver.cuDevResourceGenerateDesc\n.. autofunction:: cuda.bindings.driver.cuGreenCtxRecordEvent\n.. autofunction:: cuda.bindings.driver.cuGreenCtxWaitEvent\n.. autofunction:: cuda.bindings.driver.cuStreamGetGreenCtx\n.. autofunction:: cuda.bindings.driver.cuGreenCtxStreamCreate\n.. autoattribute:: cuda.bindings.driver.RESOURCE_ABI_VERSION\n.. autoattribute:: cuda.bindings.driver.RESOURCE_ABI_EXTERNAL_BYTES\n.. autoattribute:: cuda.bindings.driver._CONCAT_INNER\n.. autoattribute:: cuda.bindings.driver._CONCAT_OUTER\n```\n\n----------------------------------------\n\nTITLE: Setting Default CUDA Stream Environment Variable in Shell\nDESCRIPTION: This code snippet demonstrates how to set the environment variable to change the default CUDA stream to per-thread default stream (PTDS) before executing CUDA Python code. It requires a UNIX-like shell environment. When set to 1, PTDS is enabled; otherwise, the legacy default stream is used.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/release/11.6.0-notes.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_PYTHON_CUDA_PER_THREAD_DEFAULT_STREAM=1\n```\n\n----------------------------------------\n\nTITLE: External Resource Interoperability Functions for CUDA Runtime in C\nDESCRIPTION: Functions to import, destroy, and manage external memory and semaphore resources, allowing CUDA applications to interoperate with external systems like OpenGL, Vulkan, or DirectX. Key functions include cudaImportExternalMemory, cudaExternalMemoryGetMappedBuffer, cudaDestroyExternalMemory, cudaImportExternalSemaphore, and semaphore signaling/waiting functions.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_5\n\nLANGUAGE: C\nCODE:\n```\ncuda.bindings.runtime.cudaImportExternalMemory\ncuda.bindings.runtime.cudaExternalMemoryGetMappedBuffer\ncuda.bindings.runtime.cudaExternalMemoryGetMappedMipmappedArray\ncuda.bindings.runtime.cudaDestroyExternalMemory\ncuda.bindings.runtime.cudaImportExternalSemaphore\ncuda.bindings.runtime.cudaSignalExternalSemaphoresAsync\ncuda.bindings.runtime.cudaWaitExternalSemaphoresAsync\ncuda.bindings.runtime.cudaDestroyExternalSemaphore\n```\n\n----------------------------------------\n\nTITLE: Compiling CUDA Kernel String with NVRTC in Python\nDESCRIPTION: Initializes the CUDA Driver API, gets the compute capability of the default device (device 0), creates an NVRTC program from the SAXPY kernel string, compiles it into PTX targeting the device's architecture (with FMAD disabled using compiler options), and retrieves the generated PTX code as a byte string. It utilizes the `checkCudaErrors` helper for error handling.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Initialize CUDA Driver API\ncheckCudaErrors(driver.cuInit(0))\n\n# Retrieve handle for device 0\ncuDevice = checkCudaErrors(driver.cuDeviceGet(0))\n\n# Derive target architecture for device 0\nmajor = checkCudaErrors(driver.cuDeviceGetAttribute(driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, cuDevice))\nminor = checkCudaErrors(driver.cuDeviceGetAttribute(driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, cuDevice))\narch_arg = bytes(f'--gpu-architecture=compute_{major}{minor}', 'ascii')\n\n# Create program\nprog = checkCudaErrors(nvrtc.nvrtcCreateProgram(str.encode(saxpy), b\"saxpy.cu\", 0, [], []))\n\n# Compile program\nopts = [b\"--fmad=false\", arch_arg]\ncheckCudaErrors(nvrtc.nvrtcCompileProgram(prog, 2, opts))\n\n# Get PTX from compilation\nptxSize = checkCudaErrors(nvrtc.nvrtcGetPTXSize(prog))\nptx = b\" \" * ptxSize\ncheckCudaErrors(nvrtc.nvrtcGetPTX(prog, ptx))\n```\n\n----------------------------------------\n\nTITLE: Time Compilation Phases Option\nDESCRIPTION: This option generates a comma separated value table with the time taken by each compilation phase, and appends it at the end of the specified file.  Requires CUDA compilation.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_25\n\nLANGUAGE: C++\nCODE:\n```\n --time=<file-name> ( -time )\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA launch attributes for shared memory carveout and graph nodes\nDESCRIPTION: This segment describes how to set CUDA launch attributes, such as 'CU_LAUNCH_ATTRIBUTE_PREFERRED_SHARED_MEMORY_CARVEOUT', which hints at preferred shared memory distribution between shared memory and L1 cache for a kernel launch. It also notes that this setting is only a hint, with the driver capable of overriding it based on hardware capabilities. It applies to launch configurations on devices with shared memory and cache sharing resources.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_151\n\nLANGUAGE: Python\nCODE:\n```\n# No specific code snippets provided; describes attribute settings for shared memory optimization.\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_BC1_UNORM\nDESCRIPTION: Defines the CU_AD_FORMAT_BC1_UNORM flag for CUarray_format, representing 4 channel unsigned normalized block-compressed (BC1 compression) format.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_BC1_UNORM\n```\n\n----------------------------------------\n\nTITLE: Interoperability of CUDA Stream Query Function Handles in Python\nDESCRIPTION: This Python code shows how to call the CUDA stream query function with both raw and wrapped handle types, demonstrating API interoperability. Both calls should return a success status, confirming that handle wrappers can be used seamlessly in function arguments.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/release/11.6.0-notes.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> cudart.cudaStreamQuery(cudart.cudaStreamNonBlocking)\n(<cudaError_t.cudaSuccess: 0>,)\n>>> cudart.cudaStreamQuery(cudart.cudaStream_t(cudart.cudaStreamNonBlocking))\n(<cudaError_t.cudaSuccess: 0>,)\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_MEM_SYNC_DOMAIN_COUNT\nDESCRIPTION: This attribute indicates the number of memory domains the device supports.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_92\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_MEM_SYNC_DOMAIN_COUNT\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_BC4_UNORM\nDESCRIPTION: Defines the CU_AD_FORMAT_BC4_UNORM flag for CUarray_format, representing 1 channel unsigned normalized block-compressed (BC4 compression) format.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_BC4_UNORM\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_NV12\nDESCRIPTION: Defines the CU_AD_FORMAT_NV12 flag for CUarray_format, representing 8-bit YUV planar format, with 4:2:0 sampling.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_NV12\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_SIGNED_INT32\nDESCRIPTION: Defines the CU_AD_FORMAT_SIGNED_INT32 flag for CUarray_format, representing signed 32-bit integers.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_SIGNED_INT32\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit (shell)\nDESCRIPTION: Installs pre-commit to manage various tools for development and ensure code consistency.  Requires pip to be installed.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install pre-commit\n```\n\n----------------------------------------\n\nTITLE: Stream Management Functions for CUDA Runtime in C\nDESCRIPTION: Functions to create, destroy, synchronize, and query CUDA streams, as well as manage stream attributes and callbacks. These enable concurrent execution and efficient task scheduling on GPUs. Dependencies include CUDA runtime; key functions include cudaStreamCreate, cudaStreamDestroy, cudaStreamSynchronize, and callback-related functions.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_3\n\nLANGUAGE: C\nCODE:\n```\ncuda.bindings.runtime.cudaStreamCallback_t\ncuda.bindings.runtime.cudaStreamCreate\ncuda.bindings.runtime.cudaStreamCreateWithFlags\ncuda.bindings.runtime.cudaStreamCreateWithPriority\ncuda.bindings.runtime.cudaStreamGetPriority\ncuda.bindings.runtime.cudaStreamGetFlags\ncuda.bindings.runtime.cudaStreamGetId\ncuda.bindings.runtime.cudaStreamGetDevice\ncuda.bindings.runtime.cudaCtxResetPersistingL2Cache\ncuda.bindings.runtime.cudaStreamCopyAttributes\ncuda.bindings.runtime.cudaStreamGetAttribute\ncuda.bindings.runtime.cudaStreamSetAttribute\ncuda.bindings.runtime.cudaStreamDestroy\ncuda.bindings.runtime.cudaStreamWaitEvent\ncuda.bindings.runtime.cudaStreamAddCallback\ncuda.bindings.runtime.cudaStreamSynchronize\ncuda.bindings.runtime.cudaStreamQuery\ncuda.bindings.runtime.cudaStreamAttachMemAsync\ncuda.bindings.runtime.cudaStreamBeginCapture\ncuda.bindings.runtime.cudaStreamBeginCaptureToGraph\ncuda.bindings.runtime.cudaThreadExchangeStreamCaptureMode\ncuda.bindings.runtime.cudaStreamEndCapture\ncuda.bindings.runtime.cudaStreamIsCapturing\ncuda.bindings.runtime.cudaStreamGetCaptureInfo\ncuda.bindings.runtime.cudaStreamGetCaptureInfo_v3\ncuda.bindings.runtime.cudaStreamUpdateCaptureDependencies\ncuda.bindings.runtime.cudaStreamUpdateCaptureDependencies_v2\n```\n\n----------------------------------------\n\nTITLE: CUstreamMemoryBarrier_flags: CU_STREAM_MEMORY_BARRIER_TYPE_GPU\nDESCRIPTION: Defines the CU_STREAM_MEMORY_BARRIER_TYPE_GPU flag, limiting memory barrier scope to the GPU.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamMemoryBarrier_flags.CU_STREAM_MEMORY_BARRIER_TYPE_GPU\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_SIGNED_INT16\nDESCRIPTION: Defines the CU_AD_FORMAT_SIGNED_INT16 flag for CUarray_format, representing signed 16-bit integers.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_SIGNED_INT16\n```\n\n----------------------------------------\n\nTITLE: CUDA Array Formats\nDESCRIPTION: Defines and documents CUDA array formats.  These constants specify the data format for CUDA arrays, including block-compressed and planar formats for various color encodings.  This is used when creating CUDA arrays, allowing for efficient storage and manipulation of image and data.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_54\n\n\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_UNSIGNED_INT16\nDESCRIPTION: Defines the CU_AD_FORMAT_UNSIGNED_INT16 flag for CUarray_format, representing unsigned 16-bit integers.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_UNSIGNED_INT16\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_UNORM_INT16X1\nDESCRIPTION: Defines the CU_AD_FORMAT_UNORM_INT16X1 flag for CUarray_format, representing 1 channel unsigned 16-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_UNORM_INT16X1\n```\n\n----------------------------------------\n\nTITLE: Installing cuda-core from Conda\nDESCRIPTION: Installs the `cuda.core` package with CUDA 12 support using conda from the conda-forge channel. `cuda-version=12` specifies the CUDA version.  It also mentions a potential need to install `libnvjitlink` separately when using nvJitLink from conda-forge.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/docs/source/install.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ conda install -c conda-forge cuda-core cuda-version=12\n```\n\n----------------------------------------\n\nTITLE: Preparing Host Data for SAXPY in Python\nDESCRIPTION: Defines kernel launch parameters (`NUM_THREADS`, `NUM_BLOCKS`). Initializes host-side NumPy arrays for the scalar 'a', the total number of elements 'n', input vectors 'hX' and 'hY' (filled with random float32 values), and the output vector 'hOut' (initialized to zeros). Calculates the required buffer size.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nNUM_THREADS = 512  # Threads per block\nNUM_BLOCKS = 32768  # Blocks per grid\n\na = np.array([2.0], dtype=np.float32)\nn = np.array(NUM_THREADS * NUM_BLOCKS, dtype=np.uint32)\nbufferSize = n * a.itemsize\n\nhX = np.random.rand(n).astype(dtype=np.float32)\nhY = np.random.rand(n).astype(dtype=np.float32)\nhOut = np.zeros(n).astype(dtype=np.float32)\n```\n\n----------------------------------------\n\nTITLE: CUstreamMemoryBarrier_flags: CU_STREAM_MEMORY_BARRIER_TYPE_SYS\nDESCRIPTION: Defines the CU_STREAM_MEMORY_BARRIER_TYPE_SYS flag, representing a system-wide memory barrier.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamMemoryBarrier_flags.CU_STREAM_MEMORY_BARRIER_TYPE_SYS\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_UNORM_INT8X1\nDESCRIPTION: Defines the CU_AD_FORMAT_UNORM_INT8X1 flag for CUarray_format, representing 1 channel unsigned 8-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_UNORM_INT8X1\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_UNORM_INT8X2\nDESCRIPTION: Defines the CU_AD_FORMAT_UNORM_INT8X2 flag for CUarray_format, representing 2 channel unsigned 8-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_UNORM_INT8X2\n```\n\n----------------------------------------\n\nTITLE: Building Project Documentation with Shell Script\nDESCRIPTION: This shell command executes the script responsible for building the documentation for the specific installed version of the `cuda-core` package. It's a core step in the standard documentation build process. Prerequisites include installing the necessary `cuda-core` version and listing it in `versions.json`.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/docs/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./build_docs.sh\n```\n\n----------------------------------------\n\nTITLE: CUDA Device Attributes\nDESCRIPTION: Defines and documents CUDA device attributes. These constants are used to query various properties of a CUDA device. These attributes describe device capabilities and limitations like max threads per block, shared memory size, clock rate, and compute mode.  This information is valuable when developing CUDA kernels to optimize resource allocation and performance.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_57\n\n\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_RESERVED_SHARED_MEMORY_PER_BLOCK\nDESCRIPTION: This attribute specifies the amount of shared memory reserved by the CUDA driver per block, in bytes. This memory is used by the driver for internal purposes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_77\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_RESERVED_SHARED_MEMORY_PER_BLOCK\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_UNORM_INT16X4\nDESCRIPTION: Defines the CU_AD_FORMAT_UNORM_INT16X4 flag for CUarray_format, representing 4 channel unsigned 16-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_UNORM_INT16X4\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_BC5_UNORM\nDESCRIPTION: Defines the CU_AD_FORMAT_BC5_UNORM flag for CUarray_format, representing 2 channel unsigned normalized block-compressed (BC5 compression) format.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_BC5_UNORM\n```\n\n----------------------------------------\n\nTITLE: CUDA Stream Capture Modes Enums\nDESCRIPTION: Enumerates the capture modes for CUDA streams: 'GLOBAL', 'THREAD_LOCAL', and 'RELAXED'. These modes determine how stream capture is performed, affecting synchronization and scope during graph capture operations.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_153\n\nLANGUAGE: Python\nCODE:\n```\n# No specific code snippets provided; defines stream capture mode constants.\n```\n\n----------------------------------------\n\nTITLE: Pre-include Header Option\nDESCRIPTION: This option allows you to preinclude a specified header file during preprocessing. This option effectively includes the header at the beginning of the source file. The primary dependency is the CUDA compiler itself.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n --pre-include=<header> ( -include )\n```\n\n----------------------------------------\n\nTITLE: CUevent_wait_flags: CU_EVENT_WAIT_EXTERNAL\nDESCRIPTION: Defines the CU_EVENT_WAIT_EXTERNAL flag for use with CUevent_wait_flags. When using stream capture, this flag creates an event wait node instead of the default behavior. It is invalid outside of stream capture.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUevent_wait_flags.CU_EVENT_WAIT_EXTERNAL\n```\n\n----------------------------------------\n\nTITLE: Occupancy Calculation Functions for CUDA Runtime in C\nDESCRIPTION: Functions to compute optimal thread block sizes and active blocks per multiprocessor based on occupancy modeling, aiding in performance tuning. These include cudaOccupancyMaxActiveBlocksPerMultiprocessor and cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags, used for optimizing kernel launch configurations.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_7\n\nLANGUAGE: C\nCODE:\n```\ncuda.bindings.runtime.cudaOccupancyMaxActiveBlocksPerMultiprocessor\ncuda.bindings.runtime.cudaOccupancyAvailableDynamicSMemPerBlock\ncuda.bindings.runtime.cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags\n```\n\n----------------------------------------\n\nTITLE: Running Cython tests (python)\nDESCRIPTION: Executes pytest to run Cython unit tests against editable installations or installed packages. Requires pytest to be installed and the Cython tests to be built first.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython -m pytest tests/cython/\n```\n\nLANGUAGE: python\nCODE:\n```\npytest tests/cython/\n```\n\n----------------------------------------\n\nTITLE: Device Time Trace Profiler Option\nDESCRIPTION: This option enables the time profiler, outputting a JSON file. Results can be analyzed on chrome://tracing for a flamegraph visualization. Requires CUDA compilation.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_30\n\nLANGUAGE: C++\nCODE:\n```\n --fdevice-time-trace=<file-name> ( -fdevice-time-trace=<file-name> )\n```\n\n----------------------------------------\n\nTITLE: CUstreamBatchMemOpType: CU_STREAM_MEM_OP_WAIT_VALUE_64\nDESCRIPTION: Represents a :py:obj:`~.cuStreamWaitValue64` operation within CUstreamBatchMemOpType.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamBatchMemOpType.CU_STREAM_MEM_OP_WAIT_VALUE_64\n```\n\n----------------------------------------\n\nTITLE: Running pre-commit Hooks for Code Linting\nDESCRIPTION: Executes all configured pre-commit hooks against staged files (`-a`) to enforce code formatting and other requirements specified in `pyproject.toml`. The `--show-diff-on-failure` flag displays the changes required if any checks fail. This command should be run before committing code.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npre-commit run -a --show-diff-on-failure\n```\n\n----------------------------------------\n\nTITLE: CUevent_wait_flags: CU_EVENT_WAIT_DEFAULT\nDESCRIPTION: Defines the CU_EVENT_WAIT_DEFAULT flag for use with CUevent_wait_flags, representing the default event wait behavior.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUevent_wait_flags.CU_EVENT_WAIT_DEFAULT\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_advise Options\nDESCRIPTION: Provides advice to the CUDA runtime about memory access patterns.  These options, such as `CU_MEM_ADVISE_SET_READ_MOSTLY`, provide hints to optimize memory usage. These advise calls can improve memory management.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_125\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_advise.CU_MEM_ADVISE_SET_READ_MOSTLY\n```\n\n----------------------------------------\n\nTITLE: CUDA Stream Capture Status Enums\nDESCRIPTION: Defines enumeration constants representing the capture status of CUDA streams, including 'NONE' (not capturing), 'ACTIVE' (currently capturing), and 'INVALIDATED' (capture sequence invalidated but not terminated). These statuses help manage and debug stream capture processes in CUDA graphs.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_152\n\nLANGUAGE: Python\nCODE:\n```\n# No specific code snippets provided; defines stream capture status constants.\n```\n\n----------------------------------------\n\nTITLE: Builtin Initializer List Option\nDESCRIPTION: This option provides builtin definitions of `std::initializer_list` class and member functions when the C++11 or later language dialect is selected, improving C++11 compatibility. Requires the CUDA compiler and C++11 or later.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n --builtin-initializer-list={true|false} ( -builtin-initializer-list )\n```\n\n----------------------------------------\n\nTITLE: Builtin Move/Forward Option\nDESCRIPTION: This option provides builtin definitions of `std::move` and `std::forward` when the C++11 or later language dialect is selected. This is beneficial for compatibility. Requires a C++11 or later standard and the CUDA compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n --builtin-move-forward={true|false} ( -builtin-move-forward )\n```\n\n----------------------------------------\n\nTITLE: CUstreamBatchMemOpType: CU_STREAM_MEM_OP_BARRIER\nDESCRIPTION: Represents a memory barrier operation of the specified type within CUstreamBatchMemOpType.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamBatchMemOpType.CU_STREAM_MEM_OP_BARRIER\n```\n\n----------------------------------------\n\nTITLE: CUDA Surface Format Mode Class\nDESCRIPTION: Specifies whether surface data uses a forced or auto-detected format mode, affecting how data types and formats are handled during surface operations.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_62\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaSurfaceFormatMode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaSurfaceFormatMode.cudaFormatModeForced\n\n        Forced format mode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaSurfaceFormatMode.cudaFormatModeAuto\n\n        Auto format mode\n```\n\n----------------------------------------\n\nTITLE: cudaErrorStubLibrary Definition\nDESCRIPTION: This describes the `cudaErrorStubLibrary` error, which indicates that the application has loaded a CUDA stub library instead of a real driver. Applications using the stub library will experience this error during API calls. This is often due to installation issues.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorStubLibrary\n```\n\n----------------------------------------\n\nTITLE: Interoperability of Structure Member Handles in Python CUDA APIs\nDESCRIPTION: This Python snippet illustrates how to assign and retrieve values in a CUDA stream memory operation parameter structure, demonstrating interoperability between Python class wrappers and underlying primitive values. It shows setting the value with an integer and a cuuint64_t object and accessing the value to verify correct assignment.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/release/11.6.0-notes.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> waitParams = cuda.CUstreamMemOpWaitValueParams_st()\n>>> waitParams.value64 = 1\n>>> waitParams.value64\n<cuuint64_t 1>\n>>> waitParams.value64 = cuda.cuuint64_t(2)\n>>> waitParams.value64\n<cuuint64_t 2>\n```\n\n----------------------------------------\n\nTITLE: Describing cudaGraphDebugDotFlags Attributes\nDESCRIPTION: This section describes the `cudaGraphDebugDotFlags` enumeration which are attributes within the `cuda.bindings.runtime` module. It specifies flags related to the output of debug information in a graph.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_53\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaGraphDebugDotFlags.cudaGraphDebugDotFlagsExtSemasWaitNodeParams\n\n\n        Adds :py:obj:`~.cudaExternalSemaphoreWaitNodeParams` to output\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaGraphDebugDotFlags.cudaGraphDebugDotFlagsKernelNodeAttributes\n\n\n        Adds cudaKernelNodeAttrID values to output\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaGraphDebugDotFlags.cudaGraphDebugDotFlagsHandles\n\n\n        Adds node handles and every kernel function handle to output\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaGraphDebugDotFlags.cudaGraphDebugDotFlagsConditionalNodeParams\n\n\n        Adds :py:obj:`~.cudaConditionalNodeParams` to output\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_advise Options\nDESCRIPTION: Provides advice to the CUDA runtime about memory access patterns.  These options, such as `CU_MEM_ADVISE_SET_ACCESSED_BY`, provide hints to optimize memory usage. These advise calls can improve memory management.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_129\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_advise.CU_MEM_ADVISE_SET_ACCESSED_BY\n```\n\n----------------------------------------\n\nTITLE: Split Compile Threads Option\nDESCRIPTION: This option performs compiler optimizations in parallel, using multiple threads.  It accepts a number specifying the maximum threads or utilizes maximum available threads with `--split-compile=0`.  Requires the CUDA compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_26\n\nLANGUAGE: C++\nCODE:\n```\n --split-compile=<number-of-threads> ( -split-compile=<number-of-threads> )\n```\n\n----------------------------------------\n\nTITLE: cudaErrorInvalidFilterSetting Definition\nDESCRIPTION: This snippet documents the `cudaErrorInvalidFilterSetting` error. It indicates that a non-float texture was being accessed with linear filtering, an operation not supported by CUDA. This is a common mistake involving texture usage and filtering options.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorInvalidFilterSetting\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_advise Options\nDESCRIPTION: Provides advice to the CUDA runtime about memory access patterns.  These options, such as `CU_MEM_ADVISE_UNSET_PREFERRED_LOCATION`, provide hints to optimize memory usage. These advise calls can improve memory management.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_128\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_advise.CU_MEM_ADVISE_UNSET_PREFERRED_LOCATION\n```\n\n----------------------------------------\n\nTITLE: CUuserObjectRetain Flags Definition - CUDA Python\nDESCRIPTION: Defines flags related to retaining CUDA user objects, where references are transferred instead of creating new ones. The defined flag, `CU_GRAPH_USER_OBJECT_MOVE`, facilitates the efficient transfer of references within CUDA graph operations.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_162\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUuserObjectRetain_flags.CU_GRAPH_USER_OBJECT_MOVE\n\n\n    Transfer references from the caller rather than creating new references.\n```\n\n----------------------------------------\n\nTITLE: Defining CUjit_option\nDESCRIPTION: Defines options for the CUDA JIT compiler. These options control various aspects of compilation and linking, such as error log messages,  the optimization level, and buffer sizes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_144\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUjit_option.CU_JIT_ERROR_LOG_BUFFER\n```\n\n----------------------------------------\n\nTITLE: cudaErrorInvalidNormSetting Definition\nDESCRIPTION: This code documents the `cudaErrorInvalidNormSetting` error in the CUDA bindings. It signals an attempt to read an unsupported data type as a normalized float, an operation not supported by CUDA. This points to a data type incompatibility issue.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorInvalidNormSetting\n```\n\n----------------------------------------\n\nTITLE: CUDA Surface Boundary Mode Class\nDESCRIPTION: Enumerates boundary handling modes for CUDA surface operations, including zero, clamp, and trap modes, which determine how out-of-bounds surface coordinates are handled.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_61\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaSurfaceBoundaryMode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaSurfaceBoundaryMode.cudaBoundaryModeZero\n\n        Zero boundary mode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaSurfaceBoundaryMode.cudaBoundaryModeClamp\n\n        Clamp boundary mode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaSurfaceBoundaryMode.cudaBoundaryModeTrap\n\n        Trap boundary mode\n```\n\n----------------------------------------\n\nTITLE: Execution Control Functions for CUDA Runtime in C\nDESCRIPTION: Functions to set cache configurations, get function attributes, and launch host functions, controlling kernel execution behavior and performance. Dependencies include CUDA runtime; these functions are essential for optimizing performance and managing kernel launches in CUDA applications.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_6\n\nLANGUAGE: C\nCODE:\n```\ncuda.bindings.runtime.cudaFuncSetCacheConfig\ncuda.bindings.runtime.cudaFuncGetAttributes\ncuda.bindings.runtime.cudaFuncSetAttribute\ncuda.bindings.runtime.cudaLaunchHostFunc\n```\n\n----------------------------------------\n\nTITLE: Defining CUjit_option\nDESCRIPTION: Defines options for the CUDA JIT compiler. These options control various aspects of compilation and linking, such as info log messages,  the optimization level, and buffer sizes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_142\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUjit_option.CU_JIT_INFO_LOG_BUFFER\n```\n\n----------------------------------------\n\nTITLE: Defining CUfunc_cache Preferences\nDESCRIPTION: Describes various preferences for shared memory and L1 cache configurations.  These preferences are used to tune memory performance. Each attribute, such as `CU_FUNC_CACHE_PREFER_EQUAL`, indicates a specific memory configuration preference.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_111\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUfunc_cache.CU_FUNC_CACHE_PREFER_EQUAL\n```\n\n----------------------------------------\n\nTITLE: cudaErrorNotYetImplemented Definition\nDESCRIPTION: This documents the `cudaErrorNotYetImplemented` error. It indicates that the API call is not yet implemented. Note that production releases of CUDA should never return this error. This typically arises during development or beta testing.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorNotYetImplemented\n```\n\n----------------------------------------\n\nTITLE: cudaErrorMissingConfiguration Definition\nDESCRIPTION: This documents the `cudaErrorMissingConfiguration` error. It means that the device function being invoked (usually through cudaLaunchKernel()) was not previously configured via the cudaConfigureCall() function. This highlights a setup and configuration issue.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorMissingConfiguration\n```\n\n----------------------------------------\n\nTITLE: CUDA Texture Filter Mode Class\nDESCRIPTION: Specifies filtering modes for CUDA textures, such as point filtering and linear filtering, which influence how texel data is interpolated or sampled.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_64\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaTextureFilterMode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaTextureFilterMode.cudaFilterModePoint\n\n        Point filter mode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaTextureFilterMode.cudaFilterModeLinear\n\n        Linear filter mode\n```\n\n----------------------------------------\n\nTITLE: CUDA Result Codes Enumeration\nDESCRIPTION: Enumerates possible return status codes from CUDA driver API functions, such as 'CUDA_SUCCESS', 'INVALID_VALUE', 'OUT_OF_MEMORY', and various device-specific errors. These constants enable robust error handling and diagnostics in CUDA applications.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_159\n\nLANGUAGE: Python\nCODE:\n```\n# No specific code snippets provided; defines possible CUDA API result statuses.\n```\n\n----------------------------------------\n\nTITLE: cudaErrorSyncDepthExceeded Definition\nDESCRIPTION: This explains `cudaErrorSyncDepthExceeded`. This error indicates that a call to cudaDeviceSynchronize made from the device runtime failed because the call was made at grid depth greater than either the default or user-specified device limit. The description advises how to adjust the configuration to get around this.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorSyncDepthExceeded\n```\n\n----------------------------------------\n\nTITLE: cudaErrorSoftwareValidityNotEstablished Definition\nDESCRIPTION: This defines the `cudaErrorSoftwareValidityNotEstablished`. By default, the CUDA runtime may perform self and driver tests. This error indicates at least one of these tests has failed, meaning the validity of either the runtime or the driver could not be established. It was introduced in CUDA 11.2.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorSoftwareValidityNotEstablished\n```\n\n----------------------------------------\n\nTITLE: Defining CUshared_carveout\nDESCRIPTION: Defines preferences for shared memory carveout.  These settings control the balance between shared memory and L1 cache. Examples include preferring maximum shared memory or maximum L1 cache.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_115\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUshared_carveout.CU_SHAREDMEM_CARVEOUT_DEFAULT\n```\n\n----------------------------------------\n\nTITLE: CUDA Channel Format Kind Enum Documentation\nDESCRIPTION: This class enumerates the various CUDA channel formats used in data representations, including signed, unsigned, float, and compressed formats, as well as specific formats like NV12 and normalized variants. It facilitates specifying data formats for memory and image operations.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaChannelFormatKind\n\n    .. autoattribute:: cuda.bindings.runtime.cudaChannelFormatKind.cudaChannelFormatKindSigned\n\n        Signed channel format\n\n    .. autoattribute:: cuda.bindings.runtime.cudaChannelFormatKind.cudaChannelFormatKindUnsigned\n\n        Unsigned channel format\n\n    .. autoattribute:: cuda.bindings.runtime.cudaChannelFormatKind.cudaChannelFormatKindFloat\n\n        Float channel format\n\n    .. autoattribute:: cuda.bindings.runtime.cudaChannelFormatKind.cudaChannelFormatKindNone\n\n        No channel format\n\n    .. autoattribute:: cuda.bindings.runtime.cudaChannelFormatKind.cudaChannelFormatKindNV12\n\n        Unsigned 8-bit integers, planar 4:2:0 YUV format\n\n    ... [additional formats continue similarly]\n\n```\n\n----------------------------------------\n\nTITLE: cudaErrorLaunchMaxDepthExceeded Definition\nDESCRIPTION: This defines `cudaErrorLaunchMaxDepthExceeded` which indicates that a device runtime grid launch did not occur because the depth of the child grid would exceed the maximum supported number of nested grid launches. It points toward recursion depth limitations in device runtime launches.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorLaunchMaxDepthExceeded\n```\n\n----------------------------------------\n\nTITLE: cudaErrorDuplicateSurfaceName Definition\nDESCRIPTION: This documents the `cudaErrorDuplicateSurfaceName` error, which occurs when multiple surfaces (across separate CUDA source files in the application) share the same string name. This is similar to the previous ones, reflecting name collisions during compilation.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorDuplicateSurfaceName\n```\n\n----------------------------------------\n\nTITLE: Device-Updatable Kernel Node Attribute Description\nDESCRIPTION: Explains the 'deviceUpdatableKernelNode' attribute, indicating whether a kernel node can be updated on the device during execution. It includes constraints such as restrictions on node removal, attribute copying, and the necessity to upload graphs containing device-updatable nodes before launching. This attribute is crucial for dynamic kernel tuning or modifications on the device side.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_57\n\nLANGUAGE: Python\nCODE:\n```\n:cudaLaunchAttributeValue::deviceUpdatableKernelNode::deviceUpdatable can only be set to 0 or 1. Setting the field to 1 indicates that the corresponding kernel node should be device-updatable. On success, a handle will be returned via :py:obj:`~.cudaLaunchAttributeValue`::deviceUpdatableKernelNode::devNode which can be passed to the various device-side update functions to update the node's kernel parameters from within another kernel. For more information on the types of device updates that can be made, as well as the relevant limitations thereof, see :py:obj:`~.cudaGraphKernelNodeUpdatesApply`. \n\n Nodes which are device-updatable have additional restrictions compared to regular kernel nodes. Firstly, device-updatable nodes cannot be removed from their graph via :py:obj:`~.cudaGraphDestroyNode`. Additionally, once opted-in to this functionality, a node cannot opt out, and any attempt to set the deviceUpdatable attribute to 0 will result in an error. Device-updatable kernel nodes also cannot have their attributes copied to/from another kernel node via :py:obj:`~.cudaGraphKernelNodeCopyAttributes`. Graphs containing one or more device-updatable nodes also do not allow multiple instantiation, and neither the graph nor its instantiated version can be passed to :py:obj:`~.cudaGraphExecUpdate`. \n\n If a graph contains device-updatable nodes and updates those nodes from the device from within the graph, the graph must be uploaded with :py:obj:`~.cuGraphUpload` before it is launched. For such a graph, if host-side executable graph updates are made to the device-updatable nodes, the graph must be uploaded before it is launched again.\n```\n\n----------------------------------------\n\nTITLE: Defining CUjit_option\nDESCRIPTION: Defines options for the CUDA JIT compiler. These options control various aspects of compilation and linking, such as the compilation target.  If specified, it uses the current attached context.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_147\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUjit_option.CU_JIT_TARGET_FROM_CUCONTEXT\n```\n\n----------------------------------------\n\nTITLE: cudaErrorDuplicateVariableName Definition\nDESCRIPTION: This documents the `cudaErrorDuplicateVariableName` error, which occurs when multiple global or constant variables in separate CUDA source files share the same string name.  This is a compilation issue related to variable name collisions.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorDuplicateVariableName\n```\n\n----------------------------------------\n\nTITLE: Describing cudaLaunchMemSyncDomain Attributes\nDESCRIPTION: This section details the `cudaLaunchMemSyncDomain` enumeration, which determines the memory synchronization domain for kernel launches. It specifies the domain for launching CUDA kernels.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_55\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaLaunchMemSyncDomain\n\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchMemSyncDomain.cudaLaunchMemSyncDomainDefault\n\n\n        Launch kernels in the default domain\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchMemSyncDomain.cudaLaunchMemSyncDomainRemote\n\n\n        Launch kernels in the remote domain\n```\n\n----------------------------------------\n\nTITLE: CUDA Asynchronous Notification Type Class\nDESCRIPTION: Defines different notification types for CUDA asynchronous operations, allowing applications to specify notification behaviors based on budget or other criteria.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_60\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaAsyncNotificationType\n\n    .. autoattribute:: cuda.bindings.runtime.cudaAsyncNotificationType.cudaAsyncNotificationTypeOverBudget\n```\n\n----------------------------------------\n\nTITLE: Creating a CUDA Context in Python\nDESCRIPTION: Creates a CUDA context associated with the default device (device 0, whose handle `cuDevice` was obtained earlier) using the `cuCtxCreate` function from the Driver API. A context is required before performing operations like module loading or kernel launches on the GPU.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Create context\ncontext = checkCudaErrors(driver.cuCtxCreate(0, cuDevice))\n```\n\n----------------------------------------\n\nTITLE: cudaErrorDuplicateTextureName Definition\nDESCRIPTION: This explains the `cudaErrorDuplicateTextureName` error. It happens when multiple textures across separate CUDA source files share the same string name.  This is a compilation issue regarding texture name collisions.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorDuplicateTextureName\n```\n\n----------------------------------------\n\nTITLE: cudaErrorPriorLaunchFailure Definition\nDESCRIPTION: This snippet defines the `cudaErrorPriorLaunchFailure` error (now deprecated), indicating that a previous kernel launch failed. It was formerly used for device emulation of kernel launches and signals issues during the launch process.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorPriorLaunchFailure\n```\n\n----------------------------------------\n\nTITLE: CUDA Cluster Scheduling Policy Details\nDESCRIPTION: This class enumerates scheduling policies for CUDA clusters, including the default policy, to control task distribution and resource allocation within CUDA cluster environments.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaClusterSchedulingPolicy\n\n    .. autoattribute:: cuda.bindings.runtime.cudaClusterSchedulingPolicy.cudaClusterSchedulingPolicyDefault\n\n        the default policy\n\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_FLOAT\nDESCRIPTION: Defines the CU_AD_FORMAT_FLOAT flag for CUarray_format, representing 32-bit floating point.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_FLOAT\n```\n\n----------------------------------------\n\nTITLE: cudaErrorMixedDeviceExecution Definition\nDESCRIPTION: This snippet details the `cudaErrorMixedDeviceExecution` error, which is now deprecated. It indicates that mixing device and device emulation code was not allowed. This error related to compilation and linking, and indicates a configuration problem.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorMixedDeviceExecution\n```\n\n----------------------------------------\n\nTITLE: CUmemcpySrcAccessOrder Flags Definition - CUDA Python\nDESCRIPTION: Defines flags specifying access order requirements for the source pointer in memory copy operations.  These flags allow the driver to optimize memory copies based on whether access must be in stream order, can be out of stream order, or can occur even after the API call. Crucial for managing synchronization and memory lifetime.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_167\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmemcpySrcAccessOrder.CU_MEMCPY_SRC_ACCESS_ORDER_INVALID\n\n\n    Default invalid.\n\n\n.. autoattribute:: cuda.bindings.driver.CUmemcpySrcAccessOrder.CU_MEMCPY_SRC_ACCESS_ORDER_STREAM\n\n\n    Indicates that access to the source pointer must be in stream order.\n\n\n.. autoattribute:: cuda.bindings.driver.CUmemcpySrcAccessOrder.CU_MEMCPY_SRC_ACCESS_ORDER_DURING_API_CALL\n\n\n    Indicates that access to the source pointer can be out of stream order and all accesses must be complete before the API call returns. This flag is suited for ephemeral sources (ex., stack variables) when it's known that no prior operations in the stream can be accessing the memory and also that the lifetime of the memory is limited to the scope that the source variable was declared in. Specifying this flag allows the driver to optimize the copy and removes the need for the user to synchronize the stream after the API call.\n\n\n.. autoattribute:: cuda.bindings.driver.CUmemcpySrcAccessOrder.CU_MEMCPY_SRC_ACCESS_ORDER_ANY\n\n\n    Indicates that access to the source pointer can be out of stream order and the accesses can happen even after the API call returns. This flag is suited for host pointers allocated outside CUDA (ex., via malloc) when it's known that no prior operations in the stream can be accessing the memory. Specifying this flag allows the driver to optimize the copy on certain platforms.\n\n\n.. autoattribute:: cuda.bindings.driver.CUmemcpySrcAccessOrder.CU_MEMCPY_SRC_ACCESS_ORDER_MAX\n```\n\n----------------------------------------\n\nTITLE: CUDA Address Modes\nDESCRIPTION: Defines and documents CUDA address modes.  These constants specify how to handle out-of-bounds texture accesses.  Address modes control how the texture sampler behaves when the texture coordinates are outside the normal texture boundaries.  The various modes are wrap, clamp, mirror, and border.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_55\n\n\n\n----------------------------------------\n\nTITLE: cudaErrorUnmapBufferObjectFailed Definition\nDESCRIPTION: This explains the `cudaErrorUnmapBufferObjectFailed` error, which indicates that the buffer object could not be unmapped. This refers to the inverse operation as the previous error and relates to deallocation of memory.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorUnmapBufferObjectFailed\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_UNIFIED_FUNCTION_POINTERS\nDESCRIPTION: This attribute indicates if the device supports unified function pointers.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_95\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_UNIFIED_FUNCTION_POINTERS\n```\n\n----------------------------------------\n\nTITLE: CUDA Function Address Query Results\nDESCRIPTION: Constants indicating the outcome of querying driver function addresses, including success ('SUCCESS'), symbol not found ('SYMBOL_NOT_FOUND'), and insufficient version ('VERSION_NOT_SUFFICIENT'). These facilitate handling of dynamic symbol resolution in CUDA applications.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_155\n\nLANGUAGE: Python\nCODE:\n```\n# No specific code snippets provided; results for driver function address queries.\n```\n\n----------------------------------------\n\nTITLE: CUstreamWaitValue_flags: CU_STREAM_WAIT_VALUE_FLUSH\nDESCRIPTION: Defines the CU_STREAM_WAIT_VALUE_FLUSH flag. Follow the wait operation with a flush of outstanding remote writes. This ensures remote writes are visible to downstream device work. Support is restricted to selected platforms and can be queried with :py:obj:`~.CU_DEVICE_ATTRIBUTE_CAN_FLUSH_REMOTE_WRITES`.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamWaitValue_flags.CU_STREAM_WAIT_VALUE_FLUSH\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_SPARSE_CUDA_ARRAY_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports sparse CUDA arrays and sparse CUDA mipmapped arrays. Sparse arrays can be more memory efficient for certain types of data.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_78\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_SPARSE_CUDA_ARRAY_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_MEM_DECOMPRESS_ALGORITHM_MASK\nDESCRIPTION: This attribute indicates the memory decompression algorithms supported by the device. The returned value shall be interpreted as a bitmask, where the individual bits are described by the `CUmemDecompressAlgorithm` enum.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_102\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_MEM_DECOMPRESS_ALGORITHM_MASK\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_advise Options\nDESCRIPTION: Provides advice to the CUDA runtime about memory access patterns.  These options, such as `CU_MEM_ADVISE_UNSET_ACCESSED_BY`, provide hints to optimize memory usage. These advise calls can improve memory management.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_130\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_advise.CU_MEM_ADVISE_UNSET_ACCESSED_BY\n```\n\n----------------------------------------\n\nTITLE: Describing cudaGraphInstantiateFlags Attributes\nDESCRIPTION: This section describes the `cudaGraphInstantiateFlags` enumeration used during graph instantiation. It specifies various flags affecting memory management and graph behavior.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_54\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaGraphInstantiateFlags\n\n    .. autoattribute:: cuda.bindings.runtime.cudaGraphInstantiateFlags.cudaGraphInstantiateFlagAutoFreeOnLaunch\n\n\n        Automatically free memory allocated in a graph before relaunching.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaGraphInstantiateFlags.cudaGraphInstantiateFlagUpload\n\n\n        Automatically upload the graph after instantiation. Only supported by \n\n         :py:obj:`~.cudaGraphInstantiateWithParams`. The upload will be performed using the \n\n         stream provided in `instantiateParams`.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaGraphInstantiateFlags.cudaGraphInstantiateFlagDeviceLaunch\n\n\n        Instantiate the graph to be launchable from the device. This flag can only \n\n         be used on platforms which support unified addressing. This flag cannot be \n\n         used in conjunction with cudaGraphInstantiateFlagAutoFreeOnLaunch.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaGraphInstantiateFlags.cudaGraphInstantiateFlagUseNodePriority\n\n\n        Run the graph using the per-node priority attributes rather than the priority of the stream it is launched into.\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_DMA_BUF_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports buffer sharing with the dma_buf mechanism. Dma_buf is a Linux kernel mechanism for sharing buffers between devices.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_90\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_DMA_BUF_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Installing cuda-python from source (shell)\nDESCRIPTION: This snippet demonstrates how to install the cuda-python package from source. It involves cloning the repository, navigating to the `cuda_core` directory, and using `pip` to install the package. Requires git and pip to be installed.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ git clone https://github.com/NVIDIA/cuda-python\n$ cd cuda-python/cuda_core  # move to the directory where this README locates\n$ pip install .\n```\n\n----------------------------------------\n\nTITLE: CUDA Execution Affinity Types\nDESCRIPTION: Specifies types of execution affinity in CUDA such as 'SM_COUNT' for limiting streaming multiprocessors or 'MAX' for maximum affinity. These control how compute resources are allocated in contexts requiring specific hardware configurations.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_156\n\nLANGUAGE: Python\nCODE:\n```\n# No specific code snippets provided; defines execution affinity types.\n```\n\n----------------------------------------\n\nTITLE: CUstreamUpdateCaptureDependencies_flags: CU_STREAM_SET_CAPTURE_DEPENDENCIES\nDESCRIPTION: Defines the CU_STREAM_SET_CAPTURE_DEPENDENCIES flag to replace the dependency set with the new nodes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamUpdateCaptureDependencies_flags.CU_STREAM_SET_CAPTURE_DEPENDENCIES\n```\n\n----------------------------------------\n\nTITLE: Installing libnvjitlink from Conda\nDESCRIPTION: Installs the `libnvjitlink` package using conda from the conda-forge channel. This is required to use `cuda.core` with nvJitLink installed from conda-forge in certain versions.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/docs/source/install.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ conda install -c conda-forge libnvjitlink\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_SNORM_INT16X2\nDESCRIPTION: Defines the CU_AD_FORMAT_SNORM_INT16X2 flag for CUarray_format, representing 2 channel signed 16-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_SNORM_INT16X2\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_GPU_PCI_DEVICE_ID\nDESCRIPTION: This attribute represents the combined 16-bit PCI device ID and 16-bit PCI vendor ID of the GPU.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_104\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_GPU_PCI_DEVICE_ID\n```\n\n----------------------------------------\n\nTITLE: CUDA Stream Capture Status Enum Details\nDESCRIPTION: This class provides statuses for CUDA stream capture processes, indicating whether a stream is capturing, active, invalidated, or not capturing, useful for debugging and controlling CUDA stream capture sequences.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaStreamCaptureStatus\n\n    .. autoattribute:: cuda.bindings.runtime.cudaStreamCaptureStatus.cudaStreamCaptureStatusNone\n\n        Stream is not capturing\n\n    .. autoattribute:: cuda.bindings.runtime.cudaStreamCaptureStatus.cudaStreamCaptureStatusActive\n\n        Stream is actively capturing\n\n    .. autoattribute:: cuda.bindings.runtime.cudaStreamCaptureStatus.cudaStreamCaptureStatusInvalidated\n\n        Stream is part of a capture sequence that has been invalidated, but not terminated\n\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_KMT_HANDLE_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports exporting memory to a Win32 KMT handle with `cuMemExportToShareableHandle`, if requested via `cuMemCreate`. KMT handles are kernel-mode handles used on Windows systems.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_71\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_KMT_HANDLE_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Installing cuda-core from Source\nDESCRIPTION: Clones the `cuda-python` repository, navigates to the `cuda_core` directory, and installs the package using pip. Requires `cuda-python` or `cuda-bindings` 11.x or 12.x as a dependency.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/docs/source/install.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ git clone https://github.com/NVIDIA/cuda-python\n$ cd cuda-python/cuda_core\n$ pip install .\n```\n\n----------------------------------------\n\nTITLE: CUprocessState Flags Definition - CUDA Python\nDESCRIPTION: Defines the possible states of a CUDA process, including running, locked (due to CUDA API locks), checkpointed (memory contents saved), and failed (uncorrectable error during checkpoint/restore). These flags are essential for managing and monitoring CUDA process lifecycle.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_165\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUprocessState.CU_PROCESS_STATE_RUNNING\n\n\n    Default process state\n\n\n.. autoattribute:: cuda.bindings.driver.CUprocessState.CU_PROCESS_STATE_LOCKED\n\n\n    CUDA API locks are taken so further CUDA API calls will block\n\n\n.. autoattribute:: cuda.bindings.driver.CUprocessState.CU_PROCESS_STATE_CHECKPOINTED\n\n\n    Application memory contents have been checkpointed and underlying allocations and device handles have been released\n\n\n.. autoattribute:: cuda.bindings.driver.CUprocessState.CU_PROCESS_STATE_FAILED\n\n\n    Application entered an uncorrectable error during the checkpoint/restore process\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports GPUDirect RDMA APIs, such as `nvidia_p2p_get_pages`. GPUDirect RDMA allows direct memory access between GPUs and other devices.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_82\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports virtual address management. It is deprecated. Use CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_67\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Building All Documentation Versions with Shell Script\nDESCRIPTION: This shell command provides an alternative method to build documentation, generating documentation for all versions simultaneously. The script is located in the project's documentation directory and simplifies the process for generating comprehensive documentation covering multiple versions.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/docs/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n../../cuda_python/docs/build_all_docs.sh\n```\n\n----------------------------------------\n\nTITLE: Restrict Pointers Option\nDESCRIPTION: This option is a programmer assertion that all kernel pointer parameters are restrict pointers, which allows for optimizations.  There are no dependencies other than CUDA compilation.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\n --restrict ( -restrict )\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_UNORM_INT8X4\nDESCRIPTION: Defines the CU_AD_FORMAT_UNORM_INT8X4 flag for CUarray_format, representing 4 channel unsigned 8-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_UNORM_INT8X4\n```\n\n----------------------------------------\n\nTITLE: CUstreamWaitValue_flags: CU_STREAM_WAIT_VALUE_EQ\nDESCRIPTION: Defines the CU_STREAM_WAIT_VALUE_EQ flag. This flag makes the stream wait until *addr == value.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamWaitValue_flags.CU_STREAM_WAIT_VALUE_EQ\n```\n\n----------------------------------------\n\nTITLE: CUmemcpy Flags Definition - CUDA Python\nDESCRIPTION: Defines flags for memory copy operations in CUDA. Includes the `CU_MEMCPY_FLAG_DEFAULT` flag and `CU_MEMCPY_FLAG_PREFER_OVERLAP_WITH_COMPUTE` hint for the driver to potentially overlap memory copies with compute work.  These flags influence the memory transfer behavior and performance.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_166\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmemcpyFlags.CU_MEMCPY_FLAG_DEFAULT\n\n\n.. autoattribute:: cuda.bindings.driver.CUmemcpyFlags.CU_MEMCPY_FLAG_PREFER_OVERLAP_WITH_COMPUTE\n\n\n    Hint to the driver to try and overlap the copy with compute work on the SMs.\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_SNORM_INT16X1\nDESCRIPTION: Defines the CU_AD_FORMAT_SNORM_INT16X1 flag for CUarray_format, representing 1 channel signed 16-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_SNORM_INT16X1\n```\n\n----------------------------------------\n\nTITLE: Device as Default Execution Space Option\nDESCRIPTION: This option treats entities with no execution space annotation as device entities. No special dependencies.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\n --device-as-default-execution-space ( -default-device )\n```\n\n----------------------------------------\n\nTITLE: Documenting CUDA Device Attributes\nDESCRIPTION: This snippet documents various CUDA device attributes, such as the maximum number of registers per multiprocessor, whether the device supports managed memory, and others. Each attribute is linked with the associated CUDA runtime API, such as `cudaHostRegister`. These attributes are used to query the capabilities and configuration of a CUDA-enabled device.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_51\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMaxRegistersPerMultiprocessor\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrManagedMemory\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrIsMultiGpuBoard\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMultiGpuBoardGroupID\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrHostNativeAtomicSupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrSingleToDoublePrecisionPerfRatio\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrPageableMemoryAccess\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrConcurrentManagedAccess\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrComputePreemptionSupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrCanUseHostPointerForRegisteredMem\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrReserved92\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrReserved93\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrReserved94\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrCooperativeLaunch\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrCooperativeMultiDeviceLaunch\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMaxSharedMemoryPerBlockOptin\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrCanFlushRemoteWrites\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrHostRegisterSupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrPageableMemoryAccessUsesHostPageTables\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrDirectManagedMemAccessFromHost\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMaxBlocksPerMultiprocessor\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMaxPersistingL2CacheSize\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMaxAccessPolicyWindowSize\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrReservedSharedMemoryPerBlock\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrSparseCudaArraySupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrHostRegisterReadOnlySupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrTimelineSemaphoreInteropSupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMaxTimelineSemaphoreInteropSupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMemoryPoolsSupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrGPUDirectRDMASupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrGPUDirectRDMAFlushWritesOptions\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrGPUDirectRDMAWritesOrdering\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMemoryPoolSupportedHandleTypes\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrClusterLaunch\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrDeferredMappingCudaArraySupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrReserved122\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrReserved123\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrReserved124\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrIpcEventSupport\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMemSyncDomainCount\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrReserved127\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrReserved128\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrReserved129\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrNumaConfig\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrNumaId\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrReserved132\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMpsEnabled\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrHostNumaId\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrD3D12CigSupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrGpuPciDeviceId\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrGpuPciSubsystemId\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrHostNumaMultinodeIpcSupported\n```\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaDeviceAttr.cudaDevAttrMax\n```\n\n----------------------------------------\n\nTITLE: CUDA Texture Address Mode Class\nDESCRIPTION: Defines addressing modes for CUDA textures, including wrap, clamp, mirror, and border modes. These modes specify how texture coordinates outside the valid range are handled.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_63\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaTextureAddressMode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaTextureAddressMode.cudaAddressModeWrap\n\n        Wrapping address mode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaTextureAddressMode.cudaAddressModeClamp\n\n        Clamp to edge address mode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaTextureAddressMode.cudaAddressModeMirror\n\n        Mirror address mode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaTextureAddressMode.cudaAddressModeBorder\n\n        Border address mode\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_BC1_UNORM_SRGB\nDESCRIPTION: Defines the CU_AD_FORMAT_BC1_UNORM_SRGB flag for CUarray_format, representing 4 channel unsigned normalized block-compressed (BC1 compression) format with sRGB encoding.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_BC1_UNORM_SRGB\n```\n\n----------------------------------------\n\nTITLE: CUevent_record_flags: CU_EVENT_RECORD_EXTERNAL\nDESCRIPTION: Defines the CU_EVENT_RECORD_EXTERNAL flag for use with CUevent_record_flags. When using stream capture, this flag creates an event record node instead of the default behavior. It is invalid outside of stream capture.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUevent_record_flags.CU_EVENT_RECORD_EXTERNAL\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_range_attribute\nDESCRIPTION: Defines attributes that can be queried for a memory range. These attributes, such as `CU_MEM_RANGE_ATTRIBUTE_ACCESSED_BY`, provides details about how the memory range is being used and preferred location. It is typically used with CUmemGetInfo to obtain memory range information.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_133\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_range_attribute.CU_MEM_RANGE_ATTRIBUTE_ACCESSED_BY\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_MPS_ENABLED\nDESCRIPTION: This attribute indicates if contexts created on this device will be shared via MPS (Multi-Process Service).\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_99\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_MPS_ENABLED\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_BC5_SNORM\nDESCRIPTION: Defines the CU_AD_FORMAT_BC5_SNORM flag for CUarray_format, representing 2 channel signed normalized block-compressed (BC5 compression) format.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_BC5_SNORM\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_range_attribute\nDESCRIPTION: Defines attributes that can be queried for a memory range. These attributes, such as `CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION`, provides details about how the memory range is being used and preferred location. It is typically used with CUmemGetInfo to obtain memory range information.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_134\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_range_attribute.CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_range_attribute\nDESCRIPTION: Defines attributes that can be queried for a memory range. These attributes, such as `CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION_TYPE`, provides details about how the memory range is being used and preferred location. It is typically used with CUmemGetInfo to obtain memory range information.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_137\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_range_attribute.CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION_TYPE\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_SIGNED_INT8\nDESCRIPTION: Defines the CU_AD_FORMAT_SIGNED_INT8 flag for CUarray_format, representing signed 8-bit integers.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_SIGNED_INT8\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for CUDA Python Release Notes in Markdown\nDESCRIPTION: A toctree directive used in documentation systems (likely Sphinx) to organize and link to various CUDA Python release notes. The directive is configured with a maximum depth of 3 and includes links to notes for versions 12.8.0, 12.6.2, 12.6.1, and 11.8.6.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_python/docs/source/release.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{toctree}\n---\nmaxdepth: 3\n---\n\n    12.8.0 <release/12.8.0-notes>\n    12.6.2 <release/12.6.2-notes>\n    12.6.1 <release/12.6.1-notes>\n    11.8.6 <release/11.8.6-notes>\n```\n\n----------------------------------------\n\nTITLE: Use PCH File Option\nDESCRIPTION: This option uses the specified Precompiled Header (PCH) file.  This is used to speed up compilation. CUDA 12.8 or later is required.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n --use-pch=<file-name> ( -use-pch )\n```\n\n----------------------------------------\n\nTITLE: Display Error Number Option\nDESCRIPTION: This option displays diagnostic numbers for warning messages.  This is the default behavior of the compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\n --display-error-number ( -err-no )\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED\nDESCRIPTION: This attribute indicates whether the device supports virtual memory management APIs like `cuMemAddressReserve`, `cuMemCreate`, and `cuMemMap`. These APIs allow for fine-grained control over memory allocation and mapping.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_68\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: cudaErrorDeviceUninitialized Definition\nDESCRIPTION: This defines `cudaErrorDeviceUninitialized`. This is most commonly an issue because there is no context bound to the current thread. Other causes include an invalid context handle or mixing API versions. Refer to cuCtxGetApiVersion() for more details.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorDeviceUninitialized\n```\n\n----------------------------------------\n\nTITLE: Defining CUjit_option\nDESCRIPTION: Defines options for the CUDA JIT compiler. These options control various aspects of compilation and linking, such as fallback strategy when a matching cubin is not found. The choice is based on supplied `CUjit_fallback`.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_149\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUjit_option.CU_JIT_FALLBACK_STRATEGY\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_UNSIGNED_INT32\nDESCRIPTION: Defines the CU_AD_FORMAT_UNSIGNED_INT32 flag for CUarray_format, representing unsigned 32-bit integers.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_UNSIGNED_INT32\n```\n\n----------------------------------------\n\nTITLE: Brief Diagnostics Option\nDESCRIPTION: This option disables or enables showing source line and column info in a diagnostic.  `--brief-diagnostics=true` will not show the source line and column info.  CUDA compiler is required.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_24\n\nLANGUAGE: C++\nCODE:\n```\n --brief-diagnostics={true|false} ( -brief-diag )\n```\n\n----------------------------------------\n\nTITLE: Defining CUmemorytype\nDESCRIPTION: Describes the type of memory being used, such as host memory, device memory, array memory, or unified memory. These types guide memory allocation and access patterns within the CUDA system.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_120\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmemorytype.CU_MEMORYTYPE_ARRAY\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_MAX_PERSISTING_L2_CACHE_SIZE\nDESCRIPTION: This attribute indicates the maximum L2 persisting lines capacity setting in bytes. It controls how much of the L2 cache can be used for persisting data.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_74\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_MAX_PERSISTING_L2_CACHE_SIZE\n```\n\n----------------------------------------\n\nTITLE: CUstreamBatchMemOpType: CU_STREAM_MEM_OP_WAIT_VALUE_32\nDESCRIPTION: Represents a :py:obj:`~.cuStreamWaitValue32` operation within CUstreamBatchMemOpType.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamBatchMemOpType.CU_STREAM_MEM_OP_WAIT_VALUE_32\n```\n\n----------------------------------------\n\nTITLE: Defining CU_FUNC_ATTRIBUTE_MAX\nDESCRIPTION: This defines the maximum value for `CUfunction_attribute`.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_107\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUfunction_attribute.CU_FUNC_ATTRIBUTE_MAX\n```\n\n----------------------------------------\n\nTITLE: Minimal Compile Time Option\nDESCRIPTION: This option omits certain language features to reduce compile time for small programs.  This can affect the functionality of the code.  Requires CUDA compilation.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_28\n\nLANGUAGE: C++\nCODE:\n```\n --minimal ( -minimal )\n```\n\n----------------------------------------\n\nTITLE: CUDA Error Codes Explanation for Kernel and Resource Errors\nDESCRIPTION: This section describes various CUDA runtime error codes related to kernel launch failures, function loading issues, resource validation, and internal errors. Each autoattribute refers to a specific cudaError_t enumeration representing distinct error conditions, aiding in debugging CUDA applications.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorInvalidClusterSize\n\n        This indicates that a kernel launch error has occurred due to cluster misconfiguration.\n\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorFunctionNotLoaded\n\n        Indicates a function handle is not loaded when calling an API that requires a loaded function.\n\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorInvalidResourceType\n\n        This error indicates one or more resources passed in are not valid resource types for the operation.\n\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorInvalidResourceConfiguration\n\n        This error indicates one or more resources are insufficient or non-applicable for the operation.\n\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorUnknown\n\n        This indicates that an unknown internal error has occurred.\n\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorApiFailureBase\n\n```\n\n----------------------------------------\n\nTITLE: CUDA NUMA Configuration Class\nDESCRIPTION: Represents the NUMA (Non-Uniform Memory Access) configuration of a CUDA device, indicating whether the device is not a NUMA node or is a NUMA node with a specific NUMA ID.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_59\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaDeviceNumaConfig\n\n    .. autoattribute:: cuda.bindings.runtime.cudaDeviceNumaConfig.cudaDeviceNumaConfigNone\n\n        The GPU is not a NUMA node\n\n    .. autoattribute:: cuda.bindings.runtime.cudaDeviceNumaConfig.cudaDeviceNumaConfigNumaNode\n\n        The GPU is a NUMA node, cudaDevAttrNumaId contains its NUMA ID\n```\n\n----------------------------------------\n\nTITLE: CUeglColorFormat Flags Definition - CUDA Python\nDESCRIPTION: Defines flags for specifying the color format used in EGL interoperability.  These flags represent various color formats like YUV420, RGB, ARGB, luminance, and others, specifying the arrangement and ordering of color components in memory. These determine how color data is interpreted between EGL and CUDA.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_171\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_YUV420_PLANAR\n\n\n    Y, U, V in three surfaces, each in a separate surface, U/V width = 1/2 Y width, U/V height = 1/2 Y height.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_YUV420_SEMIPLANAR\n\n\n    Y, UV in two surfaces (UV as one surface) with VU byte ordering, width, height ratio same as YUV420Planar.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_YUV422_PLANAR\n\n\n    Y, U, V each in a separate surface, U/V width = 1/2 Y width, U/V height = Y height.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_YUV422_SEMIPLANAR\n\n\n    Y, UV in two surfaces with VU byte ordering, width, height ratio same as YUV422Planar.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_RGB\n\n\n    R/G/B three channels in one surface with BGR byte ordering. Only pitch linear format supported.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_BGR\n\n\n    R/G/B three channels in one surface with RGB byte ordering. Only pitch linear format supported.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_ARGB\n\n\n    R/G/B/A four channels in one surface with BGRA byte ordering.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_RGBA\n\n\n    R/G/B/A four channels in one surface with ABGR byte ordering.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_L\n\n\n    single luminance channel in one surface.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_R\n\n\n    single color channel in one surface.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_YUV444_PLANAR\n\n\n    Y, U, V in three surfaces, each in a separate surface, U/V width = Y width, U/V height = Y height.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_YUV444_SEMIPLANAR\n\n\n    Y, UV in two surfaces (UV as one surface) with VU byte ordering, width, height ratio same as YUV444Planar.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_YUYV_422\n\n\n    Y, U, V in one surface, interleaved as UYVY in one channel.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_UYVY_422\n\n\n    Y, U, V in one surface, interleaved as YUYV in one channel.\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglColorFormat.CU_EGL_COLOR_FORMAT_ABGR\n\n\n    R/G/B/A four channels in one surface with RGBA byte ordering.\n```\n\n----------------------------------------\n\nTITLE: CUuserObject Flags Definition - CUDA Python\nDESCRIPTION: Defines flags for CUDA user objects. Specifically, it defines `CU_USER_OBJECT_NO_DESTRUCTOR_SYNC`, which indicates that the destructor execution is not synchronized by any CUDA handle. This flag is relevant when managing custom objects within the CUDA runtime.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_161\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUuserObject_flags.CU_USER_OBJECT_NO_DESTRUCTOR_SYNC\n\n\n    Indicates the destructor execution is not synchronized by any CUDA handle.\n```\n\n----------------------------------------\n\nTITLE: Create PCH File Option\nDESCRIPTION: This option creates a Precompiled Header (PCH) file.  It's used to speed up compilation. Requires CUDA 12.8 or later.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n --create-pch=<file-name> ( -create-pch )\n```\n\n----------------------------------------\n\nTITLE: Instantiate Templates in PCH Option\nDESCRIPTION: This option enables or disables instantiation of templates before PCH creation. It can affect PCH file size and compilation time. CUDA 12.8+ is required and must be PCH related.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\n --instantiate-templates-in-pch={true|false} ( -instantiate-templates-in-pch )\n```\n\n----------------------------------------\n\nTITLE: CUasyncNotificationType: CU_ASYNC_NOTIFICATION_TYPE_OVER_BUDGET\nDESCRIPTION: Defines the CU_ASYNC_NOTIFICATION_TYPE_OVER_BUDGET flag for CUasyncNotificationType.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUasyncNotificationType.CU_ASYNC_NOTIFICATION_TYPE_OVER_BUDGET\n```\n\n----------------------------------------\n\nTITLE: Defining CUshared_carveout\nDESCRIPTION: Defines preferences for shared memory carveout.  These settings control the balance between shared memory and L1 cache. Examples include preferring maximum shared memory or maximum L1 cache.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_116\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUshared_carveout.CU_SHAREDMEM_CARVEOUT_MAX_SHARED\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_SNORM_INT8X1\nDESCRIPTION: Defines the CU_AD_FORMAT_SNORM_INT8X1 flag for CUarray_format, representing 1 channel signed 8-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_SNORM_INT8X1\n```\n\n----------------------------------------\n\nTITLE: CUDA CIG Data Types\nDESCRIPTION: Defines data types used in CUDA CIG (CUDA Interop with Graphics) interfaces, like 'D3D12_COMMAND_QUEUE', for interoperability with graphics APIs and hardware command queues.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_157\n\nLANGUAGE: Python\nCODE:\n```\n# No specific code snippets provided; describes CIG data types for interop scenarios.\n```\n\n----------------------------------------\n\nTITLE: Set C++ Language Dialect Option\nDESCRIPTION: This option sets the language dialect to a specific C++ standard, like C++03, C++11, C++14, C++17, or C++20. This affects the compiler's interpretation of the source code. Requires the CUDA compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n --std={c++03|c++11|c++14|c++17|c++20} ( -std )\n```\n\n----------------------------------------\n\nTITLE: Disable Warnings Option\nDESCRIPTION: This option inhibits all warning messages during compilation. Requires the CUDA compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\n --disable-warnings ( -w )\n```\n\n----------------------------------------\n\nTITLE: Defining CUfunc_cache Preferences\nDESCRIPTION: Describes various preferences for shared memory and L1 cache configurations.  These preferences are used to tune memory performance. Each attribute, such as `CU_FUNC_CACHE_PREFER_NONE`, indicates a specific memory configuration preference.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_108\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUfunc_cache.CU_FUNC_CACHE_PREFER_NONE\n```\n\n----------------------------------------\n\nTITLE: PCH Messages Option\nDESCRIPTION: This option prints a message in the compilation log if a PCH file was created or used in the current compilation. Requires CUDA 12.8+ and is PCH-related.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n --pch-messages={true|false} ( -pch-messages )\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_BC2_UNORM_SRGB\nDESCRIPTION: Defines the CU_AD_FORMAT_BC2_UNORM_SRGB flag for CUarray_format, representing 4 channel unsigned normalized block-compressed (BC2 compression) format with sRGB encoding.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_BC2_UNORM_SRGB\n```\n\n----------------------------------------\n\nTITLE: Describing cudaLaunchAttributeID Attributes\nDESCRIPTION: This section documents the `cudaLaunchAttributeID` enumeration, which defines various attributes used when launching CUDA kernels.  These attributes include options for access policies, cooperative launches, synchronization, cluster dimensions, and event recording.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_56\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaLaunchAttributeID\n\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeIgnore\n\n\n        Ignored entry, for convenient composition\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeAccessPolicyWindow\n\n\n        Valid for streams, graph nodes, launches. See :py:obj:`~.cudaLaunchAttributeValue.accessPolicyWindow`.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeCooperative\n\n\n        Valid for graph nodes, launches. See :py:obj:`~.cudaLaunchAttributeValue.cooperative`.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeSynchronizationPolicy\n\n\n        Valid for streams. See :py:obj:`~.cudaLaunchAttributeValue.syncPolicy`.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeClusterDimension\n\n\n        Valid for graph nodes, launches. See :py:obj:`~.cudaLaunchAttributeValue.clusterDim`.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeClusterSchedulingPolicyPreference\n\n\n        Valid for graph nodes, launches. See :py:obj:`~.cudaLaunchAttributeValue.clusterSchedulingPolicyPreference`.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeProgrammaticStreamSerialization\n\n\n        Valid for launches. Setting :py:obj:`~.cudaLaunchAttributeValue.programmaticStreamSerializationAllowed` to non-0 signals that the kernel will use programmatic means to resolve its stream dependency, so that the CUDA runtime should opportunistically allow the grid's execution to overlap with the previous kernel in the stream, if that kernel requests the overlap. The dependent launches can choose to wait on the dependency using the programmatic sync (cudaGridDependencySynchronize() or equivalent PTX instructions).\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeProgrammaticEvent\n\n\n        Valid for launches. Set :py:obj:`~.cudaLaunchAttributeValue.programmaticEvent` to record the event. Event recorded through this launch attribute is guaranteed to only trigger after all block in the associated kernel trigger the event. A block can trigger the event programmatically in a future CUDA release. A trigger can also be inserted at the beginning of each block's execution if triggerAtBlockStart is set to non-0. The dependent launches can choose to wait on the dependency using the programmatic sync (cudaGridDependencySynchronize() or equivalent PTX instructions). Note that dependents (including the CPU thread calling :py:obj:`~.cudaEventSynchronize()`) are not guaranteed to observe the release precisely when it is released. For example, :py:obj:`~.cudaEventSynchronize()` may only observe the event trigger long after the associated kernel has completed. This recording type is primarily meant for establishing programmatic dependency between device tasks. Note also this type of dependency allows, but does not guarantee, concurrent execution of tasks. \n\n         The event supplied must not be an interprocess or interop event. The event must disable timing (i.e. must be created with the :py:obj:`~.cudaEventDisableTiming` flag set).\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributePriority\n\n\n        Valid for streams, graph nodes, launches. See :py:obj:`~.cudaLaunchAttributeValue.priority`.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeMemSyncDomainMap\n\n\n        Valid for streams, graph nodes, launches. See :py:obj:`~.cudaLaunchAttributeValue.memSyncDomainMap`.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeMemSyncDomain\n\n\n        Valid for streams, graph nodes, launches. See :py:obj:`~.cudaLaunchAttributeValue.memSyncDomain`.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributePreferredClusterDimension\n\n\n        Valid for graph nodes and launches. Set :py:obj:`~.cudaLaunchAttributeValue.preferredClusterDim` to allow the kernel launch to specify a preferred substitute cluster dimension. Blocks may be grouped according to either the dimensions specified with this attribute (grouped into a \"preferred substitute cluster\"), or the one specified with :py:obj:`~.cudaLaunchAttributeClusterDimension` attribute (grouped into a \"regular cluster\"). The cluster dimensions of a \"preferred substitute cluster\" shall be an integer multiple greater than zero of the regular cluster dimensions. The device will attempt - on a best-effort basis - to group thread blocks into preferred clusters over grouping them into regular clusters. When it deems necessary (primarily when the device temporarily runs out of physical resources to launch the larger preferred clusters), the device may switch to launch the regular clusters instead to attempt to utilize as much of the physical device resources as possible. \n\n         Each type of cluster will have its enumeration / coordinate setup as if the grid consists solely of its type of cluster. For example, if the preferred substitute cluster dimensions double the regular cluster dimensions, there might be simultaneously a regular cluster indexed at (1,0,0), and a preferred cluster indexed at (1,0,0). In this example, the preferred substitute cluster (1,0,0) replaces regular clusters (2,0,0) and (3,0,0) and groups their blocks. \n\n         This attribute will only take effect when a regular cluster dimension has been specified. The preferred substitute cluster dimension must be an integer multiple greater than zero of the regular cluster dimension and must divide the grid. It must also be no more than `maxBlocksPerCluster`, if it is set in the kernel's `__launch_bounds__`. Otherwise it must be less than the maximum value the driver can support. Otherwise, setting this attribute to a value physically unable to fit on any particular device is permitted.\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeLaunchCompletionEvent\n\n\n        Valid for launches. Set :py:obj:`~.cudaLaunchAttributeValue.launchCompletionEvent` to record the event. \n\n         Nominally, the event is triggered once all blocks of the kernel have begun execution. Currently this is a best effort. If a kernel B has a launch completion dependency on a kernel A, B may wait until A is complete. Alternatively, blocks of B may begin before all blocks of A have begun, for example if B can claim execution resources unavailable to A (e.g. they run on different GPUs) or if B is a higher priority than A. Exercise caution if such an ordering inversion could lead to deadlock. \n\n         A launch completion event is nominally similar to a programmatic event with `triggerAtBlockStart` set except that it is not visible to `cudaGridDependencySynchronize()` and can be used with compute capability less than 9.0. \n\n         The event supplied must not be an interprocess or interop event. The event must disable timing (i.e. must be created with the :py:obj:`~.cudaEventDisableTiming` flag set).\n```\n\nLANGUAGE: Python\nCODE:\n```\n    .. autoattribute:: cuda.bindings.runtime.cudaLaunchAttributeID.cudaLaunchAttributeDeviceUpdatableKernelNode\n\n\n        Valid for graph nodes, launches. This attribute is graphs-only, and passing it to a launch in a non-capturing stream will result in an error.\n```\n\n----------------------------------------\n\nTITLE: Defining Coredump Attribute Control API Elements (CUDA Python/Sphinx)\nDESCRIPTION: Utilizes Sphinx `autoclass`, `autoattribute`, and `autofunction` directives to document the coredump control API within `cuda.bindings.driver`. This includes the `CUcoredumpSettings` and `CUCoredumpGenerationFlags` classes with their respective attributes (flags/settings), and functions for getting/setting coredump attributes globally or per-process.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_175\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: cuda.bindings.driver.CUcoredumpSettings\n\n    .. autoattribute:: cuda.bindings.driver.CUcoredumpSettings.CU_COREDUMP_ENABLE_ON_EXCEPTION\n\n\n    .. autoattribute:: cuda.bindings.driver.CUcoredumpSettings.CU_COREDUMP_TRIGGER_HOST\n\n\n    .. autoattribute:: cuda.bindings.driver.CUcoredumpSettings.CU_COREDUMP_LIGHTWEIGHT\n\n\n    .. autoattribute:: cuda.bindings.driver.CUcoredumpSettings.CU_COREDUMP_ENABLE_USER_TRIGGER\n\n\n    .. autoattribute:: cuda.bindings.driver.CUcoredumpSettings.CU_COREDUMP_FILE\n\n\n    .. autoattribute:: cuda.bindings.driver.CUcoredumpSettings.CU_COREDUMP_PIPE\n\n\n    .. autoattribute:: cuda.bindings.driver.CUcoredumpSettings.CU_COREDUMP_GENERATION_FLAGS\n\n\n    .. autoattribute:: cuda.bindings.driver.CUcoredumpSettings.CU_COREDUMP_MAX\n\n.. autoclass:: cuda.bindings.driver.CUCoredumpGenerationFlags\n\n    .. autoattribute:: cuda.bindings.driver.CUCoredumpGenerationFlags.CU_COREDUMP_DEFAULT_FLAGS\n\n\n    .. autoattribute:: cuda.bindings.driver.CUCoredumpGenerationFlags.CU_COREDUMP_SKIP_NONRELOCATED_ELF_IMAGES\n\n\n    .. autoattribute:: cuda.bindings.driver.CUCoredumpGenerationFlags.CU_COREDUMP_SKIP_GLOBAL_MEMORY\n\n\n    .. autoattribute:: cuda.bindings.driver.CUCoredumpGenerationFlags.CU_COREDUMP_SKIP_SHARED_MEMORY\n\n\n    .. autoattribute:: cuda.bindings.driver.CUCoredumpGenerationFlags.CU_COREDUMP_SKIP_LOCAL_MEMORY\n\n\n    .. autoattribute:: cuda.bindings.driver.CUCoredumpGenerationFlags.CU_COREDUMP_SKIP_ABORT\n\n\n    .. autoattribute:: cuda.bindings.driver.CUCoredumpGenerationFlags.CU_COREDUMP_SKIP_CONSTBANK_MEMORY\n\n\n    .. autoattribute:: cuda.bindings.driver.CUCoredumpGenerationFlags.CU_COREDUMP_LIGHTWEIGHT_FLAGS\n\n.. autofunction:: cuda.bindings.driver.cuCoredumpGetAttribute\n.. autofunction:: cuda.bindings.driver.cuCoredumpGetAttributeGlobal\n.. autofunction:: cuda.bindings.driver.cuCoredumpSetAttribute\n.. autofunction:: cuda.bindings.driver.cuCoredumpSetAttributeGlobal\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_MULTICAST_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports switch multicast and reduction operations.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_98\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_MULTICAST_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Include Directory Option\nDESCRIPTION: This option adds a specified directory to the list of directories searched for header files during compilation. These paths are searched after headers given to `nvrtcCreateProgram`. The required dependency is the CUDA compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n --include-path=<dir> ( -I )\n```\n\n----------------------------------------\n\nTITLE: Performing an Editable Install of cuda-python (Shell)\nDESCRIPTION: Installs `cuda-python` in editable mode from the current source directory using pip. This allows changes in the source code to be reflected immediately without reinstalling. The `-v` flag provides verbose output during installation.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/install.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ pip install -v -e .\n```\n\n----------------------------------------\n\nTITLE: Emit Warning for Diagnostic Option\nDESCRIPTION: This option emits a warning for specified diagnostic message numbers. Message numbers can be comma-separated. Requires the CUDA compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_23\n\nLANGUAGE: C++\nCODE:\n```\n --diag-warn=<error-number> ,... ( -diag-warn )\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_WRITES_ORDERING\nDESCRIPTION: This attribute indicates the ordering requirements for GPUDirect RDMA writes to the device. The return value corresponds to the `CUGPUDirectRDMAWritesOrdering` enum.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_84\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_WRITES_ORDERING\n```\n\n----------------------------------------\n\nTITLE: Defining CUmem_range_attribute\nDESCRIPTION: Defines attributes that can be queried for a memory range. These attributes, such as `CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION_TYPE`, provides details about how the memory range is being used and preferred location. It is typically used with CUmemGetInfo to obtain memory range information.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_135\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUmem_range_attribute.CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION_TYPE\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_WAIT_VALUE_NOR_V1\nDESCRIPTION: This attribute indicates whether the device supports the `CU_STREAM_WAIT_VALUE_NOR_V1` flag for stream wait operations. It is now deprecated, with `CU_STREAM_WAIT_VALUE_NOR` currently supported. This attribute is related to the v1 MemOps API.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_59\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_WAIT_VALUE_NOR_V1\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_COOPERATIVE_LAUNCH\nDESCRIPTION: This attribute indicates if the device supports launching cooperative kernels using `cuLaunchCooperativeKernel`. Cooperative kernels allow threads in a grid to synchronize and share data more efficiently.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_60\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COOPERATIVE_LAUNCH\n```\n\n----------------------------------------\n\nTITLE: cudaErrorLaunchFileScopedTex Definition\nDESCRIPTION: This snippet defines the `cudaErrorLaunchFileScopedTex` error. This error indicates that a grid launch did not occur because the kernel uses file-scoped textures which are unsupported by the device runtime. Kernels launched via the device runtime only support textures created with the Texture Object API's.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorLaunchFileScopedTex\n```\n\n----------------------------------------\n\nTITLE: Defining CUcomputemode\nDESCRIPTION: Defines compute modes for a device. These modes control how contexts are created and used on a device.  Compute modes include default, prohibited, and exclusive process modes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_123\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUcomputemode.CU_COMPUTEMODE_PROHIBITED\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Checkpointing Functions (CUDA Python/Sphinx)\nDESCRIPTION: Specifies Sphinx `autofunction` directives to generate documentation for the CUDA checkpointing functions within the `cuda.bindings.driver` module. These functions support saving and restoring GPU state for process checkpointing on Linux.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_177\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autofunction:: cuda.bindings.driver.cuCheckpointProcessGetRestoreThreadId\n.. autofunction:: cuda.bindings.driver.cuCheckpointProcessGetState\n.. autofunction:: cuda.bindings.driver.cuCheckpointProcessLock\n.. autofunction:: cuda.bindings.driver.cuCheckpointProcessCheckpoint\n.. autofunction:: cuda.bindings.driver.cuCheckpointProcessRestore\n.. autofunction:: cuda.bindings.driver.cuCheckpointProcessUnlock\n```\n\n----------------------------------------\n\nTITLE: Unsupported CUDA Functions in Release 12.3.0\nDESCRIPTION: A comprehensive list of CUDA functions that are not supported in the CUDA Python 12.3.0 release, categorized by function type including Symbol APIs, Launch Options, and individual functions.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/release/12.3.0-notes.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- Symbol APIs\n    - cudaGraphExecMemcpyNodeSetParamsFromSymbol\n    - cudaGraphExecMemcpyNodeSetParamsToSymbol\n    - cudaGraphAddMemcpyNodeToSymbol\n    - cudaGraphAddMemcpyNodeFromSymbol\n    - cudaGraphMemcpyNodeSetParamsToSymbol\n    - cudaGraphMemcpyNodeSetParamsFromSymbol\n    - cudaMemcpyToSymbol\n    - cudaMemcpyFromSymbol\n    - cudaMemcpyToSymbolAsync\n    - cudaMemcpyFromSymbolAsync\n    - cudaGetSymbolAddress\n    - cudaGetSymbolSize\n    - cudaGetFuncBySymbol\n- Launch Options\n    - cudaLaunchKernel\n    - cudaLaunchCooperativeKernel\n    - cudaLaunchCooperativeKernelMultiDevice\n- cudaSetValidDevices\n- cudaVDPAUSetVDPAUDevice\n- cudaFuncGetName\n```\n\n----------------------------------------\n\nTITLE: CUdeviceNumaConfig Flags Definition - CUDA Python\nDESCRIPTION: Defines flags for configuring NUMA (Non-Uniform Memory Access) on CUDA devices.  The flags indicate whether the GPU is a NUMA node and, if so, provides access to its NUMA ID. This configuration is relevant for systems with multiple memory domains.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_164\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdeviceNumaConfig.CU_DEVICE_NUMA_CONFIG_NONE\n\n\n    The GPU is not a NUMA node\n\n\n.. autoattribute:: cuda.bindings.driver.CUdeviceNumaConfig.CU_DEVICE_NUMA_CONFIG_NUMA_NODE\n\n\n    The GPU is a NUMA node, CU_DEVICE_ATTRIBUTE_NUMA_ID contains its NUMA ID\n```\n\n----------------------------------------\n\nTITLE: Running pytest (python)\nDESCRIPTION: Executes pytest to run tests against editable installations or installed packages. Requires pytest to be installed.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_core/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython -m pytest tests/\n```\n\nLANGUAGE: python\nCODE:\n```\npytest tests/\n```\n\n----------------------------------------\n\nTITLE: CUDA Texture Read Mode Class\nDESCRIPTION: Defines how texture data is read, either as specific element types or normalized floating-point values, affecting how texture sampling results are interpreted.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_65\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaTextureReadMode\n\n    .. autoattribute:: cuda.bindings.runtime.cudaTextureReadMode.cudaReadModeElementType\n\n        Read texture as specified element type\n\n    .. autoattribute:: cuda.bindings.runtime.cudaTextureReadMode.cudaReadModeNormalizedFloat\n\n        Read texture as normalized float\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_GPU_PCI_SUBSYSTEM_ID\nDESCRIPTION: This attribute represents the GPU PCI subsystem ID.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_105\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_GPU_PCI_SUBSYSTEM_ID\n```\n\n----------------------------------------\n\nTITLE: Set PCH Directory Option\nDESCRIPTION: This option specifies the directory to look for and create PCH files when using automatic PCH (`-pch`). It also controls the prefixing of the directory for explicit PCH files. CUDA 12.8+ is required.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n --pch-dir=<directory-name> ( -pch-dir )\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_NUMA_CONFIG\nDESCRIPTION: This attribute represents the NUMA configuration of a device. The value is of type `CUdeviceNumaConfig` enum.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_96\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_NUMA_CONFIG\n```\n\n----------------------------------------\n\nTITLE: Importing CUDA Bindings and NumPy in Python\nDESCRIPTION: Imports the necessary CUDA Driver API (`driver`) and NVRTC (`nvrtc`) modules from `cuda.bindings`, along with the NumPy library (`np`) for array manipulation and data handling.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/overview.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom cuda.bindings import driver, nvrtc\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_WITH_CUDA_VMM_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports specifying the GPUDirect RDMA flag with `cuMemCreate`. This enables direct memory access between GPUs and other devices.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_76\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_WITH_CUDA_VMM_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: CUDA Synchronization Policy Enum Summary\nDESCRIPTION: This class defines synchronization policies for CUDA operations, including auto, spin, yield, and blocking sync options, to manage thread synchronization during CUDA kernel executions.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_49\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: cuda.bindings.runtime.cudaSynchronizationPolicy\n\n    .. autoattribute:: cuda.bindings.runtime.cudaSynchronizationPolicy.cudaSyncPolicyAuto\n\n    .. autoattribute:: cuda.bindings.runtime.cudaSynchronizationPolicy.cudaSyncPolicySpin\n\n    .. autoattribute:: cuda.bindings.runtime.cudaSynchronizationPolicy.cudaSyncPolicyYield\n\n    .. autoattribute:: cuda.bindings.runtime.cudaSynchronizationPolicy.cudaSyncPolicyBlockingSync\n\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_BC4_SNORM\nDESCRIPTION: Defines the CU_AD_FORMAT_BC4_SNORM flag for CUarray_format, representing 1 channel signed normalized block-compressed (BC4 compression) format.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_BC4_SNORM\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_SNORM_INT8X2\nDESCRIPTION: Defines the CU_AD_FORMAT_SNORM_INT8X2 flag for CUarray_format, representing 2 channel signed 8-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_SNORM_INT8X2\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_BC3_UNORM\nDESCRIPTION: Defines the CU_AD_FORMAT_BC3_UNORM flag for CUarray_format, representing 4 channel unsigned normalized block-compressed (BC3 compression) format.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_BC3_UNORM\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_UNSIGNED_INT8\nDESCRIPTION: Defines the CU_AD_FORMAT_UNSIGNED_INT8 flag for CUarray_format, representing unsigned 8-bit integers.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_UNSIGNED_INT8\n```\n\n----------------------------------------\n\nTITLE: Device Syntax Check Only Option\nDESCRIPTION: This option ends device compilation after front-end syntax checking, without generating valid device code.  No dependencies.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_27\n\nLANGUAGE: C++\nCODE:\n```\n --fdevice-syntax-only ( -fdevice-syntax-only )\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_TENSOR_MAP_ACCESS_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports accessing memory using Tensor Map.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_93\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_TENSOR_MAP_ACCESS_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_COOPERATIVE_MULTI_DEVICE_LAUNCH\nDESCRIPTION: This attribute indicates if the device supports cooperative multi-device launch using `cuLaunchCooperativeKernelMultiDevice`. This feature is deprecated.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_61\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COOPERATIVE_MULTI_DEVICE_LAUNCH\n```\n\n----------------------------------------\n\nTITLE: cudaErrorInvalidKernelImage Definition\nDESCRIPTION: This documents the `cudaErrorInvalidKernelImage` error, which means that the device kernel image is invalid. This suggests problems related to compiling or linking CUDA kernels.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorInvalidKernelImage\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_WAIT_VALUE_NOR\nDESCRIPTION: This attribute indicates whether `CU_STREAM_WAIT_VALUE_NOR` is supported by MemOp APIs.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_89\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_WAIT_VALUE_NOR\n```\n\n----------------------------------------\n\nTITLE: cudaErrorInvalidDeviceFunction Definition\nDESCRIPTION: This explains the `cudaErrorInvalidDeviceFunction` error. The requested device function does not exist or is not compiled for the proper device architecture. This is a common compilation and build problem.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorInvalidDeviceFunction\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_FLUSH_WRITES_OPTIONS\nDESCRIPTION: This attribute indicates the GPU Direct RDMA flush writes options, interpreted as a bitmask. The individual bits are described by the `CUflushGPUDirectRDMAWritesOptions` enum.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_83\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_GPU_DIRECT_RDMA_FLUSH_WRITES_OPTIONS\n```\n\n----------------------------------------\n\nTITLE: cudaErrorIncompatibleDriverContext Definition\nDESCRIPTION: This snippet details the `cudaErrorIncompatibleDriverContext` error. It states that the current context is incompatible with the CUDA Runtime, likely caused by interoperability issues with the CUDA Driver API. It requires the correct CUDA Runtime/Driver version and also refers to primary driver contexts.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorIncompatibleDriverContext\n```\n\n----------------------------------------\n\nTITLE: cudaErrorStartupFailure Definition\nDESCRIPTION: This specifies the `cudaErrorStartupFailure` error, which indicates an internal startup failure in the CUDA runtime. This signals a critical failure when initializing the CUDA environment.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorStartupFailure\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_CLUSTER_LAUNCH\nDESCRIPTION: This attribute indicates if the device supports cluster launch, a feature for launching kernels across multiple devices.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_86\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_CLUSTER_LAUNCH\n```\n\n----------------------------------------\n\nTITLE: cudaErrorLaunchPendingCountExceeded Definition\nDESCRIPTION: This describes the `cudaErrorLaunchPendingCountExceeded` error. This error indicates that a device runtime grid launch failed because the launch would exceed the limit cudaLimitDevRuntimePendingLaunchCount. It notes that a higher setting for cudaLimitDevRuntimePendingLaunchCount may be required.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorLaunchPendingCountExceeded\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_MAX_ACCESS_POLICY_WINDOW_SIZE\nDESCRIPTION: This attribute specifies the maximum value of `CUaccessPolicyWindow.num_bytes`. It is related to memory access policy windows.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_75\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_MAX_ACCESS_POLICY_WINDOW_SIZE\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_MEM_DECOMPRESS_MAXIMUM_LENGTH\nDESCRIPTION: This attribute indicates the maximum length in bytes of a single decompress operation that is allowed.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_103\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_MEM_DECOMPRESS_MAXIMUM_LENGTH\n```\n\n----------------------------------------\n\nTITLE: Device Stack Protector Option\nDESCRIPTION: This option enables stack canaries in device code, making it more difficult to exploit certain types of memory safety bugs. Requires the CUDA compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_29\n\nLANGUAGE: C++\nCODE:\n```\n --device-stack-protector ( -device-stack-protector )\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_HANDLE_SUPPORTED\nDESCRIPTION: This attribute indicates if the device supports exporting memory to a Win32 NT handle using `cuMemExportToShareableHandle`, if requested through `cuMemCreate`. This is relevant for Windows-based systems and inter-process communication.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_70\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_HANDLE_SUPPORTED\n```\n\n----------------------------------------\n\nTITLE: CUgraphDebugDot Flags Definitions - CUDA Python\nDESCRIPTION: Defines flags for debugging CUDA graphs using the dot format. These flags control the level of detail included in the output, such as external semaphore information, kernel node attributes, handles, memory allocation parameters, and topological information. These flags are used with the `CUgraphDebugDot` functionality.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_160\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUgraphDebugDot_flags.CU_GRAPH_DEBUG_DOT_FLAGS_EXT_SEMAS_SIGNAL_NODE_PARAMS\n\n\n    Adds CUDA_EXT_SEM_SIGNAL_NODE_PARAMS values to output\n\n\n.. autoattribute:: cuda.bindings.driver.CUgraphDebugDot_flags.CU_GRAPH_DEBUG_DOT_FLAGS_EXT_SEMAS_WAIT_NODE_PARAMS\n\n\n    Adds CUDA_EXT_SEM_WAIT_NODE_PARAMS values to output\n\n\n.. autoattribute:: cuda.bindings.driver.CUgraphDebugDot_flags.CU_GRAPH_DEBUG_DOT_FLAGS_KERNEL_NODE_ATTRIBUTES\n\n\n    Adds CUkernelNodeAttrValue values to output\n\n\n.. autoattribute:: cuda.bindings.driver.CUgraphDebugDot_flags.CU_GRAPH_DEBUG_DOT_FLAGS_HANDLES\n\n\n    Adds node handles and every kernel function handle to output\n\n\n.. autoattribute:: cuda.bindings.driver.CUgraphDebugDot_flags.CU_GRAPH_DEBUG_DOT_FLAGS_MEM_ALLOC_NODE_PARAMS\n\n\n    Adds memory alloc node parameters to output\n\n\n.. autoattribute:: cuda.bindings.driver.CUgraphDebugDot_flags.CU_GRAPH_DEBUG_DOT_FLAGS_MEM_FREE_NODE_PARAMS\n\n\n    Adds memory free node parameters to output\n\n\n.. autoattribute:: cuda.bindings.driver.CUgraphDebugDot_flags.CU_GRAPH_DEBUG_DOT_FLAGS_BATCH_MEM_OP_NODE_PARAMS\n\n\n    Adds batch mem op node parameters to output\n\n\n.. autoattribute:: cuda.bindings.driver.CUgraphDebugDot_flags.CU_GRAPH_DEBUG_DOT_FLAGS_EXTRA_TOPO_INFO\n\n\n    Adds edge numbering information\n\n\n.. autoattribute:: cuda.bindings.driver.CUgraphDebugDot_flags.CU_GRAPH_DEBUG_DOT_FLAGS_CONDITIONAL_NODE_PARAMS\n\n\n    Adds conditional node parameters to output\n```\n\n----------------------------------------\n\nTITLE: CUeglResourceLocation Flags Definition - CUDA Python\nDESCRIPTION: Defines flags indicating the resource location for EGL interoperability, specifying whether the resource is located in system memory (`CU_EGL_RESOURCE_LOCATION_SYSMEM`) or video memory (`CU_EGL_RESOURCE_LOCATION_VIDMEM`). These flags are used to determine where the EGL resource resides in memory.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_170\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUeglResourceLocationFlags.CU_EGL_RESOURCE_LOCATION_SYSMEM\n\n\n    Resource location sysmem\n\n\n.. autoattribute:: cuda.bindings.driver.CUeglResourceLocationFlags.CU_EGL_RESOURCE_LOCATION_VIDMEM\n\n\n    Resource location vidmem\n```\n\n----------------------------------------\n\nTITLE: cudaErrorAlreadyMapped Definition\nDESCRIPTION: This defines the `cudaErrorAlreadyMapped` error. This error indicates that the resource is already mapped. It refers to memory access conflicts during resource manipulation.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorAlreadyMapped\n```\n\n----------------------------------------\n\nTITLE: Defining CUcomputemode\nDESCRIPTION: Defines compute modes for a device. These modes control how contexts are created and used on a device.  Compute modes include default, prohibited, and exclusive process modes.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_124\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUcomputemode.CU_COMPUTEMODE_EXCLUSIVE_PROCESS\n```\n\n----------------------------------------\n\nTITLE: Disable Error Number Display Option\nDESCRIPTION: This option disables the display of diagnostic numbers for warning messages.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_20\n\nLANGUAGE: C++\nCODE:\n```\n --no-display-error-number ( -no-err-no )\n```\n\n----------------------------------------\n\nTITLE: cudaErrorInvalidSurface Definition\nDESCRIPTION: This explains the `cudaErrorInvalidSurface` error. It signifies that the surface passed to the API call is not a valid surface. This error highlights issues in resource allocation for the surface object.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorInvalidSurface\n```\n\n----------------------------------------\n\nTITLE: Verbose PCH Mode Option\nDESCRIPTION: This option, when in automatic PCH mode, prints the reason in the compilation log for each PCH file that could not be used in current compilation. Requires CUDA 12.8+ and using automatic PCH.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n --pch-verbose={true|false} ( -pch-verbose )\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_UNORM_INT16X2\nDESCRIPTION: Defines the CU_AD_FORMAT_UNORM_INT16X2 flag for CUarray_format, representing 2 channel unsigned 16-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_UNORM_INT16X2\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_HALF\nDESCRIPTION: Defines the CU_AD_FORMAT_HALF flag for CUarray_format, representing 16-bit floating point.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_HALF\n```\n\n----------------------------------------\n\nTITLE: Suppress Diagnostic Option\nDESCRIPTION: This option suppresses specified diagnostic message numbers.  Message numbers can be comma-separated. Requires the CUDA compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_22\n\nLANGUAGE: C++\nCODE:\n```\n --diag-suppress=<error-number> ,... ( -diag-suppress )\n```\n\n----------------------------------------\n\nTITLE: CUoccupancy_flags: CU_OCCUPANCY_DEFAULT\nDESCRIPTION: Defines the CU_OCCUPANCY_DEFAULT flag for CUoccupancy_flags, representing default behavior.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUoccupancy_flags.CU_OCCUPANCY_DEFAULT\n```\n\n----------------------------------------\n\nTITLE: cudaErrorLaunchFileScopedSurf Definition\nDESCRIPTION: This defines `cudaErrorLaunchFileScopedSurf` which indicates that a grid launch did not occur because the kernel uses file-scoped surfaces which are unsupported by the device runtime. Kernels launched via the device runtime only support surfaces created with the Surface Object API's. This highlights the restricted features within device runtime.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorLaunchFileScopedSurf\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_SNORM_INT16X4\nDESCRIPTION: Defines the CU_AD_FORMAT_SNORM_INT16X4 flag for CUarray_format, representing 4 channel signed 16-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_SNORM_INT16X4\n```\n\n----------------------------------------\n\nTITLE: Defining CUsharedconfig\nDESCRIPTION: Defines the configuration of shared memory. Options include setting the default bank size, the four-byte bank size, or eight-byte bank size.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_113\n\nLANGUAGE: CUDA\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUsharedconfig.CU_SHARED_MEM_CONFIG_FOUR_BYTE_BANK_SIZE\n```\n\n----------------------------------------\n\nTITLE: CUstreamWriteValue_flags: CU_STREAM_WRITE_VALUE_DEFAULT\nDESCRIPTION: Defines the CU_STREAM_WRITE_VALUE_DEFAULT flag for CUstreamWriteValue_flags, representing the default behavior.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUstreamWriteValue_flags.CU_STREAM_WRITE_VALUE_DEFAULT\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_SNORM_INT8X4\nDESCRIPTION: Defines the CU_AD_FORMAT_SNORM_INT8X4 flag for CUarray_format, representing 4 channel signed 8-bit normalized integer.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_SNORM_INT8X4\n```\n\n----------------------------------------\n\nTITLE: Emit Error for Diagnostic Option\nDESCRIPTION: This option emits an error for specified diagnostic message numbers.  Message numbers can be comma-separated.  Requires CUDA compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_21\n\nLANGUAGE: C++\nCODE:\n```\n --diag-error=<error-number> ,... ( -diag-error )\n```\n\n----------------------------------------\n\nTITLE: Disable Source Include Option\nDESCRIPTION: This option disables the default behavior of the preprocessor, which adds the directory of each input source file to the include path. When this option is enabled, only the explicitly specified paths are considered. There are no specific dependencies.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n --no-source-include ( -no-source-include )\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_BC2_UNORM\nDESCRIPTION: Defines the CU_AD_FORMAT_BC2_UNORM flag for CUarray_format, representing 4 channel unsigned normalized block-compressed (BC2 compression) format.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_BC2_UNORM\n```\n\n----------------------------------------\n\nTITLE: CUarray_format: CU_AD_FORMAT_BC3_UNORM_SRGB\nDESCRIPTION: Defines the CU_AD_FORMAT_BC3_UNORM_SRGB flag for CUarray_format, representing 4 channel unsigned normalized block-compressed (BC3 compression) format with sRGB encoding.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ncuda.bindings.driver.CUarray_format.CU_AD_FORMAT_BC3_UNORM_SRGB\n```\n\n----------------------------------------\n\nTITLE: cudaErrorDeviceNotLicensed Definition\nDESCRIPTION: This explains the `cudaErrorDeviceNotLicensed` error. It indicates that the device does not have a valid Grid License. It generally points towards a licensing problem.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorDeviceNotLicensed\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_CAN_USE_64_BIT_STREAM_MEM_OPS_V1\nDESCRIPTION: This attribute indicates if the device supports 64-bit stream memory operations (MemOps) V1. It is now deprecated and superseded by newer MemOps APIs. 64-bit operations are currently supported in `cuStreamBatchMemOp` and related APIs.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_58\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_CAN_USE_64_BIT_STREAM_MEM_OPS_V1\n```\n\n----------------------------------------\n\nTITLE: cudaErrorMemoryValueTooLarge Definition\nDESCRIPTION: This snippet describes the `cudaErrorMemoryValueTooLarge` error, which has been deprecated. This indicated that an emulated device pointer exceeded the 32-bit address range. This relates to a device memory allocation limitation during emulation.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/runtime.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n.. autoattribute:: cuda.bindings.runtime.cudaError_t.cudaErrorMemoryValueTooLarge\n```\n\n----------------------------------------\n\nTITLE: Documenting CU_DEVICE_ATTRIBUTE_MEMPOOL_SUPPORTED_HANDLE_TYPES\nDESCRIPTION: This attribute indicates the handle types supported with mempool based IPC (Inter-Process Communication).\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_85\n\nLANGUAGE: text\nCODE:\n```\n.. autoattribute:: cuda.bindings.driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_MEMPOOL_SUPPORTED_HANDLE_TYPES\n```\n\n----------------------------------------\n\nTITLE: Allow __float128 Type Option\nDESCRIPTION: This option allows the `__float128` and `_Float128` types in device code and defines the macro `__CUDACC_RTC_FLOAT128__`. Requires the CUDA compiler.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/nvrtc.rst#_snippet_17\n\nLANGUAGE: C++\nCODE:\n```\n --device-float128 ( -device-float128 )\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit for Development\nDESCRIPTION: Installs the `pre-commit` tool using pip. This tool is required for managing development hooks to ensure code consistency and quality before committing changes to the repository.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install pre-commit\n```\n\n----------------------------------------\n\nTITLE: Defining Peer Access Functions (CUDA Python/Sphinx)\nDESCRIPTION: Uses Sphinx `autofunction` directives to generate documentation for CUDA Driver API peer access functions (`cuCtxDisablePeerAccess`, `cuDeviceGetP2PAttribute`) available in the `cuda.bindings.driver` Python module.\nSOURCE: https://github.com/nvidia/cuda-python/blob/main/cuda_bindings/docs/source/module/driver.rst#_snippet_172\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autofunction:: cuda.bindings.driver.cuCtxDisablePeerAccess\n.. autofunction:: cuda.bindings.driver.cuDeviceGetP2PAttribute\n```"
  }
]
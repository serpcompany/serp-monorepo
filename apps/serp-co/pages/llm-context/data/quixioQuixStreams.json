[
  {
    "owner": "quixio",
    "repo": "quix-streams",
    "content": "TITLE: Count-Based Sliding Window Aggregation with Quix Streams (Python)\nDESCRIPTION: Shows the implementation of a sliding window based on number of records using Quix Streams with Python. Specifically, a count-based sliding window of size 3 aggregates the average purchase amount. Utilizes Mean for aggregation and .final() to emit results only when a window is closed. Requires quixstreams, proper dataframe initialization, and appropriate data fields. Inputs are purchase events, outputs are averages for each window, suitable for customer purchase analytics.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\\nfrom quixstreams import Application\\nfrom quixstreams.dataframe.windows import Mean\\n\\napp = Application(...)\\nsdf = app.dataframe(...)\\n\\n\\nsdf = (\\n    # Define a count-based sliding window of 3 events\\n    .sliding_count_window(count=3)\\n\\n    # Specify the \\\"mean\\\" aggregate function\\n    .agg(average=Mean(\\\"amount\\\"))\\n\\n    # Emit updates once the window is closed\\n    .final()\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Custom Late Event Handling in Quix Streams Tumbling Windows (Python)\nDESCRIPTION: Illustrates how to define a custom callback for late/out-of-order events with Quix Streams windowed aggregations in Python. The on_late callback receives detailed event/window metadata and allows user-defined actions (e.g., logging or dead-lettering) and control over logging default behavior. The tumbling_window method takes the callback and window duration. Dependencies: quixstreams, proper dataframe setup. Useful to customize error handling for production-grade data streams.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\\n\\nfrom datetime import timedelta\\nfrom quixstreams import Application\\n\\napp = Application(...)\\nsdf = app.dataframe(...)\\n\\ndef on_late(\\n    value: Any,         # Record value\\n    key: Any,           # Record key\\n    timestamp_ms: int,  # Record timestamp\\n    late_by_ms: int,    # How late the record is in milliseconds\\n    start: int,         # Start of the target window\\n    end: int,           # End of the target window\\n    name: str,          # Name of the window state store\\n    topic: str,         # Topic name\\n    partition: int,     # Topic partition\\n    offset: int,        # Message offset\\n) -> bool:\\n    \\\"\\\"\\\"\\n    Define a callback to react on late records coming into windowed aggregations.\\n    Return `False` to suppress the default logging behavior.\\n    \\\"\\\"\\\"\\n    print(f\\\"Late message is detected at the window {(start, end)}\\\")\\n    return False\\n\\n# Define a 1-hour tumbling window and provide the \\\"on_late\\\" callback to it\\nsdf.tumbling_window(timedelta(hours=1), on_late=on_late)\\n\\n\\n# Start the application\\nif __name__ == '__main__':\\n    app.run()\\n\n```\n\n----------------------------------------\n\nTITLE: Producing Data to Kafka with Quix Streams in Python\nDESCRIPTION: Python script to create a Quix Streams application that produces chat messages to a Kafka topic. It demonstrates topic creation, message serialization, and producing events to Kafka.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/quickstart.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Create an Application - the main configuration entry point\napp = Application(broker_address=\"localhost:9092\", consumer_group=\"text-splitter-v1\")\n\n# Define a topic with chat messages in JSON format\nmessages_topic = app.topic(name=\"messages\", value_serializer=\"json\")\n\nmessages = [\n    {\"chat_id\": \"id1\", \"text\": \"Lorem ipsum dolor sit amet\"},\n    {\"chat_id\": \"id2\", \"text\": \"Consectetur adipiscing elit sed\"},\n    {\"chat_id\": \"id1\", \"text\": \"Do eiusmod tempor incididunt ut labore et\"},\n    {\"chat_id\": \"id3\", \"text\": \"Mollis nunc sed id semper\"},\n]\n\n\ndef main():\n    with app.get_producer() as producer:\n        for message in messages:\n            # Serialize chat message to send it to Kafka\n            # Use \"chat_id\" as a Kafka message key\n            kafka_msg = messages_topic.serialize(key=message[\"chat_id\"], value=message)\n\n            # Produce chat message to the topic\n            print(f'Produce event with key=\"{kafka_msg.key}\" value=\"{kafka_msg.value}\"')\n            producer.produce(\n                topic=messages_topic.name,\n                key=kafka_msg.key,\n                value=kafka_msg.value,\n            )\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Collecting Reversed Events in Tumbling Windows with Quix Streams (Python)\nDESCRIPTION: This code snippet demonstrates how to create a custom Collector subclass in Python using Quix Streams. It defines a ReversedCollect class that collects all events in a 10-minute tumbling window, then reverses their order before emitting the result. The Application and DataFrame objects are initialized, and the .agg API is used to register the collector, while .final triggers emitting only on window closure. Dependencies include the quixstreams library and compatible DataFrame/windowing API. Inputs include a stream of events, and outputs are window-aggregated event lists in reversed order for each time window. This approach is limited by memory proportional to the number of events per window.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/aggregations.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows.aggregations import Collector\n\napp = Application(...)\nsdf = app.dataframe(...)\n\nclass ReversedCollect(Collector):\n    def result(self, items):\n        # items is the list of all collected item during the window\n        return list(reversed(items))\n\nsdf = (\n    # Define a tumbling window of 10 minutes\n    sdf.tumbling_window(timedelta(minutes=10))\n\n    # Collect events in the window into a reversed list\n    .agg(events=ReversedCollect())\n\n    # Emit results only for closed windows\n    .final()\n)\n# Output:\n# {\n#   'start': <window start>, \n#   'end': <window end>, \n#   'events': [eventN, ..., event3, event2, event1] - reversed list of all events in the window\n# }\n```\n\n----------------------------------------\n\nTITLE: Transforming Streamed Temperature Data to Celsius and Publishing with Quix Streams (Python)\nDESCRIPTION: This example illustrates connecting to Kafka topics using Quix Streams, building a StreamingDataFrame pipeline that converts incoming temperature readings from Fahrenheit to Celsius, and publishing the transformed result. It demonstrates configuring topic sources with JSON serialization, using StreamingDataFrame.apply for transformation, printing to console, then publishing to an output topic, and finally running the pipeline. Required dependencies are the quixstreams Python library and an operational Kafka broker.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Define an application and input topic with JSON deserialization\napp = Application(broker_address='localhost:9092')\ninput_topic = app.topic('temperature', value_deserializer='json')\noutput_topic = app.topic('temperature-celsius', value_deserializer='json')\n\n# Create StreamingDataFrame and connect it to the input topic \nsdf = app.dataframe(topic=input_topic)\n\nsdf = (\n    # Convert the temperature value from °F to °C\n    # E.g. {\"tempF\": 68} will become {\"tempC\": 20}\n    sdf.apply(lambda value: {'tempC': (value['tempF'] - 32) * 5 / 9})\n\n    # Print the result to the console\n    .update(print)\n)\n\n# Publish data to the output topic\nsdf = sdf.to_topic(output_topic)\n\n# Run the pipeline\napp.run()\n```\n\n----------------------------------------\n\nTITLE: Initializing ProtobufSerializer Class in Python\nDESCRIPTION: Defines the constructor for the ProtobufSerializer to serialize Python dictionaries or message objects into Protocol Buffers binary format. Dependencies include the protobuf library and optional schema registry configuration. Parameters allow configuring deterministic serialization, ignoring unknown fields, and customizing serialization via SchemaRegistryClientConfig or SchemaRegistrySerializationConfig; expects a protobuf message class and returns serialized data suitable for wire transmission.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n    msg_type: Type[Message],\n    deterministic: bool = False,\n    ignore_unknown_fields: bool = False,\n    schema_registry_client_config: Optional[SchemaRegistryClientConfig] = None,\n    schema_registry_serialization_config: Optional[\n        SchemaRegistrySerializationConfig] = None)\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Validation Configuration\nDESCRIPTION: Demonstrates how to implement JSON schema validation with serialization. Includes schema definition and configuration for both input and output topics.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/serialization.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.models import JSONDeserializer, JSONSerializer\n\nMY_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"id\": {\"type\": \"number\"},\n    },\n    \"required\": [\"id\"],\n}\n\napp = Application(broker_address='localhost:9092', consumer_group='consumer')\ninput_topic = app.topic('input', value_deserializer=JSONDeserializer(schema=MY_SCHEMA))\noutput_topic = app.topic('output', value_serializer=JSONSerializer(schema=MY_SCHEMA))\n```\n\n----------------------------------------\n\nTITLE: Initializing and Chaining StreamingDataFrame Transformations with Quix Streams (Python)\nDESCRIPTION: This snippet demonstrates initializing a Quix Streams Application, creating a StreamingDataFrame, and chaining transformation operations using the Python API. It follows the recommended 'reassignment' pattern to ensure that each transformation is correctly applied to the pipeline. Dependencies include the quixstreams library, and parameters such as the broker address and data sources are set on app instantiation. Input streams are created from topics, and transformations (here with lambda functions incrementing values) are chained before starting the application.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Initialize the Appication\napp = Application(...)\n\n# Create a StreamingDataFrame\nsdf = app.dataframe()\n\n# re-assign sdf with an added operation\nsdf = sdf.apply(lambda x: x + 1)\n\n# add two more operations at once (\"chaining\" operations)\nsdf = sdf.apply(lambda x: x + 2).apply(lambda x: x + 3)\n\n# Run the application\nif __name__ == '__main__':\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Using State Store for Persistent Processing in Quix Streams\nDESCRIPTION: Demonstrates how to use the State Store for maintaining persistent state during stream processing. The example calculates and tracks the maximum observed temperature across messages with the same key.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import State\n\nsdf = app.dataframe(...)\n\n\ndef add_max_temperature(value: dict, state: State):\n    \"\"\"\n    Calculate max observed temperature and add it to the current value\n    \"\"\"\n    current_max = state.get('max_temperature')\n    if current_max is None:\n        max_temperature = value['temperature']\n    else:\n        max_temperature = max(value['temperature'], current_max)\n    state.set('max_temperature', max_temperature)\n    value['max_temperature'] = max_temperature\n\n\nsdf = sdf.update(add_max_temperature, stateful=True)\n```\n\n----------------------------------------\n\nTITLE: Consuming and Processing Kafka Data with Quix Streams in Python\nDESCRIPTION: Python script to create a Quix Streams application that consumes messages from a Kafka topic, processes them by splitting text into words, and calculates word lengths. It demonstrates creating a StreamingDataFrame, applying transformations, and running the streaming application.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/quickstart.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Create an Application - the main configuration entry point\napp = Application(\n    broker_address=\"localhost:9092\",\n    consumer_group=\"text-splitter-v1\",\n    auto_offset_reset=\"earliest\",\n)\n\n# Define a topic with chat messages in JSON format\nmessages_topic = app.topic(name=\"messages\", value_deserializer=\"json\")\n\n# Create a StreamingDataFrame - the stream processing pipeline\n# with a Pandas-like interface on streaming data\nsdf = app.dataframe(topic=messages_topic)\n\n# Print the input data\nsdf = sdf.update(lambda message: print(f\"Input:  {message}\"))\n\n# Define a transformation to split incoming sentences\n# into words using a lambda function\nsdf = sdf.apply(\n    lambda message: [{\"text\": word} for word in message[\"text\"].split()],\n    expand=True,\n)\n\n# Calculate the word length and store the result in the column\nsdf[\"length\"] = sdf[\"text\"].apply(lambda word: len(word))\n\n# Print the output result\nsdf = sdf.update(lambda row: print(f\"Output: {row}\"))\n\n# Run the streaming application\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Assigning New Columns with StreamingDataFrame.apply() in Python\nDESCRIPTION: Illustrates how `StreamingDataFrame.apply()` can compute a value based on existing dictionary keys ('sum' and 'count') and assign it to a new column ('average') within the `StreamingDataFrame`. This is useful for deriving new fields.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Calculate an average value of some metric using \"sum\" and \"count\" columns\nsdf['average'] = sdf.apply(lambda value: value['sum'] / value['count'])\n```\n```\n\n----------------------------------------\n\nTITLE: Transforming Tabular Input Data to Dictionary Form with StreamingDataFrame.apply (Python)\nDESCRIPTION: This code converts tabular data, received as columns and values lists within a single dictionary, into a flattened dictionary mapping each column to its corresponding value. It uses StreamingDataFrame.apply with a lambda function utilizing zip, making it useful for reformatting or normalizing record structures during stream processing. All transformations further down the pipeline will use the resulting dictionary. Requires incoming data shaped as {'columns': [...], 'values': [...]}.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(...)\n\n# Convert tabular data to dictionaries\nsdf = sdf.apply(\n    lambda data: {column: value for column, value in\n                  zip(data['columns'], data['values'])}\n)\n```\n\n----------------------------------------\n\nTITLE: Expanding Collections with StreamingDataFrame.apply(expand=True) in Python\nDESCRIPTION: Explains how to use `StreamingDataFrame.apply()` with the `expand=True` argument to flatten a collection (like a list returned by splitting a sentence) returned by the function. Each item in the collection (each word) is then processed individually by subsequent operations (like calculating length) in the stream pipeline.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Split a sentence into words\nsdf = sdf.apply(lambda sentence: sentence.split(' '), expand=True)\n# Get the length of each word\nsdf = sdf.apply(lambda word: len(word))\n```\n```\n\n----------------------------------------\n\nTITLE: Creating a Streaming DataFrame with Quix Streams (Python)\nDESCRIPTION: Defines a function that constructs a StreamingDataFrame, serving as the foundation for a message processing pipeline. The function accepts either a Topic or a BaseSource as input, directing streaming data accordingly. If both topic and source are provided, the source writes to the topic, and the StreamingDataFrame consumes from it. Dependencies: quixstreams and its StreamingDataFrame, Topic, BaseSource objects. Inputs: optional Topic and/or BaseSource. Output: a StreamingDataFrame instance. Limitation: Only one of topic or source is required; both may modify behavior.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef dataframe(topic: Optional[Topic] = None,\n              source: Optional[BaseSource] = None) -> StreamingDataFrame\n```\n\n----------------------------------------\n\nTITLE: Basic CSV Source Integration with Quix Streams\nDESCRIPTION: Demonstrates how to use a CSVSource to read data from a CSV file and process it with a StreamingDataframe. The example creates an Application instance, initializes a CSVSource, and sets up a dataframe to process and print the data.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources import CSVSource\n\ndef main():\n    app = Application()\n    source = CSVSource(path=\"input.csv\")\n    \n    sdf = app.dataframe(source=source)\n    sdf.print(metadata=True)\n    \n    app.run()\n    \nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Avro Serialization Configuration\nDESCRIPTION: Shows implementation of Apache Avro serialization with schema definition. Requires fastavro library and demonstrates setup for both serialization and deserialization.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/serialization.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.models.serializers.avro import AvroSerializer, AvroDeserializer\n\nMY_SCHEMA = {\n    \"type\": \"record\",\n    \"name\": \"testschema\",\n    \"fields\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"id\", \"type\": \"int\", \"default\": 0},\n    ],\n}\n\napp = Application(broker_address='localhost:9092', consumer_group='consumer')\ninput_topic = app.topic('input', value_deserializer=AvroDeserializer(schema=MY_SCHEMA))\noutput_topic = app.topic('output', value_serializer=AvroSerializer(schema=MY_SCHEMA))\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running a Basic Streaming Pipeline (Quix Streams, Python)\nDESCRIPTION: Demonstrates initializing an Application, creating a topic and a StreamingDataFrame, applying a lambda operation to streaming messages, and launching the processing pipeline. The workflow shows how to use Quix Streams to consume messages and perform custom operations. Dependencies: quixstreams Application and StreamingDataFrame. Inputs: broker address, consumer group, topic name, and a function to process each value/context. Outputs: side-effects from processing and printed output. Constraints: Code presupposes a running Kafka broker and accessible topic.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Set up an `app = Application` and  `sdf = StreamingDataFrame`;\n# add some operations to `sdf` and then run everything.\n\napp = Application(broker_address='localhost:9092', consumer_group='group')\ntopic = app.topic('test-topic')\ndf = app.dataframe(topic)\ndf.apply(lambda value, context: print('New message', value)\n\napp.run()\n```\n\n----------------------------------------\n\nTITLE: Implementing Time-based Tumbling Windows in Quix Streams\nDESCRIPTION: Shows how to create a time-based tumbling window of 1 hour to calculate average temperature from sensor readings. The example uses the Mean aggregate function and emits results for every incoming message.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows import Mean\n\napp = Application(...)\nsdf = app.dataframe(...)\n\n\nsdf = (\n    # Define a tumbling window of 1 hour\n    # You can also pass duration_ms as an integer of milliseconds\n    .tumbling_window(duration_ms=timedelta(hours=1))\n\n    # Specify the \"mean\" aggregate function\n    .agg(avg_temperature=Mean(\"temperature\"))\n\n    # Emit updates for each incoming message\n    .current()\n)\n```\n\n----------------------------------------\n\nTITLE: Example Usage of StreamingDataFrame.compose in Python\nDESCRIPTION: Demonstrates how to use the `compose` method on a `StreamingDataFrame`. After defining a dataframe and applying functions like `apply` and `filter`, `compose` is called to create an optimized function. This resulting function (`sdf` in the example) can then be invoked directly with input records.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nsdf = app.dataframe()\nsdf = sdf.apply(apply_func)\nsdf = sdf.filter(filter_func)\nsdf = sdf.compose()\n\nresult_0 = sdf({\"my\": \"record\"})\nresult_1 = sdf({\"other\": \"record\"})\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using InfluxDB3Sink with Quix Streams - Python\nDESCRIPTION: Shows how to import and instantiate an InfluxDB3Sink with essential configuration for use with a Quix Streams application. This snippet includes broker initialization, topic creation, sink instantiation (with parameters for authentication, database, measurement, fields, and tags), binding the sink to a streaming dataframe, and running the application. Dependencies include the quixstreams package with InfluxDB3 extras. Key parameters are documented in code comments and must be supplied by the user, such as token, host, organization, and database. Input records must be dicts; outputs are batched and persisted to InfluxDB v3.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/influxdb3-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.core.influxdb3 import InfluxDB3Sink\n\napp = Application(broker_address=\"localhost:9092\")\ntopic = app.topic(\"numbers-topic\")\n\n# Initialize InfluxDB3Sink\ninflux_sink = InfluxDB3Sink(\n    token=\"<influxdb-access-token>\",\n    host=\"<influxdb-host>\",\n    organization_id=\"<influxdb-org>\",\n    database=\"<influxdb-database>\",\n    measurement=\"numbers\",\n    fields_keys=[\"number\"],\n    tags_keys=[\"tag\"]\n)\n\nsdf = app.dataframe(topic)\n# Do some processing here ...\n# Sink data to InfluxDB\nsdf.sink(influx_sink)\n\nif __name__ == '__main__':\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Adding Derived Columns and Mutating Streaming Records with StreamingDataFrame (Python)\nDESCRIPTION: This snippet demonstrates two approaches for adding a derived temperature column to streaming dictionary records: 1) Assigning columns directly or via apply to compute a new field, and 2) using update with a custom function that mutates the dictionary in place. In both cases, dependencies include a properly configured StreamingDataFrame with mapping values. The 'temperatureF' field is calculated from Celsius input, and approaches are shown for both direct calculations and in-place mutation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(...)\n\n# Add a new column with temperature in Fahrenheit\n# Input: {'temperature': 9}\nsdf['temperatureF'] = (sdf['temperature'] * 9 / 5) + 32\n# Output: {'temperature': 9, 'temperatureF': 48.2}\n\n# The same can be done by assigning columns and .apply()\n# Note: here function passed to .apply() will only get \"temperature\" as an argument\nsdf['temperatureF'] = sdf['temperature'].apply(lambda temp: (temp * 9 / 5) + 32)\n\n# You can also assign the result of StreamingDataFrame.apply() to a column\nsdf['temperatureF'] = sdf.apply(lambda value: (value['temperature'] * 9 / 5) + 32)\n```\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(...)\n\ndef add_fahrenheit(value):\n    \"\"\"\n    Add a new column with temperature in Fahrenheit\n    \n    Note that this function doesn't return anything and only mutates the incoming value\n    \"\"\"\n    value['temperatureF'] = (value['temperature'] * 9 / 5) + 32\n\n# Input: {'temperature': 9}\nsdf = sdf.update(add_fahrenheit)\n# Output: {'temperature': 9, 'temperatureF': 48.2}\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Processing with Custom Functions in Python\nDESCRIPTION: This snippet demonstrates how to use state in custom functions with Quix Streams. It shows how to create an Application, define a topic, and apply a stateful function to count messages.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/stateful-processing.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application, State\napp = Application(\n    broker_address='localhost:9092', \n    consumer_group='consumer', \n)\ntopic = app.topic('topic')\n\nsdf = app.dataframe(topic)\n\ndef count_messages(value: dict, state: State):\n    total = state.get('total', default=0)\n    total += 1\n    state.set('total', total)\n    return {**value, 'total': total}\n    \n    \n# Apply a custom function and inform StreamingDataFrame \n# to provide a State instance to it using \"stateful=True\"\nsdf = sdf.apply(count_messages, stateful=True)\n```\n\n----------------------------------------\n\nTITLE: Stateful Data Transformation Example - Python\nDESCRIPTION: Demonstrates using stateful operations to store values and transform data by capitalizing string values and filtering columns.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef func(d: dict, state: State):\n    value = d[\"store_field\"]\n    if value != state.get(\"my_store_key\"):\n        state.set(\"my_store_key\") = value\n    return {k: v.upper() if isinstance(v, str) else v for k, v in d.items()}\n\nsdf = StreamingDataFrame()\nsdf = sdf.apply(func, stateful=True)\nsdf = sdf.apply(lambda d: {k: v for k,v in d.items() if isinstance(v, str)})\n```\n\n----------------------------------------\n\nTITLE: Complete Example of Producing Data to Kafka with Quix Streams in Python\nDESCRIPTION: This is a complete example that combines all the previous steps. It demonstrates creating an Application, defining a Topic, and using a Producer to send a message to a Kafka topic using Quix Streams.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/producer.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create an Application instance with Kafka configs\nfrom quixstreams import Application\n\n\napp = Application(\n    broker_address='localhost:9092', consumer_group='example'\n)\n\n# Define a topic \"my_topic\" with JSON serialization\ntopic = app.topic(name='my_topic', value_serializer='json')\n\nevent = {\"id\": \"1\", \"text\": \"Lorem ipsum dolor sit amet\"}\n\n# Create a Producer instance\nwith app.get_producer() as producer:\n    \n    # Serialize an event using the defined Topic \n    message = topic.serialize(key=event[\"id\"], value=event)\n    \n    # Produce a message into the Kafka topic\n    producer.produce(\n        topic=topic.name, value=message.value, key=message.key\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Schema Registry Client in Python for Quix Streams\nDESCRIPTION: Sets up the Schema Registry client configuration and optional serialization configuration for use with Quix Streams serializers and deserializers.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/schema-registry.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams.models import (\n    SchemaRegistryClientConfig,\n    SchemaRegistrySerializationConfig,\n)\n\nschema_registry_client_config = SchemaRegistryClientConfig(\n    url='localhost:8081',\n    basic_auth_user_info='username:password',\n)\n\n# optional and depends on serialization type \nschema_registry_serialization_config = SchemaRegistrySerializationConfig(\n    auto_register_schemas=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 File Source with Quix Streams Application\nDESCRIPTION: Example of setting up a Quix Streams application that reads data from S3 using FileSource. Shows configuration of S3Origin with AWS credentials and region, and FileSource with directory path, format, and compression settings.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/amazon-s3-source.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources.community.file import FileSource\nfrom quixstreams.sources.community.file.origins import S3Origin\n\napp = Application(broker_address=\"localhost:9092\", auto_offset_reset=\"earliest\")\n\norigin = S3Origin(\n    bucket=\"<YOUR BUCKET NAME>\",\n    aws_access_key_id=\"<YOUR KEY ID>\",\n    aws_secret_access_key=\"<YOUR SECRET KEY>\",\n    region_name=\"<YOUR REGION>\",\n)\nsource = FileSource(\n    directory=\"path/to/your/topic_folder/\",\n    origin=origin,\n    format=\"json\",\n    compression=\"gzip\",\n)\nsdf = app.dataframe(source=source).print(metadata=True)\n# YOUR LOGIC HERE!\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Implementing Kafka Replicator Source in Python using Quix Streams\nDESCRIPTION: This code snippet demonstrates how to create and use a KafkaReplicatorSource instance in a Quix Streams application. It sets up the source to read from a specified Kafka topic and broker, and then prints the data including metadata.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/kafka-source.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources.core.kafka import KafkaReplicatorSource\n\ndef main():\n    app = Application()\n    source = KafkaReplicatorSource(\n      name=\"my-source\",\n      app_config=app.config,\n      topic=\"source-topic\",\n      broker_address=\"source-broker-address\"\n    )\n\n    sdf = app.dataframe(source=source)\n    sdf.print(metadata=True)\n\n    app.run()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Producing Data to Kafka Topics with Quix Streams in Python\nDESCRIPTION: Illustrates how to publish messages from a `StreamingDataFrame` to a Kafka topic using `to_topic()`. It shows defining input/output topics with appropriate serializers/deserializers and optionally providing a `key` function to dynamically set the outgoing message key based on the message value (e.g., using 'location_id' instead of the original key).\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom quixstreams import Application\n\napp = Application(broker_address='localhost:9092', consumer_group='consumer')\n\n# Define an input topic and deserialize message keys to strings\ninput_topic = app.topic(\"input\", key_deserializer='str')\n\n# Define an output topic and serialize keys from strings to bytes\noutput_topic = app.topic(\"output\", key_serializer='str')\n\nsdf = app.dataframe(input_topic)\n\n# Publish the consumed message to the output topic with the same key (sensor ID)\nsdf = sdf.to_topic(output_topic)\n\n# Publish the consumed message to the topic, but use the location ID as a new message key \nsdf = sdf.to_topic(output_topic, key=lambda value: str(value[\"location_id\"]))\n```\n```\n\n----------------------------------------\n\nTITLE: Filtering Values Using StreamingDataFrame.apply() Alternative Syntax in Python\nDESCRIPTION: Re-illustrates filtering using the `sdf[sdf.apply()]` syntax as an alternative to `sdf.filter()`. The provided lambda function checks if 'field_a' is greater than 0, and its boolean result determines whether the value is kept or discarded.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Filter out values with \"field_a\" <= 0 using .apply() syntax\nsdf = sdf[sdf.apply(lambda value: value['field_a'] > 0)]\n```\n```\n\n----------------------------------------\n\nTITLE: Processing Data from Kafka with Quix Streams\nDESCRIPTION: A complete example showing how to process temperature data from a Kafka topic, convert it from Celsius to Fahrenheit, filter values above a threshold, and produce alerts to another topic. Demonstrates the core Application and StreamingDataFrame APIs.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# A minimal application reading temperature data in Celsius from the Kafka topic,\n# converting it to Fahrenheit and producing alerts to another topic.\n\n# Define an application that will connect to Kafka\napp = Application(\n    broker_address=\"localhost:9092\",  # Kafka broker address\n)\n\n# Define the Kafka topics\ntemperature_topic = app.topic(\"temperature-celsius\", value_deserializer=\"json\")\nalerts_topic = app.topic(\"temperature-alerts\", value_serializer=\"json\")\n\n# Create a Streaming DataFrame connected to the input Kafka topic\nsdf = app.dataframe(topic=temperature_topic)\n\n# Convert temperature to Fahrenheit by transforming the input message (with an anonymous or user-defined function)\nsdf = sdf.apply(lambda value: {\"temperature_F\": (value[\"temperature\"] * 9/5) + 32})\n\n# Filter values above the threshold\nsdf = sdf[sdf[\"temperature_F\"] > 150]\n\n# Produce alerts to the output topic\nsdf = sdf.to_topic(alerts_topic)\n\n# Run the streaming application (app automatically tracks the sdf!)\napp.run()\n```\n\n----------------------------------------\n\nTITLE: Serializing Event Data with QuixEventsSerializer Python\nDESCRIPTION: Defines the QuixEventsSerializer class (subclassing QuixSerializer), which serializes event data into the Quix EventData JSON format. Input must be a dict containing 'Id', 'Value', and 'Tags'; other fields are ignored. Ensures output JSON includes a timestamp and the three required fields, facilitating standardized storage and streaming in Quix. Non-matching or extra fields are omitted in the output.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass QuixEventsSerializer(QuixSerializer)\n```\n\n----------------------------------------\n\nTITLE: Kafka Messages After Multi-Column GroupBy Aggregation - Python\nDESCRIPTION: Provides sample outputs for a multi-column group_by with aggregation, keyed by a composite of store_id and item. Input is aggregated stream data; output is total_quantity per store_id--item key as a dict. Used to illustrate aggregation output following custom key group_by.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/groupby.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n{\"key\": \"store_2--A\", \"value\": {\"total_quantity\": 11}}\n{\"key\": \"store_4--A\", \"value\": {\"total_quantity\": 13}}\n# ...etc...\n{\"key\": \"store_4--B\", \"value\": {\"total_quantity\": 9}}\n{\"key\": \"store_2--A\", \"value\": {\"total_quantity\": 20}}\n# ...etc...\n```\n\n----------------------------------------\n\nTITLE: Selecting and Projecting Columns from Streaming Data (Python)\nDESCRIPTION: This code demonstrates selecting specific columns from incoming streaming messages using the pandas-like API of StreamingDataFrame, or by using the apply method. It extracts only the required columns (e.g., 'temperature' and 'timestamp') from each record, minimizing data sent downstream. This approach requires incoming values to be dictionaries/mapping types; otherwise, the apply method with a custom function is recommended.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(...)\n# Selecting only \"temperature\" and \"timestamp\" columns using pandas-like approach\nsdf = sdf[[\"temperature\", \"timestamp\"]]\n\n# The same can be done using .apply() with a custom function\nsdf = sdf.apply(lambda value: {'temperature': value['temperature'],\n                               'timestamp': value['timestamp']})\n```\n\n----------------------------------------\n\nTITLE: Multiple Aggregations in Quix Streams\nDESCRIPTION: This example shows how to perform multiple aggregations over the same window in Quix Streams. It calculates min, max, average temperature, total event count, and retrieves the latest sensor name for each 10-minute tumbling window.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/aggregations.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows import Min, Max, Count, Mean, Latest\n\napp = Application(...)\nsdf = app.dataframe(...)\n\n# Input:\n# {\"temperature\" : 9999, \"sensor\": \"my sensor\"}\n\nsdf = (\n    \n    # Define a tumbling window of 10 minutes\n    sdf.tumbling_window(timedelta(minutes=10))\n\n    .agg(\n        min_temp=Min(\"temperature\"),\n        max_temp=Max(\"temperature\"),\n        avg_temp=Mean(\"temperature\"),\n        total_events=Count(),\n        sensor=Latest(\"sensor\")  # Propagate the sensor name\n    )\n\n    # Emit results only for closed windows\n    .final()\n)\n\n# Output:\n# {\n#   'start': <window start>, \n#   'end': <window end>, \n#   'min_temp': 1,\n#   'max_temp': 999,\n#   'avg_temp': 34.32,\n#   'total_events': 999,\n#   'sensor': 'my sensor',\n# }\n```\n\n----------------------------------------\n\nTITLE: Implementing Kafka Producer with Quix Streams Application\nDESCRIPTION: Example demonstrating how to create and use a Producer object through Application instance to produce messages to a Kafka topic. Shows message creation and production using context manager interface.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/producer-consumer-lowlevel.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Configure an Application. \n# The config params will be used for the Consumer instance too.\napp = Application(broker_address='localhost:9092')\n\n# Create some messages to produce\nmessages = [\n  {'key': b'key1', 'value': b'value1'},\n  {'key': b'key2', 'value': b'value2'},\n]\n\n# Create a producer and start producing messages\nwith app.get_producer() as producer:\n    for message in messages:\n        producer.produce(topic='my-topic', key=message['key'], value=message['value'])\n```\n\n----------------------------------------\n\nTITLE: Implementing a Stateful Random Number Source in Quix Streams (Python)\nDESCRIPTION: Provides an example implementation of a `StatefulSource` named `RandomNumbersSource`. This source generates random numbers, stores the last generated number in its state using `self.state.set()`, retrieves the previous state using `self.state.get()`, produces the sum of the current and previous numbers to Kafka using `self.produce()`, and periodically flushes its state using `self.flush()`. The example also shows how to integrate this source into a Quix Streams `Application` and print the resulting stream.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport random\nimport time\n\nfrom quixstreams import Application\nfrom quixstreams.sources import StatefulSource\n\n\nclass RandomNumbersSource(StatefulSource):\n    def run(self):\n\n        i = 0\n        while self.running:\n            previous = self.state.get(\"number\", 0)\n            current = random.randint(0, 100)\n            self.state.set(\"number\", current)\n\n            serialized = self._producer_topic.serialize(value=current + previous)\n            self.produce(key=str(current), value=serialized.value)\n            time.sleep(0.5)\n\n            # flush the state every 10 messages\n            i += 1\n            if i % 10 == 0:\n                self.flush()\n\n\ndef main():\n    app = Application(broker_address=\"localhost:9092\")\n    source = RandomNumbersSource(name=\"random-source\")\n\n    sdf = app.dataframe(source=source)\n    sdf.print(metadata=True)\n\n    app.run()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n```\n\n----------------------------------------\n\nTITLE: GroupBy with Tumbling Window Aggregation - Python\nDESCRIPTION: Demonstrates grouping by 'item' column then applying a tumbling window aggregation that sums quantity for each item over a fixed window (e.g., 1 hour). Relies on StreamingDataFrame, the agg.Sum() aggregator, and a functioning windowing mechanism. Inputs are streaming orders by item and quantity; outputs are total quantities for each item in the specified time window. Only one group_by can be used per StreamingDataFrame.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/groupby.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsdf = StreamingDataFrame()\nsdf = sdf.group_by(\"item\")\nsdf = sdf.tumbling_window(duration_ms=3600).agg(total_quantity=agg.Sum()).final()\n```\n\n----------------------------------------\n\nTITLE: Defining a Time-Based Hopping Window on StreamingDataFrame in Python\nDESCRIPTION: Defines the `hopping_window` method for a `StreamingDataFrame`. This method creates a time-based hopping window configuration, allowing for stateful aggregations over overlapping time intervals defined by `duration_ms` and `step_ms`. It accepts duration, step, grace period (all in milliseconds or as `timedelta`), an optional name, and an optional callback for late events. It returns a `HoppingTimeWindowDefinition` object.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef hopping_window(\n    duration_ms: Union[int, timedelta],\n    step_ms: Union[int, timedelta],\n    grace_ms: Union[int, timedelta] = 0,\n    name: Optional[str] = None,\n    on_late: Optional[WindowOnLateCallback] = None\n) -> HoppingTimeWindowDefinition\n```\n\n----------------------------------------\n\nTITLE: Consuming Multiple Topics with Applications in Python using Quix Streams\nDESCRIPTION: This code snippet demonstrates how to consume multiple topics using an Application in Quix Streams. It initializes an Application, creates topics, sets up StreamingDataFrames for each input topic, applies transformations, and sends the results to an output topic.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/consuming-multiple-topics.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\napp = Application(\"localhost:9092\")\ninput_topic_a = app.topic(\"input_a\")\ninput_topic_b = app.topic(\"input_b\")\noutput_topic = app.topic(\"output\")\n\nsdf_a = app.dataframe(input_topic_a)\nsdf_a = sdf_a.apply(func_x).to_topic(output_topic)\n\nsdf_b = app.dataframe(input_topic_b)\nsdf_b.update(func_y).to_topic(output_topic)\n\napp.run()\n```\n\n----------------------------------------\n\nTITLE: Implementing Kafka Consumer with Quix Streams Application\nDESCRIPTION: Example showing how to create and use a Consumer object through Application instance to poll messages from a Kafka topic. Demonstrates subscription, message polling, error handling, and offset management for at-least-once delivery guarantees.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/producer-consumer-lowlevel.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Configure an Application. \n# The config params will be used for the Consumer instance too.\napp = Application(\n    broker_address='localhost:9092', \n    auto_offset_reset='earliest', \n    auto_commit_enable=True,\n)\n\n# Create a consumer and start a polling loop\nwith app.get_consumer() as consumer:\n    consumer.subscribe(topics=['my-topic'])\n\n    while True:\n        msg = consumer.poll(0.1)\n        if msg is None:\n            continue\n        elif msg.error():\n            print('Kafka error:', msg.error())\n            continue\n\n        value = msg.value()\n        # Do some work with the value here ...\n        \n        # Store the offset of the processed message on the Consumer \n        # for the auto-commit mechanism.\n        # It will send it to Kafka in the background.\n        # Storing offset only after the message is processed enables at-least-once delivery\n        # guarantees.\n        consumer.store_offsets(message=msg)\n```\n\n----------------------------------------\n\nTITLE: Example of Tumbling Count Window Aggregation in Python\nDESCRIPTION: Illustrates how to define and use a tumbling count window on a `StreamingDataFrame`. It creates a window that groups messages in batches of 10 using `tumbling_count_window`. An aggregation function (`agg.Sum()`) is applied using `.agg()`, and the `.current()` method specifies that results should be emitted as soon as they are updated within the window. Depends on `quixstreams.Application` and `quixstreams.dataframe.windows.aggregations`.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nimport quixstreams.dataframe.windows.aggregations as agg\n\napp = Application()\nsdf = app.dataframe(...)\nsdf = (\n    # Define a tumbling window of 10 messages\n    sdf.tumbling_count_window(count=10)\n    # Specify the aggregation function\n    .agg(value=agg.Sum())\n    # Specify how the results should be emitted downstream.\n    # \"current()\" will emit results as they come for each updated window,\n    # possibly producing multiple messages per key-window pair\n    # \"final()\" will emit windows only when they are closed and cannot\n    # receive any updates anymore.\n    .current()\n)\n```\n\n----------------------------------------\n\nTITLE: Stopping the Kafka Application Processing Loop (Quix Streams, Python)\nDESCRIPTION: Defines a method to manually stop the internal polling loop and streaming message processing for the Application. Intended for manual lifecycle management, especially in threaded scenarios. Dependency: Application instance. Key parameter: 'fail' determines if stop is due to error, affecting checkpointing. Input: optional boolean fail. Output: stops processing. Limitation: Generally not required unless fine-grained lifecycle control is needed; normally, SIGTERM or KeyboardInterrupt will suffice.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef stop(fail: bool = False)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Azure File Sink in Quix Streams Application\nDESCRIPTION: This code snippet demonstrates how to configure and use the Azure File Sink in a Quix Streams application. It creates a FileSink instance with AzureFileDestination, configures it to write JSON files to Azure, and sets up a streaming dataframe to use this sink.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/microsoft-azure-file-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.file import FileSink\nfrom quixstreams.sinks.community.file.destinations import AzureFileDestination\n\n\n# Configure the sink to write JSON files to Azure\nfile_sink = FileSink(\n    # Optional: defaults to current working directory\n    directory=\"data\",\n    # Optional: defaults to \"json\"\n    # Available formats: \"json\", \"parquet\" or an instance of Format\n    format=JSONFormat(compress=True),\n    destination=AzureFileDestination(\n        container=\"<YOUR CONTAINER NAME>\",\n        connection_string=\"<YOUR CONNECTION STRING>\",\n    )\n)\n\napp = Application(broker_address='localhost:9092', auto_offset_reset=\"earliest\")\ntopic = app.topic('sink-topic')\n\nsdf = app.dataframe(topic=topic)\nsdf.sink(file_sink)\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Topic Creation Example\nDESCRIPTION: Example demonstrating how to create input and output topics with custom serialization settings.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Specify an input and output topic for a `StreamingDataFrame` instance,\n# where the output topic requires adjusting the key serializer.\n\napp = Application()\ninput_topic = app.topic(\"input-topic\", value_deserializer=\"json\")\noutput_topic = app.topic(\n    \"output-topic\", key_serializer=\"str\", value_serializer=JSONSerializer()\n)\nsdf = app.dataframe(input_topic)\nsdf.to_topic(output_topic)\n```\n\n----------------------------------------\n\nTITLE: Implementing S3Destination for Amazon S3 Data Output - Python\nDESCRIPTION: Implements S3Destination, a concrete subclass of Destination, facilitating file output to Amazon S3 buckets. Utilizes the AWS SDK (boto3) and supports credential injection via arguments or environment variables (e.g., AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY). Manages all AWS authentication, path formatting, and error translation into custom exceptions for robust operation within cloud infrastructures.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nclass S3Destination(Destination)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Custom Source in Python\nDESCRIPTION: Example of implementing a basic custom source by subclassing Source class. The source reads lines from a file and produces messages to a Kafka topic.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/custom-sources.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams.sources.base import Source\n\nclass MySource(Source):\n    \n    def run(self):\n        with open(\"file.txt\", \"r\") as f:\n            while self.running:\n\n                line = f.readline()\n                if not line:\n                   return\n\n                msg = self.serialize(\n                    key=\"file.txt\",\n                    value=line.strip(),\n                )\n\n                self.produce(\n                    key=msg.key,\n                    value=msg.value,\n                )\n```\n\n----------------------------------------\n\nTITLE: Example of Tumbling Time Window Aggregation in Python\nDESCRIPTION: Illustrates how to define and use a tumbling time window on a `StreamingDataFrame`. It creates a 60-second window with a 10-second grace period using `tumbling_window`. An aggregation function (`agg.Sum()`) is applied using `.agg()`, and the `.current()` method specifies that results should be emitted as soon as they are updated within the window. Depends on `quixstreams.Application` and `quixstreams.dataframe.windows.aggregations`.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nimport quixstreams.dataframe.windows.aggregations as agg\n\napp = Application()\nsdf = app.dataframe(...)\n\nsdf = (\n    # Define a tumbling window of 60s and grace period of 10s\n    sdf.tumbling_window(\n        duration_ms=timedelta(seconds=60), grace_ms=timedelta(seconds=10.0)\n    )\n\n    # Specify the aggregation function\n    .agg(value=agg.Sum())\n\n    # Specify how the results should be emitted downstream.\n    # \"current()\" will emit results as they come for each updated window,\n    # possibly producing multiple messages per key-window pair\n    # \"final()\" will emit windows only when they are closed and cannot\n    # receive any updates anymore.\n    .current()\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Count-based Tumbling Windows in Quix Streams\nDESCRIPTION: Demonstrates how to create a count-based tumbling window to batch data from 3 messages before sending it to an external API. The example uses the Collect aggregate function and emits results once the window is closed.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport json\nimport urllib.request\n\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows import Collect\n\ndef external_api(value):\n    with urllib.request.urlopen(\"https://example.com\", data=json.dumps(value[\"data\"])) as rep:\n        return response.read()\n\napp = Application(...)\nsdf = app.dataframe(...)\n\n\nsdf = (\n    # Define a count-based tumbling window of 3 events\n    .tumbling_count_window(count=3)\n\n    # Specify the \"collect\" aggregate function\n    .agg(data=Collect())\n\n    # Emit updates once the window is closed\n    .final()\n)\n\n# Send a request to the external API\n# sdf.apply(external_api)\n```\n\n----------------------------------------\n\nTITLE: Defining the FileSource Abstract Base Class in Python\nDESCRIPTION: The FileSource abstract class provides an interface for extracting records from files and producing them as messages to Kafka. It handles file navigation, async file retrieval, decompression, deserialization, and replay timing.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nclass FileSource(Source)\n```\n\n----------------------------------------\n\nTITLE: Creating Sliding Count Window in StreamingDataFrame\nDESCRIPTION: Creates a count-based sliding window transformation on a StreamingDataFrame. Sliding windows continuously evaluate the stream with a fixed step of 1 message allowing for overlapping windows of a fixed size. They support stateful aggregations like sum, reduce, etc.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef sliding_count_window(\n        count: int,\n        name: Optional[str] = None) -> SlidingCountWindowDefinition\n```\n\n----------------------------------------\n\nTITLE: Using PandasDataFrameSource with Quix Streams\nDESCRIPTION: Complete example showing how to read a DataFrame from a CSV file and stream it to a Kafka topic. The example demonstrates configuring the key column, timestamp column, and naming the data source.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/pandas-source.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom quixstreams import Application\nfrom quixstreams.sources.community.pandas import PandasDataFrameSource\n\n\ndef main():\n    app = Application()\n    \n    # Read a DataFrame from a file.\n    # File format: \"Timestamp\", \"SessionID\", \"Metric\"\n    df = pd.read_csv(\"data.csv\")\n    \n    \n    # Create the PandasDataFrameSource instance and pass the df to it\n    source = PandasDataFrameSource(\n        df=df, # DataFrame to read data from \n        key_column=\"SessionID\",  # A column to be used for message keys\n        timestamp_column=\"Timestamp\", # A column to be used for message timestamps\n        name=\"data_csv-dataframe\" # The name will be included to the default topic name.\n    )\n        \n    # Pass the source instance to a StreamingDataFrame and start the application\n    sdf = app.dataframe(source=source)\n    sdf.print(metadata=True)\n\n    app.run()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Database Sink with Backpressure Handling in Python\nDESCRIPTION: This code snippet demonstrates how to create a custom database sink that extends BatchingSink. It includes setup for database connection, a write method for batch processing, and implements backpressure handling using SinkBackpressureError when a timeout occurs.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/custom-sinks.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams.sinks.base import BatchingSink, SinkBatch, SinkBackpressureError\n\n\nclass MyDatabaseSink(BatchingSink):\n    \"\"\"\n    Some sink writing data to a database\n    \"\"\"\n    def __init__(self, on_client_connect_success=None, on_client_connect_failure=None):\n        super().__init__(\n           on_client_connect_success=on_client_connect_success,\n           on_client_connect_failure=on_client_connect_failure\n        )\n        self._db_connection = None\n\n    def setup(self):\n        self._db_connection = my_database.connect('<connection credentials>')\n        self._db_connection.test()\n\n    def write(self, batch: SinkBatch):\n        # Simulate some DB connection here\n        data = [{'value': item.value} for item in batch]\n        try:\n            # Try to write data to the db\n            self._db_connection.write(data)\n        except TimeoutError:\n            # In case of timeout, tell the app to wait for 30s \n            # and retry the writing later\n            raise SinkBackpressureError(\n               retry_after=30.0, \n               topic=batch.topic, \n               partition=batch.partition,\n            )\n```\n\n----------------------------------------\n\nTITLE: Implementing BigQuery Sink in Quix Streams\nDESCRIPTION: Complete example showing how to configure and use the BigQuery sink with Quix Streams. Demonstrates setting up the application, configuring the sink with service account credentials, and implementing the data pipeline.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/google-cloud-bigquery-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.bigquery import BigQuerySink\n\napp = Application(\n    broker_address=\"localhost:9092\",\n    auto_offset_reset=\"earliest\",\n    consumer_group=\"consumer-group\",\n)\n\ntopic = app.topic(\"topic-name\")\n\n# Read the service account credentials in JSON format from some environment variable.\nservice_account_json = os.environ['BIGQUERY_SERVICE_ACCOUNT_JSON']\n\n# Initialize a sink\nbigquery_sink = BigQuerySink(\n    project_id=\"<project ID>\",\n    location=\"<location>\",\n    dataset_id=\"<dataset ID>\",\n    table_name=\"<table name>\",\n    service_account_json=service_account_json,\n    schema_auto_update=True,\n    ddl_timeout=10.0,\n    insert_timeout=10.0,\n    retry_timeout=30.0,\n)\n\nsdf = app.dataframe(topic)\nsdf.sink(bigquery_sink)\n\nif __name__ == '__main__':\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Defining the BaseSink Abstract Class in Python\nDESCRIPTION: Abstract base class for all sinks in QuixStreams. It defines the interface that all sink implementations must follow, with methods for handling data, managing connections, and flushing data to destinations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BaseSink(abc.ABC)\n```\n\n----------------------------------------\n\nTITLE: Adding a Source to the Application (Quix Streams, Python)\nDESCRIPTION: This method attaches a new data source to the Application, optionally specifying a topic for its output. When transformations or StreamingDataFrame usage are not required, this facilitates direct data source streaming. Dependencies: quixstreams, BaseSource and Topic. Input: BaseSource and optional Topic. Output: Topic. Limitation: If no topic is provided, the topic is inferred from the source, with default naming conventions ('source__' prefix).\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef add_source(source: BaseSource, topic: Optional[Topic] = None) -> Topic\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using FileSink with LocalDestination in Quix Streams\nDESCRIPTION: This snippet demonstrates how to create and configure a FileSink instance with LocalDestination for writing JSON files. It shows setting up the Application, topic, and StreamingDataFrame, and connecting the sink.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/local-file-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.file import FileSink\nfrom quixstreams.sinks.community.file.destinations import LocalDestination\nfrom quixstreams.sinks.community.file.formats import JSONFormat\n\n# Configure the sink to write JSON files\nfile_sink = FileSink(\n    # Optional: defaults to current working directory\n    directory=\"data\",\n    # Optional: defaults to \"json\"\n    # Available formats: \"json\", \"parquet\" or an instance of Format\n    format=JSONFormat(compress=True),\n    # Optional: defaults to LocalDestination(append=False)\n    destination=LocalDestination(append=True),\n)\n\napp = Application(broker_address='localhost:9092', auto_offset_reset=\"earliest\")\ntopic = app.topic('sink-topic')\n\nsdf = app.dataframe(topic=topic)\nsdf.sink(file_sink)\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Basic Redis Sink Implementation with JSON Serialization\nDESCRIPTION: Example showing how to initialize and configure a basic Redis sink with JSON serialization in a Quix Streams application. Demonstrates setting up the application, topic, and sink configuration with basic parameters.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/redis-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.redis import RedisSink\n\napp = Application(\n    broker_address=\"localhost:9092\",\n    auto_offset_reset=\"earliest\",\n    consumer_group=\"consumer-group\",\n)\n\ntopic = app.topic(\"topic-name\")\n\n# Initialize a sink\nredis_sink = RedisSink(\n    host=\"<Redis host>\",\n    port=\"<Redis port>\",\n    db=\"<Redis db>\",\n    value_serializer=json.dumps,\n    key_serializer=None,\n    password=None,\n    socket_timeout=30.0,\n)\n\nsdf = app.dataframe(topic)\nsdf.sink(redis_sink)\n\nif __name__ == '__main__':\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Filtering StreamingDataFrame with Column Expressions in Python\nDESCRIPTION: Demonstrates filtering a Quix Streams `StreamingDataFrame` based on conditions applied to its columns, similar to Pandas. Includes examples using direct comparison, `.apply()` on columns, and combining multiple conditions with logical operators (&). This approach assumes the message value is a dictionary.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n```python\nsdf = app.dataframe(...)\n\n# Filter only values with temperature higher than 60 degrees.\nsdf = sdf[sdf['temperature'] > 60]\n\n# The same can be done with .apply() and a custom function\nsdf = sdf[sdf['temperature'].apply(lambda temp: temp > 60)]\n# Or\nsdf = sdf[sdf.apply(lambda value: value['temperature'] > 60)]\n\n# Multiple conditions can also be combined using binary operators \nsdf = sdf[(sdf['temperature'] > 60) & (sdf['country'] == 'US')]\n# Or\nsdf = sdf[\n    (sdf['temperature'] > 60)\n    & sdf['country'].apply(lambda country: country.lower() == 'US')\n    ]\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Branching for Purchase Processing in Quix Streams\nDESCRIPTION: This example demonstrates how to use branching in Quix Streams to process customer purchase data. It includes filtering purchases based on membership type and total amount, and producing messages to different topics based on various conditions.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/branching.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\nSALES_TAX = 1.10\n\n\ndef get_purchase_totals(items):\n    return sum([i[\"Price\"] * i[\"Quantity\"] for i in items])\n\n\ndef message_stub(value):\n    return (f'Congratulations {value[\"First Name\"]} {value[\"Last Name\"]} your recent '\n            f'purchase totaling {value[\"Total\"]} was enough to earn you')\n\n\ndef coupon(value):\n    return f\"{message_stub(value)} a coupon!\"\n\n\ndef car_prize(value):\n    return f\"{message_stub(value)} a chance to win a car!\"\n\n\ndef cash_prize(value):\n    return f\"f{message_stub(value)} a chance to win $1 million!\"\n\n\napp = Application(\"localhost:9092\")\ncustomer_purchases = app.topic(\"customer_purchases\", value_deserializer=\"json\")\n\nsilver_topic = app.topic(\"silver_coupon\", value_serializer=\"str\")\ngold_topic = app.topic(\"gold_coupon\", value_serializer=\"str\")\ncar_topic = app.topic(\"car_prize\", value_serializer=\"str\")\ncash_topic = app.topic(\"cash_prize\", value_serializer=\"str\")\n\npurchases = app.dataframe(customer_purchases)\npurchases[\"Total\"] = purchases[\"Purchases\"].apply(get_purchase_totals) * SALES_TAX\npurchases.drop([\"Email\", \"Purchases\"])\n\nprizes = purchases[purchases[\"Total\"] >= 200.00]\ncar = prizes[prizes[\"Total\"] < 1000.00].apply(car_prize).to_topic(car_topic)\ncash = prizes[prizes[\"Total\"] >= 1000.00].apply(cash_prize).to_topic(cash_topic)\n\nsilver_coupon = purchases[\n    (purchases[\"Membership Type\"] == \"Silver\") & (purchases[\"Total\"] >= 75.00)\n].apply(coupon).to_topic(silver_topic)\n\ngold_coupon = purchases[\n    (purchases[\"Membership Type\"] == \"Gold\") & (purchases[\"Total\"] >= 50.00)\n].apply(coupon).to_topic(gold_topic)\n\napp.run()\n```\n\n----------------------------------------\n\nTITLE: LocalFileSource Implementation for Local Filesystem Access\nDESCRIPTION: A concrete implementation of FileSource that works with files in a local filesystem. It recursively processes files and sends their records to a Kafka topic.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nclass LocalFileSource(FileSource)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Apache Iceberg Sink in Python\nDESCRIPTION: Example of how to configure and use the IcebergSink in a Quix Streams application. It demonstrates setting up AWS credentials, configuring the sink, and integrating it with a StreamingDataFrame.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/apache-iceberg-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.iceberg import IcebergSink, AWSIcebergConfig\n\n# Configure S3 bucket credentials  \niceberg_config = AWSIcebergConfig(\n    aws_s3_uri=\"\", aws_region=\"\", aws_access_key_id=\"\", aws_secret_access_key=\"\"\n)\n\n# Configure the sink to write data to S3 with the AWS Glue catalog spec \niceberg_sink = IcebergSink(\n    table_name=\"glue.sink-test\",\n    config=iceberg_config,\n    data_catalog_spec=\"aws_glue\",\n)\n\napp = Application(broker_address='localhost:9092', auto_offset_reset=\"earliest\")\ntopic = app.topic('sink_topic')\n\n# Do some processing here\nsdf = app.dataframe(topic=topic).print(metadata=True)\n\n# Sink results to the IcebergSink\nsdf.sink(iceberg_sink)\n\n\nif __name__ == \"__main__\":\n    # Start the application\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Sliding Window Aggregation with Quix Streams (Python)\nDESCRIPTION: Demonstrates creating a one-hour sliding window for sensor data using Quix Streams in Python. This example defines a time-based sliding window with millisecond resolution, aggregates the average temperature using the Mean function, and emits updates on each incoming message. Requires the quixstreams library (pip install quixstreams) and appropriate Dataframe schema. Key parameters include duration_ms for window size in milliseconds and the column to aggregate. Inputs are dataframes of temperature readings, outputs are rolling averages by sensor per message.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\\nfrom quixstreams import Application\\nfrom quixstreams.dataframe.windows import Mean\\n\\napp = Application(...)\\nsdf = app.dataframe(...)\\n\\nsdf = (\\n    # Define a sliding window of 1h\\n    # You can also pass duration_ms as integer of milliseconds\\n    .sliding_window(duration_ms=timedelta(hours=1))\\n\\n    # Specify the \\\"mean\\\" aggregate function\\n    .agg(avg_temperature=Mean(\\\"temperature\\\"))\\n\\n    # Emit updates for each incoming message\\n    .current()\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Configuring GroupBy Serializers in Quix Streams using Python\nDESCRIPTION: Demonstrates how to specify custom serializers and deserializers for the GroupBy operation in Quix Streams using Python. This snippet requires the 'quixstreams' library and an initialized Application instance (referred to as 'app'). It creates a StreamingDataFrame from an input topic and groups by 'my_col_name', explicitly setting the key and value serializers and deserializers. Key parameters include 'value_deserializer', 'key_deserializer', 'value_serializer', and 'key_serializer'. The input is a topic name and column key, while the output is a grouped StreamingDataFrame. The capability to provide custom serialization is useful when adapting to non-default data formats.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/groupby.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\nsdf = app.dataframe(input_topic)\nsdf = sdf.group_by(\n    \"my_col_name\",\n    value_deserializer=\"int\",\n    key_deserializer=\"string\",\n    value_serializer=\"int\",\n    key_serializer=\"string\"\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Stateful Transformations in Quix Streams with Python\nDESCRIPTION: Demonstrates defining and applying a stateful function to transform column data in a StreamingDataFrame. The function maintains state between invocations and performs string transformations. Also shows how to apply simple transformations using lambda functions and combine results with arithmetic operations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef func(value: str, state: State):\n    if value != state.get(\"my_store_key\"):\n        state.set(\"my_store_key\") = value\n    return v.upper()\n\nsdf = StreamingDataFrame()\nsdf[\"new_col\"] = sdf[\"a_column\"][\"nested_dict_key\"].apply(func, stateful=True)\nsdf[\"new_col_2\"] = sdf[\"str_col\"].apply(lambda v: int(v)) + sdf[\"str_col2\"] + 2\n```\n\n----------------------------------------\n\nTITLE: Implementing JSONSerializer Constructor in Python\nDESCRIPTION: Constructor for JSONSerializer that handles data serialization to JSON format with optional schema validation and registry integration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(dumps: Callable[[Any], Union[str, bytes]] = default_dumps, schema: Optional[Mapping] = None, validator: Optional[\"_Validator\"] = None, schema_registry_client_config: Optional[SchemaRegistryClientConfig] = None, schema_registry_serialization_config: Optional[SchemaRegistrySerializationConfig] = None)\n```\n\n----------------------------------------\n\nTITLE: Implementing Terminal Branches in Quix Streams Python Application\nDESCRIPTION: This complete Python script exemplifies the use of terminal branches in a Quix Streams application. It processes customer purchase data, calculates totals, filters based on total amount and membership type, and sends prize/coupon messages to different topics without assigning intermediate StreamingDataFrame variables for each branch. This demonstrates chaining operations like filtering (`[]`), `.apply()`, and `.to_topic()` directly. Dependencies include the `quixstreams` library.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/branching.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\nSALES_TAX = 1.10\n\n\ndef def get_purchase_totals(items):\n    return sum([i[\"Price\"] * i[\"Quantity\"] for i in items])\n\n\ndef message_stub(value):\n    return (f'Congratulations {value[\"First Name\"]} {value[\"Last Name\"]} your recent '\n            f'purchase totaling {value[\"Total\"]} was enough to earn you')\n\n\ndef coupon(value):\n    return f\"{message_stub(value)} a coupon!\"\n\n\ndef car_prize(value):\n    return f\"{message_stub(value)} a chance to win a car!\"\n\n\ndef cash_prize(value):\n    return f\"f{message_stub(value)} a chance to win $1 million!\"\n\n\napp = Application(\"localhost:9092\")\ncustomer_purchases = app.topic(\"customer_purchases\", value_deserializer=\"json\")\n\nsilver_topic = app.topic(\"silver_coupon\", value_serializer=\"str\")\ngold_topic = app.topic(\"gold_coupon\", value_serializer=\"str\")\ncar_topic = app.topic(\"car_prize\", value_serializer=\"str\")\ncash_topic = app.topic(\"cash_prize\", value_serializer=\"str\")\n\npurchases = app.dataframe(customer_purchases)\npurchases[\"Total\"] = purchases[\"Purchases\"].apply(get_purchase_totals) * SALES_TAX\npurchases.drop([\"Email\", \"Purchases\"])\n\n\nprizes = purchases[purchases[\"Total\"] >= 200.00]\n# Removed car and cash assignments\nprizes[prizes[\"Total\"] < 1000.00].apply(car_prize).to_topic(car_topic)\nprizes[prizes[\"Total\"] >= 1000.00].apply(cash_prize).to_topic(cash_topic)\n\n# Removed silver and gold assignments\npurchases[\n    (purchases[\"Membership Type\"] == \"Silver\") & (purchases[\"Total\"] >= 75.00)\n].apply(coupon).to_topic(silver_topic)\n\npurchases[\n    (purchases[\"Membership Type\"] == \"Gold\") & (purchases[\"Total\"] >= 50.00)\n].apply(coupon).to_topic(gold_topic)\n\napp.run()\n```\n\n----------------------------------------\n\nTITLE: Running the Application Data Pipeline (Quix Streams, Python)\nDESCRIPTION: Launches the application, starting all configured streaming operations using the previously constructed StreamingDataFrames. Optional parameters allow for stopping after a specified timeout or input message count, which is useful for debugging or batch runs; otherwise, runs indefinitely for production. Dependencies: Application, StreamingDataFrame, suitable input topics. Inputs: deprecated dataframe, timeout (float, seconds), count (int, records). Output: runs side-effects and streaming pipeline. Limitation: 'dataframe' argument is deprecated; use defaults.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef run(dataframe: Optional[StreamingDataFrame] = None,\n        timeout: float = 0.0,\n        count: int = 0)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Set up an `app = Application` and  `sdf = StreamingDataFrame`;\n# add some operations to `sdf` and then run everything.\n\napp = Application(broker_address='localhost:9092', consumer_group='group')\ntopic = app.topic('test-topic')\ndf = app.dataframe(topic)\ndf.apply(lambda value, context: print('New message', value)\n\napp.run()  # could pass `timeout=5` here, for example\n```\n\n----------------------------------------\n\nTITLE: Sending Data to Sink in StreamingDataFrame\nDESCRIPTION: Method to send processed data to a specified destination. Records are added to a sink and flushed on each checkpoint. Offsets are committed only after successful flush of all sinks for all topic partitions.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef sink(sink: BaseSink)\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Kafka using ConnectionConfig in Python\nDESCRIPTION: This snippet demonstrates how to provide authentication settings for a Kafka broker using the ConnectionConfig object in a Quix Streams application.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/configuration.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.kafka.configuration import ConnectionConfig\n\nconnection = ConnectionConfig(\n    bootstrap_servers=\"my_url\",\n    security_protocol=\"sasl_plaintext\",\n    sasl_mechanism=\"PLAIN\",\n    sasl_username=\"my_user\",\n    sasl_password=\"my_pass\"\n)\n\napp = Application(broker_address=connection)\n```\n\n----------------------------------------\n\nTITLE: Custom Power Sum Aggregator in Quix Streams\nDESCRIPTION: This snippet demonstrates how to create a custom PowerSum aggregator in Quix Streams. It calculates the sum of the power of incoming data over a 10-minute tumbling window.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/aggregations.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows.aggregations import Aggregator\n\napp = Application(...)\nsdf = app.dataframe(...)\n\nclass PowerSum(Aggregator):\n    def initialize(self):\n        return 0\n\n    def agg(self, aggregated, new, timestamp):\n        if self.column is not None:\n            new = new[self.column]\n        return aggregated + (new * new)\n\n    def result(self, aggregated):\n        return aggregated\n\n# Input:\n# {\"amount\" : 2}\n# {\"amount\" : 3}\n\nsdf = (\n    # Define a tumbling window of 10 minutes\n    sdf.tumbling_window(timedelta(minutes=10))\n\n    # Aggregate the custom sum\n    .agg(sum=PowerSum())\n\n    # Emit results only for closed windows\n    .final()\n)\n# Output:\n# {\n#   'start': <window start>, \n#   'end': <window end>, \n#   'sum': 13\n# }\n```\n\n----------------------------------------\n\nTITLE: Extracting Custom Timestamps for Window Aggregations in Python\nDESCRIPTION: Demonstrates how to create a custom timestamp extractor function and use it with a Quix Streams Application. This allows using timestamps from message payloads instead of default Kafka timestamps for window aggregations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.models import TimestampType\nfrom typing import Any, Optional, List, Tuple\n\n# Create an Application\napp = Application(...)\n\n\ndef custom_ts_extractor(\n    value: Any,\n    headers: Optional[List[Tuple[str, bytes]]],\n    timestamp: float,\n    timestamp_type: TimestampType,\n) -> int:\n    \"\"\"\n    Specifying a custom timestamp extractor to use the timestamp from the message payload \n    instead of Kafka timestamp.\n    \"\"\"\n    return value[\"timestamp\"]\n\n# Passing the timestamp extractor to the topic.\n# The window functions will now use the extracted timestamp instead of the Kafka timestamp.\ntopic = app.topic(\"input-topic\", timestamp_extractor=custom_ts_extractor)\n\n# Create a StreamingDataFrame and keep processing\nsdf = app.dataframe(topic)\n\nif __name__ == '__main__':\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Checking for Null Values with StreamingSeries.isnull() in Python\nDESCRIPTION: Demonstrates how to check if a StreamingSeries value is None. Returns a boolean result that can be assigned to a new column in the StreamingDataFrame.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Check if \"column_a\" is null and assign the resulting `bool` to a new column:\n# \"is_null\"\n\nsdf = app.dataframe()\nsdf[\"is_null\"] = sdf[\"column_a\"].isnull()\n```\n\n----------------------------------------\n\nTITLE: Initializing Quix Streams Application for Local Development\nDESCRIPTION: This snippet shows a recommended pattern for initializing a Quix Streams Application that can be used for both local development and Quix Cloud deployment without code changes.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/quix-platform.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nimport os\n\napp = Application(broker_address=os.getenv(\"YOUR_ENV_VAR\", None))\n```\n\n----------------------------------------\n\nTITLE: StreamingDataFrame Basic Usage Example - Python\nDESCRIPTION: Shows basic setup and chaining of operations using StreamingDataFrame including apply, filter and topic output.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsdf = StreamingDataFrame()\nsdf = sdf.apply(a_func)\nsdf = sdf.filter(another_func)\nsdf = sdf.to_topic(topic_obj)\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Topics with Custom Settings in Quix Streams Python\nDESCRIPTION: This example shows how to create Kafka topics with custom configurations using Quix Streams. It uses the TopicConfig class to specify custom replication factors and partition numbers for input and output topics.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/topics.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.models import TopicConfig\n\n# Create an Application and tell it to create topics automatically\napp = Application(broker_address='localhost:9092', auto_create_topics=True)\n\n# Define input and output topics with custom configuration using\ninput_topic = app.topic(\n    name='input', config=TopicConfig(replication_factor=3, num_partitions=10),\n)\n\noutput_topic = app.topic(\n    name='output', config=TopicConfig(replication_factor=3, num_partitions=8),\n)\n\n# Create a bypass transformation sending messages from the input topic to the output one\nsdf = app.dataframe(input_topic).to_topic(output_topic)\n\n# Run the Application. \n# The topics will be validated and created during this function call.\n# Note: if the topics already exist, the configs will remain intact.\napp.run()\n```\n\n----------------------------------------\n\nTITLE: Configuring InfluxDB3Sink for Local Testing - Python\nDESCRIPTION: Demonstrates a minimal Python example for instantiating InfluxDB3Sink with settings for connecting to a local InfluxDB Docker instance for testing. It specifies the required host, organization_id, and token fields for local operation and notes which parameters are unused placeholders in this setup. No actual data writing or application code included; focus is on sink configuration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/influxdb3-sink.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nInfluxDB3Sink(\n    host=\"http://localhost:8181\",   # be sure to add http\n    organization_id=\"local\",        # unused, but required\n    token=\"local\",                  # unused, but required\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Temperature Aggregator in Quix Streams\nDESCRIPTION: This example shows how to create a custom TemperatureAggregator in Quix Streams that processes multiple message fields. It calculates min, max, average temperature, and total event count for each 10-minute tumbling window.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/aggregations.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows import Aggregator\n\nclass TemperatureAggregator(Aggregator):\n    def initialize(self):\n        return {\n            \"min_temp\": 0,\n            \"max_temp\": 0,\n            \"total_events\": 0,\n            \"sum_temp\": 0,\n        }\n    \n    def agg(self, old, new, ts):\n        if self.column is not None:\n            new = new[self.column]\n\n        old[\"min_temp\"] = min(old[\"min_temp\"], new)\n        old[\"max_temp\"] = max(old[\"max_temp\"], new)\n        old[\"total_events\"] += 1\n        old[\"sum_temp\"] += new\n        return old\n\n    def result(self, stored):\n        return {\n            \"min_temp\": stored[\"min_temp\"],\n            \"max_temp\": stored[\"max_temp\"],\n            \"total_events\": stored[\"total_events\"],\n            \"avg_temp\": stored[\"sum_temp\"] / stored[\"total_events\"]\n        }\n        \n\napp = Application(...)\nsdf = app.dataframe(...)\n\nsdf = (\n    \n    # Define a tumbling window of 10 minutes\n    sdf.tumbling_window(timedelta(minutes=10))\n\n    .agg(\n        value=TemperatureAggregator(column=\"Temperature\")\n    )\n\n    # Emit results only for closed windows\n    .final()\n)\n\n# Output:\n# {\n#   'start': <window start>, \n#   'end': <window end>,\n#   'value': {\n#       'min_temp': 1,\n#       'max_temp': 999,\n#       'avg_temp': 34.32,\n#       'total_events': 999,\n#   }\n# }\n```\n\n----------------------------------------\n\nTITLE: StreamingSeries Example Usage\nDESCRIPTION: Example showing various operations with StreamingSeries, including applying functions to columns, creating new fields based on conditions, performing arithmetic operations, and filtering data based on complex conditions.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n# Random methods for example purposes. More detailed explanations found under\n# various methods or in the docs folder.\n\nsdf = StreamingDataFrame()\nsdf = sdf[\"column_a\"].apply(a_func).apply(diff_func, stateful=True)\nsdf[\"my_new_bool_field\"] = sdf[\"column_b\"].contains(\"this_string\")\nsdf[\"new_sum_field\"] = sdf[\"column_c\"] + sdf[\"column_d\"] + 2\nsdf = sdf[[\"column_a\"] & (sdf[\"new_sum_field\"] >= 10)]\n```\n\n----------------------------------------\n\nTITLE: Implementing Iceberg Sink Class in Python\nDESCRIPTION: Definition of IcebergSink class that writes data batches to Apache Iceberg tables. Supports AWS-hosted Iceberg with AWS Glue catalog and includes Parquet serialization.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nclass IcebergSink(BatchingSink)\n```\n\n----------------------------------------\n\nTITLE: Dropping Columns from Streaming Messages with StreamingDataFrame (Python)\nDESCRIPTION: This snippet shows how to drop one or more columns from streamed dictionary records using the drop method of StreamingDataFrame. Options include dropping a single column, multiple columns as a list, and handling missing columns gracefully with errors='ignore'. The drop method modifies the input dictionary in place and is only applicable to mapping-like values. The returned StreamingDataFrame object allows for further chaining.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(...)\n# Dropping the \"metadata\" key from the record's value assuming it's a dictionary\nsdf.drop(\"metadata\")\n\n# You may also drop multiple keys by providing a list of names:\nsdf.drop([\"metadata\", \"timestamp\"])\n\n# You may suppress KeyErrors if some columns are missing in the value dictionary\nsdf.drop([\"missing_column\"], errors='ignore')\n```\n\n----------------------------------------\n\nTITLE: Applying Hopping Window Aggregation for Temperature Analysis\nDESCRIPTION: Configures a hopping window operation to calculate the mean temperature over 5-second windows with 1-second steps. This enables detection of prolonged high temperatures.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/anomaly-detection/tutorial.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport quixstreams.dataframe.windows.aggregations as agg\n\nsdf = sdf.hopping_window(duration_ms=5000, step_ms=1000).agg(value=agg.Mean(column=\"Temperature_C\")).current()\n```\n\n----------------------------------------\n\nTITLE: Transforming Values with StreamingDataFrame.apply() in Python\nDESCRIPTION: Demonstrates using the `StreamingDataFrame.apply()` method with a lambda function to transform each value in the stream. The result of the function becomes the new value passed downstream. This example shows adding 1 to each value.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Return a new value based on input\nsdf = sdf.apply(lambda value: value + 1)\n```\n```\n\n----------------------------------------\n\nTITLE: Handling Backpressure in BatchingSink\nDESCRIPTION: Override of the on_paused method that drops accumulated batches when the destination is experiencing backpressure, preventing memory buildup during pause periods.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef on_paused()\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using InfluxDB Sink in Quix Streams (Python)\nDESCRIPTION: This snippet demonstrates how to initialize an InfluxDB sink and use it with a Quix Streams application. It shows the setup of the Application, topic, and InfluxDB3Sink, as well as how to sink data from a StreamingDataFrame.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.core.influxdb3 import InfluxDB3Sink\n\napp = Application(broker_address=\"localhost:9092\")\ntopic = app.topic(\"numbers-topic\")\n\n# Initialize InfluxDB3Sink\ninflux_sink = InfluxDB3Sink(\n    token=\"<influxdb-access-token>\",\n    host=\"<influxdb-host>\",\n    organization_id=\"<influxdb-org>\",\n    database=\"<influxdb-database>\",\n    measurement=\"numbers\",\n    fields_keys=[\"number\"],\n    tags_keys=[\"tag\"]\n)\n\nsdf = app.dataframe(topic)\n# Do some processing here ...\n# Sink data to InfluxDB\nsdf.sink(influx_sink)\n\nif __name__ == '__main__':\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Creating Live-Updating Table for Data Stream in Quix Streams\nDESCRIPTION: Shows how to use StreamingDataFrame.print_table() to create a live-updating table that displays the most recent records, with options to limit columns and adjust column widths.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/debugging.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(...)\n# some SDF transformations happening here ...\n\n# Show last 5 records with metadata columns\nsdf.print_table(size=5, title=\"My Stream\")\n\n# For wide datasets, limit columns to improve readability\nsdf.print_table(\n    size=5,\n    title=\"My Stream\",\n    columns=[\"id\", \"name\", \"value\"],\n    column_widths={\"name\": 20}\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Topics Automatically with Quix Streams in Python\nDESCRIPTION: This snippet demonstrates how to automatically create input and output Kafka topics using Quix Streams. It sets up an Application instance with auto_create_topics=True, defines input and output topics, and creates a bypass transformation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/topics.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Create an Application and tell it to create topics automatically\napp = Application(broker_address='localhost:9092', auto_create_topics=True)\n\n# Define input and output topics\ninput_topic = app.topic('input')\noutput_topic = app.topic('output')\n\n# Create a bypass transformation sending messages from the input topic to the output one\nsdf = app.dataframe(input_topic).to_topic(output_topic)\n\n# Run the Application. \n# The topics will be validated and created during this function call.\napp.run()\n```\n\n----------------------------------------\n\nTITLE: Creating Changelog Topics in TopicManager - Python\nDESCRIPTION: This method creates and registers a changelog topic linked to a stream and store name, using a TopicConfig. The purpose is to provide backup/restore capabilities for state stores in stream apps. Ensures partition counts match on existing topics. Requires a stream_id, store_name, and configuration, and outputs a registered Topic object.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef changelog_topic(stream_id: Optional[str], store_name: str,\\n                    config: TopicConfig) -> Topic\n```\n\n----------------------------------------\n\nTITLE: Counting Items in a Window using Quix Streams\nDESCRIPTION: This snippet demonstrates how to count items in a 10-minute tumbling window using the Count aggregator in Quix Streams. It defines a window, applies the Count aggregator, and specifies to emit results only for closed windows.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/aggregations.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows import Count\n\napp = Application(...)\nsdf = app.dataframe(...)\n\n\nsdf = (\n    \n    # Define a tumbling window of 10 minutes\n    sdf.tumbling_window(timedelta(minutes=10))\n\n    # Call .agg() and provide an Aggregator or Collector to it.\n    # Here we use a built-in aggregator \"Count\".\n    # The parameter name will be used as a part of the aggregated state and returned in the result. \n    .agg(count=Count())\n\n    # Specify how the windowed results are emitted.\n    # Here, emit results only for closed windows.\n    .final()\n)\n\n# Output:\n# {\n#   'start': <window start>, \n#   'end': <window end>, \n#   'count': 9999 - total number of events in the window\n# }\n```\n\n----------------------------------------\n\nTITLE: In-place Value Update Example - Python\nDESCRIPTION: Shows how to perform in-place updates and side effects using the update method with state management.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef func(values: list, state: State):\n    value = values[0]\n    if value != state.get(\"my_store_key\"):\n        state.set(\"my_store_key\") = value\n    values.append(\"new_item\")\n\nsdf = StreamingDataFrame()\nsdf = sdf.update(func, stateful=True)\n# does not require reassigning\nsdf.update(lambda v: v.append(1))\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Source with Range Generator\nDESCRIPTION: Example of implementing a stateful source that generates sequential numbers and persists its state to Kafka. Includes state management and periodic flushing.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/custom-sources.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nimport time\n\nfrom quixstreams.sources.base import StatefulSource\n\nclass RangeSource(StatefulSource):\n    def run(self):\n        # Get the key \"current\" from the state\n        start = self.state.get(\"current\", 0) + 1\n        for i in range(start, sys.maxsize):\n            if not self.running:\n                return\n            \n            # Update the key in the state\n            self.state.set(\"current\", i)\n            serialized = self._producer_topic.serialize(value=i)\n            self.produce(key=\"range\", value=serialized.value)\n            time.sleep(0.1)\n\n            # Flush the state changes every 10 messages\n            if i % 10 == 0:\n                self.flush()\n```\n\n----------------------------------------\n\nTITLE: Establishing Coinbase API Connection in Python\nDESCRIPTION: This snippet demonstrates how to establish a connection to the Coinbase Websocket API and subscribe to ticker updates for specified product IDs.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/websocket-source/tutorial.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nws_conn = connect(self._url)\nsubscribe_payload = {\n    \"type\": \"subscribe\",\n    \"channels\": [\n        {\"name\": \"ticker\", \"product_ids\": self._product_ids},\n    ],\n}\nws_conn.send(json.dumps(subscribe_payload))\n```\n\n----------------------------------------\n\nTITLE: Filtering Values with StreamingDataFrame.filter() and Custom Logic in Python\nDESCRIPTION: Provides examples of using `StreamingDataFrame.filter()` with lambda functions to filter stream values based on custom logic applied to the message content (dictionary keys in this case). Values are filtered out if the function returns `False` or a falsy value.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Filter out values with \"field_a\" <= 0\nsdf = sdf.filter(lambda value: value['field_a'] > 0)\n\n# Filter out values where \"field_a\" is False  \nsdf = sdf.filter(lambda value: value['field_a'])\n```\n```\n\n----------------------------------------\n\nTITLE: Filtering StreamingDataFrame with Custom Functions using .filter() in Python\nDESCRIPTION: Explains how to use the `StreamingDataFrame.filter()` method with a lambda function to filter stream values. The function receives the current value, and if it returns `False`, the value is filtered out from the stream. This is useful for complex filtering logic or when values are not dictionaries.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n```python\nsdf = sdf.filter(lambda value: value['temperature'] > 60)\n```\n```\n\n----------------------------------------\n\nTITLE: Setting Message Context in Quix Streams Python\nDESCRIPTION: This function sets a MessageContext for the current message in the given contextvars.Context. It's primarily used for advanced scenarios where message key changes are needed.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/context.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef set_message_context(context: Optional[MessageContext])\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application, set_message_context, message_context\n\n# Changes the current sdf value based on what the message partition is.\ndef alter_context(value):\n    context = message_context()\n    if value > 1:\n        context.headers = context.headers + (b\"cool_new_header\", value.encode())\n        set_message_context(context)\n\napp = Application()\nsdf = app.dataframe()\nsdf = sdf.update(lambda value: alter_context(value))\n```\n\n----------------------------------------\n\nTITLE: Using Azure File Source with Quix Streams Application\nDESCRIPTION: Example of configuring and using the Azure File Source connector with a Quix Streams Application. It demonstrates setting up the AzureFileOrigin and FileSource to read files from an Azure container.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/microsoft-azure-file-source.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources.community.file import FileSource\nfrom quixstreams.sources.community.file.origins import AzureFileOrigin\n\napp = Application(broker_address=\"localhost:9092\", auto_offset_reset=\"earliest\")\n\norigin = AzureFileOrigin(\n    container=\"<YOUR CONTAINER NAME>\",\n    connection_string=\"<YOUR CONNECTION STRING>\",\n)\nsource = FileSource(\n    directory=\"path/to/your/topic_folder/\",\n    origin=origin,\n    format=\"json\",\n    compression=\"gzip\",\n)\nsdf = app.dataframe(source=source).print(metadata=True)\n# YOUR LOGIC HERE!\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Implementing Neo4j Sink with Quix Streams\nDESCRIPTION: Complete example showing how to create and configure a Neo4j sink to write data from a Kafka topic to a Neo4j database. Demonstrates setting up the application, configuring the sink with connection parameters, and defining a Cypher query for data insertion.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/neo4j-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.neo4j import Neo4jSink\n\napp = Application(broker_address=\"localhost:9092\")\ntopic = app.topic(\"topic-name\")\n\n# records structured as:\n# {\"name\": {\"first\": \"John\", \"last\": \"Doe\"}, \"age\": 28, \"city\": \"Los Angeles\"}\n\n# This assumes the given City nodes exist.\n# Notice the use of \"event\" to reference the message value.\n# Could also do things like __key, or __value.name.first.\ncypher_query = \"\"\"\nMERGE (p:Person {first_name: event.name.first, last_name: event.name.last})\nSET p.age = event.age\nMERGE (c:City {name: event.city})\nMERGE (p)-[:LIVES_IN]->(c)\n\"\"\"\n\n# Configure the sink\nneo4j_sink = Neo4jSink(\n    host=\"localhost\",\n    port=7687,\n    username=\"neo4j\",\n    password=\"local_password\",\n    cypher_query=cypher_query,\n)\n\nsdf = app.dataframe(topic=topic)\nsdf.sink(neo4j_sink)\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Running a Standalone Source in Quix Streams\nDESCRIPTION: Demonstrates how to run a source in standalone mode without processing data in the same application. This is useful when scaling processing applications to multiple instances while needing only a single instance of the source.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\ndef main():\n    app = Application()\n    \n    # Create an instance of SomeWebsocketSource\n    source = SomeWebsocketSource(url=\"wss://example.com\")\n    \n    # Register the source in the app\n    app.add_source(source)\n    \n    # Start the application\n    # The app will start SomeWebsocketSource, and it will produce data to the default intermediate topic\n    app.run()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Initializing CSVSource in Quix Streams (Python)\nDESCRIPTION: Defines the constructor (`__init__`) method signature for the `CSVSource` class. It initializes the source to read from a CSV file specified by `path` (string or Path object). Key parameters include a unique `name`, optional `key_extractor` and `timestamp_extractor` callables, a `delay` (in seconds) between producing rows, a `shutdown_timeout`, and the CSV `dialect` (defaulting to \"excel\"). Rows are produced as JSON.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n```python\ndef __init__(path: Union[str, Path],\n             name: str,\n             key_extractor: Optional[Callable[[dict], Union[str,\n                                                            bytes]]] = None,\n             timestamp_extractor: Optional[Callable[[dict], int]] = None,\n             delay: float = 0,\n             shutdown_timeout: float = 10,\n             dialect: str = \"excel\") -> None\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Google Cloud Pub/Sub to Kafka in Quix Streams\nDESCRIPTION: Example code showing how to configure and use PubSubSource with a Quix Streams Application to read from Pub/Sub and produce to Kafka. It demonstrates setting up service account credentials, project details, and subscription configuration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/google-cloud-pubsub-source.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources.community.pubsub import PubSubSource\nfrom os import environ\n\nsource = PubSubSource(\n    # Suggested: pass JSON-formatted credentials from an environment variable.\n    service_account_json = environ[\"PUBSUB_SERVICE_ACCOUNT_JSON\"],\n    project_id=\"<project ID>\",\n    topic_id=\"<topic ID>\",  # NOTE: NOT the full /x/y/z path!\n    subscription_id=\"<subscription ID>\",  # NOTE: NOT the full /x/y/z path!\n    create_subscription=True,\n)\napp = Application(\n    broker_address=\"localhost:9092\",\n    auto_offset_reset=\"earliest\",\n    consumer_group=\"gcp\",\n    loglevel=\"INFO\"\n)\nsdf = app.dataframe(source=source).print(metadata=True)\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Adding Boolean Columns in StreamingDataFrame\nDESCRIPTION: This snippet demonstrates how to add new boolean columns to a StreamingDataFrame based on the presence of other columns. It uses the 'contains' method to check for column existence.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsdf = StreamingDataFrame()\nsdf['has_column_A'] = sdf.contains('column_a')\nsdf['has_column_X_Y'] = sdf.contains(['column_x', 'column_y'])\n```\n\n----------------------------------------\n\nTITLE: Sample CSV File Format for CSV Source (CSV)\nDESCRIPTION: This snippet provides an example of the required input CSV structure with headers. Each row represents data fields to be converted to JSON and sent as Kafka messages. The CSV must have headers as the first line, with subsequent lines as data rows. Input constraints: headers are mandatory and the file format must be comma-separated values.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/csv-source.md#2025-04-23_snippet_1\n\nLANGUAGE: csv\nCODE:\n```\nfield1,field2,timestamp\nfoo1,bar1,1\nfoo2,bar2,2\nfoo3,bar3,3\n```\n\n----------------------------------------\n\nTITLE: Implementing Time-based Hopping Windows in Quix Streams\nDESCRIPTION: Shows how to create a time-based hopping window of 1 hour with a 10-minute step to calculate average temperature from sensor readings. The example uses the Mean aggregate function and emits results for every incoming message.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows import Mean\n\napp = Application(...)\nsdf = app.dataframe(...)\n\nsdf = (\n    # Define a hopping window of 1h with 10m step\n    # You can also pass duration_ms and step_ms as integers of milliseconds\n    .hopping_window(duration_ms=timedelta(hours=1), step_ms=timedelta(minutes=10))\n\n    # Specify the \"mean\" aggregate function\n    .agg(avg_temperature=Mean(\"temperature\"))\n\n    # Emit updates for each incoming message\n    .current()\n)\n```\n\n----------------------------------------\n\nTITLE: Key-Based Window Closing Strategy in Python\nDESCRIPTION: Demonstrates how to calculate a sum of values over a 10-second window using key-based closing strategy, where windows are closed only when messages with matching keys are received.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows import Sum\n\napp = Application(...)\nsdf = app.dataframe(...)\n\n# Calculate a sum of values over a window of 10 seconds \n# and use .final() to emit results only when the window is complete\nsdf = sdf.tumbling_window(timedelta(seconds=10)).agg(value=Sum()).final(closing_strategy=\"key\")\n```\n\n----------------------------------------\n\nTITLE: Interactive Data Collection with ListSink in Quix Streams\nDESCRIPTION: Demonstrates how to use ListSink to collect data from a Quix Streams Application for interactive analysis, including setting stop conditions and accessing collected data.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/debugging.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.core.list import ListSink\n\napp = Application(broker_address=\"localhost:9092\")\ntopic = app.topic(\"some-topic\")\nlist_sink = ListSink()  # sink will be a list-like object\nsdf = app.dataframe(topic=topic).sink(list_sink)\napp.run(count=50, timeout=10)  # get up to 50 records (stops if no messages for 10s)\n```\n\n----------------------------------------\n\nTITLE: Defining RocksDB Options Class\nDESCRIPTION: Configuration options class for RocksDB database including serialization functions and retry settings.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/state.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dataclasses.dataclass(frozen=True)\nclass RocksDBOptions(RocksDBOptionsType)\n```\n\n----------------------------------------\n\nTITLE: Implementing Alert Logic and Filtering\nDESCRIPTION: Defines an alert function to check if the average temperature exceeds 90°C, rounds the temperature value, and filters the stream to only pass events that trigger an alert.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/anomaly-detection/tutorial.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef should_alert(window_value: int, key, timestamp, headers) -> bool:\n    if window_value >= 90:\n        print(f'Alerting for MID {key}: Average Temperature {window_value}')\n        return True\n\nsdf = sdf.apply(lambda result: round(result['value'], 2)).filter(should_alert, metadata=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Google Cloud Pub/Sub Sink\nDESCRIPTION: Complete example showing how to configure and use PubSubSink with a streaming application. Demonstrates setting up the sink with project configuration, authentication, serialization options, and publisher options.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/google-cloud-pubsub-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom google.api_core import retry\nfrom google.cloud.pubsub_v1.types import PublisherOptions\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.pubsub import PubSubSink\n\napp = Application(broker_address=\"localhost:9092\")\ntopic = app.topic(\"topic-name\")\n\n# Read the service account credentials in JSON format from some environment variable.\nservice_account_json = os.environ[\"PUBSUB_SERVICE_ACCOUNT_JSON\"]\n\n# Configure the sink\npubsub_sink = PubSubSink(\n    project_id=\"<project ID>\",\n    topic_id=\"<topic ID>\",\n    # Optional: service account credentials as a JSON string\n    service_account_json=service_account_json,\n    # Optional: customize serialization and flush timeout\n    value_serializer=json.dumps,\n    key_serializer=str,\n    flush_timeout=10,\n    # Optional: Additional keyword arguments are passed to the PublisherClient\n    publisher_options=PublisherOptions(\n        # Configure publisher options to retry on any exception\n        retry=retry.Retry(predicate=retry.if_exception_type(Exception)),\n    )\n)\n\nsdf = app.dataframe(topic=topic)\nsdf.sink(pubsub_sink)\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Defining Output Topic in Quix Streams Application\nDESCRIPTION: This code shows how to create a Topic object for the output topic 'price_updates' within the Quix Streams Application.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/websocket-source/tutorial.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprice_updates_topic = app.topic(name=\"price_updates\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Performance Parameters for Sinks in Quix Streams (Python)\nDESCRIPTION: This example demonstrates how to configure performance parameters for sinks in Quix Streams. It shows how to set commit intervals and message limits to optimize memory usage and throughput when using batching sinks like InfluxDB.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.core.influxdb3 import InfluxDB3Sink\n\n# Commit the checkpoints after processing 1000 messages or after a 5 second interval has elapsed (whichever is sooner).\napp = Application(\n    broker_address=\"localhost:9092\",\n    commit_interval=5.0,\n    commit_every=1000,\n)\ntopic = app.topic('numbers-topic')\nsdf = app.dataframe(topic)\n\n# Create an InfluxDB sink that batches data between checkpoints.\ninflux_sink = InfluxDB3Sink(\n    token=\"<influxdb-access-token>\",\n    host=\"<influxdb-host>\",\n    organization_id=\"<influxdb-org>\",\n    database=\"<influxdb-database>\",\n    measurement=\"numbers\",\n    fields_keys=[\"number\"],\n    tags_keys=[\"tag\"]\n)\n\n# The sink will write to InfluxDB across all assigned partitions.\nsdf.sink(influx_sink)\n\nif __name__ == '__main__':\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Using Time-Based Hopping Window Aggregation in Python\nDESCRIPTION: Example demonstrating how to apply a time-based hopping window to a `StreamingDataFrame`. It initializes a Quix Streams `Application`, defines a 60-second hopping window that advances every 30 seconds with a 10-second grace period, applies a `Sum` aggregation, and specifies the `current()` emission strategy to output results as they are updated.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nimport quixstreams.dataframe.windows.aggregations as agg\nfrom datetime import timedelta\n\napp = Application()\nsdf = app.dataframe(...)\n\nsdf = (\n    # Define a hopping window of 60s with step 30s and grace period of 10s\n    sdf.hopping_window(\n        duration_ms=timedelta(seconds=60),\n        step_ms=timedelta(seconds=30),\n        grace_ms=timedelta(seconds=10)\n    )\n\n    # Specify the aggregation function\n    .agg(value=agg.Sum())\n\n    # Specify how the results should be emitted downstream.\n    # \"current()\" will emit results as they come for each updated window,\n    # possibly producing multiple messages per key-window pair\n    # \"final()\" will emit windows only when they are closed and cannot\n    # receive any updates anymore.\n    .current()\n)\n```\n\n----------------------------------------\n\nTITLE: Producing to Topics with StreamingDataFrame\nDESCRIPTION: This snippet shows how to produce the current value of a StreamingDataFrame to one or more topics. It demonstrates producing to two different topics, with an option to change the message key for one of them.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Produce to two different topics, changing the key for one of them.\n\napp = Application()\ninput_topic = app.topic(\"input_x\")\noutput_topic_0 = app.topic(\"output_a\")\noutput_topic_1 = app.topic(\"output_b\")\n\nsdf = app.dataframe(input_topic)\nsdf = sdf.to_topic(output_topic_0)\n# does not require reassigning\nsdf.to_topic(output_topic_1, key=lambda data: data[\"a_field\"])\n```\n\n----------------------------------------\n\nTITLE: Kinesis Source Class Implementation\nDESCRIPTION: Source class for reading data from Amazon Kinesis streams with at-least-once delivery guarantees.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nclass KinesisSource(StatefulSource)\n```\n\n----------------------------------------\n\nTITLE: Filtering Purchases Based on Total and Membership Type\nDESCRIPTION: Applies filters to the StreamingDataFrame to select purchases over $100 from Silver or Gold members, using custom functions and SDF operations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/purchase-filtering/tutorial.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_purchase_totals(items):\n    return sum([i[\"Price\"]*i[\"Quantity\"] for i in items])\n\nsdf = sdf[\n    (sdf[\"Purchases\"].apply(get_purchase_totals) * SALES_TAX >= 100.00)\n    & (sdf[\"Membership Type\"].isin([\"Silver\", \"Gold\"]))\n]\n```\n\n----------------------------------------\n\nTITLE: Producing Filtered Results to Kafka Topic\nDESCRIPTION: Sends the processed and filtered data to the previously defined Kafka topic for qualified customers.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/purchase-filtering/tutorial.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsdf = sdf.to_topic(customers_qualified_topic)\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Quix Application\nDESCRIPTION: Example showing how to initialize and run a basic Quix Application with a topic subscription and message processing.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Set up an `app = Application` and `sdf = StreamingDataFrame`;\n# add some operations to `sdf` and then run everything.\n\napp = Application(broker_address='localhost:9092', consumer_group='group')\ntopic = app.topic('test-topic')\ndf = app.dataframe(topic)\ndf.apply(lambda value, context: print('New message', value))\n\napp.run()\n```\n\n----------------------------------------\n\nTITLE: Customizing Pydantic Configuration Source Loading (Quix Streams, Python)\nDESCRIPTION: A class method override for Pydantic BaseSettings that controls which sources (init, env, dotenv, file secrets) are used to read settings. Used to suppress environment reads or adapt configuration source ordering. Inputs: settings class, and four different source classes; output: tuple of chosen sources. Dependency: Pydantic's settings system. Limitation: Advanced customization for configuration, not likely called directly.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef settings_customise_sources(\n    cls, settings_cls: Type[PydanticBaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource\n) -> Tuple[PydanticBaseSettingsSource, ...]\n```\n\n----------------------------------------\n\nTITLE: Testing StreamingDataFrame Operations in Python\nDESCRIPTION: Defines the `test` method signature for the `StreamingDataFrame` class. This method provides a convenient way to test the dataframe's processing logic by passing a single message defined by `value`, `key`, `timestamp`, and `headers`. An optional `MessageContext` (`ctx`) can be provided for stateful operations or `to_topic` calls, and a specific `topic` can be targeted. It returns a list containing the results produced by the dataframe for the given input.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef test(value: Any,\n         key: Any = b\"key\",\n         timestamp: int = 0,\n         headers: Optional[Any] = None,\n         ctx: Optional[MessageContext] = None,\n         topic: Optional[Topic] = None) -> List[Any]\n```\n\n----------------------------------------\n\nTITLE: Initializing PostgreSQL Sink Connector in Python\nDESCRIPTION: Constructor for PostgreSQLSink that configures the connection to a PostgreSQL database. Includes parameters for connection details, table configuration, schema management, and timeout settings.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_71\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(host: str,\n             port: int,\n             dbname: str,\n             user: str,\n             password: str,\n             table_name: str,\n             schema_auto_update: bool = True,\n             connection_timeout_seconds: int = 30,\n             statement_timeout_seconds: int = 30,\n             on_client_connect_success: Optional[\n                 ClientConnectSuccessCallback] = None,\n             on_client_connect_failure: Optional[\n                 ClientConnectFailureCallback] = None,\n             **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Using KafkaReplicatorSource to Replicate Kafka Topics in Quix Streams (Python)\nDESCRIPTION: Provides an example demonstrating how to use `KafkaReplicatorSource` within a Quix Streams `Application`. It shows the instantiation of `KafkaReplicatorSource` with necessary configurations like `name`, `app_config`, the source `topic` name, and the external `broker_address`. The source is then used to create a streaming dataframe (`sdf`), which is subsequently printed, and the application is run.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom quixstreams import Application\nfrom quixstreams.sources.kafka import KafkaReplicatorSource\n\napp = Application(\n    consumer_group=\"group\",\n)\n\nsource = KafkaReplicatorSource(\n    name=\"source-second-kafka\",\n    app_config=app.config,\n    topic=\"second-kafka-topic\",\n    broker_address=\"localhost:9092\",\n)\n\nsdf = app.dataframe(source=source)\nsdf = sdf.print()\napp.run()\n```\n```\n\n----------------------------------------\n\nTITLE: Adding Records to Batching Sink\nDESCRIPTION: Implementation of the add method for BatchingSink that appends a new record to the in-memory batch, organizing data by topic and partition for later batch processing.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef add(value: Any, key: Any, timestamp: int, headers: HeadersTuples,\n        topic: str, partition: int, offset: int)\n```\n\n----------------------------------------\n\nTITLE: Expanding and Branching SDF Processing in Python\nDESCRIPTION: This snippet shows how to apply a function to each element of a list within a StreamingDataFrame (SDF) using `expand=True`. Subsequently, it creates two branches (`sdf_a`, `sdf_b`), each applying a different function (`add(10)` and `add(20)`) and sending the results to an output topic. This pattern allows parallel processing paths for expanded elements.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/branching.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsdf = sdf.apply(lambda x: x, expand=True)  # process list per-element\nsdf_a = sdf.apply(add(10)).to_topic(output_topic)\nsdf_b = sdf.apply(add(20)).to_topic(output_topic)\n```\n\n----------------------------------------\n\nTITLE: Fetching Offsets by Timestamp - Kafka Consumer API - Python\nDESCRIPTION: Determines offsets corresponding to provided timestamps for each partition. Accepts a list of TopicPartition objects with the offset (timestamp) set, and optional timeout. Returns TopicPartition objects with resolved offsets or -1 if none found. Raises KafkaException or RuntimeError for failures. Useful for time-based consumption or replay scenarios.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef offsets_for_times(partitions: List[TopicPartition],\n                      timeout: Optional[float] = None) -> List[TopicPartition]\n```\n\n----------------------------------------\n\nTITLE: Implementing InfluxDB3Sink Class\nDESCRIPTION: Specialized sink implementation for writing data to InfluxDB v3, extending the BatchingSink class to leverage batch processing capabilities for optimal database interaction.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass InfluxDB3Sink(BatchingSink)\n```\n\n----------------------------------------\n\nTITLE: Defining Kafka Topic with JSON Serialization in Python\nDESCRIPTION: This code shows how to define a Topic object for publishing data with JSON serialization. The Topic is used for serializing data and validating topic existence in Kafka.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/producer.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define a topic \"my_topic\" with JSON serialization\ntopic = app.topic(name='my_topic', value_serializer='json')\n```\n\n----------------------------------------\n\nTITLE: Filtering Values with StreamingDataFrame.apply() Syntax in Python\nDESCRIPTION: Shows how `StreamingDataFrame.apply()` can be used within square brackets (`sdf[...]`) to filter values. The provided lambda function's boolean result determines if the value is kept (`True`) or filtered out (`False`). This example filters based on the sum of two fields.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Filter values where sum of \"field_b\" and \"field_c\" is greater than 0\nsdf = sdf[sdf.apply(lambda value: (value['field_b'] + value['field_c']) > 0)]\n```\n```\n\n----------------------------------------\n\nTITLE: Example Implementation of a Custom Source with BaseSource\nDESCRIPTION: An example of implementing a custom source by extending the BaseSource class. This RandomNumbersSource generates random numbers and sends them to a Kafka topic.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass RandomNumbersSource(BaseSource):\ndef __init__(self):\n    super().__init__()\n    self._running = False\n\ndef start(self):\n    self._running = True\n\n    while self._running:\n        number = random.randint(0, 100)\n        serialized = self._producer_topic.serialize(value=number)\n        self._producer.produce(\n            topic=self._producer_topic.name,\n            key=serialized.key,\n            value=serialized.value,\n        )\n\ndef stop(self):\n    self._running = False\n\ndef default_topic(self) -> Topic:\n    return Topic(\n        name=\"topic-name\",\n        value_deserializer=\"json\",\n        value_serializer=\"json\",\n    )\n\n\ndef main():\n    app = Application(broker_address=\"localhost:9092\")\n    source = RandomNumbersSource()\n\n    sdf = app.dataframe(source=source)\n    sdf.print(metadata=True)\n\n    app.run()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Committing Offsets - Kafka Consumer API - Python\nDESCRIPTION: Commits offsets for messages or custom topic partitions using a Kafka consumer. Relies on parameters 'message' or 'offsets' (mutually exclusive); if neither is provided, it uses current assignment offsets. Supports asynchronous or synchronous operation via the 'asynchronous' parameter. Raises KafkaException or RuntimeError on error conditions. Requires Quix Streams Kafka consumer and appropriate TopicPartition/Message objects.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef commit(message: Optional[Message] = None,\n           offsets: Optional[List[TopicPartition]] = None,\n           asynchronous: bool = True) -> Optional[List[TopicPartition]]\n```\n\n----------------------------------------\n\nTITLE: Publishing Temperature Alerts to Kafka Topic\nDESCRIPTION: Configures the StreamingDataFrame to publish alert messages to the previously defined Kafka topic for temperature alerts.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/anomaly-detection/tutorial.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsdf = sdf.to_topic(alerts_topic)\n```\n\n----------------------------------------\n\nTITLE: Writing Batches to MongoDB in Python\nDESCRIPTION: Method for writing batches to MongoDB using bulk operations without transactions.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_67\n\nLANGUAGE: python\nCODE:\n```\ndef write(batch: SinkBatch) -> None\n```\n\n----------------------------------------\n\nTITLE: Creating Repartition Topics in TopicManager - Python\nDESCRIPTION: This function generates a Kafka topic specifically for repartitioning during stream processing, using provided operation name, stream ID, and configuration. Custom serializers/deserializers can be set (default: JSON). The main use case is for grouping and repartitioning operations in streaming applications. Returns a Topic object, which is also registered with the TopicManager.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef repartition_topic(\\n        operation: str,\\n        stream_id: str,\\n        config: TopicConfig,\\n        value_deserializer: Optional[DeserializerType] = \"json\",\\n        key_deserializer: Optional[DeserializerType] = \"json\",\\n        value_serializer: Optional[SerializerType] = \"json\",\\n        key_serializer: Optional[SerializerType] = \"json\") -> Topic\n```\n\n----------------------------------------\n\nTITLE: Implementing StringDeserializer Constructor in Python\nDESCRIPTION: Constructor for StringDeserializer class that deserializes bytes to strings using specified encoding. Wraps confluent_kafka.serialization.StringDeserializer.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(codec: str = \"utf_8\")\n```\n\n----------------------------------------\n\nTITLE: Using Local File Source with Quix Streams Application\nDESCRIPTION: Example showing how to configure and use the FileSource connector with a Quix Streams Application. It demonstrates setting up a source that reads JSON files with gzip compression and replays data at original speed.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/local-file-source.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources.community.file import FileSource\n\napp = Application(broker_address=\"localhost:9092\")\nsource = FileSource(\n    directory=\"/path/to/my/topic_folder\",\n    format=\"json\",\n    compression=\"gzip\",\n    replay_speed=1.0,\n)\nsdf = app.dataframe(source=source).print(metadata=True)\n# YOUR LOGIC HERE!\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Defining BatchingSink Class for Accumulating Data\nDESCRIPTION: Specialized sink class for destinations where writing individual messages is not optimal, such as databases and object stores. It handles automatic batching of data per topic-partition in memory.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass BatchingSink(BaseSink)\n```\n\n----------------------------------------\n\nTITLE: Standalone Source with Custom Topic Configuration\nDESCRIPTION: Shows how to run a standalone source with a custom topic configuration. This example creates a websocket source and configures it to produce data to a custom topic with four partitions and a replication factor of 1.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.models.topics import TopicConfig\n\ndef main():\n    app = Application()\n    # Create an instance of SomeWebsocketSource\n    source = SomeWebsocketSource(url=\"wss://example.com\")\n    \n    # Define a topic for the CSVSource with a custom config\n    topic = app.topic(\"some-websocket-source\", config=TopicConfig(num_partitions=4, replication_factor=1))\n    \n    # Register the source and topic in the application\n    app.add_source(source=source, topic=topic)\n    \n    # Start the application\n    app.run()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Example Implementation of a Custom Source with Source Class\nDESCRIPTION: An example of implementing a custom source by extending the Source class. This RandomNumbersSource generates random numbers and sends them to a Kafka topic using the helper methods provided by Source.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport time\n\nfrom quixstreams import Application\nfrom quixstreams.sources import Source\n\n\nclass RandomNumbersSource(Source):\n    def run(self):\n        while self.running:\n            number = random.randint(0, 100)\n            serialized = self._producer_topic.serialize(value=number)\n            self.produce(key=str(number), value=serialized.value)\n            time.sleep(0.5)\n\n\ndef main():\n    app = Application(broker_address=\"localhost:9092\")\n    source = RandomNumbersSource(name=\"random-source\")\n\n    sdf = app.dataframe(source=source)\n    sdf.print(metadata=True)\n\n    app.run()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Manual Serialization with State.get_bytes and State.set_bytes in Quix Streams\nDESCRIPTION: This snippet illustrates how to manually handle serialization and deserialization using State.get_bytes and State.set_bytes methods in Quix Streams. It allows storing any type of values in the state store by converting them to bytes.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/stateful-processing.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\n\nfrom quixstreams import Application, State\napp = Application(\n    broker_address='localhost:9092', \n    consumer_group='consumer', \n)\ntopic = app.topic('topic')\n\nsdf = app.dataframe(topic)\n\ndef apply(value, state):\n    old = state.get_bytes('key', default=None)\n    if old is not None:\n        old = pickle.loads(old)\n    state.set_bytes('key', pickle.dumps(value))\n    return {\"old\": old, \"new\": value}\n    \nsdf = sdf.apply(apply, stateful=True)\n```\n\n----------------------------------------\n\nTITLE: Accumulating Items in a Window using Quix Streams\nDESCRIPTION: This example shows how to accumulate items in a 10-minute tumbling window using the Collect collector in Quix Streams. It gathers all events within each window period into a list and emits results only for closed windows.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/aggregations.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows import Collect\n\napp = Application(...)\nsdf = app.dataframe(...)\n\nsdf = (\n    # Define a tumbling window of 10 minutes\n    sdf.tumbling_window(timedelta(minutes=10))\n\n    # Collect events in the window into a list\n    .agg(events=Collect())\n\n    # Emit results only for closed windows\n    .final()\n)\n# Output:\n# {\n#   'start': <window start>, \n#   'end': <window end>, \n#   'events': [event1, event2, event3, ..., eventN] - list of all events in the window\n# }\n```\n\n----------------------------------------\n\nTITLE: Defining Tumbling Time Windows in Python (StreamingDataFrame)\nDESCRIPTION: Defines the `tumbling_window` method signature for `StreamingDataFrame`. This method creates a time-based tumbling window definition, partitioning the stream into fixed-size, non-overlapping windows based on event time. It requires the window `duration_ms` (int ms or timedelta) and accepts optional `grace_ms` (int ms or timedelta) for late data, a window `name` (str), and an `on_late` callback. It returns a `TumblingTimeWindowDefinition` object, which is used to configure subsequent aggregations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef tumbling_window(\n    duration_ms: Union[int, timedelta],\n    grace_ms: Union[int, timedelta] = 0,\n    name: Optional[str] = None,\n    on_late: Optional[WindowOnLateCallback] = None\n) -> TumblingTimeWindowDefinition\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Schema Serialization with Schema Registry in Python\nDESCRIPTION: Demonstrates how to set up JSON Schema serializer and deserializer with Schema Registry integration in Quix Streams. It includes defining a schema and creating serializer and deserializer instances.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/schema-registry.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams.models import JSONDeserializer, JSONSerializer\n\nMY_SCHEMA = {\n    \"title\": \"MyObject\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"id\": {\"type\": \"number\"},\n    },\n    \"required\": [\"id\"],\n}\n\ndeserializer = JSONDeserializer(\n    schema=MY_SCHEMA,\n    schema_registry_client_config=schema_registry_client_config,\n)\nserializer = JSONSerializer(\n    schema=MY_SCHEMA,\n    schema_registry_client_config=schema_registry_client_config,\n    schema_registry_serialization_config=schema_registry_serialization_config,\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced Redis Sink with Custom Serialization\nDESCRIPTION: Advanced implementation showing custom key serialization and MessagePack value serialization for Redis sink. Demonstrates how to customize key generation and use alternative serialization formats.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/redis-sink.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.redis import RedisSink\n\n# Assuming \"msgpack-python\" is installed\nimport msgpack\n\napp = Application(\n    broker_address=\"localhost:9092\",\n    auto_offset_reset=\"earliest\",\n    consumer_group=\"consumer-group\",\n)\n\ntopic = app.topic(\"topic-name\")\n\nredis_sink = RedisSink(\n    host=\"<Redis host>\",\n    port=\"<Redis port>\",\n    db=\"<Redis db>\",\n    # Serialize records' values using msgpack format before writing to Redis\n    value_serializer=msgpack.dumps,\n    # Combine a new Redis key from the record's key and value.\n    key_serializer=lambda key, value: f'{key}-{value}',\n)\n\nsdf = app.dataframe(topic)\nsdf.sink(redis_sink)\n\nif __name__ == '__main__':\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Initializing CSVSink with Basic Configuration\nDESCRIPTION: Demonstrates how to create and use a basic CSVSink instance with a Quix Streams application. Shows the setup of the application, topic creation, and connecting the sink to a streaming dataframe.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/csv-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.core.csv import CSVSink\n\napp = Application(broker_address=\"localhost:9092\")\ntopic = app.topic(\"input-topic\")\n\n# Initialize a CSV sink with a file path \ncsv_sink = CSVSink(path=\"file.csv\")\n\nsdf = app.dataframe(topic)\n# Do some processing here ...\n# Sink data to a CSV file\nsdf.sink(csv_sink)\n\nif __name__ == '__main__':\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Kafka Messages After Item GroupBy Aggregation - Python\nDESCRIPTION: Illustrates the output Kafka message format after grouping by 'item' and aggregating total_quantity. Assumes prior aggregation logic is applied. Inputs are streaming item/quantity data, output is total_quantity per item as a dict with the item as the key. Intended as an output example.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/groupby.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n{\"key\": \"A\", \"value\": {\"total_quantity\": 32}}\n# ...etc...\n{\"key\": \"B\", \"value\": {\"total_quantity\": 17}}\n{\"key\": \"A\", \"value\": {\"total_quantity\": 35}}\n# ...etc...\n```\n\n----------------------------------------\n\nTITLE: Initializing S3Destination Using Environment Variables in Python\nDESCRIPTION: Shows a simplified way to initialize the `S3Destination` when AWS credentials and the default region are configured via environment variables. Only the `bucket` name needs to be provided explicitly.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/amazon-s3-sink.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ns3_sink = S3Destination(bucket=\"my-bucket\")\n```\n\n----------------------------------------\n\nTITLE: Initializing BatchingSink with Connection Callbacks\nDESCRIPTION: Constructor for BatchingSink that passes optional connection success and failure callbacks to the parent BaseSink class for consistent connection handling.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(on_client_connect_success: Optional[\n    ClientConnectSuccessCallback] = None,\n             on_client_connect_failure: Optional[\n                 ClientConnectFailureCallback] = None)\n```\n\n----------------------------------------\n\nTITLE: Implementing Kinesis Sink in Python Application\nDESCRIPTION: Complete example showing how to create and configure a KinesisSink instance and integrate it with a Quix Streams application. Demonstrates setting up the sink with AWS credentials and optional configuration parameters.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/amazon-kinesis-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.kinesis import KinesisSink\n\napp = Application(broker_address=\"localhost:9092\")\ntopic = app.topic(\"topic-name\")\n\n# Configure the sink\nkinesis_sink = KinesisSink(\n    stream_name=\"<stream name>\",\n    # Optional: AWS credentials\n    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n    region_name=\"eu-west-2\",\n    # Optional: customize serialization\n    value_serializer=str,\n    key_serializer=str,\n    # Optional: Additional keyword arguments are passed to the boto3 client\n    endpoint_url=\"http://localhost:4566\",  # for LocalStack testing\n)\n\nsdf = app.dataframe(topic=topic)\nsdf.sink(kinesis_sink)\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Custom Multi-Column GroupBy and Aggregation - Python\nDESCRIPTION: Shows a custom group_by using a key function to concatenate store_id and item for multi-dimensional aggregation. Combines stateful aggregation to accumulate total quantities per (store_id, item) group. Requires custom grouping function and StreamingDataFrame. Input is message with store_id, item, and quantity; output is totals per composite key. The group_by name parameter must be unique per usage.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/groupby.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_total(message, state):\n    current_total = state.get(\"item_total\", 0)\n    current_total += int(message[\"quantity\"])\n    state.set(\"item_total\", current_total)\n    return current_total\n\ndef groupby_store_and_item(message):\n    return message[\"store_id\"] + \"--\" + message[\"item\"]\n\nsdf = StreamingDataFrame()\nsdf = sdf.group_by(key=groupby_store_and_item, name=\"store_item_gb\")\nsdf[\"total_quantity\"] = sdf.apply(calculate_total, stateful=True)\nsdf = sdf[[\"total_quantity\"]]\n```\n\n----------------------------------------\n\nTITLE: Printing Single Record with Metadata in Quix Streams\nDESCRIPTION: Demonstrates how to print the current record's value, key, timestamp, and headers using the StreamingDataFrame.print() method with metadata enabled.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/debugging.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(...)\n# some SDF transformations happening here ...\n\n# Print the current record's value, key, timestamp and headers\nsdf.print(metadata=True)\n# It will print the record's data wrapped into a dict for readability:\n# { 'value': {'number': 12183},\n#   'key': b'key',\n#   'timestamp': 1721129697951,\n#   'headers': [('header_name', b'header-value')]\n#   }\n```\n\n----------------------------------------\n\nTITLE: Implementing Avro Serialization with Schema Registry in Python\nDESCRIPTION: Shows how to set up Avro serializer and deserializer with Schema Registry integration in Quix Streams. The deserializer can automatically fetch the schema from the Schema Registry.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/schema-registry.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams.models.serializers.avro import AvroDeserializer, AvroSerializer\n\nMY_SCHEMA = {\n    \"type\": \"record\",\n    \"name\": \"testschema\",\n    \"fields\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"id\", \"type\": \"int\", \"default\": 0},\n    ],\n}\n\ndeserializer = AvroDeserializer(\n    schema_registry_client_config=schema_registry_client_config,\n)\nserializer = AvroSerializer(\n    schema=MY_SCHEMA,\n    schema_registry_client_config=schema_registry_client_config,\n    schema_registry_serialization_config=schema_registry_serialization_config,\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Kafka Timestamps in Quix Streams with Python\nDESCRIPTION: Demonstrates how to update the timestamp of processed items using StreamingDataFrame.set_timestamp() API. The timestamp is used in windowed aggregations and when producing messages to output topics.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nsdf = app.dataframe(...)\n\n# Update the timestamp to be the current epoch using \"set_timestamp\" \n# method with a callback.\n# The callback receives four positional arguments: value, key, current timestamp, and headers. \n# It must return a new timestamp as integer in milliseconds as well\n\nsdf = sdf.set_timestamp(lambda value, key, timestamp, headers: int(time.time() * 1000))\n```\n\n----------------------------------------\n\nTITLE: Using Elasticsearch Sink with Quix Streams Application in Python\nDESCRIPTION: This example demonstrates how to create an instance of ElasticsearchSink and use it with a Quix Streams application. It configures the sink to write data to a specified Elasticsearch index.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/elasticsearch-sink.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.elasticsearch import ElasticsearchSink\n\napp = Application(broker_address=\"localhost:9092\")\ntopic = app.topic(\"topic-name\")\n\n# Message structured as:\n# key: \"CID_12345\"\n# value: {\"name\": {\"first\": \"John\", \"last\": \"Doe\"}, \"age\": 28, \"city\": \"Los Angeles\"}\n\n# Configure the sink\nelasticsearch_sink = ElasticsearchSink(\n    url=\"http://localhost:9200\",\n    index=\"my_index\",\n)\n\nsdf = app.dataframe(topic=topic)\nsdf.sink(elasticsearch_sink)\n\n# Elasticsearch Document: \n# {\"_id\": \"CID_12345\", \"name\": {\"first\": \"John\", \"last\": \"Doe\"}, \"age\": 28, \"city\": \"Los Angeles\"}\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Initializing a Topic Object in Python\nDESCRIPTION: The constructor for the `Topic` class. It initializes a topic definition with its name, type (Regular, Repartition, Changelog), optional creation configuration (`TopicConfig`), serializers/deserializers for keys and values, and an optional timestamp extractor.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n        name: str,\n        topic_type: TopicType = TopicType.REGULAR,\n        create_config: Optional[TopicConfig] = None,\n        value_deserializer: Optional[DeserializerType] = None,\n        key_deserializer: Optional[DeserializerType] = BytesDeserializer(),\n        value_serializer: Optional[SerializerType] = None,\n        key_serializer: Optional[SerializerType] = BytesSerializer(),\n        timestamp_extractor: Optional[TimestampExtractor] = None,\n        quix_name: str = \"\")\n```\n\n----------------------------------------\n\nTITLE: Polling Kafka Producer for Events in Python with Quix Streams\nDESCRIPTION: Method to poll the Kafka producer for events and trigger delivery callbacks. It allows setting a timeout for the polling operation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef poll(timeout: float = 0)\n```\n\n----------------------------------------\n\nTITLE: Concatenating StreamingDataFrames\nDESCRIPTION: Method to concatenate two StreamingDataFrames, creating a new one that processes data from both origins. This can be used to combine dataframes from different topics or merge branches of the same original dataframe.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef concat(other: \"StreamingDataFrame\") -> \"StreamingDataFrame\"\n```\n\n----------------------------------------\n\nTITLE: Defining the Azure File Destination Class - Python\nDESCRIPTION: Defines the AzureFileDestination class inheriting from the base Destination interface, which provides a concrete implementation for persisting serialized data to Microsoft Azure Blob storage. This class is central for managing all interactions with Azure containers, leveraging the Azure Blob SDK and supporting credential configuration via direct input or environment variables. All data management, authentication mechanics, and batch writing logic are encapsulated within this class, and it forms the foundation for Azure-based output in the Quix Streams cloud storage pipeline.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nclass AzureFileDestination(Destination)\n```\n\n----------------------------------------\n\nTITLE: Initializing Redis Sink Connector in Python\nDESCRIPTION: Constructor for RedisSink that configures the connection to a Redis database. Includes parameters for Redis connection details, database selection, serialization options, and authentication.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_78\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(host: str,\n             port: int,\n             db: int,\n             value_serializer: Callable[[Any], Union[bytes, str]] = json.dumps,\n             key_serializer: Optional[Callable[[Any, Any], Union[bytes,\n                                                                 str]]] = None,\n             password: Optional[str] = None,\n             socket_timeout: float = 30.0,\n             on_client_connect_success: Optional[\n                 ClientConnectSuccessCallback] = None,\n             on_client_connect_failure: Optional[\n                 ClientConnectFailureCallback] = None,\n             **kwargs) -> None\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Serializers for CSVSink\nDESCRIPTION: Demonstrates how to customize the serialization of keys and values when writing to the CSV file using custom serializer functions.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/csv-sink.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom quixstreams.sinks.core.csv import CSVSink\n\n# Initialize a CSVSink with a file path \ncsv_sink = CSVSink(\n    path=\"file.csv\",\n    # Define custom serializers for keys and values here.\n    # The callables must accept one argument for key/value, and return a string\n    key_serializer=lambda key: json.dumps(key),\n    value_serializer=lambda value: str(value),\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Key and Timestamp Extractors for Kafka Messages (Python)\nDESCRIPTION: This Python snippet shows how to define and use custom extractor functions for message keys and timestamps when using CSVSource in Quix Streams. It demonstrates defining extractors that read specific CSV fields for use as the Kafka message key and message timestamp (in ms). Requires the typing and quixstreams libraries, a valid CSV input with the required headers, and a running Kafka broker at the specified address. Inputs are the file path, extractor functions, and source configuration; output is key-configured and timestamped Kafka messages in JSON format.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/csv-source.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import AnyStr\n\nfrom quixstreams import Application\nfrom quixstreams.sources.core.csv import CSVSource\n\n\ndef key_extractor(row: dict) -> AnyStr:\n    return row[\"field1\"]\n\n\ndef timestamp_extractor(row: dict) -> int:\n    return int(row[\"timestamp\"])\n\n\ndef main():\n    app = Application(broker_address=\"localhost:9092\")\n    # input.csv:\n    #   field1,field2,timestamp\n    #   foo1,bar1,1\n    #   foo2,bar2,2\n    #   foo3,bar3,3\n\n    source = CSVSource(\n        path=\"input.csv\",\n        name=\"csv\",\n        # Extract field \"field1\" from each row and use it as a message key.\n        # Keys must be either strings or bytes.\n        key_extractor=key_extractor,\n        # Extract field \"timestamp\" from each row and use it as a timestamp.\n        # Timestamps must be integers in milliseconds.\n        timestamp_extractor=timestamp_extractor,\n    )\n\n    sdf = app.dataframe(source=source)\n    sdf.print(metadata=True)\n\n    app.run()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Producing Word Counts to Kafka Topic with Custom Keys\nDESCRIPTION: Sends processed word counts to the output Kafka topic, using the word itself as the message key.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/word-count/tutorial.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsdf = sdf.to_topic(word_counts_topic, key=lambda word_count_pair: word_count_pair[0])\n```\n\n----------------------------------------\n\nTITLE: Flushing Kafka Producer Messages in Python with Quix Streams\nDESCRIPTION: Method to wait for all messages in the Producer queue to be delivered to Kafka. Returns the number of messages that couldn't be flushed within the timeout period.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef flush(timeout: Optional[float] = None) -> int\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Topics with TopicAdmin in Python\nDESCRIPTION: The `create_topics` method attempts to create multiple Kafka topics based on a provided list of `Topic` objects. It confirms their readiness and raises an exception on failure, ignoring errors if a topic already exists. Requires positive timeout values for creation acknowledgement and finalization.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef create_topics(topics: List[Topic],\n                  timeout: float = 30,\n                  finalize_timeout: float = 60)\n```\n\n----------------------------------------\n\nTITLE: Custom Timestamp Extractor Example\nDESCRIPTION: Example showing how to configure a custom timestamp extractor for a topic.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\napp = Application(...)\n\n\ndef custom_ts_extractor(\n    value: Any,\n    headers: Optional[List[Tuple[str, bytes]]],\n    timestamp: float,\n    timestamp_type: TimestampType,\n) -> int:\n    return value[\"timestamp\"]\n\ntopic = app.topic(\"input-topic\", timestamp_extractor=custom_ts_extractor)\n```\n\n----------------------------------------\n\nTITLE: Initializing FileSource with Configuration Options\nDESCRIPTION: Constructor for FileSource that accepts parameters for filepath, record processing, file format, compression, partition handling, and replay speed. It configures how files will be processed and records will be sent to Kafka.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(filepath: Union[str, Path],\n             key_setter: Optional[Callable[[object], object]] = None,\n             value_setter: Optional[Callable[[object], object]] = None,\n             timestamp_setter: Optional[Callable[[object], int]] = None,\n             file_format: Union[Format, FormatName] = \"json\",\n             compression: Optional[CompressionName] = None,\n             has_partition_folders: bool = False,\n             replay_speed: float = 1.0,\n             name: Optional[str] = None,\n             shutdown_timeout: float = 30,\n             on_client_connect_success: Optional[\n                 ClientConnectSuccessCallback] = None,\n             on_client_connect_failure: Optional[\n                 ClientConnectFailureCallback] = None)\n```\n\n----------------------------------------\n\nTITLE: Composing Functions with StreamingSeries.compose_returning() in Python\nDESCRIPTION: Shows how to compose a list of functions from a StreamingSeries and its parents into one closure that returns the transformed record. This is used to execute functions in the stream and get transformation results.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndef compose_returning() -> ReturningExecutor\n```\n\n----------------------------------------\n\nTITLE: Example Usage of PubSubSource\nDESCRIPTION: Demonstrates how to configure and use PubSubSource with a Quix Streams Application, including service account authentication and topic configuration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources.community.pubsub import PubSubSource\nfrom os import environ\n\nsource = PubSubSource(\n    # Suggested: pass JSON-formatted credentials from an environment variable.\n    service_account_json = environ[\"PUBSUB_SERVICE_ACCOUNT_JSON\"],\n    project_id=\"<project ID>\",\n    topic_id=\"<topic ID>\",  # NOTE: NOT the full /x/y/z path!\n    subscription_id=\"<subscription ID>\",  # NOTE: NOT the full /x/y/z path!\n    create_subscription=True,\n)\napp = Application(\n    broker_address=\"localhost:9092\",\n    auto_offset_reset=\"earliest\",\n    consumer_group=\"gcp\",\n    loglevel=\"INFO\"\n)\nsdf = app.dataframe(source=source).print(metadata=True)\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Initializing AWS Iceberg Config in Python\nDESCRIPTION: Constructor for AWSIcebergConfig class that configures AWS credentials and connection settings for Apache Iceberg tables stored in AWS S3 and managed by AWS Glue.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_57\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(aws_s3_uri: str,\n             aws_region: Optional[str] = None,\n             aws_access_key_id: Optional[str] = None,\n             aws_secret_access_key: Optional[str] = None,\n             aws_session_token: Optional[str] = None)\n```\n\n----------------------------------------\n\nTITLE: Writing Batches to Iceberg Tables in Python\nDESCRIPTION: Method for writing batches of data to Iceberg tables with retry logic for handling concurrent write conflicts.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_59\n\nLANGUAGE: python\nCODE:\n```\ndef write(batch: SinkBatch)\n```\n\n----------------------------------------\n\nTITLE: Accessing Topic, Partition and Offset in Quix Streams using message_context()\nDESCRIPTION: Demonstrates how to access additional Kafka message metadata (topic name, partition, offset) using the message_context() function from the quixstreams module. This function returns a MessageContext instance with message-specific fields.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import message_context\n\n# Print the offset of the current Kafka message\nsdf = sdf.update(lambda _: print('Current offset: ', message_context().offset))\n```\n\n----------------------------------------\n\nTITLE: Assigning StreamingDataFrame Operations in Quix Streams (Python)\nDESCRIPTION: Demonstrates the recommended pattern of assigning every StreamingDataFrame operation to a variable, regardless of whether the method is in-place. Shows typical initialization and chaining of operations using the Quix Streams Python API, including the use of apply, update, and to_topic methods. No external dependencies are required beyond the quixstreams package.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/dataframe-assignments.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# NOTE: this is an incomplete stub just to show usage\\nfrom quixstreams import Application\\n\\nsdf = Application().dataframe()\\nsdf = sdf.apply()\\nsdf = sdf.update()  # in-place with assigning!\\nsdf = sdf.apply().to_topic()\n```\n\n----------------------------------------\n\nTITLE: Initializing Quix Streams Application with SDK Token\nDESCRIPTION: This snippet demonstrates how to initialize a Quix Streams Application outside of Quix Cloud by passing the SDK Token as an argument.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/quix-platform.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\napp = Application(quix_sdk_token=\"MY_TOKEN\")\n```\n\n----------------------------------------\n\nTITLE: Declaring Producer Class in Python for Quix Streams Kafka Client\nDESCRIPTION: Defines a Producer class that wraps confluent_kafka.Producer, providing typed interfaces and reasonable defaults. It initializes the underlying producer on demand to avoid network calls during instantiation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Producer()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Kafka Topic Configurations with TopicAdmin in Python\nDESCRIPTION: The `inspect_topics` method fetches the configuration details for a list of specified topic names from the Kafka cluster. It returns a dictionary mapping topic names to their `TopicConfig` objects, or `None` if a topic doesn't exist. Requires a positive timeout value.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef inspect_topics(topic_names: List[str],\n                   timeout: float = 30) -> Dict[str, Optional[TopicConfig]]\n```\n\n----------------------------------------\n\nTITLE: Initializing Quix Streams Application for Anomaly Detection\nDESCRIPTION: Creates a Quix Streams Application with specified broker address, consumer group, and offset reset configuration. This setup is essential for connecting to Kafka and managing message consumption.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/anomaly-detection/tutorial.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom quixstreams import Application\n\napp = Application(\n    broker_address=os.getenv(\"BROKER_ADDRESS\", \"localhost:9092\"),\n    consumer_group=\"temperature_alerter\",\n    auto_offset_reset=\"earliest\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining CSVSource Class in Quix Streams (Python)\nDESCRIPTION: Defines the `CSVSource` class, which inherits from the base `Source` class. This class provides functionality to read data from a specified CSV file and produce it row by row.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n```python\nclass CSVSource(Source)\n```\n```\n\n----------------------------------------\n\nTITLE: Setting Up Local Azure Emulator for Testing\nDESCRIPTION: Docker command to run Azurite, an Azure emulator, for local testing without connecting to actual Azure services.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/microsoft-azure-file-source.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -d --name azurite \\\n-p 10000:10000 \\\nmcr.microsoft.com/azure-storage/azurite:latest\n```\n\n----------------------------------------\n\nTITLE: Implementing Protobuf Serialization with Schema Registry in Python\nDESCRIPTION: Illustrates how to set up Protobuf serializer and deserializer with Schema Registry integration in Quix Streams. Both serializer and deserializer require a message type and can accept optional serialization configuration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/schema-registry.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams.models.serializers.protobuf import ProtobufDeserializer, ProtobufSerializer\n\nfrom my_input_models_pb2 import InputProto\nfrom my_output_models_pb2 import OutputProto\n\ndeserializer = ProtobufDeserializer(\n    msg_type=InputProto,\n    schema_registry_client_config=schema_registry_client_config,\n    schema_registry_serialization_config=schema_registry_serialization_config,\n)\nserializer = ProtobufSerializer(\n    msg_type=OutputProto,\n    schema_registry_client_config=schema_registry_client_config,\n    schema_registry_serialization_config=schema_registry_serialization_config,\n)\n```\n\n----------------------------------------\n\nTITLE: MongoDB Sink Class Implementation in Python\nDESCRIPTION: Class for implementing a MongoDB sink that handles batched writes.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_65\n\nLANGUAGE: python\nCODE:\n```\nclass MongoDBSink(BatchingSink)\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Serialization in Quix Streams\nDESCRIPTION: Demonstrates how to configure basic serialization using string shorthand for JSON and string serialization of Kafka messages. Shows setup for both input and output topics with different serialization formats.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/serialization.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\napp = Application(broker_address='localhost:9092', consumer_group='consumer')\n# Deserializing message values from JSON to objects and message keys as strings \ninput_topic = app.topic('input', value_deserializer='json', key_deserializer='string')\n\n# Serializing message values to JSON and message keys to bytes\noutput_topic = app.topic('output', value_serializer='json', key_deserializer='bytes')\n```\n\n----------------------------------------\n\nTITLE: Updating Kafka Headers in Quix Streams with Python\nDESCRIPTION: Demonstrates how to set or update Kafka message headers using StreamingDataFrame.set_headers() API. The updated headers will be attached when producing messages to output topics.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(...)\n\nAPP_VERSION = \"v0.1.1\"\n\n# Add the value of APP_VERSION to the message headers for debugging purposes.  \n# The callback receives four positional arguments: value, key, current timestamp, and headers. \n# It must return a new set of headers as a list of (header, value) tuples.\n\nsdf = sdf.set_headers(\n    lambda value, key, timestamp, headers: [('APP_VERSION', APP_VERSION.encode())]\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing ProtobufDeserializer Class in Python\nDESCRIPTION: Defines the constructor for the ProtobufDeserializer, which decodes protobuf-encoded data into dictionaries or message objects. Dependencies include the protobuf library and optional schema registry integration. Key parameters adjust parsing preferences (dict output, enum representation, field name casing) and support for schema registry configurations, with expected input as protobuf data and output as a dictionary or protobuf message.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n    msg_type: Type[Message],\n    use_integers_for_enums: bool = False,\n    preserving_proto_field_name: bool = False,\n    to_dict: bool = True,\n    schema_registry_client_config: Optional[SchemaRegistryClientConfig] = None,\n    schema_registry_serialization_config: Optional[\n        SchemaRegistrySerializationConfig] = None)\n```\n\n----------------------------------------\n\nTITLE: Initializing StreamingDataFrame with Temperature Generator\nDESCRIPTION: Creates a StreamingDataFrame using a custom TemperatureGenerator source. This setup allows for ingesting non-Kafka data into the streaming pipeline.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/anomaly-detection/tutorial.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(source=TemperatureGenerator())\n```\n\n----------------------------------------\n\nTITLE: Example of Kafka Headers Format in Quix Streams\nDESCRIPTION: Shows the format of Kafka headers as represented in Quix Streams. Headers are key-value pairs providing additional information about messages without modifying the payload.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n[('clientID', b'client-123'), ('source', b'source-123')]\n```\n\n----------------------------------------\n\nTITLE: Application Class Definition\nDESCRIPTION: Core Application class definition for Quix Streams framework.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Application()\n```\n\n----------------------------------------\n\nTITLE: Creating Application Instance for Kafka in Python\nDESCRIPTION: This snippet demonstrates how to create an Application instance with Kafka configuration using Quix Streams. The Application is the main entry point for creating Producers, Topics, and other necessary objects.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/producer.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n\n# Create an Application instance with Kafka config\napp = Application(broker_address='localhost:9092')\n```\n\n----------------------------------------\n\nTITLE: Defining BaseSource Class in Python\nDESCRIPTION: The BaseSource class is the abstract base class for all sources in Quix Streams. It defines the interface that all sources must implement, including methods for starting, stopping, and configuring the source.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BaseSource(ABC)\n```\n\n----------------------------------------\n\nTITLE: Incrementally Unassigning Partitions from Kafka Consumer in Python\nDESCRIPTION: Method to revoke partitions from a Kafka consumer. This can be called outside an on_revoke callback to remove specific partitions from the consumer's assignment.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef incremental_unassign(partitions: List[TopicPartition])\n```\n\n----------------------------------------\n\nTITLE: Initializing AvroDeserializer Class in Python\nDESCRIPTION: Defines the constructor for the AvroDeserializer, enabling the deserialization of Avro-formatted data streams using a provided or evolving schema. Requires the fastavro library and supports schema migration, handling of record names, named types, and Unicode errors. Key parameters include schema, reader_schema, and schema_registry_client_config, with additional options for advanced union handling and error control; inputs are Avro-encoded data with expected output as deserialized Python objects.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n    schema: Optional[Schema] = None,\n    reader_schema: Optional[Schema] = None,\n    return_record_name: bool = False,\n    return_record_name_override: bool = False,\n    return_named_type: bool = False,\n    return_named_type_override: bool = False,\n    handle_unicode_errors: str = \"strict\",\n    schema_registry_client_config: Optional[SchemaRegistryClientConfig] = None\n)\n```\n\n----------------------------------------\n\nTITLE: MongoDB Sink Initialization in Python\nDESCRIPTION: Constructor for MongoDBSink that configures connection details and document update behaviors.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_66\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(url: str,\n             db: str,\n             collection: str,\n             document_matcher: Callable[\n                 [SinkItem], MongoQueryFilter] = _default_document_matcher,\n             update_method: Literal[\"UpdateOne\", \"UpdateMany\",\n                                    \"ReplaceOne\"] = \"UpdateOne\",\n             upsert: bool = True,\n             add_message_metadata: bool = False,\n             add_topic_metadata: bool = False,\n             authentication_timeout_ms: int = 15000,\n             value_selector: Optional[Callable[[MongoValue],\n                                               MongoValue]] = None,\n             **kwargs) -> None\n```\n\n----------------------------------------\n\nTITLE: Displaying StreamingDataFrame as Live Table\nDESCRIPTION: This snippet shows how to display a StreamingDataFrame as a live-updating table. It creates a table view that updates in real-time, showing the most recent records with optional metadata columns. The table can be customized with various parameters for size, columns, and update frequency.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(topic)\n# Show last 5 records, update at most every 1 second\nsdf.print_table(size=5, title=\"Live Records\", slowdown=1)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Message Context in Quix Streams Python\nDESCRIPTION: This function retrieves the MessageContext for the current message, which contains metadata such as key, timestamp, partition, and offset. It's useful for accessing message metadata in streaming applications.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/context.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef message_context() -> MessageContext\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application, message_context\n\n# Changes the current sdf value based on what the message partition is.\n\napp = Application()\nsdf = app.dataframe()\nsdf = sdf.apply(lambda value: 1 if message_context().partition == 2 else 0)\n```\n\n----------------------------------------\n\nTITLE: Setting Headers in StreamingDataFrame\nDESCRIPTION: This snippet shows how to set new message headers for a StreamingDataFrame based on the current message value and metadata. The new headers will be used when producing messages to output topics.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n\napp = Application()\ninput_topic = app.topic(\"data\")\n\nsdf = app.dataframe(input_topic)\n# Updating the record's headers based on the value and metadata\nsdf = sdf.set_headers(lambda value, key, timestamp, headers: [('id', value['id'])])\n```\n\n----------------------------------------\n\nTITLE: Handling Producer Errors in Quix Streams Application\nDESCRIPTION: This function illustrates how to handle and ignore producer exceptions in a Quix Streams application using the on_producer_error callback.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/configuration.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef on_producer_error(exc: Exception, row, logger) -> bool:\n    \"\"\"\n    Handle the producer exception and ignore it\n    \"\"\"\n    logger.error('Ignore producer exception exc=%s row=%s', exc, row)\n    return True\n```\n\n----------------------------------------\n\nTITLE: Checking Object Non-Identity with StreamingSeries.isnot() in Python\nDESCRIPTION: Shows how to check if a StreamingSeries value does not refer to the same object as another value. Returns a boolean result that can be assigned to a new column in the StreamingDataFrame.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Check if \"column_a\" is the same as \"column_b\" and assign the resulting `bool`\n# to a new column: \"is_not_same\"\n\nsdf = app.dataframe()\nsdf[\"is_not_same\"] = sdf[\"column_a\"].isnot(sdf[\"column_b\"])\n```\n\n----------------------------------------\n\nTITLE: Initializing StreamingDataFrame with Review Generator\nDESCRIPTION: Creates a StreamingDataFrame using a custom ReviewGenerator source for ingesting text data.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/word-count/tutorial.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(source=ReviewGenerator())\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using QuixEnvironmentSource in Python\nDESCRIPTION: This code snippet demonstrates how to create and use a QuixEnvironmentSource instance within a Quix Streams application. It shows the setup of the Application, creation of the QuixEnvironmentSource, and how to use it with app.dataframe() method.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/quix-source.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources.core.kafka import QuixEnvironmentSource\n\ndef main():\n    app = Application()\n    source = QuixEnvironmentSource(\n      name=\"my-source\",\n      app_config=app.config,\n      topic=\"source-topic\",\n      quix_sdk_token=\"quix-sdk-token\",\n      quix_workspace_id=\"quix-workspace-id\",\n    )\n    \n    sdf = app.dataframe(source=source)\n    sdf.print(metadata=True)\n    \n    app.run()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Checking Values in Container with StreamingSeries.isin() in Python\nDESCRIPTION: Demonstrates how to check if a StreamingSeries value is contained in another container. Returns a boolean result that can be assigned to a new column in the StreamingDataFrame.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Check if \"str_column\" is contained in a column with a list of strings and\n# assign the resulting `bool` to a new column: \"has_my_str\".\n\nsdf = app.dataframe()\nsdf[\"has_my_str\"] = sdf[\"str_column\"].isin(sdf[\"column_with_list_of_strs\"])\n```\n\n----------------------------------------\n\nTITLE: Accessing split_values Property in QuixDeserializer Python\nDESCRIPTION: Implements the 'split_values' property for QuixDeserializer, indicating whether a single message may contain multiple data rows. This property informs downstream processors to expect an iterable rather than a singular mapping, aiding compatibility with batched data processing. No dependencies required; property is read-only and intended for use within serialization workflows.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef split_values() -> bool\n```\n\n----------------------------------------\n\nTITLE: S3 File Source Initialization Method\nDESCRIPTION: Constructor method for S3FileSource that configures AWS credentials, file processing options, and message handling settings.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_47\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n        filepath: Union[str, Path],\n        bucket: str,\n        region_name: Optional[str] = getenv(\"AWS_REGION\"),\n        aws_access_key_id: Optional[str] = getenv(\"AWS_ACCESS_KEY_ID\"),\n        aws_secret_access_key: Optional[str] = getenv(\"AWS_SECRET_ACCESS_KEY\"),\n        endpoint_url: Optional[str] = getenv(\"AWS_ENDPOINT_URL_S3\"),\n        key_setter: Optional[Callable[[object], object]] = None,\n        value_setter: Optional[Callable[[object], object]] = None,\n        timestamp_setter: Optional[Callable[[object], int]] = None,\n        has_partition_folders: bool = False,\n        file_format: Union[Format, FormatName] = \"json\",\n        compression: Optional[CompressionName] = None,\n        replay_speed: float = 1.0,\n        name: Optional[str] = None,\n        shutdown_timeout: float = 30,\n        on_client_connect_success: Optional[\n            ClientConnectSuccessCallback] = None,\n        on_client_connect_failure: Optional[\n            ClientConnectFailureCallback] = None)\n```\n\n----------------------------------------\n\nTITLE: Neo4j Sink Class Implementation in Python\nDESCRIPTION: Class for implementing a Neo4j sink that handles batched writes using Cypher queries.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_68\n\nLANGUAGE: python\nCODE:\n```\nclass Neo4jSink(BatchingSink)\n```\n\n----------------------------------------\n\nTITLE: Defining the TopicConfig Dataclass in Python\nDESCRIPTION: Defines the `TopicConfig` dataclass, representing the Kafka-level configuration for a topic, such as partition count and replication factor. It's used by the `Topic` class and topic creation procedures.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dataclasses.dataclass(eq=True, frozen=True)\nclass TopicConfig()\n```\n\n----------------------------------------\n\nTITLE: Testing StreamingSeries with Sample Values in Python\nDESCRIPTION: Shows how to test a StreamingSeries instance with provided value, key, timestamp, headers, and context. This method is useful for testing transformations without running a full streaming pipeline.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef test(value: Any,\n         key: Any,\n         timestamp: int,\n         headers: Optional[Any] = None,\n         ctx: Optional[MessageContext] = None) -> Any\n```\n\n----------------------------------------\n\nTITLE: Initializing InfluxDB3 Sink Connector - Python\nDESCRIPTION: Defines the constructor for the InfluxDB3Sink, which collects processed records in memory, converts them to InfluxDB format, and flushes them in batches. It supports advanced features like field/tag selection, batching, gzip, time precision, error handling and custom client connection callbacks. Requires InfluxDB Python client and expects records as dictionaries, with numerous configurable parameters for integration in streaming data pipelines.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(token: str,\\n             host: str,\\n             organization_id: str,\\n             database: str,\\n             measurement: MeasurementSetter,\\n             fields_keys: FieldsSetter = (),\\n             tags_keys: TagsSetter = (),\\n             time_key: Optional[str] = None,\\n             time_precision: TimePrecision = \\\"ms\\\",\\n             allow_missing_fields: bool = False,\\n             include_metadata_tags: bool = False,\\n             convert_ints_to_floats: bool = False,\\n             batch_size: int = 1000,\\n             enable_gzip: bool = True,\\n             request_timeout_ms: int = 10_000,\\n             debug: bool = False,\\n             on_client_connect_success: Optional[\\n                 ClientConnectSuccessCallback] = None,\\n             on_client_connect_failure: Optional[\\n                 ClientConnectFailureCallback] = None)\n```\n\n----------------------------------------\n\nTITLE: Filtering Common Words from Word Counts\nDESCRIPTION: Implements a filter function to remove common filler words from the word count results.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/word-count/tutorial.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef should_skip(word_count_pair):\n    word, count = word_count_pair\n    return word not in ['i', 'a', 'we', 'it', 'is', 'and', 'or', 'the']\n\nsdf = sdf.filter(should_skip)\n```\n\n----------------------------------------\n\nTITLE: PandasDataFrameSource Constructor Implementation\nDESCRIPTION: Implementation of PandasDataFrameSource initialization with configuration for DataFrame reading and message production settings.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_57\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(df: pd.DataFrame,\n             key_column: str,\n             timestamp_column: str = None,\n             delay: float = 0,\n             shutdown_timeout: float = 10,\n             keep_meta_as_values: bool = True,\n             name: str = \"pandas-dataframe-source\") -> None\n```\n\n----------------------------------------\n\nTITLE: Illustrating Output Order for Expanded SDF Branches\nDESCRIPTION: This text block illustrates the sequence of output messages generated by the previous Python branching example (`sdf_a` and `sdf_b` applied after expansion) when the input is `[0, 1]`. It demonstrates that for each input element (0 and 1), both branches execute before moving to the next element, resulting in an interleaved output (10, 20, 11, 21).\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/branching.md#2025-04-23_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n10  # 0 via sdf_a: 0 + 10\n20  # 0 via sdf_b: 0 + 20\n11  # 1 via sdf_a: 1 + 10\n21  # 1 via sdf_b: 1 + 20\n```\n\n----------------------------------------\n\nTITLE: Producing Messages from a Source in Python\nDESCRIPTION: The produce method sends a message to the configured source topic in Kafka with optional parameters like key, headers, partition, and timestamp.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef produce(value: Optional[Union[str, bytes]] = None,\n            key: Optional[Union[str, bytes]] = None,\n            headers: Optional[Headers] = None,\n            partition: Optional[int] = None,\n            timestamp: Optional[int] = None,\n            poll_timeout: float = PRODUCER_POLL_TIMEOUT,\n            buffer_error_max_tries: int = PRODUCER_ON_ERROR_RETRIES) -> None\n```\n\n----------------------------------------\n\nTITLE: Writing Data to Amazon S3 via Destination - Python\nDESCRIPTION: Provides the logic for uploading serialized data to a target S3 bucket, using batch context (topic and partition) for object naming. Expects valid AWS credentials and previously established connectivity, and will propagate custom exceptions if bucket access or existence checks fail at write time. Relies on boto3 under the hood and assumes that files are organized within S3 according to the pipeline's partitioned structure.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndef write(data: bytes, batch: SinkBatch) -> None\n```\n\n----------------------------------------\n\nTITLE: Initializing QuixEnvironmentSource in Python\nDESCRIPTION: Constructor for QuixEnvironmentSource class that enables replication from Quix Cloud environments. Extends KafkaReplicatorSource with Quix-specific configuration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n    name: str,\n    app_config: \"ApplicationConfig\",\n    topic: str,\n    quix_sdk_token: str,\n    quix_workspace_id: str,\n    quix_portal_api: Optional[str] = None,\n    auto_offset_reset: Optional[AutoOffsetReset] = None,\n    consumer_extra_config: Optional[dict] = None,\n    consumer_poll_timeout: Optional[float] = None,\n    shutdown_timeout: float = 10,\n    on_consumer_error: ConsumerErrorCallback = default_on_consumer_error,\n    value_deserializer: DeserializerType = \"json\",\n    key_deserializer: DeserializerType = \"bytes\",\n    on_client_connect_success: Optional[ClientConnectSuccessCallback] = None,\n    on_client_connect_failure: Optional[ClientConnectFailureCallback] = None\n) -> None\n```\n\n----------------------------------------\n\nTITLE: Initializing AzureFileSource in Python\nDESCRIPTION: Constructor for AzureFileSource class that handles processing of files stored in Azure Filestore containers. Supports various file formats and compression types.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(connection_string: str,\n             container: str,\n             filepath: Union[str, Path],\n             key_setter: Optional[Callable[[object], object]] = None,\n             value_setter: Optional[Callable[[object], object]] = None,\n             timestamp_setter: Optional[Callable[[object], int]] = None,\n             file_format: Union[Format, FormatName] = \"json\",\n             compression: Optional[CompressionName] = None,\n             has_partition_folders: bool = False,\n             replay_speed: float = 1.0,\n             name: Optional[str] = None,\n             shutdown_timeout: float = 30,\n             on_client_connect_success: Optional[\n                 ClientConnectSuccessCallback] = None,\n             on_client_connect_failure: Optional[\n                 ClientConnectFailureCallback] = None)\n```\n\n----------------------------------------\n\nTITLE: Initializing KafkaReplicatorSource in Python\nDESCRIPTION: Constructor for KafkaReplicatorSource class that handles replication of Kafka topics. Takes configuration for source and target brokers, deserialization settings, and error handling callbacks.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n    name: str,\n    app_config: \"ApplicationConfig\",\n    topic: str,\n    broker_address: Union[str, ConnectionConfig],\n    auto_offset_reset: Optional[AutoOffsetReset] = \"latest\",\n    consumer_extra_config: Optional[dict] = None,\n    consumer_poll_timeout: Optional[float] = None,\n    shutdown_timeout: float = 10,\n    on_consumer_error: ConsumerErrorCallback = default_on_consumer_error,\n    value_deserializer: DeserializerType = \"json\",\n    key_deserializer: DeserializerType = \"bytes\",\n    on_client_connect_success: Optional[ClientConnectSuccessCallback] = None,\n    on_client_connect_failure: Optional[ClientConnectFailureCallback] = None\n) -> None\n```\n\n----------------------------------------\n\nTITLE: Configuring StreamingDataFrame Operations in Python\nDESCRIPTION: This snippet demonstrates how to set up a StreamingDataFrame with a CoinbaseSource, print each record, select specific columns, and output to a Kafka topic.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/websocket-source/tutorial.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(source=coinbase_source)\nsdf.print()\nsdf = sdf[['price', 'volume_24h']]\nsdf.to_topic(price_updates_topic)\n```\n\n----------------------------------------\n\nTITLE: Retrieving All Topics with TopicManager - Python\nDESCRIPTION: This property method returns a dictionary mapping each registered topic name to its corresponding Topic object. It provides the complete set of topics currently managed by the TopicManager. No parameters are required, and the output is a Python dictionary with topic names as keys and Topic objects as values. Requires Quix Streams with topics properly registered.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@property\\ndef all_topics() -> Dict[str, Topic]\n```\n\n----------------------------------------\n\nTITLE: Branching Examples with Sink Operations in Quix Streams (Python)\nDESCRIPTION: This snippet illustrates different approaches to branching when using sink operations in Quix Streams. It shows how to maintain the ability to branch after using SDF.sink() and provides a suggested usage pattern for sink operations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(topic)\nsdf = sdf.apply()\n\n# Approach 1... Allows branching from `sdf`\nsdf.sink()\n\n# Approach 2...Disables branching from `sdf`\nsdf = sdf.sink()\n\n# Suggested Use\nsdf = app.dataframe(topic)\n# [other operations here...]\nsdf = sdf.apply().apply()  # last transforms before a sink\nsdf.sink(influx_sink)  # do sink as a standalone call, no reassignment\nsdf = sdf.apply()  # continue different operations with another branch...\n```\n\n----------------------------------------\n\nTITLE: Running Quix Streams Consumer in Python\nDESCRIPTION: Command to run the consumer.py script, which processes the chat messages from the Kafka topic, splits them into words, and calculates word lengths using Quix Streams.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/quickstart.md#2025-04-23_snippet_4\n\nLANGUAGE: commandline\nCODE:\n```\npython consumer.py\n```\n\n----------------------------------------\n\nTITLE: Clearing State Data in Quix Streams Application\nDESCRIPTION: This snippet demonstrates how to clear all state data for a given consumer group in a Quix Streams Application. It uses the clear_state() method to delete all stored state data.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/stateful-processing.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\napp = Application(broker_address='localhost:9092')\n\n# Delete state for the app with consumer group \"consumer\"\napp.clear_state()\n```\n\n----------------------------------------\n\nTITLE: Visualizing SDF Expansion and Branching Flow\nDESCRIPTION: This diagram graphically represents the data flow for the StreamingDataFrame (SDF) expansion and branching example. It shows how the input list `[0, 1]` is expanded, and then each element (0 and 1) is processed independently by two branches, resulting in four distinct outputs (10, 20, 11, 21).\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/branching.md#2025-04-23_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nSDF\n└── [0, 1] INPUT\n    └── EXPAND\n        ├── 0\n        │   ├── + 10 = 10\n        │   └── + 20 = 20\n        └── 1\n            ├── + 10 = 11\n            └── + 20 = 21\n```\n\n----------------------------------------\n\nTITLE: Kinesis Source Example Usage\nDESCRIPTION: Example code demonstrating how to use KinesisSource with the Quix Streams Application framework.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources.community.kinesis import KinesisSource\n\n\nkinesis = KinesisSource(\n    stream_name=\"<YOUR STREAM>\",\n    aws_access_key_id=\"<YOUR KEY ID>\",\n    aws_secret_access_key=\"<YOUR SECRET KEY>\",\n    aws_region=\"<YOUR REGION>\",\n    auto_offset_reset=\"earliest\",  # start from the beginning of the stream (vs end)\n)\n\napp = Application(\n    broker_address=\"<YOUR BROKER INFO>\",\n    consumer_group=\"<YOUR GROUP>\",\n)\n\nsdf = app.dataframe(source=kinesis).print(metadata=True)\n# YOUR LOGIC HERE!\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Initializing S3Destination with Credentials and Options - Python\nDESCRIPTION: Defines the constructor for S3Destination, requiring the bucket name and supporting optional AWS credential arguments, region, endpoint URL, and additional parameters for boto3.client. Credentials can be explicitly passed or inferred via environment variables to support different runtime environments. If connectivity or permissions issues arise, custom exceptions (S3BucketNotFoundError, S3BucketAccessDeniedError) are raised to signal setup errors.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(bucket: str,\n             aws_access_key_id: Optional[str] = getenv(\"AWS_ACCESS_KEY_ID\"),\n             aws_secret_access_key: Optional[str] = getenv(\n                 \"AWS_SECRET_ACCESS_KEY\"),\n             region_name: Optional[str] = getenv(\"AWS_REGION\",\n                                                 getenv(\"AWS_DEFAULT_REGION\")),\n             endpoint_url: Optional[str] = getenv(\"AWS_ENDPOINT_URL_S3\"),\n             **kwargs) -> None\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Iceberg Sink Dependencies for AWS Glue\nDESCRIPTION: Command to install the necessary dependencies for using IcebergSink with AWS Glue data catalog.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/apache-iceberg-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: commandline\nCODE:\n```\npip install quixstreams[iceberg_aws]\n```\n\n----------------------------------------\n\nTITLE: Deserializing Data with QuixDeserializer Python\nDESCRIPTION: Defines the 'deserialize' method for QuixDeserializer, converting JSON data (either a list of dictionaries or a single dictionary) into an iterable of mappings according to a provided model_key. Key parameters include 'model_key', typically the message header value, and 'value', the JSON-parsed message body. Returns an iterable of dictionaries, enabling processing of timeseries or event data, with the ability to handle both singular and multiple rows.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef deserialize(model_key: str, value: Union[List[Mapping],                                             Mapping]) -> Iterable[Mapping]\n```\n\n----------------------------------------\n\nTITLE: Example Usage of QuixEnvironmentSource in Python\nDESCRIPTION: Example showing how to set up and use QuixEnvironmentSource to replicate data from a Quix Cloud environment to a local application broker.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources.kafka import QuixEnvironmentSource\n\napp = Application(\n    consumer_group=\"group\",\n)\n\nsource = QuixEnvironmentSource(\n    name=\"source-quix\",\n    app_config=app.config,\n    quix_workspace_id=\"WORKSPACE_ID\",\n    quix_sdk_token=\"WORKSPACE_SDK_TOKEN\",\n    topic=\"quix-source-topic\",\n)\n\nsdf = app.dataframe(source=source)\nsdf = sdf.print()\napp.run()\n```\n\n----------------------------------------\n\nTITLE: Custom Serialization for State Storage in Quix Streams\nDESCRIPTION: This code example shows how to use custom serialization for state storage in Quix Streams. It demonstrates using the Python pickle module for serialization and deserialization of all store data.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/stateful-processing.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\n\nfrom quixstreams import Application\napp = Application(\n    broker_address='localhost:9092', \n    rocksdb_options=RocksDBOptions(dumps=pickle.dumps, loads=pickle.loads) \n)\n```\n\n----------------------------------------\n\nTITLE: Custom Topic Configuration for CSVSource\nDESCRIPTION: Shows how to customize the Kafka topic used by a CSVSource by creating a Topic object with specific configuration parameters. This example sets up a custom topic with four partitions and a replication factor of 1.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources import CSVSource\nfrom quixstreams.models.topics import TopicConfig\n\ndef main():\n    app = Application()\n    # Create a CSVSource\n    source = CSVSource(path=\"input.csv\")\n    \n    # Define a topic for the CSVSource with a custom config\n    topic = app.topic(\"my_csv_source\", config=TopicConfig(num_partitions=4, replication_factor=1))\n    \n    # Pass the topic together with the CSVSource to a dataframe\n    # When the CSVSource starts, it will produce data to this topic\n    sdf = app.dataframe(topic=topic, source=source)\n    sdf.print(metadata=True)\n    \n    app.run()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Neo4j Sink Initialization in Python\nDESCRIPTION: Constructor for Neo4jSink that configures connection details and Cypher query execution.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_69\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(host: str,\n             port: int,\n             username: str,\n             password: str,\n             cypher_query: str,\n             chunk_size: int = 10000,\n             **kwargs) -> None\n```\n\n----------------------------------------\n\nTITLE: Polling Kafka Consumer for Messages in Python with Quix Streams\nDESCRIPTION: Method to consume a single message from Kafka, call callbacks, and handle events. It blocks for the specified timeout waiting for a message or returns None if no message is available.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef poll(\n    timeout: Optional[float] = None\n) -> Optional[RawConfluentKafkaMessageProto]\n```\n\n----------------------------------------\n\nTITLE: Avoided Pattern: Assigning Intermediate Filter Operation (Quix Streams, Python)\nDESCRIPTION: Shows a discouraged pattern in Quix Streams usage where the result of sdf.apply(f) is assigned to a temporary variable and later used as a filter. This approach can lead to unexpected behavior, as intermediate operation references may not function correctly. For correct filtering, the application and use should be combined in one statement.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/dataframe-assignments.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmy_filter = sdf.apply(f)\\nsdf = sdf[my_filter]\n```\n\n----------------------------------------\n\nTITLE: Declaring CSVSink Batch File Output - Python\nDESCRIPTION: Declares the CSVSink class inheriting from BatchingSink, designed to write data from all assigned partitions into a single CSV file. Intended for local debugging and uses a standard column format (key, value, timestamp, topic, partition, offset). Requires base class BatchingSink and Python's csv/json libraries for serialization.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass CSVSink(BatchingSink)\n```\n\n----------------------------------------\n\nTITLE: Implementing S3 File Source Class in Python\nDESCRIPTION: A source class for extracting records stored within files in an S3 bucket location. It recursively processes files and produces records as Kafka messages.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nclass S3FileSource(FileSource)\n```\n\n----------------------------------------\n\nTITLE: Protobuf Serialization Configuration\nDESCRIPTION: Demonstrates Protocol Buffers serialization setup. Requires protobuf library and shows configuration for both input and output topics using Protobuf message types.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/serialization.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.models.serializers.protobuf import ProtobufSerializer, ProtobufDeserializer\n\nfrom my_input_models_pb2 import InputProto\nfrom my_output_models_pb2 import OutputProto\n\napp = Application(broker_address='localhost:9092', consumer_group='consumer')\ninput_topic = app.topic('input', value_deserializer=ProtobufDeserializer(msg_type=InputProto))\noutput_topic = app.topic('output', value_serializer=ProtobufSerializer(msg_type=OutputProto))\n```\n\n----------------------------------------\n\nTITLE: Flushing Accumulated Batches in BatchingSink\nDESCRIPTION: Implementation of the flush method that writes accumulated batches to the destination and drops them afterward, typically called during checkpoint operations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef flush()\n```\n\n----------------------------------------\n\nTITLE: Initializing QuixDeserializer Constructor Python\nDESCRIPTION: Defines the QuixDeserializer constructor, allowing injection of a custom JSON 'loads' function to parse bytes or bytearrays. This enables flexible parsing strategies, defaulting to quixstreams.utils.json.loads if not specified. Essential for deserializing incoming Quix messages from their serialized (byte) format to native Python objects.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(loads: Callable[[Union[bytes, bytearray]], Any] = default_loads)\n```\n\n----------------------------------------\n\nTITLE: Implementing Kinesis Source with Quix Streams\nDESCRIPTION: Example code showing how to configure and use KinesisSource with a Quix Streams Application. Demonstrates basic setup including AWS credentials, stream configuration, and application initialization.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/amazon-kinesis-source.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources.community.kinesis import KinesisSource\n\n\nkinesis = KinesisSource(\n    stream_name=\"<YOUR STREAM>\",\n    aws_access_key_id=\"<YOUR KEY ID>\",\n    aws_secret_access_key=\"<YOUR SECRET KEY>\",\n    aws_region=\"<YOUR REGION>\",\n    auto_offset_reset=\"earliest\",  # start from the beginning of the stream (vs end)\n)\n\napp = Application(\n    broker_address=\"<YOUR BROKER INFO>\",\n    consumer_group=\"<YOUR GROUP>\",\n)\n\nsdf = app.dataframe(source=kinesis).print(metadata=True)\n# YOUR LOGIC HERE!\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Initializing Kafka Consumer in Python for Quix Streams\nDESCRIPTION: Constructor for the BaseConsumer class that takes connection settings, consumer group configuration, and offset reset behavior. It initializes the underlying consumer on demand to avoid network calls during instantiation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(broker_address: Union[str, ConnectionConfig],\n             consumer_group: Optional[str],\n             auto_offset_reset: AutoOffsetReset,\n             auto_commit_enable: bool = True,\n             logger: logging.Logger = logger,\n             error_callback: Callable[[KafkaError], None] = _default_error_cb,\n             on_commit: Optional[Callable[\n                 [Optional[KafkaError], List[TopicPartition]], None]] = None,\n             extra_config: Optional[dict] = None)\n```\n\n----------------------------------------\n\nTITLE: Initializing FileSink with Format and Destination - Python\nDESCRIPTION: Defines the FileSink constructor for configurable batch file writing with selectable directory, serialization format, and destination handler. Supports callbacks for successful or failed client authentication. By default, targets the local filesystem unless an alternate Destination is provided, with formats like JSON or Parquet available.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\\n    directory: str = \\\"\\\",\\n    format: Union[FormatName, Format] = \\\"json\\\",\\n    destination: Optional[Destination] = None,\\n    on_client_connect_success: Optional[ClientConnectSuccessCallback] = None,\\n    on_client_connect_failure: Optional[ClientConnectFailureCallback] = None\\n) -> None\n```\n\n----------------------------------------\n\nTITLE: JSONFormat Class Implementation in Python\nDESCRIPTION: Class that serializes batches of messages into JSON Lines format with optional gzip compression. This implementation extends the Format base class and supports appending to existing files.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nclass JSONFormat(Format)\n```\n\n----------------------------------------\n\nTITLE: Producing Messages to Kafka Topic in Python with Quix Streams\nDESCRIPTION: Method to produce a message to a Kafka topic with error handling for buffer overflow. It polls Kafka before producing to minimize BufferError probability and includes retry mechanisms.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef produce(topic: str,\n            value: Optional[Union[str, bytes]] = None,\n            key: Optional[Union[str, bytes]] = None,\n            headers: Optional[Headers] = None,\n            partition: Optional[int] = None,\n            timestamp: Optional[int] = None,\n            poll_timeout: float = PRODUCER_POLL_TIMEOUT,\n            buffer_error_max_tries: int = PRODUCER_ON_ERROR_RETRIES,\n            on_delivery: Optional[DeliveryCallback] = None)\n```\n\n----------------------------------------\n\nTITLE: Applying Sliding Count Window Example\nDESCRIPTION: Example of defining a sliding window of 10 messages with sum aggregation on a StreamingDataFrame. The window emits results as they are processed using the current() method.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nimport quixstreams.dataframe.windows.aggregations as agg\n\napp = Application()\nsdf = app.dataframe(...)\nsdf = (\n    # Define a sliding window of 10 messages\n    sdf.sliding_count_window(count=10)\n    # Specify the aggregation function\n    .sum(value=agg.Sum())\n    # Specify how the results should be emitted downstream.\n    # \"current()\" will emit results as they come for each updated window,\n    # possibly producing multiple messages per key-window pair\n    # \"final()\" will emit windows only when they are closed and cannot\n    # receive any updates anymore.\n    .current()\n)\n```\n\n----------------------------------------\n\nTITLE: Redis Sink Class Definition in Python\nDESCRIPTION: Class definition for RedisSink that extends BatchingSink to write data to Redis databases.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_77\n\nLANGUAGE: python\nCODE:\n```\nclass RedisSink(BatchingSink)\n```\n\n----------------------------------------\n\nTITLE: Initializing PubSubSource Class Definition\nDESCRIPTION: Base class definition for PubSubSource that enables reading from Google Cloud Pub/Sub topics and forwarding to Kafka topics.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nclass PubSubSource(Source)\n```\n\n----------------------------------------\n\nTITLE: Deserializing a Kafka Message to a Row with Topic in Python\nDESCRIPTION: The `row_deserialize` method of the `Topic` class deserializes an incoming Kafka message (conforming to `SuccessfulConfluentKafkaMessageProto`) into one or more `Row` objects using the configured key and value deserializers. It may return `None` if the message is filtered or ignored.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef row_deserialize(\n    message: SuccessfulConfluentKafkaMessageProto\n) -> Union[Row, List[Row], None]\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using FileSink with S3Destination in Python\nDESCRIPTION: Demonstrates how to set up and use the `FileSink` with an `S3Destination` in a Quix Streams application. It configures the sink to write compressed JSON files to a specified S3 bucket ('my-bucket'), providing AWS credentials, region, and an optional endpoint URL (useful for testing with LocalStack). The example shows initializing the `Application`, defining a topic, creating a `StreamingDataFrame`, and applying the configured sink.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/amazon-s3-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.file import FileSink\nfrom quixstreams.sinks.community.file.destinations import S3Destination\n\n\n# Configure the sink to write JSON files to S3\nfile_sink = FileSink(\n    # Optional: defaults to current working directory\n    directory=\"data\",\n    # Optional: defaults to \"json\"\n    # Available formats: \"json\", \"parquet\" or an instance of Format\n    format=JSONFormat(compress=True),\n    destination=S3Destination(\n        bucket=\"my-bucket\",\n        # Optional: AWS credentials\n        aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n        aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n        region_name=\"eu-west-2\",\n        # Optional: Additional keyword arguments are passed to the boto3 client\n        endpoint_url=\"http://localhost:4566\",  # for LocalStack testing\n    )\n)\n\napp = Application(broker_address='localhost:9092', auto_offset_reset=\"earliest\")\ntopic = app.topic('sink-topic')\n\nsdf = app.dataframe(topic=topic)\nsdf.sink(file_sink)\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Kinesis Source Initialization Method\nDESCRIPTION: Constructor method for KinesisSource that configures AWS credentials and stream processing options.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_52\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n        stream_name: str,\n        aws_region: Optional[str] = getenv(\"AWS_REGION\"),\n        aws_access_key_id: Optional[str] = getenv(\"AWS_ACCESS_KEY_ID\"),\n        aws_secret_access_key: Optional[str] = getenv(\"AWS_SECRET_ACCESS_KEY\"),\n        aws_endpoint_url: Optional[str] = getenv(\"AWS_ENDPOINT_URL_KINESIS\"),\n        shutdown_timeout: float = 10,\n        auto_offset_reset: AutoOffsetResetType = \"latest\",\n        max_records_per_shard: int = 1000,\n        commit_interval: float = 5.0,\n        retry_backoff_secs: float = 5.0,\n        on_client_connect_success: Optional[\n            ClientConnectSuccessCallback] = None,\n        on_client_connect_failure: Optional[\n            ClientConnectFailureCallback] = None)\n```\n\n----------------------------------------\n\nTITLE: Initializing CSV Source and Producing to Kafka with Quix Streams (Python)\nDESCRIPTION: This snippet demonstrates how to set up a CSVSource in Quix Streams, specifying the path, name, and integrating it into the Application's dataframe for processing. It reads CSV data with headers, converts each row to a JSON object, prints data with metadata, and starts the application. Dependencies include the quixstreams library and an input CSV file with expected headers. The main parameters are the file path and source name; expected input is a CSV file, and the output is JSON-formatted messages to a Kafka topic. The source processes the entire file on each run without stateful tracking.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/csv-source.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources.core.csv import CSVSource\n\ndef main():\n    app = Application()\n    # Create the Source instance with a file path and a name.\n    # The name will be included to the default topic name. \n    source = CSVSource(path=\"input.csv\", name=\"csv\")\n\n    sdf = app.dataframe(source=source)\n    sdf.print(metadata=True)\n\n    app.run()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Defining the TopicManager Class in Python\nDESCRIPTION: Defines the `TopicManager` class, responsible for managing all `Topic` instances within a Quix Streams Application. It handles topic creation, retrieval, and categorization. Intended for internal use by the `Application` class.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass TopicManager()\n```\n\n----------------------------------------\n\nTITLE: BigQuerySink Class Definition in Python\nDESCRIPTION: Sink implementation for Google BigQuery that extends the BatchingSink class. This class handles the batching and uploading of data to BigQuery tables.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nclass BigQuerySink(BatchingSink)\n```\n\n----------------------------------------\n\nTITLE: Defining QuixDeserializer for JSON-Formatted Quix Data in Python\nDESCRIPTION: Implements a subclass of JSONDeserializer specialized for parsing Quix-formatted topics, extracting TimeseriesData and EventData while ignoring other content. Requires the appropriate Quix streaming data model and JSON processing support. Expects Quix-encoded JSON data as input and yields parsed Python dictionaries for known topic data types.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass QuixDeserializer(JSONDeserializer)\n```\n\n----------------------------------------\n\nTITLE: Execution Ordering Example in Quix Streams Branching\nDESCRIPTION: This example illustrates the execution ordering in Quix Streams branching. It shows how different branches are processed in the order the operations were added, demonstrating the 'top to bottom' execution principle.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/branching.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\ndef add(n):\n    \"\"\"\n    generates adding functions rather than use lambdas\n    \"\"\"\n    def add_n(value):\n        return value + n\n    return add_n\n\napp = Application(\"localhost:9092\")\ninput_topic = app.topic(\"in\", value_deserializer=\"int\")\noutput_topic = app.topic(\"out\", value_serializer=\"int\")\n\nsdf_0 = app.dataframe(input_topic)\nsdf_0 = sdf_0.apply(add(10)).apply(add(20))\nsdf_1 = sdf_0.apply(add(70)).apply(add(1)).to_topic(output_topic)\nsdf_2 = sdf_0.apply(add(102)).to_topic(output_topic)\nsdf_0 = sdf_0.apply(add(500)).to_topic(output_topic)\n\napp.run()\n```\n\n----------------------------------------\n\nTITLE: Value Selector Implementation\nDESCRIPTION: Shows how to implement a value selector function to filter fields from the final MongoDB document.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/mongodb-sink.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams.sinks.community.mongodb import MongoDBSink\n\ndef edit_doc(my_doc: dict):\n    return {k: v for k,v in my_doc.items() if k not in [\"age\", \"zip_code\"]}\n\nsink = MongoDBSink(\n    ..., # other required stuff\n    value_selector=edit_doc,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Source Class in Python\nDESCRIPTION: The Source class is a concrete implementation of BaseSource that provides basic functionality and helper methods. It is recommended to use this class to create custom sources.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Source(BaseSource)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Client Connection in BaseSink\nDESCRIPTION: Method for establishing client connections and performing validation to confirm successful authentication and connectivity to the destination.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef setup()\n```\n\n----------------------------------------\n\nTITLE: Serializing Data in a Source in Python\nDESCRIPTION: The serialize method converts data to bytes using the producer topic serializers and returns a KafkaMessage object.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef serialize(key: Optional[object] = None,\n              value: Optional[object] = None,\n              headers: Optional[Headers] = None,\n              timestamp_ms: Optional[int] = None) -> KafkaMessage\n```\n\n----------------------------------------\n\nTITLE: Getting a Pre-configured Kafka Consumer (Quix Streams, Python)\nDESCRIPTION: Offers a way to obtain a Consumer instance with settings from the Application, allowing direct Kafka topic consumption, typically for testing or background logic. The auto_commit_enable flag controls offset management. Dependency: quixstreams, Application, and Topic. Inputs: optional boolean auto_commit_enable. Outputs: Consumer instance. Limitation: Not optimized for repeated use; offsets not committed by default unless configured, for at-least-once processing. Users must optionally call store_offsets().\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef get_consumer(auto_commit_enable: bool = True) -> Consumer\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\napp = Application(...)\ntopic = app.topic(\"input\")\n\nwith app.get_consumer() as consumer:\n    consumer.subscribe([topic.name])\n    while True:\n        msg = consumer.poll(timeout=1.0)\n        if msg is not None:\n            # Process message\n            # Optionally commit the offset\n            # consumer.store_offsets(msg)\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Elasticsearch Sink in Python\nDESCRIPTION: Constructor for ElasticsearchSink class that configures connection to Elasticsearch. Supports custom document ID generation, batching, and error handling with configurable retries.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_56\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(url: str,\n             index: str,\n             mapping: Optional[dict] = None,\n             document_id_setter: Optional[Callable[\n                 [SinkItem], Optional[str]]] = _default_document_id_setter,\n             batch_size: int = 500,\n             max_bulk_retries: int = 3,\n             ignore_bulk_upload_errors: bool = False,\n             add_message_metadata: bool = False,\n             add_topic_metadata: bool = False,\n             on_client_connect_success: Optional[\n                 ClientConnectSuccessCallback] = None,\n             on_client_connect_failure: Optional[\n                 ClientConnectFailureCallback] = None,\n             **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Initializing StreamingDataFrame with Purchase Generator Source\nDESCRIPTION: Sets up a StreamingDataFrame using a custom PurchaseGenerator Source for ingesting non-Kafka data.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/purchase-filtering/tutorial.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(source=PurchaseGenerator())\n```\n\n----------------------------------------\n\nTITLE: Initializing TopicManager in Python\nDESCRIPTION: The constructor for the `TopicManager` class. It requires a `TopicAdmin` instance for administrative tasks, the application's consumer group name, and optional timeouts for general responses and topic creation. It also takes a flag to enable/disable automatic topic creation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(topic_admin: TopicAdmin,\n             consumer_group: str,\n             timeout: float = 30,\n             create_timeout: float = 60,\n             auto_create_topics: bool = True)\n```\n\n----------------------------------------\n\nTITLE: Google Cloud Pub/Sub Sink Class Definition in Python\nDESCRIPTION: Class definition for PubSubSink that extends BaseSink to publish messages to Google Cloud Pub/Sub.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_73\n\nLANGUAGE: python\nCODE:\n```\nclass PubSubSink(BaseSink)\n```\n\n----------------------------------------\n\nTITLE: Implementing State Set Method\nDESCRIPTION: Abstract method to set a value for a given key in the state.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/state.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef set(key: K, value: V) -> None\n```\n\n----------------------------------------\n\nTITLE: Defining a Count-Based Hopping Window on StreamingDataFrame in Python\nDESCRIPTION: Defines the `hopping_count_window` method for a `StreamingDataFrame`. This method creates a count-based hopping window configuration, allowing for stateful aggregations over overlapping windows defined by the number of messages (`count`) and the step size (`step`). It accepts count, step, and an optional name. It returns a `HoppingCountWindowDefinition` object.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef hopping_count_window(\n        count: int,\n        step: int,\n        name: Optional[str] = None) -> HoppingCountWindowDefinition\n```\n\n----------------------------------------\n\nTITLE: Configuring Grace Periods for Late Events in Quix Streams Tumbling Windows (Python)\nDESCRIPTION: This snippet demonstrates how to set a grace period for a one-hour tumbling window to handle late/out-of-order events in Quix Streams streams using Python. It uses the tumbling_window API, passing both the window duration and grace period (timedelta values or integer milliseconds). Late events within the grace period are processed; those beyond are dropped. Requires quixstreams and a properly initialized Application/Dataframe. Inputs are time-stamped records; outputs are aggregated results including qualified late arrivals.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\\nfrom quixstreams import Application\\n\\napp = Application(...)\\nsdf = app.dataframe(...)\\n\\n# Define a 1 hour tumbling window with a grace period of 10 seconds.\\n# It will inform the application to wait an additional 10 seconds of event time before considering the \\n# particular window closed\\nsdf.tumbling_window(timedelta(hours=1), grace_ms=timedelta(seconds=10))\\n\n```\n\n----------------------------------------\n\nTITLE: Writing Data Batch in FileSink - Python\nDESCRIPTION: Implements the write method to serialize and write a batch of records using the configured format and destination, while handling errors through SinkBackpressureError. Expects a SinkBatch input and calls underlying format/destination layers. Raises backpressure signaling failure and recommends a retry delay if write fails.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef write(batch: SinkBatch) -> None\n```\n\n----------------------------------------\n\nTITLE: Listing Kafka Topics with TopicAdmin in Python\nDESCRIPTION: The `list_topics` method of the `TopicAdmin` class retrieves a dictionary of topic names and their corresponding metadata from the connected Kafka cluster. An optional timeout parameter specifies the maximum time to wait for a response.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef list_topics(timeout: float = -1) -> Dict[str, ConfluentTopicMetadata]\n```\n\n----------------------------------------\n\nTITLE: Serializing Timeseries Data with QuixTimeseriesSerializer Python\nDESCRIPTION: Defines the QuixTimeseriesSerializer class (subclassing QuixSerializer) to serialize data into Quix Timeseries-compliant JSON. Accepts a dictionary with string, integer, float, bytes, or bytearray values, and converts them into structured JSON arrays/fields (e.g., Timestamps, NumericValues, StringValues, BinaryValues, TagValues). Raises SerializationError for unsupported types. Input must be a dictionary as shown; output is a nested JSON object formatted for Quix ingestion.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass QuixTimeseriesSerializer(QuixSerializer)\n```\n\n----------------------------------------\n\nTITLE: Simplified Kinesis Sink Configuration with Environment Variables\nDESCRIPTION: Shows how to create a KinesisSink instance using environment variables for AWS credentials\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/amazon-kinesis-sink.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nkinesis_sink = KinesisSink(stream_name=\"<stream name>\")\n```\n\n----------------------------------------\n\nTITLE: Defining the TopicAdmin Class in Python\nDESCRIPTION: Defines the `TopicAdmin` class in Python, used for performing administrative operations on a Kafka cluster, primarily focusing on topic creation and inspection.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass TopicAdmin()\n```\n\n----------------------------------------\n\nTITLE: Defining the ApplicationConfig Class for Immutable Configuration (Quix Streams, Python)\nDESCRIPTION: Declares the ApplicationConfig class, inheriting from BaseSettings, to store immutable configuration settings for the application. Provides a structured configuration object used throughout the Application, ensuring consistent and centralized configuration management. Dependency: pydantic BaseSettings. Input/Output: None explicitly, represents a configuration container. Limitation: Intended for use only with Quix Streams Application architecture.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass ApplicationConfig(BaseSettings)\n```\n\n----------------------------------------\n\nTITLE: Using Time-Based Sliding Window Aggregation in Python\nDESCRIPTION: Example demonstrating how to apply a time-based sliding window to a `StreamingDataFrame`. It initializes a Quix Streams `Application`, defines a 60-second sliding window with a 10-second grace period, applies a `Sum` aggregation, and specifies the `current()` emission strategy to output results as they are updated.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nimport quixstreams.dataframe.windows.aggregations as agg\nfrom datetime import timedelta\n\napp = Application()\nsdf = app.dataframe(...)\n\nsdf = (\n    # Define a sliding window of 60s with a grace period of 10s\n    sdf.sliding_window(\n        duration_ms=timedelta(seconds=60),\n        grace_ms=timedelta(seconds=10)\n    )\n\n    # Specify the aggregation function\n    .agg(value=agg.Sum())\n\n    # Specify how the results should be emitted downstream.\n    # \"current()\" will emit results as they come for each updated window,\n    # possibly producing multiple messages per key-window pair\n    # \"final()\" will emit windows only when they are closed and cannot\n    # receive any updates anymore.\n    .current()\n)\n```\n\n----------------------------------------\n\nTITLE: Defining StatefulSource Class in Quix Streams (Python)\nDESCRIPTION: Defines the `StatefulSource` class, which inherits from `Source`. It is designed for custom sources that require state management. Subclasses are responsible for implementing state handling and periodically calling the `flush` method.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n```python\nclass StatefulSource(Source)\n```\n```\n\n----------------------------------------\n\nTITLE: Running a Source in Python\nDESCRIPTION: The abstract run method must be implemented by subclasses to define the main logic of the source. The subprocess runs as long as this method executes.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef run()\n```\n\n----------------------------------------\n\nTITLE: Installing Quix Streams via pip and conda\nDESCRIPTION: Commands to install Quix Streams using either pip package manager or conda. Requires Python 3.9+ and Apache Kafka 0.10+.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# PyPI\npython -m pip install quixstreams\n\n# or conda\nconda install -c conda-forge quixio::quixstreams\n```\n\n----------------------------------------\n\nTITLE: Defining State Abstract Base Class in Python\nDESCRIPTION: Abstract base class that defines the primary interface for working with key-value state data in StreamingDataFrame. Includes generic type parameters K and V for keys and values.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/state.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass State(ABC, Generic[K, V])\n```\n\n----------------------------------------\n\nTITLE: Initializing TopicAdmin in Python\nDESCRIPTION: The constructor for the `TopicAdmin` class. It initializes the admin client with Kafka connection settings and optional configuration. Requires either a broker address string (`host:port`) or a `ConnectionConfig` object for authenticated connections.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(broker_address: Union[str, ConnectionConfig],\n             logger: logging.Logger = logger,\n             extra_config: Optional[Mapping] = None)\n```\n\n----------------------------------------\n\nTITLE: Declaring BaseConsumer Class in Python for Quix Streams Kafka\nDESCRIPTION: Defines a BaseConsumer class that wraps confluent_kafka.Consumer, providing typed interfaces and reasonable defaults for consuming Kafka messages.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass BaseConsumer()\n```\n\n----------------------------------------\n\nTITLE: Seeking Partition Offset - Kafka Consumer API - Python\nDESCRIPTION: Manually sets the consumption position on a partition to a specific offset, which may be absolute or logical (e.g., OFFSET_BEGINNING). Only applicable to actively consumed partitions post-assignment. Raises KafkaException on error conditions. Requires active assignment of the target partition.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef seek(partition: TopicPartition)\n```\n\n----------------------------------------\n\nTITLE: Flushing Messages in a Source in Python\nDESCRIPTION: The flush method ensures all messages are successfully delivered to Kafka before proceeding. It can raise a CheckpointProducerTimeout if messages fail to produce before the timeout.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef flush(timeout: Optional[float] = None) -> None\n```\n\n----------------------------------------\n\nTITLE: Printing StreamingDataFrame Values\nDESCRIPTION: This snippet demonstrates how to print the current message value (and optionally, the message metadata) of a StreamingDataFrame to stdout. It can output in a dict-friendly format and includes options for pretty printing and metadata inclusion.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n\napp = Application()\ninput_topic = app.topic(\"data\")\n\nsdf = app.dataframe(input_topic)\nsdf[\"edited_col\"] = sdf[\"orig_col\"] + \"edited\"\n# print the updated message value with the newly added column\nsdf.print()\n```\n\n----------------------------------------\n\nTITLE: Pausing Partition Consumption - Kafka Consumer API - Python\nDESCRIPTION: Pauses message consumption on a given list of topic partitions. The provided partitions must be tracked and managed manually; this does not impact assignment retrieval methods. Errors cause a KafkaException. Requires a live consumer and valid TopicPartition specifications.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef pause(partitions: List[TopicPartition])\n```\n\n----------------------------------------\n\nTITLE: Accessing State Object for StatefulSource in Quix Streams (Python)\nDESCRIPTION: Defines the `state` property signature for `StatefulSource`. Accessing this property returns a `State` object, allowing interaction with the source's state store within the current transaction context. Note that the returned `State` instance becomes invalid after a `flush()` call, requiring subsequent accesses to call the property again.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n```python\n@property\ndef state() -> State\n```\n```\n\n----------------------------------------\n\nTITLE: Accessing Topic Broker Configuration in Python\nDESCRIPTION: A read-only property `broker_config` on the `Topic` class that returns the `TopicConfig` object representing the actual configuration of the topic as retrieved from the Kafka broker.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef broker_config() -> TopicConfig\n```\n\n----------------------------------------\n\nTITLE: Writing Serialized Data via Destination Base Interface - Python\nDESCRIPTION: Specifies the abstract write contract for any file output Destination. Implementations must accept bytes to serialize, along with a batch object detailing destination context (topic, partition, offset), to ensure correct routing and structuring on the filesystem. This provides uniformity for extending to multiple backend types, requiring concrete classes to implement transactional, format-aware writing operations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef write(data: bytes, batch: SinkBatch) -> None\n```\n\n----------------------------------------\n\nTITLE: Initializing CoinbaseSource in Python\nDESCRIPTION: This code snippet shows how to create a CoinbaseSource instance with specified parameters, including the websocket URL and product IDs to track.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/websocket-source/tutorial.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncoinbase_source = CoinbaseSource(\n    name=\"coinbase-source\",\n    url=\"wss://ws-feed-public.sandbox.exchange.coinbase.com\",\n    product_ids=[\"ETH-BTC\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Topic with TopicManager - Python\nDESCRIPTION: This method generates a Topic object configured with deserializers, serializers, and other creation options. The function supports specifying topic name, (de)serializers for keys and values, topic configuration, and a custom timestamp extractor. Parameters include all typical Kafka topic options; the resulting Topic is ready for registration or immediate use within the Quix Streams application.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef topic(name: str,\\n          value_deserializer: Optional[DeserializerType] = None,\\n          key_deserializer: Optional[DeserializerType] = \"bytes\",\\n          value_serializer: Optional[SerializerType] = None,\\n          key_serializer: Optional[SerializerType] = \"bytes\",\\n          create_config: Optional[TopicConfig] = None,\\n          timestamp_extractor: Optional[TimestampExtractor] = None) -> Topic\n```\n\n----------------------------------------\n\nTITLE: Retrieving Default Topic Name for Source in Quix Streams (Python)\nDESCRIPTION: Defines the `default_topic` method signature within a `Source` class. This method returns a default `Topic` object whose name is derived from the source's name. This default is used only if a specific topic hasn't been provided elsewhere and will be prefixed with \"source__\" by the Application.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n```python\ndef default_topic() -> Topic\n```\n```\n\n----------------------------------------\n\nTITLE: Accessing Changelog Topics with TopicManager in Python\nDESCRIPTION: A read-only property `changelog_topics` on the `TopicManager` class. It returns a nested dictionary containing all managed changelog topics, structured as `{topic_name: {suffix: Topic}}`.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef changelog_topics() -> Dict[Optional[str], Dict[str, Topic]]\n```\n\n----------------------------------------\n\nTITLE: Example Tabular Input Record for Transformation (JSON)\nDESCRIPTION: A JSON sample message providing tabular sensor data with columns and corresponding values as separate lists. This example is used to illustrate transforming tabular data to key-value dictionary form using StreamingDataFrame.apply.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\"columns\": [\"temperature\", \"timestamp\"], \"values\": [35.5, 1710865771.3750699]}\n```\n\n----------------------------------------\n\nTITLE: Defining Tumbling Count Windows in Python (StreamingDataFrame)\nDESCRIPTION: Defines the `tumbling_count_window` method signature for `StreamingDataFrame`. This method creates a count-based tumbling window definition, partitioning the stream into fixed-size, non-overlapping windows based on the number of messages per key. It requires the message `count` (int) per window and accepts an optional window `name` (str). It returns a `TumblingCountWindowDefinition` object for configuring subsequent aggregations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef tumbling_count_window(\n        count: int,\n        name: Optional[str] = None) -> TumblingCountWindowDefinition\n```\n\n----------------------------------------\n\nTITLE: Creating StreamingSeries from Function\nDESCRIPTION: Class method to create a StreamingSeries from a callback function. The provided function is wrapped into an Apply operation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef from_apply_callback(cls, func: ApplyWithMetadataCallback,\n                        sdf_id: int) -> \"StreamingSeries\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Current Offsets - Kafka Consumer API - Python\nDESCRIPTION: Returns the current offset positions for specified topic partitions. Accepts a list of TopicPartition objects and returns augmented objects with current offset and potential error information. Raises KafkaException or RuntimeError as needed. Used for progress tracking and state management.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef position(partitions: List[TopicPartition]) -> List[TopicPartition]\n```\n\n----------------------------------------\n\nTITLE: Handling Consumer Errors in Quix Streams Application\nDESCRIPTION: This function demonstrates how to handle and ignore consumer exceptions in a Quix Streams application using the on_consumer_error callback.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/configuration.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef on_consumer_error(exc: Exception, message, logger) -> bool:\n    \"\"\"\n    Handle the consumer exception and ignore it\n    \"\"\"\n    logger.error('Ignore consumer exception exc=%s offset=%s', exc, message.offset())\n    return True\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Resources for a Source in Python\nDESCRIPTION: The cleanup method is triggered once the run method completes. It's used to free resources and ensures the producer flushes successfully if the run completes without errors.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef cleanup(failed: bool) -> None\n```\n\n----------------------------------------\n\nTITLE: Getting a Pre-configured Kafka Producer (Quix Streams, Python)\nDESCRIPTION: This method provides a Producer instance that is pre-configured with the Application's settings, allowing direct production of data to Kafka. Use cases include testing and ad hoc data streams outside the standard application dataflow. Dependencies: quixstreams, valid Application and Topic. Input: none (uses app's config). Output: Producer instance. Limitation: Should not be used repeatedly in streaming functions due to inefficient new Producer creation. Ensure proper context management for Producer resources.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_producer() -> Producer\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\napp = Application(...)\ntopic = app.topic(\"input\")\n\nwith app.get_producer() as producer:\n    for i in range(100):\n        producer.produce(topic=topic.name, key=b\"key\", value=b\"value\")\n```\n\n----------------------------------------\n\nTITLE: Transforming Windowed Aggregation Results in Python\nDESCRIPTION: Demonstrates how to transform the default window aggregation result schema into a custom format using apply function with a count aggregation over a 10-minute window.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows import Count\n\napp = Application(...)\nsdf = app.dataframe(...)\n\nsdf = (\n    # Define a tumbling window of 10 minutes\n    sdf.tumbling_window(timedelta(minutes=10))\n    # Specify the \"count\" aggregation function\n    .agg(count=Count())\n    # Emit results only for closed windows\n    .final()\n)\n\n# Input format:\n# {\"start\": <window start ms>, \"end\": <window end ms>, \"count\": <aggregated value>\nsdf = sdf.apply(\n    lambda value: {\n        \"count\": value[\"count\"], \n        \"window\": (value[\"start\"], value[\"end\"]),\n    }\n)\n# Output format:\n# {\"count\": <aggregated value>, \"window\": (<window start ms>, <window end ms>)}\n```\n\n----------------------------------------\n\nTITLE: Generating TopicConfig with TopicManager - Python\nDESCRIPTION: This convenience function returns a TopicConfig object using optional parameters such as the number of partitions, replication factor, and any additional settings. Useful for defining topic-level configurations before topic creation or modification. Requires the Quix Streams library and expects standard configuration keys; outputs a TopicConfig object reflecting the given or default settings.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef topic_config(num_partitions: Optional[int] = None,\\n                 replication_factor: Optional[int] = None,\\n                 extra_config: Optional[dict] = None) -> TopicConfig\n```\n\n----------------------------------------\n\nTITLE: Filling Missing Data with Constant Values Using Dictionary in Python\nDESCRIPTION: This example illustrates how to use a dictionary to fill missing data with specific constant values. It transforms the input {\"x\": None} to {\"x\": 1, \"y\": 2}.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/missing-data.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# This would transform {\"x\": None} to {\"x\": 1, \"y\": 2}\nsdf.fill(x=1, y=2)\n```\n\n----------------------------------------\n\nTITLE: Accessing Non-Changelog Topics with TopicManager in Python\nDESCRIPTION: A read-only property `non_changelog_topics` on the `TopicManager` class. It returns a dictionary containing all managed topics that are *not* changelog topics (i.e., regular and repartition topics), structured as `{topic_name: Topic}`.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef non_changelog_topics() -> Dict[str, Topic]\n```\n\n----------------------------------------\n\nTITLE: Defining the Abstract Destination Contract - Python\nDESCRIPTION: Defines the Destination class as an abstract base, establishing the interface required for all file storage backends. Subclasses must implement authentication (setup), write operations, directory management, and file extension handling. This abstraction enables consistent handling of files regardless of destination type, enforcing structure for validation, error management, and data organization throughout the pipeline.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nclass Destination(ABC)\n```\n\n----------------------------------------\n\nTITLE: Kinesis Sink Initialization in Python\nDESCRIPTION: Constructor for KinesisSink that configures AWS credentials, serializers and callbacks for connecting to Kinesis streams.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_62\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n        stream_name: str,\n        aws_access_key_id: Optional[str] = getenv(\"AWS_ACCESS_KEY_ID\"),\n        aws_secret_access_key: Optional[str] = getenv(\"AWS_SECRET_ACCESS_KEY\"),\n        region_name: Optional[str] = getenv(\"AWS_REGION\",\n                                            getenv(\"AWS_DEFAULT_REGION\")),\n        aws_endpoint_url: Optional[str] = getenv(\"AWS_ENDPOINT_URL_KINESIS\"),\n        value_serializer: Callable[[Any], str] = json.dumps,\n        key_serializer: Callable[[Any], str] = bytes.decode,\n        on_client_connect_success: Optional[\n            ClientConnectSuccessCallback] = None,\n        on_client_connect_failure: Optional[\n            ClientConnectFailureCallback] = None,\n        **kwargs) -> None\n```\n\n----------------------------------------\n\nTITLE: Checking for Non-Null Values with StreamingSeries.notnull() in Python\nDESCRIPTION: Shows how to check if a StreamingSeries value is not None. Returns a boolean result that can be assigned to a new column in the StreamingDataFrame.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Check if \"column_a\" is not null and assign the resulting `bool` to a new column:\n# \"is_not_null\"\n\nsdf = app.dataframe()\nsdf[\"is_not_null\"] = sdf[\"column_a\"].notnull()\n```\n\n----------------------------------------\n\nTITLE: Emit Final Aggregation After Window Closes in Quix Streams Tumbling Window (Python)\nDESCRIPTION: Shows how to use the .final() API with tumbling windows to emit a single, final aggregate result after the window closes in Quix Streams Python. Uses Sum for aggregation and is suitable for use cases needing strictly one output per complete window (reduces duplicates). Needs quixstreams, Application, Dataframe, and a numeric data column. Inputs: events with values; Output: single sum per tumbling window.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\\nfrom quixstreams import Application\\nfrom quixstreams.dataframe.windows import Sum\\n\\napp = Application(...)\\nsdf = app.dataframe(...)\\n\\n# Calculate a sum of values over a window of 10 seconds \\n# and use .final() to emit results only when the window is complete\\nsdf = sdf.tumbling_window(timedelta(seconds=10)).agg(value=Sum()).final()\\n\n```\n\n----------------------------------------\n\nTITLE: Defining Kafka Topic for Word Counts\nDESCRIPTION: Specifies the output Kafka topic for storing word counts using the Application's topic method.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/word-count/tutorial.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nword_counts_topic = app.topic(name=\"product_review_word_counts\")\n```\n\n----------------------------------------\n\nTITLE: Adding Data to BaseSink\nDESCRIPTION: Abstract method triggered for each processed record sent to the sink, allowing for accumulation of data batches or immediate transmission with later confirmation during flush operations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@abc.abstractmethod\ndef add(value: Any, key: Any, timestamp: int, headers: HeadersTuples,\n        topic: str, partition: int, offset: int)\n```\n\n----------------------------------------\n\nTITLE: UpdateMany Implementation Example\nDESCRIPTION: Shows how to implement UpdateMany functionality for bulk document updates based on a product category.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/mongodb-sink.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmongodb_sink = MongoDBSink(\n    url=\"mongodb://localhost:27017\",\n    db=\"my_mongodb\",\n    collection=\"clothing\",\n    \n    # find all other documents with \"Shirts\" product category\n    document_matcher=lambda item: {\"product_category\": item.value[\"product_category\"]},\n    \n    # update every document that document_matcher finds\n    update_method=\"UpdateMany\",\n)\n```\n\n----------------------------------------\n\nTITLE: Fill Missing Values Examples\nDESCRIPTION: Examples showing different ways to fill missing values in a StreamingDataFrame: filling single column with None, multiple columns with None, using constant values via dictionary, and combining positional and keyword arguments.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# This would transform {\"x\": 1} to {\"x\": 1, \"y\": None}\nsdf.fill(\"y\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# This would transform {\"x\": 1} to {\"x\": 1, \"y\": None, \"z\": None}\nsdf.fill(\"y\", \"z\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# This would transform {\"x\": None} to {\"x\": 1, \"y\": 2}\nsdf.fill(x=1, y=2)\n```\n\nLANGUAGE: python\nCODE:\n```\n# This would transform {\"y\": None} to {\"x\": None, \"y\": 2}\nsdf.fill(\"x\", y=2)\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Processing Callback in Quix Streams Python Application\nDESCRIPTION: This snippet demonstrates how to use the 'on_message_processed' callback to track processed messages and stop the application after a certain count. It's useful for debugging or limiting the number of processed messages.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/configuration.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\nTOTAL_PROCESSED = 0\ndef on_message_processed(topic:str, partition: int, offset: int):\n    \"\"\"\n    Stop application after processing 100 messages.\n    \"\"\"\n    global TOTAL_PROCESSED\n    TOTAL_PROCESSED += 1\n    if TOTAL_PROCESSED == 100:\n        app.stop()\n\napp = Application(\n    broker_address='localhost:9092',\n    on_message_processed=on_message_processed,\n)\n```\n\n----------------------------------------\n\nTITLE: Combining Positional and Keyword Arguments for Filling Missing Data in Python\nDESCRIPTION: This snippet demonstrates how to use a combination of positional and keyword arguments to fill missing data. It transforms the input {\"y\": None} to {\"x\": None, \"y\": 2}.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/missing-data.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# This would transform {\"y\": None} to {\"x\": None, \"y\": 2}\nsdf.fill(\"x\", y=2)\n```\n\n----------------------------------------\n\nTITLE: Initializing Application and StreamingDataFrame in Python\nDESCRIPTION: This snippet shows how to initialize an Application and create a StreamingDataFrame object. It sets up the basic structure for subsequent data handling operations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/missing-data.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Initialize the Application\napp = Application(...)\nsdf = app.dataframe(...)\n```\n\n----------------------------------------\n\nTITLE: Accessing Kafka Keys, Timestamps and Headers in Quix Streams\nDESCRIPTION: Shows how to access Kafka message metadata (keys, timestamps, headers) in StreamingDataFrame callback functions by using the metadata=True parameter. Examples demonstrate filtering data with invalid keys and adding timestamp as a column.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# Using a message key to filter data with invalid keys\nsdf = sdf.filter(lambda value, key, timestamp, headers: key != b'INVALID', metadata=True)\n\n# Assigning a message timestamp to the value as a new column\nsdf['timestamp'] = sdf.apply(lambda value, key, timestamp, headers: timestamp, metadata=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing JSONDeserializer Constructor in Python\nDESCRIPTION: Constructor for JSONDeserializer that handles JSON data deserialization with optional schema validation and registry integration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(loads: Callable[[Union[bytes, bytearray]], Any] = default_loads, schema: Optional[Mapping] = None, validator: Optional[\"_Validator\"] = None, schema_registry_client_config: Optional[SchemaRegistryClientConfig] = None)\n```\n\n----------------------------------------\n\nTITLE: Getting Absolute Values with StreamingSeries.abs() in Python\nDESCRIPTION: Demonstrates how to get the absolute value of a StreamingSeries value. The result can be used in further calculations and assigned to a new column in the StreamingDataFrame.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Get absolute value of \"int_col\" and add it to \"other_int_col\".\n# Finally, assign the result to a new column: \"abs_col_sum\".\n\nsdf = app.dataframe()\nsdf[\"abs_col_sum\"] = sdf[\"int_col\"].abs() + sdf[\"other_int_col\"]\n```\n\n----------------------------------------\n\nTITLE: Custom Document Matcher Implementation\nDESCRIPTION: Demonstrates how to implement a custom document matcher function that matches MongoDB documents based on the last name field.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/mongodb-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams.sinks.community.mongodb import MongoDBSink\nfrom quixstreams.sinks.base.item import SinkItem\n\ndef match_on_last_name(batch_item: SinkItem):\n    return {\"_id\": batch_item.value[\"name\"][\"last\"]}\n\nsink = MongoDBSink(\n    ..., # other required stuff\n    document_matcher=match_on_last_name,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Kafka Producer in Python for Quix Streams\nDESCRIPTION: Constructor for the Producer class that takes connection settings, logging configuration, and callback handlers. It accepts either a string broker address or a ConnectionConfig object when authentication is needed.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(broker_address: Union[str, ConnectionConfig],\n             logger: logging.Logger = logger,\n             error_callback: Callable[[KafkaError], None] = _default_error_cb,\n             extra_config: Optional[dict] = None,\n             flush_timeout: Optional[float] = None)\n```\n\n----------------------------------------\n\nTITLE: Declaring TransactionalProducer Class in Python for Quix Streams Kafka\nDESCRIPTION: A specialized Producer class for transaction support in Kafka. This class is used internally when transactions are needed with a consumer.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass TransactionalProducer(Producer)\n```\n\n----------------------------------------\n\nTITLE: Dropping Columns in StreamingDataFrame\nDESCRIPTION: Method to remove columns from the message value. The value must support deletion operations like a dictionary. The operation modifies the StreamingDataFrame in-place and returns the original instance for method chaining.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef drop(columns: Union[str, List[str]],\n         errors: Literal[\"ignore\", \"raise\"] = \"raise\") -> \"StreamingDataFrame\"\n```\n\n----------------------------------------\n\nTITLE: Setting SASL Credentials - Kafka Consumer API - Python\nDESCRIPTION: Sets the username and password SASL authentication credentials for the Kafka client. These credentials are applied for further authentications, but do not impact already established broker connections. Only applicable with SASL PLAIN and SCRAM mechanisms. Expects valid string arguments for username and password.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef set_sasl_credentials(username: str, password: str)\n```\n\n----------------------------------------\n\nTITLE: Configuring a BaseSource in Python\nDESCRIPTION: The configure method is triggered before the source is started. It sets up the source's Kafka producer, the topic it will produce to, and any optional dependencies.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef configure(topic: Topic, producer: RowProducer, **kwargs) -> None\n```\n\n----------------------------------------\n\nTITLE: Implementing Delete Method\nDESCRIPTION: Abstract method to delete a value for a given key from the state. Always returns None even if key not found.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/state.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef delete(key: K)\n```\n\n----------------------------------------\n\nTITLE: Getting Assigned Store Partition for StatefulSource Instance in Quix Streams (Python)\nDESCRIPTION: Defines the `assigned_store_partition` property signature for `StatefulSource`. This read-only property returns the integer index of the specific store partition assigned to this particular instance of the `StatefulSource`.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n```python\n@property\ndef assigned_store_partition() -> int\n```\n```\n\n----------------------------------------\n\nTITLE: StreamingDataFrame GroupBy and Stateful Aggregation - Python\nDESCRIPTION: Implements group_by on the 'item' field with a stateful aggregation to compute total quantity ordered per item using Quix Streams' StreamingDataFrame API in Python. Requires the StreamingDataFrame class and a functioning state object. Takes streaming messages with item and quantity keys, outputs updated total quantities per item. Must be used as part of a larger streaming pipeline.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/groupby.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_total(message, state):\n    current_total = state.get(\"item_total\", 0)\n    current_total += int(message[\"quantity\"])\n    state.set(\"item_total\", current_total)\n    return current_total\n\nsdf = StreamingDataFrame()\nsdf = sdf.group_by(\"item\")\nsdf[\"total_quantity\"] = sdf.apply(calculate_total, stateful=True)\nsdf = sdf[[\"total_quantity\"]]\n```\n\n----------------------------------------\n\nTITLE: Adding Messages to Pub/Sub in Python\nDESCRIPTION: Method to publish a message to Google Cloud Pub/Sub, taking message value, key, timestamp and other Kafka message metadata.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_75\n\nLANGUAGE: python\nCODE:\n```\ndef add(value: Any, key: Any, timestamp: int, headers: HeadersTuples,\n        topic: str, partition: int, offset: int) -> None\n```\n\n----------------------------------------\n\nTITLE: Retrieving Consumer Group Member ID - Kafka Consumer API - Python\nDESCRIPTION: Returns the broker-assigned group member id for the Kafka client. No parameters needed; returns the member id string or None, and raises RuntimeError if called after the consumer is closed. Used for identification and tracking within Kafka consumer groups.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef memberid() -> Optional[str]\n```\n\n----------------------------------------\n\nTITLE: Setting Breakpoints in Quix Streams Application\nDESCRIPTION: Shows how to set breakpoints in a Quix Streams Application using StreamingDataFrame.update() and the pdb module for detailed examination of data processing.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/debugging.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pdb\n\nsdf = app.dataframe(...)\n# some SDF transformations happening here ...  \n\n# Set a breakpoint\nsdf.update(lambda value: pdb.set_trace())\n```\n\n----------------------------------------\n\nTITLE: Committing Offsets for Kafka Consumer in Python with Quix Streams\nDESCRIPTION: Method to commit offsets to Kafka. Only shown as a method signature in the documentation without implementation details.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef commit()\n```\n\n----------------------------------------\n\nTITLE: Example Tabular Output Record after Transformation (JSON)\nDESCRIPTION: Sample JSON record showing the expected output after transforming a tabular record with 'columns' and 'values' lists into a single dictionary mapping. Demonstrates the result of using StreamingDataFrame.apply for this reshape operation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\"temperature\": 35.5, \"timestamp\": 1710865771.3750699}\n```\n\n----------------------------------------\n\nTITLE: Defining the Topic Class in Python\nDESCRIPTION: Defines the `Topic` class, which represents a Kafka topic within a Quix Streams application. Instances are typically created via `Application.topic()` and used by `StreamingDataFrame`.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Topic()\n```\n\n----------------------------------------\n\nTITLE: Applying Function to StreamingSeries\nDESCRIPTION: Method to add a callable function to the execution list for a series. The function takes one input argument and returns one output or None, and operations can be chained together.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef apply(func: ApplyCallback) -> \"StreamingSeries\"\n```\n\n----------------------------------------\n\nTITLE: Flushing State and Producer for StatefulSource in Quix Streams (Python)\nDESCRIPTION: Defines the `flush` method signature for `StatefulSource`. Calling this method commits the current state transaction, ensures state changes are published to the changelog topic, and flushes the producer to guarantee message delivery to Kafka. An optional `timeout` in seconds can be provided; `None` uses the producer default, `-1` implies infinite timeout. Raises `CheckpointProducerTimeout` on failure.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n```python\ndef flush(timeout: Optional[float] = None) -> None\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing State Get Method\nDESCRIPTION: Abstract method to retrieve a value for a given key from the state, with optional default value if key not found.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/state.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef get(key: K, default: Optional[V] = None) -> Optional[V]\n```\n\n----------------------------------------\n\nTITLE: JSON Format Initialization Method\nDESCRIPTION: Constructor for JSONFormat class that handles JSON file reading with optional compression and custom deserialization.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(compression: Optional[CompressionName],\n             loads: Optional[Callable[[str], dict]] = None)\n```\n\n----------------------------------------\n\nTITLE: Initializing Google Cloud Pub/Sub Sink Connector in Python\nDESCRIPTION: Constructor for PubSubSink that configures the connection to Google Cloud Pub/Sub. Includes parameters for GCP project identification, topic selection, authentication, and serialization options.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_74\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(project_id: str,\n             topic_id: str,\n             service_account_json: Optional[str] = None,\n             value_serializer: Callable[[Any], Union[bytes, str]] = json.dumps,\n             key_serializer: Callable[[Any], str] = bytes.decode,\n             flush_timeout: int = 5,\n             on_client_connect_success: Optional[\n                 ClientConnectSuccessCallback] = None,\n             on_client_connect_failure: Optional[\n                 ClientConnectFailureCallback] = None,\n             **kwargs) -> None\n```\n\n----------------------------------------\n\nTITLE: Kafka Message Representation for Orders - Python\nDESCRIPTION: Shows how order data is represented as Kafka messages in Python dictionaries, with keys for partitioning and value dicts containing item and quantity. Requires a Kafka setup and expects JSON/dict-format messages. Used for illustrating message re-keying, outputting keyed dicts suitable for streaming workflows. Limitation: example is illustrative and not a runnable producer code.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/groupby.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmessage_1 = {\"key\": \"store_4\", \"value\": {\"item\": \"A\", \"quantity\": 5}}\n# ...etc...\nmessage_5 = {\"key\": \"store_2\", \"value\": {\"item\": \"B\", \"quantity\": 1}}\n```\n\n----------------------------------------\n\nTITLE: Incrementally Assigning Partitions to Kafka Consumer in Python\nDESCRIPTION: Method to incrementally assign new partitions to a Kafka consumer. This can be called outside the Consumer on_assign callback and multiple times. The assigned partitions will immediately show in Consumer.assignment() but will not be associated with the consumer group.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef incremental_assign(partitions: List[TopicPartition])\n```\n\n----------------------------------------\n\nTITLE: Configuring StatefulSource Before Start in Quix Streams (Python)\nDESCRIPTION: Defines the `configure` method signature for `StatefulSource`. This method is invoked internally before the source begins processing. It receives the target `topic` (a `Topic` object), the `producer` (a `RowProducer` instance), and optionally, the `store_partition` (a `StorePartition` instance) to configure the source for production and state management.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n```python\ndef configure(topic: Topic,\n              producer: RowProducer,\n              *,\n              store_partition: Optional[StorePartition] = None,\n              **kwargs) -> None\n```\n```\n\n----------------------------------------\n\nTITLE: Listing Topics Metadata - Kafka Consumer API - Python\nDESCRIPTION: Requests topic or cluster metadata from the Kafka cluster. Accepts a topic name and timeout, returning a ClusterMetadata object with information about the specified topic or all topics. May create topics if 'auto.create.topics.enable' is set to true. Raises KafkaException on error. Suited for administrative clustering and inspection tasks.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef list_topics(topic: Optional[str] = None,\n                timeout: Optional[float] = None) -> ClusterMetadata\n```\n\n----------------------------------------\n\nTITLE: Executing the Anomaly Detection Application\nDESCRIPTION: Defines the main execution block for running the Quix Streams Application, ensuring it's only executed when the script is run directly.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/anomaly-detection/tutorial.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Kafka Messages After Windowed GroupBy Aggregation - Python\nDESCRIPTION: Shows output format for messages after applying a group_by with a tumbling window aggregation on 'item'. Input is streaming data with item, quantity, and time information; output is a dict containing a key for item and its total_quantity in the window. Requires proper windowing and aggregation logic to be applied before producing messages.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/groupby.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n{\"key\": \"A\", \"total_quantity\": 9}\n# ...etc...\n{\"key\": \"B\", \"total_quantity\": 4}\n# ...etc...\n```\n\n----------------------------------------\n\nTITLE: Checking if StreamingSeries Contains a Value in Python\nDESCRIPTION: Shows how to check if a StreamingSeries value contains another value or object. Returns a boolean result that can be assigned to a new column in the StreamingDataFrame.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n# Check if \"column_a\" contains \"my_substring\" and assign the resulting\n# `bool` to a new column: \"has_my_substr\"\n\nsdf = app.dataframe()\nsdf[\"has_my_substr\"] = sdf[\"column_a\"].contains(\"my_substring\")\n```\n\n----------------------------------------\n\nTITLE: Defining SchemaRegistryClientConfig for Schema Registry Connection in Python\nDESCRIPTION: Defines a configuration class for establishing secure or authenticated connections to a Schema Registry, extending Pydantic's BaseSettings. Dependencies include Pydantic and proper SSL certificates for secure connection. Parameters cover endpoint URL, SSL CA cert, client key/cert locations, and HTTP basic authentication; required for offloading (de)serialization to external schema registry services.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass SchemaRegistryClientConfig(BaseSettings)\n```\n\n----------------------------------------\n\nTITLE: Implementing Get Bytes Method\nDESCRIPTION: Method to retrieve a byte value for a given key from the state, with optional default value if key not found.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/state.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_bytes(key: K, default: Optional[bytes] = None) -> Optional[bytes]\n```\n\n----------------------------------------\n\nTITLE: ParquetFormat Initialization Method in Python\nDESCRIPTION: Constructor for the ParquetFormat class that allows configuration of file extension and compression algorithm for Parquet files. Supports multiple compression options like snappy, gzip, brotli, etc.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(file_extension: str = \".parquet\",\n             compression: Compression = \"snappy\") -> None\n```\n\n----------------------------------------\n\nTITLE: Initializing BaseSink with Connection Callbacks\nDESCRIPTION: Constructor for BaseSink that accepts optional callbacks for connection success and failure events, primarily used for additional logging and error handling during client authentication.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(on_client_connect_success: Optional[\n    ClientConnectSuccessCallback] = None,\n             on_client_connect_failure: Optional[\n                 ClientConnectFailureCallback] = None)\n```\n\n----------------------------------------\n\nTITLE: Writing Data to Azure Blob Storage via Destination - Python\nDESCRIPTION: Defines the core method for writing serialized data to Azure Blob storage. Expects raw byte data and a SinkBatch object (containing topic and partition info), and handles the transfer using configured Azure SDK clients. The function presumes that connectivity and credentials have been validated at initialization. This method is required for pipeline data egress and must ensure atomic, reliable uploads matching the partitioned stream structure.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndef write(data: bytes, batch: SinkBatch) -> None\n```\n\n----------------------------------------\n\nTITLE: Implementing StringSerializer Constructor in Python\nDESCRIPTION: Constructor for StringSerializer class that serializes strings to bytes using specified encoding.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(codec: str = \"utf_8\")\n```\n\n----------------------------------------\n\nTITLE: Emitting Results for Each Message from Tumbling Window in Quix Streams (Python)\nDESCRIPTION: Demonstrates the use of the .current() API to emit aggregate results for each incoming message in a 10-second tumbling window in Quix Streams with Python. Uses the Sum aggregator and immediate result emission, which produces updates as the window is built (not final). Requires quixstreams, Application and Dataframe setup, and sum-compatible data column. Input: time-stamped messages; Output: incremental windowed sums per message.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\\nfrom quixstreams import Application\\nfrom quixstreams.dataframe.windows import Sum\\n\\napp = Application(...)\\nsdf = app.dataframe(...)\\n\\n# Calculate a sum of values over a window of 10 seconds \\n# and use .current() to emit results immediately\\nsdf = sdf.tumbling_window(timedelta(seconds=10)).agg(value=Sum()).current()\\n\\n# Results:\\n# -> Timestamp=100, value=1 -> emit {\\\"start\\\": 0, \\\"end\\\": 10000, \\\"value\\\": 1} \\n# -> Timestamp=101, value=1 -> emit {\\\"start\\\": 0, \\\"end\\\": 10000, \\\"value\\\": 2} \\n# -> Timestamp=102, value=1 -> emit {\\\"start\\\": 0, \\\"end\\\": 10000, \\\"value\\\": 3} \\n\n```\n\n----------------------------------------\n\nTITLE: Assigning Partitions to Kafka Consumer in Python\nDESCRIPTION: Method to set the consumer partition assignment to the provided list of TopicPartition objects and start consuming. May raise KafkaException or RuntimeError if called on a closed consumer.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndef assign(partitions: List[TopicPartition])\n```\n\n----------------------------------------\n\nTITLE: Initializing AzureFileDestination for Blob Storage - Python\nDESCRIPTION: Implements the constructor for AzureFileDestination, allowing the user to specify the storage account connection string and target container. This method authenticates against Azure Blob storage using the provided credentials and container name, raising explicit errors if the target container is missing or if access is denied. It requires the Azure Blob SDK and correct connection parameters as dependencies, and initializes internal state for subsequent write operations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(connection_string: str, container: str) -> None\n```\n\n----------------------------------------\n\nTITLE: Defining KafkaReplicatorSource Class in Quix Streams (Python)\nDESCRIPTION: Defines the `KafkaReplicatorSource` class, inheriting from `Source`. This source is specifically designed to replicate messages from a topic on an external Kafka broker to the Kafka broker used by the Quix Streams application. It supports running multiple instances for parallel replication.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n```python\nclass KafkaReplicatorSource(Source)\n```\n```\n\n----------------------------------------\n\nTITLE: Using Count-Based Hopping Window Aggregation in Python\nDESCRIPTION: Example demonstrating how to apply a count-based hopping window to a `StreamingDataFrame`. It initializes a Quix Streams `Application`, defines a hopping window containing 10 messages that advances every 5 messages, applies a `Sum` aggregation, and specifies the `current()` emission strategy.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nimport quixstreams.dataframe.windows.aggregations as agg\n\napp = Application()\nsdf = app.dataframe(...)\nsdf = (\n    # Define a hopping window of 10 messages with a step of 5 messages\n    sdf.hopping_count_window(\n        count=10,\n        step=5,\n    )\n    # Specify the aggregation function\n    .agg(value=agg.Sum())\n    # Specify how the results should be emitted downstream.\n    # \"current()\" will emit results as they come for each updated window,\n    # possibly producing multiple messages per key-window pair\n    # \"final()\" will emit windows only when they are closed and cannot\n    # receive any updates anymore.\n    .current()\n)\n```\n\n----------------------------------------\n\nTITLE: Storing Offsets for Kafka Consumer in Python with Quix Streams\nDESCRIPTION: Method to store offsets for a message or a list of offsets. The stored offsets will be committed according to auto-commit settings or manual commit. Requires 'enable.auto.offset.store' to be set to False.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef store_offsets(message: Optional[Message] = None,\n                  offsets: Optional[List[TopicPartition]] = None)\n```\n\n----------------------------------------\n\nTITLE: Reassigning StreamingDataFrame After Operations (Quix Streams, Python)\nDESCRIPTION: Shows the standard pattern of reassigning the StreamingDataFrame variable after each operation, including chaining methods. Intended to build a pipeline by continuously updating the DataFrame reference as operations are applied. Depends on the quixstreams package, but methods shown are meant to be illustrative stubs.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/dataframe-assignments.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# NOTE: this is an incomplete stub just to show usage\\nfrom quixstreams import Application\\n\\nsdf = Application().dataframe()\\nsdf = sdf.apply()  # reassign with new operation\\nsdf = sdf.apply().apply()  # reassign with chaining\n```\n\n----------------------------------------\n\nTITLE: PandasDataFrameSource Run Method\nDESCRIPTION: Implementation of the run method that produces data from the DataFrame row by row.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ndef run()\n```\n\n----------------------------------------\n\nTITLE: Filling Missing Data for Single Column with None in Python\nDESCRIPTION: This example demonstrates how to use the fill method to add a missing column 'y' with a None value. It transforms the input {\"x\": 1} to {\"x\": 1, \"y\": None}.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/missing-data.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# This would transform {\"x\": 1} to {\"x\": 1, \"y\": None}\nsdf.fill(\"y\")\n```\n\n----------------------------------------\n\nTITLE: Starting a Source in Python\nDESCRIPTION: The start method for the Source class marks the source as running, executes the run method, and ensures cleanup happens afterward.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef start() -> None\n```\n\n----------------------------------------\n\nTITLE: Implementing Flush Method for BaseSink\nDESCRIPTION: Abstract method triggered by Checkpoint during commits, intended for writing batched data to the destination or confirming delivery of previously sent messages. If flush fails, the checkpoint is aborted.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@abc.abstractmethod\ndef flush()\n```\n\n----------------------------------------\n\nTITLE: Setting Timestamp in StreamingDataFrame\nDESCRIPTION: This snippet demonstrates how to set a new timestamp for a StreamingDataFrame based on the current message value and metadata. The new timestamp will be used in windowed aggregations and when producing messages to output topics.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\n\napp = Application()\ninput_topic = app.topic(\"data\")\n\nsdf = app.dataframe(input_topic)\n# Updating the record's timestamp based on the value\nsdf = sdf.set_timestamp(lambda value, key, timestamp, headers: value['new_timestamp'])\n```\n\n----------------------------------------\n\nTITLE: Defining the Local File Destination Class - Python\nDESCRIPTION: Implements the LocalDestination class inheriting from Destination, allowing data to be written to local disk files. Supports both append and overwrite modes, with logic to manage file lifecycle across writes. This destination is central to pipelines where temporary or long-term storage is needed on the local host or in ephemeral execution environments.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nclass LocalDestination(Destination)\n```\n\n----------------------------------------\n\nTITLE: Writing Data to Local Files via Destination - Python\nDESCRIPTION: Implements the method to persist serialized bytes to disk, routed by the SinkBatch topic and partition context. Manages local file handles, ensuring new files are created or existing ones are appended based on initialization parameters. The method presumes local filesystem access and appropriate directory permissions are in place.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef write(data: bytes, batch: SinkBatch) -> None\n```\n\n----------------------------------------\n\nTITLE: Expected JSON Output from CSV Source (JSON Lines)\nDESCRIPTION: This snippet shows example output produced by processing the sample CSV file: each line is a JSON dictionary representing a row from the CSV. Fields from the CSV become key-value pairs in the JSON object. Output is in JSON Lines format, suitable for streaming or ingestion by Kafka.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/csv-source.md#2025-04-23_snippet_2\n\nLANGUAGE: json lines\nCODE:\n```\n{\"field1\": \"foo1\", \"field2\": \"bar1\", \"timestamp\":  \"1\"}\n{\"field1\": \"foo2\", \"field2\": \"bar2\", \"timestamp\":  \"2\"}\n{\"field1\": \"foo3\", \"field2\": \"bar3\", \"timestamp\":  \"3\"}\n```\n\n----------------------------------------\n\nTITLE: Checking Object Identity with StreamingSeries.is_() in Python\nDESCRIPTION: Demonstrates how to check if a StreamingSeries value refers to the same object as another value. Returns a boolean result that can be assigned to a new column in the StreamingDataFrame.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n# Check if \"column_a\" is the same as \"column_b\" and assign the resulting `bool`\n#  to a new column: \"is_same\"\n\nfrom quixstreams import Application\nsdf = app.dataframe()\nsdf[\"is_same\"] = sdf[\"column_a\"].is_(sdf[\"column_b\"])\n```\n\n----------------------------------------\n\nTITLE: Recommended Pattern: Column Manipulations in Assignment (Quix Streams, Python)\nDESCRIPTION: Demonstrates proper assignment of StreamingDataFrame column operations in a single statement within Quix Streams. This method ensures that the sum of columns 'x' and 'y' is immediately assigned to column 'z', avoiding intermediate step storage. Requires columns 'x' and 'y' exist within the StreamingDataFrame.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/dataframe-assignments.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# column manipulations and assignment on same assignment step!\\nsdf[\"z\"] = sdf[\"x\"] + sdf[\"y\"]\n```\n\n----------------------------------------\n\nTITLE: JSON Format Class Definition\nDESCRIPTION: Format class for handling JSON file processing with optional compression support.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nclass JSONFormat(Format)\n```\n\n----------------------------------------\n\nTITLE: Format.serialize Method Implementation in Python\nDESCRIPTION: Abstract method for serializing a batch of messages into bytes. All format implementations must provide this functionality to convert SinkBatch objects to their specific format.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_45\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef serialize(batch: SinkBatch) -> bytes\n```\n\n----------------------------------------\n\nTITLE: Defining a Time-Based Sliding Window on StreamingDataFrame in Python\nDESCRIPTION: Defines the `sliding_window` method for a `StreamingDataFrame`. This method creates a time-based sliding window configuration with a fixed step of 1ms, allowing for highly overlapping stateful aggregations over a specified `duration_ms`. It accepts duration, grace period (both in milliseconds or as `timedelta`), an optional name, and an optional callback for late events. It returns a `SlidingTimeWindowDefinition` object and is noted as more performant than hopping windows with a 1ms step.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef sliding_window(\n    duration_ms: Union[int, timedelta],\n    grace_ms: Union[int, timedelta] = 0,\n    name: Optional[str] = None,\n    on_late: Optional[WindowOnLateCallback] = None\n) -> SlidingTimeWindowDefinition\n```\n\n----------------------------------------\n\nTITLE: Starting the Sink in Application Lifecycle\nDESCRIPTION: Called during Application.run() to initialize the sink's client, facilitating a callback pattern around connection attempts for better error handling.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef start()\n```\n\n----------------------------------------\n\nTITLE: Initializing CSVSink File Output - Python\nDESCRIPTION: Defines the constructor for the CSVSink to configure file path, CSV dialect, and key/value serialization for CSV output. Accepts a file path, dialect (affecting quoting/delimiters), and custom key/value serializers (defaults to str and json.dumps). Transfers input records (key, value, etc.) to the CSV file for debugging or audit purposes.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(path: str,\\n             dialect: str = \\\"excel\\\",\\n             key_serializer: Callable[[Any], str] = str,\\n             value_serializer: Callable[[Any], str] = json.dumps)\n```\n\n----------------------------------------\n\nTITLE: Filling Missing Values in StreamingDataFrame\nDESCRIPTION: Method to fill missing values in the message value with constant values. The operation modifies the StreamingDataFrame in-place and returns the original instance for method chaining.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef fill(*columns: str, **mapping: Any) -> \"StreamingDataFrame\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Local File Destination with Append Option - Python\nDESCRIPTION: Defines the constructor for LocalDestination, exposing an optional append parameter to dictate file writing behavior. When set, files will be appended-to instead of overwritten, subject to compatibility with formatter-specific constraints. Initialization sets up internal state for file management or creates new files as needed.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(append: bool = False) -> None\n```\n\n----------------------------------------\n\nTITLE: Direct Serializer Instance Configuration\nDESCRIPTION: Shows how to configure serialization by directly passing Serializer and Deserializer instances instead of string shorthands. Uses JSONSerializer and JSONDeserializer classes.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/serialization.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.models import JSONDeserializer, JSONSerializer\n\napp = Application(broker_address='localhost:9092', consumer_group='consumer')\ninput_topic = app.topic('input', value_deserializer=JSONDeserializer())\noutput_topic = app.topic('output', value_serializer=JSONSerializer())\n```\n\n----------------------------------------\n\nTITLE: Defining TransactionState Class\nDESCRIPTION: Concrete implementation of State interface that uses transactions for state management.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/state.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass TransactionState(State)\n```\n\n----------------------------------------\n\nTITLE: Copying and Updating Application Configuration (Quix Streams, Python)\nDESCRIPTION: Defines a method to create a new copy of the ApplicationConfig instance, optionally updating it with new values provided as keyword arguments. Ensures immutability by returning a modified copy instead of mutating the original. Dependency: ApplicationConfig class. Input: **kwargs for updated settings. Output: new ApplicationConfig object. Limitation: For configuration immutability patterns.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef copy(**kwargs) -> \"ApplicationConfig\"\n```\n\n----------------------------------------\n\nTITLE: Windowed Aggregation Example Input and Output (JSON)\nDESCRIPTION: Provides sample input sensor records and expected output documents illustrating how sliding windowed aggregation transforms input data into windowed averages. These samples clarify window assignment/timestamp boundaries. While not executable code, they serve as reference input/output formats for implementation and testing.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\\\"temperature\\\": 30, \\\"timestamp\\\": 3600000}\\n{\\\"temperature\\\": 29, \\\"timestamp\\\": 4800000}\\n{\\\"temperature\\\": 28, \\\"timestamp\\\": 4800001}\\n{\\\"temperature\\\": 27, \\\"timestamp\\\": 7200000}\\n{\\\"temperature\\\": 26, \\\"timestamp\\\": 7200001}\\n\n```\n\nLANGUAGE: json\nCODE:\n```\n{\\\"avg_temperature\\\": 30, \\\"start\\\": 0, \\\"end\\\": 3600000}\\n{\\\"avg_temperature\\\": 29.5, \\\"start\\\": 1200000, \\\"end\\\": 4800000}\\n{\\\"avg_temperature\\\": 29, \\\"start\\\": 1200001, \\\"end\\\": 4800001}\\n{\\\"avg_temperature\\\": 28.5, \\\"start\\\": 3600000, \\\"end\\\": 7200000}\\n{\\\"avg_temperature\\\": 27.5, \\\"start\\\": 3600001, \\\"end\\\": 7200001}  # reading 30 is outside of the window\\n\n```\n\n----------------------------------------\n\nTITLE: Abstract Method for Reading File Contents\nDESCRIPTION: Abstract method for reading file contents that must be implemented by subclasses. It should return an open binary file stream that is ready for deserialization or decompression.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef read_file(filepath: Path) -> BinaryIO\n```\n\n----------------------------------------\n\nTITLE: Drop Columns Example\nDESCRIPTION: Example of removing columns from a StreamingDataFrame. This demonstrates dropping multiple columns from a dictionary-like message value.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# Remove columns \"x\" and \"y\" from the value.\n# This would transform {\"x\": 1, \"y\": 2, \"z\": 3} to {\"z\": 3}\n\nsdf = StreamingDataFrame()\nsdf.drop([\"x\", \"y\"])\n```\n\n----------------------------------------\n\nTITLE: Group By Customer Example - Python\nDESCRIPTION: Shows how to group and aggregate data by customer ID instead of store ID, including state management for running totals.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef func(d: dict, state: State):\n    current_total = state.get(\"customer_sum\", 0)\n    new_total = current_total + d[\"customer_spent\"]\n    state.set(\"customer_sum\", new_total)\n    d[\"customer_total\"] = new_total\n    return d\n\nsdf = StreamingDataFrame()\nsdf = sdf.group_by(\"customer_account_id\")\nsdf = sdf.apply(func, stateful=True)\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Credentials via Environment Variables in Bash\nDESCRIPTION: Provides Bash commands to set standard AWS environment variables (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_DEFAULT_REGION`). When these variables are set, AWS SDKs (like boto3, used by the S3 sink) can automatically pick them up, avoiding the need to pass credentials explicitly in the code.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/amazon-s3-sink.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AWS_ACCESS_KEY_ID=\"your_access_key\"\nexport AWS_SECRET_ACCESS_KEY=\"your_secret_key\"\nexport AWS_DEFAULT_REGION=\"eu-west-2\"\n```\n\n----------------------------------------\n\nTITLE: Format.supports_append Property Implementation in Python\nDESCRIPTION: Abstract property method that indicates whether the format supports appending data to existing files. Implementations must specify this capability.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n@property\n@abstractmethod\ndef supports_append() -> bool\n```\n\n----------------------------------------\n\nTITLE: Base Format Abstract Class Definition in Python\nDESCRIPTION: Defines the abstract base class for formatting batches in file sinks. This class establishes the interface that all format implementations must follow, including file extension, append support, and serialization methods.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nclass Format(ABC)\n```\n\n----------------------------------------\n\nTITLE: Kinesis Sink Implementation Class in Python\nDESCRIPTION: Base class for implementing a Kinesis sink with AWS credentials and configuration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nclass KinesisSink(BaseSink)\n```\n\n----------------------------------------\n\nTITLE: Processing Records from Files\nDESCRIPTION: Method that applies replay delay to records, serializes them, and produces them to Kafka. It handles the timing logic to replicate the original messaging cadence.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndef process_record(record: object)\n```\n\n----------------------------------------\n\nTITLE: Application Constructor Definition\nDESCRIPTION: Comprehensive constructor for the Application class with all configuration options.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(broker_address: Optional[Union[str, ConnectionConfig]] = None,\n             *,\n             quix_sdk_token: Optional[str] = None,\n             consumer_group: Optional[str] = None,\n             auto_offset_reset: AutoOffsetReset = \"latest\",\n             commit_interval: float = 5.0,\n             commit_every: int = 0,\n             consumer_extra_config: Optional[dict] = None,\n             producer_extra_config: Optional[dict] = None,\n             state_dir: Union[None, str, Path] = None,\n             rocksdb_options: Optional[RocksDBOptionsType] = None,\n             on_consumer_error: Optional[ConsumerErrorCallback] = None,\n             on_processing_error: Optional[ProcessingErrorCallback] = None,\n             on_producer_error: Optional[ProducerErrorCallback] = None,\n             on_message_processed: Optional[MessageProcessedCallback] = None,\n             consumer_poll_timeout: float = 1.0,\n             producer_poll_timeout: float = 0.0,\n             loglevel: Optional[Union[int, LogLevel]] = \"INFO\",\n             auto_create_topics: bool = True,\n             use_changelog_topics: bool = True,\n             quix_config_builder: Optional[QuixKafkaConfigsBuilder] = None,\n             topic_manager: Optional[TopicManager] = None,\n             request_timeout: float = 30,\n             topic_create_timeout: float = 60,\n             processing_guarantee: ProcessingGuarantee = \"at-least-once\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Quix Streams Application in Quix Cloud\nDESCRIPTION: This snippet shows how to initialize a Quix Streams Application when running directly within Quix Cloud, where the broker_address is automatically configured.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/quix-platform.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\napp = Application()\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\napp = Application(broker_address=None)\n```\n\n----------------------------------------\n\nTITLE: Implementing AvroSerializer Constructor in Python\nDESCRIPTION: Constructor for AvroSerializer that handles data serialization to Avro format with configurable schema validation and registry integration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(schema: Schema, strict: bool = False, strict_allow_default: bool = False, disable_tuple_notation: bool = False, schema_registry_client_config: Optional[SchemaRegistryClientConfig] = None, schema_registry_serialization_config: Optional[SchemaRegistrySerializationConfig] = None)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Local Elasticsearch Instance in Python\nDESCRIPTION: This code snippet shows how to connect to a local Elasticsearch instance using the API key generated during setup. It's used for testing the Elasticsearch sink locally.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/elasticsearch-sink.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams.sinks.community.elasticsearch import ElasticsearchSink\n\nelasticsearch_sink = ElasticsearchSink(\n    host=\"http://localhost:9200\",\n    index=\"<YOUR INDEX>\",\n    api_key=\"<PRINTED IN TERMINAL>\",\n)\n```\n\n----------------------------------------\n\nTITLE: Selecting Subset of Columns from StreamingDataFrame\nDESCRIPTION: Reduces the StreamingDataFrame to only include 'Email' and 'Full Name' columns.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/purchase-filtering/tutorial.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsdf = sdf[[\"Email\", \"Full Name\"]]\n```\n\n----------------------------------------\n\nTITLE: Creating Producer and Sending Messages to Kafka in Python\nDESCRIPTION: This snippet illustrates how to create a Producer instance and use it to produce messages to a Kafka topic. It includes serializing an event using the defined Topic and sending it to Kafka.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/producer.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nevent = {\"id\": \"1\", \"text\": \"Lorem ipsum dolor sit amet\"}\n\n# Create a Producer instance\nwith app.get_producer() as producer:\n    \n    # Serialize an event using the defined Topic \n    message = topic.serialize(key=event[\"id\"], value=event)\n    \n    # Produce a message into the Kafka topic\n    producer.produce(\n        topic=topic.name, value=message.value, key=message.key\n    )\n```\n\n----------------------------------------\n\nTITLE: Counting Partition Folders for Topic Configuration\nDESCRIPTION: Method that counts partition folders to intelligently set the default topic partition count. This is useful when files are organized in a partition folder structure, especially when created by Quix Streams FileSink.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ndef file_partition_counter() -> int\n```\n\n----------------------------------------\n\nTITLE: JSONFormat Initialization Method in Python\nDESCRIPTION: Constructor for the JSONFormat class that allows configuration of file extension, compression, and custom JSON serialization function. These parameters control how the JSON Lines output is formatted.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_47\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(file_extension: str = \".jsonl\",\n             compress: bool = False,\n             dumps: Optional[Callable[[Any], str]] = None) -> None\n```\n\n----------------------------------------\n\nTITLE: Monitoring Multiple Points in Data Pipeline with Quix Streams\nDESCRIPTION: Illustrates how to monitor multiple points in a data pipeline by adding multiple print_table calls at different stages of data processing.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/debugging.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsdf = app.dataframe(topic)\nsdf.print_table(title=\"Raw Input\")\n\nsdf = sdf.filter(lambda value: ...)\nsdf.print_table(title=\"Filtered Values\")\n\nsdf = sdf.apply(lambda value: ...)\nsdf.print_table(title=\"Final Output\")\n```\n\n----------------------------------------\n\nTITLE: Stateful Filtering Example - Python\nDESCRIPTION: Demonstrates filtering data based on comparison with previously stored state values.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef func(d: dict, state: State):\n    value = d[\"my_value\"]\n    if value > state.get(\"my_store_key\"):\n        state.set(\"my_store_key\") = value\n        return True\n    return False\n\nsdf = StreamingDataFrame()\nsdf = sdf.filter(func, stateful=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring File Extension and Append Compatibility for LocalDestination - Python\nDESCRIPTION: Handles the logic to set the output file extension according to the selected format and validates if appending is allowed for that format. Raises a ValueError if the format does not support append mode while append is enabled, ensuring data integrity and format compliance. This function enables safe interoperation between appendable and non-appendable file formats.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef set_extension(format: Format) -> None\n```\n\n----------------------------------------\n\nTITLE: Accessing Topic Creation Configuration in Python\nDESCRIPTION: A read-only property `create_config` on the `Topic` class that returns the `TopicConfig` object provided during initialization, intended for use when creating the topic if it doesn't exist.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef create_config() -> Optional[TopicConfig]\n```\n\n----------------------------------------\n\nTITLE: PubSubSource Constructor Implementation\nDESCRIPTION: Implementation of PubSubSource initialization with configuration for Google Cloud project, topic, subscription, and various operational parameters.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_55\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(project_id: str,\n             topic_id: str,\n             subscription_id: str,\n             service_account_json: Optional[str] = None,\n             commit_every: int = 100,\n             commit_interval: float = 5.0,\n             create_subscription: bool = False,\n             enable_message_ordering: bool = False,\n             shutdown_timeout: float = 10.0,\n             on_client_connect_success: Optional[\n                 ClientConnectSuccessCallback] = None,\n             on_client_connect_failure: Optional[\n                 ClientConnectFailureCallback] = None)\n```\n\n----------------------------------------\n\nTITLE: Defining SinkBatch Class for Data Accumulation\nDESCRIPTION: Class used to accumulate processed data between checkpoints in BatchingSink implementations, automatically created and managed by the BatchingSink class.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass SinkBatch()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Document ID Setter for Elasticsearch Sink in Python\nDESCRIPTION: This code snippet shows how to implement a custom document ID setter function for the Elasticsearch sink. The function extracts the last name from the message value to use as the document ID.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/elasticsearch-sink.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams.sinks.community.elasticsearch import ElasticsearchSink\nfrom quixstreams.sinks.base.item import SinkItem\n\ndef get_last_name(batch_item: SinkItem) -> str:\n    return batch_item.value[\"name\"][\"last\"]\n\nsink = ElasticsearchSink(\n    ..., # other required stuff\n    document_id_setter=get_last_name,\n)\n```\n\n----------------------------------------\n\nTITLE: Resuming Partition Consumption - Kafka Consumer API - Python\nDESCRIPTION: Resumes message consumption for previously paused partitions. Function expects a list of TopicPartition objects and raises KafkaException on error. Relies on the Kafka consumer being active and receiving proper partition input.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef resume(partitions: List[TopicPartition])\n```\n\n----------------------------------------\n\nTITLE: Filling Missing Data for Multiple Columns with None in Python\nDESCRIPTION: This snippet shows how to fill multiple missing columns 'y' and 'z' with None values. It transforms the input {\"x\": 1} to {\"x\": 1, \"y\": None, \"z\": None}.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/missing-data.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# This would transform {\"x\": 1} to {\"x\": 1, \"y\": None, \"z\": None}\nsdf.fill(\"y\", \"z\")\n```\n\n----------------------------------------\n\nTITLE: Getting Watermark Offsets - Kafka Consumer API - Python\nDESCRIPTION: Obtains low and high offsets (watermarks) for a specific partition. Accepts a TopicPartition, optional timeout, and a 'cached' flag to use broker or cached values. Returns a tuple with low/high offsets or None on timeout. Raises KafkaException or RuntimeError if errors occur. Requires connection to Kafka and periodic updating for accurate cached results.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef get_watermark_offsets(partition: TopicPartition,\n                          timeout: Optional[float] = None,\n                          cached: bool = False) -> Tuple[int, int]\n```\n\n----------------------------------------\n\nTITLE: JSONFormat.serialize Method Implementation in Python\nDESCRIPTION: Method that serializes a SinkBatch into bytes in JSON Lines format. Each message is converted to a JSON object with _timestamp, _key, and _value fields, with optional compression.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ndef serialize(batch: SinkBatch) -> bytes\n```\n\n----------------------------------------\n\nTITLE: Mutating Values In-Place with StreamingDataFrame.update() in Python\nDESCRIPTION: Demonstrates the `StreamingDataFrame.update()` method for modifying stream values in-place (e.g., appending to a list) or performing side effects. The function's return value is ignored, and the original (potentially mutated) value is passed downstream. Reassigning the result (`sdf = sdf.update(...)`) is optional for `.update()` itself but shown for clarity and necessary if chained with non-inplace methods.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Mutate a list by appending a new item to it\n# The updated list will be passed downstream\nsdf = sdf.update(lambda some_list: some_list.append(1))\n\n# OR instead (no reassignment):\nsdf.update(lambda some_list: some_list.append(1))\n\n```\n```\n\n----------------------------------------\n\nTITLE: Composing StreamingDataFrame Functions in Python\nDESCRIPTION: Defines the `compose` method signature for the `StreamingDataFrame` class. This method compiles all preceding functions applied to the dataframe into a single callable closure, which generally improves performance compared to sequential execution. It takes an optional `sink` callable to accumulate results and returns a dictionary mapping node IDs to their respective `VoidExecutor` closures.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef compose(sink: Optional[VoidExecutor] = None) -> dict[str, VoidExecutor]\n```\n\n----------------------------------------\n\nTITLE: Handling Backpressure Events in BaseSink\nDESCRIPTION: Method triggered when the sink is paused due to backpressure after SinkBackpressureError is raised, allowing for custom reactions to backpressure events.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef on_paused()\n```\n\n----------------------------------------\n\nTITLE: Defining Kafka Topic for Temperature Alerts\nDESCRIPTION: Specifies the Kafka topic for outputting temperature alerts. This topic will be used to publish anomaly detection results.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/anomaly-detection/tutorial.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nalerts_topic = app.topic(\"temperature_alerts\")\n```\n\n----------------------------------------\n\nTITLE: Selecting and Grouping with SQL GROUP BY - SQL\nDESCRIPTION: Demonstrates SQL syntax for selecting all columns from an Orders table and grouping items to calculate the sum of quantities per item. Requires a relational database supporting SQL and an Orders table. Input is a set of order records; output is the total quantity per item. Results depend on the schema and data contents.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/groupby.md#2025-04-23_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM Orders\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT item, SUM(quantity) FROM Orders GROUPBY item\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Batch Data in Chunks\nDESCRIPTION: Method to iterate over batch data in chunks of specified length, useful for processing large batches in manageable portions with the last chunk potentially being shorter.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef iter_chunks(n: int) -> Iterable[Iterable[SinkItem]]\n```\n\n----------------------------------------\n\nTITLE: Creating Quix Streams Application in Python\nDESCRIPTION: This snippet illustrates how to create a Quix Streams Application instance, specifying the Kafka broker address and consumer group settings.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/websocket-source/tutorial.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\napp = Application(\n    broker_address=\"localhost:9092\",  # your Kafka broker address here\n    auto_offset_reset=\"earliest\",\n)\n```\n\n----------------------------------------\n\nTITLE: Flushing Kinesis Stream Buffer in Python\nDESCRIPTION: Method to flush buffered records to Kinesis stream and wait for completion of all futures.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_64\n\nLANGUAGE: python\nCODE:\n```\ndef flush() -> None\n```\n\n----------------------------------------\n\nTITLE: Tokenizing and Counting Words in Text Reviews\nDESCRIPTION: Defines a function to tokenize text and count words, then applies it to the StreamingDataFrame with expansion.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/word-count/tutorial.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import Counter\n\ndef tokenize_and_count(text):\n    words = Counter(text.lower().replace(\".\", \" \").split()).items()\n    return words\n\nsdf = sdf.apply(tokenize_and_count, expand=True)\n```\n\n----------------------------------------\n\nTITLE: Clearing Application State (Quix Streams, Python)\nDESCRIPTION: Implements a method to entirely clear the Application's internal state. Intended for advanced scenarios requiring reset of application-level storage or computation. Dependencies: Application. Inputs/Outputs: None; performs in-place operation. Limitation: Use with care to avoid unintentional data loss.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/application.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef clear_state()\n```\n\n----------------------------------------\n\nTITLE: ParquetFormat.file_extension Property Implementation in Python\nDESCRIPTION: Property implementation that returns the configured file extension for Parquet files. This overrides the abstract property from the Format base class.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_52\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef file_extension() -> str\n```\n\n----------------------------------------\n\nTITLE: Initializing Quix Streams Application for Purchase Filtering\nDESCRIPTION: Creates a Quix Streams Application with specified broker address, consumer group, and offset reset settings.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/purchase-filtering/tutorial.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom quixstreams import Application\n\napp = Application(\n    broker_address=os.getenv(\"BROKER_ADDRESS\", \"localhost:9092\"),\n    consumer_group=\"purchase_filtering\",\n    auto_offset_reset=\"earliest\",\n)\n```\n\n----------------------------------------\n\nTITLE: Authenticating and Validating the Destination Connection - Python\nDESCRIPTION: Declares the setup abstract method that subclasses must implement to perform authentication, connectivity checks, and validation for the target storage backend. This method is critical for environments that require explicit credential handling or custom validation logic before data can be written. No parameters are taken, and return is implicit via successful setup or exception on failure.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef setup()\n```\n\n----------------------------------------\n\nTITLE: Adding Records to Kinesis Stream in Python\nDESCRIPTION: Method for buffering records before sending to Kinesis stream with batch size handling of 500 records.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_63\n\nLANGUAGE: python\nCODE:\n```\ndef add(value: Any, key: Any, timestamp: int, headers: HeadersTuples,\n        topic: str, partition: int, offset: int) -> None\n```\n\n----------------------------------------\n\nTITLE: Deriving TopicConfig from Multiple Topics in TopicManager - Python\nDESCRIPTION: This class method analyzes multiple Topic objects to derive a unified TopicConfig, suitable for internal changelog or repartition topics. It inspects config values such as retention times and sizes, and is designed for advanced use cases like stream merges or joins. Requires an iterable of Topic objects; outputs a TopicConfig reflecting merged properties.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\\ndef derive_topic_config(cls, topics: Iterable[Topic]) -> TopicConfig\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Elasticsearch Mapping in Python\nDESCRIPTION: This example demonstrates how to specify a custom mapping for the Elasticsearch index, enforcing a specific type for the 'host' field while keeping the default dynamic behavior for other fields.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/elasticsearch-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncustom_mapping = {\n    \"mappings\": {\n        \"dynamic\": \"true\",  # keeps default behavior\n        \"properties\": {\n            \"host\": {  # enforce type for `host` field\n                \"type\": \"keyword\"\n            }\n        },\n    },\n}\nelasticsearch_sink = ElasticsearchSink(\n    # ... other args here ...\n    mapping=custom_mapping,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Directory for File Storage - Python\nDESCRIPTION: Implements a method to configure the base storage directory for all file outputs of a Destination. It enforces a character whitelist (alphanumeric, spaces, dots, slashes, underscores) and will raise a ValueError if the directory name is invalid. This helps ensure safe, consistent, and portable file structure conventions across deployments.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef set_directory(directory: str) -> None\n```\n\n----------------------------------------\n\nTITLE: Running the Quix Streams Application\nDESCRIPTION: Executes the main function containing the Application setup and processing logic when the script is run directly.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/purchase-filtering/tutorial.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Consumer Group Metadata in Python\nDESCRIPTION: Method to get the consumer group metadata, which is used by the producer during consumer offset sending for an Exactly-Once Semantics (EOS) transaction.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef consumer_group_metadata() -> GroupMetadata\n```\n\n----------------------------------------\n\nTITLE: Basic MongoDB Sink Usage\nDESCRIPTION: Demonstrates basic setup and usage of the MongoDB sink with a Quix Streams application.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/mongodb-sink.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.mongodb import MongoDBSink\n\napp = Application(broker_address=\"localhost:9092\")\ntopic = app.topic(\"topic-name\")\n\n# Configure the sink\nmongodb_sink = MongoDBSink(\n    url=\"mongodb://localhost:27017\",\n    db=\"my_mongodb\",\n    collection=\"people\",\n)\n\nsdf = app.dataframe(topic=topic)\nsdf.sink(mongodb_sink)\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: ParquetFormat Class Implementation in Python\nDESCRIPTION: Class that serializes batches of messages into Parquet format using PyArrow. This implementation extends the Format base class but does not support appending to existing files.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nclass ParquetFormat(Format)\n```\n\n----------------------------------------\n\nTITLE: ParquetFormat.serialize Method Implementation in Python\nDESCRIPTION: Method that serializes a SinkBatch into bytes in Parquet format. Each message is converted into a row with _timestamp, _key, and all keys from the message value, with missing fields filled with None.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_53\n\nLANGUAGE: python\nCODE:\n```\ndef serialize(batch: SinkBatch) -> bytes\n```\n\n----------------------------------------\n\nTITLE: Starting a BaseSource in Python\nDESCRIPTION: The abstract start method is triggered in the subprocess when the source is started. The subprocess runs as long as this method executes.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef start() -> None\n```\n\n----------------------------------------\n\nTITLE: Defining BytesSerializer Class in Python\nDESCRIPTION: A serializer class that bypasses bytes without any changes in the Quix Streams library.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass BytesSerializer(Serializer)\n```\n\n----------------------------------------\n\nTITLE: Closing Kafka Consumer in Python\nDESCRIPTION: Method to close down and terminate the Kafka Consumer. It stops consuming, commits offsets (unless auto-commit is disabled), and leaves the consumer group. Registered callbacks may be called from this method.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef close()\n```\n\n----------------------------------------\n\nTITLE: Initializing a Source in Python\nDESCRIPTION: The __init__ method sets up a Source with a name, shutdown timeout, and optional callbacks for client connection events.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(\n    name: str,\n    shutdown_timeout: float = 10,\n    on_client_connect_success: Optional[ClientConnectSuccessCallback] = None,\n    on_client_connect_failure: Optional[ClientConnectFailureCallback] = None\n) -> None\n```\n\n----------------------------------------\n\nTITLE: Defining Kafka Topic for Qualified Customers\nDESCRIPTION: Creates a Topic object for the output topic where qualified customer data will be sent.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/purchase-filtering/tutorial.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncustomers_qualified_topic = app.topic(name=\"customers_coupon_qualified\")\n```\n\n----------------------------------------\n\nTITLE: Setting the Output File Extension Based on Format - Python\nDESCRIPTION: Provides the method for configuring file extension/format for a given Destination by accepting a Format instance. The extension selection logic abstracts format compatibility, enabling files to be written or appended in a manner compliant with the selected serialization format. All format-specific constraints or exceptions are handled within the implementation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef set_extension(format: Format) -> None\n```\n\n----------------------------------------\n\nTITLE: Re-keying Kafka Messages by Item - Python\nDESCRIPTION: Demonstrates re-keying Kafka messages so that the item name is used as the message key rather than store_id. This enables item-based aggregation downstream. Assumes Kafka messages are dictionaries with item and quantity keys. Useful for enabling group_by by item; code is for illustrative purposes.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/groupby.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmessage_1 = {\"key\": \"A\", \"value\": {\"item\": \"A\", \"quantity\": 5}}\n# ...etc...\nmessage_5 = {\"key\": \"B\", \"value\": {\"item\": \"B\", \"quantity\": 1}}\n```\n\n----------------------------------------\n\nTITLE: Stopping a Source in Python\nDESCRIPTION: The stop method for the Source class sets the running property to False, signaling the source to stop gracefully.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef stop() -> None\n```\n\n----------------------------------------\n\nTITLE: Serializing a Row for Kafka with Topic in Python\nDESCRIPTION: The `row_serialize` method of the `Topic` class serializes a `Row` object and a given key into a `KafkaMessage` structure suitable for producing to Kafka, using the configured key and value serializers.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef row_serialize(row: Row, key: Any) -> KafkaMessage\n```\n\n----------------------------------------\n\nTITLE: Writing Batched Data to External Destinations\nDESCRIPTION: Abstract method that implements the actual writing of data to external destinations, with capability to raise SinkBackpressureError when the destination cannot accept new writes, triggering pause of the corresponding topic partition.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@abc.abstractmethod\ndef write(batch: SinkBatch)\n```\n\n----------------------------------------\n\nTITLE: Kinesis Stream Error Handler Class in Python\nDESCRIPTION: Custom exception class for handling cases when a specified Kinesis stream does not exist.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nclass KinesisStreamNotFoundError(Exception)\n```\n\n----------------------------------------\n\nTITLE: Importing librdkafka Config for Kafka Authentication in Python\nDESCRIPTION: This code shows how to import a librdkafka configuration dictionary to create a ConnectionConfig object for Kafka authentication in a Quix Streams application.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/configuration.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.kafka.configuration import ConnectionConfig\n\nlibrdkafka_config = {\n    \"bootstrap.servers\": \"my.url\",\n    \"security.protocol\": \"sasl_plaintext\",\n    \"sasl.mechanism\": \"PLAIN\",\n    \"sasl.username\": \"my_user\",\n    \"sasl.password\": \"my_pass\"\n}\n\n# NOTE: use class directly (ConnectionConfig, NOT ConnectionConfig())\napp = Application(\n    broker_address=ConnectionConfig.from_librdkafka_dict(librdkafka_config)\n)\n```\n\n----------------------------------------\n\nTITLE: Defining SchemaRegistrySerializationConfig for Serializers in Python\nDESCRIPTION: Defines a configuration class for customizing serializer/deserializer interactions with a Schema Registry via Pydantic's BaseSettings. Supports auto-registration, normalization, schema version selection, and subject naming strategies. Dependencies include Pydantic and Confluent serializer conventions, and the parameters control behaviors such as schema registration, reference handling, and Protobuf index encoding.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass SchemaRegistrySerializationConfig(BaseSettings)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Partition Assignment - Kafka Consumer API - Python\nDESCRIPTION: Lists the current set of topic partitions assigned to the consumer. No parameters are necessary; returns a list of assigned TopicPartition objects. Raises KafkaException or RuntimeError if the consumer is closed or an error occurs. Useful for introspection and cluster monitoring tasks.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef assignment() -> List[TopicPartition]\n```\n\n----------------------------------------\n\nTITLE: Checking if a Source is Running in Python\nDESCRIPTION: The running property indicates if the source is currently running. It's set to False when stop is called to gracefully shut down the source.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef running() -> bool\n```\n\n----------------------------------------\n\nTITLE: Running the Quix Streams Application\nDESCRIPTION: Executes the main function containing the Application setup and processing logic.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/word-count/tutorial.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Committed Offsets - Kafka Consumer API - Python\nDESCRIPTION: Fetches committed offsets for given partitions using a Kafka consumer. Takes a list of TopicPartition objects and an optional timeout; returns enriched TopicPartition objects with offset and error information. Errors are surfaced as KafkaException or RuntimeError. Depends on valid consumer connection and partition definitions.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef committed(partitions: List[TopicPartition],\n              timeout: Optional[float] = None) -> List[TopicPartition]\n```\n\n----------------------------------------\n\nTITLE: Count-Based Sliding Window Example Input and Output (JSON)\nDESCRIPTION: Shows example purchase events and the expected output for a count-based sliding window average calculation. Data is formatted as JSON for use with Quix Streams or similar streaming analytics frameworks. These records clarify offset/timestamp handling in count-based window assignment.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\\\"amount\\\": 100, \\\"timestamp\\\": 121, \\\"offset\\\": 1}\\n{\\\"amount\\\": 60, \\\"timestamp\\\": 165, \\\"offset\\\": 2}\\n{\\\"amount\\\": 200, \\\"timestamp\\\": 583, \\\"offset\\\": 3}\\n{\\\"amount\\\": 40, \\\"timestamp\\\": 723, \\\"offset\\\": 4}\\n{\\\"amount\\\": 120, \\\"timestamp\\\": 1009, \\\"offset\\\": 5}\\n{\\\"amount\\\": 80, \\\"timestamp\\\": 1242, \\\"offset\\\": 6}\\n\n```\n\nLANGUAGE: json\nCODE:\n```\n{\\\"average\\\": 120, \\\"start\\\": 121, \\\"end\\\": 583}\\n{\\\"average\\\": 100, \\\"start\\\": 165, \\\"end\\\": 723}\\n{\\\"average\\\": 120, \\\"start\\\": 583, \\\"end\\\": 1009}\\n{\\\"average\\\": 80, \\\"start\\\": 723, \\\"end\\\": 1242}\\n\n```\n\n----------------------------------------\n\nTITLE: Handling Optional External Dependencies in Connectors - Python\nDESCRIPTION: This Python snippet exemplifies how to import an optional external library (e.g., 'influxdb_client_3') for a Quix Streams connector, and, if absent, raise an informative ImportError with guidance for resolving the missing dependency. This pattern ensures that connectors can gracefully handle optional dependencies that are not part of the default installation. It requires no additional parameters, relies only on standard Python exception handling, and outputs a clear error message for users regarding the installation command needed.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/contribution-guide.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntry:\\n    import influxdb_client_3\\nexcept ImportError as exc:\\n    raise ImportError(\\n        'Package \"influxdb3-python\" is missing: '\\n        \"run pip install quixstreams[influxdb3] to fix it\"\\n    ) from exc\\n\n```\n\n----------------------------------------\n\nTITLE: StreamingSeries Class Definition\nDESCRIPTION: Class that represents column operations in a StreamingDataFrame. It allows operations on columns like arithmetic, comparisons, and logical operations, and enables chaining of operations together.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/dataframe.md#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nclass StreamingSeries()\n```\n\n----------------------------------------\n\nTITLE: Installing Quix Streams with Azure File Dependencies\nDESCRIPTION: Command to install Quix Streams with the required optional dependencies for using the Azure File Source connector.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/microsoft-azure-file-source.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[azure-file]\n```\n\n----------------------------------------\n\nTITLE: Stopping a BaseSource in Python\nDESCRIPTION: The abstract stop method is triggered when the application is shutting down. The source must ensure that the run method completes soon after this is called.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef stop() -> None\n```\n\n----------------------------------------\n\nTITLE: Unsubscribing Kafka Consumer in Python with Quix Streams\nDESCRIPTION: Method to remove the current subscription for a Kafka consumer. Raises exceptions if the consumer is closed or if a Kafka error occurs.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef unsubscribe()\n```\n\n----------------------------------------\n\nTITLE: Recommended Pattern: Applying Function in Assignment (Quix Streams, Python)\nDESCRIPTION: Illustrates the correct way to apply a function as a filter to a StreamingDataFrame in Quix Streams by performing the filtering and assignment in a single step. Ensures that the result of sdf.apply(f) is used immediately as a filter, avoiding intermediate references. Requires the quixstreams package and assumes f is a defined function for filtering.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/dataframe-assignments.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# `.apply()` in the same assignment step as its filtering use!\\nsdf = sdf[sdf.apply(f)]\n```\n\n----------------------------------------\n\nTITLE: Running Quix Streams Producer in Python\nDESCRIPTION: Command to run the producer.py script, which fills a Kafka topic with sample chat messages using Quix Streams.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/quickstart.md#2025-04-23_snippet_3\n\nLANGUAGE: commandline\nCODE:\n```\npython producer.py\n```\n\n----------------------------------------\n\nTITLE: Getting Store Name for StatefulSource in Quix Streams (Python)\nDESCRIPTION: Defines the `store_name` property signature for `StatefulSource`. This read-only property returns a string representing the name designated for the source's state store.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n```python\n@property\ndef store_name() -> str\n```\n```\n\n----------------------------------------\n\nTITLE: Installing BigQuery Dependencies for Quix Streams\nDESCRIPTION: Command to install the BigQuery dependencies for Quix Streams using pip.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/google-cloud-bigquery-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: commandline\nCODE:\n```\npip install quixstreams[bigquery]\n```\n\n----------------------------------------\n\nTITLE: Getting Store Partition Count for StatefulSource in Quix Streams (Python)\nDESCRIPTION: Defines the `store_partitions_count` property signature for `StatefulSource`. This read-only property returns an integer representing the number of store partitions associated with the source. This value is used internally to configure the number of partitions for the state changelog topic.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n```python\n@property\ndef store_partitions_count() -> int\n```\n```\n\n----------------------------------------\n\nTITLE: Flushing Pub/Sub Messages in Python\nDESCRIPTION: Method to wait for all pending publish operations to complete successfully in Google Cloud Pub/Sub sink.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_76\n\nLANGUAGE: python\nCODE:\n```\ndef flush() -> None\n```\n\n----------------------------------------\n\nTITLE: Installing S3 Sink Dependencies using Bash\nDESCRIPTION: Installs the required Python dependencies for using the Quix Streams S3 sink functionality. This command uses pip to install the `quixstreams` package along with the optional `[s3]` extras, which include libraries needed for S3 interaction.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/amazon-s3-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[s3]\n```\n\n----------------------------------------\n\nTITLE: Initializing BigQuery Sink in Python\nDESCRIPTION: Constructor for BigQuerySink class that configures connection to Google Cloud BigQuery. Handles batching of records per topic partition and schema management with support for auto-updating columns.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_55\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(project_id: str,\n             location: str,\n             dataset_id: str,\n             table_name: str,\n             service_account_json: Optional[str] = None,\n             schema_auto_update: bool = True,\n             ddl_timeout: float = 10.0,\n             insert_timeout: float = 10.0,\n             retry_timeout: float = 30.0,\n             on_client_connect_success: Optional[\n                 ClientConnectSuccessCallback] = None,\n             on_client_connect_failure: Optional[\n                 ClientConnectFailureCallback] = None,\n             **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Accessing Changelog Topics List with TopicManager in Python\nDESCRIPTION: A read-only property `changelog_topics_list` on the `TopicManager` class. It returns a flat list containing all managed changelog `Topic` objects.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef changelog_topics_list() -> List[Topic]\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Topic for a BaseSource in Python\nDESCRIPTION: The abstract default_topic method is triggered when a topic is not provided to the source. The source must return a default topic configuration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef default_topic() -> Topic\n```\n\n----------------------------------------\n\nTITLE: Creating Default Kafka Topic Configuration\nDESCRIPTION: Method that creates the default Kafka topic configuration, optionally using the file structure to define the partition count based on the file_partition_counter method.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ndef default_topic() -> Topic\n```\n\n----------------------------------------\n\nTITLE: Illustrating S3 Object Structure for Sink Data\nDESCRIPTION: Displays the typical directory and file structure created by the `FileSink` within the specified S3 bucket. It shows how objects are organized hierarchically by the base directory ('data'), topic ('sink_topic'), partition number (e.g., '0', '1'), and finally the object files named after the starting offset of the data batch they contain (e.g., '0000000000000000000.jsonl').\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/amazon-s3-sink.md#2025-04-23_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nmy-bucket/\n└── data/\n    └── sink_topic/\n        ├── 0/\n        │   ├── 0000000000000000000.jsonl\n        │   ├── 0000000000000000123.jsonl\n        │   └── 0000000000000001456.jsonl\n        └── 1/\n            ├── 0000000000000000000.jsonl\n            ├── 0000000000000000789.jsonl\n            └── 0000000000000001012.jsonl\n```\n\n----------------------------------------\n\nTITLE: PandasDataFrameSource Class Definition\nDESCRIPTION: Base class definition for PandasDataFrameSource that enables reading from pandas DataFrames.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nclass PandasDataFrameSource(Source)\n```\n\n----------------------------------------\n\nTITLE: Abstract Method for Retrieving File List\nDESCRIPTION: Abstract method that must be implemented by subclasses to find all files from a root folder. Implementations should return an iterable of all files to be processed in the desired order.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef get_file_list(filepath: Path) -> Iterable[Path]\n```\n\n----------------------------------------\n\nTITLE: Implementing Exists Method\nDESCRIPTION: Abstract method to check if a key exists in the state.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/state.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef exists(key: K) -> bool\n```\n\n----------------------------------------\n\nTITLE: Installing Google Cloud Pub/Sub Dependencies for Quix Streams\nDESCRIPTION: Command to install the required dependencies for using the Pub/Sub source connector with Quix Streams.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/google-cloud-pubsub-source.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[pubsub]\n```\n\n----------------------------------------\n\nTITLE: Installing Redis Dependencies for Quix Streams\nDESCRIPTION: Command to install the Redis dependencies for Quix Streams using pip package manager.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/redis-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: commandline\nCODE:\n```\npip install quixstreams[redis]\n```\n\n----------------------------------------\n\nTITLE: Stopping Local Kafka Instance\nDESCRIPTION: Command to stop the local Kafka instance that was started with Docker Compose. This should be run from the same directory as the docker-compose.yml file.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose down\n```\n\n----------------------------------------\n\nTITLE: Setting Up a BaseSource in Python\nDESCRIPTION: The setup method is used to initialize clients and validate connections/authentication before the source starts running.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef setup()\n```\n\n----------------------------------------\n\nTITLE: Initializing StatefulSource in Quix Streams (Python)\nDESCRIPTION: Defines the constructor (`__init__`) method signature for the `StatefulSource` class. It initializes the source with a mandatory unique `name` (used for topic configuration), an optional `shutdown_timeout` (defaulting to 10 seconds), and optional callback functions `on_client_connect_success` and `on_client_connect_failure` for handling Kafka client connection events.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n```python\ndef __init__(\n    name: str,\n    shutdown_timeout: float = 10,\n    on_client_connect_success: Optional[ClientConnectSuccessCallback] = None,\n    on_client_connect_failure: Optional[ClientConnectFailureCallback] = None\n) -> None\n```\n```\n\n----------------------------------------\n\nTITLE: Defining SinkBackpressureError for Flow Control\nDESCRIPTION: Exception class for signaling backpressure events to the application, causing it to drop accumulated batches, pause assigned topic partitions for a specified timeout, and resume when the timeout elapses.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass SinkBackpressureError(QuixException)\n```\n\n----------------------------------------\n\nTITLE: Installing Azure File Sink Dependencies for Quix Streams\nDESCRIPTION: This command installs the required dependencies for using the Azure File Sink in Quix Streams. It uses pip to install the 'quixstreams' package with the 'azure-file' extra.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/microsoft-azure-file-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[azure-file]\n```\n\n----------------------------------------\n\nTITLE: Format.file_extension Property Implementation in Python\nDESCRIPTION: Abstract property method that returns the file extension used for output files. All format subclasses must implement this to specify their file extension.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_43\n\nLANGUAGE: python\nCODE:\n```\n@property\n@abstractmethod\ndef file_extension() -> str\n```\n\n----------------------------------------\n\nTITLE: Initializing LocalFileSource with Configuration\nDESCRIPTION: Constructor for LocalFileSource that passes configuration parameters to the FileSource parent class. It handles setup for processing files from the local filesystem.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(filepath: Union[str, Path],\n             key_setter: Optional[Callable[[object], object]] = None,\n             value_setter: Optional[Callable[[object], object]] = None,\n             timestamp_setter: Optional[Callable[[object], int]] = None,\n             file_format: Union[Format, FormatName] = \"json\",\n             compression: Optional[CompressionName] = None,\n             has_partition_folders: bool = False,\n             replay_speed: float = 1.0,\n             name: Optional[str] = None,\n             shutdown_timeout: float = 30,\n             on_client_connect_success: Optional[\n                 ClientConnectSuccessCallback] = None,\n             on_client_connect_failure: Optional[\n                 ClientConnectFailureCallback] = None)\n```\n\n----------------------------------------\n\nTITLE: Generating Stream IDs from Topics with TopicManager - Python\nDESCRIPTION: This method constructs a unique stream ID string by concatenating names of provided Topic objects. It is typically used internally to assign or track a stream based on its constituent topics. Takes a sequence of Topic objects and produces a deterministic string identifier.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef stream_id_from_topics(topics: Sequence[Topic]) -> str\n```\n\n----------------------------------------\n\nTITLE: Installing Pub/Sub Dependencies with pip\nDESCRIPTION: Command to install the required dependencies for using the Pub/Sub sink connector\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/google-cloud-pubsub-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[pubsub]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Local Elasticsearch Instance for Testing in Bash\nDESCRIPTION: This command sets up a local Elasticsearch instance for testing purposes using a provided script.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/elasticsearch-sink.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -fsSL https://elastic.co/start-local | sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Topics with JSON Serialization/Deserialization in Quix Streams (Python)\nDESCRIPTION: This snippet shows how to initialize a Quix Streams Application and configure input and output Kafka topics with JSON serializers/deserializers. It demonstrates controlling how messages are encoded/decoded when flowing through the stream processing pipeline. Requires the quixstreams Python package and a valid Kafka endpoint. The key parameters specify the topic name and the serializer/deserializer type.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\napp = Application(broker_address='localhost:9092')\n\n# Input topic will deserialize incoming messages from JSON bytes\ninput_topic = app.topic('input', value_deserializer='json')\n\n# Output topic will serialize outgoing message to JSON bytes\noutput_topic = app.topic('input', value_serializer='json')\n```\n\n----------------------------------------\n\nTITLE: Partition-Based Window Closing Strategy in Python\nDESCRIPTION: Shows implementation of a partition-based window closing strategy where messages advance time and close windows for the entire partition to which the key belongs.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/windowing.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows import Sum\n\napp = Application(...)\nsdf = app.dataframe(...)\n\n# Calculate a sum of values over a window of 10 seconds \n# and use .final() to emit results only when the window is complete\nsdf = sdf.tumbling_window(timedelta(seconds=10)).agg(value=Sum()).final(closing_strategy=\"partition\")\n```\n\n----------------------------------------\n\nTITLE: Registering an Existing Topic in TopicManager - Python\nDESCRIPTION: This method registers an already created Topic with the TopicManager, allowing the manager to update the topic name and configuration if needed. Requires a Topic object as input and returns the (possibly updated) Topic after registration. Ensures that custom topics are tracked and managed through Quix Streams.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/topics.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef register(topic: Topic) -> Topic\n```\n\n----------------------------------------\n\nTITLE: In-Place Operations on StreamingDataFrame (Quix Streams, Python)\nDESCRIPTION: Depicts the in-place (side effect) pattern where StreamingDataFrame operations like print and update are called without assignment. Demonstrates valid operations that mutate or consume the DataFrame directly without needing to store the result. This approach is appropriate for terminal or effectful operations, as opposed to transformations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/dataframe-assignments.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# NOTE: this is an incomplete stub just to show usage\\nfrom quixstreams import Application\\n\\nsdf = Application().dataframe()\\nsdf.print()  # no assignment\\nsdf.update().print()  # no assignment with chaining\n```\n\n----------------------------------------\n\nTITLE: Setting Up Local Kinesis Test Environment\nDESCRIPTION: Docker command for setting up a local Kinesis testing environment using LocalStack.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/amazon-kinesis-source.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -d --name kinesis \\\n-p 4566:4566 \\\n-e SERVICES=kinesis \\\n-e EDGE_PORT=4566 \\\n-e DEBUG=1 \\\nlocalstack/localstack:latest\n```\n\n----------------------------------------\n\nTITLE: Initializing Quix Streams Application in DEBUG Mode (Python)\nDESCRIPTION: This Python code snippet demonstrates initializing the Quix Streams `Application` class with the `loglevel` parameter set to \"DEBUG\". This configuration enables detailed logging output for message processing operations, which is helpful during development and debugging but is explicitly advised against for use in production environments.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/purchase-filtering/tutorial.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nApplication(loglevel=\"DEBUG\")\n```\n\n----------------------------------------\n\nTITLE: JSONFormat.file_extension Property Implementation in Python\nDESCRIPTION: Property implementation that returns the configured file extension for JSON Lines files. This overrides the abstract property from the Format base class.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_48\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef file_extension() -> str\n```\n\n----------------------------------------\n\nTITLE: Defining Message Format for Kafka Production Example in JSON\nDESCRIPTION: Specifies the expected JSON format for messages in the subsequent Kafka production example. Each message contains a temperature reading and a location ID.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n```json\n{\n  \"temperature\": 35,\n  \"location_id\": \"location-1\"\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Quix Streams with S3 Dependencies in Python\nDESCRIPTION: Command to install Quix Streams library with the required S3 dependencies using pip.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/amazon-s3-source.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[s3]\n```\n\n----------------------------------------\n\nTITLE: Installing Python Development Dependencies\nDESCRIPTION: Commands for setting up the development environment including installing required packages and development dependencies.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/CONTRIBUTING.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r requirements.txt -r requirements-dev.txt -r tests/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing Quix Streams Application for Word Counting\nDESCRIPTION: Creates a Quix Streams Application with Kafka connection settings, consumer group, and offset reset configuration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/word-count/tutorial.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom quixstreams import Application\n\napp = Application(\n    broker_address=os.getenv(\"BROKER_ADDRESS\", \"localhost:9092\"),\n    consumer_group=\"product_review_word_counter\",\n    auto_offset_reset=\"earliest\"\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for Quix Streams\nDESCRIPTION: This snippet lists the required Python packages for the Quix Streams project. It includes python-dateutil for date and time manipulation, websockets for WebSocket support, and quixstreams version 3.4.0 or compatible versions for the main Quix Streams functionality.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/websocket-source/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\npython-dateutil\nwebsockets\nquixstreams~=3.4.0\n```\n\n----------------------------------------\n\nTITLE: Handling Processing Errors in Quix Streams Application\nDESCRIPTION: This function shows how to handle and ignore processing exceptions in a Quix Streams application using the on_processing_error callback.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/configuration.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef on_processing_error(exc: Exception, row, logger) -> bool:\n    \"\"\"\n    Handle the processing exception and ignore it\n    \"\"\"\n    logger.error('Ignore processing exception exc=%s row=%s', exc, row)\n    return True\n```\n\n----------------------------------------\n\nTITLE: Example JSON Record: Temperature Reading with Metadata (JSON)\nDESCRIPTION: This is a sample JSON message demonstrating the structure of a temperature reading, including a temperature value, timestamp, and associated metadata (sensor_id). It serves as an example input for StreamingDataFrame transformation operations in the documentation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"temperature\": 35.5,\n  \"timestamp\": 1710865771.3750699,\n  \"metadata\": {\n    \"sensor_id\": \"sensor-1\"\n  }\n} \n```\n\n----------------------------------------\n\nTITLE: Avoided Pattern: Assigning Intermediate Column Operation (Quix Streams, Python)\nDESCRIPTION: Illustrates a discouraged approach where the sum of two columns is first stored in a temporary variable and later assigned, rather than combining the operation and assignment in a single statement. This may result in faulty graph construction within Quix Streams and is not recommended.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/dataframe-assignments.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmy_sum = sdf[\"x\"] + sdf[\"y\"]\\nsdf[\"z\"] = my_sum\n```\n\n----------------------------------------\n\nTITLE: Installing Quix Streams and Running Tutorial Scripts\nDESCRIPTION: Instructions for installing the Quix Streams library and running tutorial scripts. This snippet shows the typical command sequence for setting up and executing a tutorial.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams\n\npython ./path/to/producer.py\n\npython ./path/to/tutorial_app.py\n```\n\n----------------------------------------\n\nTITLE: Unassigning All Partitions from Kafka Consumer in Python\nDESCRIPTION: Method to remove the current partition assignment and stop consuming. May raise KafkaException or RuntimeError if called on a closed consumer.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/kafka.md#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndef unassign()\n```\n\n----------------------------------------\n\nTITLE: Example JSON Input Record: Fahrenheit Temperature (JSON)\nDESCRIPTION: Sample JSON record representing a single temperature reading in Fahrenheit. Used as an example payload for conversion and downstream transformation demonstrations in StreamingDataFrame pipelines.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tempF\": 68\n}\n```\n\n----------------------------------------\n\nTITLE: Local Testing Connection String Configuration\nDESCRIPTION: Example of the connection string to use when testing with a local Azurite emulator instead of a real Azure environment.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/microsoft-azure-file-source.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;\"\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Existing Quixstreams Package\nDESCRIPTION: Command to remove any existing installation of quixstreams to avoid conflicts during development.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/CONTRIBUTING.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip uninstall quixstreams\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Sink Class Definition in Python\nDESCRIPTION: Class definition for PostgreSQLSink that extends BatchingSink to write data to PostgreSQL databases.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_70\n\nLANGUAGE: python\nCODE:\n```\nclass PostgreSQLSink(BatchingSink)\n```\n\n----------------------------------------\n\nTITLE: Installing Neo4j Sink Dependencies with pip\nDESCRIPTION: Command to install the Neo4j sink connector dependencies using pip package manager.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/neo4j-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[neo4j]\n```\n\n----------------------------------------\n\nTITLE: Installing Kinesis Dependencies with Pip\nDESCRIPTION: Command to install the required dependencies for using the Kinesis sink connector\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/amazon-kinesis-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[kinesis]\n```\n\n----------------------------------------\n\nTITLE: Setting Stream Metadata in Legacy Quix Streams Python\nDESCRIPTION: Demonstrates how to set stream metadata and properties in Quix Streams versions before 2.0.0. This functionality is deprecated in versions 2.0.0 and above.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/upgrading-legacy.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Open the output topic which is where data will be streamed out to\ntopic_producer = client.get_topic_producer(topic_id_or_name = \"mytesttopic\")\n\n# Set stream ID or leave parameters empty to get stream ID generated.\nstream = topic_producer.create_stream()\nstream.properties.name = \"Hello World Python stream\"\n\n# Add metadata about time series data you are about to send. \nstream.timeseries.add_definition(\"ParameterA\").set_range(-1.2, 1.2)\nstream.timeseries.buffer.time_span_in_milliseconds = 100\n```\n\n----------------------------------------\n\nTITLE: Defining the S3BucketAccessDeniedError Exception - Python\nDESCRIPTION: Declares a specialized exception to indicate failure when access to the designated Amazon S3 bucket is denied, typically because of insufficient credentials. This exception improves diagnostics for S3Destination implementers and downstream error handlers.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nclass S3BucketAccessDeniedError(Exception)\n```\n\n----------------------------------------\n\nTITLE: MongoDB Document Structure Example\nDESCRIPTION: Shows the structure of a MongoDB document with message metadata included.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/mongodb-sink.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"field_x\": \"value_a\",\n    \"field_y\": \"value_b\",\n    \"__key\": b\"my_key\",\n    \"__headers\": {},\n    \"__timestamp\": 1234567890,\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Elasticsearch Sink Dependencies for Quix Streams in Python\nDESCRIPTION: This command installs the required dependencies for using the Elasticsearch sink connector with Quix Streams.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/elasticsearch-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[elasticsearch]\n```\n\n----------------------------------------\n\nTITLE: Declaring AzureContainerNotFoundError Exception - Python\nDESCRIPTION: Defines a custom exception AzureContainerNotFoundError inheriting from Exception, raised when an Azure file container is missing. Used in file destination handling related to Azure storage integration.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nclass AzureContainerNotFoundError(Exception)\n```\n\n----------------------------------------\n\nTITLE: CSV Output Format Example\nDESCRIPTION: Shows the structure and format of the CSV output file, including headers and sample data rows with their respective fields.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/csv-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nkey,value,timestamp,topic,partition,offset\nb'afd7e8ab-4af5-4322-8417-dbfc7a0d7694',\"{\"\"number\"\": 0}\",1722945524540,numbers-10k-keys,0,0\nb'557bae7f-14b6-46c4-abc3-12f232b54c8e',\"{\"\"number\"\": 1}\",1722945524546,numbers-10k-keys,0,1\n```\n\n----------------------------------------\n\nTITLE: Declaring FileSink for Batch File Writes - Python\nDESCRIPTION: Declaration of FileSink as a subclass of BatchingSink, grouping messages by topic and partition and writing each batch to the specified destination and format. Supports JSON, Parquet, or other formats, and can write to local or cloud-based destinations (default: local). Intended for scalable storage and archival in pipeline processing.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass FileSink(BatchingSink)\n```\n\n----------------------------------------\n\nTITLE: Generating Branches in Python using Quix Streams\nDESCRIPTION: Demonstrates how to create branches from a StreamingDataFrame (SDF) in Quix Streams. It shows that branches are generated by adding new operations to an SDF instance and storing the result as a new variable.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/branching.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsdf_0 = app.dataframe().apply(func_a)\nsdf_0 = sdf_0.apply(func_b)  # sdf_0 -> sdf_0: NOT a (new) branch (adds operation)\nsdf_1 = sdf_0.apply(func_c)  # sdf_0 -> sdf_1: generates new branch off sdf_0\nsdf_2 = sdf_0.apply(func_d)  # sdf_0 -> sdf_2: generates new branch off sdf_0\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Environment Variables\nDESCRIPTION: Example of setting AWS credentials using environment variables instead of passing them directly to the KinesisSink constructor\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/amazon-kinesis-sink.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AWS_ACCESS_KEY_ID=\"your_access_key\"\nexport AWS_SECRET_ACCESS_KEY=\"your_secret_key\"\nexport AWS_DEFAULT_REGION=\"eu-west-2\"\n```\n\n----------------------------------------\n\nTITLE: Installing InfluxDB v3 Sink Dependencies via pip - Command Line\nDESCRIPTION: Installs the optional dependencies required for InfluxDB v3 sink support in Quix Streams. The command uses pip with extras syntax to ensure all underlying libraries are available for Python applications using InfluxDB3Sink. This should be run in your project's environment before attempting to instantiate or use the sink.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/influxdb3-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: commandline\nCODE:\n```\npip install quixstreams[influxdb3]\n```\n\n----------------------------------------\n\nTITLE: File Partition Counter Method in Python\nDESCRIPTION: Method implementation for counting partitions in Azure file storage, following Azure SDK documentation recommendations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sources.md#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef file_partition_counter() -> int\n```\n\n----------------------------------------\n\nTITLE: Conceptual Diagram of SDF Branch Merging\nDESCRIPTION: This diagram illustrates the concept of merging multiple processing branches (originating from B) back into a single path (at P) within a StreamingDataFrame (SDF) context. This represents a potential future feature for consolidating branched data flows.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/branching.md#2025-04-23_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n        C ──> D ──> E\n       /             \\\nA ──> B               P ──> Q\n       \\             /\n        K ──> L --->\n```\n\n----------------------------------------\n\nTITLE: Installing MongoDB Sink Dependencies\nDESCRIPTION: Shows how to install the required dependencies for using the MongoDB sink connector with pip.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/mongodb-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[mongodb]\n```\n\n----------------------------------------\n\nTITLE: Installing Quix Streams with pip\nDESCRIPTION: Command to install Quix Streams library using pip package manager.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/quickstart.md#2025-04-23_snippet_0\n\nLANGUAGE: commandline\nCODE:\n```\npython -m pip install quixstreams\n```\n\n----------------------------------------\n\nTITLE: Separate Source File for Jupyter Notebook\nDESCRIPTION: Example of the correct way to implement a custom source for use in Jupyter notebooks by defining the source in a separate file.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/custom-sources.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%writefile source.py\n# indicate to IPython we want to write this content to a file\n\nfrom quixstreams.sources import Source\n\nimport time\nimport random\n\nclass MySource(Source):\n    def run(self):\n        while self.running:\n            msg = self.serialize(key=\"test\", value=random.randint(0, 10000))\n\n            self.produce(\n                key=msg.key,\n                value=msg.value,\n            )\n            time.sleep(1)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\n\nfrom source import MySource\n\ndef main():\n  app = Application(broker_address=\"localhost:19092\")\n  source = MySource(name=\"mysource\")\n  \n  sdf = app.dataframe(source=source)\n  sdf.print(metadata=True)\n\n  app.run()\n\nif __name__ == \"__main__\":\n  main()\n```\n\n----------------------------------------\n\nTITLE: Performing Column Operations on StreamingDataFrame in Python\nDESCRIPTION: Provides examples of using the DataFrame API on a Quix Streams `StreamingDataFrame` for column-based operations. This includes filtering based on multiple column conditions (using '|' for OR), calculating new columns from existing ones (e.g., 'average'), applying custom functions to columns using `.apply()`, and checking for null values with `.isnull()`. This API requires dictionary-like stream values.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n```python\nsdf = app.dataframe(...)\n\n# Input: {\"total\": 3, \"count\" 2}\n\n# Use columns to filter data based on condition\nsdf = sdf[(sdf['total'] > 0) | (sdf['count'] > 1)]\n\n# Calculate a new value using keys \"total\" and \"count\" \n# and set it back to the value as \"average\"\nsdf['average'] = sdf[\"total\"] / sdf[\"count\"]\n\n# Apply a custom function to a column\n# The function will receive \"average\" column value as an argument\nsdf['average_as_string'] = sdf['average'].apply(lambda avg: f\"Average: {avg}\")\n\n# Check if \"average\" is null\nsdf['average_is_null'] = sdf[\"average\"].isnull()\n```\n```\n\n----------------------------------------\n\nTITLE: Example of EventData JSON Format in Legacy Quix Streams\nDESCRIPTION: Shows the structure of an EventData message in JSON format used in Quix Streams versions before 2.0.0. It includes metadata, timestamp, tags, and event-specific information.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/upgrading-legacy.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"C\": \"JT\",\n  \"K\": \"EventData\",\n  \"V\": {\n    \"Timestamp\": 100,\n    \"Tags\": {\n      \"tag1\": \"tagValue\"\n    },\n    \"Id\": \"event1\",\n    \"Value\": \"value a\"\n  },\n  \"S\": 34,\n  \"E\": 251\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Local InfluxDB3 with Docker - Bash\nDESCRIPTION: Launches a local InfluxDB3-core container using Docker for development or testing purposes. This command runs InfluxDB on port 8181 with in-memory object store, suitable for experimenting without production credentials. Ensure Docker is installed and running on your environment before executing this command. The database is automatically created upon usage by the sink.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/influxdb3-sink.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -d --name influxdb3 \\\n-p 8181:8181 \\\nquay.io/influxdb/influxdb3-core:latest \\\nserve --node-id=host0 --object-store=memory\n```\n\n----------------------------------------\n\nTITLE: Running LocalStack for Local Kinesis Testing\nDESCRIPTION: Docker command to run LocalStack for testing Kinesis functionality locally without connecting to AWS\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/amazon-kinesis-sink.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -d --name kinesis \\\n-p 4566:4566 \\\n-e SERVICES=kinesis \\\n-e EDGE_PORT=4566 \\\n-e DEBUG=1 \\\nlocalstack/localstack:latest\n```\n\n----------------------------------------\n\nTITLE: Starting Local Kafka Instance with Docker Compose\nDESCRIPTION: Command to start a local Kafka instance using Docker Compose. This runs the Kafka setup in detached mode, allowing it to run in the background.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Running Project Tests\nDESCRIPTION: Command to execute the project's test suite using pytest.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/CONTRIBUTING.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npytest\n```\n\n----------------------------------------\n\nTITLE: Starting Local Neo4j Docker Container\nDESCRIPTION: Docker command to start a local Neo4j instance for testing purposes. Sets up the container with exposed ports and default authentication.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/neo4j-sink.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -d --name neo4j \\\n-p 7474:7474 \\\n-p 7687:7687 \\\n--env NEO4J_AUTH=neo4j/local_password \\\nneo4j:latest\n```\n\n----------------------------------------\n\nTITLE: Defining the S3BucketNotFoundError Exception - Python\nDESCRIPTION: Defines a custom exception representing an error condition where the specified Amazon S3 bucket does not exist. Intended to be raised by S3Destination during connection or write attempts for improved error reporting and troubleshooting during setup and operation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nclass S3BucketNotFoundError(Exception)\n```\n\n----------------------------------------\n\nTITLE: Declaring AzureContainerAccessDeniedError Exception - Python\nDESCRIPTION: Defines a custom exception AzureContainerAccessDeniedError inheriting from Exception, indicating that access to the specified Azure file container was denied. Facilitates error signaling in Azure cloud destination integrations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nclass AzureContainerAccessDeniedError(Exception)\n```\n\n----------------------------------------\n\nTITLE: Installing Local Editable Package\nDESCRIPTION: Command to install the package in editable mode for local development and testing.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/CONTRIBUTING.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install --editable .\n```\n\n----------------------------------------\n\nTITLE: Installing MkDocs and Material Theme Dependencies - Python\nDESCRIPTION: Installs MkDocs, the Material theme, and Material theme extensions for rendering and serving the documentation site. This step is crucial prior to serving or building documentation locally. Required for the local development server that renders the Markdown-generated documentation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/build/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython -m pip install mkdocs mkdocs-material mkdocs-material-extensions\n```\n\n----------------------------------------\n\nTITLE: Example of ParameterData JSON Format in Legacy Quix Streams\nDESCRIPTION: Demonstrates the structure of a ParameterData message in JSON format used in Quix Streams versions before 2.0.0. It includes metadata, timestamps, and different value types.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/upgrading-legacy.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"C\": \"JT\",\n  \"K\": \"ParameterData\",\n  \"V\": {\n    \"Epoch\": 1000000,\n    \"Timestamps\": [10, 11, 12],\n    \"NumericValues\": {\n      \"p0\": [100.0, 110.0, 120.0]\n      },\n    \"StringValues\": {\n      \"p1\": [\"100\", \"110\", \"120\"]\n      },\n    \"BinaryValues\": {\n      \"p2\": [\"MTAw\", \"MTEw\", \"MTIw\"]\n      }\n  },\n  \"S\": 34,\n  \"E\": 251\n}\n```\n\n----------------------------------------\n\nTITLE: Publishing Quix Streams to Anaconda\nDESCRIPTION: Command to build and publish the package to Anaconda.org using an API token\nSOURCE: https://github.com/quixio/quix-streams/blob/main/conda/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nANACONDA_API_TOKEN='<Anaconda API Token>' ./conda/release.sh\n```\n\n----------------------------------------\n\nTITLE: Installing PostgreSQL Dependencies for Quix Streams\nDESCRIPTION: Command to install the required PostgreSQL dependencies for Quix Streams using pip package manager.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/postgresql-sink.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[postgresql]\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Quix Streams\nDESCRIPTION: This snippet defines the required Python packages and their version constraints for the Quix Streams project. It includes packages for testing, HTTP requests, containerization, data serialization, database connections, and data manipulation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/tests/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ntestcontainers==4.10.0\npytest\nrequests>=2.32\ndocker>=7.1.0 # Required to use requests>=2.32\nfastavro>=1.8,<2.0\nprotobuf>=5.27.2,<7.0\ninfluxdb3-python>=0.7.0,<1.0\npyiceberg[pyarrow,glue]>=0.7\nredis[hiredis]>=5.2.0,<6\npandas>=1.0.0,<3.0\n```\n\n----------------------------------------\n\nTITLE: Example JSON Record: Temperature Reading after Projection (JSON)\nDESCRIPTION: This is a sample JSON message illustrating the result of projecting/keeping only the temperature and timestamp fields from a full temperature reading record. Used to demonstrate the effect of column selection in StreamingDataFrame operations.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/processing.md#2025-04-23_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"temperature\": 35.5,\n  \"timestamp\": 1710865771.3750699\n}\n```\n\n----------------------------------------\n\nTITLE: Serving Documentation Site Locally with MkDocs - Python\nDESCRIPTION: Runs a local MkDocs server using the specified configuration file '../../mkdocs.yml'. Allows live preview of the documentation site at 'localhost:8000'. Requires that all MkDocs and related dependencies have been installed beforehand.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/build/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmkdocs serve -f ../../mkdocs.yml\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Shell Script - Shell\nDESCRIPTION: Runs the shell script 'build.sh' to generate API documentation for the 'quixstreams' module. Assumes prior installation of required dependencies as listed in 'requirements.txt'. This encapsulates the build process, running necessary commands to output documentation files to the designated folder. Any failures or misconfigurations in environment may lead to script termination.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/build/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./build.sh\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for quix-streams\nDESCRIPTION: This snippet defines the Python package dependencies for the quix-streams project. It specifies the version range for pre-commit and includes additional requirements from a separate file for mypy.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/requirements-dev.txt#2025-04-23_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\npre-commit>=3.4,<4.3\n-r requirements-mypy.txt\n```\n\n----------------------------------------\n\nTITLE: Setting Up Local S3 Emulation with Docker for Testing\nDESCRIPTION: Docker command to run a LocalStack container that emulates S3 services for local development and testing without requiring actual AWS credentials or connectivity.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/amazon-s3-source.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -d --name s3 \\\n-p 4566:4566 \\\n-e SERVICES=s3 \\\n-e EDGE_PORT=4566 \\\n-e DEBUG=1 \\\nlocalstack/localstack:latest\n```\n\n----------------------------------------\n\nTITLE: Installing Kinesis Dependencies for Quix Streams\nDESCRIPTION: Command to install the required dependencies for using the Kinesis source connector with Quix Streams.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/amazon-kinesis-source.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[kinesis]\n```\n\n----------------------------------------\n\nTITLE: Implementing Set Bytes Method\nDESCRIPTION: Abstract method to set a byte value for a given key in the state.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/state.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@abstractmethod\ndef set_bytes(key: K, value: bytes) -> None\n```\n\n----------------------------------------\n\nTITLE: Installing Quix Streams for File Source Connector\nDESCRIPTION: Simple pip installation command to install Quix Streams, which includes the File Source connector without requiring any additional options.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/local-file-source.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams\n```\n\n----------------------------------------\n\nTITLE: Setting Up Git Pre-commit Hook\nDESCRIPTION: Command to install pre-commit hooks for maintaining code quality before commits.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/CONTRIBUTING.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Installing Quix Streams with Pandas Support\nDESCRIPTION: Command to install Quix Streams with the required pandas optional dependencies.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/pandas-source.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install quixstreams[pandas]\n```\n\n----------------------------------------\n\nTITLE: Defining BytesDeserializer Class in Python\nDESCRIPTION: A deserializer class that bypasses bytes without any changes in the Quix Streams library.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/serialization.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BytesDeserializer(Deserializer)\n```\n\n----------------------------------------\n\nTITLE: Executing a Python Script from Shell\nDESCRIPTION: This command executes a Python script named `tutorial_app.py` using the Python interpreter. It's used here to run the Quix Streams tutorial application from the command line in a suitable Python environment.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/purchase-filtering/tutorial.md#2025-04-23_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython tutorial_app.py\n```\n\n----------------------------------------\n\nTITLE: PubSub Topic Error Class Definition in Python\nDESCRIPTION: Exception class for handling cases when a specified Google Cloud Pub/Sub topic does not exist.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/api-reference/sinks.md#2025-04-23_snippet_72\n\nLANGUAGE: python\nCODE:\n```\nclass PubSubTopicNotFoundError(Exception)\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Build Requirements - Python\nDESCRIPTION: Installs the required Python dependencies for building the documentation using a requirements.txt file. Assumes that 'requirements.txt' is present in the 'docs/build' directory. This prepares the environment so that the documentation build script executes correctly. Outputs any errors or dependency conflicts encountered during installation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/build/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Aggregating Over a Single Column in Quix Streams\nDESCRIPTION: This snippet demonstrates how to perform aggregation over a single column using the Min aggregator in Quix Streams. It calculates the minimum temperature over a 10-minute tumbling window.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/aggregations.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nfrom quixstreams import Application\nfrom quixstreams.dataframe.windows import Min\n\napp = Application(...)\nsdf = app.dataframe(...)\n\n# Input:\n# {\"temperature\" : 9999}\n\nsdf = (\n    # Define a tumbling window of 10 minutes\n    sdf.tumbling_window(timedelta(minutes=10))\n\n    # Calculate the Min aggregation over the \"temperature\" column \n    .agg(min_temperature=Min(column=\"temperature\"))\n\n    # Emit results only for closed windows\n    .final()\n)\n\n# Output:\n# {\n#   'start': <window start>, \n#   'end': <window end>, \n#   'min_temperature': 9999  - minimum temperature\n# }\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for Quix Streams\nDESCRIPTION: This snippet lists the required Python packages and their versions for the Quix Streams project. It includes mypy and mypy-extensions for type checking, as well as type stubs for jsonschema, protobuf, and requests to enhance type hinting capabilities.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/requirements-mypy.txt#2025-04-23_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nmypy==1.15.0\nmypy-extensions==1.0.0\ntypes-jsonschema==4.23.0.20241208\ntypes-protobuf==5.29.1.20250403\ntypes-requests==2.32.0.20250328\n```\n\n----------------------------------------\n\nTITLE: Implementing PostgreSQL Sink with Quix Streams\nDESCRIPTION: Example implementation of PostgreSQL sink in Python using Quix Streams. Shows how to initialize PostgreSQLSink with connection parameters and integrate it with StreamingDataFrame for data processing.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/postgresql-sink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sinks.community.postgresql import PostgreSQLSink\n\napp = Application(broker_address=\"localhost:9092\")\ntopic = app.topic(\"numbers-topic\")\n\n# Initialize PostgreSQLSink\npostgres_sink = PostgreSQLSink(\n    host=\"localhost\",\n    port=5432,\n    dbname=\"mydatabase\",\n    user=\"myuser\",\n    password=\"mypassword\",\n    table_name=\"numbers\",\n    schema_auto_update=True\n)\n\nsdf = app.dataframe(topic)\n# Do some processing here ...\n# Sink data to PostgreSQL\nsdf.sink(postgres_sink)\n\nif __name__ == '__main__':\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Custom Source Implementation in Jupyter Notebook\nDESCRIPTION: Example showing how to implement a custom source in a Jupyter notebook environment, demonstrating the multiprocessing limitation.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sources/custom-sources.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.sources import Source\n\nimport random\nimport time\n\nclass MySource(Source):\n    def run(self):\n        while self.running:\n            msg = self.serialize(key=\"test\", value=random.randint(0, 10000))\n\n            self.produce(\n                key=msg.key,\n                value=msg.value,\n            )\n            time.sleep(1)\n\ndef main():\n    app = Application(broker_address=\"localhost:19092\")\n    source = MySource(name=\"mysource\")\n  \n    sdf = app.dataframe(source=source)\n    sdf.print(metadata=True)\n\n    app.run()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Changing State File Path in Quix Streams Application\nDESCRIPTION: This code snippet shows how to change the default state directory when initializing a Quix Streams Application. It allows specifying a custom path for storing state data.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/advanced/stateful-processing.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\napp = Application(\n    broker_address='localhost:9092', \n    state_dir=\"folder/path/here\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Full Name Column to StreamingDataFrame\nDESCRIPTION: Creates a new 'Full Name' column by combining 'First Name' and 'Last Name' using a custom function.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/tutorials/purchase-filtering/tutorial.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_full_name(customer):\n    return f'{customer[\"First Name\"]} {customer[\"Last Name\"]}'\n\nsdf[\"Full Name\"] = sdf.apply(get_full_name)\n```\n\n----------------------------------------\n\nTITLE: Testing Quix Streams Package\nDESCRIPTION: Command to test the package build without uploading to Anaconda\nSOURCE: https://github.com/quixio/quix-streams/blob/main/conda/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./conda/test.sh\n```\n\n----------------------------------------\n\nTITLE: Local MongoDB Docker Setup\nDESCRIPTION: Command to start a local MongoDB instance using Docker for testing.\nSOURCE: https://github.com/quixio/quix-streams/blob/main/docs/connectors/sinks/mongodb-sink.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -d --name mongodb \\\n-p 27017:27017 \\\nmongodb/mongodb-community-server:latest\n```"
  }
]
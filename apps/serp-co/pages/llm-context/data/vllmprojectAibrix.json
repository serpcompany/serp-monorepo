[
  {
    "owner": "vllm-project",
    "repo": "aibrix",
    "content": "TITLE: Installing AIBrix Components in Kubernetes\nDESCRIPTION: Commands to install AIBrix components in a Kubernetes cluster using specific release versions. This installs both the dependency and core components of AIBrix.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/quickstart.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f https://github.com/vllm-project/aibrix/releases/download/v0.2.1/aibrix-dependency-v0.2.1.yaml\nkubectl create -f https://github.com/vllm-project/aibrix/releases/download/v0.2.1/aibrix-core-v0.2.1.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix Stable Version on Kubernetes\nDESCRIPTION: Commands to install the stable version of AIBrix on Kubernetes clusters. This installs both the dependencies (Envoy Gateway and KubeRay) and the core AIBrix components using specific release versions.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/installation.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install component dependencies\nkubectl create -f https://github.com/vllm-project/aibrix/releases/download/v0.2.1/aibrix-dependency-v0.2.1.yaml\n\n# Install aibrix components\nkubectl create -f https://github.com/vllm-project/aibrix/releases/download/v0.2.1/aibrix-core-v0.2.1.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining ScalingAlgorithm Interface in Go\nDESCRIPTION: This code snippet defines the ScalingAlgorithm interface, which is a common interface for all scaling algorithms. It requires the implementation of the ComputeTargetReplicas method, which calculates the number of replicas based on current metrics and scaling specifications.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/controller/podautoscaler/algorithm/README.md#2025-04-22_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ntype ScalingAlgorithm interface {\n    ComputeTargetReplicas(currentPodCount float64, context ScalingContext) int32\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking Model API Endpoints\nDESCRIPTION: Commands to query the AIBrix gateway API, including listing available models, using the completions API, and using the chat completions API with a deployed model.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/quickstart.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# list models\ncurl -v http://${ENDPOINT}/v1/models\n\n# completion api\ncurl -v http://${ENDPOINT}/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"deepseek-r1-distill-llama-8b\",\n        \"prompt\": \"San Francisco is a\",\n        \"max_tokens\": 128,\n        \"temperature\": 0\n    }'\n\n# chat completion api\ncurl http://${ENDPOINT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"deepseek-r1-distill-llama-8b\",\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"help me write a random generator in python\"}\n    ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix for Local Testing\nDESCRIPTION: Commands for cloning the AIBrix repository and installing nightly dependencies and components for local development and testing.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Local Testing\ngit clone https://github.com/vllm-project/aibrix.git\ncd aibrix\n\n# Install nightly aibrix dependencies\nkubectl create -k config/dependency\n\n# Install nightly aibrix components\nkubectl create -k config/default\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix via pip\nDESCRIPTION: Simple command for installing the AIBrix package using pip package manager.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install aibrix\n```\n\n----------------------------------------\n\nTITLE: Installing Stable AIBrix Distribution\nDESCRIPTION: Commands for installing the stable release version of AIBrix, including component dependencies and core components using kubectl.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Install component dependencies\nkubectl create -k \"github.com/vllm-project/aibrix/config/dependency?ref=v0.2.1\"\n\n# Install aibrix components\nkubectl create -k \"github.com/vllm-project/aibrix/config/overlays/release?ref=v0.2.1\"\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix Nightly Version on Kubernetes\nDESCRIPTION: Commands to install the latest development version of AIBrix by cloning the repository and applying Kubernetes manifests using kustomize for both dependencies and core components.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/installation.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# clone the latest repo\ngit clone https://github.com/vllm-project/aibrix.git\ncd aibrix\n\n# Install component dependencies\nkubectl create -k config/dependency\nkubectl create -k config/default\n```\n\n----------------------------------------\n\nTITLE: Setting Up Gateway Endpoint\nDESCRIPTION: Commands to obtain the API gateway endpoint for accessing the deployed models. Provides two options: using LoadBalancer for production clusters or port forwarding for dev environments.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/quickstart.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Option 1: Kubernetes cluster with LoadBalancer support\nLB_IP=$(kubectl get svc/envoy-aibrix-system-aibrix-eg-903790dc -n envoy-gateway-system -o=jsonpath='{.status.loadBalancer.ingress[0].ip}')\nENDPOINT=\"${LB_IP}:80\"\n\n# Option 2: Dev environment without LoadBalancer support. Use port forwarding way instead\nkubectl -n envoy-gateway-system port-forward service/envoy-aibrix-system-aibrix-eg-903790dc 8888:80 &\nENDPOINT=\"localhost:8888\"\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix Autoscaler Component\nDESCRIPTION: Command to install only the AIBrix autoscaler controller component using Kubernetes kustomize.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/installation.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -k config/standalone/autoscaler-controller/\n```\n\n----------------------------------------\n\nTITLE: Building AIBrix from source\nDESCRIPTION: Command to install AIBrix in development mode from source code. This allows for making changes to the code and having them reflect immediately.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# This may take several minutes\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing CRDs to Kubernetes Cluster\nDESCRIPTION: Command to install the Custom Resource Definitions (CRDs) into the Kubernetes cluster. This step needs to be repeated after modifying project code, especially API definitions.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nmake install\n```\n\n----------------------------------------\n\nTITLE: Experiment 2: Benchmarking with 8 LoRA Models and Limited Max LoRAs\nDESCRIPTION: Script for benchmarking with 8 LoRA models while varying concurrency from 1 to 32. This experiment tests the effect of limiting max_loras to 8 on performance. Results are saved to benchmark_unmerged_multi_lora_8_max_loras_8.jsonl.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nfor concurrency in {1..32}; do\n    output_file=\"benchmark_unmerged_multi_lora_8_max_loras_8.jsonl\"\n    echo \"Running benchmark with concurrency ${concurrency} and output file ${output_file}\"\n\n    python3 benchmark.py \\\n        --dataset-path \"/workspace/ShareGPT_V3_unfiltered_cleaned_split.json\" \\\n        --concurrency ${concurrency} \\\n        --output-file-path \"${output_file}\" \\\n        --deployment-endpoints \"deployment3\" \\\n        --models 8\ndone\n```\n\n----------------------------------------\n\nTITLE: Checking AIBrix Pod Status\nDESCRIPTION: Command to check the status of AIBrix pods in the aibrix-system namespace, showing the expected output with all pods in a running state.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/quickstart.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nNAME                                         READY   STATUS    RESTARTS   AGE\naibrix-controller-manager-56576666d6-gsl8s   1/1     Running   0          5h24m\naibrix-gateway-plugins-c6cb7545-r4xwj        1/1     Running   0          5h24m\naibrix-gpu-optimizer-89b9d9895-t8wnq         1/1     Running   0          5h24m\naibrix-kuberay-operator-6dcf94b49f-l4522     1/1     Running   0          5h24m\naibrix-metadata-service-6b4d44d5bd-h5g2r     1/1     Running   0          5h24m\naibrix-redis-master-84769768cb-fsq45         1/1     Running   0          5h24m\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing Docker Image\nDESCRIPTION: Commands to build and push the AIBrix Docker image to a specified registry. The image should be published to a personal registry with proper permissions.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nmake docker-build docker-push IMG=<some-registry>/aibrix:tag\n```\n\n----------------------------------------\n\nTITLE: Querying AIBrix API using curl in Bash\nDESCRIPTION: A sample curl command to interact with the AIBrix API endpoint for chat completions using the deepseek-r1-distill-llama-8b model. This shows how to set up the endpoint variable and format a JSON request to test the deployment.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/terraform-gcp.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nENDPOINT=\"<YOUR PUBLIC IP>\"\n\ncurl http://${ENDPOINT}/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"deepseek-r1-distill-llama-8b\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"help me write a random generator in python\"}\n        ]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark for Different Routing Strategies\nDESCRIPTION: Series of commands to run benchmarks for different routing strategies (K8s service, HTTP route, random, least-request, and throughput). Each command specifies output files for results and logs with the same load parameters.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# service port-forwarding\nOUTPUT_FILE=k8s-service.jsonl locust -f benchmark.py --host http://localhost:8887 --headless --users 30 --spawn-rate 0.08 --run-time 10m --csv benchmark_gateway_httproute.csv --csv-full-history --logfile benchmark_gateway_httproute.log\n\n# gateway port-forwarding\nOUTPUT_FILE=http-route.jsonl locust -f benchmark.py --host http://localhost:8888 --headless --users 30 --spawn-rate 0.08 --run-time 10m --csv benchmark_gateway_httproute.csv --csv-full-history --logfile benchmark_gateway_httproute.log\n\nOUTPUT_FILE=random.jsonl ROUTING_STRATEGY=random locust -f benchmark.py --host http://localhost:8888 --headless --users 30 --spawn-rate 0.08 --run-time 10m --csv benchmark_gateway_random.csv --csv-full-history --logfile benchmark_gateway_random.log\n\nOUTPUT_FILE=least-request.jsonl ROUTING_STRATEGY=least-request locust -f benchmark.py --host http://localhost:8888 --headless --users 30 --spawn-rate 0.08 --run-time 10m --csv benchmark_gateway_least_request.csv --csv-full-history --logfile benchmark_gateway_least_request.log\n\nOUTPUT_FILE=throughput.jsonl ROUTING_STRATEGY=throughput locust -f benchmark.py --host http://localhost:8888 --headless --users 30 --spawn-rate 0.08 --run-time 10m --csv benchmark_gateway_throughput.csv --csv-full-history --logfile benchmark_gateway_throughput.log\n```\n\n----------------------------------------\n\nTITLE: Creating a LoRA Adapter with API Key Authentication\nDESCRIPTION: YAML configuration for deploying a LoRA adapter that includes API key authentication. This shows how to secure LoRA adapter access in production environments.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/lora-dynamic-loading.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# ../../../samples/adapter/adapter-api-key.yaml\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Base Model in Kubernetes for LoRA Adaptation\nDESCRIPTION: YAML configuration for deploying a base model in Kubernetes that will be used with LoRA adapters. This sets up the foundation for attaching different LoRA adapters to the same base model.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/lora-dynamic-loading.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# ../../../samples/adapter/base.yaml\n\n```\n\n----------------------------------------\n\nTITLE: Creating Custom vLLM Docker Image for Distributed Inference\nDESCRIPTION: Dockerfile for creating a custom vLLM container with Ray integration. It builds upon the vLLM OpenAI base image, installs necessary dependencies for health checks, and replaces a utility file to remove placement group validation logic.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/distributed/README.md#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM vllm/vllm-openai:v0.6.2\nRUN apt update && apt install -y wget # important for future healthcheck\nRUN pip3 install ray[default] # important for future healthcheck\nCOPY utils.py /usr/local/lib/python3.12/dist-packages/vllm/executor/ray_utils.py\nENTRYPOINT [\"\"]\n```\n\n----------------------------------------\n\nTITLE: Starting vLLM OpenAI API Server with DeepSeek LLM\nDESCRIPTION: Shell command to start a vLLM server that exposes an OpenAI-compatible API endpoint. It configures the server to use the DeepSeek LLM 7B chat model with specific parameters like maximum model length and chunked prefill.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/client/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport API_KEY=${API_KEY}\npython3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 \\\n--port \"8000\" \\\n--model /root/models/deepseek-llm-7b-chat \\\n--trust-remote-code \\\n--max-model-len \"4096\" \\\n--api-key ${API_KEY} \\\n--enable-chunked-prefill\n```\n\n----------------------------------------\n\nTITLE: Benchmarking GPU Performance for Model in AIBrix\nDESCRIPTION: Commands for benchmarking a specific model's performance on a GPU type. This is the first step in the optimizer-based autoscaling process to gather performance metrics for the model.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/autoscaling/optimizer-based-autoscaling.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward [pod_name] 8010:8000 1>/dev/null 2>&1 &\n# Wait for port-forward taking effect.\naibrix_benchmark -m deepseek-llm-7b-chat -o [path_to_benchmark_output]\n```\n\n----------------------------------------\n\nTITLE: Configuring Distributed KV Cache Cluster in Kubernetes\nDESCRIPTION: YAML configuration for setting up a distributed KV cache cluster in AIBrix, including pod affinity settings to ensure co-location with inference services.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/distributed-kv-cache.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# ../../../samples/kvcache/kvcache.yaml\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Test Client with Streaming Enabled\nDESCRIPTION: Shell command to run a test client that sends workloads to the vLLM server. The client uses a sample workload file and enables streaming mode to collect fine-grained metrics like TTFT (Time To First Token) and TPOT (Time Per Output Token).\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/client/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport API_KEY=${API_KEY}\npython3 client.py \\\n--workload-path \"../generator/output/constant.jsonl\" \\\n--endpoint \"http://localhost:8000\" \\\n--model /root/models/deepseek-llm-7b-chat \\\n--api-key ${API_KEY} \\\n--streaming \\\n--output-file-path output.jsonl\n```\n\n----------------------------------------\n\nTITLE: Deploying Multiple LoRA Models with vLLM\nDESCRIPTION: Deploys 32 LoRA models with a single base model using vLLM (Deployment 3). Each LoRA model is given a unique name (model-1 to model-32) but uses the same weights for simplicity of management.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 vllm serve meta-llama/Llama-2-7b-hf --host \"0.0.0.0\" --port \"8070\" --enable-lora --lora-modules \\\nmodel-1=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-2=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-3=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-4=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-5=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-6=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-7=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-8=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-9=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-10=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-11=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-12=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-13=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-14=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-15=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-16=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-17=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-18=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-19=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-20=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-21=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-22=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-23=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-24=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-25=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-26=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-27=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-28=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-29=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-30=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-31=/models/Fredh99/llama-2-7b-sft-lora \\\nmodel-32=/models/Fredh99/llama-2-7b-sft-lora \\\n--chat-template llama-2-chat.jinja --max-lora-rank=64 --max-loras 32\n```\n\n----------------------------------------\n\nTITLE: Testing Chat Completions API\nDESCRIPTION: cURL command to test the chat completions endpoint with a simple prompt\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/lora/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n     \"model\": \"text2sql-lora-1\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n----------------------------------------\n\nTITLE: Creating a LoRA Model Adapter in Kubernetes\nDESCRIPTION: YAML configuration for setting up a LoRA model adapter that extends the base model with specific fine-tuning. This demonstrates how to register a custom adaptation while reusing the base model infrastructure.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/lora-dynamic-loading.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# ../../../samples/adapter/adapter.yaml\n\n```\n\n----------------------------------------\n\nTITLE: Enabling AI Engine Runtime in Deployment YAML\nDESCRIPTION: YAML configuration to enable the AI Engine Runtime in a Kubernetes deployment. It specifies the runtime container, command, ports, and volume mounts.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- name: aibrix-runtime\n  image: aibrix/runtime:v0.1.0\n  command:\n  - aibrix_runtime\n  - --port\n  - \"8080\"\n  ports:\n  - containerPort: 8080\n    protocol: TCP\n  volumeMounts:\n  - mountPath: /models\n    name: model-hostpath\nvolumes:\n- name: model-hostpath\n  hostPath:\n    path: /root/models\n    type: DirectoryOrCreate\n```\n\n----------------------------------------\n\nTITLE: Testing Model Invocation with cURL\nDESCRIPTION: cURL command to test the model invocation using the OpenAI-compatible interface. This sends a chat completion request to the deployed model with an authorization token.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer test-key-1234567890\" \\\n  -d '{\n     \"model\": \"llama2-7b\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n----------------------------------------\n\nTITLE: Deploying DeepSeek 33B Instruct Model in Kubernetes\nDESCRIPTION: This YAML defines a Kubernetes Deployment for the DeepSeek 33B Instruct model. It specifies resource requirements, environment variables, and volume mounts for the model storage.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/controller/modeladapter/README.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deepseek-33b-instruct\n  namespace: default\n  labels:\n    model.aibrix.ai/name: deepseek-33b-instruct\n    adapter.model.aibrix.ai/enabled: \"true\"\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      model.aibrix.ai/name: deepseek-33b-instruct\n  template:\n    metadata:\n      labels:\n        model.aibrix.ai/name: deepseek-33b-instruct\n    spec:\n      containers:\n      - name: deepseek-33b-instruct\n        image: your-docker-registry/deepseek-33b-instruct:latest\n        resources:\n          requests:\n            nvidia.com/gpu: \"2\"  # Assuming you need a GPU\n          limits:\n            nvidia.com/gpu: \"2\"\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MODEL_PATH\n          value: \"/models/deepseek-33b-instruct\"\n        volumeMounts:\n        - name: model-storage\n          mountPath: /models\n      volumes:\n      - name: model-storage\n        persistentVolumeClaim:\n          claimName: model-pvc\n```\n\n----------------------------------------\n\nTITLE: Analyzing GPU Performance with Varied Max LoRA Settings in Python\nDESCRIPTION: This script analyzes benchmark results for the unmerged_multi_lora approach with different max_loras settings (1, 2, 4, 8). It extracts latency metrics from JSONL files, calculates average values by concurrency level, and creates a bar chart to compare performance across different max_loras configurations.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/plot/aibrix0.1-lora.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom glob import glob\nimport re\nfrom collections import defaultdict\n\n# Function to calculate average latencies grouped by concurrency\ndef extract_latencies_by_concurrency(file_path):\n    latencies_by_concurrency = defaultdict(list)\n    with open(file_path, 'r') as f:\n        for line in f:\n            result = json.loads(line)\n            concurrency = result['concurrency']\n            latency = result['latency']\n            latencies_by_concurrency[concurrency].append(latency)\n    avg_latencies_by_concurrency = {k: sum(v) / len(v) for k, v in latencies_by_concurrency.items()}  \n    return avg_latencies_by_concurrency\n\n# Directory containing the .jsonl files\ndirectory = 'benchmark_unmerged_multi_lora_8_max_loras'  # change this to the actual folder path\n\n# Dictionary to store data\ndata = defaultdict(lambda: defaultdict(list))\n\n# Read all .jsonl files\nfor file_path in glob(os.path.join(directory, '*.jsonl')):\n    # Extract the approach name and number of applications from the file name\n    file_name = os.path.basename(file_path)\n    parts = file_name.split('_')\n    print(parts)\n    max_lora = int(parts[-1].split('.')[0])\n    avg_latencies = extract_latencies_by_concurrency(file_path)\n    approach = f\"unmerged_multi_lora_8_max_loras_{max_lora}\"\n    for concurrency, avg_latency in avg_latencies.items():\n        data[concurrency][approach] = avg_latency\n    \n\n# Create a DataFrame to store the mean GPU seconds\nresults = []\nfor concurrency, approaches in data.items():\n    row = {'Concurrency': concurrency}\n    for approach, latency in approaches.items():\n        row[approach] = latency\n    results.append(row)\ndf = pd.DataFrame(results)\n\n# Plot configuration\napproaches = []\nfor max_lora in [1, 2, 4, 8]:\n    approaches.append(f\"unmerged_multi_lora_8_max_loras_{max_lora}\")\n# approaches = [\"unmerged_multi_lora\", \"merged\", \"unmerged_single_lora\"]\ndf = df.sort_values('Concurrency')\ndf.set_index('Concurrency', inplace=True)\ndf = df[approaches]\n\n# Plotting the grouped bar plot\ndf.plot(kind='bar', figsize=(12, 6))\nplt.xlabel('Concurrency')\nplt.ylabel('Mean GPU Seconds')\nplt.title('Mean GPU Seconds of Requests Varied by --max-loras')\nplt.legend(title='Approach')\nplt.grid(True)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Downloading Model from TOS using AI Runtime in Bash\nDESCRIPTION: Uses AI Runtime's downloader module to fetch a model from a TOS bucket and store it locally.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m aibrix.downloader \\\n    --model-uri tos://aibrix-model-artifacts/deepseek-coder-6.7b-instruct/ \\\n    --local-dir /tmp/aibrix/models_tos/\n```\n\n----------------------------------------\n\nTITLE: Sending Chat Completion Request to AIBrix Gateway\nDESCRIPTION: This curl command demonstrates how to send a chat completion request to the AIBrix gateway, specifying the model and input message.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/gateway-plugins.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v http://${ENDPOINT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"deepseek-r1-distill-llama-8b\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n    \"temperature\": 0.7\n}'\n```\n\n----------------------------------------\n\nTITLE: Sending Chat Completion Request to DeepSeek R1 Model via curl\nDESCRIPTION: Example curl command for sending a chat completion request to the DeepSeek R1 671B model. The request includes a system message, user message, and custom routing strategy header.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/samples/deepseek-r1/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://${ENDPOINT}/v1/chat/completions \\\n    -H \"Content-Type: application/json\" -H \"routing-strategy: least-request\" \\\n    -d '{\n        \"model\": \"deepseek-r1-671b\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Dockerfile Configuration for vLLM Distributed Inference\nDESCRIPTION: Dockerfile configuration to extend the vLLM OpenAI image with additional dependencies required for distributed inference. Adds wget for health checks and Ray default package for cluster management.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/multi-node-inference.rst#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM vllm/vllm-openai:v0.6.1.post2\nRUN apt update && apt install -y wget # important for future healthcheck\nRUN pip3 install ray[default] # important for future healthcheck\nENTRYPOINT [\"\"]\n```\n\n----------------------------------------\n\nTITLE: Comparing Per-LoRA GPU Performance Across Approaches in Python\nDESCRIPTION: This script calculates average GPU seconds per LoRA across different numbers of LoRAs for the unmerged_multi_lora approach, and compares with the merged and unmerged_single_lora approaches. It processes JSONL files, extracts latency data, and creates a bar chart with horizontal lines representing the baseline approaches.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/plot/aibrix0.1-lora.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom glob import glob\nimport re\nfrom collections import defaultdict\n\n# Function to calculate average latency from a list of latencies\ndef calculate_average_latency(file_path):\n    latencies = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            result = json.loads(line)\n            latencies.append(result['latency'])\n    return sum(latencies) / len(latencies) if latencies else float('nan')\n\n# Directory containing the .jsonl files\ndirectory = 'benchmark_result_pin_concurrency'  # change this to the actual folder path\n\n# Dictionary to store the latencies\nlatencies = defaultdict(list)\nunmerged_multi_lora = defaultdict(list)\n\n# Read all .jsonl files and extract latencies\nfor file_path in glob(os.path.join(directory, '*.jsonl')):\n    # Extract the approach name and concurrency value from the file name\n    file_name = os.path.basename(file_path)\n\n    parts = file_name.split('_')\n    if parts[1] == 'unmerged' and 'multi' in parts:\n        #print(parts)\n        approach = '_'.join(parts[1:4])\n        num_loras = int(parts[4])\n        concurrency = int(parts[-1].replace('.jsonl', ''))\n        avg_latency = calculate_average_latency(file_path)\n        unmerged_multi_lora[num_loras].append(avg_latency)\n    else:\n        approach = parts[1]\n    \n        if 'unmerged_single_lora' in file_name:\n            approach = 'unmerged_single_lora'\n        if 'benchmark_merged' in file_name:\n            approach = 'merged'\n        concurrency = int(parts[-1].replace('.jsonl', ''))\n        avg_latency = calculate_average_latency(file_path)\n        print(f\"approach {approach} avg_latency {avg_latency}\")\n        latencies[approach].append(avg_latency)\n\n# Calculate average latencies for unmerged_multi_lora\navg_per_lora_gpu_seconds = {num_loras: (sum(latencies) / len(latencies)) / num_loras\n                            for num_loras, latencies in unmerged_multi_lora.items()}\n\n# Calculate average latencies for merged and unmerged_single_lora\navg_merged_latency = sum(latencies['merged']) / len(latencies['merged'])\navg_unmerged_single_lora_latency = sum(latencies['unmerged_single_lora']) / len(latencies['unmerged_single_lora'])\n\n# Plot the bar plot\nfig, ax = plt.subplots(figsize=(5, 3))\nx = list(avg_per_lora_gpu_seconds.keys())\ny = list(avg_per_lora_gpu_seconds.values())\nax.bar(x, y, color='blue', edgecolor='black')\n\n# Add horizontal lines for merged and unmerged_single_lora\nax.axhline(y=avg_merged_latency, color='red', linestyle='--', linewidth=2, label='Merged')\nax.axhline(y=avg_unmerged_single_lora_latency, color='green', linestyle='--', linewidth=2, label='Unmerged Single Lora')\n```\n\n----------------------------------------\n\nTITLE: Downloading Model from HuggingFace using AI Engine Runtime\nDESCRIPTION: Python command to use the AI Engine Runtime's downloader module for downloading a model from HuggingFace to a local directory.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m aibrix.downloader \\\n    --model-uri deepseek-ai/deepseek-coder-6.7b-instruct \\\n    --local-dir /tmp/aibrix/models_hf/\n```\n\n----------------------------------------\n\nTITLE: Sending Requests to LoRA-Enhanced Model Using cURL\nDESCRIPTION: Bash command to send inference requests to the LoRA-enhanced model. This demonstrates how to target a specific LoRA adapter by name in the model parameter.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/lora-dynamic-loading.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# send request to base model\ncurl -v http://${ENDPOINT}/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"qwen-code-lora\",\n        \"prompt\": \"San Francisco is a\",\n        \"max_tokens\": 128,\n        \"temperature\": 0\n    }'\n```\n\n----------------------------------------\n\nTITLE: Generating GPU-Model Profile with SLO in AIBrix\nDESCRIPTION: Commands for generating a profile for a specific model-GPU combination with defined SLO metrics and cost parameters. This profile is used by the GPU Optimizer to make scaling decisions.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/autoscaling/optimizer-based-autoscaling.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n aibrix-system port-forward svc/aibrix-redis-master 6379:6379 1>/dev/null 2>&1 &\n# Wait for port-forward taking effect.\naibrix_gen_profile deepseek-llm-7b-chat-v100 --cost [cost1] [SLO-metric] [SLO-value] -o \"redis://localhost:6379/?model=deepseek-llm-7b-chat\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Model from TOS with AIBrix Runtime\nDESCRIPTION: This command applies a Kubernetes configuration file to download a model from TOS (presumably a storage service) using the AIBrix runtime system.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/runtime/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f runtime-tos-download.yaml\n```\n\n----------------------------------------\n\nTITLE: Uploading Batch Data for Job Submission in Python\nDESCRIPTION: This snippet demonstrates how to create a BatchDriver instance and upload batch data from a JSON file. It returns a job ID for subsequent operations.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/batch/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n_driver = BatchDriver()\njob_id = _driver.upload_batch_data(\"./one_job_input.json\")\n```\n\n----------------------------------------\n\nTITLE: Making Inference Request to vLLM API\nDESCRIPTION: Example curl command demonstrating how to make a completion request to the deployed vLLM inference endpoint, specifying the model, prompt, and generation parameters.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/distributed/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://vllm-server-raycluster-kf6cq-head-svc.default.svc.cluster.local:8000/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"facebook/opt-13b\",\n\"prompt\": \"San Francisco is a\",\n\"max_tokens\": 128,\n\"temperature\": 0\n}'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Model Information via API in Bash\nDESCRIPTION: Sends a GET request to retrieve information about all available models, including base models and loaded LoRA adapters.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET  http://localhost:8000/v1/models | jq\n```\n\n----------------------------------------\n\nTITLE: Sending Chat Completion Request with Custom Routing Strategy\nDESCRIPTION: This curl command demonstrates how to send a chat completion request to the AIBrix gateway with a specified routing strategy.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/gateway-plugins.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v http://${ENDPOINT}/v1/chat/completions \\\n-H \"routing-strategy: least-request\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"your-model-name\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n    \"temperature\": 0.7\n}'\n```\n\n----------------------------------------\n\nTITLE: Deploying Inference Service with KV Cache Support in Kubernetes\nDESCRIPTION: YAML configuration for deploying an inference service that integrates with the distributed KV cache. Includes volume mounts for the KV cache socket and necessary environment variables.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/distributed-kv-cache.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# ../../../samples/kvcache/deployment.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Optimizer Logs in AIBrix\nDESCRIPTION: Command for viewing the logs of the GPU optimizer component, which provide insights into its optimization process, including calculated costs and suggested GPU allocations.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/autoscaling/optimizer-based-autoscaling.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs <aibrix-gpu-optimizer-podname> -n aibrix-system -f\n```\n\n----------------------------------------\n\nTITLE: Retrieving LoadBalancer IP for AIBrix Gateway\nDESCRIPTION: This snippet shows how to retrieve the external IP of the LoadBalancer service for the AIBrix gateway using kubectl and store it in environment variables.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/gateway-plugins.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nLB_IP=$(kubectl get svc/envoy-aibrix-system-aibrix-eg-903790dc -n envoy-gateway-system -o=jsonpath='{.status.loadBalancer.ingress[0].ip}')\nENDPOINT=\"${LB_IP}:80\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Base Model with Separate LoRA Weights\nDESCRIPTION: Deploys a vLLM server with the base model and separate LoRA weights (Deployment 2). Enables LoRA support with maximum rank of 64 and serves on port 8071.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 vllm serve meta-llama/Llama-2-7b-hf --host \"0.0.0.0\" --port \"8071\" --enable-lora --lora-modules model-1=/models/Fredh99/llama-2-7b-sft-lora --chat-template llama-2-chat.jinja --max-lora-rank=64\n```\n\n----------------------------------------\n\nTITLE: Sending Authenticated Chat Completion Request to AIBrix Gateway\nDESCRIPTION: This curl command shows how to send an authenticated chat completion request to the AIBrix gateway, including an API key in the Authorization header.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/gateway-plugins.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v http://${ENDPOINT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer any_key\" \\\n-d '{\n    \"model\": \"deepseek-r1-distill-llama-8b\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n    \"temperature\": 0.7\n}'\n```\n\n----------------------------------------\n\nTITLE: Analyzing vLLM Performance Metrics\nDESCRIPTION: Python command to analyze performance metrics collected during testing. It specifies a goodput target for analysis, particularly focusing on TPOT (Time Per Output Token) with a target of 0.5 seconds.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/client/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython analyze.py --trace output.jsonl --output output --goodput-target tpot:0.5\n```\n\n----------------------------------------\n\nTITLE: Loading LoRA Adapter via API\nDESCRIPTION: cURL command to load a LoRA adapter by sending a POST request with adapter details\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/lora/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST http://localhost:8000/v1/load_lora_adapter \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"lora_name\": \"text2sql-lora-1\", \"lora_path\": \"yard1/llama-2-7b-sql-lora-test\"}'\n```\n\n----------------------------------------\n\nTITLE: Generating Production Stack YAML files using Helm\nDESCRIPTION: Helm commands to generate YAML configuration files for Production Stack (PS) deployments with different settings. The commands render templates for both the full stack setup and a naive Kubernetes stack.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/ps-experiment/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm template vllm vllm/vllm-stack -f stack.yaml > ps_stack.yaml\nhelm template vllm vllm/vllm-stack -f naive.yaml > ps_k8s_stack.yaml\n```\n\n----------------------------------------\n\nTITLE: Describing APA PodAutoscaler Events\nDESCRIPTION: This command describes the events associated with the APA PodAutoscaler, showing scaling activities and decisions for the Llama deployment.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nkubectl describe podautoscalers podautoscaler-example-mock-llama-apa -n aibrix-system\n```\n\n----------------------------------------\n\nTITLE: Testing Completions API\nDESCRIPTION: cURL command to test the completions endpoint with a basic prompt\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/lora/README.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"text2sql-lora-1\",\n    \"prompt\": \"Say this is a test\",\n    \"max_tokens\": 7,\n    \"temperature\": 0\n  }'\n```\n\n----------------------------------------\n\nTITLE: Creating Performance Comparison Plot with Matplotlib in Python\nDESCRIPTION: Creates a detailed performance comparison plot showing loading times across different concurrency levels (8-24) and chunk sizes (1M-16M) for AIBrix Stream Loader and Transformer Loader. The visualization compares performance for both 6 GiB and 14 GiB datasets using line plots with markers.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/plot/aibrix0.1-downloader.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\n# Data from both images with labels for 6 GiB and 14 GiB datasets\n\n# Concurrency levels are consistent across both datasets\nconcurrency_levels = [8, 12, 16, 20, 24]\n\n# AIBrix Stream Loader times for 6 GiB and 14 GiB datasets with chunk sizes (from both images)\naibrix_14GB = {\n    '1M': [32.22, 22.18, 21.09, 21.96, 22.61],\n}\naibrix_6GB = {\n    '1M': [16.99, 11.78, 12.50, 14.03, 14.32],\n    '4M': [17.20, 12.28, 13.09, 14.48, 14.85],\n    '16M': [17.86, 13.02, 13.09, 14.49, 15.12]\n}\n\n# Transformer Loader times for 6 GiB and 14 GiB datasets\ntransformer_loader_14GB = [46.47, None, 50.06, None, 49.99]\ntransformer_loader_6GB = [20.14, 23.93, 24.70, 25.19, 25.25]\n\n# Function to filter out None values for continuous plotting\ndef filter_none_values(concurrency_levels, times):\n    filtered_concurrency = [c for c, t in zip(concurrency_levels, times) if t is not None]\n    filtered_times = [t for t in times if t is not None]\n    return filtered_concurrency, filtered_times\n\n# Plotting the combined data\nplt.figure(figsize=(12, 8))\n\n# Plot for 14 GiB data\nconcurrency_filtered, times_filtered = filter_none_values(concurrency_levels, transformer_loader_14GB)\nplt.plot(concurrency_filtered, times_filtered, marker='o', linestyle='--', label='Transformer Loader (14 GiB)', color='blue')\n\nconcurrency_filtered, times_filtered = filter_none_values(concurrency_levels, aibrix_14GB['1M'])\nplt.plot(concurrency_filtered, times_filtered, marker='o', label='AIBrix Stream Loader chunk-size 1M (14 GiB)', color='blue')\n\n# Plot for 6 GiB data\nconcurrency_filtered, times_filtered = filter_none_values(concurrency_levels, transformer_loader_6GB)\nplt.plot(concurrency_levels, transformer_loader_6GB, marker='o', linestyle='--', label='Transformer Loader (6 GiB)', color='red')\n\nfor chunk_size, times in aibrix_6GB.items():\n    plt.plot(concurrency_levels, times, marker='o', label=f'AIBrix Stream Loader chunk-size {chunk_size} (6 GiB)', color='red')\n\n# Adding labels and title\nplt.xlabel(\"Concurrency Level\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Combined AIBrix Stream Loader and Transformer Loader Performance for 6 GiB and 14 GiB\")\nplt.legend()\nplt.grid()\n\n# Show plot\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Unloading LoRA Adapter\nDESCRIPTION: cURL command to unload a specific LoRA adapter by name\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/lora/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST http://localhost:8000/v1/unload_lora_adapter \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"lora_name\": \"text2sql-lora-1\"}'\n```\n\n----------------------------------------\n\nTITLE: Viewing AIBrix Controller Manager Logs\nDESCRIPTION: This complex command retrieves and displays logs from the AIBrix controller manager pod, focusing on KPA algorithm execution.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods -n aibrix-system -o name | grep aibrix-controller-manager | head -n 1 | xargs -I {} kubectl logs {} -n aibrix-system\n```\n\n----------------------------------------\n\nTITLE: Implementing Prefix Cache Aware Routing in Shell\nDESCRIPTION: Pseudo-code for the prefix-cache aware routing algorithm. It includes functions for load balancing, prefix matching, and pod selection based on running requests and prefix match percentages.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/plugins/gateway/algorithms/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nfunc prefix_cache_routing(ready_pods []*v1.Pod) {\n    if check_load_imbalance(ready_pods) {\n        target_pod = select_pod_with_least_running_requests(ready_pods)\n    } else {\n        match_pods, prefix_hashes = match_prefix(ready_pods)\n        if len(match_pod) > 0 {\n            target_pod = select_least_loaded_match_pod(match_pods)\n        }\n    }\n\n    // if no target pod is selected, fallback to select pod with least request\n    if target_pod == nil {\n        target_pod = select_pod_with_least_running_requests(ready_pods)\n    }\n}\n\nfunc check_load_imbalance(ready_pods) {\n    // filter pods with min and max number of running requests\n    min_pod = select_pod_min_running_requests()\n    max_pod = select_pod_max_running_requests()\n    \n    // if difference between max & min running requests count \n    // is more than configurable ABS_RUNNING_REQUEST_COUNT (default: 8)\n    // then load is imbalanced\n    if max_pod - min_pod > ABS_RUNNING_REQUEST_COUNT {\n        return true\n    }\n    return false\n}\n\nfunc match_prefix(input_tokens, ready_pods) {\n    // input_tokens are split based off configurable block_sizes and \n    // hash is calculated for each token_block\n    hashes = calculate_hashes(input_tokens)\n\n    // checks if token_block exists on ready_pods [prefix_match], \n    // if present calculate pod_name: prefix_match_percent\n    match_pods_with_prefix_match_percent = check_hashes_on_ready_pods(hashes, ready_pods)\n}\n\nfunc select_least_loaded_match_pod(match_pods_with_prefix_match_percent, ready_pods) {\n    mean = calculate_mean_running_request(ready_pods)   \n    std_dev = calculate_std_dev_running_request(ready_pods)\n\n    // sort match_pods in decreasing perfix_match_percent and \n    // for same prefix_match_percent, sort in increasing running_request count.\n    sort(match_pods_with_prefix_match_percent)\n\n    // select match pod with highest prefix and running_request < (mean + std_dev)\n    for pod := range match_pods_with_prefix_match_percent {\n        if pod.running_request < mean + load_factor*std_dev {\n            return pod\n        }\n    }\n}\n\n// selects pod with minimum running requests, similar to least-request routing algorithm\nfunc select_pod_with_least_running_requests(ready_pods) {\n    return select_pod_min_running_requests()\n}\n```\n\n----------------------------------------\n\nTITLE: Verifying Deployment Status with kubectl\nDESCRIPTION: Command output showing pods running across different nodes with their IP addresses, confirming successful deployment of inference service and KV cache components.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/distributed-kv-cache.rst#2025-04-22_snippet_4\n\nLANGUAGE: RST\nCODE:\n```\nNAME                                            READY   STATUS              RESTARTS   AGE     IP               NODE                                           NOMINATED NODE   READINESS GATES\ndeepseek-coder-7b-instruct-85664648c7-xgp9h     1/1     Running             0          2m41s   192.168.59.224   ip-192-168-41-184.us-west-2.compute.internal   <none>           <none>\ndeepseek-coder-7b-kvcache-7d5896cd89-dcfzt      1/1     Running             0          2m31s   192.168.37.154   ip-192-168-41-184.us-west-2.compute.internal   <none>           <none>\ndeepseek-coder-7b-kvcache-etcd-0                1/1     Running             0          2m31s   192.168.19.197   ip-192-168-3-183.us-west-2.compute.internal    <none>           <none>\n```\n\n----------------------------------------\n\nTITLE: Making Authenticated Requests to LoRA-Enhanced Model\nDESCRIPTION: Bash command to send authenticated requests to a LoRA-enhanced model using API key authorization. This demonstrates how to include authentication headers when accessing secured LoRA endpoints.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/lora-dynamic-loading.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# send request to base model\ncurl -v http://${ENDPOINT}/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer sk-kFJ12nKsFakefVmGpj3QzX65s4RbN2xJqWzPYCjYu7wT3BFake\" \\\n    -d '{\n        \"model\": \"qwen-code-lora-with-key\",\n        \"prompt\": \"San Francisco is a\",\n        \"max_tokens\": 128,\n        \"temperature\": 0\n    }'\n```\n\n----------------------------------------\n\nTITLE: Testing Gateway With Routing Strategy\nDESCRIPTION: cURL command to test the API gateway with least-request routing strategy and additional headers\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/lora/README.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncurl -v http://localhost:8888/v1/chat/completions \\\n  -H \"user: your-user-name\" \\\n  -H \"model: text2sql-lora-1\" \\\n  -H \"routing-strategy: least-request\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer any_key\" \\\n  -d '{\n     \"model\": \"text2sql-lora-1\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n----------------------------------------\n\nTITLE: PodAutoscaler Configuration for V100 GPU\nDESCRIPTION: This YAML snippet shows a sample PodAutoscaler configuration for a V100 GPU in the heterogeneous setup.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/heterogeneous-gpu.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deepseek-coder-7b-v100\n  labels:\n    model.aibrix.ai/name: \"deepseek-coder-7b\"\n    model.aibrix.ai/min_replicas: \"1\" # min replica for gpu optimizer when no workloads.\n... rest yaml deployments\n```\n\n----------------------------------------\n\nTITLE: Viewing AIBrix System Components with kubectl\nDESCRIPTION: Command output showing the AIBrix system pods after deployment, including the KV cache and etcd components.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/distributed-kv-cache.rst#2025-04-22_snippet_0\n\nLANGUAGE: RST\nCODE:\n```\nNAME                                        READY   STATUS    RESTARTS   AGE\ndeepseek-coder-7b-kvcache-596965997-p86cx   0/1     Pending   0          2m\ndeepseek-coder-7b-kvcache-etcd-0            1/1     Running   0          2m\n```\n\n----------------------------------------\n\nTITLE: Configuring Controller Manager for Direct vLLM Integration\nDESCRIPTION: YAML configuration highlighting the flag that enables runtime sidecar support. This shows how to modify the controller-manager deployment to directly synchronize with vLLM for LoRA loading.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/lora-dynamic-loading.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  containers:\n  - args:\n    - --leader-elect\n    - --health-probe-bind-address=:8081\n    - --metrics-bind-address=0\n    - --enable-runtime-sidecar # this line should be removed\n```\n\n----------------------------------------\n\nTITLE: Starting vLLM Engine with LoRA Support in Bash\nDESCRIPTION: Launches the vLLM engine with LoRA support enabled, necessary for dynamic LoRA adapter management.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nVLLM_ALLOW_RUNTIME_LORA_UPDATING=true vllm serve Qwen/Qwen2.5-Coder-1.5B-Instruct --enable-lora\n```\n\n----------------------------------------\n\nTITLE: Launching AI Runtime Server with vLLM Metrics\nDESCRIPTION: Bash command to launch the AI Runtime Server, configuring it to use vLLM as the inference engine and specifying the metrics endpoint.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nINFERENCE_ENGINE=vllm INFERENCE_ENGINE_ENDPOINT=\"http://localhost:8000\" aibrix_runtime --port 8080\n```\n\n----------------------------------------\n\nTITLE: Describing HTTPRoute for AIBrix Model\nDESCRIPTION: This command provides detailed information about a specific HTTPRoute object for an AIBrix model deployment.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/gateway-plugins.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl describe httproute deepseek-r1-distill-llama-8b-router -n aibrix-system\nName:         deepseek-r1-distill-llama-8b-router\nNamespace:    aibrix-system\nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  gateway.networking.k8s.io/v1\nKind:         HTTPRoute\nMetadata:\n  Creation Timestamp:  2025-02-16T17:56:03Z\n  Generation:          1\n  Resource Version:    2641\n  UID:                 2f3f9620-bf7c-487a-967e-2436c3809178\nSpec:\n  Parent Refs:\n    Group:      gateway.networking.k8s.io\n    Kind:       Gateway\n    Name:       aibrix-eg\n    Namespace:  aibrix-system\n  Rules:\n    Backend Refs:\n      Group:\n      Kind:       Service\n      Name:       deepseek-r1-distill-llama-8b\n      Namespace:  default\n      Port:       8000\n      Weight:     1\n    Matches:\n      Headers:\n        Name:   model\n        Type:   Exact\n        Value:  deepseek-r1-distill-llama-8b\n      Path:\n        Type:   PathPrefix\n        Value:  /\n    Timeouts:\n      Request:  120s\nStatus:\n  Parents:\n    Conditions:\n      Last Transition Time:  2025-02-16T17:56:03Z\n      Message:               Route is accepted\n      Observed Generation:   1\n      Reason:                Accepted\n      Status:                True\n      Type:                  Accepted\n      Last Transition Time:  2025-02-16T17:56:03Z\n      Message:               Resolved all the Object references for the Route\n      Observed Generation:   1\n      Reason:                ResolvedRefs\n      Status:                True\n      Type:                  ResolvedRefs\n    Controller Name:         gateway.envoyproxy.io/gatewayclass-controller\n    Parent Ref:\n      Group:      gateway.networking.k8s.io\n      Kind:       Gateway\n      Name:       aibrix-eg\n      Namespace:  aibrix-system\nEvents:           <none>\n```\n\n----------------------------------------\n\nTITLE: Viewing Inference API Response\nDESCRIPTION: Command output showing the response from the inference API, including HTTP headers and the JSON response with the generated text from the model.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/distributed-kv-cache.rst#2025-04-22_snippet_8\n\nLANGUAGE: RST\nCODE:\n```\n*   Trying [::1]:8888...\n* Connected to localhost (::1) port 8888\n> POST /v1/chat/completions HTTP/1.1\n> Host: localhost:8888\n> User-Agent: curl/8.4.0\n> Accept: */*\n> Content-Type: application/json\nHandling connection for 8888\n> Authorization: XXXXXXXXXXXXXXXXXXXXXXXX\n> Content-Length: 174\n>\n< HTTP/1.1 200 OK\n< date: Thu, 30 Jan 2025 23:50:08 GMT\n< server: uvicorn\n< content-type: application/json\n< x-went-into-resp-headers: true\n< transfer-encoding: chunked\n<\n* Connection #0 to host localhost left intact\n{\n  \"id\": \"chat-60f0247aa9294f8abb61e8f24c1503c2\",\n  \"object\": \"chat.completion\",\n  \"created\": 1738281009,\n  \"model\": \"deepseek-coder-7b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"It seems like you're trying to create a container with the name \\\"vllm-openai\\\". However, your question is missing some context. Could you please provide more details? Are you using Docker, Kubernetes, or another container orchestration tool? Or are you asking how to create a container for a specific application or service? The details will help me provide a more accurate answer.\",\n        \"tool_calls\": []\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\",\n      \"stop_reason\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 76,\n    \"total_tokens\": 161,\n    \"completion_tokens\": 85\n  },\n  \"prompt_logprobs\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Starting AI Runtime with vLLM Engine in Bash\nDESCRIPTION: Initializes the AI Runtime service, configuring it to use the vLLM engine for inference.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nINFERENCE_ENGINE=vllm INFERENCE_ENGINE_ENDPOINT=\"http://localhost:8000\" aibrix_runtime --port 8080\n```\n\n----------------------------------------\n\nTITLE: Running AIBrix End-to-End Tests with Environment Variables in Bash\nDESCRIPTION: Command for executing end-to-end tests for the AIBrix project with configurable environment variables. KIND_E2E=true sets up a Kind cluster for testing, and INSTALL_AIBRIX=true installs AIBrix components as part of the test process.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/test/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nKIND_E2E=true INSTALL_AIBRIX=true make test-e2e\n```\n\n----------------------------------------\n\nTITLE: Checking Batch Job Status in Python\nDESCRIPTION: This snippet shows how to check the status of a batch job using the job ID. The status can be PENDING, IN_PROGRESS, or COMPLETED.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/batch/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nstatus = _driver.get_job_status(job_id)\n```\n\n----------------------------------------\n\nTITLE: Running Python Client for Throughput Routing Strategy Testing\nDESCRIPTION: Python client command for testing the throughput routing strategy via the gateway endpoint. This strategy optimizes for maximum overall throughput across service instances.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython client.py \\\n--dataset-path \"/tmp/ShareGPT_V3_unfiltered_cleaned_split.json\" \\\n--endpoint \"http://101.126.81.102:80\" \\\n--num-prompts 2000 \\\n--interval 0.05 \\\n--output-file-path \"throughput-v2.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Scaling Deployment and Testing Metrics\nDESCRIPTION: Commands to scale the deployment and test metrics. This demonstrates how the metrics values are inversely proportional to the number of replicas, which is useful for testing autoscaling.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nkubectl scale deployment llama2-7b --replicas=1\ncurl http://localhost:8000/metrics\n```\n\n----------------------------------------\n\nTITLE: Sending Chat Completion Request with User Identifier for Rate Limiting\nDESCRIPTION: This curl command shows how to send a chat completion request to the AIBrix gateway with a user identifier for rate limiting purposes.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/gateway-plugins.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v http://${ENDPOINT}/v1/chat/completions \\\n-H \"user: your-user-id\" \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer any_key\" \\\n-d '{\n    \"model\": \"your-model-name\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n    \"temperature\": 0.7\n}'\n```\n\n----------------------------------------\n\nTITLE: Unloading LoRA Adapter via API in Bash\nDESCRIPTION: Sends a POST request to unload a previously loaded LoRA adapter from the running model.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8080/v1/lora_adapter/unload \\\n-H \"Content-Type: application/json\" \\\n-d '{\"lora_name\": \"lora-1\"}'\n```\n\n----------------------------------------\n\nTITLE: Example Optimizer-based KPA Configuration in YAML\nDESCRIPTION: A reference to an example YAML configuration for Kubernetes Pod Autoscaler (KPA) using the optimizer-based approach in AIBrix.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/autoscaling/optimizer-based-autoscaling.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# This is a reference to a literalinclude in the original documentation\n```\n\n----------------------------------------\n\nTITLE: Deploying and Configuring AIBrix PodAutoscaler with Nginx Demo\nDESCRIPTION: Commands to deploy an Nginx application and configure the AIBrix PodAutoscaler to maintain CPU usage below 10% by automatically creating and managing a Horizontal Pod Autoscaler.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# Create nginx\nkubectl apply -f config/samples/autoscaling_v1alpha1_demo_nginx.yaml\n# Create AIBrix-pa\nkubectl apply -f config/samples/autoscaling_v1alpha1_podautoscaler.yaml\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get podautoscalers --all-namespaces\n```\n\nLANGUAGE: log\nCODE:\n```\n>>> NAMESPACE   NAME                    AGE\n>>> default     podautoscaler-example   24s\n\nkubectl get deployments.apps\n\n>>> NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n>>> nginx-deployment   1/1     1            1           8s\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get hpa\n```\n\nLANGUAGE: log\nCODE:\n```\n>>> NAME                        REFERENCE                     TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\n>>> podautoscaler-example-hpa   Deployment/nginx-deployment   0%/10%    1         10        1          2m28s\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Provider for AIBrix Stack in Terraform\nDESCRIPTION: This snippet sets up the Kubernetes provider for Terraform. It uses the local execution context, which assumes that kubectl is configured correctly on the local machine.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/deployment/terraform/kubernetes/README.md#2025-04-22_snippet_0\n\nLANGUAGE: hcl\nCODE:\n```\nterraform {\n  required_providers {\n    kubernetes = {\n      source = \"hashicorp/kubernetes\"\n    }\n  }\n}\n\nprovider \"kubernetes\" {\n  config_path = \"~/.kube/config\"\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying AIBrix Manager to Kubernetes\nDESCRIPTION: Commands to deploy the AIBrix Manager to a Kubernetes cluster, specifying the image using the IMG environment variable. For local testing, any name can be used without pushing to a remote registry.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nmake deploy IMG=<some-registry>/aibrix:tag\n```\n\n----------------------------------------\n\nTITLE: Generating Constant QPS Workload in Python\nDESCRIPTION: Python command to generate a workload file with constant target QPS using synthetic patterns.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport TARGET_QPS=1\n\npython workload_generator.py --prompt-file $PROMPT_FILE --interval-ms 1000 --duration-ms 300000 --target-qps $TARGET_QPS --trace-type constant --model \"Qwen/Qwen2.5-Coder-7B-Instruct\" --output-dir \"output\" --output-format jsonl\n```\n\n----------------------------------------\n\nTITLE: Generating Load to Trigger Autoscaling\nDESCRIPTION: Command to run a simple load generator that increases the CPU usage of the Nginx service, demonstrating how the PodAutoscaler reacts by scaling up the number of replicas.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkubectl run load-generator --image=busybox -- /bin/sh -c \"while true; do wget -q -O- http://nginx-service.default.svc.cluster.local; done\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods\n```\n\nLANGUAGE: log\nCODE:\n```\n>>> NAME                                READY   STATUS    RESTARTS   AGE\n>>> load-generator                      1/1     Running   0          86s\n>>> nginx-deployment-5b85cc87b7-gr94j   1/1     Running   0          56s\n>>> nginx-deployment-5b85cc87b7-lwqqk   1/1     Running   0          56s\n>>> nginx-deployment-5b85cc87b7-q2gmp   1/1     Running   0          4m33s\n```\n\n----------------------------------------\n\nTITLE: Checking LoRA Adapter Status in Kubernetes\nDESCRIPTION: Bash commands to check the status of a deployed LoRA adapter in Kubernetes. This shows how to verify that the adapter is properly initialized, scheduled, and ready for use.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/lora-dynamic-loading.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl describe modeladapter qwen-code-lora\n.....\nStatus:\n  Conditions:\n    Last Transition Time:  2025-02-16T19:14:50Z\n    Message:               Starting reconciliation\n    Reason:                ModelAdapterPending\n    Status:                Unknown\n    Type:                  Initialized\n    Last Transition Time:  2025-02-16T19:14:50Z\n    Message:               ModelAdapter default/qwen-code-lora has been allocated to pod default/qwen-coder-1-5b-instruct-5587f4c57d-kml6s\n    Reason:                Scheduled\n    Status:                True\n    Type:                  Scheduled\n    Last Transition Time:  2025-02-16T19:14:55Z\n    Message:               ModelAdapter default/qwen-code-lora is ready\n    Reason:                ModelAdapterAvailable\n    Status:                True\n    Type:                  Ready\n  Instances:\n    qwen-coder-1-5b-instruct-5587f4c57d-kml6s\n  Phase:  Running\nEvents:   <none>\n```\n\n----------------------------------------\n\nTITLE: Creating ServiceMonitor for Prometheus Metrics Collection\nDESCRIPTION: YAML configuration for setting up a Prometheus ServiceMonitor to collect metrics from the DeepSeek deployment. This configuration targets the Ray head node and specifies the service port to scrape.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/samples/deepseek-r1/README.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: deepseek-r1-svc-discover\n  namespace: default\n  labels:\n    volcengine.vmp: \"true\"\nspec:\n  endpoints:\n  - port: service\n  namespaceSelector:\n    matchNames:\n    - default\n  selector:\n    matchLabels:\n      ray.io/node-type: head\n```\n\n----------------------------------------\n\nTITLE: Testing Kubernetes Engine Service with curl Request\nDESCRIPTION: curl command to test the Kubernetes vLLM engine service by sending a completion request to the standard endpoint, verifying that the service is properly operational before running benchmarks.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/ps-experiment/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# only kubernetes naive use k8s service for engine\ncurl -v http://vllm-engine-service:80/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n    \"prompt\": \"San Francisco is a\",\n    \"max_tokens\": 128,\n    \"temperature\": 0\n}'\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix Distributed Inference Component\nDESCRIPTION: Command to install only the AIBrix distributed inference controller component using Kubernetes kustomize.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/installation.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -k config/standalone/distributed-inference-controller/\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Pattern Workload in Python\nDESCRIPTION: Python command to generate a workload file based on synthetic traffic, input lengths, and output lengths patterns.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython workload_generator.py --prompt-file $PROMPT_FILE --interval-ms 1000 --duration-ms 300000 --trace-type synthetic --traffic-pattern \"slight_fluctuation\" --prompt-len-pattern \"slight_fluctuation\" --completion-len-pattern \"slight_fluctuation\" --model \"Qwen/Qwen2.5-Coder-7B-Instruct\" --output-dir \"./output\" --output-format jsonl\n```\n\n----------------------------------------\n\nTITLE: Checking Kubernetes Service and Pod Status\nDESCRIPTION: These commands display the status of the deployed Kubernetes service and pods for the heterogeneous GPU setup.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/heterogeneous-gpu.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get svc\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods\n```\n\n----------------------------------------\n\nTITLE: Creating and Submitting a Batch Job in Python\nDESCRIPTION: This code creates and submits a batch job for inference using the BatchDriver. It requires the job ID, endpoint name, and job duration as parameters.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/batch/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n_driver.create_job(job_id, \"sample_endpoint\", \"20m\")\n```\n\n----------------------------------------\n\nTITLE: Experiment 1: Benchmarking Merged LoRA Model with Varying Concurrency\nDESCRIPTION: Script for benchmarking the merged LoRA model (Deployment 1) with varying concurrency levels from 1 to 32. Results are saved to benchmark_merged.jsonl.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nfor concurrency in {1..32}; do\n    output_file=\"benchmark_merged.jsonl\"\n    echo \"Running benchmark with concurrency ${concurrency} and output file ${output_file}\"\n\n    python3 benchmark.py \\\n        --dataset-path \"/workspace/ShareGPT_V3_unfiltered_cleaned_split.json\" \\\n        --concurrency ${concurrency} \\\n        --output-file-path \"${output_file}\" \\\n        --deployment-endpoints \"deployment1\"\ndone\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix Model Adapter (Lora) Component\nDESCRIPTION: Command to install only the AIBrix model adapter controller component for Lora fine-tuning using Kubernetes kustomize.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/installation.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -k config/standalone/model-adapter-controller\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Multi-turn Conversation Dataset with Python\nDESCRIPTION: This snippet shows how to use the multiturn_prefix_sharing_dataset.py script to generate synthetic multi-turn conversation datasets with controlled prompt lengths, number of turns, and number of sessions.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/dataset-generator/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython multiturn_prefix_sharing_dataset.py \\\n    --prompt-length-mean 100 \\\n    --prompt-length-std 10 \\\n    --num-turns-mean 10 \\\n    --num-turns-std 1 \\\n    --num-sessions-mean 10 \\\n    --num-sessions-std 1\n```\n\n----------------------------------------\n\nTITLE: Creating nvkind Kubernetes Cluster for AIBrix\nDESCRIPTION: This command creates a single-node Kubernetes cluster with GPU support using nvkind and a custom configuration template.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/lambda.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnvkind cluster create --config-template=./hack/lambda-cloud/nvkind-cluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Building and Deploying AIBrix Manager to Kubernetes\nDESCRIPTION: Commands to build a Docker image of the AIBrix controller manager and deploy it to a Kubernetes cluster, useful for testing RBAC configurations.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmake docker-build IMG=aibrix/aibrix-controller-manager:v0.1.1\nmake deploy IMG=aibrix/aibrix-controller-manager:v0.1.1\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Service for Text2SQL LoRA Model\nDESCRIPTION: This YAML defines a Kubernetes Service for the Text2SQL LoRA model. It specifies the service type, port, and selector for routing traffic to the appropriate pods.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/controller/modeladapter/README.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Service\nmetadata:\n  creationTimestamp: \"2024-07-14T21:42:57Z\"\n  labels:\n    model.aibrix.ai/name: llama2-70b\n    adapter.model.aibrix.ai/name: text2sql-lora-1\n  name: text2sql-lora-1\n  namespace: default\n  ownerReferences:\n  - apiVersion: model.aibrix.ai/v1alpha1\n    blockOwnerDeletion: true\n    controller: true\n    kind: ModelAdapter\n    name: text2sql-lora-1\n    uid: 61fd3d3c-8549-4742-8f43-7df8c66f0a6d\n  resourceVersion: \"789949\"\n  uid: bef1fb3e-27d2-4663-ac87-14ef721c3693\nspec:\n  clusterIP: None\n  clusterIPs:\n  - None\n  internalTrafficPolicy: Cluster\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n  - name: http\n    port: 8000\n    protocol: TCP\n    targetPort: 8000\n  publishNotReadyAddresses: true\n  selector:\n    model.aibrix.ai/name: llama2-70b\n  sessionAffinity: None\n  type: ClusterIP\nstatus:\n  loadBalancer: {}\n```\n\n----------------------------------------\n\nTITLE: Docker Build Command for vLLM Distributed Image\nDESCRIPTION: Shell command to build the custom Docker image for distributed inference with specified tag.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/multi-node-inference.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t aibrix/vllm-openai:v0.6.1.post2-distributed .\n```\n\n----------------------------------------\n\nTITLE: Deploying Heterogeneous GPU Configuration in Kubernetes\nDESCRIPTION: This command applies a Kubernetes configuration for heterogeneous GPU deployment using L20 and V100 GPUs.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/heterogeneous-gpu.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f samples/heterogeneous\n```\n\n----------------------------------------\n\nTITLE: Downloading and Converting ShareGPT Dataset with Shell and Python\nDESCRIPTION: This snippet shows how to download the ShareGPT dataset and convert it into a sessioned prompt output using the converter.py script. It includes both the download command and the conversion command.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/dataset-generator/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexport TARGET_DATASET=\"/tmp/ShareGPT_V3_unfiltered_cleaned_split.json\"\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json -O ${TARGET_DATASET}\n\npython converter.py  \\\n    --path ${TARGET_DATASET} \\\n    --type sharegpt \\\n    --tokenizer deepseek-ai/deepseek-llm-7b-chat\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix Dependencies\nDESCRIPTION: This command installs the required dependencies for AIBrix from a specified GitHub repository using kubectl.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/lambda.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -k \"github.com/vllm-project/aibrix/config/dependency?ref=v0.2.1\"\n```\n\n----------------------------------------\n\nTITLE: Cleaning and Reinstalling aibrix in Kubernetes\nDESCRIPTION: Commands to clean up existing installations and reinstall the latest code by deleting and recreating the Kubernetes configurations.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/development/development.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete -f config/default\nkubectl delete -f config/dependency\n```\n\n----------------------------------------\n\nTITLE: AIBrix Installation Commands\nDESCRIPTION: kubectl commands for installing AIBrix dependency and core components.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/samples/deepseek-r1/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f https://github.com/vllm-project/aibrix/releases/download/v0.2.1/aibrix-dependency-v0.2.1.yaml\nkubectl create -f https://github.com/vllm-project/aibrix/releases/download/v0.2.1/aibrix-core-v0.2.1.yaml\n```\n\n----------------------------------------\n\nTITLE: Building the AIBrix Installer\nDESCRIPTION: Command to build the AIBrix installer for the image built and published in the registry. Creates an install.yaml file in the dist directory with necessary resources.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nmake build-installer IMG=<some-registry>/aibrix:tag\n```\n\n----------------------------------------\n\nTITLE: Building and Running AIBrix Manager Locally\nDESCRIPTION: Commands to build and verify the installation of AIBrix, including checking the Custom Resource Definitions (CRDs) and starting the manager locally for development.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd $AIBrix_HOME\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get crds | grep podautoscalers\n```\n\nLANGUAGE: log\nCODE:\n```\n# podautoscalers.autoscaling.aibrix.ai\n```\n\nLANGUAGE: shell\nCODE:\n```\nmake run\n```\n\nLANGUAGE: log\nCODE:\n```\n2024-07-29T11:37:40+08:00\tINFO\tsetup\tstarting manager\n2024-07-29T11:37:40+08:00\tINFO\tstarting server\t{\"kind\": \"health probe\", \"addr\": \"[::]:8081\"}\n2024-07-29T11:37:40+08:00\tINFO\tcontroller-runtime.metrics\tStarting metrics server\n...\nStarting workers\t{\"controller\": \"podautoscaler\", \"controllerGroup\": \"autoscaling.aibrix.ai\", \"controllerKind\": \"PodAutoscaler\", \"worker count\": 1}\n...\n```\n\n----------------------------------------\n\nTITLE: Installing and Verifying GPU Runtimes\nDESCRIPTION: Bash commands for installing and verifying the GPU runtimes necessary for container execution on LambdaLabs or similar GPU cloud instances.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/distributed/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./install.sh # Install nvkind and GPU runtimes\n./verify.sh # Verify GPU runtime is working\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for AIBrix on Lambda Cloud\nDESCRIPTION: This script installs necessary dependencies including nvkind, kubectl, Helm, Go, and the NVIDIA Container Toolkit. It also configures Docker settings for GPU compatibility.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/lambda.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbash hack/lambda-cloud/install.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for TOS Model Download in Bash\nDESCRIPTION: Defines necessary environment variables for downloading a model from TOS (TencentOS), including general settings and TOS-specific credentials.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# General settings\nexport DOWNLOADER_ALLOW_FILE_SUFFIX=\"json, safetensors\"\nexport DOWNLOADER_NUM_THREADS=16\n# AWS settings\nexport TOS_ACCESS_KEY=<INPUT YOUR TOS ACCESS KEY>\nexport TOS_SECRET_KEY=<INPUT YOUR TOS SECRET KEY>\nexport TOS_ENDPOINT=<INPUT YOUR TOS ENDPOINT> # e.g. https://tos-s3-cn-beijing.volces.com\nexport TOS_REGION=<INPUT YOUR TOS REGION> # e..g cn-beijing\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LoRA Benchmark\nDESCRIPTION: Installs the necessary system packages and Python libraries required for running LoRA benchmarks, including huggingface_hub, transformers, openai, peft, and vllm.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\napt update && apt install -y vim wget zip jq\npip3 install huggingface_hub transformers openai peft vllm\n```\n\n----------------------------------------\n\nTITLE: Defining EndpointSlice for Text2SQL LoRA Service in Kubernetes\nDESCRIPTION: This YAML defines an EndpointSlice for the Text2SQL LoRA service. It specifies the IP address and port for the endpoint, which is used for service discovery and load balancing.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/controller/modeladapter/README.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\naddressType: IPv4\napiVersion: discovery.k8s.io/v1\nendpoints:\n- addresses:\n  - 10.1.2.133\n  conditions: {}\nkind: EndpointSlice\nmetadata:\n  creationTimestamp: \"2024-07-14T21:42:59Z\"\n  generation: 1\n  labels:\n    kubernetes.io/service-name: text2sql-lora-1\n  name: text2sql-lora-1\n  namespace: default\n  ownerReferences:\n  - apiVersion: model.aibrix.ai/v1alpha1\n    blockOwnerDeletion: true\n    controller: true\n    kind: ModelAdapter\n    name: text2sql-lora-1\n    uid: 61fd3d3c-8549-4742-8f43-7df8c66f0a6d\n  resourceVersion: \"789958\"\n  uid: bf913402-b97d-426d-89a9-8ea734ba8a7a\nports:\n- name: http\n  port: 80\n  protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for S3 Model Download in Bash\nDESCRIPTION: Defines necessary environment variables for downloading a model from AWS S3, including general settings and AWS-specific credentials.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# General settings\nexport DOWNLOADER_ALLOW_FILE_SUFFIX=\"json, safetensors\"\nexport DOWNLOADER_NUM_THREADS=16\n# AWS settings\nexport AWS_ACCESS_KEY_ID=<INPUT YOUR AWS ACCESS KEY ID>\nexport AWS_SECRET_ACCESS_KEY=<INPUT YOUR AWS SECRET ACCESS KEY>\nexport AWS_ENDPOINT_URL=<INPUT YOUR AWS ENDPOINT URL> # e.g. https://s3.us-west-2.amazonaws.com\nexport AWS_REGION=<INPUT YOUR AWS REGION> # e.g. us-west-2\n```\n\n----------------------------------------\n\nTITLE: Deploying vLLM Instance with RayJob\nDESCRIPTION: Kubernetes command to apply job configuration for launching the vLLM instance using Ray.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/distributed/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f job.yaml\n```\n\n----------------------------------------\n\nTITLE: Testing Routing Strategy with Custom Header\nDESCRIPTION: cURL command to test routing strategy functionality using a custom header. This example shows how to specify 'random' as the routing strategy for the request.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ncurl -v http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer test-key-1234567890\" \\\n  -H \"routing-strategy: random\" \\\n  -d '{\n     \"model\": \"llama2-7b\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n----------------------------------------\n\nTITLE: Configuring Endpoints for Text2SQL LoRA Service in Kubernetes\nDESCRIPTION: This YAML defines the Endpoints resource for the Text2SQL LoRA service. It specifies the IP address, node name, and port for the service endpoint, which is used for service discovery and load balancing.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/controller/modeladapter/README.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Endpoints\nmetadata:\n  annotations:\n    endpoints.kubernetes.io/last-change-trigger-time: \"2024-07-14T21:42:57Z\"\n  creationTimestamp: \"2024-07-14T21:42:57Z\"\n  labels:\n    model.aibrix.ai/name: llama2-70b\n    adapter.model.aibrix.ai/name: text2sql-lora-1\n    service.kubernetes.io/headless: \"\"\n  name: text2sql-lora-1\n  namespace: default\n  resourceVersion: \"789951\"\n  uid: 7f64255c-ff58-49fa-9ec3-f19164f884ba\nsubsets:\n- addresses:\n  - ip: 10.1.2.133\n    nodeName: docker-desktop\n    targetRef:\n      kind: Pod\n      name: lora-test\n      namespace: default\n      uid: 408484b6-38e9-4fa1-8b2c-e57753a0f220\n  ports:\n  - name: http\n    port: 8000\n    protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Creating Release Branch for Minor Version in AIBrix Project\nDESCRIPTION: Commands for creating a new release branch from the main branch for a minor version release (e.g., v0.1.0). This includes fetching and rebasing the main branch, creating and pushing the new release branch.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/development/release.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout main && git fetch origin main && git rebase origin/main\ngit checkout -b release-0.1 # cut from main branch\ngit push origin release-0.1\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for vLLM Simulator\nDESCRIPTION: Command to build a Docker image for the vLLM simulator with A100 GPU simulation. This creates a base image tagged as 'aibrix/vllm-simulator:nightly'.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\ndocker build -t aibrix/vllm-simulator:nightly --build-arg SIMULATION=a100 -f Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Retrieving Batch Job Results in Python\nDESCRIPTION: This code retrieves the results of a batch job. If the job is completed, it returns all results; otherwise, it may contain partial results for the input requests.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/batch/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresults = _driver.retrieve_job_result(job_id)\n```\n\n----------------------------------------\n\nTITLE: Creating KPA-based AIBrix PodAutoscaler\nDESCRIPTION: This command creates an autoscaler of type KPA using a YAML configuration file.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f config/samples/autoscaling_v1alpha1_kpa.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting Up Benchmark Client Environment and Dependencies\nDESCRIPTION: Bash commands to prepare the benchmark client environment by installing necessary system packages and Python dependencies, as well as downloading the required benchmark code files from GitHub.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/ps-experiment/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# setup environments\nsudo apt update && sudo apt install vim curl -y\n\n# install benchmark dependencies.\n# append `-i https://pypi.tuna.tsinghua.edu.cn/simple` in CN env.\npip3 install openai pandas tqdm\n\n# Prepare the benchmark codes.\n# https://raw.githubusercontent.com/vllm-project/production-stack/refs/heads/main/benchmarks/multi-round-qa/multi-round-qa.py\n# https://raw.githubusercontent.com/vllm-project/production-stack/refs/heads/main/benchmarks/multi-round-qa/utils.py\n# https://raw.githubusercontent.com/vllm-project/production-stack/refs/heads/main/benchmarks/multi-round-qa/run.sh\nvim multi-round-qa.py\nvim utils.py\nvim run.sh\n```\n\n----------------------------------------\n\nTITLE: Deploying Aibrix Gateway\nDESCRIPTION: Sets up Aibrix gateway components in Kubernetes\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/vllm/README.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nmake docker-build-all\nkubectl create -k config/dependency\nkubectl create -k config/default\n```\n\n----------------------------------------\n\nTITLE: Deploying Simulator Model with Kubectl\nDESCRIPTION: Commands to deploy and delete the simulator model using Kubernetes. The deployment configuration is located in the config/simulator directory.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -k config/simulator\n\n# you can run following command to delete the deployment \nkubectl delete -k config/simulator\n```\n\n----------------------------------------\n\nTITLE: Generating Performance Profile Based on SLO Target\nDESCRIPTION: Python commands to generate profiles for the vLLM model based on SLO targets, with both direct python and make shortcut methods.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython optimizer/profiling/gen_profile.py simulator-llama2-7b-a100 -o \"redis://localhost:6379/?model=llama2-7b\"\n# Or use make\nmake DP=simulator-llama2-7b-a100 gen-profile\n```\n\n----------------------------------------\n\nTITLE: Finding Autoscaling Sample Files in AIBrix\nDESCRIPTION: Location of sample configuration files for autoscaling in the AIBrix repository, which can be used as templates for setting up autoscaling.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/autoscaling/optimizer-based-autoscaling.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhttps://github.com/vllm-project/aibrix/tree/main/samples/autoscaling\n```\n\n----------------------------------------\n\nTITLE: Downloading Facebook OPT-125M Model\nDESCRIPTION: Downloads the facebook/opt-125m model locally using huggingface-cli\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/vllm/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhuggingface-cli download facebook/opt-125m\n```\n\n----------------------------------------\n\nTITLE: Building Docker Images for aibrix\nDESCRIPTION: Command to build nightly docker images for the aibrix project using make.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/development/development.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-build-all\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding for Redis Access\nDESCRIPTION: Commands to expose Redis service locally for profile generation, including both direct kubectl command and make shortcut.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Make sure Redis is accessable locally:\nkubectl -n aibrix-system port-forward svc/aibrix-redis-master 6379:6379 1>/dev/null 2>&1 &\n# Or use make\nmake debug-init\n```\n\n----------------------------------------\n\nTITLE: Downloading Models from Hugging Face Hub\nDESCRIPTION: Downloads the base Llama-2-7b model and SQL LoRA weights from Hugging Face Hub using the snapshot_download function. Requires a Hugging Face token for access.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom huggingface_hub import snapshot_download\n\nos.environ['HF_TOKEN']=\"hf_xxxxxxxx\"\n\nbase_model_path = snapshot_download(repo_id=\"meta-llama/Llama-2-7b-hf\", ignore_patterns=[ \"*.bin\"])\nsql_lora_path = snapshot_download(repo_id=\"Fredh99/llama-2-7b-sft-lora\", local_dir=\"/models/Fredh99/llama-2-7b-sft-lora\")\n```\n\n----------------------------------------\n\nTITLE: Listing HTTPRoute Objects for AIBrix Gateway\nDESCRIPTION: This command lists all HTTPRoute objects across namespaces, which are used by the gateway for dynamic request routing.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/gateway-plugins.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get httproute -A\nNAMESPACE       NAME                                  HOSTNAMES   AGE\naibrix-system   aibrix-reserved-router                            17m # reserved router\naibrix-system   deepseek-r1-distill-llama-8b-router               14m # created for each model deployment\n....\n```\n\n----------------------------------------\n\nTITLE: Deploying Model for Linux\nDESCRIPTION: Creates and deletes Kubernetes resources for Linux deployment\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/vllm/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -k vllm/linux\n\nkubectl delete -k vllm/linux\n```\n\n----------------------------------------\n\nTITLE: Downloading Model from HuggingFace with AIBrix Runtime\nDESCRIPTION: This command applies a Kubernetes configuration file to download a model from HuggingFace using the AIBrix runtime system.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/runtime/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f runtime-hf-download.yaml\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark Script for GPU Profiling\nDESCRIPTION: Command to execute the benchmark.sh script with a deployment name parameter. The script profiles GPU performance and stores results in the result directory.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/optimizer/profiling/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbenchmark.sh [your deployment name]\n```\n\n----------------------------------------\n\nTITLE: Loading Docker Image to Kind Cluster\nDESCRIPTION: Optional command to load the Docker image into a Kind Kubernetes cluster. This step is only necessary for Kind users, as Docker Desktop on Mac shares the local image repository with Kubernetes.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkind load docker-image aibrix/vllm-mock:nightly\n```\n\n----------------------------------------\n\nTITLE: Testing Gateway with Direct API Request\nDESCRIPTION: Curl command for testing the gateway with an API request to the LLM endpoint. This uses a simplified header structure without explicit routing strategy, demonstrating the clean API compatibility approach.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl  http://localhost:8888/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer any_key\" \\\n  -d '{\n     \"model\": \"llama2-70b\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n----------------------------------------\n\nTITLE: Debugging Gateway Service\nDESCRIPTION: Command to debug service exposure by listing services in the envoy-gateway-system namespace, showing the LoadBalancer service with its external IP address.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/quickstart.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get svc -n envoy-gateway-system\nNAME                                     TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                   AGE\nenvoy-aibrix-system-aibrix-eg-903790dc   LoadBalancer   10.96.239.246   101.18.0.4    80:32079/TCP                              10d\nenvoy-gateway                            ClusterIP      10.96.166.226   <none>        18000/TCP,18001/TCP,18002/TCP,19001/TCP   10d\n```\n\n----------------------------------------\n\nTITLE: Downloading Model from AWS S3 with AIBrix Runtime\nDESCRIPTION: This command applies a Kubernetes configuration file to download a model from AWS S3 using the AIBrix runtime system.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/runtime/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f runtime-s3-download.yaml\n```\n\n----------------------------------------\n\nTITLE: Exporting KubeRay Operator Manifests with Helm\nDESCRIPTION: Command to extract KubeRay operator manifests from a Helm chart with specific configurations. The command includes enabling custom features, setting environment variables, and exporting the CRDs to a specified output directory.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/config/dependency/kuberay-operator/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhelm template kuberay-operator kuberay/kuberay-operator --namespace aibrix-system --version 1.2.1 --include-crds --set env[0].name=ENABLE_PROBES_INJECTION --set-string env[0].value=false --set fullnameOverride=kuberay-operator --set featureGates[0].name=RayClusterStatusConditions --set featureGates[0].enabled=true --output-dir ./config/dependency\n```\n\n----------------------------------------\n\nTITLE: Scaling to Multiple Replicas and Checking Metrics\nDESCRIPTION: Commands to scale the deployment to multiple replicas and check metrics. This shows how metrics values decrease proportionally with more replicas.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nkubectl scale deployment llama2-7b --replicas=5\ncurl http://localhost:8000/metrics\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for HuggingFace Model Download\nDESCRIPTION: Bash commands to set environment variables required for downloading models from HuggingFace, including file suffix allowlist and number of threads.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# General settings\nexport DOWNLOADER_ALLOW_FILE_SUFFIX=\"json, safetensors\"\nexport DOWNLOADER_NUM_THREADS=16\n# HuggingFace settings\nexport HF_ENDPOINT=https://hf-mirror.com  # set it when env is in CN region\n```\n\n----------------------------------------\n\nTITLE: RayClusterFleet YAML Configuration\nDESCRIPTION: YAML configuration reference for RayClusterFleet deployment, referenced in the documentation for cluster setup. The actual YAML content is loaded from an external file.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/multi-node-inference.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n../../../samples/distributed/fleet-two-node.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying aibrix to Kubernetes Development Environment\nDESCRIPTION: Commands to deploy the latest code changes to a Kubernetes development environment by creating the dependency and default configurations.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/development/development.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f config/dependency\nkubectl delete -f config/default\n```\n\n----------------------------------------\n\nTITLE: Creating and Pushing Tags for AIBrix Project Release\nDESCRIPTION: Commands for creating and pushing a tag for a new release version. This includes fetching and rebasing the release branch, creating a version tag, and pushing it to the remote repository.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/development/release.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# make sure you fetch the earlier PR locally\ngit fetch origin release-0.1\ngit rebase origin/release-0.1\n\n# create the tag\ngit tag v0.1.0\n\n# push the tag\ngit push origin v0.1.0\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding for Service Access\nDESCRIPTION: Command output showing port forwarding setup from localhost to the Envoy Gateway service, enabling local access to the inference API.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/distributed-kv-cache.rst#2025-04-22_snippet_6\n\nLANGUAGE: RST\nCODE:\n```\nForwarding from 127.0.0.1:8888 -> 10080\nForwarding from [::1]:8888 -> 10080\n```\n\n----------------------------------------\n\nTITLE: Setting Up NVIDIA GPU Operator for AIBrix\nDESCRIPTION: This script installs the NVIDIA GPU Operator using Helm and configures the cloud provider for integration with Lambda Cloud.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/lambda.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbash ./hack/lambda-cloud/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Merging Base Model and LoRA Weights\nDESCRIPTION: Combines a base model (Llama-2-7b) with LoRA weights to create a merged model. Loads the base model and tokenizer, applies LoRA weights, and saves the merged model to a specified output path.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig\n\ndef merge_base_and_lora(base_model_name_or_path, lora_model_name_or_path, output_path):\n    # Load the base model and tokenizer\n    print(\"Loading base model...\")\n    base_model = AutoModelForCausalLM.from_pretrained(base_model_name_or_path)\n    tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path)\n    \n    # Load the LoRA weights\n    print(\"Loading LoRA model...\")\n    peft_config = PeftConfig.from_pretrained(lora_model_name_or_path)\n    lora_model = PeftModel.from_pretrained(base_model, lora_model_name_or_path)\n    \n    # Merge LoRA weights into the base model\n    print(\"Merging LoRA weights into the base model...\")\n    lora_model = lora_model.merge_and_unload()\n    \n    # Save the merged model\n    print(f\"Saving the merged model to {output_path}...\")\n    lora_model.save_pretrained(output_path)\n    tokenizer.save_pretrained(output_path)\n\n    print(f\"Merged model saved successfully at {output_path}!\")\n\nmerge_base_and_lora('meta-llama/Llama-2-7b-hf', '/models/Fredh99/llama-2-7b-sft-lora', '/models/merge-model-weights')\n```\n\n----------------------------------------\n\nTITLE: Creating a User in AIBrix Metadata Service\nDESCRIPTION: This curl command sends a POST request to create a new user with specified name, RPM (requests per minute), and TPM (tokens per minute) values.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/metadata/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:8090/CreateUser \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"your-user-name\",\"rpm\": 100,\"tpm\": 1000}'\n```\n\n----------------------------------------\n\nTITLE: Running Autoscaling Experiment Command\nDESCRIPTION: Command to execute the overnight autoscaling benchmark experiment with a specific workload file.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/autoscaling/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./overnight_run.sh workload/workload/25min_up_and_down/25min_up_and_down.jsonl\n```\n\n----------------------------------------\n\nTITLE: Checking and Overriding Metrics\nDESCRIPTION: Commands to check current metrics and override specific metric values. This example shows how to set the GPU cache usage percentage to a custom value.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n# check metrics\ncurl -X GET http://localhost:8000/metrics\n\n# override metrics\ncurl -X POST http://localhost:8000/set_metrics -H \"Content-Type: application/json\" -d '{\"gpu_cache_usage_perc\": 75.0}'\n```\n\n----------------------------------------\n\nTITLE: Testing API Request with Verbose Output\nDESCRIPTION: cURL command to test the model API with verbose output. This demonstrates a basic chat completion request with authorization.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ncurl -v http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer test-key-1234567890\" \\\n  -d '{\n     \"model\": \"llama2-7b\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n----------------------------------------\n\nTITLE: Making API Request to AIBrix Endpoint\nDESCRIPTION: Example curl command demonstrating how to make a chat completion request to the deployed AIBrix endpoint. The request uses the deepseek-r1-distill-llama-8b model to generate a response for writing a random generator in Python.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/deployment/terraform/gcp/docs.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nENDPOINT=\"<YOUR PUBLIC IP>\"\n\ncurl http://${ENDPOINT}/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"deepseek-r1-distill-llama-8b\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"help me write a random generator in python\"}\n        ]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Reading User Data from AIBrix Metadata Service\nDESCRIPTION: This curl command sends a POST request to retrieve user data for a specified user name.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/metadata/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:8090/ReadUser \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"your-user-name\"}'\n```\n\n----------------------------------------\n\nTITLE: Generating Analysis Report Command\nDESCRIPTION: Python command to generate analysis reports from experiment results.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/autoscaling/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython <aibrix_root_repo>/benchmarks/plot/plot-everything.py <experiment_home_dir>\n```\n\n----------------------------------------\n\nTITLE: Running AIBrix Tests\nDESCRIPTION: Command to run tests for the AIBrix build, verifying functionality.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset for Benchmarking\nDESCRIPTION: Downloads the ShareGPT_Vicuna_unfiltered dataset which will be used for benchmarking the LoRA deployments.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n```\n\n----------------------------------------\n\nTITLE: Testing AIBrix Gateway with curl Request\nDESCRIPTION: curl command to test the AIBrix gateway service by sending a completion request, noting the different model name format required for AIBrix compared to the other services.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/ps-experiment/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# aibrix gateway is the entrypoint for aibrix related experiments,\n# note the model name is different.\ncurl -v http://envoy-aibrix-system-aibrix-eg-903790dc.envoy-gateway-system:80/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"llama3-1-8b\",\n\"prompt\": \"San Francisco is a\",\n\"max_tokens\": 128,\n\"temperature\": 0\n}'\n```\n\n----------------------------------------\n\nTITLE: Building Custom vLLM Docker Image\nDESCRIPTION: Dockerfile for creating a custom vLLM image with upgraded NCCL and Ray dependencies to address stability issues.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/samples/deepseek-r1/README.md#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM vllm/vllm-openai:v0.7.3\nRUN pip3 install -U ray[default,adag]==2.40.0\nRUN pip3 install -U nvidia-nccl-cu12\nENTRYPOINT [\"\"]\n```\n\n----------------------------------------\n\nTITLE: Generating Per-Pod Analysis Command\nDESCRIPTION: Python command to generate per-pod analysis graphs from experiment logs.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/autoscaling/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython <aibrix_root_repo>/benchmarks/plot/plot_per_pod.py experiment_results/25min_test/25min_up_and_down-apa-least-request-20250209-064742/pod_logs\n```\n\n----------------------------------------\n\nTITLE: Running Locust Load Testing\nDESCRIPTION: Basic command to run the Locust load testing tool with a benchmark script against a specified host. Locust will be used to generate load for testing gateway performance.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nlocust -f benchmark.py --host http://localhost:8887\n```\n\n----------------------------------------\n\nTITLE: Interacting with Base Model Using cURL\nDESCRIPTION: Bash commands to expose the base model endpoint and send inference requests to it. This establishes the baseline model behavior before applying LoRA adaptations.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/lora-dynamic-loading.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Expose endpoint\nLB_IP=$(kubectl get svc/envoy-aibrix-system-aibrix-eg-903790dc -n envoy-gateway-system -o=jsonpath='{.status.loadBalancer.ingress[0].ip}')\nENDPOINT=\"${LB_IP}:80\"\n\n# send request to base model\ncurl -v http://${ENDPOINT}/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"qwen-coder-1-5b-instruct\",\n        \"prompt\": \"San Francisco is a\",\n        \"max_tokens\": 128,\n        \"temperature\": 0\n    }'\n```\n\n----------------------------------------\n\nTITLE: Downloading Model from AWS S3 using AI Runtime in Bash\nDESCRIPTION: Uses AI Runtime's downloader module to fetch a model from an AWS S3 bucket and store it locally.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m aibrix.downloader \\\n    --model-uri s3://aibrix-model-artifacts/deepseek-coder-6.7b-instruct/ \\\n    --local-dir /tmp/aibrix/models_s3/\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding for GPU Optimizer and Updating Profiles\nDESCRIPTION: Commands to access the GPU Optimizer service and notify it that profiles are ready for optimization.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl -n aibrix-system port-forward svc/aibrix-gpu-optimizer 8080:8080 1>/dev/null 2>&1 &\n\ncurl http://localhost:8080/update_profile/llama2-7b\n```\n\n----------------------------------------\n\nTITLE: Installing Sphinx Dependencies for AIBrix Documentation\nDESCRIPTION: Installs the required Sphinx packages and templates using pip from a requirements file. This is a prerequisite step before building the HTML documentation.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements-docs.txt\n```\n\n----------------------------------------\n\nTITLE: Making Gateway Inference Request\nDESCRIPTION: Sends a completion request through the Aibrix gateway\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/vllm/README.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ncurl -v POST \"http://localhost:8888/v1/completions\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer test-key-1234567890\" \\\n  --data '{\n    \"model\": \"facebook-opt-125m\",\n    \"prompt\": \"Once upon a time,\",\n    \"max_tokens\": 512,\n    \"temperature\": 0.5\n  }'\n```\n\n----------------------------------------\n\nTITLE: Syncing Container Images to Volcano Engine Registry for AIBrix\nDESCRIPTION: Commands for syncing container images from DockerHub to Volcano Engine Container Registry. This process is necessary to make the images available in VKE environments.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/development/release.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./hack/release/sync-images.sh v0.2.1 aibrix-container-registry-cn-beijing.cr.volces.com\n./hack/release/sync-images.sh v0.2.1 aibrix-container-registry-cn-shanghai.cr.volces.com\n```\n\n----------------------------------------\n\nTITLE: Setting Up Gateway Rate Limits with AIBrix\nDESCRIPTION: Commands to port-forward the AIBrix metadata service and create a user with specific rate limits. This demonstrates how to configure requests per minute (rpm) and tokens per minute (tpm).\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n# note: not mandatory to create user to access gateway API\n\nkubectl -n aibrix-system port-forward svc/aibrix-metadata-service 8090:8090 &\n\ncurl http://localhost:8090/CreateUser \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"your-user-name\",\"rpm\": 100,\"tpm\": 1000}'\n```\n\n----------------------------------------\n\nTITLE: Starting Independent Visualization for Monitoring\nDESCRIPTION: Command to run an independent visualization tool for monitoring workload patterns.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython -m loadmonitor.visualizer\n```\n\n----------------------------------------\n\nTITLE: Building HTML Documentation with Sphinx for AIBrix\nDESCRIPTION: Runs the make command to generate HTML pages from the Sphinx documentation source. After execution, the documentation will be available at 'docs/build/html/index.html'.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Deploying Merged LoRA Model with vLLM\nDESCRIPTION: Deploys a vLLM server with the merged LoRA weights model (Deployment 1). Uses CUDA on device 0, serves on port 8071, and applies the Llama-2 chat template.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 vllm serve /models/merge-model-weights --served-model-name model-1 --host \"0.0.0.0\" --port \"8071\" --chat-template llama-2-chat.jinja\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for vLLM Mock\nDESCRIPTION: Command to build a Docker image for the mocked vLLM application. This creates a base image tagged as 'aibrix/vllm-mock:nightly'.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\ndocker build -t aibrix/vllm-mock:nightly -f Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Setting up Kind Cluster\nDESCRIPTION: Creates a Kind cluster using configuration and builds/loads runtime components\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/vllm/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkind create cluster --config=./development/vllm/kind-config.yaml\nmake docker-build-all\nkind load docker-image aibrix/runtime:nightly\n```\n\n----------------------------------------\n\nTITLE: Building and Updating Gateway Plugins in Kubernetes\nDESCRIPTION: Commands for building Docker images for gateway plugins and updating the deployment in Kubernetes. This section helps with local testing and iteration on the gateway implementation.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-build-plugins\naibrix/plugins:9bd45a9915b71936ff0001a6fbfc32f10b65e480\n\nk edit deployment aibrix-gateway-plugins\n\nk delete pod aibrix-gateway-plugins-759b87dc65-j9qs8 # commit is exact same, we just need to update once\n```\n\n----------------------------------------\n\nTITLE: Deploying Benchmark Client in Kubernetes\nDESCRIPTION: Command to apply the client.yaml configuration file to deploy a benchmark client pod in the Kubernetes cluster, used for running performance tests within the same inference cluster.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/ps-experiment/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f client.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Running Mélange Solver in Python\nDESCRIPTION: This snippet shows how to install required dependencies and execute the Mélange solver using a configuration file. It also mentions the default output location for the solver results.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/optimizer/solver/melange/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Tested on Python 3.9.18\n\n# 1. Install the necessary dependencies\npip install -r requirements.txt\n\n# See the melange/profiling/profiling-instructions.md for instructions on how to obtain the GPU information needed as the solver's input.\n\n# 2. Execute the solver with your own input configuration\npython -m melange.main -c melange/config/example.json\n\n# 3. By default, the solver will save the output in a JSON file named as \"melange_result.json\" at the root directory\n```\n\n----------------------------------------\n\nTITLE: Making API Request to AIBrix Endpoint in Bash\nDESCRIPTION: A Bash script example showing how to make a chat completion request to the AIBrix API endpoint after deployment. The script sends a JSON payload to generate Python code for a random generator.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/deployment/terraform/gcp/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nENDPOINT=\"<YOUR PUBLIC IP>\"\n\ncurl http://${ENDPOINT}/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"deepseek-r1-distill-llama-8b\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"help me write a random generator in python\"}\n        ]\n    }'\n```\n\n----------------------------------------\n\nTITLE: Deploying and Accessing AIBrix Metadata Service\nDESCRIPTION: These commands deploy the AIBrix metadata service using kubectl and set up port forwarding to access it locally.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/metadata/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f docs/development/app/metadata.yaml\nkubectl -n aibrix-system port-forward svc/aibrix-metadata-service 8090:8090 &\n```\n\n----------------------------------------\n\nTITLE: Making Inference Request\nDESCRIPTION: Sends a test completion request to the deployed model endpoint\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/vllm/README.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncurl -v http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer test-key-1234567890\" \\\n  -d '{\n     \"model\": \"facebook-opt-125m\",\n     \"prompt\": \"Say this is a test\",\n     \"temperature\": 0.5,\n     \"max_tokens\": 512\n   }'\n```\n\n----------------------------------------\n\nTITLE: RDMA Network Configuration YAML\nDESCRIPTION: Kubernetes YAML configuration for enabling RDMA networking and IPC lock capabilities.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/samples/deepseek-r1/README.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n  k8s.volcengine.com/pod-networks: |\n    [\n      {\n        \"cniConf\":{\n            \"name\":\"rdma\"\n        }\n      },\n      .....\n      {\n        \"cniConf\":{\n            \"name\":\"rdma\"\n        }\n      }\n    ]\n```\n\nLANGUAGE: yaml\nCODE:\n```\n  securityContext:\n    capabilities:\n      add:\n      - IPC_LOCK\n```\n\n----------------------------------------\n\nTITLE: Exposing Service Endpoints with Port Forwarding\nDESCRIPTION: Commands for port forwarding to expose the service endpoints. Two options are provided: forwarding the envoy service or directly forwarding the model service.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n# Option 1: Port forward the envoy service\nkubectl -n envoy-gateway-system port-forward service/envoy-aibrix-system-aibrix-eg-903790dc 8000:80 &\n\n# Option 2: Port forward the model service\nkubectl -n default port-forward svc/llama2-7b 8000:8000 &\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Local Debugging\nDESCRIPTION: This command sets up port forwarding for local debugging of the Llama service.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nkubectl port-forward svc/llama2-70b 8000:8000\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark Workload for Model Scaling Test\nDESCRIPTION: Python command to generate workload for testing how the model scales using the benchmark toolkit.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# Make sure gateway's local access, see docs/development/simulator/README.md for details.\npython optimizer/profiling/gpu_benchmark.py --backend=vllm --port 8888 --request-rate=10 --num-prompts=100 --input_len 2000 --output_len 128 --model=llama2-7b\n```\n\n----------------------------------------\n\nTITLE: Updating User Data in AIBrix Metadata Service\nDESCRIPTION: This curl command sends a POST request to update an existing user's RPM and TPM values.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/metadata/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:8090/UpdateUser \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"your-user-name\",\"rpm\": 1000,\"tpm\": 10000}'\n```\n\n----------------------------------------\n\nTITLE: Setting up Port Forwarding\nDESCRIPTION: Configures port forwarding for the model service\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/vllm/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nkubectl port-forward svc/facebook-opt-125m 8000:8000 &\n```\n\n----------------------------------------\n\nTITLE: Configuring ModelAdapter for Text2SQL LoRA in Kubernetes\nDESCRIPTION: This YAML defines a ModelAdapter custom resource for a Text2SQL LoRA model. It specifies the base model, additional configuration, and pod selector for the adapter.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/controller/modeladapter/README.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: model.aibrix.ai/v1alpha1\nkind: ModelAdapter\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"model.aibrix.ai/v1alpha1\",\"kind\":\"ModelAdapter\",\"metadata\":{\"annotations\":{},\"name\":\"text2sql-lora-1\",\"namespace\":\"default\"},\"spec\":{\"additionalConfig\":{\"model-artifact\":\"jeffwan/rank-1\"},\"baseModel\":\"llama2-70b\",\"podSelector\":{\"matchLabels\":{\"model.aibrix.ai\":\"llama2-70b\"}},\"schedulerName\":\"default-model-adapter-scheduler\"}}\n  creationTimestamp: \"2024-07-14T21:09:18Z\"\n  generation: 2\n  name: text2sql-lora-1\n  namespace: default\n  resourceVersion: \"788513\"\n  uid: 61fd3d3c-8549-4742-8f43-7df8c66f0a6d\nspec:\n  additionalConfig:\n    model-artifact: jeffwan/rank-1\n  baseModel: llama2-70b\n  podSelector:\n    matchLabels:\n      model.aibrix.ai/name: llama2-70b\n  schedulerName: default-model-adapter-scheduler\nstatus:\n  phase: Configuring\n```\n\n----------------------------------------\n\nTITLE: JSONL to CSV Conversion Utility - Python\nDESCRIPTION: Utility function to convert benchmark results from JSONL files to CSV format. Processes latency data from multiple approaches and concurrency levels, applying scaling factors based on machine count.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/plot/aibrix0.1-lora.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nfrom collections import defaultdict\nimport pandas as pd\nfrom glob import glob\nimport re\n\ndef convert_files_csv(directory, multiplier):\n    data = defaultdict(lambda: defaultdict(list))\n    for file_path in glob(os.path.join(directory, '*.jsonl')):\n        file_name = os.path.basename(file_path)\n        parts = file_name.split('_')\n        approach = '_'.join(parts[1:-2])\n        concurrency = int(parts[-1].replace('.jsonl', ''))\n        \n        match = re.match(r'.*_(\\d+)$', approach)\n        num_machines = 1 if match else multiplier\n    \n        avg_latency = calculate_average_latency(file_path)\n        avg_latency *= num_machines\n        data[concurrency][approach] = avg_latency\n\n    df = pd.DataFrame(index=sorted(data.keys()), columns=sorted({approach for d in data.values() for approach in d}))\n    return df.to_csv()\n```\n\n----------------------------------------\n\nTITLE: Deploying Mocked Model with Kubectl\nDESCRIPTION: Commands to deploy and delete the mocked model using Kubernetes. The deployment configuration is located in the config/mock directory.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -k config/mock\n\n# you can run following command to delete the deployment \nkubectl delete -k config/mock\n```\n\n----------------------------------------\n\nTITLE: Loading Docker Images for Mac\nDESCRIPTION: Loads the vLLM CPU environment Docker image for MacOS systems\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/vllm/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull aibrix/vllm-cpu-env:macos\nkind load docker-image aibrix/vllm-cpu-env:macos\n```\n\n----------------------------------------\n\nTITLE: Converting Client Traces to Dataset with Python\nDESCRIPTION: This snippet demonstrates how to use the converter.py script to convert client traces (output.jsonl) containing both prompts and completions into a prompt file suitable for the workload generator.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/dataset-generator/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport TRACE=\"output.jsonl\"\npython converter.py \\\n    --path ${TRACE} \\\n    --type trace \\\n    --tokenizer deepseek-ai/deepseek-llm-7b-chat\n```\n\n----------------------------------------\n\nTITLE: Deploying Mocked Llama Service\nDESCRIPTION: These commands deploy a mocked Llama service using a YAML configuration and verify its deployment status.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f docs/development/app/deployment.yaml\nkubectl get deployments --all-namespaces |grep llama2\n```\n\n----------------------------------------\n\nTITLE: Setting up Envoy Port Forwarding\nDESCRIPTION: Configures port forwarding for the Envoy gateway service\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/vllm/README.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nkubectl -n envoy-gateway-system port-forward service/envoy-aibrix-system-aibrix-eg-903790dc 8888:80 &\n```\n\n----------------------------------------\n\nTITLE: Running Python Client for HTTP Route Testing\nDESCRIPTION: Python client command for testing the HTTP route routing strategy using the ShareGPT dataset. It targets the gateway endpoint instead of direct service access.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython client.py \\\n--dataset-path \"/tmp/ShareGPT_V3_unfiltered_cleaned_split.json\" \\\n--endpoint \"http://101.126.81.102:80\" \\\n--num-prompts 2000 \\\n--interval 0.05 \\\n--output-file-path \"httproute-v2.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Performance Benchmarks with Different Configurations\nDESCRIPTION: Bash commands to execute the benchmark script (run.sh) with various configurations, testing different systems (Kubernetes, PS Router, AIBrix) and model routing strategies at different QPS patterns.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/ps-experiment/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbash run.sh meta-llama/Llama-3.1-8B-Instruct http://vllm-engine-service:80/v1/ naive\nbash run.sh meta-llama/Llama-3.1-8B-Instruct http://vllm-router-service:80/v1/ ps_naive\nbash run.sh meta-llama/Llama-3.1-8B-Instruct http://vllm-router-service:80/v1/ ps_stack_high_low\nbash run.sh meta-llama/Llama-3.1-8B-Instruct http://vllm-router-service:80/v1/ ps_stack_low_high\nbash run.sh llama3-1-8b http://envoy-aibrix-system-aibrix-eg-903790dc.envoy-gateway-system:80/v1/ aibrix_naive_7\nbash run.sh llama3-1-8b http://envoy-aibrix-system-aibrix-eg-903790dc.envoy-gateway-system:80/v1/ aibrix_high_low\n```\n\n----------------------------------------\n\nTITLE: Checking Deployed AIBrix Manager Logs\nDESCRIPTION: Commands to view logs from the deployed AIBrix controller manager pod in the Kubernetes cluster, either once or continuously.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods -n aibrix-system -o name | grep aibrix-controller-manager | head -n 1 | xargs -I {} kubectl logs {} -n aibrix-system\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods -n aibrix-system -o name | grep aibrix-controller-manager | head -n 1 | xargs -I {} kubectl logs -f {} -n aibrix-system\n```\n\nLANGUAGE: log\nCODE:\n```\n2024-08-05T10:20:03Z    INFO    Starting EventSource    {\"controller\": \"podautoscaler\", \"controllerGroup\": \"autoscaling.aibrix.ai\", \"controllerKind\": \"PodAutoscaler\", \"source\": \"kind source: *v1alpha1.PodAutoscaler\"}\n2024-08-05T10:20:03Z    INFO    Starting EventSource    {\"controller\": \"podautoscaler\", \"controllerGroup\": \"autoscaling.aibrix.ai\", \"controllerKind\": \"PodAutoscaler\", \"source\": \"kind source: *v2.HorizontalPodAutoscaler\"}\n2024-08-05T10:20:03Z    INFO    Starting Controller     {\"controller\": \"podautoscaler\", \"controllerGroup\": \"autoscaling.aibrix.ai\", \"controllerKind\": \"PodAutoscaler\"}\n2024-08-05T10:20:03Z    INFO    Starting EventSource    {\"controller\": \"modeladapter\", \"controllerGroup\": \"model.aibrix.ai\", \"controllerKind\": \"ModelAdapter\", \"source\": \"kind source: *v1alpha1.ModelAdapter\"}\n2024-08-05T10:20:03Z    INFO    Starting EventSource    {\"controller\": \"modeladapter\", \"controllerGroup\": \"model.aibrix.ai\", \"controllerKind\": \"ModelAdapter\", \"source\": \"kind source: *v1.Service\"}\n2024-08-05T10:20:03Z    INFO    Starting EventSource    {\"controller\": \"modeladapter\", \"controllerGroup\": \"model.aibrix.ai\", \"controllerKind\": \"ModelAdapter\", \"source\": \"kind source: *v1.EndpointSlice\"}\n2024-08-05T10:20:03Z    INFO    Starting EventSource    {\"controller\": \"modeladapter\", \"controllerGroup\": \"model.aibrix.ai\", \"controllerKind\": \"ModelAdapter\", \"source\": \"kind source: *v1.Pod\"}\n2024-08-05T10:20:03Z    INFO    Starting Controller     {\"controller\": \"modeladapter\", \"controllerGroup\": \"model.aibrix.ai\", \"controllerKind\": \"ModelAdapter\"}\n2024-08-05T10:20:03Z    INFO    Starting workers        {\"controller\": \"modeladapter\", \"controllerGroup\": \"model.aibrix.ai\", \"controllerKind\": \"ModelAdapter\", \"worker count\": 1}\n2024-08-05T10:20:03Z    INFO    Starting workers        {\"controller\": \"podautoscaler\", \"controllerGroup\": \"autoscaling.aibrix.ai\", \"controllerKind\": \"PodAutoscaler\", \"worker count\": 1}\n```\n\n----------------------------------------\n\nTITLE: Defining JSONL Prompt Collection Format\nDESCRIPTION: Specifies the expected JSONL format for plain prompts collection, with an optional 'completion' field.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"prompt\": XXX, \"completion\": AAA}\n{\"prompt\": YYY, \"completion\": AAA}\n{\"prompt\": ZZZ, \"completion\": AAA}\n```\n\n----------------------------------------\n\nTITLE: Generating Workload from JSON Config in Python\nDESCRIPTION: Python command to generate a workload file using JSON configuration files for traffic, prompt length, and completion length patterns.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython workload_generator.py --prompt-file $PROMPT_FILE --interval-ms 1000 --duration-ms 1400000 --trace-type synthetic --traffic-pattern-config config/traffic-config.json --prompt-len-pattern-config config/prompt-len-config.json --completion-len-pattern-config config/completion-len-config.json --model \"Qwen/Qwen2.5-Coder-7B-Instruct\" --output-dir \"./output\" --output-format jsonl\n```\n\n----------------------------------------\n\nTITLE: Running Python Client for Random Routing Strategy Testing\nDESCRIPTION: Python client command for testing the random routing strategy via the gateway endpoint. The command structure is identical to other strategies but outputs to a different file.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython client.py \\\n--dataset-path \"/tmp/ShareGPT_V3_unfiltered_cleaned_split.json\" \\\n--endpoint \"http://101.126.81.102:80\" \\\n--num-prompts 2000 \\\n--interval 0.05 \\\n--output-file-path \"random-v2.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix Core Components\nDESCRIPTION: This command installs the core components of AIBrix from a specified GitHub repository using kubectl.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/lambda.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -k \"github.com/vllm-project/aibrix/config/overlays/release?ref=v0.2.1\"\n```\n\n----------------------------------------\n\nTITLE: Viewing KPA Algorithm Logs\nDESCRIPTION: This command retrieves and filters logs from the AIBrix controller manager, focusing on KPA algorithm execution for the Llama deployment.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods -n aibrix-system -o name | grep aibrix-controller-manager | head -n 1 | xargs -I {} kubectl logs {} -n aibrix-system |grep 'KPA'\n```\n\n----------------------------------------\n\nTITLE: Defining Grafana MaaS Prompt Length File Format\nDESCRIPTION: Specifies the expected CSV format for Grafana MaaS prompt length statistics file.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_7\n\nLANGUAGE: csv\nCODE:\n```\n\"Time\",\"P50\",\"P70\",\"P90\",\"P99\"\n```\n\n----------------------------------------\n\nTITLE: Model Deployment and Autoscaling Commands\nDESCRIPTION: kubectl commands for deploying the model and configuring autoscaling strategy.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/samples/deepseek-r1/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f deepseek-r1-ai-runtime.yaml\nkubectl apply -f deepseek-r1-autoscaling.yaml\n```\n\n----------------------------------------\n\nTITLE: Generating Workload from Grafana Statistics in Python\nDESCRIPTION: Python command to generate a workload file based on Grafana exported CSV statistics files for traffic, prompt length, and completion length.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nexport TRAFFIC_FILE=${PATH_TO_TRAFFIC_FILE}\nexport PROMPT_LEN_FILE=${PATH_TO_PROMPT_LEN_FILE}\nexport COMPLETION_LEN_FILE=${PATH_TO_COMPLETION_LEN_FILE}\n\npython workload_generator.py --prompt-file $PROMPT_FILE --interval-ms 1000 --duration-ms 1800000 --trace-type internal --traffic-file \"$TRAFFIC_FILE\" --prompt-len-file \"$PROMPT_LEN_FILE\" --completion-len-file \"$COMPLETION_LEN_FILE\"  --model \"Qwen/Qwen2.5-Coder-7B-Instruct\" --output-dir \"./output\" --output-format jsonl --qps-scale 1.0 --output-scale 1.0 --input-scale 1.0 --internal-trace-type \"maas\"\n```\n\n----------------------------------------\n\nTITLE: Exposing Kubernetes Service Ports for Debugging\nDESCRIPTION: Command to use port-forwarding to access the Llama2 service running in the Kubernetes cluster for debugging purposes.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl port-forward svc/llama2-70b 8000:8000 -n aibrix-system\n```\n\n----------------------------------------\n\nTITLE: Verifying AIBrix Component Installation\nDESCRIPTION: This command lists all pods in the aibrix-system namespace to verify successful installation of AIBrix components.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/lambda.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n aibrix-system\n```\n\n----------------------------------------\n\nTITLE: Creating APA Autoscaler for Mocked Llama\nDESCRIPTION: This command creates an APA-type autoscaler for the mocked Llama deployment and verifies its creation.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f config/samples/autoscaling_v1alpha1_mock_llama_apa.yaml\nkubectl get podautoscalers --all-namespaces\n```\n\n----------------------------------------\n\nTITLE: Defining Workload Generator Output Format\nDESCRIPTION: Specifies the JSON format of the generated workload file, including prompts, prompt lengths, and output lengths.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n[\n    [[\"Prompt1\", prompt_len_1, output_len_1, null],[\"Prompt2\", prompt_len_2, output_len_2, null], ...],\n    [[\"Prompt3\", prompt_len_3, output_len_3, null],[\"Prompt4\", prompt_len_4, output_len_4, null], ...],\n    ...\n]\n```\n\n----------------------------------------\n\nTITLE: Generating Workload from Azure LLM Trace in Shell and Python\nDESCRIPTION: Shell and Python commands to download Azure LLM Trace and generate a workload file based on it.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nwget https://raw.githubusercontent.com/Azure/AzurePublicDataset/refs/heads/master/data/AzureLLMInferenceTrace_conv.csv -O /tmp/AzureLLMInferenceTrace_conv.csv\nexport AZURE_TRACE_NAME=/tmp/AzureLLMInferenceTrace_conv.csv\npython workload_generator.py --prompt-file $SHAREGPT_FILE_PATH --num-prompts 100 --interval-ms 1000 --duration-ms 600000 --trace-type azure --trace-file \"$AZURE_TRACE_NAME\" --group-interval-seconds 1 --model \"Qwen/Qwen2.5-Coder-7B-Instruct\" --output-dir \"output\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding for vLLM Pod Access\nDESCRIPTION: Command to expose a Kubernetes pod interface locally using port forwarding, necessary for benchmark preparation.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Make sure pod is accessable locally:\nkubectl port-forward [pod_name] 8010:8000 1>/dev/null 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Getting Envoy Gateway Service Information\nDESCRIPTION: Command output displaying the Envoy Gateway service details needed for setting up port forwarding to access the inference service.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/distributed-kv-cache.rst#2025-04-22_snippet_5\n\nLANGUAGE: RST\nCODE:\n```\nNAME                                     TYPE           CLUSTER-IP       EXTERNAL-IP                                       PORT(S)                                   AGE\nenvoy-aibrix-system-aibrix-eg-903790dc   LoadBalancer   172.19.190.6     10.0.1.4,2406:d440:105:cf01:6f1b:7f4d:12da:c5a5   80:30904/TCP                              3d\n```\n\n----------------------------------------\n\nTITLE: Deleting Existing Autoscalers\nDESCRIPTION: These commands delete any existing autoscalers for the mocked Llama deployment before creating a new one.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete podautoscalers.autoscaling.aibrix.ai podautoscaler-example-mock-llama -n aibrix-system\nkubectl delete podautoscalers.autoscaling.aibrix.ai podautoscaler-example-mock-llama-apa -n aibrix-system\n```\n\n----------------------------------------\n\nTITLE: Plotting Resource Usage Bar and Line Charts - Python\nDESCRIPTION: Creates bar and line charts comparing GPU resource usage across different concurrency levels for multiple LoRA approaches using matplotlib and pandas. Processes CSV data containing GPU seconds metrics.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/plot/aibrix0.1-lora.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom io import StringIO\n\ntitle = \"Overall Resource Time Used (GPU Seconds)\"\ncsv_data = \"\"\"Concurrency,AIBrix,Merged Model,Single LoRA\n1,1.0,4.0,4.0\n4,2.0,4.0,4.0\n16,6.0,7.5,8.0\n64,27.0,24.0,28.0\n256,106.0,100.0,108.0\n\"\"\"\n\ncsv_file = StringIO(csv_data)\ndf = pd.read_csv(csv_file)\ndf['Concurrency'] = df['Concurrency'].astype(str)\ndf.set_index('Concurrency', inplace=True)\n\n# Bar plot\nplt.figure(figsize=(6, 3))\nfor i, approach in enumerate(approaches):\n    plt.bar(bar_positions + i * bar_width, df[approach], width=bar_width, label=approach)\n\n# Line plot\nplt.figure(figsize=(6, 3))\nfor approach in df.columns:\n    plt.plot(df.index, df[approach], marker='o', linestyle='-', label=approach)\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Model Information Response in JSON\nDESCRIPTION: Example JSON response from the model information API, showing details of the base model and loaded LoRA adapters.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"object\": \"list\",\n    \"data\": [\n        {\n            \"id\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n            \"object\": \"model\",\n            \"created\": 1738218097,\n            \"owned_by\": \"vllm\",\n            \"root\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n            \"parent\": null,\n            \"max_model_len\": 32768,\n            \"permission\": [\n                {\n                \"id\": \"modelperm-c2e9860095b745b6b8be7133c5ab1fcf\",\n                \"object\": \"model_permission\",\n                \"created\": 1738218097,\n                \"allow_create_engine\": false,\n                \"allow_sampling\": true,\n                \"allow_logprobs\": true,\n                \"allow_search_indices\": false,\n                \"allow_view\": true,\n                \"allow_fine_tuning\": false,\n                \"organization\": \"*\",\n                \"group\": null,\n                \"is_blocking\": false\n                }\n            ]\n        },\n        {\n            \"id\": \"lora-1\",\n            \"object\": \"model\",\n            \"created\": 1738218097,\n            \"owned_by\": \"vllm\",\n            \"root\": \"bharati2324/Qwen2.5-1.5B-Instruct-Code-LoRA-r16v2\",\n            \"parent\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n            \"max_model_len\": null,\n            \"permission\": [\n                {\n                \"id\": \"modelperm-c21d06b59af0435292c70cd612e68b01\",\n                \"object\": \"model_permission\",\n                \"created\": 1738218097,\n                \"allow_create_engine\": false,\n                \"allow_sampling\": true,\n                \"allow_logprobs\": true,\n                \"allow_search_indices\": false,\n                \"allow_view\": true,\n                \"allow_fine_tuning\": false,\n                \"organization\": \"*\",\n                \"group\": null,\n                \"is_blocking\": false\n                }\n            ]\n        },\n        {\n            \"id\": \"lora-2\",\n            \"object\": \"model\",\n            \"created\": 1738218097,\n            \"owned_by\": \"vllm\",\n            \"root\": \"bharati2324/Qwen2.5-1.5B-Instruct-Code-LoRA-r16v2\",\n            \"parent\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n            \"max_model_len\": null,\n            \"permission\": [\n                {\n                \"id\": \"modelperm-bf2af850171242f7a9f4ccd9ecd313cd\",\n                \"object\": \"model_permission\",\n                \"created\": 1738218097,\n                \"allow_create_engine\": false,\n                \"allow_sampling\": true,\n                \"allow_logprobs\": true,\n                \"allow_search_indices\": false,\n                \"allow_view\": true,\n                \"allow_fine_tuning\": false,\n                \"organization\": \"*\",\n                \"group\": null,\n                \"is_blocking\": false\n                }\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining JSONL Sessioned Prompts Collection Format\nDESCRIPTION: Specifies the expected JSONL format for sessioned prompts collection, with an optional 'completions' field.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"session_id\": 0, \"prompts\": [\"prompt 1\", \"prompt 2\"], \"completions\": [\"completion 1\", \"completion 2\"]}\n{\"session_id\": 1, \"prompts\": [\"prompt 3\", \"prompt 4\"], \"completions\": [\"completion 3\", \"completion 4\"]}\n```\n\n----------------------------------------\n\nTITLE: Deleting a User from AIBrix Metadata Service\nDESCRIPTION: This curl command sends a POST request to delete a user with the specified name from the metadata service.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/pkg/metadata/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:8090/DeleteUser \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"your-user-name\"}'\n```\n\n----------------------------------------\n\nTITLE: Setting up development environment and running code quality checks\nDESCRIPTION: Commands to install development dependencies using Poetry and run linting, formatting, and type checking scripts for code quality assurance.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# install dependencies\npoetry install --no-root --with dev\n\n# linting, formatting and type checking\nbash ./scripts/format.sh\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Mock Base Model\nDESCRIPTION: Dockerfile command to build a mock vLLM base model image tagged as nightly\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/lora/README.md#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\ndocker build -t aibrix/vllm-mock:nightly -f Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Performance Reduction Analysis - Python\nDESCRIPTION: Calculates and visualizes percentage reduction in GPU seconds for AIBrix compared to other approaches. Creates a bar chart showing performance improvements across different concurrency levels.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/plot/aibrix0.1-lora.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(StringIO(csv_data))\ndf['Merged Model Reduction (%)'] = ((df['Merged Model'] - df['AIBrix']) / df['Merged Model']) * 100\ndf['Single LoRA Reduction (%)'] = ((df['Single LoRA'] - df['AIBrix']) / df['Single LoRA']) * 100\n\nplt.figure(figsize=(6, 3))\nplt.bar(x, merged_model_reduction, width=bar_width, label='Merged Model Reduction (%)', align='center')\nplt.bar([i + bar_width for i in x], single_lora_reduction, width=bar_width, label='Single LoRA Reduction (%)', align='edge')\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining Grafana CloudIDE Prompt/Completion Length File Format\nDESCRIPTION: Specifies the expected CSV format for Grafana CloudIDE prompt and completion length statistics files.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_10\n\nLANGUAGE: csv\nCODE:\n```\n\"Time\",\"recv_bytes\",\"sent_bytes\"\n```\n\n----------------------------------------\n\nTITLE: Creating KPA Autoscaler for Mocked Llama\nDESCRIPTION: This command creates a KPA-type autoscaler for the mocked Llama deployment using a YAML configuration.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f config/samples/autoscaling_v1alpha1_mock_llama.yaml\n```\n\n----------------------------------------\n\nTITLE: Testing Gateway Without Routing Strategy\nDESCRIPTION: cURL command to test the API gateway endpoint without specific routing headers\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/lora/README.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncurl -v http://localhost:8888/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer any_key\" \\\n  -d '{\n     \"model\": \"text2sql-lora-1\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n----------------------------------------\n\nTITLE: Extracting and Visualizing Latency Metrics Across Different LoRA Approaches in Python\nDESCRIPTION: This script reads benchmark results from JSONL files for three LoRA approaches (unmerged_multi_lora, merged, unmerged_single_lora), calculates mean GPU seconds by concurrency level, and generates a grouped bar plot for comparison. It extracts latency data from the files and applies multipliers based on the number of applications.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/plot/aibrix0.1-lora.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom glob import glob\nimport re\nfrom collections import defaultdict\n\n# Function to calculate average latencies grouped by concurrency\ndef extract_latencies_by_concurrency(file_path):\n    latencies_by_concurrency = defaultdict(list)\n    with open(file_path, 'r') as f:\n        for line in f:\n            result = json.loads(line)\n            concurrency = result['concurrency']\n            latency = result['latency']\n            latencies_by_concurrency[concurrency].append(latency)\n    avg_latencies_by_concurrency = {k: sum(v) / len(v) for k, v in latencies_by_concurrency.items()}  \n    return avg_latencies_by_concurrency\n\n# Directory containing the .jsonl files\ndirectory = 'benchmark_result_pin_lora_8'  # change this to the actual folder path\n\n# Dictionary to store data\ndata = defaultdict(lambda: defaultdict(list))\n\n# Number of applications for each approach\nnum_apps = {\n    \"unmerged_multi_lora\": 8,  # Assuming the number of applications\n    \"merged\": 1,\n    \"unmerged_single_lora\": 1\n}\nnum_apps = 8\n# Read all .jsonl files\nfor file_path in glob(os.path.join(directory, '*.jsonl')):\n    # Extract the approach name and number of applications from the file name\n    file_name = os.path.basename(file_path)\n    parts = file_name.split('_')\n    print(parts)\n    # benchmark_merged_concurrency.jsonl      benchmark_unmerged_multi_lora_8.jsonl   benchmark_unmerged_single_lora.jsonl\n    if \"unmerged_multi_lora\" in file_name:\n        approach = \"unmerged_multi_lora\"\n        avg_latencies = extract_latencies_by_concurrency(file_path)\n        for concurrency, avg_latency in avg_latencies.items():\n            data[concurrency][approach] = avg_latency\n    elif \"merged_\" in file_name or \"unmerged_single_lora\" in file_name:\n        if 'unmerged_single_lora' in file_name:\n            approach = 'unmerged_single_lora'\n        if 'benchmark_merged' in file_name:\n            approach = 'merged'\n        # approach = '_'.join(parts[1:3])\n        avg_latencies = extract_latencies_by_concurrency(file_path)\n        for concurrency, avg_latency in avg_latencies.items():\n            data[concurrency][approach] = avg_latency * num_apps\n\n# Create a DataFrame to store the mean GPU seconds\nresults = []\nfor concurrency, approaches in data.items():\n    row = {'Concurrency': concurrency}\n    for approach, latency in approaches.items():\n        row[approach] = latency\n    results.append(row)\ndf = pd.DataFrame(results)\n\n# Plot configuration\napproaches = [\"unmerged_multi_lora\", \"merged\", \"unmerged_single_lora\"]\ndf = df.sort_values('Concurrency')\ndf.set_index('Concurrency', inplace=True)\ndf = df[approaches]\n\n# Plotting the grouped bar plot\ndf.plot(kind='bar', figsize=(12, 6))\nplt.xlabel('Concurrency')\nplt.ylabel('Mean GPU Seconds')\nplt.title('Mean GPU Seconds of Requests by Concurrency Level')\nplt.legend(title='Approach')\nplt.grid(True)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Describing PodAutoscaler Events\nDESCRIPTION: This command describes the events associated with a specific PodAutoscaler, showing scaling activities and decisions.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nkubectl describe podautoscalers podautoscaler-example-kpa\n```\n\n----------------------------------------\n\nTITLE: Running Python Client for K8s Direct Service Testing\nDESCRIPTION: Python client command for testing direct K8s service performance using the ShareGPT dataset. It specifies the endpoint, prompt count, interval between requests, and output file.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython client.py \\\n--dataset-path \"/tmp/ShareGPT_V3_unfiltered_cleaned_split.json\" \\\n--endpoint \"http://101.126.24.162:8000\" \\\n--num-prompts 2000 \\\n--interval 0.05 \\\n--output-file-path \"k8s-v2.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Test Dataset from Hugging Face\nDESCRIPTION: Command to download a test dataset from Hugging Face, specifically the ShareGPT Vicuna unfiltered dataset, which will be used for benchmarking the gateway routing strategies.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split.json\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up AIBrix Resources and Related Deployments in Kubernetes\nDESCRIPTION: A comprehensive cleanup script that removes AIBrix resources including podautoscalers, uninstalls the AIBrix system, removes the cascaded HPA, and deletes the demo Nginx deployment and load generator.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\n# Remove AIBrix resources\nkubectl delete podautoscalers.autoscaling.aibrix.ai podautoscaler-example\nkubectl delete podautoscalers.autoscaling.aibrix.ai podautoscaler-example-mock-llama -n aibrix-system\nkubectl delete podautoscalers.autoscaling.aibrix.ai podautoscaler-example-mock-llama-apa -n aibrix-system\n\nmake uninstall && make undeploy\n\n# Remove the cascaded HPA\nkubectl delete hpa podautoscaler-example-hpa\n\n# Remove the demo Nginx deployment and load generator\nkubectl delete deployment nginx-deployment\nkubectl delete pod load-generator\nkubectl delete deployment llama2-70b -n aibrix-system\n```\n\n----------------------------------------\n\nTITLE: Describing PodAutoscaler Custom Resource\nDESCRIPTION: Command to view detailed information about a PodAutoscaler custom resource, including its scaling conditions and events.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/autoscaling/metric-based-autoscaling.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl describe podautoscaler <podautoscaler-name>\n```\n\n----------------------------------------\n\nTITLE: Configuring Matplotlib Plot Labels and Display\nDESCRIPTION: This code snippet configures a Matplotlib plot by setting the X and Y axis labels, adding a title related to GPU seconds per application, enabling the legend, and displaying the plot. It appears to be visualizing performance metrics related to GPU usage across different applications.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/plot/aibrix0.1-lora.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Labels and title\nax.set_xlabel('Number of Applications')\nax.set_ylabel('Per Application GPU Seconds')\nax.set_title('GPU Seconds Per Application')\nax.legend()\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Collecting and Exporting Benchmark Results\nDESCRIPTION: Commands to collect, compress, and extract benchmark results from the client pod to the local machine for further analysis and visualization, transferring CSV files generated by the benchmark runs.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/ps-experiment/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# inside the pod\nmkdir benchmark_output\nmv *.csv benchmark_output\ntar -cvf benchmark_output.tar benchmark_output/\n\n# outside pod\nkubectl cp benchmark-client:/home/ray/benchmark_output.tar /tmp/benchmark_output.tar\n```\n\n----------------------------------------\n\nTITLE: Accessing AIBrix Autoscaling Sample Files\nDESCRIPTION: Command to access the directory containing sample autoscaling configuration files for AIBrix.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/autoscaling/metric-based-autoscaling.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhttps://github.com/vllm-project/aibrix/tree/main/samples/autoscaling\n```\n\n----------------------------------------\n\nTITLE: Testing Production Stack Router Service with curl Request\nDESCRIPTION: curl command to test the Production Stack router service by sending a completion request to verify that the PS router is properly operational before running benchmarks.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/ps-experiment/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# ps router is the entrypoint for ps related experiments\ncurl -v http://vllm-router-service:80/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n\"prompt\": \"San Francisco is a\",\n\"max_tokens\": 128,\n\"temperature\": 0\n}'\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Optimizer Metrics in AIBrix\nDESCRIPTION: Commands for checking the custom metrics exposed by the GPU optimizer, which indicate the number of suggested replicas for a specific model deployment.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/autoscaling/optimizer-based-autoscaling.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward svc/aibrix-gpu-optimizer 8080:8080\n\ncurl http://localhost:8080/metrics/default/deepseek-llm-7b-chat-v100\n# HELP vllm:deployment_replicas Number of suggested replicas.\n# TYPE vllm:deployment_replicas gauge\nvllm:deployment_replicas{model_name=\"deepseek-llm-7b-chat\"} 1\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for VLLm Project Solver\nDESCRIPTION: This code snippet lists the required Python packages and their versions for the VLLm project's solver component. It includes numpy for numerical computations, pulp for linear programming, pandas for data manipulation, and ruamel.yaml for YAML parsing.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/optimizer/solver/melange/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# used for the solver\nnumpy\npulp==2.8.0\npandas\nruamel.yaml==0.18.6\n```\n\n----------------------------------------\n\nTITLE: Applying AIBrix Sample Instances\nDESCRIPTION: Command to apply sample configurations from the config/samples directory, creating AIBrix CRD instances such as PodAutoscaler and ModelAdapter.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nkubectl apply -k config/samples/\n```\n\n----------------------------------------\n\nTITLE: Undeploying AIBrix Controller\nDESCRIPTION: Command to undeploy the AIBrix controller from the Kubernetes cluster.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nmake undeploy\n```\n\n----------------------------------------\n\nTITLE: Verifying PodAutoscaler Creation\nDESCRIPTION: This command lists all PodAutoscalers across all namespaces to verify the creation of the KPA scaler.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get podautoscalers --all-namespaces\n```\n\n----------------------------------------\n\nTITLE: Checking Inference Service Pod Status\nDESCRIPTION: Command output showing the status of the inference service pod after deployment, indicating successful startup with two containers.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/distributed-kv-cache.rst#2025-04-22_snippet_2\n\nLANGUAGE: RST\nCODE:\n```\nNAME                                          READY   STATUS    RESTARTS   AGE\ndeepseek-coder-7b-instruct-6b885ffd8b-2kfjv   2/2     Running   0          4m\n```\n\n----------------------------------------\n\nTITLE: Specifying Sphinx Documentation Requirements\nDESCRIPTION: A requirements file that lists all necessary Sphinx packages and extensions for building documentation. It includes the core Sphinx package, theme support via sphinx-book-theme, and various extensions for enhanced functionality such as click command documentation, code copy buttons, toggle buttons, emoji support, type hint documentation, and design components.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/requirements-docs.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nsphinx==8.0.2\nsphinx-book-theme==1.1.3\nsphinx-click==6.0.0\nsphinx-copybutton==0.5.2\nsphinx-togglebutton==0.3.2\nsphinxemoji==0.3.1\nsphinx-autodoc-typehints==2.4.1\nsphinx_design==0.6.1\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: This code snippet defines the required Python packages and their versions for the project. It includes OpenAI's API, Hugging Face Transformers, vLLM (likely for efficient language model inference), and data analysis tools like Matplotlib, Pandas, and NumPy.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/autoscaling/requirements_bench_pa.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nopenai==1.51.2\ntransformers==4.45.2\nvllm==0.6.3\nmatplotlib==3.9.2\npandas==2.2.3\nnumpy==1.26.4\n```\n\n----------------------------------------\n\nTITLE: Experiment 1: Benchmarking Unmerged Single LoRA with Varying Concurrency\nDESCRIPTION: Script for benchmarking the unmerged single LoRA model (Deployment 2) with varying concurrency levels from 1 to 32. Results are saved to benchmark_unmerged_single_lora.jsonl.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nfor concurrency in {1..32}; do\n    output_file=\"benchmark_unmerged_single_lora.jsonl\"\n    echo \"Running benchmark with concurrency ${concurrency} and output file ${output_file}\"\n\n    python3 benchmark.py \\\n        --dataset-path \"/workspace/ShareGPT_V3_unfiltered_cleaned_split.json\" \\\n        --concurrency ${concurrency} \\\n        --output-file-path \"${output_file}\" \\\n        --deployment-endpoints \"deployment2\"\ndone\n```\n\n----------------------------------------\n\nTITLE: Displaying Help for GPU Profile Generation Script\nDESCRIPTION: Command to show the help message for the gpu-profile.py script, which is used to generate GPU profiles based on specified SLOs.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/optimizer/profiling/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython gen-profile.py -h\n```\n\n----------------------------------------\n\nTITLE: Uninstalling AIBrix CRDs\nDESCRIPTION: Command to remove the Custom Resource Definitions (CRDs) from the Kubernetes cluster.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nmake uninstall\n```\n\n----------------------------------------\n\nTITLE: Verifying AIBrix Installation on Lambda Cloud\nDESCRIPTION: This script verifies that NVIDIA drivers and Docker integration are correctly configured by checking GPU availability and running a test container.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/lambda.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbash ./hack/lambda-cloud/verify.sh\n```\n\n----------------------------------------\n\nTITLE: Sourcing Updated Bash Configuration\nDESCRIPTION: This command sources the updated .bashrc file to apply the new environment settings after installation.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/lambda.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsource ~/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Experiment 1: Benchmarking Multiple LoRA Models with Varying Concurrency\nDESCRIPTION: Script for benchmarking multiple LoRA models (Deployment 3) with varying concurrency levels from 1 to 32. Uses 4 LoRA models and saves results to benchmark_unmerged_multi_lora_4.jsonl.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nfor concurrency in {1..32}; do\n    output_file=\"benchmark_unmerged_multi_lora_4.jsonl\"\n    echo \"Running benchmark with concurrency ${concurrency} and output file ${output_file}\"\n\n    python3 benchmark.py \\\n        --dataset-path \"/workspace/ShareGPT_V3_unfiltered_cleaned_split.json\" \\\n        --concurrency ${concurrency} \\\n        --output-file-path \"${output_file}\" \\\n        --deployment-endpoints \"deployment3\"\ndone\n```\n\n----------------------------------------\n\nTITLE: Configuring CBC Solver for Arm-based Macs in Python\nDESCRIPTION: This code snippet shows how to configure the COIN CBC ILP solver for use with PuLP on Arm-based Mac platforms. It involves uncommenting and modifying the solver configuration in the melange/solver.py file.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/optimizer/solver/melange/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsolver= pulp.getSolver('COIN_CMD', path='/opt/homebrew/opt/cbc/bin/cbc', msg=0)\nproblem.solve(solver)\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example\nDESCRIPTION: Example of the experiment results directory structure showing different autoscaling test configurations.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/autoscaling/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nls experiment_results/25min_test\n\n25min_up_and_down-apa-least-request-20250209-064742\n25min_up_and_down-hpa-least-request-20250209-055214\n25min_up_and_down-kpa-least-request-20250209-061957\n25min_up_and_down-optimizer-kpa-least-request-20250209-071531\n```\n\n----------------------------------------\n\nTITLE: Experiment 3: Benchmarking with Fixed Concurrency and Varying LoRA Count\nDESCRIPTION: Script for benchmarking with a fixed concurrency of 1 while varying the number of LoRA models from 1 to 32. This tests the impact of LoRA model count on performance. Each configuration's results are saved to a separate output file.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nfor model_number in {1..32}; do\n    output_file=\"benchmark_unmerged_multi_lora_${model_number}_concurrency_1.jsonl\"\n    echo \"Running benchmark with model number ${model_number} and output file ${output_file}\"\n\n    python3 benchmark.py \\\n        --dataset-path \"/workspace/ShareGPT_V3_unfiltered_cleaned_split.json\" \\\n        --concurrency 1 \\\n        --output-file-path \"${output_file}\" \\\n        --deployment-endpoints \"deployment3\" \\\n        --models ${model_number}\ndone\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Deployment Scale Target Configuration\nDESCRIPTION: YAML configuration snippet showing the scaleTargetRef configuration for the autoscaling deployment.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/autoscaling/README.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: deepseek-llm-7b-chat (*this one)\n...\n```\n\n----------------------------------------\n\nTITLE: Updating GPU Optimizer Components in Kubernetes\nDESCRIPTION: Commands to update Aibrix components and redeploy the GPU Optimizer in a Kubernetes environment.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd ../../../../ && make docker-build-runtime\nkubectl create -k config/dependency\nkubectl create -k config/default\nkubectl delete -k config/overlays/dev/gpu-optimizer\nkubectl apply -k config/overlays/dev/gpu-optimizer\n```\n\n----------------------------------------\n\nTITLE: Example Response from DeepSeek R1 Model API\nDESCRIPTION: Sample JSON response from the DeepSeek R1 model API showing the structure of a chat completion response, including the generated content, token usage information, and metadata.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/samples/deepseek-r1/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n{\"id\":\"chatcmpl-d26583d2-96e5-42c4-a322-133c7d0e505d\",\"object\":\"chat.completion\",\"created\":1740967604,\"model\":\"deepseek-r1-671b\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"reasoning_content\":null,\"content\":\"<think>\\nOkay, the user is asking which team won the World Series in 2020. Let me recall, the World Series is the championship series of Major League Baseball (MLB) in the United States. I remember that 2020 was a unique year because of the COVID-19 pandemic, which affected the schedule and format of the season. The season was shortened, and there were some changes to the playoff structure.\\n\\nI think the Los Angeles Dodgers won the World Series around that time. Let me verify. The 2020 World Series was held at a neutral site, which was Globe Life Field in Arlington, Texas, to minimize travel and reduce the risk of COVID-19 spread. The Dodgers faced the Tampa Bay Rays. The Dodgers were led by players like Mookie Betts, Corey Seager, and Clayton Kershaw. They won the series in six games. The clinching game was Game 6, where the Dodgers beat the Rays 3-1. That victory gave the Dodgers their first title since 1988, ending a long drought.\\n\\nWait, let me make sure I got the opponent right. Was it the Rays or another team? Yes, I'm pretty confident it was the Rays because earlier in the playoffs, teams like the Braves and Dodgers were in the National League, while the Rays were the American League champions. The Rays had a strong team with players like Randy Arozarena, who had a standout postseason. But the Dodgers ultimately triumphed. So the answer should be the Los Angeles Dodgers. Let me double-check a reliable source if I'm unsure. Confirming now... yes, the Dodgers won the 2020 World Series against the Tampa Bay Rays in six games. So the user needs to know both the winner and maybe a bit of context, like it being in a neutral location. Okay, ready to provide a concise answer with those details.\\n</think>\\n\\nThe Los Angeles Dodgers won the 2020 World Series, defeating the Tampa Bay Rays in six games. This championship marked the Dodgers' first title since 1988. Notably, the 2020 series was held at Globe Life Field in Arlington, Texas—a neutral site—due to COVID-19 health and safety protocols.\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":19,\"total_tokens\":472,\"completion_tokens\":453,\"prompt_tokens_details\":null},\"prompt_logprobs\":null}%\n```\n\n----------------------------------------\n\nTITLE: Downloading Models from Volcano Engine\nDESCRIPTION: Placeholder for downloading models using Toscli from Volcano Engine, with instructions to be added later.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nTODO: Add Toscli tutorials\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding for Experiment 1\nDESCRIPTION: Kubernetes commands to set up port forwarding for the gateway service and model service. This enables testing gateway overhead compared to direct service access as a baseline.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n envoy-gateway-system port-forward service/envoy-aibrix-system-aibrix-eg-903790dc 8888:80\nkubectl port-forward svc/deepseek-coder-7b-instruct 8887:8000 -n aibrix-system\n```\n\n----------------------------------------\n\nTITLE: Endpoint Exposure Configuration\nDESCRIPTION: Commands to expose the service endpoint through LoadBalancer.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/samples/deepseek-r1/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Option 1: Kubernetes cluster with LoadBalancer support\nLB_IP=$(kubectl get svc/envoy-aibrix-system-aibrix-eg-903790dc -n envoy-gateway-system -o=jsonpath='{.status.loadBalancer.ingress[0].ip}')\nENDPOINT=\"${LB_IP}:80\"\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Prefix-sharing Dataset with Python\nDESCRIPTION: This snippet demonstrates how to use the synthetic_prefix_sharing_dataset.py script to generate synthetic datasets with controlled prompt lengths and shared prompt percentages for various workload types.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/dataset-generator/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nNUM_PREFIX=10\nNUM_SAMPLES=200\n## Toolbench Workload\npython synthetic_prefix_sharing_dataset.py --app-name programming --prompt-length 1835 --prompt-length-std 742 --shared-proportion 0.85 --shared-proportion-std 0.13 --num-samples-per-prefix ${NUM_SAMPLES} --num-prefix ${NUM_PREFIX} --randomize-order\n\n## Embodied Agent Workload\npython synthetic_prefix_sharing_dataset.py --app-name programming --prompt-length 2285 --prompt-length-std 471 --shared-proportion 0.97 --shared-proportion-std 0.14 --num-samples-per-prefix ${NUM_SAMPLES} --num-prefix ${NUM_PREFIX} --randomize-order\n\n## Programming Workload\npython synthetic_prefix_sharing_dataset.py --app-name programming --prompt-length 3871 --prompt-length-std 1656 --shared-proportion 0.97 --shared-proportion-std 0.074 --num-samples-per-prefix ${NUM_SAMPLES} --num-prefix ${NUM_PREFIX} --randomize-order\n\n## Video QA Workload\npython synthetic_prefix_sharing_dataset.py --app-name programming --prompt-length 9865 --prompt-length-std 5976 --shared-proportion 0.88 --shared-proportion-std 0.32 --num-samples-per-prefix ${NUM_SAMPLES} --num-prefix ${NUM_PREFIX} --randomize-order\n\n## LooGLE Workload\npython synthetic_prefix_sharing_dataset.py --app-name programming --prompt-length 23474 --prompt-length-std 6105 --shared-proportion 0.91 --shared-proportion-std 0.24 --num-samples-per-prefix ${NUM_SAMPLES} --num-prefix ${NUM_PREFIX} --randomize-order\n```\n\n----------------------------------------\n\nTITLE: Cloning the AIBrix Repository\nDESCRIPTION: Commands to fork and clone the AIBrix repository from GitHub to your local environment.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/vllm-project/aibrix.git\ncd aibrix\n```\n\n----------------------------------------\n\nTITLE: Defining Grafana MaaS Traffic File Format\nDESCRIPTION: Specifies the expected CSV format for Grafana MaaS traffic statistics file.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_6\n\nLANGUAGE: csv\nCODE:\n```\n\"Time\",\"Total\",\"Success\",\"4xx Error\"\n2024-10-1 00:00:00,100,99,1\n```\n\n----------------------------------------\n\nTITLE: Deploying Mock Model to Kubernetes\nDESCRIPTION: Kubernetes command to create resources for the mock model deployment\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/lora/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -k config/mock\n```\n\n----------------------------------------\n\nTITLE: Loading Simulator Docker Image to Kind Cluster\nDESCRIPTION: Optional command to load the simulator Docker image into a Kind Kubernetes cluster. This step is only necessary for Kind users.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkind load docker-image aibrix/vllm-simulator:nightly\n```\n\n----------------------------------------\n\nTITLE: Defining Grafana MaaS Completion Length File Format\nDESCRIPTION: Specifies the expected CSV format for Grafana MaaS completion length statistics file.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_8\n\nLANGUAGE: csv\nCODE:\n```\n\"Time\",\"P50\",\"P70\",\"P95\",\"P99\"\n```\n\n----------------------------------------\n\nTITLE: Defining Grafana CloudIDE Traffic File Format\nDESCRIPTION: Specifies the expected CSV format for Grafana CloudIDE traffic statistics file.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/generator/workload-generator/README.md#2025-04-22_snippet_9\n\nLANGUAGE: csv\nCODE:\n```\n\"Time\",\"Rate\"\n```\n\n----------------------------------------\n\nTITLE: Checking Available Models\nDESCRIPTION: cURL command to list all available models with JSON formatting\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/lora/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:8000/v1/models | jq .\n```\n\n----------------------------------------\n\nTITLE: Retrieving Envoy Proxy External IP and Port for AIBrix Gateway\nDESCRIPTION: This snippet demonstrates how to retrieve the external IP and port for the Envoy proxy to access the AIBrix gateway using kubectl.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/gateway-plugins.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nNAME                                     TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                   AGE\nenvoy-aibrix-system-aibrix-eg-903790dc   LoadBalancer   10.96.239.246   101.18.0.4    80:32079/TCP                              10d\nenvoy-gateway                            ClusterIP      10.96.166.226   <none>        18000/TCP,18001/TCP,18002/TCP,19001/TCP   10d\n```\n\n----------------------------------------\n\nTITLE: Loading LoRA Adapter via API in Bash\nDESCRIPTION: Sends a POST request to load a LoRA adapter into the running model, specifying the adapter name and path.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/runtime.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8080/v1/lora_adapter/load \\\n-H \"Content-Type: application/json\" \\\n-d '{\"lora_name\": \"lora-2\", \"lora_path\": \"bharati2324/Qwen2.5-1.5B-Instruct-Code-LoRA-r16v2\"}'\n```\n\n----------------------------------------\n\nTITLE: Installing Kubernetes Components\nDESCRIPTION: Command to set up the required Kubernetes components using a setup script.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/distributed/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./setup.sh\n```\n\n----------------------------------------\n\nTITLE: Updating Resource Requirements for AIBrix Components in Kubernetes\nDESCRIPTION: Commands to update resource allocations for envoy gateway and gateway plugin components in the AIBrix system, setting CPU and memory requests and limits to 2 CPU cores and 8GB memory for production-grade setups.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/ps-experiment/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# update envoy gateway              \nkubectl edit deployment envoy-aibrix-system-aibrix-eg-903790dc -n envoy-gateway-system\n# update gateway plugin\nkubectl edit deployment aibrix-gateway-plugins -n aibrix-system\n\n# update code\n            requests:\n              cpu: 2\n              memory: 8Gi\n            limits:\n              cpu: 2\n              memory: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Testing Inference Service with cURL\nDESCRIPTION: Shell command to test the inference service using cURL, sending a chat completion request to the model with distributed KV cache support.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/distributed-kv-cache.rst#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncurl -v \"http://localhost:8888/v1/chat/completions\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: XXXXXXXXXXXXXXXXXXXXXXXX\" \\\n  -d '{\n     \"model\": \"deepseek-coder-7b-instruct\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Created container vllm-openai\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n----------------------------------------\n\nTITLE: Defining Apache License 2.0 Header for Aibrix Project\nDESCRIPTION: This code snippet contains the full text of the Apache License 2.0 header, customized for the Aibrix project. It includes the copyright notice, permissions, and conditions for using the software.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/hack/boilerplate.go.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n/*\nCopyright 2024 The Aibrix Team.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix Python Module\nDESCRIPTION: This command installs the AIBrix Python module, which is required for GPU optimization and benchmarking.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/heterogeneous-gpu.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip3 install aibrix\n```\n\n----------------------------------------\n\nTITLE: Loading Docker Images for Linux\nDESCRIPTION: Loads the vLLM CPU environment Docker image for Linux AMD64 systems\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/vllm/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull aibrix/vllm-cpu-env:linux-amd64\nkind load docker-image aibrix/vllm-cpu-env:linux-amd64\n```\n\n----------------------------------------\n\nTITLE: Running Python Client for Least-Request Routing Strategy Testing\nDESCRIPTION: Python client command for testing the least-request routing strategy via the gateway endpoint. This strategy routes to instances with fewer active connections.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython client.py \\\n--dataset-path \"/tmp/ShareGPT_V3_unfiltered_cleaned_split.json\" \\\n--endpoint \"http://101.126.81.102:80\" \\\n--num-prompts 2000 \\\n--interval 0.05 \\\n--output-file-path \"least-request-v2.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Controller and Model Adapter\nDESCRIPTION: Kubernetes command to apply the model adapter configuration\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/lora/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f development/tutorials/lora/model_adapter.yaml\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Model on Kubernetes Pod\nDESCRIPTION: These commands set up port forwarding and run the AIBrix benchmark tool for a specific model on a Kubernetes pod.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/heterogeneous-gpu.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward [pod_name] 8010:8000 1>/dev/null 2>&1 &\n# Wait for port-forward taking effect.\naibrix_benchmark -m deepseek-coder-7b -o [path_to_benchmark_output]\n```\n\n----------------------------------------\n\nTITLE: Conditional QPS Values Assignment for Benchmark Types\nDESCRIPTION: Bash script snippet showing how to conditionally set QPS (Queries Per Second) values based on whether the benchmark is for a naive implementation or optimized setup, allowing for different testing patterns.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/ps-experiment/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nif [[ \"$KEY\" == \"*naive*\" ]]; then\n   QPS_VALUES=(0.1 0.5 0.9 1.3 1.7 2.1 2.5 2.9 3.3 3.7 4.1)\nelse\n   QPS_VALUES=(4.1 3.7 3.3 2.9 2.5 2.1 1.7 1.3 0.9 0.5 0.1)\nfi\n```\n\n----------------------------------------\n\nTITLE: Configuring HuggingFace Token for Simulator\nDESCRIPTION: JSON configuration for setting the HuggingFace token needed by the model tokenizer in the simulator. This is required before building the simulator image.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/app/README.md#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"huggingface_token\": \"your huggingface token\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Verifying Nginx Deployment in Kubernetes\nDESCRIPTION: These commands create a demo Nginx deployment with default 1 replica and verify its status.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/podautoscaler/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f config/samples/autoscaling_v1alpha1_demo_nginx.yaml\nkubectl get deployments -n default\n```\n\n----------------------------------------\n\nTITLE: Sending Test Request with Curl\nDESCRIPTION: Example curl command for sending a request to the LLM API endpoint. It specifies the deepseek-coder-7b-instruct model with a simple prompt. This represents the basic client operation being benchmarked.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/gateway/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v http://localhost:8888/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer sk-any-key\" \\\n    -d '{\n        \"model\": \"deepseek-coder-7b-instruct\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n        \"max_tokens\": 128\n    }'\n```\n\n----------------------------------------\n\nTITLE: Checking Kubernetes Node Status\nDESCRIPTION: This command retrieves the status of Kubernetes nodes to ensure they are in 'Ready' state after cluster creation.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/lambda.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get nodes\n```\n\n----------------------------------------\n\nTITLE: Creating NVKind Cluster Configuration\nDESCRIPTION: Command to create an NVKind cluster using a single node configuration template for GPU cloud instances.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/tutorials/distributed/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnvkind cluster create --config-template=nvkind-single-node.yaml\n```\n\n----------------------------------------\n\nTITLE: Viewing Pod Autoscaler Logs in AIBrix\nDESCRIPTION: Command to check the logs of the AIBrix controller manager that collects metrics from each pod for autoscaling purposes.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/autoscaling/metric-based-autoscaling.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs <aibrix-controller-manager-podname> -n aibrix-system -f\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix Using YAML Bundle\nDESCRIPTION: Command for users to install the AIBrix project using the generated YAML bundle without dependencies.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/<org>/aibrix/<tag or branch>/dist/install.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting Up Port Forwarding for AI Model Access in Kubernetes\nDESCRIPTION: Commands to set up port forwarding to access the Envoy Gateway service in a Kubernetes cluster and define the endpoint for API requests. This allows accessing services without a LoadBalancer.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/samples/deepseek-r1/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n envoy-gateway-system port-forward service/envoy-aibrix-system-aibrix-eg-903790dc 8888:80 &\nENDPOINT=\"localhost:8888\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for GPU Profiling Tools\nDESCRIPTION: Command to install the required dependencies for the GPU profiling tools using pip.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/optimizer/profiling/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Removing Deployed AIBrix Instances\nDESCRIPTION: Command to remove the deployed AIBrix instances (Custom Resources) from the Kubernetes cluster.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nkubectl delete -k config/samples/\n```\n\n----------------------------------------\n\nTITLE: Building the AIBrix Project\nDESCRIPTION: Command to build the AIBrix project after navigating to the cloned directory.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Local Testing Without Remote Registry\nDESCRIPTION: Command for deploying AIBrix without pushing to a remote DockerHub, useful for local testing.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/README.md#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nmake deploy IMG=example.com/aibrix:v1\n```\n\n----------------------------------------\n\nTITLE: Deploying Model for Mac\nDESCRIPTION: Creates and deletes Kubernetes resources for Mac deployment\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/development/vllm/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -k vllm/macos\n\nkubectl delete -k vllm/macos\n```\n\n----------------------------------------\n\nTITLE: Installing AIBrix KV Cache Component\nDESCRIPTION: Command to install only the AIBrix KV cache controller component using Kubernetes kustomize.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/getting_started/installation/installation.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -k config/standalone/kv-cache-controller\n```\n\n----------------------------------------\n\nTITLE: Generating GPU Profiles for Optimizer\nDESCRIPTION: These commands set up Redis port forwarding and generate GPU profiles for the optimizer using AIBrix tools.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/docs/source/features/heterogeneous-gpu.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n aibrix-system port-forward svc/aibrix-redis-master 6379:6379 1>/dev/null 2>&1 &\n# Wait for port-forward taking effect.\naibrix_gen_profile deepseek-coder-7b-v100 --cost [cost1] [SLO-metric] [SLO-value] -o \"redis://localhost:6379/?model=deepseek-coder-7b\"\naibrix_gen_profile deepseek-coder-7b-l20 --cost [cost2] [SLO-metric] [SLO-value] -o \"redis://localhost:6379/?model=deepseek-coder-7b\"\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output for Mélange Solver Results\nDESCRIPTION: This snippet demonstrates the structure of the JSON output produced by the Mélange solver. It includes the recommended GPU allocation and the total hourly cost.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/python/aibrix/aibrix/gpu_optimizer/optimizer/solver/melange/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"A10G\": 3,\n    \"A100-80GB\": 1,\n    \"cost\": 6.7\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading Chat Template for Llama-2\nDESCRIPTION: Downloads the Llama-2 chat template in Jinja format, which will be used to structure conversations for the LLM during benchmarking.\nSOURCE: https://github.com/vllm-project/aibrix/blob/main/benchmarks/scenarios/lora/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nwget https://raw.githubusercontent.com/chujiezheng/chat_templates/refs/heads/main/chat_templates/llama-2-chat.jinja\n```"
  }
]
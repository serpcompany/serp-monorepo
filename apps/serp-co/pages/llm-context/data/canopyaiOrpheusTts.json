[
  {
    "owner": "canopyai",
    "repo": "orpheus-tts",
    "content": "TITLE: Generating Speech with Orpheus TTS in Python\nDESCRIPTION: This Python code snippet demonstrates how to use the OrpheusModel class to generate speech from a given text prompt, and then save it as a WAV file. It uses the `canopylabs/orpheus-tts-0.1-finetune-prod` model, sets the voice to \"tara\", and outputs the generated audio chunks into \"output.wav\".\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom orpheus_tts import OrpheusModel\nimport wave\nimport time\n\nmodel = OrpheusModel(model_name =\"canopylabs/orpheus-tts-0.1-finetune-prod\")\nprompt = '''Man, the way social media has, um, completely changed how we interact is just wild, right? Like, we're all connected 24/7 but somehow people feel more alone than ever. And don't even get me started on how it's messing with kids' self-esteem and mental health and whatnot.'''\n\nstart_time = time.monotonic()\nsyn_tokens = model.generate_speech(\n   prompt=prompt,\n   voice=\"tara\",\n   )\n\nwith wave.open(\"output.wav\", \"wb\") as wf:\n   wf.setnchannels(1)\n   wf.setsampwidth(2)\n   wf.setframerate(24000)\n\n   total_frames = 0\n   chunk_counter = 0\n   for audio_chunk in syn_tokens: # output streaming\n      chunk_counter += 1\n      frame_count = len(audio_chunk) // (wf.getsampwidth() * wf.getnchannels())\n      total_frames += frame_count\n      wf.writeframes(audio_chunk)\n   duration = total_frames / wf.getframerate()\n\n   end_time = time.monotonic()\n   print(f\"It took {end_time - start_time} seconds to generate {duration:.2f} seconds of audio\")\n```\n\n----------------------------------------\n\nTITLE: Streaming TTS with OrpheusCpp (Python)\nDESCRIPTION: Demonstrates streaming text-to-speech using the OrpheusCpp class. It initializes the OrpheusCpp model, streams audio chunks for a given text, and saves the concatenated audio to a WAV file.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/additional_inference_options/no_gpu/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom scipy.io.wavfile import write\nfrom orpheus_cpp import OrpheusCpp\nimport numpy as np\n\norpheus = OrpheusCpp(verbose=False, lang=\"en\")\n\ntext = \"I really hope the project deadline doesn't get moved up again.\"\nbuffer = []\nfor i, (sr, chunk) in enumerate(orpheus.stream_tts_sync(text, options={\"voice_id\": \"tara\"})):\n   buffer.append(chunk)\n   print(f\"Generated chunk {i}\")\nbuffer = np.concatenate(buffer, axis=1)\nwrite(\"output.wav\", 24_000, np.concatenate(buffer))\n```\n\n----------------------------------------\n\nTITLE: Installing Orpheus TTS and vllm Dependencies with Pip\nDESCRIPTION: This series of commands navigates to the Orpheus-TTS directory, installs the necessary packages for Orpheus TTS using pip, and then reverts to a specific version of vllm due to a reported bug.  vllm is used for fast inference.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd Orpheus-TTS && pip install orpheus-speech # uses vllm under the hood for fast inference\n\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install vllm==0.7.3\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Installs the necessary Python packages for pretraining Orpheus-TTS, including transformers, trl, wandb, flash_attn, datasets, and torch. The user might need to adjust the version of `flash_attn` based on their specific torch, CUDA, and Python versions.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/pretrain/readme.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install transformers trl wandb flash_attn datasets torch\n```\n\n----------------------------------------\n\nTITLE: Cloning the Orpheus TTS Repository using Git\nDESCRIPTION: This command clones the Orpheus-TTS repository from GitHub, allowing users to access the codebase, models, and other resources.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/canopyai/Orpheus-TTS.git\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Finetuning with Pip\nDESCRIPTION: This command installs necessary Python packages, including transformers, datasets, wandb, trl, flash_attn, and torch, for finetuning the Orpheus TTS model using pip. These packages are required for data handling, model training, and experiment tracking.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install transformers datasets wandb trl flash_attn torch\n```\n\n----------------------------------------\n\nTITLE: Install orpheus-cpp\nDESCRIPTION: Installs the orpheus-cpp package using pip. This is a prerequisite for running the streaming inference examples.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/additional_inference_options/no_gpu/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install orpheus-cpp\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face and Wandb with CLI\nDESCRIPTION: These commands allow you to log in into your Hugging Face and Weights & Biases (wandb) accounts using the command line interface.  This is necessary to push/pull datasets and models and track the training progress.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login <enter your HF token>\nwandb login <wandb token>\n```\n\n----------------------------------------\n\nTITLE: Run WebRTC Streaming Example (bash)\nDESCRIPTION: Runs the WebRTC streaming example using the orpheus_cpp module. This command starts a server that allows streaming audio via WebRTC.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/additional_inference_options/no_gpu/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m orpheus_cpp\n```\n\n----------------------------------------\n\nTITLE: Install llama-cpp-python (Linux/Windows)\nDESCRIPTION: Installs the llama-cpp-python package with CPU support. This is a dependency for orpheus-cpp.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/additional_inference_options/no_gpu/README.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n```\n\n----------------------------------------\n\nTITLE: Install llama-cpp-python (MacOS Apple Silicon)\nDESCRIPTION: Installs the llama-cpp-python package with Metal support for Apple Silicon Macs. This is a dependency for orpheus-cpp.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/additional_inference_options/no_gpu/README.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/metal\n```\n\n----------------------------------------\n\nTITLE: Launching the Training Script with Accelerate\nDESCRIPTION: This command launches the `train.py` script using `accelerate`, enabling distributed training across multiple GPUs or machines. The accelerate library simplifies the process of running PyTorch training scripts in a distributed environment.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch train.py\n```\n\n----------------------------------------\n\nTITLE: Launching Pretraining Script\nDESCRIPTION: Launches the pretraining script `pretrain.py` using the `accelerate` library, which facilitates distributed training across multiple nodes. The configuration, including datasets and hyperparameters, is expected to be defined in a YAML file.\nSOURCE: https://github.com/canopyai/orpheus-tts/blob/main/pretrain/readme.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch pretrain.py\n```"
  }
]
[
  {
    "owner": "splunk",
    "repo": "public-o11y-docs",
    "content": "TITLE: Configuring Exporter TLS Settings including mTLS and Insecure Modes - YAML\nDESCRIPTION: This YAML snippet illustrates how to configure TLS settings for various OpenTelemetry Collector exporters under different security scenarios: full mTLS with client and CA certificates, insecure plaintext transport, and secure connections that skip server certificate verification. Required fields include cert/key/cafile paths or the 'insecure' flag. The 'endpoint' specifies the backend. These configurations depend on the Collector's exporters supporting TLS fields and the presence of files referenced. Fields like 'server_name_override', while described, are not shown but can be added as needed. Inputs are YAML stanzas per exporter; outputs impact TLS behavior per exporter instance.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/common-config/collector-common-config-tls.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otlp:\n    endpoint: myserver.local:55690\n    tls:\n      insecure: false\n      ca_file: server.crt\n      cert_file: client.crt\n      key_file: client.key\n      min_version: \"1.1\"\n      max_version: \"1.2\"\n  otlp/insecure:\n    endpoint: myserver.local:55690\n    tls:\n      insecure: true\n  otlp/secure_no_verify:\n    endpoint: myserver.local:55690\n    tls:\n      insecure: false\n      insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Complete OAuth2 Client Extension YAML Configuration\nDESCRIPTION: A comprehensive YAML example detailing the implementation of the `oauth2client` extension along with its integration into the service pipeline. This includes all necessary configuration parameters such as client credentials, token URL, endpoint parameters, scopes, and TLS settings. It outlines how to set up receivers and exporters to utilize the OAuth2 Client Auth extension, demonstrating its application in a telemetry data pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/oauth2client-extension.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  oauth2client:\n    client_id: someclientid\n    client_secret: someclientsecret\n    endpoint_params:\n      audience: someaudience\n    token_url: https://example.com/oauth2/default/v1/token\n    scopes: [\"api.metrics\"]\n    # tls settings for the token client\n    tls:\n      insecure: true\n      ca_file: /var/lib/mycert.pem\n      cert_file: certfile\n      key_file: keyfile\n    # timeout for the token client\n    timeout: 2s\n  \nreceivers:\n  hostmetrics:\n    scrapers:\n      memory:\n  otlp:\n    protocols:\n      grpc:\n  \nexporters:\n  otlphttp/withauth:\n    endpoint: http://localhost:9000\n    auth:\n      authenticator: oauth2client\n  \notlp/withauth:\n    endpoint: 0.0.0.0:5000\n    tls:\n      ca_file: /tmp/certs/ca.pem\n    auth:\n      authenticator: oauth2client\n\nservice:\n  extensions: [oauth2client]\n  pipelines:\n    metrics:\n      receivers: [hostmetrics]\n      processors: []\n      exporters: [otlphttp/withauth, otlp/withauth]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Core for .NET on Windows\nDESCRIPTION: This PowerShell command installs the OpenTelemetry Core for .NET using a downloaded zip file. Ensure the download path is correct before executing.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_17\n\nLANGUAGE: powershell\nCODE:\n```\nInstall-OpenTelemetryCore -LocalPath \"C:\\Users\\Administrator\\Downloads\\splunk-opentelemetry-dotnet-windows.zip\"\n```\n\n----------------------------------------\n\nTITLE: Creating and Tagging a Custom Span - OpenTelemetry Go\nDESCRIPTION: Demonstrates starting a new span named \"hello\" using the tracer and associating a context. Sets attributes (tags) on the span using trace.WithAttributes and ensures the span ends after the code block. This pattern is necessary for custom trace generation and tagging within custom logic, requiring a valid context and the OpenTelemetry Go API.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/go-manual-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: go\nCODE:\n```\nfunc() {\n   ctx, span := tracer.Start(ctx, \"hello\", trace.WithAttributes(attribute.String(\"foo\", \"bar\")))\n   defer span.End()\n   // your logic for \"hello\" span\n}()\n```\n\n----------------------------------------\n\nTITLE: Acquiring a Tracer with OpenTelemetry Java\nDESCRIPTION: This snippet demonstrates how to obtain a tracer instance using the getTracer method in OpenTelemetry Java. The tracer is necessary for creating custom spans. There are no special dependencies other than the OpenTelemetry Java API.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-manual-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport io.opentelemetry.api.trace.Tracer;\n\nTracer tracer = openTelemetry.getTracer(\"instrumentation-scope-name\", \"instrumentation-scope-version\");\n```\n\n----------------------------------------\n\nTITLE: Starting a Trace Span with OpenTelemetry in C++\nDESCRIPTION: Shows the basic creation of a trace span named \"HandleRequest\" using a previously obtained `tracer` object. The returned `span` object represents an operation and its duration should be managed, typically ending when the operation completes (e.g., via RAII or explicit call to `End()`).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/cpp/instrument-cpp.rst#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nauto span = tracer->StartSpan(\"HandleRequest\");\n```\n\n----------------------------------------\n\nTITLE: Instrumenting an Azure WebJob Function with SplunkFunction Attribute (.NET/C#)\nDESCRIPTION: Illustrates usage of the custom SplunkFunction attribute to instrument an Azure WebJob function for tracing. When the ProcessTimer function is triggered by a TimerTrigger, the attribute ensures an OpenTelemetry Activity span is started and stopped, automatically capturing errors and adding function metadata. Logger messages within the function are also captured. This pattern requires the SplunkFunctionAttribute to be defined and OpenTelemetry properly initialized.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/azure-webjobs.rst#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\npublic class Functions\n{\n    [SplunkFunction]\n    public void ProcessTimer([TimerTrigger(\"1/5 * * * * *\")] TimerInfo timerInfo, ILogger logger)\n    {\n        logger.LogInformation(\"Hello World!\");\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing a Custom Span in PHP\nDESCRIPTION: Demonstrates how to create a custom span using a Tracer instance. The code shows starting a span named 'rollTheDice' before an operation (rolling dice multiple times) and ending the span after the operation completes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-manual-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: php\nCODE:\n```\n<?php\npublic function roll($rolls) {\n   $span = $this->tracer->spanBuilder(\"rollTheDice\")->startSpan();\n   $result = [];\n   for ($i = 0; $i < $rolls; $i++) {\n      $result[] = $this->rollOnce();\n   }\n   $span->end();\n   return $result;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Authentication Extensions in OpenTelemetry Collector (YAML)\nDESCRIPTION: This example demonstrates how to configure authentication extensions in OpenTelemetry Collector, including OIDC for server-side authentication and OAuth2 for client-side authentication. It shows how to set up TLS certificates, token settings, and how to reference these authentication mechanisms in receiver configurations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/common-config/collector-common-config-auth.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n   oidc:\n      # see the blog post on securing the otelcol for information\n      # on how to setup an OIDC server and how to generate the TLS certs\n      # required for this example\n      # https://medium.com/opentelemetry/securing-your-opentelemetry-collector-1a4f9fa5bd6f\n      issuer_url: http://localhost:8080/auth/realms/opentelemetry\n      audience: account\n\noauth2client:\n   client_id: someclientid\n   client_secret: someclientsecret\n   token_url: https://example.com/oauth2/default/v1/token\n   scopes: [\"api.metrics\"]\n   # tls settings for the token client\n   tls:\n      insecure: true\n      ca_file: /var/lib/mycert.pem\n      cert_file: certfile\n      key_file: keyfile\n   # timeout for the token client\n   timeout: 2s\n\nreceivers:\n   otlp/with_auth:\n      protocols:\n         grpc:\n            endpoint: localhost:4318\n            tls:\n               cert_file: /tmp/certs/cert.pem\n               key_file: /tmp/certs/cert-key.pem\n            auth:\n               ## oidc is the extension name to use as the authenticator for this receiver\n               authenticator: oidc\n\n   otlphttp/withauth:\n      endpoint: http://localhost:9000\n      auth:\n         authenticator: oauth2client\n```\n\n----------------------------------------\n\nTITLE: Setting Deployment Environment Attribute (Shell)\nDESCRIPTION: Sets the 'deployment.environment' resource attribute for all instrumented applications. This value appears in the Splunk APM UI. If not specified, the environment defaults to 'unknown'. Appends the attribute to the 'OTEL_RESOURCE_ATTRIBUTES' environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\n--deployment-environment <value>\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Deployment for Multi-Container Instrumentation\nDESCRIPTION: YAML configuration for a Kubernetes deployment with multiple containers and language-specific instrumentation annotations. It demonstrates how to set up auto-instrumentation for Java, Node.js, and Python applications within the same pod.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nselector:\n  matchLabels:\n    app: my-pod-with-multi-containers-multi-instrumentations\nreplicas: 1\ntemplate:\n  metadata:\n    labels:\n      app: my-pod-with-multi-containers-multi-instrumentations\n    annotations:\n      instrumentation.opentelemetry.io/inject-java: \"true\"\n      instrumentation.opentelemetry.io/java-container-names: \"myapp,myapp2\"\n      instrumentation.opentelemetry.io/inject-nodejs: \"true\"\n      instrumentation.opentelemetry.io/python-container-names: \"myapp3\"\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Instrumentation in Node.js\nDESCRIPTION: Enhance Node.js applications by adding custom or third-party instrumentations compatible with OpenTelemetry JS using the start() function.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/instrumentation/instrument-nodejs-application.rst#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require('@splunk/otel');\nconst { getInstrumentations } = require('@splunk/otel/lib/instrumentations');\n\nstart({\n   tracing: {\n      instrumentations: [\n         ...getInstrumentations(), // Adds default instrumentations\n         new MyCustomInstrumentation(),\n         new AnotherInstrumentation(),\n      ],\n   },\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Splunk OpenTelemetry Go Instrumentation (Go)\nDESCRIPTION: Demonstrates the basic Go code required to initialize the Splunk OpenTelemetry Go distribution within an application. It imports the `distro` package, calls `distro.Run()` to start the instrumentation, and includes a deferred call to `sdk.Shutdown()` to ensure all telemetry data is flushed before the application exits. This code should be placed early in the application's lifecycle, typically in the `main` function.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/instrument-go-application.rst#2025-04-22_snippet_4\n\nLANGUAGE: go\nCODE:\n```\npackage main\n\nimport (\n   \"context\"\n   \"github.com/signalfx/splunk-otel-go/distro\"\n)\n\nfunc main() {\n   sdk, err := distro.Run()\n   if err != nil {\n      panic(err)\n   }\n   // Flush all spans before the application exits\n   defer func() {\n      if err := sdk.Shutdown(context.Background()); err != nil {\n         panic(err)\n      }\n   }()\n\n   // ...\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Event Span in Splunk RUM using OpenTelemetry API (JavaScript)\nDESCRIPTION: This JavaScript snippet demonstrates how to initialize an OpenTelemetry tracer and create a custom event span named 'blog.likes'. It uses the '@opentelemetry/api' package to get a tracer associated with 'blogLoader', starts a span representing the custom event 'blog.likes', adds a 'workflow.name' attribute for identification in Splunk RUM, and then ends the span after some time. This allows tracking the duration and occurrence of the 'blog.likes' user interaction.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/rum-scenario-library/spa-custom-event.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport {trace} from '@opentelemetry/api'\n\nconst tracer = trace.getTracer('blogLoader');\nconst span = tracer.startSpan('blog.likes', {\n  attributes: {\n      'workflow.name': 'blog.likes'\n  }\n});\n\n// time passes\nspan.end();\n```\n\n----------------------------------------\n\nTITLE: OTTL Filter Conditions Configuration\nDESCRIPTION: Demonstrates how to configure detailed filtering conditions using OpenTelemetry Transformation Language (OTTL) for traces, metrics, and logs. Includes examples for filtering spans, span events, metrics, datapoints, and log records based on various attributes and conditions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filter-processor.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  filter:\n    traces:\n      span:\n        - 'attributes[\"attribute.label\"] == \"attribute_value\"'\n        - 'resource.attributes[\"host.name\"] == \"localhost\"'\n      # Checked only if `span` is not dropped \n      spanevent:\n        - 'attributes[\"label\"] == true'\n        - 'IsMatch(name, \".*http.*\") == false'\n       # If all span events are dropped, the span is dropped\n      metrics:\n        metric:\n          - 'name == \"metric.name\" and attributes[\"label\"] == \"value\"'\n          - 'type == METRIC_DATA_TYPE_HISTOGRAM'\n        # Checked only if `metric` is not dropped \n        datapoint:\n          - 'metric.type == METRIC_DATA_TYPE_SUMMARY'\n          - 'resource.attributes[\"service.name\"] == \"my_service_name\"'\n         # If all datapoints are dropped, the metric is dropped \n      logs:\n        log_record:\n          - 'IsMatch(body, \".*token.*\") == true'\n          - 'severity_number < SEVERITY_NUMBER_WARN'\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Counter Metric Instrument in .NET (C#)\nDESCRIPTION: Defines a custom counter metric named \"custom.counter\" of type `long` using the `meter` instance. The counter includes a description for clarity. This instrument will be used to record increments for a specific event or value within the application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/manual-dotnet-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: csharp\nCODE:\n```\nvar counter = meter.CreateCounter<long>(\"custom.counter\", description: \"Custom counter's description\");\n```\n\n----------------------------------------\n\nTITLE: Installing Browser RUM via CDN Script\nDESCRIPTION: HTML snippet for loading the Browser RUM agent from Splunk CDN and initializing it with configuration parameters like realm, access token, and application details.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/install-rum-browser.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--//\n\nIMPORTANT: Replace the <version> placeholder in the src URL with a\nversion from https://github.com/signalfx/splunk-otel-js-web/releases\n\n//-->\n<script src=\"https://cdn.signalfx.com/o11y-gdi-rum/<version>/splunk-otel-web.js\" crossorigin=\"anonymous\"></script>\n<script>\n   SplunkRum.init({\n      realm: '<realm>',\n      rumAccessToken: '<your_rum_token>',\n      applicationName: '<your_app_name>',\n      version: '<your_app_version>',\n      deploymentEnvironment: '<your_environment_name>'\n   });\n</script>\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Counter in Python with OpenTelemetry\nDESCRIPTION: Demonstrates how to create a counter metric and increment it during application execution in Python using OpenTelemetry.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npeanut_counter = meter.create_counter(\n   \"peanut.counter\", unit=\"1\", description=\"Counts the number of consumed peanuts\"\n)\n\ndef do_stuff(work_item):\n   peanut_counter.add(1, {\"work.type\": work_item.work_type})\n   print(\"Collecting peanuts...\")\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector for Splunk Observability Cloud\nDESCRIPTION: Sample YAML configuration file that shows how to configure the upstream OpenTelemetry Collector to send metrics and traces to Splunk Observability Cloud. The configuration includes receivers, processors, and exporters setup.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/other-ingestion-methods/upstream-collector.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nhttps://raw.githubusercontent.com/signalfx/splunk-otel-collector/main/cmd/otelcol/config/collector/upstream_agent_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenTelemetry Tracing in Python for Splunk Observability Cloud\nDESCRIPTION: Code snippet for importing and configuring the start_otel function to initialize tracing in a Python application. Includes commented examples of optional configuration settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splunk_otel.tracing import start_otel\n\nstart_otel()\n\n# Also accepts optional settings. For example:\n#\n# start_tracing(\n#   service_name='<my-python-service>',\n#   span_exporter_factories=[OTLPSpanExporter]\n#   access_token='<access_token>',\n#   max_attr_length=12000,\n#   trace_response_header_enabled=True,\n#   resource_attributes={\n#    'service.version': '<your_version>',\n#    'deployment.environment': '<your_environment>',\n#  })\n```\n\n----------------------------------------\n\nTITLE: Creating a Meter in Python with OpenTelemetry\nDESCRIPTION: Creates a meter object in Python using OpenTelemetry. Includes commented-out code that doesn't match the surrounding context.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmeter := otel.Meter(\"ExampleService\")\n```\n\n----------------------------------------\n\nTITLE: Configuring AlwaysOn Profiling in Python\nDESCRIPTION: Example code showing how to activate the profiler programmatically with custom configuration options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/instrument-python-application.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splunk_otel.profiling import start_profiling\n\n# Activates CPU profiling\n# All arguments are optional\nstart_profiling(\n   service_name='my-python-service', \n   resource_attributes={\n      'service.version': '3.1'\n      'deployment.environment': 'production', \n   }\n   endpoint='http://localhost:4317'\n)\n```\n\n----------------------------------------\n\nTITLE: Declaring Syslog Receiver in OpenTelemetry Collector Configuration (YAML)\nDESCRIPTION: Demonstrates how to enable the syslog receiver by adding it to the 'receivers' section of the OpenTelemetry Collector configuration file. This is a minimal configuration with default settings. No parameters are specified for the receiver, indicating all defaults apply. This snippet requires the Splunk Distribution of the OpenTelemetry Collector and should be placed within a collector's YAML configuration document. The syslog receiver activation is essential for ingesting syslogs via TCP or UDP as further specified elsewhere in the configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/syslog-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  syslog:\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Tracer with OpenTelemetry - Go\nDESCRIPTION: Obtains a tracer instance for the service \"ExampleService\" using the OpenTelemetry API in Go. The created tracer can be used to create spans for distributed tracing. The tracer's argument (service name) helps identify the component or service originating the trace data. No dependencies beyond the OpenTelemetry library are required.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/go-manual-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: go\nCODE:\n```\ntracer := otel.Tracer(\"ExampleService\")\n```\n\n----------------------------------------\n\nTITLE: Integrating Logging Exporter into Telemetry Pipelines using YAML\nDESCRIPTION: This YAML snippet shows an example of configuring the logging exporter within multiple pipelines (traces, metrics, logs) in the OpenTelemetry Collector. Each pipeline specifies receivers, processors, and exporters, where logging is included as one of the enabled exporters. This setup allows for debugging and diagnosis across different telemetry streams, assuming the proper receivers and processors are available. Inputs include the list of receivers/processors/exporters; output is enhanced logging for specified pipelines.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/logging-exporter.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      receivers: [jaeger, otlp, zipkin]\n      processors:\n      - memory_limiter\n      - batch\n      - resourcedetection\n      exporters: [otlphttp, signalfx, logging]\n    metrics:\n      receivers: [hostmetrics, otlp, signalfx]\n      processors: [memory_limiter, batch, resourcedetection]\n      exporters: [signalfx, logging]\n    logs:\n      receivers: [fluentforward, otlp]\n      processors:\n      - memory_limiter\n      - batch\n      - resourcedetection\n      exporters: [splunk_hec, logging]\n```\n\n----------------------------------------\n\nTITLE: Configuring Gateway Collector YAML for Agent Data Reception\nDESCRIPTION: YAML configuration for the gateway collector showing how to set up receivers, exporters and service pipelines to receive data from agent collectors. Includes settings for handling traces, metrics, and internal metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployment-modes.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n   http_forwarder:\n      egress:\n         endpoint: \"https://api.${SPLUNK_REALM}.signalfx.com\"\n\nreceivers:\n   otlp:\n      protocols:\n         grpc:\n         http:\n   signalfx:\n\nexporters:\n   otlphttp:\n      access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n      endpoint: \"https://ingest.${SPLUNK_REALM}.signalfx.com/v2/trace/otlp\"\n   signalfx:\n      access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n      realm: \"${SPLUNK_REALM}\"\n   signalfx/internal:\n      access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n      realm: \"${SPLUNK_REALM}\"\n      sync_host_metadata: true\n\nservice:\n   extensions: [http_forwarder]\n   pipelines:\n      traces:\n         receivers: [otlp]\n         processors:\n         - memory_limiter\n         - batch\n         exporters: [otlphttp]\n      metrics:\n         receivers: [otlp]\n         processors: [memory_limiter, batch]\n         exporters: [signalfx]\n      metrics/internal:\n         receivers: [prometheus/internal]\n         processors: [memory_limiter, batch, resourcedetection/internal]\n         exporters: [signalfx/internal]\n```\n\n----------------------------------------\n\nTITLE: Configuring Go Instrumentation in Kubernetes Deployment (YAML)\nDESCRIPTION: Example Kubernetes Deployment YAML configuration showing how to inject necessary OpenTelemetry environment variables into a container running a Go application. It uses the Downward API (`fieldRef: status.hostIP`) to dynamically set the Splunk OpenTelemetry Agent address for `OTEL_EXPORTER_OTLP_ENDPOINT`. It also sets `OTEL_SERVICE_NAME` and `OTEL_RESOURCE_ATTRIBUTES`. Replace placeholders like `<serviceName>` and `<environmentName>` with actual values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/instrument-go-application.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nspec:\n  selector:\n    matchLabels:\n      app: your-application\n  template:\n    spec:\n      containers:\n        - name: myapp\n          env:\n            - name: SPLUNK_OTEL_AGENT\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: OTEL_EXPORTER_OTLP_ENDPOINT\n              value: \"http://$(SPLUNK_OTEL_AGENT):4317\"\n            - name: OTEL_SERVICE_NAME\n              value: \"<serviceName>\"\n            - name: OTEL_RESOURCE_ATTRIBUTES\n              value: \"deployment.environment=<environmentName>\"\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector for Observability Cloud in Bash\nDESCRIPTION: Helm command to install the Splunk OpenTelemetry Collector configured to send data to Splunk Observability Cloud, specifying realm, access token, and cluster name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install my-splunk-otel-collector --set=\"splunkObservability.realm=us0,splunkObservability.accessToken=xxxxxx,clusterName=my-cluster\" splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Attributes Environment Variable (Linux)\nDESCRIPTION: Sets the OTEL_RESOURCE_ATTRIBUTES environment variable in a Linux bash shell. This optional variable allows specifying additional resource attributes like deployment environment and service version, which are attached to telemetry data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_RESOURCE_ATTRIBUTES='deployment.environment=<envtype>,service.version=<version>'\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenTelemetry for Isolated Worker Process Azure Function in C#\nDESCRIPTION: This code snippet demonstrates how to initialize OpenTelemetry in a .NET isolated worker process Azure function. It sets up the tracer provider, configures HTTP client instrumentation, adds resource detectors, and sets up the OTLP exporter to send spans to Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/azure/instrument-azure-functions-dotnet.rst#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Hosting;\nusing OpenTelemetry;\nusing OpenTelemetry.Exporter;\nusing OpenTelemetry.ResourceDetectors.Azure;\nusing OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\nusing System.Diagnostics;\n\n// Get environment variables from function configuration\n// You need a valid Splunk Observability Cloud access token and realm\nvar serviceName = Environment.GetEnvironmentVariable(\"WEBSITE_SITE_NAME\") ?? \"Unknown\";\nvar accessToken = Environment.GetEnvironmentVariable(\"SPLUNK_ACCESS_TOKEN\")?.Trim();\nvar realm = Environment.GetEnvironmentVariable(\"SPLUNK_REALM\")?.Trim();\n\nArgumentNullException.ThrowIfNull(accessToken, \"SPLUNK_ACCESS_TOKEN\");\nArgumentNullException.ThrowIfNull(realm, \"SPLUNK_REALM\");\n\nvar tp = Sdk.CreateTracerProviderBuilder()\n   // Use Add[instrumentation-name]Instrumentation to instrument missing services\n   // Use Nuget to find different instrumentation libraries\n   .AddHttpClientInstrumentation(opts =>\n   {\n      // This filter prevents background (parent-less) http client activity\n      opts.FilterHttpWebRequest = req => Activity.Current?.Parent != null;\n      opts.FilterHttpRequestMessage = req => Activity.Current?.Parent != null;\n   })\n   // Use AddSource to add your custom DiagnosticSource source names\n   //.AddSource(\"My.Source.Name\")\n   // Creates root spans for function executions\n   .AddSource(\"Microsoft.Azure.Functions.Worker\")\n   .SetSampler(new AlwaysOnSampler())\n   .ConfigureResource(configure => configure\n      .AddService(serviceName: serviceName, serviceVersion: \"1.0.0\")\n      // See https://github.com/open-telemetry/opentelemetry-dotnet-contrib/tree/main/src/OpenTelemetry.ResourceDetectors.Azure\n      // for other types of Azure detectors\n      .AddDetector(new AppServiceResourceDetector()))\n   .AddOtlpExporter(opts =>\n   {\n      opts.Endpoint = new Uri($\"https://ingest.{realm}.signalfx.com/v2/trace/otlp\");\n      opts.Protocol = OtlpExportProtocol.HttpProtobuf;\n      opts.Headers = $\"X-SF-TOKEN={accessToken}\";\n   })\n   .Build();\n\nvar host = new HostBuilder()\n   .ConfigureFunctionsWorkerDefaults()\n   .ConfigureServices(services => services.AddSingleton(tp))\n   .Build();\n\nhost.Run();\n```\n\n----------------------------------------\n\nTITLE: Formatting Ruby Logger with Trace Correlation Data\nDESCRIPTION: This snippet demonstrates how to configure the Ruby standard logger to include trace metadata in log statements. It uses the Splunk::Otel::Logging.format_correlation function to add service name, trace ID, and span ID to each log line.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nlogger.formatter = proc do |severity, datetime, progname, msg|  \n   \"#{Splunk::Otel::Logging.format_correlation} #{msg}\\n\"\nend\n```\n\n----------------------------------------\n\nTITLE: Diagram of Back-end Instrumentation\nDESCRIPTION: This snippet provides a mermaid diagram illustrating how back-end application and serverless functions interact with Splunk Observability Cloud. It shows the flow of traces, metrics, and logs from back-end applications to the Splunk Distribution of OpenTelemetry Collector and further to the Splunk APM, including an alternative path via API directly to Splunk APM.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/application.rst#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n\n      accTitle: Back-end instrumentation diagram \n      accDescr: OpenTelemetry instrumentation encompasses back-end applications and serverless functions. Instrumentation sends back-end application metrics, traces, and logs to the Splunk Distribution of OpenTelemetry Collector, which sends them to Splunk APM. Instrumentation also sends back-end application metrics, traces, and logs to Splunk APM through the API. Instrumentation sends serverless function metrics, traces, and logs to Splunk APM using the API.\n\n      subgraph \"\\nOpenTelemetry instrumentation\"\n\n\n      A[\"Back-end applications\n      (Go, Python, Ruby, ...)\"]\n      B[\"Serverless functions \n      (AWS Lambda, Azure, GCP)\"]\n      end\n\n      A -- \"traces, metrics, logs\" --> O\n\n      O[\"Splunk Distribution of \n      OpenTelemetry Collector\"]\n      O --> M[\"Splunk APM\"]\n\n      A -- \"traces, metrics, logs (API)\" --> M\n      B -- \"traces, metrics, logs (API)\" --> M\n```\n\n----------------------------------------\n\nTITLE: Setting Splunk Environment Variables in Windows PowerShell\nDESCRIPTION: Configuration of environment variables in Windows PowerShell to enable direct data transmission to Splunk Observability Cloud. Requires setting access token and realm variables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_18\n\nLANGUAGE: powershell\nCODE:\n```\n$env:SPLUNK_ACCESS_TOKEN=<access_token>\n$env:SPLUNK_REALM=<realm>\n```\n\n----------------------------------------\n\nTITLE: Running Splunk OpenTelemetry Collector Docker Container with Inline Configuration\nDESCRIPTION: Docker command to run the Splunk OpenTelemetry Collector container with an inline YAML configuration. Uses a heredoc to define the configuration and passes it as an environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nCONFIG_YAML=$(cat <<-END\nreceivers:\n   hostmetrics:\n      collection_interval: 1s\n      scrapers:\n         cpu:\nexporters:\n   debug:\n      # Can be changed to info\n      verbosity: detailed\nservice:\n   pipelines:\n      metrics:\n         receivers: [hostmetrics]\n         exporters: [logging]\nEND\n)\n\ndocker run --rm \\\n    -e SPLUNK_CONFIG_YAML=${CONFIG_YAML} \\\n    --name otelcol quay.io/signalfx/splunk-otel-collector:latest\n```\n\n----------------------------------------\n\nTITLE: Enabling the Resource Processor in Telemetry Pipelines (YAML)\nDESCRIPTION: This YAML configuration snippet demonstrates how to enable the resource processor within different telemetry pipelines (metrics, logs, and traces) in the 'service' section of the Collector configuration. It activates the processor by specifying 'resource' in each pipeline's 'processors' list. This ensures that resource attribute manipulation, as defined in the processor configuration, is applied to all telemetry types. The snippet assumes prior definition of the 'resource' processor. No additional dependencies are required beyond the Collector installation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/resource-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      processors: [resource]\n    logs:\n      processors: [resource]\n    traces:\n      processors: [resource]\n```\n\n----------------------------------------\n\nTITLE: Configuring Filter Processors for Metrics and Logs in YAML\nDESCRIPTION: This snippet demonstrates how to configure filter processors for including and excluding metrics and logs based on various criteria such as metric names and host names.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filter-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  filter/includemetrics:\n    metrics:\n        # Drop nonmatching metrics from the pipeline\n        include:\n          match_type: strict\n          metric_names:\n            - good_metric\n            - great_metric\n  filter/excludemetrics:\n    metrics:\n      # Drop matching metrics from the pipeline\n      exclude:\n        match_type: strict\n        metric_names:\n          - a_metric\n          - another_metric\n          - a_third_metric\n  filter/mixedlogs:\n    logs:\n       # Include filters are applied before exclude filters\n       include:\n         match_type: strict\n         record_attributes:\n           - key: host.name\n             value: \"(host1|anotherhost2)\"\n       exclude:\n         match_type: strict\n         record_attributes:\n           - key: host.name\n             value: wrong_host_.*\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector on Linux\nDESCRIPTION: This bash command downloads and runs the installer script for the Splunk OpenTelemetry Collector. It requires setting environment variables for the Splunk realm, memory allocation, and access token.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh;\nsudo sh /tmp/splunk-otel-collector.sh --realm $SPLUNK_REALM --memory $SPLUNK_MEMORY_TOTAL_MIB -- $SPLUNK_ACCESS_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Downloading Splunk OpenTelemetry Java Agent with PowerShell\nDESCRIPTION: This PowerShell command downloads the latest JAR file of the Splunk OpenTelemetry Java agent using Invoke-WebRequest. It's essential for setting up the Java agent for application instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/instrument-java-application.rst#2025-04-22_snippet_1\n\nLANGUAGE: PowerShell\nCODE:\n```\nInvoke-WebRequest -Uri https://github.com/signalfx/splunk-otel-java/releases/latest/download/splunk-otel-javaagent.jar -OutFile splunk-otel-javaagent.jar\n```\n\n----------------------------------------\n\nTITLE: Advanced Prometheus Scrape Configuration\nDESCRIPTION: This YAML snippet demonstrates an advanced Prometheus scrape configuration in the OpenTelemetry Collector. It includes specifying static targets, using Kubernetes service discovery, and applying relabeling filters. Relabelling can modify targets and labels before scraping begins. Dynamic configurations are made possible through Prometheus service discovery mechanisms.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/prometheus-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n   receivers:\n     prometheus:\n       config:\n         scrape_configs:\n           - job_name: 'otel-collector'\n             scrape_interval: 5s\n             static_configs:\n               - targets: ['0.0.0.0:8888']\n           - job_name: k8s\n             kubernetes_sd_configs:\n             - role: pod\n             relabel_configs:\n             - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n               regex: \"true\"\n               action: keep\n             metric_relabel_configs:\n             - source_labels: [__name__]\n               regex: \"(request_duration_seconds.*|response_duration_seconds.*)\"\n               action: keep\n```\n\n----------------------------------------\n\nTITLE: Initializing and Cleaning Up OpenTelemetry Tracer in C++\nDESCRIPTION: Provides C++ code to initialize the OpenTelemetry tracing system within an application's `main` function. It sets up an OTLP HTTP exporter pointing to a local collector (`http://localhost:4318/v1/traces`), configures a batch span processor, and sets the global tracer provider. Helper functions `InitTracer` and `CleanupTracer` manage the lifecycle. Depends on the OpenTelemetry C++ SDK and exporter libraries.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/cpp/instrument-cpp.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"opentelemetry/exporters/ostream/span_exporter_factory.h\"\n#include \"opentelemetry/sdk/trace/exporter.h\"\n#include \"opentelemetry/sdk/trace/processor.h\"\n#include \"opentelemetry/sdk/trace/simple_processor_factory.h\"\n#include \"opentelemetry/sdk/trace/tracer_provider_factory.h\"\n#include \"opentelemetry/trace/provider.h\"\n\nusing namespace std;\nnamespace trace_api = opentelemetry::trace;\nnamespace trace_sdk = opentelemetry::sdk::trace;\nnamespace trace_exporter = opentelemetry::exporter::trace;\nnamespace otlp = opentelemetry::exporter::otlp;\n\nnamespace {\n    void InitTracer() {\n        trace_sdk::BatchSpanProcessorOptions bspOpts{};\n        // creates a new options object and sets the OTLP endpoint URL\n        otlp::OtlpHttpExporterOptions opts;\n        opts.url = \"http://localhost:4318/v1/traces\";\n\n        // pass the options object as an argument for the exporter creator\n        auto exporter = otlp::OtlpHttpExporterFactory::Create(opts);\n        auto processor = trace_sdk::BatchSpanProcessorFactory::Create(std::move(exporter), bspOpts);\n        std::shared_ptr<trace_api::TracerProvider> provider = trace_sdk::TracerProviderFactory::Create(std::move(processor));\n        trace_api::Provider::SetTracerProvider(provider);\n    }\n\n    void CleanupTracer() {\n        std::shared_ptr<opentelemetry::trace::TracerProvider> none;\n        trace_api::Provider::SetTracerProvider(none);\n    }\n}\n\nint main() {\n    InitTracer();\n\n    // Other application code\n\n    CleanupTracer();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Starting a Span with OpenTelemetry in Go\nDESCRIPTION: This snippet demonstrates starting a new span using an OpenTelemetry tracer's Start method. It takes a context, a span name (matching the previous operationName concept), and option parameters for span configuration. The span and new context are returned for further scoped instrumentation. This approach unifies context propagation and span parenting, replacing previous SignalFx conventions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/migrate-signalfx-go-to-otel.rst#2025-04-22_snippet_5\n\nLANGUAGE: go\nCODE:\n```\n   ctx, span := tracer.Start(ctx, \"BusinessOperation\", /* options ... */)\n```\n\n----------------------------------------\n\nTITLE: Formatting Log Messages with Trace Metadata in Pino - JSON\nDESCRIPTION: This snippet demonstrates a sample JSON-formatted log message from a Node.js application using the Pino logging library with Splunk OpenTelemetry JS trace metadata injection enabled. Key trace attributes (trace_id, span_id, trace_flags) and resource attributes (service.name, service.version, service.environment) can be included depending on configuration. The input is a log event; the output is a JSON log string containing structured tracing fields for easy correlation in Splunk. This format requires Pino and the Splunk OpenTelemetry JS distribution to be integrated and configured in the application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"level\":30,\n  \"time\":1979374615686,\n  \"pid\":728570,\n  \"hostname\":\"my_host\",\n  \"trace_id\":\"f8e261432221096329baf5e62090d856\",\n  \"span_id\":\"3235afe76b55fe51\",\n  \"trace_flags\":\"01\",\n  \"url\":\"/lkasd\",\n  \"msg\":\"request handler\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Trace Sampling in Splunk OTel JS\nDESCRIPTION: This JavaScript snippet demonstrates how to configure trace sampling to reduce the span volume by dropping spans named 'unwanted'. It depends on '@splunk/otel' and '@opentelemetry/sdk-trace-base' packages. The function shouldSample determines the sampling decision either to record or not based on the span name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/performance.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require(\"@splunk/otel\");\nconst { SamplingDecision } = require(\"@opentelemetry/sdk-trace-base\");\n\nstart({\ntracing: {\n   tracerConfig: {\n      sampler: {\n        shouldSample: (context, traceId, spanName, spanKind, attributes, links) => {\n           if (spanName ===  \"unwanted\") {\n              return { decision: SamplingDecision.NOT_RECORD };\n           }\n\n           return { decision: SamplingDecision.RECORD };\n        },\n        toString: () => return \"CustomSampler\",\n      }\n   },\n},\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Attribute Manipulation with Resource Processor in OpenTelemetry Collector (YAML)\nDESCRIPTION: This YAML snippet defines the configuration for the resource processor, which manipulates resource attributes in trace, metric, or log telemetry within the Collector. It demonstrates adding or updating an attribute, copying an attribute's value, and deleting an attribute, using the 'attributes' list. Dependencies include having the Splunk Distribution of OpenTelemetry Collector installed, and the processor must be activated in the configuration. Key parameters are 'key' for the attribute name, 'value' or 'from_attribute' for the attribute value source, and 'action' for the type of operation. The snippet must be included in the 'processors' section to function. Expected input is a YAML configuration, and output is the manipulation of resource attributes at runtime.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/resource-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nresource:\n  attributes:\n  # Adds the attribute overriding existing values\n  - key: cloud.availability_zone\n    value: zone-1\n    action: upsert\n  # Copies the value of an attribute into another  \n  - key: k8s.cluster.name\n    from_attribute: k8s-cluster\n    action: insert\n   # Removes an attribute \n  - key: redundant-attribute\n    action: delete\n```\n\n----------------------------------------\n\nTITLE: Instrumenting NodeJS Azure Function with OpenTelemetry\nDESCRIPTION: This code snippet demonstrates how to instrument a NodeJS Azure function using OpenTelemetry. It includes setting up a tracer, defining the function logic, and using an instrumentation wrapper to generate a root span for Azure Functions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/azure/instrument-azure-functions-nodejs.rst#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { app, HttpRequest, HttpResponseInit, InvocationContext } from \"@azure/functions\";\nimport { trace, Span } from \"@opentelemetry/api\";\n\nconst tracer = trace.getTracer('splunk-example-azure', '0.1.0');\n\nexport async function myhttptrigger(request: HttpRequest, context: InvocationContext): Promise<HttpResponseInit> {\n   context.log(`Http function processed request for url \"${request.url}\"`);\n\n   const response = // run your function logic here.\n\n   return { body: `Hello, ${response}!` };\n};\n\n// Universal wrapper method that helps to generate root span for Azure Functions\nexport const instrumentationWrapper = <T extends (...args: Parameters<T>) => Promise<Awaited<ReturnType<T>>>>(func: T) =>\n   async (...args: Parameters<T>): Promise<Awaited<ReturnType<T>>> => {\n      \n      let result: Promise<Awaited<ReturnType<T>>>;\n      let functionName = func.name;\n   \n      await tracer.startActiveSpan(functionName, async (span: Span) => {\n            \n            // setup custom attributes for root span, specific to your Azure Functions.\n            span.setAttribute(\"foo\", 1);\n            span.setAttribute(\"bar\", \"Hello World!\");\n            span.setAttribute(\"baz\", [1, 2, 3])\n   \n            result = await func(...args)\n   \n            span.end();\n         });\n      \n      return result;\n   }\n\napp.http('myhttptrigger', {\n   methods: ['GET', 'POST'],\n   authLevel: 'anonymous',\n   handler: instrumentationWrapper(myhttptrigger)\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Span Attributes in Go using OpenTelemetry SDK\nDESCRIPTION: Demonstrates creating a new trace span with initial attributes and adding further attributes after creation using the OpenTelemetry Go SDK. It utilizes `tracer.Start` with `trace.WithAttributes` and `span.SetAttributes`. The OTEL_RESOURCE_ATTRIBUTES environment variable for global tags is also mentioned.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/add-context-trace-span.rst#2025-04-22_snippet_4\n\nLANGUAGE: go\nCODE:\n```\nimport (\n   // ...\n   \"go.opentelemetry.io/otel\"\n)      \n\nfunc myFunc(ctx context.Context) {\n\n   // Create a named tracer\n   tracer := otel.Tracer(\"example.com/myFunc\")\n\n   // Create a span with custom attributes\n   ctx, span = tracer.Start(ctx, \"attributesAtCreation\", trace.WithAttributes(attribute.String(\"hello\", \"splunk\")))\n   defer span.End()\n\n   // Add attributes after creation\n   span.SetAttributes(attribute.Bool(\"isTrue\", true), attribute.String(\"stringAttr\", \"Hello there!\"))\n\n   // Other activities\n}\n\n// You can also set global tags using the OTEL_RESOURCE_ATTRIBUTES\n// environment variable, which accepts a list of comma-separated key-value\n// pairs. For example, key1:val1,key2:val2.\n```\n\n----------------------------------------\n\nTITLE: Example Server Trace Response Headers (HTTP)\nDESCRIPTION: Illustrates the format of HTTP response headers added by the agent instrumentation to link RUM requests with server traces. It includes `Access-Control-Expose-Headers` to expose `Server-Timing`, and the `Server-Timing` header itself containing the trace context (`traceparent` format). This feature is active by default.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/configuration/advanced-java-otel-configuration.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nAccess-Control-Expose-Headers: Server-Timing\nServer-Timing: traceparent;desc=\"00-<serverTraceId>-<serverSpanId>-01\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Splunk OpenTelemetry SDK in Go\nDESCRIPTION: This snippet illustrates initializing the Splunk OpenTelemetry SDK using the distro.Run() function and correctly shutting it down when the application exits. It replaces the deprecated tracing.Start usage from SignalFx. You must import the distro package, ensure Go 1.18+, and handle potential errors during SDK startup and shutdown. The key parameter is the returned sdk, which must be shut down to release resources and flush trace data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/migrate-signalfx-go-to-otel.rst#2025-04-22_snippet_0\n\nLANGUAGE: go\nCODE:\n```\n   sdk, err := distro.Run()\n   if err != nil {\n      panic(err)\n   }\n   defer func() {\n      // A context with a deadline can be passed here instead if needed\n      if err := sdk.Shutdown(context.Background()); err != nil {\n         panic(err)\n      }\n   }()\n   /* ... */\n```\n\n----------------------------------------\n\nTITLE: Instrumenting HTTP Server for Server Trace Information in Go\nDESCRIPTION: Provides a Go code example for instrumenting a standard `net/http` server to include server trace information in HTTP response headers. It utilizes `splunkhttp` and `otelhttp` middleware handlers after initializing the Splunk OTel distribution (`distro.Run()`) to enable correlation between RUM data and backend traces.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/configuration/advanced-go-otel-configuration.rst#2025-04-22_snippet_4\n\nLANGUAGE: go\nCODE:\n```\npackage main\n\nimport (\n   \"net/http\"\n   \"github.com/signalfx/splunk-otel-go/distro\"\n   \"github.com/signalfx/splunk-otel-go/instrumentation/net/http/splunkhttp\"\n   \"go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp\"\n)\n\nfunc main() {\n   distro.Run()\n   var handler http.Handler = http.HandlerFunc(\n      func(w http.ResponseWriter, r *http.Request) {\n         w.Write([]byte(\"Hello\"))\n      }\n   )\n   handler = splunkhttp.NewHandler(handler)\n   handler = otelhttp.NewHandler(handler, \"my-service\")\n   http.ListenAndServe(\":9090\", handler)\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Int64Counter Instrument with OpenTelemetry - Go\nDESCRIPTION: Creates an Int64Counter named \"business.requests.count\" using the meter, providing metrics for business logic. It specifies a unit and a description for the metric. Handles errors as required by the instrument creation API. Requires the OpenTelemetry Go metric package and a previously created meter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/go-manual-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: go\nCODE:\n```\n\tcounter, err := meter.Int64Counter(\n  \t\"business.requests.count\",\n\tmetric.WithUnit(\"{request}\"),\n\tmetric.WithDescription(\"The numer of business requests.\"),\n)\nif err != nil {\n\t// handle error (e.g. log it)\n}\n```\n\n----------------------------------------\n\nTITLE: Upgrading Splunk OpenTelemetry Python Package with pip\nDESCRIPTION: Command to upgrade the Splunk Distribution of OpenTelemetry Python to version 2.0 using pip package manager.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/migration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade splunk-opentelemetry\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Span Tracing in JavaScript\nDESCRIPTION: Examples of creating and managing spans for form processing and navigation events using OpenTelemetry tracer\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nasync function processForm(form) {\n   const span = tracer.startSpan('process form');\n   \n   // Wait for processing to be done\n   span.end();\n}\n\n// Example of a callback function\nfunction markCompleted(item) {\n   const span = tracer.startSpan('item complete');\n\n   processCompletion(item, function() {\n      // ... Update item display\n      span.end();\n   });\n}\n\n// Example of hook system provided by another library\nrouter.beforeEach((transition) => {\n   transition.span = tracer.startSpan('navigate', {\n      attributes: {\n         'router.path': transition.path\n      }\n   });\n});\n\nrouter.afterEach((transition) => {\n   if (transition.span) {\n      transition.span.end();\n   }\n});\n```\n\n----------------------------------------\n\nTITLE: Activating Python Application Instrumentation\nDESCRIPTION: Example showing how to prefix Python commands with opentelemetry-instrument to enable instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/instrument-python-application.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 main.py --port=8000\n```\n\nLANGUAGE: bash\nCODE:\n```\nopentelemetry-instrument python3 main.py --port=8000\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variables in Prometheus Configuration\nDESCRIPTION: This YAML configuration snippet illustrates employing environment variables within the Prometheus receiver definition in the OpenTelemetry Collector. The ${<var>} syntax allows injecting dynamic values into the configuration. Modify existing configurations by replacing '$' with '$$' to prevent misinterpretation as environment variables by the Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/prometheus-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n   prometheus:\n     config:\n       scrape_configs:\n         - job_name: ${JOBNAME}\n           scrape_interval: 5s\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Deployment for Node.js Instrumentation\nDESCRIPTION: Example Kubernetes Deployment YAML snippet demonstrating how to configure environment variables for the Splunk OTel Node.js instrumentation. It uses the Downward API (`fieldRef: status.hostIP`) to set the Collector agent's IP and defines `OTEL_EXPORTER_OTLP_ENDPOINT`, `OTEL_SERVICE_NAME`, and `OTEL_RESOURCE_ATTRIBUTES`. The container's command is modified to run the application with the instrumentation agent preloaded (`node -r @splunk/otel/instrument`).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nspec:\n  selector:\n     matchLabels:\n        app: your-application\n  template:\n     spec:\n        containers:\n        - name: myapp\n          image: your-app-image\n          env:\n           - name: SPLUNK_OTEL_AGENT\n             valueFrom:\n               fieldRef:\n                 fieldPath: status.hostIP\n           - name: OTEL_EXPORTER_OTLP_ENDPOINT\n             value: \"http://$(SPLUNK_OTEL_AGENT):4317\"\n           - name: OTEL_SERVICE_NAME\n             value: \"<serviceName>\"\n           - name: OTEL_RESOURCE_ATTRIBUTES\n             value: \"deployment.environment=<environmentName>\"\n          command:\n           - node\n           - -r @splunk/otel/instrument\n           - <your-app>.js\n```\n\n----------------------------------------\n\nTITLE: Defining vCenter Receiver with Credentials and Advanced Options in YAML\nDESCRIPTION: This comprehensive YAML snippet configures the vCenter receiver with endpoint, username, password (using an environment variable), collection interval, and disables a specific metric. It must be placed under the 'receivers' section, and requires valid access credentials and a reachable vCenter/ESXi endpoint. Adjust the collection interval for larger vCenter environments. Inputs are credential strings and intervals; password supports environment variables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/vcenter-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nvcenter:\n  endpoint: http://vcsa.host.localnet\n  username: otelu\n  password: ${env:VCENTER_PASSWORD}\n  collection_interval: 5m\n  metrics:\n    vcenter.host.cpu.utilization:\n      enabled: false\n```\n\n----------------------------------------\n\nTITLE: Initializing Programmatic Instrumentation (JavaScript)\nDESCRIPTION: Shows the basic setup for programmatically instrumenting a Node.js application. It requires the `start` function from `@splunk/otel` and calls it early in the application's entry point script, providing configuration like `serviceName` and `endpoint`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require('@splunk/otel');\n\nstart({\n   serviceName: 'my-node-service',\n   endpoint: 'http://localhost:4317'\n});\n\n// Rest of your main module\n```\n\n----------------------------------------\n\nTITLE: Setting Deployment Environment and Service Version in Linux\nDESCRIPTION: Configures resource attributes for deployment environment and service version in Linux, which help categorize and filter your application data in Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_RESOURCE_ATTRIBUTES='deployment.environment=<envtype>,service.version=<version>'\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Pipeline in YAML\nDESCRIPTION: Example configuration showing a traces pipeline with multiple receivers, processors, and exporters. The pipeline demonstrates how to set up data collection from OTLP, Jaeger, and Zipkin sources, process them using memory limiter and batch processors, and export to multiple destinations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/data-processing.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp, jaeger, zipkin]\n      processors: [memory_limiter, batch]\n      exporters: [otlp, splunk_hec, jaeger, zipkin]\n```\n\n----------------------------------------\n\nTITLE: Multi-Container Pod Instrumentation YAML for Multiple Languages\nDESCRIPTION: Kubernetes deployment manifest showing how to instrument multiple containers with different languages (Java and Node.js) in the same pod using language-specific container-names annotations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment-with-multi-containers-multi-instrumentations\nspec:\n```\n\n----------------------------------------\n\nTITLE: Configuring TracerProvider in Python for Manual Tracing\nDESCRIPTION: Sets up a TracerProvider with a BatchSpanProcessor and ConsoleSpanExporter for manual tracing in Python applications without auto-instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprovider = TracerProvider()\nprocessor = BatchSpanProcessor(ConsoleSpanExporter())\nprovider.add_span_processor(processor)\n\ntrace.set_tracer_provider(provider)\ntracer = trace.get_tracer(\"tracer.name\")\n```\n\n----------------------------------------\n\nTITLE: Running Python Application with Splunk OTel Instrumentation\nDESCRIPTION: Activates the Splunk OpenTelemetry Python agent by prefixing the Python command with splunk-py-trace. This enables automatic instrumentation of your application at runtime.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nsplunk-py-trace python3 main.py --port=8000\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct Data Export to Splunk Observability Cloud (Linux Bash)\nDESCRIPTION: Sets environment variables in Bash for the Splunk OpenTelemetry Java agent to send trace data directly to Splunk Observability Cloud, bypassing the default local OTel Collector. Requires substituting `<access_token>` with a valid Splunk access token and `<realm>` with the correct Splunk realm. It configures the OTLP endpoint, protocol, and disables the logs exporter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/instrument-java-application.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n      export SPLUNK_ACCESS_TOKEN=<access_token>\n      export OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=http/protobuf\n      export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://ingest.<realm>.signalfx.com/v2/trace/otlp\n      export OTEL_LOGS_EXPORTER=none\n```\n\n----------------------------------------\n\nTITLE: Disabling Log Collection for Splunk Observability Cloud in YAML\nDESCRIPTION: This configuration snippet demonstrates how to turn off log collection for Splunk Observability Cloud while preserving AlwaysOn Profiling data for APM by setting the log_data_enabled option to false.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-exporter.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsplunk_hec/profiling:\n  token: \"${SPLUNK_HEC_TOKEN}\"\n  endpoint: \"${SPLUNK_HEC_URL}\"\n  source: \"otel\"\n  sourcetype: \"otel\"\n  log_data_enabled: false\n```\n\n----------------------------------------\n\nTITLE: Activating Debug Logging - Splunk OpenTelemetry Java\nDESCRIPTION: This snippet activates debug logging by passing the runtime argument or setting an environment variable, enabling more verbose output from the Java agent to aid in troubleshooting.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/common-java-troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n[otel.javaagent 2023-05-09 15:22:40:172 +0200] [main] DEBUG io.opentelemetry.javaagent.tooling.VersionLogger - Running on Java 17.0.2. JVM OpenJDK 64-Bit Server VM - Eclipse Adoptium - 17.0.2+8\n[otel.javaagent 2023-05-09 15:22:40:264 +0200] [main] DEBUG io.opentelemetry.sdk.internal.JavaVersionSpecific - Using the APIs optimized for: Java 9+\n```\n\n----------------------------------------\n\nTITLE: Initializing Tracer and Creating Custom Event (Browser - NPM) in JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates initializing an OpenTelemetry tracer using the '@opentelemetry/api' package (via NPM) and creating a custom timed event (span). It starts a span named 'test.module.load', adds a 'workflow.name' attribute essential for UI visibility in Splunk RUM, simulates a time passage, and then ends the span to record its duration. This is used for monitoring specific operations within a web application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/RUM-custom-events.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport {trace} from '@opentelemetry/api'\n\nconst tracer = trace.getTracer('appModuleLoader');\nconst span = tracer.startSpan('test.module.load', {\nattributes: {\n      'workflow.name': 'test.module.load'\n}\n});\n// time passes\nspan.end();\n```\n\n----------------------------------------\n\nTITLE: Activating Splunk AlwaysOn Profiling Programmatically (Python)\nDESCRIPTION: This Python code snippet shows how to activate Splunk AlwaysOn CPU Profiling directly within an application using the `start_profiling` function from the `splunk_otel.profiling` library. It demonstrates setting optional parameters like `service_name`, `resource_attributes` (for service version and environment), and the OTLP `endpoint`. This serves as an alternative to enabling profiling via the `SPLUNK_PROFILER_ENABLED` environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/profiling/get-data-in-profiling.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom splunk_otel.profiling import start_profiling\n\n# Activates CPU profiling\n# All arguments are optional\nstart_profiling(\n   service_name='my-python-service',\n   resource_attributes={\n      'service.version': '3.1'\n      'deployment.environment': 'production',\n   }\n   endpoint='http://localhost:4317'\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing ActivitySource for Custom .NET Traces (C#)\nDESCRIPTION: Creates a static `ActivitySource` instance named \"Examples.ManualInstrumentations.Registered\". This source acts as a factory for generating custom spans (`Activity` objects) within the application. The name must be registered using the `OTEL_DOTNET_AUTO_TRACES_ADDITIONAL_SOURCES` environment variable for the auto-instrumentation agent to collect spans from this source.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/manual-dotnet-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nprivate static readonly ActivitySource RegisteredActivity = new ActivitySource(\"Examples.ManualInstrumentations.Registered\");\n```\n\n----------------------------------------\n\nTITLE: Creating and Tagging a Custom Span (.NET Activity) (C#)\nDESCRIPTION: Starts a new custom span (`Activity`) named \"Custom Span Name\" using an `ActivitySource`. The code includes a conditional check (`activity?.IsAllDataRequested`) to ensure that tags (like \"foo\": \"bar1\") and other potentially expensive operations are only performed when the span is actively being sampled and recorded by the OpenTelemetry pipeline. The `using` block guarantees the span's duration is correctly measured.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/manual-dotnet-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\nusing (var activity = RegisteredActivity.StartActivity(\"Custom Span Name\"))\n{\n   // Check if the activity is sampled and if full data collection is enabled.\n   // This ensures that tags and other custom attributes are only set when the activity is being recorded.\n   // Note: Ensure that skipping logic based on sampling does not interfere with essential business operations.\n   if(activity?.IsAllDataRequested)\n   {\n      // your logic for custom activity\n      activity.SetTag(\"foo\", \"bar1\");\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Exporter in .NET\nDESCRIPTION: Configures the OpenTelemetry OTLP exporter with endpoint, protocol, and authentication settings for Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/azure/instrument-azure-functions-dotnet.rst#2025-04-22_snippet_2\n\nLANGUAGE: csharp\nCODE:\n```\n.AddOtlpExporter(opts =>\n{\n   opts.Endpoint = new Uri($\"https://ingest.{realm}.signalfx.com/v2/trace/otlp\");\n   opts.Protocol = OtlpExportProtocol.HttpProtobuf;\n   opts.Headers = $\"X-SF-TOKEN={accessToken}\";\n})\n.Build();\n```\n\n----------------------------------------\n\nTITLE: Creating and Recording Metrics with OpenTelemetry Metrics API (JavaScript)\nDESCRIPTION: This snippet demonstrates the OpenTelemetry approach to creating and recording custom metrics in Node.js using 'metrics.getMeter', 'createObservableGauge', and 'createCounter' methods. It replaces equivalent SignalFx client operations during migration. 'cpu' is recorded as an observable gauge, while 'clicks' is incremented as a counter. Requires '@opentelemetry/api-metrics' installed and a correctly initialized OpenTelemetry environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/nodejs-otel-metrics.rst#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n// OpenTelemetry\nconst meter = metrics.getMeter('my-meter');\nmeter.createObservableGauge('cpu', result => {\n   result.observe(42);\n});\nconst counter = meter.createCounter('clicks');\ncounter.add(99);\n```\n\n----------------------------------------\n\nTITLE: Validity Check of AlwaysOn Profiling Activation - Java Agent\nDESCRIPTION: Logs output from the Java agent indicating the activation of AlwaysOn Profiling, marked by an INFO level message stating 'JFR profiler is active'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/common-java-troubleshooting.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n[otel.javaagent 2021-09-28 18:17:04:246 +0000] [main] INFO com.splunk.opentelemetry.profiler.JfrActivator - JFR profiler is active.\n```\n\n----------------------------------------\n\nTITLE: Setting and Retrieving Global Attributes in JavaScript for Browser RUM\nDESCRIPTION: This snippet demonstrates how to use setGlobalAttributes to add attributes to all new spans, and how to retrieve those attributes using getGlobalAttributes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-api-reference.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nSplunkRum.setGlobalAttributes({\n  'enduser.id': 'Test User',\n});\n// All new spans now include enduser.id\n\nSplunkRum.setGlobalAttributes({\n  'dark_mode.enabled': darkModeToggle.status,\n});\n// All new spans now include enduser.id and dark_mode.enabled\n\nSplunkRum.setGlobalAttributes()\n// New spans no longer include those attributes\n\nconst attrs = SplunkRum.getGlobalAttributes();\n/* console.log(attrs)\n{\n  'enduser.id': 'Test User',\n  'dark_mode.enabled': true\n}\n*/\n```\n\n----------------------------------------\n\nTITLE: Including Cumulative to Delta Processor in Service Pipeline\nDESCRIPTION: This YAML configuration snippet demonstrates how to include the cumulative to delta processor in the service pipeline of the OpenTelemetry Collector. It shows the processor being added to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/cumulative-to-delta-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      processors: [cumulativetodelta]\n```\n\n----------------------------------------\n\nTITLE: Configuring Export to Splunk Observability Cloud via Environment Variables - Shell\nDESCRIPTION: This shell snippet sets environment variables to configure the OpenTelemetry exporter to send trace data directly to the Splunk Observability Cloud. 'OTEL_EXPORTER_OTLP_TRACES_HEADERS' is used for passing the access token, and 'OTEL_EXPORTER_OTLP_ENDPOINT' for specifying the ingest endpoint URL based on the user's realm. Inputs are the secret access token and realm value. Output is direct export of trace data to Splunk without passing through a Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/instrument-php-application.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nOTEL_EXPORTER_OTLP_TRACES_HEADERS=x-sf-token=<access_token>\\nOTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.<realm>.signalfx.com/trace/otlp\n```\n\n----------------------------------------\n\nTITLE: Defining Resource Attributes at Runtime using Java Agent Properties - Shell\nDESCRIPTION: This snippet demonstrates setting the 'mdc.resource-attributes' system property for the OpenTelemetry Java agent to expose resource attributes (such as service name and deployment environment) into the MDC context, which can then be used by application logging frameworks. No dependencies beyond the Splunk OpenTelemetry Java agent are required. The property should be set during application startup, and the specified attributes become available for log formatting templates.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n-Dotel.instrumentation.common.mdc.resource-attributes=service.name,deployment.environment\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Agent in Kubernetes Deployment\nDESCRIPTION: This YAML and Docker snippet demonstrates how to edit a Dockerfile and Kubernetes deployment configuration to include and expose the environment variables necessary for the OpenTelemetry Java agent within a Kubernetes deployment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/instrument-java-application.rst#2025-04-22_snippet_6\n\nLANGUAGE: docker\nCODE:\n```\nADD https://github.com/signalfx/splunk-otel-java/releases/latest/download/splunk-otel-javaagent.jar .\n      ENTRYPOINT [\"java\",\"-javaagent:splunk-otel-javaagent.jar\",\"-jar\",\"./<myapp>.jar\"]\n```\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\n      kind: Deployment\n      spec:\n      selector:\n         matchLabels:\n            app: your-application\n      template:\n         spec:\n            containers:\n            - name: myapp\n               env:\n                  - name: SPLUNK_OTEL_AGENT\n                  valueFrom:\n                     fieldRef:\n                        fieldPath: status.hostIP\n                  - name: OTEL_EXPORTER_OTLP_ENDPOINT\n                  value: \"http://$(SPLUNK_OTEL_AGENT):4318\"\n```\n\n----------------------------------------\n\nTITLE: Manual Span Creation with OpenTelemetry in Go\nDESCRIPTION: This snippet demonstrates manual span creation using the OpenTelemetry Go API after migration. It initializes a tracer, configures span options for attributes and type, and starts a new span associated with a context. The span is ended with span.End() in a deferred manner. Required dependencies include opentelemetry-go, and key parameters include the operation name, attributes (such as client), and the context for parent-child relationships. The expected input is a context and attribute values; the output is a trace span for observability.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/migrate-signalfx-go-to-otel.rst#2025-04-22_snippet_3\n\nLANGUAGE: go\nCODE:\n```\n   func BusinessOperation(ctx context.Context, client string) {\n      tracer := otel.Tracer(\"app-name\")\n      opts := []trace.SpanStartOption{\n         trace.WithAttributes(attribute.String(\"client\", client)),\n         trace.WithSpanKind(trace.SpanKindInternal),\n      }\n      ctx, span := tracer.Start(ctx, \"BusinessOperation\", opts...)\n      defer span.End()\n      /* ... */\n   }\n```\n\n----------------------------------------\n\nTITLE: Fluentd Log Collection Configuration\nDESCRIPTION: YAML configuration to enable Fluentd as the log collection engine in Kubernetes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-logs.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nlogsEngine: fluentd\n```\n\n----------------------------------------\n\nTITLE: Setting Service Name Environment Variable (Shell)\nDESCRIPTION: Sets the `OTEL_SERVICE_NAME` environment variable, which is crucial for identifying your application or service within Splunk Observability Cloud. Replace `<yourServiceName>` with the actual name of your service. Examples provided for both Linux (bash) and Windows PowerShell.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/instrument-go-application.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_SERVICE_NAME=<yourServiceName>\n```\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_SERVICE_NAME=<yourServiceName>\n```\n\n----------------------------------------\n\nTITLE: React Error Boundary Integration\nDESCRIPTION: Implementation of error boundary component in React with Splunk RUM integration\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nimport React from 'react';\nimport SplunkRum from '@splunk/otel-web';\n\nclass ErrorBoundary extends React.Component {\n   componentDidCatch(error, errorInfo) {\n// To avoid loading issues due to content blockers\n// when using the CDN version of the Browser RUM\n// agent, add if (window.SplunkRum) checks around\n// SplunkRum API calls\n      SplunkRum.error(error, errorInfo)\n   }\n\n   // Rest of your error boundary component\n   render() {\n      return this.props.children\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Registering OpenTelemetry for Windows Service\nDESCRIPTION: PowerShell command to register OpenTelemetry instrumentation for a Windows service. This configures the service to be monitored with the specified service name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n# Set up your Windows Service instrumentation\nRegister-OpenTelemetryForWindowsService -WindowsServiceName \"<your-windows-service-name>\" -OTelServiceName \"<your-OTel-service-name>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring a Single OracleDB Receiver in Splunk OTel Collector (YAML)\nDESCRIPTION: This YAML snippet declares a single oracledb receiver under the receivers section, providing its datasource connection string. The datasource should follow Go Oracle driver syntax (as noted in the comment). Required for collecting Oracle Database metrics. Replace placeholder values with actual database connection details. This must be included and referenced in the metrics pipeline to activate the receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/oracledb-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nreceivers:\\n  oracledb:\\n    # Refer to Oracle Go Driver go_ora documentation for full connection string options\\n    datasource: \"oracle://<username>:<password>@<host>:<port>/<database>\"\n```\n\n----------------------------------------\n\nTITLE: Filtering Logs Using Resource Attributes and OTTL Conditions in YAML\nDESCRIPTION: This configuration demonstrates various ways to filter logs using resource attributes, severity levels, and OpenTelemetry Transformation Language (OTTL) conditions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filter-processor.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nfilter/mixed:\n  logs:\n    include:\n      match_type: strict\n      resource_attributes:\n        - key: should_include\n          value: \"true\"\n    exclude:\n      match_type: regexp\n      resource_attributes:\n        - key: host.name\n          value: banned_host_.*\n\nfilter/severity:\n  logs:\n    exclude:\n      match_type: strict\n      severity_texts:\n        - \"DEBUG\"\n        - \"DEBUG2\"\n        - \"DEBUG3\"\n        - \"DEBUG4\"\n    include:\n      match_type: regexp\n      severity_texts:\n        - \"INFO[2-4]?\"\n\nfilter/recordattributes:\n  logs:\n    exclude:\n      match_type: strict\n      record_attributes:\n        - key: should_exclude\n          value: \"true\"\n\nfilter/includeexclude:\n  logs:\n    include:\n      severity_number:\n        min: \"INFO\"\n        match_undefined: true\n   exclude:\n      severity_number:\n        min: \"ERROR\"\n\nfilter/ottl:\n  logs:\n    log_record:\n      - 'attributes[\"test\"] == \"pass\"'\n```\n\n----------------------------------------\n\nTITLE: Initializing Tracing and Creating Custom Spans in Node.js\nDESCRIPTION: Shows how to initialize the Splunk OpenTelemetry Node.js distribution using `@splunk/otel`'s `start` function, obtain a tracer instance from `@opentelemetry/api`, and create a custom trace span named 'make-random'. The example demonstrates starting an active span, adding a custom attribute ('random-result'), and ending the span. It includes a simple function `randomNumber` that encapsulates this logic and is called periodically using `setInterval`. Dependencies include `@splunk/otel` and `@opentelemetry/api`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/manual-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require('@splunk/otel');\nconst opentelemetry = require('@opentelemetry/api');\n\nstart({\n  serviceName: 'my-service',\n});\n\nconst tracer = opentelemetry.trace.getTracer('example-app', '0.1.0');\n\nfunction randomNumber() {\n  return tracer.startActiveSpan('make-random', (span) => {\n    const result = Math.random() * 42;\n    span.setAttribute('random-result', result);\n    span.end();\n    return result;\n  });\n}\n\nsetInterval(() => {\n  console.log(randomNumber());\n}, 1000);\n```\n\n----------------------------------------\n\nTITLE: Running Node.js Application with Automatic Instrumentation (bash)\nDESCRIPTION: Executes a Node.js application (<your-app.js>) using the node command, preloading the Splunk OpenTelemetry instrumentation agent via the '-r @splunk/otel/instrument' flag. This enables automatic instrumentation based on environment variables and default settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nnode -r @splunk/otel/instrument <your-app.js>\n```\n\n----------------------------------------\n\nTITLE: Configuring Redaction Processor in YAML for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration snippet demonstrates the main settings for the Redaction processor. It includes options for allowing all keys, specifying allowed and ignored keys, defining blocked values using regular expressions, and setting the summary verbosity level.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/redaction-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  redaction:\n    # allow_all_keys is a flag which when set to true, which can disables the\n    # allowed_keys list. The list of blocked_values is applied regardless. If\n    # you just want to block values, set this to true.\n    allow_all_keys: false\n    # allowed_keys is a list of span attribute keys that are kept on the span and\n    # processed. The list is designed to fail closed. If allowed_keys is empty,\n    # no span attributes are allowed and all span attributes are removed. To\n    # allow all keys, set allow_all_keys to true.\n    allowed_keys:\n      - description\n      - group\n      - id\n      - name\n    # Ignore the following attributes, allow them to pass without redaction.\n    # Any keys in this list are allowed so they don't need to be in both lists.\n    ignored_keys:\n      - safe_attribute\n    # blocked_values is a list of regular expressions for blocking values of\n    # allowed span attributes. Values that match are masked\n    blocked_values:\n      - \"4[0-9]{12}(?:[0-9]{3})?\" ## Visa credit card number\n      - \"(5[1-5][0-9]{14})\"       ## MasterCard number\n    # summary controls the verbosity level of the diagnostic attributes that\n    # the processor adds to the spans when it redacts or masks other\n    # attributes. In some contexts a list of redacted attributes leaks\n    # information, while it is valuable when integrating and testing a new\n    # configuration. Possible values:\n    # - `debug` includes both redacted key counts and names in the summary\n    # - `info` includes just the redacted key counts in the summary\n    # - `silent` omits the summary attributes\n    summary: debug\n```\n\n----------------------------------------\n\nTITLE: Setting Service Name via Environment Variable (Linux Shell)\nDESCRIPTION: Demonstrates how to set the OpenTelemetry service name (`OTEL_SERVICE_NAME`) using an environment variable in a Linux shell environment. This name identifies the service being instrumented within Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/configuration/advanced-java-otel-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_SERVICE_NAME=my-java-app\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Start Function in JavaScript\nDESCRIPTION: This snippet configures the `start` function to set up general options for the Splunk Distribution of OpenTelemetry JS. It covers configuring metrics, profiling, and tracing using a JavaScript object. No external libraries other than `@splunk/otel` are needed specifically for this setup.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/advanced-nodejs-otel-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nstart({\n   // general options like `serviceName` and `endpoint`\n   metrics: {\n      // configuration passed to metrics signal\n   },\n   profiling: {\n      // configuration passed to profiling signal\n   },\n   tracing: {\n      // configuration passed to tracing signal\n   },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Splunk (Linux Bash)\nDESCRIPTION: This Bash snippet sets up environment variables to enable direct data transmission to Splunk Observability Cloud. Required dependencies are an active access token and the appropriate realm. Key parameters include SPLUNK_ACCESS_TOKEN for authentication and SPLUNK_REALM for specifying the Splunk endpoint. No outputs are generated directly by this configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/instrumentation/instrument-nodejs-application.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport SPLUNK_ACCESS_TOKEN=<access_token>\\nexport SPLUNK_REALM=<realm>\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Pipelines for Span Metrics Connector\nDESCRIPTION: YAML configuration for including the Span Metrics connector in the service pipelines. It shows how to set up the connector as an exporter in the traces pipeline and as a receiver in the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/span-metrics-connector.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      receivers: [nop]\n      exporters: [spanmetrics]\n    metrics:\n      receivers: [spanmetrics]\n      exporters: [nop]\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct Data Export to Splunk Observability Cloud (Windows PowerShell)\nDESCRIPTION: Sets environment variables using PowerShell syntax for the Splunk OpenTelemetry Java agent to send trace data directly to Splunk Observability Cloud, bypassing the default local OTel Collector. Requires substituting `<access_token>` with a valid Splunk access token and `<realm>` with the correct Splunk realm. It configures the OTLP endpoint, protocol, and disables the logs exporter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/instrument-java-application.rst#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n      $env:SPLUNK_ACCESS_TOKEN=<access_token>\n      $env:OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=http/protobuf\n      $env:OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://ingest.<realm>.signalfx.com/v2/trace/otlp\n      $env:OTEL_LOGS_EXPORTER=none\n```\n\n----------------------------------------\n\nTITLE: Configuring Separate Log Pipelines for Splunk Platform and Observability Cloud in YAML\nDESCRIPTION: This configuration example demonstrates how to set up separate log pipelines for Splunk Platform logs and AlwaysOn Profiling data for Splunk Observability Cloud. It includes receiver and exporter configurations for both pipelines.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-exporter.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  # Default OTLP receiver--used by Splunk platform logs\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n      http:\n        endpoint: 0.0.0.0:4318\n   # OTLP receiver for AlwaysOn Profiling data\n  otlp/profiling:\n    protocols:\n      grpc:\n      # Make sure to configure your agents\n      # to use the custom port for logs when\n      # setting SPLUNK_PROFILER_LOGS_ENDPOINT\n        endpoint: 0.0.0.0:4319\n\nexporters:\n  # Export logs to Splunk platform\n  splunk_hec/platform:\n    token: \"<splunk_token>\"\n    endpoint: \"https://splunk:8088/services/collector\"\n    source: \"otel\"\n    sourcetype: \"otel\"\n    index: \"main\"\n    disable_compression: false\n    timeout: 10s\n    tls:\n      insecure_skip_verify: true\n   # Export profiling data to Splunk Observability Cloud\n  splunk_hec/profiling:\n    token: \"<splunk_o11y_token>\"\n    endpoint: \"https://ingest.<realm>.signalfx.com/v1/log\"\n    source: \"otel\"\n    sourcetype: \"otel\"\n    log_data_enabled: false\n\nprocessors:\n  batch:\n  memory_limiter:\n    check_interval: 2s\n    limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}\n\n# Other settings\n\nservice:\n  pipelines:\n    # Traces and metrics pipelines\n    # Logs pipeline for Splunk platform\n    logs/platform:\n      receivers: [fluentforward, otlp]\n      processors:\n      - memory_limiter\n      - batch\n      exporters: [splunk_hec/platform]\n     # Logs pipeline for AlwaysOn Profiling\n    logs/profiling:\n      receivers: [otlp/profiling]\n      processors:\n      - memory_limiter\n      - batch\n      exporters: [splunk_hec/profiling]\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Metric Readers and Exporters in Node.js\nDESCRIPTION: Illustrates how to provide custom metric readers and exporters when initializing Splunk OpenTelemetry Node.js metrics using the `metricReaderFactory` setting. This example configures both a `PrometheusExporter` and a `PeriodicExportingMetricReader` which uses an `OTLPMetricExporter` configured for OTLP over HTTP. Note that using `metricReaderFactory` overrides the default `exportInterval` and `endpoint` configurations. Dependencies include `@splunk/otel`, `@opentelemetry/exporter-prometheus`, `@opentelemetry/exporter-metrics-otlp-http`, and `@opentelemetry/sdk-metrics-base`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/manual-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require('@splunk/otel');\nconst { PrometheusExporter } = require('@opentelemetry/exporter-prometheus');\nconst { OTLPMetricExporter } = require('@opentelemetry/exporter-metrics-otlp-http');\nconst { PeriodicExportingMetricReader } = require('@opentelemetry/sdk-metrics-base');\n\nstart({\n  serviceName: 'my-service',\n  metrics: {\n    metricReaderFactory: () => {\n      return [\n        new PrometheusExporter(),\n        new PeriodicExportingMetricReader({\n          exportIntervalMillis: 1000,\n          exporter: new OTLPMetricExporter({ url: 'http://localhost:4318' })\n        })\n      ]\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Browser RUM via Self-hosted Script\nDESCRIPTION: HTML snippet for loading a self-hosted Browser RUM agent script and initializing it with required configuration parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/install-rum-browser.rst#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"http://example.domain/path/splunk-otel-web.js\"></script>\n<script>\n   SplunkRum.init({\n      realm: '<realm>',\n      rumAccessToken: '<your_rum_token>',\n      applicationName: '<your_app_name>',\n      version: '<your_app_version>',\n      deploymentEnvironment: '<your_environment_name>'\n   });\n</script>\n```\n\n----------------------------------------\n\nTITLE: Configuring Pod Association Rules\nDESCRIPTION: Example of configuring pod association rules for the Kubernetes attributes processor, demonstrating different source types.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-attributes-processor.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\npod_association:\n  - sources:\n    - from: resource_attribute\n      name: k8s.pod.name\n    - from: resource_attribute\n      name: k8s.namespace.name\n```\n\nLANGUAGE: yaml\nCODE:\n```\npod_association:\n  - sources:\n    - from: resource_attribute\n      name: ip\n  - sources:\n    - from: connection\n```\n\n----------------------------------------\n\nTITLE: Disabling Log Data in Splunk Distribution of OpenTelemetry Collector Configuration\nDESCRIPTION: Basic configuration showing how to disable log data collection in the Splunk Distribution of OpenTelemetry Collector. This setting is available in version 0.49.0 and higher.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-exporter.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsource: \"otel\"\nsourcetype: \"otel\"\nlog_data_enabled: false\n```\n\n----------------------------------------\n\nTITLE: Setting Cluster Receiver Resource Limits\nDESCRIPTION: YAML configuration for setting resource limits on the cluster receiver deployment. Suitable for clusters with approximately 100 nodes, allocating 1 CPU and 2Gi memory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-sizing.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nclusterReceiver:\n  resources:\n    limits:\n      cpu: 1\n      memory: 2Gi\n```\n\n----------------------------------------\n\nTITLE: Starting OpenTelemetry Collector with Environment Variables\nDESCRIPTION: Command to start the collector using SPLUNK_REALM and SPLUNK_ACCESS_TOKEN environment variables with a specified config file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nSPLUNK_REALM=<realm> SPLUNK_ACCESS_TOKEN=<token> <download dir>/otelcol_<platform>_<arch> --config=<path to config file>\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Windowsperfcounters Receivers with Different Intervals - YAML\nDESCRIPTION: This configuration defines multiple windowsperfcounters receivers, each tailored to different performance counters (e.g., memory and processor) and collection intervals. Such an approach allows collecting specific counters at frequencies matching their volatility. Metric definitions include both active and idle processor time, with state attributes and support for configuration of object instances. Dependencies: OpenTelemetry Collector on Windows. Outputs: separate metric streams for memory and processor, each with customized intervals.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/windowsperfcounters-receiver.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  windowsperfcounters/memory:\n    metrics:\n      bytes.committed:\n        description: Number of bytes committed to memory\n        unit: By\n        gauge:\n    collection_interval: 30s\n    perfcounters:\n      - object: Memory\n        counters:\n          - name: Committed Bytes\n            metric: bytes.committed\n\n  windowsperfcounters/processor:\n    collection_interval: 1m\n    metrics:\n      processor.time:\n        description: CPU active and idle time\n        unit: \"%\"\n        gauge:\n    perfcounters:\n      - object: \"Processor\"\n        instances: \"*\"\n        counters:\n          - name: \"% Processor Time\"\n            metric: processor.time\n            attributes:\n              state: active\n      - object: \"Processor\"\n        instances: [\"0\", \"1\"]\n        counters:\n          - name: \"% Idle Time\"\n            metric: processor.time\n            attributes:\n              state: idle\n\n  # ...\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [windowsperfcounters/memory, windowsperfcounters/processor]\n```\n\n----------------------------------------\n\nTITLE: Applying Splunk OpenTelemetry Initialization in Program.cs (C#)\nDESCRIPTION: This C# snippet demonstrates how to invoke the `AddSplunkOpenTelemetry` extension method within the application's `Program.cs` file. It integrates the Splunk OpenTelemetry configuration, defined in the helper class, into the .NET `WebApplicationBuilder` pipeline before the application is built and run. This ensures telemetry instrumentation is active from application startup. Requires the `SplunkOpenTelemetry` helper class to be defined and accessible.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/azure-webapps.rst#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar builder = WebApplication.CreateBuilder(args);\nvar app = builder\n    .AddSplunkOpenTelemetry()\n    .Build()\n```\n\n----------------------------------------\n\nTITLE: Configuring OkHttp Client for RUM\nDESCRIPTION: Example showing how to instrument OkHttp client using the Call.Factory wrapper for RUM monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/configure-rum-android-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nprivate Call.Factory buildOkHttpClient(SplunkRum splunkRum) {\n   return splunkRum.createRumOkHttpCallFactory(new OkHttpClient());\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct Export to Splunk Observability Cloud (Shell)\nDESCRIPTION: Sets the environment variables required to send telemetry data directly to the Splunk Observability Cloud ingest endpoints, bypassing the need for a local OpenTelemetry Collector. Requires a Splunk `access_token` and the correct `realm`. The `OTEL_METRICS_EXPORTER=none` variable (shown for Linux) disables metrics export, potentially useful if only traces are needed or metrics are configured separately. Examples provided for Linux (bash) and Windows PowerShell.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/instrument-go-application.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport SPLUNK_ACCESS_TOKEN=<access_token>\nexport SPLUNK_REALM=<realm>\nexport OTEL_METRICS_EXPORTER=none\n```\n\nLANGUAGE: shell\nCODE:\n```\n$env:SPLUNK_ACCESS_TOKEN=<access_token>\n$env:SPLUNK_REALM=<realm>\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Monitor in Splunk OpenTelemetry Collector\nDESCRIPTION: YAML configuration example showing how to activate the MongoDB integration in the Splunk Distribution of OpenTelemetry Collector by adding the smartagent/mongodb receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/mongodb.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/mongodb:\n    type: collectd/mongodb\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry SDK in Ruby\nDESCRIPTION: Creates an initializer file to configure the OpenTelemetry SDK and activate all instrumentations. This configuration is essential for capturing telemetry data from the Ruby application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/instrument-ruby.rst#2025-04-22_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire 'opentelemetry/sdk'\nrequire 'opentelemetry/instrumentation/all'\nOpenTelemetry::SDK.configure do |c|\n    c.use_all() # activates all instrumentation\nend\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Log Filtering in Kubernetes Agent\nDESCRIPTION: Example of overriding default agent configuration to exclude logs from specific pods using a custom filter processor and pipeline configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  config:\n    processors:\n      # Exclude logs from pods named 'podNameX'\n      filter/exclude_logs_from_pod:\n        logs:\n          exclude:\n            match_type: regexp\n            resource_attributes:\n              - key: k8s.pod.name\n                value: '^(podNameX)$'\n    # Define the logs pipeline with the default values as well as your new processor component\n    service:\n      pipelines:\n        logs:\n          processors:\n            - memory_limiter\n            - k8sattributes\n            - filter/logs\n            - batch\n            - resourcedetection\n            - resource\n            - resource/logs\n            - filter/exclude_logs_from_pod\n```\n\n----------------------------------------\n\nTITLE: Activating Java Profiler via JVM System Properties - bash\nDESCRIPTION: Starts a Java application with profiling enabled by passing required system properties to the JVM. Enables both CPU and memory profiling and sets OTLP exporter endpoint. Requires splunk-otel-javaagent.jar on the filesystem and the proper application JAR. Inputs: agent JAR, property flags, target application; Output: Application launches with profiling data sent to specified endpoint. Must run on supported JVM and OTLP Collector versions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/profiling/get-data-in-profiling.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\njava -javaagent:./splunk-otel-javaagent.jar \\\n-Dsplunk.profiler.enabled=true \\\n-Dsplunk.profiler.memory.enabled=true \\\n-Dotel.exporter.otlp.endpoint=http(s)://collector:4317 \\\n-jar <your_application>.jar\n```\n\n----------------------------------------\n\nTITLE: Installing Zero-Code Instrumentation with System-wide Coverage\nDESCRIPTION: Bash command to install the Splunk OpenTelemetry Collector with system-wide zero-code instrumentation for all supported languages. Requires realm and access token parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-instrumentation --realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Checking Fluentd Capabilities on Linux\nDESCRIPTION: This command checks the activated capabilities for the Fluentd (td-agent) Ruby binary. It's used to verify if necessary capabilities like dac_override and dac_read_search are set.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-config-logs.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo /opt/td-agent/bin/fluent-cap-ctl --get -f /opt/td-agent/bin/ruby\n```\n\n----------------------------------------\n\nTITLE: Integrating Android RUM with Browser RUM in WebViews\nDESCRIPTION: Code snippet showing how to integrate Android RUM with Splunk Browser RUM in WebViews by sharing session IDs between both instrumentations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_10\n\nLANGUAGE: kotlin\nCODE:\n```\nimport android.webkit.WebView\nimport com.splunk.rum.SplunkRum\n\n//...\n/*\nMake sure that the WebView instance only loads pages under\nyour control and instrumented with Splunk Browser RUM. The\nintegrateWithBrowserRum() method can expose the splunk.rumSessionId\nof your user to every site/page loaded in the WebView instance.\n*/\noverride fun onViewCreated(view: View, @Nullable savedInstanceState: Bundle?) {\n   super.onViewCreated(view, savedInstanceState)\n   binding.webView.setWebViewClient(LocalContentWebViewClient(assetLoader))\n   binding.webView.loadUrl(\"https://subdomain.example.com/instrumented-page.html\")\n   binding.webView.getSettings().setJavaScriptEnabled(true)\n   binding.webView.addJavascriptInterface(WebAppInterface(getContext()), \"Android\")\n   SplunkRum.getInstance().integrateWithBrowserRum(binding.webView)\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Runtime Metrics Collection with Splunk OpenTelemetry JS (JavaScript)\nDESCRIPTION: This snippet demonstrates how to activate runtime metrics in a Node.js application by importing the 'start' method from the '@splunk/otel' package and setting 'runtimeMetricsEnabled' to true within the 'metrics' configuration object. The start method initializes OpenTelemetry instrumentation to collect and export memory and event loop metrics. 'serviceName' is a required parameter identifying the application, and '@splunk/otel' must be installed as a dependency. Outputs are exported metrics as described in the document; no explicit input/output in the snippet.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/nodejs-otel-metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require('@splunk/otel');\n\nstart({\n   serviceName: 'my-service',\n   metrics: {\n     runtimeMetricsEnabled: true,\n   }\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring ActiveMQ Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration example showing how to activate the ActiveMQ integration in the OpenTelemetry Collector configuration file. Includes the receiver setup and pipeline configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-messaging/apache-activemq.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/activemq:\n    type: collectd/activemq\n    ...  # Additional config\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/activemq]\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Collector Validation Error Output\nDESCRIPTION: Command-line output showing the validation error that occurs when the YAML configuration has indentation issues. The error indicates that 'syslog' is an invalid key.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-troubleshoot.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nError: failed to get config: cannot unmarshal the configuration: 1 error(s) decoding:\n\n* '' has invalid keys: syslog\n2024/02/19 18:17:36 main.go:89: application run finished with error: failed to get config: cannot unmarshal the configuration: 1 error(s) decoding\n```\n\n----------------------------------------\n\nTITLE: Creating a New Metric from an Existing One (YAML)\nDESCRIPTION: This configuration example demonstrates how to use the `metricstransform` processor to create a new metric (`host.cpu.utilization`) by inserting a copy of an existing metric (`host.cpu.usage`). It specifies the source metric using `include`, the action `insert`, and the desired `new_name` for the created metric.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# create host.cpu.utilization from host.cpu.usage\ninclude: host.cpu.usage\naction: insert\nnew_name: host.cpu.utilization\noperations:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Creating a New Metric Matching Specific Label Values (YAML)\nDESCRIPTION: This example shows how to configure the `metricstransform` processor to create a new metric (`host.cpu.utilization`) from an existing metric (`host.cpu.usage`), but only for data points that have a specific label value (`container=my_container`). It uses `action: insert`, `new_name`, `match_type: strict`, and the experimental `experimental_match_labels` field for filtering.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# create host.cpu.utilization from host.cpu.usage where we have metric label \\\"container=my_container\\\"\ninclude: host.cpu.usage\naction: insert\nnew_name: host.cpu.utilization\nmatch_type: strict\nexperimental_match_labels: {\\\"container\\\": \\\"my_container\\\"}\noperations:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Running OpenTelemetry Collector Windows binary with command-line options\nDESCRIPTION: PowerShell commands to set up environment variables and run the OpenTelemetry Collector binary. This snippet shows how to view available options, set required environment variables, and start the collector with configuration options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows-manual.rst#2025-04-22_snippet_1\n\nLANGUAGE: PowerShell\nCODE:\n```\n# see available command-line options\nPS> & '<download dir>\\otelcol_windows_amd64.exe' --help\nUsage of otelcol:\n    --config string          Locations to the config file(s), note that only a single location can be set per flag entry e.g. --config=/path/to/first --config=path/to/second. (default \"[]\")\n    --feature-gates string   Comma-delimited list of feature gate identifiers. Prefix with '-' to disable the feature. '+' or no prefix will enable the feature. (default \"[]\")\n    --no-convert-config      Do not translate old configurations to the new format automatically. By default, old configurations are translated to the new format for backward compatibility.\n    --set string             Set arbitrary component config property. The component has to be defined in the config file and the flag has a higher precedence. Array config properties are overridden and maps are joined. Example --set=processors.batch.timeout=2s (default \"[]\")\n    -v, --version                Version of the collector.\n\n# set the SPLUNK_REALM and SPLUNK_ACCESS_TOKEN env vars required in our default config files\nPS> $env:SPLUNK_REALM = \"<realm>\"\nPS> $env:SPLUNK_ACCESS_TOKEN = \"<token>\"\n\n# start the collector\nPS> & '<download dir>\\otelcol_windows_amd64.exe' --config=<path to config file>\n\n# alternatively, use the SPLUNK_CONFIG env var instead of the --config command-line option\nPS> $env:SPLUNK_CONFIG = \"<path to config file>\"\nPS> & '<download dir>\\otelcol_windows_amd64.exe'\n\n# type Ctrl-c to stop the collector\n```\n\n----------------------------------------\n\nTITLE: Activating Kubelet Stats Receiver in OpenTelemetry Collector (YAML)\nDESCRIPTION: Adds the kubeletstats receiver to the receivers section in the OpenTelemetry Collector's configuration. This enables metrics collection from Kubernetes nodes via the kubelet API. No additional dependencies are required beyond the Collector itself. Expects YAML and should be included in the higher-level configuration under 'receivers'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubelet-stats-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Python Package\nDESCRIPTION: Installs the Splunk OpenTelemetry Python package with all dependencies using pip. This package provides automatic instrumentation capabilities for Python applications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"splunk-opentelemetry[all]\"\n```\n\n----------------------------------------\n\nTITLE: Integrating OTLP/HTTP Exporter into Service Pipelines in YAML\nDESCRIPTION: This YAML configuration snippet demonstrates how to enable the configured `otlphttp` exporter by adding it to the `exporters` list within the `metrics` and `traces` pipelines under the `service` section of the OpenTelemetry Collector configuration. This step is necessary to ensure that processed metrics and traces are actually sent using the defined `otlphttp` exporter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlphttp-exporter.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      exporters: [otlphttp]\n    traces:\n      exporters: [otlphttp]\n```\n\n----------------------------------------\n\nTITLE: Initializing Splunk OpenTelemetry for .NET Azure Web App in C#\nDESCRIPTION: This C# code defines a static helper class `SplunkOpenTelemetry` with an extension method `AddSplunkOpenTelemetry` for `WebApplicationBuilder`. It configures OpenTelemetry tracing and metrics, reading Splunk credentials (access token, realm) and service name from environment variables (`SPLUNK_ACCESS_TOKEN`, `SPLUNK_REALM`, `WEBSITE_SITE_NAME`). It sets up instrumentation for ASP.NET Core, HTTP clients, runtime, and processes, and configures OTLP exporters to send data to specific Splunk Observability Cloud endpoints. It also includes logic to add the `Server-Timing` header for RUM integration based on the `SPLUNK_TRACE_RESPONSE_HEADER_ENABLED` environment variable. Requires OpenTelemetry NuGet packages and specific environment variables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/azure-webapps.rst#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nusing OpenTelemetry.Exporter;\nusing OpenTelemetry.Metrics;\nusing OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\nusing System.Diagnostics;\n\nnamespace <YourNamespaceHere>.Extensions;\n\npublic static class SplunkOpenTelemetry\n{\n    private static readonly string AccessToken;\n    private static readonly string Realm;\n\n    static SplunkOpenTelemetry()\n    {\n        // Get environment variables from function configuration\n        // You need a valid Splunk Observability Cloud access token and realm\n        AccessToken = Environment.GetEnvironmentVariable(\"SPLUNK_ACCESS_TOKEN\")?.Trim()\n            ?? throw new ArgumentNullException(\"SPLUNK_ACCESS_TOKEN\");\n\n        Realm = Environment.GetEnvironmentVariable(\"SPLUNK_REALM\")?.Trim()\n            ?? throw new ArgumentNullException(\"SPLUNK_REALM\");\n    }\n\n    public static WebApplicationBuilder AddSplunkOpenTelemetry(this WebApplicationBuilder builder)\n    {\n        var serviceName = Environment.GetEnvironmentVariable(\"WEBSITE_SITE_NAME\") ?? \"Unknown\";\n        var enableTraceResponseHeaderValue = Environment.GetEnvironmentVariable(\"SPLUNK_TRACE_RESPONSE_HEADER_ENABLED\")?.Trim();\n\n        builder.Services.AddOpenTelemetry()\n            .ConfigureResource(cfg => cfg\n                // See https://github.com/open-telemetry/opentelemetry-dotnet-contrib/tree/main/src/OpenTelemetry.Resources.Azure\n                // for other types of Azure detectors\n                .AddAzureAppServiceDetector()\n                .AddService(serviceName: serviceName, serviceVersion: \"1.0.0\"))\n            .WithTracing(t => t\n                // Use Add[instrumentation-name]Instrumentation to instrument missing services\n                // Use Nuget to find different instrumentation libraries\n                .AddHttpClientInstrumentation(opts =>\n                {\n                    // This filter prevents background (parent-less) http client activity\n                    opts.FilterHttpWebRequest = req => Activity.Current?.Parent != null;\n                    opts.FilterHttpRequestMessage = req => Activity.Current?.Parent != null;\n                })\n                .AddAspNetCoreInstrumentation(opts =>\n                {\n                    // Enables Splunk RUM integration when configuration contains SPLUNK_TRACE_RESPONSE_HEADER_ENABLED=True\n                    if (bool.TryParse(enableTraceResponseHeaderValue, out bool isEnabled) && isEnabled)\n                    {\n                        opts.EnrichWithHttpRequest = (activity, request) =>\n                        {\n                            var response = request.HttpContext.Response;\n                            ServerTimingHeader.SetHeaders(activity, response.Headers, (headers, key, value) =>\n                            {\n                                headers.TryAdd(key, value);\n                            });\n                        };\n                    }\n                })\n                // Use AddSource to add your custom DiagnosticSource source names\n                //.AddSource(\"My.Source.Name\")\n                .SetSampler(new AlwaysOnSampler())\n                .AddOtlpExporter(opts =>\n                {\n                    opts.Endpoint = new Uri($\"https://ingest.{Realm}.signalfx.com/v2/trace/otlp\");\n                    opts.Protocol = OtlpExportProtocol.HttpProtobuf;\n                    opts.Headers = $\"X-SF-TOKEN={AccessToken}\";\n                }))\n            .WithMetrics(m => m\n                // Use Add[instrumentation-name]Instrumentation to instrument missing services\n                // Use Nuget to find different instrumentation libraries\n                .AddAspNetCoreInstrumentation()\n                .AddHttpClientInstrumentation()\n                .AddRuntimeInstrumentation()\n                .AddProcessInstrumentation()\n                .AddOtlpExporter(opts =>\n                {\n                    opts.Endpoint = new Uri($\"https://ingest.{Realm}.signalfx.com/v2/datapoint/otlp\");\n                    opts.Headers = $\"X-SF-TOKEN={AccessToken}\";\n                }));\n\n        return builder;\n    }\n\n    private static class ServerTimingHeader\n    {\n        private const string Key = \"Server-Timing\";\n        private const string ExposeHeadersHeaderName = \"Access-Control-Expose-Headers\";\n\n        public static void SetHeaders<T>(Activity activity, T carrier, Action<T, string, string> setter)\n        {\n            setter(carrier, Key, ToHeaderValue(activity));\n            setter(carrier, ExposeHeadersHeaderName, Key);\n        }\n\n        private static string ToHeaderValue(Activity activity)\n        {\n            var sampled = ((int)activity.Context.TraceFlags).ToString(\"D2\");\n            return $\"traceparent;desc=\\\"00-{activity.TraceId}-{activity.SpanId}-{sampled}\\\"\";\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Metrics Using Names, Expressions, and OTTL Conditions in YAML\nDESCRIPTION: This configuration shows how to filter metrics using metric names, expressions, and OpenTelemetry Transformation Language (OTTL) conditions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filter-processor.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nfilter/mixed:\n  metrics:\n    # Include using metric names\n    include:\n      match_type: strict\n      metric_names:\n        - a_metric\n        - another_metric\n    # Exclude using regular expressions\n    exclude:\n      match_type: regexp\n      metric_names:\n        - prefix/.*\n        - prefix_.*\n        - .*/suffix\n        - .*_suffix\n        - .*/contains/.*\n        - .*_contains_.*\n        - full/name/match\n        - full_name_match\n\nfilter/expr:\n  metrics:\n    include:\n      match_type: expr\n      expressions:\n        - Label(\"label1\") == \"text\"\n        - HasLabel(\"label2\")\n\nfilter/ottl:\n  metrics:\n    metric:\n      - 'name == \"a_name\"'\n   datapoint:\n      - 'attributes[\"attributename\"] == \"value\"'\n```\n\n----------------------------------------\n\nTITLE: Registering and Recording Custom Meters with Micrometer API - Java\nDESCRIPTION: This Java snippet demonstrates how to create and register custom counters and timers using the Micrometer API within an application monitored by the Splunk OpenTelemetry Java Agent 1.x. Dependencies include the 'micrometer-core' library. Meters are created and registered against the global Micrometer registry to enable metric collection by the agent. Key components include the creation of a Counter and a Timer, registration with the global registry, and calls to increment or record metrics within application methods. Inputs and outputs depend on your metric recording logic; ensure the Micrometer classes are available at runtime.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/version1x/java-manual-instrumentation-1x.rst#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nclass MyClass {\\nCounter myCounter = Metrics.counter(\"my_custom_counter\");\\n  Timer myTimer = Timer.builder(\"my_custom_timer\").register(Metrics.globalRegistry);\\n\\n  int foo() {\\n    myCounter.increment();\\n    return myTimer.record(this::fooImpl);\\n  }\\n\\n  private int fooImpl() {\\n     // ...\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenTelemetry for Splunk in Azure WebJobs (.NET/C#)\nDESCRIPTION: Defines a static helper class and attribute to initialize OpenTelemetry metrics and tracing for Azure WebJobs in .NET. The code sets up resource providers, sampling, exporters (including OTel Protocol export to Splunk), and an attribute for function-level activity tracking. Required NuGet dependencies include OpenTelemetry core, exporters, resource detectors, and instrumentation libraries. Inputs are environment variables for Splunk access token and realm; outputs are metrics and traces pushed to Splunk via OTLP endpoints. The approach isolates configuration and ensures every Azure WebJob Function can be traced and monitored, but does require correct environment variable setup and function attribute decoration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/azure-webjobs.rst#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nusing Microsoft.Azure.WebJobs;\nusing Microsoft.Azure.WebJobs.Host;\nusing Microsoft.Extensions.DependencyInjection;\nusing OpenTelemetry.Exporter;\nusing OpenTelemetry.Logs;\nusing OpenTelemetry.Metrics;\nusing OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\nusing System.Diagnostics;\n\nnamespace <YourNamespaceHere>.Helpers;\n\ninternal static class SplunkOpenTelemetry\n{\n    private static readonly string AccessToken;\n    private static readonly string Realm;\n\n    static SplunkOpenTelemetry()\n    {\n        AccessToken = Environment.GetEnvironmentVariable(\"SPLUNK_ACCESS_TOKEN\")?.Trim()\n            ?? throw new ArgumentNullException(\"SPLUNK_ACCESS_TOKEN\");\n\n        Realm = Environment.GetEnvironmentVariable(\"SPLUNK_REALM\")?.Trim()\n            ?? throw new ArgumentNullException(\"SPLUNK_REALM\");\n    }\n\n    public static IWebJobsBuilder AddSplunkOpenTelemetry(this IWebJobsBuilder builder)\n    {\n        // Get environment variables from function configuration\n        // You need a valid Splunk Observability Cloud access token and realm\n        var serviceName = Environment.GetEnvironmentVariable(\"WEBSITE_SITE_NAME\") ?? \"Unknown\";\n        var enableTraceResponseHeaderValue = Environment.GetEnvironmentVariable(\"SPLUNK_TRACE_RESPONSE_HEADER_ENABLED\")?.Trim();\n\n        builder.Services.AddOpenTelemetry()\n            .ConfigureResource(cfg => cfg\n                // See https://github.com/open-telemetry/opentelemetry-dotnet-contrib/tree/main/src/OpenTelemetry.Resources.Azure\n                // for other types of Azure detectors\n                .AddAzureAppServiceDetector()\n                .AddService(serviceName: serviceName, serviceVersion: \"1.0.0\"))\n            .WithTracing(t => t\n                // Use Add[instrumentation-name]Instrumentation to instrument missing services\n                // Use Nuget to find different instrumentation libraries\n                .AddHttpClientInstrumentation(opts =>\n                {\n                    // This filter prevents background (parent-less) http client activity\n                    opts.FilterHttpWebRequest = req => Activity.Current?.Parent != null;\n                    opts.FilterHttpRequestMessage = req => Activity.Current?.Parent != null;\n                })\n                // Use AddSource to add your custom DiagnosticSource source names\n                //.AddSource(\"My.Source.Name\")\n                // Automatically creates the root span with function start\n                .AddSource(SplunkFunctionAttribute.ActivitySourceName)\n                .SetSampler(new AlwaysOnSampler())\n                .AddConsoleExporter()\n                .AddOtlpExporter(opts =>\n                {\n                    opts.Endpoint = new Uri($\"https://ingest.{Realm}.signalfx.com/v2/trace/otlp\");\n                    opts.Protocol = OtlpExportProtocol.HttpProtobuf;\n                    opts.Headers = $\"X-SF-TOKEN={AccessToken}\";\n                }))\n            .WithMetrics(m => m\n                // Use Add[instrumentation-name]Instrumentation to instrument missing services\n                // Use Nuget to find different instrumentation libraries\n                .AddHttpClientInstrumentation()\n                .AddRuntimeInstrumentation()\n                .AddProcessInstrumentation()\n                .AddOtlpExporter(opts =>\n                {\n                    opts.Endpoint = new Uri($\"https://ingest.{Realm}.signalfx.com/v2/datapoint/otlp\");\n                    opts.Headers = $\"X-SF-TOKEN={AccessToken}\";\n                }));\n\n        return builder;\n    }\n}\n\ninternal class SplunkFunctionAttribute : FunctionInvocationFilterAttribute\n{\n    public const string ActivitySourceName = \"Splunk.Azure.WebJob\";\n\n    private static readonly ActivitySource ActivitySource = new(ActivitySourceName);\n\n    private Activity? _currentActivity;\n\n    public override Task OnExecutingAsync(FunctionExecutingContext executingContext, CancellationToken cancellationToken)\n    {\n        _currentActivity = ActivitySource.StartActivity(executingContext.FunctionName, ActivityKind.Server);\n        _currentActivity?.AddTag(\"faas.name\", executingContext.FunctionName);\n        _currentActivity?.AddTag(\"faas.instance\", executingContext.FunctionInstanceId);\n\n        return base.OnExecutingAsync(executingContext, cancellationToken);\n    }\n\n    public override Task OnExecutedAsync(FunctionExecutedContext executedContext, CancellationToken cancellationToken)\n    {\n        if (!executedContext.FunctionResult.Succeeded)\n        {\n            if (executedContext.FunctionResult.Exception != null)\n            {\n                _currentActivity?.SetStatus(Status.Error.WithDescription(executedContext.FunctionResult.Exception.Message));\n                _currentActivity?.RecordException(executedContext.FunctionResult.Exception);\n            }\n            else\n            {\n                _currentActivity?.SetStatus(Status.Error);\n            }\n        }\n\n        _currentActivity?.Stop();\n\n        return base.OnExecutedAsync(executedContext, cancellationToken);\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Service Pipelines in YAML\nDESCRIPTION: This YAML configuration example shows how to properly set up service pipelines in the OpenTelemetry Collector. It demonstrates how to configure traces pipeline with receivers, processors, and exporters to avoid the 'not used by any pipeline' error.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/missing_pipeline_configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n  # Pipelines can contain multiple subsections, one per pipeline.\n    traces:\n    # Traces is the pipeline type.\n      receivers: [otlp, jaeger, zipkin]\n      processors: [memory_limiter, batch]\n      exporters: [otlp, jaeger, zipkin]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Instrumentation Libraries in Ruby\nDESCRIPTION: This bash command installs the OpenTelemetry SDK and all instrumentation libraries using Bundler. It adds these gems to the project's dependencies.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/ruby-manual-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbundle add opentelemetry-sdk opentelemetry-instrumentation-all\n```\n\n----------------------------------------\n\nTITLE: Running Splunk OpenTelemetry Collector Docker Container with Custom Configuration\nDESCRIPTION: Docker command to run the Splunk OpenTelemetry Collector container with a custom configuration file. Uses environment variables and volume mounting to provide the configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -e SPLUNK_ACCESS_TOKEN=12345 -e SPLUNK_REALM=us0 \\\n    -e SPLUNK_CONFIG=/etc/collector.yaml -p 13133:13133 -p 14250:14250 \\\n    -p 14268:14268 -p 4317:4317 -p 6060:6060 -p 8888:8888 \\\n    -p 9080:9080 -p 9411:9411 -p 9943:9943 \\\n    -v \"${PWD}/collector.yaml\":/etc/collector.yaml:ro \\\n    --name otelcol quay.io/signalfx/splunk-otel-collector:latest\n```\n\n----------------------------------------\n\nTITLE: Activating Transform Processor in OpenTelemetry Collector Configuration\nDESCRIPTION: This YAML snippet demonstrates adding the transform processor to a pipeline within the OpenTelemetry Collector configuration. It specifies the error mode and defines trace, metric, and log statements contexts. Ensure the Splunk Distribution of OpenTelemetry Collector is deployed and that the TCP log receiver is configured.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/transform-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntransform: error_mode: ignore # Statements can be trace, metric, or log <trace|metric|log>_statements: - context: <context> statements: - <statement> - <statement> - <statement> - context: <context> statements: - <statement> - <statement> - <statement>\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OpenTelemetry SDK to Use All Available Instrumentations in Ruby\nDESCRIPTION: This code configures the Splunk OpenTelemetry SDK to use all available instrumentation libraries. It shows how to set up the configuration in a Rails initializer file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/ruby-manual-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: ruby\nCODE:\n```\n# config/initializers/opentelemetry.rb\nrequire \"splunk/otel\"\n...\nSplunk::Otel.configure do |c|\nc.use_all()\nend\n```\n\n----------------------------------------\n\nTITLE: Importing and Initializing Splunk Session Recorder via NPM - JavaScript\nDESCRIPTION: Shows how to import the SplunkSessionRecorder module and initialize it in a modern JavaScript/TypeScript project following installation via npm. The initialization function takes a configuration object with at minimum 'realm' and 'rumAccessToken' properties. Used in applications bundled by build tools like webpack. All parameters must be adjusted as per deployment. No global script inclusion is needed.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/rum-session-replay.rst#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport SplunkSessionRecorder from '@splunk/otel-web-session-recorder'\\n\\nSplunkSessionRecorder.init({\\n    realm: '<realm>',\\n    rumAccessToken: '<your_rum_token>'\\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Pipeline for Kubernetes Events\nDESCRIPTION: Configuration for setting up the OpenTelemetry pipeline to collect Kubernetes events, including resource processors and service pipeline definition.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-events.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resource/add_event_k8s:\n    attributes:\n      - action: insert\n        key: kubernetes_cluster\n        value: CHANGEME\n\nreceivers:\n  smartagent/kubernetes-events:\n   type: kubernetes-events\n   alwaysClusterReporter: true\n\nservice:\n  pipelines:\n    logs/events:\n      exporters:\n        - signalfx\n      processors:\n        - memory_limiter\n        - batch\n        - resourcedetection\n        - resource/add_event_k8s\n      receivers:\n        - smartagent/kubernetes-events\n```\n\n----------------------------------------\n\nTITLE: Defining Detect and Publish Statements in SignalFlow - SignalFlow\nDESCRIPTION: This snippet shows a concise example of defining an alert condition in SignalFlow using the detect function and publishing a label for display in the UI. It is required to have Splunk Observability Cloud access and knowledge of SignalFlow syntax. The detect statement triggers on the condition when A exceeds 1000, and publish associates a human-readable label with the rule. The parameter 'A' should be defined earlier as a metric stream or variable. Input: variable A and threshold. Output: new rule with specified label is reflected in the Alert Rules tab.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/create-detectors-for-alerts.rst#2025-04-22_snippet_0\n\nLANGUAGE: SignalFlow\nCODE:\n```\ndetect(when(A > 1000)).publish('Weekly Starting Monday')\n```\n\n----------------------------------------\n\nTITLE: Running Application with Programmatic Instrumentation Entry Point (bash)\nDESCRIPTION: Runs a Node.js application (<your-app.js>) by first loading the custom entry point script (<entry-point.js>) containing the programmatic `start()` call, using the node `-r` flag. This ensures the instrumentation is configured before the main application code runs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nnode -r <entry-point.js> <your-app.js>\n```\n\n----------------------------------------\n\nTITLE: Initializing Meter for Custom .NET Metrics (C#)\nDESCRIPTION: Creates a `Meter` instance named \"My.Application\" with version \"1.0\". This meter serves as the entry point for creating custom metric instruments (like counters, gauges). The name must be registered using the `OTEL_DOTNET_AUTO_METRICS_ADDITIONAL_SOURCES` environment variable for the auto-instrumentation agent to collect metrics from this source. The `using` statement manages the meter's lifecycle.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/manual-dotnet-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\nusing var meter = new Meter(\"My.Application\", \"1.0\");\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry for uWSGI with post_fork Signal\nDESCRIPTION: Python code that initializes Splunk OpenTelemetry in response to uWSGI's post_fork signal, ensuring proper tracing in a multi-process environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/instrument-python-frameworks.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport uwsgidecorators\nfrom splunk_otel import init_splunk_otel\n\n@uwsgidecorators.postfork\ndef start_otel():\n   init_splunk_otel()\n```\n\n----------------------------------------\n\nTITLE: Reporting Custom Events with OpenTelemetry Swift API\nDESCRIPTION: Illustrates how to use the OpenTelemetry Swift API to report custom events in a React Native application. This example shows timing a function and adding attributes to the span.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/manual-rum-react-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: swift\nCODE:\n```\nfunc calculateTax() {\n   let tracer = OpenTelemetrySDK.instance.tracerProvider.get(instrumentationName: \"MyApp\")\n   let span = tracer.spanBuilder(spanName: \"calculateTax\").startSpan()\n   span.setAttribute(key: \"numClaims\", value: claims.count)\n   span.setAttribute(key: \"workflow.name\", value: \"<your_workflow>\") // This allows the event to appear in the UI\n //...\n //...\n   span.end() // You can also use defer for this\n}\n```\n\n----------------------------------------\n\nTITLE: Upgrading Collector with Helm while overriding configuration\nDESCRIPTION: This command upgrades the OpenTelemetry Collector using Helm while also applying a new configuration from a config.yaml file, combined with preserving other existing values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade splunk-otel-collector --values config.yaml splunk-otel-collector-chart/splunk-otel-collector --reuse-values\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Helper in Program.cs for Azure WebJobs (.NET/C#)\nDESCRIPTION: Demonstrates modifying the Program.cs or startup file of an Azure WebJob to use the custom AddSplunkOpenTelemetry extension method. This integrates the SplunkOpenTelemetry helper and ensures OpenTelemetry is initialized when the web job host is built. Requires that the SplunkOpenTelemetry helper is available and its dependencies are installed. Input is the HostBuilder and context configuration; output is an Azure WebJobs host with OpenTelemetry enabled.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/azure-webjobs.rst#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nvar builder = new HostBuilder()\n.ConfigureWebJobs(context =>\n{\n    context.AddSplunkOpenTelemetry();\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Cassandra Monitor in Splunk OpenTelemetry Collector\nDESCRIPTION: This YAML configuration activates the Cassandra integration in the Splunk OpenTelemetry Collector by defining the receiver and adding it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/cassandra.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/cassandra:\n    type: collectd/cassandra\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Agent in Kubernetes Deployment\nDESCRIPTION: YAML configuration for deploying the Python agent in Kubernetes using the Downward API to expose environment variables to Kubernetes resources. This enables tracing for Python applications running in Kubernetes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nspec:\n  selector:\n    matchLabels:\n      app: your-application\n  template:\n    spec:\n      containers:\n        - name: myapp\n          env:\n            - name: SPLUNK_OTEL_AGENT\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: OTEL_EXPORTER_OTLP_ENDPOINT\n              value: \"http://$(SPLUNK_OTEL_AGENT):4317\"\n            - name: OTEL_SERVICE_NAME\n              value: \"<serviceName>\"\n            - name: OTEL_RESOURCE_ATTRIBUTES\n              value: \"deployment.environment=<environmentName>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug Logging in Kubernetes Collector\nDESCRIPTION: YAML configuration to enable debug level logging for the Splunk OpenTelemetry Collector's telemetry service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  telemetry:\n    logs:\n      level: \"debug\"\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector for Splunk Platform in Bash\nDESCRIPTION: Helm command to install the Splunk OpenTelemetry Collector configured to send data to Splunk Enterprise or Splunk Cloud Platform, specifying endpoint, token, indexes, and cluster name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install my-splunk-otel-collector --set=\"splunkPlatform.endpoint=https://127.0.0.1:8088/services/collector,splunkPlatform.token=xxxxxx,splunkPlatform.metricsIndex=k8s-metrics,splunkPlatform.index=main,clusterName=my-cluster\" splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OpenTelemetry SDK with Sinatra Instrumentation in Ruby\nDESCRIPTION: This code configures the Splunk OpenTelemetry SDK to use the Sinatra instrumentation library. It demonstrates how to require the Splunk OTel library and set up the instrumentation with options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/ruby-manual-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"splunk/otel\"\nSplunk::Otel.configure do |c|\nc.use \"OpenTelemetry::Instrumentation::Sinatra\", { opt: \"value\" }\nend\n```\n\n----------------------------------------\n\nTITLE: Migrating from SignalFx Client to OpenTelemetry API - JavaScript\nDESCRIPTION: Illustrates how to replace SignalFx client code with the OpenTelemetry API for metrics in a Node.js project. The first snippet shows legacy SignalFx usage, while the second adopts '@opentelemetry/api-metrics' alongside '@splunk/otel', activating metrics via the 'metrics' configuration. This update ensures compatibility with newer OpenTelemetry conventions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/configuration/nodejs-otel-metrics.rst#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n// SignalFx\\nconst { start } = require('@splunk/otel');\\nconst { getSignalFxClient } = start({ serviceName: 'my-service' });\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\n// OpenTelemetry\\nconst { start } = require('@splunk/otel');\\nconst { metrics } = require('@opentelemetry/api-metrics');\\n\\nstart({\\n   serviceName: 'my-service',\\n   metrics: true, // activate metrics with default configuration\\n});\n```\n\n----------------------------------------\n\nTITLE: Managing Global Attributes in iOS RUM\nDESCRIPTION: This example shows how to define and manage global attributes that will be added to all reported data. It demonstrates setting attributes during initialization, adding them later, and removing specific attributes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: swift\nCODE:\n```\nimport SplunkOtel\n//...\nSplunkRumBuilder(realm: \"<realm>\", rumAuth: \"<rum-token>\")\n// You can set the globalAttributes option to the map at initialization\n   .deploymentEnvironment(environment: \"<environment>\")\n   .setApplicationName(\"<your_app_name>\")\n   .build()\n\n// You can also call the ``setGlobalAttributes`` function\n// anywhere in your code using the same map\nSplunkRum.setGlobalAttributes([])\n\n// To remove a global attribute, pass the key name to removeGlobalAttribute\nSplunkRum.removeGlobalAttribute(\"key2\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple OracleDB Receivers for Multiple Instances (YAML)\nDESCRIPTION: This YAML snippet demonstrates defining multiple oracledb receivers to monitor several Oracle Database instances. Each receiver is uniquely named and given a separate datasource. Comments remind users to review connection string options in the Go Oracle driver documentation. Used when tracking more than one database in a single OTel Collector deployment. Each named receiver can be referenced separately in pipelines.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/oracledb-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nreceivers:\\n  oracledb/aninstance:\\n    # Refer to Oracle Go Driver go_ora documentation for full connection string options\\n    datasource: \"oracle://<username>:<password>@<host>:<port>/<database>\"\\n  \\n  oracledb/anotherinstance:\\n    # Refer to Oracle Go Driver go_ora documentation for full connection string options\\n    datasource: \"oracle://<username>:<password>@<host>:<port>/<database>\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Meter with OpenTelemetry - Go\nDESCRIPTION: Initializes a meter for the service \"ExampleService\" using the OpenTelemetry Go API, which is necessary for registering metric instruments. The meter must be created before creating counters, histograms, or other metric instruments. There are no input parameters besides the meter name, and the output is a meter instance.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/go-manual-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: go\nCODE:\n```\nmeter := otel.Meter(\"ExampleService\")\n```\n\n----------------------------------------\n\nTITLE: Applying Filter Processors to OpenTelemetry Collector Pipelines in YAML\nDESCRIPTION: This configuration shows how to apply filter processors to different pipelines (traces, metrics, logs) in the OpenTelemetry Collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filter-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      receivers: [jaeger, otlp, zipkin]\n      processors:\n      - filter/traces\n      - memory_limiter\n      - batch\n      - resourcedetection\n      exporters: [otlphttp, signalfx]\n    metrics:\n      receivers: [hostmetrics, otlp, signalfx]\n      processors:\n      - filter/includemetrics\n      - filter/excludemetrics\n      - memory_limiter\n      - batch\n      - resourcedetection\n      exporters: [signalfx]\n    logs:\n      receivers: [fluentforward, otlp]\n      processors:\n      - filter/mixedlogs\n      - memory_limiter\n      - batch\n      - resourcedetection\n      exporters: [splunk_hec]\n```\n\n----------------------------------------\n\nTITLE: Configuring GKE ARM Support\nDESCRIPTION: Configuration example for enabling ARM workload support on Google Kubernetes Engine by setting the distribution parameter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndistribution: gke\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Purefa Receiver Configuration Example (YAML)\nDESCRIPTION: This example demonstrates a more complex configuration for two Pure Storage FlashArrays. It includes authentication settings, endpoint configurations, and various array-specific settings for both a FlashArray using an OpenMetrics exporter and one using native on-box metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/purefa-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  bearertokenauth/array01:\n    token: \"...\"\n  bearertokenauth/array02:\n    token: \"...\"\n\nreceivers:\n  purefa/array01:\n    fa_array_name: foobar01\n    endpoint: http://127.0.0.1:9490/metrics\n    array:\n      - address: array01\n        auth:\n          authenticator: bearertokenauth/array01\n    hosts:\n      - address: array01\n        auth:\n          authenticator: bearertokenauth/array01\n    directories:\n      - address: array01\n        auth:\n          authenticator: bearertokenauth/array01\n    pods:\n      - address: array01\n        auth:\n          authenticator: bearertokenauth/array01\n    volumes:\n      - address: array01\n        auth:\n          authenticator: bearertokenauth/array01\n    env: dev\n    settings:\n      reload_intervals:\n        array: 20s\n        hosts: 60s\n        directories: 60s\n        pods: 60s\n        volumes: 60s\n\n  purefa/array02:\n    fa_array_name: foobar02\n    endpoint: https://127.0.0.1/metrics\n    tls:\n      insecure_skip_verify: true\n    array:\n      - address: array02\n        auth:\n          authenticator: bearertokenauth/array02\n    hosts:\n      - address: array02\n        auth:\n          authenticator: bearertokenauth/array02\n    directories:\n      - address: array02\n        auth:\n          authenticator: bearertokenauth/array02\n    pods:\n      - address: array02\n        auth:\n          authenticator: bearertokenauth/array02\n    volumes:\n      - address: array02\n        auth:\n          authenticator: bearertokenauth/array02\n    env: production\n    settings:\n      reload_intervals:\n        array: 20s\n        hosts: 60s\n        directories: 60s\n        pods: 60s\n        volumes: 60s\n\nservice:\n  extensions: [bearertokenauth/array01,bearertokenauth/array02]\n  pipelines:\n    metrics:\n      receivers: [purefa/array01,purefa/array02]\n```\n\n----------------------------------------\n\nTITLE: Integrating Memory Limiter into OpenTelemetry Collector Service Pipelines (YAML)\nDESCRIPTION: This YAML configuration snippet shows how to activate the previously defined `memory_limiter` processor by adding it to the pipelines within the `service` section of the OpenTelemetry Collector configuration. It includes the `memory_limiter` as the first processor in the `metrics`, `logs`, and `traces` pipelines to ensure it acts early in the data processing flow.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/memory-limiter-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      processors: [memory_limiter]\n    logs:\n      processors: [memory_limiter]\n    traces:\n      processors: [memory_limiter]\n```\n\n----------------------------------------\n\nTITLE: Activating Debug Logging in Splunk OpenTelemetry JS - JavaScript\nDESCRIPTION: This JavaScript snippet shows how to enable debug-level diagnostic logging for a Node.js application instrumented with the Splunk Distribution of OpenTelemetry JS, by setting the 'logLevel' argument to 'debug' inside the start() function. Dependencies include '@splunk/otel' or equivalent instrumentation library. The metrics, profiling, and tracing keys accept additional configuration options; 'logLevel' controls the verbosity. Inputs: none; Output: initializes telemetry with debug logging. Suitable for troubleshooting instrumentation issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/troubleshooting/common-nodejs-troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nstart({\n   logLevel: 'debug',\n   metrics: {\n      // configuration passed to metrics signal\n   },\n   profiling: {\n      // configuration passed to profiling signal\n   },\n   tracing: {\n      // configuration passed to tracing signal\n   },\n});\n```\n\n----------------------------------------\n\nTITLE: Declaring the Zipkin Receiver in OpenTelemetry Collector Configuration (YAML)\nDESCRIPTION: This YAML code snippet demonstrates how to add the Zipkin receiver to the receivers section of the OpenTelemetry Collector configuration file. The receiver is configured to listen on all network interfaces (0.0.0.0) at port 9412, but both the host and port can be adjusted as needed. No external dependencies are required except for the OpenTelemetry Collector itself. The snippet expects to be part of the main configuration, and the 'endpoint' parameter determines where the Collector will accept incoming Zipkin-formatted span data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/zipkin-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  zipkin:\n    endpoint: 0.0.0.0:9412\n```\n\n----------------------------------------\n\nTITLE: Creating OpenTelemetry Histogram with Explicit Bucket Boundaries in Java\nDESCRIPTION: This Java code demonstrates how to create an OpenTelemetry histogram with custom explicit bucket boundaries. It sets up age ranges as bucket boundaries, applies them to a histogram builder, and configures additional properties like description and unit.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/histograms.rst#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nvoid exampleWithCustomBuckets(Meter meter) {\n   DoubleHistogramBuilder originalBuilder = meter.histogramBuilder(\"people.ages\");\n   ExtendedLongHistogramBuilder builder = (ExtendedLongHistogramBuilder) originalBuilder.ofLongs();\n   List<Long> bucketBoundaries = Arrays.asList(0L, 5L, 12L, 18L, 24L, 40L, 50L, 80L, 115L);\n   LongHistogram histogram =\n      builder\n            .setAdvice(advice -> advice.setExplicitBucketBoundaries(bucketBoundaries))\n            .setDescription(\"A distribution of people's ages\")\n            .setUnit(\"years\")\n            .build();\n   addDataToHistogram(histogram);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics and Logs Pipelines with SignalFx Exporter in YAML\nDESCRIPTION: Example configuration for adding the SignalFx exporter to both metrics and logs pipelines, including the necessary receivers and processors.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-exporter.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [signalfx]\n      processors: [memory_limiter, batch, resourcedetection]\n      exporters: [signalfx]\n    logs:\n      receivers: [signalfx]\n      processors: [memory_limiter, batch, resourcedetection]\n      exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans Using Resource Attributes and OTTL Conditions in YAML\nDESCRIPTION: This configuration demonstrates how to filter spans using resource attributes and OpenTelemetry Transformation Language (OTTL) conditions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filter-processor.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nfilter/spans:\n  spans:\n    include:\n      match_type: strict\n      services:\n        - ponygame\n        - ponytest\n      attributes:\n        - key: an_attribute\n          value: \"(valid_value|another_value)\"\n    exclude:\n      match_type: regexp\n      attributes:\n        - key: bad_attributes\n          value: \"(invalid_value|another_value)\"\n\nfilter/ottl:\n  traces:\n    span:\n      - 'attributes[\"test\"] == \"value\"'\n      - 'attributes[\"test\"] == \"value2\"'\n```\n\n----------------------------------------\n\nTITLE: Configuration Settings Migration\nDESCRIPTION: Illustrates the mapping of SignalFx Java Agent system properties and environment variables to their OpenTelemetry counterparts. Key parameters such as service names, environment, and endpoints are mapped directly to equivalent OTel settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/migrate-signalfx-java-agent-to-otel.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nsignalfx.service.name -> otel.service.name=<service_name>\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nSIGNALFX_SERVICE_NAME -> OTEL_SERVICE_NAME=<service_name>\n```\n\n----------------------------------------\n\nTITLE: Configuring Logs Pipeline for Custom Event Sending in YAML\nDESCRIPTION: A YAML configuration example showing how to set up a logs pipeline that uses a SmartAgent receiver and SignalFx exporter for event submission. The configuration includes a Resource Detection processor to ensure host identity and other useful information is available as event dimensions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/event-sending-functionality.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    logs:\n      receivers:\n        - smartagent/<receiver>\n# Adds the Resource Detection processor to the logs pipeline.\n      processors:\n        - resourcedetection\n      exporters:\n        - signalfx\n```\n\n----------------------------------------\n\nTITLE: Configuring Span Metrics Connector with Prometheus Remote Write\nDESCRIPTION: Example YAML configuration for using the Span Metrics connector with the Prometheus Remote Write exporter. It demonstrates how to set up receivers, exporters, and connectors for processing spans and exporting metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/span-metrics-connector.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      http:\n      grpc:\n\nexporters:\n  prometheusremotewrite:\n    endpoint: http://localhost:9090/api/v1/write\n    target_info:\n      enabled: true\n\nconnectors:\n  spanmetrics:\n    namespace: span.metrics\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [spanmetrics]\n    metrics:\n      receivers: [spanmetrics]\n      exporters: [prometheusremotewrite]\n```\n\n----------------------------------------\n\nTITLE: Creating a Meter with OpenTelemetry Java\nDESCRIPTION: This snippet shows how to create a meter using OpenTelemetry Java to enable custom metric recording. It involves retrieving a meter instance from the global OpenTelemetry object.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-manual-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nOpenTelemetry openTelemetry = GlobalOpenTelemetry.get();\nMeter sampleMeter = openTelemetry.getMeter(\"foo.example.metrics\");\n```\n\n----------------------------------------\n\nTITLE: Minimal Playbook Configuration for Splunk OpenTelemetry Collector\nDESCRIPTION: Example Ansible playbook showing minimal required configuration to deploy the Splunk OpenTelemetry Collector. Requires root access and includes essential variables for access token and realm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/deployments-linux-ansible.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n- name: Install the Splunk Distribution of OpenTelemetry Collector\n  hosts: all\n  become: yes\n  tasks:\n    - name: \"Include splunk_otel_collector\"\n      include_role:\n        name: \"signalfx.splunk_otel_collector.collector\"\n      vars:\n        splunk_access_token: YOUR_ACCESS_TOKEN\n        splunk_realm: SPLUNK_REALM\n```\n\n----------------------------------------\n\nTITLE: .NET Application Deployment YAML Without Instrumentation (linux-x64)\nDESCRIPTION: Basic Kubernetes deployment manifest for a .NET application running on linux-x64 before adding OpenTelemetry instrumentation annotations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-dotnet-app\n  namespace: monitoring\nspec:\n  template:\n    spec:\n      containers:\n      - name: my-dotnet-app\n        image: my-dotnet-app:latest\n```\n\n----------------------------------------\n\nTITLE: Adding the Splunk OpenTelemetry Collector Helm Repository in Bash\nDESCRIPTION: Command to add the Splunk OpenTelemetry Collector Helm chart repository to your local Helm configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart\n```\n\n----------------------------------------\n\nTITLE: Running Splunk OpenTelemetry Collector Docker Container with Specific User\nDESCRIPTION: This bash command shows how to run the Splunk OpenTelemetry Collector Docker container with a specific user and group, while also mounting the Docker socket.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v /var/run/docker.sock:/var/run/docker.sock:ro --user \"splunk-otel-collector:$(stat -c '%g' /var/run/docker.sock)\" quay.io/signalfx/splunk-otel-collector:latest <...>\n```\n\n----------------------------------------\n\nTITLE: Customizing Python Logging Instrumentation for Trace Correlation\nDESCRIPTION: Example of how to customize the format and level of log statements by passing arguments to the LoggingInstrumentor class.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nLoggingInstrumentor(logging_format='%(msg)s [span_id=%(span_id)s]')\n```\n\n----------------------------------------\n\nTITLE: Configuring Simple Prometheus Receiver in YAML\nDESCRIPTION: This snippet demonstrates how to configure the Simple Prometheus receiver in the OpenTelemetry Collector's YAML configuration file. It includes settings for collection interval, service account usage, endpoint, and TLS configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/simple-prometheus-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  prometheus_simple/endpointname:\n    collection_interval: 10s\n    use_service_account: true\n    endpoint: \"172.17.0.5:9153\"\n    tls:\n      ca_file: \"/path/to/ca\"\n      cert_file: \"/path/to/cert\"\n      key_file: \"/path/to/key\"\n      insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Enabling Metrics with OpenTelemetry Metrics API (JavaScript)\nDESCRIPTION: This example illustrates initializing metric collection in Node.js using OpenTelemetry's 'metrics' API and the Splunk distribution. The code imports 'start' from '@splunk/otel' and 'metrics' from '@opentelemetry/api-metrics', then starts telemetry with 'metrics: true' for default metrics collection. This setup is the recommended replacement for the SignalFx pattern and requires both '@splunk/otel' and '@opentelemetry/api-metrics' as dependencies. Inputs include configuration with 'serviceName'; outputs are collected and exported metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/nodejs-otel-metrics.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// OpenTelemetry\nconst { start } = require('@splunk/otel');\nconst { metrics } = require('@opentelemetry/api-metrics');\n\nstart({\n   serviceName: 'my-service',\n   metrics: true, // activate metrics with default configuration\n});\n```\n\n----------------------------------------\n\nTITLE: Log Body to Attribute Upsert Using Transform Processor (YAML)\nDESCRIPTION: This YAML snippet shows how to use the 'transform' processor in an OpenTelemetry Collector pipeline to parse JSON log bodies and merge parsed key/value pairs into the log's attributes. The 'merge_maps' OTTL function takes existing attributes and the result of 'ParseJSON', with an 'upsert' operation to combine them. This is useful when log data is embedded in the message body rather than in attributes, a common scenario prior to metric conversion.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sum-connector.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\\n  transform/logs:\\n    log_statements:\\n      - context: log\\n        statements:\\n          - merge_maps(attributes, ParseJSON(body), \\\"upsert\\\")\n```\n\n----------------------------------------\n\nTITLE: Adding Session Change Event Listener in JavaScript for Browser RUM\nDESCRIPTION: This example demonstrates how to add an event listener to track changes of session ID and update metadata accordingly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-api-reference.rst#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nSplunkRum.addEventListener('session-changed', (event) => {\n  LiveChat.setMetadata('splunk.sessionId', event.payload.sessionId);\n});\n```\n\n----------------------------------------\n\nTITLE: Tomcat and TomEE Java Agent Configuration on Linux\nDESCRIPTION: Append the javaagent argument to the setenv.sh script in Tomcat or TomEE on Linux to enable the Splunk Java agent when the server starts.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-servers-instructions.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nCATALINA_OPTS=\"$CATALINA_OPTS -javaagent:/path/to/splunk-otel-javaagent.jar\"\n```\n\n----------------------------------------\n\nTITLE: Setting B3 Propagator for Backward Compatibility in Linux\nDESCRIPTION: Shows how to configure the b3multi trace propagator in Linux for backward compatibility with the SignalFx Ruby Tracing Library.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/configuration/advanced-ruby-otel-configuration.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_PROPAGATORS=b3multi\n```\n\n----------------------------------------\n\nTITLE: Configuring a Prometheus Receiver in OpenTelemetry Collector\nDESCRIPTION: This YAML configuration snippet shows how to set up a Prometheus receiver within the internal metrics pipeline of the OpenTelemetry Collector. It demonstrates adding a new Prometheus entry under the receivers section and includes configuring scrape settings such as job name and scrape interval. Ensure that the Prometheus/internal receiver remains in the configuration to avoid disrupting the default dashboard.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/prometheus-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   receivers:\n     prometheus:\n       config:\n         scrape_configs:\n           - job_name: 'sample-name'\n             scrape_interval: 5s\n             static_configs:\n               - targets: ['0.0.0.0:8888']\n```\n\n----------------------------------------\n\nTITLE: Configuring Agent Collector YAML for Gateway Data Flow\nDESCRIPTION: YAML configuration for the agent collector showing how to set up receivers, processors, exporters and service pipelines to forward data to a gateway collector. Includes settings for traces, metrics, and logs forwarding.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployment-modes.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n   hostmetrics:\n      collection_interval: 10s\n      scrapers:\n         cpu:\n         disk:\n         filesystem:\n         memory:\n         network:\n\nprocessors:\n   resourcedetection:\n      detectors: [system,env,gce,ec2]\n      override: true\n   resource/add_environment:\n      attributes:\n         - action: insert\n            value: staging\n            key: deployment.environment\n\nexporters:\n   otlp:\n      endpoint: \"${SPLUNK_GATEWAY_URL}:4317\"\n      tls:\n         insecure: true\n   signalfx:\n      access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n      api_url: \"http://${SPLUNK_GATEWAY_URL}:6060\"\n      ingest_url: \"http://${SPLUNK_GATEWAY_URL}:9943\"\n      sync_host_metadata: true\n      correlation:\n   otlp:\n      endpoint: \"${SPLUNK_GATEWAY_URL}:4317\"\n\nservice:\n   extensions: [health_check, http_forwarder, zpages]\n   pipelines:\n      traces:\n         receivers: [jaeger, zipkin]\n         processors: [memory_limiter, batch, resourcedetection, resource/add_environment]\n         exporters: [otlp, signalfx]\n      metrics:\n         receivers: [hostmetrics]\n         processors: [memory_limiter, batch, resourcedetection]\n         exporters: [otlp]\n      metrics/internal:\n         receivers: [prometheus/internal]\n         processors: [memory_limiter, batch, resourcedetection]\n         exporters: [signalfx]\n      logs:   \n         receivers: [otlp]\n         processors: [memory_limiter, batch, resourcedetection]\n         exporters: [otlp]\n```\n\n----------------------------------------\n\nTITLE: Initializing Splunk RUM for iOS in Swift\nDESCRIPTION: Initialize the iOS RUM library with configuration parameters including realm, authentication token, deployment environment, and application name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/install-rum-ios.rst#2025-04-22_snippet_0\n\nLANGUAGE: swift\nCODE:\n```\nimport SplunkOtel\n//...\nSplunkRumBuilder(realm: \"<realm>\", rumAuth: \"<rum-token>\")\n// Call functions to configure additional options\n   .deploymentEnvironment(environment: \"<environment>\")\n   .setApplicationName(\"<your_app_name>\")\n   .build()\n```\n\n----------------------------------------\n\nTITLE: Configuring Jetty with Java Agent\nDESCRIPTION: Add the javaagent argument to Jetty configurations via command line or configuration files like jetty.sh and start.ini to use the Splunk Java agent.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-servers-instructions.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\njava -javaagent:/path/to/splunk-otel-javaagent.jar -jar start.jar\n```\n\nLANGUAGE: shell\nCODE:\n```\nJAVA_OPTIONS=\"${JAVA_OPTIONS} -javaagent:/path/to/splunk-otel-javaagent.jar\"\n```\n\nLANGUAGE: shell\nCODE:\n```\n-javaagent:/path/to/splunk-otel-javaagent.jar\n```\n\n----------------------------------------\n\nTITLE: Installing System-wide Node.js Zero-Code Instrumentation\nDESCRIPTION: Command to install the Splunk OpenTelemetry Collector with system-wide Node.js instrumentation using the installer script.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsh /tmp/splunk-otel-collector.sh --with-instrumentation --realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Configuring Postgresql Monitor with Smart Agent Receiver in YAML\nDESCRIPTION: This snippet demonstrates how to add a Postgresql monitor to the Collector using the Smart Agent receiver. It includes the receiver configuration and how to add it to a metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/smartagent-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/postgresql:\n    type: postgresql\n    host: mypostgresinstance\n    port: 5432\n    dimensionClients:\n      - signalfx # Instructs the receiver to use this exporter for dimension updates\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers:\n        - smartagent/postgresql\n      exporters:\n        - signalfx\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Chrony Receiver in YAML\nDESCRIPTION: Basic YAML configuration to activate the Chrony receiver in the OpenTelemetry Collector. It specifies the default endpoint and timeout settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/chrony-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  chrony/defaults:\n    endpoint: unix:///var/run/chrony/chronyd.sock # The default port by chronyd to allow cmd access\n    timeout: 10s # Allowing at least 10s for chronyd to respond before giving up\n```\n\n----------------------------------------\n\nTITLE: Adding Filter Processor to Logs Pipeline\nDESCRIPTION: Shows how to add the filter processor to the logs pipeline in the service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-edit.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n  #\n  # Other pipelines\n  #\n    logs:\n      # Add syslog at the end of the list\n      receivers: [fluentforward, otlp, syslog]\n      processors:\n      - memory_limiter\n      - batch\n      - resourcedetection\n      - filter/severity_text\n      exporters: [splunk_hec, splunk_hec/profiling]\n```\n\n----------------------------------------\n\nTITLE: Testing Collector with Synthetic Trace Data\nDESCRIPTION: Command line instructions for testing the Collector's trace collection capabilities using synthetic Zipkin data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/splunk-collector-troubleshooting.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -OL https://raw.githubusercontent.com/openzipkin/zipkin/master/zipkin-lens/testdata/yelp.json\ncurl -X POST localhost:9411/api/v2/spans -H'Content-Type: application/json' -d @yelp.json\n```\n\n----------------------------------------\n\nTITLE: Adding to a Counter Instrument - OpenTelemetry Go\nDESCRIPTION: Performs a measurement by incrementing the previously created counter by 1. This pattern is used for recording metric events, with 'ctx' as the context and 1 as the increment value. Requires a declared counter instrument and an active context.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/go-manual-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: go\nCODE:\n```\ncounter.Add(ctx, 1);\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Servers in OpenTelemetry Collector YAML\nDESCRIPTION: The YAML snippet details configuration for an HTTP server using OpenTelemetry Collector. It includes CORS settings to handle cross-origin requests, 'endpoint' for listening address, and settings for metadata inclusion. The 'auth' section defines authentication parameters like 'request_params'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/common-config/collector-common-config-http.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      http:\n        include_metadata: true\n        auth:\n          request_params:\n          - token\n          authenticator: some-authenticator-extension\n        cors:\n          allowed_origins:\n            - https://foo.bar.com\n            - https://*.test.com\n          allowed_headers:\n            - Example-Header\n          max_age: 7200\n        endpoint: 0.0.0.0:55690\n        compression_algorithms: [\"\", \"gzip\"]\n\nprocessors:\n  attributes:\n    actions:\n      - key: http.client_ip\n        from_context: X-Forwarded-For\n        action: upsert\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Timed Events with OpenTelemetry SDK in iOS\nDESCRIPTION: This code shows how to use the OpenTelemetry Swift API to create and time custom events in your iOS application. It creates a span to measure the duration of a function and adds attributes including a workflow name for UI display.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: swift\nCODE:\n```\nfunc calculateTax() {\n   let tracer = OpenTelemetrySDK.instance.tracerProvider.get(instrumentationName: \"MyApp\")\n   let span = tracer.spanBuilder(spanName: \"calculateTax\").startSpan()\n   span.setAttribute(key: \"numClaims\", value: claims.count)\n   span.setAttribute(key: \"workflow.name\", value: \"<your_workflow>\") // This allows the event to appear in the UI\n //...\n //...\n   span.end() // You can also use defer for this\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Workflow Span in JavaScript\nDESCRIPTION: Demonstrates how to create a workflow span with attributes using the OpenTelemetry API in JavaScript.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport {trace} from '@opentelemetry/api'\n\nconst tracer = trace.getTracer('appModuleLoader');\nconst span = tracer.startSpan('test.module.load', {\nattributes: {\n\n   'workflow.name': 'test.module.load'\n}\n});\n\n// Time passes\nspan.end();\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Allocation for Splunk OpenTelemetry Collector\nDESCRIPTION: This bash command demonstrates how to configure memory allocation for the Splunk OpenTelemetry Collector during installation. It uses the --memory parameter to set the total allocated memory in mebibytes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh;\nsudo sh /tmp/splunk-otel-collector.sh --realm $SPLUNK_REALM --memory $SPLUNK_MEMORY_TOTAL_MIB \\\n    -- $SPLUNK_ACCESS_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenTelemetry AutoInstrumentation on Windows .NET Framework\nDESCRIPTION: This snippet enumerates the necessary environment variables to configure OpenTelemetry auto-instrumentation on Windows using the .NET Framework. It requires setting paths to DLL files and GUID for the profiler. Key parameters include the installation location and profiler CLSID.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/configuration/advanced-dotnet-configuration.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nEnvironment variable: CORECLR_ENABLE_PROFILING\nValue: 1\n\nEnvironment variable: COR_PROFILER\nValue: {918728DD-259F-4A6A-AC2B-B85E1B658318}\n\nEnvironment variable: COR_PROFILER_PATH_64\nValue: $installationLocation\\win-x64\\OpenTelemetry.AutoInstrumentation.Native.dll\n\nEnvironment variable: COR_PROFILER_PATH_32\nValue: $installationLocation\\win-x86\\OpenTelemetry.AutoInstrumentation.Native.dll\n\nEnvironment variable: OTEL_DOTNET_AUTO_HOME\nValue: $installationLocation\n\nEnvironment variable: OTEL_DOTNET_AUTO_PLUGINS\nValue: Splunk.OpenTelemetry.AutoInstrumentation.Plugin, Splunk.OpenTelemetry.AutoInstrumentation\n```\n\n----------------------------------------\n\nTITLE: Statsd Metric Name Converter Configuration\nDESCRIPTION: Example configuration for converting Statsd metric names using pattern matching.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/statsd.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nconverters:\n  - pattern: \"cluster.cds_{traffic}_{mesh}_{service}-vn_{}.{action}\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Name with Splunk Instrumentation C++\nDESCRIPTION: This C++ snippet shows how to configure the service name for the Splunk Distribution of OpenTelemetry using the splunk::OpenTelemetryOptions object. The WithServiceName method configures the logical service name for exported telemetry. Dependencies include the Splunk C++ SDK. This approach is deprecated and is to be replaced by environment variable configuration. The key parameter is the string passed to WithServiceName representing the application\\'s service name. Input: C++ code using splunk::OpenTelemetryOptions. Output: Options object configured for the Splunk exporter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/cpp/migrate-from-splunk-cpp.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nsplunk::OpenTelemetryOptions options = splunk::OpenTelemetryOptions().WithServiceName(\"my-service\")\n```\n\n----------------------------------------\n\nTITLE: Activating Data Type Collection in JavaScript\nDESCRIPTION: This snippet demonstrates activating specific data types like metrics and profiling by setting them to true. This procedure turns on these features with default options in the Splunk Distribution of OpenTelemetry JS for Node.js without requiring additional configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/advanced-nodejs-otel-configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nstart({\n   // general options like `serviceName` and `endpoint`\n   metrics: true, // turn metrics on with default options\n   profiling: true, // turn profiling on with default options\n});\n```\n\n----------------------------------------\n\nTITLE: Installing the Collector with Downloaded RPM Package (yum)\nDESCRIPTION: Commands to install the required libcap dependency and install the Splunk OTel Collector using a locally downloaded RPM package with yum.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyum install -y libcap  # Required for enabling cap_dac_read_search and cap_sys_ptrace capabilities on the Collector\nrpm -ivh <path to splunk-otel-collector rpm>\n```\n\n----------------------------------------\n\nTITLE: Configuring AlwaysOn Profiling in OpenTelemetry JS\nDESCRIPTION: This JavaScript snippet configures the AlwaysOn Profiling feature for Splunk's OpenTelemetry JS distribution, enabling profiling and tracing for a specified service programmatically via the `start` function.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/advanced-nodejs-otel-configuration.rst#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nstart({\n   serviceName: '<service-name>',\n   profiling: true,\n   tracing: {\n      // configuration passed to tracing signal\n   },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Procstat Monitor Receiver\nDESCRIPTION: Basic configuration to activate the procstat monitor in the Collector configuration. Defines the receiver type and enables it in the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/procstat.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/procstat:\n    type: telegraf/procstat\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Advanced Elasticsearch Receiver Configuration Example\nDESCRIPTION: Detailed YAML configuration example for the Elasticsearch receiver, showcasing various optional settings such as node filters, index filters, and authentication.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/elasticsearch-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  elasticsearch:\n    metrics:\n      elasticsearch.node.fs.disk.available:\n        enabled: false\n    nodes: [\"_local\"]\n    skip_cluster_metrics: true\n    indices: [\".geoip_databases\"]\n    endpoint: http://localhost:9200\n    username: otel\n    password: password\n    collection_interval: 10s\n```\n\n----------------------------------------\n\nTITLE: Configuring Rule-Based Trace Sampling (Bash)\nDESCRIPTION: Example of configuring trace sampling using the 'rules' sampler via environment variables in Bash. It sets the sampler type (`OTEL_TRACES_SAMPLER`) and provides arguments (`OTEL_TRACES_SAMPLER_ARG`) to drop traces for the '/healthcheck' endpoint while using 'parentbased_always_on' as the fallback strategy for other requests. This affects downstream requests as well.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/configuration/advanced-java-otel-configuration.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_TRACES_SAMPLER=rules\nexport OTEL_TRACES_SAMPLER_ARG=drop=/healthcheck;fallback=parentbased_always_on\n```\n\n----------------------------------------\n\nTITLE: Initializing Splunk RUM Session Replay via CDN - HTML\nDESCRIPTION: Demonstrates the correct initialization order for setting up Splunk RUM and session replay using CDN-hosted JavaScript files. This approach requires adding script tags for both the core Splunk RUM agent and the session recorder. It shows initializing each with configuration objects specifying parameters like realm, rumAccessToken, applicationName, version, and deploymentEnvironment. Expected inputs include valid realm and token strings. The session recorder must be initialized after the RUM agent to function properly. No additional dependencies are required beyond the provided CDN links.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/rum-session-replay.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<script src=\\\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\\\" crossorigin=\\\"anonymous\\\"></script>\\n<script src=\\\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web-session-recorder.js\\\" crossorigin=\\\"anonymous\\\"></script>\\n<script>\\nSplunkRum.init({\\n    realm: '<realm>',\\n    rumAccessToken: '<your_rum_token>',\\n    applicationName: '<your_app_name>',\\n    version: '<your_app_version>',\\n    deploymentEnvironment: '<your_environment_name>'\\n});\\nSplunkSessionRecorder.init({\\n    realm: '<realm>',\\n    rumAccessToken: '<your_rum_token>'\\n});\\n</script>\n```\n\n----------------------------------------\n\nTITLE: Filtering Kubernetes Elements in OpenTelemetry Collector YAML Configuration\nDESCRIPTION: This configuration demonstrates how to filter various Kubernetes elements such as containers, pods, nodes, namespaces, and clusters in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filter-processor.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  config:\n    processors:\n      # Exclude specific metrics from containers named 'containerXName' or 'containerYName'\n      filter/exclude_metrics_from_container:\n        metrics:\n          exclude:\n            match_type: regexp\n            resource_attributes:\n              - key: k8s.container.name\n                value: '^(containerXName|containerYName)$'\n      \n      # Exclude logs from pods named 'podNameX'\n      filter/exclude_logs_from_pod:\n        logs:\n          exclude:\n            match_type: regexp\n            resource_attributes:\n              - key: k8s.pod.name\n                value: '^(podNameX)$'\n      \n      # Exclude logs from nodes named 'nodeNameX'\n      filter/exclude_logs_from_node:\n        logs:\n          exclude:\n            match_type: regexp\n            resource_attributes:\n              - key: k8s.node.name\n                value: '^(nodeNameX)$'\n      \n      # Exclude spans from traces for services housed in containers named 'containerXName' or 'containerYName'\n      filter/exclude_spans_from_traces_from_container:\n        spans:\n          exclude:\n            match_type: regexp\n            attributes:\n              - key: k8s.container.name\n                value: '^(containerXName|containerYName)$'\n      \n      # Exclude all telemetry data (metrics, logs, traces) from a namespace named 'namespaceX'\n      filter/exclude_all_telemetry_data_from_namespace:\n        logs:\n          exclude:\n            match_type: regexp\n            resource_attributes:\n              - key: k8s.namespace.name\n                value: '^(namespaceX)$'\n        metrics:\n          exclude:\n            match_type: regexp\n            resource_attributes:\n              - key: k8s.namespace.name\n                value: '^(namespaceX)$'\n        traces:\n          span:\n            - 'attributes[\"k8s.namespace.name\"] != \"namespaceX\"'\n      \n      # Exclude metrics from a cluster named 'clusterX'\n      filter/exclude_metrics_from_cluster:\n        metrics:\n          exclude:\n            match_type: regexp\n            resource_attributes:\n              - key: k8s.cluster.name\n                value: '^(clusterX)$'\n```\n\n----------------------------------------\n\nTITLE: Initializing Splunk OpenTelemetry JS Distribution (JavaScript)\nDESCRIPTION: Demonstrates how to programmatically initialize the Splunk Distribution of OpenTelemetry JS. This involves importing the 'start' function from '@splunk/otel' and calling it with the necessary configuration options, replacing the old SignalFx initialization.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/troubleshooting/migrate-signalfx-nodejs-agent-to-otel.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n.. code-block:: javascript\n\n  const { start } = require('@splunk/otel');\n\n  start({\n   // your new options here\n  });\n```\n\n----------------------------------------\n\nTITLE: Initializing Splunk RUM for iOS in Objective-C\nDESCRIPTION: Initialize the iOS RUM library in Objective-C with configuration parameters including realm, authentication token, deployment environment, and application name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/install-rum-ios.rst#2025-04-22_snippet_1\n\nLANGUAGE: objective-c\nCODE:\n```\n@import SplunkOtel;\n\nSplunkRumBuilder *builder = [[SplunkRumBuilder alloc] initWithRealm:@\"<realm>\"  rumAuth: @\"<rum-token>\"]]];\n[builder deploymentEnvironmentWithEnvironment:@\"<environment-name>\"];\n[builder setApplicationName:@\"<your_app_name>\"];\n[builder build];\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenTelemetry in Linux\nDESCRIPTION: Export environment variables necessary for Splunk OpenTelemetry configuration on Linux systems. Ensure proper application identification and metric exporting.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/instrumentation/instrument-nodejs-application.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_SERVICE_NAME=<yourServiceName>\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_RESOURCE_ATTRIBUTES='deployment.environment=<envtype>,service.version=<version>'\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport SPLUNK_METRICS_ENABLED='true'\n```\n\n----------------------------------------\n\nTITLE: Activating Debug Logging for Android Instrumentation\nDESCRIPTION: This code snippet demonstrates how to activate debug logging for Splunk RUM in an Android application. It adds the enableDebug() method to the SplunkRum.builder() configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nSplunkRum.builder()\n   .setApplicationName(\"<name_of_app>\")\n   .setRealm(\"<realm>\"\")\n   .setRumAccessToken(\"<rumAccessToken>\")\n   .enableDebug()\n   .build(this);\n}\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Deployment Configuration\nDESCRIPTION: YAML configuration for deploying the Python agent in Kubernetes using environment variables and the Downward API.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/instrument-python-application.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nspec:\n  selector:\n    matchLabels:\n      app: your-application\n  template:\n    spec:\n      containers:\n        - name: myapp\n          env:\n            - name: SPLUNK_OTEL_AGENT\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: OTEL_EXPORTER_OTLP_ENDPOINT\n              value: \"http://$(SPLUNK_OTEL_AGENT):4317\"\n            - name: OTEL_SERVICE_NAME\n              value: \"<serviceName>\"\n            - name: OTEL_RESOURCE_ATTRIBUTES\n              value: \"deployment.environment=<environmentName>\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Jaeger Exporter in OpenTelemetry JS\nDESCRIPTION: This JavaScript snippet sets up the Jaeger exporter via the `@opentelemetry/exporter-jaeger` package, enabling the tracing functionality for a Node.js service using Splunk's instrumentation library. It involves initializing the `JaegerExporter` with configuration options generated through the `start` function.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/advanced-nodejs-otel-configuration.rst#2025-04-22_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nconst { start } = require('@splunk/otel');\nconst { JaegerExporter } = require('@opentelemetry/exporter-jaeger');\nstart({\n   serviceName: 'my-node-service',\n   tracing: {\n      spanExporterFactory: (options) => {\n      return new JaegerExporter({\n         serviceName: options.serviceName,\n         // Additional config\n      })\n      }\n   },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Observer and Receiver Creator for Kubelet Stats in YAML\nDESCRIPTION: This configuration example demonstrates how to set up the Kubernetes observer and use it with the Receiver Creator to collect kubelet stats from Kubernetes nodes. It includes settings for authentication and metric collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/receiver-creator-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  k8s_observer:\n    auth_type: serviceAccount\n    node: ${env:K8S_NODE_NAME}\n    observe_pods: true\n    observe_nodes: true\n\nreceivers:\n  receiver_creator:\n    watch_observers: [k8s_observer]\n    receivers:\n      kubeletstats:\n        rule: type == \"k8s.node\"\n        config:\n          auth_type: serviceAccount\n          collection_interval: 10s\n          endpoint: \"`endpoint`:`kubelet_endpoint_port`\"\n          extra_metadata_labels:\n            - container.id\n          metric_groups:\n            - container\n            - pod\n            - node\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Services in Docker Compose\nDESCRIPTION: Defines two logging services that continuously generate JSON log messages to output files. Each service depends on the OpenTelemetry collector and mounts a local volume for log output.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/docker-compose.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  logging1:\n    image: bash:latest\n    container_name: logging1\n    command: bash -c \"while(true) do echo '{\\\"message\\\":\\\"logging1\\\"}' >> /output/file.log ; sleep 1; done\"\n    depends_on:\n      - otelcollector\n    volumes:\n      - ./output1:/output\n  logging2:\n    image: bash:latest\n    container_name: logging2\n    command: bash -c \"while(true) do echo '{\\\"message\\\":\\\"logging2\\\"}' >> /output/file.log ; sleep 1; done\"\n    depends_on:\n      - otelcollector\n    volumes:\n      - ./output2:/output\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Metric Provider in Python with OpenTelemetry\nDESCRIPTION: Configures a MeterProvider with a PeriodicExportingMetricReader and ConsoleMetricExporter for custom metrics in Python.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmetric_reader = PeriodicExportingMetricReader(ConsoleMetricExporter())\nprovider = MeterProvider(metric_readers=[metric_reader])\n\nmetrics.set_meter_provider(provider)\nmeter = metrics.get_meter(\"my.meter.name\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk HEC Exporter in OpenTelemetry Collector\nDESCRIPTION: Configures the Splunk HTTP Event Collector (HEC) exporter with token, endpoint, and TLS settings for sending logs to Splunk Enterprise.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/collector-splunk.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  splunk_hec/logs:\n    token: \"00000000-0000-0000-0000-0000000000000\"\n    endpoint: \"https://splunk:8088/services/collector\"\n    tls:\n      insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Updating OpenTelemetry Configuration with Exporter in Ruby\nDESCRIPTION: Modifies the OpenTelemetry configuration to include the OTLP exporter package. This update is necessary for enabling direct data export to Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/instrument-ruby.rst#2025-04-22_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\nrequire 'opentelemetry/sdk'\nrequire 'opentelemetry/instrumentation/all'\nrequire 'opentelemetry-exporter-otlp'\nOpenTelemetry::SDK.configure do |c|\n    c.use_all() # activates all instrumentation\nend\n```\n\n----------------------------------------\n\nTITLE: Log Obfuscation with Update and Hash Actions\nDESCRIPTION: An example configuration demonstrating how to remove or redact sensitive information from logs by using update, delete, and hash actions. This setup also uses regular expressions to match log bodies.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/attributes-processor.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nattributes/log_body_regexp:\n  include:    \n    match_type: regexp\n      log_bodies: [\"AUTH.*\"]\n  actions:\n    - key: password\n      action: update\n      value: \"Redacted\"\n    - key: apitoken\n      action: delete\n    - key: email\n      action: hash\n```\n\n----------------------------------------\n\nTITLE: ECS Observer Configuration Example\nDESCRIPTION: Complete YAML configuration for setting up the ECS Observer extension with Prometheus target discovery and metrics pipeline configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-ecs-ec2.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  ecs_observer:\n    refresh_interval: 10s\n    cluster_name: 'lorem-ipsum-cluster'\n    cluster_region: 'us-west-2'\n    result_file: '/etc/ecs_sd_targets.yaml'\n    task_definitions:\n      - arn_pattern: \"^arn:aws:ecs:us-west-2:906383545488:task-definition/lorem-ipsum-task:[0-9]+$\"\n        metrics_ports: [9113]\n        metrics_path: /metrics\nreceivers:\n  prometheus:\n    config:\n      scrape_configs:\n        - job_name: 'lorem-ipsum-nginx'\n          scrape_interval: 10s\n          file_sd_configs:\n            - files:\n                - '/etc/ecs_sd_targets.yaml'\nprocessors:\n  batch:\n  resourcedetection:\n    detectors: [ecs]\n    override: false    \nexporters:\n  signalfx:\n    access_token: ${SPLUNK_ACCESS_TOKEN}\n    realm: ${SPLUNK_REALM}\nservice:\n  extensions: [ecs_observer]\n  pipelines:\n    metrics:\n      receivers: [prometheus]\n      processors: [batch, resourcedetection]\n      exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Activating Metrics Collection Environment Variable (Linux)\nDESCRIPTION: Sets the SPLUNK_METRICS_ENABLED environment variable to 'true' in a Linux bash shell. This activates the automatic collection of runtime metrics for the Node.js application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport SPLUNK_METRICS_ENABLED='true'\n```\n\n----------------------------------------\n\nTITLE: Configuring Session-Based Sampling with npm in JavaScript\nDESCRIPTION: This snippet shows how to configure the Splunk RUM agent to collect data from 50% of sessions using the npm package. It demonstrates the usage of SessionBasedSampler with a ratio of 0.5.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/configure-rum-browser-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport SplunkOtelWeb, {SessionBasedSampler} from '@splunk/otel-web';\n\nSplunkOtelWeb.init({ \n  realm: '<realm>',\n  rumAccessToken: '<your_rum_token>', \n  applicationName: '<application-name>',\n  tracer: {\n      sampler: new SessionBasedSampler({\n        ratio: 0.5 \n      }),\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Service User for Splunk Collector (Shell)\nDESCRIPTION: Specifies the user for the splunk-otel-collector service. This option will create the specified user if it does not already exist on the system.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n--service-user <user>\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nsplunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Defining the Memory Limiter Processor in OpenTelemetry Collector YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to define the `memory_limiter` processor within the `processors` section of the OpenTelemetry Collector configuration file. It sets the `check_interval` to 1 second, the hard memory limit (`limit_mib`) to 4000 MiB, and the spike limit (`spike_limit_mib`) to 800 MiB. This configuration helps prevent the Collector from exceeding memory limits.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/memory-limiter-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  memory_limiter:\n    check_interval: 1s\n    limit_mib: 4000\n    spike_limit_mib: 800\n```\n\n----------------------------------------\n\nTITLE: Configuring Memcached Receiver in OpenTelemetry Collector YAML\nDESCRIPTION: This snippet shows how to activate the Memcached integration by adding the necessary configuration to the OpenTelemetry Collector YAML file. It includes setting up the receiver and adding it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cache/memcached.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/collectd/memcached:\n    type: collectd/memcached\n    ...  # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/collectd/memcached]\n```\n\n----------------------------------------\n\nTITLE: Adding and Updating OpenTelemetry Helm Repository - Shell\nDESCRIPTION: This shell command snippet adds the OpenTelemetry Helm repository and updates the local Helm repo cache.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts\n      helm repo update\n```\n\n----------------------------------------\n\nTITLE: Recording Custom RUM Events in Android Dialog Interactions\nDESCRIPTION: Shows how to track user interactions by recording a custom RUM event when a user rejects a help dialog, providing visibility into user behavior.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\npublic Dialog onCreateDialog(Bundle savedInstanceState) {\n   LayoutInflater inflater = LayoutInflater.from(activity);\n   View alertView = inflater.inflate(R.layout.sample_mail_dialog, null);\n   AlertDialog.Builder builder = new AlertDialog.Builder(activity);\n   builder.setView(alertView)\n            .setNegativeButton(R.string.cancel, (dialog, id) ->\n               // Record a simple \"zero duration\" span with the provided name and attributes\n                  SplunkRum.getInstance().addRumEvent(\"User Rejected Help\", HELPER_ATTRIBUTES));\n   return builder.create();\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Redaction and Privacy Controls in Session Replay - JavaScript\nDESCRIPTION: Illustrates a series of configuration scenarios for the SplunkSessionRecorder.init function, targeting privacy options like text and input redaction. Shows how to disable global text redaction, customize which elements are redacted via class or selector, and fine-tune input masking, including allowing passwords only. These configurations influence what user data is obscured in recordings. No external dependencies are required; parameters should be set based on compliance and privacy requirements. Non-string options (like regexp) must be valid JavaScript objects.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/rum-session-replay.rst#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n// Will disable text redaction on all elements except elements with default 'rr-mask' class\\nSplunkSessionRecorder.init({\\n    // ... other configuration options\\n    maskTextSelector: false\\n});\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// Will redact only elements with 'my-custom-mask-class' class\\nSplunkSessionRecorder.init({\\n    // ...\\n    maskTextClass: 'my-custom-mask-class',\\n    maskTextSelector: false\\n});\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// Redacts elements with class names starting with \\\"sensitive-\\\" or with specified IDs\\nSplunkSessionRecorder.init({\\n    // ...\\n    maskTextClass: /^sensitive-.*$/,\\n    maskTextSelector: '#private-info, #hidden-section'\\n});\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// Will disable input redaction on all elements except password inputs\\nSplunkSessionRecorder.init({\\n    // ...\\n    maskAllInputs: false,\\n    maskInputOptions: {\\n        password: true\\n    }\\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Java Agent on Linux\nDESCRIPTION: These shell commands set important environment variables for configuring the OpenTelemetry Java agent. Variables like OTEL_SERVICE_NAME and OTEL_EXPORTER_OTLP_ENDPOINT are critical for defining the service details and endpoint locations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/instrument-java-application.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_SERVICE_NAME=<yourServiceName>\n\nexport OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\n----------------------------------------\n\nTITLE: Configuring Collector for Profiling in Kubernetes\nDESCRIPTION: This YAML configuration activates profiling for the Collector in Kubernetes. It sets the access token, realm, and enables both logs and profiling.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkObservability:\n  accessToken: CHANGEME\n  realm: us0\n  logsEnabled: true\n  profilingEnabled: true\n```\n\n----------------------------------------\n\nTITLE: Adding OTLP Receiver to Service Pipelines in YAML\nDESCRIPTION: This configuration snippet demonstrates how to add the OTLP receiver to the service pipelines section for receiving traces and metrics data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlp-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n    metrics:\n      receivers: [otlp]\n```\n\n----------------------------------------\n\nTITLE: Adding DiagnosticSource Dependency for .NET Metrics (XML)\nDESCRIPTION: Adds the `System.Diagnostics.DiagnosticSource` NuGet package reference to the .NET project file. This dependency is required for manual metric instrumentation using `Meter` and related types as part of the OpenTelemetry .NET SDK integration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/manual-dotnet-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<PackageReference Include=\"System.Diagnostics.DiagnosticSource\" Version=\"9.0.0\" />\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Receiver in OpenTelemetry Collector (YAML)\nDESCRIPTION: This YAML configuration snippet illustrates how to configure the OTLP receiver in the Splunk Distribution of the OpenTelemetry Collector. It enables the collector to receive spans using the OpenTelemetry Protocol (OTLP) via both gRPC (on port 4317) and HTTP (on port 4318) protocols on the specified endpoints.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-spans-traces/span-formats.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# To receive spans in OTLP format\n\nreceivers:\n   otlp:\n      protocols:\n         grpc:\n            endpoint: 0.0.0.0:4317\n         http:\n            endpoint: 0.0.0.0:4318\n```\n\n----------------------------------------\n\nTITLE: Basic HTTP Monitor Configuration in YAML\nDESCRIPTION: Shows the basic receiver configuration for the HTTP monitor in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/http.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nreceivers:\n  smartagent/http:\n    type: http\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenTelemetry Java Agent\nDESCRIPTION: Demonstrates how to set environment variables to configure the OpenTelemetry Java agent. Environment variables are the preferred configuration method for adjusting agent settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/version1x/java-1x-otel-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_SERVICE_NAME=my-java-app\n```\n\n----------------------------------------\n\nTITLE: Configuring NTPQ Receiver in Splunk OpenTelemetry Collector\nDESCRIPTION: This YAML configuration activates the NTPQ integration by adding the smartagent/ntpq receiver to the Collector configuration. It specifies the receiver type and allows for additional configuration options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/ntpq.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/ntpq:\n    type: telegraf/ntpq\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Detailed OTLP/HTTP Exporter Configuration Example in YAML\nDESCRIPTION: This YAML snippet provides a comprehensive configuration example for the `otlphttp` exporter. It includes settings for specific endpoints (`traces_endpoint`, `metrics_endpoint`, and a general `endpoint`), authentication headers (`X-SF-Token`), request timeout (`10s`), buffer sizes (`read_buffer_size`, `write_buffer_size`), sending queue parameters (`enabled`, `num_consumers`, `queue_size`), retry-on-failure strategies (`enabled`, `initial_interval`, etc.), and explicitly enables gzip compression.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlphttp-exporter.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nendpoint: \"https://1.2.3.4:1234\"\ntraces_endpoint: https://ingest.<realm>.signalfx.com/v2/trace/otlp\nmetrics_endpoint: https://ingest.<realm>.signalfx.com/v2/datapoint/otlp\nheaders:\n  X-SF-Token: <access_token>\ntimeout: 10s\nread_buffer_size: 123\nwrite_buffer_size: 345\nsending_queue:\n  enabled: true\n  num_consumers: 2\n  queue_size: 10\nretry_on_failure:\n  enabled: true\n  initial_interval: 10s\n  randomization_factor: 0.7\n  multiplier: 1.3\n  max_interval: 60s\n  max_elapsed_time: 10m\ncompression: gzip\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector to a Specific Namespace in Bash\nDESCRIPTION: Helm command to install the Splunk OpenTelemetry Collector to a specific Kubernetes namespace using a values file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm -n otel install my-splunk-otel-collector -f values.yaml splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Setting Global Attributes During Initialization in Android RUM\nDESCRIPTION: Demonstrates defining global attributes like app version and custom key-value pairs that will be added to all reported telemetry data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nSplunkRum.builder()\n         .setGlobalAttributes(\n               Attributes.builder()\n                        .put(\"key\", \"value\")\n                        .put(StandardAttributes.APP_VERSION, BuildConfig.VERSION_NAME)\n                        .build())\n```\n\n----------------------------------------\n\nTITLE: Running a uWSGI Application with OpenTelemetry\nDESCRIPTION: Command to run a uWSGI application with HTTP server, specifying the WSGI file and callable, while enabling master mode and threading for proper instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/instrument-python-frameworks.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuwsgi --http :9090 --wsgi-file <your_app.py> --callable <your_wsgi_callable> --master --enable-threads\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Pipeline in YAML\nDESCRIPTION: This YAML configuration snippet sets up a pipeline for AlwaysOn Profiling using the Splunk HTTP Event Collector exporter in the OpenTelemetry Collector. It requires the SPLUNK_ACCESS_TOKEN and SPLUNK_INGEST_URL environment variables to send profiling data. Ensure that both otlp/gRPC receiver and memory limiter processors are enabled. The configuration should be manually updated for Collector versions lower than 0.44.0.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/profiling/profiling-troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\\n  otlp:\\n    protocols:\\n      grpc:\\n\\nexporters:\\n  # Profiling\\n  splunk_hec/profiling:\\n    token: \"${SPLUNK_ACCESS_TOKEN}\"\\n    endpoint: \"${SPLUNK_INGEST_URL}/v1/log\"\\n    log_data_enabled: false\\n\\nprocessors:\\n  batch:\\n  memory_limiter:\\n    check_interval: 2s\\n    limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}\\n\\nservice:\\n  pipelines:\\n    logs/profiling:\\n      receivers: [otlp]\\n      processors: [memory_limiter, batch]\\n      exporters: [splunk_hec, splunk_hec/profiling]\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx Exporter for Metrics and Events in YAML\nDESCRIPTION: Default configuration for the SignalFx exporter to send metrics and events, as well as enable trace and metrics correlation. Includes settings for access token, API URL, and ingest URL.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-exporter.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsignalfx:\n  access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n  api_url: \"${SPLUNK_API_URL}\"\n  ingest_url: \"${SPLUNK_INGEST_URL}\"\n  # Use instead when sending to gateway (http forwarder extension ingress endpoint)\n  #api_url: http://${SPLUNK_GATEWAY_URL}:6060\n  #ingest_url: http://${SPLUNK_GATEWAY_URL}:9943\n  sync_host_metadata: true\n```\n\n----------------------------------------\n\nTITLE: Reporting Instantaneous Events in iOS RUM\nDESCRIPTION: This code demonstrates how to record an event with no duration (instantaneous event) using the reportEvent function. It includes multiple attributes to provide context about the event.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_7\n\nLANGUAGE: swift\nCODE:\n```\nlet dictionary: NSDictionary = [\n                  \"attribute1\": \"hello\",\n                  \"attribute2\": \"world!\",\n                  \"attribute3\": 3\n]\nSplunkRum.reportEvent(name: \"testEvent\", attributes: dictionary)\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Stats Receiver with Service Account Authentication (YAML)\nDESCRIPTION: Configures the kubeletstats receiver to use service account authentication and references the K8S_NODE_NAME environment variable for the endpoint. Key fields include collection_interval, auth_type, insecure_skip_verify, and the use of a file exporter. Appropriate for Kubernetes clusters using service account tokens for authentication to the kubelet API.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubelet-stats-receiver.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 20s\n    auth_type: \"serviceAccount\"\n    endpoint: \"${K8S_NODE_NAME}:10250\"\n    insecure_skip_verify: true\nexporters:\n  file:\n    path: \"fileexporter.txt\"\nservice:\n  pipelines:\n    metrics:\n      receivers: [kubeletstats]\n      exporters: [file]\n\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OTel Package (npm)\nDESCRIPTION: Installs the Splunk Distribution of OpenTelemetry JS package using npm. This package is required for automatic and programmatic instrumentation of Node.js applications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @splunk/otel\n```\n\n----------------------------------------\n\nTITLE: Creating CPU Utilization Metric from Usage\nDESCRIPTION: Creates a new host.cpu.utilization metric from host.cpu.usage where pod label matches a non-empty value pattern.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: host.cpu.usage\naction: insert\nnew_name: host.cpu.utilization\nmatch_type: regexp\nexperimental_match_labels: {\"pod\": \"(.|\\\\s)*\\\\S(.|\\\\s)*\"}\noperations:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Example Enriched Pino Log Message with Trace Metadata (JSON)\nDESCRIPTION: A sample log message generated by the Pino logging library, automatically enriched by the Splunk Distribution of OpenTelemetry JS. It includes trace context like 'trace_id', 'span_id', and 'trace_flags', alongside resource attributes like 'service.name' (implied context) and 'deployment.environment', facilitating correlation between logs and traces in Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"level\":30,\"time\":1979374615686,\"pid\":728570,\"hostname\":\"my_host\",\"trace_id\":\"f8e261432221096329baf5e62090d856\",\"span_id\":\"3235afe76b55fe51\",\"trace_flags\":\"01\",\"url\":\"/lkasd\",\"msg\":\"request handler\"}\n```\n\n----------------------------------------\n\nTITLE: Programmatically Instrumenting Node.js Applications\nDESCRIPTION: Use the start() function from the @splunk/otel package to manually initialize tracing and metrics for fine-grained control over the Node.js application performance.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/instrumentation/instrument-nodejs-application.rst#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require('@splunk/otel');\n\nstart({\n   serviceName: 'my-node-service',\n   endpoint: 'http://localhost:4318'\n});\n\n// Rest of your main module\n```\n\nLANGUAGE: javascript\nCODE:\n```\nstart({\n   serviceName: 'my-node-service',\n   metrics: { runtimeMetricsEnabled: true },\n   profiling: { memoryProfilingEnabled: true }\n});\n```\n\n----------------------------------------\n\nTITLE: Installing the Collector with Debian Package Repository\nDESCRIPTION: Commands to set up the Splunk OTel Collector Debian package repository, add the GPG key, and install the Collector and optional auto-instrumentation package.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://splunk.jfrog.io/splunk/otel-collector-deb/splunk-B3CD4420.gpg > /etc/apt/trusted.gpg.d/splunk.gpg\necho 'deb https://splunk.jfrog.io/splunk/otel-collector-deb release main' > /etc/apt/sources.list.d/splunk-otel-collector.list\napt-get update\napt-get install -y splunk-otel-collector\n\n# Optional: install Splunk OpenTelemetry automatic discovery for language runtimes\napt-get install -y splunk-otel-auto-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Adding Span Tags in .NET using ActivitySource\nDESCRIPTION: Provides an example of setting multiple tags (attributes) with different value types (integer, string, array) on a .NET Activity object, which corresponds to an OpenTelemetry span. It uses the `SetTag` method and mentions the OTEL_RESOURCE_ATTRIBUTES environment variable for global tags.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/add-context-trace-span.rst#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\n// Splunk Distribution of OpenTelemetry .NET\n\nusing var myActivity = MyActivitySource.StartActivity(\"SayHello\");\n\nactivity?.SetTag(\"operation.value\", 1);\nactivity?.SetTag(\"operation.name\", \"Saying hello!\");\nactivity?.SetTag(\"operation.other-stuff\", new int[] { 1, 2, 3 });\n\n// You can also set global tags using the OTEL_RESOURCE_ATTRIBUTES\n// environment variable, which accepts a list of comma-separated key-value\n// pairs. For example, key1:val1,key2:val2.\n```\n\n----------------------------------------\n\nTITLE: Sample SignalFx Receiver Configurations with TLS and Passthrough (YAML)\nDESCRIPTION: Provides sample configurations for the SignalFx receiver, illustrating different settings. It includes a basic instance (`signalfx`), an instance named `signalfx/allsettings` specifying a `localhost` endpoint and enabling `access_token_passthrough`, and an instance named `signalfx/tls` demonstrating TLS configuration using `cert_file` and `key_file` for secure communication.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsignalfx:\nsignalfx/allsettings:\n  # endpoint specifies the network interface and port which will receive\n  # SignalFx metrics.\n  endpoint: localhost:9943\n  access_token_passthrough: true\nsignalfx/tls:\n  tls:\n    cert_file: /test.crt\n    key_file: /test.key\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Receiver for Agent DaemonSet\nDESCRIPTION: Configuration snippet for adding MySQL receiver to the Collector agent to collect metrics from every node.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-add.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  config:\n    receivers:\n      mysql:\n        endpoint: localhost:3306\n        ...\n```\n\n----------------------------------------\n\nTITLE: Redacting and Hashing Span Tags in OpenTelemetry Collector\nDESCRIPTION: This YAML configuration for the Splunk Distribution of OpenTelemetry Collector exemplifies a processor that deletes, hashes, or redacts sensitive span tags such as \"user.password\", \"user.name\", and credit card details to conceal sensitive information before ingestion. This processor is part of a strategy to comply with data protection standards.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/set-up-apm/sensitive-data-controls.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions: \n    ...  \nprocessors:\n  attributes/update:\n    actions:\n      - key: user.password\n        action: delete\n      - key: user.name\n        action: hash\n      - key: credit.card.number\n        value: redacted\n        action: update\n      - key: cvv\n        value: redacted\n        action: update\n      - key: credit.card.expiration.date\n        value: redacted\n        action: update\nservice:\n    ...\n...\n```\n\nLANGUAGE: yaml\nCODE:\n```\n... \nservice:\n    pipelines:\n    traces:\n        receivers: ...\n        processors: [..., attributes/update, ...] \n        ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Attribute Detection Using 'resource_attributes' - YAML\nDESCRIPTION: This YAML snippet illustrates the recommended configuration approach for the resource detection processor in the Splunk OpenTelemetry Collector. Here, the 'system' detector is enabled, and fine-grained attribute toggling is achieved via 'resource_attributes', enabling or disabling collection for specific fields like 'host.name', 'host.id', and 'os.type'. Each attribute key under 'resource_attributes' has an 'enabled' flag. This setup requires collector version 0.81 or newer and is designed to be modular and clear for administrators.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/resourcedetection-processor.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n  resourcedetection:\n    detectors: [system]\n    system:\n      resource_attributes:\n        host.name:\n          enabled: true\n        host.id:\n          enabled: true\n        os.type:\n          enabled: false\n\n```\n\n----------------------------------------\n\nTITLE: Starting and Ending a Custom Timed Workflow (Android) in Java\nDESCRIPTION: This Java snippet demonstrates how to instrument a custom timed workflow in an Android application using the Splunk RUM SDK. It calls `SplunkRum.getInstance().startWorkflow()` to begin recording a workflow named \"Main thread working hard\", which returns an OpenTelemetry `Span`. A potentially long-running task is executed within a `try...finally` block, ensuring that `hardWorker.end()` is called to stop the timer and record the workflow duration, even if errors occur.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/RUM-custom-events.rst#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nbinding.buttonWork.setOnClickListener(v -> {\n   Span hardWorker =\n            SplunkRum.getInstance().startWorkflow(\"Main thread working hard\");\n   try {\n         Random random = new Random();\n         long startTime = System.currentTimeMillis();\n         while (true) {\n            random.nextDouble();\n            if (System.currentTimeMillis() - startTime > 20_000) {\n            break;\n            }\n         }\n   } finally {\n         hardWorker.end();\n   }\n});\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Java Agent Integration in Tomcat\nDESCRIPTION: Dockerfile to build a Tomcat container with the Splunk OpenTelemetry Java agent baked in. Downloads and installs the agent in /opt/splunk directory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-fargate-java.rst#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM tomcat:9.0-jre8-alpine\n\nRUN apk add curl\n\n# Create a work directory to copy the agent artifacts\nRUN mkdir -p /opt/splunk\n\n# Download and extract agent artifacts to the work directory\nRUN curl -L0 https://github.com/signalfx/splunk-otel-java/releases/latest/download/splunk-otel-javaagent.jar \\\n-o /opt/splunk/splunk-otel-javaagent.jar\n\nWORKDIR /usr/local/tomcat/webapps\n\nEXPOSE 8080\n\nCMD [\"/usr/local/tomcat/bin/catalina.sh\", \"run\"]\n```\n\n----------------------------------------\n\nTITLE: Sample OpenTelemetry Collector Configuration for .NET Profiling\nDESCRIPTION: This YAML snippet shows a sample configuration for the Splunk Distribution of OpenTelemetry Collector, specifically tailored for receiving and exporting .NET AlwaysOn Profiling data. It defines an OTLP receiver, a Splunk HEC exporter for profiling data (`splunk_hec/profiling`), necessary processors (batch, memory_limiter), and a `logs/profiling` pipeline connecting them. Ensure the Collector version is 0.34+ (0.44+ for memory profiling).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  # Profiling\n  splunk_hec/profiling:\n    token: \"${SPLUNK_ACCESS_TOKEN}\"\n    endpoint: \"${SPLUNK_INGEST_URL}/v1/log\"\n    log_data_enabled: false\n\nprocessors:\n  batch:\n  memory_limiter:\n    check_interval: 2s\n    limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}\n\nservice:\n  pipelines:\n    logs/profiling:\n      receivers: [otlp]\n      processors: [memory_limiter, batch]\n      exporters: [splunk_hec, splunk_hec/profiling]\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Filtering for Kubernetes\nDESCRIPTION: YAML configuration that sets up log filtering processors to exclude logs from specific pods and nodes using regular expressions, and defines the log processing pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/collector-config-tutorial-edit.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  config:\n    processors:\n      filter/exclude_logs_from_pod:\n        logs:\n          exclude:\n            match_type: regexp\n            resource_attributes:\n              - key: k8s.pod.name\n                value: '^(podNameX)$'\n      filter/exclude_logs_from_node:\n        logs:\n          exclude:\n            match_type: regexp\n            resource_attributes:\n              - key: k8s.node.name\n                value: '^(nodeNameX)$'\n    service:\n      pipelines:\n        logs:\n          processors:\n            - memory_limiter\n            - k8sattributes\n            - filter/logs\n            - batch\n            - resourcedetection\n            - resource\n            - resource/logs\n            - filter/exclude_logs_from_pod\n            - filter/exclude_logs_from_node\n```\n\n----------------------------------------\n\nTITLE: Creating SAP HANA Monitoring User and Permissions\nDESCRIPTION: SQL script to create a restricted monitoring user with the necessary permissions to access SAP HANA monitoring views. Creates a dedicated role and grants appropriate SELECT privileges on system monitoring tables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/sap-hana.rst#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n--Create the user\nCREATE RESTRICTED USER otel_monitoring_user PASSWORD <password>;\n\n--Enable user login\nALTER USER otel_monitoring_user ENABLE CLIENT CONNECT;\n\n--Create the monitoring role\nCREATE ROLE OTEL_MONITORING;\n\n--Grant permissions to the relevant views\nGRANT CATALOG READ TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_BACKUP_CATALOG TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_BLOCKED_TRANSACTIONS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_CONNECTIONS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_CS_ALL_COLUMNS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_CS_TABLES TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_DATABASE TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_DISKS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_HOST_RESOURCE_UTILIZATION TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_LICENSES TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_RS_TABLES TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICE_COMPONENT_MEMORY TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICE_MEMORY TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICE_REPLICATION TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICE_STATISTICS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICE_THREADS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICES TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_VOLUME_IO_TOTAL_STATISTICS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_WORKLOAD TO OTEL_MONITORING;\nGRANT SELECT ON _SYS_STATISTICS.STATISTICS_CURRENT_ALERTS TO OTEL_MONITORING;\n\n--Add the OTEL_MONITOR role to the monitoring user\nGRANT OTEL_MONITORING TO otel_monitoring_user;\n```\n\n----------------------------------------\n\nTITLE: Configuring Journald Event Collection in YAML for Splunk OpenTelemetry Collector\nDESCRIPTION: This YAML configuration snippet enables and configures journald event collection for the Splunk Distribution of OpenTelemetry Collector in Kubernetes. It specifies the directory for journald logs, lists service units to collect from, and provides an option to route logs to a specific Splunk index.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-logs.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nlogsCollection:\n  journald:\n    enabled: true\n    directory: /run/log/journal\n    # List of service units to collect and configuration for each. Update the list as needed.\n    units:\n      - name: kubelet\n        priority: info\n      - name: docker\n        priority: info\n      - name: containerd\n        priority: info\n    # Optional: Route journald logs to a separate Splunk Index by specifying the index\n    # value. Make sure the index exists in Splunk and is configured to receive HEC\n    # traffic (not applicable to Splunk Observability Cloud).\n    index: \"\"\n```\n\n----------------------------------------\n\nTITLE: Setting Global Attributes in React Native RUM\nDESCRIPTION: Demonstrates how to set global attributes for all reported data in Splunk RUM. Global attributes are key-value pairs useful for reporting app or user-specific values as tags.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/manual-rum-react-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nSplunkRum.setGlobalAttributes({\n   'enduser.id': 'Test User',\n});\n```\n\n----------------------------------------\n\nTITLE: Grouping Kubernetes Metrics\nDESCRIPTION: Groups pod and container metrics into separate ResourceMetrics with corresponding resource labels.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\n- include: ^k8s\\.pod\\.(.*)$$\n  match_type: regexp\n  action: group\n  group_resource_labels: {\"resouce.type\": \"k8s.pod\", \"source\": \"kubelet\"}\n- include: ^container\\.(.*)$$\n  match_type: regexp\n  action: group\n  group_resource_labels: {\"resouce.type\": \"container\", \"source\": \"kubelet\"}\n```\n\n----------------------------------------\n\nTITLE: Uninstalling .NET Instrumentation on Windows\nDESCRIPTION: These PowerShell commands deactivate and uninstall the .NET instrumentation on Windows. They include options for unregistering from IIS, Windows Service, or the current session, followed by uninstalling OpenTelemetry Core.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_20\n\nLANGUAGE: powershell\nCODE:\n```\n# Run the unregister command for your situation\nUnregister-OpenTelemetryForIIS\nUnregister-OpenTelemetryForWindowsService\nUnregister-OpenTelemetryForCurrentSession\n\n# Uninstall OpenTelemetry for .NET\nUninstall-OpenTelemetryCore\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Metrics Auto-detection in YAML\nDESCRIPTION: YAML configuration to enable automatic discovery and scraping of Prometheus metrics from pods in the Kubernetes cluster.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nautodetect:\n   prometheus: true\n```\n\n----------------------------------------\n\nTITLE: Spring PetClinic Kubernetes Deployment Configuration\nDESCRIPTION: YAML configuration for deploying the Spring PetClinic application with OpenTelemetry instrumentation enabled.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-java-traces-tutorial/deploy-collector-k8s-java.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: spring-petclinic\n  namespace: petclinic\nspec:\n  selector:\n    matchLabels:\n      app: spring-petclinic\n  template:\n    metadata:\n      labels:\n        app: spring-petclinic\n      annotations:\n        # Activates zero-code instrumentation for the Java application\n        instrumentation.opentelemetry.io/inject-java: \"true\"\nspec:\n  containers:\n    - name: petclinic-app\n      # Java application to instrument\n      image: ghcr.io/pavolloffay/spring-petclinic:latest\n      imagePullPolicy: Always\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Timed Event with Attributes (iOS) in Swift\nDESCRIPTION: This Swift snippet illustrates creating a timed custom event (span) within an iOS application using the OpenTelemetry Swift SDK. Inside the `calculateTax` function, it retrieves a tracer, starts a span named \"calculateTax\", sets custom attributes including \"numClaims\" and the essential \"workflow.name\" (required for the event to appear in the Splunk RUM UI), performs some operations, and then calls `span.end()` to complete the event recording and capture its duration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/RUM-custom-events.rst#2025-04-22_snippet_3\n\nLANGUAGE: swift\nCODE:\n```\nfunc calculateTax() {\n      let tracer = OpenTelemetrySDK.instance.tracerProvider.get(instrumentationName: \"MyApp\")\n      let span = tracer.spanBuilder(spanName: \"calculateTax\").startSpan()\n      span.setAttribute(key: \"numClaims\", value: claims.count)\n      span.setAttribute(key: \"workflow.name\", value: \"<your_workflow>\") // This allows the event to appear in the UI\n   //...\n   //...\n      span.end() // You can also use defer for this\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OTel Package in Dockerfile\nDESCRIPTION: Dockerfile commands to install the `@splunk/otel` npm package within a container image build process and set appropriate read permissions for the installed package files. This is necessary for deploying instrumented Node.js applications in containers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_15\n\nLANGUAGE: docker\nCODE:\n```\n# Install the @splunk/otel package\nRUN npm install @splunk/otel\n# Set appropriate permissions\nRUN chmod -R go+r /node_modules/@splunk/otel\n```\n\n----------------------------------------\n\nTITLE: Complete Splunk Enterprise Receiver Configuration Example\nDESCRIPTION: Provides a comprehensive example of configuring the Splunk Enterprise receiver, including authentication settings for multiple instances and pipeline setup.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-enterprise-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  basicauth/indexer:\n      client_auth:\n          username: admin\n          password: securityFirst\n  basicauth/cluster_master:\n      client_auth:\n          username: admin\n          password: securityFirst\n\nreceivers:\n  splunkenterprise:\n      indexer:\n          auth: \n            authenticator: basicauth/indexer\n          endpoint: \"https://localhost:8089\"\n          timeout: 45s\n      cluster_master:\n          auth: \n            authenticator: basicauth/cluster_master\n          endpoint: \"https://localhost:8089\"\n          timeout: 45s\n\nexporters:\n  logging:\n    loglevel: info\n\nservice:\n  extensions: [basicauth/indexer, basicauth/cluster_master]\n  pipelines:\n    metrics:\n      receivers: [splunkenterprise]\n      exporters: [logging]\n```\n\n----------------------------------------\n\nTITLE: Basic SAP HANA Monitor Configuration\nDESCRIPTION: Basic YAML configuration for activating the SAP HANA monitor in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/sap-hana.rst#2025-04-22_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nreceivers:\n  smartagent/hana:\n    type: hana\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop JMX for NameNode in Collector\nDESCRIPTION: YAML configuration for the Hadoop JMX integration to collect metrics from a NameNode. Specifies the host, port, and nodeType for the NameNode JMX connection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoopjmx.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/collectd/hadoopjmx:\n    type: collectd/hadoopjmx\n    host: 127.0.0.1\n    port: 5677\n    nodeType: nameNode\n```\n\n----------------------------------------\n\nTITLE: Setting Service Name via Java System Property (Shell)\nDESCRIPTION: Shows how to configure the OpenTelemetry service name using a Java system property (`-Dotel.service.name`) passed as a command-line argument when starting the Java application with the agent. System properties override corresponding environment variables if both are set.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/configuration/advanced-java-otel-configuration.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\njava -javaagent:./splunk-otel-javaagent.jar \\\n-Dotel.service.name=<my-java-app> \\\n-jar <myapp>.jar\n```\n\n----------------------------------------\n\nTITLE: Reporting Handled Exceptions in Android RUM\nDESCRIPTION: Shows how to report handled exceptions to appear as errors in the Splunk RUM UI, allowing visibility into application errors that don't crash the app.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\npublic boolean onOptionsItemSelected(MenuItem item) {\n   int id = item.getItemId();\n   if (id == R.id.action_settings) {\n      SplunkRum.getInstance()\n         .addRumException(\n            new UnsupportedOperationException(\"Unimplemented Feature: Settings\"),\n            SETTINGS_FEATURE_ATTRIBUTES);\n      return true;\n   }\n   return super.onOptionsItemSelected(item);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Trace Sampling with OpenTelemetry JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates how to configure trace sampling using OpenTelemetry to manage agent overhead. It involves setting up a custom sampling decision based on the span name, such as dropping spans named 'unwanted'. Dependencies include @splunk/otel and @opentelemetry/sdk-trace-base. The configured trace sampling aims to optimize resource usage by adjusting the span volume.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/performance.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require(\"@splunk/otel\");\nconst { SamplingDecision } = require(\"@opentelemetry/sdk-trace-base\");\n\nstart({\n  tracing: {\n    tracerConfig: {\n      sampler: {\n        shouldSample: (context, traceId, spanName, spanKind, attributes, links) => {\n          if (spanName ===  \"unwanted\") {\n            return { decision: SamplingDecision.NOT_RECORD };\n          }\n\n          return { decision: SamplingDecision.RECORD };\n        },\n        toString: () => return \"CustomSampler\",\n      }\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Annotate Unstructured Logs with Trace Metadata using Log Package in Go\nDESCRIPTION: This Go snippet illustrates adding trace metadata directly into an unstructured log message using the standard log package. Trace ID, span ID, and trace flags are appended to the log message to facilitate trace-log correlation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_2\n\nLANGUAGE: go\nCODE:\n```\n// Add the metadata following an order you can parse later on\nlog.Printf(\n\t\"(trace_id: %s, span_id: %s, trace_flags: %s): failed to fetch URL: %s\",\n\tspanContext.TraceID().String(),\n\tspanContext.SpanID().String(),\n\tspanContext.TraceFlags().String(),\n\turl,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector in Host Monitoring Mode for Related Content\nDESCRIPTION: Example configuration for the OpenTelemetry Collector in host monitoring (agent) mode that enables Related Content between Infrastructure and APM. Includes hostmetrics receiver for infrastructure data, resourcedetection processor for host information, and signalfx exporter for correlation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/relatedcontent-collector-apm.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  hostmetrics:\n    collection_interval: 10s\n    scrapers:\n      cpu:\n      disk:\n      filesystem:\n      memory:\n      network:\n\nprocessors:\n  resourcedetection:\n    detectors: [system,env,gcp,ec2]\n    override: true\n  resource/add_environment:\n    attributes:\n      - action: insert\n        value: staging\n        key: deployment.environment\n\nexporters:\n  # Traces\n  otlphttp:\n    access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n    endpoint: \"${SPLUNK_TRACE_URL}\"\n  # Metrics + Events + APM correlation calls\n  signalfx:\n    access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n    api_url: \"${SPLUNK_API_URL}\"\n    ingest_url: \"${SPLUNK_INGEST_URL}\"\n    sync_host_metadata: true\n    correlation:\n\nservice:\n  extensions: [health_check, http_forwarder, zpages]\n  pipelines:\n    traces:\n      receivers: [jaeger, zipkin]\n      processors: [memory_limiter, batch, resourcedetection, resource/add_environment]\n      exporters: [otlphttp, signalfx]\n    metrics:\n      receivers: [hostmetrics]\n      processors: [memory_limiter, batch, resourcedetection]\n      exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Adding Apache Spark Receiver to Metrics Pipeline in OpenTelemetry Collector (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to include the 'apachespark' receiver within the 'metrics' pipeline in the service section of the OpenTelemetry Collector configuration. This ensures that metrics collected by the receiver are processed and sent through the pipeline. Dependencies include that the receiver has been defined and that the pipeline is correctly set up in the Collector configuration file. This setup enables the transfer of monitored Spark metrics to subsequent processors and exporters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/apache-spark-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [apachespark]\n```\n\n----------------------------------------\n\nTITLE: Adding User Metadata During RUM Agent Initialization\nDESCRIPTION: Shows how to add user identification metadata as global attributes when initializing the Splunk RUM agent. This is useful when user data is accessible at initialization time.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/manual-rum-react-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst RumConfig: ReactNativeConfiguration = {\n   realm: '<realm>',\n   rumAccessToken: '<rum-access-token>',\n   applicationName: '<your-app-name>',\n   environment: '<your-environment>',\n   globalAttributes: {\n      enduser.id: 'user-id-123456',\n      enduser.role: 'premium'\n   },\n}\n```\n\n----------------------------------------\n\nTITLE: Adding User Identification During Initialization in Android RUM\nDESCRIPTION: Shows how to add user identification metadata as a global attribute during the initialization of the Splunk RUM agent.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nSplunkRum.builder()\n         .setGlobalAttributes(\n               Attributes.builder()\n                        // Adds existing userId\n                        .put(\"enduser.id\", \"user-id-123456\")\n                        .build())\n```\n\n----------------------------------------\n\nTITLE: Configuring .NET Receiver in Splunk Collector\nDESCRIPTION: Basic configuration to activate the .NET integration in the Splunk OpenTelemetry Collector. Defines the smartagent/dotnet receiver with type dotnet.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/microsoft-dotnet.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/dotnet:\n    type: dotnet\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Activating Ruby Auto-Instrumentation with Splunk OpenTelemetry\nDESCRIPTION: Ruby code snippet that activates automatic instrumentation by configuring the Splunk OpenTelemetry agent with auto_instrument set to true.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/instrument-ruby-application.rst#2025-04-22_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"splunk/otel\"\nrequire \"opentelemetry/instrumentation/all\"\nSplunk::Otel.configure(auto_instrument: true)\n```\n\n----------------------------------------\n\nTITLE: Adding NGINX VTS Monitor to Service Pipeline\nDESCRIPTION: Configuration snippet showing how to add the Prometheus NGINX VTS monitor to the metrics pipeline in the service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-nginx-vts.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/prometheus-nginx-vts]\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Batching with Size and Timeout in YAML\nDESCRIPTION: This example shows how to configure the batch processor to send batches after collecting 5,000 spans, data points, or logs, with a 15-second timeout as a fallback condition.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/batch-processor.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  batch/custom:\n    send_batch_size: 5000\n    timeout: 15s\n```\n\n----------------------------------------\n\nTITLE: Transferring APM Context to Custom URL - Splunk Observability Cloud - Text/URL\nDESCRIPTION: This example shows how to parameterize a custom URL to receive context from Splunk APM, such as arbitrary properties, service name, and time range. The placeholders {{key}}, {{value}}, {{properties.sf_service}}, {{start_time}}, and {{end_time}} are replaced at runtime with the appropriate values, allowing integration with external web applications. No external code dependencies are required, but endpoint applications should be able to consume the query parameters for context filtering.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-create-data-links.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttps://www.example.com/search/?field={{key}}&value={{value}}&service={{properties.sf_service}}&st={{start_time}}&et={{end_time}}\n```\n\n----------------------------------------\n\nTITLE: Initializing Browser RUM Agent - Basic Configuration\nDESCRIPTION: Basic initialization of the Splunk Browser RUM agent using CDN script with core configuration parameters including realm, access token, application name and version.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/configure-rum-browser-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\" crossorigin=\"anonymous\"></script>\n<script>\n      SplunkRum.init(\n      {\n         realm: 'us0',\n         rumAccessToken: 'ABC123...789',\n         applicationName: 'my-awesome-app',\n         version: '1.0.1'\n         // Any additional settings\n      });\n</script>\n```\n\n----------------------------------------\n\nTITLE: Running Splunk OpenTelemetry Collector Docker Container\nDESCRIPTION: Docker command to run the Splunk OpenTelemetry Collector container. Sets environment variables for access token and realm, maps ports, and specifies the container image.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -e SPLUNK_ACCESS_TOKEN=12345 -e SPLUNK_REALM=us0 \\\n    -p 13133:13133 -p 14250:14250 -p 14268:14268 -p 4317:4317 -p 6060:6060 \\\n    -p 7276:7276 -p 8888:8888 -p 9080:9080 -p 9411:9411 -p 9943:9943 \\\n    --name otelcol quay.io/signalfx/splunk-otel-collector:latest\n```\n\n----------------------------------------\n\nTITLE: Starting and Ending Workflows in Android RUM\nDESCRIPTION: Demonstrates how to track long-running operations using the workflow API, which creates spans that must be manually ended to properly record duration metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nbinding.buttonWork.setOnClickListener(v -> {\n   Span hardWorker =\n         SplunkRum.getInstance().startWorkflow(\"Main thread working hard\");\n   try {\n      Random random = new Random();\n      long startTime = System.currentTimeMillis();\n      while (true) {\n         random.nextDouble();\n         if (System.currentTimeMillis() - startTime > 20_000) {\n            break;\n         }\n      }\n   } finally {\n      hardWorker.end();\n   }\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OTel Collector Agent for Gateway Forwarding (YAML)\nDESCRIPTION: This YAML snippet shows a partial configuration for the Splunk OpenTelemetry Collector service, specifically the `logs` pipeline within the `service` section. It's configured for an agent collector that forwards data to a gateway collector. It defines an `otlp` receiver and configures the pipeline to use an `otlp` exporter instead of the direct `splunk_hec` exporter, indicating that the data is being sent to another OTLP endpoint (the gateway).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/profiling/get-data-in-profiling.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n   pipelines:\n      logs:\n         receivers: [otlp]\n         processors:\n         - memory_limiter\n         - batch\n         - resourcedetection\n         #- resource/add_environment\n         #exporters: [splunk_hec, splunk_hec/profiling]\n         # Use instead when sending to gateway\n         exporters: [otlp]\n```\n\n----------------------------------------\n\nTITLE: Setting OTLP Endpoint Programmatically in Go\nDESCRIPTION: Illustrates setting the OTLP exporter endpoint environment variable directly within Go code using `os.Setenv`. This method should be used before initializing the OpenTelemetry distribution (`distro.Run()`) and allows overriding any system-level environment variables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/configuration/advanced-go-otel-configuration.rst#2025-04-22_snippet_2\n\nLANGUAGE: go\nCODE:\n```\n// Before running distro.Run()\nos.Setenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"http://localhost:4317\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Trace and Resource Attribute Logging with Logback - XML\nDESCRIPTION: This Logback XML config adds trace_id, span_id, service.name, deployment.environment, and trace_flags to each log message using a pattern layout. Typically added in 'logback.xml', it relies on attribute propagation via the Splunk OTel Java agent. Compatible with Logback 1.0+ and requires setting up the corresponding MDC fields.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<configuration>\\n   <appender name=\\\"STDOUT\\\" class=\\\"ch.qos.logback.core.ConsoleAppender\\\">\\n      <encoder>\\n         <pattern>%d{yyyy-MM-dd HH:mm:ss} - %logger{36} - %msg trace_id=%X{trace_id} span_id=%X{span_id} service=%X{service.name}, env=%X{deployment.environment} trace_flags=%X{trace_flags} %n</pattern>\\n      </encoder>\\n   </appender>\\n   <root level=\\\"info\\\">\\n      <appender-ref ref=\\\"STDOUT\\\" />\\n   </root>\\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Applying OpenTelemetry Annotations in Different Namespaces\nDESCRIPTION: Command to patch a Kubernetes deployment in a different namespace to use the OpenTelemetry Collector from the monitoring namespace.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nkubectl patch deployment <my-deployment> -n <my-namespace> -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"instrumentation.opentelemetry.io/inject-<application_language>\":\"monitoring/splunk-otel-collector\"}}}}}}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk HEC Exporter for Logs Pipeline in YAML\nDESCRIPTION: This snippet shows a basic configuration for a Splunk HEC exporter instance in a logs pipeline. It includes settings for the HEC token, endpoint, source, and sourcetype.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-exporter.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  # ...\n  splunk_hec:\n    token: \"<hec-token>\"\n    endpoint: \"<hec-endpoint>\"\n    # Source. See https://docs.splunk.com/Splexicon:Source\n    source: \"otel\"\n    # Source type. See https://docs.splunk.com/Splexicon:Sourcetype\n    sourcetype: \"otel\"\n\n# ...\n```\n\n----------------------------------------\n\nTITLE: Configuring and Initializing the Android RUM Agent\nDESCRIPTION: Code snippet showing how to configure and initialize the Splunk RUM agent in the Application.onCreate() method, including setting application name, environment, and other configuration options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_5\n\nLANGUAGE: kotlin\nCODE:\n```\nimport com.splunk.rum.SplunkRum\nimport com.splunk.rum.StandardAttributes\nimport io.opentelemetry.api.common.Attributes\n\nclass MyApplication extends Application {\n   private final String realm = \"<realm>\";\n   private final String rumAccessToken = \"<your_RUM_access_token>\";\n\n   @Override\n   public void onCreate() {\n      super.onCreate();\n\n      SplunkRum.builder()\n               .setApplicationName(\"<name_of_app>\")\n               .setDeploymentEnvironment(\"<name_of_env>\") // Environment\n               .setRealm(realm)\n               .setRumAccessToken(rumAccessToken)\n               .setGlobalAttributes(\n                     Attributes.builder() // Add the application version. Alternatively, you\n                        // can pass BuildConfig.VERSION_NAME as the value.\n                        .put(StandardAttributes.APP_VERSION, \"<version_of_app>\")\n                        .build()\n               )\n               // Turn off instrumentation of background processes\n               .disableBackgroundTaskReporting(BuildConfig.<id_of_application>)\n               // Activates debug logging if needed\n               //.enableDebug()\n               .build(this);\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Feature Gates\nDESCRIPTION: Activate or deactivate features for otel-collector components using featureGates configuration in Helm commands. This example illustrates activating and deactivating features across agent, clusterReceiver, and gateway using Helm install command.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n   helm install {name} --set agent.featureGates=+feature1 --set clusterReceiver.featureGates=feature2 --set gateway.featureGates=-feature2 {other_flags}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Exporter Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration example for activating the Prometheus Exporter integration in the Collector configuration. Includes receiver setup with discovery rules and extra dimensions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-exporter.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/prometheus-exporter:\n    type: prometheus-exporter\n    discoveryRule: port >= 9100 && port <= 9500 && container_image =~ \"exporter\"\n    extraDimensions:\n       metric_source: prometheus    \n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Reporting Zero-Duration Custom Event (Android) in Java\nDESCRIPTION: This Java snippet shows how to report a simple, zero-duration custom event in an Android application using the Splunk RUM SDK. Within the `onCreateDialog` method, when the negative button (cancel) of an AlertDialog is clicked, it calls `SplunkRum.getInstance().addRumEvent()` to record an event named \"User Rejected Help\" along with predefined attributes (`HELPER_ATTRIBUTES`). This is useful for tracking discrete user actions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/RUM-custom-events.rst#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic Dialog onCreateDialog(Bundle savedInstanceState) {\n      LayoutInflater inflater = LayoutInflater.from(activity);\n      View alertView = inflater.inflate(R.layout.sample_mail_dialog, null);\n      AlertDialog.Builder builder = new AlertDialog.Builder(activity);\n      builder.setView(alertView)\n            .setNegativeButton(R.string.cancel, (dialog, id) ->\n                  // Record a simple \"zero duration\" span with the provided name and attributes\n                     SplunkRum.getInstance().addRumEvent(\"User Rejected Help\", HELPER_ATTRIBUTES));\n      return builder.create();\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Header Capture\nDESCRIPTION: Example demonstrating how to configure the capture of custom HTTP request and response headers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/configure-rum-android-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nbuilder.setCapturedRequestHeaders(asList(\"X-My-Custom-Request-Header\"))\nbuilder.setCapturedResponseHeaders(asList(\"X-My-Custom-Response-Header\"))\n```\n\n----------------------------------------\n\nTITLE: Including Cloud Foundry Receiver in Metrics Pipeline\nDESCRIPTION: This configuration example demonstrates how to include the Cloud Foundry receiver in the metrics pipeline of the service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/cloudfoundry-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [cloudfoundry]\n```\n\n----------------------------------------\n\nTITLE: Displaying Server Trace Information via HTTP Headers\nDESCRIPTION: Illustrates the HTTP response headers ('Access-Control-Expose-Headers' and 'Server-Timing') added by the Splunk OpenTelemetry Java agent instrumentation. These headers expose the server's 'traceId' and 'spanId' in 'traceparent' format to link Real User Monitoring (RUM) requests with server-side traces. This functionality is enabled by default and supported by frameworks like Servlet API and Netty. It can be disabled by setting `SPLUNK_TRACE_RESPONSE_HEADER_ENABLED` to `false`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/version1x/java-1x-otel-configuration.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nAccess-Control-Expose-Headers: Server-Timing\nServer-Timing: traceparent;desc=\"00-<serverTraceId>-<serverSpanId>-01\"\n```\n\n----------------------------------------\n\nTITLE: Configuring EC2 Resource Detection with Tag Collection\nDESCRIPTION: Configuration for detecting EC2 resources, environment variables, and selected tags without overwriting existing metadata.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/resourcedetection-processor.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/ec2:\n    detectors: [env, ec2]\n    timeout: 2s\n    override: false\n    ec2:\n      resource_attributes:\n        host.name:\n          enabled: false\n        host.id:\n          enabled: true\n      tags:\n        - ^tag1$\n        - ^tag2$\n        - ^label.*$\n```\n\n----------------------------------------\n\nTITLE: Setting B3 Propagator for Backward Compatibility in Windows PowerShell\nDESCRIPTION: Shows how to configure the b3multi trace propagator in Windows PowerShell for backward compatibility with the SignalFx Ruby Tracing Library.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/configuration/advanced-ruby-otel-configuration.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_PROPAGATORS=b3multi\n```\n\n----------------------------------------\n\nTITLE: Context-Aware Span Creation\nDESCRIPTION: Example showing how to create child spans using the OpenTelemetry Context API\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nimport {trace, context} from '@opentelemetry/api';\n\n// Create a tracer\nconst tracer = trace.getTracer('my-application', '1.0.0');\n\nasync function processForm(form) {\n   const span = tracer.startSpan('process form');\n   await context.with(trace.setSpan(context.active(), span), async () => {\n      \n      await client.send(form); // client.send would create a XHR span using instrumentation\n\n   });\n   span.end();\n}\n```\n\n----------------------------------------\n\nTITLE: Instrumenting In-Process Azure Function\nDESCRIPTION: Shows how to instrument an in-process Azure Function with OpenTelemetry, including adding FaaS-specific attributes to spans.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/azure/instrument-azure-functions-dotnet.rst#2025-04-22_snippet_4\n\nLANGUAGE: csharp\nCODE:\n```\npublic static class ExampleFunction\n{\n   [FunctionName(\"ExampleFunction\")]\n   public static async Task<IActionResult> Run(\n         [HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\", Route = null)] HttpRequest req,\n         ILogger log, ExecutionContext context)\n   {\n         Activity.Current.AddTag(\"faas.invocation_id\", context.InvocationId.ToString());\n         Activity.Current.AddTag(\"faas.name\", Environment.GetEnvironmentVariable(\"WEBSITE_SITE_NAME\") + \"/\" + context.FunctionName);\n\n         string responseMessage = \"The current time is \" + DateTime.Now.ToLongTimeString();\n         return new OkObjectResult(responseMessage);\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Manual Activation of Kubernetes Objects Receiver\nDESCRIPTION: YAML configuration to manually activate the Kubernetes Objects receiver in the Collector configuration, specifying authentication type and object collection details.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-objects-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nk8sobjects:\n  auth_type: serviceAccount\n  objects:\n    - name: pods\n      mode: pull\n      label_selector: environment in (production),tier in (frontend)\n      field_selector: status.phase=Running\n      interval: 15m\n    - name: events\n      mode: watch\n      group: events.k8s.io\n      namespaces: [default]\n```\n\n----------------------------------------\n\nTITLE: Importing OpenTelemetry API Modules - Go\nDESCRIPTION: Imports the OpenTelemetry API into your Go application, providing access to tracing and metrics functionality. The import is required before creating tracers, meters, or instruments. No function call or variable assignment is performed in this snippet; instead, it ensures all required OpenTelemetry API symbols are available.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/go-manual-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nimport \"go.opentelemetry.io/otel\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Metric Export in Python\nDESCRIPTION: Python code for setting up OpenTelemetry metric exporter, reader, and provider to send metrics to a collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/other-ingestion-methods/send-custom-metrics.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nexporter = OTLPMetricExporter(endpoint='<collector_ip_address>:4317', headers=None, insecure=True)\nreader = PeriodicExportingMetricReader(exporter)\nprovider = MeterProvider(metric_readers=[reader])\nset_meter_provider(provider)\n```\n\n----------------------------------------\n\nTITLE: Registering Context Propagators in JavaScript\nDESCRIPTION: This snippet demonstrates how to register context propagators using the OpenTelemetry API. It shows the usage of B3Propagator as an example.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/configure-rum-browser-instrumentation.rst#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport {propagation} from '@opentelemetry/api'; \nimport {B3Propagator} from '@opentelemetry/propagator-b3';\n\npropagation.setGlobalPropagator(new B3Propagator());\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Actions with OpenTelemetry Spans in JavaScript\nDESCRIPTION: Example showing how to instrument a CPU-intensive function using OpenTelemetry spans with custom attributes to track performance metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/migrate-manual-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport {trace} from '@opentelemetry/api'\n\nfunction calculateEstateTax(estate) {\n   const span = trace.getTracer('estate').startSpan('calculateEstateTax');\n   span.setAttribute('estate.jurisdictionCount', estate.jurisdictions.length);\n   var taxOwed = 0;\n   // ...\n   span.setAttribute('isTaxOwed', taxOwed > 0);\n   span.end();\n   return taxOwed;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Zero-Code Instrumentation with Environment Setting\nDESCRIPTION: Installation command that includes setting a deployment environment attribute for better resource identification. Demonstrates adding the deployment-environment parameter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-instrumentation --deployment-environment prod \\\n--realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Node.js Application Deployment YAML Without Instrumentation\nDESCRIPTION: Basic Kubernetes deployment manifest for a Node.js application before adding OpenTelemetry instrumentation annotations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nodejs-app\n  namespace: monitoring\nspec:\n  template:\n    spec:\n      containers:\n      - name: my-nodejs-app\n        image: my-nodejs-app:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring React Native RUM Agent with ReactNativeConfiguration Object\nDESCRIPTION: Example showing how to create a ReactNativeConfiguration object with basic settings including realm, access token, application name, environment, debug mode, and URL ignore patterns.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/configure-rum-react-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst RumConfig: ReactNativeConfiguration = {\n   realm: '<realm>',\n   rumAccessToken: '<rum-access-token>',\n   applicationName: '<your-app-name>',\n   environment: '<your-environment>',\n   debug: true,\n   /**\n      * URLs that partially match any regex in ignoreUrls aren't traced.\n      * URLs that are exact matches of strings in ignoreUrls aren't traced.\n   */\n   ignoreUrls: ['http://sampleurl.org'],\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Attributes Processor Actions\nDESCRIPTION: This YAML configuration snippet shows how to activate and configure the attributes processor in a pipeline. It demonstrates various actions like delete, upsert, update, insert, hash, and convert on attributes. Dependencies include having the Splunk Distribution of OpenTelemetry Collector. A typical use case is modifying attributes in a data processing pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/attributes-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  attributes/example:\n    actions:\n      - key: db.table\n        action: delete\n      - key: redacted_span\n        value: \"new_value\"\n        action: upsert\n      - key: copy_key\n        from_attribute: key_original\n        action: update\n      - key: account_id\n        value: 33445\n        action: insert\n      - key: account_password\n        action: delete\n      - key: account_email\n        action: hash\n      - key: http.status_code\n        action: convert\n        converted_type: int\n```\n\n----------------------------------------\n\nTITLE: Using Custom SplunkRum Error in JavaScript\nDESCRIPTION: Example demonstrating how to use the custom SplunkRum.error method to log errors to RUM telemetry.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-errors.rst#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\naxios.get('/users').then(users => {\n   showUsers(users)\n}).catch(error => {\n   showErrorMessage()\n   if (window.SplunkRum) {\n   SplunkRum.error('error getting users', error)\n   }\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Limiter Processor in OpenTelemetry Collector\nDESCRIPTION: YAML configuration for the memory_limiter processor to prevent out of memory issues. The configuration includes parameters for ballast size, check interval, memory limits, and spike limits to control memory usage.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/out_of_memory_error.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  memory_limiter:\n     ballast_size_mib: 2000\n     check_interval: 5s\n        # Check_interval is the time between measurements of memory usage for the  purposes of avoiding goingover the limits. \n        # The default is 0. Values below 1s are not recommended, as this can result in unnecessary CPU consumption.\n     limit_mib: 4000\n        # ​​Maximum amount of memory, in MiB, targeted to be allocated by the process heap.\n        # The total memory usage of the process is typically about 50 MiB higher than this value.\n     spike_limit_mib: 500\n        # The maximum, in MiB, spike expected between the measurements of memory usage.\n     ballast_size_mib: 2000\n        # BallastSizeMiB is the size, in MiB, of the ballast size being used by the process.\n        # This must match the value of the mem-ballast-size-mib command line option (if used).\n        # Otherwise, the memory limiter does not work correctly.\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Snowflake Receiver Configuration Example (YAML)\nDESCRIPTION: This snippet provides a complete configuration example for the Snowflake receiver, including authentication details, collection interval, and metric settings. It demonstrates how to enable or disable specific metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/snowflake-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  snowflake:\n    username: snowflakeuser\n    password: securepassword\n    account: bigbusinessaccount\n    warehouse: metricWarehouse\n    collection_interval: 5m\n    metrics:\n      snowflake.database.bytes_scanned.avg:\n        enabled: true\n      snowflake.query.bytes_deleted.avg:\n        enabled: false\n```\n\n----------------------------------------\n\nTITLE: Setting Node Name in Pod Spec for Service Account Authentication (YAML)\nDESCRIPTION: Sets an environment variable K8S_NODE_NAME to the Kubernetes node's name using Kubernetes fieldRef in the pod specification. This is necessary for the kubeletstats receiver to dynamically reference the current node when using service account authentication. It is used alongside the main receiver configuration and should be included in the pod's env section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubelet-stats-receiver.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  - name: K8S_NODE_NAME\n    valueFrom:\n      fieldRef:\n        fieldPath: spec.nodeName\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Monitor for Primary Shard Stats\nDESCRIPTION: YAML configuration example showing how to set up an Elasticsearch monitor to collect statistics from primary shards only. The configuration specifies the monitor type, host, port, and enables primary shard statistics collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/elasticsearch.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmonitors:\n- type: elasticsearch\n  host: localhost\n  port: 9200\n  enableIndexStatsPrimaries: true\n```\n\n----------------------------------------\n\nTITLE: Sending Metrics with SignalFx Client (JavaScript)\nDESCRIPTION: This snippet provides the legacy method for sending custom metrics using the SignalFx client in Node.js. It shows a direct call to 'getSignalFxClient().send()', where metrics are defined as gauges and cumulative counters with explicit values and timestamps. The code is meant for users migrating from SignalFx and showcases how to structure metric payloads. Requires a SignalFx client instance and appropriate configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/nodejs-otel-metrics.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// SignalFx\ngetSignalFxClient().send({\n   gauges: [{ metric: 'cpu', value: 42, timestamp: 1442960607000}],\n   cumulative_counters: [{ metric: 'clicks', value: 99, timestamp: 1442960607000}],\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring Gateway Mode Configuration File Path in YAML\nDESCRIPTION: Path specification for the gateway configuration file and listen interface settings. The default gateway configuration file is located at /otelcol/config/ta-gateway-config.yaml.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-addon/collector-addon-configure-instance.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nSplunk_config: ta-gateway-config.yaml\nsplunk_listen_interface: \"0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Webpack for Node.js Instrumentation - JavaScript\nDESCRIPTION: This example demonstrates how to modify the 'webpack.config.js' file so that OpenTelemetry can instrument the 'express' framework by including it in the 'externals' array. The configuration sets 'externalsType' to 'node-commonjs' so that require calls are preserved, essential for OpenTelemetry's automatic instrumentation. Ensure all listed external modules (e.g., 'express') are installed in node_modules. No exports or direct invocation; this is loaded by Webpack during bundling.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/troubleshooting/common-nodejs-troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nmodule.exports = {\n   // ...\n   externalsType: \"node-commonjs\",\n   externals: [\n      \"express\"\n   // See https://github.com/open-telemetry/opentelemetry-js-contrib/tree/main/plugins/node\n   // for a list of supported instrumentations. Use the require name of the library or framework,\n   // not the name of the instrumentation. For example, \"tedious\" instead of \"instrumentation-tedious\".\n   ]\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes API Authentication in YAML\nDESCRIPTION: YAML configuration structure showing available options for authenticating with the Kubernetes API server, including service account, TLS, and kubeConfig methods.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-cluster.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nkubernetesAPI:\n  authType: serviceAccount  # none, tls, serviceAccount, kubeConfig\n  skipVerify: false\n  clientCertPath: \n  clientKeyPath: \n  caCertPath: \n```\n\n----------------------------------------\n\nTITLE: Activating Profiling Programmatically (JavaScript)\nDESCRIPTION: Demonstrates how to activate CPU and Memory AlwaysOn Profiling within the application code using the `start` function from `@splunk/otel`. The `profiling` option activates CPU profiling, and `memoryProfilingEnabled: true` within it activates memory profiling.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nstart({\n   serviceName: '<service-name>',\n   endpoint: 'collectorhost:port',\n   profiling: {                       // Activates CPU profiling\n      memoryProfilingEnabled: true,   // Activates Memory profiling\n   }\n});\n```\n\n----------------------------------------\n\nTITLE: Disabling Log Data Export to Splunk Observability Cloud in YAML\nDESCRIPTION: This configuration example shows how to turn off log data export to Splunk Observability Cloud, for instance when using Log Observer Connect, by setting log_data_enabled to false in the Splunk HEC exporter settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-exporter.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsplunk_hec:\n  token: \"${SPLUNK_HEC_TOKEN}\"\n  endpoint: \"${SPLUNK_HEC_URL}\"\n```\n\n----------------------------------------\n\nTITLE: Adding OpenTelemetry Dependencies in CMakeLists.txt (C++)\nDESCRIPTION: Integrates the built OpenTelemetry C++ libraries into a C++ project using CMake. It finds the OpenTelemetry package configuration and links the necessary include directories and libraries to the specified target (replace `foo` with your application's target name). Assumes the OpenTelemetry C++ libraries have been built and installed in a location findable by CMake.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/cpp/instrument-cpp.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nfind_package(opentelemetry-cpp CONFIG REQUIRED)\n\ntarget_include_directories(foo PRIVATE ${OPENTELEMETRY_CPP_INCLUDE_DIRS})\ntarget_link_libraries(foo PRIVATE ${OPENTELEMETRY_CPP_LIBRARIES})\n```\n\n----------------------------------------\n\nTITLE: Defining Custom SLI Metrics using SignalFlow in Python\nDESCRIPTION: This SignalFlow code snippet demonstrates how to define 'good' and 'total' event streams for a custom metric Service Level Indicator (SLI) configuration within a Splunk Observability Cloud SLO. It uses the `data()` function to fetch metric time series (MTS) for 'good' and 'total' events, and the `filter()` function to differentiate successful requests (`sf_error` dimension is 'false') for the 'good' events variable (G). The 'total' events variable (T) represents all relevant events. These variables are then used as the numerator and denominator for calculating the SLI.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/slo/create-slo.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nG = data('good.metric', filter=filter('sf_error', 'false'))\nT = data('total.metric')\n```\n\n----------------------------------------\n\nTITLE: Configuring Rack Instrumentation for Server Trace Information in Ruby\nDESCRIPTION: Shows how to configure Rack instrumentation to connect Real User Monitoring (RUM) requests with server trace data. This includes setting up the necessary middleware in a Rack::Builder application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/configuration/advanced-ruby-otel-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nSplunk::Otel.configure do |c|\n   c.use \"OpenTelemetry::Instrumentation::Rack\"\nend\n\n# Add the middleware in Rack::Builder\nRack::Builder.app do\n   use OpenTelemetry::Instrumentation::Rack::Middlewares::TracerMiddleware\n   use Splunk::Otel::Rack::RumMiddleware\n   run ->(_env) { [200, { \"content-type\" => \"text/plain\" }, [\"OK\"]] }\nend\n```\n\n----------------------------------------\n\nTITLE: Enabling Specific OpenTelemetry Instrumentations with Shell\nDESCRIPTION: This shell snippet shows how to configure the environment to selectively enable specific OpenTelemetry instrumentations, such as Bunyan, while disabling the common default ones. This approach helps reduce agent overhead by only activating necessary instrumentations. It involves setting environment variables like OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED and OTEL_INSTRUMENTATION_BUNYAN_ENABLED.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/performance.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED=false\nexport OTEL_INSTRUMENTATION_BUNYAN_ENABLED=true\n```\n\n----------------------------------------\n\nTITLE: Adding User Identification Metadata After Initialization\nDESCRIPTION: This example shows how to add user identification metadata as global attributes after the Splunk RUM agent has been initialized. This approach is useful when user data isn't available during initialization.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: swift\nCODE:\n```\nSplunkRum.setGlobalAttributes([\"enduser.id\": \"user-id-123456\"])\nSplunkRum.setGlobalAttributes([\"enduser.id\": \"128762\"]);\nSplunkRum.setGlobalAttributes([\"enduser.role': \"premium\"]);\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Distribution with Helm Command Line Parameters\nDESCRIPTION: Command line examples for configuring different Kubernetes distributions when installing the Collector using Helm. Options shown for AKS, EKS, GKE, and OpenShift deployments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# aks deployment\n--set distribution=aks,cloudProvider=azure \n\n# eks deployment\n--set distribution=eks,cloudProvider=aws \n\n# eks/fargate deployment (with recommended gateway)\n--set distribution=eks/fargate,gateway.enabled=true,cloudProvider=aws \n\n# gke deployment\n--set distribution=gke,cloudProvider=gcp \n\n# gke/autopilot deployment\n--set distribution=gke/autopilot,cloudProvider=gcp \n\n# openshift deployment (openshift can run on multiple cloud providers, so cloudProvider is excluded here)\n--set distribution=openshift   \n```\n\n----------------------------------------\n\nTITLE: Building a Gauge Metric\nDESCRIPTION: This example demonstrates how to define a gauge metric using the Meter instance in OpenTelemetry Java. It includes setting a description and unit while registering a callback to record values, suitable for tracking resources like player hit points. Callback registration involves recording dynamic values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-manual-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nsampleMeter\n   .gaugeBuilder(\"player.hitpoints\")\n   .setDescription(\"A player's currently remaining hit points.\")\n   .setUnit(\"HP\")\n   .ofLongs()\n   .buildWithCallback(res -> {\n       long hitpoints = currentPlayer.hitpoints();\n       String playerName = currentPlayer.name();\n       res.record(hitpoints, Attributes.of(stringKey(\"name\"), playerName)));\n   });\n   .buildWithCallback(\n      result -> result.record(Runtime.getRuntime().totalMemory(), Attributes.empty()));\n```\n\n----------------------------------------\n\nTITLE: Including JMX Receiver in Metrics Pipeline\nDESCRIPTION: This configuration snippet demonstrates how to include the JMX receiver in the 'metrics' pipeline of the 'service' section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/jmx-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [jmx]\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Attributes via Environment Variable in Shell\nDESCRIPTION: This shell command sets resource attributes using the OTEL_RESOURCE_ATTRIBUTES environment variable for Splunk OpenTelemetry Go instrumentation. The attributes are injected into all traces for enhanced metadata and correlation in observability backends. Each key-value pair is comma-separated, with keys and values as strings. You must ensure proper quoting and placement within your application startup script.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/migrate-signalfx-go-to-otel.rst#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n   export OTEL_RESOURCE_ATTRIBUTES=\"ab-test-value=red,owner=Lisa\"\n```\n\n----------------------------------------\n\nTITLE: Adding Extra Metadata Labels to Kubelet Stats Receiver (YAML)\nDESCRIPTION: Enables the addition of metadata attributes (e.g., container.id) to metrics via extra_metadata_labels. Sets up the receiver to authenticate using a service account, gather metrics at a 10-second interval, and connect using the K8S_NODE_NAME variable for endpoint resolution. This configuration enhances metric enrichment for more detailed observability.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubelet-stats-receiver.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 10s\n    auth_type: \"serviceAccount\"\n    endpoint: \"${K8S_NODE_NAME}:10250\"\n    insecure_skip_verify: true\n    extra_metadata_labels:\n      - container.id\n\n```\n\n----------------------------------------\n\nTITLE: Managing Node.js Instrumentations via Environment Variables\nDESCRIPTION: This shell script configures the Splunk OTel JS agent to disable all common default instrumentations and enable only the Bunyan instrumentation. Users should configure `OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED` and specific `OTEL_INSTRUMENTATION_<NAME>_ENABLED` environment variables to manage instrumentations effectively.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/performance.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED=false\nexport OTEL_INSTRUMENTATION_BUNYAN_ENABLED=true\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry PHP Extension with Environment Variables - Bash\nDESCRIPTION: This snippet sets environment variables to enable the OpenTelemetry PHP autoloader, specify the service name, and set deployment environment attributes. These variables should be exported or set prior to launching the PHP application process. Inputs are user-defined environment values. Outputs are customized telemetry data sent according to these values. Environment variables should be adapted to the service and deployment needs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/instrument-php-application.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nOTEL_PHP_AUTOLOAD_ENABLED=true \\\nOTEL_SERVICE_NAME=\\\"<your-service-name>\\\" \\\nOTEL_RESOURCE_ATTRIBUTES=\\\"deployment.environment=<your_env>\\\" \\\n```\n\n----------------------------------------\n\nTITLE: Specifying Advanced Logging Exporter Sampling in YAML\nDESCRIPTION: This YAML configuration displays the use of detailed verbosity in the logging exporter, alongside sampling parameters: sampling_initial (number of initial messages per second) and sampling_thereafter (subsequent frequency). The snippet illustrates how to control log volume for high-frequency telemetry output, minimizing resource impact while still capturing diagnostic information. Inputs are verbosity and sampling keys; outputs are sample logs based on the specified thresholds.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/logging-exporter.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  logging:\n    verbosity: detailed\n    sampling_initial: 5\n    sampling_thereafter: 200\n```\n\n----------------------------------------\n\nTITLE: Metric Names for Data Flow Monitoring in OpenTelemetry Collector\nDESCRIPTION: Essential metrics for monitoring data ingestion and export in the OpenTelemetry Collector. These metrics track accepted spans, metric points, and logs for both receiving and sending operations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/metrics-internal-collector.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\notelcol_receiver_accepted_spans\notelcol_receiver_accepted_metric_points\notelcol_receiver_accepted_logs\notecol_exporter_sent_spans\notelcol_exporter_sent_metric_points\notelcol_exporter_sent_logs\n```\n\n----------------------------------------\n\nTITLE: Configuring Histogram Data Export to Splunk in YAML\nDESCRIPTION: The YAML snippet illustrates how to configure the sending of OTLP histogram data to Splunk Observability Cloud by enabling the `send_otlp_histograms` option. It requires setting proper access, API, and ingest URLs, which are specified through environment variables. The focus is on modifying the `signalfx` exporter configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/histograms.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   exporters:\\n     signalfx:\\n       access_token: \\\"${SPLUNK_ACCESS_TOKEN}\\\"\\n       api_url: \\\"${SPLUNK_API_URL}\\\"\\n       ingest_url: \\\"${SPLUNK_INGEST_URL}\\\"\\n       sync_host_metadata: true\\n       correlation:\\n       send_otlp_histograms: true\n```\n\n----------------------------------------\n\nTITLE: Creating an Observable Gauge in Python with OpenTelemetry\nDESCRIPTION: Sets up an observable gauge metric with a callback function to periodically collect temperature data in Python.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmeter.create_observable_gauge(\n   \"city.temperature\",\n   callbacks=[get_temp_data],\n   description=\"Mean temperature of the city\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting User ID Globally in JavaScript\nDESCRIPTION: Shows how to set a user ID globally using the Splunk RUM JavaScript API.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport SplunkRum from '@splunk/otel-web';\n\nSplunkRum.setGlobalAttributes({\n   'enduser.id': 'Test User'\n});\n```\n\n----------------------------------------\n\nTITLE: Including Kubernetes Cluster Receiver in Metrics Pipeline\nDESCRIPTION: Configuration snippet showing how to include the Kubernetes cluster receiver in the metrics pipeline of the service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-cluster-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [k8s_cluster]\n```\n\n----------------------------------------\n\nTITLE: Recording Data for a Custom .NET Counter Metric (C#)\nDESCRIPTION: Records a measurement by incrementing the value of the `counter` metric instrument by 1. This method call generates a data point for the \"custom.counter\" metric, which will be collected by the OpenTelemetry pipeline if the associated `Meter` is registered.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/manual-dotnet-instrumentation.rst#2025-04-22_snippet_6\n\nLANGUAGE: csharp\nCODE:\n```\ncounter.Add(1);\n```\n\n----------------------------------------\n\nTITLE: Activating Systemd Monitor in Collector Configuration\nDESCRIPTION: YAML configuration snippet to activate the systemd monitor in the OpenTelemetry Collector. It defines the receiver and adds it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/systemd.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/systemd:\n    type: collectd/systemd\n    ...  # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/systemd]\n```\n\n----------------------------------------\n\nTITLE: Server Trace Information Headers\nDESCRIPTION: This plaintext snippet shows the HTTP headers used to connect Real User Monitoring (RUM) requests from mobile and web apps with server trace data. The Server-Timing header includes parameters in traceparent format to facilitate this connection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/configuration/advanced-dotnet-configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nAccess-Control-Expose-Headers: Server-Timing\nServer-Timing: traceparent;desc=\"00-<serverTraceId>-<serverSpanId>-01\"\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Lambda Environment Variables\nDESCRIPTION: Configuration settings for connecting Lambda functions to EC2 Collector gateway, including OTLP endpoints and protocol settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/aws/otel-lambda-layer/instrumentation/lambda-ec2-collector-gateway.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT: http://10.0.0.123:4318/v1/traces\nOTEL_TRACES_EXPORTER: otlp\nOTLP_EXPORTER_OTLP_TRACES_PROTOCOL: http/protobuf\nSPLUNK_LAMBDA_LOCAL_COLLECTOR_ENABLED: false\nSPLUNK_METRICS_ENDPOINT: http://10.0.0.123:9943\n```\n\n----------------------------------------\n\nTITLE: Configuring Cumulative to Delta Processor in YAML\nDESCRIPTION: This snippet shows how to configure the cumulative to delta processor in the OpenTelemetry Collector's YAML configuration file. It demonstrates how to include and exclude specific metrics for conversion to delta temporality.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/cumulative-to-delta-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n   cumulativetodelta:\n      include:\n            metrics:\n               - <metric_1_name>\n               - <metric_2_name>\n               - <metric_n_name>\n            match_type: strict\n      #\n      #  Exclude rules take precedence over include rules\n      #\n      exclude:\n            metrics:\n               - \".*metric.*\"\n            match_type: regexp\n```\n\n----------------------------------------\n\nTITLE: Grouping Metrics by Host Attribute\nDESCRIPTION: Configuration example for grouping metrics based on the 'host.name' attribute, demonstrating how to reassociate metrics with specific hosts.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/groupbyattrs-processor.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  groupbyattrs:\n    keys:\n      - host.name\n```\n\n----------------------------------------\n\nTITLE: Configuring AlwaysOn Profiling for Node.js\nDESCRIPTION: YAML configuration for enabling AlwaysOn Profiling in Node.js applications with custom sampling interval settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-advanced-config.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\noperatorcrds:\n  install: true\noperator:\n  enabled: true\ninstrumentation:  \n  spec:\n    nodejs:\n      env:\n      - name: SPLUNK_PROFILER_ENABLED\n        value: \"true\"\n      - name: SPLUNK_PROFILER_MEMORY_ENABLED\n        value: \"true\"\n      - name: SPLUNK_PROFILER_CALL_STACK_INTERVAL\n        value: 5000\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry SDK with Sinatra Instrumentation in Ruby\nDESCRIPTION: This code configures the OpenTelemetry SDK to use the Sinatra instrumentation library. It demonstrates how to require the SDK and set up the configuration block with options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/ruby-manual-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"opentelemetry/sdk\"\nOpenTelemetry::SDK.configure do |c|\n   c.use \"OpenTelemetry::Instrumentation::Sinatra\", { opt: \"value\" }\nend\n```\n\n----------------------------------------\n\nTITLE: Referencing Splunk Environment Variables in RST Format\nDESCRIPTION: This RST table lists all Splunk-specific environment variables for the OpenTelemetry Collector, including their names, descriptions, and whether they're part of the default configuration. Variables include authentication tokens, endpoints, memory settings, and configuration options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/collector-env-vars.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n    :widths: 15 75 10\n    :width: 100\n    :header-rows: 1\n\n    *   - Name\n        - Description\n        - Default config?\n    *   - ``SPLUNK_ACCESS_TOKEN`` \n        - The Splunk access token to authenticate requests\n        - Yes\n    *   - ``SPLUNK_API_URL`` \n        - The Splunk API URL. For example, https://api.us0.signalfx.com\n        - Yes\n    *   - ``SPLUNK_BALLAST_SIZE_MIB`` (deprecated)\n        - ``memory_ballast`` is deprecated. If you're using this variable, see :ref:`how to update your configuration <collector-upgrade-memory-ballast>`.\n        - No\n    *   - ``SPLUNK_BUNDLE_DIR`` \n        - The path to the Smart Agent bundle. For example, ``/usr/lib/splunk-otel-collector/agent-bundle``\n        - Yes\n    *   - ``SPLUNK_COLLECTD_DIR``\n        - The path to the collectd config directory for the Smart Agent. For example, ``/usr/lib/splunk-otel-collector/agent-bundle/run/collectd``\n        - Yes\n    *   - ``SPLUNK_CONFIG`` \n        - Destination path of the Collector custom configuration file \n        - No\n    *   - ``SPLUNK_CONFIG_YAML`` \n        - Specifies your custom configuration YAML. This is useful in environments where access to the underlying file system is not readily available\n        - No\n    *   - ``SPLUNK_DEBUG_CONFIG_SERVER`` \n        - By default, the Collector provides a sensitive value-redacting, local config server listening at http://localhost:55554/debug/configz/effective, which is helpful in troubleshooting. To disable it, set ``SPLUNK_DEBUG_CONFIG_SERVER`` to any value other than ``true``. To set the desired port to listen to, use ``SPLUNK_DEBUG_CONFIG_SERVER_PORT``\n        - No\n    *   - ``SPLUNK_HEC_TOKEN`` \n        - The Splunk HEC authentication token\n        - Yes\n    *   - ``SPLUNK_HEC_URL`` \n        - The Splunk HEC endpoint URL. For example, https://ingest.us0.signalfx.com/v1/log\n        - Yes\n    *   - ``SPLUNK_INGEST_URL`` \n        - The Splunk ingest URL. For example, https://ingest.us0.signalfx.com\n        - Yes\n    *   - ``SPLUNK_LISTEN_INTERFACE`` \n        - The network interface the agent receivers listen on. ``0.0.0.0`` by default\n        - Yes\n    *   - ``SPLUNK_MEMORY_LIMIT_MIB`` \n        - Use it to set the memory limit for the ``memory_limiter`` processor. 512 MiB by default \n        - No\n    *   - ``SPLUNK_MEMORY_TOTAL_MIB`` \n        - Total memory in MiB to allocate to the Collector\n        - No\n    *   - ``SPLUNK_REALM`` \n        - Your Splunk realm\n        - No\n    *   - ``SPLUNK_TRACE_URL`` \n        - The Splunk trace endpoint URL. For example, https://ingest.us0.signalfx.com/v2/trace\n        - Yes\n```\n\n----------------------------------------\n\nTITLE: Changing Trace Propagator Programmatically in Go\nDESCRIPTION: Demonstrates how to change the trace context propagator after the Splunk OpenTelemetry distribution has been initialized (`distro.Run()`). This example sets the propagator to use W3C `TraceContext` using `otel.SetTextMapPropagator`. This influences how trace context is injected into and extracted from requests.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/configuration/advanced-go-otel-configuration.rst#2025-04-22_snippet_3\n\nLANGUAGE: go\nCODE:\n```\ndistro.Run()\n// Change propagator after distro.Run() has been invoked\notel.SetTextMapPropagator(propagation.TraceContext{})\n```\n\n----------------------------------------\n\nTITLE: Configuring ResourceDetection Processor in OpenTelemetry Helm Values\nDESCRIPTION: YAML configuration for overriding host attributes in OpenTelemetry Collector using the resourcedetection processor. This configuration enables system detection and applies the override to both metrics and traces pipelines.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/other-ingestion-methods/set-host-name-attribute-override.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nagentCollector:\n  configOverride:\n    processors:\n      resourcedetection:\n        detectors:\n          - system\n        override: true\n    service:\n      pipelines:\n        metrics:\n          processors:\n            - memory_limiter\n            - batch\n            - resourcedetection\n            # Add other custom processors as needed.\n        traces:\n          processors:\n            - memory_limiter\n            - batch\n            - resourcedetection\n            # Add other custom processors as needed.\n```\n\n----------------------------------------\n\nTITLE: Instrumenting React Navigation\nDESCRIPTION: Code example demonstrating how to instrument React Navigation by using the startNavigationTracking function with a navigation reference.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/install-rum-react.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { startNavigationTracking } from '@splunk/otel-react-native';\n\nexport default function App() {\n   const navigationRef = useNavigationContainerRef();\n   return (\n      <NavigationContainer\n         ref={navigationRef}\n         onReady={() => {\n            startNavigationTracking(navigationRef);\n         }}\n      >\n         <Stack.Navigator>\n         // ...\n         </Stack.Navigator>\n      </NavigationContainer>\n   );\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Span Attributes in Java using OpenTelemetry SDK\nDESCRIPTION: Demonstrates how to obtain the current span and add a custom attribute (span tag) named 'my.attribute' with a specific value using the OpenTelemetry Java SDK. It also notes the possibility of setting global tags via the OTEL_RESOURCE_ATTRIBUTES environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/add-context-trace-span.rst#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n// Splunk Distribution of OpenTelemetry Java\n\nimport io.opentelemetry.api.trace.Span;\n\nSpan customizedSpan = Span.current();\n\ncustomizedSpan.setAttribute(\"my.attribute\",\"value\");\n\n// You can also set global tags using the OTEL_RESOURCE_ATTRIBUTES\t\n// environment variable, which accepts a list of comma-separated key-value\n// pairs. For example, key1:val1,key2:val2.  \n```\n\n----------------------------------------\n\nTITLE: Initializing SplunkRum with Configuration in Swift\nDESCRIPTION: Example showing how to initialize the SplunkRum module with various configuration options in Swift, including realm, RUM token, debug mode, global attributes, environment name, application name, URL filtering, and screen name spans.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/configure-rum-ios-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: swift\nCODE:\n```\nimport SplunkOtel\n//...\nSplunkRumBuilder(realm: \"<realm>\", rumAuth: \"<rum-token>\")\n  // Call functions to configure additional options\n  .debug(enabled: true)\n  .globalAttributes(globalAttributes: [\"strKey\": \"strVal\", \"intKey\": 7, \"doubleKey\": 1.5, \"boolKey\": true])\n  .deploymentEnvironment(environment: \"env\")\n  .setApplicationName(\"<your_app_name>\")\n  .ignoreURLs(ignoreURLs: try! NSRegularExpression(pattern: \".*ignore_this.*\"))\n  .screenNameSpans(enabled: true)\n  // The build method always come last\n  .build()\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk HEC Receiver with All Available Settings\nDESCRIPTION: Comprehensive YAML configuration example for the Splunk HEC receiver, including endpoint, access token passthrough, custom paths, field mappings, and TLS settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  # ...\n  splunk_hec:\n  # Address and port the Splunk HEC receiver should bind to\n  endpoint: localhost:8088\n  # Whether to preserve incoming access token\n  access_token_passthrough: true\n  # Path accepting raw HEC events (logs only)\n  raw_path: \"/foo\"\n  # Path reporting health checks\n  health_path: \"/bar\"\n  # Define field mappings\n  hec_metadata_to_otel_attrs:\n    source: \"file.name\"\n    sourcetype: \"foobar\"\n    index: \"myindex\"\n    host: \"myhostfield\"\n  # Optional TLS settings\n  tls:\n    # Both cert_file and\n    # key_file are required\n    # for TLS connections\n    cert_file: /test.crt\n    key_file: /test.key\n```\n\n----------------------------------------\n\nTITLE: Adding User Metadata During Initialization in HTML\nDESCRIPTION: Shows how to add user identification metadata as global attributes when initializing the Splunk RUM agent in HTML.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\" crossorigin=\"anonymous\"></script>\n<script>\nSplunkRum.init({\n   realm: '<realm>',\n   rumAccessToken: '<RUM access token>',\n   applicationName: '<application-name>',\n   globalAttributes: {\n      // The following data is already available\n      'enduser.id': 42,\n      'enduser.role': 'admin',\n   },\n});\n</script>\n```\n\n----------------------------------------\n\nTITLE: Activating Kubernetes Objects Receiver with Helm\nDESCRIPTION: Helm configuration to activate the Kubernetes Objects receiver, specifying object types, collection modes, and selectors.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-objects-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nk8sObjects:\n  - name: pods\n    mode: pull\n    label_selector: environment in (production),tier in (frontend)\n    field_selector: status.phase=Running\n    interval: 15m\n  - name: events\n    mode: watch\n    group: events.k8s.io\n    namespaces: [default]\n```\n\n----------------------------------------\n\nTITLE: Modifying Profiling Settings in YAML Configuration - YAML\nDESCRIPTION: Shows how to toggle the profilingEnabled and logsEnabled options in the collector's values.yaml file. This controls whether logs and profiling data pipelines are active. Instrumentation libraries must be separately configured for proper data collection. Set profilingEnabled or logsEnabled to true or false as needed. No external dependencies required beyond editing YAML configuration files for Helm or similar.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/profiling/get-data-in-profiling.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# This option enables only the shared pipeline for logs and profiling data.\\n# There is no active collection of profiling data.\\n# Instrumentation libraries must be configured to send it to the collector.\\n# If you don't use AlwaysOn Profiling for Splunk APM, you can disable it.\\nprofilingEnabled: false\n```\n\nLANGUAGE: yaml\nCODE:\n```\nlogsEnabled: false\n```\n\n----------------------------------------\n\nTITLE: Running Docker container with custom configuration using --config flag\nDESCRIPTION: PowerShell command to run the OpenTelemetry Collector with a mounted custom configuration file using the --config command line argument instead of the SPLUNK_CONFIG environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows-manual.rst#2025-04-22_snippet_3\n\nLANGUAGE: PowerShell\nCODE:\n```\n$ docker run --rm -e SPLUNK_ACCESS_TOKEN=12345 -e SPLUNK_REALM=us0 `\n      -p 13133:13133 -p 14250:14250 -p 14268:14268 -p 4317:4317 -p 6060:6060 `\n      -p 8888:8888 -p 9080:9080 -p 9411:9411 -p 9943:9943 `\n      -v ${PWD}\\splunk_config:c:\\splunk_config:RO `\n      --name otelcol quay.io/signalfx/splunk-otel-collector-windows:latest `\n      --config c:\\splunk_config\\gateway_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser RUM Sampling - NPM Implementation\nDESCRIPTION: Example of configuring sampling settings for RUM data collection using NPM implementation, showing how to import and use samplers from OpenTelemetry core.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/configure-rum-browser-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// When using npm you can get samplers directly from @opentelemetry/core\nimport {AlwaysOnSampler, AlwaysOffSampler} from '@opentelemetry/core';\nimport SplunkOtelWeb from '@splunk/otel-web';\n\nSplunkOtelWeb.init({\n   beaconEndpoint: 'https://rum-ingest..signalfx.com/v1/rum',\n   rumAccessToken: '<your_rum_token>', \n   applicationName: '<application-name>', \n   tracer: { \n      sampler: userShouldBeTraced() ? new SplunkRum.AlwaysOnSampler() : new SplunkRum.AlwaysOffSampler(),\n   },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Smart Agent Receivers and Pipelines in OTel Collector\nDESCRIPTION: Example YAML configuration showing how to set up Smart Agent receivers for PostgreSQL, process list, and Kafka monitoring, along with corresponding pipeline configurations for metrics, logs, and traces using the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-monitors.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n   smartagent/postgresql:\n      type: postgresql\n      host: mypostgresinstance\n      port: 5432\n      dimensionClients:\n         - signalfx\n   smartagent/processlist:\n      type: processlist\n   smartagent/kafka:\n      type: collectd/kafka\n      host: mykafkabroker\n      port: 7099\n      clusterName: mykafkacluster\n      intervalSeconds: 5\n\nprocessors:\n   resourcedetection:\n      detectors:\n         - system\n\nexporters:\n   signalfx:\n      access_token: \"${SIGNALFX_ACCESS_TOKEN}\"\n      realm: us1\n   otlphttp:\n      access_token: \"${SIGNALFX_ACCESS_TOKEN}\"\n      traces_endpoint: https://ingest.us1.signalfx.com/v2/trace/otlp\n\nservice:\n   pipelines:\n      metrics:\n         receivers:\n            - smartagent/postgresql\n            - smartagent/kafka\n            - otlp\n         processors:\n            - resourcedetection\n         exporters:\n            - signalfx\n      logs:\n         receivers:\n            - smartagent/processlist\n         processors:\n            - resourcedetection\n         exporters:\n            - signalfx\n      traces:\n         receivers:\n            - otlp\n         processors:\n            - resourcedetection\n         exporters:\n            - otlphttp\n```\n\n----------------------------------------\n\nTITLE: Creating ClusterRoleBinding for Kubernetes Objects Receiver\nDESCRIPTION: YAML configuration to create a ClusterRoleBinding, granting the ClusterRole to the service account for the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-objects-receiver.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: otelcontribcol\nsubjects:\n- kind: ServiceAccount\n  name: otelcontribcol\n  namespace: default\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser RUM Sampling - CDN Implementation\nDESCRIPTION: Example of configuring sampling settings for RUM data collection using CDN implementation, demonstrating conditional sampling based on user login status.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/configure-rum-browser-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\" crossorigin=\"anonymous\"></script>\n<script>\n   var shouldTrace = isUserLoggedIn();\n\n   SplunkRum.init({\n      realm: '<realm>',\n      rumAccessToken: '<your_rum_token>',\n      applicationName: '<application-name>',\n      tracer: {\n         sampler: shouldTrace ? new SplunkRum.AlwaysOnSampler() : new SplunkRum.AlwaysOffSampler(),\n      },\n   });\n</script>\n```\n\n----------------------------------------\n\nTITLE: Adding SQL Server Monitor to Metrics Pipeline\nDESCRIPTION: Configuration snippet showing how to add the SQL Server monitor to the service pipeline for metrics collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/microsoft-sql-server.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/sqlserver]\n```\n\n----------------------------------------\n\nTITLE: Installing Kernel Headers on Debian\nDESCRIPTION: This bash command installs the required kernel headers for Debian systems to run the OpenTelemetry Collector eBPF Helm chart. It requires `apt-get` to be available and properly configured, and assumes internet connectivity for package download.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install --yes linux-headers-$(uname -r)\n```\n\n----------------------------------------\n\nTITLE: Reporting Zero-Duration Custom Event with Attributes (iOS) in Swift\nDESCRIPTION: This Swift snippet demonstrates how to report an instantaneous (zero-duration) custom event in an iOS application using the Splunk RUM iOS SDK. It creates an `NSDictionary` containing custom attributes (\"attribute1\", \"attribute2\", \"attribute3\") and then calls `SplunkRum.reportEvent()` with the event name \"testEvent\" and the attributes dictionary to record the event. This is suitable for tracking specific occurrences without a duration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/RUM-custom-events.rst#2025-04-22_snippet_4\n\nLANGUAGE: swift\nCODE:\n```\nlet dictionary: NSDictionary = [\n                  \"attribute1\": \"hello\",\n                  \"attribute2\": \"world!\",\n                  \"attribute3\": 3\n]\nSplunkRum.reportEvent(name: \"testEvent\", attributes: dictionary)\n```\n\n----------------------------------------\n\nTITLE: Initializing Session Replay via Splunk CDN - HTML\nDESCRIPTION: This snippet shows the minimal initialization sequence for configuring Splunk session replay using the CDN. It includes loading the session recorder JavaScript and initializing it with realm and token configuration options. Used after the main RUM agent is initialized, inputs include the desired realm and a valid RUM access token. Dependencies are loaded via CDN, ensuring ease of adoption.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/rum-session-replay.rst#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<script src=\\\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web-session-recorder.js\\\" crossorigin=\\\"anonymous\\\"></script>\\n<script>\\nSplunkSessionRecorder.init({\\n    realm: '<realm>',\\n    rumAccessToken: '<your_rum_token>'\\n});\\n</script>\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Exporter with TLS in YAML\nDESCRIPTION: Updated configuration structure for the OTLP exporter showing the moved 'insecure' parameter under the TLS section. This change was required when upgrading from version 0.35.0 to 0.36.0.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/collector-upgrade.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otlp:\n    endpoint: \"${SPLUNK_GATEWAY_URL}:4317\"\n    tls:\n      insecure: true\n```\n\n----------------------------------------\n\nTITLE: Chi Server with Trace Metadata Annotation for Log Messages in Go\nDESCRIPTION: This full example demonstrates a chi server implementation in Go, which extracts trace metadata for incoming requests and annotates log messages using the Zap logging library. Key components include trace metadata retrieval, logger augmentation, and basic route setup with error handling for log enrichment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_3\n\nLANGUAGE: go\nCODE:\n```\npackage main\n\nimport (\n\t\"context\"\n\t\"net/http\"\n\n\t\"github.com/go-chi/chi\"\n\t\"github.com/signalfx/splunk-otel-go/instrumentation/github.com/go-chi/chi/splunkchi\"\n\t\"go.opentelemetry.io/otel/trace\"\n\t\"go.uber.org/zap\"\n)\n\nfunc withTraceMetadata(ctx context.Context, logger *zap.Logger) *zap.Logger {\n\tspanContext := trace.SpanContextFromContext(ctx)\n\tif !spanContext.IsValid() {\n\t\t// ctx does not contain a valid span.\n\t\t// There is no trace metadata to add.\n\t\treturn logger\n\t}\n\treturn logger.With(\n\t\tzap.String(\"trace_id\", spanContext.TraceID().String()),\n\t\tzap.String(\"span_id\", spanContext.SpanID().String()),\n\t\tzap.String(\"trace_flags\", spanContext.TraceFlags().String()),\n\t)\n}\n\nfunc helloHandler(logger *zap.Logger) http.HandlerFunc {\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\tl := withTraceMetadata(r.Context(), logger)\n\n\t\tn, err := w.Write([]byte(\"Hello World!\\n\"))\n\t\tif err != nil {\n\t\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\t\tl.Error(\"failed to write request response\", zap.Error(err))\n\t\t} else {\n\t\t\tl.Info(\"request handled\", zap.Int(\"response_bytes\", n))\n\t\t}\n\t}\n}\n\nfunc main() {\n\tlogger, err := zap.NewProduction()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer logger.Sync()\n\n\trouter := chi.NewRouter()\n\trouter.Use(splunkchi.Middleware())\n\trouter.Get(\"/hello\", helloHandler(logger))\n\tif err := http.ListenAndServe(\":8080\", router); err != nil {\n\t\tpanic(err)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Tracer in JavaScript\nDESCRIPTION: Demonstrates how to create a custom tracer using the OpenTelemetry API in JavaScript for single-page applications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nimport {trace} from '@opentelemetry/api';\n\n// Create a tracer\nconst tracer = trace.getTracer('my-application', '1.0.0');\n\n// Example of an async/await function\n```\n\n----------------------------------------\n\nTITLE: Enhanced Metrics Configuration\nDESCRIPTION: Configuration example for collecting enhanced (custom) metrics including JVM, HTTP, process, and thread pool stats.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/elasticsearch.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmonitors:\n- type: elasticsearch\n  host: localhost\n  port: 9200\n  enableEnhancedHTTPStats: true\n  enableEnhancedJVMStats: true\n  enableEnhancedProcessStats: true\n  enableEnhancedThreadPoolStats: true\n  enableEnhancedTransportStats: true\n  enableEnhancedNodeIndicesStats:\n   - indexing\n   - warmer\n   - get\n```\n\n----------------------------------------\n\nTITLE: Querying Movie Database with Oracle DB\nDESCRIPTION: Configuration example for querying a movie database using Oracle DB driver and generating metrics with the SQL Query receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sqlquery-receiver.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsqlquery:\n  datasource: \"oracle://otel:password@localhost:51521/XE\"\n  driver: oracle\n  queries:\n    - sql: \"select count(*) as count, genre, avg(imdb_rating) as avg from otel.movie group by genre\"\n      metrics:\n        - metric_name: genre.count\n          value_column: \"COUNT\"\n          attribute_columns: [ GENRE ]\n        - metric_name: genre.imdb\n          value_column: \"AVG\"\n          attribute_columns: [ GENRE ]\n          value_type: \"double\"\n```\n\n----------------------------------------\n\nTITLE: Customizing eBPF Network Telemetry\nDESCRIPTION: Deactivate specific network metrics or categories using the Helm chart configuration. Supports disabling and re-enabling metrics as needed. Example shows disabling all HTTP metrics and a specific UDP metric, while retaining certain health checks.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n   reducer:\n     disableMetrics:\n       - http.all\n       - udp.bytes\n```\n\nLANGUAGE: yaml\nCODE:\n```\n   reducer:\n     disableMetrics:\n       - http.all\n       - ebpf_net.all\n     enableMetrics:\n       - ebpf_net.collector_health\n```\n\n----------------------------------------\n\nTITLE: Creating/Updating an Elasticsearch Watch via API using cURL (Shell/JSON)\nDESCRIPTION: This shell command uses cURL to send a PUT request to the Elasticsearch Watch API, creating or updating a watch named `cluster_health_watch`. The command includes an inline JSON payload (`-d '{...}') defining the watch: it triggers every 60 seconds (`schedule`), checks cluster health (`input`), always meets the condition (`condition`), and sends a CRITICAL alert to Splunk On-Call via a webhook (`actions`). The `$service_api_key` and `$routing_key` in the webhook path must be replaced with valid Splunk On-Call credentials.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/elasticsearch-watcher-integration.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl -XPUT 'http://localhost:9200/_watcher/watch/cluster_health_watch' -d '{\n\"trigger\" : {\n\"schedule\" : { \"interval\" : \"60s\" }\n},\n\"input\" : {\n\"http\" : {\n\"request\" : {\n\"host\" : \"localhost\",\n\"port\" : 9200,\n\"path\" : \"/_cluster/health\"\n}\n}\n},\n\"condition\" : {\n\"always\" : {}\n},\n\"actions\" : {\n\"victorops\" : {\n\"webhook\" : {\n\"scheme\" : \"https\",\n\"method\" : \"POST\",\n\"host\" : \"alert.victorops.com\",\n\"port\" : 443,\n\"path\" : \"/integrations/generic/20131114/alert/$service_api_key/$routing_key\",\n\"body\" : \"{\\\"message_type\\\": \\\"CRITICAL\\\",\\\"monitoring_tool\\\": \\\"Elastic Watcher\\\",\\\"entity_id\\\": \\\"{{ctx.id}}\\\",\\\"entity_display_name\\\": \\\"{{ctx.watch_id}}\\\",\\\"state_message\\\": \\\"{{ctx.watch_id}}\\\",\\\"elastic_watcher_payload\\\": {{#toJson}}ctx.payload{{/toJson}} }\",\n\"headers\" : {\"Content-type\": \"application/json\"}\n}\n}\n}\n}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Pipelines in Splunk OpenTelemetry Collector\nDESCRIPTION: Defines the service section that connects components into pipelines, specifying how data flows through the Collector from receivers to exporters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-start.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      receivers:\n      - otlp\n      processors:\n      - batch\n      exporters:\n      - otlp\n  # Extensions don't go inside pipelines\n  extensions: [health_check]\n```\n\n----------------------------------------\n\nTITLE: Configuring Chrony NTP Monitor in Splunk OpenTelemetry Collector (YAML)\nDESCRIPTION: This snippet shows how to activate and configure the Chrony NTP monitor integration in the Splunk OpenTelemetry Collector configuration file. It includes setting up the receiver and adding it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/chrony.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/chrony:\n    type: collectd/chrony\n    ...  # Additional config\n\nservice:\n pipelines:\n   metrics:\n     receivers: [smartagent/chrony]\n```\n\n----------------------------------------\n\nTITLE: WebLogic Java Agent Configuration\nDESCRIPTION: Set the javaagent path in WebLogic domain startup scripts for both Linux and Windows. Add the agent path to startWebLogic.sh or startWebLogic.cmd to enable the Splunk agent.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-servers-instructions.rst#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nexport JAVA_OPTIONS=\"$JAVA_OPTIONS -javaagent:/path/to/splunk-otel-javaagent.jar\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nset JAVA_OPTIONS=%JAVA_OPTIONS% -javaagent:\"<Drive>:\\path\\to\\splunk-otel-javaagent.jar\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Splunk (Windows PowerShell)\nDESCRIPTION: This PowerShell snippet sets environment variables for Windows to send telemetry data directly to Splunk Observability Cloud. It requires an access token and the specific realm associated with the user\\'s Splunk instance. SPLUNK_ACCESS_TOKEN is used for secure communication, and SPLUNK_REALM directs the data to the correct path. There are no immediate outputs from this setup.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/instrumentation/instrument-nodejs-application.rst#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n$env:SPLUNK_ACCESS_TOKEN=<access_token>\\n$env:SPLUNK_REALM=<realm>\n```\n\n----------------------------------------\n\nTITLE: Configuring Receiver TLS and mTLS Options - YAML\nDESCRIPTION: This YAML example provides different TLS configurations for OpenTelemetry Collector receivers, including standard TLS, mTLS with required client authentication, and a configuration with no TLS. The standard configuration uses cert and key files for the server, while the mTLS version adds 'client_ca_file' to require client certificates. These settings require relevant certificate/key files and that the receiver protocol (e.g., gRPC) supports TLS. Each block is a YAML section under 'receivers', and the correct settings ensure receiver security and interoperability. Required for mTLS: 'client_ca_file' and corresponding client-side trust.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/common-config/collector-common-config-tls.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: mysite.local:55690\n        tls:\n          cert_file: server.crt\n          key_file: server.key\n  otlp/mtls:\n    protocols:\n      grpc:\n        endpoint: mysite.local:55690\n        tls:\n          client_ca_file: client.pem\n          cert_file: server.crt\n          key_file: server.key\n  otlp/notls:\n    protocols:\n      grpc:\n        endpoint: mysite.local:55690\n```\n\n----------------------------------------\n\nTITLE: Edit Resources and Spans for Size\nDESCRIPTION: This YAML code snippet illustrates how to edit telemetry resources and spans to limit the number of attributes and their lengths. It removes unwanted keys and truncates attribute values, which is beneficial for data size management.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/transform-processor.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntransform: error_mode: ignore trace_statements: - context: resource statements: # Only keep the following keys - keep_keys(attributes, [\"service.name\", \"service.namespace\", \"cloud.region\", \"process.command_line\"]) - limit(attributes, 100, []) - truncate_all(attributes, 4096) - context: span statements: - limit(attributes, 100, []) - truncate_all(attributes, 4096)\n```\n\n----------------------------------------\n\nTITLE: Example of Trace Sampling Configuration\nDESCRIPTION: Provides an example of configuring the trace sampler to exclude specific endpoints like '/healthcheck' using rules defined in environment variables and sampler arguments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/version1x/java-1x-otel-configuration.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_TRACES_SAMPLER=rules\nexport OTEL_TRACES_SAMPLER_ARG=drop=/healthcheck;fallback=parentbased_always_on\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenTracing Support in Splunk OpenTelemetry .NET via Environment Variable\nDESCRIPTION: Sets the `OTEL_DOTNET_AUTO_OPENTRACING_ENABLED` environment variable to `true`. This configuration is necessary during migration from the SignalFx Instrumentation for .NET if the application code was manually instrumented using OpenTracing APIs. Enabling this ensures that the Splunk Distribution of OpenTelemetry .NET recognizes and processes these existing OpenTracing spans.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/migrate-signalfx-dotnet-to-dotnet-otel.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nOTEL_DOTNET_AUTO_OPENTRACING_ENABLED=true\n```\n\n----------------------------------------\n\nTITLE: Including Kubernetes Objects Receiver in Logs Pipeline\nDESCRIPTION: YAML configuration to include the Kubernetes Objects receiver in the logs pipeline of the service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-objects-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlogs/objects:\n  receivers:\n    - k8sObjects\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector with Custom Memory Allocation in PowerShell\nDESCRIPTION: This PowerShell script downloads and installs the Splunk OpenTelemetry Collector with custom configuration parameters including access token, realm, and memory allocation. It uses a web client to download the installation script and passes parameters to customize the installation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/windows-config.rst#2025-04-22_snippet_0\n\nLANGUAGE: PowerShell\nCODE:\n```\n& {Set-ExecutionPolicy Bypass -Scope Process -Force; $script = ((New-Object System.Net.WebClient).DownloadString('https://dl.signalfx.com/splunk-otel-collector.ps1')); $params = @{access_token = \"SPLUNK_ACCESS_TOKEN\"; realm = \"SPLUNK_REALM\"; memory = \"SPLUNK_MEMORY_TOTAL_MIB\"}; Invoke-Command -ScriptBlock ([scriptblock]::Create(\". {$script} $(&{$args} @params)\"))}\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Instrumentation Version in Kubernetes\nDESCRIPTION: YAML configuration showing how to specify a custom version for Java instrumentation in the values.yaml file. Sets the Java instrumentation version to v1.27.0.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-advanced-config.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nclusterName: myCluster\nsplunkObservability:\n  realm: <splunk-realm>\n  accessToken: <splunk-access-token>\nenvironment: prd\ncertmanager:\n  enabled: true\noperatorcrds:\n  install: true\noperator:\n  enabled: true\ninstrumentation:\n  spec: \n    java:\n      repository: ghcr.io/signalfx/splunk-otel-java/splunk-otel-java\n      tag: v1.27.0\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Attributes for OpenTelemetry Instrumentation\nDESCRIPTION: Bash command to set a custom attribute for OpenTelemetry instrumentation using the OTEL_RESOURCE_ATTRIBUTES environment variable in a Kubernetes deployment. This example adds a 'build.id' attribute to every span.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nkubectl set env deployment/<my-deployment> OTEL_RESOURCE_ATTRIBUTES=build.id=feb2023_v2\n```\n\n----------------------------------------\n\nTITLE: Configuring FileLog Receivers in OpenTelemetry Collector\nDESCRIPTION: Defines two filelog receivers to collect logs from different container paths. Each receiver is uniquely named and specifies the file path to monitor for log collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/collector-splunk.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog/output1:\n    include: [ /output1/file.log ]\n  filelog/output2:\n    include: [ /output2/file.log ]\n```\n\n----------------------------------------\n\nTITLE: Displaying Prometheus Time Series from OTel Metrics\nDESCRIPTION: Example showing how OTel sum metrics are represented as Prometheus time series, including target_info and calls_total metrics with various labels.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/span-metrics-connector.rst#2025-04-22_snippet_3\n\nLANGUAGE: prometheus\nCODE:\n```\ntarget_info{job=\"shippingservice\", instance=\"...\", ...} 1\ncalls_total{span_name=\"/Address\", service_name=\"shippingservice\", span_kind=\"SPAN_KIND_SERVER\", status_code=\"STATUS_CODE_UNSET\", ...} 142\n```\n\n----------------------------------------\n\nTITLE: Deploying Ruby Agent in Kubernetes with Environment Variables\nDESCRIPTION: YAML configuration for deploying a Ruby application in Kubernetes with the proper environment variables configured for the Splunk OpenTelemetry agent.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/instrument-ruby-application.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nspec:\n  selector:\n    matchLabels:\n      app: your-application\n  template:\n    spec:\n      containers:\n        - name: myapp\n          env:\n            - name: SPLUNK_OTEL_AGENT\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP\n            - name: OTEL_EXPORTER_OTLP_ENDPOINT\n              value: \"http://$(SPLUNK_OTEL_AGENT):4318\"\n            - name: OTEL_SERVICE_NAME\n              value: \"<serviceName>\"\n            - name: OTEL_RESOURCE_ATTRIBUTES\n              value: \"deployment.environment=<environmentName>\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up Processor in Pipelines\nDESCRIPTION: This YAML example demonstrates how to add attributes processors to different pipelines such as traces, metrics, and logs. Dependencies include appropriate receivers, processors, and exporters as part of the Splunk OpenTelemetry configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/attributes-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      receivers: [jaeger, otlp, zipkin]\n      processors:\n      - attributes/traces\n      - memory_limiter\n      - batch\n      - resourcedetection\n      exporters: [otlphttp, signalfx]\n    metrics:\n      receivers: [hostmetrics, otlp, signalfx]\n      processors:\n      - attributes/metrics\n      - memory_limiter\n      - batch\n      - resourcedetection\n      exporters: [signalfx]\n    logs:\n      receivers: [fluentforward, otlp]\n      processors:\n      - attributes/logs\n      - memory_limiter\n      - batch\n      - resourcedetection\n      exporters: [splunk_hec]\n```\n\n----------------------------------------\n\nTITLE: Registering OpenTelemetry for .NET and .NET Framework Applications\nDESCRIPTION: PowerShell command to register OpenTelemetry instrumentation for the current PowerShell session. This sets up instrumentation for any application launched in the same session.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# Set up environment to start instrumentation from the current PowerShell session\nRegister-OpenTelemetryForCurrentSession -OTelServiceName \"<your-service-name>\"\n```\n\n----------------------------------------\n\nTITLE: Sanitizing PII in JavaScript\nDESCRIPTION: Demonstrates how to redact Personally Identifiable Information (PII) when initializing the Splunk RUM Browser instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nSplunkRum.init({\n// ...\nexporter: {\n// You can use the entire span as an optional second argument of the sanitizer if needed\n   onAttributesSerializing: (attributes) => ({\n      ...attributes,\n      'http.url': /secret\\=/.test(attributes['http.url']) ? '[redacted]' : attributes['http.url'],\n   }),\n},\n});\n```\n\n----------------------------------------\n\nTITLE: Adding User Identification After Initialization in Android RUM\nDESCRIPTION: Demonstrates how to add user identification data as global attributes after the RUM agent has been initialized, useful when user data becomes available later.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nsplunkRum.setGlobalAttribute(\"enduser.id\", \"123456L\");\nsplunkRum.setGlobalAttribute(\"enduser.type\", \"loggedInUser\");\nsplunkRum.setGlobalAttribute(\"enduser.role\", \"premium\");\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Exporter Package in Ruby\nDESCRIPTION: Installs the OpenTelemetry exporter package using the gem command. This package is required for sending data directly to Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/instrument-ruby.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngem install opentelemetry-exporter-otlp\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Pattern for Trace Context with Log4j in application.properties - Text\nDESCRIPTION: Intended for Spring Boot apps using Log4j, this snippet shows how to set a logging pattern in 'application.properties' to include trace and resource attributes directly in console output. Each log will display: timestamp, logger, message, trace_id, span_id, service.name, deployment.environment, and trace_flags. Requires the Splunk OTel Java agent and proper MDC field propagation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nlogging.pattern.console = %d{yyyy-MM-dd HH:mm:ss} - %logger{36} - %msg trace_id=%X{trace_id} span_id=%X{span_id} service=%X{service.name}, env=%X{deployment.environment} trace_flags=%X{trace_flags} %n\n```\n\n----------------------------------------\n\nTITLE: Basic MySQL Monitor Configuration in YAML\nDESCRIPTION: YAML configuration to activate the MySQL integration in the Splunk OpenTelemetry Collector, specifying host, port, and database credentials.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/mysql.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/mysql:\n    type: collectd/mysql\n    host: 127.0.0.1\n    port: 3306\n    username: <global-username-for-all-db>\n    password: <global-password-for-all-db>\n    databases:\n      - name: <name-of-db>\n        username: <username> #Overrides global username\n        password: <password> #Overrides global password\n```\n\n----------------------------------------\n\nTITLE: Configuring Jaeger Receiver with gRPC Protocol in YAML\nDESCRIPTION: Sample YAML configuration for activating the Jaeger receiver with gRPC protocol. It demonstrates basic setup and an example with a custom endpoint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/jaeger-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  jaeger:\n    protocols:\n      grpc:\n  jaeger/withendpoint:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:14260\n```\n\n----------------------------------------\n\nTITLE: Adding User Metadata After Initialization in JavaScript\nDESCRIPTION: Demonstrates how to add user identification metadata as global attributes after initializing the Splunk RUM agent in JavaScript.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport SplunkRum from '@splunk/otel-web';\n\nconst user = await (await fetch('/api/user')).json();\n// Spans generated prior to this call don't have user metadata\nSplunkRum.setGlobalAttributes({\n   'enduser.id': user ? user.id : undefined,\n   'enduser.role': user ? user.role : undefined,\n});\n```\n\n----------------------------------------\n\nTITLE: Checking Helm release name and chart version\nDESCRIPTION: Command to list and confirm the Helm release name and chart version for the OpenTelemetry Collector installation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list -f <Release_Name>\n```\n\n----------------------------------------\n\nTITLE: Configuring Collector Installation with Chef Run List\nDESCRIPTION: Example Chef configuration that shows how to include the splunk_otel_collector recipe in the run_list and set the required splunk_access_token and splunk_realm attributes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/deployments-linux-chef.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{\n    \"splunk-otel-collector\": {\n        \"splunk_access_token\": \"<SPLUNK_ACCESS_TOKEN>\",\n        \"splunk_realm\": \"<SPLUNK_REALM>\",\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring RBAC for Kubernetes Attributes Processor\nDESCRIPTION: YAML configuration for setting up role-based access control (RBAC) for the Kubernetes attributes processor, granting necessary permissions for pods and namespaces.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-attributes-processor.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n   name: collector\n   namespace: <col_namespace>\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n   name: otel-collector\nrules:\n   - apiGroups: [\"\"]\n   resources: [\"pods\", \"namespaces\"]\n   verbs: [\"get\", \"watch\", \"list\"]\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n   name: otel-collector\nsubjects:\n- kind: ServiceAccount\n  name: collector\n  namespace: <col_namespace>\nroleRef:\n   kind: ClusterRole\n   name: otel-collector\n   apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Collector with Chocolatey\nDESCRIPTION: PowerShell command to install the Splunk OpenTelemetry Collector using the Chocolatey package manager. The command includes parameters for Splunk access token and realm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows-msi.rst#2025-04-22_snippet_5\n\nLANGUAGE: PowerShell\nCODE:\n```\nchoco install splunk-otel-collector --params=\"'/SPLUNK_ACCESS_TOKEN:MY_SPLUNK_ACCESS_TOKEN /SPLUNK_REALM:MY_SPLUNK_REALM'\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving k8s-watcher container logs with kubectl\nDESCRIPTION: This command retrieves logs from the k8s-watcher container within the k8s-collector pod to troubleshoot Kubernetes metadata issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-troubleshoot.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs <POD_NAME> -c k8s-watcher\n```\n\n----------------------------------------\n\nTITLE: Adding Micrometer Core Dependency with Maven - Java\nDESCRIPTION: This Maven snippet adds the 'micrometer-core' dependency with a specified version (1.7.5 or later) to enable custom metric instrumentation using Micrometer in a Java application. The dependency should be added to your pom.xml file. Ensure that your build system is Maven and that the project can download dependencies from Maven Central or another compatible repository.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/version1x/java-manual-instrumentation-1x.rst#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\\n  <groupId>io.micrometer</groupId>\\n  <artifactId>micrometer-core</artifactId>\\n  <version>1.7.5</version>\\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Activating Kubernetes Cluster Receiver in YAML Configuration\nDESCRIPTION: Example configuration for activating the Kubernetes cluster receiver in the Collector configuration file. It includes basic settings and metadata exporter configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-cluster-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  k8s_cluster:\n    auth_type: kubeConfig\n    collection_interval: 30s\n    node_conditions_to_report: [\"Ready\", \"MemoryPressure\"]\n    allocatable_types_to_report: [\"cpu\",\"memory\"]\n    metadata_exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Session ID in JavaScript for Browser RUM\nDESCRIPTION: This example shows how to retrieve the current session ID using getSessionId and add it to the application metadata.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-api-reference.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nLiveChat.onChatStarted(() => {\n  LiveChat.setMetadata('splunk.sessionId', SplunkRum.getSessionId());\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Jenkins Monitor Configuration Example\nDESCRIPTION: Complete example of basic Jenkins monitor configuration with host, port, and metrics key settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/jenkins.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/jenkins:\n    type: collectd/jenkins\n    host: 127.0.0.1\n    port: 8080\n    metricsKey: reallylongmetricskey\n```\n\n----------------------------------------\n\nTITLE: Activating the NGINX Receiver in the Metrics Pipeline using YAML\nDESCRIPTION: YAML configuration snippet demonstrating how to activate the previously defined `nginx` receiver within the OpenTelemetry Collector. It includes the `nginx` receiver in the `receivers` list under the `service.pipelines.metrics` section, ensuring that metrics collected by this receiver are processed and exported through the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/nginx-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n   service:\n     pipelines:\n       metrics:\n         receivers: [nginx]\n```\n\n----------------------------------------\n\nTITLE: Creating and Ending a Span\nDESCRIPTION: This example illustrates how to create and manage a custom span in a Java application, using OpenTelemetry. It involves starting a span, making it current, handling application logic, and ensuring the span is ended properly. Exception handling is demonstrated within the application logic.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-manual-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport io.opentelemetry.api.trace.Span;\nimport io.opentelemetry.context.Scope;\n\n// ...\n@GetMapping(\"/rolldice\")\npublic List<Integer> index(@RequestParam(\"player\") Optional<String> player,\n      @RequestParam(\"rolls\") Optional<Integer> rolls) {\n   Span span = tracer.spanBuilder(\"rollTheDice\")\n      .setAttribute(\"player.name\", player.orElse(\"unknown\"))\n      .startSpan();\n\n   // Make the span the current span\n   try (Scope scope = span.makeCurrent()) {\n\n      //.. Application logic\n\n   } catch(Throwable t) {\n      span.recordException(t);\n      throw t;\n   } finally {\n      span.end();\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Toggling AlwaysOn CPU Profiling for SDKs (Shell)\nDESCRIPTION: Activates ('--enable-profiler') or deactivates ('--disable-profiler', default) AlwaysOn CPU Profiling for activated SDKs that support it via the 'SPLUNK_PROFILER_ENABLED' environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\n--[enable|disable]-profiler\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n--disable-profiler\n```\n\n----------------------------------------\n\nTITLE: Adding DNS Monitor to Metrics Pipeline\nDESCRIPTION: Configuration to add the DNS monitor to the service pipelines section for metrics collection. This connects the receiver to the metrics processing pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/dns.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n pipelines:\n   metrics:\n     receivers: [smartagent/dns]\n```\n\n----------------------------------------\n\nTITLE: Enabling Dyno Metadata for Splunk OpenTelemetry Connector\nDESCRIPTION: Command to enable Heroku runtime-dyno-metadata which is required by the Splunk OpenTelemetry Connector to set global dimensions such as app_name, app_id, and dyno_id.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/heroku.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nheroku labs:enable runtime-dyno-metadata\n```\n\n----------------------------------------\n\nTITLE: Configuring OTel Collector Pipeline for Splunk AlwaysOn Profiling (YAML)\nDESCRIPTION: Sample YAML configuration for the Splunk Distribution of OpenTelemetry Collector (v0.34+) to process AlwaysOn Profiling data. It defines an 'otlp' receiver, a 'splunk_hec/profiling' exporter configured for Splunk ingest, necessary processors (batch, memory_limiter), and a 'logs/profiling' pipeline connecting these components. Requires environment variables like SPLUNK_ACCESS_TOKEN, SPLUNK_INGEST_URL, and SPLUNK_MEMORY_LIMIT_MIB.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/common-java-troubleshooting.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  # Profiling\n  splunk_hec/profiling:\n    token: \"${SPLUNK_ACCESS_TOKEN}\"\n    endpoint: \"${SPLUNK_INGEST_URL}/v1/log\"\n    log_data_enabled: false\n\nprocessors:\n  batch:\n  memory_limiter:\n    check_interval: 2s\n    limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}\n\nservice:\n  pipelines:\n    logs/profiling:\n      receivers: [otlp]\n      processors: [memory_limiter, batch]\n      exporters: [splunk_hec, splunk_hec/profiling]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Collector via Helm\nDESCRIPTION: Commands to deploy the OpenTelemetry Collector using Helm, with support for both direct parameter setting and values file configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install splunk-otel-collector \\\n--set=\"splunkRealm=$REALM\" \\\n--set=\"splunkAccessToken=$ACCESS_TOKEN\" \\\n--set=\"clusterName=<MY-CLUSTER>\" \\\n--set=\"logsEnabled=false\" \\\n--set=\"environment=$<MY-ENV>\" \\\nsplunk-otel-collector-chart/splunk-otel-collector \\\n-f ~/workshop/k3s/otel-collector.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm install my-splunk-otel-collector --values my_values.yaml splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Stats Receiver with TLS Authentication (YAML)\nDESCRIPTION: Demonstrates how to configure the kubeletstats receiver with TLS authentication for secure communication. Requires file paths to the CA certificate, private key, and public certificate. Additional parameters like collection_interval, endpoint, and insecure_skip_verify can be set. The pipeline routes metrics to a file exporter for testing purposes. This snippet is intended for environments where API security is mandatory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubelet-stats-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 20s\n    auth_type: \"tls\"\n    ca_file: \"/path/to/ca.crt\"\n    key_file: \"/path/to/apiserver.key\"\n    cert_file: \"/path/to/apiserver.crt\"\n    endpoint: \"192.168.64.1:10250\"\n    insecure_skip_verify: true\n\nexporters:\n  file:\n    path: \"fileexporter.txt\"\n    \nservice:\n  pipelines:\n    metrics:\n      receivers: [kubeletstats]\n      exporters: [file]\n\n```\n\n----------------------------------------\n\nTITLE: Tracking Navigation in Jetpack Compose with Android RUM\nDESCRIPTION: Demonstrates how to track navigation events in Jetpack Compose by monitoring route changes and updating the screen name accordingly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_10\n\nLANGUAGE: kotlin\nCODE:\n```\nval navController = rememberNavController()\nval currentBackEntry by navController.currentBackStackEntryAsState()\nval currentRoute = currentBackEntry?.destination?.route\n\nLaunchedEffect(currentRoute) {\n   if (currentRoute != null) {\n      lastRoute = currentRoute\n      SplunkRum.getInstance().experimentalSetScreenName(currentRoute)\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry in ASP.NET web.config\nDESCRIPTION: XML configuration for ASP.NET applications to set OpenTelemetry service name and resource attributes. This is added to the web.config file's appSettings section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_8\n\nLANGUAGE: xml\nCODE:\n```\n<appSettings>\n   <add key=\"OTEL_SERVICE_NAME\" value=\"my-service-name\" />\n   <add key=\"OTEL_RESOURCE_ATTRIBUTES\" value=\"deployment.environment=test,service.version=1.0.0\" />\n</appSettings>\n```\n\n----------------------------------------\n\nTITLE: Annotate Structured Logs with Trace Metadata using Zap in Go\nDESCRIPTION: This Go snippet shows how to annotate structured log events using the zap logging library to include trace metadata fields such as trace ID, span ID, and trace flags. This helps in correlating logs with trace data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nlogger, _ := zap.NewProduction()\ndefer logger.Sync()\nlogger = logger.With(\n\tzap.String(\"trace_id\", spanContext.TraceID().String()),\n\tzap.String(\"span_id\", spanContext.SpanID().String()),\n\tzap.String(\"trace_flags\", spanContext.TraceFlags().String()),\n)\nlogger.Info(\"Failed to fetch URL\", zap.String(\"URL\", url))\n```\n\n----------------------------------------\n\nTITLE: Initializing Android RUM Agent in Kotlin\nDESCRIPTION: Basic configuration of the Splunk RUM Android agent showing how to set the realm, access token and application name in the Application class.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/configure-rum-android-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: kotlin\nCODE:\n```\nclass MyApplication extends Application {\n   private final String realm = \"<realm>\";\n   private final String rumAccessToken = \"<your_RUM_access_token>\";\n\n   @Override\n   public void onCreate() {\n      super.onCreate();\n\n   SplunkRum.builder()\n         .setApplicationName(\"<name_of_app>\")\n         .setRealm(\"<realm>\"\")\n         .setRumAccessToken(\"<rumAccessToken>\")\n         .build(this);\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Tracer Instance in PHP\nDESCRIPTION: Creates a Tracer instance from a TracerProvider. It requires an instrumentation scope name and optionally accepts a version, schema URL, and resource attributes. This tracer is used to create spans.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-manual-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: php\nCODE:\n```\n// Acquire the tracer only where needed\n\n$tracer = $tracerProvider->getTracer(\n   'instrumentation-scope-name', // Name (Required)\n   'instrumentation-scope-version', // Version\n   'http://example.com/my-schema', // Schema URL\n   ['foo' => 'bar'] // Resource attributes\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Jaeger gRPC Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration to activate the Jaeger gRPC monitor in the collector configuration. Sets up the receiver and adds it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/jaeger-grpc.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/jaeger-grpc: \n    type: jaeger-grpc\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Adding OpenTelemetry Server Timing Propagator via Composer - Shell\nDESCRIPTION: This shell command installs the OpenTelemetry server timing propagator using Composer, which is required to connect Real User Monitoring (RUM) traces with server traces in APM for tracing correlation. The propagator package must be specified at the provided version or higher. Composer and PHP are required.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/instrument-php-application.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nphp composer.phar install open-telemetry/opentelemetry-propagation-server-timing:^0.0.2\n```\n\n----------------------------------------\n\nTITLE: Disabling Istio Tracing for Collector Pods via Annotations (YAML)\nDESCRIPTION: This YAML configuration snippet shows how to add specific pod annotations (`proxy.istio.io/config`) to the Splunk OpenTelemetry Collector components (`otelK8sClusterReceiver` and `otelCollector`) to disable Istio tracing within those specific pods. This is necessary if the Istio proxy sidecar is injected into the Collector pods, preventing the Collector itself from generating unwanted trace data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/istio/istio.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# ...\notelK8sClusterReceiver:\n   podAnnotations:\n      proxy.istio.io/config: '{\"tracing\":{}}'\notelCollector:\n   podAnnotations:\n      proxy.istio.io/config: '{\"tracing\":{}}'\n```\n\n----------------------------------------\n\nTITLE: Initializing SignalFx Client for Metrics Collection (JavaScript)\nDESCRIPTION: This illustrative snippet—provided to support migration—shows the legacy way of collecting metrics in Node.js applications using the SignalFx client in conjunction with the '@splunk/otel' 'start' method. The code imports 'getSignalFxClient' (previously returned from 'start'), which is later replaced by OpenTelemetry APIs. The example is deprecated and for migration guidance only; ensure '@splunk/otel' and SignalFx clients are installed if using this pattern.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/nodejs-otel-metrics.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// SignalFx\nconst { start } = require('@splunk/otel');\nconst { getSignalFxClient } = start({ serviceName: 'my-service' });\n```\n\n----------------------------------------\n\nTITLE: Verifying Splunk OTel Collector Service Status\nDESCRIPTION: This command checks the status of the splunk-otel-collector service using systemctl.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl status splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Labels and Annotations Extraction in YAML\nDESCRIPTION: Example configuration showing how to extract annotations and labels from Kubernetes pods and namespaces. Demonstrates setting tag names, keys, and using regular expressions for value extraction.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-attributes-processor.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nannotations:\n# Extracts value of annotation from pods with key `annotation-one`\n# and inserts it as a tag with key `a1`\n  - tag_name: a1 \n    key: annotation-one\n    from: pod\n# Extracts value of annotation from namespaces with key `annotation-two` \n# with regular expressions and inserts it as a tag with key `a2`\n  - tag_name: a2\n    key: annotation-two\n    regex: field=(?P<value>.+)\n    from: namespace\n\nlabels:\n# Extracts value of label from namespaces with key `label1`\n# and inserts it as a tag with key `l1`\n  - tag_name: l1\n    key: label1\n    from: namespace\n# Extracts value of label from pods with key `label2` with\n#  regular expressions and inserts it as a tag with key `l2`\n  - tag_name: l2\n    key: label2\n    regex: field=(?P<value>.+)\n    from: pod\n```\n\n----------------------------------------\n\nTITLE: Installing the Collector with Downloaded Debian Package\nDESCRIPTION: Commands to install the required libcap2-bin dependency and install the Splunk OTel Collector using a locally downloaded Debian package.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\napt-get update && apt-get install -y libcap2-bin  # Required for enabling cap_dac_read_search and cap_sys_ptrace capabilities on the Collector\ndpkg -i <path to splunk-otel-collector deb>\n```\n\n----------------------------------------\n\nTITLE: Installing Express Framework as a Dependency - Shell\nDESCRIPTION: This shell command installs the 'express' framework into node_modules, ensuring it's available for require calls and for OpenTelemetry instrumentation when using the externals configuration in webpack.config.js. Dependencies: Node.js and npm must be installed. Input: none; Output: installs 'express' via npm. Run this from the root of your project.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/troubleshooting/common-nodejs-troubleshooting.rst#2025-04-22_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n# Install the library or framework and add it to node_modules\nnpm install express\n```\n\n----------------------------------------\n\nTITLE: Deactivating Specific Instrumentations in Splunk OpenTelemetry SDK Configuration for Ruby\nDESCRIPTION: This snippet demonstrates how to deactivate specific instrumentations when using the 'use_all' function in the Splunk OpenTelemetry SDK configuration. It shows how to disable the ActiveRecord instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/ruby-manual-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: ruby\nCODE:\n```\nSplunk::Otel.configure do |c|\nc.use_all({ 'OpenTelemetry::Instrumentation::ActiveRecord' => { enabled: false } })\nend\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Lambda Execution Wrapper for Java in Splunk OpenTelemetry\nDESCRIPTION: Examples of different execution wrapper configurations for Java Lambda functions. Options include wrappers for regular handlers, API Gateway with HTTP context propagation, and streaming handlers. Note that only AWS SDK v2 instrumentation is activated by default.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/lambda-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Select the most appropriate value\n\n# Wraps regular handlers that implement RequestHandler\n/opt/otel-handler\n\n# Same as otel-handler, but proxied through API Gateway,\n# with HTTP context propagation activated\n/opt/otel-proxy-handler\n\n# Wraps streaming handlers that implement RequestStreamHandler\n/opt/otel-stream-handler\n```\n\n----------------------------------------\n\nTITLE: Including Memory Ballast Extension in Service Configuration\nDESCRIPTION: This snippet demonstrates how to include the Memory Ballast extension in the service section of the configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/memory-ballast-extension.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  extensions: [memory_ballast]\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk HEC Exporter for Splunk Cloud or Enterprise in YAML\nDESCRIPTION: This example shows a more detailed configuration for the Splunk HEC exporter when sending data to Splunk Cloud Platform or Splunk Enterprise. It includes additional options like index, compression, timeout, and TLS settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-exporter.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n   splunk_hec:\n      # Splunk HTTP Event Collector token.\n      token: \"00000000-0000-0000-0000-0000000000000\"\n      # URL to a Splunk instance to send data to.\n      endpoint: \"https://splunk:8088/services/collector\"\n      # Optional Splunk source: https://docs.splunk.com/Splexicon:Source\n      source: \"otel\"\n      # Optional Splunk source type: https://docs.splunk.com/Splexicon:Sourcetype\n      sourcetype: \"otel\"\n      # Splunk index, optional name of the Splunk index targeted.\n      index: \"metrics\"\n      # Whether to deactivate gzip compression over HTTP. Defaults to false.\n      disable_compression: false\n      # HTTP timeout when sending data. Defaults to 10s.\n      timeout: 10s\n      # Whether to skip checking the certificate of the HEC endpoint when sending data over HTTPS. Defaults to false.\n      tls:\n        insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQ Monitor for Cluster Receiver\nDESCRIPTION: Configuration for adding RabbitMQ monitoring to the cluster receiver, including receiver setup and pipeline configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-add.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nclusterReceiver:\n  config:\n    receivers:\n      smartagent/rabbitmq:\n        type: collectd/rabbitmq\n        host: rabbitmq-service\n        port: 5672\n        username: otel\n        password: ${env:RABBITMQ_PASSWORD}\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers:\n        - smartagent/rabbitmq\n```\n\n----------------------------------------\n\nTITLE: Configuring Helm Values for OpenTelemetry Collector\nDESCRIPTION: YAML configuration for the Splunk Distribution of OpenTelemetry Collector Helm chart, including cluster name, Splunk Observability Cloud credentials, environment settings, and operator configurations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-java-traces-tutorial/config-k8s-for-java.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nclusterName: my-cluster\n\n# your credentials for Splunk Observability Cloud\nsplunkObservability:\n  realm: <splunk-realm>\n  accessToken: <splunk-access-token>\n\n# deployment environment value, which tags the data sent by your application\nenvironment: prd\ncertmanager:\n  enabled: true\noperatorcrds:\n  install: true\noperator:\n  enabled: true\n```\n\n----------------------------------------\n\nTITLE: Enabling Trace Response Headers in Linux Shell\nDESCRIPTION: Shows how to set the SPLUNK_TRACE_RESPONSE_HEADER_ENABLED environment variable in Linux to enable adding server trace information to HTTP response headers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/configuration/advanced-ruby-otel-configuration.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport SPLUNK_TRACE_RESPONSE_HEADER_ENABLED=true\n```\n\n----------------------------------------\n\nTITLE: Wrapping App Component with OtelWrapper\nDESCRIPTION: Example showing how to wrap a React Native App component with the OtelWrapper component to initialize OpenTelemetry instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/install-rum-react.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { OtelWrapper, startNavigationTracking } from '@splunk/otel-react-native';\nimport type { ReactNativeConfiguration } from '@splunk/otel-react-native';\n\nconst AppWithOtelWrapper = () => (\n<OtelWrapper configuration={RumConfig}>\n   <App />\n</OtelWrapper>\n);\n\nexport default AppWithOtelWrapper;\n```\n\n----------------------------------------\n\nTITLE: Including Kubernetes Attributes Processor in Service Pipelines\nDESCRIPTION: Example of how to include the Kubernetes attributes processor in all pipelines of the service section in the configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-attributes-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      processors: [k8sattributes/demo]\n    logs:\n      processors: [k8sattributes/demo]\n    traces:\n      processors: [k8sattributes/demo]\n```\n\n----------------------------------------\n\nTITLE: Compacting Data with Group by Attributes Processor\nDESCRIPTION: Configuration for compacting telemetry data by matching Resource and InstrumentationLibrary properties, used in conjunction with the batch processor.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/groupbyattrs-processor.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  batch:\n  groupbyattrs:\n\npipelines:\n  traces:\n    processors: [batch, groupbyattrs/grouping]\n    ...\n```\n\n----------------------------------------\n\nTITLE: Adding Hadoop Receiver to Metrics Pipeline in Splunk OpenTelemetry Collector\nDESCRIPTION: This YAML snippet demonstrates how to add the configured Hadoop receiver to the metrics pipeline in the Splunk OpenTelemetry Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoop.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/hadoop]\n```\n\n----------------------------------------\n\nTITLE: Configuring Metadata Extraction\nDESCRIPTION: Example of configuring metadata extraction for the Kubernetes attributes processor, including default and custom attributes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-attributes-processor.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nk8sattributes:\n  auth_type: \"serviceAccount\"\n  passthrough: false\n  filter:\n    node_from_env_var: KUBE_NODE_NAME\n  extract:\n    metadata:\n      - k8s.pod.name\n      - k8s.pod.uid\n      - k8s.deployment.name\n      - k8s.namespace.name\n      - k8s.node.name\n      - k8s.pod.start_time\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Receiver to Extract Headers\nDESCRIPTION: This configuration example shows how to set up the Kafka receiver to extract specific headers from Kafka messages. It enables header extraction and specifies which headers to extract.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kafka-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kafka:\n    topic: test\n    header_extraction: \n      extract_headers: true\n      headers: [\"header1\", \"header2\"]\n```\n\n----------------------------------------\n\nTITLE: Deactivating ClusterReceiver in Kubernetes\nDESCRIPTION: This YAML configuration deactivates the clusterReceiver to avoid cluster-wide metrics duplication when running multiple installations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nclusterReceiver:\n  enabled: false\n```\n\n----------------------------------------\n\nTITLE: Restarting Windows Service after Configuration\nDESCRIPTION: PowerShell command to restart a Windows service after making configuration changes. This ensures the new OpenTelemetry settings are applied to the service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_12\n\nLANGUAGE: powershell\nCODE:\n```\nRestart-Service -Name \"<your-windows-service-name>\" -Force\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Lambda Dependencies\nDESCRIPTION: Commands to install the required OpenTelemetry Lambda instrumentation and Splunk OTel Go distribution packages.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/aws/otel-lambda-layer/instrumentation/go-lambdas.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngo get -u go.opentelemetry.io/contrib/instrumentation/github.com/aws/aws-lambda-go/otellambda\ngo get -u github.com/signalfx/splunk-otel-go/distro\n```\n\n----------------------------------------\n\nTITLE: Activating Only Rails Instrumentation in Splunk OpenTelemetry SDK for Ruby\nDESCRIPTION: This code shows how to activate only the Rails instrumentation using a single 'use' statement in the Splunk OpenTelemetry SDK configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/ruby-manual-instrumentation.rst#2025-04-22_snippet_6\n\nLANGUAGE: ruby\nCODE:\n```\nSplunk::Otel.configure do |c|\nc.use 'OpenTelemetry::Instrumentation::Rails'\nend\n```\n\n----------------------------------------\n\nTITLE: Including Resource Detection Processor in Pipelines\nDESCRIPTION: Example of including the resource detection processor in metrics, logs, and traces pipelines.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/resourcedetection-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      processors: [resourcedetection]\n    logs:\n      processors: [resourcedetection]\n    traces:\n      processors: [resourcedetection]\n```\n\n----------------------------------------\n\nTITLE: Configuring Secret for Token Storage in Kubernetes\nDESCRIPTION: This YAML configuration specifies that tokens should be provided as a pre-created secret instead of clear text in the config file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nsecret:\n  create: false\n  name: your-secret\n```\n\n----------------------------------------\n\nTITLE: Creating a Span with Attribute in JavaScript\nDESCRIPTION: Demonstrates how to create a span with an attribute using the OpenTelemetry API in JavaScript.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport {trace} from '@opentelemetry/api'\n\nconst span = trace.getTracer('searchbox').startSpan('search');\nspan.setAttribute('searchLength', searchString.length);\n// Time passes\nspan.end();\n```\n\n----------------------------------------\n\nTITLE: Starting a Local Kubernetes Cluster with Minikube\nDESCRIPTION: Command to initialize and start a local Kubernetes cluster using Minikube, which creates a virtual environment for running Kubernetes locally.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/collector-config-tutorial-start.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nminikube start\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Splunk OpenTelemetry Collector Pillar in Salt\nDESCRIPTION: Example of a basic Salt pillar configuration for the Splunk OpenTelemetry Collector, defining essential parameters like access token, realm, repository URL, configuration path, and service user/group.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/deployments-linux-salt.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsplunk-otel-collector:\n   splunk_access_token: \"MY_ACCESS_TOKEN\"\n   splunk_realm: \"SPLUNK_REALM\"\n   splunk_repo_base_url: https://splunk.jfrog.io/splunk\n   splunk_otel_collector_config: '/etc/otel/collector/agent_config.yaml'\n   splunk_service_user: splunk-otel-collector\n   splunk_service_group: splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OpenTelemetry Collector Exporters and Pipelines\nDESCRIPTION: This snippet shows the basic configuration for Splunk OpenTelemetry Collector, including exporter settings for traces and metrics, and pipeline configurations. It uses environment variables for access token and realm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/relatedcontent-collector-apm.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntraces_endpoint: \"https://ingest.${SPLUNK_REALM}.signalfx.com/v2/trace/otlp\"\nsignalfx:\n  access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n  realm: \"${SPLUNK_REALM}\"\n\nservice:\n  extensions: [http_forwarder]\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors:\n      - memory_limiter\n      - batch\n      exporters: [otlphttp]\n    metrics:\n      receivers: [otlp]\n      processors: [memory_limiter, batch]\n      exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans in iOS RUM Applications\nDESCRIPTION: This code demonstrates how to use the spanFilter function to modify or drop spans that may contain personally identifiable information (PII). The example shows how to drop spans with a specific name and redact sensitive URL values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: swift\nCODE:\n```\noptions.spanFilter = { spanData in\n   var spanData = spanData\n   if spanData.name == \"DropThis\" {\n      // Spans with the name \"DropThis\" aren't sent\n      return nil\n   }\n   var atts = spanData.attributes\n   // Change values for all URLs\n   atts[\"http.url\"] = .string(\"redacted\")\n   return spanData.settingAttributes(atts)\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Global Attributes in TypeScript for Browser RUM\nDESCRIPTION: The setGlobalAttributes method adds a list of attributes to every new span. It takes an optional Attributes object as an argument and returns void.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-api-reference.rst#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nSplunkRum.setGlobalAttributes(attributes?: Attributes): void;\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk Terraform Provider in Terraform\nDESCRIPTION: Basic Terraform configuration that sets up the Splunk Terraform provider (signalfx). This snippet shows how to specify the required provider, authenticate with your token, and provides a placeholder for adding resources like dashboards.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-terraformconfig.rst#2025-04-22_snippet_0\n\nLANGUAGE: terraform\nCODE:\n```\nterraform {\n  required_providers {\n    splunk = {\n      source = \"splunk-terraform/signalfx\"\n      version = \"6.22.0\"\n    }\n  }\n}\n\nprovider \"signalfx\" {\n  auth_token = \"${var.signalfx_auth_token}\"\n}\n\n# Add resources\nresource \"signalfx_dasboard\" \"default\" {\n  # ...\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Span Duration Percentiles using SignalFlow - None\nDESCRIPTION: This set of SignalFlow queries computes the p50, p90, and p99 percentiles for 'spans' duration for a given service, dividing by 1,000,000 to convert to milliseconds, and publishes them with appropriate labels. Dependence on the dashboard variable '$service' for dynamic filtering. Inputs are filtered span metrics; outputs are percentile values for visualization.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/grafana-create-queries.rst#2025-04-22_snippet_3\n\nLANGUAGE: none\nCODE:\n```\nA = histogram('spans', filter=filter('sf_service', '$service')).percentile(pct=50) / 1000000\nA.publish(label='p50')\nB = histogram('spans', filter=filter('sf_service', '$service')).percentile(pct=90) / 1000000\nB.publish(label='p90')\nC = histogram('spans', filter=filter('sf_service', '$service')).percentile(pct=99) / 1000000\nC.publish(label='p99')\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Pod Verification Command\nDESCRIPTION: Command and expected output for verifying the deployment of OpenTelemetry resources in Kubernetes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl get pods\n# NAME                                                          READY\n# NAMESPACE     NAME                                                            READY   STATUS\n# monitoring    splunk-otel-collector-agent-lfthw                               2/2     Running\n# monitoring    splunk-otel-collector-k8s-cluster-receiver-856f5fbcf9-pqkwg     1/1     Running\n```\n\n----------------------------------------\n\nTITLE: Setting Fluentd Capabilities and Restarting Service on Linux\nDESCRIPTION: These commands install the capng_c gem, add necessary capabilities to the Fluentd Ruby binary, reload the systemd daemon, and restart the td-agent service. Used when upgrading td-agent to ensure proper capabilities are set.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-config-logs.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo td-agent-gem install capng_c\nsudo /opt/td-agent/bin/fluent-cap-ctl --add \"dac_override,dac_read_search\" -f /opt/td-agent/bin/ruby\nsudo systemctl daemon-reload\nsudo systemctl restart td-agent\n```\n\n----------------------------------------\n\nTITLE: Angular Error Handler Integration\nDESCRIPTION: Implementation of error handlers for Angular versions 1.x and 2.x with Splunk RUM integration\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nimport {NgModule, ErrorHandler} from '@angular/core';\nimport SplunkRum from '@splunk/otel-web';\n\nclass SplunkErrorHandler implements ErrorHandler {\n   handleError(error) {\n// To avoid loading issues due to content blockers\n// when using the CDN version of the Browser RUM\n// agent, add if (window.SplunkRum) checks around\n// SplunkRum API calls\n      SplunkRum.error(error, info)\n   }\n}\n\n@NgModule({\n   providers: [\n      {\n         provide: ErrorHandler,\n         useClass: SplunkErrorHandler\n      }\n   ]\n})\nclass AppModule {}\n```\n\n----------------------------------------\n\nTITLE: Creating a Tracer in Python for Custom Spans\nDESCRIPTION: Creates a tracer object that will be used to generate custom spans in a Python application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntracer = trace.get_tracer(\"tracer.name\")\n```\n\n----------------------------------------\n\nTITLE: Basic GenericJMX Receiver Configuration in YAML\nDESCRIPTION: Basic configuration to activate the GenericJMX monitor in the OpenTelemetry Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/genericjmx.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/genericjmx:\n    type: collectd/genericjmx\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Visualizing Default Logs Pipeline with Mermaid Diagram\nDESCRIPTION: A flowchart showing how logs data flows through the system from different receivers through processors to exporters. The diagram includes both standard and SignalFx-specific pipelines.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/collector-config-ootb.rst#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n\n   accTitle: Default logs pipeline diagram\n   accDescr: Receivers send logs to the logs/memory_limiter processor. The logs/memory_limiter processor sends logs to the batch processor, and the batch processor sends logs to the resource detection processor. The resource detection processor sends logs to the exporter. The SignalFx logs pipeline follows the same steps, but uses internal receivers, processors, and exporters to send logs. \n\n   %% LR indicates the direction (left-to-right)\n\n   %% You can define classes to style nodes and other elements\n   classDef receiver fill:#00FF00\n   classDef processor fill:#FF9900\n   classDef exporter fill:#FF33FF\n\n   %% Each subgraph determines what's in each category\n   subgraph Receivers\n      direction LR\n      logs/signalfx/signalfx/in:::receiver\n      logs/signalfx/smartagent/processlist:::receiver\n      logs/fluentforward:::receiver\n      logs/otlp:::receiver\n   end\n\n   subgraph Processor\n      direction LR\n      logs/signalfx/memory_limiter:::processor --> logs/signalfx/batch:::processor --> logs/signalfx/resourcedetection:::processor\n      logs/memory_limiter:::processor --> logs/batch:::processor --> logs/resourcedetection:::processor\n   end\n\n   subgraph Exporters\n      direction LR\n      logs/signalfx/signalfx/out:::exporter\n      logs/splunk_hec:::exporter\n   end\n\n   %% Connections beyond categories are added later\n   logs/signalfx/signalfx/in --> logs/signalfx/memory_limiter\n   logs/signalfx/resourcedetection --> logs/signalfx/signalfx/out\n   logs/signalfx/smartagent/processlist --> logs/signalfx/memory_limiter\n   logs/fluentforward --> logs/memory_limiter\n   logs/resourcedetection --> logs/splunk_hec\n   logs/otlp --> logs/memory_limiter\n```\n\n----------------------------------------\n\nTITLE: Configuring Syslog Receiver with TCP Settings\nDESCRIPTION: Configures the syslog receiver with TCP protocol settings, including listen address and RFC5424 protocol specification.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-edit.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  syslog:\n    tcp:\n      listen_address: \"0.0.0.0:54526\"\n    protocol: rfc5424\n```\n\n----------------------------------------\n\nTITLE: Adding Span Attributes in Ruby using OpenTelemetry SDK\nDESCRIPTION: Shows how to access the current span and add a custom attribute ('animals') with an array value using the OpenTelemetry Ruby SDK. It mentions using the OTEL_RESOURCE_ATTRIBUTES environment variable for setting global tags.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/add-context-trace-span.rst#2025-04-22_snippet_5\n\nLANGUAGE: ruby\nCODE:\n```\n# OpenTelemetry Ruby\n\nrequire \"opentelemetry/sdk\"\n\ncurrent_span = OpenTelemetry::Trace.current_span\n\ncurrent_span.set_attribute(\"animals\", [\"elephant\", \"tiger\"])\n\n# You can also set global tags using the OTEL_RESOURCE_ATTRIBUTES\t\n# environment variable, which accepts a list of comma-separated key-value\n# pairs. For example, key1:val1,key2:val2.  \n```\n\n----------------------------------------\n\nTITLE: Configuring vCenter Receiver in OpenTelemetry Collector with YAML\nDESCRIPTION: This YAML snippet demonstrates how to add the vCenter receiver to the 'receivers' section of the OpenTelemetry Collector configuration. No parameters are set here, activating the receiver with default values. This snippet requires the OpenTelemetry Collector and is referenced as the base point for further configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/vcenter-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  vcenter:\n```\n\n----------------------------------------\n\nTITLE: Reinstalling OpenTelemetry Collector with updated CRD configuration\nDESCRIPTION: Helm command to reinstall the OpenTelemetry Collector with the recommended CRD configuration after removing existing CRDs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release-name> splunk-otel-collector --set operatorcrds.install=true,operator.enabled=true <extra_args>\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Zero-Code Instrumentation with AlwaysOn Profiling\nDESCRIPTION: Command to install the Splunk OpenTelemetry Collector with Node.js instrumentation and enable AlwaysOn Profiling features for CPU, memory, and metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-instrumentation --deployment-environment prod \\\n--realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN> \\\n--enable-profiler --enable-profiler-memory --enable-metrics\n```\n\n----------------------------------------\n\nTITLE: Creating Server-Timing Headers for Trace Context\nDESCRIPTION: This example shows the format of a Server-Timing HTTP header used to pass trace context from APM to RUM. The traceparent value contains version, trace ID, parent ID, and trace flag information.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nServer-Timing: traceparent;desc=\"00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Syslog Receiver for UDP and RFC3164 Protocol (YAML)\nDESCRIPTION: Shows how to configure the syslog receiver for UDP input with RFC3164 message format and UTC time zone. 'udp' is specified with 'listen_address' set to all interfaces and port 54526. The 'protocol' is set to 'rfc3164' and an explicit 'location' (timezone) of UTC is specified. Required dependencies include the Splunk OTel Collector. Required parameter is 'listen_address'; optional 'protocol' and 'location' control parsing behavior. Input is RFC3164-format syslog data over UDP, output is formatted log entries.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/syslog-receiver.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  syslog:\n    udp:\n      listen_address: \"0.0.0.0:54526\"\n    protocol: rfc3164\n    location: UTC    \n\n```\n\n----------------------------------------\n\nTITLE: ECS Task Definition with Baked-in Agent\nDESCRIPTION: ECS task definition JSON for running a Java application with the OpenTelemetry agent baked into the container image.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-fargate-java.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"family\": \"agent-baked-in-example\",\n  \"containerDefinitions\": [\n      {\n          \"name\": \"tomcat\",\n          \"image\": \"username/tomcat-with-splunk-java-agent:latest\",\n          \"cpu\": 0,\n          \"portMappings\": [\n              {\n                  \"name\": \"tomcat-8080-tcp\",\n                  \"containerPort\": 8080,\n                  \"protocol\": \"tcp\",\n                  \"appProtocol\": \"http\"\n              }\n        ],\n          \"essential\": true,\n          \"environment\": [\n              {\n                  \"name\": \"OTEL_SERVICE_NAME\",\n                  \"value\": \"myservice\"\n              },\n              {\n                  \"name\": \"OTEL_RESOURCE_ATTRIBUTES\",\n                  \"value\": \"deployment.environment=test,service.version=1.0\"\n              },\n              {\n                  \"name\": \"JAVA_TOOL_OPTIONS\",\n                  \"value\": \"-javaagent:/opt/splunk/splunk-otel-javaagent.jar\"\n              }\n          ],\n          \"environmentFiles\": [],\n          \"mountPoints\": [],\n          \"volumesFrom\": [],\n          \"dependsOn\": [],\n          \"ulimits\": [],\n          \"logConfiguration\": {\n              \"logDriver\": \"awslogs\",\n              \"options\": {\n                  \"awslogs-create-group\": \"true\",\n                  \"awslogs-group\": \"/ecs/agent-baked-in-example\",\n                  \"awslogs-region\": \"eu-west-1\",\n                  \"awslogs-stream-prefix\": \"ecs\"\n              },\n              \"secretOptions\": []\n          }\n      }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector during PowerShell Installation\nDESCRIPTION: PowerShell command to install the Splunk Distribution of OpenTelemetry Collector with a configuration parameter. This example shows how to specify the Splunk access token during installation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows-msi.rst#2025-04-22_snippet_1\n\nLANGUAGE: PowerShell\nCODE:\n```\nStart-Process -Wait msiexec \"/i PATH_TO_MSI /qn SPLUNK_ACCESS_TOKEN=<my_access_token>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring the MySQL Receiver in YAML\nDESCRIPTION: This YAML configuration snippet is used to set up the MySQL receiver within the Splunk OpenTelemetry Collector for metric gathering. It specifies connection details to a MySQL database, such as endpoint, username, and password, along with various parameters like `statement_events` for metrics collection and `tls` configuration for secure connections.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/mysql-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  receivers:\\n    mysql:\\n      endpoint: localhost:3306\\n      username: otel\\n      password: \\${env:MYSQL_PASSWORD}\\n      database: otel\\n      collection_interval: 10s\\n      initial_delay: 1s\\n      statement_events:\\n        digest_text_limit: 120\\n        time_limit: 24h\\n        limit: 250\n```\n\nLANGUAGE: yaml\nCODE:\n```\n  service:\\n    pipelines:\\n      metrics:\\n        receivers:\\n          - mysql\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector with Custom Distribution and Cloud Provider in Bash\nDESCRIPTION: Helm command to install the Splunk OpenTelemetry Collector with custom distribution and cloud provider settings in addition to standard parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart\nhelm install my-splunk-otel-collector --set=\"splunkRealm=us0,splunkAccessToken=xxxxxx,clusterName=my-cluster\" --set=distribution={value},cloudProvider={value} splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Configuring Host Metrics Receiver in YAML\nDESCRIPTION: Basic YAML configuration for the host metrics receiver, specifying collection interval and scrapers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/host-metrics-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nhostmetrics:\n  collection_interval: <duration> # The default is 1m.\n  scrapers:\n    <scraper1>:\n    <scraper2>:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk Observability EKS Add-on with Secure Token Handling\nDESCRIPTION: YAML configuration for Splunk Observability Cloud setup with the EKS Add-on, using a separate secret for token management. This configuration specifies the realm, cluster name, and cloud provider settings while setting up secure token handling.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s-addon-eks.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkObservability:\n    realm: <REALM>\nclusterName: <EKS_CLUSTER_NAME>\ncloudProvider: aws\ndistribution: eks\n\nsecret:\n    create: false\n    name: splunk-otel-collector\n    validateSecret: false\n```\n\n----------------------------------------\n\nTITLE: Basic Telegraf Win Performance Counters Configuration\nDESCRIPTION: Basic configuration to activate the Windows Performance Counters integration in the collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/win_perf_counters.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/telegraf/win_perf_counters:\n    type: telegraf/win_perf_counters\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry PHP Extension on Linux - Shell\nDESCRIPTION: This snippet provides shell commands to install the necessary build tools and the OpenTelemetry PHP extension via PECL on Linux. Dependencies include gcc, make, and autoconf, which are installed before obtaining the extension. The user must have root or sudo privileges, and PECL must be available on the system. Inputs are terminal commands; outputs are the installed extension ready for further configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/instrument-php-application.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install gcc make autoconf\\npecl install opentelemetry\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry API Package\nDESCRIPTION: Command to install the OpenTelemetry API package using npm for custom instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @opentelemetry/api\n```\n\n----------------------------------------\n\nTITLE: Editing Kubernetes Resources\nDESCRIPTION: Commands to edit ConfigMaps and DaemonSets in Kubernetes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl edit cm <name>\nkubectl edit ds <name>\n```\n\n----------------------------------------\n\nTITLE: Configuring CouchDB Receiver in OpenTelemetry Collector\nDESCRIPTION: YAML configuration to activate the CouchDB integration in the OpenTelemetry Collector. This defines a Smart Agent receiver of type 'couchdb'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/apache-couchdb.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/couchdb:\n    type: couchdb\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Setting OTLP Exporter Endpoint Environment Variable (Windows PowerShell)\nDESCRIPTION: Sets the OTEL_EXPORTER_OTLP_ENDPOINT environment variable using Windows PowerShell. This optional variable specifies the host and port of the Splunk Distribution of OpenTelemetry Collector if it's not running on the default localhost.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\n----------------------------------------\n\nTITLE: Integrating Consul Monitor in OpenTelemetry Collector Service Pipeline\nDESCRIPTION: YAML configuration snippet showing how to add the Consul monitor to the metrics pipeline in the OpenTelemetry Collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/consul.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/consul]\n```\n\n----------------------------------------\n\nTITLE: Configuring TCP Log Receiver Activation in YAML\nDESCRIPTION: This YAML configuration snippet demonstrates how to enable the TCP log receiver (`tcplog`) within the OpenTelemetry Collector's `agent_config.yaml`. It specifies the network interface (`0.0.0.0` for all interfaces) and port (`54525`) on which the receiver should listen for incoming TCP log connections. This is the primary step to activate the receiver component.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/tcp-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  tcplog:\n    listen_address: \"0.0.0.0:54525\"\n```\n\n----------------------------------------\n\nTITLE: Convert Datapoints to Different Types\nDESCRIPTION: The snippet demonstrates how to convert datapoints to different types based on metric names with functions like \"convert_sum_to_gauge\" and \"convert_gauge_to_sum\", useful for telemetry data transformation in the transform processor of OpenTelemetry.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/transform-processor.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ntransform: metric_statements: - context: metric statements: - set(description, \"Sum\") where type == \"Sum\" - context: datapoint statements: - convert_sum_to_gauge() where metric.name == \"system.processes.count\" - convert_gauge_to_sum(\"cumulative\", false) where metric.name == \"prometheus_metric\"\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenTelemetry Instrumentation for .NET AWS Lambda\nDESCRIPTION: This code snippet demonstrates how to implement OpenTelemetry instrumentation for a .NET AWS Lambda function. It includes the necessary dependencies, configuration for Splunk Observability Cloud, and a template for the Lambda function handler.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/aws/otel-lambda-layer/instrumentation/dotnet-lambdas.rst#2025-04-22_snippet_0\n\nLANGUAGE: csharp\nCODE:\n```\nusing Amazon.Lambda.Core;\nusing OpenTelemetry;\nusing OpenTelemetry.Exporter;\nusing OpenTelemetry.Instrumentation.AWSLambda;\nusing OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\nusing System.Diagnostics;\n\nnamespace DotNetInstrumentedLambdaExample;\n\npublic class Function\n{\n   public static readonly TracerProvider TracerProvider;\n\n   static Function()\n   {\n      TracerProvider = ConfigureSplunkTelemetry()!;\n   }\n\n   // Note: Do not forget to point function handler to here.\n   public string TracingFunctionHandler(string input, ILambdaContext context)\n      => AWSLambdaWrapper.Trace(TracerProvider, FunctionHandler, input, context);\n\n   public string FunctionHandler(string input, ILambdaContext context)\n   {\n      // TODO: Your function handler code here\n   }\n\n   private static TracerProvider ConfigureSplunkTelemetry()\n   {\n      var serviceName = Environment.GetEnvironmentVariable(\"AWS_LAMBDA_FUNCTION_NAME\") ?? \"Unknown\";\n      var accessToken = Environment.GetEnvironmentVariable(\"SPLUNK_ACCESS_TOKEN\")?.Trim();\n      var realm = Environment.GetEnvironmentVariable(\"SPLUNK_REALM\")?.Trim();\n\n      ArgumentNullException.ThrowIfNull(accessToken, \"SPLUNK_ACCESS_TOKEN\");\n      ArgumentNullException.ThrowIfNull(realm, \"SPLUNK_REALM\");\n\n      var builder = Sdk.CreateTracerProviderBuilder()\n            // Use Add[instrumentation-name]Instrumentation to instrument missing services\n            // Use Nuget to find different instrumentation libraries\n            .AddHttpClientInstrumentation()\n            .AddAWSInstrumentation()\n            // Use AddSource to add your custom DiagnosticSource source names\n            //.AddSource(\"My.Source.Name\")\n            .SetSampler(new AlwaysOnSampler())\n            .AddAWSLambdaConfigurations(opts => opts.DisableAwsXRayContextExtraction = true)\n            .ConfigureResource(configure => configure\n                  .AddService(serviceName, serviceVersion: \"1.0.0\")\n                  // Different resource detectors can be found at\n                  // https://github.com/open-telemetry/opentelemetry-dotnet-contrib/tree/main/src/OpenTelemetry.Resources.AWS#usage\n                  .AddAWSEBSDetector())\n            .AddOtlpExporter(opts =>\n            {\n               opts.Endpoint = new Uri($\"https://ingest.{realm}.signalfx.com/v2/trace/otlp\");\n               opts.Protocol = OtlpExportProtocol.HttpProtobuf;\n               opts.Headers = $\"X-SF-TOKEN={accessToken}\";\n            });\n\n      return builder.Build()!;\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting OTLP Endpoint Protocol for SDKs (Shell)\nDESCRIPTION: Defines the protocol (e.g., 'grpc', 'http/protobuf') for the OTLP endpoint used by activated SDKs. Sets the 'OTEL_EXPORTER_OTLP_PROTOCOL' environment variable. If omitted, each SDK uses its default protocol.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\n--otlp-endpoint-protocol <protocol>\n```\n\n----------------------------------------\n\nTITLE: Deactivating Specific Instrumentation in OpenTelemetry SDK Configuration for Ruby\nDESCRIPTION: This Ruby code demonstrates how to deactivate a specific instrumentation (ActiveRecord in this case) when configuring the OpenTelemetry SDK to use all other available instrumentations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/ruby-manual-instrumentation.rst#2025-04-22_snippet_7\n\nLANGUAGE: ruby\nCODE:\n```\nOpenTelemetry::SDK.configure do |c|\nc.use_all({ 'OpenTelemetry::Instrumentation::ActiveRecord' => { enabled: false } })\nend\n```\n\n----------------------------------------\n\nTITLE: Configuring System Metadata Collection\nDESCRIPTION: Configuration for using all available sources to determine the host name and collect specific system attributes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/resourcedetection-processor.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/system:\n    detectors: [\"system\"]\n    system:\n      hostname_sources: [\"lookup\", \"cname\", \"dns\", \"os\"]\n      resource_attributes:\n        host.name:\n          enabled: true\n        host.id:\n          enabled: true\n```\n\n----------------------------------------\n\nTITLE: Default SignalFx Exporter Configuration Parameters\nDESCRIPTION: Core configuration parameters for the SignalFx exporter including request limits, buffering, retries, and attribute syncing settings. These settings control the behavior of the exporter's data transmission and processing.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-exporter.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmax_requests: 20\nmax_buffered: 10000\nmax_retries: 2\nlog_updates: false\nretry_delay: 30s\ncleanup_interval: 1m\nsync_attributes:\n  k8s.pod.uid: k8s.pod.uid\n  container.id: container.id\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth2 Client Extension in YAML\nDESCRIPTION: This code snippet demonstrates how to activate the `oauth2client` extension within the `extensions` section of a configuration file. It highlights essential settings necessary for the OAuth2 client credentials flow, including `client_id`, `client_secret`, and `token_url`. Additional parameters like `scopes`, `endpoint_params`, and TLS and timeout settings are also illustrated. This configuration allows HTTP and gRPC exporters to authenticate using OAuth2 tokens.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/oauth2client-extension.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  oauth2client:\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry in .NET Framework app.config\nDESCRIPTION: XML configuration for .NET Framework Windows services to set OpenTelemetry resource attributes. This is added to the app.config file's appSettings section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_11\n\nLANGUAGE: xml\nCODE:\n```\n<appSettings>\n   <add key=\"OTEL_RESOURCE_ATTRIBUTES\" value=\"deployment.environment=test,service.version=1.0.0\" />\n</appSettings>\n```\n\n----------------------------------------\n\nTITLE: Custom Configuration BOSH Deployment\nDESCRIPTION: YAML configuration for deploying the OpenTelemetry Collector with custom configuration settings using BOSH\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-pivotal-cloudfoundry.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: splunk-otel-collector\n\nreleases:\n  - name: splunk-otel-collector\n    version: latest\n\nstemcells:\n  - alias: default\n    os: ubuntu-bionic\n    version: latest\n\nupdate:\n  canaries: 1\n  max_in_flight: 1\n  canary_watch_time: 1000-30000\n  update_watch_time: 1000-30000\n\ninstance_groups:\n  - name: splunk-otel-collector\n    instances: 1\n    azs: [z1, z2]\n    jobs:\n      - name: splunk-otel-collector\n        release: bosh\n        properties:\n          otel:\n            config_yaml: |\n              receivers:\n                cloudfoundry:\n                  rlp_gateway:\n                    endpoint: \"https://log-stream.sys.<TAS environment name>.cf-app.com\"\n                  uaa:\n                    endpoint: \"https://uaa.sys.<TAS environment name>.cf-app.com\"\n                    username: \"...\"\n                    password: \"...\"\n\n              exporters:\n                signalfx:\n                  access_token: \"...\"\n                  realm: \"...\"\n\n              processors:\n                resourcedetection:\n                  detectors: [ system ]\n\n              service:\n                pipelines:\n                  metrics:\n                    receivers: [ cloudfoundry ]\n                    processors: [ resourcedetection ]\n                    exporters: [ signalfx ]\n\n    vm_type: default\n    stemcell: default\n    networks:\n      - name: default\n```\n\n----------------------------------------\n\nTITLE: Configure log4net ConsoleAppender with Trace Attributes\nDESCRIPTION: This code snippet demonstrates how to configure a log4net ConsoleAppender to include trace attributes such as trace_id, span_id, and trace_flags in log messages. The configuration is set within the 'conversionPattern' to enrich logs written to existing log destinations with tracing context.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n<appender name=\"ConsoleAppender\" type=\"log4net.Appender.ConsoleAppender\">\n   <layout type=\"log4net.Layout.PatternLayout\">\n      <conversionPattern value=\"%date [%thread] %-5level %logger - %message span_id=%property{span_id} trace_id=%property{trace_id} trace_flags=%property{trace_flags} test_key=%property{test_key}%newline\" />\n   </layout>\n</appender>\n```\n\n----------------------------------------\n\nTITLE: Configuring Services and Pipelines for Telemetry Data\nDESCRIPTION: This YAML snippet demonstrates how to configure service pipelines and includes the transform processor among others like memory_limiter and batch. It is targeted to traces, metrics, and logs, and specifies multiple exporters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/transform-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice: pipelines: traces: receivers: [jaeger, otlp, zipkin] processors: - transform - memory_limiter - batch - resourcedetection exporters: [otlphttp, signalfx] metrics: receivers: [hostmetrics, otlp, signalfx] processors: - transform - memory_limiter - batch - resourcedetection exporters: [signalfx] logs: receivers: [fluentforward, otlp] processors: - transform - memory_limiter - batch - resourcedetection exporters: [splunk_hec]\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Receiver in YAML\nDESCRIPTION: This snippet shows how to configure the OTLP receiver in the OpenTelemetry Collector configuration file. It specifies endpoints for both gRPC and HTTP protocols.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlp-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: \"${HOST_LISTEN_INTERFACE}:1234\"\n      http:\n        endpoint: \"${HOST_LISTEN_INTERFACE}:5678\"\n```\n\n----------------------------------------\n\nTITLE: Validating Splunk OpenTelemetry Collector Configuration\nDESCRIPTION: Command to validate the Collector configuration file to ensure it follows proper format and structure before deployment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-start.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\notelcol validate --config=sample.yaml\n```\n\n----------------------------------------\n\nTITLE: Changing SignalFlow `data()` to `histogram()` for Histogram Metrics\nDESCRIPTION: Shows the first step in modifying a SignalFlow program to explicitly handle a metric as a histogram, particularly when the metric name (e.g., 'service_latency') might be shared with non-histogram types. This change is made in the SignalFlow editor accessed via 'View SignalFlow' in the Plot Editor.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/chart-builder.rst#2025-04-22_snippet_1\n\nLANGUAGE: SignalFlow\nCODE:\n```\ndata('service_latency')\n```\n\nLANGUAGE: SignalFlow\nCODE:\n```\nhistogram('service_latency')\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Configuration for Collectd and OpenTelemetry\nDESCRIPTION: Docker compose configuration that sets up collectd and OpenTelemetry Collector services. Mounts configuration files and sets up container dependencies.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/other-ingestion-collectd.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n   collectd:\n      build: collectd\n      container_name: collectd\n      depends_on:\n         - otelcollector\n      volumes:\n         - ./collectd/http.conf:/etc/collectd/collectd.conf.d/http.conf\n         - ./collectd/metrics.conf:/etc/collectd/collectd.conf.d/metrics.conf\n\n# OpenTelemetry Collector\notelcollector:\n   image:  quay.io/signalfx/splunk-otel-collector:latest\n   container_name: otelcollector\n   command: [\"--config=/etc/otel-collector-config.yml\", \"--set=service.telemetry.logs.level=debug\"]\n   volumes:\n      - ./otel-collector-config.yml:/etc/otel-collector-config.yml\n```\n\n----------------------------------------\n\nTITLE: Configuring Load Balancing Exporter with Static List in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up the load balancing exporter using a static list of host names for multiple back ends. It includes receiver, exporter, and service pipeline configurations for traces and logs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/loadbalancing-exporter.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: localhost:4317\n\nprocessors:\n\nexporters:\n  loadbalancing:\n    routing_key: \"service\"\n    protocol:\n      otlp:\n        # all options from the OTLP exporter are supported\n        # except the endpoint\n        timeout: 1s\n    resolver:\n      static:\n        hostnames:\n          - backend-1:4317\n          - backend-2:4317\n          - backend-3:4317\n          - backend-4:4317\n      # Notice to config a headless service DNS in Kubernetes  \n      # dns:\n      #  hostname: otelcol-headless.observability.svc.cluster.local        \n\nservice:\n  pipelines:\n    traces:\n      receivers:\n        - otlp\n      processors: []\n      exporters:\n        - loadbalancing\n    logs:\n      receivers:\n        - otlp\n      processors: []\n      exporters:\n        - loadbalancing\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry SDK and Instrumentation Packages in Ruby\nDESCRIPTION: Installs the OpenTelemetry SDK and all instrumentation packages using Bundler. This is the first step in instrumenting a Ruby application for Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/instrument-ruby.rst#2025-04-22_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nbundle add opentelemetry-sdk opentelemetry-instrumentation-all\n```\n\n----------------------------------------\n\nTITLE: Adding NTPQ Monitor to Metrics Pipeline in Splunk OpenTelemetry Collector\nDESCRIPTION: This YAML configuration adds the NTPQ monitor to the metrics pipeline in the Collector's service section. It ensures that the NTPQ metrics are processed and exported.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/ntpq.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/ntpq]\n```\n\n----------------------------------------\n\nTITLE: Loading Default Linux Collector Configuration in YAML\nDESCRIPTION: This snippet loads the default configuration file for the Linux (Debian/RPM) Installer collector packages. It includes settings for various components and services of the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-config-ootb.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n.. github:: yaml\n  :url: https://raw.githubusercontent.com/signalfx/splunk-otel-collector/main/cmd/otelcol/config/collector/agent_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Activating Runtime Metrics Collection - Splunk OTel JS - JavaScript\nDESCRIPTION: Demonstrates enabling Node.js runtime metrics collection via the Splunk OpenTelemetry JavaScript distribution. Requires the '@splunk/otel' package. The core parameter is 'runtimeMetricsEnabled' within the 'metrics' configuration object passed to the 'start' method. This instrumentation automatically collects and exports Node.js process and memory metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/configuration/nodejs-otel-metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst { start } = require('@splunk/otel');\\n\\nstart({\\n   serviceName: 'my-service',\\n   metrics: {\\n     runtimeMetricsEnabled: true,\\n   }\\n});\n```\n\n----------------------------------------\n\nTITLE: Server-Timing Header Components for Trace Context\nDESCRIPTION: This example breaks down the components of a traceparent value in a Server-Timing header, showing the version, trace ID, parent ID, and trace flags that make up a complete trace context.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nversion=00 trace-id=4bf92f3577b34da6a3ce929d0e0e4736\nparent-id=00f067aa0ba902b7 trace-flags=01\n```\n\n----------------------------------------\n\nTITLE: Activating the Metrics Pipeline in OpenTelemetry Collector\nDESCRIPTION: This snippet shows how to activate the metrics pipeline in the OpenTelemetry Collector configuration to process metrics from multiple receivers including the Kong Prometheus receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/kong.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n   receivers: [hostmetrics, otlp, signalfx, prometheus/kong]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Collector eBPF - Shell\nDESCRIPTION: This shell command installs the OpenTelemetry Collector with eBPF support using Helm in a specified Kubernetes namespace, configuring the endpoint address.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nhelm --namespace=<NAMESPACE> install my-opentelemetry-ebpf \\\n          --set=\"endpoint.address=<Gateway Service Name>.<Gateway Service Namespace>.svc.cluster.local\" \\\n          open-telemetry/opentelemetry-ebpf\n```\n\n----------------------------------------\n\nTITLE: Creating a Workflow Span with Error in JavaScript\nDESCRIPTION: Shows how to create a workflow span with error attributes using the OpenTelemetry API in JavaScript.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport {trace} from '@opentelemetry/api'\n\nconst tracer = trace.getTracer('appModuleLoader');\nconst span = tracer.startSpan('test.module.load', {\nattributes: {\n   'workflow.name': 'test.module.load',\n   'error': true,\n   'error.message': 'Custom workflow error message'\n}\n});\n\nspan.end();\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Flask with uWSGI for OpenTelemetry\nDESCRIPTION: Python code that explicitly instruments a Flask application when using uWSGI. It initializes Splunk OpenTelemetry after fork and instruments the existing Flask app instance with FlaskInstrumentor.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/instrument-python-frameworks.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# app.py\nimport uwsgidecorators\nfrom splunk_otel import init_splunk_otel\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@uwsgidecorators.postfork\ndef setup_otel():\n   init_splunk_otel()\n   # Instrument the Flask app instance explicitly\n   FlaskInstrumentor().instrument_app(app)\n\n@app.route('/')\ndef hello_world():\n   return 'Hello, World!'\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Metrics Receiver in YAML\nDESCRIPTION: Basic configuration for activating the Kafka metrics receiver, specifying protocol version and scrapers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kafkametrics-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kafkametrics:\n    protocol_version: 2.0.0\n    scrapers:\n      - brokers\n      - topics\n      - consumers\n```\n\n----------------------------------------\n\nTITLE: Summing Attribute Values in Spans (YAML)\nDESCRIPTION: This full configuration demonstrates the setup for summing values from a specific span attribute. The configuration includes a 'foo' receiver, defines the 'sum' connector with a 'spans' section outputting to a custom metric name, and specifies which attribute to sum. It integrates the connector into the pipelines under 'service', routing traces data to the connector and the summed result to the 'bar' exporter. Inputs include span telemetry with the specified attribute, and output is a time series with the summed metric.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sum-connector.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\\n  foo:\\nconnectors:\\n  sum:\\n    spans:\\n      my.example.metric.name:\\n        source_attribute: attribute.with.numerical.value\\n\\nexporters:\\n  bar:\\n\\nservice:\\n  pipelines:\\n    metrics/sum:\\n       receivers: [sum]\\n       exporters: [bar]\\n    traces:\\n       receivers: [foo]\\n       exporters: [sum]\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Timed Events with OpenTelemetry Instance in iOS\nDESCRIPTION: This alternative approach shows how to create custom timed events using the OpenTelemetry instance. It creates a span to measure a function's duration and adds custom attributes to provide additional context.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_6\n\nLANGUAGE: swift\nCODE:\n```\nfunc calculateTax() {\n   let tracer = OpenTelemetry.instance.tracerProvider.get(instrumentationName: \"MyApp\", instrumentationVersion: nil)\n   let span = tracer.spanBuilder(spanName: \"calculateTax\").startSpan()\n   span.setAttribute(key: \"numClaims\", value: claims.count)\n //...\n //...\n   span.end() // You can also use defer for this\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP/HTTP Token Passthrough using Headers Setter in YAML\nDESCRIPTION: This YAML configuration demonstrates how to implement access token passthrough for the `otlphttp` exporter, associating data points with an organization token passed in request metadata. It configures the `headers_setter` extension to upsert the `X-SF-TOKEN` header from context, enables `include_metadata: true` in the `otlp` receiver's HTTP protocol settings, specifies `X-SF-Token` in the `batch` processor's `metadata_keys`, and finally configures the `otlphttp` exporter to use the `headers_setter` as its authenticator via the `auth` setting. A default token `\"mytoken\"` is also specified in the exporter's headers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlphttp-exporter.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  headers_setter:\n    headers:\n      - action: upsert\n        key: X-SF-TOKEN\n        from_context: X-SF-TOKEN\n\nreceivers:\n  otlp:\n    protocols:\n      http:\n        include_metadata: true\n\nprocessors:\n  batch:\n    metadata_keys:\n    - X-SF-Token\n\nexporters:\n  otlphttp:\n    metrics_endpoint: https://ingest.<realm>.signalfx.com/v2/datapoint/otlp\n    traces_endpoint: https://ingest.<realm>.signalfx.com/v2/trace/otlp\n    headers:\n        \"X-SF-Token\": \"mytoken\"\n    auth:\n      authenticator: headers_setter\n```\n\n----------------------------------------\n\nTITLE: Getting the TracerProvider in PHP\nDESCRIPTION: Retrieves the global OpenTelemetry TracerProvider instance using the `Globals` class. This provider is the entry point for creating Tracers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-manual-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: php\nCODE:\n```\n$tracerProvider = Globals::tracerProvider();\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Collector Configuration Template\nDESCRIPTION: Template configuration for the OpenTelemetry Collector including extensions, receivers, processors, and exporters setup.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-nomad.rst#2025-04-22_snippet_2\n\nLANGUAGE: none\nCODE:\n```\ntemplate {\n data        = <<EOF\n# The following example shows how to set up your Collector configuration file.\nextensions:\n  health_check: null\n  zpages: null\nreceivers:\n  hostmetrics:\n    collection_interval: 10s\n    scrapers:\n      cpu: null\n      disk: null\n      filesystem: null\n      load: null\n      memory: null\n      network: null\n      paging: null\n      processes: null\nprocessors:\n  batch: null\n  memory_limiter:\n    check_interval: 2s\n    limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}\nexporters:\n  signalfx:\n    access_token: ${SPLUNK_ACCESS_TOKEN}\n    api_url: https://api.${SPLUNK_REALM}.signalfx.com\n    correlation: null\n    ingest_url: https://ingest.${SPLUNK_REALM}.signalfx.com\n    sync_host_metadata: true\n  debug:\n    verbosity: detailed\nservice:\n  extensions:\n  - health_check\n  - zpages\n  pipelines:\n    metrics:\n      exporters:\n      - logging\n      - signalfx\n      processors:\n      - memory_limiter\n      - batch\n      receivers:\n      - hostmetrics\n      - signalfx\nEOF\n    destination = \"local/config/otel-agent-config.yaml\"\n}\n```\n\n----------------------------------------\n\nTITLE: Statsd Pipeline Configuration\nDESCRIPTION: Configuration to add the Statsd monitor to the metrics pipeline receivers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/statsd.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/statsd]\n```\n\n----------------------------------------\n\nTITLE: Setting Metric Aggregation Temporality (DELTA) in Node.js\nDESCRIPTION: Explains how to configure the aggregation temporality for custom metrics in Node.js using the Splunk OpenTelemetry distribution. This example uses the `metricReaderFactory` to set up a `PeriodicExportingMetricReader` with an `OTLPMetricExporter` (using gRPC) and explicitly sets the `temporalityPreference` to `AggregationTemporality.DELTA`, causing metrics to be reported relative to the last collection interval instead of cumulatively (the default). Dependencies include `@splunk/otel`, `@opentelemetry/exporter-metrics-otlp-grpc`, and `@opentelemetry/sdk-metrics-base`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/manual-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require('@splunk/otel');\nconst { OTLPMetricExporter } = require('@opentelemetry/exporter-metrics-otlp-grpc');\nconst { AggregationTemporality, PeriodicExportingMetricReader } = require('@opentelemetry/sdk-metrics-base');\n\nstart({\n  serviceName: 'my-service',\n  metrics: {\n    metricReaderFactory: () => {\n      return [\n        new PeriodicExportingMetricReader({\n          exporter: new OTLPMetricExporter({\n            temporalityPreference: AggregationTemporality.DELTA\n          })\n        })\n      ]\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Running a Flask+uWSGI Application with OpenTelemetry\nDESCRIPTION: Command to run a Flask application with uWSGI, specifying the appropriate parameters for HTTP serving, application file, callable, master mode, and threading support.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/instrument-python-frameworks.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuwsgi --http :9090 --wsgi-file <your_app.py> --callable <your_wsgi_callable> --master --enable-threads\n```\n\n----------------------------------------\n\nTITLE: Configuring Trace and Resource Attribute Logging with Log4j - XML\nDESCRIPTION: This configuration snippet for Log4j (typically placed in 'log4j2.xml') adds trace and resource attributes (trace_id, span_id, service.name, deployment.environment, trace_flags) to each logged JSON event using the JsonLayout. It requires the Log4j 2 library and the Splunk OTel Java agent. Modify the keys as needed to capture additional MDC context properties. The output enables downstream log analysis by consistently logging trace context.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<Configuration status=\\\"WARN\\\">\\n   <Appenders>\\n      <Console name=\\\"STDOUT\\\" target=\\\"SYSTEM_OUT\\\">\\n         <JsonLayout compact=\\\"true\\\" eventEol=\\\"true\\\">\\n            <KeyValuePair key=\\\"trace_id\\\" value=\\\"${ctx:trace_id}\\\"/>\\n            <KeyValuePair key=\\\"span_id\\\" value=\\\"${ctx:span_id}\\\"/>\\n            <KeyValuePair key=\\\"service.name\\\" value=\\\"${ctx:service.name}\\\"/>\\n            <KeyValuePair key=\\\"environment\\\" value=\\\"${ctx:deployment.environment}\\\"/>\\n            <KeyValuePair key=\\\"trace_sampled\\\" value=\\\"${ctx:trace_flags}\\\"/>\\n         </JsonLayout>\\n      </Console>\\n   </Appenders>\\n   <!-- More configuration -->\\n</Configuration>\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Exporter Endpoint (Shell)\nDESCRIPTION: Optionally sets the `OTEL_EXPORTER_OTLP_ENDPOINT` environment variable to specify the network address (host and port) of the OpenTelemetry Collector instance where telemetry data should be sent. This is necessary if the Collector is not running on the default `localhost:4317`. Replace `<yourCollectorEndpoint>:<yourCollectorPort>` with the correct values. Examples provided for both Linux (bash) and Windows PowerShell.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/instrument-go-application.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\n----------------------------------------\n\nTITLE: Customizing AlertManager Configurations\nDESCRIPTION: This YAML snippet serves as a template for advanced AlertManager configurations, allowing custom fields and proxy URL. It guides the setup of alert notifications with specific display names, message types, and proxy settings for external alert delivery. Dependencies include a properly configured proxy and Prometheus environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/prometheus-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nroute:\n   group_by: ['alertname', 'cluster', 'service']\n   group_wait: 30s\n   group_interval: 5m\n   repeat_interval: 3h \n   receiver: victorOps-receiver \n\nreceivers:\n- name: victorOps-receiver\n  victorops_configs:\n    - api_key: \n      routing_key: \n      entity_display_name: '{{ .CommonAnnotations.summary }}'\n      message_type: '{{ .CommonLabels.severity }}'\n      state_message: 'Alert: {{ .CommonLabels.alertname }}. Summary:{{ .CommonAnnotations.summary }}. RawData: {{ .CommonLabels }}'\n      custom_fields:\n         : '{{ .CommonLabels.eai_nbr }}'\n      # We must set a proxy to be able to send alerts to external systems\n      http_config:\n         proxy_url: 'http://internet.proxy.com:3128'\n```\n\n----------------------------------------\n\nTITLE: Configuring Collectd Uptime Receiver in OpenTelemetry Collector\nDESCRIPTION: YAML configuration for setting up the Collectd Uptime monitor receiver in the OpenTelemetry Collector. Defines the receiver configuration with type collectd/uptime.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/collectd-uptime.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/collectd/uptime:\n    type: collectd/uptime\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Integrating SignalFx Receiver into Metrics and Logs Pipelines (YAML)\nDESCRIPTION: Demonstrates how to include the configured `signalfx` receiver in both the `metrics` and `logs` pipelines within the `service` section of the OpenTelemetry Collector configuration. It also shows standard processors (`memory_limiter`, `batch`) and the corresponding `signalfx` exporter needed to process and send the collected data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [signalfx]\n      processors: [memory_limiter, batch]\n      exporters: [signalfx]\n    logs:\n      receivers: [signalfx]\n      processors: [memory_limiter, batch]\n      exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Adding PostgreSQL Receiver to OpenTelemetry Collector Configuration\nDESCRIPTION: This snippet shows how to enable the PostgreSQL receiver in the configuration file for the Splunk OpenTelemetry Collector. It specifies settings such as the endpoint, authentication details, and TLS configuration. The receiver should be included in the 'metrics' pipeline under the 'service' section of the configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/postgresql-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   receivers:\\n     postgresql:\\n       endpoint: localhost:5432\\n       transport: tcp\\n       username: otel\\n       password: ${env:PGSQL_PASSWORD}\\n       databases:\\n         - otel\\n       collection_interval: 10s\\n       tls:\\n         insecure: false\\n         insecure_skip_verify: false\\n         ca_file: /home/otel/authorities.crt\\n         cert_file: /home/otel/mypostgrescert.crt\\n         key_file: /home/otel/mypostgreskey.key\n```\n\n----------------------------------------\n\nTITLE: Running Bootstrap Script for Instrumentation Installation\nDESCRIPTION: Executes the bootstrap script to automatically install instrumentation for all supported packages in your Python environment. This prepares your application for tracing without manual instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsplunk-py-trace-bootstrap\n```\n\n----------------------------------------\n\nTITLE: Adding Collectd Uptime to Metrics Pipeline\nDESCRIPTION: YAML configuration showing how to add the Collectd Uptime monitor to the metrics pipeline in the service configuration section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/collectd-uptime.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/collectd/uptime]\n```\n\n----------------------------------------\n\nTITLE: Creating New Metric by Scaling Existing Metric\nDESCRIPTION: This configuration example demonstrates how to create a new metric 'pod.memory.usage.bytes' by scaling the 'pod.memory.usage.megabytes' metric using the scale rule type.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-generation-processor.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nrules:\n    - name: pod.memory.usage.bytes\n      unit: Bytes\n      type: scale\n      metric1: pod.memory.usage.megabytes\n      operation: multiply\n      scale_by: 1048576\n```\n\n----------------------------------------\n\nTITLE: Docker Browser Version Inspection Commands\nDESCRIPTION: Shell commands to pull the Splunk synthetics runner Docker image and inspect browser type and version labels.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull quay.io/signalfx/splunk-synthetics-runner:latest\ndocker inspect -f '{{ index .Config.Labels \"browser-type\" }}' quay.io/signalfx/splunk-synthetics-runner:latest\ndocker inspect -f '{{ index .Config.Labels \"browser-version\" }}' quay.io/signalfx/splunk-synthetics-runner:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry SDK with All Instrumentations in Ruby on Rails\nDESCRIPTION: This Ruby code, typically placed in a Rails initializer, configures the OpenTelemetry SDK to use all available instrumentation libraries in a Rails application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/ruby-manual-instrumentation.rst#2025-04-22_snippet_6\n\nLANGUAGE: ruby\nCODE:\n```\n# config/initializers/opentelemetry.rb\nrequire \"opentelemetry/sdk\"\n...\nOpenTelemetry::SDK.configure do |c|\nc.use_all()\nend\n```\n\n----------------------------------------\n\nTITLE: Mounting Secrets in Splunk Helm Chart\nDESCRIPTION: This snippet demonstrates how to mount a custom secret within agent, clusterReceiver, and gateway components of the Splunk Helm chart. Dependencies include preconfigured Kubernetes secrets. Use extraVolumes and extraVolumeMounts to specify the secretName and the mountPath respectively for various components.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n  agent:\n    extraVolumes:\n      - name: custom-tls\n        secret:\n          secretName: my-custom-tls\n    extraVolumeMounts:\n      - name: custom-tls\n        mountPath: /etc/ssl/certs/\n        readOnly: true\n\n  clusterReceiver:\n    extraVolumes:\n      - name: custom-tls\n        secret:\n          secretName: my-custom-tls\n    extraVolumeMounts:\n      - name: custom-tls\n        mountPath: /etc/ssl/certs/\n        readOnly: true\n\n  gateway:\n    extraVolumes:\n      - name: custom-tls\n        secret:\n          secretName: my-custom-tls\n    extraVolumeMounts:\n      - name: custom-tls\n        mountPath: /etc/ssl/certs/\n        readOnly: true\n```\n\n----------------------------------------\n\nTITLE: Displaying Splunk OpenTelemetry Collector Installer Options\nDESCRIPTION: This bash command downloads the Splunk OpenTelemetry Collector installer script and displays all available configuration options using the -h flag.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh;\nsh /tmp/splunk-otel-collector.sh -h\n```\n\n----------------------------------------\n\nTITLE: Configuring Envoy StatsD Sink for AppMesh\nDESCRIPTION: Configuration for the Envoy StatsD sink on AppMesh, including address settings and prefix configuration for metric collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/aws-appmesh.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nstats_sinks:\n -\n  name: \"envoy.statsd\"\n  config:\n   address:\n    socket_address:\n     address: \"127.0.0.1\"\n     port_value: 8125\n     protocol: \"UDP\"\n   prefix: statsd.appmesh\n```\n\n----------------------------------------\n\nTITLE: Default Logging Format for Python Trace Correlation in Splunk Observability Cloud\nDESCRIPTION: The default logging format used by the Splunk OTel Python agent to include trace metadata (trace_id, span_id, and service.name) in log statements.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n%(asctime)s %(levelname)s [%(name)s] [%(filename)s:%(lineno)d] [trace_id=%(otelTraceID)s span_id=%(otelSpanID)s service.name=%(otelServiceName)s] - %(message)s\n```\n\n----------------------------------------\n\nTITLE: Example of Attribute Extraction Configuration in YAML\nDESCRIPTION: Practical example of configuring attribute extraction from a span name using a specific regular expression.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/span-processor.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nspan/to_attributes:\n  name:\n    to_attributes:\n      rules:\n        - ^/api/v1/document/(?P<documentId>.*)/update$\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Receiver with SASL and TLS\nDESCRIPTION: This example demonstrates how to configure the Kafka receiver to connect to Kafka using SASL authentication and TLS encryption. It sets the username, password, SASL mechanism, and disables insecure connections.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kafka-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kafka:\n    auth:\n      sasl:\n        username: \"user\"\n        password: \"secret\"\n        mechanism: \"SCRAM-SHA-512\"\n      tls:\n        insecure: false\n```\n\n----------------------------------------\n\nTITLE: Advanced HTTP Check Receiver Configuration Example\nDESCRIPTION: Detailed YAML configuration example for the HTTP check receiver, including multiple targets with different methods and custom headers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/http-check-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  httpcheck:\n    targets:\n      - endpoint: http://endpoint:80\n        method: GET\n      - endpoint: http://localhost:8080/health\n        method: GET\n      - endpoint: http://localhost:8081/health\n        method: POST\n        headers:\n          test-header: \"test-value\"\n    collection_interval: 10s\n```\n\n----------------------------------------\n\nTITLE: Creating MySQL User for Monitor Access\nDESCRIPTION: SQL commands to create a MySQL user with minimal privileges required for the monitor to collect metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/mysql.rst#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER '<username>'@'localhost' IDENTIFIED BY '<password>';\n-- Give appropriate permissions\n-- (\"GRANT USAGE\" is synonymous to \"no privileges\")\nGRANT USAGE ON *.* TO '<username>'@'localhost';\n-- Permissions for the stats options\nGRANT REPLICATION CLIENT ON *.* TO '<username>'@'localhost';\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Generation Processor in YAML\nDESCRIPTION: This snippet shows the basic structure for configuring the metricsgeneration processor in the YAML configuration file. It includes the processor definition and rule specification.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-generation-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  metricsgeneration:\n    rules:\n      - name: <new_metric_name>\n        unit: <new_metric_unit>\n        type: {calculate, scale}\n        metric1: <first_operand_metric>\n        metric2: <second_operand_metric>\n        operation: {add, subtract, multiply, divide, percent}\n```\n\n----------------------------------------\n\nTITLE: Journald Log Collection Command\nDESCRIPTION: Command for collecting logs from a systemd service via journald and redirecting them to a file for analysis during migration validation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\njournalctl -u my-service >my-service.log\n```\n\n----------------------------------------\n\nTITLE: Running Splunk OpenTelemetry Collector Docker Container with Socket Access\nDESCRIPTION: This bash command demonstrates how to run the Splunk OpenTelemetry Collector Docker container with access to the Docker socket. It mounts the socket and adds the container user to the required group.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v /var/run/docker.sock:/var/run/docker.sock:ro --group-add $(stat -c '%g' /var/run/docker.sock) quay.io/signalfx/splunk-otel-collector:latest <...>\n```\n\n----------------------------------------\n\nTITLE: Importing OpenTelemetry Trace Module in Python\nDESCRIPTION: Imports the OpenTelemetry trace module to enable manual instrumentation with custom spans.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\n```\n\n----------------------------------------\n\nTITLE: Backfilling Missing Attributes in Spans\nDESCRIPTION: This snippet showcases a configuration to backfill spans missing certain attributes using insert and upsert actions, then delete the temporary attributes to maintain cleanliness.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/attributes-processor.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nattributes/complex:\n  actions:\n    - key: operation\n      value: default\n      action: insert\n    - key: svc.operation\n      from_attribute: operation\n      action: upsert\n    - key: operation\n      action: delete\n```\n\n----------------------------------------\n\nTITLE: Adding Span Processor to Service Pipelines in YAML\nDESCRIPTION: Example of how to add the span processor to the service pipelines section of the configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/span-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      processors: [span]\n```\n\n----------------------------------------\n\nTITLE: Detailed Metrics Transform Processor Configuration Example (YAML)\nDESCRIPTION: Provides a comprehensive template for configuring the `metricstransform` processor. It outlines the structure for defining transformations, including selecting metrics (`include`, `match_type`, `experimental_match_labels`), specifying actions (`action`, `new_name`, `aggregation_type`), and defining detailed operations (`operations`) like adding, updating, or deleting labels and values, toggling types, scaling, and aggregating.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  metricstransform:\n  # transforms is a list of transformations with each element transforming a metric selected by metric name\n    transforms:\n    \n        # SPECIFY WHICH METRIC(S) TO MATCH\n      \n        # include specifies the metric name used to determine which metric(s) to operate on\n      - include: <metric_name>\n        \n        # match_type specifies whether the include name should be used as a strict match or regexp match, default = strict\n        match_type: {strict, regexp}\n  \n        # experimental_match_labels specifies the label set against which the metric filter will work. If experimental_match_labels is specified, transforms will only be applied to those metrics which \n        # have the provided metric label values. This works for both strict and regexp match_type. This is an experimental feature.\n        experimental_match_labels: {<label1>: <label_value1>, <label2>: <label_value2>}\n      \n        # SPECIFY THE ACTION TO TAKE ON THE MATCHED METRIC(S)\n      \n        # action specifies if the operations (specified below) are performed on metrics in place (update), on an inserted clone (insert), or on a new combined metric (combine)\n        action: {update, insert, combine}\n      \n        # SPECIFY HOW TO TRANSFORM THE METRIC GENERATED AS A RESULT OF APPLYING THE ABOVE ACTION\n      \n        # new_name specifies the updated name of the metric; if action is insert or combine, new_name is required\n        new_name: <new_metric_name_inserted>\n      \n        # aggregation_type defines how combined data points will be aggregated; if action is combine, aggregation_type is required\n        aggregation_type: {sum, mean, min, max}\n      \n        # submatch_case specifies the case that should be used when adding label values based on regexp submatches when performing a combine action; leave blank to use the submatch value as is\n        submatch_case: {lower, upper}\n      \n        # operations contain a list of operations that will be performed on the resulting metric(s)\n        operations:\n            # action defines the type of operation that will be performed, see examples below for more details\n          - action: {add_label, update_label, delete_label_value, toggle_scalar_data_type, experimental_scale_value, aggregate_labels, aggregate_label_values}\n            # label specifies the label to operate on\n            label: <label>\n            # new_label specifies the updated name of the label; if action is add_label, new_label is required\n            new_label: <new_label>\n            # aggregated_values contains a list of label values that will be aggregated; if action is aggregate_label_values, aggregated_values is required\n            aggregated_values: [values...]\n            # new_value specifies the updated name of the label value; if action is add_label or aggregate_label_values, new_value is required\n            new_value: <new_value>\n            # label_value specifies the label value for which points should be deleted; if action is delete_label_value, label_value is required\n            label_value: <label_value>\n            # label_set contains a list of labels that will remain after aggregation; if action is aggregate_labels, label_set is required\n            label_set: [labels...]\n            # aggregation_type defines how data points will be aggregated; if action is aggregate_labels or aggregate_label_values, aggregation_type is required\n            aggregation_type: {sum, mean, min, max}\n            # experimental_scale specifies the scalar to apply to values\n            experimental_scale: <scalar>\n            # value_actions contain a list of operations that will be performed on the selected label\n            value_actions:\n                # value specifies the value to operate on\n              - value: <current_label_value>\n                # new_value specifies the updated value\n                new_value: <new_label_value>\n```\n\n----------------------------------------\n\nTITLE: Configuring deployment.environment Tag Using the Resource Processor (OpenTelemetry Collector, YAML)\nDESCRIPTION: This YAML configuration snippet demonstrates how to set the `deployment.environment` span tag using the `resource/add_environment` processor within the Splunk Distribution of OpenTelemetry Collector. The snippet must be inserted in the collector's config file, and the section should be uncommented and configured with the appropriate environment value (e.g., 'staging', 'production'). Key parameters include the action ('insert'), the value (your environment name), and the key (`deployment.environment`). This configuration associates the tag with the host or container rather than only the application. Dependencies include the Splunk OpenTelemetry Collector. Limitation: This does not dynamically set the environment; the value is static per config file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/set-up-apm/environments.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nresource/add_environment:\n    attributes:\n    action: insert\n    value: staging/production/...\n    key: deployment.environment\n```\n\n----------------------------------------\n\nTITLE: Configuring OTEL Collector Attributes Processor (Insert)\nDESCRIPTION: YAML configuration snippet for the Splunk Distribution of OpenTelemetry Collector. It defines an `attributes` processor named 'setenduser.role' that uses the 'insert' action to add the tag 'enduser.role: admin' to spans, but only if the 'enduser.role' tag does not already exist.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/add-context-trace-span.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  ...\n  attributes/setenduser.role:\n    actions:\n    - key: enduser.role\n        value: \"admin\"\n        action: insert\n```\n\n----------------------------------------\n\nTITLE: Including Group by Attributes Processor in Service Pipelines\nDESCRIPTION: Example of including the Group by Attributes processor in service pipelines for metrics, logs, and traces.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/groupbyattrs-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      processors: [groupbyattrs]\n    logs:\n      processors: [groupbyattrs]\n    traces:\n      processors: [groupbyattrs]\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Build and Run Commands\nDESCRIPTION: Shell commands to build and run the docker-compose setup and check collector logs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/other-ingestion-collectd.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$> docker compose up --build\n```\n\nLANGUAGE: bash\nCODE:\n```\n$> docker logs otelcollector\n```\n\n----------------------------------------\n\nTITLE: Source Type Annotation Configuration\nDESCRIPTION: Bash command showing how to set custom sourcetype for pod logs using Kubernetes annotations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-logs.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl annotate pod -n <my-namespace> <my-pod> splunk.com/sourcetype=kube:apiserver-audit\n```\n\n----------------------------------------\n\nTITLE: Default SignalFx Receiver Configuration in OpenTelemetry Collector (YAML)\nDESCRIPTION: Specifies the default configuration for the SignalFx receiver within the OpenTelemetry Collector's `receivers` section. It defines the network endpoint (`0.0.0.0:9943`) where the receiver listens for incoming SignalFx proto format data. The `access_token_passthrough` option, shown commented out, controls whether the incoming SignalFx access token should be preserved.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  signalfx:\n    endpoint: 0.0.0.0:9943\n    # Whether to preserve incoming access token and\n    # use instead of exporter token. Default value is false.\n    # access_token_passthrough: true\n```\n\n----------------------------------------\n\nTITLE: Initializing Browser RUM in JavaScript\nDESCRIPTION: JavaScript initialization code for the npm package installation method, showing how to import and configure the Browser RUM agent.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/install-rum-browser.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport SplunkOtelWeb from '@splunk/otel-web';\nSplunkOtelWeb.init({\n   realm: '<realm>',\n   rumAccessToken: '<your_rum_token>',\n   applicationName: '<your_application_name>',\n   version: '<your_app_version>',\n   deploymentEnvironment: '<your_environment_name>'\n});\n```\n\n----------------------------------------\n\nTITLE: Retrieving Pod Statistics from Kubelet Summary API\nDESCRIPTION: Queries the Kubelet Summary API to verify pod statistics including CPU, memory, and network usage, which are required for the Collector to generate metrics correctly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-container.rst#2025-04-22_snippet_5\n\nLANGUAGE: none\nCODE:\n```\nkubectl get --raw \"/api/v1/nodes/\"${NODE_NAME}\"/proxy/stats/summary\" | jq '.pods[] | select(.podRef.name=='\\\"$POD_NAME\\\"') | {\"pod\": {\"name\": .podRef.name, \"cpu\": .cpu, \"memory\": .memory, \"network\": .network}} | del(.pod.network.interfaces)'\n{\n  \"pod\": {\n    \"name\": \"splunk-otel-collector-agent-6llkr\",\n    \"cpu\": {\n      \"time\": \"2022-05-20T18:38:47Z\",\n      \"usageNanoCores\": 10774467,\n      \"usageCoreNanoSeconds\": 1709095026234\n    },\n    \"memory\": {\n      \"time\": \"2022-05-20T18:38:47Z\",\n      \"availableBytes\": 781959168, \n      # Could be absent if pod memory limits were missing.\n      \"usageBytes\": 267563008,\n      \"workingSetBytes\": 266616832,\n      \"rssBytes\": 257036288,\n      \"pageFaults\": 0,\n      \"majorPageFaults\": 0\n    },\n    \"network\": {\n      \"time\": \"2022-05-20T18:38:55Z\",\n      \"name\": \"eth0\",\n      \"rxBytes\": 105523812442,\n      \"rxErrors\": 0,\n      \"txBytes\": 98159696431,\n      \"txErrors\": 0\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Snowflake Performance Metrics Collection Configuration (YAML)\nDESCRIPTION: This configuration example shows how to set up the SQL monitor to collect Snowflake performance and usage metrics. It includes connection parameters and references an external YAML file for queries.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/sql.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/sql:\n    type: sql\n    intervalSeconds: 3600\n    dbDriver: snowflake\n    params:\n      account: \"account.region\"\n      database: \"SNOWFLAKE\"\n      schema: \"ACCOUNT_USAGE\"\n      role: \"ACCOUNTADMIN\"\n      user: \"user\"\n      password: \"password\"\n    connectionString: \"{{.user}}:{{.password}}@{{.account}}/{{.database}}/{{.schema}}?role={{.role}}\"\n    queries:\n      {\"#from\": \"/etc/signalfx/snowflake-metrics.yaml\"}\n```\n\n----------------------------------------\n\nTITLE: Checking Log Generation on Linux\nDESCRIPTION: Commands to verify if the source is generating logs on Linux systems by monitoring log files and journald service\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/troubleshoot-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntail -f /var/log/myTestLog.log\njournalctl -u my-service.service -f\n```\n\n----------------------------------------\n\nTITLE: Adding SAPM Exporter to Service Pipeline in YAML\nDESCRIPTION: This configuration snippet demonstrates how to include the SAPM exporter in a traces pipeline within the services section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-apm-exporter.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    # To complete the configuration, include the exporter in a traces metrics pipeline. \n    traces:\n        receivers: [nop]\n        processors: [nop]\n        exporters: [sapm]\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Atlas Receiver with Environment Variables in YAML\nDESCRIPTION: This snippet shows how to configure the MongoDB Atlas receiver using environment variables for the public and private keys in the YAML configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/mongodb-atlas-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  mongodbatlas:\n    public_key: ${MONGODB_ATLAS_PUBLIC_KEY}\n    private_key: ${MONGODB_ATLAS_PRIVATE_KEY}\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Attributes for .NET Session\nDESCRIPTION: PowerShell command to set environment and service version attributes for OpenTelemetry in the current session. These attributes help identify the service and its environment in the observability platform.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_7\n\nLANGUAGE: powershell\nCODE:\n```\n# Configure environment and service version for current PowerShell session\n$env:OTEL_RESOURCE_ATTRIBUTES='deployment.environment=<envtype>,service.version=<version>'\n```\n\n----------------------------------------\n\nTITLE: Building OpenTelemetry Collector with Additional Target Systems\nDESCRIPTION: This command-line example demonstrates how to build the OpenTelemetry Collector with support for additional target systems in the JMX Metric Gatherer.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/jmx-receiver.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngo build -ldflags \"-X github.com/open-telemetry/opentelemetry-collector-contrib/receiver/jmxreceiver.MetricsGathererHash=<sha256hash>\n      -X github.com/open-telemetry/opentelemetry-collector-contrib/receiver/jmxreceiver.AdditionalTargetSystems=newtarget,othernewtarget\" ...\n```\n\n----------------------------------------\n\nTITLE: Stopping and Uninstalling Smart Agent Service on Windows\nDESCRIPTION: PowerShell commands to stop and uninstall the SignalFx Smart Agent Windows service when installed from a ZIP file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_14\n\nLANGUAGE: PowerShell\nCODE:\n```\nSignalFxAgent\\bin\\signalfx-agent.exe -service \"stop\"\nSignalFxAgent\\bin\\signalfx-agent.exe -service \"uninstall\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Node Statistics from Kubelet Summary API\nDESCRIPTION: Queries the Kubelet Summary API to verify node statistics including CPU, memory, and network usage, which are required for the Collector to generate metrics correctly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-container.rst#2025-04-22_snippet_2\n\nLANGUAGE: none\nCODE:\n```\nkubectl get --raw \"/api/v1/nodes/\"${NODE_NAME}\"/proxy/stats/summary\" | jq '{\"node\": {\"name\": .node.nodeName, \"cpu\": .node.cpu, \"memory\": .node.memory, \"network\": .node.network}} | del(.node.network.interfaces)'\n{\n  \"node\": {\n    \"name\": \"node-1\",\n    \"cpu\": {\n      \"time\": \"2022-05-20T18:12:08Z\",\n      \"usageNanoCores\": 149771849,\n      \"usageCoreNanoSeconds\": 2962750554249399\n    },\n    \"memory\": {\n      \"time\": \"2022-05-20T18:12:08Z\",\n      \"availableBytes\": 2701385728,  \n      # Could be absent if node memory allocations were missing.\n      \"usageBytes\": 3686178816,\n      \"workingSetBytes\": 1421492224,\n      \"rssBytes\": 634343424,\n      \"pageFaults\": 18632526,\n      \"majorPageFaults\": 726\n    },\n    \"network\": {\n      \"time\": \"2022-05-20T18:12:08Z\",\n      \"name\": \"eth0\",\n      \"rxBytes\": 105517219156,\n      \"rxErrors\": 0,\n      \"txBytes\": 98151853779,\n      \"txErrors\": 0\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Traefik Receiver Configuration in YAML\nDESCRIPTION: Basic configuration example showing how to activate the Traefik monitor in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/traefik.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/traefik:\n    type: traefik\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Pipeline\nDESCRIPTION: Configuration showing how to add the Prometheus Go monitor to the service pipeline metrics receivers section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-go.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/prometheus-go]\n```\n\n----------------------------------------\n\nTITLE: Complete Cloud Foundry Receiver Configuration Example\nDESCRIPTION: This snippet provides a full configuration example for the Cloud Foundry receiver, including RLP gateway and UAA settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/cloudfoundry-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  cloudfoundry:\n    rlp_gateway:\n      endpoint: \"https://log-stream.sys.example.internal\"\n      tls:\n        insecure_skip_verify: false\n      shard_id: \"opentelemetry\"\n    uaa:\n      endpoint: \"https://uaa.sys.example.internal\"\n      tls:\n        insecure_skip_verify: false\n      username: \"otelclient\"\n      password: \"changeit\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Limiter Processor for OpenTelemetry Collector\nDESCRIPTION: YAML configuration for the memory_limiter processor which should be defined on every Collector instance. This processor should be placed first in the pipeline, immediately after receivers, to prevent the Collector from consuming excessive memory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n   memory_limiter:\n      check_interval: \n      limit_mib: \n      spike_limit_mib:\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Attributes on Linux\nDESCRIPTION: Shell command to set environment and service version attributes for OpenTelemetry on Linux. These attributes help identify the service and its environment in the observability platform.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_RESOURCE_ATTRIBUTES='deployment.environment=<envtype>,service.version=<version>'\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct OTLP Export to Splunk Cloud via gRPC (Bash)\nDESCRIPTION: Sets environment variables to configure the OpenTelemetry SDK to send trace data directly to the Splunk Observability Cloud ingest endpoint using the OTLP/gRPC protocol, bypassing a local collector. Requires replacing the placeholders `<access_token>` with a valid Splunk ingest access token and `<realm>` with the appropriate Splunk Observability Cloud realm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/cpp/instrument-cpp.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nOTEL_EXPORTER_OTLP_PROTOCOL=grpc\nOTEL_EXPORTER_OTLP_TRACES_HEADERS=x-sf-token=<access_token>\nOTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.<realm>.signalfx.com\n```\n\n----------------------------------------\n\nTITLE: Sample OTLP Exporter Configuration with Endpoint and TLS in YAML\nDESCRIPTION: This YAML snippet provides example configurations for the OTLP exporter. It shows how to set the gRPC endpoint (`endpoint`) and configure TLS using certificate and key files (`cert_file`, `key_file`). It also demonstrates creating a second instance (`otlp/2`) configured to use an insecure connection (`insecure: true`).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlp-exporter.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n.. code-block:: yaml\n\n  exporters:\n    otlp:\n      endpoint: otelcol2:4317\n      tls:\n        cert_file: file.cert\n        key_file: file.key\n    otlp/2:\n      endpoint: otelcol2:4317\n      tls:\n        insecure: true\n```\n\n----------------------------------------\n\nTITLE: Triggering a Critical Incident - Splunk On-Call REST Endpoint - JSON\nDESCRIPTION: Provides an example JSON request body for creating (triggering) a critical incident in Splunk On-Call, including all key fields for incident identification and description. Required fields include message_type, entity_id, entity_display_name, and state_message. The payload is sent as the POST request body to the REST endpoint, and a successful request results in incident creation with corresponding details.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rest-endpoint-integration-guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \\u201cmessage_type\\u201d:\\u201cCRITICAL\\u201d, \\u201centity_id\\u201d:\\u201cdisk\\nspace/db01.mycompany.com\\u201d, \\u201centity_display_name\\u201d:\\u201cCritically Low Disk\\nSpace on DB01\\u201d, \\u201cstate_message\\u201d:\\u201cThe disk is really really full. Here is\\nabunch of information about the problem\\u201d }\n```\n\n----------------------------------------\n\nTITLE: Adding Filter Processor for Log Severity\nDESCRIPTION: Adds a filter processor to exclude logs with severity level 5 or lower using regular expressions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-edit.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n     processors:\n       filter/severity_text:\n         logs:\n           exclude:\n             match_type: regexp\n               severity_texts:\n               - -[5-7]-\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector with Fluentd on Linux\nDESCRIPTION: This command downloads and installs the Splunk OpenTelemetry Collector with Fluentd enabled. It requires the Splunk realm and access token as parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-config-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-fluentd --realm $SPLUNK_REALM -- $SPLUNK_ACCESS_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Splunk OpenTelemetry Collector Installation\nDESCRIPTION: Defines essential environment variables needed for the Collector installation, including the Splunk realm, access token, and memory allocation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-start.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport SPLUNK_REALM=\"<splunk_o11y_cloud_realm>\"\nexport SPLUNK_ACCESS_TOKEN=\"<splunk_access_token>\"\nexport SPLUNK_MEMORY_TOTAL_MIB=\"512\"\n```\n\n----------------------------------------\n\nTITLE: Pipeline Configuration for Filesystems Monitor\nDESCRIPTION: Configuration to add the filesystems monitor to the metrics and logs pipelines.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/filesystems.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/filesystems]\n    logs:\n      receivers: [smartagent/filesystems]\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes DaemonSet for Auto-Discovery\nDESCRIPTION: YAML configuration for activating discovery mode in the Helm chart with PostgreSQL authentication properties. Includes environment variable setup for secure credential management.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-third-party.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n\n  #...\n\n  discovery:\n    enabled: true # Turned off by default\n    properties:\n      extensions:\n        k8s_observer:\n          config:\n            auth_type: serviceAccount  # Default auth_type value\n      receivers:\n        postgres:\n          config:\n            # Environment variables populated by secret data\n            username: '${env:POSTGRES_USER}'\n            password: '${env:POSTGRES_PASSWORD}'\n            tls:\n              insecure: true\n  # Activates auto discovery in UI. Only available for the Collector version 0.109.0 or higher\n  featureGates: splunk.continuousDiscovery             \n\n# ...\n\nextraEnvs:\n   # Environment variables using a manually created secret\n   - name: POSTGRES_USER\n     valueFrom:\n       secretKeyRef:\n         name: postgres-monitoring\n         key: username\n   - name: POSTGRES_PASSWORD\n     valueFrom:\n       secretKeyRef:\n         name: postgres-monitoring\n         key: password\n```\n\n----------------------------------------\n\nTITLE: Basic Helm Values Configuration for OpenTelemetry Collector\nDESCRIPTION: Initial YAML configuration for deploying the OpenTelemetry Collector with basic settings including cluster name, Splunk credentials, and operator enablement.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nclusterName: <your_cluster_name>\n\n# Your Splunk Observability Cloud realm and access token\nsplunkObservability:\n  realm: <splunk_realm>\n  accessToken: <splunk_access_token>\n\n# Activates the OpenTelemetry Kubernetes Operator\noperator:\n  enabled: true\n\n# Installs the Custom Resource Definitions (CRDs) required by the operator\n# Must be set unless CRDs are pre-installed manually\noperatorcrds: \n  install: true\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Datasource with TCP Connection\nDESCRIPTION: Example configuration for connecting to a MySQL datasource through TCP with the SQL Query receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sqlquery-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  sqlquery:\n    driver: mysql\n    datasource: \"user:password@tcp(host:port)/schema\"\n    queries:\n      - sql: \"your_query\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Stats Receiver to Skip Certificate Verification\nDESCRIPTION: YAML configuration snippet to disable certificate verification in the OTel agent's Kubelet Stats receiver, which allows metrics collection with invalid or self-signed certificates.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-missing-metrics.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  config:\n    receivers:\n      kubeletstats:\n        insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Multi-Host Pipeline Configuration\nDESCRIPTION: Shows how to configure the metrics pipeline for multiple HTTP monitor receivers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/http.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/host1, smartagent/host2]\n```\n\n----------------------------------------\n\nTITLE: Activating Metrics Collection Environment Variable (Windows PowerShell)\nDESCRIPTION: Sets the SPLUNK_METRICS_ENABLED environment variable to 'true' using Windows PowerShell. This activates the automatic collection of runtime metrics for the Node.js application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n$env:SPLUNK_METRICS_ENABLED='true'\n```\n\n----------------------------------------\n\nTITLE: Rotating Access Tokens via Splunk Observability Cloud API - Bash\nDESCRIPTION: This bash code snippet shows how to rotate an access token using the Splunk Observability Cloud API's token secret rotation endpoint. It uses curl to send a POST request to the /v2/token/{name}/rotate endpoint, passing grace period and expiry settings via query parameters. Relevant headers for content type and API authorization (X-SF-TOKEN) must be set, and the correct realm, token name, and API token should be provided. The expected input includes secure credentials, the realm, token name, and optional grace period and expiry values; the output is the HTTP response from the API. Requires curl and a valid Splunk Observability Cloud API token.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/authentication-tokens/org-tokens.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X  POST \"https://api.{realm}.signalfx.com/v2/token/{name}/rotate?graceful={gracePeriod}&secondsUntilExpiry={secondsUntilExpiry}\" \\\n   -H \"Content-type: application/json\" \\\n   -H \"X-SF-TOKEN: <your-user-session-api-token-value>\"\n```\n\n----------------------------------------\n\nTITLE: Defining Splunk On-Call Resources in Terraform\nDESCRIPTION: This Terraform HCL code defines various Splunk On-Call resources declaratively. It includes creating a team ('DevOps-Test'), a user ('john.doe-test'), assigning the user to the team, defining email and phone contact methods for the user, setting up an escalation policy ('High Severity') for the team, and creating a routing key ('infrastructure-high-severity') linked to the escalation policy. Note that only user-level permissions can be set via Terraform; admin privileges require UI changes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/manage-splunk-oncall-using-terraform.rst#2025-04-22_snippet_3\n\nLANGUAGE: terraform\nCODE:\n```\n# Create a team within victorops\nresource \"victorops_team\" \"team\" {\nname = \"DevOps-Test\"\n}\n# Create a user within the victorops organization\nresource \"victorops_user\" \"user1\" {\nfirst_name       = \"John\"\nlast_name        = \"Doe\"\nuser_name        = \"john.doe-test\"\nemail            = \"john.doe@example.org\"\nis_admin         = \"false\"\n}\n# Assign user to a team\nresource \"victorops_team_membership\" \"test-membership\" {\nteam_id          = victorops_team.team.id\nuser_name        = victorops_user.user1.user_name\n}\n# Create email contact method for a user\nresource \"victorops_contact\" \"contact_email\" {\nuser_name    = victorops_user.user1.user_name\ntype         = \"email\"\nvalue        = \"john.doe2@example.org\"\nlabel        = \"test email\"\n}\n# Create phone number contact method for a user\nresource \"victorops_contact\" \"contact_phone\" {\nuser_name    = victorops_user.user1.user_name\ntype         = \"phone\"\nvalue        = \"+12345678900\"\nlabel        = \"test phone\"\n}\n# Create an Escalation Policy for the team\nresource \"victorops_escalation_policy\" \"devops_high_severity\" {\nname    = \"High Severity\"\nteam_id = victorops_team.team.id\nstep {\ntimeout = 0\nentries = [\n{\ntype = \"user\"\nusername = \"john.doe-test\"\n}\n]\n}\n}\n# Create a Routing Key that uses the above Escalation Policy\nresource \"victorops_routing_key\" \"infrastructure_high_severity\" {\nname = \"infrastructure-high-severity\"\ntargets = [victorops_escalation_policy.devops_high_severity.id]\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Span Naming Configuration in YAML\nDESCRIPTION: Practical example of configuring span naming using specific attributes and a separator.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/span-processor.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nspan:\n  name:\n    from_attributes: [\"db.svc\", \"operation\"]\n    separator: \"::\"\n```\n\n----------------------------------------\n\nTITLE: Configuring CoreDNS Receiver in OpenTelemetry Collector (YAML)\nDESCRIPTION: This snippet shows how to activate the CoreDNS integration by adding the necessary configuration to the OpenTelemetry Collector. It defines the receiver and adds it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/coredns.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/coredns:\n    type: coredns\n    ...  # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/coredns]\n```\n\n----------------------------------------\n\nTITLE: ECS Task Definition with Sidecar Agent\nDESCRIPTION: ECS task definition JSON for running a Java application with the OpenTelemetry agent as a sidecar container.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-fargate-java.rst#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n \"family\": \"agent-init-container-example\",\n \"containerDefinitions\": [\n     {\n         \"name\": \"tomcat\",\n         \"image\": \"tomcat:9.0\",\n         \"cpu\": 0,\n         \"portMappings\": [\n             {\n                 \"name\": \"tomcat-8080-tcp\",\n                 \"containerPort\": 8080,\n                 \"protocol\": \"tcp\",\n                 \"appProtocol\": \"http\"\n             }\n         ],\n         \"essential\": true,\n         \"environment\": [\n             {\n                 \"name\": \"OTEL_SERVICE_NAME\",\n                 \"value\": \"myservice\"\n             },\n             {\n                 \"name\": \"OTEL_RESOURCE_ATTRIBUTES\",\n                 \"value\": \"deployment.environment=test,service.version=1.0\"\n             },\n             {\n                 \"name\": \"JAVA_TOOL_OPTIONS\",\n                 \"value\": \"-javaagent:/opt/splunk/splunk-otel-javaagent.jar\"\n             }\n         ],\n         \"environmentFiles\": [],\n         \"mountPoints\": [],\n         \"volumesFrom\": [\n             {\n                 \"sourceContainer\": \"splunk-java-agent\",\n                 \"readOnly\": false\n             }\n         ],\n         \"dependsOn\": [\n             {\n                 \"containerName\": \"splunk-java-agent\",\n                 \"condition\": \"START\"\n             }\n         ]\n     }\n ]\n}\n```\n\n----------------------------------------\n\nTITLE: Default Collector Container Definition for AWS Fargate\nDESCRIPTION: JSON configuration for deploying the OpenTelemetry Collector container with default settings in AWS Fargate. Includes environment variables for access token, realm, configuration path, and image exclusions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-fargate.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"environment\": [\n  {\n    \"name\": \"SPLUNK_ACCESS_TOKEN\",\n    \"value\": \"MY_SPLUNK_ACCESS_TOKEN\"\n  },\n  {\n    \"name\": \"SPLUNK_REALM\",\n    \"value\": \"MY_SPLUNK_REALM\"\n  },\n  {\n    \"name\": \"SPLUNK_CONFIG\",\n    \"value\": \"/etc/otel/collector/fargate_config.yaml\"\n  },\n  {\n    \"name\": \"ECS_METADATA_EXCLUDED_IMAGES\",\n    \"value\": \"[\\\"quay.io/signalfx/splunk-otel-collector*\\\"]\"\n  }\n],\n\"image\": \"quay.io/signalfx/splunk-otel-collector:0.33.0\",\n\"essential\": true,\n\"name\": \"splunk_otel_collector\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding OpenTelemetry Extension Configuration to php.ini - Linux (INI)\nDESCRIPTION: This configuration snippet is added to the php.ini file on Linux to enable the OpenTelemetry extension. It loads the shared library for the extension and is required for the instrumentation to work. The configuration must be placed in the proper section of php.ini and match the installed extension file name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/instrument-php-application.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[opentelemetry]\\nextension=opentelemetry.so\n```\n\n----------------------------------------\n\nTITLE: Multi-line Log Processing Configuration\nDESCRIPTION: YAML configuration for processing multi-line logs in Kubernetes using the OpenTelemetry Collector with regex pattern matching.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-logs.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlogsCollection:\n  containers:\n    multilineConfigs:\n      - namespaceName:\n          value: default\n        podName:\n          value: buttercup-app-.*\n          useRegexp: true\n        containerName:\n          value: server\n          firstEntryRegex: ^[^\\s].*\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables in IIS Application Pools (applicationHost.config XML)\nDESCRIPTION: This XML snippet demonstrates how to declare environment variables for a specific IIS application pool using the <environmentVariables> block within the applicationHost.config file. The example sets OpenTelemetry resource attributes (deployment environment and service version). Prerequisite: administrator access to the server and the ability to modify applicationHost.config. Inputs include variable names and values; outputs are changes to the environment for IIS worker processes. Edits to this file require an IIS reset or pool recycle to take effect; incorrect XML may hinder app pool startup.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/advanced-config-iis-apps.rst#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<environmentVariables>\\n    <add name=\\\"OTEL_RESOURCE_ATTRIBUTES\\\" value=\\\"deployment.environment=test,service.version=1.0.0\\\" />\\n</environmentVariables>\n```\n\n----------------------------------------\n\nTITLE: Vue.js Error Handler Integration\nDESCRIPTION: Implementation of error handlers for Vue.js versions 2.x and 3.x with Splunk RUM integration\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nimport Vue from 'vue';\nimport SplunkRum from '@splunk/otel-web';\n\nconst app = createApp(App);\n\napp.config.errorHandler = function (error, vm, info) {\n// To avoid loading issues due to content blockers\n// when using the CDN version of the Browser RUM\n// agent, add if (window.SplunkRum) checks around\n// SplunkRum API calls\n   SplunkRum.error(error, info)\n}\napp.mount('#app')\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache HTTP Server Monitor in YAML\nDESCRIPTION: Example YAML configuration for activating the Apache HTTP Server monitor in the Splunk OpenTelemetry Collector. Shows basic setup and additional options for host, port, and custom URL.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/apache-httpserver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/apache:\n    type: collectd/apache\n    ... # Additional config\n```\n\nLANGUAGE: yaml\nCODE:\n```\n    type: collectd/apache\n    host: localhost\n    port: 80\n    url: \"http://{{.Host}}:{{.Port}}/server-status?auto\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/apache]\n```\n\n----------------------------------------\n\nTITLE: Log Filtering Using Kubernetes Annotations\nDESCRIPTION: Bash commands demonstrating how to use annotations to control log ingestion and filtering at pod and namespace levels.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-logs.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# annotates a namespace\nkubectl annotate namespace <my-namespace> splunk.com/exclude=true\n\n# annotates a pod\nkubectl annotate pod -n <my-namespace> <my-pod> splunk.com/exclude=true\n```\n\n----------------------------------------\n\nTITLE: Viewing Available Command Line Options for Splunk OpenTelemetry Collector\nDESCRIPTION: This PowerShell command shows how to access the help documentation for the Splunk OpenTelemetry Collector, displaying all available command line options and their descriptions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/windows-config.rst#2025-04-22_snippet_4\n\nLANGUAGE: PowerShell\nCODE:\n```\n& 'C:\\Program Files\\Splunk\\OpenTelemetry Collector\\otelcol.exe' --help\n```\n\n----------------------------------------\n\nTITLE: Adding Attributes Processor to Pipeline Configuration in YAML\nDESCRIPTION: Configuration snippet demonstrating how to include the attributes processor in the processing pipeline of the collector to enable dimension removal.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/configure-remove.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n... \nservice:\n    pipelines:\n    traces:\n        receivers: ...\n        processors: [..., attributes/delete, ...] \n        ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Agent-to-Gateway Mode Configuration in YAML\nDESCRIPTION: Configuration settings for an agent that forwards data to a gateway, including the gateway URL specification in the configuration file located at /otelcol/config/ta-agent-to-gateway-config.yaml.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-addon/collector-addon-configure-instance.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nSplunk_config: ta-agent-to-gateway-config.yaml\nsplunk_gateway_url: \"<gateway-ip-address>\"\n```\n\n----------------------------------------\n\nTITLE: Multiple MySQL Databases Configuration in YAML\nDESCRIPTION: YAML configuration example showing how to connect to multiple MySQL databases using the Splunk OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/mysql.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/mysql:\n    type: collectd/mysql\n    host: 127.0.0.1\n    port: 3306\n    databases:\n      - name: <name>\n        username: <username>\n        password: <password>\n      - name: <name>\n        username: <username>\n        password: <password>\n```\n\n----------------------------------------\n\nTITLE: Setting Pod Name to Environment Variable\nDESCRIPTION: Sets a pod name to an environment variable for use in subsequent commands that need to reference a specific pod.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-container.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nPOD_NAME=splunk-otel-collector-agent-6llkr\n```\n\n----------------------------------------\n\nTITLE: Advanced PHP-FPM Monitor Configuration\nDESCRIPTION: This YAML configuration demonstrates advanced options for the PHP-FPM monitor. It includes settings for host, port, HTTPS usage, and a custom URL template for accessing the FPM status page.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/php-fpm.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/ collectd/php-fpm:\n    type: collectd/php-fpm\n    host: localhost\n    port: 80\n    useHTTPS: true # will be ignored\n    url: \"http://{{.host}}:{{.port}}/fpm-status?json\"    \n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Querying Movie Database with PostgreSQL\nDESCRIPTION: Complete configuration example for querying a movie database using PostgreSQL and generating metrics with the SQL Query receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sqlquery-receiver.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  sqlquery:\n    driver: postgres\n    datasource: \"host=localhost port=5432 user=postgres password=s3cr3t sslmode=disable\"\n    storage: file_storage\n    queries:\n      - sql: \"select count(*) as count, genre from movie group by genre\"\n        metrics:\n          - metric_name: movie.genres\n            value_column: \"count\"\n            attribute_columns: [\"genre\"]\n            static_attributes:\n              dbinstance: mydbinstance\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry .NET Module using PowerShell on Windows\nDESCRIPTION: PowerShell commands to download and install the Splunk Distribution of OpenTelemetry .NET module. This script downloads the module from GitHub and imports it into the current PowerShell session.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_3\n\nLANGUAGE: powershell\nCODE:\n```\n# Download and import the PowerShell module\n$module_url = \"https://github.com/signalfx/splunk-otel-dotnet/releases/latest/download/Splunk.OTel.DotNet.psm1\"\n$download_path = Join-Path $env:temp \"Splunk.OTel.DotNet.psm1\"\nInvoke-WebRequest -Uri $module_url -OutFile $download_path\nImport-Module $download_path\n\n# Install the Splunk distribution using the PowerShell module\nInstall-OpenTelemetryCore\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Direct Data Export on Windows\nDESCRIPTION: These PowerShell commands set environment variables for the Splunk access token and realm, enabling direct data export to Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_18\n\nLANGUAGE: powershell\nCODE:\n```\n$env:SPLUNK_ACCESS_TOKEN=<access_token>\n$env:SPLUNK_REALM=<realm>\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Pipeline Processors in YAML\nDESCRIPTION: Defines the basic structure of data processing pipelines for logs, metrics, and traces with various processors including memory limiter, Kubernetes attributes, filtering, and resource detection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filter-processor.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    logs:\n      processors:\n        - memory_limiter\n        - k8sattributes\n        - filter/logs\n        - batch\n        - resourcedetection\n        - resource\n        - resource/logs\n        - filter/exclude_logs_from_pod\n        - filter/exclude_logs_from_node\n```\n\n----------------------------------------\n\nTITLE: Retrieving Process Environment Variables in Linux Shell\nDESCRIPTION: This Linux shell command displays the environment variables for a process identified by its process ID (`<pid>`) by reading the `/proc/<pid>/environ` file. This helps verify the OpenTelemetry environment variables used by the instrumented .NET application during troubleshooting.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncat /proc/<pid>/environ # where <pid> is the process ID\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk On-Call Integration in Sensu JSON (JSON)\nDESCRIPTION: This JSON configuration snippet modifies the Sensu configuration to integrate with Splunk On-Call. Key elements include defining a handler for Splunk On-Call (\"victorops\"), a sample check (\"tmp_check\") routed through that handler, and providing the required API endpoint and routing key for Splunk On-Call service in the \"victorops\" section. To function, replace the \\u003cintegration_service_api_endpoint\\u003e with your actual service API endpoint and place the file in the etc/sensu/conf.d directory. The handler executes the Ruby script for triggering incidents in Splunk On-Call. All parameters, especially \"api_url\" and \"routing_key\", must be customized accordingly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/sensu-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"handlers\": {\n   \"victorops\": {\n      \"type\": \"pipe\",\n   \"command\": \"/etc/sensu/handlers/victorops.rb\"\n   }\n},\n\"checks\": {\n   \"tmp_check\": {\n      \"description\": \"check that /tmp exists \",\n      \"handler\": \"victorops\",\n      \"command\": \"ls /tmp\",\n      \"interval\": 30,\n      \"subscribers\": [ \"all_servers\" ]\n   }\n},\n\"victorops\" : {\n   \"api_url\": \"\\u003cintegration_service_api_endpoint\\u003e\",\n   \"routing_key\" : \"everyone\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: Sphinx/RST documentation structure defining the troubleshooting guide sections and navigation\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _otel-troubleshooting:\n\n*************************************************************************\nTroubleshoot the Collector\n*************************************************************************\n\n.. meta::\n      :description: Describes known issues when collecting telemetry data using the OpenTelemetry Collector and the Splunk Distribution of OpenTelemetry Collector.\n\n.. toctree::\n    :maxdepth: 4\n    :titlesonly:\n    :hidden:\n\n    General troubleshooting <splunk-collector-troubleshooting.rst>\n    Troubleshoot log collection <troubleshoot-logs.rst>\n    Support checklist <support-checklist.rst>\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Service in Docker Compose\nDESCRIPTION: Defines the OpenTelemetry Collector service that processes logs from the logging services. Includes volume mounts for configuration and log files, port forwarding, and dependency on Splunk service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/docker-compose.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\notelcollector:\n  image:  quay.io/signalfx/splunk-otel-collector:latest\n  container_name: otelcollector\n  command: [\"--config=/etc/otel-collector-config.yml\"]\n  volumes:\n    - ./otel-collector-config.yml:/etc/otel-collector-config.yml\n    - ./output1:/output1\n    - ./output2:/output2\n  depends_on:\n    splunk:\n      condition: service_healthy\n  ports:\n    - 18088:8088\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Attributes (Shell)\nDESCRIPTION: Optionally sets the `OTEL_RESOURCE_ATTRIBUTES` environment variable to enrich telemetry data with additional metadata, such as the service version and deployment environment. These attributes help in filtering and analyzing data in Splunk Observability Cloud. Replace `<version>` and `<environment>` with appropriate values. Examples provided for both Linux (bash) and Windows PowerShell.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/instrument-go-application.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_RESOURCE_ATTRIBUTES=\"service.version=<version>,deployment.environment=<environment>\"\n```\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_RESOURCE_ATTRIBUTES=\"service.version=<version>,deployment.environment=<environment>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Pipeline with PostgreSQL Receiver\nDESCRIPTION: This snippet demonstrates how to incorporate the PostgreSQL receiver into the 'metrics' pipeline of the service configuration section. The receivers section must include 'postgresql' to enable metric collection from PostgreSQL databases.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/postgresql-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n   service:\\n     pipelines:\\n       metrics:\\n         receivers: [postgresql]\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Processors in OpenTelemetry Collector\nDESCRIPTION: Defines processors for log transformation including batch processing, attribute transformation for index assignment, and grouping by attributes for organized log handling.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/collector-splunk.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  batch:\n  transform:\n    log_statements:\n      - context: log\n        statements:\n          - set(attributes[\"com.splunk.index\"], \"index1\")\n          - set(attributes[\"com.splunk.index\"], \"index2\") where ParseJSON(body)[\"message\"] == \"logging2\"\n  groupbyattrs:\n    keys:\n      - com.splunk.index\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct Data Export to Splunk Observability Cloud in Windows PowerShell\nDESCRIPTION: PowerShell commands that set environment variables required to send telemetry data directly to Splunk Observability Cloud without using a local collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/instrument-ruby-application.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n$env:SPLUNK_ACCESS_TOKEN=<access_token>\n$env:SPLUNK_REALM=<realm>\n```\n\n----------------------------------------\n\nTITLE: Setting OpenTelemetry Resource Attributes in Kubernetes YAML\nDESCRIPTION: Defines environment variables `OTEL_SERVICE_NAME` and `OTEL_RESOURCE_ATTRIBUTES` within a Kubernetes deployment configuration (implied). These variables set the application's service name and deployment environment attribute for OpenTelemetry tracing and metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/instrument-java-application.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n                  - name: OTEL_SERVICE_NAME\n                  value: \"<serviceName>\"\n                  - name: OTEL_RESOURCE_ATTRIBUTES\n                  value: \"deployment.environment=<environmentName>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Java OpenTelemetry in AWS Fargate\nDESCRIPTION: This snippet defines the required environment variables for Java OpenTelemetry instrumentation in an AWS Fargate task. It sets the service name, resource attributes, and configures the Java agent.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-fargate-java.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n\"environment\": [\n    {\n        \"name\": \"OTEL_SERVICE_NAME\",\n        \"value\": \"myservice\"\n    },\n    {\n        \"name\": \"OTEL_RESOURCE_ATTRIBUTES\",\n        \"value\": \"deployment.environment=test,service.version=1.0\"\n    },\n    {\n        \"name\": \"JAVA_TOOL_OPTIONS\",\n        \"value\": \"-javaagent:/opt/splunk/splunk-otel-javaagent.jar\"\n    }\n],\n```\n\n----------------------------------------\n\nTITLE: Extracting Trace Metadata with OpenTelemetry in Go\nDESCRIPTION: This Go code snippet demonstrates how to use the OpenTelemetry Trace API to extract trace metadata from a context. It retrieves a SpanContext that contains IDs and trace flags, which can then be used to annotate logs. The function checks if the extracted SpanContext is valid before proceeding.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nspanContext := trace.SpanContextFromContext(ctx)\nif !spanContext.IsValid() {\n\t// ctx does not contain a valid span.\n\t// There is no trace metadata to add.\n\treturn\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Zero-Code Instrumentation with Advanced Profiling\nDESCRIPTION: Installation command that enables advanced features including CPU profiling, memory profiling, and metrics collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-instrumentation --deployment-environment prod \\\n--realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN> \\\n--enable-profiler --enable-profiler-memory --enable-metrics\n```\n\n----------------------------------------\n\nTITLE: Recommended Istio Tracing Configuration for Full-Fidelity Data (YAML)\nDESCRIPTION: This YAML snippet reiterates the IstioOperator configuration, specifically emphasizing the recommended settings for `sampling` (100) and `max_path_tag_length` (99999). Setting sampling to 100 ensures all traces are captured with correct root spans, and a large max path tag length prevents truncation of important tags, optimizing data for full-fidelity retention in Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/istio/istio.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# tracing.yaml\n\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nmetadata:\n  name: istio-operator\nspec:\n  meshConfig:\n     # Requires Splunk Log Observer\n     accessLogFile: /dev/stdout\n     # Requires Splunk APM\n     enableTracing: true\n     defaultConfig:\n        tracing:\n           max_path_tag_length: 99999\n           sampling: 100\n           zipkin:\n              address: $(HOST_IP):9411\n           custom_tags:\n              environment.deployment:\n                 literal:\n                    value: dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Filelog Receiver for Plaintext Log Files\nDESCRIPTION: YAML configuration example for tailing and parsing plaintext log files using the Filelog receiver. Includes regex parsing for timestamp and severity extraction.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filelog-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include: [ /simple.log ]\n    operators:\n      - type: regex_parser\n        regex: '^(?P<time>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) (?P<sev>[A-Z]*) (?P<msg>.*)$'\n        timestamp:\n          parse_from: attributes.time\n          layout: '%Y-%m-%d %H:%M:%S'\n        severity:\n          parse_from: attributes.sev\n```\n\n----------------------------------------\n\nTITLE: Enabling OracleDB Receiver in the Metrics Pipeline of OTel Collector (YAML)\nDESCRIPTION: This YAML code illustrates how to include an oracledb receiver in the metrics pipeline within the Collector's service section. All receivers referenced in the metrics pipeline must be defined elsewhere in the configuration. Required for integrating metric collection from Oracle Database into the full OTel Collector pipeline workflow.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/oracledb-receiver.rst#2025-04-22_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nservice:\\n  pipelines:\\n    metrics:\\n      receivers:\\n        - oracledb\n```\n\n----------------------------------------\n\nTITLE: Viewing Splunk OpenTelemetry Collector Service Logs on Linux\nDESCRIPTION: This command allows viewing of the splunk-otel-collector service logs and errors in the systemd journal. It uses journalctl to access the logs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-config.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo journalctl -u splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Deploying OpenTelemetry Collector with Kubernetes Objects Receiver\nDESCRIPTION: YAML configuration for deploying the OpenTelemetry Collector with the Kubernetes Objects receiver as a single replica in Kubernetes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-objects-receiver.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: otelcontribcol\n  labels:\n     app: otelcontribcol\n\nspec:\n   replicas: 1\n   selector:\n      matchLabels:\n      app: otelcontribcol\ntemplate:\n   metadata:\n      labels:\n         app: otelcontribcol\n   spec:\n      serviceAccountName: otelcontribcol\n      containers:\n      - name: otelcontribcol\n        image: otelcontribcol:latest # specify image\n        args: [\"--config\", \"/etc/config/config.yaml\"]\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n        imagePullPolicy: IfNotPresent\n      volumes:\n        - name: config\n          configMap:\n            name: otelcontribcol\n```\n\n----------------------------------------\n\nTITLE: Configuring Batching by Metadata in YAML\nDESCRIPTION: This snippet demonstrates how to configure batching based on metadata, specifically by tenant ID, with a limit of 10 batcher processes before raising errors.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/batch-processor.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  batch:\n    # batch data by tenant-id\n    metadata_keys:\n    - tenant_id\n    \n    # limit to 10 batcher processes before raising errors\n    metadata_cardinality_limit: 10\n```\n\n----------------------------------------\n\nTITLE: Log Inclusion Annotation Commands\nDESCRIPTION: Bash commands for including specific pods and namespaces in log collection using Kubernetes annotations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-logs.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# annotates a namespace\nkubectl annotate namespace <my-namespace> splunk.com/include=true\n\n# annotates a pod\nkubectl annotate pod -n <my-namespace> <my-pod> splunk.com/include=true\n```\n\n----------------------------------------\n\nTITLE: Instrumenting Isolated Worker Process Function\nDESCRIPTION: Demonstrates manual instrumentation of an Azure Function using isolated worker process model, including helper functions for starting and finishing spans with proper attributes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/azure/instrument-azure-functions-dotnet.rst#2025-04-22_snippet_3\n\nLANGUAGE: csharp\nCODE:\n```\nusing System.Diagnostics;\nusing System.Net;\nusing Microsoft.Azure.Functions.Worker;\nusing Microsoft.Azure.Functions.Worker.Http;\nusing Microsoft.Extensions.Logging;\n\nnamespace OtelManualIsolatedExample\n{\n   public class ExampleFunction\n   {\n      private readonly ILogger _logger;\n\n      public ExampleFunction(ILoggerFactory loggerFactory)\n      {\n            _logger = loggerFactory.CreateLogger<ExampleFunction>();\n      }\n      public static ActivitySource ManualInstrumentationSource = new ActivitySource(\"manualInstrumentation\");\n      public static Activity? StartActivity(HttpRequestData req, FunctionContext fc)\n      {\n            var answer = ManualInstrumentationSource.StartActivity(req.Method.ToUpper() + \" \" + req.Url.AbsolutePath, ActivityKind.Server);\n            answer?.AddTag(\"http.url\", req.Url);\n            answer?.AddTag(\"faas.invocation_id\", fc.InvocationId.ToString());\n            answer?.AddTag(\"faas.name\", Environment.GetEnvironmentVariable(\"WEBSITE_SITE_NAME\") + \"/\" + fc.FunctionDefinition.Name);\n            return answer;\n      }\n      public static HttpResponseData FinishActivity(HttpResponseData response, Activity? activity)\n      {\n            activity?.AddTag(\"http.status_code\", ((int)response.StatusCode));\n            return response;\n      }\n\n      [Function(\"ExampleFunction\")]\n      public HttpResponseData Run([HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\")] HttpRequestData req, FunctionContext fc)\n      {\n            using (var activity = StartActivity(req, fc))\n            {\n               var response = req.CreateResponse(HttpStatusCode.OK);\n               response.Headers.Add(\"Content-Type\", \"text/plain; charset=utf-8\");\n\n               response.WriteString(\"The current time is \" + DateTime.Now.ToLongTimeString());\n\n               return FinishActivity(response, activity);\n            }\n      }\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring JMX Remote Connection Properties in Java\nDESCRIPTION: Java command-line configuration for enabling JMX remote monitoring. Sets the JMX port to 5000, disables authentication and SSL, and explicitly configures the RMI port to match the JMX port for firewall compatibility.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/genericjmx.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\njava \\\n  -Dcom.sun.management.jmxremote.port=5000 \\\n  -Dcom.sun.management.jmxremote.authenticate=false \\\n  -Dcom.sun.management.jmxremote.ssl=false \\\n  -Dcom.sun.management.jmxremote.rmi.port=5000 \\\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration example showing how to activate the Kafka integration by adding the smartagent/kafka receiver to the OpenTelemetry Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-messaging/apache-kafka.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/kafka:\n    type: collectd/kafka\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Average CPU Usage Aggregation in Elasticsearch\nDESCRIPTION: This JSON snippet shows how to configure an average CPU usage aggregation in Elasticsearch using the 'terms' bucket aggregation and 'avg' metric aggregation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/elasticsearch-query.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"aggs\":{\n    \"host\" : {\n    \"terms\":{\"field\" : \"host\"},\n    \"aggs\": {\n        \"average_cpu_usage\": {\n        \"avg\": {\n            \"field\": \"cpu_utilization\"\n        }\n        }\n    }\n    }\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Updated path for operator auto-instrumentation configuration\nDESCRIPTION: The new simplified YAML configuration path for operator auto-instrumentation after version 0.108.0.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ninstrumentation:\n  endpoint: XXX\n  ...\n```\n\n----------------------------------------\n\nTITLE: Replacing Java Agent in Startup Script\nDESCRIPTION: The snippet shows how to update the application startup script to replace the SignalFx Java agent with the Splunk OTel Java agent. No dependencies are required other than having the Java agent accessible at the specified path. Ensure you use the correct path to the Splunk OTel Java agent jar.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/migrate-signalfx-java-agent-to-otel.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-javaagent:/path/to/splunk-otel-javaagent.jar\n```\n\n----------------------------------------\n\nTITLE: Creating a MongoDB User with Read-Only Roles\nDESCRIPTION: Example showing how to create a MongoDB user with minimal read-only roles (readAnyDatabase and clusterMonitor) for monitoring a secured MongoDB deployment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/mongodb.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ndb.createUser( {\n  user: \"<username>\",\n  pwd: \"<password>\",\n  roles: [ { role: \"readAnyDatabase\", db: \"admin\" }, { role: \"clusterMonitor\", db: \"admin\" } ]\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Telemetry Data Collection\nDESCRIPTION: Configuration for enabling/disabling different types of telemetry data collection for Splunk Observability Cloud and Splunk Platform.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-add.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkObservability:\n  metricsEnabled: true\n  tracesEnabled: true\n  logsEnabled: true\nsplunkPlatform:\n  metricsEnabled: true\n  logsEnabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom OPcache Path in OpenTelemetry Collector\nDESCRIPTION: YAML configuration example showing how to specify a custom path for the OPcache status script using the 'path' option.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cache/opcache.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmonitors:\n - type: collectd/opcache\n   host: localhost\n   port: 80\n   path: \"/opcache\"\n```\n\n----------------------------------------\n\nTITLE: Setting Custom OTLP Endpoint for SDKs (Shell)\nDESCRIPTION: Specifies the OTLP endpoint (host:port) for traces, logs, and metrics captured by activated SDKs. Sets the 'OTEL_EXPORTER_OTLP_ENDPOINT' environment variable. If omitted, each SDK uses its default endpoint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\n--otlp-endpoint <host:port>\n```\n\n----------------------------------------\n\nTITLE: Including Excluded Metrics in SignalFx Exporter (YAML)\nDESCRIPTION: Instructs the SignalFx exporter to include specific metrics that are excluded by default. Lists metric names under include_metrics. Needs to be placed in the exporters section of the OpenTelemetry Collector config. Useful for organizations requiring visibility into additional container memory metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubelet-stats-receiver.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  signalfx:\n    include_metrics:\n      - metric_names:\n          - container.memory.rss.bytes\n          - container.memory.available.bytes  \n\n```\n\n----------------------------------------\n\nTITLE: Enabling Go OpenTelemetry Debug Logging via Environment Variable (Shell)\nDESCRIPTION: Sets the `OTEL_LOG_LEVEL` environment variable to `debug` to increase the verbosity of the Go instrumentation logs. This is useful for troubleshooting issues with span generation or export. Remember to unset this variable after troubleshooting to avoid excessive logging.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/common-go-troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_LOG_LEVEL=\"debug\"\n```\n\n----------------------------------------\n\nTITLE: Activating Filelog Receiver in YAML Configuration\nDESCRIPTION: Basic YAML configuration to activate the Filelog receiver in the OpenTelemetry Collector. Includes both the receiver definition and service pipeline configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filelog-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n\nservice:\n  pipelines:\n    logs:\n      receivers: [filelog]\n```\n\n----------------------------------------\n\nTITLE: Timestamp-based Muting with RegEx in Splunk On-Call Rules Engine\nDESCRIPTION: This set of rules mutes alerts for a specific routing key on certain days of the week using timestamp-based matching with RegEx.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-transformations.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nRule 1:\nWhen routing_key matches teamA using Wildcard\nSet teamA to a new value ${{Alert_received_week_time_utc}}\n\nRule 2:\nWhen teamA matches \\d*-W\\d*-[467].* using RegEx\nSet message_type to new value INFO\n```\n\n----------------------------------------\n\nTITLE: Adding Syslog Receiver to Logs Pipeline in OpenTelemetry Collector (YAML)\nDESCRIPTION: Shows how to incorporate the syslog receiver into the logs pipeline within the 'service' section of the configuration file. The 'receivers' key lists the 'syslog' receiver as the data source for log ingestion. This snippet is required in addition to the receiver declaration and ensures that incoming syslog data is routed through the logging pipeline. It assumes the syslog receiver is already enabled under the 'receivers' section. Outputs are log entries received via the specified syslog protocol.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/syslog-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    logs:\n      receivers: [syslog]\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk Platform EKS Add-on with Secure Token Handling\nDESCRIPTION: YAML configuration for Splunk Platform setup with the EKS Add-on, using a separate secret for token management. This specifies the HEC endpoint, cluster name, and cloud provider settings while setting up secure token handling.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s-addon-eks.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkPlatform:\n    endpoint: http://localhost:8088/services/collector\nclusterName: <EKS_CLUSTER_NAME>\ncloudProvider: aws\ndistribution: eks\n\nsecret:\n    create: false\n    name: splunk-otel-collector\n    validateSecret: false\n```\n\n----------------------------------------\n\nTITLE: .NET Application Deployment YAML With OpenTelemetry Instrumentation (linux-x64)\nDESCRIPTION: Kubernetes deployment manifest for a .NET application running on linux-x64 with OpenTelemetry auto-instrumentation enabled through appropriate annotations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-dotnet-app\n  namespace: monitoring\nspec:\n  template:\n    metadata:\n      annotations:\n        instrumentation.opentelemetry.io/otel-dotnet-auto-runtime: \"linux-x64\"\n        instrumentation.opentelemetry.io/inject-dotnet: \"monitoring/splunk-otel-collector\"\n      spec:\n        containers:\n        - name: my-dotnet-app\n          image: my-dotnet-app:latest\n```\n\n----------------------------------------\n\nTITLE: Including Probabilistic Sampler in OpenTelemetry Collector Pipeline\nDESCRIPTION: This YAML configuration demonstrates how to include the probabilistic sampler processor in the logs pipeline of the OpenTelemetry Collector's service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/probabilistic-sampler-processor.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    logs:\n      processors: [probabilistic_sampler]\n```\n\n----------------------------------------\n\nTITLE: Basic BOSH Deployment Configuration\nDESCRIPTION: Basic YAML configuration for deploying the OpenTelemetry Collector with BOSH, including release version, stemcells, and instance group settings\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-pivotal-cloudfoundry.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: splunk-otel-collector\n\nreleases:\n  - name: splunk-otel-collector\n    version: latest\n\nstemcells:\n  - alias: default\n    os: ubuntu-bionic\n    version: latest\n\nupdate:\n  canaries: 1\n  max_in_flight: 1\n  canary_watch_time: 1000-30000\n  update_watch_time: 1000-30000\n\ninstance_groups:\n  - name: splunk-otel-collector\n    instances: 1\n    azs: [z1, z2]\n    jobs:\n      - name: splunk-otel-collector\n        release: splunk-otel-collector\n        properties:\n          cloudfoundry:\n            rlp_gateway:\n              endpoint: \"https://log-stream.sys.<TAS environment name>.cf-app.com\"\n              shard_id: \"otelcol\"\n              tls:\n                insecure_skip_verify: false\n            uaa:\n              endpoint: \"https://uaa.sys.<TAS environment name>.cf-app.com\"\n              username: \"...\"\n              password: \"...\"\n              tls:\n                insecure_skip_verify: false\n          splunk:\n            access_token: \"...\"\n            realm: \"...\"\n    vm_type: default\n    stemcell: default\n    networks:\n      - name: default\n```\n\n----------------------------------------\n\nTITLE: Creating Server-Timing Header for Splunk APM Integration\nDESCRIPTION: Shows an example of a manually created Server-Timing header for integrating React Native RUM with Splunk APM. The header includes trace context information following a specific format.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/manual-rum-react-instrumentation.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nServer-Timing: traceparent;desc=\"00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01\"\n```\n\n----------------------------------------\n\nTITLE: ARN References for Splunk OpenTelemetry Lambda Layer (x86_64)\nDESCRIPTION: GitHub reference to the ARN values needed for adding the Splunk OpenTelemetry Lambda layer to AWS Lambda functions running on x86_64 architecture.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/aws/otel-lambda-layer/instrument-lambda-functions.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n.. github:: yaml\n            :url: https://raw.githubusercontent.com/signalfx/lambda-layer-versions/main/splunk-apm/splunk-apm.md\n```\n\n----------------------------------------\n\nTITLE: Adding MySQL Monitor to Metrics Pipeline\nDESCRIPTION: YAML configuration snippet to add the MySQL monitor to the metrics pipeline in the Splunk OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/mysql.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/mysql]\n```\n\n----------------------------------------\n\nTITLE: Configuring NGINX Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration to activate the NGINX integration in the Collector configuration file. Specifies the receiver type and required parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/nginx.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/nginx:\n    type: collectd/nginx\n    host: <host>\n    port: <port>\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Metrics Collection with Splunk OpenTelemetry JS (JavaScript)\nDESCRIPTION: This code snippet details how to enable internal debug metrics in a Node.js application using the Splunk Distribution of OpenTelemetry JS. It passes 'debugMetricsEnabled: true' within the metrics configuration when calling 'start', allowing collection of debug-specific runtime profiler metrics. Requires '@splunk/otel' as a dependency and 'serviceName' configuration. Outputs include debug metrics as described for troubleshooting and support usage.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/nodejs-otel-metrics.rst#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require('@splunk/otel');\n\nstart({\n   serviceName: 'my-service',\n   metrics: {\n     debugMetricsEnabled: true,\n   }\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Load Balancing Exporter with Kubernetes Resolver in YAML\nDESCRIPTION: This YAML configuration shows how to set up the load balancing exporter using a Kubernetes resolver. It includes receiver, exporter, and service pipeline configurations for traces and logs, with specific Kubernetes service and port settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/loadbalancing-exporter.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n  grpc:\n    endpoint: localhost:4317\n\nprocessors:\n\nexporters:\n  loadbalancing:\n    routing_key: \"service\"\n    protocol:\n    otlp:\n        # all options from the OTLP exporter are supported\n        # except the endpoint\n        timeout: 1s\n    resolver:\n    # use k8s service resolver, if collector runs in kubernetes environment\n    k8s:\n        service: lb-svc.kube-public\n        ports:\n        - 15317\n        - 16317\n\nservice:\n  pipelines:\n    traces:\n    receivers:\n        - otlp\n    processors: []\n    exporters:\n        - loadbalancing\n    logs:\n    receivers:\n        - otlp\n    processors: []\n    exporters:\n        - loadbalancing\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx Exporter in Gateway Mode\nDESCRIPTION: Example configuration for setting up a SignalFx exporter in gateway mode with metrics pipeline. Demonstrates how to configure access tokens, API URLs, and pipeline settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/splunk-collector-troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  signalfx:\n      access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n      api_url: \"http://${SPLUNK_GATEWAY_URL}:6060\"\n      ingest_url: \"http://${SPLUNK_GATEWAY_URL}:9943\"\n      sync_host_metadata: true\n      correlation:\n  # Other exporters\n\nservice:\n  extensions: [health_check, http_forwarder, zpages]\n  pipelines:\n      metrics/internal:\n            receivers: [prometheus/internal]\n            processors: [memory_limiter, batch, resourcedetection]\n            exporters: [signalfx]\n      # Other pipelines\n```\n\n----------------------------------------\n\nTITLE: Running Splunk OpenTelemetry Collector Docker Container for Auto Discovery\nDESCRIPTION: This bash command demonstrates how to run the Splunk OpenTelemetry Collector Docker container with access to the Docker socket for auto-discovery of other containers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v /var/run/docker.sock:/var/run/docker.sock:ro --group-add <socket_gid>\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQ Monitor for Agent\nDESCRIPTION: Configuration for adding RabbitMQ monitoring to the Collector agent DaemonSet, including receiver setup and pipeline configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-add.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  config:\n    receivers:\n      smartagent/rabbitmq:\n        type: collectd/rabbitmq\n        host: localhost\n        port: 5672\n        username: otel\n        password: ${env:RABBITMQ_PASSWORD}\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers:\n        - smartagent/rabbitmq\n```\n\n----------------------------------------\n\nTITLE: Setting Explicit Trace Endpoint URL (Shell)\nDESCRIPTION: Overrides the default trace endpoint URL inferred from the specified realm. Allows setting a custom URL for sending trace data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n--trace-url <url>\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://ingest.REALM.signalfx.com/v2/trace\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry SDK to Use All Instrumentation Libraries in Ruby\nDESCRIPTION: This Ruby code configures the OpenTelemetry SDK to use all available instrumentation libraries. It requires the necessary modules and sets up the configuration block.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/ruby-manual-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"opentelemetry/sdk\"\nrequire \"opentelemetry/instrumentation/all\"\nOpenTelemetry::SDK.configure do |c|\n   c.use_all()\nend\n```\n\n----------------------------------------\n\nTITLE: Configuring Span Processor in YAML\nDESCRIPTION: Basic configuration structure for activating the span processor in the OpenTelemetry Collector configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/span-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nresource:\n  span:\n    name:\n    status:\n```\n\n----------------------------------------\n\nTITLE: Adding User Identification Metadata During Initialization\nDESCRIPTION: This code demonstrates how to add user identification metadata as global attributes when initializing the Splunk RUM agent. It uses OpenTelemetry specification attributes like enduser.id to link traces to specific users.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: swift\nCODE:\n```\nimport SplunkOtel\n//...\nSplunkRumBuilder(realm: \"<realm>\", rumAuth: \"<rum-token>\")\n   .globalAttributes(globalAttributes: [\"enduser.id\": \"user-id-123456\"])\n   .build()\n```\n\n----------------------------------------\n\nTITLE: Configuring Nagios Main Configuration File (nagios.cfg)\nDESCRIPTION: Shows required modifications within the main Nagios configuration file (`nagios.cfg` or `icinga.cfg`). First, ensure Nagios environment macros are enabled by setting `enable_environment_macros=1`. Second, add a `cfg_file` directive pointing to the location of the Splunk On-Call specific configuration file (`victorops.cfg`), adjusting the path if necessary. These changes are crucial for the plugin to function correctly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_6\n\nLANGUAGE: nagios-config\nCODE:\n```\nenable_environment_macros=1\n```\n\nLANGUAGE: nagios-config\nCODE:\n```\ncfg_file=/usr/local/nagios/etc/victorops.cfg\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Collector Logs\nDESCRIPTION: kubectl command to view logs from the Splunk OpenTelemetry Collector agent pod.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs {splunk-otel-collector-agent-pod}\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Kubernetes Attributes Processor in YAML\nDESCRIPTION: Basic configuration for the Kubernetes attributes processor in the Splunk Distribution of OpenTelemetry Collector for Kubernetes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-attributes-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  k8sattributes:\n```\n\n----------------------------------------\n\nTITLE: Skipping Default Fluentd Repository Configuration (Shell)\nDESCRIPTION: Prevents the installer from creating the default apt/yum repository definition file pointing to 'https://packages.treasuredata.com' for Fluentd ('td-agent'). Use this if a pre-configured repository providing the 'td-agent' package already exists.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\n--skip-fluentd-repo\n```\n\n----------------------------------------\n\nTITLE: Configuring Filelog Receiver for JSON Log Files\nDESCRIPTION: YAML configuration example for tailing and parsing JSON log files using the Filelog receiver. Includes file path matching and JSON parsing with timestamp extraction.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filelog-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include: [ /var/log/myservice/*.json ]\n    operators:\n      - type: json_parser\n        timestamp:\n          parse_from: attributes.time\n          layout: '%Y-%m-%d %H:%M:%S'\n```\n\n----------------------------------------\n\nTITLE: vSphere Monitor Pipeline Configuration in YAML\nDESCRIPTION: Configuration example showing how to add the vSphere monitor to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/vsphere.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/vsphere]\n```\n\n----------------------------------------\n\nTITLE: Installing the Collector with Downloaded RPM Package (dnf)\nDESCRIPTION: Commands to install the required libcap dependency and install the Splunk OTel Collector using a locally downloaded RPM package with dnf.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndnf install -y libcap  # Required for enabling cap_dac_read_search and cap_sys_ptrace capabilities on the Collector\nrpm -ivh <path to splunk-otel-collector rpm>\n```\n\n----------------------------------------\n\nTITLE: Configuring Zipkin Receiver in OpenTelemetry Collector (YAML)\nDESCRIPTION: This YAML configuration snippet demonstrates setting up the Zipkin receiver in the Splunk Distribution of the OpenTelemetry Collector. It configures the collector to listen on the specified endpoint (0.0.0.0:9411 by default) for spans sent in Zipkin v1 or v2 JSON format.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-spans-traces/span-formats.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# To receive spans in Zipkin format\n\nreceivers:\n   zipkin:\n      endpoint: 0.0.0.0:9411\n```\n\n----------------------------------------\n\nTITLE: Activating Specific Metrics in Host Metrics Receiver\nDESCRIPTION: This example demonstrates how to enable a specific CPU utilization metric in the host metrics receiver configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/activate-deactivate-native-metrics.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  hostmetrics:\n    scrapers:\n      process:\n        metrics:\n          process.cpu.utilization:\n            enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Pipelines for MongoDB Atlas Receiver in YAML\nDESCRIPTION: This snippet shows how to include the MongoDB Atlas receiver in the metrics and logs pipelines of the service section in the YAML configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/mongodb-atlas-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [mongodbatlas]\n   logs:\n      receivers: [mongodbatlas]\n```\n\n----------------------------------------\n\nTITLE: Setting Trace Propagator to b3multi (Linux Shell)\nDESCRIPTION: Configures the trace context propagator to 'b3multi' using an environment variable in a Linux shell. This is recommended for backward compatibility with older versions of the Splunk Distribution of OpenTelemetry Java or the SignalFx Java Agent.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/configuration/advanced-java-otel-configuration.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_PROPAGATORS=b3multi\n```\n\n----------------------------------------\n\nTITLE: Activating Debug Metrics Collection - Splunk OTel JS - JavaScript\nDESCRIPTION: Provides an example for enabling debug metrics collection through the Splunk OpenTelemetry JavaScript distribution. Requires the '@splunk/otel' package. The 'debugMetricsEnabled' property is set in the 'metrics' configuration object passed to the 'start' method. This mode is used primarily for internal debugging and troubleshooting with Splunk support.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/configuration/nodejs-otel-metrics.rst#2025-04-22_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst { start } = require('@splunk/otel');\\n\\nstart({\\n   serviceName: 'my-service',\\n   metrics: {\\n     debugMetricsEnabled: true,\\n   }\\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring UDP Log Receiver in OpenTelemetry Collector (YAML)\nDESCRIPTION: This YAML snippet shows how to add the udplog receiver to the receivers section in the agent_config.yaml file. It specifies the listen_address parameter, which designates the network address and UDP port where logs will be collected. This configuration requires the Splunk OpenTelemetry Collector to be installed and expects log traffic on UDP port 54525. The key \"listen_address\" must be populated with the appropriate bind address and port as needed for deployment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/udp-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  udplog:\n    listen_address: \"0.0.0.0:54525\"\n```\n\n----------------------------------------\n\nTITLE: Base64 Encoding Authentication String (JavaScript)\nDESCRIPTION: JavaScript command to encode username and password combination in base64 format for API authentication.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/auth.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nbtoa(\"myusername:mypassword\")\n```\n\n----------------------------------------\n\nTITLE: Adding .NET Monitor to Metrics Pipeline\nDESCRIPTION: Configuration to add the .NET monitor to the metrics pipeline in the collector's service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/microsoft-dotnet.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n pipelines:\n   metrics:\n     receivers: [smartagent/dotnet]\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Host Metrics Receivers in YAML\nDESCRIPTION: YAML configuration demonstrating how to set up multiple host metrics receivers with different collection intervals.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/host-metrics-receiver.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  hostmetrics:\n    collection_interval: 30s\n    scrapers:\n      cpu:\n      memory:\n\n  hostmetrics/disk:\n    collection_interval: 1m\n    scrapers:\n      disk:\n      filesystem:\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [hostmetrics, hostmetrics/disk]\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Receiver for Kong in OpenTelemetry Collector\nDESCRIPTION: This code snippet shows how to configure the Prometheus receiver in the OpenTelemetry Collector to scrape metrics from Kong Gateway's metrics endpoint. It sets up a job named 'kong' with a 10-second scrape interval targeting the Kong admin API port 8001.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/kong.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprometheus/kong:\n   config:\n      scrape_configs:\n         - job_name: 'kong'\n         scrape_interval: 10s\n         static_configs:\n            - targets: ['0.0.0.0:8001']\n```\n\n----------------------------------------\n\nTITLE: Skipping Default Collector Repository Configuration (Shell)\nDESCRIPTION: Prevents the installer from creating the default apt, yum, or zypper repository definition file pointing to 'https://splunk.jfrog.io'. Use this if a pre-configured repository providing the 'splunk-otel-collector' package already exists on the target system.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n--skip-collector-repo\n```\n\n----------------------------------------\n\nTITLE: Basic OpenTelemetry Collector Configuration for Consul Integration\nDESCRIPTION: YAML configuration to activate the Consul datastore monitor in the OpenTelemetry Collector. This defines the receiver and adds it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/consul.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/consul:\n    type: collectd/consul\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Toggling Zero-Code Instrumentation via systemd (Shell)\nDESCRIPTION: Controls whether to install the 'splunk-otel-auto-instrumentation' package and configure zero-code instrumentation using a systemd drop-in file. Use '--with-systemd-instrumentation' to enable or '--without-systemd-instrumentation' to disable (default). This method applies instrumentation only to applications running as systemd services. Cannot be used with '--with-instrumentation'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n--with[out]-systemd-instrumentation\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n--without-systemd-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector for Both Destinations in Bash\nDESCRIPTION: Helm command to install the Splunk OpenTelemetry Collector configured to send data to both Splunk Observability Cloud and Splunk Platform, specifying parameters for both destinations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install my-splunk-otel-collector --set=\"splunkPlatform.endpoint=https://127.0.0.1:8088/services/collector,splunkPlatform.token=xxxxxx,splunkPlatform.metricsIndex=k8s-metrics,splunkPlatform.index=main,splunkObservability.realm=us0,splunkObservability.accessToken=xxxxxx,clusterName=my-cluster\" splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Adding and Installing OpenTelemetry eBPF Helm Chart\nDESCRIPTION: This shell script snippet demonstrates how to add the OpenTelemetry repository and install the OpenTelemetry eBPF Helm chart using specific values from the `otel-ebpf-values.yaml` file. It assumes that the helm tool is correctly set up and configured.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts\nhelm repo update open-telemetry\nhelm install my-opentelemetry-ebpf -f ./otel-ebpf-values.yaml open-telemetry/opentelemetry-ebpf\n```\n\n----------------------------------------\n\nTITLE: Collecting Additional Volume Metadata in Kubelet Stats Receiver (YAML)\nDESCRIPTION: Configures the kubeletstats receiver to collect additional volume-related metadata (such as k8s.volume.type) by specifying extra_metadata_labels and a k8s_api_config section. The receiver uses service account authentication and enriches collected metrics with storage resource info when processing persistent volumes. Useful for detailed storage monitoring in complex Kubernetes environments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubelet-stats-receiver.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 10s\n    auth_type: \"serviceAccount\"\n    endpoint: \"${K8S_NODE_NAME}:10250\"\n    insecure_skip_verify: true\n    extra_metadata_labels:\n      - k8s.volume.type\n    k8s_api_config:\n      auth_type: serviceAccount\n\n```\n\n----------------------------------------\n\nTITLE: Reviewing Logging Exporter Output using journalctl (Bash)\nDESCRIPTION: This bash command enables users to stream the output logs generated by the logging exporter component of the Splunk OpenTelemetry Collector. It leverages journalctl with the -u (unit) and -f (follow) flags to continuously tail the service logs. This command requires systemd and administrative privilege to access the collector's logs; the output is real-time console log data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/logging-exporter.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\njournalctl -u splunk-otel-collector.service -f\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Observer and Receiver Creator in YAML\nDESCRIPTION: This example shows how to set up the Docker observer extension and use it with the Receiver Creator receiver to monitor nginx containers. It includes Docker API settings and a rule for creating nginx receivers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/receiver-creator-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  docker_observer:\n    endpoint: path/to/docker.sock\n    excluded_images: ['redis', 'another_image_name']\n    api_version: 1.42\n    timeout: 15s\n\nreceivers:\n  receiver_creator:\n    watch_observers: [docker_observer]\n    receivers:\n      nginx:\n        rule: type == \"container\" and name matches \"nginx\" and port == 80\n        config:\n          endpoint: '`endpoint`/status'\n          collection_interval: 10s\n```\n\n----------------------------------------\n\nTITLE: Deploying Splunk OpenTelemetry Collector with Basic Configuration in Puppet\nDESCRIPTION: Basic Puppet manifest to deploy the Splunk OpenTelemetry Collector on Linux with minimal required parameters including collector version, access token, and realm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/deployments-linux-puppet.rst#2025-04-22_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nclass { splunk_otel_collector:\n  collector_version => 'VERSION'\n  splunk_access_token => 'SPLUNK_ACCESS_TOKEN',\n  splunk_realm => 'SPLUNK_REALM',\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HAProxy Receiver in OpenTelemetry Collector Pipeline\nDESCRIPTION: Example YAML configuration for adding the HAProxy monitor to the metrics pipeline in the OpenTelemetry Collector configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/haproxy.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/haproxy]\n```\n\n----------------------------------------\n\nTITLE: Error Handling with Splunk RUM in JavaScript\nDESCRIPTION: Shows how to report caught errors to Splunk RUM using the SplunkRum.error function, which accepts strings, arrays of strings, Error and ErrorEvent objects.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/migrate-manual-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\ntry {\n   doSomething();\n} catch (e) {\n   SplunkRum.error(e);\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating SignalFx Gateway Prometheus Remote Write Receiver into Metrics Pipeline (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to include the previously defined `signalfxgatewayprometheusremotewritereceiver` in the `metrics` pipeline within the `service` section of the Splunk Distribution of the OpenTelemetry Collector configuration file. This step is necessary to activate the receiver and make it process incoming Prometheus remote write requests.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-gateway-prometheus-remote-write-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [signalfxgatewayprometheusremotewritereceiver]\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Persistence Queues\nDESCRIPTION: Settings for persistent data queues to ensure data continuity during collector restarts. Persistent queues are configured for logs, metrics, and traces. Default storage path can be overridden using corresponding configuration options in the values.yaml file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\n    agent:\n      config:\n        exporters:\n            splunk_hec/platform_logs:\n              sending_queue:\n                storage: null\n```\n\nLANGUAGE: yaml\nCODE:\n```\n    agent:\n      config:\n        exporters:\n          splunk_hec/platform_metrics:\n            sending_queue:\n              storage: null\n```\n\n----------------------------------------\n\nTITLE: Deploying Splunk OpenTelemetry Collector with Puppet on Windows\nDESCRIPTION: This code snippet shows the basic deployment definition for the Splunk OpenTelemetry Collector using Puppet. It includes the minimum required parameters: collector_version, splunk_access_token, and splunk_realm. These values should be replaced with actual values specific to your environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/deployments-windows-puppet.rst#2025-04-22_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nclass { splunk_otel_collector:\n  collector_version => 'VERSION'\n  splunk_access_token => 'SPLUNK_ACCESS_TOKEN',\n  splunk_realm => 'SPLUNK_REALM',\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating Metrics Transform Processor into a Pipeline (YAML)\nDESCRIPTION: This configuration snippet demonstrates how to include the `metricstransform` processor within the `metrics` pipeline under the `service` section of the OpenTelemetry Collector configuration, ensuring it processes metrics flowing through that pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      processors: [metricstransform]\n```\n\n----------------------------------------\n\nTITLE: Detailed Logparser Configuration Example\nDESCRIPTION: Sample configuration demonstrating various Logparser options including file paths, watch method, measurement name, and pattern matching.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/logparser.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/logparser:\n    type: telegraf/logparser\n    files:\n     - '$file'\n    watchMethod: poll\n    fromBeginning: true     \n    measurementName: test-measurement \n    patterns:\n     - \"%{COMMON_LOG_FORMAT}\" \n    timezone: UTC\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Receiver in OpenTelemetry Collector\nDESCRIPTION: YAML configuration for setting up a custom Prometheus receiver to scrape metrics from a specific endpoint every 10 seconds.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/other-ingestion-methods/send-custom-metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprometheus/custom:\n  config:\n    scrape_configs:\n      - job_name: 'otel-collector-custom'\n        scrape_interval: 10s\n        static_configs:\n          - targets: [ '0.0.0.0:8889' ]\n```\n\n----------------------------------------\n\nTITLE: Obtaining an OpenTelemetry Tracer Instance in C++\nDESCRIPTION: Demonstrates how to retrieve the globally configured OpenTelemetry `TracerProvider` and then obtain a specific `Tracer` instance associated with a library name ('foo_library') and version ('1.0.0'). This tracer object is required to create spans. Assumes `InitTracer` has been called previously.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/cpp/instrument-cpp.rst#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nauto provider = opentelemetry::trace::Provider::GetTracerProvider();\nauto tracer = provider->GetTracer(\"foo_library\", \"1.0.0\");\n```\n\n----------------------------------------\n\nTITLE: Detailed JMX Receiver Configuration Example\nDESCRIPTION: This example provides a comprehensive configuration for the JMX receiver, including jar path, endpoint, target system, collection interval, and other settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/jmx-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  jmx:\n    jar_path: /opt/opentelemetry-java-contrib-jmx-metrics.jar\n    endpoint: my_jmx_host:12345\n    target_system: jvm\n    collection_interval: 10s\n    initial_delay: 1s\n    # optional: the same as specifying OTLP receiver endpoint.\n    otlp:\n      endpoint: mycollectorotlpreceiver:4317\n    username: my_jmx_username\n    # determined by the environment variable value\n    password: ${env:MY_JMX_PASSWORD}\n    resource_attributes: my.attr=my.value,my.other.attr=my.other.value\n    log_level: info\n    additional_jars:\n      - /path/to/other.jar\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Atlas Receiver for Log Collection in YAML\nDESCRIPTION: This example demonstrates how to configure the MongoDB Atlas receiver to collect logs, including audit and host logs, for a specific MongoDB project.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/mongodb-atlas-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    mongodbatlas:\n     logs:\n       enabled: true\n       projects: \n         - name: \"Your MongoDB project\"\n           collect_audit_logs: true\n           collect_host_logs: true\n```\n\n----------------------------------------\n\nTITLE: Configuring CORS for OTLP HTTP Receiver in YAML\nDESCRIPTION: This configuration example shows how to set up Cross-Origin Resource Sharing (CORS) for the OTLP HTTP receiver, including allowed origins, headers, and max age.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlp-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      http:\n        endpoint: \"localhost:4318\"\n        cors:\n          allowed_origins:\n            - http://test.com\n            - https://*.example.com\n          allowed_headers:\n            - Example-Header\n          max_age: 7200\n```\n\n----------------------------------------\n\nTITLE: Configuring Process Scraper in YAML\nDESCRIPTION: YAML configuration for the process scraper, allowing filtering by process names and error handling options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/host-metrics-receiver.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nprocess:\n  <include|exclude>:\n    names: [ <process name>, ... ]\n    match_type: <strict|regexp>\n  mute_process_name_error: <true|false>\n  mute_process_exe_error: <true|false>\n  mute_process_io_error: <true|false>\n  scrape_process_delay: <time>\n```\n\n----------------------------------------\n\nTITLE: Checking Pairing Status via ACS CLI\nDESCRIPTION: Command to check the status of organization pairing using ACS CLI. Requires the pairing ID and O11y access token.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/unified-id/unified-identity.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nacs observability pairing-status-by-id --pairing-id \"<enter-pairing-id>\" --o11y-access-token \"<enter-o11y-access-token>\"\n```\n\n----------------------------------------\n\nTITLE: Adding Kafka Monitor to Metrics Pipeline\nDESCRIPTION: Configuration example showing how to add the Kafka monitor to the metrics pipeline in the OpenTelemetry Collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-messaging/apache-kafka.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/kafka]\n```\n\n----------------------------------------\n\nTITLE: Adding Conviva Monitor to Metrics Pipeline (YAML)\nDESCRIPTION: This YAML configuration adds the Conviva monitor to the metrics pipeline in the OpenTelemetry Collector, enabling metric collection from Conviva.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-conviva/conviva.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/conviva]\n```\n\n----------------------------------------\n\nTITLE: Checking Pairing Status via API\nDESCRIPTION: Curl command to check organization pairing status using the API endpoint. Requires stack name, admin API token, and O11y API token.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/unified-id/unified-identity.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET 'https://admin.splunk.com/<enter-stack-name>/adminconfig/v2/observability/sso-pairing/<enter-pairing-id>' \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer <enter-splunk-admin-api-token>\" \\\n-H \"o11y-access-token: <enter-o11y-api-token>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser RUM Instrumentations\nDESCRIPTION: Example showing how to configure specific RUM instrumentations including custom event tracking, longtask monitoring, and websocket monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/configure-rum-browser-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nSplunkRum.init(\n   {\n      beaconEndpoint: 'https://rum-ingest.us0.signalfx.com/v1/rum',\n      rumAccessToken: 'ABC123…789',\n      applicationName: 'my-awesome-app',\n      instrumentations:\n      {\n         interactions:\n         {\n            // Adds``gamepadconneted`` events to the\n            // list of events collected by default\n           eventNames: [\n             ...SplunkRum.DEFAULT_AUTO_INSTRUMENTED_EVENT_NAMES,\n             'gamepadconnected'\n           ],\n         },\n         longtask: false, // Deactivates monitoring for longtasks\n         websocket: true, // Activates monitoring for websockets\n      },\n   });\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Exporter Queue Size\nDESCRIPTION: YAML configuration to adjust the sending queue size for the OTLP HTTP exporter to prevent out-of-memory situations under heavy load.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-sizing.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  config:\n    exporters:\n      otlphttp:\n        sending_queue:\n          queue_size: 512\n```\n\n----------------------------------------\n\nTITLE: Splunk On-Call API Endpoint Format\nDESCRIPTION: The URL format used for outgoing communication with Splunk On-Call on port 443.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/splunk-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://alert.victorops.com/integrations/generic/20131114/alert/<your_api_key>/<your_routing_key>\n```\n\n----------------------------------------\n\nTITLE: Setting OTLP Exporter Endpoint Environment Variable (Linux)\nDESCRIPTION: Sets the OTEL_EXPORTER_OTLP_ENDPOINT environment variable in a Linux shell. This optional variable specifies the host and port of the Splunk Distribution of OpenTelemetry Collector if it's not running on the default localhost.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\n----------------------------------------\n\nTITLE: Handling Standard Promise Rejection in JavaScript\nDESCRIPTION: Example demonstrating how Browser RUM agent handles standard promise rejections with Error objects.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-errors.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nnew Promise((resolve, reject) => {\n   reject(new Error('broken'))\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring bearertokenauth Extension in YAML for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration demonstrates how to set up the bearertokenauth extension in the OpenTelemetry Collector. It includes examples of configuring the extension with different schemes and tokens, and how to use it with OTLP exporters in a metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/bearertokenauth-extension.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  bearertokenauth:\n    token: \"somerandomtoken\"\n    filename: \"file-containing.token\"\n  bearertokenauth/withscheme:\n    scheme: \"Bearer\"\n    token: \"randomtoken\"\n\nreceivers:\n  hostmetrics:\n    scrapers:\n      memory:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  otlp/withauth:\n    endpoint: 0.0.0.0:5000\n    ca_file: /tmp/certs/ca.pem\n    auth:\n      authenticator: bearertokenauth\n\n  otlphttp/withauth:\n    endpoint: http://localhost:9000\n    auth:\n      authenticator: bearertokenauth/withscheme\n\nservice:\n  extensions: [bearertokenauth, bearertokenauth/withscheme]\n  pipelines:\n    metrics:\n      receivers: [hostmetrics]\n      processors: []\n      exporters: [otlp/withauth, otlphttp/withauth]\n```\n\n----------------------------------------\n\nTITLE: Basic Kubernetes Proxy Receiver Configuration in YAML\nDESCRIPTION: Basic configuration to activate the Kubernetes proxy monitor in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-proxy.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/kubernetes-proxy\n    type: kubernetes-proxy\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Upgrading OpenTelemetry Collector using YUM on RPM-based Linux Systems\nDESCRIPTION: Command to upgrade the Splunk OpenTelemetry Collector on RPM-based systems using the yum package manager. Requires root privileges and preserves modified configuration files.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-upgrade.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum upgrade splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Configuring Conviva Receiver in OpenTelemetry Collector (YAML)\nDESCRIPTION: This YAML snippet shows how to activate the Conviva integration by adding it to the receivers section of the OpenTelemetry Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-conviva/conviva.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/conviva:\n    type: conviva\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring HAProxy Receiver in YAML\nDESCRIPTION: This snippet shows how to activate the HAProxy receiver in the OpenTelemetry Collector configuration file. It specifies the endpoint, collection interval, and metrics settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/haproxy-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  haproxy:\n    endpoint: file:///var/run/haproxy.ipc\n    collection_interval: 1m\n    metrics:    \n```\n\n----------------------------------------\n\nTITLE: Basic Integration Configuration for Microsoft SQL Server\nDESCRIPTION: Basic configuration example showing how to activate the SQL Server integration in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/microsoft-sql-server.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/sqlserver:\n    type: telegraf/sqlserver\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Visualizing Changes in Distribution with Splunk Histograms\nDESCRIPTION: Histograms are employed to track and analyze distribution changes for a given metric across numerous sources. Useful for spotting unexpected changes in data patterns over time, such as latency variations. Inputs include the metric of choice and switching the visualization style to histograms. The output is a dynamic histogram depicting variations in data distribution.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/gain-insights-through-chart-analytics.rst#2025-04-22_snippet_4\n\nLANGUAGE: none\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Configuring zPages Extension - OpenTelemetry Collector - YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure the zPages extension for the OpenTelemetry Collector. The 'extensions' section specifies a zpages extension with a custom HTTP endpoint ('localhost:55679' by default) for accessing live debugging routes. Dependencies include a running OpenTelemetry Collector instance that supports extensions. The key parameter 'endpoint' controls the network interface and port for the zPages HTTP server. The expected input is a YAML configuration file, and the snippet's output is an activated zPages HTTP service for diagnostics. The endpoint value may be limited by network accessibility policies.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/zpages-extension.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  zpages:\n    endpoint: \"localhost:55679\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Cluster Monitor Settings in YAML\nDESCRIPTION: YAML configuration showing the main monitor settings including cluster reporting, namespace filtering, and node condition monitoring options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-cluster.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nalwaysClusterReporter: false\nnamespace: \nnodeConditionTypesToReport:\n  - Ready\n```\n\n----------------------------------------\n\nTITLE: Configuring Supervisor Receiver in OpenTelemetry Collector (YAML)\nDESCRIPTION: This snippet shows how to activate the Supervisor integration by adding the receiver configuration to the OpenTelemetry Collector. It includes the receiver definition and how to add it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/supervisor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/supervisor:\n    type: supervisor\n    ...  # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/supervisor]\n```\n\n----------------------------------------\n\nTITLE: Define LogicMonitor Alert Payload for Splunk On-Call Integration (JSON)\nDESCRIPTION: This JSON object defines the payload structure sent from LogicMonitor to Splunk On-Call via a custom HTTP delivery integration. It includes various LogicMonitor tokens (like `##MESSAGE##`, `##ALERTID##`, `##HOST##`) mapped to Splunk On-Call fields. The `message_type` is initially set to `CRITICAL` but should be adjusted (e.g., to `ACKNOWLEDGED` or `CLEARED`) for different alert activities as described in the surrounding text. This payload facilitates creating, acknowledging, and resolving incidents in Splunk On-Call based on LogicMonitor alerts.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/logicmonitor-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ “message_type”:“CRITICAL”, “state_message”:“##MESSAGE##”,\n“entity_id”:“##ALERTID##”, “monitoring_tool”:“LogicMonitor”,\n“entity_display_name”:“##LEVEL## alert on ##HOST##”,\n“GROUP”:“##GROUP##”, “START”:“##START##”,\n“DESCRIPTION”:“##SERVICEDESCRIPTION##”,\n“SERVICE_CHECKPOINT”:“##CHECKPOINT##”,\n“SERVICE_GROUP”:“##SERVICEGROUP##”,\n“CLIENT_URL”:“https://##COMPANY##.logicmonitor.com”,\n“ALERT_URL”:“##AlertDetailURL##”, “ADMIN”:“##ADMIN##”,\n“ALERTID”:“##ALERTID##”, “ALERTTYPE”:“##ALERTTYPE##”,\n“ALERTSTATUS”:“##ALERTSTATUS##”, “CMDLINE”:“##CMDLINE##”,\n“DATAPOINT”:“##DATAPOINT##”, “DATASOURCE”:“##DATASOURCE##”,\n“DPDESCRIPTION”:“##DPDESCRIPTION##”,\n“DSIDESCRIPTION”:“##DSIDESCRIPTION##”, “DURATION”:“##DURATION##”,\n“EVENTCODE”:“##EVENTCODE##”, “EXITCODE”:“##EXITCODE##”,\n“FACILITY”:“##FACILITY##”, “GENERALCODE”:“##GENERALCODE##”,\n“HOST”:“##HOST##”, “INSTANCE”:“##INSTANCE##”, “LEVEL”:“##LEVEL##”,\n“LOGFILE”:“##LOGFILE##”, “MESSAGE”:“##MESSAGE##”,\n“SOURCENAME”:“##SOURCENAME##”, “SPECIFICCODE”:“##SPECIFICCODE##”,\n“STARTEPOCH”:“##STARTEPOCH##”, “STDERR”:“##STDERR##”,\n“STDOUT”:“##STDOUT##”, “THRESHOLD”:“##THRESHOLD##”,\n“TRAPOID”:“##TRAPOID##”, “TYPE”:“##TYPE##”, “VALUE”:“##VALUE##” }\n```\n\n----------------------------------------\n\nTITLE: Configuring Webhook Payload for StatusCast Integration\nDESCRIPTION: JSON payload configuration for StatusCast webhook that defines the monitor name and state of alerts. The monitorName property maps to a StatusCast template, while state represents the current status of the incident.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/statuscast-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{\n\n   monitorName: \"${{ALERT.monitor_name}}\",\n   state: \"${{ALERT.entity_state}}\"\n\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Python Package\nDESCRIPTION: Command to install the Splunk OpenTelemetry Python package with all dependencies using pip.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/instrument-python-application.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"splunk-opentelemetry[all]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Agent Resource Limits in Kubernetes\nDESCRIPTION: YAML configuration to set CPU and memory resource limits for the Collector agent. Allocates 500m CPU and 1Gi memory to the agent container.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-sizing.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  resources:\n    limits:\n      cpu: 500m\n      memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring Basicauth for Simple Prometheus Receiver in YAML\nDESCRIPTION: This example demonstrates how to configure the basicauth extension for use with the Simple Prometheus Receiver in the OpenTelemetry Collector. It includes settings for the receiver, exporter, and extension.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/basic-auth-extension.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  lightprometheus/myjob:\n    auth:\n      authenticator: basicauth\n    collection_interval: 1s\n    endpoint: \"http://localhost:8000/metrics\"\n\nexporters:\n  otlp:\n    endpoint: \"${OTLP_ENDPOINT}\"\n    tls:\n      insecure: true\n\nextensions:\n  basicauth:\n    client_auth:\n      username: foo\n      password: bar\n\nservice:\n  extensions: [ basicauth ]\n  pipelines:\n    metrics:\n      receivers: [ lightprometheus/myjob ]\n      exporters: [ otlp ]\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Lambda Execution Wrapper for Node.js in Splunk OpenTelemetry\nDESCRIPTION: The execution wrapper path configuration for Node.js Lambda functions when using Splunk OpenTelemetry.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/lambda-configuration.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n/opt/nodejs-otel-handler\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Events Collection Configuration\nDESCRIPTION: YAML configuration for collecting Kubernetes events using the cluster receiver with various filtering options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-logs.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nclusterReceiver.k8sObjects:\n  - name: pods\n    mode: pull\n    label_selector: environment in (production),tier in (frontend)\n    field_selector: status.phase=Running\n    interval: 15m\n  - name: events\n    mode: watch\n    group: events.k8s.io\n    namespaces: [default]\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Spark Receiver in OpenTelemetry Collector (YAML)\nDESCRIPTION: This YAML snippet illustrates how to define the 'apachespark' receiver in the OpenTelemetry Collector's configuration file. The receiver fetches metrics from a Spark cluster, with required parameters like 'endpoint' (default 'http://localhost:4040'), optional 'collection_interval', and an array of specific Spark application names to monitor. Dependencies include a running instance of the Splunk Distribution of the OpenTelemetry Collector version compatible with Spark 3.3.2+ and network access to the Spark REST API endpoints. Input parameters include application names and endpoints; output will be relevant metrics collected at the defined interval.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/apache-spark-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  apachespark:\n    collection_interval: 60s\n    endpoint: http://localhost:4040\n    application_names:\n    - PythonStatusAPIDemo\n    - PythonLR\n```\n\n----------------------------------------\n\nTITLE: Testing Splunk On-Call Alerts from Localhost\nDESCRIPTION: cURL command to test alert functionality from localhost to Splunk On-Call. Used for troubleshooting alert delivery issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/splunk-integration-guide.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ncurl -X POST \"https://alert.victorops.com/integrations/generic/20131114/alert/SPLUNK_API_KEY –insecure -H\"accept: application/json\" -H \"Content-Type: application/json\" -d '{\"message_type\": \"INFO\", \"monitoring_tool\": \"splunk\", \"state_message\": \"Test Alert from localhost\", \"entity_display_name\": \"Test Alert\"}'\n```\n\n----------------------------------------\n\nTITLE: Deploying Node.js Collector in Kubernetes\nDESCRIPTION: Modify application Dockerfile and configure Kubernetes Deployment to run a Node.js app with OpenTelemetry, facilitating extensive monitoring and data analysis.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/instrumentation/instrument-nodejs-application.rst#2025-04-22_snippet_6\n\nLANGUAGE: docker\nCODE:\n```\n# Install the @splunk/otel package\nRUN npm install @splunk/otel\n# Set appropriate permissions\nRUN chmod -R go+r /node_modules/@splunk/otel\n```\n\n----------------------------------------\n\nTITLE: Configuring Host Variable in Universal Forwarder inputs.conf\nDESCRIPTION: Configuration stanza for inputs.conf to ensure the Universal Forwarder uses the correct host determination logic, complementing the FQDN configuration in server.conf.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-with-the-uf.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n[default]\nhost=$decideOnStartup\n```\n\n----------------------------------------\n\nTITLE: Complete Microsoft SQL Server Receiver Configuration\nDESCRIPTION: A comprehensive example configuration for the Microsoft SQL Server receiver with authentication details.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/microsoft-sql-server.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/sqlserver:\n     type: telegraf/sqlserver\n     host: <host_name>\n     port: 1433\n     userID: <user_id>\n     password: <password>\n     appName: sqlserver\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Discovery Properties Configuration\nDESCRIPTION: YAML configuration for overriding PostgreSQL receiver discovery settings with environment variables\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-advanced-config.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsplunk.discovery:\n  receivers:\n    postgresql:\n      username: \"${PG_USERNAME_ENVVAR}\"\n      password: \"${PG_PASSWORD_ENVVAR}\"\n```\n\n----------------------------------------\n\nTITLE: Starting the OpenTelemetry Collector Service with PowerShell\nDESCRIPTION: PowerShell command to start the Splunk OpenTelemetry Collector Windows service after installation. This is required if you don't restart the system after installation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows-msi.rst#2025-04-22_snippet_2\n\nLANGUAGE: PowerShell\nCODE:\n```\nStart-Service splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Standalone Java Agent Container\nDESCRIPTION: Dockerfile to create a separate container for the Splunk OpenTelemetry Java agent that can be used as a sidecar.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-fargate-java.rst#2025-04-22_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM alpine:latest\n\nRUN apk add --no-cache curl\n\n# Create a directory for the agent artifacts\nRUN mkdir -p /opt/splunk\nWORKDIR /opt/splunk\n\n# Download the Splunk Java agent\nRUN curl -L0 https://github.com/signalfx/splunk-otel-java/releases/latest/download/splunk-otel-javaagent.jar \\\n-o splunk-otel-javaagent.jar\n\n# Expose the /opt/splunk directory as a shared volume\nVOLUME [\"/opt/splunk\"]\n\nCMD tail -f /dev/null\n```\n\n----------------------------------------\n\nTITLE: Redacting Sensitive Data in Span Attributes for Android RUM\nDESCRIPTION: Shows how to replace sensitive data in span attributes using regex pattern matching to protect PII while still maintaining useful telemetry.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nSplunkRum.builder()\n         .filterSpans(spanFilter ->\n            spanFilter \n                  .replaceSpanAttribute(StandardAttributes.HTTP_URL,\n                     value -> Pattern.compile.. _\"(user|pass:\\\\w+\")\n                           .matcher(value)\n                           .replaceAll(\"$1=<redacted>\")))\n```\n\n----------------------------------------\n\nTITLE: Configuring Group by Attributes Processor in YAML\nDESCRIPTION: Basic configuration for the Group by Attributes processor, specifying attribute keys for grouping telemetry data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/groupbyattrs-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  groupbyattrs:\n    keys:\n      - foo\n      - bar\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Endpoint Settings\nDESCRIPTION: YAML configuration for setting up metrics endpoint configuration in Kubernetes environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-advanced-config.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\noperatorcrds:\n  install: true\noperator:\n  enabled: true\ninstrumentation:\n  spec:\n    env:\n    - name: SPLUNK_OTEL_AGENT\n      valueFrom:\n        fieldRef:\n          apiVersion: v1\n          fieldPath: status.hostIP\n    - name: SPLUNK_METRICS_ENDPOINT\n      value: http://$(SPLUNK_OTEL_AGENT):9943/v2/datapoint\n```\n\n----------------------------------------\n\nTITLE: Activating the OTLP Exporter in YAML\nDESCRIPTION: This YAML snippet shows the basic configuration required to activate the OTLP exporter within the OpenTelemetry Collector configuration file. It simply adds 'otlp' under the 'exporters' section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlp-exporter.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n.. code-block:: yaml\n\n  exporters:\n    otlp:\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OTel Collector with Deployment Environment\nDESCRIPTION: This command installs the Splunk OTel Collector with systemd instrumentation and sets a deployment environment. It requires replacing the realm, access token, and environment placeholders.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-systemd-instrumentation --deployment-environment prod \\\n--realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Adding Procstat Monitor to Metrics Pipeline\nDESCRIPTION: Configuration snippet showing how to add the procstat monitor to the service pipelines section for metrics collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/procstat.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/procstat]\n```\n\n----------------------------------------\n\nTITLE: Initializing SplunkRum with Configuration in Objective-C\nDESCRIPTION: Example showing how to initialize the SplunkRum module with various configuration options in Objective-C, including beacon URL, RUM token, global attributes, debug mode, environment name, application name, URL filtering, and screen name spans.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/configure-rum-ios-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: objective-c\nCODE:\n```\n@import SplunkOtel;\n//...\n\nSplunkRumBuilder *builder = [[SplunkRumBuilder alloc] initWithBeaconUrl:@\"https://rum-ingest.<realm>.signalfx.com/v1/rum\"  rumAuth: @\"<rum-token>\"]]];\n[builder globalAttributesWithGlobalAttributes:[NSDictionary dictionary]];\n[builder debugWithEnabled:true];\n[builder deploymentEnvironmentWithEnvironment:@\"environment-name\"];\n[builder setApplicationName:@\"<your_app_name>\"];\nNSError* error = nil;\n[builder ignoreURLsWithIgnoreURLs: [NSRegularExpression regularExpressionWithPattern: @\".*ignore_this.*\" options: 0 error: &error]];\n[builder screenNameSpansWithEnabled:true];\n// The build method always come last\n[builder build];\n```\n\n----------------------------------------\n\nTITLE: Summing Log Attribute Values with Conditions and Attribute Keys (YAML)\nDESCRIPTION: This example demonstrates a configuration where the Sum connector processes logs, summing numerical values from a specified 'source_attribute', applies a condition to exclude logs where the attribute is 'NULL', and generates separate metric data points per unique 'payment.processor' value, defaulting to 'unspecified_processor' when absent. The configuration includes receivers, exporters, and two service pipelines, illustrating advanced attribute extraction and conditional filtering.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sum-connector.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\\n  foo:\\nconnectors:\\n  sum:\\n    logs:\\n      checkout.total:\\n        source_attribute: total.payment\\n        conditions:\\n          - attributes[\\\"total.payment\\\"] != \\\"NULL\\\"\\n        attributes:\\n          - key: payment.processor\\n            default_value: unspecified_processor\\nexporters:\\n  bar:\\n\\nservice:\\n  pipelines:\\n    metrics/sum:\\n       receivers: [sum]\\n       exporters: [bar]\\n    logs:\\n       receivers: [foo]\\n       exporters: [sum]\n```\n\n----------------------------------------\n\nTITLE: Adding Sum Connector to Service Pipelines (YAML)\nDESCRIPTION: In this example, the YAML configuration shows how to integrate the 'sum' connector into both metrics and traces pipelines under the 'service' section. The 'metrics/sum' pipeline receives from 'sum', while the 'traces' pipeline sends to 'sum' as an exporter. This allows flexible placement of the connector in different telemetry flows depending on desired behavior and supported pipeline types.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sum-connector.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\\n  pipelines:\\n    metrics/sum:\\n       receivers: [sum]\\n    traces:\\n       exporters: [sum]\n```\n\n----------------------------------------\n\nTITLE: Creating a Current Span in Python with OpenTelemetry\nDESCRIPTION: Demonstrates how to create a span as the current span in a Python function using a context manager. The span automatically closes when exiting the 'with' block.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef reticulate_splines():\n   with tracer.start_as_current_span(\"span-name\") as span:\n      print(\"Reticulating splines...\")\n      # When the 'with' block goes out of scope, the 'span' is closed\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OTel Collector Proxy via Kubernetes (YAML)\nDESCRIPTION: Defines HTTP_PROXY and HTTPS_PROXY environment variables within the `env` section of a Kubernetes container specification (e.g., in a Deployment or DaemonSet). This applies proxy settings to the collector pod.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/allow-services.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n   - name: HTTP_PROXY\n     value: '<proxy.address:port>'\n   - name: HTTPS_PROXY\n     value: '<proxy.address:port>'\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Pipelines for SignalFx Forwarder\nDESCRIPTION: Configuration for adding the SignalFx Forwarder monitor to the service pipelines section, enabling both metrics and traces collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/signalfx-forwarder.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/signalfx-forwarder]\n    traces:\n      receivers: [smartagent/signalfx-forwarder]\n```\n\n----------------------------------------\n\nTITLE: Registering OpenTelemetry for IIS Application\nDESCRIPTION: PowerShell command to register OpenTelemetry instrumentation for an IIS application. This command will restart IIS to apply the changes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# Set up IIS instrumentation\n# IIS is restarted as a result\nRegister-OpenTelemetryForIIS\n```\n\n----------------------------------------\n\nTITLE: Reporting Custom Errors in React Native RUM\nDESCRIPTION: Shows the function signature for reporting handled errors, exceptions, and messages in Splunk RUM. This allows for custom error reporting in React Native applications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/manual-rum-react-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nreportError: (err: any, isFatal?: boolean)\n```\n\n----------------------------------------\n\nTITLE: Including RabbitMQ Receiver in Metrics Pipeline\nDESCRIPTION: This configuration snippet demonstrates how to include the RabbitMQ receiver in the metrics pipeline of the OpenTelemetry Collector service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/rabbitmq-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [rabbitmq]\n```\n\n----------------------------------------\n\nTITLE: Enabling SignalFx Gateway Prometheus Remote Write Receiver (YAML)\nDESCRIPTION: This YAML snippet shows the default configuration block required to enable the `signalfxgatewayprometheusremotewritereceiver` within the `receivers` section of the Splunk Distribution of the OpenTelemetry Collector configuration file. This is the first step in setting up the receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-gateway-prometheus-remote-write-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  signalfxgatewayprometheusremotewritereceiver:\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Observer and Receiver Creator in YAML\nDESCRIPTION: This snippet demonstrates how to configure the Kubernetes observer extension and the Receiver Creator receiver to collect kubelet stats from Kubernetes nodes. It includes settings for the observer, receiver creator, and service pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/receiver-creator-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  k8s_observer:\n    auth_type: serviceAccount\n    node: ${env:K8S_NODE_NAME}\n    observe_pods: true\n    observe_nodes: true\n\nreceivers:\n  receiver_creator/k8skubeletstats:\n    watch_observers: [k8s_observer]\n    receivers:\n      kubeletstats:\n        rule: type == \"k8s.node\"\n        config:\n          auth_type: serviceAccount\n          collection_interval: 15s\n          endpoint: '`endpoint`:`kubelet_endpoint_port`'\n          extra_metadata_labels:\n            - container.id\n          metric_groups:\n            - container\n            - pod\n            - node\n\nservice:\n  extensions: [k8s_observer]\n  pipelines:\n    metrics:\n      receivers: [receiver_creator/k8skubeletstats]\n```\n\n----------------------------------------\n\nTITLE: Implementing Twilio Live Call Routing Function in JavaScript\nDESCRIPTION: This snippet refers to the core JavaScript function for Twilio used to route incoming calls to Splunk On-Call teams. The code is to be copied from the provided GitHub repository (https://github.com/victorops/twilio-live-call-routing/blob/master/Splunk-On-Call-Twilio) and replaces the default template in Twilio Functions. Required dependencies include Twilio's runtime environment, the specified npm modules, and properly-configured environment variables (such as API credentials, team identifiers, and escalation policies). Inputs include Voice call webhooks from Twilio; outputs include routing calls, optionally transcribing voicemails, and invoking incidents via VictorOps API. Edits to the function allow voicemail voice changes and menu customization as described in the instructions. Limitations include adherence to Twilio's constraints (e.g., voicemail length) and exact format requirements for Splunk On-Call variables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/new-twilio.rst#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Function code to be copied from external repository:\n// https://github.com/victorops/twilio-live-call-routing/blob/master/Splunk-On-Call-Twilio\n\n// Example code block placement in Twilio Console:\n/*\n  Original placeholder code removed. Paste the repository code here.\n  Edits may include:\n     - Changing voice (e.g., replace 'woman' with 'Polly.Salli')\n     - Modifying greeting to enforce desired pronunciation\n     - Implementing multi-team menu logic using environment variables (TEAM_1, TEAM_2, ESC_POL_1, etc.)\n     - Handling voicemails with optional transcription\n*/\n// (Full code found in linked GitHub repository)\n\n```\n\n----------------------------------------\n\nTITLE: Activating Desugaring in Kotlin Applications\nDESCRIPTION: Code snippet showing how to enable desugaring in a Kotlin Android application by configuring compileOptions and adding the necessary dependency in build.gradle.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_0\n\nLANGUAGE: kotlin\nCODE:\n```\nandroid {\n   compileOptions {\n      //...\n      coreLibraryDesugaringEnabled = true\n      sourceCompatibility = JavaVersion.VERSION_1_8 // Java 8 and higher\n      targetCompatibility = JavaVersion.VERSION_1_8 // Java 8 and higher\n      //...\n   }\n}\n\ndependencies {\n   //...\n   coreLibraryDesugaring(\"com.android.tools:desugar_jdk_libs:1.1.5\")\n   //...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Workload Rule in Splunk Cloud Platform\nDESCRIPTION: A configuration snippet for creating a Workload Rule that limits Log Observer Connect searches to 5 minutes. This helps maintain a responsive experience for users and reduces the chances of search queuing.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/logs/scp.rst#2025-04-22_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nPredicate: user=[your_Log_Observer_Connect_service-account_name] AND runtime>5m\nSchedule: Always on\nAction: Abort search\n```\n\n----------------------------------------\n\nTITLE: Service Pipeline Configuration for Kubernetes Proxy\nDESCRIPTION: Configuration to add the Kubernetes proxy monitor to the metrics pipeline service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-proxy.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n   pipelines:\n     metrics:\n       receivers: [smartagent/kubernetes-proxy]\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQ Receiver in YAML\nDESCRIPTION: This snippet shows how to activate and configure the RabbitMQ receiver in the OpenTelemetry Collector configuration file. It specifies the endpoint, username, password, and collection interval.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/rabbitmq-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  rabbitmq:\n    endpoint: http://localhost:15672\n    username: otelu\n    password: ${env:RABBITMQ_PASSWORD}\n    collection_interval: 10s\n```\n\n----------------------------------------\n\nTITLE: Managing Kubernetes Configuration\nDESCRIPTION: Various kubectl config commands for managing Kubernetes configurations including context, users, and authentication settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl config [subcommand]\n\n# Examples\n\n# Show kubeconfig settings\nkubectl config view\n\n# Save namespace for all subsequent kubectl commands in context\nkubectl config set-context --current --namespace=ggckad-s2\n\n# Get the password for the e2e user\nkubectl config view -o jsonpath='{.users[?(@.name == \"e2e\")].user.password}'\n\n# Display the first user\nkubectl config view -o jsonpath='{.users[].name}'\n\n# Get a list of users \nkubectl config view -o jsonpath='{.users[*].name}'\n\n# Display list of contexts \nkubectl config get-contexts \n\n# Display the current-context\nkubectl config current-context \n\n# Set the default context to my-cluster-name\nkubectl config use-context my-cluster-name \n\n# Add a new user to your kubeconfig that supports basic authorization\nkubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword \n\n# Set a context utilizing a specific username and namespace\nkubectl config set-context gce --user=cluster-admin --namespace=foo \\ && kubectl config use-context gce\n```\n\n----------------------------------------\n\nTITLE: Testing OPcache Status Script via curl\nDESCRIPTION: Command to test the OPcache status script to verify it's working correctly, along with expected JSON output sample.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cache/opcache.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost/monitoring/opcache_stat.php\n{\n\"opcache_enabled\": true,\n\"cache_full\": false,\n\"restart_pending\": false,\n\"restart_in_progress\": false,\n\"memory_usage\": {\n    \"used_memory\": 82614848,\n    \"free_memory\": 183437232,\n    \"wasted_memory\": 2383376,\n    \"current_wasted_percentage\": 0.88787674903869629\n},\n#...\n```\n\n----------------------------------------\n\nTITLE: Activating Metrics Pipeline Configuration\nDESCRIPTION: YAML configuration for activating the metrics pipeline with multiple receivers including hostmetrics, otlp, signalfx, and the configured prometheus/flink receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/prometheus-generic.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n   receivers: [hostmetrics, otlp, signalfx, prometheus/flink]\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk RUM Access in Android Application\nDESCRIPTION: This Java code snippet instructs on configuring the property file needed to run the sample Android demo app for Splunk RUM. Before running the app, users must define the 'rum.realm' and 'rum.access.token' properties in the 'local.properties' file for their selected realm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/sample-app.rst#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nrum.realm=<realm>\\nrum.access.token=<Splunk RUM access token>\n```\n\n----------------------------------------\n\nTITLE: Sending Custom Metrics: SignalFx vs OpenTelemetry APIs - JavaScript\nDESCRIPTION: Shows how to send custom metrics using both SignalFx and OpenTelemetry APIs in Node.js. The first snippet uses SignalFx's client API to send gauge and counter values, while the second uses OpenTelemetry Meter to create and observe gauges and counters. Migrating to OpenTelemetry API improves future compatibility and leverages wider ecosystem support.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/configuration/nodejs-otel-metrics.rst#2025-04-22_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n// SignalFx\\ngetSignalFxClient().send({\\n   gauges: [{ metric: 'cpu', value: 42, timestamp: 1442960607000}],\\n   cumulative_counters: [{ metric: 'clicks', value: 99, timestamp: 1442960607000}],\\n})\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\n// OpenTelemetry\\nconst meter = metrics.getMeter('my-meter');\\nmeter.createObservableGauge('cpu', result => {\\n   result.observe(42);\\n});\\nconst counter = meter.createCounter('clicks');\\ncounter.add(99);\n```\n\n----------------------------------------\n\nTITLE: Activating PHP-FPM Monitor in Collector Configuration\nDESCRIPTION: This YAML configuration activates the PHP-FPM monitor in the Splunk Distribution of OpenTelemetry Collector. It defines the receiver type and adds it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/php-fpm.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/ collectd/php-fpm:\n    type: collectd/php-fpm\n    ... # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/collectd/php-fpm]\n```\n\n----------------------------------------\n\nTITLE: Importing OpenTelemetry Types for Asynchronous Metrics in Python\nDESCRIPTION: Imports the necessary types from OpenTelemetry metrics module for implementing asynchronous metric collection in Python.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Iterable\nfrom opentelemetry.metrics import CallbackOptions, Observation\n```\n\n----------------------------------------\n\nTITLE: Deploying OpenTelemetry Collector for Windows using Docker\nDESCRIPTION: PowerShell command to run the Splunk OpenTelemetry Collector Docker container with required environment variables and port mappings. This command deploys the latest image with access token and realm configurations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows-manual.rst#2025-04-22_snippet_0\n\nLANGUAGE: PowerShell\nCODE:\n```\n$ docker run --rm -e SPLUNK_ACCESS_TOKEN=12345 -e SPLUNK_REALM=us0  `\n\t    -p 13133:13133 -p 14250:14250 -p 14268:14268 -p 4317:4317 -p 6060:6060  `\n\t    -p 8888:8888 -p 9080:9080 -p 9411:9411 -p 9943:9943 `\n\t    --name=otelcol quay.io/signalfx/splunk-otel-collector-windows:latest\n```\n\n----------------------------------------\n\nTITLE: Applying Collector Configuration with Helm\nDESCRIPTION: Helm command to upgrade the OpenTelemetry Collector configuration with new filter and HEC settings, including SSL configuration for development purposes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/collector-config-tutorial-edit.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --reuse-values -f ./filter.yaml -f ./hec.yaml splunk-otel-collector-1709226095 splunk-otel-collector-chart/splunk-otel-collector --set=\"splunkPlatform.insecureSkipVerify=true\"\n```\n\n----------------------------------------\n\nTITLE: .NET Application Deployment YAML Without Instrumentation (linux-musl-x64)\nDESCRIPTION: Basic Kubernetes deployment manifest for a .NET application running on linux-musl-x64 before adding OpenTelemetry instrumentation annotations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-dotnet-app\n  namespace: monitoring\nspec:\n  template:\n    spec:\n      containers:\n       - name: my-dotnet-app\n         image: my-dotnet-app:latest\n```\n\n----------------------------------------\n\nTITLE: Adding Varnish Monitor to Pipeline - YAML Configuration\nDESCRIPTION: YAML configuration to add the Varnish monitor to the metrics pipeline in the collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/varnish.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/varnish]\n```\n\n----------------------------------------\n\nTITLE: Testing Splunk On-Call API Connection with cURL\nDESCRIPTION: cURL command to test the connection to Splunk On-Call API endpoint for sending alerts. Used to verify network connectivity and API functionality.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/splunk-integration-guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: curl\nCODE:\n```\ncurl -X POST https://alert.victorops.com/integrations/generic/20131114/alert/SPLUNK_API_KEY –insecure -H\"accept: application/json\" -H \"Content-Type: application/json\" -d '{\"message_type\": \"INFO\", \"monitoring_tool\": \"splunk\", \"state_message\": \"Test Alert\", \"entity_display_name\": \"Test Alert\"}'\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry .NET on Linux\nDESCRIPTION: Shell commands to download and install the Splunk Distribution of OpenTelemetry .NET on Linux systems. The script is downloaded from GitHub and executed to install the instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ncurl -sSfL https://github.com/signalfx/splunk-otel-dotnet/releases/latest/download/splunk-otel-dotnet-install.sh -O\n# Install the distribution\nsh ./splunk-otel-dotnet-install.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OTel Auto-Instrumentation Package Manually\nDESCRIPTION: These commands manually install the splunk-otel-auto-instrumentation package on Debian and RPM-based systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nsudo dpkg -i <path to splunk-otel-auto-instrumentation deb>\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo rpm -Uvh <path to splunk-otel-auto-instrumentation rpm>\n```\n\n----------------------------------------\n\nTITLE: Enabling Data Point Logging in SignalFx Exporter\nDESCRIPTION: YAML configuration snippet for enabling data point logging in the SignalFx exporter of the OpenTelemetry Collector to validate metrics collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n   signalfx:\n      ...\n      log_data_points: true\n      ...\n```\n\n----------------------------------------\n\nTITLE: Adding Attributes Processor to OTEL Collector Pipeline\nDESCRIPTION: YAML configuration example showing how to integrate a defined attributes processor (e.g., 'attributes/settenant') into the 'traces' pipeline within the 'service' section of the OpenTelemetry Collector configuration. It's placed after the 'batch' processor and before 'queued_retry'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/add-context-trace-span.rst#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      receivers: ...\n      processors: [...,  batch, attributes/settenant,  queued_retry, ...] \n      ...\n```\n\n----------------------------------------\n\nTITLE: Integrating iOS RUM with Splunk Browser RUM in Swift\nDESCRIPTION: This code snippet demonstrates how to integrate iOS RUM with Splunk Browser RUM using Swift. It imports the necessary modules, creates a WebView instance, and uses the SplunkRum.integrateWithBrowserRum() method to share the splunk.rumSessionId between both instrumentations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/install-rum-ios.rst#2025-04-22_snippet_7\n\nLANGUAGE: swift\nCODE:\n```\nimport WebKit\nimport SplunkOtel\n\n...\n   /*\nMake sure that the WebView instance only loads pages under\nyour control and instrumented with Splunk Browser RUM. The\nintegrateWithBrowserRum() method can expose the splunk.rumSessionId\nof your user to every site/page loaded in the WebView instance.\n*/\n   let webview: WKWebView = ...\n   SplunkRum.integrateWithBrowserRum(webview)\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OTel Collector Agent using Puppet\nDESCRIPTION: This Puppet code snippet defines the `splunk_otel_collector` class to deploy and configure the Splunk OpenTelemetry Collector agent on managed EC2 nodes. It requires the `splunk_otel_collector` module from Puppet Forge and sets essential parameters like the Splunk access token, realm, and the source/destination paths for the agent's configuration file. This configuration is part of the guided setup for Linux monitoring within Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/scenarios-tutorials/scenario-collector.rst#2025-04-22_snippet_0\n\nLANGUAGE: puppet\nCODE:\n```\nclass { splunk_otel_collector:\nsplunk_access_token => '<kai_token>',\nsplunk_realm => 'us0',\ncollector_config_source => 'file:///etc/otel/collector/agent_config.yaml',\ncollector_config_dest => '/etc/otel/collector/agent_config.yaml',\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Metrics Pipeline for Splunk\nDESCRIPTION: YAML configuration showing how to set up a pipeline for collecting Prometheus metrics and sending them to Splunk HEC. This defines the receivers, processors, and exporters for the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-exporter.rst#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\npipelines:\n  metrics:\n      receivers: [prometheus]\n      processors: [batch]\n      exporters: [splunk_hec/metrics]\n\nreceivers:\n  prometheus:\n    config:\n      scrape_configs:\n        - job_name: 'otel-collector'\n          scrape_interval: 5s\n          static_configs:\n            - targets: ['<container_name>:<container_port>']\n```\n\n----------------------------------------\n\nTITLE: Jenkins Configuration with All Enhanced Metrics\nDESCRIPTION: Configuration example showing how to enable all enhanced metrics monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/jenkins.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/jenkins:\n    type: collectd/jenkins\n    host: 127.0.0.1\n    port: 8080\n    metricsKey: reallylongmetricskey\n    enhancedMetrics: true\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Direct Data Export on Linux\nDESCRIPTION: These shell commands set environment variables for the Splunk access token and realm on Linux, enabling direct data export to Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\nexport SPLUNK_ACCESS_TOKEN=<access_token>\nexport SPLUNK_REALM=<realm>\n```\n\n----------------------------------------\n\nTITLE: Advanced Kubernetes Observer Configuration\nDESCRIPTION: Extended configuration example showing Kubernetes observer setup with receiver creator and multiple monitoring rules.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-proxy.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  k8s_observer:\n  host_observer:\n\nreceivers:\n  receiver_creator/1:\n    watch_observers: [k8s_observer]\n    receivers:\n      smartagent/kubernetes-kubeproxy:\n        rule: type == \"pod\" && name matches \"kube-proxy\"\n        type: kubernetes-proxy\n        port: 10249\n        extraDimensions:\n          metric_source: kubernetes-proxy\n\n      prometheus_simple:\n        rule: type == \"pod\" && annotations[\"prometheus.io/scrape\"] == \"true\"\n        config:\n          metrics_path: '`\"prometheus.io/path\" in annotations ? annotations[\"prometheus.io/path\"] : \"/metrics\"`'\n          endpoint: '`endpoint`:`\"prometheus.io/port\" in annotations ? annotations[\"prometheus.io/port\"] : 9090`'\n\n      redis/1:\n        rule: type == \"port\" && port == 6379\n        config:\n          password: secret\n          collection_interval: `pod.annotations[\"collection_interval\"]`\n      resource_attributes:\n          service.name: `pod.labels[\"service_name\"]`\n\n      redis/2:\n        rule: type == \"port\" && port == 6379\n        resource_attributes:\n          app: `pod.labels[\"app\"]`\n          source: redis\n\n  receiver_creator/2:\n    watch_observers: [host_observer]\n    receivers:\n      redis/on_host:\n        rule: type == \"port\" && port == 6379 && is_ipv6 == true\n        resource_attributes:\n          service.name: redis_on_host\n\nprocessors:\n  exampleprocessor:\n\nexporters:\n  exampleexporter:\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [receiver_creator/1, receiver_creator/2]\n      processors: [exampleprocessor]\n      exporters: [exampleexporter]\n  extensions: [k8s_observer, host_observer]\n```\n\n----------------------------------------\n\nTITLE: Listing Specific Domains to Allow for Splunk Observability Cloud (Shell)\nDESCRIPTION: Provides a detailed list of specific domains and endpoints for various Splunk Observability Cloud services (API, UI, Ingest, SignalFlow, RUM, CDN, Package Repositories). This list should be used if wildcard URLs are not permitted by network security policies. Replace `<YOUR_REALM>` with the specific realm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/allow-services.rst#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n# Splunk Observability Cloud API base URL (https://dev.splunk.com/observability/docs/apibasics/api_list)\napi.<YOUR_REALM>.signalfx.com\n\n# Splunk Observability Cloud user interface\napp.<YOUR_REALM>.signalfx.com\ncustomer-api.<YOUR_REALM>.signalfx.com\n\n# CDN for Splunk Observability Cloud files and installers\n# Note that files might be hosted on Github.com\ndl.signalfx.com\n\n# Backfill API base URL (https://dev.splunk.com/observability/reference/api/backfill/latest)\nbackfill.<YOUR_REALM>.signalfx.com\n\n# Data ingest API base URL (https://dev.splunk.com/observability/docs/datamodel/ingest/)\ningest.<YOUR_REALM>.signalfx.com\n\n# SignalFlow API base URL (https://dev.splunk.com/observability/reference/api/signalflow/latest)\nstream.<YOUR_REALM>.signalfx.com\n\n# RUM ingest endpoint \nrum-ingest.<YOUR_REALM>.signalfx.com/v1/rum\n   \n# For td-agent/Fluentd on Linux and Windows\npackages.treasuredata.com\n   \n# For DEB/RPM collector packages\nsplunk.jfrog.io \njfrog-prod-use1-shared-virginia-main.s3.amazonaws.com\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Sensu Handler for Splunk On-Call (Ruby)\nDESCRIPTION: This Ruby script implements a Sensu handler that creates and resolves incidents on Splunk On-Call via its API using POST requests. It reads configuration and event data from Sensu, constructs a payload based on check status, and handles HTTPS requests to the Splunk On-Call endpoint. The handler requires Sensu (with 'sensu-handler' gem), valid routing/API endpoint in its settings, and network access to Splunk On-Call. Key event parameters include action type, client name, and check status; output is an HTTP result logged to standard output. Timeouts and API errors are managed gracefully, and the script should be placed at /etc/sensu/handlers/victorops.rb.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/sensu-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\n#!/usr/bin/env ruby\n#!/usr/bin/env ruby\n# This handler creates and resolves victorops incidents\n#\n# Released under the same terms as Sensu (the MIT license); see LICENSE\n# for details.\n\nrequire 'rubygems' if RUBY_VERSION < '1.9.0'\nrequire 'sensu-handler'\nrequire 'uri'\nrequire 'net/http'\nrequire 'net/https'\nrequire 'json'\n\nclass VictorOps < Sensu::Handler\ndef handle\n   config = settings['victorops']\n   incident_key = @event['client']['name'] + '/' + @event['check']['name']\n\n   description = @event['check']['notification']\n   description ||= [@event['client']['name'], @event['check']['name'], @event['check']['output']].join(' : ')\n   host = @event['client']['name']\n   entity_id = incident_key\n   state_message  = description\n   begin\n   Timeout.timeout(10) do\n\n      case @event['action']\n      when 'create'\n         case @event['check']['status']\n         when 1\n         message_type = 'WARNING'\n         else\n         message_type = 'CRITICAL'\n         end\n      when 'resolve'\n         message_type = 'RECOVERY'\n      end\n\n      payload = Hash.new\n      payload[:message_type] = message_type\n      payload[:state_message] = state_message.chomp\n      payload[:entity_id] = entity_id\n      payload[:host_name] = host\n      payload[:monitoring_tool] = 'sensu'\n\n      # Add in client data\n      payload[:check] = @event['check']\n      payload[:client] = @event['client']\n\n      uri   = URI(\"#{config['api_url'].chomp('/')}/#{config['routing_key']}\")\n      https = Net::HTTP.new(uri.host, uri.port)\n\n      https.use_ssl = true\n\n      request      = Net::HTTP::Post.new(uri.path)\n      request.body = payload.to_json\n      response     = https.request(request)\n\n      if response.code == '200'\n         puts \"victorops -- #{@event['action'].capitalize}'d incident -- #{incident_key}\"\n      else\n         puts \"victorops -- failed to #{@event['action']} incident -- #{incident_key}\"\n         puts \"victorops -- response: #{response.inspect}\"\n      end\n   end\n   rescue Timeout::Error\n   puts 'victorops -- timed out while attempting to ' + @event['action'] + ' a incident -- ' + incident_key\n   end\nend\nend\n```\n\n----------------------------------------\n\nTITLE: Creating Splunk Data Links\nDESCRIPTION: This snippet defines a resource for a data link in Terraform linking a Splunk APM inferred service to a specific AppDynamics tier. It includes parameters such as property name, property value, and target application URL.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-datalinks-terraform/apm-create-data-links-terraform-file.rst#2025-04-22_snippet_2\n\nLANGUAGE: terraform\nCODE:\n```\n# A link to a Splunk AppDynamics service\nresource \"signalfx_data_link\" \"<data-link-id>\" {\nproperty_name        = \"sf_service\"\nproperty_value       = \"<splunk-inferred-service-name>\"\n\ntarget_appd_url {\nname        = \"<data-link-ui-label>\"\nurl         = \"<https://www.example.saas.appdynamics.com/#/application=1234&component=5678>\"\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Activating Debug Logging in Python for OpenTelemetry Troubleshooting\nDESCRIPTION: Shows how to import the logging module and configure the logging level to DEBUG to help troubleshoot Python instrumentation issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/troubleshooting/common-python-troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OpenTelemetry Collector Credentials\nDESCRIPTION: Sample configuration for setting up access token and realm in the Splunk OpenTelemetry Collector configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nSPLUNK_ACCESS_TOKEN=<access_token>\nSPLUNK_REALM=<realm>\n```\n\n----------------------------------------\n\nTITLE: Creating SQL Authentication Login for Microsoft SQL Server\nDESCRIPTION: SQL commands to create login credentials with necessary permissions for the Microsoft SQL Server integration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/microsoft-sql-server.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nUSE master;\nGO\nCREATE LOGIN [<user_id>] WITH PASSWORD = '<YOUR PASSWORD HERE>';\nGO\nGRANT VIEW SERVER STATE TO [<user_id>];\nGO\nGRANT VIEW ANY DEFINITION TO [<user_id>];\nGO\n```\n\n----------------------------------------\n\nTITLE: Adding Version Label to CPU Metric\nDESCRIPTION: Adds a version label with OpenTelemetry collector version to system.cpu.usage metric.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: system.cpu.usage\naction: update\noperations:\n    - action: add_label\n      new_label: version\n      new_value: opentelemetry collector {{version}}\n```\n\n----------------------------------------\n\nTITLE: Basic SNMP Monitor Configuration in YAML\nDESCRIPTION: Basic configuration example showing how to activate the SNMP integration in the Collector configuration\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/snmp.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/snmp:\n    type: telegraf/snmp\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Exporter with gRPC Client Settings in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure an OTLP exporter within the OpenTelemetry Collector, specifying gRPC client settings. It sets the target `endpoint`, configures `auth` using an extension, defines `tls` settings by referencing certificate and key files, and adds custom request `headers`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/common-config/collector-common-config-grpc.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n   otlp:\n      endpoint: otelcol2:55690\n      auth:\n         authenticator: some-authenticator-extension\n      tls:\n         ca_file: ca.pem\n         cert_file: cert.pem\n         key_file: key.pem\n      headers:\n         test1: \"value1\"\n         \"test 2\": \"value 2\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk HEC Exporter for Metrics\nDESCRIPTION: Configuration for the splunk_hec exporter to send metrics to Splunk Cloud Platform or Splunk Enterprise. It specifies the token, endpoint, source, sourcetype, and index settings required for proper data ingestion.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-exporter.rst#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n    splunk_hec/metrics:\n        # Splunk HTTP Event Collector token.\n        token: \"00000000-0000-0000-0000-0000000000000\"\n        # URL to a Splunk instance to send data to.\n        endpoint: \"https://splunk:8088/services/collector\"\n        # Optional Splunk source: https://docs.splunk.com/Splexicon:Source\n        source: \"app\"\n        # Optional Splunk source type: https://docs.splunk.com/Splexicon:Sourcetype\n        sourcetype: \"jvm_metrics\"\n        # Splunk index, optional name of the Splunk index targeted.\n        index: \"metrics\"\n```\n\n----------------------------------------\n\nTITLE: Activating Memory Ballast Extension in YAML Configuration\nDESCRIPTION: This snippet shows how to activate the Memory Ballast extension in the extensions section of the configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/memory-ballast-extension.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  memory_ballast:\n```\n\n----------------------------------------\n\nTITLE: HTTP Monitor Pipeline Configuration\nDESCRIPTION: Demonstrates how to add the HTTP monitor to the metrics pipeline in the collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/http.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/http]\n```\n\n----------------------------------------\n\nTITLE: .NET Application Deployment YAML With OpenTelemetry Instrumentation (linux-musl-x64)\nDESCRIPTION: Kubernetes deployment manifest for a .NET application running on linux-musl-x64 with OpenTelemetry auto-instrumentation enabled through appropriate annotations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-dotnet-app\n  namespace: monitoring\nspec:\ntemplate:\n  metadata:\n    annotations:\n        instrumentation.opentelemetry.io/otel-dotnet-auto-runtime: \"linux-musl-x64\"\n        instrumentation.opentelemetry.io/inject-dotnet: \"monitoring/splunk-otel-collector\"\n  spec:\n    containers:\n    - name: my-dotnet-app\n      image: my-dotnet-app:latest\n```\n\n----------------------------------------\n\nTITLE: Including Simple Prometheus Receiver in Metrics Pipeline\nDESCRIPTION: This YAML snippet shows how to include the Simple Prometheus receiver in the metrics pipeline of the OpenTelemetry Collector's service section. It demonstrates the proper syntax for adding the receiver to the pipeline configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/simple-prometheus-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers:\n        - prometheus_simple/endpointname\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Collector with Fluentd on Windows\nDESCRIPTION: PowerShell command to install the Splunk Distribution of OpenTelemetry Collector with Fluentd enabled for log collection. Sets execution policy to bypass for the process and uses parameters for access token, realm, and enables Fluentd.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/windows-config-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: PowerShell\nCODE:\n```\n& {Set-ExecutionPolicy Bypass -Scope Process -Force; $script = ((New-Object System.Net.WebClient).DownloadString('https://dl.signalfx.com/splunk-otel-collector.ps1')); $params = @{access_token = \"<SPLUNK_ACCESS_TOKEN>\"; realm = \"<SPLUNK_REALM>\"; with_fluentd = 1}; Invoke-Command -ScriptBlock ([scriptblock]::Create(\". {$script} $(&{$args} @params)\"))}\n```\n\n----------------------------------------\n\nTITLE: Defining Kubernetes ConfigMap for Grafana Data Source (YAML)\nDESCRIPTION: Kubernetes manifest (config_basic.yaml) defining a ConfigMap named `splunk-basic-cm`. This ConfigMap contains a Grafana provisioning file (`grafana.yaml`) that automatically configures the Splunk Observability Cloud data source. Requires replacing `<splunk-o11y-realm>` and `<splunk-o11y-access-token>` with actual Splunk O11y credentials.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: splunk-basic-cm\n  namespace: default\ndata:\n  grafana.yaml: |-\n    {\n        \"apiVersion\": 1,\n        \"datasources\": [\n        {\n           \"editable\": true,\n           \"name\": \"Splunk Observability Cloud\",\n           \"type\": \"cisco-splunko11y-datasource\",\n           \"orgId\": 1,\n           \"isDefault\":true,\n           \"jsonData\":{\n              \"realm\":\"<splunk-o11y-realm>\",\n              \"apiKey\":\"<splunk-o11y-access-token>\"\n           },\n           \"version\": 1,\n        }]\n        }\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk Session Recorder via NPM - Shell\nDESCRIPTION: To install the Splunk session recorder for use in a JavaScript/TypeScript project, this shell command uses npm to add '@splunk/otel-web-session-recorder' as a project dependency. Requires npm and Node.js. This step must be run before importing and initializing the package in source code.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/rum-session-replay.rst#2025-04-22_snippet_3\n\nLANGUAGE: html\nCODE:\n```\nnpm install @splunk/otel-web-session-recorder\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Metrics Receiver with TLS Authentication\nDESCRIPTION: Example configuration setting the collection interval to 5 seconds and configuring TLS authentication for the Kafka metrics receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kafkametrics-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kafkametrics:\n    brokers: 10.10.10.10:9092\n    protocol_version: 2.0.0\n    scrapers:\n      - brokers\n      - topics\n      - consumers\n    auth:\n      tls:\n        ca_file: ca.pem\n        cert_file: cert.pem\n        key_file: key.pem\n    collection_interval: 5s\n```\n\n----------------------------------------\n\nTITLE: Queue Monitoring Metrics in OpenTelemetry Collector\nDESCRIPTION: Metrics for monitoring queue capacity and performance in the OpenTelemetry Collector. These track queue size, capacity, and enqueue failures.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/metrics-internal-collector.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\notelcol_exporter_queue_capacity\notelcol_exporter_queue_size\notelcol_exporter_enqueue_failed_spans\notelcol_exporter_enqueue_failed_metric_points\notelcol_exporter_enqueue_failed_log_records\n```\n\n----------------------------------------\n\nTITLE: Combining HTTP Request Metrics\nDESCRIPTION: Combines multiple HTTP request metrics into a single metric with method labels.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: ^Web Service \\(\\*\\)/Total (?P<http_method>.*) Requests$\nmatch_type: regexp\naction: combine\nnew_name: iis.requests\nsubmatch_case: lower\noperations:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Adding Couchbase Receiver to Metrics Pipeline in YAML\nDESCRIPTION: This YAML configuration adds the Couchbase receiver to the metrics pipeline in the OpenTelemetry Collector. It ensures that metrics from the Couchbase integration are processed.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/couchbase.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n pipelines:\n   metrics:\n     receivers: [smartagent/couchbase]\n```\n\n----------------------------------------\n\nTITLE: Applying Terraform Configuration\nDESCRIPTION: This command applies the configuration plan previously generated, creating the specified resources and updating all states according to the current Terraform file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-datalinks-terraform/apm-create-data-links-terraform-file.rst#2025-04-22_snippet_6\n\nLANGUAGE: none\nCODE:\n```\nterraform apply \"<plan-file-name>\"\n```\n\n----------------------------------------\n\nTITLE: Comprehensive SAPM Exporter Configuration with All Settings in YAML\nDESCRIPTION: This example showcases all available settings for the SAPM exporter, including endpoint, authentication, workers, connections, timeouts, and retry options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-apm-exporter.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  sapm/customname:\n    # Endpoint is the destination to where traces are sent in SAPM format.\n    # The endpoint must be a full URL and include the scheme, port, and path. \n    # For example, https://ingest.us0.signalfx.com/v2/trace\n    endpoint: test-endpoint\n    # Authentication token provided by Splunk Observability Cloud.\n    access_token: abcd1234\n    # Number of workers that should be used to export traces.\n    # The exporter can make as many requests in parallel as the number of workers.\n    num_workers: 3\n    # Used to set a limit to the maximum idle HTTP connections the exporter can keep open.\n    max_connections: 45\n    access_token_passthrough: false\n    # Timeout for every attempt to send data to the back end.\n    # The default value is 5s.\n    timeout: 10s\n    sending_queue:\n      enabled: true\n      num_consumers: 2\n      queue_size: 10\n    retry_on_failure:\n      enabled: true\n      initial_interval: 10s\n      max_interval: 60s\n      max_elapsed_time: 10m\n\nservice:\n  pipelines:\n    traces:\n        receivers: [nop]\n        processors: [nop]\n        exporters: [sapm]\n```\n\n----------------------------------------\n\nTITLE: Adding Splunk HEC Exporter to Services Pipeline in YAML\nDESCRIPTION: This configuration snippet demonstrates how to add the Splunk HEC exporter to the services section of the configuration file, specifically for the logs pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-exporter.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  # ...\n  pipelines:\n    logs:\n      receivers: [fluentforward, otlp]\n      processors:\n      - memory_limiter\n      - batch\n      - resourcedetection\n      exporters: [splunk_hec]\n```\n\n----------------------------------------\n\nTITLE: Querying Service Calls by Name using SignalFlow - None\nDESCRIPTION: This SignalFlow query counts the number of calls for a specific service, dynamically using a dashboard variable ('$service') as the value for 'sf_service'. The count is published as 'OK'. It requires the Grafana variable to be properly configured and passed. Input is the 'spans' metric, filtered by the selected service; output is a count labeled 'OK'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/grafana-create-queries.rst#2025-04-22_snippet_1\n\nLANGUAGE: none\nCODE:\n```\nA = histogram('spans', filter=filter('sf_service', '$service')).count().publish(label='OK')\n```\n\n----------------------------------------\n\nTITLE: Configuring Extended Stats Aggregation in Elasticsearch\nDESCRIPTION: This JSON snippet demonstrates how to set up an extended stats aggregation in Elasticsearch using the 'terms' bucket aggregation and 'extended_stats' metric aggregation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/elasticsearch-query.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n\"aggs\":{\n\"host\" : {\n    \"terms\":{\"field\" : \"host\"},\n    \"aggs\": {\n    \"cpu_usage_stats\": {\n        \"extended_stats\": {\n        \"field\": \"cpu_utilization\"\n        }\n    }\n    }\n}\n}\n}\n```\n\n----------------------------------------\n\nTITLE: ECS Observer Configuration\nDESCRIPTION: YAML configuration for the ECS Observer extension to discover Prometheus targets in ECS tasks.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-fargate.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  ecs_observer:\n    refresh_interval: 10s\n    cluster_name: 'lorem-ipsum-cluster'\n    cluster_region: 'us-west-2'\n    result_file: '/etc/ecs_sd_targets.yaml'\n    task_definitions:\n      - arn_pattern: \"^arn:aws:ecs:us-west-2:906383545488:task-definition/lorem-ipsum-task:[0-9]+$\"\n        metrics_ports: [9113]\n        metrics_path: /metrics\nreceivers:\n  prometheus:\n    config:\n      scrape_configs:\n        - job_name: 'lorem-ipsum-nginx'\n          scrape_interval: 10s\n          file_sd_configs:\n            - files:\n                - '/etc/ecs_sd_targets.yaml'\nprocessors:\n  batch:\n  resourcedetection:\n    detectors: [ecs]\n    override: false    \nexporters:\n  signalfx:\n    access_token: ${SPLUNK_ACCESS_TOKEN}\n    realm: ${SPLUNK_REALM}\nservice:\n  extensions: [ecs_observer]\n  pipelines:\n    metrics:\n      receivers: [prometheus]\n      processors: [batch, resourcedetection]\n      exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Adding Net-IO Monitor to Metrics Pipeline\nDESCRIPTION: Configuration snippet showing how to add the net-io monitor to the metrics pipeline receivers section of the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/net-io.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n pipelines:\n   metrics:\n     receivers: [net-io]\n```\n\n----------------------------------------\n\nTITLE: Configuring Distribution Parameters in YAML\nDESCRIPTION: Example YAML configuration for setting distribution parameters in a values file, showing how to specify GKE as the distribution type along with access token, realm and cluster name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkObservability:\n  accessToken: xxxxxx\n  realm: us0\nclusterName: my-k8s-cluster\ndistribution: gke\n```\n\n----------------------------------------\n\nTITLE: Retrieving Kubernetes Node and Container Runtime Information\nDESCRIPTION: This command displays information about Kubernetes nodes including version and container runtime details, helping identify potential compatibility issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-container.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl get nodes -o wide\nNAME         STATUS   VERSION   CONTAINER-RUNTIME\nnode-1       Ready    v1.19.6   containerd://1.4.1\n```\n\n----------------------------------------\n\nTITLE: Adding OpenTelemetry Extension Configuration to php.ini - Windows (INI)\nDESCRIPTION: This php.ini snippet loads the OpenTelemetry DLL extension on Windows installations, which must be placed in the directory specified by 'extension_dir'. The DLL should have been downloaded as per previous instructions. This setting enables the extension for automatic instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/instrument-php-application.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[opentelemetry]\\nextension=php_opentelemetry.dll\n```\n\n----------------------------------------\n\nTITLE: SQL Monitor Configuration Example for Postgres (YAML)\nDESCRIPTION: This example demonstrates a complete SQL monitor configuration for a Postgres database. It includes connection details, query definition, and metric specifications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/sql.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/sql:\n    type: sql\n    host: localhost\n    port: 5432\n    dbDriver: postgres\n    params:\n      user: \"${env:SQL_USERNAME}\"\n      password: \"${env:SQL_PASSWORD}\"\n    connectionString: 'host={{.host}} port={{.port}} dbname=main user={{.user}} password={{.password}} sslmode=disable'\n    queries:\n      - query: 'SELECT COUNT(*) as count, country, status FROM customers GROUP BY country, status;'\n        metrics:\n          - metricName: \"customers\"\n            valueColumn: \"count\"\n            dimensionColumns: [\"country\", \"status\"]\n```\n\n----------------------------------------\n\nTITLE: Granting Permissions for Splunk OTel Java Agent in JSM Policy (Java)\nDESCRIPTION: Provides a Java code snippet to be added to the Java Security Manager (JSM) policy file. This grants 'AllPermission' to the Splunk OpenTelemetry Java agent JAR file, resolving 'AccessControlException' errors. Replace '<path to splunk-otel-java.jar>' with the actual path to the agent JAR.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/common-java-troubleshooting.rst#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\ngrant codeBase \"file:<path to splunk-otel-java.jar>\" {\n   permission java.security.AllPermission;\n};\n```\n\n----------------------------------------\n\nTITLE: Node Down Trigger Payload\nDESCRIPTION: JSON payload template for triggering node down alerts in SolarWinds. Contains critical message type and node status information.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/solarwinds-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message_type\":\"CRITICAL\",\n  \"monitor_name\":\"SolarWinds\",\n  \"monitoring_tool\":\"SolarWinds\",\n  \"alert_rule\":\"${N=Alerting;M=AlertName}\",\n  \"state_message\":\"${NodeName} is ${Status}\",\n  \"entity_display_name\":\"${NodeName} is ${Status}\",\n  \"entity_id\":\"${N=Alerting;M=AlertObjectID}\",\n  \"host_name\":\"${NodeName}\",\n  \"ip_address\":\"${Node.IP_Address}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting B3 Multi Propagator for Windows\nDESCRIPTION: PowerShell command to set the B3 multi propagator for backward compatibility with SignalFx Python Tracing Library on Windows systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/configuration/advanced-python-otel-configuration.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_PROPAGATORS=b3multi\n```\n\n----------------------------------------\n\nTITLE: Configuring Non-Root User Mode\nDESCRIPTION: Configuration for running the OpenTelemetry Collector container with non-root user permissions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  securityContext:\n    runAsUser: 20000\n    runAsGroup: 20000\n```\n\n----------------------------------------\n\nTITLE: Inviting SignalFx App to Private Slack Channel using Slash Command (Shell)\nDESCRIPTION: Provides the Slack slash command required to invite the 'SignalFx' app into a private channel. This step is necessary before Splunk Observability Cloud can send alert notifications to that specific private channel. The command format includes specifying your organization's realm (e.g., US1), except for the US0 realm where the realm parameter is omitted. Replace `<YOUR_ORG_REALM>` with your specific Splunk Observability Cloud realm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/notif-services/slack.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n/invite @SignalFx, <YOUR_ORG_REALM>\n```\n\nLANGUAGE: shell\nCODE:\n```\n.. note:: For US0 realm, the invitation command is ``/invite @SignalFx``.\n```\n\n----------------------------------------\n\nTITLE: Formatting Links in Markdown for Splunk Landing Page Text\nDESCRIPTION: This snippet demonstrates how to create hyperlinks using Markdown for the customizable text area on a team's landing page in Splunk Observability Cloud. No dependencies are required beyond basic Markdown support within the UI. Usage involves placing the target text and URL inside the Markdown brackets as shown; the result is an inline clickable link where \\\"link\\\" is the label and \\\"url\\\" is the destination.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/user-management/teams/configure-page.rst#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n`[link](url)`\n```\n\n----------------------------------------\n\nTITLE: Default Metric Exclusion List in SignalFx Exporter YAML\nDESCRIPTION: List of metrics excluded by default in the SignalFx exporter, including CPU, disk, memory, filesystem, network, and Kubernetes metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-exporter.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nexclude_metrics:\n\n# Metrics in SignalFx Agent Format\n- metric_names:\n  # CPU metrics.\n  - cpu.interrupt\n  - cpu.nice\n  - cpu.softirq\n  - cpu.steal\n  - cpu.system\n  - cpu.user\n  - cpu.utilization_per_core\n  - cpu.wait\n\n  # Disk-IO metrics\n  - disk_ops.pending\n\n  # Virtual memory metrics\n  - vmpage_io.memory.in\n  - vmpage_io.memory.out\n\n# Metrics in OpenTelemetry Convention\n\n# CPU Metrics\n- metric_name: system.cpu.time\n  dimensions:\n    state: [idle, interrupt, nice, softirq, steal, system, user, wait]\n\n- metric_name: cpu.idle\n  dimensions:\n    cpu: [\"*\"]\n\n# Memory metrics\n- metric_name: system.memory.usage\n  dimensions:\n    state: [inactive]\n\n# Filesystem metrics\n- metric_name: system.filesystem.usage\n  dimensions:\n    state: [reserved]\n- metric_name: system.filesystem.inodes.usage\n\n# Disk-IO metrics\n- metric_names:\n  - system.disk.merged\n  - system.disk.io\n  - system.disk.time\n  - system.disk.io_time\n  - system.disk.operation_time\n  - system.disk.pending_operations\n  - system.disk.weighted_io_time\n\n# Network-IO metrics\n- metric_names:\n  - system.network.packets\n  - system.network.dropped\n  - system.network.tcp_connections\n  - system.network.connections\n\n# Processes metrics\n- metric_names:\n  - system.processes.count\n  - system.processes.created\n\n# Virtual memory metrics\n- metric_names:\n  - system.paging.faults\n  - system.paging.usage\n- metric_name: system.paging.operations\n  dimensions:\n    type: [minor]\n\n k8s metrics\n- metric_names:\n  - k8s.cronjob.active_jobs\n  - k8s.job.active_pods\n  - k8s.job.desired_successful_pods\n  - k8s.job.failed_pods\n  - k8s.job.max_parallel_pods\n  - k8s.job.successful_pods\n  - k8s.statefulset.desired_pods\n  - k8s.statefulset.current_pods\n  - k8s.statefulset.ready_pods\n  - k8s.statefulset.updated_pods\n  - k8s.hpa.max_replicas\n  - k8s.hpa.min_replicas\n  - k8s.hpa.current_replicas\n  - k8s.hpa.desired_replicas\n\n  # matches all container limit metrics but k8s.container.cpu_limit and k8s.container.memory_limit\n  - /^k8s\\.container\\..+_limit$/\n  - '!k8s.container.memory_limit'\n - '!k8s.container.cpu_limit'\n\n  # matches all container request metrics but k8s.container.cpu_request and k8s.container.memory_request\n  - /^k8s\\.container\\..+_request$/\n  - '!k8s.container.memory_request'\n  - '!k8s.container.cpu_request'\n\n  # matches any node condition but k8s.node.condition_ready\n  - /^k8s\\.node\\.condition_.+$/\n  - '!k8s.node.condition_ready'\n\n  # kubelet metrics\n  # matches (container|k8s.node|k8s.pod).memory...\n  - /^(?i:(container)|(k8s\\.node)|(k8s\\.pod))\\.memory\\.available$/\n  - /^(?i:(container)|(k8s\\.node)|(k8s\\.pod))\\.memory\\.major_page_faults$/\n  - /^(?i:(container)|(k8s\\.node)|(k8s\\.pod))\\.memory\\.page_faults$/\n  - /^(?i:(container)|(k8s\\.node)|(k8s\\.pod))\\.memory\\.rss$/\n  - /^(?i:(k8s\\.node)|(k8s\\.pod))\\.memory\\.usage$/\n  - /^(?i:(container)|(k8s\\.node)|(k8s\\.pod))\\.memory\\.working_set$/\n\n  # matches (k8s.node|k8s.pod).filesystem...\n  - /^k8s\\.(?i:(node)|(pod))\\.filesystem\\.available$/\n  - /^k8s\\.(?i:(node)|(pod))\\.filesystem\\.capacity$/\n  - /^k8s\\.(?i:(node)|(pod))\\.filesystem\\.usage$/\n\n  # matches (k8s.node|k8s.pod).cpu.time\n  - /^k8s\\.(?i:(node)|(pod))\\.cpu\\.time$/\n\n  # matches (container|k8s.node|k8s.pod).cpu.utilization\n  - /^(?i:(container)|(k8s\\.node)|(k8s\\.pod))\\.cpu\\.utilization$/\n\n  # matches k8s.node.network.io and k8s.node.network.errors\n  - /^k8s\\.node\\.network\\.(?:(io)|(errors))$/\n\n  # matches k8s.volume.inodes, k8s.volume.inodes and k8s.volume.inodes.used\n  - /^k8s\\.volume\\.inodes(\\.free|\\.used)*$/\n```\n\n----------------------------------------\n\nTITLE: Complete Logparser Pipeline Configuration\nDESCRIPTION: Example showing a complete logs pipeline configuration including the Logparser receiver, resource detection processor, and SignalFx exporter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/logparser.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    logs:\n      receivers:\n        - smartagent/logparser\n      processors:\n        - resourcedetection\n      exporters:\n        - signalfx\n```\n\n----------------------------------------\n\nTITLE: Required Log Fields for Splunk Related Content\nDESCRIPTION: Specifies the essential log fields required to enable the Related Content feature in Splunk Log Observer. Presence of `host.name`, `service.name`, `span_id`, or `trace_id` allows linking logs to APM traces and Infrastructure Monitoring entities. Fields may need remapping if non-standard names are used.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/relatedcontent.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n- ``host.name``\n- ``service.name``\n- ``span_id``\n- ``trace_id``\n```\n\n----------------------------------------\n\nTITLE: Configuring Filelog Receiver with Splunk Otel Collector - YAML\nDESCRIPTION: This YAML code snippet demonstrates how to set up the Filelog receiver in a Splunk Otel Collector configuration. It specifies collector receivers, including log source definitions, log processing options, and mappings for sourcetypes tailored for proper ingestion into Splunk. Prerequisites include an existing Splunk Otel Collector deployment. Key parameters may define log file paths, start positions (such as 'start_at'), and routing settings. Inputs are the YAML config options; the output is a functioning collector reading logs as expected by Splunk. The configuration must be saved as a YAML file and requires proper permissions to access log sources.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filelog-receiver.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n## Example YAML configuration for Splunk Otel Collector\n## for ingesting logs via Filelog receiver with sourcetype mapping.\nreceivers:\n  filelog:\n    include:\n      - /path/to/logs/*.log  # Path to monitored log files\n    start_at: end            # 'end' means only process new lines\n    operators:\n      - type: json_parser\n      # Additional pipeline operators as needed...\n    attributes:\n      sourcetype: my_custom_sourcetype\n\nprocessors:\n  batch:\n\nexporters:\n  splunk_hec:\n    token: YOUR_SPLUNK_HEC_TOKEN\n    endpoint: https://splunk-hec.example.com:8088\n    source: filelog\n\nservice:\n  pipelines:\n    logs:\n      receivers: [filelog]\n      processors: [batch]\n      exporters: [splunk_hec]\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Container Stats Receiver in OpenTelemetry Collector\nDESCRIPTION: YAML configuration snippet for activating the Docker container stats integration in the Splunk OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/docker.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/docker-container-stats:\n    type: docker-container-stats\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Basic Expvar Receiver Configuration in YAML\nDESCRIPTION: Example showing how to activate the Expvar integration in the Collector configuration by adding the receiver and including it in the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/expvar.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/expvar:\n    type: expvar\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Example of Mixed Metric Configuration - Splunk O11y YAML\nDESCRIPTION: This YAML example shows a combination of metrics being enabled and disabled to illustrate real-world utilization scenarios. It also highlights the evaluation of the `disableMetrics` flag before the `enableMetrics`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  disableMetrics:\n    - http.all\n    - tcp.syn_timeouts\n    - tcp.new_sockets\n    - tcp.resets\n    - udp.bytes\n    - udp.packets\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  enableMetrics:\n    - http.all\n    - ebpf_net.codetiming_min_ns\n    - ebpf_net.entrypoint_info\n```\n\n----------------------------------------\n\nTITLE: Configuring Collectd df Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration to activate the Collectd df monitor in the collector configuration. Shows how to define the receiver and add it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/collectd-df.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/collectd/df:\n    type: collectd/df\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Alert Template JSON for Dotcom Monitor Integration with Splunk On-Call\nDESCRIPTION: JSON configurations for Error Source, Test Source, and Uptime Source fields in the Dotcom monitor Alert Template. These JSON structures define the message format for different alert types.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/dotcommonitor-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"message_type\":\"critical\",\"monitoring_tool\":\"Dotcom-Monitor\",\"state_message\":\"Error occurred during the device monitoring at <%Monitor_DateTime%> Monitoring location:<%Location%>\",\"entity_id\":\"<%Site_Name%>\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"message_type\":\"critical\",\"monitoring_tool\":\"Dotcom-Monitor\",\"state_message\":\"Test message\",\"entity_id\":\"Test message\"}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"message_type\":\"recovery\",\"monitoring_tool\":\"Dotcom-Monitor\",\"state_message\":\"Device <%Site_Name%> detected to be back online at <%Monitor_DateTime%> from monitoring location: <%Location%>\",\"entity_id\":\"<%Site_Name%>\"}\n```\n\n----------------------------------------\n\nTITLE: Running Interactive Bash Shell in Splunk OpenTelemetry Collector Container\nDESCRIPTION: Docker command to run an interactive bash shell in the Splunk OpenTelemetry Collector container for checking the Collector status.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it containerID bash\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector with Custom Repository Settings\nDESCRIPTION: This bash command shows how to install the Splunk OpenTelemetry Collector while skipping the creation of default repository files. It uses the --skip-collector-repo and --skip-fluentd-repo options to use configured repos on the target system.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --realm $SPLUNK_REALM --skip-collector-repo --skip-fluentd-repo \\\n -- $SPLUNK_ACCESS_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Retrieving OTP from SMS via XMLHttpRequest - JavaScript\nDESCRIPTION: This JavaScript snippet synchronously retrieves an OTP from a virtual phone number's SMS service by sending a GET request to a user-specified URL, and extracts a 6-digit code from the first inbound message using a regular expression. Requires access to a server endpoint that returns recent SMS messages in JSON format. Input parameters include the SMS service URL. Upon a successful response, the script returns the extracted OTP for later use in browser test steps. Dependencies include an accessible endpoint (URL) returning a JSON object with an 'inbounds' array and 'body' text fields; script must run in environments permitting synchronous XMLHttpRequest.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/auth.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nfunction getOtp() {\n  const url = \"https://your-page.example.com/sms\";\n  var request = new XMLHttpRequest();\n  request.open(\"GET\", url, false);\n  request.send();\n  if (request.status == 200) {\n    return parseOtp(JSON.parse(request.responseText));\n  }\n  return;\n}\n\nfunction parseOtp(jsonResponse) {\n  const firstInbound = jsonResponse.inbounds[0];\n  if (firstInbound && firstInbound.body) {\n    // Extract the number using a regular expression\n    const match = firstInbound.body.match(/\\b\\d{6}\\b/);\n    if (match) {\n      return match[0]; // Return the first matched number\n    }\n  }\n  return;\n}\nreturn getOtp();\n```\n\n----------------------------------------\n\nTITLE: Setting OTLP Endpoint via Environment Variable in Linux Shell\nDESCRIPTION: Demonstrates how to configure the OTLP exporter endpoint using the `export` command in a Linux shell environment. This environment variable tells the Go instrumentation where to send OTLP data, typically to an OpenTelemetry Collector or directly to Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/configuration/advanced-go-otel-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317\n```\n\n----------------------------------------\n\nTITLE: Adding OpenTelemetry Collector Helm Repository\nDESCRIPTION: Commands to add the Splunk OpenTelemetry Collector Helm chart repository, with an option to update simultaneously.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart\n\n# Use these two commands together to add and update the repository at the same time\nhelm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart && helm repo update\n```\n\n----------------------------------------\n\nTITLE: Adding Python User Base Directory to PATH in Shell\nDESCRIPTION: Shows how to add the Python user base directory to the PATH environment variable when pip installs packages into the user local environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/troubleshooting/common-python-troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport PATH=\"<user-base-path>:$PATH\"\n```\n\n----------------------------------------\n\nTITLE: Setting Service Name with OpenTelemetry C++ via Environment Variable\nDESCRIPTION: This Bash shell snippet demonstrates replacing code-level service name assignment with an environment variable OTEL_SERVICE_NAME before running a C++ application instrumented with OpenTelemetry. The export statement sets the logical service name for telemetry. This approach is the recommended method for configuration with upstream OpenTelemetry. Prerequisites: OpenTelemetry C++ instrumentation. Key parameter: OTEL_SERVICE_NAME environment variable. Input: None directly; affects all subsequently launched processes. Output: Telemetry exported with specified logical service name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/cpp/migrate-from-splunk-cpp.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_SERVICE_NAME=\"my-service\"\n```\n\n----------------------------------------\n\nTITLE: Adding Protocols Monitor to Service Pipeline\nDESCRIPTION: Configuration snippet showing how to add the protocols monitor to the metrics pipeline in the service configuration section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/network-protocols.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/protocols]\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OpenTelemetry Collector in Gateway Mode with SignalFx Exporter\nDESCRIPTION: This configuration example shows how to set up the Splunk OpenTelemetry Collector in gateway mode using the SignalFx exporter for both traces and metrics. It disables aggregation at the gateway by setting empty translation_rules and exclude_metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/relatedcontent-collector-apm.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  # Traces\n  otlphttp:\n    access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n    endpoint: \"https://ingest.${SPLUNK_REALM}.signalfx.com/v2/trace/otlp\"\n  # Metrics + Events\n  signalfx:\n    access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n    realm: \"${SPLUNK_REALM}\"\n    translation_rules: []\n    exclude_metrics: []\n\nservice:\n  extensions: [http_forwarder]\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors:\n      - memory_limiter\n      - batch\n      exporters: [otlphttp]\n    metrics:\n      receivers: [signalfx]\n      processors: [memory_limiter, batch]\n      exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Receivers for AppMesh\nDESCRIPTION: Basic configuration for activating the AppMesh integration in the OpenTelemetry Collector using the smartagent receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/aws-appmesh.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/appmesh:\n    type: appmesh\n      ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Custom Discovery Receiver Configuration\nDESCRIPTION: YAML template for creating custom discovery receiver configurations with rules and settings\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-advanced-config.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# <some-receiver-type-with-optional-name.discovery.yaml>\n  <receiver_type>(/<receiver_name>):\n     enabled: <true | false> # true by default\n     rule:\n        <observer_type>(/<observer_name>): <receiver creator rule for this observer>\n     config:\n        default:\n           <default embedded receiver config>\n        <observer_type>(/<observer_name>):\n           <observer-specific config items, merged with `default`>\n     status:\n        metrics:\n           <discovery receiver metric status entries>\n        statements:\n           <discovery receiver statement status entries>\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenTelemetry for In-Process Azure Function in C#\nDESCRIPTION: This code snippet shows how to initialize OpenTelemetry in a .NET 8 in-process Azure function. It defines a startup function that configures the tracer provider, sets up HTTP client and ASP.NET Core instrumentation, and configures the OTLP exporter to send spans to Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/azure/instrument-azure-functions-dotnet.rst#2025-04-22_snippet_1\n\nLANGUAGE: csharp\nCODE:\n```\nusing Microsoft.Azure.Functions.Extensions.DependencyInjection;\nusing Microsoft.Extensions.DependencyInjection;\nusing OpenTelemetry;\nusing OpenTelemetry.Exporter;\nusing OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\nusing System;\nusing System.Collections.Generic;\n\n// Decorate assembly with startup function\n[assembly: FunctionsStartup(typeof(OtelManualExample.Startup))]\n\nnamespace OtelManualExample\n{\n   public class Startup : FunctionsStartup\n   {\n      public override void Configure(IFunctionsHostBuilder builder)\n      {\n         // Get environment variables from function configuration\n         // You need a valid Splunk Observability Cloud access token and realm\n         var serviceName = Environment.GetEnvironmentVariable(\"WEBSITE_SITE_NAME\") ?? \"Unknown\";\n         var accessToken = Environment.GetEnvironmentVariable(\"SPLUNK_ACCESS_TOKEN\")?.Trim();\n         var realm = Environment.GetEnvironmentVariable(\"SPLUNK_REALM\")?.Trim();\n\n         ArgumentNullException.ThrowIfNull(accessToken, \"SPLUNK_ACCESS_TOKEN\");\n         ArgumentNullException.ThrowIfNull(realm, \"SPLUNK_REALM\");\n\n         var tp = Sdk.CreateTracerProviderBuilder()\n            // Use Add[instrumentation-name]Instrumentation to instrument missing services\n            // Use Nuget to find different instrumentation libraries\n            .AddHttpClientInstrumentation(opts =>\n            {\n                // This filter prevents background (parent-less) http client activity\n                opts.FilterHttpRequestMessage = req => Activity.Current?.Parent != null;\n                opts.FilterHttpWebRequest = req => Activity.Current?.Parent != null;\n            })\n            .AddAspNetCoreInstrumentation()\n            // Use AddSource to add your custom DiagnosticSource source names\n            //.AddSource(\"My.Source.Name\")\n            .SetSampler(new AlwaysOnSampler())\n            // Add resource attributes to all spans\n            .SetResourceBuilder(\n               ResourceBuilder.CreateDefault()\n               .AddService(serviceName: serviceName, serviceVersion: \"1.0.0\")\n               .AddAzureAppServiceDetector()\n               .AddAttributes(new Dictionary<string, object>() {\n                  { \"faas.instance\", Environment.GetEnvironmentVariable(\"WEBSITE_INSTANCE_ID\") }\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug Log Level for OpenTelemetry Collector\nDESCRIPTION: YAML configuration snippet for setting the log level to debug in the OpenTelemetry Collector telemetry service to aid in troubleshooting during migration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n   telemetry:\n      logs:\n         level: debug\n```\n\n----------------------------------------\n\nTITLE: Configuring Span Metrics Connector in YAML\nDESCRIPTION: Sample YAML configuration for activating the Span Metrics connector in the OpenTelemetry Collector. It includes settings for histogram buckets, dimensions, and other options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/span-metrics-connector.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  nop:\n\nexporters:\n  nop:\n\nconnectors:\n  spanmetrics:\n    histogram:\n      explicit:\n        buckets: [100us, 1ms, 2ms, 6ms, 10ms, 100ms, 250ms]\n    dimensions:\n      - name: http.method\n        default: GET\n      - name: http.status_code\n    exemplars:\n      enabled: true\n    exclude_dimensions: ['status.code']\n    dimensions_cache_size: 1000\n    aggregation_temporality: \"AGGREGATION_TEMPORALITY_CUMULATIVE\"    \n    metrics_flush_interval: 15s\n    metrics_expiration: 5m\n    events:\n      enabled: true\n      dimensions:\n        - name: exception.type\n        - name: exception.message\n    resource_metrics_key_attributes:\n      - service.name\n      - telemetry.sdk.language\n      - telemetry.sdk.name\n```\n\n----------------------------------------\n\nTITLE: Basic Kubernetes API Server Configuration\nDESCRIPTION: Basic configuration to activate the kubernetes-apiserver integration in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-apiserver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/kubernetes-apiserver:\n    type: kubernetes-apiserver\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring OTEL Collector Attributes Processor (Upsert)\nDESCRIPTION: YAML configuration snippet for the Splunk Distribution of OpenTelemetry Collector. It defines an `attributes` processor named 'settenant' using the 'upsert' action. This copies the value from an existing attribute 'myTenant' to a new attribute 'tenant', creating or overwriting the 'tenant' attribute.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/add-context-trace-span.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  ...\n  attributes/settenant:\n    actions:\n    - key: tenant\n        from_attribute: myTenant\n        action: upsert \n```\n\n----------------------------------------\n\nTITLE: Installing OpenShift Cluster Monitor with Helm\nDESCRIPTION: Helm command to install the Splunk OpenTelemetry Collector with OpenShift-specific configuration. The command sets various parameters including cloud provider, distribution type, access token, cluster name, namespace, and realm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/openshift-cluster.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhelm install --set cloudProvider=' ' --set distribution='openshift' --set splunkObservability.accessToken='******' --set clusterName='cluster1' --namespace='namespace1' --set splunkObservability.realm='us0' --set gateway.enabled='false' --generate-name splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Metrics YAML Configuration for Kubernetes Controller Manager\nDESCRIPTION: Retrieves metrics configuration from a YAML file that defines available metrics for the Kubernetes Controller Manager monitor.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-controller-manager.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"metrics-yaml\" url=\"https://raw.githubusercontent.com/signalfx/splunk-otel-collector/main/internal/signalfx-agent/pkg/monitors/kubernetes/controllermanager/metadata.yaml\"></div>\n```\n\n----------------------------------------\n\nTITLE: Using Wildcard Filter in SignalFlow Data Selection - SignalFlow Language\nDESCRIPTION: This pseudo-SignalFlow snippet demonstrates how to use a filter with a wildcard in the data selection function. The filter limits incoming timeseries data to hosts matching the wildcard 'kafka*east', and such usage counts as one wildcard toward the maximum wildcards per filter function limit. Exceeding this limit causes program validation errors before execution.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_1\n\nLANGUAGE: SignalFlow\nCODE:\n```\ndata('jvm.load', filter=filter('host', 'kafka*east'))\n```\n\n----------------------------------------\n\nTITLE: Configuring ECS Metadata Receiver in OpenTelemetry Collector (YAML)\nDESCRIPTION: This snippet shows how to activate the ECS metadata integration by adding the necessary configuration to the OpenTelemetry Collector. It includes setting up the receiver and adding it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/amazon-ecs-metadata.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/ecs-metadata:\n    type: ecs-metadata\n    ...  # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/ecs-metadata]\n```\n\n----------------------------------------\n\nTITLE: Creating Webhook Payload for Coralogix-Splunk On-Call Integration\nDESCRIPTION: This JSON payload is used to configure the webhook in Coralogix for integration with Splunk On-Call. It defines the structure of the alert information sent from Coralogix to Splunk On-Call, including various fields like alert severity, description, and relevant URLs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/coralogix-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"message_type\": \"CRITICAL\", \"entity_id\": \"$ALERT_ID\",\n\"entity_display_name\": \"$ALERT_NAME\",\n\"alert_severity\": \"$EVENT_SEVERITY\",\n\"state_message\": \"$LOG_TEXT\",\n\"description\": \"$ALERT_DESCRIPTION\",\n\"alert_action\": \"$ALERT_ACTION\", \"alert_url\": \"$ALERT_URL\",\n\"log_url\": \"$LOG_URL\", \"monitoring_tool\": \"Coralogix\",\n\"team\": \"$TEAM_NAME\", \"application\": \"$APPLICATION_NAME\",\n\"subsystem\": \"$SUBSYSTEM_NAME\", \"ipAddress\": \"$IP_ADDRESS\",\n\"timestamp\": \"$EVENT_TIMESTAMP\", \"hitCount\": \"$HIT_COUNT\" }\n```\n\n----------------------------------------\n\nTITLE: Checking OpenTelemetry Collector Service Status\nDESCRIPTION: Check the status of the OpenTelemetry Collector service on the host. This command is only available for Linux and uses systemctl to retrieve the service status.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl status splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Pipeline in OpenTelemetry Collector\nDESCRIPTION: YAML configuration for setting up the metrics pipeline in the OpenTelemetry Collector with receivers, processors, and exporters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/other-ingestion-methods/send-custom-metrics.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  receivers: [ prometheus/custom, otlp, ... ]\n  processors: [ memory_limiter, batch, resourcedetection ]\n  exporters: [ signalfx ]\n```\n\n----------------------------------------\n\nTITLE: Dimension Example in RST\nDESCRIPTION: Example showing dimension values for metrics in Splunk Synthetic Monitoring\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/browser-test/browser-test-metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nsuccess - true if the run succeeds; false if it fails\\nlocation_id - The ID of the location for this run\\ntest_id - The ID of this test\\ntest_type - The test type dimension for Browser tests is set to browser\\ntest - The test property is the human readable name of the test_id\\nlocation - The location property is the human readable name of the location_id\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Receiver in OpenTelemetry Collector\nDESCRIPTION: This YAML snippet demonstrates how to configure the MongoDB receiver within the OpenTelemetry Collector's `receivers` section. It specifies connection details like hosts (endpoint and transport), authentication credentials (username, password using an environment variable), data collection frequency (`collection_interval`), initial startup delay (`initial_delay`), and basic TLS settings (disabling security checks for this example). This configuration block defines how the Collector connects to and interacts with the MongoDB instance.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/mongodb-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  mongodb:\n    hosts:\n      - endpoint: localhost:27017\n        transport: tcp\n    username: otel\n    password: ${env:MONGODB_PASSWORD}\n    collection_interval: 60s\n    initial_delay: 1s\n    tls:\n      insecure: true\n      insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Configuring 15% Log Sampling by Trace ID in OpenTelemetry Collector\nDESCRIPTION: This YAML configuration sets up the probabilistic sampler processor to sample 15% of log records according to their trace ID using the OpenTelemetry specification.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/probabilistic-sampler-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  probabilistic_sampler:\n    sampling_percentage: 15\n```\n\n----------------------------------------\n\nTITLE: Validation Output for Splunk OpenTelemetry Collector Configuration\nDESCRIPTION: Sample output from a successful configuration validation showing environment variables being set and memory allocation for the Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-start.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n2024/02/19 16:28:44 settings.go:479: Set config to [sample.yaml]\n2024/02/19 16:28:44 settings.go:532: Set ballast to 168 MiB\n2024/02/19 16:28:44 settings.go:548: Set memory limit to 460 MiB\n2024/02/19 16:28:44 settings.go:415: set \"SPLUNK_LISTEN_INTERFACE\" to \"0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Modifying Reducer Footprint\nDESCRIPTION: This example configures the reducer to handle telemetry data by using multiple shards per stage. Each stage, namely ingest, matching, and aggregation, is set to utilize 4 shards to expand processing capacity.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n   reducer:\n     ingestShards: 4\n     matchingShards: 4\n     aggregationShards: 4\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Splunk Observability Cloud Export\nDESCRIPTION: Sets environment variables to configure the OTLP exporter for sending data directly to Splunk Observability Cloud. This includes setting the endpoint, protocol, and authentication token.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/instrument-ruby.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_EXPORTER_OTLP_ENDPOINT=\"http://ingest.<realm>.signalfx.com\"\nexport OTEL_EXPORTER_OTLP_PROTOCOL=\"grpc\"\nexport OTEL_EXPORTER_OTLP_TRACES_HEADERS=\"x-sf-token=<access_token>\"\n```\n\n----------------------------------------\n\nTITLE: Including Batch Processor in Service Pipelines YAML\nDESCRIPTION: This code demonstrates how to include the batch processor in all pipelines of the service section in the configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/batch-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      processors: [batch]\n    logs:\n      processors: [batch]\n    traces:\n      processors: [batch]\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Operator for Splunk APM Tracing (YAML)\nDESCRIPTION: This YAML configuration defines an `IstioOperator` resource (saved as `tracing.yaml`) to configure the Istio mesh for tracing with Splunk APM. It enables tracing, sets the Zipkin exporter address to the Splunk OpenTelemetry Collector (`$(HOST_IP):9411`), configures sampling rate (100%), maximum tag length, and adds a custom `environment.deployment` tag. Access logs are directed to stdout for potential collection by Splunk Log Observer.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/istio/istio.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# tracing.yaml\n\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nmetadata:\n  name: istio-operator\nspec:\n  meshConfig:\n     # Requires Splunk Log Observer\n     accessLogFile: /dev/stdout\n     # Requires Splunk APM\n     enableTracing: true\n     defaultConfig:\n        tracing:\n           max_path_tag_length: 99999\n           sampling: 100\n           zipkin:\n              address: $(HOST_IP):9411\n           custom_tags:\n              environment.deployment:\n                 literal:\n                    value: dev\n```\n\n----------------------------------------\n\nTITLE: Adding Image URL Annotation via REST API in JSON\nDESCRIPTION: This snippet illustrates how to add an image URL annotation to an alert using a REST-style integration. It includes standard alert fields and an image URL annotation pointing to a graph.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-annotations.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"monitoring_tool\": \"API\", \"message_type\":\"INFO\",\n\"entity_id\":\"disk.space/db01\", \"entity_display_name\":\"Approaching Low\nDisk Space on DB01\", \"state_message\":\"The disk is really really full.\nHere is a bunch of information about the problem\",\n\"vo_annotate.i.Graph\":\"https://community.iotawatt.com/uploads/db6340/original/1X/266a3917cc86317830ae9cda3e91c7689a6c73a7.png\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring NGINX Server Status Module\nDESCRIPTION: NGINX server configuration to expose status metrics. This configuration enables the stub_status module and restricts access to localhost.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/nginx.rst#2025-04-22_snippet_2\n\nLANGUAGE: nginx\nCODE:\n```\nserver {\n  location /nginx_status {\n    stub_status on;\n    access_log off;\n    allow 127.0.0.1; # The source IP address of OpenTelemetry Collector.\n    deny all;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Successful Helm Chart Installation Output\nDESCRIPTION: Example output message displayed after successfully installing the Splunk OpenTelemetry Collector Helm chart, showing the deployment details.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/collector-config-tutorial-start.rst#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nNAME: splunk-otel-collector-1709226095\nLAST DEPLOYED: Thu Feb 29 18:01:36 2024\nNAMESPACE: default\nSTATUS: deployed\nNOTES:\nSplunk OpenTelemetry Collector is installed and configured to send data to Splunk Observability realm <realm>.\n```\n\n----------------------------------------\n\nTITLE: Configuring DNS Query Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration to activate the DNS Query Input integration in the collector configuration. Defines the receiver type and must be added to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/dns.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/dns:\n    type: telegraf/dns\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Attributes Processor\nDESCRIPTION: YAML configuration showing how to set up the attributes processor to modify span tags. Demonstrates copying from existing keys and adding new environment tags.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/tags.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  # Overrides an existing tag for a span.\n  attributes/copyfromexistingkey:\n    actions:\n      - key: SPAN_TAG_KEY\n        from_attribute: \"SPAN_TAG_VALUE\"\n        action: upsert\n  # Adds a tag to spans without a tag.\n  attributes/newenvironment:\n    actions:\n      - key: SPAN_TAG_KEY\n        value: \"SPAN_TAG_VALUE\"\n        action: insert\n```\n\n----------------------------------------\n\nTITLE: Migrating to OpenTelemetry eBPF Helm Chart - Shell Command\nDESCRIPTION: This snippet provides shell commands to migrate from the `networkExplorer` to the OpenTelemetry Collector eBPF Helm chart. It involves updating Helm chart repositories, installing necessary charts, and configuring the service endpoint. Users need access to Kubernetes and installed Helm CLI.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get svc | grep splunk-otel-collector-gateway\n```\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts\nhelm repo update open-telemetry\nhelm install my-opentelemetry-ebpf -f ./otel-ebpf-values.yaml open-telemetry/opentelemetry-ebpf\n```\n\n----------------------------------------\n\nTITLE: Disabling Database Query Normalization in Splunk APM Java Instrumentation\nDESCRIPTION: Describes how to turn off the default sanitization (masking PII/secrets) of SQL queries captured by Splunk APM Java instrumentation. This is achieved by setting the `otel.instrumentation.common.db-statement-sanitizer.enabled` system property to `false`. Disabling this feature might expose sensitive data in Splunk Observability Cloud and requires careful consideration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/db-query-perf/db-perf-troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\notel.instrumentation.common.db-statement-sanitizer.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Activating SQL Monitor in Collector Configuration (YAML)\nDESCRIPTION: This snippet shows how to activate the SQL monitor by adding it to the Collector configuration file. It includes the basic receiver setup and pipeline configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/sql.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/sql:\n    type: sql\n    ...  # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/sql]\n```\n\n----------------------------------------\n\nTITLE: Pipeline Configuration for Win Performance Counters\nDESCRIPTION: Configuration snippet showing how to add the monitor to the metrics pipeline receivers section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/win_perf_counters.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/telegraf/win_perf_counters]\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Host IP Environment Variable (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define an environment variable named `K8S_NODE_IP` within a Kubernetes pod configuration. It uses `valueFrom` and `fieldRef` to dynamically fetch the IP address of the node (`status.hostIP`) where the pod is running. This variable is intended to be used when setting the `SPLUNK_PROFILER_LOGS_ENDPOINT` for Kubernetes deployments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/profiling/get-data-in-profiling.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n- name: K8S_NODE_IP\n  valueFrom:\n    fieldRef:\n      apiVersion: v1\n      fieldPath: status.hostIP\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Windows Services Monitor - All Services\nDESCRIPTION: Configuration snippet for monitoring all Windows services using the Telegraf win_services receiver in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/win-services.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/win_services:\n    type: telegraf/win_services # monitor all services\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/win_services]\n```\n\n----------------------------------------\n\nTITLE: Customizing Status Mapping with severityDecoder - Handlebars\nDESCRIPTION: Shows how to customize the mapping of a status variable to a severity code using the severityDecoder function. This function supports mapping various string or variable inputs (e.g., ok, Major, default) to numeric severity levels, and can be overridden for specific use cases. Inputs are template context variables, and outputs are severity numbers or custom values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/notif-services/splunkplatform.rst#2025-04-22_snippet_1\n\nLANGUAGE: Handlebars\nCODE:\n```\n``{{{severityDecoder ok='ok' Major='not_ok' default='empty'}}}``\n```\n\n----------------------------------------\n\nTITLE: Setting Cloud Provider with Helm Command\nDESCRIPTION: Command line example for specifying the cloud provider when installing the Collector with Helm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n--set cloudProvider={azure|gcp|eks|openshift} \n```\n\n----------------------------------------\n\nTITLE: Adding Specific Version of Splunk OpenTelemetry Collector Heroku Buildpack\nDESCRIPTION: Command to add a specific version of the Splunk OpenTelemetry Collector Heroku buildpack for production environments by specifying a tag name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/heroku.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nheroku buildpacks:add https://github.com/signalfx/splunk-otel-collector-heroku.git#<TAG_NAME>\n```\n\n----------------------------------------\n\nTITLE: Configuring FQDN Hostname in Universal Forwarder server.conf\nDESCRIPTION: Configuration stanza for server.conf to ensure the Universal Forwarder captures the fully qualified domain name (FQDN) of the host, which is used for host identification in Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-with-the-uf.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n[general]\nhostnameOption = fullyqualifiedname\n```\n\n----------------------------------------\n\nTITLE: Stopping OpenTelemetry Collector Service\nDESCRIPTION: Stop the OpenTelemetry Collector service on the host. This command uses systemctl to manage the service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl stop splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Upgrading OpenTelemetry Collector on Debian-based Linux Systems\nDESCRIPTION: Commands to update the Splunk OpenTelemetry Collector on Debian-based systems using apt-get. Requires root privileges and handles package updates while preserving modified configuration files.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-upgrade.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install --only-upgrade splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Activating MongoDB Receiver in Metrics Pipeline\nDESCRIPTION: This YAML configuration snippet shows how to activate the previously defined MongoDB receiver by adding its identifier (`mongodb`) to the `receivers` array within the `metrics` pipeline under the `service` section of the OpenTelemetry Collector configuration. This ensures that metrics collected by the MongoDB receiver are processed and exported through the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/mongodb-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [mongodb]\n```\n\n----------------------------------------\n\nTITLE: Configuring Runtime Metrics Collection for Java\nDESCRIPTION: YAML configuration for enabling runtime metrics collection in Java applications with endpoint configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-advanced-config.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\noperatorcrds:\n  install: true\noperator:\n  enabled: true\ninstrumentation:  \n  spec:\n    java:\n      env:\n      - name: SPLUNK_METRICS_ENABLED\n        value: \"true\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Metrics Access in GitLab\nDESCRIPTION: Configuration for nginx metrics access control, showing how to allow connections from specific IP ranges\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-gitlab/gitlab.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nserver {\n    ...\n    location /metrics {\n    ...\n    allow 172.17.0.0/16;\n    deny all;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Monitoring Specific Systemd Services\nDESCRIPTION: Advanced YAML configuration example for monitoring specific systemd services (docker and ubuntu-fan) with a 10-second interval.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/systemd.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/systemd:\n    type: collectd/systemd\n    intervalSeconds: 10\n    services:\n      - docker\n      - ubuntu-fan\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Event in JavaScript\nDESCRIPTION: Illustrates how to create a custom event using the OpenTelemetry API in JavaScript.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport {trace} from '@opentelemetry/api'\n\nconst tracer = trace.getTracer('appModuleLoader');\nconst span = tracer.startSpan('test.module.load', {\nattributes: {\n   'workflow.name': 'test.module.load'\n}\n});\n// time passes\nspan.end();\n```\n\n----------------------------------------\n\nTITLE: Configuring Span Naming in YAML\nDESCRIPTION: Configuration for naming a span using attributes and a separator.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/span-processor.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nspan:\n  name:\n    from_attributes: [<key1>, <key2>, ...]\n    separator: <value>\n```\n\n----------------------------------------\n\nTITLE: Configuring Logstash Monitor in OpenTelemetry Collector\nDESCRIPTION: YAML configuration snippet showing how to activate the Logstash monitor in the Splunk Distribution of OpenTelemetry Collector. This sets up the smartagent receiver with the logstash monitor type.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/logstash.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/logstash:\n    type: logstash\n      ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Gateway for Related Content\nDESCRIPTION: Example configuration for the OpenTelemetry Collector in gateway mode, showing how to set up the http_forwarder extension to forward API calls to Splunk Observability Cloud and receive data from agent collectors.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/relatedcontent-collector-apm.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  http_forwarder:\n    egress:\n      endpoint: \"https://api.${SPLUNK_REALM}.signalfx.com\"\n\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n      http:\n  signalfx:\n\nexporters:\n  # Traces\n  otlphttp:\n    access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Nagios Monitor Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration snippet showing how to activate the Nagios monitor receiver in the collector configuration. Includes required parameters for command and service definition.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/nagios.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/nagios:\n    type: nagios\n    command: <command>\n    service: <service>\n    timeout: 7 #9 by default\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Java Agent on Windows PowerShell\nDESCRIPTION: These PowerShell commands set environment variables needed for configuring the OpenTelemetry Java agent. Specifically, OTEL_SERVICE_NAME and OTEL_EXPORTER_OTLP_ENDPOINT, are vital for service name assignment and endpoint configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/instrument-java-application.rst#2025-04-22_snippet_3\n\nLANGUAGE: PowerShell\nCODE:\n```\n$env:OTEL_SERVICE_NAME=<yourServiceName>\n\n$env:OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Environment Variables\nDESCRIPTION: Use the Kubernetes Downward API to expose necessary environment variables for OpenTelemetry configuration in Node.js applications within Kubernetes deployments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/instrumentation/instrument-nodejs-application.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nspec:\nselector:\n   matchLabels:\n      app: your-application\ntemplate:\n   spec:\n      containers:\n      - name: myapp\n        image: your-app-image\n        env:\n         - name: SPLUNK_OTEL_AGENT\n           valueFrom:\n             fieldRef:\n               fieldPath: status.hostIP\n         - name: OTEL_EXPORTER_OTLP_ENDPOINT\n           value: \"http://$(SPLUNK_OTEL_AGENT):4318\"\n         - name: OTEL_SERVICE_NAME\n           value: \"<serviceName>\"\n         - name: OTEL_RESOURCE_ATTRIBUTES\n           value: \"deployment.environment=<environmentName>\"\n        command:\n         - node\n         - -r @splunk/otel/instrument\n         - <your-app>.js\n```\n\n----------------------------------------\n\nTITLE: Restarting OpenTelemetry Collector on Linux\nDESCRIPTION: Command to restart the Splunk OpenTelemetry Collector service on Linux systems after configuration updates.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl restart splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Configuring Filesystem Scraper in YAML\nDESCRIPTION: YAML configuration for the filesystem scraper, allowing filtering by devices, filesystem types, and mount points.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/host-metrics-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nfilesystem:\n  <include_devices|exclude_devices>:\n    devices: [ <device name>, ... ]\n    match_type: <strict|regexp>\n  <include_fs_types|exclude_fs_types>:\n    fs_types: [ <filesystem type>, ... ]\n    match_type: <strict|regexp>\n  <include_mount_points|exclude_mount_points>:\n    mount_points: [ <mount point>, ... ]\n    match_type: <strict|regexp>\n```\n\n----------------------------------------\n\nTITLE: Diagnosing .NET Assembly Version Conflicts\nDESCRIPTION: Error message that appears when there's a dependency version conflict during .NET instrumentation installation. This typically occurs when the application requires a different version of a dependency than what's provided by the instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nUnhandled exception. System.IO.FileNotFoundException: Could not load file or assembly 'Microsoft.Extensions.DependencyInjection.Abstractions, Version=7.0.0.0, Culture=neutral, PublicKeyToken=adb9793829ddae60'. The system cannot find the file specified.\n\nFile name: 'Microsoft.Extensions.DependencyInjection.Abstractions, Version=7.0.0.0, Culture=neutral, PublicKeyToken=adb9793829ddae60'\n   at Microsoft.AspNetCore.Builder.WebApplicationBuilder..ctor(WebApplicationOptions options, Action`1 configureDefaults)\n   at Microsoft.AspNetCore.Builder.WebApplication.CreateBuilder(String[] args)\n   at Program.<Main>$(String[] args) in /Blog.Core/Blog.Core.Api/Program.cs:line 26\n```\n\n----------------------------------------\n\nTITLE: Configuring Additional Systemd Metrics\nDESCRIPTION: YAML configuration example showing how to enable additional metrics for systemd services using configuration flags.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/systemd.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/systemd:\n    type: collectd/systemd\n    intervalSeconds: 10\n    services:\n      - docker\n      - ubuntu-fan\n    sendActiveState: true\n```\n\n----------------------------------------\n\nTITLE: Implementing Splunk OpenTelemetry Collector in an Ansible Playbook\nDESCRIPTION: Example Ansible playbook that demonstrates how to use the Splunk OpenTelemetry Collector role with minimal required configuration. The playbook includes important notes about Windows compatibility with the 'become' directive.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/deployments-windows-ansible.rst#2025-04-22_snippet_1\n\nLANGUAGE: PowerShell\nCODE:\n```\n- name: Install the Splunk Distribution of OpenTelemetry Collector\n  hosts: all\n  become: yes\n  # For Windows \"become: yes\" will raise error.\n  # \"The Powershell family is incompatible with the sudo become plugin\". Remove \"become: yes\" tag to run on Windows\n  tasks:\n    - name: \"Include splunk_otel_collector\"\n      include_role:\n        name: \"signalfx.splunk_otel_collector.collector\"\n      vars:\n        splunk_access_token: YOUR_ACCESS_TOKEN\n        splunk_hec_token: YOUR_HEC_TOKEN\n        splunk_realm: SPLUNK_REALM\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Receiver in YAML\nDESCRIPTION: Sample YAML configuration for activating the Redis receiver in the OpenTelemetry Collector. It specifies the endpoint, credentials, collection interval, and TLS settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/redis-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  redis:\n    endpoint: \"localhost:6379\"\n    username: \"test\"\n    password: \"test\"\n    collection_interval: 10s\n    tls:\n      insecure: true\n```\n\n----------------------------------------\n\nTITLE: Adding Host Machine Logs Configuration in Kubernetes\nDESCRIPTION: YAML configuration example showing how to add log files from Kubernetes host machines using extraVolumes, extraVolumeMounts, and extraFileLogs settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlogsCollection:\n  extraFileLogs:\n    filelog/audit-log:\n      include: [/var/log/kubernetes/apiserver/audit.log]\n      start_at: beginning\n      include_file_path: true\n      include_file_name: false\n      resource:\n        com.splunk.source: /var/log/kubernetes/apiserver/audit.log\n        host.name: 'EXPR(env(\"K8S_NODE_NAME\"))'\n        com.splunk.sourcetype: kube:apiserver-audit\nagent:\n  extraVolumeMounts:\n    - name: audit-log\n      mountPath: /var/log/kubernetes/apiserver\n  extraVolumes:\n    - name: audit-log\n      hostPath:\n        path: /var/log/kubernetes/apiserver\n```\n\n----------------------------------------\n\nTITLE: Overriding TLS Configuration\nDESCRIPTION: This snippet updates the TLS configuration to use custom CA, key, and cert files for the kubeletstats receiver in the agent. Prerequisite is the configured TLS secret. Important parameters include auth_type, ca_file path, key_file, cert_file, and insecure_skip_verify flag which should not be set for production environments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n  agent:\n    config:\n      receivers:\n        kubeletstats:\n          auth_type: \"tls\"\n          ca_file: \"/etc/ssl/certs/custom_ca.crt\"\n          key_file: \"/etc/ssl/certs/custom_key.key\"\n          cert_file: \"/etc/ssl/certs/custom_cert.crt\"\n          insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Replacing Substrings with replace - Handlebars\nDESCRIPTION: Shows the replace helper, which substitutes all occurrences of a target substring in a string with a specified replacement. Used for cleaning up or customizing content in alert messages. Inputs are the base string, the substring to replace, and its replacement.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/notif-services/splunkplatform.rst#2025-04-22_snippet_7\n\nLANGUAGE: Handlebars\nCODE:\n```\n``{{replace abbreviated '...' ''}}``\n```\n\n----------------------------------------\n\nTITLE: Restarting an IIS Application Pool (PowerShell)\nDESCRIPTION: This PowerShell command restarts the given IIS application pool by name using Restart-WebAppPool. Required dependency: WebAdministration module must be available. The -Name parameter should be given the exact application pool name and is case sensitive. Restarting the application pool applies configuration changes without recycling the whole web server; errors in the app pool name will fail silently or throw exceptions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/advanced-config-iis-apps.rst#2025-04-22_snippet_4\n\nLANGUAGE: powershell\nCODE:\n```\nRestart-WebAppPool -Name <app-pool>\n```\n\n----------------------------------------\n\nTITLE: Detailed Health Check Extension Configuration in YAML\nDESCRIPTION: This snippet provides a more detailed configuration example for the health_check extension. It includes multiple instances with different settings, demonstrating various configuration options and potential invalid configurations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/health-check-extension.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nhealth_check:\nhealth_check/1:\n  endpoint: \"localhost:13\"\n  tls:\n    ca_file: \"/path/to/ca\"\n    key_file: \"/path/to/key\"\n    cert_file: \"/path/to/cert\"\n  check_collector_pipeline:\n    enabled: false\n    interval: \"5m\"\n    exporter_failure_threshold: 5\nhealth_check/missingendpoint:\n  endpoint: \"\"\n  check_collector_pipeline:\n    enabled: false\n    interval: \"5m\"\n    exporter_failure_threshold: 5\nhealth_check/invalidthreshold:\n  endpoint: \"localhost:13\"\n  check_collector_pipeline:\n    enabled: false\n    interval: \"5m\"\n    exporter_failure_threshold: -1\nhealth_check/invalidpath:\n  endpoint: \"localhost:13\"\n  path: \"invalid\"\n  check_collector_pipeline:\n    enabled: false\n    interval: \"5m\"\n    exporter_failure_threshold: 5\n```\n\n----------------------------------------\n\nTITLE: Required AWS Permissions for ECS Observer\nDESCRIPTION: YAML configuration showing the required read-only permissions that must be added to the customer-managed policy attached to the task role.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-ecs-ec2.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\necs:List*\necs:Describe*\n```\n\n----------------------------------------\n\nTITLE: Getting the MeterProvider in PHP\nDESCRIPTION: Retrieves the global OpenTelemetry MeterProvider instance using the `Globals` class. This provider is the entry point for creating Meters and instruments for metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-manual-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: php\nCODE:\n```\n$meterProvider = Globals::meterProvider();\n```\n\n----------------------------------------\n\nTITLE: Restarting IIS after Configuration Changes\nDESCRIPTION: PowerShell command to restart IIS after making configuration changes. This ensures the new OpenTelemetry settings are applied to the IIS-hosted applications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_9\n\nLANGUAGE: powershell\nCODE:\n```\nStart-Process \"iisreset.exe\" -NoNewWindow -Wait\n```\n\n----------------------------------------\n\nTITLE: Tracking Kafka Consumer Group Lag\nDESCRIPTION: This SignalFlow function monitors Kafka consumer groups for lag in processing messages. It alerts when lag exceeds a threshold over a set time period, with parameters for defining trigger and clear thresholds to manage lag monitoring effectively.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/autodetect/autodetect-list.rst#2025-04-22_snippet_3\n\nLANGUAGE: SignalFlow\nCODE:\n```\n\"https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/kafka/consumer.flow#L5\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector for HAProxy in YAML\nDESCRIPTION: Example YAML configuration for activating the HAProxy integration in the OpenTelemetry Collector. This shows how to add the HAProxy receiver to the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/haproxy.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    smartagent/haproxy:\n        type: haproxy\n        ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Named Instance on Windows\nDESCRIPTION: This snippet is used to configure a named SQL Server instance on Windows. It highlights the need to specify 'computer_name' and 'instance_name' along with enabling resource attributes. This ensures metrics collection from specific server instances when running on Windows environments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/mssql-server-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\\n  sqlserver:\\n    collection_interval: 10s\\n    computer_name: CustomServer\\n    instance_name: CustomInstance\\n    resource_attributes:\\n      sqlserver.computer.name:\\n        enabled: true\\n      sqlserver.instance.name:\\n        enabled: true\n```\n\n----------------------------------------\n\nTITLE: Installing the OpenTelemetry Collector using Shell Script\nDESCRIPTION: Command to download and execute the Splunk OpenTelemetry Collector installation script. Requires specifying your Splunk realm and access token for proper configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-with-the-uf.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --realm SPLUNK_REALM -- SPLUNK_ACCESS_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Example Server-Timing HTTP Response Headers\nDESCRIPTION: Shows the HTTP response headers added by the Go HTTP instrumentation when server trace information is enabled. The `Access-Control-Expose-Headers` header makes `Server-Timing` accessible to scripts, and `Server-Timing` contains the `traceparent` (including trace ID and span ID) needed for RUM correlation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/configuration/advanced-go-otel-configuration.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nAccess-Control-Expose-Headers: Server-Timing\nServer-Timing: traceparent;desc=\"00-<serverTraceId>-<serverSpanId>-01\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx Forwarder Receiver\nDESCRIPTION: Basic configuration to activate the SignalFx Forwarder integration in the Collector configuration. Specifies the receiver type and allows for additional configuration options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/signalfx-forwarder.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/signalfx-forwarder:\n    type: signalfx-forwarder\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Instrumentations in Shell\nDESCRIPTION: These shell commands configure the Splunk Distribution of OpenTelemetry JS by disabling all default instrumentations and enabling the 'bunyan' instrumentation. It modifies the environment variables applicable to the Node.js program.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/advanced-nodejs-otel-configuration.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED=false\nexport OTEL_INSTRUMENTATION_BUNYAN_ENABLED=true\n```\n\n----------------------------------------\n\nTITLE: Service Pipeline Configuration\nDESCRIPTION: Configuration to add the Elasticsearch monitor to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/elasticsearch.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/elasticsearch]\n```\n\n----------------------------------------\n\nTITLE: Deactivating Debug Logging Programmatically in Node.js\nDESCRIPTION: This JavaScript snippet shows how to deactivate debug logging programmatically using the OpenTelemetry API. It imports the 'diag' object from '@opentelemetry/api' and calls 'diag.setLogger()' without arguments to reset the logger, effectively disabling debug output. This should be used cautiously as debug logs are helpful for troubleshooting.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/troubleshooting/common-nodejs-troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nconst { diag } = require('@opentelemetry/api');\ndiag.setLogger();\n```\n\n----------------------------------------\n\nTITLE: Configuring Basicauth Extension for Server and Client Authentication in YAML\nDESCRIPTION: This snippet demonstrates how to configure the basicauth extension for both server and client authentication in the OpenTelemetry Collector's YAML configuration file. It includes settings for htpasswd file and inline credentials, as well as client authentication.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/basic-auth-extension.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  basicauth/server:\n    htpasswd: \n      file: .htpasswd\n      inline: |\n        ${env:BASIC_AUTH_USERNAME}:${env:BASIC_AUTH_PASSWORD}\n\n  basicauth/client:\n    client_auth: \n      username: username\n      password: password\n\nreceivers:\n  otlp:\n    protocols:\n      http:\n        auth:\n          authenticator: basicauth/server\n\nprocessors:\n\nexporters:\n  otlp:\n    auth:\n      authenticator: basicauth/client\n```\n\n----------------------------------------\n\nTITLE: Sending Jenkins Build Start Notification to Splunk On-Call via cURL\nDESCRIPTION: This shell command uses cURL to send an HTTP POST request with a JSON payload to a Splunk On-Call REST endpoint. The payload contains Jenkins build details extracted from environment variables (`BUILD_NAME`, `BUILD_DISPLAY_NAME`, `BUILD_ID`) and sends an 'INFO' type message indicating a build is starting. This command should be placed in a Jenkins 'Execute shell' build step, and the `SPLUNKONCALL_REST_ENDPOINT_URL` placeholder must be replaced with the actual URL provided by the Splunk On-Call REST Endpoint integration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/jenkins-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST --header 'Accept: application/json' --data '{ \"entity_id\": \"'${BUILD_NAME}'\", \"message_type\": \"INFO\", \"state_message\": \"Jenkins Build: '${BUILD_DISPLAY_NAME}' is underway\", \"BUILD_ID\": \"'${BUILD_ID}'\" }' '**SPLUNKONCALL_REST_ENDPOINT_URL**'\n```\n\n----------------------------------------\n\nTITLE: Calculating Percentage using Plot References in Chart Builder Formula\nDESCRIPTION: Illustrates how to use a time series expression within the Splunk Observability Cloud chart builder's 'Signals' field to create a derived metric. This specific formula calculates a percentage based on the values of two other plots, labeled A and B (e.g., cache hits and misses). Requires plots A and B to be defined. The formula field supports basic arithmetic operators (+, /, -, *), parentheses, numbers, and plot reference letters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/chart-builder.rst#2025-04-22_snippet_0\n\nLANGUAGE: Formula\nCODE:\n```\n(A/(A+B)) * 100\n```\n\n----------------------------------------\n\nTITLE: Toggling Zero-Code Instrumentation via ld.so.preload (Shell)\nDESCRIPTION: Controls whether to install the 'splunk-otel-auto-instrumentation' package and configure zero-code instrumentation using '/etc/ld.so.preload'. Use '--with-instrumentation' to enable or '--without-instrumentation' to disable (default). This method applies instrumentation globally to all supported processes. Cannot be used with '--with-systemd-instrumentation'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n--with[out]-instrumentation\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n--without-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Example Kubernetes API Server Configuration\nDESCRIPTION: Complete example configuration showing basic setup with host, port and extra dimensions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-apiserver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/kubernetes-apiserver:\n    type: kubernetes-apiserver\n    host: localhost\n    port: 443\n    extraDimensions:\n      metric_source: kubernetes-apiserver\n```\n\n----------------------------------------\n\nTITLE: Adding Required Gems to Gemfile for Ruby Instrumentation\nDESCRIPTION: Adds the necessary gems to your project's Gemfile to install the Splunk Distribution of OpenTelemetry Ruby and all available instrumentation libraries.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/instrument-ruby-application.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ngem \"splunk-otel\", \"~> 1.0\"\ngem \"opentelemetry-instrumentation-all\", \"~> 0.27\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SAPM Exporter with Basic Settings in YAML\nDESCRIPTION: This snippet shows a basic SAPM exporter configuration with settings for access token, endpoint, max connections, and number of workers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-apm-exporter.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  sapm:\n    access_token: <access_token>\n    access_token_passthrough: true\n    endpoint: https://ingest.<realm>.signalfx.com/v2/trace\n    max_connections: 100\n    num_workers: 8\n```\n\n----------------------------------------\n\nTITLE: Activating Sum Connector in OpenTelemetry Collector (YAML)\nDESCRIPTION: This snippet demonstrates how to declare the 'sum' connector in the 'connectors' section of an OpenTelemetry Collector YAML configuration file. This is the first required step to enable attribute value summing in any telemetry pipeline. The 'sum' key under 'connectors' activates the connector for further pipeline configuration. No other parameters are specified here, making it suitable for basic setups.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sum-connector.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconnectors:\\n  sum:\n```\n\n----------------------------------------\n\nTITLE: Configuring Container Dependencies for Java Agent in AWS Fargate\nDESCRIPTION: This snippet shows how to configure volume sharing and container dependencies for the Java agent. It ensures the application container can access the OpenTelemetry Java agent JAR file from the splunk-java-agent container.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-fargate-java.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n\"volumesFrom\": [\n  {\n    \"sourceContainer\": \"splunk-java-agent\",\n    \"readOnly\": false\n  }\n  ],\n\"dependsOn\": [\n  {\n    \"containerName\": \"splunk-java-agent\",\n    \"condition\": \"START\"\n  }\n  ],\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Collector with Fluentd and Custom HEC Endpoint\nDESCRIPTION: PowerShell command to install the Splunk Distribution of OpenTelemetry Collector with custom HTTP Event Collector endpoint URL and token. Configures Fluentd to send logs to a specified HEC endpoint rather than the default realm-based one.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/windows-config-logs.rst#2025-04-22_snippet_1\n\nLANGUAGE: PowerShell\nCODE:\n```\n& {Set-ExecutionPolicy Bypass -Scope Process -Force; $script = ((New-Object System.Net.WebClient).DownloadString('https://dl.signalfx.com/splunk-otel-collector.ps1')); $params = @{access_token = \"<SPLUNK_ACCESS_TOKEN>\"; realm = \"<SPLUNK_REALM>\"; hec_url = \"<SPLUNK_HEC_URL>\"; hec_token = \"<SPLUNK_HEC_TOKEN>\"}; Invoke-Command -ScriptBlock ([scriptblock]::Create(\". {$script} $(&{$args} @params)\"))}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Watchtower Container for Automatic Updates\nDESCRIPTION: Shell command to run the Watchtower container that automatically checks for and applies updates to the private runner Docker image. It requires access to the Docker socket and uses label-enable for targeted updates.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d \\\n--name watchtower \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\nv2tec/watchtower --label-enable --cleanup\n```\n\n----------------------------------------\n\nTITLE: Complete vSphere Monitor Configuration Example in YAML\nDESCRIPTION: Extended configuration example showing full setup including receiver config, exporters, and service pipeline configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/vsphere.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/vsphere:\n    type: vsphere\n    host: hostname\n    username: user\n    password: pass\n    insecureSkipVerify: true\nexporters:\n  signalfx:\n    access_token: abc123\n    realm: us2\nservice:\n  pipelines:\n    metrics:\n      receivers:\n        - smartagent/vsphere\n      exporters:\n        - signalfx\n```\n\n----------------------------------------\n\nTITLE: Filtering Collected Metric Groups in Kubelet Stats Receiver (YAML)\nDESCRIPTION: Shows how to limit the set of metrics collected by the kubeletstats receiver to specific metric groups. Only node and pod-related metrics will be reported, as defined by the metric_groups array. This setup is suitable when users are only interested in specific resources and want to reduce unnecessary metric ingestion.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubelet-stats-receiver.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 10s\n    auth_type: \"serviceAccount\"\n    endpoint: \"${K8S_NODE_NAME}:10250\"\n    insecure_skip_verify: true\n    metric_groups:\n      - node\n      - pod\n\n```\n\n----------------------------------------\n\nTITLE: Enabling Trace Response Headers in Windows PowerShell\nDESCRIPTION: Shows how to set the SPLUNK_TRACE_RESPONSE_HEADER_ENABLED environment variable in Windows PowerShell to enable adding server trace information to HTTP response headers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/configuration/advanced-ruby-otel-configuration.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n$env:SPLUNK_TRACE_RESPONSE_HEADER_ENABLED=true\n```\n\n----------------------------------------\n\nTITLE: Configuring the NGINX Receiver Endpoint in YAML\nDESCRIPTION: YAML configuration snippet defining the NGINX receiver within the OpenTelemetry Collector configuration. It specifies the target NGINX status endpoint URL (`endpoint`) and the frequency (`collection_interval`) at which metrics should be fetched. This receiver relies on the NGINX `ngx_http_stub_status_module` being enabled and accessible.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/nginx-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  receivers:\n    nginx:\n      endpoint: \"http://localhost:80/status\"\n      collection_interval: 10s\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Velero Receiver\nDESCRIPTION: Basic configuration for activating the Prometheus Velero monitor in the Collector configuration. Defines the receiver type, discovery rule, and port settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-velero.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/velero:\n    type: prometheus/velero\n    discoveryRule: container_image =~ \"velero\" && port == 8085\n    port: 8085    \n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Default Metrics Collection Configuration\nDESCRIPTION: Simple configuration to collect default (non-custom) metrics from Elasticsearch.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/elasticsearch.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmonitors:\n- type: elasticsearch\n  host: localhost\n  port: 9200\n```\n\n----------------------------------------\n\nTITLE: Advanced GenericJMX Configuration Example\nDESCRIPTION: Detailed example showing JMX monitor configuration with MBean definitions for threading metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/genericjmx.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/genericjmx:\n    type: collectd/genericjmx\n    host: my-java-app\n    port: 7099\n    mBeanDefinitions:\n      threading:\n        objectName: java.lang:type=Threading\n        values:\n          - type: gauge\n            table: false\n            instancePrefix: jvm.threads.count\n            attribute: ThreadCount\n```\n\n----------------------------------------\n\nTITLE: Multi-Container Pod Instrumentation YAML for Java\nDESCRIPTION: Kubernetes deployment manifest showing how to instrument multiple containers in the same pod by specifying container names in the OpenTelemetry annotations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment-with-multiple-containers\nspec:\n  selector:\n    matchLabels:\n      app: my-pod-with-multiple-containers\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: my-pod-with-multiple-containers\n      annotations:\n        instrumentation.opentelemetry.io/inject-java: \"true\"\n        instrumentation.opentelemetry.io/container-names: \"myapp,myapp2\"\n```\n\n----------------------------------------\n\nTITLE: Setting Service Name Environment Variable (Linux)\nDESCRIPTION: Sets the OTEL_SERVICE_NAME environment variable in a Linux shell. This variable is required to identify the service in Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_SERVICE_NAME=<yourServiceName>\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud Foundry Firehose Nozzle Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration example for activating the Cloud Foundry Firehose Nozzle monitor in the Splunk Distribution of the OpenTelemetry Collector. This configuration defines both the receiver and includes it in the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloudfoundry/cloudfoundry-firehose-nozzle.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/cloudfoundry-firehose-nozzle\n    type: cloudfoundry-firehose-nozzle\n    ... # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/cloudfoundry-firehose-nozzle]\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Spans and Tags in PHP with SignalFx GlobalTracer\nDESCRIPTION: This PHP code demonstrates how to manually create a custom trace span using the SignalFx GlobalTracer within a function. It shows retrieving the global tracer instance, starting an active span named 'myApplicationLogic', adding custom tags ('indicator', 'widget') based on function parameters and results, handling potential exceptions by tagging the span with 'error', and ensuring the span is properly finished using a `finally` block. This approach requires the SignalFx Tracing Library for PHP, although the library is noted as deprecated in the surrounding documentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/instrumentation/php-manual-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: php\nCODE:\n```\nuse SignalFx\\GlobalTracer; // Suggested namespace over OpenTracing for GlobalTracer\n\nfunction myApplicationLogic($indicator) {\n   $tracer = GlobalTracer::get(); //  Will provide the tracer instance used by provided instrumentations\n   $span = $tracer->startActiveSpan('myApplicationLogic')->getSpan();\n   $span->setTag('indicator', $indicator);\n\n   try {\n      $widget = myAdditionalApplicationLogic($indicator);\n      $span->setTag('widget', $widget);\n      return $widget;\n   } catch (Exception $e) {\n      $span->setTag('error', true);\n      throw $e;\n   } finally {\n      $span->finish();\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: GenericJMX Pipeline Configuration in YAML\nDESCRIPTION: Configuration to add the GenericJMX monitor to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/genericjmx.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/genericjmx]\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Sampling with Priority in OpenTelemetry Collector\nDESCRIPTION: This YAML configuration sets up the probabilistic sampler processor to sample logs with a 15% rate, while also considering a priority attribute for sampling decisions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/probabilistic-sampler-processor.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  probabilistic_sampler:\n    sampling_percentage: 15\n    sampling_priority: priority\n```\n\n----------------------------------------\n\nTITLE: Manually Setting Screen Names in iOS RUM\nDESCRIPTION: This snippet shows how to customize screen names in your application using the setScreenName function. The custom screen name persists until the next call to the function and overrides the default ViewController names.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: swift\nCODE:\n```\nSplunkRum.setScreenName(\"AccountSettingsTab\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Exec Input Plugin\nDESCRIPTION: Configuration example for the Telegraf Exec Input plugin that executes commands at intervals and parses metrics from their output.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/telegraf.rst#2025-04-22_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n# Read metrics from one or more commands that can output to stdout\n[[inputs.exec]]\n\n## Commands array\ncommands = [\"sh /testfolder/testscript.sh\"]\ntimeout = \"30s\"\ndata_format = \"influx\"\n\n## Environment variables\n## Array of \"key=value\" pairs to pass as environment variables\n## e.g. \"KEY=value\", \"USERNAME=John Doe\",\n## \"LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs\"\n# environment = []\n\n## Measurement name suffix\n## Used for separating different commands\n# name_suffix = \"\"\n\n## Ignore Error Code\n## If set to true, a non-zero error code in not considered an error and the\n## plugin will continue to parse the output.\n# ignore_error = false\n```\n\n----------------------------------------\n\nTITLE: Injecting Service and Environment Environment Variables into Log4j Log Layouts - XML\nDESCRIPTION: This Log4j XML snippet shows how to use environment variables (such as OTEL_SERVICE_NAME and OTEL_ENV_NAME) in pattern layouts for serverless or environment-driven deployments. The pattern injects service name and deployment environment at log time. Requires proper environment variable configuration and Log4j support for variable resolution.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<PatternLayout>\\n   <pattern>\\n      service.name=${OTEL_SERVICE_NAME}, deployment.environment=${OTEL_ENV_NAME} %m%n\\n   </pattern>\\n</PatternLayout>\n```\n\n----------------------------------------\n\nTITLE: Critical Alert JSON Payload for Zendesk Trigger\nDESCRIPTION: JSON payload for creating new incidents in Splunk On-Call when new tickets are created in Zendesk. Includes ticket details and metadata.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zendesk-bi-directional-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \n   \"entity_id\":\"{{ticket.id}}\", \n   \"message_type\":\"CRITICAL\",\n   \"state_message\":\"{{ticket.comments_formatted}}\",\n   \"monitoring_tool\":\"Zendesk\", \n   \"alert_url\":\"{{ticket.link}}\",\n   \"ticket_id\":\"{{ticket.id}}\", \n   \"Ticket External I.D.\":\"{{ticket.external_id}}\", \n   \"Ticket Origin\":\"{{ticket.via}}\",\n   \"Ticket Status\":\"{{ticket.status}}\", \n   \"Ticket Priority\":\"{{ticket.priority}}\" \n}\n```\n\n----------------------------------------\n\nTITLE: Referencing Go's ParseDuration Function\nDESCRIPTION: Reference to the time.ParseDuration function in Golang which is used to parse the collection_interval string value into a duration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/collector-settings-collectioninterval.rst#2025-04-22_snippet_2\n\nLANGUAGE: go\nCODE:\n```\ntime.ParseDuration\n```\n\n----------------------------------------\n\nTITLE: Checking and deleting existing CRDs for migration\nDESCRIPTION: Commands to check for existing OpenTelemetry CRDs and remove them before reinstalling with the new configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get crds | grep opentelemetry\nkubectl delete crd opentelemetrycollectors.opentelemetry.io\nkubectl delete crd opampbridges.opentelemetry.io\nkubectl delete crd instrumentations.opentelemetry.io\n```\n\n----------------------------------------\n\nTITLE: Pulling Latest Private Runner Docker Image\nDESCRIPTION: Shell command to pull the latest version of the Splunk Synthetic Monitoring private runner Docker image from the Quay.io repository.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull http://quay.io/signalfx/splunk-synthetics-runner:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom API Server Settings\nDESCRIPTION: Example configuration for connecting to a non-standard Kubernetes API server with custom ports and TLS settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  config:\n    receivers:\n      receiver_creator:\n        receivers:\n          smartagent/kubernetes-apiserver:\n            rule: type == \"port\" && port == 3443 && pod.labels[\"k8s-app\"] == \"kube-apiserver\"\n            config:\n              clientCertPath: /etc/myapiserver/clients-ca.crt\n              clientKeyPath: /etc/myapiserver/clients-ca.key\n              skipVerify: true\n              useHTTPS: true\n              useServiceAccount: false\n```\n\n----------------------------------------\n\nTITLE: Selecting the First Non-Empty Value using coalesce - Handlebars\nDESCRIPTION: Demonstrates using the coalesce function to select the first non-empty value from multiple options, falling back to a default string if none are present. The function is applied in context to specify the order of dimensions for a variable, useful in alert customization. Expects variables like dimensions.host and dimensions.aws_arn, with a final fallback string if all are empty.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/notif-services/splunkplatform.rst#2025-04-22_snippet_0\n\nLANGUAGE: Handlebars\nCODE:\n```\n| {{{coalesce dimensions.host dimensions.aws_arn ‘No ID Found!’}}}\n\n```\n\n----------------------------------------\n\nTITLE: Adding Version Label to Multiple System Metrics\nDESCRIPTION: Adds a version label to all metrics starting with 'system.' using regex matching.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: ^system\\.\nmatch_type: regexp\naction: update\noperations:\n    - action: add_label\n      new_label: version\n      new_value: opentelemetry collector {{version}}\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Exporters for gRPC Trace Ingestion in Splunk\nDESCRIPTION: Configuration for setting up the OTLP exporter in the Splunk OpenTelemetry Collector to send traces via gRPC. Requires environment variables for SPLUNK_REALM and SPLUNK_ACCESS_TOKEN.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/other-ingestion-methods/grpc-data-ingest.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otlp:\n     endpoint: ingest.${SPLUNK_REALM}.signalfx.com:443\n     headers:\n        \"X-SF-Token\": \"${SPLUNK_ACCESS_TOKEN}\"\n```\n\n----------------------------------------\n\nTITLE: Advanced Chrony Receiver Configuration Example\nDESCRIPTION: Comprehensive YAML configuration example for the Chrony receiver, including endpoint, timeout, collection interval, and specific metric enablement.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/chrony-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  chrony:\n    endpoint: unix:///var/run/chrony/chronyd.sock\n    timeout: 10s\n    collection_interval: 30s\n    metrics:\n      ntp.skew:\n        enabled: true\n      ntp.stratum:\n        enabled: true\n```\n\n----------------------------------------\n\nTITLE: Detailed SAP HANA Monitor Configuration\nDESCRIPTION: Complete YAML configuration example showing how to configure the SAP HANA monitor with host, port, and credentials.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/sap-hana.rst#2025-04-22_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nreceivers:\n  smartagent/hana:\n    type: hana\n    host: myhost.hana.us.hanacloud.ondemand.com\n    port: <sap_hana_port>\n    username: <username>\n    password: <password>\n```\n\n----------------------------------------\n\nTITLE: Activating AlwaysOn Profiling in Java Application\nDESCRIPTION: This bash command demonstrates how to activate AlwaysOn Profiling in a Java application, including memory profiling, by setting system properties. It also configures the OpenTelemetry endpoint for exporting profiling data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/instrument-java-application.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\njava -javaagent:./splunk-otel-javaagent.jar \\\n   -Dsplunk.profiler.enabled=true \\\n   -Dsplunk.profiler.memory.enabled=true \\\n   -Dotel.exporter.otlp.endpoint=http(s)://collector:4318 \\\n   -jar <your_application>.jar\n```\n\n----------------------------------------\n\nTITLE: Enabling CLI Tracing with SignalFx PHP Instrumentation (Shell)\nDESCRIPTION: This shell snippet demonstrates activating the SignalFx PHP instrumentation for tracing command-line (CLI) sessions. It sets the SIGNALFX_TRACE_CLI_ENABLED environment variable to true and runs PHP programs such as artisan and custom CLI scripts. Dependencies include a properly installed SignalFx Tracing Library for PHP; ensure the environment variable is set prior to calling any instrumented PHP code. Inputs are commands to run traced PHP scripts, and the output is the creation of root spans tracking their lifetime. The approach is limited to Unix-like shells and requires correct environmental setup.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/configuration/advanced-php-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport SIGNALFX_TRACE_CLI_ENABLED=true\nphp artisan migrate:fresh\nphp myTracedCliScript.php\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Exporters for SDKs (Shell)\nDESCRIPTION: Specifies a comma-separated list of exporters (e.g., 'otlp', 'prometheus') for metrics collected by activated SDKs. Set to 'none' to disable metrics collection/export. Sets the 'OTEL_METRICS_EXPORTER' environment variable. If omitted, each SDK uses its default exporter(s).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\n--metrics-exporter <exporters>\n```\n\n----------------------------------------\n\nTITLE: Building OpenTelemetry Collector with Custom JMX Metric Gatherer\nDESCRIPTION: This command-line snippet shows how to build the OpenTelemetry Collector with a custom JMX Metric Gatherer JAR by specifying its SHA256 hash.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/jmx-receiver.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngo build -ldflags \"-X github.com/open-telemetry/opentelemetry-collector-contrib/receiver/jmxreceiver.MetricsGathererHash=<sha256hash>\" ...\n```\n\n----------------------------------------\n\nTITLE: Creating ClusterRole for Kubernetes Objects Receiver\nDESCRIPTION: YAML configuration to create a ClusterRole with permissions for collecting pods and events in Kubernetes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-objects-receiver.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - events\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups: \n  - \"events.k8s.io\"\n  resources:\n  - events\n  verbs:\n  - watch\n  - list\n```\n\n----------------------------------------\n\nTITLE: Defining Carbon Receiver Section in reStructuredText\nDESCRIPTION: Sets up the document structure for the Carbon receiver documentation using reStructuredText syntax. Includes a section title, metadata for description, and a brief content placeholder.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/carbon-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _carbon-receiver:\n\n****************************\nCarbon receiver\n****************************\n\n.. meta::\n      :description: Receives metrics in Carbon plaintext protocol.\n\nThe Splunk Distribution of the OpenTelemetry Collector supports the Carbon receiver. Documentation is planned for a future release. \n\nTo find information about this component in the meantime, see :new-page:`Carbon receiver <https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/carbonreceiver>` on GitHub.\n```\n\n----------------------------------------\n\nTITLE: Basic Elasticsearch Monitor Configuration\nDESCRIPTION: Basic configuration to activate the Elasticsearch integration in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/elasticsearch.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/elasticsearch:\n    type: elasticsearch\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Node.js Agent Path (YAML)\nDESCRIPTION: Shows an example of how to set the `NODE_OPTIONS` environment variable within a configuration file (e.g., `/etc/splunk/zeroconfig/node.conf` or `/usr/lib/systemd/system.conf.d/00-splunk-otel-auto-instrumentation.conf`) to point to a custom installation path for the Splunk OpenTelemetry Node.js agent. This is required if the agent is not installed in the default location `/usr/lib/splunk-instrumentation/splunk-otel-js`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_29\n\nLANGUAGE: yaml\nCODE:\n```\nNODE_OPTIONS=-r /custom/nodejs/install/path/@splunk/otel/instrument\n```\n\n----------------------------------------\n\nTITLE: Basic Health Check Extension Configuration in YAML\nDESCRIPTION: This example demonstrates a basic configuration for the health_check extension. It includes settings for the endpoint, TLS configuration, path, and collector pipeline check options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/health-check-extension.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  health_check:\n  health_check/1:\n    endpoint: \"localhost:13\"\n    tls:\n      ca_file: \"/path/to/ca.crt\"\n      cert_file: \"/path/to/cert.crt\"\n      key_file: \"/path/to/key.key\"\n    path: \"/health/status\"\n    check_collector_pipeline:\n      enabled: true\n      interval: \"5m\"\n      exporter_failure_threshold: 5\n```\n\n----------------------------------------\n\nTITLE: Deactivating OpenTelemetry Instrumentation for an IIS Application Pool (PowerShell)\nDESCRIPTION: This PowerShell command disables OpenTelemetry instrumentation for the specified IIS application pool with the Disable-OpenTelemetryForIISAppPool cmdlet. The command requires the OpenTelemetry.DotNet.Auto.psm1 module to be imported. Use the -AppPoolName parameter to specify the target application pool; parameter is case sensitive. The expected result is removal of instrumentation from the pool. Use for troubleshooting or to revert changes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/advanced-config-iis-apps.rst#2025-04-22_snippet_3\n\nLANGUAGE: powershell\nCODE:\n```\nDisable-OpenTelemetryForIISAppPool -AppPoolName <app-pool>\n```\n\n----------------------------------------\n\nTITLE: Installing Required PHP Instrumentations Using Composer - Bash\nDESCRIPTION: This set of Bash commands uses Composer to install two PHP packages: the OpenTelemetry OTLP exporter and the PHP HTTP Guzzle7 adapter, both essential for exporting telemetry data and supporting instrumented HTTP requests. Composer and the required PHP environment must be installed. Inputs are the package names and versions. Outputs are installed dependencies under the PHP application's vendor directory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/instrument-php-application.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nphp composer.phar install open-telemetry/exporter-otlp:^1.0.3\\nphp composer.phar install php-http/guzzle7-adapter:^1.0\n```\n\n----------------------------------------\n\nTITLE: Defining Configuration Options for PostgreSQL Monitor in ReStructuredText\nDESCRIPTION: This code snippet defines a table of configuration options for the PostgreSQL monitor. It includes options such as host, port, masterDBName, connectionString, and others, along with their types and descriptions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/postgresql.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. list-table::\n   :widths: 18 18 18 18\n   :header-rows: 1\n\n   - \n\n      - Option\n      - Required\n      - Type\n      - Description\n   - \n\n      - ``host``\n      - no\n      - ``string``\n      - \n   - \n\n      - ``port``\n      - no\n      - ``integer``\n      - (**default:** ``0``)\n   - \n\n      - ``masterDBName``\n      - no\n      - ``string``\n      - The primary database to which the agent first connects to query\n         the list of databases available in the server. This database\n         should be accessible to the user specified with\n         ``connectionString`` and ``params`` below, and that user should\n         have permission to query ``pg_database``. If you want to filter\n         which databases are monitored, use the ``databases`` option\n         below. (**default:** ``postgres``)\n   - \n\n      - ``connectionString``\n      - no\n      - ``string``\n      - See Connection String Parameters\n   - \n\n      - ``params``\n      - no\n      - ``map of strings``\n      - Parameters to the connection string that can be templated into\n         the connection string with the syntax ``{{.key}}``.\n   - \n\n      - ``databases``\n      - no\n      - ``list of strings``\n      - List of databases to send database-specific metrics about. If\n         omitted, metrics about all databases will be sent. \n         (**default:** ``[*]``)\n   - \n\n      - ``databasePollIntervalSeconds``\n      - no\n      - ``integer``\n      - How frequently to poll for new/deleted databases in the DB\n         server. Defaults to the same as ``intervalSeconds`` if not set.\n         (**default:** ``0``)\n   - \n\n      - ``logQueries``\n      - no\n      - ``bool``\n      - If ``true,`` queries will be logged at the info level.\n         (**default:** ``false``)\n   - \n\n      - ``topQueryLimit``\n      - no\n      - ``integer``\n      - The number of top queries to consider when publishing\n         query-related metrics (**default:** ``10``)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenStack Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration to activate the OpenStack integration in the collector configuration. Specifies the receiver type and adds it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/openstack.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/openstack:\n    type: collectd/openstack\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Manually Starting OpenTelemetry Collector Executable on Windows\nDESCRIPTION: This PowerShell command manually starts the OpenTelemetry Collector executable using the default configuration file. It should be run as an administrator if experiencing unexpected start failures.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows.rst#2025-04-22_snippet_2\n\nLANGUAGE: PowerShell\nCODE:\n```\n& 'C:\\Program Files\\Splunk\\OpenTelemetry Collector\\otelcol.exe' --config 'C:\\ProgramData\\Splunk\\OpenTelemetry Collector\\agent_config.yaml'\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Language SDKs for Zero-Code Instrumentation (Shell)\nDESCRIPTION: Enables or disables zero-code instrumentation for specific languages. Provide a comma-separated list of SDKs (currently 'java', 'node', 'dotnet'). Use '--with-instrumentation-sdk java,node' to enable only Java and Node.js. .NET ('dotnet') is only supported on x86_64/amd64. The default enables all supported SDKs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n--with[out]-instrumentation-sdk <sdk>\n```\n\nLANGUAGE: shell\nCODE:\n```\n--with-instrumentation-sdk java\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n--with-instrumentation-sdk java,nodejs,dotnet\n```\n\n----------------------------------------\n\nTITLE: Configuring Syslog Receiver for TCP and RFC5424 Protocol (YAML)\nDESCRIPTION: Provides an example of configuring the syslog receiver for TCP input and RFC5424 message format. The configuration includes specifying the 'tcp' property with a 'listen_address' on all interfaces at port 54526, and 'protocol' set to 'rfc5424'. This setup is suitable for environments sending syslog data using TCP. Dependencies include the Splunk OTel Collector and correct network permissions for the specified port. Key parameters are 'listen_address' for interface/port binding and 'protocol' for syslog message parsing standard. Inputs are RFC5424-compliant syslog messages over TCP; output is parsed log entries.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/syslog-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  syslog:\n    tcp:\n      listen_address: \"0.0.0.0:54526\"\n    protocol: rfc5424\n\n```\n\n----------------------------------------\n\nTITLE: Time-based Alert Muting with RegEx in Splunk On-Call Rules Engine\nDESCRIPTION: This rule mutes alerts that come in between 8 am and 12 pm UTC by changing the message_type to INFO.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-transformations.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nWhen Alert_received_time_utc matches .*T(0[8-9]|1[0-1]):.*\n\nSet message_type to INFO\n```\n\n----------------------------------------\n\nTITLE: Activating sqlserver Receiver in YAML\nDESCRIPTION: This snippet demonstrates how to activate the Microsoft SQL Server receiver by adding configurations in the 'receivers' section of your YAML configuration file. It includes details on setting the collection interval and database connection parameters, such as username, password, server, and port. Additionally, it shows inclusion in the 'metrics' pipeline of the 'service' section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/mssql-server-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\\n    sqlserver:\\n      collection_interval: 10s\\n    sqlserver/1:\\n      collection_interval: 5s\\n      username: sa\\n      password: securepassword\\n      server: 0.0.0.0\\n      port: 1433\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\\n  pipelines:\\n    metrics:\\n      receivers:\\n        - sqlserver\n```\n\n----------------------------------------\n\nTITLE: Configuring OracleDB Receiver for High Availability (HA) (YAML)\nDESCRIPTION: This snippet shows how to configure an oracledb receiver's datasource string for high availability scenarios. The datasource includes a service_name and an explicit server definition in the query parameters. All other configuration and integration steps are similar to the single-instance approach. Users should ensure their connection string fits their Oracle HA topology and Go Oracle driver expectations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/oracledb-receiver.rst#2025-04-22_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nreceivers:\\n  oracledb:\\n    # Refer to Oracle Go Driver go_ora documentation for full connection string options\\n    datasource: \"oracle://<username>:<password>@<host>:<port>/<service_name>?<server>=<host>:<port>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Pipeline for AppMesh Metrics\nDESCRIPTION: Configuration for adding the AppMesh monitor to the metrics pipeline in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/aws-appmesh.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/appmesh]\n```\n\n----------------------------------------\n\nTITLE: Configuring Gauge Metrics in Windows Performance Counters Receiver\nDESCRIPTION: Example configuration that emits the 'Memory/Committed Bytes' counter as the 'bytes.committed' metric using the gauge metric type. The configuration specifies the metric description, unit, collection interval, and which performance counters to collect.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/windowsperfcounters-receiver.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  windowsperfcounters:\n    metrics:\n      bytes.committed:\n        description: the number of bytes committed to memory\n        unit: By\n        gauge:\n    collection_interval: 30s\n    perfcounters:\n    - object: Memory\n      counters:\n        - name: Committed Bytes\n          metric: bytes.committed\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [windowsperfcounters]\n```\n\n----------------------------------------\n\nTITLE: Integrating TCP Log Receiver into Logs Pipeline in YAML\nDESCRIPTION: This YAML configuration snippet illustrates how to integrate the enabled `tcplog` receiver into the data processing pipeline. By adding `tcplog` to the `receivers` list under `service.pipelines.logs`, it ensures that logs collected by this receiver are processed through the defined logs pipeline. This step is necessary to route the collected data for further processing and exporting.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/tcp-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    logs:\n      receivers: [tcplog]\n```\n\n----------------------------------------\n\nTITLE: Defining Grafana Docker Image with Splunk Plugin\nDESCRIPTION: Sample Dockerfile content to build a custom Grafana image. It starts from an official Grafana image, sets environment variables (including allowing unsigned plugins and configuring paths), copies the Splunk plugin into the correct directory, exposes the Grafana port, and sets the entrypoint. Requires the Splunk plugin (`cisco-splunko11y-datasource`) to be present in the build context.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM grafana/grafana-oss:11.2.5\n\nENV GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=\"cisco-splunko11y-datasource\" \\\n\n    GF_PATHS_PLUGINS=\"/usr/share/grafana/grafana-plugins\" \\\n\n    GF_SERVER_ROOT_URL=\"%(protocol)s://%(domain)s:%(http_port)s/grafanaplugin/\" \\\n\n    GF_SERVER_SERVE_FROM_SUB_PATH=\"true\"\n\nWORKDIR /usr/share/grafana\n\nUSER root\n\nRUN mkdir -p /usr/share/grafana/grafana-plugins\n\nCOPY cisco-splunko11y-datasource /usr/share/grafana/grafana-plugins/cisco-splunko11y-datasource\n\nEXPOSE 3000\n\nUSER grafana\n\nWORKDIR /\n\nENTRYPOINT [ \"/run.sh\" ]\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Go Distribution (Shell)\nDESCRIPTION: This command uses the Go toolchain to download and install the Splunk Distribution of OpenTelemetry Go package. This is a prerequisite for instrumenting a Go application. Ensure Go is installed and configured in your environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/instrumentation/instrument-go-application.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngo get github.com/signalfx/splunk-otel-go/distro\n```\n\n----------------------------------------\n\nTITLE: Filtering Spans to Remove Attributes in Android RUM\nDESCRIPTION: Demonstrates how to remove specific span attributes like HTTP user agent to protect sensitive information when using the Splunk RUM Android agent.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nSplunkRum.builder()\n         .filterSpans(spanFilter ->\n            spanFilter\n                  .removeSpanAttribute(stringKey(\"http.user_agent\")))\n```\n\n----------------------------------------\n\nTITLE: Modifying URLs with Regular Expressions in JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates how to use the `onAttributesSerializing` command to redact sensitive information from URLs in Splunk RUM. It uses regular expressions to replace token values in URLs to prevent leaks of sensitive data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/sensitive-data-rum.rst#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nonAttributesSerializing: (attributes) => ({\n            ...attributes,\n            'http.url': typeof attributes['http.url'] === 'string'\n              ? attributes['http.url'].replace(/([?&]token=)[^&]+(&|$)/g, '$1<token>$2')\n              : attributes['http.url']\n\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Options for Hadoop JMX in yarn-env.sh\nDESCRIPTION: JVM options to enable JMX for YARN NodeManager and ResourceManager in the yarn-env.sh file. This configures JMX to use non-SSL connections without authentication on specific ports.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoopjmx.rst#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport YARN_NODEMANAGER_OPTS=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=8002 $YARN_NODEMANAGER_OPTS\"\nexport YARN_RESOURCEMANAGER_OPTS=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=5680 $YARN_RESOURCEMANAGER_OPTS\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom OPcache URL in OpenTelemetry Collector\nDESCRIPTION: YAML configuration example showing how to specify a full custom URL for the OPcache status script using the 'url' option.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cache/opcache.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmonitors:\n - type: collectd/opcache\n   host: localhost\n   port: 80\n   useHTTPS: true\n   url: \"http://{{.host}}:{{.port}}/opcache\"\n   # useHTTPS is ignored.\n```\n\n----------------------------------------\n\nTITLE: Configuring Endpoint for Network Explorer - YAML\nDESCRIPTION: Defines how to configure the `endpoint.address` for the eBPF Helm chart using YAML, ensuring it points to the appropriate Splunk OpenTelemetry Collector gateway service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nendpoint:\n  address: <my-splunk-otel-collector-gateway>\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Payload for Splunk On-Call Outbound Webhook to Jira Comments\nDESCRIPTION: This JSON payload is used in a Splunk On-Call outbound webhook configuration ('Integrations' > 'Outgoing Webhooks') to send chat messages from an incident timeline to the corresponding Jira issue as comments. The `${{CHAT.TEXT}}` variable dynamically inserts the content of the chat message into the `body` field of the JSON payload sent to the Jira REST API endpoint for issue comments (e.g., `https://<YOUR_DOMAIN>.atlassian.net/rest/api/2/issue/${{Jira.issue.key}}/comment`).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/jira-integration-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"body\": \"Via Splunk On-Call Timeline: ${{CHAT.TEXT}}\"}\n```\n\n----------------------------------------\n\nTITLE: Enabling Observability Capabilities in Splunk Cloud Platform\nDESCRIPTION: Command to add prepackaged Splunk Observability Cloud roles to Splunk Cloud Platform instance. This process takes approximately 30 minutes to complete.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/centralized-rbac.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nacs observability enable-capabilities\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Soft Memory Limit with GOMEMLIMIT for Splunk Collector\nDESCRIPTION: Instructions for configuring Splunk Collector memory management after memory_ballast deprecation. This explains how to use GOMEMLIMIT environment variable to control garbage collection frequency by setting a soft memory limit.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/collector-upgrade-memory-ballast.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nGOMEMLIMIT\n```\n\nLANGUAGE: shell\nCODE:\n```\nSPLUNK_MEMORY_TOTAL_MIB\n```\n\nLANGUAGE: shell\nCODE:\n```\nSPLUNK_BALLAST_SIZE_MIB\n```\n\n----------------------------------------\n\nTITLE: Adding indexes_list_all Capability in authorize.conf for Splunk Enterprise\nDESCRIPTION: Creates a capability stanza in authorize.conf to control which roles can list all indexes. This configuration deactivates the indexes_list_all capability for all roles by default, allowing administrators to explicitly enable it for specific roles.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/logs/LOconnect-troubleshoot.rst#2025-04-22_snippet_0\n\nLANGUAGE: conf\nCODE:\n```\n[capability::indexes_list_all]\n```\n\n----------------------------------------\n\nTITLE: Installing the Collector with YUM Repository\nDESCRIPTION: Commands to install the required libcap dependency, set up the Splunk OTel Collector RPM repository, and install the Collector and optional zero-code instrumentation package using yum.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyum install -y libcap  # Required for enabling cap_dac_read_search and cap_sys_ptrace capabilities on the Collector\n\ncat <<EOH > /etc/yum.repos.d/splunk-otel-collector.repo\n[splunk-otel-collector]\nname=Splunk OpenTelemetry Collector Repository\nbaseurl=https://splunk.jfrog.io/splunk/otel-collector-rpm/release/$basearch\ngpgcheck=1\ngpgkey=https://splunk.jfrog.io/splunk/otel-collector-rpm/splunk-B3CD4420.pub\nenabled=1\nEOH\n\nyum install -y splunk-otel-collector\n\n# Optional: install Splunk OpenTelemetry zero-code instrumentation\nyum install -y splunk-otel-auto-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Instrumenting OkHttp Client for Splunk RUM\nDESCRIPTION: Code snippet showing how to instrument an OkHttp client using Splunk RUM to track HTTP requests and responses in the application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_6\n\nLANGUAGE: kotlin\nCODE:\n```\nprivate fun buildOkHttpClient(splunkRum: SplunkRum): Call.Factory {\n   return splunkRum.createRumOkHttpCallFactory(OkHttpClient())\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Attributes from URL\nDESCRIPTION: This example illustrates how to create new attributes by extracting components from an HTTP URL using the extract action with a specified pattern. This is useful for disaggregating a URL into protocol, domain, path, and query parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/attributes-processor.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nattributes/createattributes:\n  actions:\n  # Creates four new attributes (defined in pattern) from the\n  # value of the http.url attribute\n    - key: \"http.url\"\n      pattern: ^(?P<http_protocol>.*):\\\\/\\\\/(?P<http_domain>.*)\\\\/(?P<http_path>.*)(\\\\?|\\\\&)(?P<http_query_params>.*)\n      action: extract\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop JMX for NodeManager in Collector\nDESCRIPTION: YAML configuration for the Hadoop JMX integration to collect metrics from a NodeManager. Specifies the host, port, and nodeType for the NodeManager JMX connection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoopjmx.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/collectd/hadoopjmx:\n    type: collectd/hadoopjmx\n    host: 127.0.0.1\n    port: 8002\n    nodeType: nodeManager\n```\n\n----------------------------------------\n\nTITLE: Incorrect YAML Indentation Example for OpenTelemetry Collector\nDESCRIPTION: Example of incorrectly indented YAML configuration that will cause validation errors in the OpenTelemetry Collector. The 'syslog' receiver is not properly indented under the 'receivers' section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-troubleshoot.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\nsyslog:\n# Rest of the configuration\n```\n\n----------------------------------------\n\nTITLE: Adding NGINX Monitor to Metrics Pipeline\nDESCRIPTION: Configuration to add the NGINX monitor to the metrics pipeline in the Collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/nginx.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/nginx]\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector with Network Explorer - Bash\nDESCRIPTION: This snippet shows the bash commands to install the Splunk OpenTelemetry Collector using Helm in a Kubernetes cluster to collect only Network Explorer telemetry. It disables agents and cluster receivers, and sets a single gateway replica.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm --namespace=<NAMESPACE> install my-splunk-otel-collector \\\n          --set=\"splunkObservability.realm=<REALM>\" \\\n          --set=\"splunkObservability.accessToken=<ACCESS_TOKEN>\" \\\n          --set=\"clusterName=<CLUSTER_NAME>\" \\\n          --set=\"agent.enabled=false\" \\\n          --set=\"clusterReceiver.enabled=false\" \\\n          --set=\"gateway.replicaCount=1\" \\\n          splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Activating OpenTelemetry Instrumentation for an IIS Application Pool (PowerShell)\nDESCRIPTION: This PowerShell command enables OpenTelemetry instrumentation for the specified IIS application pool using the Enable-OpenTelemetryForIISAppPool cmdlet. Prerequisite: the OpenTelemetry.DotNet.Auto.psm1 module must be imported. The -AppPoolName parameter specifies which app pool to affect and is case sensitive. On success, the instrumentation will be active for the targeted pool; incorrect pool names will cause errors.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/advanced-config-iis-apps.rst#2025-04-22_snippet_2\n\nLANGUAGE: powershell\nCODE:\n```\nEnable-OpenTelemetryForIISAppPool -AppPoolName <app-pool>\n```\n\n----------------------------------------\n\nTITLE: Enabling Kubelet Stats Receiver in Metrics Pipeline (YAML)\nDESCRIPTION: Configures the OpenTelemetry Collector to incorporate the kubeletstats receiver into the service pipeline for metrics. This snippet assumes the receiver has been defined, and that the Collector service will route metrics from kubeletstats through any listed exporters. It should be placed in the service section under pipelines.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubelet-stats-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [kubeletstats]\n\n```\n\n----------------------------------------\n\nTITLE: Installing Zero-Code Instrumentation Package on RPM-based Systems\nDESCRIPTION: Command to install the splunk-otel-auto-instrumentation RPM package using rpm. Requires root privileges and the package file path.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nrpm -ivh <path to splunk-otel-auto-instrumentation rpm>\n```\n\n----------------------------------------\n\nTITLE: Resolving Naming Collisions in iOS Swift Code with SplunkOtel\nDESCRIPTION: Demonstrates how to avoid naming collisions between your code and the SplunkOtel library by explicitly specifying the module name as a prefix for types that might conflict.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: swift\nCODE:\n```\nimport SplunkOtel\n//...\nvar a = MyModule.MyConflictingType()\n```\n\n----------------------------------------\n\nTITLE: Basic Logstash TCP Receiver Configuration in YAML\nDESCRIPTION: Basic configuration example showing how to activate the Logstash TCP integration in the Collector configuration\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/logstash-tcp.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/logstash-tcp:\n    type: logstash-tcp\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Pattern for Trace Context with Logback in application.properties - Text\nDESCRIPTION: For Spring Boot projects using Logback, add this line to 'application.properties' to format console log entries with trace_id, span_id, service.name, deployment.environment, and trace_flags. Requires Logback 1.0+ and the Splunk OTel Java agent for MDC field population.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nlogging.pattern.console = %d{yyyy-MM-dd HH:mm:ss} - %logger{36} - %msg %logger{36} - %msg trace_id=%X{trace_id} span_id=%X{span_id} service=%X{service.name}, env=%X{deployment.environment} trace_flags=%X{trace_flags} %n %n\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Receiver for Cluster Receiver\nDESCRIPTION: Configuration snippet for adding MySQL receiver to the cluster receiver to collect metrics from a single endpoint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-add.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nclusterReceiver:\n  config:\n    receivers:\n      mysql:\n        endpoint: mysql-k8s-service:3306\n        ...\n```\n\n----------------------------------------\n\nTITLE: URL Annotation Payload - Splunk On-Call REST Endpoint - JSON\nDESCRIPTION: Illustrates a JSON payload including a URL annotation for Splunk On-Call alerts. Fields such as 'vo_annotate.u.Runbook' provide a human-readable title for the link. Standard incident details apply, with the annotation field allowing additional context or resources to be directly visible in the incident UI.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rest-endpoint-integration-guide.rst#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{ \\u201cmonitoring_tool\\u201d: \\u201cAPI\\u201d, \\u201cmessage_type\\u201d:\\u201cINFO\\u201d,\\n\\u201centity_id\\u201d:\\u201cdisk.space/db01\\u201d, \\u201centity_display_name\\u201d:\\u201cApproaching Low\\nDisk Space on DB01\\u201d, \\u201cstate_message\\u201d:\\u201cThe disk is really really full.\\nHere is a bunch of information about the problem\\u201d,\\n\\u201cvo_annotate.u.Runbook\\u201d:\\u201chttps://help.Splunk On-Call.com/knowledge-base/rest-endpoint-integration-guide/\\u201d\\n }\n```\n\n----------------------------------------\n\nTITLE: Creating an Observable Gauge Metric in PHP\nDESCRIPTION: Demonstrates creating an observable gauge named 'queued' using a Meter obtained from the MeterProvider. The gauge observes the count of items in a queue variable (`$queue`) via a callback function and uses a `MetricReader` to collect the metric values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-manual-instrumentation.rst#2025-04-22_snippet_6\n\nLANGUAGE: php\nCODE:\n```\n$queue = [\n   'job1',\n   'job2',\n   'job3',\n];\n$reader = $meterProvider\n   ->getMeter('demo_meter')\n   ->createObservableGauge('queued', 'jobs', 'The number of jobs enqueued')\n   ->observe(static function (ObserverInterface $observer) use (&$queue): void {\n      $observer->observe(count($queue));\n   });\n$reader->collect();\narray_pop($queue);\n$reader->collect();\n```\n\n----------------------------------------\n\nTITLE: Including Elasticsearch Receiver in Metrics Pipeline\nDESCRIPTION: YAML configuration to include the Elasticsearch receiver in the metrics pipeline of the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/elasticsearch-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers:\n        - elasticsearch\n```\n\n----------------------------------------\n\nTITLE: Configuring Collectd Custom Plugin Receiver in OpenTelemetry Collector (YAML)\nDESCRIPTION: This snippet shows how to activate the Collectd custom plugin integration by adding the necessary configuration to the OpenTelemetry Collector. It defines the receiver and adds it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/collectd-plugin.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/custom:\n    type: collectd/custom\n    ... # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/custom/collectd]\n```\n\n----------------------------------------\n\nTITLE: Publishing Data Streams with Labels in SignalFlow - SignalFlow\nDESCRIPTION: This snippet demonstrates how to publish a data stream with a descriptive label using SignalFlow. The data function fetches a metric (here, 'cpu.idle'), and publish associates a UI label ('CPU idle') for visualization within the detector chart. This requires the metric name and intended label for clarity in plots. The output is a labeled data stream accessible in subsequent SignalFlow operations and visually in the UI. Limitations: metric name must exist; used in conjunction with further alert logic.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/create-detectors-for-alerts.rst#2025-04-22_snippet_1\n\nLANGUAGE: SignalFlow\nCODE:\n```\nA = data('cpu.idle'.publish(label='CPU idle')\n```\n\n----------------------------------------\n\nTITLE: Enabling Metric Categories and Individual Metrics - Splunk O11y YAML\nDESCRIPTION: This code snippet illustrates how to enable entire categories or individual metrics by using YAML in Splunk OpenTelemetry. The `enableMetrics` key can be used to specify which metrics should be active. These settings are essential for collecting the desired metric data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  enableMetrics:\n    - tcp.all \n    - udp.all\n    - dns.all\n    - http.all\n    - ebpf_net.all\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  enableMetrics:\n    - tcp.bytes\n    - tcp.rtt.num_measurements\n    - tcp.active\n    - tcp.rtt.average\n    - tcp.packets\n    - tcp.retrans\n    - tcp.syn_timeouts\n    - tcp.new_sockets\n    - tcp.resets\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  enableMetrics:\n    - udp.bytes\n    - udp.packets\n    - udp.active\n    - udp.drops\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  enableMetrics:\n    - dns.client.duration.average\n    - dns.server.duration.average\n    - dns.active_sockets\n    - dns.responses\n    - dns.timeouts\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  enableMetrics:\n    - http.client.duration.average\n    - http.server.duration.average\n    - http.active_sockets\n    - http.status_code\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  enableMetrics:\n    - ebpf_net.span_utilization_fraction\n    - ebpf_net.pipeline_metric_bytes_discarded\n    - ebpf_net.codetiming_min_ns\n    - ebpf_net.entrypoint_info\n    - ebpf_net.otlp_grpc.requests_sent\n```\n\n----------------------------------------\n\nTITLE: Planning Terraform Changes for Splunk On-Call\nDESCRIPTION: This shell command runs Terraform's planning phase. It analyzes the Terraform configuration files, compares the defined resources with the actual state of resources in Splunk On-Call (managed via the state file), and generates an execution plan outlining the changes (creations, modifications, deletions) needed to reach the desired state. It does not make any actual changes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/manage-splunk-oncall-using-terraform.rst#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nterraform plan\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus NGINX Ingress Receiver\nDESCRIPTION: Basic configuration for activating the Prometheus NGINX Ingress monitor in the Collector configuration. Specifies the monitor type and discovery rules.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-nginx-ingress.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/prometheus-nginx-ingress:\n    type: prometheus/nginx-ingress\n    discoveryRule: container_image =~ \"nginx-ingress-controller\" && port == 10254\n    port: 10254    \n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Attributes Processor for Dimension Removal in YAML\nDESCRIPTION: Configuration example showing how to set up the attributes processor to delete specific dimensions (hostname and source_host) from incoming data before ingestion. This helps reduce data cardinality and associated costs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/configure-remove.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions: \n    ...  \nprocessors:\n  attributes/delete:\n    actions:\n      - key: hostname\n        action: delete\n      - key: source_host\n        action: delete            \nservice:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Advanced Windows Performance Counters Receiver Configuration - YAML\nDESCRIPTION: This YAML configuration caters to advanced scenarios by supporting parameterized collection intervals, delayed starts, and more detailed metric and counter definitions, including attributes for targets and counter instances. Metrics can be defined as gauges or sums with additional properties. Inputs: customized durations, object and instance names, counter mappings. Outputs: highly customized performance metrics with user-defined granularity.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/windowsperfcounters-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nwindowsperfcounters:\n  collection_interval: <duration>\n  initial_delay: <duration>\n  metrics:\n    <metric name 1>:\n      description: <description>\n      unit: <unit type>\n      gauge: null\n    <metric name 2>:\n      description: <description>\n      unit: <unit type>\n      sum: null\n      aggregation: <cumulative or delta>\n      monotonic: <true or false>\n  perfcounters:\n  - object: <object name>\n    instances:\n      - <instance name>\n    counters:\n      - name: <counter name>\n        metric: <metric name>\n        attributes:\n        <key>: <value>\n```\n\n----------------------------------------\n\nTITLE: Setting Skipped Instrumentations via Terminal\nDESCRIPTION: This PowerShell command sets the SkippedInstrumentations property directly from the terminal. The semicolon separator is replaced with '%3B' for command-line compatibility.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_2\n\nLANGUAGE: Powershell\nCODE:\n```\ndotnet build -p:SkippedInstrumentations=StackExchange.Redis%3BMongoDB.Driver.Core\n```\n\n----------------------------------------\n\nTITLE: Glassfish and Payara Java Agent via asadmin\nDESCRIPTION: Use the asadmin command to add the Splunk OpenTelemetry Java agent by setting the javaagent option on Linux or Windows. This command enables the agent for Glassfish and Payara.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-servers-instructions.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n<server_install_dir>/bin/asadmin create-jvm-options \"-javaagent\\:/path/to/splunk-otel-javaagent.jar\"\n```\n\nLANGUAGE: shell\nCODE:\n```\n<server_install_dir>\\bin\\asadmin.bat create-jvm-options '-javaagent\\:<Drive>\\:\\\\path\\\\to\\\\splunk-otel-javaagent.jar'\n```\n\n----------------------------------------\n\nTITLE: Disk Monitor Configuration Settings in RST\nDESCRIPTION: RestructuredText table showing configuration options for the disk monitor, including disk selection and ignore settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/disk.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n\n   - \n\n      - Option\n      - Definition\n      - Default value\n   - \n\n      - Disk\n      - Include specific Disk(s)\n      - \"sda\" \"/^hd/\"\n   - \n\n      - IgnoreSelected\n      - Ignore the designation of specific Disks\n      - ``false``\n```\n\n----------------------------------------\n\nTITLE: Configuring Logz.io Custom Endpoint Body for Splunk On-Call Alerts (JSON)\nDESCRIPTION: This JSON object defines the request body for the Logz.io custom alert endpoint that sends notifications to Splunk On-Call. It maps Logz.io alert template variables (like `{{alert_title}}`, `{{alert_description}}`) to Splunk On-Call incident fields (`message_type`, `entity_id`, `entity_display_name`, etc.). This payload must be pasted into the 'Body' field when creating the custom endpoint in Logz.io, with the HTTP method set to POST.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/logz-io-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \\u201Cmessage_type\\u201D: \\u201CCRITICAL\\u201D, \\u201Centity_id\\u201D: \\u201C{{alert_title}}\\u201D, \\u201Centity_display_name\\u201D: \\u201C{{alert_description}}\\u201D, \\u201Calert_severity\\u201D: \\u201C{{alert_severity}}\\u201D, \\u201Cstate_message\\u201D: \\u201C{{alert_event_samples}}\\u201D, \\u201Cmonitoring_tool\\u201D: \\u201CLogz.io\\u201D }\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics and Profiling Programmatically (JavaScript)\nDESCRIPTION: Illustrates configuring runtime metrics and memory profiling programmatically using the `start` function. The `metrics` object with `runtimeMetricsEnabled: true` enables metrics, and the `profiling` object with `memoryProfilingEnabled: true` enables memory profiling.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nstart({\n   serviceName: 'my-node-service',\n   metrics: { runtimeMetricsEnabled: true },\n   profiling: { memoryProfilingEnabled: true }\n});\n```\n\n----------------------------------------\n\nTITLE: Enabling Parameter Persistence for Chocolatey Upgrades\nDESCRIPTION: Enables the Chocolatey feature that remembers and reuses the arguments from previous installations during upgrades. This ensures that your configuration options are preserved when upgrading the Splunk OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/windows-upgrade.rst#2025-04-22_snippet_0\n\nLANGUAGE: PowerShell\nCODE:\n```\nchoco feature enable -n=useRememberedArgumentsForUpgrades\n```\n\n----------------------------------------\n\nTITLE: Correctly Indented OpenTelemetry Collector YAML Configuration\nDESCRIPTION: Properly indented YAML configuration for the 'syslog' receiver under the 'receivers' section. This demonstrates the correct indentation with 2 spaces that will pass validation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-troubleshoot.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  syslog:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: \"${SPLUNK_LISTEN_INTERFACE}:4317\"\n      http:\n        endpoint: \"${SPLUNK_LISTEN_INTERFACE}:4318\"\n```\n\n----------------------------------------\n\nTITLE: Including Kafka Metrics Receiver in Service Pipeline\nDESCRIPTION: Configuration to include the Kafka metrics receiver in the metrics pipeline of the service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kafkametrics-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [kafkametrics]\n```\n\n----------------------------------------\n\nTITLE: Setting Node Conditions to Report in Kubernetes Cluster Receiver\nDESCRIPTION: Configuration example for specifying which node conditions the Kubernetes cluster receiver should report as metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-cluster-receiver.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# ...\nk8s_cluster:\n  node_conditions_to_report:\n    - Ready\n    - MemoryPressure\n# ...\n```\n\n----------------------------------------\n\nTITLE: Alternative Initialization Method for React Native RUM\nDESCRIPTION: Alternative approach to initialize the React Native RUM library directly using the SplunkRum.init method instead of wrapping the App component.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/install-rum-react.rst#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { SplunkRum } from '@splunk/otel-react-native';\n\nconst Rum = SplunkRum.init({\n   realm: '<realm>',\n   applicationName: '<name-of-app>',\n   rumAccessToken: '<access-token>',\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Host Name in OpenTelemetry for Windows PowerShell\nDESCRIPTION: Uses the OTEL_RESOURCE_ATTRIBUTES environment variable to override the default host name in Windows PowerShell. Replace <host_name> with your desired host name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/apm-api-define-host.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_RESOURCE_ATTRIBUTES=host.name=<host_name>\n```\n\n----------------------------------------\n\nTITLE: Setting Deployment Environment in Android RUM\nDESCRIPTION: Sets the deployment environment name (e.g., 'dev') that appears in the 'deployment.environment' attribute of all spans.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/rum-android-data-model.rst#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\ndeploymentEnvironment(String)\n```\n\n----------------------------------------\n\nTITLE: Activating Apache Web Server Receiver OpenTelemetry YAML\nDESCRIPTION: This snippet demonstrates how to activate the Apache Web Server receiver by adding the 'apache' section to the 'receivers' part of the YAML configuration file. The 'endpoint' specifies the URL of the Apache status endpoint, and 'collection_interval' defines the frequency at which metrics are collected. The configuration is added to the service's 'metrics' pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/apache-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\\n  apache:\\n    endpoint: \\\"http://localhost:8080/server-status?auto\\\"\\n    collection_interval: 10s\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\\n  pipelines:\\n    metrics:\\n      receivers: [apache]\n```\n\n----------------------------------------\n\nTITLE: Including Splunk Enterprise Receiver in Metrics Pipeline\nDESCRIPTION: Shows how to include the Splunk Enterprise receiver in the metrics pipeline of the service section in the configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-enterprise-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [splunkenterprise]\n```\n\n----------------------------------------\n\nTITLE: Ignore Specific Endpoints in Java Application Tracing\nDESCRIPTION: These commands configure sampling rules to ignore specific endpoints, such as a health check, in trace data collection by the OpenTelemetry Java agent. This helps in filtering out unwanted trace data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/instrument-java-application.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_TRACES_SAMPLER=rules\n      export OTEL_TRACES_SAMPLER_ARG=drop=/healthcheck;fallback=parentbased_always_on\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Kubernetes Attributes Processor Configuration\nDESCRIPTION: Advanced configuration example for the Kubernetes attributes processor, including extracted metadata, annotations, labels, and association lists.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-attributes-processor.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nk8sattributes/demo:\n  auth_type: \"serviceAccount\"\n  passthrough: false\n  filter:\n    node_from_env_var: <KUBE_NODE_NAME>\n  extract:\n    metadata:\n      - k8s.pod.name\n      - k8s.pod.uid\n      - k8s.deployment.name\n      - k8s.namespace.name\n      - k8s.node.name\n      - k8s.pod.start_time\n  annotations:\n    - key_regex: opentel.* # extracts Keys & values of annotations matching regex `opentel.*`\n      from: pod\n  labels:\n    - key_regex: opentel.* # extracts Keys & values of labels matching regex `opentel.*`\n      from: pod\n  pod_association:\n    - sources:\n       - from: resource_attribute\n         name: k8s.pod.ip\n    - sources:\n       - from: resource_attribute\n         name: k8s.pod.uid\n    - sources:\n       - from: connection\n```\n\n----------------------------------------\n\nTITLE: Overriding Default Metric Exclusions in SignalFx Exporter YAML\nDESCRIPTION: Example of how to override default metric exclusions and manually include specific metrics using the include_metrics option in the SignalFx exporter configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-exporter.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  signalfx:\n    include_metrics:\n      - metric_names: [cpu.interrupt, cpu.user, cpu.system]\n      - metric_name: system.cpu.time\n        dimensions:\n          state: [interrupt, user, system]\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Response for Average CPU Usage Aggregation\nDESCRIPTION: This JSON snippet shows the response from Elasticsearch for the average CPU usage aggregation, containing buckets for different hosts with their respective average CPU usage values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/elasticsearch-query.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"aggregations\" : {\n\"host\" : {\n    \"doc_count_error_upper_bound\" : 0,\n    \"sum_other_doc_count\" : 0,\n    \"buckets\" : [\n    {\n        \"key\" : \"helsinki\",\n        \"doc_count\" : 13802,\n        \"average_cpu_usage\" : {\n        \"value\" : 49.77438052456166\n        }\n    },\n    {\n        \"key\" : \"lisbon\",\n        \"doc_count\" : 13802,\n        \"average_cpu_usage\" : {\n        \"value\" : 49.919866685987536\n        }\n    },\n    {\n        \"key\" : \"madrid\",\n        \"doc_count\" : 13802,\n        \"average_cpu_usage\" : {\n        \"value\" : 49.878350963628456\n        }\n    },\n    {\n        \"key\" : \"nairobi\",\n        \"doc_count\" : 13802,\n        \"average_cpu_usage\" : {\n        \"value\" : 49.99789885523837\n        }\n    }\n    ]\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Nagios Monitor to Service Pipeline\nDESCRIPTION: Configuration snippet showing how to add the Nagios monitor to the metrics pipeline in the collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/nagios.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/nagios]\n```\n\n----------------------------------------\n\nTITLE: Configuring Restricted HAProxy TCP Socket in YAML\nDESCRIPTION: Example YAML configuration for setting up a more restricted TCP socket in HAProxy. This includes defining a backend server, frontend proxy, and using ACLs to control access.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/haproxy.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nglobal\n    daemon\n    stats socket localhost:9000\n    stats timeout 2m\n\nbackend stats-backend\n    mode tcp\n    server stats-localhost localhost:9000\n\nfrontend stats-frontend\n    bind *:9001\n    default_backend stats-backend\n    acl ...\n    acl ...\n```\n\n----------------------------------------\n\nTITLE: Configuring JMX Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic YAML configuration to activate the JMX monitor in the Collector's smart agent receiver\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/jmx.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/jmx:\n    type: jmx\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug Logging and Log Export\nDESCRIPTION: Extended YAML configuration that enables debug logging and configures the collector to export its own logs to Splunk Platform or Splunk Observability.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  config:\n    service:\n      telemetry:\n        logs:\n          # Enable debug logging from the collector.\n          level: debug\n# Optional for exporting logs.\nlogsCollection:\n  containers:\n    # Enable the logs from the collector/agent to be collected at the container level.\n    excludeAgentLogs: false\n```\n\n----------------------------------------\n\nTITLE: Modifying Splunk On-Call Webhook URL for Custom Alert Severity\nDESCRIPTION: This example shows how to append a query parameter (`?scalyrMessageType=WARNING`) to the Splunk On-Call webhook URL used in the Dataset `alertAddress`. This allows sending additional context to Splunk On-Call, which can be used by the Rules Engine (Enterprise feature) to alter the alert's severity (e.g., change `message_type` from the default `CRITICAL` to `WARNING`). Replace placeholders with your specific endpoint details.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/scalyr-integration.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nvictorops:webhookUrl=https://alert.victorops.com/integrations/generic/20131114/alert/<unique_victorops-generated_endpoint>/<routing_key>/?scalyrMessageType=WARNING\n```\n\n----------------------------------------\n\nTITLE: Upgrading Splunk OTel Auto-Instrumentation on Debian\nDESCRIPTION: These commands update and upgrade the splunk-otel-auto-instrumentation package on Debian-based systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get --only-upgrade splunk-otel-auto-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Collector with CRD configuration for new users\nDESCRIPTION: Helm command for new users to install the OpenTelemetry Collector with proper CRD configuration using the crds/ directory method.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release-name> splunk-otel-collector-chart/splunk-otel-collector --set operatorcrds.install=true,operator. enabled=true <extra_args>\n```\n\n----------------------------------------\n\nTITLE: Setting up custom procPath for Memory monitoring in Linux\nDESCRIPTION: Configuration for setting a custom path to the Linux proc filesystem when it's mounted somewhere other than the default /proc location.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cache/memory.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocPath: /proc\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Endpoints\nDESCRIPTION: Table of exposed endpoints showing protocol, address, port, and endpoint combinations with their respective descriptions. These endpoints are used for health checks, trace collection, metrics monitoring, and various receiver functionalities.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/exposed-endpoints.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp(s)://0.0.0.0:13133/ - Health check extension\nhttp(s)://0.0.0.0:[6831|6832|14250|14268]/api/traces - Jaeger receiver\nhttp(s)://localhost:55679/debug/[tracez|pipelinez] - zPages extension\nhttp(s)://0.0.0.0:[4317|4318] - OTLP receiver\nhttp(s)://0.0.0.0:6060 - HTTP forwarder\nhttp://localhost:8888/metrics - Internal Prometheus metrics\nhttp(s)://localhost:8006 - Fluent forward receiver\nhttp(s)://0.0.0.0:9080 - Smart Agent receiver\nhttp(s)://0.0.0.0:9411/api/[v1|v2]/spans - Zipkin receiver\nhttp(s)://0.0.0.0:9943 - SignalFx receiver\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Components in Splunk OpenTelemetry Collector Configuration\nDESCRIPTION: Defines the core components for a Collector configuration including extensions, receivers, processors, and exporters in YAML format.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-start.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  health_check:\n    endpoint: \"${SPLUNK_LISTEN_INTERFACE}:13133\"\n\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: \"${SPLUNK_LISTEN_INTERFACE}:4317\"\n      http:\n        endpoint: \"${SPLUNK_LISTEN_INTERFACE}:4318\"\n\nprocessors:\n  batch:\n\nexporters:\n  otlp:\n    endpoint: \"${SPLUNK_GATEWAY_URL}:4317\"\n    tls:\n      insecure: true\n```\n\n----------------------------------------\n\nTITLE: Inspecting Performance Counter Sets and Instances - PowerShell\nDESCRIPTION: This set of PowerShell commands allows the user to list all available performance counter sets, fetch instances for a specific counter set, and open the Windows Performance Monitor GUI. The commands assist in identifying counters and their structures for use in Collector configurations. Inputs: (optional) name of performance object. Outputs: Lists of counter sets or instances as applicable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/windowsperfcounters-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\nGet-Counter -ListSet *\n```\n\nLANGUAGE: powershell\nCODE:\n```\nGet-Counter -List \"<perf_object_name>\"\n```\n\nLANGUAGE: powershell\nCODE:\n```\nperfmon /sys\n```\n\n----------------------------------------\n\nTITLE: Updating Prometheus Receiver Port in OpenTelemetry Collector\nDESCRIPTION: YAML configuration to update the Prometheus internal receiver to use port 8889 instead of the default 8888. This modification is necessary when resolving port conflicts.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/splunk-collector-troubleshooting.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  prometheus/internal:\n    config:\n      scrape_configs:\n      - job_name: 'otel-collector'\n        scrape_interval: 10s\n        static_configs:\n        - targets: ['0.0.0.0:8889']\n```\n\n----------------------------------------\n\nTITLE: Configuring Logs Exporter for SDKs (Shell)\nDESCRIPTION: Specifies the exporter (e.g., 'otlp') for logs collected by activated SDKs. Set to 'none' to disable log collection/export. Sets the 'OTEL_LOGS_EXPORTER' environment variable. If omitted, each SDK uses its default exporter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\n--logs-exporter <exporter>\n```\n\n----------------------------------------\n\nTITLE: Advanced SNMP Monitor Configuration in YAML\nDESCRIPTION: Detailed configuration example showing SNMP monitor setup with specific fields and OIDs\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/snmp.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/snmp:\n    type: telegraf/snmp\n    agents: \"127.0.0.1:161\"\n    version: 2\n    community: \"public\"\n    fields:\n        name: \"uptime\"\n        oid: \".1.3.6.1.2.1.1.3.0\"\n```\n\n----------------------------------------\n\nTITLE: Cassandra Node Status Monitoring Script\nDESCRIPTION: Groovy script that queries Cassandra JMX endpoint to extract node status metrics and other operational data points\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/jmx.rst#2025-04-22_snippet_2\n\nLANGUAGE: groovy\nCODE:\n```\n// Query the JMX endpoint for a single MBean.\nss = util.queryJMX(\"org.apache.cassandra.db:type=StorageService\").first()\n\n// Copied and modified from https://github.com/apache/cassandra\ndef parseFileSize(String value) {\n    if (!value.matches(\"\\\\d+(\\\\.\\\\d+)? (GiB|KiB|MiB|TiB|bytes)\")) {\n        throw new IllegalArgumentException(\n            String.format(\"value %s is not a valid human-readable file size\", value));\n    }\n    if (value.endsWith(\" TiB\")) {\n        return Math.round(Double.valueOf(value.replace(\" TiB\", \"\")) * 1e12);\n    }\n    else if (value.endsWith(\" GiB\")) {\n        return Math.round(Double.valueOf(value.replace(\" GiB\", \"\")) * 1e9);\n    }\n    else if (value.endsWith(\" KiB\")) {\n        return Math.round(Double.valueOf(value.replace(\" KiB\", \"\")) * 1e3);\n    }\n    else if (value.endsWith(\" MiB\")) {\n        return Math.round(Double.valueOf(value.replace(\" MiB\", \"\")) * 1e6);\n    }\n    else if (value.endsWith(\" bytes\")) {\n        return Math.round(Double.valueOf(value.replace(\" bytes\", \"\")));\n    }\n    else {\n        throw new IllegalStateException(String.format(\"FileUtils.parseFileSize() reached an illegal state parsing %s\", value));\n    }\n}\n\nlocalEndpoint = ss.HostIdToEndpoint.get(ss.LocalHostId)\ndims = [host_id: ss.LocalHostId, cluster_name: ss.ClusterName]\n\noutput.sendDatapoints([\n    // Equivalent of \"Up/Down\" in the `nodetool status` output.\n    // 1 = Live; 0 = Dead; -1 = Unknown\n    util.makeGauge(\n        \"cassandra.status\",\n        ss.LiveNodes.contains(localEndpoint) ? 1 : (ss.DeadNodes.contains(localEndpoint) ? 0 : -1),\n        dims),\n\n    util.makeGauge(\n        \"cassandra.state\",\n        ss.JoiningNodes.contains(localEndpoint) ? 3 : (ss.LeavingNodes.contains(localEndpoint) ? 2 : 1),\n        dims),\n\n    util.makeGauge(\n        \"cassandra.load\",\n        parseFileSize(ss.LoadString),\n        dims),\n\n    util.makeGauge(\n        \"cassandra.ownership\",\n        ss.Ownership.get(InetAddress.getByName(localEndpoint)),\n        dims)\n    ])\n```\n\n----------------------------------------\n\nTITLE: Image URL Annotation Payload - Splunk On-Call REST Endpoint - JSON\nDESCRIPTION: Example JSON payload demonstrating the use of an image URL annotation via 'vo_annotate.i.Graph'. This allows referencing external images relevant to the incident. Payload includes all typical alert fields, with the annotation URL pointing to an image; annotation size limits apply.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rest-endpoint-integration-guide.rst#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{ \\u201cmonitoring_tool\\u201d: \\u201cAPI\\u201d, \\u201cmessage_type\\u201d:\\u201cINFO\\u201d,\\n\\u201centity_id\\u201d:\\u201cdisk.space/db01\\u201d, \\u201centity_display_name\\u201d:\\u201cApproaching Low\\nDisk Space on DB01\\u201d, \\u201cstate_message\\u201d:\\u201cThe disk is really really full.\\nHere is a bunch of information about the problem\\u201d,\\n\\u201cvo_annotate.i.Graph\\u201d:\\u201chttps://community.iotawatt.com/uploads/db6340/original/1X/266a3917cc86317830ae9cda3e91c7689a6c73a7.png\\u201d\\n }\n```\n\n----------------------------------------\n\nTITLE: Building OpenTelemetry C++ Libraries using Bash and CMake\nDESCRIPTION: Navigates into the cloned `opentelemetry-cpp` directory, creates a `build` subdirectory, configures the project using CMake, and then compiles the OpenTelemetry C++ libraries. This requires CMake (version 3.20+) and a C++14 compatible compiler to be installed.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/cpp/instrument-cpp.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd opentelemetry-cpp\nmkdir build\ncd build\ncmake ..\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector Ansible Collection\nDESCRIPTION: Command to install the Ansible collection for Splunk OpenTelemetry Collector from Ansible Galaxy.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/deployments-linux-ansible.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nansible-galaxy collection install signalfx.splunk_otel_collector\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Watcher Webhook Action for Splunk On-Call (JSON)\nDESCRIPTION: This JSON snippet defines the `actions` object within an Elasticsearch Watch configuration, specifically for sending alerts to Splunk On-Call via a webhook. It specifies the Splunk On-Call alert API endpoint, HTTP method (POST), headers, and the structure of the alert payload using Mustache templates (`{{...}}`) to dynamically insert watch context. The `$service_api_key` and `$routing_key` placeholders must be replaced with actual values from your Splunk On-Call integration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/elasticsearch-watcher-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"actions\": {\n \"victorops\": {\n   \"webhook\": {\n     \"scheme\": \"https\",\n     \"host\": \"alert.victorops.com\",\n     \"port\": 443,\n     \"method\": \"post\",\n     \"path\": \"/integrations/generic/20131114/alert/$service_api_key/$routing_key\",\n     \"params\": {},\n     \"headers\": {\n       \"Content-type\": \"application/json\"\n     },\n     \"body\": \"{\\\"message_type\\\": \\\"CRITICAL\\\",\\\"monitoring_tool\\\": \\\"Elastic Watcher\\\",\\\"entity_id\\\": \\\"{{ctx.id}}\\\",\\\"entity_display_name\\\": \\\"{{ctx.watch_id}}\\\",\\\"state_message\\\": \\\"{{ctx.watch_id}}\\\",\\\"elastic_watcher_payload\\\": {{#toJson}}ctx.payload{{/toJson}} }\"\n   }\n }\n}\n```\n\n----------------------------------------\n\nTITLE: Resource Configuration for Splunk OpenTelemetry Collector - YAML\nDESCRIPTION: This YAML configuration sets the default resource limits for Splunk's OpenTelemetry Collector on a Kubernetes node.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nresources:\n      limits:\n         cpu: 4\n         memory: 8Gi\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Python Requirements\nDESCRIPTION: Package requirements for implementing OpenTelemetry metrics collection in Python with specific version constraints.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/other-ingestion-methods/send-custom-metrics.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nopentelemetry-api==1.12.0\nopentelemetry-sdk==1.12.0\nopentelemetry-proto==1.12.0\nopentelemetry-exporter-otlp-proto-grpc==1.12.0\n```\n\n----------------------------------------\n\nTITLE: Including Chrony Receiver in Metrics Pipeline\nDESCRIPTION: YAML configuration to include the Chrony receiver in the metrics pipeline of the OpenTelemetry Collector service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/chrony-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers:\n        - chrony/defaults\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OTel Collector Proxy via Bash Commands (Linux/Systemd)\nDESCRIPTION: Provides Bash commands to configure proxy settings for both the Splunk OpenTelemetry Collector installer script (by modifying `/etc/environment`) and the running service (by creating a Systemd override file). Requires restarting the shell session, reloading the systemd daemon, and restarting the collector service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/allow-services.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Add proxy settings to the environment for the installer script\n\ncat <<EOF | sudo tee -a /etc/environment\nNO_PROXY=<address,anotheraddress>\nHTTP_PROXY=http://<proxy.address:port>\nHTTPS_PROXY=http://<proxy.address:port>\nEOF\n\n# You might need to restart your shell session at this point.\n\n# Add proxy configuration to the service-proxy.conf\n# file in /etc/systemd/system/splunk-otel-collector.service.d/\n\nsudo mkdir -p /etc/systemd/system/splunk-otel-collector.service.d/\n\ncat <<EOF | sudo tee -a /etc/systemd/system/splunk-otel-collector.service.d/service-proxy.conf\n[Service]\nEnvironment=\"NO_PROXY=<address,anotheraddress>\"\nEnvironment=\"HTTP_PROXY=http://<proxy.address:port>\"\nEnvironment=\"HTTPS_PROXY=http://<proxy.address:port>\"\nEOF\n\n# Reload systemd and splunk-otel-collector service afterwards\n\nsudo systemctl daemon-reload\nsudo systemctl restart splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Verifying updated access token\nDESCRIPTION: Command to verify the value of the updated access token after performing the upgrade.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm get values <Release_Name>\n```\n\n----------------------------------------\n\nTITLE: Server-Timing Header Format for Lambda Trace Information\nDESCRIPTION: The Splunk OpenTelemetry Lambda layer adds Server-Timing headers to HTTP responses to connect Real User Monitoring (RUM) requests with serverless trace data. The header contains the traceId and spanId parameters in traceparent format.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/aws/otel-lambda-layer/advanced-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAccess-Control-Expose-Headers: Server-Timing\nServer-Timing: traceparent;desc=\"00-<serverTraceId>-<serverSpanId>-01\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Webpack Externals for Node.js Instrumentation\nDESCRIPTION: This JavaScript snippet demonstrates how to configure Webpack to exclude specific Node.js modules, like 'express', from the bundle using the 'externals' option with 'externalsType' set to 'node-commonjs'. This allows OpenTelemetry to intercept the standard 'require' calls for these modules and apply instrumentation, which wouldn't be possible if they were bundled.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/troubleshooting/common-nodejs-troubleshooting.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nmodule.exports = {\n   // ...\n   externalsType: \"node-commonjs\",\n   externals: [\n      \"express\"\n   // See https://github.com/open-telemetry/opentelemetry-js-contrib/tree/main/plugins/node\n   // for a list of supported instrumentations. Use the require name of the library or framework,\n   // not the name of the instrumentation. For example, \"tedious\" instead of \"instrumentation-tedious\".\n   ]\n};\n```\n\n----------------------------------------\n\nTITLE: Using Publish Statements in SignalFlow\nDESCRIPTION: Shows the correct usage of publish statements in SignalFlow, including labeling. Each stream should have at most one publish statement as the last method.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/chart-builder.rst#2025-04-22_snippet_5\n\nLANGUAGE: SignalFlow\nCODE:\n```\nA = data('cpu.utilization').publish(label='A')\nB = (A).mean().publish(label='avg')\n```\n\n----------------------------------------\n\nTITLE: Configuring cAdvisor Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration example showing how to enable the cAdvisor monitor in the OpenTelemetry Collector. Defines the receiver configuration with type cadvisor.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/cadvisor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/cadvisor: \n    type: cadvisor\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Upgrading Zero-Code Instrumentation Package on Debian\nDESCRIPTION: Command to upgrade the splunk-otel-auto-instrumentation Debian package using dpkg. Requires sudo privileges and the package file path.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nsudo dpkg -i <path to splunk-otel-auto-instrumentation deb>\n```\n\n----------------------------------------\n\nTITLE: Initializing Splunk RUM with customized error instrumentation in HTML\nDESCRIPTION: This snippet demonstrates how to initialize Splunk RUM with the option to disable error instrumentation. It includes loading the Splunk RUM script and configuring it with a realm, access token, application name, version, and instrumentation options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/install-rum-browser.rst#2025-04-22_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\" crossorigin=\"anonymous\"></script>\n<script>\n   SplunkRum.init({\n      realm: '<realm>',\n      rumAccessToken: '<your_rum_token>',\n      applicationName: '<your_app_name>',\n      version: '<your_app_version>',\n      instrumentations: { errors: false }\n   });\n</script>\n```\n\n----------------------------------------\n\nTITLE: Collecting _Total Instance Data Separately for CPU - YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure the receiver to capture total CPU active and idle time using only the _Total instance, avoiding double counting by mapping this aggregate instance to a dedicated metric. Inputs: Processor object, instance _Total, relevant counter. Output: metric representing the total CPU time as an aggregate.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/windowsperfcounters-receiver.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nwindowsperfcounters:\n  metrics:\n    processor.time.total:\n      description: Total CPU active and idle time\n      unit: \"%\"\n      gauge:\n  collection_interval: 30s\n  perfcounters:\n    - object: \"Processor\"\n      instances:\n        - \"_Total\"\n      counters:\n        - name: \"% Processor Time\"\n          metric: processor.time.total\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Receiver with Environment Variables\nDESCRIPTION: Example of how to use environment variables to configure the Redis receiver, specifically for setting the password.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/redis-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  redis:\n    endpoint: \"localhost:6379\"\n    collection_interval: 10s\n    password: ${env:REDIS_PASSWORD}\n```\n\n----------------------------------------\n\nTITLE: Creating Oracle Database User and Granting Privileges (SQL)\nDESCRIPTION: This snippet shows SQL commands to create a new Oracle Database user and assign required read-only permissions on key system views and tablespaces. Required for the Oracle Database receiver to query metrics. Replace <username> and <password> with actual values. Permissions must be granted for all monitored metrics; missing privileges may cause metrics collection to fail. No external dependencies beyond Oracle Database itself.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/oracledb-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n-- Create user and set a password\\nCREATE USER <username> IDENTIFIED BY <password>;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nGRANT SELECT ON V_$SESSION TO <username>;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nGRANT SELECT ON V_$SYSSTAT TO <username>;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nGRANT SELECT ON V_$RESOURCE_LIMIT TO <username>;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nGRANT SELECT ON DBA_TABLESPACES TO <username>;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nGRANT SELECT ON DBA_DATA_FILES TO <username>;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nGRANT SELECT ON DBA_TABLESPACE_USAGE_METRICS TO <username>;\n```\n\n----------------------------------------\n\nTITLE: Linux/Windows Data Flow Diagram\nDESCRIPTION: Mermaid flowchart showing data flow through the OpenTelemetry Collector in Linux and Windows environments, including receivers, processors, exporters and data paths to Splunk platforms.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/opentelemetry.rst#2025-04-22_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n\n    accTitle: Splunk Distribution of the OpenTelemetry Collector diagram.\n    accDescr: The Splunk Distribution of the OpenTelemetry Collector contains receivers, processors, exporters, and extensions. Receivers gather metrics and logs from infrastructure, and metrics, traces, and logs from back-end applications. Receivers send data to processors, and processors send data to exporters. Exporters send data to Splunk Observability Cloud and Splunk Cloud Platform. Front-end experiences send data directly to Splunk Observability Cloud through RUM instrumentation.\n\n    subgraph \"\\nSplunk OpenTelemetry Collector for Linux and Windows\"\n    receivers\n    processors\n    exporters\n    extensions\n    end\n\n    Infrastructure -- \"metrics\" --> receivers\n    B[Back-end services] -- \"traces, metrics\" --> receivers\n    C[Front-end experiences] -- \"traces\" --> S[Splunk Observability Cloud]\n\n    receivers --> processors\n    processors --> exporters\n\n    exporters --> S[Splunk Observability Cloud]\n    exporters --> P[Splunk Cloud Platform]\n```\n\n----------------------------------------\n\nTITLE: Setting OTel Collector Endpoint in Linux\nDESCRIPTION: Configures the endpoint URL for the Splunk OpenTelemetry Collector in Linux when it's running on a different host than your application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\n----------------------------------------\n\nTITLE: Activating Kafka Receiver in YAML Configuration\nDESCRIPTION: This snippet shows how to activate the Kafka receiver in the OpenTelemetry Collector configuration file. It sets the protocol version and includes the receiver in a metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kafka-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kafka:\n    protocol_version: 2.0.0\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [kafka]\n```\n\n----------------------------------------\n\nTITLE: Including Snowflake Receiver in Metrics Pipeline (YAML)\nDESCRIPTION: This configuration example shows how to include the Snowflake receiver in the metrics pipeline of the service section in the OpenTelemetry Collector configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/snowflake-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [snowflake]\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Compose Configuration for Private Runner\nDESCRIPTION: YAML configuration for Docker Compose defining a Splunk Synthetic Monitoring private runner service with required environment variables, capabilities, and restart policy.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3'\nservices:\n  runner:\n    image: quay.io/signalfx/splunk-synthetics-runner:latest\n    environment:\n      RUNNER_TOKEN: YOUR_TOKEN_HERE\n    cap_add:\n      - NET_ADMIN\n    restart: always\n```\n\n----------------------------------------\n\nTITLE: Initializing Splunk RUM with Crash Reporting in Swift\nDESCRIPTION: Initialize both the iOS RUM library and the Crash Reporting module in Swift, allowing the application to send crash reports to Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/install-rum-ios.rst#2025-04-22_snippet_2\n\nLANGUAGE: swift\nCODE:\n```\nimport SplunkOtel\nimport SplunkOtelCrashReporting\n\nimport SplunkOtel\n//...\nSplunkRumBuilder(realm: \"<realm>\", rumAuth: \"<rum-token>\")\n   .deploymentEnvironment(environment: \"<environment>\")\n   .setApplicationName(\"<your_app_name>\")\n   .build()\n// Initialize crash reporting module after the iOS agent\nSplunkRumCrashReporting.start()\n```\n\n----------------------------------------\n\nTITLE: Configuring Fluent Forward Receiver in YAML for Splunk OpenTelemetry Collector\nDESCRIPTION: This YAML configuration snippet shows the default setup for the Fluent Forward receiver in the Splunk Distribution of OpenTelemetry Collector. It configures the receiver to listen on 127.0.0.1:8006 and includes it in the logs pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/fluentd-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  fluentforward:\n    endpoint: 127.0.0.1:8006\n\nservice:\n  pipelines:\n    logs:\n      receivers: [fluentforward]\n```\n\n----------------------------------------\n\nTITLE: Setting Docker Platform for macOS Silicon\nDESCRIPTION: Command to set Docker platform to linux/amd64 for macOS Silicon chip compatibility with splunk/splunk image.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/deploy-verify-environment.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_DEFAULT_PLATFORM=linux/amd64\n```\n\n----------------------------------------\n\nTITLE: Basic Statsd Receiver Configuration\nDESCRIPTION: Basic configuration to activate the Statsd receiver in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/statsd.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/statsd:\n    type: statsd\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring VictorOps Alert Extension in YAML\nDESCRIPTION: This YAML configuration sets up the VictorOps (Splunk On-Call) alert extension for AppDynamics. It includes API keys, routing keys, and connection settings for the integration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/appdynamics-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n#VictorOps Org Key \nvoOrganizationKey: \"<YOUR_SERVICE_API_KEY>\"\n\n#VictorOps Routing Key \nvoRoutingKey: \"<YOUR_ROUTING_KEY>\"\n\n#scheme used (http/https) \nprotocol: \"https\"\n\n#VictorOps host \nvoAlertHost: \"<alert.victorops.com>\"\n\n#VictorOps url path \nvoAlertUrlPath: \"</integrations/generic/20131114/alert>\"\n\n#http timeouts \nconnectTimeout: 10000 \nsocketTimeout: 10000\n\n#control level of details in VO alert \nshowDetails: false\n```\n\n----------------------------------------\n\nTITLE: Disabling Profiling Data in Splunk HEC Exporter Configuration in YAML\nDESCRIPTION: This snippet shows how to turn off AlwaysOn Profiling data for a specific host or container by setting the profiling_data_enabled option to false in the Splunk HEC exporter settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-exporter.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsplunk_hec:\n  token: \"${SPLUNK_HEC_TOKEN}\"\n  endpoint: \"${SPLUNK_HEC_URL}\"\n  source: \"otel\"\n  sourcetype: \"otel\"\n  profiling_data_enabled: false\n```\n\n----------------------------------------\n\nTITLE: Defining DevOps Contact Configuration in Nagios\nDESCRIPTION: Configuration for setting up a DevOps contact in Nagios that integrates with Splunk On-Call using the VictorOps_Contact template.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndefine contact{\nuse            VictorOps_Contact\nname           VictorOps_devops\ncontact_name   VictorOps_devops\nalias          VictorOps_devops\n}\n```\n\n----------------------------------------\n\nTITLE: Uninstalling both Collector and Fluentd using the installer script on Linux\nDESCRIPTION: This command downloads the latest installer script and runs it with the uninstall option to remove both the Splunk OpenTelemetry Collector and Fluentd (td-agent) packages from a Linux system.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-uninstall.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh;\nsudo sh /tmp/splunk-otel-collector.sh --uninstall\n```\n\n----------------------------------------\n\nTITLE: Rotating a Specific Access Token with 7-Day Grace Period via API - Bash\nDESCRIPTION: This bash code snippet demonstrates how to use curl to rotate the access token 'myToken' in the us0 realm of Splunk Observability Cloud, setting a grace period of 6,048,000 seconds (7 days) for the previous token secret. The API call requires authorization via X-SF-TOKEN as a header and a valid API token value, and content type must be application/json. Inputs include the specific token name (myToken), realm (us0), and authorization token (123456abcd); the API response will indicate success or error. This example helps automate token rotation workflows for operational or integration purposes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/authentication-tokens/org-tokens.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"https://api.us0.signalfx.com/v2/token/myToken/rotate?graceful=6048000\" \\\n   -H \"Content-type: application/json\" \\\n   -H \"X-SF-TOKEN: <123456abcd>\"\n```\n\n----------------------------------------\n\nTITLE: Ember.js Error Handler Integration\nDESCRIPTION: Implementation of error handler for Ember.js using Splunk RUM integration\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nimport Ember from 'ember';\nimport SplunkRum from '@splunk/otel-web';\n\nEmber.onerror = function(error) {\n// To avoid loading issues due to content blockers\n// when using the CDN version of the Browser RUM\n// agent, add if (window.SplunkRum) checks around\n// SplunkRum API calls\n   SplunkRum.error(error)\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Basic CloudWatch API Data Collection IAM Policy in AWS\nDESCRIPTION: This JSON policy grants permissions for Splunk Observability Cloud to collect AWS metrics using the CloudWatch API. It includes required permissions for metric collection, region discovery, organization details, and tag synchronization.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-prereqs.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\", \n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cloudwatch:GetMetricData\",\n        \"cloudwatch:ListMetrics\",\n        \"ec2:DescribeRegions\",\n        \"organizations:DescribeOrganization\",\n        \"tag:GetResources\",\n        \"cloudformation:ListResources\",\n        \"cloudformation:GetResource\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Java Application Deployment YAML With OpenTelemetry Instrumentation\nDESCRIPTION: Kubernetes deployment manifest for a Java application with OpenTelemetry auto-instrumentation enabled through the inject-java annotation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-java-app\n  namespace: monitoring\nspec:\n  template:\n    metadata:\n      annotations:\n        instrumentation.opentelemetry.io/inject-java: \"true\"\n    spec:\n      containers:\n      - name: my-java-app\n        image: my-java-app:latest\n```\n\n----------------------------------------\n\nTITLE: Resolve Incident Request - Splunk On-Call REST Endpoint - JSON\nDESCRIPTION: Sample JSON request payload for resolving a previously opened incident in Splunk On-Call. The entity_id is kept consistent with the original incident. The required message_type is set to RECOVERY, with a revised state_message and other identifying fields; this is POSTed to the REST endpoint to mark the incident as resolved.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rest-endpoint-integration-guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \\u201cmessage_type\\u201d:\\u201cRECOVERY\\u201d, \\u201centity_id\\u201d:\\u201cdisk\\nspace/db01.mycompany.com\\u201d, \\u201centity_display_name\\u201d:\\u201cCritically Low Disk\\nSpace on DB01\\u201d, \\u201cstate_message\\u201d:\\u201cMemory was added to the disk. All is\\nwell now\\u201d }\n```\n\n----------------------------------------\n\nTITLE: Configuring HAProxy Socket in YAML\nDESCRIPTION: Example YAML configuration for defining the HAProxy socket file location in the HAProxy configuration file. This shows how to set up a stats socket for monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/haproxy.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nglobal\n    daemon\n    stats socket /var/run/haproxy.sock\n    stats timeout 2m\n```\n\n----------------------------------------\n\nTITLE: Muting Noisy Alerts with Wildcard Match in Splunk On-Call Rules Engine\nDESCRIPTION: This rule mutes noisy alerts by changing the message_type to INFO when the state_message contains the word 'spam'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-transformations.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nWhen state_message matches *spam* using Wildcard match\n\nSet message_type to INFO\n```\n\n----------------------------------------\n\nTITLE: Configuring Pod Annotations for Prometheus Scraping in YAML\nDESCRIPTION: YAML configuration to add annotations to pods that expose Prometheus metrics, defining the scrape path and port for the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmetadata:\n   annotations:\n      prometheus.io/scrape: \"true\"\n      prometheus.io/path: /metrics\n      prometheus.io/port: \"8080\"\n```\n\n----------------------------------------\n\nTITLE: Identifying Received Span Bytes Metric in Splunk APM\nDESCRIPTION: Splunk Observability Cloud metric identifier (`sf.org.apm.numSpanBytesReceived`) representing the number of bytes Splunk APM accepts from ingested span data after decompression and discarding invalid/throttled spans. This metric appears in the 'Trace Volume' chart on the APM Subscription Usage page for both TAPM and host-based subscription plans.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/apm-billing-usage-index.rst#2025-04-22_snippet_1\n\nLANGUAGE: metric\nCODE:\n```\nsf.org.apm.numSpanBytesReceived\n```\n\n----------------------------------------\n\nTITLE: Ending a Span in OpenTelemetry Go\nDESCRIPTION: This snippet instructs on ending a span with OpenTelemetry's span.End() method in Go. Used with defer, it ensures spans are properly closed and telemetry is sent. Omitting this call can result in incomplete trace data in the backend. Works with spans created via OpenTelemetry only.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/migrate-signalfx-go-to-otel.rst#2025-04-22_snippet_6\n\nLANGUAGE: go\nCODE:\n```\n   defer span.End()\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Custom Metrics in PHP\nDESCRIPTION: Includes necessary OpenTelemetry SDK classes for creating custom metrics, such as `ConsoleMetricExporterFactory`, `MeterProvider`, and `ExportingReader`. It also requires the Composer `vendor/autoload.php` file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-manual-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: php\nCODE:\n```\nuse OpenTelemetry\\SDK\\Metrics\\MetricExporter\\ConsoleMetricExporterFactory;\nuse OpenTelemetry\\SDK\\Metrics\\MeterProvider;\nuse OpenTelemetry\\SDK\\Metrics\\MetricReader\\ExportingReader;\n\nrequire 'vendor/autoload.php';\n```\n\n----------------------------------------\n\nTITLE: Scaling CPU Usage Values\nDESCRIPTION: Scales system.cpu.usage values by multiplying them by 1000 to convert from seconds to milliseconds.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: system.cpu.usage\naction: update\noperations:\n    - action: experimental_scale_value\n      experimental_scale: 1000\n```\n\n----------------------------------------\n\nTITLE: Adding Jaeger gRPC Monitor to Service Pipeline\nDESCRIPTION: Configuration to add the Jaeger gRPC monitor to the metrics pipeline in the service section of the collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/jaeger-grpc.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/jaeger-grpc]\n```\n\n----------------------------------------\n\nTITLE: Example Server-Timing Header with Traceparent in Shell\nDESCRIPTION: This snippet shows an example of a Server-Timing header containing a traceparent value. The traceparent includes version, trace-id, parent-id, and trace-flags.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nServer-Timing: traceparent;desc=\"00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01\"\n```\n\n----------------------------------------\n\nTITLE: Adding Logstash Monitor to Metrics Pipeline\nDESCRIPTION: YAML configuration snippet showing how to add the Logstash monitor to the metrics pipeline in the OpenTelemetry Collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/logstash.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/logstash]\n```\n\n----------------------------------------\n\nTITLE: Configuring Host Bindings for Splunk OpenTelemetry Collector\nDESCRIPTION: This bash command shows how to configure the Splunk OpenTelemetry Collector to use host bindings when running in a Docker container.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n--set=splunk.discovery.extensions.docker_observer.config.use_host_bindings=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop Receiver in Splunk OpenTelemetry Collector\nDESCRIPTION: This YAML snippet shows how to configure the Hadoop receiver in the Splunk OpenTelemetry Collector. It sets up the smartagent/hadoop receiver with the collectd/hadoop type.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoop.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/hadoop:\n    type: collectd/hadoop\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Retrieving Kubernetes Resources with kubectl get\nDESCRIPTION: Display one or many resources running on Kubernetes. The command can be used to retrieve pods, configmaps, or daemonsets from a specific namespace or across all namespaces.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n <namespace>\nkubectl get configmap\nkubectl get ds\n```\n\n----------------------------------------\n\nTITLE: Configuring Related Content Tags for Kubernetes Logs\nDESCRIPTION: Defines the specific combinations of Kubernetes-related tags (fields) required in logs to enable the Related Content feature. These combinations ensure unique identification of Kubernetes resources like nodes, pods, and containers for accurate linking.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/relatedcontent.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n- ``k8s.cluster.name`` + ``k8s.node.name``\n- ``k8s.cluster.name`` + ``k8s.node.name`` (optional) + ``k8s.pod.name``\n- ``k8s.cluster.name`` + ``k8s.node.name`` (optional) + ``k8s.pod.name`` (optional) + ``container.id``\n```\n\n----------------------------------------\n\nTITLE: Google Chrome Recorder Navigate Step\nDESCRIPTION: Example of a navigation step in Google Chrome Recorder format showing how to navigate to a URL with assertions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/browser-test/set-up-browser-test.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{\n // Google Chrome Recorder\n  \"type\": \"navigate\",\n  \"url\": \"www.buttercupgames.com\",\n  \"assertedEvents\": [\n     {\n        \"type\": \"navigation\",\n        \"url\": \"www.buttercupgames.com\",\n        \"title\": \"Buttercup Games\"\n     }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Monitor Receiver\nDESCRIPTION: Basic configuration to activate the Zookeeper monitor in the Collector configuration. Specifies the receiver type and enables metric collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/apache-zookeeper.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n   smartagent/zookeeper:\n   type: collectd/zookeeper\n   ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Importing the OpenTelemetry PowerShell Module for IIS Instrumentation (PowerShell)\nDESCRIPTION: This PowerShell command imports the OpenTelemetry.DotNet.Auto.psm1 module, which provides cmdlets for enabling or disabling OpenTelemetry instrumentation on IIS application pools in .NET Framework. Prerequisites: PowerShell access, the OpenTelemetry.DotNet.Auto.psm1 file available in the module path. No parameters required for import; successful import makes instrumentation commands available. The application pool name parameter used later is case sensitive.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/advanced-config-iis-apps.rst#2025-04-22_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\nImport-Module \\\"OpenTelemetry.DotNet.Auto.psm1\\\"\n```\n\n----------------------------------------\n\nTITLE: Restarting Fluentd Service After Configuration Changes\nDESCRIPTION: PowerShell commands to restart the Fluentd Windows service after making configuration changes. This applies any modifications made to the Fluentd configuration files.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/windows-config-logs.rst#2025-04-22_snippet_2\n\nLANGUAGE: PowerShell\nCODE:\n```\nStop-Service fluentdwinsvc\nStart-Service fluentdwinsvc\n```\n\n----------------------------------------\n\nTITLE: Adding Span Attributes in Python using OpenTelemetry SDK\nDESCRIPTION: Illustrates retrieving the current trace span and setting a custom attribute ('my.attribute') using the OpenTelemetry Python SDK. It also mentions the alternative of defining global tags through the OTEL_RESOURCE_ATTRIBUTES environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/add-context-trace-span.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Splunk Distribution of OpenTelemetry Python\n\nfrom opentelemetry import trace\n\ncustomizedSpan = trace.get_current_span()\n\ncustomizedSpan.set_attribute(\"my.attribute\", \"value\")\n\n# You can also set global tags using the OTEL_RESOURCE_ATTRIBUTES\t\n# environment variable, which accepts a list of comma-separated key-value\n# pairs. For example, key1:val1,key2:val2.  \n```\n\n----------------------------------------\n\nTITLE: Describing Kubernetes Resources\nDESCRIPTION: Command to check and describe Kubernetes system configurations for specific pods in a namespace.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl describe -n <namepsace> pod <pod-name>\n```\n\n----------------------------------------\n\nTITLE: Upgrading Auto Instrumentation via DNF for .NET (Bash)\nDESCRIPTION: Upgrades the `splunk-otel-auto-instrumentation` package using the DNF package manager on modern RPM-based systems like Fedora or CentOS 8+. This is specifically shown in the context of upgrading the instrumentation for .NET applications. Requires `sudo` privileges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\nsudo dnf upgrade splunk-otel-auto-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Analyzing Go OpenTelemetry Log for Dropped Spans (Text)\nDESCRIPTION: Example debug log message indicating that the Go instrumentation dropped spans. The `total_dropped` value (1320 in this example) represents the cumulative number of spans dropped, likely because the batch span processor's queue was full. This suggests spans are being created faster than they can be exported or there are network issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/common-go-troubleshooting.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nexporting spans {\"count\": 364, \"total_dropped\": 1320}\n```\n\n----------------------------------------\n\nTITLE: Defining AutoDetect Detectors in SignalFlow - SignalFlow\nDESCRIPTION: This set of SignalFlow functions defines various AutoDetect detectors for monitoring specific metrics in Kubernetes, Oracle, Redis, and Splunk environments. The functions identify key parameters such as trigger threshold, sensitivity, and filters necessary for detecting anomalies or suboptimal conditions in the systems. Users can customize these parameters to tailor alerts to their specific operational requirements.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/autodetect/autodetect-list.rst#2025-04-22_snippet_4\n\nLANGUAGE: SignalFlow\nCODE:\n```\nSee the function in :new-page:`SignalFlow library <https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/k8s/daemonsets.flow#L5>` repository on GitHub.\n```\n\nLANGUAGE: SignalFlow\nCODE:\n```\nSee the function in :new-page:`SignalFlow library <https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/k8s/deployments.flow#L5>` repository on GitHub.\n```\n\nLANGUAGE: SignalFlow\nCODE:\n```\nSee the function in :new-page:`SignalFlow library <https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/k8s/containers.flow#L5>` repository on GitHub.\n```\n\nLANGUAGE: SignalFlow\nCODE:\n```\nSee the function in :new-page:`SignalFlow library <https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/k8s/nodes.flow#L21>` repository on GitHub.\n```\n\nLANGUAGE: SignalFlow\nCODE:\n```\nSee the function in :new-page:`SignalFlow library <https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/k8s/nodes.flow#L5>` repository on GitHub.\n```\n\nLANGUAGE: SignalFlow\nCODE:\n```\nSee the function in the :new-page:`SignalFlow library <https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/db/oracle.flow#L50>` repository on GitHub.\n```\n\nLANGUAGE: SignalFlow\nCODE:\n```\nSee the function in the :new-page:`SignalFlow library <https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/db/oracle.flow#L5>` repository on GitHub.\n```\n\nLANGUAGE: SignalFlow\nCODE:\n```\nSee the function in the :new-page:`SignalFlow library <https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/db/oracle.flow#L137>` repository on GitHub.\n```\n\nLANGUAGE: SignalFlow\nCODE:\n```\nSee the function in the :new-page:`SignalFlow library <https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/db/redis.flow#L6>` repository on GitHub.\n```\n\nLANGUAGE: SignalFlow\nCODE:\n```\nSee the APM ``operational.flow`` function in :new-page:`SignalFlow library <https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/apm/operational.flow#L4>` repository on GitHub.\n```\n\n----------------------------------------\n\nTITLE: Specifying Basic Authentication in Webhook URL (Password)\nDESCRIPTION: Illustrates how to include basic authentication credentials (username and password) directly within the webhook endpoint URL when configuring an outbound webhook in Splunk On-Call.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/admin/get-started/custom-outbound-webhooks.rst#2025-04-22_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nhttp://username:password@example.com/\n```\n\n----------------------------------------\n\nTITLE: Identifying Assembly Permission Errors in IIS-hosted .NET Applications\nDESCRIPTION: Error message that appears when using .NET Framework with IIS-hosted applications, indicating assembly loading permission issues. This requires a registry modification to resolve.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n[Exception] System.IO.FileLoadException \n[Message] \"Loading this assembly would produce a different grant set from other instances.\"\n```\n\n----------------------------------------\n\nTITLE: Activating Snowflake Receiver in OpenTelemetry Collector Configuration (YAML)\nDESCRIPTION: This snippet shows how to activate the Snowflake receiver in the OpenTelemetry Collector configuration file. It demonstrates adding the 'snowflake' receiver to the 'receivers' section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/snowflake-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  snowflake:\n```\n\n----------------------------------------\n\nTITLE: Checking OpenTelemetry Collector Logs\nDESCRIPTION: Commands to view collector logs on the host system using journalctl or tail.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\njournalctl -u splunk-otel-collector -f\ntail -100 /var/log/messages\n```\n\n----------------------------------------\n\nTITLE: Configuring Jaeger Thrift Receiver in OpenTelemetry Collector (YAML)\nDESCRIPTION: This YAML configuration snippet shows how to set up the Jaeger receiver within the Splunk Distribution of the OpenTelemetry Collector to accept spans in various Jaeger Thrift formats (gRPC, thrift_binary, thrift_compact, thrift_http) on specified endpoints. This allows the Collector to ingest trace data from applications instrumented with Jaeger Thrift clients.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-spans-traces/span-formats.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# To receive spans in Jaeger Thrift format\n\nreceivers:\n   jaeger:\n      protocols:\n         grpc:\n            endpoint: 0.0.0.0:14250\n         thrift_binary:\n            endpoint: 0.0.0.0:6832\n         thrift_compact:\n            endpoint: 0.0.0.0:6831\n         thrift_http:\n            endpoint: 0.0.0.0:14268\n```\n\n----------------------------------------\n\nTITLE: Integrating the Zipkin Receiver in the Service Pipeline (YAML)\nDESCRIPTION: This YAML snippet shows how to register the Zipkin receiver as a source for trace data within the service section of the OpenTelemetry Collector configuration. The receiver is referenced by name under the 'receivers' list for the 'traces' pipeline, ensuring that spans received via Zipkin will flow through the configured trace pipeline. The component name must match the definition in the receivers section. This integration is necessary for the Collector to process Zipkin traces.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/zipkin-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      receivers: [zipkin]\n```\n\n----------------------------------------\n\nTITLE: OTLP Exporter Error Log for Failed Span Export\nDESCRIPTION: Shows error log entries when the OTLP exporter can't send trace data to the OpenTelemetry Collector, indicating connectivity issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/troubleshooting/common-python-troubleshooting.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nDEBUG:opentelemetry.exporter.otlp.proto.grpc.exporter:Waiting 1s before retrying export of span\nDEBUG:opentelemetry.exporter.otlp.proto.grpc.exporter:Waiting 2s before retrying export of span\n```\n\n----------------------------------------\n\nTITLE: Activating Splunk Enterprise Receiver in YAML Configuration\nDESCRIPTION: Demonstrates how to activate the Splunk Enterprise receiver by adding it to the receivers section of the configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-enterprise-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  splunkenterprise:\n```\n\n----------------------------------------\n\nTITLE: Multi-Host HTTP Monitoring Configuration\nDESCRIPTION: Example configuration for monitoring multiple HTTP endpoints with separate receiver configurations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/http.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/host1:\n    type: http\n    ... # Additional config for host 1\n  smartagent/host2:\n    type: http\n    ... # Additional config for host 2\n```\n\n----------------------------------------\n\nTITLE: Installing .NET Zero-Code Instrumentation with Deployment Environment\nDESCRIPTION: Command to install the Splunk OpenTelemetry Collector with .NET instrumentation and define deployment environment resource attribute.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-instrumentation --deployment-environment prod \\\n--realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Setting Priority Class in Values YAML\nDESCRIPTION: Configuration example showing how to apply a custom priority class to the Collector pods in a values.yaml file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\npriorityClassName: splunk-otel-agent-priority\n```\n\n----------------------------------------\n\nTITLE: Receive Failure Metrics in OpenTelemetry Collector\nDESCRIPTION: Metrics that indicate reception failures and export failures in the OpenTelemetry Collector. Used to monitor client errors and backend connectivity issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/metrics-internal-collector.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\notelcol_receiver_refused_spans\notelcol_receiver_refused_metric_points\notelcol_receiver_refused_logs\notelcol_exporter_send_failed_spans\notelcol_exporter_send_failed_metric_points\notelcol_exporter_send_failed_logs\n```\n\n----------------------------------------\n\nTITLE: Installing Python Instrumentation Packages\nDESCRIPTION: Bootstrap command to install instrumentation for all supported packages in the environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/instrument-python-application.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nopentelemetry-bootstrap -a install\n```\n\n----------------------------------------\n\nTITLE: Setting Additional Environment Variables for Splunk OTel Collector in Chef\nDESCRIPTION: Example showing how to configure custom environment variables for the OpenTelemetry Collector service using the collector_additional_env_vars attribute. This allows the Collector to expand environment variable references in its configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/deployments-windows-chef.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncollector_additional_env_vars: {'MY_CUSTOM_VAR1' => 'value1', 'MY_CUSTOM_VAR2' => 'value2'}\n```\n\n----------------------------------------\n\nTITLE: Including Troubleshooting Components with HTML Wrapper\nDESCRIPTION: RST directives and HTML elements used to include external troubleshooting components content with div markers for start and end points.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-gitlab/gitlab.rst#2025-04-22_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"include-start\" id=\"troubleshooting-components.rst\"></div>\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /_includes/troubleshooting-components.rst\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n```\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"include-stop\" id=\"troubleshooting-components.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Creating Splunk Observability Access Token Secret with kubectl\nDESCRIPTION: kubectl command to create a Kubernetes secret containing the Splunk Observability Cloud access token. This command creates the secret directly without requiring a YAML file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s-addon-eks.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl create secret generic splunk-otel-collector \\\n    --from-literal=splunk_observability_access_token=<YOUR_ACCESS_TOKEN> \\\n    -n splunk-monitoring\n```\n\n----------------------------------------\n\nTITLE: Enabling Kubernetes collector in Network Explorer configuration YAML\nDESCRIPTION: This YAML configuration enables the Kubernetes collector by setting the enabled flag to true, allowing Network Explorer to collect Kubernetes metadata.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-troubleshoot.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nk8sCollector:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Activating Health Check Extension in YAML Configuration\nDESCRIPTION: This snippet shows how to activate the health_check extension in the OpenTelemetry Collector configuration file. It adds the extension to the 'extensions' section and includes it in the 'service' section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/health-check-extension.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  health_check:\n\nservice:\n  extensions: [health_check]\n```\n\n----------------------------------------\n\nTITLE: Identifying Active Containers Metric in Splunk APM (Host Plan)\nDESCRIPTION: Splunk Observability Cloud metric identifier (`sf.org.apm.numContainers`) representing the number of containers actively sending data to Splunk APM. This metric is specifically used in the 'Containers' chart on the APM Subscription Usage page for organizations with a host-based subscription plan.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/apm-billing-usage-index.rst#2025-04-22_snippet_5\n\nLANGUAGE: metric\nCODE:\n```\nsf.org.apm.numContainers\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Events Query Command\nDESCRIPTION: Command to view events happening in your Kubernetes cluster.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-events.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl get events -o yaml --all-namespaces\n```\n\n----------------------------------------\n\nTITLE: Downloading Nagios Plugin DEB Package using Shell\nDESCRIPTION: Downloads the Splunk On-Call Nagios plugin DEB package (version 1.4.20) from GitHub releases using the `wget` command. This package is suitable for Debian-based Linux distributions (like Ubuntu, Debian).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-nagios-1.4.20/victorops-nagios_1.4.20_all.deb\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP/HTTP Exporter Endpoints and Headers in YAML\nDESCRIPTION: This YAML snippet shows a basic configuration for the `otlphttp` exporter within the OpenTelemetry Collector. It defines the `traces_endpoint` and `metrics_endpoint` for sending trace and metric data respectively to specific Splunk Observability Cloud ingest URLs. It also includes the `headers` section to specify the required `X-SF-Token` for authentication with Splunk Observability Cloud, using `<access_token>` as a placeholder.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlphttp-exporter.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otlphttp:\n    # The target URL to send trace data to. By default it's set to ``https://ingest.${SPLUNK_REALM}.signalfx.com/v2/trace/otlp``.\n    traces_endpoint: https://ingest.<realm>.signalfx.com/v2/trace/otlp\n    # The target URL to send metrics data to. By default it's set to ``https://ingest.${SPLUNK_REALM}.signalfx.com/v2/datapoint/otlp``.\n    metrics_endpoint: https://ingest.<realm>.signalfx.com/v2/datapoint/otlp\n    # Set of HTTP headers added to every request.\n    headers:\n      # X-SF-Token is the authentication token provided by Splunk Observability Cloud.\n      X-SF-Token: <access_token>\n```\n\n----------------------------------------\n\nTITLE: Configuring Tomcat Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration example showing how to set up the Tomcat receiver in the OpenTelemetry Collector. Includes the receiver configuration and pipeline setup.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/apache-tomcat.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/tomcat:\n    type: collectd/tomcat\n    ...  # Additional config\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/tomcat]\n```\n\n----------------------------------------\n\nTITLE: Creating XCFramework for Splunk RUM iOS Library\nDESCRIPTION: Command to generate an XCFramework from iOS device and simulator archives, creating a distributable framework that can be imported into other iOS projects.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/install-rum-ios.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nxcodebuild -create-xcframework -archive archives/SplunkRum-iOS.xcarchive -framework SplunkOtel.framework -archive archives/SplunkRum-iOS_Simulator.xcarchive -framework SplunkOtel.framework -output xcframeworks/SplunkOtel.xcframework\n```\n\n----------------------------------------\n\nTITLE: Configuring Alternative Metrics Port in OpenTelemetry Collector\nDESCRIPTION: YAML configuration to modify the telemetry metrics address port from 8888 to 8889 to resolve port conflicts. This snippet shows how to update the service.telemetry.metrics section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/splunk-collector-troubleshooting.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  telemetry:\n    metrics:\n      address: \":8889\"\n```\n\n----------------------------------------\n\nTITLE: Installing the Collector with DNF Repository\nDESCRIPTION: Commands to install the required libcap dependency, set up the Splunk OTel Collector RPM repository, and install the Collector and optional zero-code instrumentation package using dnf.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndnf install -y libcap  # Required for enabling cap_dac_read_search and cap_sys_ptrace capabilities on the Collector\n\ncat <<EOH > /etc/yum.repos.d/splunk-otel-collector.repo\n[splunk-otel-collector]\nname=Splunk OpenTelemetry Collector Repository\nbaseurl=https://splunk.jfrog.io/splunk/otel-collector-rpm/release/\\$basearch\ngpgcheck=1\ngpgkey=https://splunk.jfrog.io/splunk/otel-collector-rpm/splunk-B3CD4420.pub\nenabled=1\nEOH\n\ndnf install -y splunk-otel-collector\n\n# Optional: install Splunk OpenTelemetry zero-code instrumentation\ndnf install -y splunk-otel-auto-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Basic vSphere Monitor Configuration in YAML\nDESCRIPTION: Basic configuration example showing how to activate the vSphere monitor in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/vsphere.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/vsphere:\n    type: vsphere\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Notification Script Parameters in PRTG\nDESCRIPTION: These parameters are entered into the 'Parameter' field within the PRTG notification settings when configuring the 'EXECUTE PROGRAM' action. They map PRTG's internal variables (like %sitename%, %device%, %status%) to the input parameters expected by the 'prtgtovictorops.ps1' PowerShell script. The 'URL_to_notify' placeholder must be replaced with the actual Service API Endpoint obtained from Splunk On-Call.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/prtg-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n-API_URL 'URL_to_notify' -SiteName '%sitename' -Device '%device'\n-DeviceId '%deviceid' -Name '%name' -Status '%status' -Down '%down'\n-DateTime '%datetime' -LinkDevice '%linkdevice' -Message '%message'\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Observer Configuration\nDESCRIPTION: Extended configuration example showing how to use the Kubernetes observer with receiver creator for pod discovery.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-apiserver.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  k8s_observer:\n\nreceivers:\n  receiver_creator/1:\n    watch_observers: [k8s_observer]\n    receivers:\n      smartagent/kubernetes-apiserver:\n        rule: type == \"pod\" && labels[\"k8s-app\"] == \"kube-apiserver\"\n        type: kubernetes-apiserver\n        port: 443\n        extraDimensions:\n          metric_source: kubernetes-apiserver\n\nprocessors:\n  exampleprocessor:\n\nexporters:\n  exampleexporter:\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [receiver_creator/1]\n      processors: [exampleprocessor]\n      exporters: [exampleexporter]\n  extensions: [k8s_observer]\n```\n\n----------------------------------------\n\nTITLE: Adding Note Annotation via REST API in JSON\nDESCRIPTION: This example shows how to add a note annotation to an alert using a REST-style integration. It includes standard alert fields and a note annotation with a 1124 character limit.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-annotations.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"monitoring_tool\": \"API\", \"message_type\":\"INFO\",\n\"entity_id\":\"disk.space/db01\", \"entity_display_name\":\"Approaching Low\nDisk Space on DB01\", \"state_message\":\"The disk is really really full.\nHere is a bunch of information about the problem\",\n\"vo_annotate.s.Note\":\"Once Disk Space is critically low there will be an\nincident!\" }\n```\n\n----------------------------------------\n\nTITLE: Including UDP Receiver in Logs Pipeline of OpenTelemetry Collector (YAML)\nDESCRIPTION: The snippet demonstrates how to include the udplog receiver in the logs pipeline within the service section of the configuration file. By listing udplog under receivers, the Collector processes incoming UDP log data through the logs pipeline. This configuration presumes that the receiver is defined elsewhere in the file, and is a necessary step to route incoming logs for further processing, exporting, and storage.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/udp-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    logs:\n      receivers: [udplog]        \n```\n\n----------------------------------------\n\nTITLE: Defining JSON Payload for Splunk On-Call to HipChat Webhook\nDESCRIPTION: This JSON object represents an example payload for a Splunk On-Call Outgoing Webhook configured for HipChat integration (Multi-Room setup). It specifies message properties like color, content (dynamically including the alert's display name using `${{ALERT.entity_display_name}}`), notification behavior, and text format according to the HipChat API. This payload is sent via HTTP POST to the configured HipChat room URL.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/hipchat-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \"color\": \"red\", \"message\": \"${{ALERT.entity_display_name}}\", \"notify\":\ntrue, \"message_format\": \"text\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct Data Export to Splunk Observability Cloud in Linux\nDESCRIPTION: Shell commands for Linux that set environment variables required to send telemetry data directly to Splunk Observability Cloud without using a local collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/instrument-ruby-application.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport SPLUNK_ACCESS_TOKEN=<access_token>\nexport SPLUNK_REALM=<realm>\n```\n\n----------------------------------------\n\nTITLE: Filtering Synthetic Runs by Port Test Type in Splunk\nDESCRIPTION: The filter `test_type=port` is used with metrics like `synthetics.run.count` to isolate and count synthetic runs originating from Port tests (TCP/UDP) in Splunk Synthetic Monitoring. This helps track usage for network service availability monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/synthetics-usage.rst#2025-04-22_snippet_4\n\nLANGUAGE: Splunk Metric/Filter\nCODE:\n```\ntest_type=port\n```\n\n----------------------------------------\n\nTITLE: Adding Windows IIS Monitor to Metrics Pipeline\nDESCRIPTION: Configuration snippet showing how to add the Windows IIS monitor to the metrics pipeline in the service configuration section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/microsoft-windows-iis.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/windows-iis]\n```\n\n----------------------------------------\n\nTITLE: Configuring Twilio SMS Function for Splunk On-Call Integration\nDESCRIPTION: JavaScript function that handles incoming SMS messages from Twilio and creates incidents in Splunk On-Call. Requires Twilio credentials and Splunk On-Call API keys. Sends a confirmation message back to the sender and creates an incident with the message content.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/twilio-sms-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst qs = require('qs');\nconst got = require('got');\nconst _ = require('lodash');\n\nexports.handler = function(context, event, callback) {\n  const {ROUTING_KEY, VICTOROPS_TWILIO_SERVICE_API_KEY} = context;\n  console.log(`${ROUTING_KEY} ${VICTOROPS_TWILIO_SERVICE_API_KEY}`);\n\n  var got = require('got');\n\n  let twiml = new Twilio.twiml.MessagingResponse();\n  twiml.message({\n    to: event.From\n  }, 'Incident Created');\n\n  var alert = {\n    monitoring_tool: 'Twilio',\n    message_type: 'critical',\n    entity_display_name: `${event.Body}`,\n    state_message: `From ${event.From} – ${event.Body}`,\n    entity_id: `${event.From}`\n  };\n\n  console.log(alert);\n\n  got.post(`https://alert.victorops.com/integrations/generic/20131114/alert/${VICTOROPS_TWILIO_SERVICE_API_KEY}/${ROUTING_KEY}`,\n    {\n      body: alert,\n      headers: {\n        'accept': 'application/json',\n        'Content-Type': 'application/json'\n      },\n      json: true\n    }).then(function(response) {\n      console.log(response.body);\n      callback(null, twiml);\n    }).catch(function(error) {\n      console.log(error);\n      callback(error);\n    });\n};\n```\n\n----------------------------------------\n\nTITLE: Installing Telegraf on Debian-based Linux\nDESCRIPTION: Shell commands to install Telegraf from the InfluxData repository, including repository key setup and package installation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/telegraf.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl --silent --location -O \\ https://repos.influxdata.com/influxdata-archive.key \\ && echo \"943666881a1b8d9b849b74caebf02d3465d6beb716510d86a39f6c8e8dac7515  influxdata-archive.key\" \\\n| sha256sum -c - && cat influxdata-archive.key \\\n| gpg --dearmor \\\n| sudo tee /etc/apt/trusted.gpg.d/influxdata-archive.gpg > /dev/null \\\n&& echo 'deb [signed-by=/etc/apt/trusted.gpg.d/influxdata-archive.gpg] https://repos.influxdata.com/debian stable main' \\\n| sudo tee /etc/apt/sources.list.d/influxdata.list\nsudo apt-get update && sudo apt-get install telegraf\n```\n\n----------------------------------------\n\nTITLE: Manually Tracking Navigation Events in Android RUM\nDESCRIPTION: Demonstrates how to explicitly set screen names to track navigation in UI frameworks that don't use the standard Fragment/Activity lifecycle.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nSplunkRum.getInstance().experimentalSetScreenName(screenName);\n```\n\n----------------------------------------\n\nTITLE: Adding Micrometer Core Dependency with Gradle - Java\nDESCRIPTION: This Gradle code snippet declares a dependency on the 'micrometer-core' library (version 1.7.5 or later) for Micrometer-based metric instrumentation in Java applications. Place this statement in your build.gradle file under the dependencies section. This enables the use of Micrometer metrics within your application using Gradle as the build system.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/version1x/java-manual-instrumentation-1x.rst#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimplementation(\"io.micrometer:micrometer-core:1.7.5\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Global Attributes in Splunk RUM JavaScript\nDESCRIPTION: Shows how to set global attributes during RUM initialization for storing static properties like account type and app release version.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/migrate-manual-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nSplunkRum.init( {\n   beaconEndpoint: '...',\n   rumAccessToken: '...',\n   globalAttributes: {\n      'account.type': goldStatus,\n      'app.release': getReleaseNumber(),\n   },\n});\n```\n\n----------------------------------------\n\nTITLE: Monitoring Token-Specific Computation Throttling in Splunk\nDESCRIPTION: This metric (`sf.org.numComputationsThrottledByToken`) tracks the number of computations throttled specifically for an individual token. It helps identify if a single token is responsible for hitting computation capacity limits compared to the overall organization limit monitored by `sf.org.numComputationsThrottled`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.numComputationsThrottledByToken\n```\n\n----------------------------------------\n\nTITLE: Monitoring Kafka Under-Replicated Partitions\nDESCRIPTION: This SignalFlow function identifies under-replicated Kafka partitions, alerting when certain thresholds are met over a specific duration. Customizable parameters involve setting trigger thresholds and sensitivity levels to ensure timely detection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/autodetect/autodetect-list.rst#2025-04-22_snippet_2\n\nLANGUAGE: SignalFlow\nCODE:\n```\n\"https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/kafka/broker.flow#L18\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Gateway Mode Endpoint\nDESCRIPTION: YAML configuration for specifying a gateway endpoint in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-advanced-config.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nclusterName: myCluster\nsplunkObservability:\n  realm: <splunk-realm>\n  accessToken: <splunk-access-token>\nenvironment: prd\ncertmanager:\n  enabled: true\noperatorcrds:\n  install: true\noperator:\n  enabled: true\ninstrumentation:\n  spec:\n    exporter:\n      endpoint: <gateway-endpoint>\n```\n\n----------------------------------------\n\nTITLE: HTML Component Headers Definition\nDESCRIPTION: HTML embed tags defining section headers with anchors for the different collector components.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components.rst#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<embed>\n  <h2>Receivers<a name=\"collector-components-receivers\" class=\"headerlink\" href=\"#collector-components-receivers\" title=\"Permalink to this headline\">¶</a></h2>\n</embed>\n```\n\n----------------------------------------\n\nTITLE: Adding Internal Metrics Monitoring to Smart Agent Configuration\nDESCRIPTION: Configuration snippet for enabling the internal-metrics monitor in Smart Agent to track its performance and load. This helps in debugging issues and ensuring the Collector isn't overloaded.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nmonitors:\n   - type: internal-metrics\n```\n\n----------------------------------------\n\nTITLE: Integrating OTLP Exporter into Service Pipelines in YAML\nDESCRIPTION: This YAML configuration demonstrates how to include the previously defined 'otlp' exporter in the processing pipelines for metrics, logs, and traces within the OpenTelemetry Collector's service configuration. It assumes 'otlp' is already declared under 'exporters'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlp-exporter.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n.. code-block:: yaml\n\n  service:\n    pipelines:\n      metrics:\n        processors: [otlp]\n      logs:\n        processors: [otlp]\n      traces:\n        processors: [otlp]\n```\n\n----------------------------------------\n\nTITLE: Adding Docker Container Stats Receiver to Metrics Pipeline\nDESCRIPTION: YAML configuration snippet for adding the Docker container stats receiver to the metrics pipeline in the Splunk OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/docker.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n pipelines:\n   metrics:\n     receivers: [smartagent/docker-container-stats]\n```\n\n----------------------------------------\n\nTITLE: Analyzing Go OpenTelemetry Log for Successful Span Export (Text)\nDESCRIPTION: Example debug log message indicating that the Go instrumentation successfully exported a batch of spans. The `count` field shows the number of spans exported in this batch (154 in this example), and `total_dropped` shows the cumulative number of dropped spans (0 here). A non-zero `count` suggests spans are being exported, and potential issues might be in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/common-go-troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nexporting spans {\"count\": 154, \"total_dropped\": 0}\n```\n\n----------------------------------------\n\nTITLE: Configuring Net-IO Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration for activating the net-io monitor in the Collector configuration. Specifies the receiver type and allows for additional configuration options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/net-io.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/net-io:\n    type: net-io\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Enabling Gateway Mode Configuration\nDESCRIPTION: YAML configuration for enabling gateway mode and disabling agent mode in the Collector setup.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-advanced-config.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nclusterName: myCluster\nsplunkObservability:\n  realm: <splunk-realm>\n  accessToken: <splunk-access-token>\nenvironment: prd\ncertmanager:\n  enabled: true\noperatorcrds:\n  install: true\noperator:\n  enabled: true\nagent:\n  enabled: false\ngateway:\n  enabled: true\n```\n\n----------------------------------------\n\nTITLE: Installing .NET Zero-Code Instrumentation with AlwaysOn Profiling\nDESCRIPTION: Command to install the Splunk OpenTelemetry Collector with .NET instrumentation and enable AlwaysOn Profiling features for CPU, memory, and metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-instrumentation --deployment-environment prod \\\n--realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN> \\\n--enable-profiler --enable-profiler-memory --enable-metrics\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OTel Collector Proxy via Docker Compose (YAML)\nDESCRIPTION: Defines HTTP_PROXY and HTTPS_PROXY environment variables for the `otelcol` service within a Docker Compose YAML file. This ensures the container running the collector uses the specified proxy.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/allow-services.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n   otelcol:\n      environment:\n         - HTTP_PROXY='<proxy.address:port>'\n         - HTTPS_PROXY='<proxy.address:port>'\n```\n\n----------------------------------------\n\nTITLE: Adding Span Attributes in Node.js using OpenTelemetry SDK\nDESCRIPTION: Shows how to get the active span from the current context and add a custom attribute ('my.attribute') using the OpenTelemetry JS SDK. It requires an existing span in the context and mentions the OTEL_RESOURCE_ATTRIBUTES environment variable for global tags.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/add-context-trace-span.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// Splunk Distribution of OpenTelemetry JS\n\nconst { context, trace } = require('@opentelemetry/api');\n\n// A span must already exist in the context\n\nconst customizedSpan = trace.getSpan(context.active());\n\ncustomizedSpan.setAttribute('my.attribute', 'value');\n\n// You can also set global tags using the OTEL_RESOURCE_ATTRIBUTES\t\n// environment variable, which accepts a list of comma-separated key-value\n// pairs. For example, key1:val1,key2:val2.  \n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Collector Support Bundle Script Output\nDESCRIPTION: Command-line output from running the splunk-support-bundle.sh script, which collects system information for customer support. The output shows the steps taken to gather information and the location of the created tarball.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-troubleshoot.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nINFO: Creating temporary directory...\nINFO: Checking for commands...\nINFO: Getting configuration...\nINFO: Getting status...\nINFO: Getting logs...\nWARN: Permission denied to directory (/var/log/td-agent).\nINFO: Getting metric information...\nINFO: Getting zpages information...\nINFO: Getting host information...\nINFO: Creating tarball...\nINFO: Support bundle available at: /tmp/splunk-support-bundle-1708263625.tar.gz\n      Please attach this to your support case\n```\n\n----------------------------------------\n\nTITLE: Container Security Context with NET_ADMIN Configuration\nDESCRIPTION: YAML configuration for container security context with NET_ADMIN capability and privilege escalation for network shaping.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncontainers:\n  securityContext:\n    capabilities:\n      add:\n      - NET_ADMIN\n    allowPrivilegeEscalation: true\n```\n\n----------------------------------------\n\nTITLE: Redacting Sensitive Attributes in Java for Android\nDESCRIPTION: This Java snippet for Android uses `removeSpanAttribute` and `rejectSpansByName` commands to redact user agent information and ignore spans with specific names in Splunk RUM.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/sensitive-data-rum.rst#2025-04-22_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n.removeSpanAttribute(stringKey(\"http.user_agent\"))\n  .rejectSpansByName(spanName -> spanName.contains(\"ignored\"))\n   // sensitive data in the login http.url attribute\n   // is redacted before data moves to the exporter\n\n```\n\n----------------------------------------\n\nTITLE: WebSphere Liberty Profile Java Agent Configuration\nDESCRIPTION: Add the Java agent path to the jvm.options file for either a single or multiple server setups in WebSphere Liberty Profile, enabling the Splunk agent.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-servers-instructions.rst#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n-javaagent:/path/to/splunk-otel-javaagent.jar\n```\n\n----------------------------------------\n\nTITLE: Slack Webhook Payload for Annotations\nDESCRIPTION: JSON payload template for sending annotations to Slack channels using Custom Outgoing Webhooks\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/slack-integration-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n{ \"channel\": \"#general\", \"username\": \"Splunk On-Call\", \"icon_url\": \"https://victorops.com/assets/img/branding/logo-yellow-mark.png\", \"attachments\": [ { \"fallback\":\"What this image is in case it does not render\", \"title\": \"https://en.wikipedia.org/wiki/Australian_Cattle_Dog\", \"image_url\": \"http://i.dailymail.co.uk/i/newpix/2018/04/21/05/4B606CDA00000578-0-image-a-32_1524284530816.jpg\", \"color\": \"danger\" } ] }\n```\n\n----------------------------------------\n\nTITLE: Monitoring Host Count Against Entitlement in Splunk\nDESCRIPTION: This metric (`sf.org.numResourcesMonitored`) tracks the number of hosts being monitored. It is used to monitor usage against the host entitlement limit defined in the contract to prevent potential overage charges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.numResourcesMonitored\n```\n\n----------------------------------------\n\nTITLE: Generating Webhook Signature in Scala\nDESCRIPTION: This Scala code snippet demonstrates how to generate a signature for authenticating webhook requests from Splunk On-Call. It uses HMAC-SHA1 hashing and Base64 encoding to create a signature that can be compared with the X-VictorOps-Signature header.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/escalation-webhooks.rst#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nimport javax.crypto.Mac \nimport javax.crypto.spec.SecretKeySpec import javax.xml.bind.DatatypeConverter \ndef generateSignature(key: String, url:String, postData: Map[String, String]) = { \n   val contents =postData.toList.sorted.foldLeft(url) { case (s, (key, value)) =>s\"${s}${key}$value\" } \n   val mac = Mac.getInstance(\"HmacSHA1\")\n   mac.init(new SecretKeySpec(key.getBytes, \"HmacSHA1\"))\n   DatatypeConverter.printBase64Binary(mac.doFinal(contents.getBytes(\"utf-8\")))\n}\n```\n\n----------------------------------------\n\nTITLE: OTLP Span Export Error - Debug Logs\nDESCRIPTION: An error log example showing failure in exporting spans via the OTLP exporter, suggesting connectivity issues between the agent and the OTel Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/common-java-troubleshooting.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n[BatchSpanProcessor_WorkerThread-1] ERROR io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter - Failed to export spans. Server is UNAVAILABLE. Make sure your collector is running and reachable from this network. Full error message:UNAVAILABLE: io exception\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Deployment Definition for Splunk Synthetics Runner\nDESCRIPTION: YAML configuration for deploying Splunk Synthetics runners in a Kubernetes cluster. The configuration includes resource limits, security context settings, and reference to the runner token secret.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_26\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: splunk-o11y-synthetics-runner\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: splunk-o11y-synthetics-runner\n  template:\n    metadata:\n      labels:\n        app: splunk-o11y-synthetics-runner\n    spec:\n      containers:\n        - name: splunk-o11y-synthetics-runner\n          image: quay.io/signalfx/splunk-synthetics-runner:latest\n          imagePullPolicy: Always\n          env:\n            - name: RUNNER_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: runner-token-secret\n                  key: RUNNER_TOKEN\n          securityContext:\n            capabilities:\n              add:\n                - NET_ADMIN\n            allowPrivilegeEscalation: true\n          resources:\n            limits:\n              cpu: \"2\"\n              memory: 8Gi\n            requests:\n              cpu: \"2\"\n              memory: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring Metadata Exporters for Kubernetes Cluster Receiver\nDESCRIPTION: Example configuration for specifying metadata exporters to sync with metadata collected by the Kubernetes cluster receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-cluster-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  k8s_cluster:\n    auth_type: serviceAccount\n    metadata_exporters:\n    - signalfx\n```\n\n----------------------------------------\n\nTITLE: Setting Up Socket.io with CDN Distribution\nDESCRIPTION: Example showing how to activate socket.io instrumentation with CDN distribution and expose the io function globally.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-instrumentations.rst#2025-04-22_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"/location/to/splunk-otel-web.js\"></script>\n<script>\nSplunkRum.init({\n   // ...\n   instrumentations: {\n      socketio: true\n   }\n});\n</script>\n<script src=\"/app.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Using Global Variable Syntax in Browser and API Tests\nDESCRIPTION: Demonstrates the syntax for using global variables in test fields. Variables must be prefixed with 'env.' and enclosed in double curly braces.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/global-variables.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n{{env.dev-username}}\n{{env.staging-url}}\n```\n\n----------------------------------------\n\nTITLE: Activating JMX Receiver in YAML Configuration\nDESCRIPTION: This snippet shows how to activate the JMX receiver by adding it to the 'receivers' section of the configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/jmx-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  jmx:\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Pipeline with Attributes\nDESCRIPTION: YAML configuration showing how to include attribute processors in the OpenTelemetry pipeline along with other processors like memory limiter and batch processor.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/tags.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      processors:\n        - memory_limiter\n        - batch\n        - resourcedetection\n        - attributes/copyfromexistingkey\n        - attributes/newenvironment\n```\n\n----------------------------------------\n\nTITLE: Configuring Disk Scraper in YAML\nDESCRIPTION: YAML configuration for the disk scraper, allowing inclusion or exclusion of specific devices.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/host-metrics-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndisk:\n  <include|exclude>:\n    devices: [ <device name>, ... ]\n    match_type: <strict|regexp>\n```\n\n----------------------------------------\n\nTITLE: Setting Clear Threshold for Custom MTS Usage Alerts\nDESCRIPTION: This numerical value represents the threshold below which an active alert for custom metric time series (MTS) usage percentage will be cleared. The alert clears when the usage drops below 90 (presumably percent).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/autodetect/autodetect-list.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n90\n```\n\n----------------------------------------\n\nTITLE: Example Kubernetes Proxy Monitor Configuration\nDESCRIPTION: Complete example configuration showing host, port, and extra dimensions settings for the Kubernetes proxy monitor.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-proxy.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/kubernetes-proxy:\n    type: kubernetes-proxy\n    host: localhost\n    port: 10249\n    extraDimensions:\n      metric_source: kubernetes-proxy\n```\n\n----------------------------------------\n\nTITLE: Manual Span Creation with SignalFx tracer Package in Go\nDESCRIPTION: This snippet shows manual span creation using the SignalFx tracer package, using options for tags and span type, and establishing parent-child relationships with context. It demonstrates constructing option slices conditionally, creating a span with a name and user-defined attributes, and ensuring spans are finished. Migration requires updating to OpenTelemetry, as shown in following snippets.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/migrate-signalfx-go-to-otel.rst#2025-04-22_snippet_2\n\nLANGUAGE: go\nCODE:\n```\n   func BusinessOperation(ctx context.Context, client string) {\n      opts := []tracer.StartSpanOption{\n         tracer.Tag(\"client\", client),\n         tracer.SpanType(\"internal\"),\n      }\n      if parent, ok := tracer.SpanFromContext(ctx); ok {\n         opts = append(opts, tracer.ChildOf(parent.Context()))\n      }\n      span := tracer.StartSpan(\"BusinessOperation\", opts...)\n      defer span.Finish()\n      /* ... */\n   }\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQ Pipeline\nDESCRIPTION: Configuration to add the RabbitMQ monitor to the metrics pipeline in the OpenTelemetry Collector service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-messaging/rabbitmq.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/rabbitmq]\n```\n\n----------------------------------------\n\nTITLE: Creating Runner Token Secret in Kubernetes\nDESCRIPTION: Shell command for creating a Kubernetes secret to store the Splunk Synthetics runner authentication token.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create secret generic runner-token-secret \\\n--from-literal=RUNNER_TOKEN=YOUR_TOKEN_HERE\n```\n\n----------------------------------------\n\nTITLE: Basic Kubernetes Events Monitor Configuration\nDESCRIPTION: Basic configuration snippet for activating the Kubernetes events monitor in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-events.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n   smartagent/kubernetes-events:\n   type: kubernetes-events\n   ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Updating Chart/Dashboard SignalFlow Filters for OTel v2.0 Migration (SignalFlow)\nDESCRIPTION: This SignalFlow example demonstrates updating filters in Splunk APM charts and dashboards for the OpenTelemetry Java Agent 2.0 migration. It uses an OR condition (`filter('http_response_status_code', '200') or filter('http_status_code', '200')`) to query data using both the new (`http.response.status_code`) and old (`http.status_code`) attributes, ensuring data continuity. The query retrieves the count of successful, non-error requests for the 'adservice'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/migrate-apm-custom-reporting.rst#2025-04-22_snippet_0\n\nLANGUAGE: signalflow\nCODE:\n```\nA = data('service.request.count', filter=filter('sf_dimensionalized', 'true') and filter('sf_service', 'adservice') and (filter('http_response_status_code', '200') or filter('http_status_code', '200')) and filter('sf_error', 'false')).publish(label='A')\n```\n\n----------------------------------------\n\nTITLE: Creating Priority Class for GKE Autopilot\nDESCRIPTION: YAML definition for creating a high-priority class to ensure the Collector agent DaemonSet pods are scheduled correctly in GKE Autopilot environments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncat <<EOF | kubectl apply -f -\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: splunk-otel-agent-priority\nvalue: 1000000\nglobalDefault: false\ndescription: \"Higher priority class for the Splunk Distribution of OpenTelemetry Collector pods.\"\nEOF\n```\n\n----------------------------------------\n\nTITLE: Defining Recovery Alert Email Template - HP SiteScope Integration - ini\nDESCRIPTION: This snippet defines the email template for RECOVERY alerts used in the HP SiteScope integration with Splunk On-Call. Save this as 'Splunk On-Call_RECOVERY' in the 'templates.mail' folder on the SiteScope server. The content uses the same dynamic SiteScope placeholders as the other templates, ensuring relevant status and recovery details are included in the notification. There are no additional dependencies; ensure the action configuration in SiteScope references this template for recovery events. This ensures recovery alerts are properly formatted and trigger correct workflow downstream.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/hp-sitescope-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[Subject: <siteScopeHost>/<groupID>/<name>/<alert::name> RECOVERY]\\n\\nThis alert is from SiteScope at <newSiteScopeURL>\\n\\nMonitor: <groupID>:<name>\\nTags: <tag>\\nGroup: <group>\\nStatus: <state>\\nSample #: <sample>\\n\\nTime: <time>\\n\\n---------------------- Detail ----------------------\\n\\n<mainParameters>\\n\\n<mainStateProperties>\n```\n\n----------------------------------------\n\nTITLE: Monitoring MTS Creation Rate in Splunk\nDESCRIPTION: This metric (`sf.org.numMetricTimeSeriesCreated`) tracks the number of Metric Time Series (MTS) created. It is relevant for monitoring against the MTS creations per minute limit.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.numMetricTimeSeriesCreated\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct Data Export to Splunk Observability Cloud in Windows\nDESCRIPTION: Sets environment variables in Windows PowerShell to send telemetry data directly to Splunk Observability Cloud instead of through a local collector. Requires an access token and realm-specific endpoint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_14\n\nLANGUAGE: powershell\nCODE:\n```\n$env:SPLUNK_ACCESS_TOKEN=<access_token>\n$env:OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=http/protobuf\n$env:OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://ingest.<realm>.signalfx.com/v2/trace/otlp\n```\n\n----------------------------------------\n\nTITLE: Volume Space Alert Reset Payload\nDESCRIPTION: JSON payload template for resetting volume space alerts in SolarWinds. Includes available space information.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/solarwinds-integration.rst#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"alert_rule\":\"${N=Alerting;M=AlertName}\",\n  \"entity_display_name\":\"${NodeName} ${SQL: SELECT REPLACE ('''${Caption}''','\\\\', ' ')} has ${VolumeSpaceAvailable} free\",\n  \"entity_id\":\"${N=Alerting;M=AlertObjectID}\",\n  \"host_name\":\"${NodeName}\",\n  \"ip_address\":\"${Node.IP_Address}\",\n  \"message_type\":\"RECOVERY\",\n  \"monitor_name\":\"SolarWinds\",\n  \"monitoring_tool\":\"SolarWinds\",\n  \"state_message\":\"${NodeName} ${SQL: SELECT REPLACE ('''${Caption}''','\\\\', ' ')} has ${VolumeSpaceAvailable} free\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GitLab Runner Prometheus Metrics\nDESCRIPTION: Configuration for GitLab Runner's Prometheus metrics HTTP server to allow network connections on port 9252\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-gitlab/gitlab.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlisten_address = \"0.0.0.0:9252\"\n...\n```\n\n----------------------------------------\n\nTITLE: Setting Host Name in OpenTelemetry for Linux\nDESCRIPTION: Uses the OTEL_RESOURCE_ATTRIBUTES environment variable to override the default host name in Linux bash environment. Replace <host_name> with your desired host name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/apm-api-define-host.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_RESOURCE_ATTRIBUTES=host.name=<host_name>\n```\n\n----------------------------------------\n\nTITLE: Adding User Metadata After RUM Agent Initialization\nDESCRIPTION: Demonstrates how to add user identification metadata as global attributes after the Splunk RUM agent has been initialized. This is useful when user data becomes available after initialization.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/manual-rum-react-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nSplunkRum.setGlobalAttributes({\n   'enduser.id': 'user-id-123456',\n   'enduser.role': 'premium'\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring VictorOps Organization Settings in Icinga 2\nDESCRIPTION: Configuration settings for organization ID, API key, contact email, and optional parameters in Icinga 2.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nconst VictorOps_ORGANIZATION_ID = \"my-org\"\nconst VictorOps_ORGANIZATION_KEY = \"5913e634-XXXX-XXXX-XXXX-a7500d926a44\"\n```\n\n----------------------------------------\n\nTITLE: Setting Security Group for AWS PrivateLink Configuration\nDESCRIPTION: When creating a VPC endpoint for AWS PrivateLink, you need to specify security groups that control traffic. This configuration sets the inbound rule to HTTPS protocol on port 443.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/private-connectivity/aws-privatelink.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nIPv4\n```\n\n----------------------------------------\n\nTITLE: Suppressing Debug Output in Plugin Utilities (CentOS)\nDESCRIPTION: Instructs users to add the line `DEBUG_FILE=/dev/null` to the plugin's utility script located at `/opt/victorops/nagios_plugin/bin/utils`. This configuration is specifically mentioned for CentOS systems and serves to redirect any debug output generated by the script to `/dev/null`, effectively discarding it.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nDEBUG_FILE=/dev/null\n```\n\n----------------------------------------\n\nTITLE: Configuring JBoss EAP and WildFly with Java Agent on Linux\nDESCRIPTION: Add javaagent argument to standalone.conf in JBoss EAP and WildFly for standalone mode on Linux. This ensures the Splunk OpenTelemetry Java agent is enabled by specifying its path in the configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-servers-instructions.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nJAVA_OPTS=\"$JAVA_OPTS -javaagent:/path/to/splunk-otel-javaagent.jar\"\n```\n\n----------------------------------------\n\nTITLE: RMI Connection Error Example in JMX\nDESCRIPTION: Example error message when JMX connection fails due to RMI port connectivity issues. Shows a typical error when the JMX port is reachable but the RMI port is blocked by a firewall.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/genericjmx.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nCreating MBean server connection failed: java.rmi.ConnectException: Connection refused to host: 172.17.0.3; nested exception is:\n     java.net.ConnectException: Connection timed out (Connection timed out)\n```\n\n----------------------------------------\n\nTITLE: Configure Multiple Config Files\nDESCRIPTION: Command to run the Collector with multiple configuration files specified simultaneously.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/install-the-collector.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./otelcol --config=file:/path/to/first/file --config=file:/path/to/second/file\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose Services\nDESCRIPTION: Command to start the log collection services defined in docker-compose.yml file in detached mode.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/deploy-verify-environment.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Migrating Deprecated Detector Attributes Configuration - YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure the Splunk OpenTelemetry Collector's resource detection processor using the old, now deprecated, 'attributes' field for specifying which host-level attributes to collect. The 'resourcedetection' processor is set up to use only the 'system' detector and declares a list of attributes. Since this syntax is deprecated in collector versions after 0.81, it should only be used for maintaining legacy configurations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/resourcedetection-processor.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n  resourcedetection:\n    detectors: [system]\n    # Deprecated in version 0.81\n    attributes: ['host.name', 'host.id']\n\n```\n\n----------------------------------------\n\nTITLE: Creating a New Metric Matching Label Values with Regex (YAML)\nDESCRIPTION: This configuration snippet illustrates creating a new metric (`host.cpu.utilization`) based on an existing metric (`host.cpu.usage`), filtered by label values that match a regular expression. Although the specific regex is not provided in the snippet, the context indicates it would be defined within `experimental_match_labels` and used in conjunction with `match_type: regexp` (not explicitly shown but implied by the example title).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# create host.cpu.utilization from host.cpu.usage where we have metric label match regexp \\\"container=my_container*\\\"\ninclude: host.cpu.usage\naction: insert\nnew_name: host.cpu.utilization\nmatch_type: regexp # Assuming regexp based on surrounding text, although not in the snippet\nexperimental_match_labels: {\\\"container\\\": \\\"my_container.*\\\"} # Example regex based on comment\noperations:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring JMX Pipeline in Service Configuration\nDESCRIPTION: YAML configuration for adding the JMX monitor to the metrics pipeline service\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/jmx.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/jmx]\n```\n\n----------------------------------------\n\nTITLE: Configuring OTel Collector OTLP Receiver for gRPC and HTTP (YAML)\nDESCRIPTION: This YAML configuration snippet demonstrates how to set up the OTLP receiver within an OpenTelemetry Collector configuration file. It defines endpoints for both gRPC (listening on port 4317) and HTTP/protobuf (listening on port 4318) protocols. The listening interface is specified using the environment variable `${SPLUNK_LISTEN_INTERFACE}`. This ensures the Collector can accept telemetry data regardless of which protocol the Node.js agent is configured to use.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/breaking-changes.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   otlp:\n     protocols:\n       grpc:\n         endpoint: \"${SPLUNK_LISTEN_INTERFACE}:4317\"\n       http:\n         endpoint: \"${SPLUNK_LISTEN_INTERFACE}:4318\"\n```\n\n----------------------------------------\n\nTITLE: Adding Java Monitor to Metrics Pipeline\nDESCRIPTION: Configuration snippet showing how to add the Java monitor to the service pipelines metrics receivers section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/java-monitor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [java-monitor]\n```\n\n----------------------------------------\n\nTITLE: Service Pipeline Configuration\nDESCRIPTION: Configuration to add the kubernetes-apiserver monitor to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-apiserver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/kubernetes-apiserver]\n```\n\n----------------------------------------\n\nTITLE: Setting Required Environment Variables for Splunk OpenTelemetry Connector\nDESCRIPTION: Commands to set the required environment variables SPLUNK_ACCESS_TOKEN and SPLUNK_REALM for the Splunk OpenTelemetry Connector to authenticate with Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/heroku.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nheroku config:set SPLUNK_ACCESS_TOKEN=<YOUR_ACCESS_TOKEN>\nheroku config:set SPLUNK_REALM=<YOUR_REALM>\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Detection Processor in YAML\nDESCRIPTION: Main configuration for the resource detection processor, specifying detectors and resource attribute settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/resourcedetection-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nresourcedetection:\n  detectors: [system, ec2]\n  override: true\n  system:\n    resource_attributes:\n      host.name:\n        enabled: true\n      host.id:\n        enabled: false\n  ec2:\n    resource_attributes:\n      host.name:\n        enabled: false\n      host.id:\n        enabled: true\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OTel Collector with Profiling and Metrics\nDESCRIPTION: This command installs the Splunk OTel Collector with systemd instrumentation, deployment environment, and enables profiling for CPU, memory, and metrics collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-systemd-instrumentation --deployment-environment prod \\\n--realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN> \\\n--enable-profiler --enable-profiler-memory --enable-metrics\n```\n\n----------------------------------------\n\nTITLE: Configuring Deployment Environment\nDESCRIPTION: YAML configuration example for specifying the deployment environment, which adds a deployment.environment attribute to all telemetry data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkObservability:\n  accessToken: xxxxxx\n  realm: us0\nenvironment: production\n```\n\n----------------------------------------\n\nTITLE: Building Android RUM Library Locally via Command Line\nDESCRIPTION: Bash commands for cloning the Splunk RUM repository and building the library locally by publishing to the local Maven repository.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/signalfx/splunk-otel-android.git\n./gradlew publishToMavenLocal\n```\n\n----------------------------------------\n\nTITLE: Reducing Go OpenTelemetry Batch Span Processor Batch Size (Bash)\nDESCRIPTION: Sets the `OTEL_BSP_MAX_EXPORT_BATCH_SIZE` environment variable to a smaller value (128). This configuration is suggested when the network has limited bandwidth, as smaller batches might increase export frequency and help drain the queue faster, potentially reducing dropped spans.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/common-go-troubleshooting.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_BSP_MAX_EXPORT_BATCH_SIZE=128\n```\n\n----------------------------------------\n\nTITLE: Identifying Received Traces Metric in Splunk APM (TAPM Plan)\nDESCRIPTION: Splunk Observability Cloud metric identifier (`sf.org.apm.numTracesReceived`) representing the number of traces Splunk APM receives and processes. This metric is specifically used in the 'TAPM' chart on the APM Subscription Usage page for organizations with a TAPM subscription plan to monitor trace ingestion.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/apm-billing-usage-index.rst#2025-04-22_snippet_0\n\nLANGUAGE: metric\nCODE:\n```\nsf.org.apm.numTracesReceived\n```\n\n----------------------------------------\n\nTITLE: Sending Test Alert to Splunk On-Call\nDESCRIPTION: Splunk search command to send a test INFO alert to the Splunk On-Call timeline. Can be modified to create actual incidents by changing INFO to CRITICAL.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/splunk-integration-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n| sendalert victorops param.message_type=\"INFO\"\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents Configuration\nDESCRIPTION: Sphinx documentation configuration using RST markup to define the table of contents and page metadata for Splunk integration scenarios.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/scenarios/integration-scenario-landing.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _scenarios-integration-scenario-landing:\n\n******************************************************************************************************\nSplunk Observability Cloud and the Splunk platform scenarios\n******************************************************************************************************\n\n.. meta::\n  :description: Use case scenarios for Splunk Observability Cloud and the Splunk platform.\n\n\n.. toctree::\n    :maxdepth: 3\n    :hidden:\n\n    Splunk ITSI and Splunk Observability Cloud scenario <integration-scenario1>\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Metrics Address for Splunk OpenTelemetry Collector\nDESCRIPTION: This PowerShell command demonstrates how to modify the registry to change the default exposed metrics address of the Collector to 0.0.0.0:9090. It modifies the ImagePath value in the Windows registry.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/windows-config.rst#2025-04-22_snippet_2\n\nLANGUAGE: PowerShell\nCODE:\n```\nSet-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\splunk-otel-collector\" -name \"ImagePath\" -value \"C:\\Program Files\\Splunk\\OpenTelemetry Collector\\otelcol.exe --metrics-addr 0.0.0.0:9090\"\n```\n\n----------------------------------------\n\nTITLE: Removing SignalFx Tracing Dependencies in Python\nDESCRIPTION: Commands to manually remove dependencies of signalfx-tracing if they weren't automatically uninstalled by the package manager.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/troubleshooting/migrate-signalfx-python-agent-to-otel.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall opentracing\npip uninstall jaeger-client\n```\n\n----------------------------------------\n\nTITLE: Gateway Deployment Commands\nDESCRIPTION: Commands to clone the repository and deploy the Collector as a gateway using Nomad.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-nomad.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n$ git clone https://github.com/signalfx/splunk-otel-collector.git\n$ cd splunk-otel-collector/deployments/nomad\n$ nomad run otel-gateway.nomad\n```\n\n----------------------------------------\n\nTITLE: Activating Cloud Foundry Receiver in YAML Configuration\nDESCRIPTION: This snippet shows how to activate the Cloud Foundry receiver in the 'receivers' section of the configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/cloudfoundry-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  cloudfoundry:\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector with Service Name Dimensions for Redis\nDESCRIPTION: Example configuration for the Splunk Distribution of OpenTelemetry Collector that sets up a Redis monitor with service.name dimension. This allows the Dependencies tab in navigators to show which services are running on specific host instances.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/use-navigators.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceiver_creator:\n  receivers:\n    smartagent/redis:\n     rule: type == \"pod\" && name contains \"redis\"\n     config:\n       type: collectd/redis\n       host: redis-cart\n       port: 6379\n       extraDimensions:\n         service.name: redis-cart\n```\n\n----------------------------------------\n\nTITLE: Configuring Network Protocols Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration to activate the protocols monitor in the OpenTelemetry Collector. Defines the receiver configuration using the collectd/protocols type.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/network-protocols.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/protocols:\n    type: collectd/protocols\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Adding MongoDB Monitor to Service Pipeline\nDESCRIPTION: YAML configuration showing how to add the MongoDB monitor to the service.pipelines.metrics.receivers section of the Collector configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/mongodb.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/mongodb]\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OTel Collector with Profiling in Linux\nDESCRIPTION: Bash command to install the Splunk OpenTelemetry Collector with CPU and memory profiling enabled\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-advanced-config.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-instrumentation --deployment-environment prod \\\n--realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN> \\\n--enable-profiler --enable-profiler-memory\n```\n\n----------------------------------------\n\nTITLE: Specifying Custom NPM Path for Node.js Instrumentation\nDESCRIPTION: Option to specify a custom path to the npm executable when installing Node.js zero-code instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n--npm-path /custom/path/to/npm\n```\n\n----------------------------------------\n\nTITLE: Defining Warning Alert Email Template - HP SiteScope Integration - ini\nDESCRIPTION: This snippet is a plaintext/ini-formatted email template for the HP SiteScope integration, specifically designed for WARNING alerts. To use, add the content to a file named 'Splunk On-Call_WARNING' within the 'templates.mail' directory on the SiteScope server. Like the critical template, it leverages SiteScope dynamic variables in both the subject and body for customized notification. No external code dependencies; relies only on SiteScope's templating system. This file is triggered whenever a resource warning is detected, formatting the outgoing email accordingly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/hp-sitescope-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[Subject: <siteScopeHost>/<groupID>/<name>/<alert::name> WARNING]\\n\\nThis alert is from SiteScope at <newSiteScopeURL>\\n\\nMonitor: <groupID>:<name>\\nTags: <tag>\\nGroup: <group>\\nStatus: <state>\\nSample #: <sample>\\n\\nTime: <time>\\n\\n---------------------- Detail ----------------------\\n\\n<mainParameters>\\n\\n<mainStateProperties>\n```\n\n----------------------------------------\n\nTITLE: Creating Server-Timing Header for Trace Context\nDESCRIPTION: Shows the format of a Server-Timing header used to provide server trace context for Splunk RUM.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nServer-Timing: traceparent;desc=\"00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01\"\n```\n\n----------------------------------------\n\nTITLE: Defining Supported Windows Versions Table in reStructuredText\nDESCRIPTION: This code snippet creates a table in reStructuredText format that lists the supported Windows versions for different Collector installation methods. It includes various installation methods and their corresponding supported 64-bit Windows versions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/requirements/collector-windows.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. list-table::\n  :header-rows: 1\n  :widths: 40 60\n  :width: 100%\n\n  * - Install method\n    - Supported versions (64-bit)\n  * - Installer script\n    - Windows 10 Pro and Home, Windows Server 2016, 2019, 2022, 2025\n  * - Windows installer (MSI)\n    - Windows 10 Pro and Home, Windows Server 2016, 2019, 2022, 2025\n  * - Ansible\n    - Windows 10 Pro and Home, Windows Server 2016, 2019, 2022, 2025\n  * - Chef\n    - Windows 10 Pro and Home, Windows Server 2019, 2022\n  * - Nomad\n    - Windows 10 Pro and Home, Windows Server 2016, 2019\n  * - Puppet\n    - Windows 10 Pro and Home, Windows Server 2016, 2019\n  * - Docker\n    - Windows 10 Pro and Home, Windows Server 2019, 2022\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Monitor Settings in YAML\nDESCRIPTION: Configuration options for the CPU monitor in Smart Agent. The main setting is reportPerCPU which determines whether stats are generated for individual CPU cores or just system-wide metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/cpu.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreportPerCPU: false  # Optional boolean setting. If true, generates stats for each CPU core with cpu dimension\n```\n\n----------------------------------------\n\nTITLE: MongoDB SSL/TLS Configuration\nDESCRIPTION: Command to configure MongoDB discovery for non-SSL/TLS enabled servers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-third-party.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n--set splunk.discovery.receivers.mongodb.config.tls::insecure=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Initial Delay Parameter in Splunk Observability\nDESCRIPTION: Sets the initial delay before the receiver begins collecting metrics. The default value is 1 second, and this parameter controls how long the system waits before starting the first metrics collection cycle.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/collector-settings-initialdelay.rst#2025-04-22_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ninitial_delay: 1s\n```\n\n----------------------------------------\n\nTITLE: Configuring Application Name in Android RUM\nDESCRIPTION: Defines the application name used by the Splunk RUM Android agent in the 'app' attribute of all spans.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/rum-android-data-model.rst#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\napplicationName(String)\n```\n\n----------------------------------------\n\nTITLE: Verifying Statsd Installation with Netcat\nDESCRIPTION: Command to send test statsd metrics locally using netcat to verify the installation is working correctly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/statsd.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n$ echo \"statsd.test:1|g\" | nc -w 1 -u 127.0.0.1 8125\n```\n\n----------------------------------------\n\nTITLE: Defining SRE Contact Group Configuration\nDESCRIPTION: Contact group configuration for SRE team that defines the routing key used in Splunk On-Call alerts.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndefine contactgroup{\ncontactgroup_name         sre\nalias                     VictorOps SRE contact group\nmembers                   VictorOps_sre\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Exec Input Monitor with OpenTelemetry Collector\nDESCRIPTION: Basic configuration example showing how to activate the Exec Input integration by adding it to the Collector configuration as a receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/exec-input.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/exec:\n    type: telegraf/exec\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Converting waitForElement to assert_element_not_present in Synthetic Monitoring\nDESCRIPTION: Comparison of a Google Chrome Recorder waitForElement step (with visible:false) and its equivalent Splunk Synthetic Monitoring assert_element_not_present implementation. Used to check for the absence of elements in the DOM.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/browser-test/set-up-browser-test.rst#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{\n// Google Chrome Recorder\n   \"type\": \"waitForElement\",\n   \"selectors\": [\n      [\n         \"body\",\n         \"#homepage_product_brand-example\",\n         \".css-4t2fjl\",\n         \".eanm77i0\"\n      ]\n   ],\n   \"visible\": false\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n// Splunk Synthetic Monitoring code snippet\n      \"name\": \"\",\n      \"type\": \"assert_element_not_present\",\n      \"wait_for_nav\": false,\n      \"selector_type\": \"css\",\n      \"selector\": \"body,#homepage_product_brand-example\"\n      }\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenTelemetry in Windows\nDESCRIPTION: Set environment variables in a Windows PowerShell environment, enabling integration with Splunk OpenTelemetry for service tracking and metric collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/instrumentation/instrument-nodejs-application.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_SERVICE_NAME=<yourServiceName>\n```\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_RESOURCE_ATTRIBUTES='deployment.environment=<envtype>,service.version=<version>'\n```\n\nLANGUAGE: shell\nCODE:\n```\n$env:SPLUNK_METRICS_ENABLED='true'\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Pipeline in OpenTelemetry Collector\nDESCRIPTION: Defines the service pipeline that connects receivers, processors, and exporters for log data flow through the collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/collector-splunk.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    logs:\n      receivers: [ filelog/output1, filelog/output2 ]\n      processors: [ transform, groupbyattrs, batch ]\n      exporters: [ splunk_hec/logs ]\n```\n\n----------------------------------------\n\nTITLE: Escaping Characters in Strings with encodeString - Handlebars\nDESCRIPTION: Applies the encodeString function to escape quote and newline characters in a given string variable. Useful for rendering alert payload fields where special characters may interfere with formatting or JSON encoding. Expects a string variable (e.g., messageTitle) and outputs a safely escaped version.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/notif-services/splunkplatform.rst#2025-04-22_snippet_2\n\nLANGUAGE: Handlebars\nCODE:\n```\n``{{{encodeString messageTitle}}}``\n```\n\n----------------------------------------\n\nTITLE: Using b3multi Trace Propagator in Shell\nDESCRIPTION: These shell commands configure the trace propagator to use `b3multi` for backward compatibility with the SignalFx Tracing Library in Linux and Windows environments. It sets the `OTEL_PROPAGATORS` environment variable accordingly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/advanced-nodejs-otel-configuration.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_PROPAGATORS=b3multi\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Lambda Function Handler in JSON\nDESCRIPTION: This JSON snippet demonstrates how to configure the AWS Lambda function handler in the aws-lambda-tools-defaults.json file. It sets the function handler to the TracingFunctionHandler method in the instrumented Lambda function.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/aws/otel-lambda-layer/instrumentation/dotnet-lambdas.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Information\": [\n        \"This file provides default values for the deployment wizard inside Visual Studio and the AWS Lambda commands added to the .NET Core CLI.\",\n        \"To learn more about the Lambda commands with the .NET Core CLI execute the following command at the command line in the project root directory.\",\n        \"dotnet lambda help\",\n        \"All the command line options for the Lambda command can be specified in this file.\"\n    ],\n    \"profile\": \"default\",\n    \"region\": \"us-west-2\",\n    \"configuration\": \"Release\",\n    \"function-architecture\": \"x86_64\",\n    \"function-runtime\": \"dotnet8\",\n    \"function-memory-size\": 512,\n    \"function-timeout\": 30,\n    \"function-handler\": \"AWSLambdaSample::AWSLambdaSample.Function::TracingFunctionHandler\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Sinatra Instrumentation in Ruby Gemfile\nDESCRIPTION: This snippet shows how to add the Sinatra instrumentation gem to a Ruby project's Gemfile. It specifies the gem name and version constraint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/ruby-manual-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngem \"opentelemetry-instrumentation-sinatra\", \"~> 0.21\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenShift Resource Detection with TLS\nDESCRIPTION: Configuration for collecting resource attributes from OpenShift and Kubernetes API using TLS and a service token.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/resourcedetection-processor.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/openshift:\n    detectors: [openshift]\n    timeout: 2s\n    override: false\n    openshift:\n      address: \"127.0.0.1:4444\"\n      token: \"<token>\"\n      tls:\n        insecure: false\n        ca_file: \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Pipeline for Prometheus Exporter\nDESCRIPTION: Configuration for adding the Prometheus Exporter monitor to the metrics pipeline in the Collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-exporter.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/prometheus-exporter]\n```\n\n----------------------------------------\n\nTITLE: Upgrading Collector with Helm while keeping existing configuration values\nDESCRIPTION: This command upgrades the OpenTelemetry Collector using Helm while preserving all previously configured values using the --reuse-values flag.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade splunk-otel-collector splunk-otel-collector-chart/splunk-otel-collector \n--reuse-values\n```\n\n----------------------------------------\n\nTITLE: Sending Logs to Splunk Cloud with Filelog Receiver\nDESCRIPTION: YAML configuration for sending logs to Splunk Cloud using the Filelog receiver. Includes regex parsing, field manipulation, and Splunk HEC exporter configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/filelog-receiver.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include: [ /output/file.log ]\n    operators:\n      - type: regex_parser\n        regex: '(?P<before>.*)\\d\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d(?P<after>.*)'\n        parse_to: body.parsed\n        output: before_and_after\n      - id: before_and_after\n        type: add\n        field: body\n        value: EXPR(body.parsed.before + \"XXX-XXX-XXXX\" + body.parsed.after)\n\nexporters:\n  # Logs\n  splunk_hec:\n    token: \"${SPLUNK_HEC_TOKEN}\"\n    endpoint: \"${SPLUNK_HEC_URL}\"\n    source: \"otel\"\n    sourcetype: \"otel\"\n\nservice:\n  pipelines:\n    logs:\n      receivers: [filelog, otlp]\n      processors:\n      - memory_limiter\n      - batch\n      - resourcedetection\n      #- resource/add_environment\n      exporters: [splunk_hec]\n```\n\n----------------------------------------\n\nTITLE: HTML Embed in ReStructuredText\nDESCRIPTION: HTML embedding within ReStructuredText document to display a section heading.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/intro-to-mobile.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n\n  <embed>\n    <h2>What can I do with Splunk Observability Cloud for Mobile?</h2>\n  </embed>\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Web Server for Metrics Exposure\nDESCRIPTION: Apache configuration snippet to enable the mod_status module and expose status metrics. This configuration should be added to the Apache server settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/apache-httpserver.rst#2025-04-22_snippet_1\n\nLANGUAGE: apache\nCODE:\n```\nExtendedStatus on\n<Location /mod_status>\nSetHandler server-status\n</Location>\n```\n\n----------------------------------------\n\nTITLE: Applying Istio Operator Configuration using istioctl (Shell)\nDESCRIPTION: This shell command uses the `istioctl` command-line tool to apply the Istio configuration defined in the `tracing.yaml` file. This command updates the Istio service mesh according to the specifications in the YAML file, activating the tracing settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/istio/istio.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nistioctl install -f ./tracing.yaml\n```\n\n----------------------------------------\n\nTITLE: Expvar Pipeline Configuration in YAML\nDESCRIPTION: Configuration showing how to add the Expvar monitor to the metrics pipeline receivers section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/expvar.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/expvar]\n```\n\n----------------------------------------\n\nTITLE: Configuring ASP.NET Environment Variables\nDESCRIPTION: This XML snippet demonstrates how to set environment variables for ASP.NET applications using the appSettings block in the web.config file. These settings include the OTEL_SERVICE_NAME, which identifies the service name during instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/configuration/advanced-dotnet-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<configuration>\n   <appSettings>\n      <add key=\"OTEL_SERVICE_NAME\" value=\"my-service-name\" />\n   </appSettings>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Pipelines with Basicauth Extension in YAML\nDESCRIPTION: This snippet shows how to include the basicauth extension in the service section of the OpenTelemetry Collector's configuration file, setting up pipelines for traces with OTLP receiver and exporter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/basic-auth-extension.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  extensions: [basicauth/server, basicauth/client]\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: []\n      exporters: [otlp]\n```\n\n----------------------------------------\n\nTITLE: Building iOS Archives for XCFramework Creation\nDESCRIPTION: Commands to create iOS device and simulator archives for the SplunkOtel framework, which is a prerequisite for building an XCFramework.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/install-rum-ios.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nxcodebuild archive -project SplunkRum.xcodeproj -scheme SplunkOtel -destination \"generic/platform=iOS\" -archivePath \"archives/SplunkRum-iOS\"\n```\n\n----------------------------------------\n\nTITLE: Updating Location Data in React Native RUM\nDESCRIPTION: Demonstrates the method for setting latitude and longitude as global attributes in Splunk RUM. Note that this method is deprecated and will be removed in a future version.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/manual-rum-react-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nupdateLocation: (latitude: number, longitude: number)\n```\n\n----------------------------------------\n\nTITLE: JavaScript Support Limitations in Splunk Synthetics\nDESCRIPTION: Describes the JavaScript runtime environment limitations in Splunk Synthetics. The platform only provides V8 engine support without Node.js runtime, limiting access to features like file system operations, HTTP requests, event loops, and module imports.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/api-test/api-test.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Unsupported features:\nfs.readFile()           // No filesystem access\nhttp.request()          // No HTTP requests\nsetTimeout()           // No event loops\nrequire()              // No module system\nmodule.exports         // No module exports\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata Table for Amazon RDS in reStructuredText\nDESCRIPTION: This code snippet defines a table in reStructuredText format that lists the metadata properties imported by Infrastructure Monitoring for Amazon Relational Database Service (RDS). It includes the original RDS property names, custom property names used in Infrastructure Monitoring, and descriptions of each property.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/monitor/aws-infra-metadata.rst#2025-04-22_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 30 30 60\n   :width: 100%\n\n   *  - :strong:`RDS Name`\n      - :Strong:`Custom Property`\n      - :Strong:`Description`\n\n   *  - AvailabilityZone\n      - aws_availability_zone\n      - Name of the DB instance Availability Zone\n\n   *  - DBClusterIdentifier\n      - aws_db_cluster_identifier\n      - If the DB instance is a member of a DB cluster, contains the name of the DB cluster\n\n   *  - DBInstanceClass\n      - aws_db_instance_class\n      - Name of the compute and memory capacity class of the DB instance\n\n   *  - DBInstanceStatus\n      - aws_db_instance_status\n      - Current state of the DB instance\n\n   *  - Engine\n      - aws_engine\n```\n\n----------------------------------------\n\nTITLE: Selective Processing Using Delete Action\nDESCRIPTION: This snippet shows a configuration for selective processing where the delete action is applied to specific services. It emphasizes attribute exclusion based on matching services with strict match type.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/attributes-processor.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nattributes/selectiveprocessing:\n  include:\n    match_type: strict\n    services: [\"service1\", \"service2\"]\n  actions:\n    - key: sensitive_field\n      action: delete\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Splunk OpenTelemetry Collector using PowerShell\nDESCRIPTION: This PowerShell script identifies the Splunk OpenTelemetry Collector installation in the Windows registry and executes its uninstaller. The script throws an error if the Collector is not installed.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/windows-uninstall.rst#2025-04-22_snippet_0\n\nLANGUAGE: PowerShell\nCODE:\n```\n$MyProgram = Get-ItemProperty HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\uninstall\\* | Where { $_.DisplayName -eq \"Splunk OpenTelemetry Collector\" }\n\nif (!$MyProgram) { throw \"Splunk OpenTelemetry Collector is not installed\" }\n\ncmd /c $MyProgram.UninstallString\n```\n\n----------------------------------------\n\nTITLE: Upgrading OpenTelemetry Collector with Chocolatey\nDESCRIPTION: Command to upgrade the Splunk OpenTelemetry Collector using the Chocolatey package manager on Windows systems. This will fetch and install the latest version available in the Chocolatey repository.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/windows-upgrade.rst#2025-04-22_snippet_1\n\nLANGUAGE: PowerShell\nCODE:\n```\nchoco upgrade splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Installing Nagios Plugin DEB Package using dpkg (Shell)\nDESCRIPTION: Installs the downloaded Nagios plugin DEB package using the `dpkg -i` command. Replace `<path_to_file>` with the actual path to the downloaded .deb file. This command requires root privileges, often executed with `sudo`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndpkg -i <path_to_file>\n```\n\n----------------------------------------\n\nTITLE: Changing Display Name with Variable Expansion in Splunk On-Call Rules Engine\nDESCRIPTION: This rule changes the display name of an alert card by setting the entity_display_name to the value of another field.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-transformations.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nWhen entity_display_name matches * using Wildcard\n\nSet entity_display_name to ${{field_you_choose}}\n```\n\n----------------------------------------\n\nTITLE: Logstash TCP Pipeline Configuration in YAML\nDESCRIPTION: Configuration example showing how to add the Logstash TCP monitor to the metrics pipeline\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/logstash-tcp.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/logstash-tcp]\n```\n\n----------------------------------------\n\nTITLE: Defining Critical Alert Email Template - HP SiteScope Integration - ini\nDESCRIPTION: This snippet shows a plaintext/ini-formatted email template for HP SiteScope that is used when generating CRITICAL alerts for Splunk On-Call. Place this content into a file named 'Splunk On-Call_CRITICAL' under the SiteScope 'templates.mail' directory. The template uses SiteScope-provided variables (such as \\<siteScopeHost\\>, \\<groupID\\>, \\<name\\>, etc.) to populate subject and body fields. No external dependencies are required; make sure SiteScope's email integration is enabled. The file is used to ensure alert emails contain all necessary diagnostic context when a critical state occurs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/hp-sitescope-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[Subject: <siteScopeHost>/<groupID>/<name>/<alert::name> CRITICAL]\\n\\nThis alert is from SiteScope at <newSiteScopeURL>\\n\\nMonitor: <groupID>:<name>\\nTags: <tag>\\nGroup: <group>\\nStatus: <state>\\nSample #: <sample>\\n\\nTime: <time>\\n\\n---------------------- Detail ----------------------\\n\\n<mainParameters>\\n\\n<mainStateProperties>\n```\n\n----------------------------------------\n\nTITLE: Configuring Processlist Monitor in OpenTelemetry Collector (YAML)\nDESCRIPTION: This snippet shows how to activate the processlist integration by adding it to the Collector configuration. It demonstrates the receiver configuration and how to include it in the logs pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/host-processlist.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/processlist:\n    type: processlist\n    ...  # Additional config\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    logs/signalfx:\n      receivers: [signalfx, smartagent/processlist]\n      exporters: [signalfx]\n      processors: [memory_limiter, batch, resourcedetection]\n```\n\n----------------------------------------\n\nTITLE: Sending SignalFx PHP Data Directly via Apache Configuration\nDESCRIPTION: Configures direct data export to Splunk Observability Cloud within an Apache configuration file. It uses `SetEnv` to define the `SIGNALFX_ACCESS_TOKEN` and sets `SIGNALFX_ENDPOINT_URL` to the appropriate ingest URL for the specific realm, bypassing the need for a local OTel Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/instrumentation/instrument-php-application.rst#2025-04-22_snippet_8\n\nLANGUAGE: aconf\nCODE:\n```\nSetEnv SIGNALFX_ACCESS_TOKEN=<access_token>\nSetEnv SIGNALFX_ENDPOINT_URL=https://ingest.<realm>.signalfx.com/v2/trace/signalfxv1\n```\n\n----------------------------------------\n\nTITLE: Defining the Logging Exporter in YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to declare the logging exporter in the OpenTelemetry Collector configuration file. The exporter is set up with the verbosity parameter, where available levels are \"basic\", \"normal\", and \"detailed\". Dependencies include the Collector itself, and this section is typically placed under the exporters section alongside other exporters. Inputs are exporter configuration keys; output is console log emission during telemetry pipeline activity.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/logging-exporter.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n   # ...\n   # Other exporters\n   # ...\n   logging:\n      # loglevel is deprecated; use verbosity instead\n      # Available levels are \"basic\", \"normal\", and \"detailed\"\n      verbosity: detailed\n```\n\n----------------------------------------\n\nTITLE: Renaming Multiple CPU Metrics Using Regex\nDESCRIPTION: Uses regular expressions to rename all system.cpu metrics to system.processor.*.stat pattern.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: ^system\\.cpu\\.(.*)$$\nmatch_type: regexp\naction: update\nnew_name: system.processor.$${1}.stat\n```\n\n----------------------------------------\n\nTITLE: Setting Collector Permissions with setcap\nDESCRIPTION: Commands to set the required Linux capabilities for the Collector to run without root permissions. This allows the Collector to bypass file permission checks and manage arbitrary processes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsetcap CAP_SYS_PTRACE,CAP_DAC_READ_SEARCH=+eip /usr/bin/otelcol\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Monitor Receiver in Splunk Collector\nDESCRIPTION: Basic configuration to activate the Java monitor receiver in the Splunk Collector configuration file. Defines the receiver type and adds it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/java-monitor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/java-monitor:\n    type: java-monitor\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Automatic Discovery Flow Diagram\nDESCRIPTION: A flowchart showing the three main steps of the automatic discovery process: connecting to cloud environment, deploying the OpenTelemetry Collector, and running the application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/discovery-mode.rst#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n\n   accTitle: Automatic discovery and configuration process diagram\n   accDescr: Step one. Connect to your cloud environment. Step two. Ensure the OpenTelemetry Collector is running. Step three. Run your application.\n\n   X[\"Connect to your \\n cloud environment\"]\n\n   Y[\"Deploy the Splunk Distribution \\n of the OpenTelemetry Collector \\n in your environment\"]\n\n   Z[\"Run your application\"]\n   \n   X --> Y --> Z\n```\n\n----------------------------------------\n\nTITLE: Including Redaction Processor in OpenTelemetry Collector Pipeline\nDESCRIPTION: This YAML configuration snippet shows how to include the Redaction processor in the traces pipeline of the OpenTelemetry Collector service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/redaction-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      processors: [redaction]\n```\n\n----------------------------------------\n\nTITLE: Installing VictorOps Nagios Plugin (RPM)\nDESCRIPTION: Command to install the VictorOps Nagios plugin using rpm on RPM-based systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/check_mk-integration.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrpm -i <path_to_file>\n```\n\n----------------------------------------\n\nTITLE: Manually Installing Auto Instrumentation RPM Package for Node.js (Bash)\nDESCRIPTION: Installs or upgrades the `splunk-otel-auto-instrumentation` package using a downloaded RPM (`.rpm`) file. The `-Uvh` flags stand for upgrade, verbose, and hash progress bar. Replace `<path to splunk-otel-auto-instrumentation rpm>` with the actual file path. This command is used for manual upgrades on RPM-based systems in the context of Node.js instrumentation. Requires `sudo` privileges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nsudo rpm -Uvh <path to splunk-otel-auto-instrumentation rpm>\n```\n\n----------------------------------------\n\nTITLE: Example Dimension Key-Value Pair in Splunk Observability Cloud\nDESCRIPTION: Examples of properly formatted dimension key-value pairs in Splunk Observability Cloud, where keys must be UTF-8 strings (max 128 characters) and values must be UTF-8 strings (max 256 characters).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/metric-names.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\"hostname\": \"production1\"\n\"region\": \"emea\"\n```\n\n----------------------------------------\n\nTITLE: Starting Live Documentation Preview using Make (Shell)\nDESCRIPTION: Executes the 'livehtml' target from the Makefile within the Docker container. This command typically starts a local development server (often accessible at http://localhost:8888) that serves the built documentation and automatically rebuilds and reloads the browser when source files are modified, providing a live preview.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nmake livehtml\n```\n\n----------------------------------------\n\nTITLE: Activating Elasticsearch Receiver in YAML Configuration\nDESCRIPTION: Basic YAML configuration to activate the Elasticsearch receiver in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/elasticsearch-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    elasticsearch:\n```\n\n----------------------------------------\n\nTITLE: Muting Alerts with RegEx in Splunk On-Call Rules Engine\nDESCRIPTION: This rule uses RegEx to look for 'error' or 'ERROR' in the subject field and sets the message_type to INFO to mute noisy alerts.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-transformations.rst#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nWhen subject matches ^((?!error|ERROR).)*$ using RegEx\n\nSet message_type to INFO\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Pod Environment Variables for Host IP - YAML\nDESCRIPTION: Configures a Kubernetes Pod to expose its node (host) IP as the K8S_NODE_IP environment variable, which can be used by profiled applications for OTLP endpoint configuration. Places env block in pod spec, uses valueFrom with fieldRef to dynamically pull status.hostIP. Required when OTLP endpoints must point to node IP addresses in Kubernetes environments. Standard Kubernetes 1.x+ is required.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/profiling/get-data-in-profiling.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\\n- name: K8S_NODE_IP\\n  valueFrom:\\n    fieldRef:\\n      apiVersion: v1\\n      fieldPath: status.hostIP\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk Observability Cloud Provider\nDESCRIPTION: This snippet configures the Splunk Observability Cloud provider in Terraform by setting the authentication token and API URL using defined variables. It demonstrates how to point Terraform to the appropriate Splunk signalfx endpoints.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-datalinks-terraform/apm-create-data-links-terraform-file.rst#2025-04-22_snippet_1\n\nLANGUAGE: terraform\nCODE:\n```\n# Configure the Splunk Observability Cloud provider\nprovider \"signalfx\" {\nauth_token = \"${var.signalfx_auth_token}\"\napi_url    = \"${var.signalfx_api_url}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example OTLP Receiver Configuration - OTel Collector\nDESCRIPTION: This YAML snippet demonstrates configuring the OTLP receiver protocol settings for the OpenTelemetry Collector, used to match the Java agent's protocol choices.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/common-java-troubleshooting.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\notlp:\n  protocols:\n    grpc:\n      endpoint: \"${SPLUNK_LISTEN_INTERFACE}:4317\"\n    http:\n      endpoint: \"${SPLUNK_LISTEN_INTERFACE}:4318\"\n```\n\n----------------------------------------\n\nTITLE: Running Node.js Application with OpenTelemetry Instrumentation - Bash\nDESCRIPTION: This bash snippet enables automatic instrumentation of a Node.js application using the Splunk Distribution of OpenTelemetry JS. The `-r` (require) flag preloads `@splunk/otel/instrument` before running your code, allowing instrumentation without direct code changes. Prerequisite: `@splunk/otel` must be installed. Replace `<your-app.js>` with your application entry point script. Outputs telemetry directly as configured in OpenTelemetry.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/troubleshooting/migrate-signalfx-nodejs-agent-to-otel.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnode -r @splunk/otel/instrument <your-app.js>\n```\n\n----------------------------------------\n\nTITLE: Installing VictorOps Nagios Plugin using apt (Debian)\nDESCRIPTION: Provides an alternative method using `apt install` to install the downloaded VictorOps Nagios plugin `.deb` package. This command handles dependencies automatically. Requires `sudo` for elevated privileges. Replace `<path_to_file>` with the path to the downloaded file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install <path_to_file>\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Query Receiver in YAML\nDESCRIPTION: Basic configuration for activating the SQL Query receiver in the OpenTelemetry Collector configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sqlquery-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  sqlquery:\n    driver: your.driver\n    datasource: \"your_data_source\"\n    queries:\n      - sql: \"your_query\"\n```\n\n----------------------------------------\n\nTITLE: Counting Splunk RUM Custom Event Requests/Errors\nDESCRIPTION: The metric 'rum.workflow.count' in Splunk RUM tracks the total number of custom events, including both successful requests and errors. This is applicable to both browser and mobile RUM.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/RUM-metrics.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nrum.workflow.count\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic cgroups Monitor in YAML\nDESCRIPTION: Basic configuration to activate the cgroups monitor integration in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/cgroups.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/cgroups: \n    type: cgroups\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring CoreDNS Monitor for Kubernetes Environment (YAML)\nDESCRIPTION: This example configuration is specific to a Kubernetes environment. It sets up the CoreDNS monitor with a discovery rule to find CoreDNS pods and adds extra dimensions to the metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/coredns.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/coredns:\n    type: coredns\n    discoveryRule: kubernetes_pod_name =~ \"coredns\" && port == 9153\n    extraDimensions:\n      metric_source: \"k8s-coredns\"\n```\n\n----------------------------------------\n\nTITLE: Modify SELinux Policy for Network Explorer - Bash\nDESCRIPTION: This bash script modifies the SELinux Super-Privileged Container (SPC) policy to grant additional permissions needed by Network Explorer kernel collector on Red Hat Enterprise Linux CoreOS with SELinux activated.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntmp_dir=$(mktemp -d -t EBPF_NET-XXXXX)\n\n      cat > \"${tmp_dir}/spc_bpf_allow.te\" <<END\n      module spc_bpf_allow 1.0;\n      require {\n          type spc_t;\n          class bpf {map_create map_read map_write prog_load prog_run};\n      }\n      #============= spc_t ==============\n\n      allow spc_t self:bpf { map_create map_read map_write prog_load prog_run };\n      END\n      checkmodule -M -m -o \"${tmp_dir}/spc_bpf_allow.mod\" \"${tmp_dir}/spc_bpf_allow.te\"\n      semodule_package -o \"${tmp_dir}/spc_bpf_allow.pp\" -m \"${tmp_dir}/spc_bpf_allow.mod\"\n      semodule -i \"${tmp_dir}/spc_bpf_allow.pp\"\n```\n\n----------------------------------------\n\nTITLE: Configure Java Agent System Properties\nDESCRIPTION: Illustrates adding system properties to runtime parameters for configuring the OpenTelemetry Java agent. System properties can override existing environment variables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/version1x/java-1x-otel-configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\njava -javaagent:./splunk-otel-javaagent.jar \\\n-Dotel.service.name=<my-java-app> \\\n-jar <myapp>.jar\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Lambda Execution Wrapper for Python in Splunk OpenTelemetry\nDESCRIPTION: The execution wrapper path configuration for Python Lambda functions when using Splunk OpenTelemetry.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/lambda-configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n/opt/otel-instrument\n```\n\n----------------------------------------\n\nTITLE: Configuring Span Status in YAML\nDESCRIPTION: Example configuration for setting the status of a span, including error description.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/span-processor.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nspan/set_status:\n  status:\n    code: Error\n    description: \"some error description\"\n```\n\n----------------------------------------\n\nTITLE: Collectd Metrics Configuration\nDESCRIPTION: Configuration for collectd's df plugin to collect disk space metrics with a 3600-second interval.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/other-ingestion-collectd.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n<LoadPlugin df>\n   Interval 3600\n</LoadPlugin>\n<Plugin df>\n   ValuesPercentage true\n</Plugin>\n```\n\n----------------------------------------\n\nTITLE: Adding cAdvisor Monitor to Metrics Pipeline\nDESCRIPTION: Configuration example showing how to add the cAdvisor monitor to the metrics pipeline in the service section of the collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/cadvisor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/cadvisor]\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment and Plugin Settings in Values.yaml\nDESCRIPTION: Configure the Helm values.yaml file to allow the loading of unsigned plugins, specify the plugin ZIP path, and set up the necessary data source for the Splunk Observability Cloud plugin. Ensure environment variables, plugin paths, and datasource settings are correctly added.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-helm.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n    GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: cisco-splunko11y-datasource\n```\n\nLANGUAGE: yaml\nCODE:\n```\nplugins:\n  - https://<s3-bucket-path>/grafana-plugin-23.1.0-21.zip;cisco-splunko11y-datasource\n```\n\nLANGUAGE: yaml\nCODE:\n```\ndatasources:\n  # Splunk Observability Cloud plugin\n  datasources.yaml:\n    apiVersion: 1\n    datasources:\n      - name: cisco-splunko11y-datasource\n        type: cisco-splunko11y-datasource\n        isDefault: true\n        editable: true\n        version: 1\n        jsonData:\n          realm: <splunk-o11y-realm>\n          apiKey: <splunk-o11y-access-token>\n```\n\n----------------------------------------\n\nTITLE: JSON Payload for Resolving Datadog Monitors in Splunk On-Call\nDESCRIPTION: This JSON payload is used in a custom outgoing webhook to resolve Datadog monitors from Splunk On-Call. It uses a placeholder for the Datadog monitor ID.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/datadog-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"resolve\": [\n       {\n          \"${{ALERT.datadog_monitor_id}}\": \"ALL_GROUPS\"\n       }\n    ]\n }\n```\n\n----------------------------------------\n\nTITLE: Adding Maven Central to build.gradle\nDESCRIPTION: Code snippet showing how to add Maven Central to the repositories section in the main build.gradle file to access the Android RUM agent dependency.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_2\n\nLANGUAGE: kotlin\nCODE:\n```\nallprojects {\n   repositories {\n      google()\n   //...\n      mavenCentral()\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OTel Collector Proxy via Ansible (YAML)\nDESCRIPTION: Demonstrates setting proxy configuration variables (`splunk_otel_collector_proxy_http`, `splunk_otel_collector_proxy_https`, `splunk_otel_collector_no_proxy`) when using the `signalfx.splunk_otel_collector.collector` Ansible role to install and manage the Splunk OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/allow-services.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Install Splunk OpenTelemetry Collector\n  hosts: all\n  become: 'yes'\n  tasks:\n    - name: Include splunk_otel_collector\n      include_role:\n        name: signalfx.splunk_otel_collector.collector\n      vars:\n        splunk_access_token: YOUR_ACCESS_TOKEN\n        splunk_realm: SPLUNK_REALM\n        # Set the proxy address, respectively for http_proxy and https_proxy environment variables\n        # It must be a full URL like http://user:pass@10.0.0.42. Not used by Ansible itself.\n        splunk_otel_collector_proxy_http: proxy.address:<port>\n        splunk_otel_collector_proxy_https: proxy.address:<port>\n        # Set the ip or hosts that don't use proxy settings. Only used if splunk_otel_collector_proxy_http\n        # or splunk_otel_collector_proxy_https is defined. Default is localhost,127.0.0.1,::1)\n        splunk_otel_collector_no_proxy): 127.0.0.1\n```\n\n----------------------------------------\n\nTITLE: Setting Debug Log Level for Ruby OpenTelemetry\nDESCRIPTION: Sets the OTEL_LOG_LEVEL environment variable to 'debug' to increase the verbosity of Ruby instrumentation for troubleshooting purposes. This should be disabled after troubleshooting to prevent system overload.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/troubleshooting/common-ruby-troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_LOG_LEVEL=\"debug\"\n```\n\n----------------------------------------\n\nTITLE: Advanced Filesystems Monitor Configuration\nDESCRIPTION: Extended configuration example showing how to collect additional metrics from specific filesystem types with extra dimensions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/filesystems.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsmartagent/filesystems:\n   type: filesystems\n   extraMetrics:\n   - df_complex.reserved\n   - df_inodes.free\n   - df_inodes.used\n   - percent_inodes.free\n   - percent_inodes.used\n   - percent_bytes.free\n   - percent_bytes.reserved\n   - percent_bytes.used\n   fsTypes:\n   - ext3\n   - ext4\n   - nfs\n   - xfs\n   - btrfs\n   sendModeDimension: true\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Zero-Code Instrumentation for systemd Services\nDESCRIPTION: Command to install the Splunk OpenTelemetry Collector with Node.js instrumentation specifically for applications running as systemd services.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-systemd-instrumentation --realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Conditionally Rendering Content with notEmpty - Handlebars\nDESCRIPTION: Illustrates the use of the notEmpty block helper to conditionally include content in the payload only if a target map or object is not empty. Limited to the inputs and dimensions built-in variables; skips rendering if the target is empty. Useful for clean templating and reducing noise in alert payloads.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/notif-services/splunkplatform.rst#2025-04-22_snippet_3\n\nLANGUAGE: Handlebars\nCODE:\n```\nThis example only prints if dimensions is not empty: {{#notEmpty dimensions}}\n``{{/notEmpty}}``\n```\n\n----------------------------------------\n\nTITLE: Using AutoDetect Detector for AWS Route 53 Health Checks\nDESCRIPTION: This SignalFlow function is used to detect unhealthy statuses in AWS Route 53 health check endpoints. It triggers an alert when the endpoint is unhealthy for a specified duration and resets when the endpoint regains health. Dependencies include setting appropriate sensitivity and threshold parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/autodetect/autodetect-list.rst#2025-04-22_snippet_0\n\nLANGUAGE: SignalFlow\nCODE:\n```\n\"https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/aws/route53.flow#L41\"\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: ReStructuredText markup defining the documentation structure for Ruby OpenTelemetry requirements including section headers, meta descriptions, and includes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/ruby-otel-requirements.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _ruby-otel-requirements:\n\n*************************************************************\nOpenTelemetry Ruby compatibility and requirements\n*************************************************************\n\n.. meta::\n    :description: This is what you need to instrument any Ruby application using the OpenTelemetry instrumentation for Ruby.\n\n.. _supported-ruby-libraries:\n\nSupported libraries and frameworks\n=================================================\n\n.. raw:: html\n\n   <div class=\"include-start\" id=\"requirements/ruby.rst\"></div>\n\n.. include:: /_includes/requirements/ruby.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"requirements/ruby.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Including Troubleshooting Components in RST\nDESCRIPTION: This snippet shows how to include troubleshooting components from an external file using RST directives. It uses raw HTML to wrap the include statement for better control over rendering.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/host-metadata.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"troubleshooting-components.rst\"></div>\n\n.. include:: /_includes/troubleshooting-components.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"troubleshooting-components.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Policy Document for EC2 Cost and Usage Data\nDESCRIPTION: This code snippet shows the necessary permissions to include in the AWS Policy Document to import EC2 cost and usage data into Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/monitor/aws-infra-metadata.rst#2025-04-22_snippet_0\n\nLANGUAGE: none\nCODE:\n```\n\"ec2:DescribeInstances\",\n\"ec2:DescribeInstanceStatus\",\n\"ec2:DescribeTags\",\n\"ec2:DescribeReservedInstances\",\n\"ec2:DescribeReservedInstancesModifications\",\n\"organizations:DescribeOrganization\",\n```\n\n----------------------------------------\n\nTITLE: Uninstalling only the Fluentd package on RPM-based Linux systems\nDESCRIPTION: Commands for uninstalling just the Fluentd (td-agent) package on RPM-based Linux systems using yum, dnf, or zypper package managers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-uninstall.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum remove td-agent\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo dnf remove td-agent\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo zypper remove td-agent\n```\n\n----------------------------------------\n\nTITLE: Installing Nagios Plugin RPM Package using rpm (Shell)\nDESCRIPTION: Installs the downloaded Nagios plugin RPM package using the `rpm -i` command. Replace `<path_to_file>` with the actual path to the downloaded .rpm file. This command requires root privileges, often executed with `sudo`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nrpm -i <path_to_file>\n```\n\n----------------------------------------\n\nTITLE: Constructing a Conditional Webhook URL with Variables\nDESCRIPTION: Example of a webhook destination URL constructed using a variable populated by the Splunk On-Call Alert Rules Engine. This allows the webhook to fire conditionally, as the URL is only valid (`https://dev.oscato.com/2tn6xfh`) when the variable `${{ALERT.vo-webhook-field}}` is populated with `oscato.com` by a matching rule.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/admin/get-started/custom-outbound-webhooks.rst#2025-04-22_snippet_3\n\nLANGUAGE: url\nCODE:\n```\nhttps://dev.${{ALERT.vo-webhook-field}}/2tn6xfh\n```\n\n----------------------------------------\n\nTITLE: Sample Certificate Verification Error Log\nDESCRIPTION: Example log output showing a TLS certificate verification failure when the Kubelet stats receiver attempts to scrape metrics from the Kubernetes API endpoint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-missing-metrics.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n2024-02-28T01:11:24.614Z    error   scraperhelper/scrapercontroller.go:200  Error scraping metrics  {\"kind\": \"receiver\", \"name\": \"kubeletstats\", \"data_type\": \"metrics\", \"error\": \"Get \\\"https://10.202.38.255:10250/stats/summary\\\": tls: failed to verify certificate: x509: cannot validate certificate for 10.202.38.255 because it doesn't contain any IP SANs\", \"scraper\": \"kubeletstats\"}\ngo.opentelemetry.io/collector/receiver/scraperhelper.(*controller).scrapeMetricsAndReport\n  go.opentelemetry.io/collector/receiver@v0.93.0/scraperhelper/scrapercontroller.go:200\ngo.opentelemetry.io/collector/receiver/scraperhelper.(*controller).startScraping.func1\n  go.opentelemetry.io/collector/receiver@v0.93.0/scraperhelper/scrapercontroller.go:176\n```\n\n----------------------------------------\n\nTITLE: Constructing Kibana Filter URL with Trace ID - Splunk Observability Cloud Configuration - Text/URL\nDESCRIPTION: This snippet demonstrates how to build a dynamic Kibana URL for use in a global data link, enabling users to filter logs in Kibana by a specific trace ID from Splunk APM. The URL includes placeholder variables for time range and traceId (e.g., {{start_time}}, {{end_time}}, {{value}}), which are substituted at runtime. Required dependencies include access to both Splunk Observability Cloud and a properly configured Kibana instance. This URL is intended to be inserted in the data link configuration UI, and expects the traceId to be available as a variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-create-data-links.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttp://<yourKibanaFQDN>/kibana/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:'{{start_time}}',mode:absolute,to:'{{end_time}}'))&_a=(columns:!(_source),interval:auto,query:(language:kuery,query:'traceId:{{value}}'),sort:!('@timestamp',desc))\n```\n\n----------------------------------------\n\nTITLE: Renaming State Label for Multiple CPU Metrics\nDESCRIPTION: Renames the 'state' label to 'cpu_state' for all system.cpu.* metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: ^system\\.cpu\\.\naction: update\noperations:\n    - action: update_label\n      label: state\n      new_label: cpu_state\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Monitor Receiver in OpenTelemetry Collector\nDESCRIPTION: YAML configuration for enabling the memory monitoring in the Splunk OpenTelemetry Collector using the SmartAgent receiver with collectd/memory type.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cache/memory.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/collectd/memory: \n    type: collectd/memory\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Monitoring Organization-Wide Computation Throttling in Splunk\nDESCRIPTION: This metric (`sf.org.numComputationsThrottled`) tracks the total number of computations throttled across the entire organization. Comparing this with `sf.org.numComputationsThrottledByToken` helps diagnose whether throttling issues are widespread or isolated to specific tokens.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.numComputationsThrottled\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Endpoint for OpenTelemetry Collector - Shell\nDESCRIPTION: This shell snippet demonstrates how to set the new OTLP metrics endpoint property for the OpenTelemetry Java agent during migration to version 2.5.0 or higher. It highlights the deprecation of the legacy property and recommends using the new -Dotel.exporter.otlp.metrics.endpoint syntax or the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable. The property specifies the HTTP/protobuf endpoint (e.g., http://localhost:4318/v1/metrics) for the Collector. Ensure all services use at least version 2.5.0, and update this property in custom deployments. The legacy property is shown for backward reference only.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/migrate-metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Legacy property and value: -Dsplunk.metrics.endpoint=http(s)://collector:9943\\n# You can also use the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable\\n-Dotel.exporter.otlp.metrics.endpoint=http://localhost:4318/v1/metrics\n```\n\n----------------------------------------\n\nTITLE: Enabling Environment Macros in Nagios/Icinga Configuration\nDESCRIPTION: Sets the `enable_environment_macros` directive to `1` within the main Icinga configuration file (e.g., `icinga.cfg` or `nagios.cfg`). This setting is crucial as it allows the Splunk On-Call notification script to access necessary alert details via environment variables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\nenable_environment_macros=1\n```\n\n----------------------------------------\n\nTITLE: Filtering Splunk RUM Page-Level Metrics by Page Name\nDESCRIPTION: The dimension 'sf_node_name' is associated with page-level metrics (those prefixed with 'rum.node.') in Splunk RUM. It allows users to filter and analyze metrics based on specific page names or identifiers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/RUM-metrics.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nsf_node_name\n```\n\n----------------------------------------\n\nTITLE: Setting OTLP Endpoint via Environment Variable in Windows PowerShell\nDESCRIPTION: Shows how to configure the OTLP exporter endpoint using environment variables in Windows PowerShell. This sets the target destination for OTLP data exported by the Go instrumentation on Windows systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/configuration/advanced-go-otel-configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317\n```\n\n----------------------------------------\n\nTITLE: Renaming a System CPU Metric\nDESCRIPTION: Simple metric rename configuration that changes system.cpu.usage to system.cpu.usage_time.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: system.cpu.usage\naction: update\nnew_name: system.cpu.usage_time\n```\n\n----------------------------------------\n\nTITLE: Regular Expression for Validating Traceparent Header\nDESCRIPTION: Provides a regular expression pattern for validating the traceparent value in the Server-Timing header.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/manual-rum-browser-instrumentation.rst#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n00-([0-9a-f]{32})-([0-9a-f]{16})-01\n```\n\n----------------------------------------\n\nTITLE: Using b3multi Trace Propagator in Windows PowerShell\nDESCRIPTION: This Windows PowerShell command sets the `OTEL_PROPAGATORS` environment variable to `b3multi`, ensuring backward compatibility with the SignalFx Tracing Library for Windows-based systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/advanced-nodejs-otel-configuration.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_PROPAGATORS=b3multi\n```\n\n----------------------------------------\n\nTITLE: Collectd HTTP Configuration\nDESCRIPTION: Configuration for collectd's write_http plugin to send data to OpenTelemetry Collector over HTTP on port 8081.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/other-ingestion-collectd.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nLoadPlugin write_http\n<Plugin \"write_http\">\n   <Node \"collector\">\n      URL \"http://otelcollector:8081\"\n      Format JSON\n      VerifyPeer false\n      VerifyHost false\n   </Node>\n</Plugin>\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Splunk OpenTelemetry Collector (Shell)\nDESCRIPTION: Removes the Splunk OpenTelemetry Collector for Linux from the system.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n--uninstall\n```\n\n----------------------------------------\n\nTITLE: Basic Jenkins Receiver Configuration in YAML\nDESCRIPTION: Basic configuration example showing how to activate the Jenkins integration in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/jenkins.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/jenkins:\n    type: collectd/jenkins\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Incorrect Past Relative Time Interval Specification in Splunk\nDESCRIPTION: Illustrates an incorrect way to specify a past relative time interval. The earlier time point (e.g., -3m) must always precede the later time point (e.g., -1m) when using the 'to' syntax. This specific format will not work.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/use-time-range-selector.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n-1m to -3m\n```\n\n----------------------------------------\n\nTITLE: Identifying Active Hosts Metric in Splunk APM (Host Plan)\nDESCRIPTION: Splunk Observability Cloud metric identifier (`sf.org.apm.numHosts`) representing the number of hosts actively sending data to Splunk APM. This metric is specifically used in the 'Hosts' chart on the APM Subscription Usage page for organizations with a host-based subscription plan.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/apm-billing-usage-index.rst#2025-04-22_snippet_4\n\nLANGUAGE: metric\nCODE:\n```\nsf.org.apm.numHosts\n```\n\n----------------------------------------\n\nTITLE: Starting OpenTelemetry Collector Service\nDESCRIPTION: Start the OpenTelemetry Collector service on the host. This command uses systemctl to manage the service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl start splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Service Pipeline Configuration for Kubernetes Events\nDESCRIPTION: Configuration for adding the Kubernetes events monitor to the service pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-events.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  logs/events:\n    receivers:\n      - smartagent/kubernetes-events\n```\n\n----------------------------------------\n\nTITLE: Activating Node.js Profiler via Application Code - javascript\nDESCRIPTION: Illustrates enabling the Node.js AlwaysOn Profiler directly in application code using the start function. Requires the Splunk OpenTelemetry JS distribution and Node.js 16+. Key parameters: serviceName, endpoint, and profiling with memoryProfilingEnabled. Inputs: config object; Output: profiler starts for specified service, exporting CPU/memory traces. Endpoint should reference correct collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/profiling/get-data-in-profiling.rst#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nstart({\n   serviceName: '<service-name>',\n   endpoint: 'collectorhost:port',\n   profiling: {                       // Activates CPU profiling\n      memoryProfilingEnabled: true,   // Activates Memory profiling\n   }\n});\n```\n\n----------------------------------------\n\nTITLE: Uninstalling SignalFx Tracing Library using npm (Bash)\nDESCRIPTION: This command removes the deprecated 'signalfx-tracing' package from a Node.js project using the npm package manager. This is the first step in migrating to the Splunk Distribution of OpenTelemetry JS.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/troubleshooting/migrate-signalfx-nodejs-agent-to-otel.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n.. code-block:: bash\n\n   npm uninstall --save signalfx-tracing  \n```\n\n----------------------------------------\n\nTITLE: Basic OPcache Receiver Configuration for OpenTelemetry Collector\nDESCRIPTION: YAML configuration to activate the OPcache integration in the OpenTelemetry Collector. This defines the receiver and adds it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cache/opcache.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/opcache:\n    type: collectd/opcache\n    ...  # Additional config\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/opcache]\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced UDP Protocol Options for Jaeger Receiver in YAML\nDESCRIPTION: Example YAML configuration showing advanced options for UDP protocols (thrift_binary and thrift_compact) in the Jaeger receiver. It includes settings for queue size, packet size, workers, and socket buffer size.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/jaeger-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprotocols:\n  thrift_binary:\n    endpoint: 0.0.0.0:6832\n    queue_size: 5_000\n    max_packet_size: 131_072\n    workers: 50\n    socket_buffer_size: 8_388_608\n```\n\n----------------------------------------\n\nTITLE: Disabling Metric Categories and Individual Metrics - Splunk O11y YAML\nDESCRIPTION: This code snippet demonstrates how to disable entire categories or individual metrics within the Splunk OpenTelemetry framework using YAML configurations. Metrics can be turned off by specifying them under the `disableMetrics` key. No additional dependencies are required to apply these configurations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  disableMetrics:\n    - tcp.all\n    - udp.all\n    - dns.all\n    - http.all\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  disableMetrics:\n    - tcp.bytes\n    - tcp.rtt.num_measurements\n    - tcp.active\n    - tcp.rtt.average\n    - tcp.packets\n    - tcp.retrans\n    - tcp.syn_timeouts\n    - tcp.new_sockets\n    - tcp.resets\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  disableMetrics:\n    - udp.bytes\n    - udp.packets\n    - udp.active\n    - udp.drops\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  disableMetrics:\n    - dns.client.duration.average\n    - dns.server.duration.average\n    - dns.active_sockets\n    - dns.responses\n    - dns.timeouts\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  disableMetrics:\n    - http.client.duration.average\n    - http.server.duration.average\n    - http.active_sockets\n    - http.status_code\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n  disableMetrics:\n    - ebpf_net.bpf_log\n    - ebpf_net.otlp_grpc.bytes_sent\n    - ebpf_net.otlp_grpc.failed_requests\n    - ebpf_net.otlp_grpc.metrics_sent\n    - ebpf_net.otlp_grpc.requests_sent\n    - ebpf_net.otlp_grpc.successful_requests\n    - ebpf_net.otlp_grpc.unknown_response_tags\n```\n\n----------------------------------------\n\nTITLE: SNMP Pipeline Configuration in YAML\nDESCRIPTION: Configuration example showing how to add the SNMP monitor to the metrics pipeline\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/snmp.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/snmp]\n```\n\n----------------------------------------\n\nTITLE: Configuring Sample Payload for New Relic Integration\nDESCRIPTION: This JSON sample payload is used to configure webhook notifications from New Relic to Splunk On-Call, providing details such as impacted entities, incident data, and event type. It requires the setup of New Relic integration and correct routing keys. The payload is customizable as per specific alert conditions and notification requirements.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/new-relic-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n      \"impactedEntities\": {{json entitiesData.names}},\n      \"totalIncidents\": {{json totalIncidents}},\n      \"trigger\": {{ json triggerEvent }},\n      \"isCorrelated\": {{ json isCorrelated }},\n      \"createdAt\": {{ createdAt }},\n      \"updatedAt\": {{ updatedAt }},\n      \"sources\": [\"newrelic\"],\n      \"alertPolicyNames\":{{ json accumulations.policyName }},\n      \"alertConditionNames\": {{ json accumulations.conditionName }},\n      \"workflowName\": {{ json workflowName }},\n      \"monitoring_tool\":\"New Relic\",\n      \"incident_id\":{{ json issueId }},\n      \"condition_name\" : {{ json accumulations.conditionName }},\n      \"details\" : {{ json annotations.title.[0] }},\n      \"severity\" : \"CRITICAL\",\n      \"current_state\" : {{#if issueClosedAtUtc}} \"CLOSED\" {{else if issueAcknowledgedAt}} \"ACKNOWLEDGED\" {{else}} \"OPEN\"{{/if}},\n      \"event_type\": \"INCIDENT\"\n      }\n```\n\n----------------------------------------\n\nTITLE: Filtering Synthetic Runs by HTTP Test Type in Splunk\nDESCRIPTION: Apply the filter `test_type=http` to metrics like `synthetics.run.count` to count synthetic runs that are specifically HTTP tests in Splunk Synthetic Monitoring. This focuses usage tracking on basic HTTP(S) endpoint checks.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/synthetics-usage.rst#2025-04-22_snippet_3\n\nLANGUAGE: Splunk Metric/Filter\nCODE:\n```\ntest_type=http\n```\n\n----------------------------------------\n\nTITLE: Initializing Splunk RUM with Crash Reporting in Objective-C\nDESCRIPTION: Initialize both the iOS RUM library and the Crash Reporting module in Objective-C, allowing the application to send crash reports to Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/install-rum-ios.rst#2025-04-22_snippet_3\n\nLANGUAGE: objective-c\nCODE:\n```\n@import SplunkOtel;\n@import SplunkOtelCrashReporting;\n//...\nSplunkRumBuilder *builder = [[SplunkRumBuilder alloc] initWithRealm:@\"<realm>\"  rumAuth: @\"<rum-token>\"]]];\n[builder deploymentEnvironmentWithEnvironment:@\"<environment-name>\"];\n[builder setApplicationName:@\"<your_app_name>\"];\n[builder build];\n// Initialize crash reporting module after the iOS agent\n[SplunkRumCrashReporting start]\n```\n\n----------------------------------------\n\nTITLE: Example Environment Tag in Application Instrumentation\nDESCRIPTION: An example showing how deployment environment tags can be specified at the application level for distinguishing between different environments in a Kubernetes deployment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/tag-decision-support.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndeployment.environment\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenTelemetry Tracing in Python without Auto-instrumentation\nDESCRIPTION: Imports required OpenTelemetry modules for manual tracing when auto-instrumentation is not being used.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import (\n   BatchSpanProcessor,\n   ConsoleSpanExporter,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Exporter for Splunk RUM in JavaScript\nDESCRIPTION: This snippet shows how to configure the Splunk RUM agent to use the OTLP exporter instead of the default Zipkin exporter. It demonstrates setting the exporter.otlp option to true.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/configure-rum-browser-instrumentation.rst#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nSplunkRum.init({\n   realm: '<realm>',\n   rumAccessToken: '<your_rum_token>',\n   applicationName: '<application-name>',\n   deploymentEnvironment: '<deployment-env>',\n   exporter: {\n      otlp: true\n   },\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Minimum Required Fields for Zabbix to Splunk On-Call Notifications\nDESCRIPTION: Required text fields that must be included in the Zabbix notification payload to Splunk On-Call. This is the minimum configuration needed for proper integration between Zabbix and Splunk On-Call.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zabbix-integration.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nVO_ORGANIZATION_ID= YOUR_ORG_SLUG_HERE CONTACTEMAIL= YOUR_EMAIL_HERE\nESC.HISTORY={ESC.HISTORY} EVENT.ACK.HISTORY={EVENT.ACK.HISTORY}\nEVENT.ACK.STATUS={EVENT.ACK.STATUS} EVENT.DATE={EVENT.DATE}\nEVENT.TIME={EVENT.TIME} HOSTNAME={HOSTNAME} HOST.NAME={HOST.NAME}\nHOST.NAME1={HOST.NAME1} TRIGGER.KEY={TRIGGER.KEY} TIME={TIME}\nTRIGGER.ID={TRIGGER.ID} TRIGGER.NAME={TRIGGER.NAME}\nTRIGGER.NSEVERITY={TRIGGER.NSEVERITY}\nTRIGGER.SEVERITY={TRIGGER.SEVERITY} TRIGGER.STATUS={TRIGGER.STATUS}\n```\n\n----------------------------------------\n\nTITLE: Converting Strings to Raw JSON with json - Handlebars\nDESCRIPTION: Uses the json helper to transform a string or variable (such as a map or list) into its raw JSON representation for integration with external APIs or payloads. Useful for producing valid JSON fragments from values managed in the detector template context.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/notif-services/splunkplatform.rst#2025-04-22_snippet_4\n\nLANGUAGE: Handlebars\nCODE:\n```\n``{{{json dimensions}}}``\n```\n\n----------------------------------------\n\nTITLE: Minikube Successful Cluster Creation Message\nDESCRIPTION: Example success message that appears when Minikube has successfully created a local Kubernetes cluster.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/collector-config-tutorial-start.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nDone! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n```\n\n----------------------------------------\n\nTITLE: Adding Splunk OpenTelemetry Collector User to Docker Group (Linux)\nDESCRIPTION: Command to add the splunk-otel-collector user to the docker group for accessing the Docker API.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/docker.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nusermod -aG docker splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Restarting the Collector Service\nDESCRIPTION: Command to restart the OpenTelemetry Collector service to apply configuration changes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-edit.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsudo systemctl restart splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Initializing Terraform Environment\nDESCRIPTION: This shell command initializes the Terraform working directory. It reads the Terraform configuration files (including the one specifying the required providers), downloads the necessary provider plugins (like the Splunk On-Call provider), and sets up the backend for storing state. This command must be run after creating or modifying the provider configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/manage-splunk-oncall-using-terraform.rst#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nterraform init\n```\n\n----------------------------------------\n\nTITLE: Defining Database Contact Configuration in Nagios\nDESCRIPTION: Configuration for setting up a Database team contact in Nagios that integrates with Splunk On-Call using the VictorOps_Contact template.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndefine contact{\nuse            VictorOps_Contact\nname           VictorOps_database\ncontact_name   VictorOps_database\nalias          VictorOps_database\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Top or Bottom N Elements with Splunk\nDESCRIPTION: This functionality is leveraged for listing top or bottom N values using metrics in a list chart type, such as illustrating `cpu.utilization.` Users can choose the count or percentage to filter results. Key interactions include sorting and customizing display fields for a cleaner presentation. The output is a ranked list chart based on user-defined parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/gain-insights-through-chart-analytics.rst#2025-04-22_snippet_3\n\nLANGUAGE: none\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Updating Instrumentation Entry Point with Splunk OpenTelemetry - JavaScript\nDESCRIPTION: This snippet shows how to switch to the Splunk Distribution of OpenTelemetry JS for instrumentation entry in Node.js. It uses the `@splunk/otel` package's `start` function with a configuration object. Dependencies: the `@splunk/otel` npm package must be installed. Options passed to `start` define the OpenTelemetry tracing configuration. No return value, as tracing is globally initialized.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/troubleshooting/migrate-signalfx-nodejs-agent-to-otel.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require('@splunk/otel');\n\nstart({\n // your new options here\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Filesystems Monitor Configuration in YAML\nDESCRIPTION: Basic configuration to activate the filesystems monitor in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/filesystems.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/filesystems:\n    type: filesystems\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk Enterprise Service in Docker Compose\nDESCRIPTION: Defines the Splunk Enterprise service that receives logs from the Collector. Includes environment configuration, health checks, volume mounts for persistence, and port forwarding.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/docker-compose.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsplunk:\n  image: splunk/splunk:9.1.2\n  container_name: splunk\n  environment:\n    - SPLUNK_START_ARGS=--accept-license\n    - SPLUNK_HEC_TOKEN=00000000-0000-0000-0000-0000000000000\n    - SPLUNK_PASSWORD=changeme\n  ports:\n    - 18000:8000\n  healthcheck:\n    test: ['CMD', 'curl', '-f', 'http://localhost:8000']\n    interval: 5s\n    timeout: 5s\n    retries: 20\n  volumes:\n    - ./splunk.yml:/tmp/defaults/default.yml\n    - /opt/splunk/var\n    - /opt/splunk/etc\n```\n\n----------------------------------------\n\nTITLE: Setting Python Logging Level for Trace Correlation\nDESCRIPTION: Example of how to set the logging level by passing it as an argument to the LoggingInstrumentor class.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nLoggingInstrumentor(log_level=logging.DEBUG)\n```\n\n----------------------------------------\n\nTITLE: Injecting Service and Environment Environment Variables into JBoss LogManager Logs - Text\nDESCRIPTION: This property line for JBoss LogManager enables pattern-based log output with injected OTEL_SERVICE_NAME and OTEL_ENV_NAME environment variables. Useful in serverless environments where service and environment context is determined by env vars. Set in 'logging.properties' and ensure environment variables are accessible by the logging system.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nformatter.PATTERN.pattern=service=${OTEL_SERVICE_NAME}, env=${OTEL_ENV_NAME}\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Parameters for Site 24x7-Splunk On-Call Webhook Integration - JSON\nDESCRIPTION: This JSON object is used in the Site 24x7 webhook configuration to define custom parameters sent to Splunk On-Call. It includes message_type, monitoring_tool, state_message, entity_display_name, and entity_id. These parameters map Site 24x7 incident properties to fields expected by Splunk On-Call. No external dependencies are required; variables referenced (e.g., $INCIDENT_REASON, $MONITORNAME) are dynamically populated by Site 24x7 at runtime. Inputs are strings representing incident and monitor details, and output is a structured incident notification payload.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/site24x7-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"message_type\": \"critical\",\n    \"monitoring_tool\": \"Site24x7\",\n    \"state_message\": \"$INCIDENT_REASON\",\n    \"entity_display_name\": \"$INCIDENT_REASON\",       \"entity_id\":\"$MONITORNAME\"\n }\n```\n\n----------------------------------------\n\nTITLE: Adding SolrCloud Monitor to Metrics Pipeline\nDESCRIPTION: Configuration to add the SolrCloud monitor to the metrics pipeline in the Collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/solr.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/solr]\n```\n\n----------------------------------------\n\nTITLE: Configuring Lita Adapter for Splunk On-Call (Ruby)\nDESCRIPTION: This Ruby code snippet demonstrates how to configure a Lita instance to use the Splunk On-Call (VictorOps) adapter. It sets the mention name to 'lita', specifies 'victorops' as the adapter, and retrieves the required Splunk On-Call API token from the `VICTOROPS_TOKEN` environment variable. This token is necessary for authentication.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/litabot-integration-guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\nLita.configure do |config| \n   config.robot.mention_name = 'lita'\n   config.robot.adapter = 'victorops'\n   config.adapters.victorops.token = ENV['VICTOROPS_TOKEN'] \nend\n```\n\n----------------------------------------\n\nTITLE: Including HAProxy Receiver in Metrics Pipeline\nDESCRIPTION: This configuration snippet demonstrates how to include the HAProxy receiver in the metrics pipeline of the OpenTelemetry Collector service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/haproxy-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [haproxy]\n```\n\n----------------------------------------\n\nTITLE: Configuring Kapacitor Integration in INI\nDESCRIPTION: Configures the Kapacitor configuration file to enable VictorOps integration with specified API key and routing key. Adjust settings like 'enabled', 'api-key', and 'routing-key' for integration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/kapacitor-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n[victorops]\n   enabled = true\n   api-key = \"558e7ebc-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n   routing-key = \"Sample_route\"\n```\n\n----------------------------------------\n\nTITLE: Reset Action Configuration in JSON\nDESCRIPTION: This JSON snippet defines a reset action for SolarWinds alerts, containing placeholders for dynamic content insertion. Dependencies include correct setup of SolarWinds with configured entities. It requires parameters such as alert name and entity details.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/solarwinds-integration.rst#2025-04-22_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"message_type\":\"RECOVERY\",\n  \"monitor_name\":\"SolarWinds\",\n  \"monitoring_tool\":\"SolarWinds\",\n  \"alert_rule\":\"${N=Alerting;M=AlertName}\",\n  \"state_message\":\"${NodeName} ${N=SwisEntity;M=ComponentAlert.ComponentName} is ${N=SwisEntity;M=Status;F=Status}\",\n  \"entity_display_name\":\"${NodeName} ${N=SwisEntity;M=ComponentAlert.ComponentName} is ${N=SwisEntity;M=Status;F=Status}\",\n  \"entity_id\":\"${N=Alerting;M=AlertObjectID}\",\n  \"host_name\":\"${NodeName}\",\n  \"ip_address\":\"${Node.IP_Address}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Monitoring Accepted Spans Received in Splunk RUM (Metric)\nDESCRIPTION: This metric counts the number of spans received and accepted by Splunk RUM after applying limits. It is used in conjunction with `grossSpansReceived` and `numSpansDroppedThrottle` to monitor the Spans Per Minute (SPM) limit.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-rum.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.rum.numSpansReceived\n```\n\n----------------------------------------\n\nTITLE: Setting Container Name Environment Variable\nDESCRIPTION: This command sets an environment variable with the name of a selected container for further evaluation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-container.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nCONTAINER_NAME=otel-collector\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Attributes for Host Metrics Receiver in Shell\nDESCRIPTION: This snippet demonstrates how to set resource attributes for the host metrics receiver using the OTEL_RESOURCE_ATTRIBUTES environment variable. It sets the service name and version as example attributes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/host-metrics-receiver.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_RESOURCE_ATTRIBUTES=\"service.name=<name_of_service>,service.version=<version_of_service>\"\n```\n\n----------------------------------------\n\nTITLE: Monitoring Gross Span Bytes Received in Splunk RUM (Metric)\nDESCRIPTION: This metric monitors the total size (in bytes) of spans received by Splunk RUM before any throttling or filtering. It helps in understanding the incoming data volume relative to the Bytes Per Minute (BPM) limit.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-rum.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.rum.grossSpanBytesReceived\n```\n\n----------------------------------------\n\nTITLE: Importing PowerShell Module for Offline Installation\nDESCRIPTION: PowerShell command to import the Splunk OpenTelemetry .NET module from a local file during offline installation. This is used when the server doesn't have internet access.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_16\n\nLANGUAGE: powershell\nCODE:\n```\n# Make sure the Download path is correct\n\nImport-Module C:\\Users\\Administrator\\Downloads\\Splunk.OTel.DotNet.psm1\n```\n\n----------------------------------------\n\nTITLE: Querying Custom Metrics Using SignalFlow in Splunk Observability Cloud\nDESCRIPTION: This SignalFlow query retrieves the top 10 custom metrics being ingested by filtering for metrics with category type 3. The query counts metrics by their name and publishes the results with the label 'A'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/metric-categories.rst#2025-04-22_snippet_0\n\nLANGUAGE: SignalFlow\nCODE:\n```\nA = data('*', filter=filter('sf_mtsCategoryType', '3')).count(by=\"sf_metric\").top(10).publish(label='A')\n```\n\n----------------------------------------\n\nTITLE: Activating Splunk HEC Receiver in YAML Configuration\nDESCRIPTION: Basic YAML configuration to activate the Splunk HEC receiver in the OpenTelemetry Collector. This snippet shows how to add the receiver to the configuration file and include it in the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n   splunk_hec:\n\nservice:\n   pipelines:\n      metrics:\n         receivers: [splunk_hec]\n```\n\n----------------------------------------\n\nTITLE: Configuring Direct Data Export to Splunk Observability Cloud in Linux\nDESCRIPTION: Sets environment variables in Linux to send telemetry data directly to Splunk Observability Cloud instead of through a local collector. Requires an access token and realm-specific endpoint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nexport SPLUNK_ACCESS_TOKEN=<access_token>\nexport OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=http/protobuf\nexport OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://ingest.<realm>.signalfx.com/v2/trace/otlp\n```\n\n----------------------------------------\n\nTITLE: Configuring NTP Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration to activate the NTP monitor integration by defining the receiver and adding it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/ntp.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/ntp:\n    type: ntp\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud Provider in YAML\nDESCRIPTION: YAML configuration example showing how to set the cloud provider along with other essential parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkObservability:\n  accessToken: xxxxxx\n  realm: us0\nclusterName: my-k8s-cluster\ncloudProvider: aws\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Data Flow Diagram\nDESCRIPTION: Mermaid flowchart showing data flow through the OpenTelemetry Collector in Kubernetes environments, including receivers, processors, exporters and data paths to Splunk platforms.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/opentelemetry.rst#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n\n    accTitle: Splunk Distribution of the OpenTelemetry Collector for Kubernetes diagram.\n    accDescr: The Splunk Distribution of OpenTelemetry Collector contains receivers, processors, exporters, and extensions. Receivers gather metrics and logs from infrastructure, and metrics, traces, and logs from back-end applications. Receivers send data to processors, and processors send data to exporters. Exporters send data to Splunk Observability Cloud and Splunk Cloud Platform. Front-end experiences send data directly to Splunk Observability Cloud through RUM instrumentation.\n\n    subgraph \"\\nSplunk OpenTelemetry Collector for Kubernetes\"\n    receivers\n    processors\n    exporters\n    extensions\n    end\n\n    Infrastructure -- \"metrics, logs (native OTel)\" --> receivers\n    B[Back-end services] -- \"traces, metrics, logs (native OTel)\" --> receivers\n    C[Front-end experiences] -- \"traces\" --> S[Splunk Observability Cloud]\n\n    receivers --> processors\n    processors --> exporters\n\n    exporters --> S[Splunk Observability Cloud]\n    exporters --> P[Splunk Cloud Platform]\n    exporters -- \"metrics, logs (traces not supported)\" --> U[Splunk Enterprise]\n```\n\n----------------------------------------\n\nTITLE: Setting Service Group for Splunk Collector (Shell)\nDESCRIPTION: Specifies the group for the splunk-otel-collector service. This option will create the specified group if it does not already exist on the system.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nsplunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Adding CouchDB Monitor to OpenTelemetry Collector Pipeline\nDESCRIPTION: YAML configuration that adds the CouchDB monitor to the metrics pipeline in the OpenTelemetry Collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/apache-couchdb.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/couchdb]\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS IAM Policy for Splunk Observability Cloud Integration\nDESCRIPTION: A comprehensive IAM policy JSON example that includes permissions for collecting AWS metrics, tags, and properties for various AWS services. The policy includes both standard resource access permissions and specific Cassandra permissions with restricted resource paths.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-prereqs.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"airflow:ListEnvironments\",\n        \"airflow:GetEnvironment\",\n        \"apigateway:GET\",\n        \"autoscaling:DescribeAutoScalingGroups\",\n        \"cloudformation:ListResources\",\n        \"cloudformation:GetResource\",\n        \"cloudfront:GetDistributionConfig\",\n        \"cloudfront:ListDistributions\",\n        \"cloudfront:ListTagsForResource\",\n        \"cloudwatch:GetMetricData\",\n        \"cloudwatch:ListMetrics\",\n        \"directconnect:DescribeConnections\",\n        \"dynamodb:DescribeTable\",\n        \"dynamodb:ListTables\",\n        \"dynamodb:ListTagsOfResource\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeInstanceStatus\",\n        \"ec2:DescribeNatGateways\",\n        \"ec2:DescribeRegions\",\n        \"ec2:DescribeReservedInstances\",\n        \"ec2:DescribeReservedInstancesModifications\",\n        \"ec2:DescribeTags\",\n        \"ec2:DescribeVolumes\",\n        \"ecs:DescribeClusters\",\n        \"ecs:DescribeServices\",\n        \"ecs:DescribeTasks\",\n        \"ecs:ListClusters\",\n        \"ecs:ListServices\",\n        \"ecs:ListTagsForResource\",\n        \"ecs:ListTaskDefinitions\",\n        \"ecs:ListTasks\",\n        \"eks:DescribeCluster\",\n        \"eks:ListClusters\",\n        \"elasticache:DescribeCacheClusters\",\n        \"elasticloadbalancing:DescribeLoadBalancerAttributes\",\n        \"elasticloadbalancing:DescribeLoadBalancers\",\n        \"elasticloadbalancing:DescribeTags\",\n        \"elasticloadbalancing:DescribeTargetGroups\",\n        \"elasticmapreduce:DescribeCluster\",\n        \"elasticmapreduce:ListClusters\",\n        \"es:DescribeElasticsearchDomain\",\n        \"es:ListDomainNames\",\n        \"kafka:DescribeCluster\",\n        \"kafka:DescribeClusterV2\",\n        \"kafka:ListClusters\",\n        \"kafka:ListClustersV2\",\n        \"kinesis:DescribeStream\",\n        \"kinesis:ListShards\",\n        \"kinesis:ListStreams\",\n        \"kinesis:ListTagsForStream\",\n        \"kinesisanalytics:DescribeApplication\",\n        \"kinesisanalytics:ListApplications\",\n        \"kinesisanalytics:ListTagsForResource\",\n        \"lambda:GetAlias\",\n        \"lambda:ListFunctions\",\n        \"lambda:ListTags\",\n        \"logs:DeleteSubscriptionFilter\",\n        \"logs:DescribeLogGroups\",\n        \"logs:DescribeSubscriptionFilters\",\n        \"logs:PutSubscriptionFilter\",\n        \"network-firewall:ListFirewalls\",\n        \"network-firewall:DescribeFirewall\",\n        \"organizations:DescribeOrganization\",\n        \"rds:DescribeDBInstances\",\n        \"rds:DescribeDBClusters\",\n        \"rds:ListTagsForResource\",\n        \"redshift:DescribeClusters\",\n        \"redshift:DescribeLoggingStatus\",\n        \"s3:GetBucketLocation\",\n        \"s3:GetBucketLogging\",\n        \"s3:GetBucketNotification\",\n        \"s3:GetBucketTagging\",\n        \"s3:ListAllMyBuckets\",\n        \"s3:ListBucket\",\n        \"s3:PutBucketNotification\",\n        \"sqs:GetQueueAttributes\",\n        \"sqs:ListQueues\",\n        \"sqs:ListQueueTags\",\n        \"states:ListActivities\",\n        \"states:ListStateMachines\",\n        \"tag:GetResources\",\n        \"workspaces:DescribeWorkspaces\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cassandra:Select\"\n      ],\n      \"Resource\": [\n        \"arn:aws:cassandra:*:*:/keyspace/system/table/local\",\n        \"arn:aws:cassandra:*:*:/keyspace/system/table/peers\",\n        \"arn:aws:cassandra:*:*:/keyspace/system_schema/*\",\n        \"arn:aws:cassandra:*:*:/keyspace/system_schema_mcs/table/tags\",\n        \"arn:aws:cassandra:*:*:/keyspace/system_schema_mcs/table/tables\",\n        \"arn:aws:cassandra:*:*:/keyspace/system_schema_mcs/table/columns\"\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SolrCloud Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration to activate the SolrCloud monitor in the Collector configuration. Defines the receiver with type collectd/solr.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/solr.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/solr:\n    type: collectd/solr\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Socket.io Global Variable\nDESCRIPTION: Example demonstrating how to use a custom global variable name for the socket.io client initialization.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-instrumentations.rst#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nSplunkRum.init({\n// ...\ninstrumentations: {\n   socketio: {\n      target: 'socketIoClient',\n      },\n   },\n});\n// Expose the io object in your bundle\nwindow.socketIoClient = io;\n```\n\n----------------------------------------\n\nTITLE: Uninstalling OpenTelemetry Collector using Helm in Kubernetes\nDESCRIPTION: Command to uninstall the Splunk Distribution of OpenTelemetry Collector from a Kubernetes environment. This removes all Kubernetes components associated with the chart, deletes the release, and removes release history.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-uninstall.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm uninstall my-splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Initializing Session Recorder - Self-Hosted Deployment - HTML\nDESCRIPTION: Instructs on loading the Splunk session recorder using a self-hosted JavaScript file served from a custom path of your application. The file must be downloaded and made publicly accessible, then loaded as a script tag after SplunkRum.init. This only includes the script tag for inclusion; initialization with JavaScript must follow in the application's own code. Requires serving the script from your infrastructure with appropriate CORS policies.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/rum-session-replay.rst#2025-04-22_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<script src=\\\"<your-self-hosted-path>/splunk-otel-web-session-recorder.js\\\" crossorigin=\\\"anonymous\\\"></script>\n```\n\n----------------------------------------\n\nTITLE: Configuring zPages Extension for OpenTelemetry Collector\nDESCRIPTION: YAML configuration for setting up the zPages extension in the OpenTelemetry Collector, which exposes diagnostic endpoints on a specified network interface and port.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n   zpages:\n      endpoint: 0.0.0.0:55679\n```\n\n----------------------------------------\n\nTITLE: Configuring Global Alert Address for Splunk On-Call in Dataset\nDESCRIPTION: This configuration snippet shows how to set the top-level `alertAddress` field in Dataset's alert configuration file to direct all alerts to a Splunk On-Call webhook endpoint. Replace `$api_key` and `$routing_key` with your actual Splunk On-Call API key and desired routing key.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/scalyr-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   alertAddress: \"victorops:webhookUrl=https://alert.victorops.com/integrations/generic/20131114/alert/$api_key/$routing_key\",\n   alerts: [\n      {\n         trigger: \"alert expression 1\",\n         description: \"\"\n      },\n      {\n         trigger: \"alert expression 2\",\n         description: \"\"\n      }\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying new access token while maintaining collector version\nDESCRIPTION: Helm upgrade command to update only the access token while preserving the current chart version and other configuration values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade --reuse-values --version <Current_Chart_Version> --set splunkObservability.accessToken=<New_Access_Token> <Release_Name> splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: ReStructuredText markup for tutorial documentation structure including table of contents, metadata, and section headers\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/about-collector-config-tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _about-collector-configuration-tutorial-k8s:\n\n*****************************************************************************************\nTutorial: Configure the Splunk Distribution of the OpenTelemetry Collector on Kubernetes\n*****************************************************************************************\n\n.. meta::\n    :description: Follow this tutorial for a walkthrough of configuring the Splunk Distribution of OpenTelemetry Collector to collect telemetry in a Kubernetes environment.\n\n.. toctree::\n   :hidden:\n   :maxdepth: 3\n\n   collector-config-tutorial-start\n   collector-config-tutorial-edit\n```\n\n----------------------------------------\n\nTITLE: Activating HTTP Check Receiver in YAML Configuration\nDESCRIPTION: Basic YAML configuration to activate the HTTP check receiver in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/http-check-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  httpcheck:\n```\n\n----------------------------------------\n\nTITLE: Configuring Related Content Metadata for Splunk IM\nDESCRIPTION: Lists the required metadata combinations for Splunk Infrastructure Monitoring (IM) to enable the Related Content feature. These combinations, involving host, Kubernetes cluster, node, pod, container, or service names, allow linking IM data to other Observability Cloud components. The default Splunk OpenTelemetry Collector for Kubernetes provides these.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/relatedcontent.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-  ``host.name``. It falls back on ``host``, ``aws_private_dns_name`` (AWS), ``instance_name`` (GCP), ``azure_computer_name`` (Azure)\n- ``k8s.cluster.name``\n- ``k8s.cluster.name`` + ``k8s.node.name``\n- ``k8s.cluster.name`` + ``k8s.node.name`` (optional) + ``k8s.pod.name``\n- ``k8s.cluster.name`` + ``k8s.node.name`` (optional) + ``k8s.pod.name`` (optional) + ``container.id``\n- ``service.name``\n- ``service.name`` + ``deployment.environment`` (optional) + ``k8s.cluster.name`` (optional)\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenTelemetry C++ Repository using Bash\nDESCRIPTION: Clones the official OpenTelemetry C++ source code repository from GitHub into the current directory. This is the first step in building the necessary libraries for instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/cpp/instrument-cpp.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/open-telemetry/opentelemetry-cpp.git\n```\n\n----------------------------------------\n\nTITLE: Configuring JVM Options for Hadoop JMX in hadoop-env.sh\nDESCRIPTION: JVM options to enable JMX for Hadoop NameNode and DataNode in the hadoop-env.sh file. This configures JMX to use non-SSL connections without authentication on specific ports.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoopjmx.rst#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport HADOOP_NAMENODE_OPTS=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=5677 $HADOOP_NAMENODE_OPTS\"\nexport HADOOP_DATANODE_OPTS=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=5679 $HADOOP_DATANODE_OPTS\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Kubernetes Private Runner\nDESCRIPTION: Applies the deployment configuration YAML to a Kubernetes cluster for setting up a private runner.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_31\n\nLANGUAGE: shell\nCODE:\n```\noc apply -f deployment.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating Service Account for OpenTelemetry Collector\nDESCRIPTION: YAML configuration to create a service account for the OpenTelemetry Collector in Kubernetes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-objects-receiver.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n   labels:\n      app: otelcontribcol\n   name: otelcontribcol\n```\n\n----------------------------------------\n\nTITLE: Embedding HTML for SMS Section in reStructuredText\nDESCRIPTION: This snippet embeds HTML content to create a section header for SMS notifications in the reStructuredText document. It includes an anchor link for easy navigation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/notifications/notification-types.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n  \n    <embed>\n      <h2>SMS<a name=\"sms\" class=\"headerlink\" href=\"#sms\" title=\"Permalink to this headline\">¶</a></h2>\n    </embed>\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Actions in AppDynamics XML\nDESCRIPTION: This XML snippet configures custom actions in the AppDynamics custom.xml file. It specifies the victorops-alert action for both Linux/Unix and Windows environments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/appdynamics-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<custom-actions>\n   victorops-alert\n<!-- For Linux/Unix *.sh -->\n   victorops-alert.sh\n<!-- For windows *.bat -->\n   <!--<executable>victorops-alert.bat</executable>-->\n</custom-actions>\n```\n\n----------------------------------------\n\nTITLE: Sample AWS Integration JSON Response\nDESCRIPTION: This JSON object shows a sample response from the Splunk Observability Cloud API when retrieving AWS integration configurations. It includes two integrations with their properties such as authentication method, enabled status, regions, and other configuration parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"count\": 2,\n   \"results\": [\n      {\n         \"authMethod\": \"ExternalId\",\n         \"created\": 1674862496869,\n         \"createdByName\": null,\n         \"creator\": \"E-tkECKAsAA\",\n         \"customCloudWatchNamespaces\": null,\n         \"enableAwsUsage\": true,\n         \"enableCheckLargeVolume\": true,\n         \"enabled\": false,\n         \"externalId\": \"fyprhjmtpxttxwqhotep\",\n         \"id\": \"integration-id\",\n         \"importCloudWatch\": true,\n         \"largeVolume\": false,\n         \"lastUpdated\": 1674862497253,\n         \"lastUpdatedBy\": \"E-tkECKAsAA\",\n         \"lastUpdatedByName\": \"John Smith\",\n         \"name\": \"AWS Dev\",\n         \"pollRate\": 300000,\n         \"regions\": [ \"us-east-1\", \"us-east-2\", \"us-west-1\", \"us-west-2\" ],\n         \"roleArn\": null,\n         \"services\": [],\n         \"sfxAwsAccountArn\": \"arn:aws:iam::134183635603:root\",\n         \"syncCustomNamespacesOnly\": false,\n         \"syncLoadBalancerTargetGroupTags\": false,\n         \"type\": \"AWSCloudWatch\"\n      },\n      {\n         \"authMethod\": \"ExternalId\",\n         \"created\": 1522297476849,\n         \"createdByName\": null,\n         \"creator\": \"CGa4fY-AoAA\",\n         \"customCloudWatchNamespaces\": null,\n         \"enableAwsUsage\": true,\n         \"enableCheckLargeVolume\": false,\n         \"enabled\": true,\n         \"externalId\": \"uoejtvhsjnbcbdbfvbhg\",\n         \"id\": \"DZTsWRwAkAA\",\n         \"importCloudWatch\": false,\n         \"largeVolume\": false,\n         \"lastUpdated\": 1671440367214,\n         \"lastUpdatedBy\": \"CGa4fY-AoAA\",\n         \"lastUpdatedByName\": \"John Doe\",\n         \"name\": \"AWS Prod\",\n         \"pollRate\": 300000,\n         \"regions\": [ \"us-east-1\", \"us-east-2\", \"us-west-1\", \"us-west-2\" ],\n         \"roleArn\": \"arn:aws:iam::123456789012:role/splunk-o11y-role\",\n         \"services\": [],\n         \"sfxAwsAccountArn\": \"arn:aws:iam::134183635603:root\",\n         \"syncCustomNamespacesOnly\": false,\n         \"type\": \"AWSCloudWatch\"\n      }\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Restarting OpenTelemetry Collector on Windows\nDESCRIPTION: PowerShell commands to stop and start the Splunk OpenTelemetry Collector service on Windows systems after configuration updates.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_10\n\nLANGUAGE: PowerShell\nCODE:\n```\nStop-Service splunk-otel-collector\nStart-Service splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Filtering Dropped Spans by Size Limit Exceeded in Splunk RUM (Dimension)\nDESCRIPTION: This dimension can be added to the `sf.org.rum.numSpansDroppedInvalid` metric to specifically filter for spans that were dropped because they exceeded the 128kB span size limit.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-rum.rst#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nspanTooLarge\n```\n\n----------------------------------------\n\nTITLE: Manually Installing Auto Instrumentation RPM Package for .NET (Bash)\nDESCRIPTION: Installs or upgrades the `splunk-otel-auto-instrumentation` package using a downloaded RPM (`.rpm`) file. The `-Uvh` flags stand for upgrade, verbose, and hash progress bar. Replace `<path to splunk-otel-auto-instrumentation rpm>` with the actual file path. This command is used for manual upgrades on RPM-based systems in the context of .NET instrumentation. Requires `sudo` privileges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\nsudo rpm -Uvh <path to splunk-otel-auto-instrumentation rpm>\n```\n\n----------------------------------------\n\nTITLE: Updating OpenTelemetry Collector on Kubernetes with Helm\nDESCRIPTION: Helm command to upgrade an existing Splunk OpenTelemetry Collector deployment on Kubernetes with updated values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nhelm upgrade my-splunk-otel-collector --values my_values.yaml splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Retrieving OTP from Nylas Email API via XMLHttpRequest - JavaScript\nDESCRIPTION: This JavaScript snippet fetches an OTP from an email inbox using the Nylas API, performing a synchronous GET with custom headers to retrieve unread messages from a specific sender and subject line. It expects the Nylas Grant ID and API Key, as well as filtering parameters (sender, subject), to be substituted by the user. Upon a successful response, it parses the message snippet to extract an 8-digit OTP using a regular expression; otherwise, it returns error codes. Dependencies include Nylas API access, valid authentication tokens, and an endpoint returning a 'data' array with 'snippet' fields. The script must be executed in an environment supporting XMLHttpRequest and HTTP headers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/auth.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nfunction getOtp() {\n  const grantId = \"<NYLAS_GRANT_ID>\";\n  const jwToken = \"<NYLAS_API_KEY>\";\n  const from = \"noreply@github.com\";\n  const subject = \"Your GitHub launch code\";\n  const unread = \"true\";\n  const url = \"https://api.us.nylas.com/v3/grants/\" + grantId + \"/messages?limit=1&unread=\" + unread + \"from=\" + from + \"&subject=\" + subject;\n  var request = new XMLHttpRequest();\n  request.open(\"GET\", url, false);\n  request.setRequestHeader('Authorization', 'Bearer ' + jwToken)\n  request.send();\n  if (request.status == 200) {\n    return parseOtp(JSON.parse(request.responseText));\n  }\n  return \"ERR\";\n}\n\nfunction parseOtp(jsonResponse) {\n  const firstInbound = jsonResponse. data[0];\n  if (firstInbound && firstInbound.snippet) {\n    // Extract the number using a regular expression\n    const match = firstInbound.snippet.match(/\\b\\d{8}\\b/);\n    if (match) {\n      return match[0]; // Return the first matched number\n    }\n  }\n  return \"NO-OTP\";\n}\nreturn getOtp();\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Log Collection Commands\nDESCRIPTION: Commands for collecting information and logs from Kubernetes pods, including pod description and container logs for both the OpenTelemetry Collector and Fluentd containers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl describe pod my-pod kubectl logs my-pod otel-collector >my-pod-otel.log kubectl logs my-pod fluentd >my-pod-fluentd.log\n```\n\n----------------------------------------\n\nTITLE: Enabling Istio Autodetection in Splunk OpenTelemetry Collector Helm Values (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to enable Istio autodiscovery within the Splunk OpenTelemetry Collector's Helm configuration by setting `autodetect.istio` to `true`. This setting simplifies the integration process and is typically added to a custom `values.yaml` file used during Helm installation or upgrade.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/istio/istio.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nautodetect:\n   istio: true\n```\n\n----------------------------------------\n\nTITLE: Adding Hubot-VictorOps Dependency to package.json (JSON)\nDESCRIPTION: Updates your project's package.json to include 'hubot-victorops' as a dependency, ensuring that the correct adapter version (>=0.0.2) is available for Hubot runtime execution. Modifying package.json ensures future installs via npm ci/install fetch this package consistently.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/hubot-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n...\n\"dependencies\": {\n  \"hubot-victorops\": \">=0.0.2\",\n  ...\n}\n...\n```\n\n----------------------------------------\n\nTITLE: Adding OpenStack Monitor to Metrics Pipeline\nDESCRIPTION: Configuration section that adds the OpenStack monitor to the service metrics pipeline receivers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/openstack.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/openstack]\n```\n\n----------------------------------------\n\nTITLE: Success Response Example - Splunk On-Call REST Endpoint - JSON\nDESCRIPTION: Demonstrates the format of a success response returned by the Splunk On-Call REST Endpoint after a successful alert or incident creation. No external dependencies are required to parse this JSON response, but awareness of the schema is necessary. The response includes a 'result' indicating status and an 'entity_id' referencing the incident; inputs are not required, and this output is received as-is upon a successful request.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rest-endpoint-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \\u201cresult\\u201d:\\u201csuccess\\u201d, \\u201centity_id\\u201d:\\u201cYour entity_id here\\u201d }\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OTel Collector Proxy via Systemd Environment File (Shell)\nDESCRIPTION: Sets the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables within the Splunk OpenTelemetry Collector's Systemd environment file (`/etc/otel/collector/splunk-otel-collector.conf`). This method applies proxy settings directly to the collector service managed by Systemd.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/allow-services.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Edit the example systemd environment file in\n# /etc/otel/collector/splunk-otel-collector.conf.example\n\nNO_PROXY=<address,anotheraddress>\nHTTP_PROXY=http://<proxy.address:port>\nHTTPS_PROXY=http://<proxy.address:port>\n```\n\n----------------------------------------\n\nTITLE: Creating TLS Certificate Secret\nDESCRIPTION: Command to create a Kubernetes secret containing custom TLS certificates for secure communication.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic my-custom-tls --from-file=ca.crt=/path/to/custom_ca.crt --from-file=apiserver.key=/path/to/custom_key.key --from-file=apiserver.crt=/path/to/custom_cert.crt -n <namespace>\n```\n\n----------------------------------------\n\nTITLE: Verifying Runtime Status - Java Instrumentation\nDESCRIPTION: The 'jps -lvm' command helps verify that the Java runtime is active and correctly lists Java Virtual Machines. This is crucial to check if the instrumented JVM is running.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/common-java-troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n37602 target/spring-petclinic-2.4.5.jar -javaagent:./splunk-otel-javaagent.jar -Dotel.resource.attributes=service.name=pet-store-demo,deployment.environment=prod,service.version=1.2.0 -Dotel.javaagent.debug=true\n38262 jdk.jcmd/sun.tools.jps.Jps -lvm -Dapplication.home=/usr/lib/jvm/java-16-openjdk-amd64 -Xms8m -Djdk.module.main=jdk.jcmd\n```\n\n----------------------------------------\n\nTITLE: Response to Trigger Incident Request - Splunk On-Call REST Endpoint - JSON\nDESCRIPTION: Demonstrates the typical JSON response for a successful critical incident trigger request. Contains the result and entity_id returned by the API, confirming the incident was registered. The response is parsed to check the status of the operation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rest-endpoint-integration-guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \\u201cresult\\u201d : \\u201csuccess\\u201d, \\u201centity_id\\u201d : \\u201cdisk space/db01.mycompany.com\\u201d }\n```\n\n----------------------------------------\n\nTITLE: Enabling .NET Core Host Tracing via Environment Variables\nDESCRIPTION: These environment variables are used to enable verbose tracing for the .NET Core host (`COREHOST_TRACE=1`) and specify the output file (`COREHOST_TRACEFILE=corehost_verbose_tracing.log`). This is useful for troubleshooting low-level issues like assembly loading failures.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nCOREHOST_TRACE=1\nCOREHOST_TRACEFILE=corehost_verbose_tracing.log\n```\n\n----------------------------------------\n\nTITLE: Acknowledgement Alert JSON Payload for Zendesk Trigger\nDESCRIPTION: JSON payload for acknowledging incidents in Splunk On-Call when tickets are set to pending in Zendesk.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zendesk-bi-directional-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{ \n   \"entity_id\":\"{{ticket.id}}\", \n   \"message_type\":\"ACKNOWLEDGEMENT\",\n   \"state_message\":\"{{ticket.comments_formatted}}\",\n   \"monitoring_tool\":\"Zendesk\", \n   \"alert_url\":\"{{ticket.link}}\",\n   \"ticket_id\":\"{{ticket.id}}\", \n   \"Ticket External I.D.\":\"{{ticket.external_id}}\", \n   \"Ticket Origin\":\"{{ticket.via}}\",\n   \"Ticket Status\":\"{{ticket.status}}\", \n   \"Ticket Priority\":\"{{ticket.priority}}\" \n}\n```\n\n----------------------------------------\n\nTITLE: Setting B3 Multi Propagator for Linux\nDESCRIPTION: Shell command to set the B3 multi propagator for backward compatibility with SignalFx Python Tracing Library on Linux systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/configuration/advanced-python-otel-configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_PROPAGATORS=b3multi\n```\n\n----------------------------------------\n\nTITLE: Setting Service Name Environment Variable in Windows\nDESCRIPTION: Sets the OTEL_SERVICE_NAME environment variable in Windows PowerShell, which is required to identify your application in Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_4\n\nLANGUAGE: powershell\nCODE:\n```\n$env:OTEL_SERVICE_NAME=<yourServiceName>\n```\n\n----------------------------------------\n\nTITLE: Example Output of Terraform Apply Command\nDESCRIPTION: This shows sample output from the 'terraform apply' command, indicating the successful creation of various Splunk On-Call resources (team, user, escalation policy, contacts, routing key, team membership) as defined in the Terraform configuration. It lists each resource being created and confirms completion along with the number of resources added, changed, or destroyed.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/manage-splunk-oncall-using-terraform.rst#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nvictorops_team.team: Creating...\nvictorops_user.user1: Creating...\nvictorops_team.team: Creation complete after 1s [id=team-0mnx4iUkiP2OkqCt]\nvictorops_escalation_policy.devops_high_severity: Creating...\nvictorops_user.user1: Creation complete after 2s [id=john.doe-test]\nvictorops_contact.contact_email: Creating...\nvictorops_team_membership.test-membership: Creating...\nvictorops_contact.contact_phone: Creating...\nvictorops_escalation_policy.devops_high_severity: Creation complete after 1s [id=pol-tRi4Mn8fyGoN6p8b]\nvictorops_routing_key.infrastructure_high_severity: Creating...\nvictorops_contact.contact_phone: Creation complete after 1s [id=554c80cf-b6b7-465d-ab9f-0884b41a98fc]\nvictorops_contact.contact_email: Creation complete after 1s [id=a56f04be-04fa-4edb-a349-1705e1ac5a1c]\nvictorops_routing_key.infrastructure_high_severity: Creation complete after 1s [id=infrastructure-high-severity]\nvictorops_team_membership.test-membership: Creation complete after 2s [id=team-0mnx4iUkiP2OkqCt_john.doe-test]\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n```\n\n----------------------------------------\n\nTITLE: Starting the Documentation Build Environment (Shell)\nDESCRIPTION: Executes the 'start.sh' script located in the current directory. This script is intended to set up and launch the Docker container environment required for building the project documentation. Ensure the script has execution permissions before running.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./start.sh\n```\n\n----------------------------------------\n\nTITLE: Initializing Socket.io with Standalone Build\nDESCRIPTION: Example showing how to activate socket.io instrumentation with the standalone build using script tags. Demonstrates basic configuration with the splunk-otel-web.js library.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-instrumentations.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"/location/to/splunk-otel-web.js\"></script>\n<script>\n   SplunkRum.init({\n      // ...\n      instrumentations: {\n         socketio: true\n      }\n   });\n</script>\n<script src=\"/socket.io/socket.io.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: SAP HANA Pipeline Configuration\nDESCRIPTION: YAML configuration showing how to add the SAP HANA monitor to the metrics pipeline in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/sap-hana.rst#2025-04-22_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/hana]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Container Stats from Kubernetes\nDESCRIPTION: This command fetches detailed statistics for a specific container in a Kubernetes pod, including CPU and memory usage.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-container.rst#2025-04-22_snippet_8\n\nLANGUAGE: none\nCODE:\n```\nkubectl get --raw \"/api/v1/nodes/\"${NODE_NAME}\"/proxy/stats/summary\" | jq '.pods[] | select(.podRef.name=='\\\"$POD_NAME\\\"') | .containers[] | select(.name=='\\\"$CONTAINER_NAME\\\"') | {\"container\": {\"name\": .name, \"cpu\": .cpu, \"memory\": .memory}}'\n{\n  \"container\": {\n    \"name\": \"otel-collector\",\n    \"cpu\": {\n      \"time\": \"2022-05-20T18:42:15Z\",\n      \"usageNanoCores\": 6781417,\n      \"usageCoreNanoSeconds\": 1087899649154\n    },\n    \"memory\": {\n      \"time\": \"2022-05-20T18:42:15Z\",\n      \"availableBytes\": 389480448, \n      # Could be absent if container memory limits were missing.\n      \"usageBytes\": 135753728,\n      \"workingSetBytes\": 134807552,\n      \"rssBytes\": 132923392,\n      \"pageFaults\": 93390,\n      \"majorPageFaults\": 0\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating ConfigMap for OpenTelemetry Collector\nDESCRIPTION: YAML configuration to create a ConfigMap for otelcontribcol, including receiver and exporter settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-objects-receiver.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\ndata:\n  config.yaml: |\n    receivers:\n      k8sobjects:\n        objects:\n          - name: pods\n            mode: pull\n          - name: events\n            mode: watch\n    exporters:\n      otlp:\n        endpoint: <OTLP_ENDPOINT>\n        tls:\n          insecure: true\n\n    service:\n      pipelines:\n        logs:\n          receivers: [k8sobjects]\n          exporters: [otlp]\n```\n\n----------------------------------------\n\nTITLE: Exposing Socket.io Client in Bundle\nDESCRIPTION: Example showing how to expose the socket.io client in a bundle file and initialize a connection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-instrumentations.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { io } from 'socket.io-client';\nwindow.io = io;\nconst socket = io();\n// ...\n```\n\n----------------------------------------\n\nTITLE: Collecting Diagnostic Data for OpenTelemetry Collector Memory Issues\nDESCRIPTION: Commands to collect diagnostic information from a problematic OpenTelemetry Collector pod. These commands capture goroutine status and heap information for debugging memory issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/out_of_memory_error.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncurl http://127.0.0.1:1777/debug/pprof/goroutine?debug=2 (http://127.0.0.1:1777/debug/pprof/goroutine?debug=2)\ncurl http://127.0.0.1:1777/debug/pprof/heap > heap.out\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser RUM Instrumentations in TypeScript\nDESCRIPTION: Configuration example showing how to selectively disable different RUM instrumentations for troubleshooting purposes. Each instrumentation can be toggled individually.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ninstrumentations: {\n  // Comment out lines one by one to turn on each item\n  // and test which instrumentation is causing issues.\n  document: false,\n  errors: false,\n  fetch: false,\n  interactions: false,\n  longtask: false,\n  postload: false,\n  webvitals: false,\n  xhr: false,\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet API Settings in YAML\nDESCRIPTION: Configuration structure for the kubeletAPI object that defines connection and authentication settings for the Kubelet instance. Includes options for URL, authentication type, TLS settings, and debug logging.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-network-stats.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nkubeletAPI:\n  url: \"http://<current node hostname>:10255\"\n  authType: \"none|tls|serviceAccount\"\n  skipVerify: true\n  caCertPath: \"path/to/ca/cert\"\n  clientCertPath: \"path/to/client/cert\"\n  clientKeyPath: \"path/to/client/key\"\n  logResponses: false\n```\n\n----------------------------------------\n\nTITLE: Creating PRTG to Splunk On-Call Notification Script in PowerShell\nDESCRIPTION: This PowerShell script, saved as 'prtgtovictorops.ps1', processes alert details passed from PRTG, formats them into a JSON payload compatible with Splunk On-Call, and sends the payload to the specified Splunk On-Call Service API Endpoint. It defines parameters corresponding to PRTG alert variables, converts the PRTG status to a Splunk On-Call message type (critical, recovery, warning, info), ensures TLS 1.2 is used for the HTTPS request, logs the payload, and sends the alert data via a POST request.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/prtg-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: powershell\nCODE:\n```\nParam( [string]$API_URL,  [string]$MessageType,\n[string]$SiteName,  [string]$Device,\n[string]$DeviceId,  [string]$Name,\n[string]$Status,  [string]$Down,\n[string]$DateTime,  [string]$LinkDevice, [string]$Message )\n\nAdd-Type -AssemblyName System.Web.Extensions function ConvertTo-Json\n([Object] $value) {\n[System.Web.Script.Serialization.JavaScriptSerializer] $jsSerializer =\nNew-Object ‘System.Web.Script.Serialization.JavaScriptSerializer'\n$jsSerializer.Serialize($value) }\n\nfunction setMessageType ([string]$inputString) {\n  If ($inputString -like “Up*”) { return ‘recovery' }\n  elseif ($inputString -like \"Down*\")  {  return 'critical'  }\n  elseif ($inputString -like “Warning*”) { return ‘warning' }\n  else { return ‘info' }\n}\n\n$postVOAlert = ConvertTo-Json(@{\n    message_type = SetMessageType($Status);\n    entity_id = $DeviceId;\n    entity_display_name = $Device;\n    monitoring_tool = “PRTG”;\n    site_name = $SiteName;\n    link_device = \"<$($LinkDevice)|$($Device) $($Name)>\";\n    status =\"$($Status) $($Down) on $($DateTime)\";\n    state_message = $Message;\n})\n\n[Net.ServicePointManager]::SecurityProtocol =\n[Net.SecurityProtocolType]::Tls12\n$postVOAlert | Out-File -FilePath vo.log\n\n[System.Net.WebClient] $webclient = New-Object ‘System.Net.WebClient'\n$webclient.Headers.Add(“Content-Type”,“application/json”)\n$webclient.UploadData($API_URL, [System.Text.Encoding]::UTF8.GetBytes($postVOAlert)) | Out-File -FilePath vo.log -Append\n```\n\n----------------------------------------\n\nTITLE: Installation Success Message for Splunk OpenTelemetry Collector\nDESCRIPTION: Confirmation message displayed when the Collector has been successfully installed on Linux.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-start.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nThe Splunk OpenTelemetry Collector for Linux has been successfully installed.\n```\n\n----------------------------------------\n\nTITLE: Including Redis Receiver in Metrics Pipeline\nDESCRIPTION: YAML configuration snippet showing how to include the Redis receiver in the metrics pipeline of the OpenTelemetry Collector service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/redis-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [redis]\n```\n\n----------------------------------------\n\nTITLE: Disabling Background Task Reporting in Android Instrumentation\nDESCRIPTION: This code snippet shows how to deactivate instrumentation for a background task by passing the application ID to the disableBackgroundTaskReporting() method in the Splunk RUM configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic class SampleApplication extends Application {\n\n   @Override\n   public void onCreate() {\n      super.onCreate();\n\n      SplunkRum.builder()\n\n         // Other Settings\n         // ...\n\n         // Turn off instrumentation of background processes\n         .disableBackgroundTaskReporting(BuildConfig.<id_of_application>)\n         .build(this);\n      }\n   }\n```\n\n----------------------------------------\n\nTITLE: Retrieving AWS Integration Configuration from Splunk API\nDESCRIPTION: This curl command retrieves the current AWS integration configuration from the Splunk Observability Cloud API. It uses the GET method to fetch integration objects of type AWSCloudWatch, with parameters for pagination and sorting by last updated timestamp.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request GET https://api.<realm>.signalfx.com/v2/integration?type=AWSCloudWatch&offset=0&limit=50&orderBy=-lastUpdated\n--header \"X-SF-TOKEN:\" \\\n--header \"Content-Type:application/json\" > integration.json\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk Synthetics Runner with Helm\nDESCRIPTION: Shell commands for adding the Splunk Synthetics Helm repository and installing the runner chart. The installation includes setting the runner token as a secret.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add synthetics-helm-charts https://splunk.github.io/synthetics-helm-charts/\nhelm repo update\nhelm install my-splunk-synthetics-runner synthetics-helm-charts/splunk-synthetics-runner \\\n--set=synthetics.secret.runnerToken=YOUR_TOKEN_HERE \\\n--set synthetics.secret.create=true \n```\n\n----------------------------------------\n\nTITLE: RST Document Structure with Role and Class Definitions\nDESCRIPTION: RST markup defining the document structure with custom roles and classes for formatting different sections of the Splunk Observability Cloud documentation\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/get-started.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _get-started:\n\nGet started with Splunk Observability Cloud\n******************************************************\n\n.. meta::\n    :description: Learn how to get started with Splunk Observability Cloud.\n\n.. role:: icon-info\n.. rst-class:: newparawithicon\n\n:icon-info:`.` :strong:`Introduction to Splunk Observability Cloud`\n```\n\n----------------------------------------\n\nTITLE: Java Application Deployment YAML Without Instrumentation\nDESCRIPTION: Basic Kubernetes deployment manifest for a Java application before adding OpenTelemetry instrumentation annotations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-java-app\n  namespace: monitoring\nspec:\n  template:\n    spec:\n    containers:\n    - name: my-java-app\n        image: my-java-app:latest\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Payload for New Case in Desk.com Integration\nDESCRIPTION: This JSON snippet defines the payload structure for creating a new case in the Desk.com integration. It includes case details such as ID, subject, description, priority, customer info, and a direct URL to the case.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/desk-com-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"entity_id\":\"{{case.id}}\", \"message_type\":\"CRITICAL\", \"state_message\":\"New Case: {{case.id}} about {{case.subject}}\", \"Case Description\":\"{{case.description}}\", \"Case Priority\":\"{{case.priority}}\", \"Customer\":\"{{case.customer}}\", \"Case Email\":\"{{case.emails}}\", \"alert_url\":\"{{case.direct_url}}\" }\n```\n\n----------------------------------------\n\nTITLE: Azure Metrics RST Documentation Structure\nDESCRIPTION: RST markup defining the structure and metadata for Azure metrics documentation, including section headers and external references.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/azure/azure-metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _azure-metrics:\n\n*********************************************\nAzure metrics in Splunk Observability Cloud\n*********************************************\n\n.. meta::\n   :description: These are the metrics available for the Azure integration with Splunk Observability Cloud, grouped according to Azure resource.\n```\n\n----------------------------------------\n\nTITLE: Enabling Control Plane Metrics with Prometheus\nDESCRIPTION: Configuration to activate Kubernetes control plane metrics using the OpenTelemetry Prometheus receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nfeatureGates:\n  useControlPlaneMetricsHistogramData: true\n```\n\n----------------------------------------\n\nTITLE: Retrieving Kubernetes Pod Logs and Information\nDESCRIPTION: Commands for collecting pod descriptions and logs from both otel-collector and fluentd containers in Kubernetes environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/support-checklist.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl describe pod my-pod\nkubectl logs my-pod otel-collector >my-pod-otel.log\nkubectl logs my-pod fluentd >my-pod-fluentd.log\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes Deployment for Splunk Synthetics Runner\nDESCRIPTION: Shell command for applying the Kubernetes deployment configuration to create or update Splunk Synthetics runner instances.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f deployment.yaml\n```\n\n----------------------------------------\n\nTITLE: Retrieving Splunk OpenTelemetry Collector Service Name\nDESCRIPTION: This shell command retrieves the service name of the Splunk OpenTelemetry Collector by filtering the output of `kubectl get svc` using grep. It is useful for setting the `endpoint.address` option in the `otel-ebpf-values.yaml` file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get svc | grep splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Upgrading Auto Instrumentation via APT for .NET (Bash)\nDESCRIPTION: Updates the package list and upgrades only the `splunk-otel-auto-instrumentation` package using the APT package manager on Debian-based systems. This is specifically shown in the context of upgrading the instrumentation for .NET applications. Requires `sudo` privileges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get --only-upgrade splunk-otel-auto-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Identifying HipChat Room ID and Auth Token from URL\nDESCRIPTION: This URL structure is generated by HipChat when creating a custom integration for a specific room as part of the Multi-Room setup. It serves as the endpoint for sending messages to that room, highlighting where the `ROOM_ID` and `AUTH_TOKEN_HERE` are located within the URL. These components are necessary for configuring the Splunk On-Call outgoing webhook.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/hipchat-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://**YOUR_DOMAIN**.hipchat.com/v2/room/**ROOM_ID**/notification?auth_token=\\ **AUTH_TOKEN_HERE**\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis List Length Monitoring\nDESCRIPTION: Configuration example for monitoring the length of Redis lists by specifying database index and key pattern.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/redis.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsendListLengths: [{databaseIndex: $db_index, keyPattern: \"$key_name\"}]\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in reStructuredText for Log Observer Connect Documentation\nDESCRIPTION: A reStructuredText toctree directive that defines the hidden navigation structure for Log Observer Connect documentation, listing all the documentation files in a hierarchical structure.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/logs/lo-connect-landing.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :maxdepth: 3\n    :hidden:\n\n    intro-logconnect\n    scp\n    set-up-logconnect\n    LOconnect-troubleshoot\n    LOconnect-default-index\n    LOconnect-scenario\n    timeline\n    severity-key\n    queries\n    raw-logs-display\n    keyword\n    open-logs-splunk\n    alias\n    logs-individual-log-connect\n    message-field\n    aggregations\n    logviews\n    timestamp\n    logs-save-share\n    forward-logs\n    lo-transition\n    lo-connect-limits\n```\n\n----------------------------------------\n\nTITLE: Creating Splunk-managed Metric Streams IAM Policy in AWS\nDESCRIPTION: This JSON policy provides the permissions required for Splunk-managed Metric Streams to collect AWS CloudWatch metrics. It includes permissions for managing metric streams and allowing the iam:PassRole permission for specific Splunk roles.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-prereqs.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n  {\n    \"Effect\": \"Allow\",\n    \"Action\": [\n      \"cloudwatch:GetMetricStream\"\n      \"cloudwatch:ListMetricStreams\",\n      \"cloudwatch:PutMetricStream\",\n      \"cloudwatch:DeleteMetricStream\",\n      \"cloudwatch:StartMetricStreams\",\n      \"cloudwatch:StopMetricStreams\",\n      \"ec2:DescribeRegions\",\n      \"organizations:DescribeOrganization\",\n      \"tag:GetResources\"\n    ],\n    \"Resource\": \"*\"\n  },\n  {\n    \"Effect\": \"Allow\",\n    \"Action\": [\n      \"iam:PassRole\"\n    ],\n    \"Resource\": \"arn:aws:iam::*:role/splunk-metric-streams*\"\n  }\n]\n}\n```\n\n----------------------------------------\n\nTITLE: SignalFx Metrics Export Configuration with Disabled Aggregation\nDESCRIPTION: YAML configuration showing how to configure the SignalFx exporter for metrics with disabled aggregation at the gateway level. Includes separate configurations for agent and gateway metrics handling.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployment-modes.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n   hostmetrics:\n      collection_interval: 10s\n      scrapers:\n         cpu:\n         disk:\n         filesystem:\n         memory:\n         network:\n\nexporters:\n   otlphttp:\n      access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n      traces_endpoint: \"https://ingest.${SPLUNK_REALM}.signalfx.com/v2/trace/otlp\"\n   signalfx:\n      access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n      realm: \"${SPLUNK_REALM}\"\n      translation_rules: []\n      exclude_metrics: []\n   signalfx/internal:\n      access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n      realm: \"${SPLUNK_REALM}\"\n      sync_host_metadata: true\n\nservice:\n   extensions: [http_forwarder]\n   pipelines:\n      traces:\n         receivers: [otlp]\n         processors:\n         - memory_limiter\n         - batch\n         exporters: [otlphttp]\n      metrics:\n         receivers: [signalfx]\n         processors: [memory_limiter, batch]\n         exporters: [signalfx]\n      metrics/internal:\n         receivers: [prometheus/internal]\n         processors: [memory_limiter, batch, resourcedetection/internal]\n         exporters: [signalfx/internal]\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf OpenTelemetry Output Plugin\nDESCRIPTION: Configuration for the Telegraf OpenTelemetry Output plugin to send metrics over gRPC.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/telegraf.rst#2025-04-22_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n# Send OpenTelemetry metrics over gRPC\n[[outputs.opentelemetry]]\n```\n\n----------------------------------------\n\nTITLE: Adding Redis Monitor to Service Pipeline\nDESCRIPTION: Configuration snippet showing how to add the Redis monitor to the service pipelines metrics receivers section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/redis.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/redis]\n```\n\n----------------------------------------\n\nTITLE: Including Metrics Generation Processor in Service Pipeline\nDESCRIPTION: This configuration snippet demonstrates how to include the metricsgeneration processor in the metrics pipeline of the service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-generation-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      processors: [metricsgeneration]\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Payload for AppDynamics SaaS Integration\nDESCRIPTION: This JSON template creates a payload for the AppDynamics SaaS integration with Splunk On-Call. It includes conditional logic to set message types based on event types and structures the alert data for Splunk On-Call ingestion.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/appdynamics-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n#foreach(${eventList} in ${fullEventsByTypeMap.values()})\n\n   #foreach(${event} in ${eventList})\n\n      #if ($event.eventType == \"POLICY_OPEN_CRITICAL\")\n\n            #set ( $message_type = \"CRITICAL\" )\n\n      #elseif ($event.eventType == \"POLICY_UPGRADED\")\n\n            #set ( $message_type = \"CRITICAL\" )\n\n      #elseif ($event.eventType == \"ERROR\")\n\n            #set ( $message_type = \"CRITICAL\" )\n\n      #elseif ($event.eventType == \"APPLICATION_ERROR\")\n\n            #set ( $message_type = \"CRITICAL\" )\n\n      #elseif ($event.eventType == \"POLICY_CLOSE_WARNING\")\n\n            #set ( $message_type = \"RECOVERY\" )\n\n      #elseif ($event.eventType == \"POLICY_CLOSE_CRITICAL\")\n\n            #set ( $message_type = \"RECOVERY\" )\n\n      #elseif ($event.eventType == \"POLICY_CANCELED_CRITICAL\")\n\n            #set ( $message_type = \"RECOVERY\" )\n\n      #else\n\n            #set ( $message_type = \"WARNING\" )\n\n      #end\n\n   {\n\n      \"message_type\":\"${message_type}\",\n\n      #latestEvent.incident.id is the AppDynamics incident ID for the health rule. 1 incident can include multiple events.\n      \"entity_id\":\"${latestEvent.incident.id}\",\n      \n      #latestEvent.id is the AppDynamics event ID for the triggering HTTP action.\n      \"event_id\":\"${latestEvent.id}\",\n\n      \"state_message\":\"${event.eventMessage}\",\n\n      \"alert_url\":\"${event.deepLink}\",\n\n      \"ad_event_type\":\"${event.eventType}\",\n\n      \"monitoring_tool\":\"AppDynamics\"\n\n   }\n\n   #end\n\n#end\n```\n\n----------------------------------------\n\nTITLE: Adding Volley HTTP Dependency\nDESCRIPTION: Gradle configuration for adding the Splunk Volley HTTP instrumentation dependency.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/configure-rum-android-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\ndependencies {\n   //...\n   implementation(\"com.splunk:splunk-otel-android-volley:+\")\n   //...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Worker Nodes for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration sets up the Splunk OpenTelemetry Collector for Windows worker nodes in Kubernetes. It specifies the Windows image repository and adjusts probe timings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nisWindows: true\nimage:\n  otelcol:\n    repository: quay.io/signalfx/splunk-otel-collector-windows\nlogsEngine: otel\nreadinessProbe:\n  initialDelaySeconds: 60\nlivenessProbe:\n  initialDelaySeconds: 60\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: Sphinx/RST documentation structure defining the page hierarchy and metadata for Splunk platform reliability documentation\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/practice-reliability/splunkplatform-landing.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _splunkplatform-landing:\n\n***********************************************************************************\nPractice reliability with Splunk Observability Cloud and the Splunk platform\n***********************************************************************************\n\n.. meta::\n   :description: This page provides an overview of how reliability engineers can maintain reliability with Splunk Observability Cloud components.\n\n\n.. toctree::\n    :maxdepth: 3\n    :hidden:\n\n    Measure and alert on SLIs <slis>\n    Respond to incidents <incident-response>\n    Collaborate on observability <collaboration>\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose Services\nDESCRIPTION: Shell command to start Docker Compose services defined in the docker-compose.yml file, which launches the private runner container.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Attributes Environment Variable (Windows PowerShell)\nDESCRIPTION: Sets the OTEL_RESOURCE_ATTRIBUTES environment variable using Windows PowerShell. This optional variable allows specifying additional resource attributes like deployment environment and service version, which are attached to telemetry data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_RESOURCE_ATTRIBUTES='deployment.environment=<envtype>,service.version=<version>'\n```\n\n----------------------------------------\n\nTITLE: Sysdig and Splunk On-Call Integration RST Documentation\nDESCRIPTION: RST documentation that details the setup process for integrating Sysdig cloud monitoring with Splunk On-Call, including requirements and configuration steps for both platforms.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/sysdig-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _sysdig-spoc:\n\nSysdig integration for Splunk On-Call\n***************************************************\n\n.. meta::\n    :description: Configure the Sysdig integration for Splunk On-Call.\n\nSysdig cloud is the container-native monitoring solution, built for visibility, alerting, and troubleshooting of container and microservice environments. The following guide walks you through the necessary steps to integrate Sysdig with Splunk On-Call.\n\n\nRequirements\n==================\n\nThis integration is compatible with the following versions of Splunk On-Call:\n\n- Starter\n- Growth\n- Enterprise\n\n\nSplunk On-Call configuration\n====================================\n\nFrom the Splunk On-Call web portal select :guilabel:`Settings` then :guilabel:`Alert Behavior` then :guilabel:`Integrations`.\n\n.. image:: /_images/spoc/settings-alert-behavior-integrations-e1480978368974.png\n   :alt: Integrations menu\n\nSelect the :guilabel:`Sysdig` integration.\n\n.. image:: /_images/spoc/Integrations-victorops-9.png\n   :alt: Sysdig integration button\n\nCopy the service API key to your clipboard.\n\n.. image:: /_images/spoc/Integrations-victorops-10.png\n   :alt: API key for the Sysdig integration\n\n\nSysdig configuration\n====================================\n\nSelect the :guilabel:`Settings Menu`, then select :guilabel:`Notifications`. Select the plus next to :guilabel:`MY CHANNELS`. In the menu, select :guilabel:`VictorOps` (now Splunk On-Call).\n\n.. image:: /_images/spoc/Sysdig2.png\n   :alt: Select Splunk On-Call\n\nIn the following screen, paste your Splunk On-Call API key you previously copied to your clipboard, place in an appropriate routing key, give the channel a name, and then turn on :guilabel:`Resolve VictorOps incidents`.\n\nFinally, select :guilabel:`Create channel`.\n\n.. image:: /_images/spoc/Sysdig3.png\n   :alt: Create the channel\n```\n\n----------------------------------------\n\nTITLE: Inserting HTML Embed for Section Header\nDESCRIPTION: RST directive to embed HTML content, creating a section header with an anchor link for 'Send infrastructure data to Splunk Observability Cloud'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/compute/compute.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <embed>\n      <h2>Send infrastructure data to Splunk Observability Cloud<a name=\"compute-data\" class=\"headerlink\" href=\"#compute-data\" title=\"Permalink to this headline\">¶</a></h2>\n   </embed>\n```\n\n----------------------------------------\n\nTITLE: Creating Test Incident in Splunk On-Call\nDESCRIPTION: Splunk search command to send a test alert that creates an incident in Splunk On-Call timeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/splunk-integration-guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n| sendalert victorops param.message_type=\"INFO\"\n```\n\n----------------------------------------\n\nTITLE: Adding New Alert Field with Wildcard Match in Splunk On-Call Rules Engine\nDESCRIPTION: This rule adds a new field to alerts from a specific monitoring tool. It demonstrates how to create a new field and set its value.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-transformations.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nWhen monitoring_tool matches your_tool using Wildcard Match\n\nSet new_field_name to value of new field\n```\n\n----------------------------------------\n\nTITLE: Setting OTel Collector Endpoint in Windows\nDESCRIPTION: Configures the endpoint URL for the Splunk OpenTelemetry Collector in Windows PowerShell when it's running on a different host than your application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_6\n\nLANGUAGE: powershell\nCODE:\n```\n$env:OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\n----------------------------------------\n\nTITLE: Applying Terraform Changes to Splunk On-Call\nDESCRIPTION: This shell command executes the plan generated by 'terraform plan' (or generates one if not run previously) to create, update, or delete resources in Splunk On-Call via its API. It prompts the user for confirmation ('yes') before proceeding with the modifications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/manage-splunk-oncall-using-terraform.rst#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nterraform apply\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop JMX for ResourceManager in Collector\nDESCRIPTION: YAML configuration for the Hadoop JMX integration to collect metrics from a ResourceManager. Specifies the host, port, and nodeType for the ResourceManager JMX connection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoopjmx.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/collectd/hadoopjmx:\n    type: collectd/hadoopjmx\n    host: 127.0.0.1\n    port: 5680\n    nodeType: resourceManager\n```\n\n----------------------------------------\n\nTITLE: Including Network Explorer Information with Raw HTML Markers\nDESCRIPTION: Combines raw HTML div markers with an RST include directive to embed content from the network-explorer-info.rst file. The HTML markers are likely used for conditional processing or styling of the included content.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"troubleshooting-components.rst\"></div>\n\n.. include:: /_includes/network-explorer-info.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"troubleshooting-components.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Installing the Hubot VictorOps Adapter via npm (Bash)\nDESCRIPTION: Installs the 'hubot-victorops' adapter from the official GitHub repository using npm, ensuring that Hubot can connect to Splunk On-Call. Requires Node.js and npm pre-installed. This command fetches the adapter directly from the given Git URL and adds it to your node_modules.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/hubot-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install git://github.com/victorops/hubot-victorops.git\n```\n\n----------------------------------------\n\nTITLE: Adding Monitor to Service Pipeline\nDESCRIPTION: Configuration to add the Prometheus NGINX Ingress monitor to the metrics pipeline in the Collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-nginx-ingress.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/prometheus/nginx-ingress]\n```\n\n----------------------------------------\n\nTITLE: Setting collection_interval for Splunk Receiver\nDESCRIPTION: Sets the interval at which the receiver collects metrics, with a default value of 10 seconds. The value must be a string that can be parsed by Golang's time.ParseDuration function.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/collector-settings-collectioninterval.rst#2025-04-22_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ncollection_interval\n```\n\n----------------------------------------\n\nTITLE: OpenShift Deployment Definition for Splunk Synthetics Runner\nDESCRIPTION: YAML configuration for deploying Splunk Synthetics runners in an OpenShift cluster. The configuration includes container definitions and references to the runner token secret.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_30\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: splunk-o11y-synthetics-runner\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: splunk-o11y-synthetics-runner\n  template:\n    metadata:\n      labels:\n        app: splunk-o11y-synthetics-runner\n    spec:\n      containers:\n        - name: splunk-o11y-synthetics-runner\n          image: quay.io/signalfx/splunk-synthetics-runner:latest\n          imagePullPolicy: Always\n          env:\n            - name: RUNNER_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: runner-token-secret\n                  key: RUNNER_TOKEN\n          securityContext:\n```\n\n----------------------------------------\n\nTITLE: Clearing Explicit Screen Names in Android RUM\nDESCRIPTION: Shows how to clear explicit screen names when transitioning back to Activity-based views to ensure correct tracking in mixed-view applications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\n// doubled to clear both the last view and the previous last view\nSplunkRum.getInstance().experimentalSetScreenName(null)\nSplunkRum.getInstance().experimentalSetScreenName(null)\n```\n\n----------------------------------------\n\nTITLE: Disabling Logs in Kubernetes Helm Chart Configuration\nDESCRIPTION: Configuration for disabling logs when deploying the Collector in Kubernetes using the Helm chart. This should be added to the splunkObservability section of a custom chart or values.yaml file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/splunk-hec-exporter.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkObservability:\n  # Other settings\n  logsEnabled: false\n```\n\n----------------------------------------\n\nTITLE: Setting Service Name Environment Variable (Windows PowerShell)\nDESCRIPTION: Sets the OTEL_SERVICE_NAME environment variable using Windows PowerShell. This variable is required to identify the service in Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_SERVICE_NAME=<yourServiceName>\n```\n\n----------------------------------------\n\nTITLE: Analyzing Go OpenTelemetry Log for Connection Error (Text)\nDESCRIPTION: Example error log messages indicating a failure to connect to the target endpoint. Messages like `context deadline exceeded` and `transport: Error while dialing dial tcp: missing address` suggest potential issues with the endpoint being down, unreachable, or incorrectly configured in the exporter settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/common-go-troubleshooting.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n2022/03/02 20:29:29 context deadline exceeded\n2022/03/02 20:29:29 max retry time elapsed: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial tcp: missing address\"\n```\n\n----------------------------------------\n\nTITLE: Activating AlwaysOn Profiling in Python Code\nDESCRIPTION: Programmatically activates the AlwaysOn Profiling feature from within Python application code. This enables CPU profiling with optional configuration for service name, resource attributes, and endpoint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom splunk_otel.profiling import start_profiling\n\n# Activates CPU profiling\n# All arguments are optional\nstart_profiling(\n   service_name='my-python-service', \n   resource_attributes={\n      'service.version': '3.1'\n      'deployment.environment': 'production', \n   }\n   endpoint='http://localhost:4317'\n)\n```\n\n----------------------------------------\n\nTITLE: Finding Kubernetes collector pod name with kubectl\nDESCRIPTION: This command lists all pods in the current namespace to help identify the k8s-collector pod name needed for log retrieval.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-troubleshoot.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods\n```\n\n----------------------------------------\n\nTITLE: Starting Prometheus and AlertManager Configuration\nDESCRIPTION: The shell commands provided are used to start Prometheus and AlertManager. The first command launches Prometheus with a designated configuration file and points it to the AlertManager instance. The second command starts AlertManager using its configuration file. Dependencies include existing configuration files for Prometheus (`prometheus.yml`) and AlertManager (`alertmanager.yml`).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/prometheus-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./prometheus -config.file=prometheus.yml -alertmanager.url=http://localhost:9093\n```\n\nLANGUAGE: shell\nCODE:\n```\n./alertmanager -config.file=alertmanager.yml\n```\n\n----------------------------------------\n\nTITLE: Identifying OTLP Exporter Connection Errors in Logs\nDESCRIPTION: This log excerpt shows a typical error message indicating that the OTLP exporter failed to connect to the specified OpenTelemetry Collector endpoint (127.0.0.1:55681 in this case) due to a connection refusal (ECONNREFUSED). This points to issues with the collector's availability, configuration, or network connectivity.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/troubleshooting/common-nodejs-troubleshooting.rst#2025-04-22_snippet_2\n\nLANGUAGE: log\nCODE:\n```\n@opentelemetry/instrumentation-http http.ClientRequest return request\n{\"stack\":\"Error: connect ECONNREFUSED 127.0.0.1:55681\\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1148:16)\\n    at TCPConnectWrap.callbackTrampoline (internal/async_hooks.js:131:17)\",\"message\":\"connect ECONNREFUSED 127.0.0.1:55681\",\"errno\":\"-111\",\"code\":\"ECONNREFUSED\",\"syscall\":\"connect\",\"address\":\"127.0.0.1\",\"port\":\"55681\",\"name\":\"Error\"}\n```\n\n----------------------------------------\n\nTITLE: Regular Expression for Valid traceparent Header\nDESCRIPTION: Provides the regular expression pattern that a valid traceparent header value must match when generating Server-Timing headers for Splunk APM integration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/manual-rum-react-instrumentation.rst#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n00-([0-9a-f]{32})-([0-9a-f]{16})-01\n```\n\n----------------------------------------\n\nTITLE: YAML Monitor Configuration Settings\nDESCRIPTION: Common configuration parameters for Splunk monitors including discovery rules, dimensions, span tags, intervals, and metric transformations. These settings control monitor behavior, data collection, and output formatting.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/monitors-common-config.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntype: \"string\"              # The type of monitor\ndiscoveryRule: \"string\"      # Rule for matching configuration with endpoints\nvalidateDiscoveryRule: false  # Enable/disable discovery rule validation\nextraDimensions:             # Additional dimensions for datapoints\n  key: \"value\"\nextraSpanTags:               # Additional span tags\n  key: \"value\"\nintervalSeconds: 0           # Emission interval for datapoints\nsolo: false                  # Enable exclusive configuration execution\ndisableHostDimensions: false # Disable host-specific dimensions\n```\n\n----------------------------------------\n\nTITLE: Installing SignalFx PHP Tracing Library using Shell\nDESCRIPTION: Executes the downloaded `signalfx-setup.php` script using PHP to install the tracing library. The `--php-bin=all` option attempts to install it for all detected PHP installations; omitting it allows interactive selection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/instrumentation/instrument-php-application.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nphp signalfx-setup.php --php-bin=all\n```\n\n----------------------------------------\n\nTITLE: Conditional Equality Checks with eq - Handlebars\nDESCRIPTION: Demonstrates block-level equality checking via the eq helper for conditional rendering. Compares two values or variables and renders the 'yes' branch if equal or 'no' otherwise, using else blocks. Useful for branching logic when assembling alert payloads or formatting output.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/notif-services/splunkplatform.rst#2025-04-22_snippet_8\n\nLANGUAGE: Handlebars\nCODE:\n```\n``{{#eq a b}}yes{{else}}no{{/eq}}``\n``{{#eq a 2}}yes{{else}}no{{/eq}}``\n```\n\n----------------------------------------\n\nTITLE: Granting SELECT Permission for Specific Database\nDESCRIPTION: SQL command to grant SELECT permission to a user for a specific database, if separate DB names are defined for connection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/mysql.rst#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SELECT ON <db_name>.* TO '<user_name>'@'localhost';\n```\n\n----------------------------------------\n\nTITLE: Checking Kubernetes Container Logs with kubectl logs\nDESCRIPTION: Check logs in a Kubernetes container. This command allows viewing logs from specific pods, containers, or based on labels. It supports various options like streaming logs, showing logs from a specific time duration, or displaying a certain number of recent log lines.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nsudo kubectl logs <pod-name | type/name> -l <label> -f -c <container-name>\n\n# Examples\n\n# Return snapshot logs from pod nginx with only one container\nkubectl logs nginx \n\n# Return snapshot logs from pod nginx with multiple containers\nkubectl logs nginx --all-containers=true \n\n# Return snapshot logs from all containers in pods defined by label app=nginx\nkubectl logs -l app=nginx --all-containers=true \n\n# Return snapshot of previous terminated ruby container logs from pod web-1\nkubectl logs web-1 -p -c ruby \n\n# Begin streaming the logs of the ruby container in pod web-1\nkubectl logs web-1 -f -c ruby \n\n# Begin streaming the logs from all containers in pods defined by label app=nginx\nkubectl logs -f -l app=nginx --all-containers=true \n\n# Display only the most recent 20 lines of output in pod nginx\nkubectl logs nginx --tail=20\n\n# Show all logs from pod nginx written in the last hour\nkubectl logs nginx --since=1h \n\n# Show logs from a kubelet with an expired serving certificate\nkubectl logs nginx --insecure-skip-tls-verify-backend \n\n# Return snapshot logs from first container of a job named hello\nkubectl logs job/hello \n\n# Return snapshot logs from container nginx-1 of a deployment named nginx\nkubectl logs deployment/nginx -c nginx-1 \n```\n\n----------------------------------------\n\nTITLE: Creating Runner Token Secret in OpenShift\nDESCRIPTION: Shell command for creating an OpenShift secret to store the Splunk Synthetics runner authentication token.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\noc create secret generic runner-token-secret --from-literal=RUNNER_TOKEN=YOUR_TOKEN_HERE\n```\n\n----------------------------------------\n\nTITLE: Sending Alerts to Splunk On-Call with Default Routing Key\nDESCRIPTION: Uses TICKscript to send alerts to Splunk On-Call with the routing key as configured in the Kapacitor file. Assumes 'victorOps' integration is appropriately configured.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/kapacitor-integration-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: unknown\nCODE:\n```\nstream\n  |alert()\n    .victorOps()\n```\n\n----------------------------------------\n\nTITLE: Setting Deployment Environment and Service Version in Windows\nDESCRIPTION: Configures resource attributes for deployment environment and service version in Windows PowerShell, which help categorize and filter your application data in Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_8\n\nLANGUAGE: powershell\nCODE:\n```\n$env:OTEL_RESOURCE_ATTRIBUTES='deployment.environment=<envtype>,service.version=<version>'\n```\n\n----------------------------------------\n\nTITLE: RST Section Definition - Alert Admin\nDESCRIPTION: RST markup defining the document title and metadata for Alert admin documentation\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/user-roles/alert-admin.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _alert-admin:\n\n************************************************************************\nGet started as an Alert admin\n************************************************************************\n\n.. meta::\n   :description: About the alert admin  role in Splunk On-Call.\n```\n\n----------------------------------------\n\nTITLE: Manually Installing Auto Instrumentation Debian Package for .NET (Bash)\nDESCRIPTION: Installs or upgrades the `splunk-otel-auto-instrumentation` package using a downloaded Debian (`.deb`) file. Replace `<path to splunk-otel-auto-instrumentation deb>` with the actual file path. This command is used for manual upgrades on Debian-based systems in the context of .NET instrumentation. Requires `sudo` privileges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\nsudo dpkg -i <path to splunk-otel-auto-instrumentation deb>\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster Name in YAML\nDESCRIPTION: YAML configuration example showing how to set the Kubernetes cluster name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nclusterName: my-k8s-cluster\n```\n\n----------------------------------------\n\nTITLE: Example Log Entries for Unsupported .NET Version\nDESCRIPTION: These log entries illustrate the type of messages generated by the .NET instrumentation when the application's .NET version is not supported. Seeing these logs indicates a need to either update the .NET runtime or check compatibility requirements.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n[Information] Rule Engine: Error in StartupHook initialization: 6.0.36 is not supported \n[Error] Rule 'Minimum Supported Framework Version Validator' failed: Verifies that the application is running on a supported version of the .NET runtime. \n```\n\n----------------------------------------\n\nTITLE: Running Splunk OpenTelemetry Collector Binary with Help Option\nDESCRIPTION: Command to run the Splunk OpenTelemetry Collector binary file with the help option to display available command-line options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n$ <download dir>/otelcol_<platform>_<arch> --help\n```\n\n----------------------------------------\n\nTITLE: Adding resource/add_environment Processor to OpenTelemetry Collector Pipelines (YAML)\nDESCRIPTION: This snippet shows how to attach the `resource/add_environment` processor to different pipelines (metrics, logs, traces) in the `service` section of the Splunk Distribution of OpenTelemetry Collector config file. By including it in relevant pipelines, all spans, logs, or metrics passing through these pipelines will have the configured `deployment.environment` tag applied. This configuration is essential for enabling filtering and environment-specific analytics or alerts within Splunk APM. Prerequisite: The `resource/add_environment` processor must be defined elsewhere in the configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/set-up-apm/environments.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n    pipelines:\n        metrics:\n            processors: [resource/add_environment]\n        logs:\n            processors: [resource/add_environment]\n        traces:\n            processors: [resource/add_environment]\n```\n\n----------------------------------------\n\nTITLE: Toggling AlwaysOn Memory Profiling for SDKs (Shell)\nDESCRIPTION: Activates ('--enable-profiler-memory') or deactivates ('--disable-profiler-memory', default) AlwaysOn Memory Profiling for activated SDKs that support it via the 'SPLUNK_PROFILER_MEMORY_ENABLED' environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\n--[enable|disable]-profiler-memory\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n--disable-profiler-memory\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OTel Collector with Metrics in Linux\nDESCRIPTION: Bash command to install the Splunk OpenTelemetry Collector with runtime metrics collection enabled\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-advanced-config.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-instrumentation --deployment-environment prod \\\n--realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN> \\\n--enable-metrics\n```\n\n----------------------------------------\n\nTITLE: Setting Trace Response Headers in Linux Environment\nDESCRIPTION: Environment variable configuration to enable Splunk trace response headers for Linux systems. This setting allows the application to add traceparent information in Server-Timing headers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/advanced-nodejs-otel-configuration.rst#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nexport SPLUNK_TRACE_RESPONSE_HEADER_ENABLED=true\n```\n\n----------------------------------------\n\nTITLE: Verifying Fluentd uninstallation on Linux systems\nDESCRIPTION: Command to verify that Fluentd (td-agent) has been successfully uninstalled by checking its systemd service status. If uninstalled, the command should return that the service could not be found.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-uninstall.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl status td-agent\n```\n\n----------------------------------------\n\nTITLE: Modified AWS Integration Configuration with Status Metrics Enabled\nDESCRIPTION: This JSON snippet shows how to modify an AWS integration configuration to enable status check metrics. The highlighted line shows the addition of 'ignoreAllStatusMetrics' set to false, which enables collection of status metrics that are disabled by default.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-troubleshooting.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"authMethod\": \"ExternalId\",\n   \"customCloudWatchNamespaces\": null,\n   \"enableAwsUsage\": true,\n   \"enableCheckLargeVolume\": true,\n   \"enabled\": false,\n   \"externalId\": \"fyprhjmtpxttxwqhotep\",\n   \"id\": \"integration-id\",\n   \"ignoreAllStatusMetrics\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Alert Message Template in ConnectWise Automate\nDESCRIPTION: This code snippet defines the template for alert messages in ConnectWise Automate. It includes various placeholders for alert details such as name, status, client name, computer name, and more. This template is used for both success and failure alert messages.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/connectwise-automate-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nname:%NAME% status:%STATUS% clientName:%CLIENTNAME% computerName:%COMPUTERNAME% locationName:%LOCATIONNAME% fieldName:%FIELDNAME% result:%RESULT% failCount:%FailCount% when:%when% contactName:%ContactName%\n```\n\n----------------------------------------\n\nTITLE: CRD configuration for new users in Helm values\nDESCRIPTION: YAML configuration for new users to deploy Custom Resource Definitions (CRDs) via the crds/ directory when installing the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\noperatorcrds:\n  install: true\noperator:\n  enabled: true\n```\n\n----------------------------------------\n\nTITLE: Downloading Splunk OpenTelemetry Java Agent with Curl\nDESCRIPTION: This shell command uses curl to download the latest JAR file of the Splunk OpenTelemetry Java agent. It's a prerequisite for installing the Java agent to enable application monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/instrument-java-application.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -L https://github.com/signalfx/splunk-otel-java/releases/latest/download/splunk-otel-javaagent.jar \\\n         -o splunk-otel-javaagent.jar\n```\n\n----------------------------------------\n\nTITLE: RST Meta Description\nDESCRIPTION: ReStructuredText meta directive defining the document description for SEO purposes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/get-android-data-in.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. meta::\n   :description: Learn how to instrument your Android applications in Splunk Observability Cloud real user monitoring / RUM instrumentation, and what data you can collect.\n```\n\n----------------------------------------\n\nTITLE: Triggering a Critical Incident with cURL - Splunk On-Call REST Endpoint - Bash\nDESCRIPTION: Demonstrates using the cURL command-line tool to POST a JSON payload to the Splunk On-Call REST Endpoint for creating a critical incident. Requires cURL utility and the Splunk On-Call endpoint key and routing key. JSON data is supplied using the -d flag, and the endpoint URL must be populated with your integration-specific keys; the correct content-type header may be required in practice.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rest-endpoint-integration-guide.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST -d \\u2018{\\u201centity_id\\u201d:\\u201cID of the\\nincident\\u201d,\\u201cmessage_type\\u201d:\\u201ccritical\\u201d,\\u201cstate_message\\u201d:\\u201chi, this is some\\nstate message.\\u201d}'\\nhttps://alert.victorops.com/integrations/generic/20131114/alert/[YOUR_REST_ENDPOINT_KEY]/[ROUTING_KEY_HERE]\n```\n\n----------------------------------------\n\nTITLE: HTML Include Directive in RST\nDESCRIPTION: RST code for including external troubleshooting components documentation using HTML div tags for marking include sections.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/exec-input.rst#2025-04-22_snippet_3\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"troubleshooting-components.rst\"></div>\n\n.. include:: /_includes/troubleshooting-components.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"troubleshooting-components.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Adding CPUFreq Monitor to Metrics Pipeline\nDESCRIPTION: Configuration example showing how to add the CPUFreq monitor to the metrics pipeline in the service section of the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/cpufreq.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/collectd/cpufreq]\n```\n\n----------------------------------------\n\nTITLE: Default Kubernetes Log Fields for Splunk Related Content\nDESCRIPTION: Lists the standard fields automatically injected into Kubernetes logs by the Splunk Distribution of the OpenTelemetry Collector. These fields (`k8s.cluster.name`, `k8s.node.name`, etc.) are crucial for enabling Related Content and should not be modified.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/relatedcontent.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n- ``k8s.cluster.name``\n- ``k8s.node.name``\n- ``k8s.pod.name``\n- ``container.id``\n- ``k8s.namespace.name``\n- ``kubernetes.workload.name``\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Auto-Clear Condition Using SignalFlow - SignalFlow\nDESCRIPTION: This snippet shows a custom SignalFlow expression for clearing alerts based on specific inactivity and activity windows. The example uses the detect function combined with the when clause to specify that an alert triggers if the metric A is absent ('None') for 10 seconds but is not absent for 40 seconds. Dependencies include a configured detector in Splunk Observability Cloud that supports the SignalFlow language. The input is time series metric A, and the output is the triggering or clearing of alerts based on these timing constraints; limitations include the requirement for the metric to be present and the need to tune window sizes to match expected signal intervals.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/auto-clearing-alerts.rst#2025-04-22_snippet_0\n\nLANGUAGE: SignalFlow\nCODE:\n```\ndetect(when(A is None, '10s') and not when(A is None, '40s'))\n```\n\n----------------------------------------\n\nTITLE: Checking Kubernetes collector configuration in YAML\nDESCRIPTION: This YAML snippet shows a configuration where the Kubernetes collector is disabled, which would prevent Network Explorer from gathering Kubernetes metadata.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-troubleshoot.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nk8sCollector:\n  enabled: false\n```\n\n----------------------------------------\n\nTITLE: Installing React Native RUM Library with npm or yarn\nDESCRIPTION: Commands to install the @splunk/otel-react-native package using npm or yarn. For iOS applications, an additional pod installation command is required.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/install-rum-react.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# npm\nnpm install @splunk/otel-react-native\n\n# yarn\nyarn add @splunk/otel-react-native\n```\n\nLANGUAGE: bash\nCODE:\n```\n(cd ios && pod install)\n```\n\n----------------------------------------\n\nTITLE: Blocking Span Tags via APM Visibility Filter API\nDESCRIPTION: This JSON payload for the Splunk APM Visibility Filter API shows how to block specific span tags (such as \"user.email\" and \"credit.card.number\") for operations within a particular service. The API call targets sensitive data, hiding these tags across the APM UI and API responses for a specified duration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/set-up-apm/sensitive-data-controls.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"description\": \"Data blocked due to leak on 04/03/21\",\n    \"startTime\": \"2021-04-03T15:00:00.073876Z\",\n    \"matcher\": {\n    \n        \"sf_service\": \"checkoutService\",\n        \"sf_operation\": \"readCartDetails\"\n    },\n    \"hiddenTags\": [\"user.email\", \"credit.card.number\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Grouping Calls by Metric Dimension using SignalFlow - None\nDESCRIPTION: This SignalFlow example groups the number of spans by their 'sf_operation' dimension, counts them, and publishes the result with the label 'Count'. It requires access to the 'spans' metric and leverages SignalFlow's 'by' grouping feature. The output is a set of counts, one per 'sf_operation' value.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/grafana-create-queries.rst#2025-04-22_snippet_2\n\nLANGUAGE: none\nCODE:\n```\nA = histogram('spans').count(by=['sf_operation']).publish(label='Count')\n```\n\n----------------------------------------\n\nTITLE: Docker Image Verification Public Key (Legacy)\nDESCRIPTION: Legacy public key used to verify Docker images of the Collector for versions prior to 0.93.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/install-the-collector.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n-----BEGIN PUBLIC KEY-----\nMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAw+sL4Mx2Ip9AxTSp7Iw2\nk69tlJ8RqYNngJyecOLLiQkubgIQdnAkQurTfCPCuCHChvGGw3WCV617oJR25D0h\nNzOvS9wIXc1mEdsHCFbOuAVnJ7GLALmci6sR09jPiQnl2X58+edI/2g6j77G1Lz3\nB/aOK4p70Ro2TTE6Xj6XACeLkAZGu1W3UQfrJiYkGz4PovWMyeF2J88RcwrrdOLn\ni5iFeLR5EL8TtoQCXUyqJFpuXpBkLbMedrpZAODqBcg3iwfeACcguO2X1cCWFXM+\nubN1fzf2c+WrO3sg8io1cHTctX2GG+9r7DbqRuo0Ejj2D0fTi/JoVBCTXNxn2Drg\nL86Y5+mtpUN+MlnzZRFEbCqN2fC9CO1LlriD+3NKAuW7OVM10S/+eHApUQi1Ao5A\nABfjRxHWn2SISC5pmgYDeg90Lf0BTjX2+qn1HuJXDZUyD1XEeXedqE+/m9mgEU2s\nuYOqk6ecD/qowv2gvkwd742XvfpZhaMCdehtVJwB5HLAv4VtQQYLECgMrqipAALy\nbAExcAb0i16mMJi2QCPh44BrzcLQW/SZxYr9sg3IQXWBE84XbuzSyHJwBjvyxgf5\n2+TlQ3bUY73ssOe/WV3FAdDHh0ekQdOKO4plPPMXmdYCH2dY5ji5bunY+kKHayT7\npqX7nYPWHh4c2RvHkE3Tth8CAwEAAQ==\n-----END PUBLIC KEY-----\n```\n\n----------------------------------------\n\nTITLE: Traefik Service Pipeline Configuration in YAML\nDESCRIPTION: Configuration snippet showing how to add the Traefik monitor to the metrics pipeline in the service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/traefik.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/traefik]\n```\n\n----------------------------------------\n\nTITLE: Handling Promise Type Error in JavaScript\nDESCRIPTION: Example showing how Browser RUM agent handles type errors in promise chains when accessing properties of null values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-errors.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nnew Promise((resolve, reject) => {\n   resolve(null)\n}).then((val) => {\n   val.prop = 1\n})\n```\n\n----------------------------------------\n\nTITLE: Fetching AWS Instance Metadata with cURL\nDESCRIPTION: This code uses cURL to fetch metadata for an AWS instance, which helps in uniquely identifying the instance using 'instanceId', 'region', and 'accountId'. Ensure you have curl installed and reachable network access to the given INSTANCE_URL.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/monitor/aws.rst#2025-04-22_snippet_0\n\nLANGUAGE: none\nCODE:\n```\ncurl http://<INSTANCE_URL>/latest/dynamic/instance-identity/document\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Ballast with Percentage of Total Memory in YAML\nDESCRIPTION: This configuration example sets the memory ballast to use 20% of the total available memory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/memory-ballast-extension.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  memory_ballast:\n    size_in_percentage: 20\n```\n\n----------------------------------------\n\nTITLE: Checking PowerShell Version via Command Line\nDESCRIPTION: This command, executed in a PowerShell terminal, retrieves the currently installed version of PowerShell. This is useful for troubleshooting the PRTG integration, as the notification script requires a sufficiently recent version of PowerShell to function correctly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/prtg-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$PSVersionTable.PSVersion\n```\n\n----------------------------------------\n\nTITLE: Updating OpenTelemetry Collector Helm Repository\nDESCRIPTION: Command to update the Splunk OpenTelemetry Collector Helm chart repository.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo update https://signalfx.github.io/splunk-otel-collector-chart\n```\n\n----------------------------------------\n\nTITLE: Example Logs from .NET Console Application\nDESCRIPTION: This is an example of logs produced by a sample console application using Splunk Distribution of OpenTelemetry .NET. The logs are enriched with tracing metadata and show attributes such as trace ID, span ID, and different resource attributes. The example illustrates how logs are structured and what kind of metadata is included.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n2024-02-15 15:23:17 2024-02-15T13:23:17.704Z    info    ResourceLog #0\n2024-02-15 15:23:17 Resource SchemaURL: \n2024-02-15 15:23:17 Resource attributes:\n2024-02-15 15:23:17      -> splunk.distro.version: Str(1.4.0)\n2024-02-15 15:23:17      -> container.id: Str(c894cdb646a29616b5f713195cf810be898ca99c311cac8d9d25d8561dd6964b)\n2024-02-15 15:23:17      -> telemetry.distro.name: Str(splunk-otel-dotnet)\n2024-02-15 15:23:17      -> telemetry.distro.version: Str(1.4.0)\n2024-02-15 15:23:17      -> telemetry.sdk.name: Str(opentelemetry)\n2024-02-15 15:23:17      -> telemetry.sdk.language: Str(dotnet)\n2024-02-15 15:23:17      -> telemetry.sdk.version: Str(1.7.0)\n2024-02-15 15:23:17      -> service.name: Str(Example.LogTraceCorrelation.Console)\n2024-02-15 15:23:17      -> deployment.environment: Str(dev)\n2024-02-15 15:23:17      -> service.version: Str(1.0.0)\n2024-02-15 15:23:17 ScopeLogs #0\n2024-02-15 15:23:17 ScopeLogs SchemaURL: \n2024-02-15 15:23:17 InstrumentationScope  \n2024-02-15 15:23:17 LogRecord #0\n2024-02-15 15:23:17 ObservedTimestamp: 2024-02-15 13:23:13.1358363 +0000 UTC\n2024-02-15 15:23:17 Timestamp: 2024-02-15 13:23:13.1358363 +0000 UTC\n2024-02-15 15:23:17 SeverityText: Information\n2024-02-15 15:23:17 SeverityNumber: Info(9)\n2024-02-15 15:23:17 Body: Str(Hello from {activity})\n2024-02-15 15:23:17 Attributes:\n2024-02-15 15:23:17      -> activity: Str(LogWrappingActivity)\n2024-02-15 15:23:17 Trace ID: 17512c0247942df04fb30e6090eacb2c\n2024-02-15 15:23:17 Span ID: dc281b062178e72f\n2024-02-15 15:23:17 Flags: 1\n```\n\n----------------------------------------\n\nTITLE: Configuring NGINX for PHP-FPM Status Access\nDESCRIPTION: This NGINX configuration snippet allows access to the PHP-FPM status and ping endpoints. It sets up a location block to pass requests to the PHP-FPM socket and includes necessary FastCGI parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/php-fpm.rst#2025-04-22_snippet_0\n\nLANGUAGE: nginx\nCODE:\n```\nlocation ~ ^/(status|ping)$ {\n  access_log off;\n  fastcgi_pass unix:/run/php/php-fpm.sock;\n  include fastcgi_params;\n  fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Auto-Instrumentation on Debian Systems\nDESCRIPTION: Command to install the Splunk OpenTelemetry auto-instrumentation package on Debian-based systems using dpkg.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo dpkg -i <path to splunk-otel-auto-instrumentation deb>\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Documentation File - API Test Setup\nDESCRIPTION: ReStructuredText markup documenting the process of setting up API tests in Splunk Synthetic Monitoring, including configuration options, custom properties, and validation steps.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/api-test/set-up-api-test.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _set-up-api-test:\n\n**************************************\nSet up an API test\n**************************************\n\n.. meta::\n    :description: Steps to set up an API test in Splunk Synthetic Monitoring to check the functionality and performance of API endpoints.\n\nAn API test provides a flexible way to check the functionality and performance of API endpoints. See :ref:`api-test` for an overview of API tests in Splunk Synthetic Monitoring.\n```\n\n----------------------------------------\n\nTITLE: Setting GKE Autopilot Distribution in YAML\nDESCRIPTION: Configuration example for running the Collector in Google Kubernetes Engine Autopilot mode by setting the appropriate distribution value.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndistribution: gke/autopilot\n```\n\n----------------------------------------\n\nTITLE: Checking OpenTelemetry Instrumentation Configuration\nDESCRIPTION: Command to verify that the OpenTelemetry instrumentation is correctly set up in the collector namespace.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl get otelinst\n# NAME                          AGE   ENDPOINT\n# splunk-instrumentation        3m   http://$(SPLUNK_OTEL_AGENT):4317\n```\n\n----------------------------------------\n\nTITLE: Visualizing Splunk Infrastructure Monitoring Hierarchy using Mermaid Flowchart\nDESCRIPTION: A mermaid flowchart diagram showing the hierarchical relationship between different components of Splunk Infrastructure Monitoring. The flowchart illustrates how realms contain organizations, which further contain dashboard groups, navigators, detectors, and teams - all the way down to metrics, alerts, and notifications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/intro-to-infrastructure.rst#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#FFFFFF',\n      'primaryTextColor': '#000000',\n      'primaryBorderColor': '#000000',\n      'nodeBorder':'#000000',\n      'lineColor': '#000000',\n    }\n  }\n}%%\n\n\nflowchart TB\n  accTitle: Splunk Infrastructure Monitoring hierarchy\n  accDescr: In Splunk Infrastructure Monitoring, realm is the all-encompassing top level. A realm contains multiple organizations. Each organization contains dashboard groups, navigators, detectors, and teams. Teams contain users. Both dashboard groups and navigators contain dashboards. Dashboards contain charts. Charts and detectors use metrics to operate. Detectors can generate alerts and send notifications.\n\n    %% LR indicates the direction (left-to-right)\n\n    %% You can define classes to style nodes and other elements\n    classDef default fill:#FFFFFF, stroke:#000\n\n    subgraph Splunk Infrastructure Monitoring hierarchy\n    direction TB\n    realm[Realm]--contains-->org[Organizations]--contain-->dashboardGroup[Dashboard groups] & navigator[Navigators] & detector[Detectors] & teams[Teams]\n    dashboardGroup --contain-->dashboard[Dashboards]\n    navigator--contain-->dashboard--contain-->chart[Charts]\n    chart--use-->metric[Metrics]\n    detector--use-->metric\n    detector--generate-->alert[Alerts]\n    teams--contain-->users[Users]\n    alert--send-->notification[Notifications]\n    end\n```\n\n----------------------------------------\n\nTITLE: Setting Attributes on a Span in PHP\nDESCRIPTION: Shows how to add metadata (attributes) to an existing span using the `setAttribute` method. This example sets the code function name and file path using OpenTelemetry `TraceAttributes` constants.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-manual-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: php\nCODE:\n```\n$span->setAttribute(TraceAttributes::CODE_FUNCTION, 'rollOnce');\n$span->setAttribute(TraceAttributes::CODE_FILEPATH, __FILE__);\n```\n\n----------------------------------------\n\nTITLE: Creating UAA Client for Cloud Foundry Firehose Access\nDESCRIPTION: Command to create a Cloud Foundry User Account and Authentication (UAA) user with logs.admin permissions required to access the RLP Gateway. This creates a client named 'signalfx-nozzle' with appropriate authorization grants.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloudfoundry/cloudfoundry-firehose-nozzle.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n$ uaac client add my-v2-nozzle \\\n    --name signalfx-nozzle \\\n    --secret <signalfx-nozzle client secret> \\\n    --authorized_grant_types client_credentials,refresh_token \\\n    --authorities logs.admin\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Ruby Agent in Windows PowerShell\nDESCRIPTION: PowerShell commands for configuring essential environment variables for the Ruby agent in Windows, including service name, collector endpoint, and resource attributes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/instrument-ruby-application.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_SERVICE_NAME=<yourServiceName>\n```\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_RESOURCE_ATTRIBUTES='deployment.environment=<envtype>,service.version=<version>'\n```\n\n----------------------------------------\n\nTITLE: Handling Syntax Error in JavaScript\nDESCRIPTION: Example showing how Browser RUM agent handles syntax errors with invalid code.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-errors.rst#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nvar abc=;\n```\n\n----------------------------------------\n\nTITLE: Configuring HAProxy TCP Socket in YAML\nDESCRIPTION: Example YAML configuration for setting up a TCP socket for HAProxy stats. This shows how to configure the global section to use a TCP address and port for the stats socket.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/haproxy.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nglobal\n    daemon\n    stats socket localhost:9000\n    stats timeout 2m\n```\n\n----------------------------------------\n\nTITLE: Applying Helm Chart Updates in Kubernetes\nDESCRIPTION: Bash command to upgrade the Splunk OTel Collector chart with new configuration values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-advanced-config.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade splunk-otel-collector splunk-otel-collector-chart/splunk-otel-collector --version <CURRENT_VERSION> -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Example Assembly Not Found Error Message\nDESCRIPTION: This error message indicates that a required assembly, specified in the dependencies manifest for OpenTelemetry auto-instrumentation (`OpenTelemetry.AutoInstrumentation.AdditionalDeps.deps.json`), could not be located. Troubleshooting this typically involves enabling host tracing.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nAn assembly specified in the application dependencies manifest (OpenTelemetry.AutoInstrumentation.AdditionalDeps.deps.json) was not found\n```\n\n----------------------------------------\n\nTITLE: Basic Prometheus Node Receiver Configuration in YAML\nDESCRIPTION: Demonstrates how to activate the Prometheus Node integration by configuring the receiver in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-node.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/prometheus-node:\n    type: prometheus/node\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenTelemetry AutoInstrumentation on Linux for .NET\nDESCRIPTION: This snippet provides the configuration for setting up environment variables required for OpenTelemetry auto-instrumentation on Linux systems for .NET applications. It details paths for shared objects and plugins, requiring dependencies like glibc or musl for respective binaries.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/configuration/advanced-dotnet-configuration.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nEnvironment variable: CORECLR_ENABLE_PROFILING\nValue: 1\n\nEnvironment variable: CORECLR_PROFILER\nValue: {918728DD-259F-4A6A-AC2B-B85E1B658318}\n\nEnvironment variable: CORECLR_PROFILER_PATH\nValue: $INSTALL_DIR/linux-x64/OpenTelemetry.AutoInstrumentation.Native.so (glibc) |br| $INSTALL_DIR/linux-musl-x64/OpenTelemetry.AutoInstrumentation.Native.so (musl)\n\nEnvironment variable: DOTNET_ADDITIONAL_DEPS\nValue: $INSTALL_DIR\\AdditionalDeps\n\nEnvironment variable: DOTNET_SHARED_STORE\nValue: $INSTALL_DIR\\store\n\nEnvironment variable: DOTNET_STARTUP_HOOKS\nValue: $INSTALL_DIR\\net\\OpenTelemetry.AutoInstrumentation.StartupHook.dll\n\nEnvironment variable: OTEL_DOTNET_AUTO_HOME\nValue: $INSTALL_DIR\n\nEnvironment variable: OTEL_DOTNET_AUTO_PLUGINS\nValue: Splunk.OpenTelemetry.AutoInstrumentation.Plugin, Splunk.OpenTelemetry.AutoInstrumentation\n\nNote: The default installation path on Linux is $HOME/.otel-dotnet-auto.\n```\n\n----------------------------------------\n\nTITLE: Publishing Test Message for AWS CloudWatch Integration in JSON\nDESCRIPTION: This JSON payload is used to test the AWS CloudWatch integration with Splunk On-Call. It includes required fields such as AlarmName, NewStateValue, StateChangeTime, and AlarmDescription. The payload triggers a test alarm in Splunk On-Call.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/aws-cloudwatch-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"AlarmName\":\"VictorOps - CloudWatch Integration TEST\",\"NewStateValue\":\"ALARM\",\"NewStateReason\":\"failure\",\"StateChangeTime\":\"2017-12-14T01:00:00.000Z\",\"AlarmDescription\":\"VictorOps\\n   - CloudWatch Integration TEST\"}\n```\n\n----------------------------------------\n\nTITLE: Toggling Fluentd Installation and Configuration (Shell)\nDESCRIPTION: Controls whether to install and configure Fluentd ('td-agent') to forward log events to the Splunk OpenTelemetry Collector. Use '--with-fluentd' to enable or '--without-fluentd' to disable (default).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\n--with[out]-fluentd\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n--without-fluentd\n```\n\n----------------------------------------\n\nTITLE: MySQL Discovery Properties Configuration\nDESCRIPTION: YAML configuration for setting MySQL credentials in discovery properties file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-third-party.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nsplunk.discovery.receivers.mysql.config.username: \"<username>\"\nsplunk.discovery.receivers.mysql.config.password: \"<password>\"\n```\n\n----------------------------------------\n\nTITLE: Setting Trace Response Headers in Windows PowerShell\nDESCRIPTION: Environment variable configuration to enable Splunk trace response headers for Windows PowerShell. This setting allows the application to add traceparent information in Server-Timing headers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/advanced-nodejs-otel-configuration.rst#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n$env:SPLUNK_TRACE_RESPONSE_HEADER_ENABLED=true\n```\n\n----------------------------------------\n\nTITLE: Uninstalling only the Collector package on RPM-based Linux systems\nDESCRIPTION: Commands for uninstalling just the Splunk OpenTelemetry Collector package on RPM-based Linux systems using yum, dnf, or zypper package managers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-uninstall.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum remove splunk-otel-collector\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo dnf remove splunk-otel-collector\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo zypper remove splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Adding Android RUM Agent Dependency in Groovy\nDESCRIPTION: Groovy code snippet showing how to add the Splunk RUM agent and OpenTelemetry instrumentation dependencies to the application's build.gradle file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_4\n\nLANGUAGE: groovy\nCODE:\n```\ndependencies {\n//...\n   // Set the desired version of the RUM agent.\n   // See available releases: https://github.com/signalfx/splunk-otel-android/releases\n   implementation 'com.splunk:splunk-otel-android:+'\n   implementation 'io.opentelemetry.android:instrumentation:+'\n//...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authentication Header for Jira Outbound Webhook in Splunk On-Call\nDESCRIPTION: Shows the format for the `Authorization` header required when setting up an outbound webhook from Splunk On-Call to Jira. It uses Basic Authentication, requiring a Base64 encoded string of `Jira_USERNAME:Jira_API_TOKEN`. This header is added as a custom header in the Splunk On-Call outgoing webhook configuration under 'Integrations' > 'Outgoing Webhooks'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/jira-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nAuthorization: Basic\n<Encoded_String>\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Performance Counters Receiver - YAML\nDESCRIPTION: This YAML snippet shows how to enable and configure the windowsperfcounters receiver in the OpenTelemetry Collector. It defines a receiver under the \"receivers\" section, specifies metric definitions and collection interval, and lists the targeted performance counter object and counters. The configuration is referenced in the \"metrics\" pipeline section to activate metric collection. Dependencies: Splunk OpenTelemetry Collector, Windows host. Inputs: Counter objects, metric definitions. Outputs: Windows performance metrics collected at defined intervals.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/windowsperfcounters-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  windowsperfcounters:\n    metrics:\n      bytes.committed:\n        description: the number of bytes committed to memory\n        unit: By\n        gauge:\n    collection_interval: 30s\n    perfcounters:\n    - object: Memory\n      counters:\n        - name: Committed Bytes\n          metric: bytes.committed\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers:\n        - windowsperfcounters\n```\n\n----------------------------------------\n\nTITLE: Installing Browser RUM via NPM\nDESCRIPTION: Shell command for installing the Browser RUM agent npm package.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/install-rum-browser.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @splunk/otel-web --save\n```\n\n----------------------------------------\n\nTITLE: Restarting Splunk OpenTelemetry Collector Service to Apply Configuration Changes\nDESCRIPTION: These PowerShell commands stop and restart the Splunk OpenTelemetry Collector service to apply configuration changes made to the registry or configuration files. This is necessary after any configuration modifications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/windows-config.rst#2025-04-22_snippet_3\n\nLANGUAGE: PowerShell\nCODE:\n```\nStop-Service splunk-otel-collector\nStart-Service splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML Path Environment Variables for Linux and Windows\nDESCRIPTION: Default path configuration for the OpenTelemetry Collector configuration files on Linux and Windows systems. Specifies the location where the collector looks for either agent or gateway configuration files.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployment-modes.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Linux:\n/etc/otel/collector/<gateway or agent>_config.yaml\n\n# Windows:\nC:\\ProgramData\\Splunk\\OpenTelemetry Collector\\<gateway or agent>_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Sending Recovery Notification via JSON Webhook Payload - Panopta Integration - JSON\nDESCRIPTION: This JSON payload is intended for recovery (clear) notifications in Splunk On-Call, used within a second Panopta webhook. Dependencies include a configured Panopta integration, Splunk On-Call REST endpoint URL, and POST request method. It reuses the same field structure as the critical alert, but with message_type set to RECOVERY. The payload should be placed in the 'Clear Webhook' configuration and serves to signal a resolved or recovered incident to Splunk On-Call with the relevant entity details.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/panopta-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"message_type\":\"RECOVERY\",\"entity_id\":\"$name\", \"state_message\":\"$items - $reasons\",\"monitoring_tool\":\"Panopta\"}\n```\n\n----------------------------------------\n\nTITLE: Sending Metrics via REST API using curl\nDESCRIPTION: Example curl command demonstrating how to send a gauge metric to Splunk Observability Cloud. The request includes authentication via org token, metric name, dimensions, and value. The endpoint URL requires specification of the appropriate realm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/other-ingestion-methods/rest-APIs-for-datapoints.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST \\\n    --header \"Content-Type: application/json\" \\\n    --header \"X-SF-TOKEN: <ORG_TOKEN>\" \\\n    --data \\\n    '{\n        \"gauge\": [\n            {\n                \"metric\": \"memory.free\",\n                \"dimensions\": { \"host\": \"server1\" },\n                \"value\": 42\n            }\n        ]\n    }' \\\n    https://ingest.<REALM>.signalfx.com/v2/datapoint\n```\n\n----------------------------------------\n\nTITLE: Configuring Metro for React Native 0.67 and Lower\nDESCRIPTION: Metro configuration for React Native 0.67 and lower versions that forces Metro to use browser-specific packages needed by the OpenTelemetry instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/install-rum-react.rst#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst defaultResolver = require('metro-resolver');\nmodule.exports = {\nresolver: {\n   resolveRequest: (context, realModuleName, platform, moduleName) => {\n      const resolved = defaultResolver.resolve(\n      {\n         ...context,\n         resolveRequest: null,\n      },\n      moduleName,\n      platform,\n      );\n      if (\n      resolved.type === 'sourceFile' &&\n      resolved.filePath.includes('@opentelemetry')\n      ) {\n      resolved.filePath = resolved.filePath.replace(\n         'platform\\\\node',\n         'platform\\\\browser',\n      );\n      return resolved;\n      }\n      return resolved;\n   },\n},\ntransformer: {\n   getTransformOptions: async () => ({\n      transform: {\n      experimentalImportSupport: false,\n      inlineRequires: true,\n      },\n   }),\n},\n};\n```\n\n----------------------------------------\n\nTITLE: Checking Mutating Webhook Configuration in Kubernetes\nDESCRIPTION: Command to verify that the OpenTelemetry Operator's mutating webhook is correctly installed in the collector namespace.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl get mutatingwebhookconfiguration.admissionregistration.k8s.io\n# NAME                                      WEBHOOKS   AGE\n# splunk-otel-collector-opentelemetry-operator-mutation   3          14m\n```\n\n----------------------------------------\n\nTITLE: Listing Additional URLs to Allow for Splunk Observability Cloud (Shell)\nDESCRIPTION: Lists specific third-party URLs required for package downloads (Treasure Data, JFrog Artifactory, S3) used by Splunk Observability Cloud components like agents or collectors. These need to be allowed in addition to the primary service URLs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/allow-services.rst#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npackages.treasuredata.com\nsplunk.jfrog.io \njfrog-prod-use1-shared-virginia-main.s3.amazonaws.com\n```\n\n----------------------------------------\n\nTITLE: Downloading VictorOps Nagios Plugin (RPM)\nDESCRIPTION: Command to download the VictorOps Nagios plugin RPM package from GitHub.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/check_mk-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-nagios-1.4.20/victorops-nagios-1.4.20-1.noarch.rpm\n```\n\n----------------------------------------\n\nTITLE: Renaming Memory State Label Values\nDESCRIPTION: Renames specific memory state label values for the system.memory.usage metric.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: system.memory.usage\naction: update\noperations:\n    - action: update_label\n      label: state\n      value_actions:\n        - value: slab_reclaimable\n          new_value: sreclaimable\n        - value: slab_unreclaimable\n          new_value: sunreclaimable\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Splunk OTel Collector Attributes in Chef\nDESCRIPTION: Example Chef configuration showing how to set the required Splunk access token and realm attributes for the OpenTelemetry Collector. These are the minimum required settings to connect the Collector to Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/deployments-windows-chef.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{\n    \"splunk-otel-collector\": {\n        \"splunk_access_token\": \"<SPLUNK_ACCESS_TOKEN>\",\n        \"splunk_realm\": \"<SPLUNK_REALM>\",\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Routing Configuration in Icinga 2\nDESCRIPTION: Example configuration for routing alerts to specific teams using custom user objects and notification rules.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nobject User \"devops\" {\n import \"generic­user\"\n display_name = \"devops\"\n}\napply Notification \"notify­devops­service\" to Service {\n import \"notify­victorops­service\"\n users = [\"devops\"]\n assign where match(\"*load*\", service.name)\n}\napply Notification \"notify­devops­host\" to Service {\n import \"notify­victorops­host\"\n users = [\"devops\"]\n assign where match(\"*.production.myorg.com\", host.name)\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Access Tokens Starting with Hyphen (Shell)\nDESCRIPTION: Use the double hyphen '--' before the access token if the token itself begins with a hyphen '-'. This prevents the token from being misinterpreted as another command-line option.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n--\n```\n\nLANGUAGE: shell\nCODE:\n```\n-- -MY-ACCESS-TOKEN\n```\n\n----------------------------------------\n\nTITLE: Constructing Jira Issue Creation URL with Dynamic Fields in Splunk On-Call Rules Engine\nDESCRIPTION: This URL demonstrates how to construct a link within the Splunk On-Call Rules Engine ('Settings' > 'Alert Rules Engine') to pre-fill a Jira issue creation form via a direct HTML link. It uses Splunk On-Call variable expansion (`${{state_message}}`, `${{entity_id}}`, `${{labels}}`) to dynamically populate the Jira issue's description, summary, and labels based on the triggering alert data. This URL is added as a URL Annotation in an Alert Rule. Requires replacing `YOUR_DOMAIN_HERE` with the actual Jira domain and potentially adjusting `pid` (Project ID) and `issuetype` based on the specific Jira configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/jira-integration-guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: url\nCODE:\n```\nhttps://YOUR_DOMAIN_HERE.atlassian.net/secure/CreateIssueDetails!init.jspa?pid=10506&issuetype=1&description=${{state_message}}&summary=${{entity_id}}&labels=${{labels}}\n```\n\n----------------------------------------\n\nTITLE: Handling Axios HTTP Error in JavaScript\nDESCRIPTION: Example showing how Browser RUM agent handles HTTP errors in Axios requests with console error logging.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-errors.rst#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\naxios.get('/users').then(users => {\n   showUsers(users)\n}).catch(error => {\n   showErrorMessage()\n   console.error('error getting users', error)\n})\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector\nDESCRIPTION: YAML configuration for the OpenTelemetry Collector to receive metrics from Telegraf and export them to SignalFx.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/telegraf.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n   otlp:\n     protocols:\n       http:\n       grpc:\n   signalfx:\n\nexporters:\n   signalfx:\n      access_token: \"SPLUNK_TOKEN\"\n      realm: \"us0\"\n\nservice:\n  pipelines:\n      metrics:\n          receivers: [otlp]\n          exporters: [signalfx]\n      metrics/internal:\n          receivers: [signalfx]\n          processors:\n          exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Batch Processor in YAML\nDESCRIPTION: This snippet shows the default configuration for the batch processor in the Splunk Distribution of the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/batch-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  batch:\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Go Receiver\nDESCRIPTION: Basic configuration example showing how to activate the Prometheus Go integration by setting up the receiver configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-go.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/prometheus-go:\n    type: prometheus-go\n    host: localhost\n    port: 2112\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring VictorOps Contact Groups in Icinga\nDESCRIPTION: Sets up contact groups for routing alerts to specific teams in Splunk On-Call, using contactgroup_name as the routing key.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndefine contactgroup{\ncontactgroup_name         devops\nalias                     VictorOps DevOps contact group\nmembers                   VictorOps_devops\n}\n```\n\n----------------------------------------\n\nTITLE: Stopping Running Docker Container\nDESCRIPTION: Shell command to stop a running Docker container by referencing its ID or name, part of the manual upgrade process for private runners.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndocker stop <container_id_or_name>\n```\n\n----------------------------------------\n\nTITLE: Splunk Synthetic Monitoring Click Element with Navigation\nDESCRIPTION: Equivalent Splunk Synthetic Monitoring code for click event with navigation handling.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/browser-test/set-up-browser-test.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{\n// Splunk Synthetic Monitoring code snippet\n   \"name\": \"\",\n   \"type\": \"click_element\",\n   \"selector_type\": \"css\",\n   \"selector\": \"div:nth-of-type(2) > div:nth-of-type(2) a > div\",\n   \"wait_for_nav\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Formatting Splunk OnCall Service Email without Routing Key\nDESCRIPTION: Example of how to format the Splunk OnCall service email address without a team routing key for general alert routing.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/alertsite-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\ndb212e48-……8669@alert.victorops.com\n```\n\n----------------------------------------\n\nTITLE: Configuring Zabbix User Authentication for Ack-Back Feature\nDESCRIPTION: Shell configuration example for setting up Zabbix user credentials in the local.zabbix.conf file. This configuration is required to enable acknowledgment functionality from Splunk On-Call back to Zabbix.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zabbix-integration.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nZABBIX_USER=admin ZABBIX_PASSWD=fooblyboo\n```\n\n----------------------------------------\n\nTITLE: Visualizing Gateway Deployment Flow (Mermaid)\nDESCRIPTION: This Mermaid flowchart diagram illustrates the data flow in an example gateway deployment for Splunk AlwaysOn Profiling. It depicts the sequence: (1) Instrumentation agent sends data to (2) Collector in host (agent) mode, which forwards it to (3) Collector in data forwarding (gateway) mode, which finally sends data to (4) Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/profiling/get-data-in-profiling.rst#2025-04-22_snippet_7\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n\n   accTitle: Example gateway deployment diagram\n   accDescr: Step one. Point the instrumentation agent to the collector in host (agent) monitoring mode. Step two. Configure the collector in host (agent) monitoring mode. Step three. Configure the collector in data forwarding (gateway) mode. Step four. Send data to Splunk Observability Cloud.\n\ninstrumentation[\"`**(1)** Instrumentation agent`\"] --> collector[\"`**(2)** Collector in host (agent) monitoring mode`\"] --> datacollector[\"`**(3)** Collector in data forwarding (gateway) mode`\"] --> SOC[\"`**(4)** Splunk Observability Cloud`\"]\n```\n\n----------------------------------------\n\nTITLE: Creating New Metric Using Two Existing Metrics\nDESCRIPTION: This example shows how to create a new metric 'pod.cpu.utilized' by dividing 'pod.cpu.usage' by 'node.cpu.limit' using the calculate rule type.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-generation-processor.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nrules:\n    - name: pod.cpu.utilized\n      type: calculate\n      metric1: pod.cpu.usage\n      metric2: node.cpu.limit\n      operation: divide\n```\n\n----------------------------------------\n\nTITLE: Restarting Splunk OpenTelemetry Collector Service on Linux\nDESCRIPTION: This command restarts the Splunk OpenTelemetry Collector service after configuration changes. It uses systemctl to manage the service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-config.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl restart splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Listing Pod Names for a Specific Node\nDESCRIPTION: Retrieves a list of pod names running on a specific node to help with selecting a pod for further investigation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-container.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl get --raw \"/api/v1/nodes/\"${NODE_NAME}\"/proxy/stats/summary\" | jq '.pods[].podRef.name'\n```\n\n----------------------------------------\n\nTITLE: WebSphere Traditional Java Agent Configuration\nDESCRIPTION: Use the WebSphere Admin Console to specify the Splunk Java agent path in the Generic JVM arguments field, ensuring the agent is active.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-servers-instructions.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n-javaagent:/path/to/splunk-otel-javaagent.jar\n```\n\n----------------------------------------\n\nTITLE: Starting OpenTelemetry Collector with SPLUNK_CONFIG\nDESCRIPTION: Alternative command to start the collector using SPLUNK_CONFIG environment variable instead of the --config flag.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nSPLUNK_CONFIG=<path to config file> SPLUNK_REALM=<realm> SPLUNK_ACCESS_TOKEN=<token> <download dir>/otelcol_<platform>_<arch>\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Twilio Functions\nDESCRIPTION: Required environment variables for basic Twilio Functions setup including API keys and configuration parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/twilio-live-call-routing-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nEAM_1\nESC_POL_1\nNUMBER_OF_MENUS\nVICTOROPS_API_ID\nVICTOROPS_API_KEY\nVICTOROPS_TWILIO_SERVICE_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Pipeline\nDESCRIPTION: Configuration to add the Zookeeper monitor to the metrics pipeline, enabling metric collection and processing.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/apache-zookeeper.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n     pipelines:\n       metrics:\n         receivers: [smartagent/zookeeper]\n```\n\n----------------------------------------\n\nTITLE: Disabling Splunk Java Agent Console Logging (Bash)\nDESCRIPTION: Shows the command-line argument needed to run the Splunk Java agent in silent mode, preventing it from outputting logs to the console. Add '-Dotel.javaagent.logging=none' as a Java system property when starting the application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/common-java-troubleshooting.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n-Dotel.javaagent.logging=none\n```\n\n----------------------------------------\n\nTITLE: Validating and Listing Certificates via kubectl - Bash\nDESCRIPTION: This command lists existing Certificate resources in the Kubernetes cluster to verify their availability and readiness using kubectl get certificates. The output includes columns such as NAME, READY status, SECRET, and AGE, which help diagnose if the certificates required for the operator are properly created and in use. Dependencies include CRDs for cert-manager installed in the cluster and kubectl access. Ensure you have permissions to read Certificate resources.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/troubleshoot-zeroconfig-k8s.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get certificates\n# NAME                                          READY   SECRET                                                           AGE\n# splunk-otel-collector-operator-serving-cert   True    splunk-otel-collector-operator-controller-manager-service-cert   5m\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector with Helm\nDESCRIPTION: Command to install the Splunk OpenTelemetry Collector using Helm with a custom values.yaml configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-java-traces-tutorial/deploy-collector-k8s-java.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm install splunk-otel-collector -f ./values.yaml splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Specifying Basic Authentication in Webhook URL (API Key)\nDESCRIPTION: Shows an alternative method for basic authentication in the webhook URL using a username and an API key instead of a password.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/admin/get-started/custom-outbound-webhooks.rst#2025-04-22_snippet_1\n\nLANGUAGE: http\nCODE:\n```\nhttp://username:api_key@example.com/\n```\n\n----------------------------------------\n\nTITLE: Activating Hadoop JMX Integration in Collector Configuration\nDESCRIPTION: YAML configuration to activate the Hadoop JMX integration in the Splunk Distribution of OpenTelemetry Collector. This adds the smartagent/collectd/hadoopjmx receiver and includes it in the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoopjmx.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/collectd/hadoopjmx:\n    type: collectd/hadoopjmx\n    ... # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      monitors: [smartagent/collectd/hadoopjmx]\n```\n\n----------------------------------------\n\nTITLE: Setting Cluster Name with Helm Command\nDESCRIPTION: Command line example for specifying the Kubernetes cluster name when installing the Collector with Helm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n--set clusterName=my-k8s-cluster\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Response for Extended Stats Aggregation\nDESCRIPTION: This JSON snippet shows the response from Elasticsearch for the extended stats aggregation, containing buckets for different hosts with various statistical metrics for CPU usage.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/elasticsearch-query.rst#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"aggregations\" : {\n\"host\" : {\n    \"doc_count_error_upper_bound\" : 0,\n    \"sum_other_doc_count\" : 0,\n    \"buckets\" : [\n    {\n        \"key\" : \"helsinki\",\n        \"doc_count\" : 13996,\n        \"cpu_usage_stats\" : {\n        \"count\" : 13996,\n        \"min\" : 0.0,\n        \"max\" : 100.0,\n        \"avg\" : 49.86660474421263,\n        \"sum\" : 697933.0\n        }\n    },\n    {\n        \"key\" : \"lisbon\",\n        \"doc_count\" : 13996,\n        \"cpu_usage_stats\" : {\n        \"count\" : 13996,\n        \"min\" : 0.0,\n        \"max\" : 100.0,\n        \"avg\" : 49.88225207202058,\n        \"sum\" : 698152.0\n        }\n    },\n    {\n        \"key\" : \"madrid\",\n        \"doc_count\" : 13996,\n        \"cpu_usage_stats\" : {\n        \"count\" : 13996,\n        \"min\" : 0.0,\n        \"max\" : 100.0,\n        \"avg\" : 49.92469276936267,\n        \"sum\" : 698746.0\n        }\n    },\n    {\n        \"key\" : \"nairobi\",\n        \"doc_count\" : 13996,\n        \"cpu_usage_stats\" : {\n        \"count\" : 13996,\n        \"min\" : 0.0,\n        \"max\" : 100.0,\n        \"avg\" : 49.98320948842527,\n        \"sum\" : 699565.0\n        }\n    }\n    ]\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Splunk On-Call Email Contact Configuration\nDESCRIPTION: Configuration for sending Nagios alerts to Splunk On-Call through email, including organization ID and key settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ndefine contact{ \ncontact_name VictorOps_Email \n_VO_ORGANIZATION_ID xxxxxxxxxxxxx\n_VO_ORGANIZATION_KEY xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n_VO_MONITOR_NAME\nalias    VictorOps_Email\nservice_notification_period    24x7\nhost_notification_period    24x7\nservice_notification_options    w,u,c,r\nhost_notification_options    d,r\nservice_notification_commands    notify-victorops-by-email\nhost_notification_commands    notify-victorops-by-email\nregister    1\n_VO_ALERT_DOMAIN    alert.victorops.com\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Optional AWS Regions for Splunk Observability Cloud\nDESCRIPTION: This snippet lists the optional AWS regions supported by Splunk Observability Cloud, including their region codes and corresponding geographical locations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-prereqs.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``af-south-1``: Africa (Cape Town)\n* ``ap-east-1``: Asia Pacific (Hong Kong)\n* ``ap-south-2``: Asia Pacific (Hyderabad)\n* ``ap-southeast-3``: Asia Pacific (Jakarta)\n* ``ap-southeast-4``: Asia Pacific (Melbourne)\n* ``eu-central-2``: Europe (Zurich)\n* ``eu-south-1``: Europe (Milan)\n* ``eu-south-2``: Europe (Spain)\n* ``me-central-1``: Middle East (UAE)\n* ``me-south-1``: Middle East (Bahrain)\n```\n\n----------------------------------------\n\nTITLE: Converting customStep to run_javascript in Synthetic Monitoring\nDESCRIPTION: Comparison of a Google Chrome Recorder customStep with its equivalent Splunk Synthetic Monitoring run_javascript implementation. Used for executing custom JavaScript code during test execution.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/browser-test/set-up-browser-test.rst#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n{\n// Google Chrome Recorder\n   \"type\": \"customStep\",\n   \"timeout\": 5000,\n   \"target\": \"main\",\n   \"name\": \"customParam\",\n   \"parameters\": {}\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\n{\n// Splunk Synthetic Monitoring code snippet\n   \"name\": \"Unsupported step customStep\",\n   \"type\": \"run_javascript\",\n   \"value\": \"\",\n   \"wait_for_nav\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Discovery Properties Example\nDESCRIPTION: Command demonstrating how to set PostgreSQL username configuration for discovery mode.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-third-party.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n--set splunk.discovery.receivers.smartagent/postgresql.config.params::username='${PG_USERNAME_ENVVAR}'\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace\nDESCRIPTION: Command to create a new Kubernetes namespace called 'petclinic' to isolate the application resources.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-java-traces-tutorial/config-k8s-for-java.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create namespace petclinic\n```\n\n----------------------------------------\n\nTITLE: Monitoring REST API Call Rate in Splunk\nDESCRIPTION: This metric (`sf.org.numRestCalls`) tracks the number of REST API calls made per endpoint per minute (e.g., to api.signalfx.com). It helps monitor usage against the maximum API call limit to avoid HTTP 429 errors.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.numRestCalls\n```\n\n----------------------------------------\n\nTITLE: Measuring Splunk RUM Custom Event Latency (P75)\nDESCRIPTION: The metric 'rum.workflow.time.ns.p75' measures the 75th percentile latency of custom events in nanoseconds within Splunk RUM. This metric provides insight into the performance of custom workflows for both browser and mobile applications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/RUM-metrics.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nrum.workflow.time.ns.p75\n```\n\n----------------------------------------\n\nTITLE: Server Timing Headers for APM Integration\nDESCRIPTION: Example of HTTP response headers added by application instrumentation to enable linking between Synthetic and APM spans. These headers contain trace and span IDs in traceparent format.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/set-up-synthetics/set-up-synthetics.rst#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nAccess-Control-Expose-Headers: Server-Timing\nServer-Timing: traceparent;desc=\"00-<serverTraceId>-<serverSpanId>-01\"\n```\n\n----------------------------------------\n\nTITLE: Installing Zenoss DEB Package\nDESCRIPTION: Commands to download and install the Zenoss plugin using DEB package manager on Debian-based systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zenoss-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-zenoss-0.22.44/victorops-zenoss_0.22.44_all.deb\n```\n\nLANGUAGE: shell\nCODE:\n```\ndpkg -i <path_to_file>\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt install <path_to_file>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Process Environment Variables in Windows PowerShell\nDESCRIPTION: This PowerShell command retrieves the environment variables associated with a running process identified by its process ID (`<pid>`). It's used during troubleshooting to verify the OpenTelemetry configuration settings applied to the instrumented .NET application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n[System.Diagnostics.Process]::GetProcessById(<pid>).StartInfo.EnvironmentVariables\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Applications to Use TLS 1.2 (JVM Argument)\nDESCRIPTION: Specifies a Java Virtual Machine (JVM) system property to enforce the use of TLS 1.2 for all HTTPS connections made by the Java application. This argument is passed when launching the Java application (e.g., `java -Dhttps.protocols=TLSv1.2 ...`). Note that most Java systems using JDK 8 or later default to TLS 1.2.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/admin/get-started/tls-security-protocol.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n-Dhttps.protocols=TLSv1.2\n```\n\n----------------------------------------\n\nTITLE: Resolved Trace Context Data in Shell\nDESCRIPTION: This snippet shows the resolved trace context data from the Server-Timing header. It includes the version, trace-id, parent-id, and trace-flags.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nversion=00 trace-id=4bf92f3577b34da6a3ce929d0e0e4736\nparent-id=00f067aa0ba902b7 trace-flags=01\n```\n\n----------------------------------------\n\nTITLE: Activating Kafka Producer Monitor in Collector Configuration (YAML)\nDESCRIPTION: This snippet shows how to activate the Kafka producer monitor in the Collector configuration file. It includes adding the receiver and specifying it in the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/kafka-producer.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/collectd/kafka_producer:\n    type: collectd/kafka_producer\n    ... # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/ collectd/kafka_producer]\n```\n\n----------------------------------------\n\nTITLE: Configuring Couchbase Receiver in OpenTelemetry Collector YAML\nDESCRIPTION: This YAML snippet shows how to configure the Couchbase receiver in the OpenTelemetry Collector. It sets up the smartagent/couchbase receiver with the collectd/couchbase type.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/couchbase.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/couchbase:\n    type: collectd/couchbase\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Checking Cert Manager-Related Logs via kubectl - Bash\nDESCRIPTION: These commands retrieve logs from different components of the cert manager in a Kubernetes deployment using specific app labels with kubectl logs. The commands should be run in an environment where kubectl is configured for the cluster. Each command targets a different cert manager pod: certmanager, cainjector, and webhook, aiding in diagnosing issues with certificate management. Outputs are the logs for each specified component, helping troubleshoot certificate-related issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/troubleshoot-zeroconfig-k8s.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs -l app=certmanager\nkubectl logs -l app=cainjector\nkubectl logs -l app=webhook\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Receiver in Collector\nDESCRIPTION: Basic configuration to activate the Redis integration in the Collector by defining the receiver and adding it to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/redis.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/redis:\n    type: collectd/redis\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Adding Rails Instrumentation to Ruby Gemfile\nDESCRIPTION: This snippet demonstrates how to add the Rails instrumentation library to a Ruby project's Gemfile, specifying the gem name and version constraint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/ruby-manual-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: ruby\nCODE:\n```\ngem \"opentelemetry-instrumentation-rails\", \"~> 0.27\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Past Relative Time Range in Splunk\nDESCRIPTION: Enter a relative time range unit (m, h, d, w) preceded by a minus (-) to view data from a specific time window up to the present. This example shows how to view data from the last 5 minutes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/use-time-range-selector.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-5m\n```\n\n----------------------------------------\n\nTITLE: RST List Table Definition for Log Observer Connect Search Query Limits\nDESCRIPTION: RST markup that creates a table listing the search query limits for Log Observer Connect, including the maximum number of saved search queries and maximum number of logs processed for fields summary.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/logs/lo-connect-limits.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 50, 50\n\n   * - :strong:`Limit name`\n     - :strong:`Default limit value`\n\n   * - Maximum number of saved search queries\n     - 1,000\n\n   * - Maximum number of logs processed for fields summary\n     - 150,000\n```\n\n----------------------------------------\n\nTITLE: Retrieving k8s-relay container logs with kubectl\nDESCRIPTION: This command retrieves logs from the k8s-relay container within the k8s-collector pod to troubleshoot Kubernetes metadata issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-troubleshoot.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs <POD_NAME> -c k8s-relay\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata Table for Elastic Beanstalk in reStructuredText\nDESCRIPTION: This code snippet defines a table in reStructuredText format that lists the metadata properties imported by Infrastructure Monitoring for AWS Elastic Beanstalk. It includes the original Elastic Beanstalk property names, custom property names used in Infrastructure Monitoring, and descriptions of each property.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/monitor/aws-infra-metadata.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 30 30 60\n   :width: 100%\n\n   *  -  :strong:`Elastic Beanstalk Name`\n      -  :strong:`Custom Property`\n      -  :strong:`Description`\n\n   *  -  ApplicationName\n      -  aws_application_name\n      -  The name of the application associated with this environment\n\n   *  -  SolutionStackName\n      -  aws_solution_stack_name\n      -  The name of the SolutionStack deployed with this environment\n\n   *  -  TemplateName\n      -  aws_template_name\n      -  The name of the configuration template used to originally launch this environment\n\n   *  -  Status\n      -  aws_status\n      -  The current operational status of the environment. \n\n         Possible values are:\n         \n         * Aborting: The environment is aborting a deployment\n         * Launching: The environment is in the process of initial deployment\n         * LinkingFrom: The environment is linked to by another environment. See Environment links for details\n         * LinkingTo: The environment is in the process of linking to another environment. See Environment links for details\n         * Updating: The environment is updating its configuration settings or application version\n         * Ready: The environment is available to have an action performed on it, such as update or terminate\n         * Terminating: The environment is shutting down\n         * Terminated: The environment is not running\n\n   *  -  VersionLabel\n      -  aws_version_label\n      -  The application version deployed in this environment\n```\n\n----------------------------------------\n\nTITLE: Configuring Auto-detection for Prometheus and Istio\nDESCRIPTION: Configuration for enabling automatic detection of Prometheus and Istio telemetry sources.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-add.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkObservability:\n  accessToken: xxxxxx\n  realm: us0\nclusterName: my-k8s-cluster\nautodetect:\n  istio: true\n  prometheus: true\n```\n\n----------------------------------------\n\nTITLE: Custom Alert Message in Splunk Using Helper Functions\nDESCRIPTION: This snippet offers an example of an alert message constructed with conditional helper functions and variables within Splunk. It is designed to format messages based on alert states and trigger conditions, pulling in rule and detector names, timestamps, signal values, dimensions, runbook URLs, and tips. Assumed dependencies would be an activated Splunk environment using alert messaging. Inputs involve alert-related data and helper function syntaxes, while the output results in a formatted alert message ready for notification delivery.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/alert-message-variables-reference.rst#2025-04-22_snippet_1\n\nLANGUAGE: none\nCODE:\n```\n{{#if anomalous}}\n   Rule \"{{ruleName}}\" in detector \"{{detectorName}}\" triggered at {{timestamp}}.\n{{else}}\n   Rule \"{{ruleName}}\" in detector \"{{detectorName}}\" cleared at {{timestamp}}.\n{{/if}}\n\n{{#if anomalous}}\nTriggering condition: {{{readableRule}}}\n{{/if}}\n\n{{#if anomalous}}Signal value: {{inputs.A.value}}\n{{else}}Current signal value: {{inputs.A.value}}\n{{/if}}\n\n{{#notEmpty dimensions}}\nSignal details:\n{{{dimensions}}}\n{{/notEmpty}}\n\n{{#if anomalous}}\n{{#if runbookUrl}}Runbook: {{{runbookUrl}}}{{/if}}\n{{#if tip}}Tip: {{{tip}}}{{/if}}\n{{/if}}\n```\n\n----------------------------------------\n\nTITLE: Setting Service Name Environment Variable in Linux\nDESCRIPTION: Sets the OTEL_SERVICE_NAME environment variable in Linux, which is required to identify your application in Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_SERVICE_NAME=<yourServiceName>\n```\n\n----------------------------------------\n\nTITLE: Smart Agent Default Configuration Example\nDESCRIPTION: Example YAML configuration file for the SignalFx Smart Agent, showing default values and monitor configurations that will need to be migrated to the OpenTelemetry Collector format.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsignalFxAccessToken: {\"#from\": \"env:SIGNALFX_ACCESS_TOKEN\"}\ningestUrl: https://ingest.us1.signalfx.com\napiUrl: https://api.us1.signalfx.com\n\nbundleDir: /opt/my-smart-agent-bundle\n\nprocPath: /my_custom_proc\netcPath: /my_custom_etc\nvarPath: /my_custom_var\nrunPath: /my_custom_run\nsysPath: /my_custom_sys\n\nobservers:\n   - type: k8s-api\n\ncollectd:\n   readThreads: 10\n   writeQueueLimitHigh: 1000000\n   writeQueueLimitLow: 600000\nconfigDir: \"/tmp/signalfx-agent/collectd\"\n\nmonitors:\n   - type: collectd/activemq\n      discoveryRule: container_image =~ \"activemq\" && private_port == 1099\n      extraDimensions:\n         my_dimension: my_dimension_value\n   - type: collectd/apache\n      discoveryRule: container_image =~ \"apache\" && private_port == 80\n   - type: postgresql\n      discoveryRule: container_image =~ \"postgresql\" && private_port == 7199\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Payload for Closed/Resolved Case in Desk.com Integration\nDESCRIPTION: This JSON snippet defines the payload structure for updating a closed or resolved case in the Desk.com integration. It includes case details and sets the message type to 'RECOVERY' to indicate case resolution.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/desk-com-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"entity_id\":\"{{case.id}}\", \"message_type\":\"RECOVERY\", \"state_message\":\"New Case: {{case.id}} about {{case.subject}}\", \"Case Description\":\"{{case.description}}\", \"Case Priority\":\"{{case.priority}}\", \"Customer\":\"{{case.customer}}\", \"Case Email\":\"{{case.emails}}\", \"alert_url\":\"{{case.direct_url}}\" }\n```\n\n----------------------------------------\n\nTITLE: Restarting Collector's gateway deployment\nDESCRIPTION: Command to restart the OpenTelemetry Collector's gateway deployment after updating the access token (when gateway.enabled=true).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nkubectl rollout restart deployment <Release_Name>\n```\n\n----------------------------------------\n\nTITLE: Including Troubleshooting Components Documentation in RST\nDESCRIPTION: Includes a reStructuredText file containing troubleshooting components documentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/http.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /_includes/troubleshooting-components.rst\n```\n\n----------------------------------------\n\nTITLE: Jenkins Configuration with Enhanced Metrics Selection\nDESCRIPTION: Configuration example showing how to include specific enhanced metrics for monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/jenkins.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/jenkins:\n    type: collectd/jenkins\n    host: 127.0.0.1\n    port: 8080\n    metricsKey: reallylongmetricskey\n    includeMetrics:\n    - \"vm.daemon.count\"\n    - \"vm.terminated.count\"\n```\n\n----------------------------------------\n\nTITLE: Installing Twilio Function Dependencies\nDESCRIPTION: Required Node.js module dependencies and their versions for the Twilio Function implementation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/twilio-live-call-routing-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nxmldom: 0.1.27\nlodash: 4.17.10\nfs: 0.0.1-security\ntwilio: 3.6.3\ngot: 9.6.0\nutil: 0.11.0\n```\n\n----------------------------------------\n\nTITLE: Visualizing Default Metrics Pipeline with Mermaid Diagram\nDESCRIPTION: A flowchart showing how metrics data flows through the system from different receivers through processors to exporters. The diagram includes both standard and internal metrics pipelines.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/collector-config-ootb.rst#2025-04-22_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n\n   accTitle: Default metric pipeline diagram\n   accDescr: Receivers send logs to the metrics/memory_limiter processor. The metrics/memory_limiter processor sends metrics to the batch processor, and the batch processor sends metrics to the resource detection processor. The resource detection processor sends metrics to the exporter. The internal metrics pipeline follows the same steps, but uses internal receivers, processors, and exporters to send metrics. \n\n   %% LR indicates the direction (left-to-right)\n\n   %% You can define classes to style nodes and other elements\n   classDef receiver fill:#00FF00\n   classDef processor fill:#FF9900\n   classDef exporter fill:#FF33FF\n\n   %% Each subgraph determines what's in each category\n   subgraph Receivers\n      direction LR\n      metrics/hostmetrics:::receiver\n      metrics/otlp:::receiver\n      metrics/signalfx/in:::receiver\n      metrics/internal/prometheus/internal:::receiver\n   end\n\n   subgraph Processor\n      direction LR\n      metrics/memory_limiter:::processor --> metrics/batch:::processor --> metrics/resourcedetection:::processor\n      metrics/internal/memory_limiter:::processor --> metrics/internal/batch:::processor --> metrics/internal/resourcedetection:::processor\n   end\n\n   subgraph Exporters\n      direction LR\n      metrics/signalfx/out:::exporter\n      metrics/internal/signalfx/out:::exporter\n   end\n\n   %% Connections beyond categories are added later\n   metrics/hostmetrics --> metrics/memory_limiter\n   metrics/resourcedetection --> metrics/signalfx/out\n   metrics/otlp --> metrics/memory_limiter\n   metrics/signalfx/in --> metrics/memory_limiter\n   metrics/internal/prometheus/internal --> metrics/internal/memory_limiter\n   metrics/internal/resourcedetection --> metrics/internal/signalfx/out\n```\n\n----------------------------------------\n\nTITLE: Adding Android RUM Agent Dependency in Kotlin\nDESCRIPTION: Kotlin code snippet showing how to add the Splunk RUM agent and OpenTelemetry instrumentation dependencies to the application's build.gradle file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_3\n\nLANGUAGE: kotlin\nCODE:\n```\ndependencies {\n//...\n   // Set the desired version of the RUM agent.\n   // See available releases: https://github.com/signalfx/splunk-otel-android/releases\n   implementation(\"com.splunk:splunk-otel-android:+\")\n   implementation(\"io.opentelemetry.android:instrumentation:+\")\n//...\n}\n```\n\n----------------------------------------\n\nTITLE: Checking current access token values\nDESCRIPTION: Command to retrieve the current values configured for a Helm release, which can be used to check the current access token.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm get values <Release_Name>\n```\n\n----------------------------------------\n\nTITLE: RestructuredText Reference Directive\nDESCRIPTION: RestructuredText reference label directive for the collector built-in dashboard section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-builtin-dashboard.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _collector-builtin-dashboard:\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Collector Permissions\nDESCRIPTION: Command to set custom Linux capabilities for the Collector after installation, allowing for tailored permissions based on specific system requirements.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsetcap {CUSTOM_CAPABILITIES}=+eip /usr/bin/otelcol\n```\n\n----------------------------------------\n\nTITLE: Specifying Terraform Provider for Splunk\nDESCRIPTION: This snippet sets up the Terraform provider for Splunk using the signalfx provider from Splunk Terraform registry. It specifies the version of the provider required and includes variable definitions for authentication token and API URL.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-datalinks-terraform/apm-create-data-links-terraform-file.rst#2025-04-22_snippet_0\n\nLANGUAGE: terraform\nCODE:\n```\n# Specify the Terraform provider and version\nterraform {\nrequired_providers {\nsignalfx = {\nsource  = \"splunk-terraform/signalfx\"\nversion = \"~> <current-splunk-terraform-provider-version>\"\n}\n}\n}\n\n# The following variable blocks can also be located in a variables.tf file in the same directory\nvariable \"signalfx_auth_token\" {\ndescription = \"The user API access authentication token for your org\"\ntype        = string\ndefault     = \"\"\n}\nvariable \"signalfx_api_url\" {\ndescription = \"The API URL of your org\"\ntype        = string\ndefault     = \"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Restarting Collector's cluster receiver deployment\nDESCRIPTION: Command to restart the OpenTelemetry Collector's cluster receiver deployment after updating the access token (when clusterReceiver.enabled=true).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nkubectl rollout restart deployment <Release_Name>-k8s-cluster-receiver\n```\n\n----------------------------------------\n\nTITLE: Sending Alerts to Splunk On-Call with Custom Routing Key\nDESCRIPTION: Modifies TICKscript to route alerts to Splunk On-Call using a specified routing key 'Another_route'. Relaies on previously set configurations in Kapacitor.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/kapacitor-integration-guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: unknown\nCODE:\n```\nstream\n  |alert()\n    .victorOps()\n      .routingKey('Another_route')\n```\n\n----------------------------------------\n\nTITLE: Verifying Missing Metrics with Splunk SPL\nDESCRIPTION: SPL command to check if Kubernetes pod and node metrics are missing by counting metric values in the otel_metrics index.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-missing-metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n| mstats count(_value) as \"Val\" where index=\"otel_metrics_0_93_3\" AND metric_name IN (k8s.pod.*, k8s.node.*) by metric_name\n```\n\n----------------------------------------\n\nTITLE: Verifying Elasticsearch Watcher Status using cURL (Shell)\nDESCRIPTION: This shell command uses cURL to send a GET request to the Elasticsearch Watcher API's `_watcher/stats` endpoint. It's used to verify if the Watcher service is running on the specified Elasticsearch node (localhost:9200 in this example). The `pretty` parameter requests a formatted JSON response.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/elasticsearch-watcher-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -XGET 'http://localhost:9200/_watcher/stats?pretty'\n```\n\n----------------------------------------\n\nTITLE: Importing Namespace for Runtime Information in C#\nDESCRIPTION: This C# code snippet shows the necessary `using` directive to import the `System.Runtime.InteropServices` namespace. This namespace is required to access runtime environment details, specifically for obtaining the runtime identifier (RID) using `RuntimeInformation.RuntimeIdentifier`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_2\n\nLANGUAGE: c\nCODE:\n```\nusing System.Runtime.InteropServices\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk HEC Settings in YAML\nDESCRIPTION: YAML configuration for specifying the Splunk HTTP Event Collector (HEC) endpoint and token. Optionally includes index configuration for log storage.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/collector-config-tutorial-edit.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkPlatform:\n  endpoint: \"<your_hec_endpoint>\"\n  token: \"<your_hec_token>\"\n  index: \"<your_index>\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Metrics and Resource Attributes\nDESCRIPTION: This snippet specifies the configuration for enabling certain metrics and resource attributes that are disabled by default. By setting options such as 'sqlserver.instance.name' in the receiver configuration, users can ensure more comprehensive metric collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/mssql-server-receiver.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\\n    sqlserver:\\n      collection_interval: 10s\\n    sqlserver/1:\\n      collection_interval: 5s\\n      username: sa\\n      password: securepassword\\n      server: 0.0.0.0\\n      port: 1433\\n      resource_attributes:\\n        sqlserver.instance.name:\\n          enabled: true\n```\n\n----------------------------------------\n\nTITLE: Visualizing Data Aggregation Flow with Mermaid Diagram\nDESCRIPTION: A flowchart diagram illustrating how metrics pipeline management (MPM) processes incoming metric time series (MTS), performs aggregation on selected MTS, and outputs both aggregated MTS and any raw MTS that are kept.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/metrics-pipeline/mpm-rule-agreggation.rst#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n\naccTitle: Data aggregation diagram\naccDescr: Metrics pipeline management (MPM) receives raw incoming metric time series (MTS). You choose an MTS to aggregate, and perform the aggregation, then you choose whether to keep or drop the raw MTS. MPM keeps the aggregated MTS and any raw MTS that you chose to keep.\n\nRaw[(Incoming raw MTS)] ---|MPM|ChooseDimensions{\"Choose MTS to aggregate\"} ---|Perform aggregation|CreateNew(\"New aggregated MTS with rolled-up\nmetrics\") ---|Keep or drop raw MTS|OriginalMTS[(Kept MTS and new MTS)]\n```\n\n----------------------------------------\n\nTITLE: Monitoring Gross Spans Received in Splunk RUM (Metric)\nDESCRIPTION: This metric counts the total number of spans received by Splunk RUM before any throttling or filtering based on the Spans Per Minute (SPM) limit. It helps assess the total incoming span volume before limits are applied.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-rum.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.rum.grossSpansReceived\n```\n\n----------------------------------------\n\nTITLE: Adding a Supported Aggregation Method to SignalFlow Histogram\nDESCRIPTION: Demonstrates adding a supported aggregation method, such as `.sum()`, to a SignalFlow program after specifying a metric using `histogram()`. This is necessary because `histogram()` requires a subsequent method (like sum, average, percentile, etc.) to produce plottable data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/chart-builder.rst#2025-04-22_snippet_3\n\nLANGUAGE: SignalFlow\nCODE:\n```\nhistogram('service_latency').sum()\n```\n\n----------------------------------------\n\nTITLE: Error Message for Missing Exporter Endpoint\nDESCRIPTION: Error message displayed when neither SPLUNK_REALM and SPLUNK_ACCESS_TOKEN pair nor custom exporter endpoint is configured.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/aws/otel-lambda-layer/troubleshooting-lambda-layer.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n[ERROR] Exporter endpoint must be set when SPLUNK_REALM is not set. To export data, either set a realm and access token or a custom exporter endpoint.\n```\n\n----------------------------------------\n\nTITLE: Creating Empty Commit for Existing Heroku Projects\nDESCRIPTION: Command to create an empty commit before deploying the app with the buildpack. This step is necessary when adding the buildpack to an existing Heroku project.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/heroku.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit commit --allow-empty -m \"empty commit\"\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Collector Pipeline Reference Error\nDESCRIPTION: Command-line output showing the error that occurs when a pipeline references a receiver that hasn't been defined in the configuration. This example shows the error when 'syslog' is referenced but not defined.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-troubleshoot.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nError: service::pipelines::logs: references receiver \"syslog\" which is not configured\n2024/02/19 18:37:42 main.go:89: application run finished with error: service::pipelines::logs: references receiver \"syslog\" which is not configured\n```\n\n----------------------------------------\n\nTITLE: Building iOS Simulator Archives for XCFramework Creation\nDESCRIPTION: Commands to create iOS simulator archives for the SplunkOtel framework, which is a prerequisite for building an XCFramework.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/install-rum-ios.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nxcodebuild archive -project SplunkRum.xcodeproj -scheme SplunkOtel -destination \"generic/platform=iOS Simulator\" -archivePath \"archives/SplunkRum-iOS_Simulator\"\n```\n\n----------------------------------------\n\nTITLE: Setting Dynamic Global Attributes in Splunk RUM JavaScript\nDESCRIPTION: Demonstrates how to update or add global attributes dynamically during runtime for properties that change over time.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/migrate-manual-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nSplunkRum.setGlobalAttributes({\n   'account.type': goldStatus,\n   'app.release': getReleaseNumber(),\n   'dark_mode.enabled': darkModeToggle.status,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret YAML for Splunk OTel Collector\nDESCRIPTION: YAML definition for creating a Kubernetes secret containing the access tokens for Splunk Observability Cloud and/or Splunk Platform. This secret stores sensitive credentials used by the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s-addon-eks.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n    name: splunk-otel-collector\n    namespace: splunk-monitoring\ntype: Opaque\ndata:\n    splunk_observability_access_token: <YOUR_ACCESS_TOKEN> # Replace with your actual access token\n    splunk_platform_hec_token: <YOUR_HEC_TOKEN>  # Add this line only if using with splunkPlatform\n```\n\n----------------------------------------\n\nTITLE: Installing Podman Private Runner\nDESCRIPTION: Starts a private runner container using Podman with necessary network capabilities.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_34\n\nLANGUAGE: shell\nCODE:\n```\npodman run --cap-add NET_ADMIN -e \"RUNNER_TOKEN=YOUR_TOKEN_HERE\" \\\nquay.io/signalfx/splunk-synthetics-runner:latest\n```\n\n----------------------------------------\n\nTITLE: Navigating to Docker Compose Project Directory\nDESCRIPTION: Shell command to change to the directory containing the Docker Compose project files before uninstalling the private runner.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\ncd /path/to/your/docker-compose-directory\n```\n\n----------------------------------------\n\nTITLE: Initializing Custom Metrics Collection in Node.js\nDESCRIPTION: Demonstrates initializing the Splunk OpenTelemetry Node.js distribution to send custom application metrics. It shows configuration options within the `start` function, including setting the `serviceName`, customizing resource attributes using `resourceFactory`, setting the `exportIntervalMillis`, and specifying the OTLP endpoint. The example also shows how to obtain a meter instance, create a counter metric ('clicks'), and record a value using `.add()`. Dependencies include `@splunk/otel`, `@opentelemetry/resources`, and `@opentelemetry/api`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/manual-instrumentation.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require('@splunk/otel');\nconst { Resource } = require('@opentelemetry/resources');\nconst { metrics } = require('@opentelemetry/api');\n\n// All fields are optional.\nstart({\n  // Takes preference over OTEL_SERVICE_NAME environment variable\n  serviceName: 'my-service',\n  metrics: {\n    // The suggested resource is filled in using OTEL_RESOURCE_ATTRIBUTES\n    resourceFactory: (suggestedResource: Resource) => {\n      return suggestedResource.merge(new Resource({\n        'my.property': 'xyz',\n        'build': 42,\n      }));\n    },\n    exportIntervalMillis: 1000, // default: 5000\n    // The default exporter used is OTLP over gRPC\n    endpoint: 'http://collector:4317',\n  },\n});\n\nconst meter = metrics.getMeter('my-meter');\nconst counter = meter.createCounter('clicks');\ncounter.add(3);\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk Enterprise Indexes\nDESCRIPTION: Defines two Splunk Enterprise indexes with paths and size configurations for storing collected log data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/collector-splunk.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nsplunk:\n  conf:\n    indexes:\n      directory: /opt/splunk/etc/apps/search/local\n      content:\n        index1:\n          coldPath: $SPLUNK_DB/index1/colddb\n          datatype: event\n          homePath: $SPLUNK_DB/index1/db\n          maxTotalDataSizeMB: 512000\n          thawedPath: $SPLUNK_DB/index1/thaweddb\n        index2:\n          coldPath: $SPLUNK_DB/index2/colddb\n          datatype: event\n          homePath: $SPLUNK_DB/index2/db\n          maxTotalDataSizeMB: 512000\n          thawedPath: $SPLUNK_DB/index2/thaweddb\n```\n\n----------------------------------------\n\nTITLE: Embedding HTML for WhatsApp Section in reStructuredText\nDESCRIPTION: This snippet embeds HTML content to create a section header for WhatsApp notifications in the reStructuredText document. It includes an anchor link for easy navigation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/notifications/notification-types.rst#2025-04-22_snippet_3\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n  \n    <embed>\n      <h2>WhatsApp<a name=\"whatsapp\" class=\"headerlink\" href=\"#whatsapp\" title=\"Permalink to this headline\">¶</a></h2>\n    </embed>\n```\n\n----------------------------------------\n\nTITLE: RST Note Block - Maintenance Mode\nDESCRIPTION: RST markup for a note about Maintenance Mode functionality in Splunk On-Call\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/user-roles/alert-admin.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. note:: Maintenance Mode does not stop the alerts from coming into Splunk On-Call, just from paging the on-call users when they do come in.\n```\n\n----------------------------------------\n\nTITLE: ARN References for Splunk OpenTelemetry Lambda Layer (ARM64)\nDESCRIPTION: GitHub reference to the ARN values needed for adding the Splunk OpenTelemetry Lambda layer to AWS Lambda functions running on Graviton2 ARM64 architecture.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/aws/otel-lambda-layer/instrument-lambda-functions.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n.. github:: yaml\n            :url: https://raw.githubusercontent.com/signalfx/lambda-layer-versions/main/splunk-apm/splunk-apm-arm.md\n```\n\n----------------------------------------\n\nTITLE: Response to Resolve Incident Request - Splunk On-Call REST Endpoint - JSON\nDESCRIPTION: The JSON response showing successful resolution of an incident in Splunk On-Call via the REST Endpoint. Includes the operation result and the entity_id confirming which incident was affected. Typically parsed to verify completion.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rest-endpoint-integration-guide.rst#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ \\u201cresult\\u201d : \\u201csuccess\\u201d, \\u201centity_id\\u201d : \\u201cdisk space/db01.mycompany.com\\u201d }\n```\n\n----------------------------------------\n\nTITLE: Configuring NGINX for OPcache Monitoring\nDESCRIPTION: NGINX configuration snippet for setting up access to the OPcache monitoring script. This configuration restricts access to localhost only for security purposes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cache/opcache.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlocation ~ /monitoring/.*\\.php$ {\n    #access_log off;\n    allow 127.0.0.1;\n    allow ::1;\n    deny all;\n    include fastcgi_params;\n    fastcgi_split_path_info ^(.+\\.php)(/.*)$;\n    fastcgi_param  PHP_ADMIN_VALUE \"open_basedir=/var/log:/usr/bin:/srv/http/monitoring\";\n    fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n    fastcgi_pass php-handler;\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Logs from Kubernetes Collector Pods\nDESCRIPTION: These commands extract logs from agent and cluster receiver pods, saving them to log files for troubleshooting. The logs contain operational details that help diagnose runtime issues with the Collector components.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-support.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs my-splunk-otel-collector-agent-fzn4q otel-collector > my-splunk-otel-collector-agent.log\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs my-splunk-otel-collector-k8s-cluster-receiver-7545499bc7-vqdsl > my-splunk-otel-collector-k8s-cluster-receiver.log\n```\n\n----------------------------------------\n\nTITLE: Using Splunk>VictorOps Bot Channel Mapping Command - Microsoft Teams (Markdown)\nDESCRIPTION: Demonstrates the syntax to invoke the Splunk>VictorOps bot's channel mapping functionality within a Microsoft Teams channel. This command enables the definition or modification of mapping between Splunk On-Call teams or escalation policies and selected Teams channels. To execute, ensure the app is installed and the user has at least team owner permissions. No external dependencies are required; command syntax: @Splunk VictorOps mapchannel. No parameters required; output is a card-based mapping UI in the Teams client.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/microsoft-teams-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n``@Splunk VictorOps mapchannel``\n```\n\n----------------------------------------\n\nTITLE: Jenkins Pipeline Configuration in YAML\nDESCRIPTION: Configuration showing how to add the Jenkins monitor to the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/jenkins.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/jenkins]\n```\n\n----------------------------------------\n\nTITLE: Configuring Attribute Extraction from Span Name in YAML\nDESCRIPTION: Configuration for extracting attributes from a span name using regular expressions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/span-processor.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspan/to_attributes:\n  name:\n    to_attributes:\n      rules:\n        - regexp-rule1\n        - regexp-rule2\n        - regexp-rule3\n      break_after_match: <true|false>\n```\n\n----------------------------------------\n\nTITLE: Sending Test Alert to Riemann in Ruby\nDESCRIPTION: These Ruby commands, to be executed in IRB, create a Riemann::Client instance and submit a test event simulating a critical HTTP request scenario. They require the ruby 'riemann-client' gem and an active Riemann server. Input parameters include host, service, metric, state, description, and tags; output is the submission of an event to the Riemann stream, which should propagate to VictorOps if integration is configured.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/riemann-integration.rst#2025-04-22_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\nr = Riemann::Client.new\\nr << { host: \\\"www1\\\", service: \\\"http req\\\", metric: 2.53, state: \\\"critical\\\", description: \\\"Request took 2.53 seconds.\\\", tags: [\\\"http\\\"] }\n```\n\n----------------------------------------\n\nTITLE: Metric Customization for OpenTelemetry Collector - YAML\nDESCRIPTION: Configurations for enabling or disabling specific metrics collected by the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n        disableMetrics:\n```\n\n----------------------------------------\n\nTITLE: Activating Desugaring in Groovy Applications\nDESCRIPTION: Code snippet showing how to enable desugaring in a Groovy Android application by configuring compileOptions and adding the necessary dependency in build.gradle.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_1\n\nLANGUAGE: groovy\nCODE:\n```\nandroid {\n   compileOptions {\n      //...\n      coreLibraryDesugaringEnabled true\n      sourceCompatibility JavaVersion.VERSION_1_8 // Java 8 and higher\n      targetCompatibility JavaVersion.VERSION_1_8 // Java 8 and higher\n      //...\n   }\n}\n\ndependencies {\n   //...\n   coreLibraryDesugaring 'com.android.tools:desugar_jdk_libs:1.1.5'\n   //...\n}\n```\n\n----------------------------------------\n\nTITLE: Verifying Installation of Required Tools\nDESCRIPTION: Commands to verify the successful installation of Minikube, Podman, and Helm by checking their versions. The comments show example version outputs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/collector-config-tutorial-start.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nminikube version\n# minikube version: v1.32.0\npodman -v\n# podman version 4.9.3\nhelm version\n# version.BuildInfo{Version:\"v3.14.2\", ...}\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata Table for Amazon EMR in reStructuredText\nDESCRIPTION: This code snippet defines a table in reStructuredText format that lists the metadata properties imported by Infrastructure Monitoring for Amazon Elastic MapReduce (EMR). It includes the original EMR property names, custom property names used in Infrastructure Monitoring, and descriptions of each property.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/monitor/aws-infra-metadata.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 30 30 60\n   :width: 100%\n\n   *  - :strong:`EMR Name`\n      - :strong:`Custom Property`\n      - :strong:`Description`\n\n   *  - Id\n      - aws_cluster_id\n      - AWS identifier of the cluster\n\n   *  - Name\n      - aws_cluster_name\n      - The name you gave the cluster\n\n   *  - AutoScalingRole\n      - aws_auto_scaling_role\n      - An Amazon Identity and Access Management (IAM) role for automatic scaling policies\n\n   *  - CustomAmiId\n      - aws_custom_ami_id\n      - The ID of a custom Amazon EBS-backed Linux Amazon Machine Image (AMI) if the cluster uses a custom AMI\n\n   *  - InstanceCollectionType\n      - aws_instance_collection_type\n      - The instance group configuration of the cluster\n\n   *  - LogUri\n      - aws_log_uri\n      - The path to the Amazon S3 location where logs for this cluster are stored\n\n   *  - MasterPublicDnsName\n      - aws_master_public_dns_name\n      - The DNS name of the primary node\n\n   *  - ReleaseLabel\n      - aws_release_label\n      - The Amazon EMR release label, which determines the version of open-source application packages installed on the cluster\n\n   *  - RepoUpgradeOnBoot\n      - aws_repo_upgrade_on_boot\n      - Applies only when CustomAmiID is used\n\n   *  - RequestedAmiVersion\n      - aws_requested_ami_version\n      - The AMI version requested for this cluster\n\n   *  - RunningAmiVersion\n      - aws_running_ami_version\n      - The AMI version running on this cluster\n\n   *  - ScaleDownBehavior\n      - aws_scale_down_behavior\n      - The way that individual Amazon EC2 instances terminate when an automatic scale-in activity occurs or an instance group is resized\n\n   *  - SecurityConfiguration\n      - aws_security_configuration\n      - The name of the security configuration applied to the cluster\n\n   *  - ServiceRole\n      - aws_service_role\n      - The IAM role that the Amazon EMR service uses to access AWS resources on your behalf\n\n   *  - Status\n      - aws_status\n      - The current status details about the cluster\n\n   *  - AutoTerminate\n      - aws_auto_terminate\n      - Specifies whether the cluster terminates after completing all steps\n\n   *  - TerminationProtected\n      - aws_termination_protected\n      - Indicates whether Amazon EMR locks the cluster to prevent the EC2 instances from being terminated by an API call or user intervention, or in the event of a cluster error\n\n   *  - VisibleToAllUsers\n      - aws_visible_to_all_users\n      - Indicates whether the cluster is visible to all IAM users of the AWS account associated with the cluster\n\n   *  - NormalizedInstanceHours\n      - aws_normalized_instance_hours\n      - An approximation of the cost of the cluster, represented in m1.small/hours\n```\n\n----------------------------------------\n\nTITLE: Restarting Fluentd Windows Service with PowerShell\nDESCRIPTION: PowerShell commands to stop and start the Fluentd Windows service after configuration changes. This is needed to apply any changes made to the Fluentd configuration files.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows-msi.rst#2025-04-22_snippet_3\n\nLANGUAGE: PowerShell\nCODE:\n```\n- Stop-Service fluentdwinsvc\n- Start-Service fluentdwinsvc\n```\n\n----------------------------------------\n\nTITLE: Calculating Percentages in Data Visualization with Splunk\nDESCRIPTION: This involves using formulas to convert raw metrics, like `zipper.missCount` and `zipper.hitCount`, into meaningful percentages to assess performance metrics. Inputs typically include the related metrics, and the output is a plot reflecting the ratio or percentage calculation. This method assists in visualizing performance metrics effectively by converting them into percentages.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/gain-insights-through-chart-analytics.rst#2025-04-22_snippet_1\n\nLANGUAGE: none\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Adding Rails Instrumentation Gem to Ruby Gemfile\nDESCRIPTION: This snippet shows how to add the Rails instrumentation gem to a Ruby project's Gemfile. It specifies the gem name and version constraint for Rails instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/ruby-manual-instrumentation.rst#2025-04-22_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\ngem \"opentelemetry-instrumentation-rails\", \"~> 0.27\"\n```\n\n----------------------------------------\n\nTITLE: Creating Another Image Reference in reStructuredText\nDESCRIPTION: This code block inserts another image into the document using reStructuredText syntax. It specifies the image file path, width, and alternative text for accessibility.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/mobile/mobile-settings-menu.rst#2025-04-22_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. image:: /_images/spoc/mob-settings2.png\n    :width: 100%\n    :alt: Add Splunk On-Call to your contacts.\n```\n\n----------------------------------------\n\nTITLE: Configuring Riemann for VictorOps Integration - Clojure\nDESCRIPTION: This Clojure code configures Riemann to send alerts to VictorOps using an API key and routing key. Dependencies include the Riemann server and VictorOps integration library for Clojure. The code defines a stream that routes events with different state values to the appropriate VictorOps alert endpoints. Replace the API key and routing key placeholders with actual values. Input: Riemann events; Output: alerts sent to VictorOps depending on state. Must be added to the Riemann config file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/riemann-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: clojure\nCODE:\n```\n(let [vo (victorops \\\"1394aab4-XXXXX-XXXX-XXXX-XXXXXXXXXXXX\\\"\\n                 \\\"Sample_route\\\")]\\n  (streams\\n    (changed-state\\n      (where (state \\\"info\\\") (:info vo))\\n      (where (state \\\"warning\\\") (:resolve vo))\\n      (where (state \\\"critical\\\") (:critical vo))\\n      (where (state \\\"ok\\\") (:recovery vo)))))\n```\n\n----------------------------------------\n\nTITLE: Configuring AlwaysOn Profiling Pipeline for Splunk OpenTelemetry Collector - YAML\nDESCRIPTION: This YAML configuration snippet defines a 'logs/profiling' pipeline for the Splunk Distribution of OpenTelemetry Collector version 0.34 or higher. It sets up OTLP and Splunk HEC receivers/exporters for profiling data, includes processors for batching and memory limiting, and references environment variables for security and flexibility. Required dependencies include the Splunk Collector with the OTLP receiver and HEC exporter enabled. Inputs: environment variables such as SPLUNK_ACCESS_TOKEN and SPLUNK_INGEST_URL; Output: routes profiling log data to Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/troubleshooting/common-nodejs-troubleshooting.rst#2025-04-22_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      http:\n\nexporters:\n  # Profiling\n  splunk_hec/profiling:\n    token: \"${SPLUNK_ACCESS_TOKEN}\"\n    endpoint: \"${SPLUNK_INGEST_URL}/v1/log\"\n    log_data_enabled: false\n\nprocessors:\n  batch:\n  memory_limiter:\n    check_interval: 2s\n    limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}\n\nservice:\n  pipelines:\n    logs/profiling:\n      receivers: [otlp]\n      processors: [memory_limiter, batch]\n      exporters: [splunk_hec, splunk_hec/profiling]\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS CloudWatch Integration JSON Template\nDESCRIPTION: JSON configuration template for AWS CloudWatch integration. Includes settings for import options, regions, polling rate, and account information.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-troubleshooting.rst#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n      \"ignoreAllStatusMetrics\": false,\n      \"importCloudWatch\": true,\n      \"largeVolume\": false,\n      \"name\": \"AWS Dev\",\n      \"pollRate\": 300000,\n      \"regions\": [ \"us-east-1\", \"us-east-2\", \"us-west-1\", \"us-west-2\" ],\n      \"roleArn\": null,\n      \"services\": [],\n      \"sfxAwsAccountArn\": \"arn:aws:iam::134183635603:root\",\n      \"syncCustomNamespacesOnly\": false,\n      \"syncLoadBalancerTargetGroupTags\": false,\n      \"type\": \"AWSCloudWatch\"\n   }\n```\n\n----------------------------------------\n\nTITLE: Disabling Gzip Compression for OTLP Exporter in YAML\nDESCRIPTION: This YAML configuration snippet illustrates how to disable the default gzip compression for the OTLP exporter. Setting the 'compression' option to 'none' under the specific exporter's configuration achieves this.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlp-exporter.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n.. code-block:: yaml\n\n  exporters:\n    otlp:\n      ...\n      compression: none\n```\n\n----------------------------------------\n\nTITLE: Specifying Past Relative Time Interval in Splunk\nDESCRIPTION: Define a relative time window that occurred entirely in the past by specifying a start and end point relative to now, separated by 'to'. The earlier time point must come first. This example shows data from between 1 and 3 minutes ago.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/use-time-range-selector.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n-3m to -1m\n```\n\n----------------------------------------\n\nTITLE: Capturing Non-Newline Characters with RegEx in Rules Engine\nDESCRIPTION: A regular expression pattern used to capture one or more characters that are not newline characters. This is useful for extracting specific data from alert fields.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-variable-expansion.rst#2025-04-22_snippet_2\n\nLANGUAGE: regex\nCODE:\n```\n\"([^\\\\n]+)\"\n```\n\n----------------------------------------\n\nTITLE: JMX Port Exposure Configuration\nDESCRIPTION: Java properties configuration to expose JMX ports for inbound connections without authentication or encryption.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/genericjmx.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\njava \\\n  -Dcom.sun.management.jmxremote.port=5000 \\\n  -Dcom.sun.management.jmxremote.authenticate=false \\\n  -Dcom.sun.management.jmxremote.ssl=false \\\n  -Dcom.sun.management.jmxremote.rmi.port=5000 \\\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Webhook Payload for Splunk On-Call to Webex Teams Integration\nDESCRIPTION: JSON payload template for formatting incident notifications sent from Splunk On-Call to Webex Teams. The payload includes the alert entity name and state message formatted with markdown.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/webex-teams-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"markdown\": \"${{{ALERT.entity_display_name}}}<br>{{ALERT.state_message}}\" }\n```\n\n----------------------------------------\n\nTITLE: Example Debug Log Output for OpenTelemetry Trace\nDESCRIPTION: Shows an example of what debug log entries look like when debug logging is activated for Python OpenTelemetry.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/troubleshooting/common-python-troubleshooting.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n...\n[opentelemetry.auto.trace 2021-10-10 10:57:05:814 +0200] [main] DEBUG io.opencensus.tags.Tags - <Could not load lite implementation for TagsComponent, now using default implementation for TagsComponent.3>\n[opentelemetry.auto.trace 2021-10-10 10:57:05:722 +0200] [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent0 - direct buffer constructor: unavailable\n...\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Collector Health Check Response\nDESCRIPTION: JSON response from the OpenTelemetry Collector health check extension endpoint showing that the server is available. It includes server status, uptime information, and timestamp.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-troubleshoot.rst#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"status\": \"Server available\",\n   \"upSince\": \"2020-11-11T04:12:31.6847174Z\",\n   \"uptime\": \"49.0132518s\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk On-Call Provider Authentication\nDESCRIPTION: This Terraform configuration block configures the Splunk On-Call (VictorOps) provider. It requires the user's Splunk On-Call API ID and API Key for authentication, allowing Terraform to interact with the Splunk On-Call REST API. These credentials must be obtained from the Splunk On-Call web interface.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/manage-splunk-oncall-using-terraform.rst#2025-04-22_snippet_2\n\nLANGUAGE: terraform\nCODE:\n```\nprovider \"victorops\" \n{\napi_id  = \"your api id goes here\"\napi_key = \"your api key goes here\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Browser RUM Agent Realms\nDESCRIPTION: Lists the available realm codes for different geographic regions where the Browser RUM agent operates. Includes realm codes for US (us0, us1, us2), Europe (eu0, eu1, eu2), and Asia-Pacific (au0, jp0).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/requirements/realm.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nus0, us1, us2\n```\n\nLANGUAGE: plaintext\nCODE:\n```\neu0, eu1, eu2\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nau0, jp0\n```\n\n----------------------------------------\n\nTITLE: Listing Splunk OpenTelemetry Collector Configuration Directory\nDESCRIPTION: Shows the file structure of the Collector configuration directory after installation, displaying available configuration files.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-start.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n.\n|-- agent_config.yaml\n|-- config.d\n|-- fluentd\n|-- gateway_config.yaml\n|-- splunk-otel-collector.conf\n|-- splunk-otel-collector.conf.example\n`-- splunk-support-bundle.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring AlertSite Email Subject for Splunk OnCall Integration\nDESCRIPTION: Example of how to format the email subject for AlertSite alerts to properly trigger and resolve incidents in Splunk OnCall. The subject must include specific keywords like CRITICAL or PROBLEM to open a new incident, and RESOLVED or OK to resolve an incident.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/alertsite-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n[AlertSite] Monitor Alert - $DESCRIP CRITICAL\n```\n\n----------------------------------------\n\nTITLE: Document Template for New Pages - reStructuredText\nDESCRIPTION: Provides the base template for all new documentation pages written in reStructuredText. The template includes required label anchors for cross-referencing, a header with the page title, meta description for SEO, and a placeholder for main content. To use this template, fill in the appropriate label, title, and description. The template is intended as a starting point for contributors and mandates use of reStructuredText features like labels and meta directives.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _label-for-the-page:\n\n************************************************************\nTitle of the page\n************************************************************\n\n.. meta::\n   :description: Description for search engines\n\nContent goes here.\n```\n\n----------------------------------------\n\nTITLE: Configuring AlertManager for Splunk On-Call\nDESCRIPTION: This YAML configuration snippet is used for setting up AlertManager to work with Splunk On-Call. It requires replacing the `api_key` with the Splunk On-Call Service API Key. The snippet configures alert grouping, intervals, and the receiver, which is set to `victorOps-receiver`. Main dependencies include having Prometheus installed and configured with AlertManager.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/prometheus-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nroute:\n   group_by: ['alertname', 'cluster', 'service']\n   group_wait: 30s\n   group_interval: 5m\n   repeat_interval: 3h\n   receiver: victorOps-receiver\n\nreceivers:\n  - name: victorOps-receiver\n    victorops_configs:\n      - api_key: 558e7ebc-XXXX-XXXX-XXXX-XXXXXXXXXXXX\n        routing_key: Sample_route\n        state_message: 'Alert: {{ .CommonLabels.alertname }}. Summary:{{ .CommonAnnotations.summary }}. RawData: {{ .CommonLabels }}'\n```\n\n----------------------------------------\n\nTITLE: Listing Wildcard URLs to Allow for Splunk Observability Cloud (Shell)\nDESCRIPTION: Provides general wildcard URLs that need to be allowed in network security policies (firewalls, proxies) to ensure connectivity to Splunk Observability Cloud services. Replace `<YOUR_REALM>` with the specific realm.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/allow-services.rst#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n\\*.signalfx.com\n\n\\*.<YOUR_REALM>.signalfx.com\n```\n\n----------------------------------------\n\nTITLE: RST Table Structure for Synthetic Test Types\nDESCRIPTION: ReStructuredText markup defining a table that lists different types of synthetic tests and their corresponding scenario references.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/synth-scenarios/synth-landing.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 20 80\n\n   * - :strong:`Test type`\n     - :strong:`Scenario`\n\n   * - Browser test \n     - :ref:`browser-test-scenario`\n\n   * - API test \n     - :ref:`api-test-scenario`\n   \n   * - Uptime test \n     - :ref:`uptime-test-scenario`\n```\n\n----------------------------------------\n\nTITLE: Prometheus Node Pipeline Configuration in YAML\nDESCRIPTION: Shows how to add the Prometheus Node monitor to the metrics pipeline in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-node.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/prometheus-node]\n```\n\n----------------------------------------\n\nTITLE: Running Discovery Mode with Dry Run\nDESCRIPTION: Command to run the collector in discovery mode with dry run option to test configurations without applying them.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-third-party.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbin/otelcol --discovery --dry-run\n```\n\n----------------------------------------\n\nTITLE: RST Table Definition for OpenTelemetry Receivers\nDESCRIPTION: A reStructuredText (RST) table listing all available receivers for the OpenTelemetry Collector. The table includes receiver names, descriptions, and supported pipeline types.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/otel-receivers-table.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :widths: 25 55 20\n   :header-rows: 1\n   :width: 100%\n\n   * - Name\n     - Description\n     - Pipeline types\n```\n\n----------------------------------------\n\nTITLE: Adding Collectd df Monitor to Metrics Pipeline\nDESCRIPTION: Configuration snippet showing how to add the Collectd df monitor to the service pipelines metrics receivers section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/collectd-df.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/collectd/df]\n```\n\n----------------------------------------\n\nTITLE: Setting Alert Sensitivity for Custom MTS Usage Percentage\nDESCRIPTION: This value defines the sensitivity condition for triggering an alert based on custom metric time series (MTS) usage percentage. An alert triggers if 100% of the data points within a 30-minute window meet the alert threshold.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/autodetect/autodetect-list.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n100% of 30m\n```\n\n----------------------------------------\n\nTITLE: Complete Logstash Configuration Example with Metrics Plugin\nDESCRIPTION: Comprehensive example demonstrating the configuration of Logstash with timer and meter metrics using the Metrics filter plugin\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/logstash-tcp.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ninput {\n  file {\n    path => \"/var/log/auth.log\"\n    start_position => \"beginning\"\n    tags => [\"auth_log\"]\n  }\n\n  # A contrived file that contains timing messages\n  file {\n    path => \"/var/log/durations.log\"\n    tags => [\"duration_log\"]\n    start_position => \"beginning\"\n  }\n}\n\nfilter {\n  if \"duration_log\" in [tags] {\n    dissect {\n      mapping => {\n        \"message\" => \"Processing took %{duration} seconds\"\n      }\n      convert_datatype => {\n        \"duration\" => \"float\"\n      }\n    }\n    if \"_dissectfailure\" not in [tags] { # Filter out bad events\n      metrics {\n        timer => { \"process_time\" => \"%{duration}\" }\n        flush_interval => 10\n        # This makes the timing stats pertain to only the previous 5 minutes\n        # instead of since Logstash last started.\n        clear_interval => 300\n        add_field => {\"type\" => \"processing\"}\n        add_tag => \"metric\"\n      }\n    }\n  }\n  # Count the number of logins using SSH from /var/log/auth.log\n  if \"auth_log\" in [tags] and [message] =~ /sshd.*session opened/ {\n    metrics {\n      # This determines how often metric events will be sent to the agent, and\n      # thus how often data points will be emitted.\n      flush_interval => 10\n      # The name of the meter will be used to construct the name of the metric\n      # in Splunk Infrastructure Monitoring. For this example, a data point called `logins.count` would\n      # be generated.\n      meter => \"logins\"\n      add_tag => \"metric\"\n    }\n  }\n}\n\noutput {\n  # This can be helpful to debug\n  stdout { codec => rubydebug }\n\n  if \"metric\" in [tags] {\n    tcp {\n      port => 8900\n      # The agent will connect to Logstash\n      mode => \"server\"\n      # Needs to be \"0.0.0.0\" if running in a container.\n      host => \"127.0.0.1\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Grafana UI URL Format for Dashboard Import\nDESCRIPTION: Example URL format for accessing the Grafana user interface to import dashboards. Replace `<host-name>/<IP-address>` with the appropriate node IP or hostname and `<port>` with the NodePort specified in the Service manifest (e.g., 32001).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_12\n\nLANGUAGE: none\nCODE:\n```\n        http://<host-name>/<IP-address>:<port>\n```\n\n----------------------------------------\n\nTITLE: Configuring Null Storage for Splunk HEC Exporter in YAML\nDESCRIPTION: This YAML configuration snippet sets the storage for the sending queue of the Splunk HEC exporter to null, effectively disabling persistent storage for the queue.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nagent:\n  config:\n    exporters:\n      splunk_hec/platform_traces:\n        sending_queue:\n          storage: null\n```\n\n----------------------------------------\n\nTITLE: Running Docker container with custom configuration using SPLUNK_CONFIG\nDESCRIPTION: PowerShell command to run the OpenTelemetry Collector with a mounted custom configuration file using the SPLUNK_CONFIG environment variable. This example shows how to mount a local directory containing configuration files.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows-manual.rst#2025-04-22_snippet_2\n\nLANGUAGE: PowerShell\nCODE:\n```\n$ docker run --rm -e SPLUNK_ACCESS_TOKEN=12345 -e SPLUNK_REALM=us0 `\n\t    -e SPLUNK_CONFIG=c:\\splunk_config\\gateway_config.yaml -p 13133:13133  `\n\t    -p 14250:14250 -p 14268:14268 -p 4317:4317 -p 6060:6060 -p 8888:8888 -p 9080:9080 `\n\t    -p 9411:9411 -p 9943:9943 -v ${PWD}\\splunk_config:c:\\splunk_config:RO `\n\t    --name otelcol quay.io/signalfx/splunk-otel-collector-windows:latest\n```\n\n----------------------------------------\n\nTITLE: Using Splunk>VictorOps Bot Configure Command - Microsoft Teams (Markdown)\nDESCRIPTION: Shows the syntax to trigger the Splunk>VictorOps app configuration workflow using the bot in Microsoft Teams. Executing this command opens a card that allows administrators or owners to input configuration details such as API credentials. Prerequisites: user must have admin privileges in both Teams and Splunk On-Call, and the app must be installed. Input is the command itself; output is an embedded card UI. No parameters required.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/microsoft-teams-integration-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n``@Splunk VictorOps configure``\n```\n\n----------------------------------------\n\nTITLE: Example Log Format for Filtering\nDESCRIPTION: Shows an example of a syslog message with severity level 5 that would be filtered out.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-edit.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n*Apr 29 03:02:42: %LINEPROTO-5-UPDOWN: Line protocol on Interface GigabitEthernet0/0, changed state to down\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Receiver for Apache Flink\nDESCRIPTION: YAML configuration for setting up the Prometheus receiver to scrape metrics from Apache Flink. Specifies the job name, scrape interval, and target endpoint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/prometheus-generic.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprometheus/flink:\n   config:\n      scrape_configs:\n         - job_name: 'apache-flink'\n         scrape_interval: 10s\n         static_configs:\n            - targets: ['0.0.0.0:9249']\n```\n\n----------------------------------------\n\nTITLE: Accessing Payload Field in Rules Engine (Generic Syntax)\nDESCRIPTION: Demonstrates the syntax for accessing a field from the alert payload in the Rules Engine. This allows dynamic insertion of alert data into annotations or transformations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-variable-expansion.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n{{field_name}}\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Auto-Instrumentation on RPM-based Systems\nDESCRIPTION: Command to install the Splunk OpenTelemetry auto-instrumentation package on RPM-based systems like RHEL, CentOS, or Fedora.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo rpm -ivh <path to splunk-otel-auto-instrumentation rpm>\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Collector for Windows using PowerShell\nDESCRIPTION: This PowerShell command downloads and executes the installer script for the Splunk Distribution of OpenTelemetry Collector. It requires setting the Splunk realm and access token as parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows.rst#2025-04-22_snippet_0\n\nLANGUAGE: PowerShell\nCODE:\n```\n& {Set-ExecutionPolicy Bypass -Scope Process -Force; $script = ((New-Object System.Net.WebClient).DownloadString('https://dl.signalfx.com/splunk-otel-collector.ps1')); $params = @{access_token = \"SPLUNK_ACCESS_TOKEN\"; realm = \"SPLUNK_REALM\"}; Invoke-Command -ScriptBlock ([scriptblock]::Create(\". {$script} $(&{$args} @params)\"))}\n```\n\n----------------------------------------\n\nTITLE: Converting DER Certificate to PEM Format with OpenSSL in Shell\nDESCRIPTION: Converts a certificate from DER format to PEM format using OpenSSL. This is a prerequisite before uploading the certificate to Splunk Observability Cloud as part of the ADFS integration process. Ensure OpenSSL is installed on the system.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/SSO/sso-ADSF.rst#2025-04-22_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nopenssl x509 -inform der -in certificate.cer -out certificate.pem\n```\n\n----------------------------------------\n\nTITLE: Update and Install Helm Chart for Splunk OpenTelemetry - Bash\nDESCRIPTION: This set of commands demonstrates how to update the repository and install the Splunk Distribution of OpenTelemetry Collector using Helm for an OpenShift environment, setting various parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart\n\n      helm repo update\n\n      helm --namespace=<NAMESPACE> install my-splunk-otel-collector \\\n      --set=\"splunkObservability.realm=<REALM>\" \\\n      --set=\"splunkObservability.accessToken=<ACCESS_TOKEN>\" \\\n      --set=\"distribution=openshift\" \\\n      --set=\"clusterName=<CLUSTER_NAME>\" \\\n      --set=\"agent.enabled=true\" \\\n      --set=\"clusterReceiver.enabled=true\" \\\n      --set=\"gateway.replicaCount=1\" \\\n      splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Activating TLS in PowerShell for OpenTelemetry Collector Installation\nDESCRIPTION: This PowerShell command activates TLS 1.2 in PowerShell, which may be necessary for the installer script to function properly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows.rst#2025-04-22_snippet_1\n\nLANGUAGE: PowerShell\nCODE:\n```\n[Net.ServicePointManager]::SecurityProtocol = [Net.ServicePointManager]::SecurityProtocol -bor [Net.SecurityProtocolType]::Tls12\n```\n\n----------------------------------------\n\nTITLE: Installing Externalized Dependencies for Webpack\nDESCRIPTION: This shell command installs the 'express' package using npm. This step is necessary when using the Webpack 'externals' configuration, as the specified external packages must be present in the 'node_modules' folder for the application's 'require' calls to resolve them correctly at runtime.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/troubleshooting/common-nodejs-troubleshooting.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# Install the library or framework and add it to node_modules\nnpm install express\n```\n\n----------------------------------------\n\nTITLE: Running Python Application without Instrumentation\nDESCRIPTION: Example of a standard Python application command without instrumentation, shown for comparison purposes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython3 main.py --port=8000\n```\n\n----------------------------------------\n\nTITLE: Setting Service Name via Environment Variable (Windows PowerShell)\nDESCRIPTION: Demonstrates how to set the OpenTelemetry service name (`OTEL_SERVICE_NAME`) using an environment variable in a Windows PowerShell environment. This name identifies the service being instrumented within Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/configuration/advanced-java-otel-configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_SERVICE_NAME=my-java-app\n```\n\n----------------------------------------\n\nTITLE: Checking Collector Pod Logs in Kubernetes\nDESCRIPTION: Command to check the logs of the Collector agent pod to identify potential error messages related to missing metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-missing-metrics.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n {namespace} logs {collector-agent-pod-name}\n```\n\n----------------------------------------\n\nTITLE: Downloading Nagios Plugin RPM Package using Shell\nDESCRIPTION: Downloads the Splunk On-Call Nagios plugin RPM package (version 1.4.20) from GitHub releases using the `wget` command. This package is suitable for RPM-based Linux distributions (like CentOS, Fedora, RHEL).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-nagios-1.4.20/victorops-nagios-1.4.20-1.noarch.rpm\n```\n\n----------------------------------------\n\nTITLE: Installing Rails Instrumentation using Bundler in Ruby\nDESCRIPTION: This bash command installs the Rails instrumentation gem using Bundler, adding it to the project's dependencies with a specific version constraint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/ruby-manual-instrumentation.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nbundle add opentelemetry-instrumentation-rails --version \"~> 0.27\"\n```\n\n----------------------------------------\n\nTITLE: Example Elasticsearch Watcher Status Response (JSON)\nDESCRIPTION: This JSON snippet shows an example response from the `_watcher/stats` API endpoint. The key field `\"watcher_state\" : \"started\"` confirms that the Elasticsearch Watcher service is active and running. Other details like the number of configured watches (`watch_count`) and thread pool statistics are also included.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/elasticsearch-watcher-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n\"watcher_state\" : \"started\",\n\"watch_count\" : 5,\n\"execution_thread_pool\" : {\n\"queue_size\" : 0,\n\"max_size\" : 10\n},\n\"manually_stopped\" : false\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Idle CPU Metrics\nDESCRIPTION: Removes data points with 'idle' state label value from system.cpu.usage metric.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: system.cpu.usage\naction: update\noperations:\n    - action: delete_label_value\n      label: state\n      label_value: idle\n```\n\n----------------------------------------\n\nTITLE: Installing Zero-Code Instrumentation for Systemd Services\nDESCRIPTION: Installation command specific for instrumenting applications running as systemd services.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-systemd-instrumentation --realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Using RegEx Capture Group in Rules Engine (Generic Syntax)\nDESCRIPTION: Shows the syntax for using a regular expression capture group in the Rules Engine. This method allows for more complex data extraction and insertion into rules.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-variable-expansion.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n{{\\n}}\n```\n\n----------------------------------------\n\nTITLE: Changing Routing Key with Wildcard Match in Splunk On-Call Rules Engine\nDESCRIPTION: This rule changes the routing key for alerts related to a specific host (db03) to send them to the Development team instead of the Database team.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-transformations.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nWhen entity_id matches db03* using Wildcard Match\n\nSet routing_key to devs\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Template for Catchpoint-Splunk On-Call Integration\nDESCRIPTION: This JSON template is used to configure the alert webhook in Catchpoint for integration with Splunk On-Call. It defines the structure of the alert payload, including message type, monitoring tool, entity details, and state message.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/catchpoint-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"message_type\":\":math:`{switch('`\\ {notificationLevelId}','0','warning','1','critical','2','recovery','3','recovery')}\",\n\"monitoring_tool\": \"Catchpoint\", \"entity_display_name\": \"Catchpoint\nAlert for ${testName} on node :math:`{nodeDetails(`\\ {nodeName})}\",\n\"entity_id\": \":math:`{testId}\\_`\\ {AlertInitialTriggerDateUtcEpoch}\",\n\"state_message\": \"Alert Type\nID-:math:`{alertTypeId}, Test Type ID-`\\ {testTypeId},\nNode-:math:`{nodeDetails('`\\ {nodeName}')}, Product-\n:math:`{productId}, Test\\_url-`\\ {testUrl}\" }\n```\n\n----------------------------------------\n\nTITLE: SSL Handshake Error Log for OTLP Channel Pipeline\nDESCRIPTION: Shows error logs when Python agent attempts to send trace data to Splunk Observability Cloud ingest endpoint using OTLP, which is not supported.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/troubleshooting/common-python-troubleshooting.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nE0908 16:23:32.337704280    5881 ssl_transport_security.cc:1468] Handshake failed with fatal error SSL_ERROR_SSL: error:10000095:SSL routines:OPENSSL_internal:ERROR_PARSING_EXTENSION.\nE0908 16:23:32.556405854    5881 ssl_transport_security.cc:1468] Handshake failed with fatal error SSL_ERROR_SSL: error:10000095:SSL routines:OPENSSL_internal:ERROR_PARSING_EXTENSION.\n```\n\n----------------------------------------\n\nTITLE: Component Alert Trigger Payload\nDESCRIPTION: Partial JSON payload template for triggering component-based alerts in SolarWinds.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/solarwinds-integration.rst#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message_type\":\"CRITICAL\",\n  \"monitor_name\":\"SolarWinds\",\n  \"monitoring_tool\":\"SolarWinds\",\n  \"alert_rule\":\"${N=Alerting;M=AlertName}\",\n  \"state_message\":\"${NodeName} ${N=SwisEntity;M=ComponentAlert.ComponentName} is ${N=SwisEntity;M=Status;F=Status}\"\n```\n\n----------------------------------------\n\nTITLE: Trust Relationships for AWS Metric Streams Role\nDESCRIPTION: Trust relationships configuration for the AWS role used by Metric Streams. It allows the CloudWatch Metric Streams service to assume this role.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-ts-ms-aws.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"streams.metrics.cloudwatch.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Ignoring Payment URLs in JavaScript\nDESCRIPTION: This JavaScript snippet shows how to configure the `ignoreUrls` setting to drop any URLs containing the '/payment/' path, which may involve sensitive transactions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/sensitive-data-rum.rst#2025-04-22_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nignoreUrls: [/\\/payment\\//]\n```\n\n----------------------------------------\n\nTITLE: Example Dimension Value Illustrating Data Source Identification\nDESCRIPTION: This snippet shows an example dimension key-value pair (`environment:lab`). It is used within the text to illustrate how the dimension report can help identify data sources, specifically highlighting that users might discover data being sent from environments (like a lab) that they potentially do not want to monitor using Infrastructure Monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/monitor-imm-billing-usage.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nenvironment:lab\n```\n\n----------------------------------------\n\nTITLE: Modifying Splunk OpenTelemetry Collector Service Parameters in Windows Registry\nDESCRIPTION: This PowerShell command updates the ImagePath registry value to modify command line options for the Splunk OpenTelemetry Collector service. It provides a template for customizing service execution parameters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/windows-config.rst#2025-04-22_snippet_1\n\nLANGUAGE: PowerShell\nCODE:\n```\nSet-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\splunk-otel-collector\" -name \"ImagePath\" -value \"C:\\Program Files\\Splunk\\OpenTelemetry Collector\\otelcol.exe OPTIONS\"\n```\n\n----------------------------------------\n\nTITLE: Adding Custom CA Certificates to Private Runner\nDESCRIPTION: Command to run a private runner container with custom CA certificates mounted from the host.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_43\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -e \"RUNNER_TOKEN=<insert-token>\" --volume=`pwd`/certs:/usr/local/share/ca-certificates/my_certs/ quay.io/signalfx/splunk-synthetics-runner:latest bash -c \"sudo update-ca-certificates && bundle exec bin/start_runner\"\n```\n\n----------------------------------------\n\nTITLE: RST Document Structure for Synthetic Monitoring Documentation\nDESCRIPTION: ReStructuredText markup defining the document structure for Splunk Synthetic Monitoring documentation, including title, meta description, and section headers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/intro-synthetics.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _intro-synthetics:\n\n********************************************\nIntroduction to Splunk Synthetic Monitoring\n********************************************\n\n.. meta::\n    :description: Monitor the performance of your web pages and applications by running synthetic Browser, Uptime, and API tests with Splunk Synthetic Monitoring.\n```\n\n----------------------------------------\n\nTITLE: Listing GovCloud AWS Regions for Splunk Observability Cloud\nDESCRIPTION: This snippet enumerates the GovCloud AWS regions supported by Splunk Observability Cloud, including their region codes and corresponding geographical locations. It also mentions limitations specific to GovCloud regions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-prereqs.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``us-gov-east-1``: AWS GovCloud (US-East)\n* ``us-gov-west-1``: AWS GovCloud (US-West)\n```\n\n----------------------------------------\n\nTITLE: Splunk Search Query for Index1\nDESCRIPTION: Search query to view logs from the logging1 service in Splunk Web UI.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/deploy-verify-environment.rst#2025-04-22_snippet_2\n\nLANGUAGE: splunk\nCODE:\n```\nindex=index1\n```\n\n----------------------------------------\n\nTITLE: Downloading VictorOps Nagios Plugin (RPM)\nDESCRIPTION: Uses `wget` to download the `.rpm` package file for the VictorOps Nagios plugin (compatible with Icinga) from the specified GitHub release URL. This is the first step for installing the plugin on RPM-based systems like CentOS or RHEL.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-nagios-1.4.20/victorops-nagios-1.4.20-1.noarch.rpm\n```\n\n----------------------------------------\n\nTITLE: Creating Architecture Diagram with Mermaid Flowchart\nDESCRIPTION: A Mermaid flowchart diagram illustrating the Splunk Observability Cloud architecture with four main components: data collection, data ingestion, data processing and retention, and analytics. The diagram shows how data flows through the system and details each component's function.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/o11y-architecture.rst#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#FFFFFF',\n      'primaryTextColor': '#000000',\n      'primaryBorderColor': '#000000',\n      'nodeBorder':'#000000',\n      'lineColor': '#000000',\n      'fontSize': '22px',\n    }\n  }\n}%%\n\nflowchart LR\n\n    accTitle: Splunk Observability Cloud architecture diagram\n    accDescr: Splunk Observability Cloud architecture can be broken down into 4 main components, data collection, data ingestion, data procesisng and retention, and analytics. Splunk Observability Cloud uses OpenTelemetry as the default method of data collection, which gives you a single set of instrumentation across different data types, such as distributed traces and metrics. You can also send Splunk Enterprise or Splunk Cloud Platform logs to Splunk Observability Cloud with the use of Log Observer Connect. Once you get your data in, OpenTelemetry Collector can aggregate, parse, extract, enrich, or delete your data as needed. The underlying mechanism for data ingestion is the Quantizer, which offers rollups and dynamic lag adjustment. Trace assembly and metadata extraction are also parts of data ingestion. Data processing and retention includes trace indexing and storage, trace metricization, as well as metrics routing and storage. Lastly, Splunk Observability Cloud offers various analytics tools for your data, including but not limited to, tracing analysis, predictive analysis, incident analysis, anomaly detection, SignalFlow, and historical baselines.\n    \n    %% LR indicates the direction (left-to-right)\n\n\n    classDef default fill:#FFFFFF, stroke:#000\n    classDef platform fill:#acd1a4, stroke:#000\n    classDef loc fill:#fdf8a4, stroke:#000\n    classDef dataColor fill:#d9d9d9, stroke:#000\n    classDef otelColor fill:#afcedb, stroke:#000\n    classDef ingestionColor fill:#fbc477, stroke:#000\n    classDef processingColor fill:#fab9b4, stroke:#000\n    classDef analyticsColor fill:#f999cb, stroke:#000\n\n    log-->splunkPlatform[(Splunk platform)]:::platform-->logObserver[(Log Observer Connect)]:::loc-->analytics\n    \n    subgraph o11yArchitecture[&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbspSplunk Observability Cloud Architecture]\n    direction LR\n      data-->otel-->ingestion\n      ingestion-->processingRetention-->analytics\n\n      class data dataColor\n      \n      subgraph data[Data sources]\n          direction LR\n          log(Logs)\n          disTrace(Distributed traces)\n          metric(Metrics)\n      end \n      \n      class otel otelColor\n\n      subgraph otel[OpenTelemetry Collector]\n          direction LR\n          aggregate((aggregate))\n          parse((parse, extract, enrich))\n          delete((delete))\n      end\n\n      class ingestion ingestionColor\n\n      subgraph ingestion[Ingestion]\n          direction LR \n          traceAssembly(Trace assembly)\n          quantizer(Quantizer)---rollups(Rollups)\n          quantizer---lagAdjust(Dynamic lag adjustment)\n          metadataExtraction(Metadata extraction)\n      end\n\n      class processingRetention processingColor\n\n      subgraph processingRetention[Processing and retention]\n          direction LR \n          indexStorage(Trace indexing and storage)\n          traceMetricization(Trace metricization)\n          metricsManagement(Metrics routing and storage)\n      end\n\n      class analytics analyticsColor\n\n      subgraph analytics[Analytics]\n          direction LR \n          traceAnalyis(Tracing analysis)\n          predictiveAnalysis(Predictive analytics)\n          incidentAnalysis(Incident analysis)\n          anommalyDetection(Anomaly detection)\n          signalflow(SignalFlow)\n          historicalBaseline(Historical baselines)\n      end\n\n    end\n```\n\n----------------------------------------\n\nTITLE: Installing Minikube, Helm, and Podman using Homebrew\nDESCRIPTION: Command to install Minikube (local Kubernetes cluster), Helm (Kubernetes package manager), and Podman (container runtime) using Homebrew on macOS.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/collector-config-tutorial-start.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbrew install minikube helm podman\n```\n\n----------------------------------------\n\nTITLE: Extracting Splunk OpenTelemetry Collector Tar Archive\nDESCRIPTION: Command to extract the tar.gz archive of the Splunk OpenTelemetry Collector distribution.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ntar xzf splunk-otel-collector_<version>_<arch>.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Defining DevOps Contact Group Configuration\nDESCRIPTION: Contact group configuration for DevOps team that defines the routing key used in Splunk On-Call alerts.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndefine contactgroup{\ncontactgroup_name         devops\nalias                     VictorOps DevOps contact group\nmembers                   VictorOps_devops\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Discovery Progress via kubectl\nDESCRIPTION: Shell command to verify discovery progress and statement evaluations using kubectl and grep commands\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-third-party.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ kubectl -n monitoring logs splunk-otel-collector-agent | grep -i disco\nDiscovering for next 10s...\nSuccessfully discovered \"postgresql\" using \"k8s_observer\" endpoint \"k8s_observer/e8a10f52-4f2a-468c-be7b-7f3c673b1c8e/(5432)\".\nDiscovery complete.\n```\n\n----------------------------------------\n\nTITLE: Submitting Support Ticket Detail Block - Bash\nDESCRIPTION: This Bash-formatted snippet serves as a template for users to copy and paste when submitting a support ticket to Splunk Support. It collects vital connection details (OrgID, Realm, Instance Name) and requests that the management port (8089) be opened and specific IP addresses be added to the instance's allow list for Log Observer Connect integration. No dependencies are required, as this snippet is intended for textual input within a case description (not for execution). Values surrounded by angle brackets should be replaced with organization-specific information before submitting.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/logs/scp.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOrgID: <enter-orgid>\\nRealm: <enter-realm>\\nInstance Name: <instance-name>\\nRequest: Please securely open our Splunk Cloud Platform instance management port (8089) and add the IP addresses of the above realm to our allow list so that we can enable Log Observer Connect.\n```\n\n----------------------------------------\n\nTITLE: Failure Response Example - Splunk On-Call REST Endpoint - JSON\nDESCRIPTION: Shows a failure response format explaining the error reason for a failed alert or incident request to Splunk On-Call. Parsing requires basic JSON support. The structure includes a 'result' status and a 'message' string describing the error, such as missing required fields; useful for debugging integration issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rest-endpoint-integration-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \\u201cresult\\u201d:\\u201cfailure\\u201d, \\u201cmessage\\u201d:\\u201cMissing fields: message_type\\u201d }\n```\n\n----------------------------------------\n\nTITLE: Defining the VictorOps Command Poll Service in Nagios Configuration\nDESCRIPTION: This configuration snippet defines the 'VictorOps Command Poll' service within the Nagios `victorops.cfg` file. This service is used by the Ack-Back feature to retrieve alert acknowledgement commands from Splunk On-Call. To enable Ack-Back, the `active_checks_enabled` setting within this definition must be changed from 0 to 1.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/ack-back.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndefine service \nactive_checks_enabled   0 \nuse    VictorOps_Service \nservice_description    VictorOps Command Poll \ncheck_command    check_victorops_cmds ...\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Collector with PowerShell\nDESCRIPTION: PowerShell command to install the Splunk Distribution of OpenTelemetry Collector using the MSI installer. The command specifies the path to the MSI file and runs it with quiet mode enabled.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows-msi.rst#2025-04-22_snippet_0\n\nLANGUAGE: PowerShell\nCODE:\n```\nStart-Process -Wait msiexec \"/i PATH_TO_MSI /qn\"  \n```\n\n----------------------------------------\n\nTITLE: Testing TLS 1.2 Connection with wget on Linux (Bash)\nDESCRIPTION: Demonstrates how to test TLS 1.2 connectivity using the `wget` command on Linux systems. This command attempts to fetch content from `www.google.com` explicitly using the TLS 1.2 protocol to verify client capability. Requires the `wget` utility to be installed.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/admin/get-started/tls-security-protocol.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget -secure-protocol=TLSv1_2 https://www.google.com\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Agent for Data Forwarding Mode\nDESCRIPTION: Example configuration for the OpenTelemetry Collector agent when operating in data forwarding mode (gateway). Shows how to forward host metrics and traces to a gateway collector while maintaining Related Content functionality.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/relatedcontent-collector-apm.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  hostmetrics:\n    collection_interval: 10s\n    scrapers:\n      cpu:\n      disk:\n      filesystem:\n      memory:\n      network:\n\nprocessors:\n  resourcedetection:\n    detectors: [system,env,gcp,ec2]\n    override: true\n  resource/add_environment:\n    attributes:\n      - action: insert\n        value: staging\n        key: deployment.environment\n\nexporters:\n  # Traces\n  otlp:\n    endpoint: \"${SPLUNK_GATEWAY_URL}:4317\"\n    tls:\n      insecure: true\n  # Metrics + Events + APM correlation calls\n  signalfx:\n    access_token: \"${SPLUNK_ACCESS_TOKEN}\"\n    api_url: \"http://${SPLUNK_GATEWAY_URL}:6060\"\n    ingest_url: \"http://${SPLUNK_GATEWAY_URL}:9943\"\n\nservice:\n  extensions: [health_check, http_forwarder, zpages]\n  pipelines:\n    traces:\n      receivers: [jaeger, zipkin]\n      processors: [memory_limiter, batch, resourcedetection, resource/add_environment]\n      exporters: [otlp, signalfx]\n    metrics:\n      receivers: [hostmetrics]\n      processors: [memory_limiter, batch, resourcedetection]\n      exporters: [otlp]\n```\n\n----------------------------------------\n\nTITLE: Checking Log Generation on Windows\nDESCRIPTION: PowerShell command to verify if the source is generating logs on Windows systems\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/troubleshoot-logs.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nGet-Content myTestLog.log\n```\n\n----------------------------------------\n\nTITLE: Upgrading OpenTelemetry Collector using DNF on RPM-based Linux Systems\nDESCRIPTION: Command to upgrade the Splunk OpenTelemetry Collector on RPM-based systems using the dnf package manager. Requires root privileges and preserves modified configuration files.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-upgrade.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo dnf upgrade splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Converting ADFS Certificate from CER to PEM Format\nDESCRIPTION: Command to convert an ADFS certificate from .cer format to .pem format using the OpenSSL tool. This conversion is necessary for uploading the certificate to Splunk Observability Cloud for the SSO integration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/SSO/sso-ADSF.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nopenssl x509 -inform der -in certificate.cer -out certificate.pem\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk Observability EKS Add-on without Secure Token Handling\nDESCRIPTION: YAML configuration for Splunk Observability Cloud with the access token embedded directly in the configuration. This less secure approach includes the token as plain text in the EKS Add-on configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s-addon-eks.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkObservability:\n    accessToken: <YOUR_ACCESS_TOKEN>\n    realm: <REALM>\nclusterName: <EKS_CLUSTER_NAME>\ncloudProvider: aws\ndistribution: eks\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response for AWS Instance Metadata\nDESCRIPTION: An example JSON response structure showing key AWS instance metadata fetched using cURL. Main fields like 'instanceId', 'region', and 'accountId' are critical for uniquely identifying AWS instances.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/monitor/aws.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"devpayProductCodes\" : null,\n   \"privateIp\" : \"10.1.15.204\",\n   \"availabilityZone\" : \"us-east-1a\",\n   \"version\" : \"2010-08-31\",\n   \"accountId\" : \"134183635603\",\n   \"instanceId\" : \"i-a99f9802\",\n   \"billingProducts\" : null,\n   \"instanceType\" : \"c3.2xlarge\",\n   \"pendingTime\" : \"2015-09-02T16:45:40Z\",\n   \"imageId\" : \"ami-2ef44746\",\n   \"kernelId\" : null,\n   \"ramdiskId\" : null,\n   \"architecture\" : \"x86_64\",\n   \"region\" : \"us-east-1\"\n}\n```\n\n----------------------------------------\n\nTITLE: Identifying Monitoring MetricSets Cardinality in Splunk APM\nDESCRIPTION: Splunk Observability Cloud metric identifier (`sf.org.apm.numMonitoringMetricSets`) representing the cardinality of Monitoring MetricSets (MMS), specifically for histogram MMS. Data is available at 10-minute intervals with a 1-hour look-back period. This metric appears in the 'Monitoring MetricSets' chart for both TAPM and host-based subscription plans.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/apm-billing-usage-index.rst#2025-04-22_snippet_3\n\nLANGUAGE: metric\nCODE:\n```\nsf.org.apm.numMonitoringMetricSets\n```\n\n----------------------------------------\n\nTITLE: Configuring ASP.NET Monitor Receiver\nDESCRIPTION: Basic configuration example showing how to activate the ASP.NET monitor receiver in the Collector configuration. Includes receiver definition and pipeline configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-languages/asp-dot-net.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/aspdotnet:\n    type: aspdotnet\n    ...  # Additional config\n```\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/aspdotnet]\n```\n\n----------------------------------------\n\nTITLE: Converting CPU Usage to Double Type\nDESCRIPTION: Changes the data type of system.cpu.usage metric from integer to double.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: system.cpu.usage\naction: update\noperations:\n    - action: toggle_scalar_data_type\n```\n\n----------------------------------------\n\nTITLE: Embedding Network Explorer Dimensions List from GitHub YAML (HTML)\nDESCRIPTION: This HTML snippet employs a `div` element with specific attributes (`class=\"metrics-yaml\"`, `url`) to dynamically include the list of dimensions applicable to Network Explorer metrics. The dimension definitions are sourced from a YAML file hosted at the provided URL in the `open-telemetry/opentelemetry-ebpf` GitHub repository. Similar to the metrics snippet, this requires custom processing by the documentation system to fetch and display the YAML data.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-metrics.rst#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"metrics-yaml\" url=\"https://raw.githubusercontent.com/open-telemetry/opentelemetry-ebpf/main/docs/metrics/dimensions.yaml\"></div>\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: ReStructuredText documentation defining the tutorial structure, prerequisites, and navigation for setting up container log collection with Splunk OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/about-logs-collector-splunk-tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _about-logs-collector-splunk-tutorial:\n\n***********************************************************************\nTutorial: Use the Collector to send container logs to Splunk Enterprise\n***********************************************************************\n\n.. meta::\n   :description: Learn how to use the Splunk distribution of the OpenTelemetry Collector to send Docker container logs to a Splunk Enterprise instance.\n\n.. toctree::\n   :hidden:\n   :maxdepth: 4\n\n   docker-compose.rst\n   collector-splunk.rst\n   deploy-verify-environment.rst\n```\n\n----------------------------------------\n\nTITLE: Defining Database Contact Group Configuration\nDESCRIPTION: Contact group configuration for Database team that defines the routing key used in Splunk On-Call alerts.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndefine contactgroup{\ncontactgroup_name         database\nalias                     VictorOps Database contact group\nmembers                   VictorOps_database\n}\n```\n\n----------------------------------------\n\nTITLE: Slack Webhook URL Template\nDESCRIPTION: Example of Slack webhook URL format with dynamic field for routing\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/slack-integration-guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nhttps://hooks.slack.com/services/TCUG253D8/B07G6SF7X8P/${{ALERT.slackwebhook-field}}\n```\n\n----------------------------------------\n\nTITLE: Dynamic API Key and Routing Key Configuration in Splunk Search\nDESCRIPTION: Example format for setting API key and routing key values dynamically within a Splunk search query. Used to configure alert parameters dynamically.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/splunk-integration-guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n<alert search> | eval 'param.api_key'=\"xxxxxxxxxx\" | eval 'param.routing_key'=\"xxx\"\n```\n\n----------------------------------------\n\nTITLE: SNMP Discovery Rule Configuration in YAML\nDESCRIPTION: Configuration example demonstrating SNMP monitor setup with a container discovery rule\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/snmp.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/snmp:\n    type: telegraf/snmp\n    discoveryRule: container_name =~ \"snmp\" && port == 161\n    version: 2\n    community: \"public\"\n      fields:\n        name: \"uptime\"\n        oid: \".1.3.6.1.2.1.1.3.0\"\n```\n\n----------------------------------------\n\nTITLE: Including Splunk On-Call Configuration in Nagios/Icinga\nDESCRIPTION: Adds a `cfg_file` directive to the main Icinga configuration file (e.g., `icinga.cfg` or `nagios.cfg`), pointing to the location of the `victorops.cfg` file provided by the plugin. This instructs Icinga to load the Splunk On-Call specific contact, service, and command definitions. The example path `/usr/local/nagios/etc/victorops.cfg` should be adjusted based on the actual installation location.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\ncfg_file=/usr/local/nagios/etc/victorops.cfg\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Clients in OpenTelemetry Collector YAML\nDESCRIPTION: This YAML configuration example demonstrates how to set up HTTP clients in OpenTelemetry Collector. Key parameters include 'endpoint' for the server connection, 'tls' for securing communications, and custom 'headers'. The 'compression' field specifies the compression type and 'cookies' enable storing server response cookies.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/common-config/collector-common-config-http.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporter:\n  otlphttp:\n    endpoint: otelcol2:55690\n    auth:\n      authenticator: some-authenticator-extension\n    tls:\n      ca_file: ca.pem\n      cert_file: cert.pem\n      key_file: key.pem\n    headers:\n      test1: \"value1\"\n      \"test 2\": \"value 2\"\n    compression: zstd\n    cookies:\n      enabled: true\n```\n\n----------------------------------------\n\nTITLE: Defining SLO Event Streams from Synthetics Data using SignalFlow (Python)\nDESCRIPTION: This SignalFlow program defines two data streams for calculating an SLO based on Synthetics check results. The variable 'G' captures the count of successful runs ('synthetics.run.count') for a specific test ('Monitoring Services - Emby check') by filtering on the 'success' dimension being 'true'. The variable 'T' captures the total count of runs for the same test. These streams represent 'good events' and 'total events' respectively, used as the numerator and denominator for a custom metric SLO.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/slo/custom-metric-scenario.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nG = data('synthetics.run.count', filter=filter('test', 'Monitoring Services - Emby check') and filter('success', 'true'))\nT = data('synthetics.run.count', filter=filter('test', 'Monitoring Services - Emby check'))\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy for Private Runner\nDESCRIPTION: Docker run command with proxy environment variables for environments with restricted direct internet access.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_44\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --cap-add NET_ADMIN -e \"RUNNER_TOKEN=*****\" -e \"no_proxy=.signalfx.com,.amazonaws.com,127.0.0.1,localhost\" -e \"https_proxy=http://172.17.0.1:1234\" -e \"http_proxy=http://172.17.0.1:1234\" quay.io/signalfx/splunk-synthetics-runner:latest\n```\n\n----------------------------------------\n\nTITLE: reStructuredText Directive to Mark Start of Included Content\nDESCRIPTION: This reStructuredText directive inserts raw HTML to define a starting point marker (`<div>` with specific ID) for content included from another file (`/_includes/requirements/php.rst`). It's used by the documentation build system.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-otel-requirements.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"requirements/php.rst\"></div>\n```\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"include-start\" id=\"requirements/php.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Starting Python Application with Splunk OTel Instrumentation\nDESCRIPTION: Example command to start a Python application using the Splunk OpenTelemetry instrumentation after migration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/troubleshooting/migrate-signalfx-python-agent-to-otel.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsplunk-py-trace python main.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Varnish Receiver - YAML Configuration\nDESCRIPTION: Basic YAML configuration to activate the Varnish monitor in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/varnish.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/varnish:\n    type: telegraf/varnish\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Calculating Burn Rate Threshold for Alerting (Math Notation)\nDESCRIPTION: This formula calculates the burn rate threshold used by Splunk Observability Cloud's multiwindow alerting mechanism. It determines the rate at which the error budget is consumed relative to the SLO compliance window and the configured long monitoring window. Key inputs include the SLO compliance window duration (in hours), the percentage of error budget consumed that should trigger an alert, and the duration of the long window (in hours).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/slo/burn-rate-alerts.rst#2025-04-22_snippet_1\n\nLANGUAGE: math\nCODE:\n```\n\\text{Burn rate threshold} = \\frac{\\text{SLO compliance window (in hours) * Error budget consumed}}{\\text{Long window (in hours) * 100%}}\n```\n\n----------------------------------------\n\nTITLE: Upgrading Splunk OTel Auto-Instrumentation on RPM-based Systems\nDESCRIPTION: These commands upgrade the splunk-otel-auto-instrumentation package on RPM-based systems using different package managers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum upgrade splunk-otel-auto-instrumentation\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo dnf upgrade splunk-otel-auto-instrumentation\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo zypper refresh\nsudo zypper update splunk-otel-auto-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Ruby Agent in Linux\nDESCRIPTION: Shell commands for configuring essential environment variables for the Ruby agent in Linux, including service name, collector endpoint, and resource attributes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/instrument-ruby-application.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_SERVICE_NAME=<yourServiceName>\n```\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_EXPORTER_OTLP_ENDPOINT=<yourCollectorEndpoint>:<yourCollectorPort>\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_RESOURCE_ATTRIBUTES='deployment.environment=<envtype>,service.version=<version>'\n```\n\n----------------------------------------\n\nTITLE: reStructuredText Directive to Mark End of Included Content\nDESCRIPTION: This reStructuredText directive inserts raw HTML to define an ending point marker (`<div>` with specific ID) for content included from another file (`/_includes/requirements/php.rst`). It complements the `include-start` marker.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-otel-requirements.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"requirements/php.rst\"></div>\n```\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"include-stop\" id=\"requirements/php.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Nomad Environment Configuration\nDESCRIPTION: Environment variable configuration block for the Nomad job specification, including Splunk access tokens and memory settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-nomad.rst#2025-04-22_snippet_1\n\nLANGUAGE: none\nCODE:\n```\nenv {\n  SPLUNK_ACCESS_TOKEN = \"<SPLUNK_ACCESS_TOKEN>\"\n  SPLUNK_REALM = \"<SPLUNK_REALM>\"\n  SPLUNK_MEMORY_TOTAL_MIB = 2048\n  // You can specify more environment variables to override default values.\n}\n```\n\n----------------------------------------\n\nTITLE: Alert Rules Engine Configuration\nDESCRIPTION: Example of routing key configuration in Alert Rules Engine\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/slack-integration-guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nWhen routing_key matches test\nSet slackwebhook-field to new value X8VM8fMXYoJYgEcupBWFmSD7\n```\n\n----------------------------------------\n\nTITLE: Enabling Profiling via Helm Chart - YAML - bash\nDESCRIPTION: Demonstrates enabling the profiling functionality in the Splunk OpenTelemetry Collector Helm chart via the command line. Requires Helm installed and access to modify values during chart installation. The --set flag directly sets the splunkObservability.profilingEnabled property to 'true', turning on profiling. Input: none, Output: Helm will deploy the collector with profiling enabled. Ensure Helm and the correct chart version are used.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/profiling/get-data-in-profiling.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n--set splunkObservability.profilingEnabled='true'\n```\n\n----------------------------------------\n\nTITLE: Configuring Session-Based Sampling with CDN in HTML\nDESCRIPTION: This snippet demonstrates how to configure the Splunk RUM agent to collect data from 50% of sessions using the CDN distribution. It shows the usage of SplunkRum.SessionBasedSampler with a ratio of 0.5.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/configure-rum-browser-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\" crossorigin=\"anonymous\"></script>\n<script>\n  SplunkRum.init({\n    realm: '<realm>',\n    rumAccessToken: '<your_rum_token>',\n    applicationName: '<application-name>',\n    tracer: {\n      sampler: new SplunkRum.SessionBasedSampler({\n      ratio: 0.5\n      }),\n    },\n  });\n</script>\n```\n\n----------------------------------------\n\nTITLE: Using Splunk On-Call Variables in Webhook Payloads\nDESCRIPTION: Demonstrates the syntax for embedding dynamic Splunk On-Call variables within the custom payload of an outbound webhook. Variables must be enclosed in `${{...}}`. Examples show a generic variable, the incident alert count, and accessing a specific alert field.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/admin/get-started/custom-outbound-webhooks.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n${{variable_name}}\n```\n\nLANGUAGE: text\nCODE:\n```\n${{STATE.ALERT_COUNT}}\n```\n\nLANGUAGE: text\nCODE:\n```\n${{ALERT.field_name}}\n```\n\n----------------------------------------\n\nTITLE: Uninstalling only the Collector package on Debian-based Linux systems\nDESCRIPTION: Command for uninstalling just the Splunk OpenTelemetry Collector package on Debian-based Linux systems using apt-get, leaving Fluentd (td-agent) installed if present.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-uninstall.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get purge splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Setting Recovery State for AWS CloudWatch Integration in JSON\nDESCRIPTION: This JSON snippet shows how to modify the NewStateValue field to send a recovery signal to Splunk On-Call. Changing the value to 'OK' indicates that the alarm has been resolved.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/aws-cloudwatch-integration-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"NewStateValue\":\"OK\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Consumer Monitor in Splunk Collector\nDESCRIPTION: This YAML configuration activates the Kafka consumer integration and adds it to the metrics pipeline in the Splunk OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/kafka-consumer.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/ collectd/kafka_consumer:\n    type: collectd/kafka_consumer\n    ... # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/collectd/kafka_consumer]\n```\n\n----------------------------------------\n\nTITLE: Monitoring Dropped Spans due to Invalid Size in Splunk RUM (Metric)\nDESCRIPTION: This metric measures the number of spans dropped because their individual size exceeded the 128kB limit. It helps identify issues with oversized RUM events or spans being sent.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-rum.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.rum.numSpansDroppedInvalid\n```\n\n----------------------------------------\n\nTITLE: Specifying Start URL Template for Splunk On-Call SSO in Google Apps (Text)\nDESCRIPTION: Provides the Start URL template (`https://portal.victorops.com/auth/sso/<<org-slug-here>>`) to be entered in the Google Apps SAML configuration for Splunk On-Call. The placeholder `<<org-slug-here>>` must be replaced with the actual organization's slug. This URL can be used by end-users to initiate the SSO login flow from the Identity Provider side. This value is entered during step 6 of the configuration process.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/admin/sso/sp-sso-google.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhttps://portal.victorops.com/auth/sso/<<org-slug-here>>\n```\n\n----------------------------------------\n\nTITLE: Including zPages Extension in Service Section - OpenTelemetry Collector - YAML\nDESCRIPTION: This YAML snippet shows how to enable the zPages extension for the collector service by including it in the 'service.extensions' list. It should be added alongside other active extensions in your main configuration YAML. No additional dependencies besides the existence of a valid zPages extension definition are needed. The input is a YAML configuration file; the output is that zPages will be recognized as an enabled extension component at Collector startup. Ensure the 'zpages' identifier matches the key in your 'extensions' section. This configuration is mandatory for zPages to be activated by the collector process.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/zpages-extension.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  extensions: [zpages]  \n```\n\n----------------------------------------\n\nTITLE: Managing the Collector systemd Service\nDESCRIPTION: Commands to restart, check status of, and view logs for the Splunk OTel Collector systemd service after installation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl restart splunk-otel-collector\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl status splunk-otel-collector\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo journalctl -u splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Docker Image Verification Public Key (v0.93+)\nDESCRIPTION: Public key used to verify Docker images of the Collector for versions 0.93 or higher using cosign verification.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/install-the-collector.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n-----BEGIN PUBLIC KEY-----\nMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAnQi0COAQC5XgALFAyTW3\nJXiHvXJLIFKK+LKS8iVo0Ec2tsABJ5usp5yCYTENJHoS1fLC5XcY5nyM4fqxjWJK\n3FqDa/inWkryNgpW8Mx5LhjGiIxiBiMnONFh0cZNctbQ7mNTBZgisiwThDTOovtW\n660MCCeZdPAMdSHaDym7GWAQi1tVWMioI2r9s5DUbwbzK5z9z/HZuX9Su2KJxxG6\nTagdZB6EyhkdyV/LR1ud5R/5P2ouRt/DpIj/iSRnkTV28wDRSf//QR75SiyDW2zo\nph5MmkD88H5aTw22cJ35sFo3S+NLxakrQZzyH1G4oY6vpQ8h0hMYJ9zAJZxA/kzN\nmLZ/V4QVj8tkJaj7igcOKpfatQUu7n6HapCLhoNAcrnDskf23V4PxUJ+MIAN2vwP\nSUMTI2rKrEPpilAKup4l7EsxX2Dm73umh/xyWaKlpw8kAsB9dLuSj3gnh/k3SX6q\nwWkQASkbjBRe+iPrkVcRNfOvIHp/bg8kd5q4JwEIDh4x/JEf/l6zLpEar8hh2dSW\nVbbHBd5Xo9Ge5BjwXOcoDhvUQqNJdLBJruhvhn7Ogy5Paw5TGhdawfjxT2yXeqbE\nJuv6qdo/mSimkpR8lkQT7OsfAQbCPeyFvZKb22hXj6tCTVJncwCJLe/FBdXJhRep\nY6NEdmKZLGodXv4zLNVr7SkCAwEAAQ==\n-----END PUBLIC KEY-----\n```\n\n----------------------------------------\n\nTITLE: Defining Kubernetes Deployment and Service for Grafana (YAML)\nDESCRIPTION: Kubernetes manifest (deploy.yaml) defining a Deployment and a NodePort Service. The Deployment runs the custom Grafana Docker image (containing the Splunk plugin) and mounts a ConfigMap for data source provisioning. The Service exposes the Grafana instance on port 32001 of the cluster nodes. Requires replacement of `<imageName>:<tagName>` with the actual image details.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: splunk-grafana\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: splunk-grafana\n  template:\n    metadata:\n      name: splunk-grafana\n      labels:\n        app: splunk-grafana\n    spec:\n      containers:\n      - name: grafana-plugin-container\n        image: docker.io/<imageName>:<tagName>\n        ports:\n        - name: grafana\n          containerPort: 3000\n        volumeMounts:\n        - mountPath: /etc/grafana/provisioning/datasources\n          name: splunk-basic-cm\n          readOnly: false\n      volumes:\n      - name: splunk-basic-cm\n        configMap:\n          defaultMode: 420\n          name: splunk-basic-cm\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: splunk-grafana-service\n  namespace: default\nspec:\n  selector:\n    app: splunk-grafana\n  type: NodePort \n  ports:\n  - port: 3000\n    targetPort: 3000\n    nodePort: 32001\n```\n\n----------------------------------------\n\nTITLE: Deleting Kubernetes Resources\nDESCRIPTION: Command to delete Kubernetes resources using a source file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo kubectl delete -f <file-name>\n```\n\n----------------------------------------\n\nTITLE: Splunk Search Query for Index2\nDESCRIPTION: Search query to view logs from the logging2 service in Splunk Web UI.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/deploy-verify-environment.rst#2025-04-22_snippet_3\n\nLANGUAGE: splunk\nCODE:\n```\nindex=index2\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OTel Collector Proxy via PowerShell (Windows)\nDESCRIPTION: Sets system-wide environment variables for HTTP/HTTPS/NO_PROXY, configures WinHTTP proxy settings using `netsh`, and sets registry keys for Internet Settings proxy (used for downloads) on Windows using PowerShell commands. These commands configure proxy settings for both the collector's runtime communication and file downloads.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/allow-services.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Set proxy settings for Collector communications\n\n[Environment]::SetEnvironmentVariable(“http_proxy”,”http://<proxy.address:port>”,”Machine”)\n[Environment]::SetEnvironmentVariable(\"https_proxy\",\"http://<proxy.address:port>\",\"Machine\")\n[Environment]::SetEnvironmentVariable(\"no_proxy\",\"<address>\",\"Machine\")\nnetsh winhttp set proxy \"http://<proxy.address:port>\"\n\n# Set proxy settings to download Collector files\n\nSet-ItemProperty -Path 'HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings' -name ProxyServer -Value \"http://<proxy.address:port>\"\nSet-ItemProperty -Path 'HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings' -name ProxyEnable -Value 1\n```\n\n----------------------------------------\n\nTITLE: Moving Nagios Plugin Configuration File using Shell\nDESCRIPTION: Moves the Splunk On-Call (VictorOps) specific Nagios configuration file (`victorops.cfg`) from the default plugin installation directory (`/opt/victorops/nagios_plugin/nagios_conf/`) to the standard Nagios configuration directory (`/usr/local/nagios/etc`). The target directory might need adjustment based on the specific Nagios installation path. Requires appropriate permissions, potentially `sudo`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmv /opt/victorops/nagios_plugin/nagios_conf/victorops.cfg /usr/local/nagios/etc\n```\n\n----------------------------------------\n\nTITLE: Activating Debug Logging Programmatically in Node.js\nDESCRIPTION: This JavaScript snippet demonstrates how to programmatically activate debug logging for the Splunk Distribution of OpenTelemetry JS by setting the 'logLevel' property to 'debug' within the 'start' function's configuration object. This is an alternative to setting the OTEL_LOG_LEVEL environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/troubleshooting/common-nodejs-troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: js\nCODE:\n```\nstart({\n   logLevel: 'debug',\n   metrics: {\n      // configuration passed to metrics signal\n   },\n   profiling: {\n      // configuration passed to profiling signal\n   },\n   tracing: {\n      // configuration passed to tracing signal\n   },\n});\n```\n\n----------------------------------------\n\nTITLE: Monitoring MTS Creation Calls Limited by New Properties (Org-Level) in Splunk\nDESCRIPTION: This metric (`sf.org.numPropertyLimitedMetricTimeSeriesCreateCalls`) monitors the number of Metric Time Series (MTS) creation calls rejected due to exceeding the weekly limit on new custom property or dimension keys at the organization level. This applies to both MTS and ETS.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.numPropertyLimitedMetricTimeSeriesCreateCalls\n```\n\n----------------------------------------\n\nTITLE: Monitoring MTS Creation Calls Limited by New Properties (Token-Level) in Splunk\nDESCRIPTION: This metric (`sf.org.numPropertyLimitedMetricTimeSeriesCreateCallsByToken`) monitors the number of Metric Time Series (MTS) creation calls rejected due to exceeding the weekly limit on new custom property or dimension keys, specifically tracked per token. This applies to both MTS and ETS.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.numPropertyLimitedMetricTimeSeriesCreateCallsByToken\n```\n\n----------------------------------------\n\nTITLE: Volume Space Alert Trigger Payload\nDESCRIPTION: JSON payload template for triggering volume space alerts in SolarWinds. Includes available space information.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/solarwinds-integration.rst#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"alert_rule\":\"${N=Alerting;M=AlertName}\",\n  \"entity_display_name\":\"${NodeName} ${SQL: SELECT REPLACE ('''${Caption}''','\\\\', ' ')} has ${VolumeSpaceAvailable} free\",\n  \"entity_id\":\"${N=Alerting;M=AlertObjectID}\",\n  \"host_name\":\"${NodeName}\",\n  \"ip_address\":\"${Node.IP_Address}\",\n  \"message_type\":\"CRITICAL\",\n  \"monitor_name\":\"SolarWinds\",\n  \"monitoring_tool\":\"SolarWinds\",\n  \"state_message\":\"${NodeName} ${SQL: SELECT REPLACE ('''${Caption}''','\\\\', ' ')} has ${VolumeSpaceAvailable} free\"\n}\n```\n\n----------------------------------------\n\nTITLE: Aggregating CPU Usage Labels\nDESCRIPTION: Aggregates all labels except 'state' using sum aggregation for system.cpu.usage metric.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: system.cpu.usage\naction: update\noperations:\n    - action: aggregate_labels\n      label_set: [ state ]\n      aggregation_type: sum\n```\n\n----------------------------------------\n\nTITLE: Deleting OpenTelemetry Collector release for CRD migration\nDESCRIPTION: Command to delete an existing OpenTelemetry Collector release before migrating to the recommended CRD deployment method.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete <release-name>\n```\n\n----------------------------------------\n\nTITLE: RST Table Definition for Test Status Types\nDESCRIPTION: A reStructuredText table definition that lists and describes different test status types in Splunk Synthetic Monitoring, including Pending, Available, No Data, and Failure states.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-status/test-status.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 20, 80\n\n   * - :strong:`Current status`\n     - :strong:`Description`\n\n   * - Pending\n     - Splunk Synthetic Monitoring is still retrieving the status of this test. \n\n   * - Available\n     - The test is functioning properly. If the test is active, data is being collected at the set interval and can be viewed in the :guilabel:`Test History` page. If the test is paused, it can be unpaused and will resume collecting data.\n\n   * - No Data \n     - The test isn't currently collecting data. \n\n   * - Failure\n     - The test encountered a failure.\n```\n\n----------------------------------------\n\nTITLE: Verifying Nagios Configuration\nDESCRIPTION: Command to verify the Nagios configuration after making changes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/check_mk-integration.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n/omd/sites/;/bin/nagios -v /omd/sites//tmp/nagios/nagios.cfg\n```\n\n----------------------------------------\n\nTITLE: SQL Replace Function for SolarWinds Alert\nDESCRIPTION: SQL query to replace backslash characters in SolarWinds alert captions to ensure proper HTTP post functionality.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/solarwinds-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT REPLACE ('''${Caption}''','\\\\', ' ')\n```\n\n----------------------------------------\n\nTITLE: Creating Tanzu Tile\nDESCRIPTION: Command to create the BOSH release and build the Tanzu tile for the OpenTelemetry Collector\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-pivotal-cloudfoundry.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./make-latest-tile\n```\n\n----------------------------------------\n\nTITLE: Truncating Strings with abbreviate - Handlebars\nDESCRIPTION: Demonstrates the use of abbreviate to truncate a string variable to a specified maximum length, with a minimum width of 4 characters. Useful for shortening lengthy strings in alert payloads to maintain readability.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/notif-services/splunkplatform.rst#2025-04-22_snippet_6\n\nLANGUAGE: Handlebars\nCODE:\n```\n``{{abbreviate long_str 5}}``\n```\n\n----------------------------------------\n\nTITLE: AWS Policy for Metric Streams\nDESCRIPTION: Sample AWS policy for the role used by Metric Streams. It allows actions to put records into the specified Firehose delivery stream.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-ts-ms-aws.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"firehose:PutRecord\",\n                \"firehose:PutRecordBatch\"\n            ],\n            \"Resource\": [\n                \"arn:aws:firehose:eu-west-2:906383545488:deliverystream/PUT-HTP-7pH7O\"\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding Dynamic Supported Library Information\nDESCRIPTION: This HTML div is used within a reStructuredText document to dynamically embed information about supported instrumented libraries. It specifies a section ('instrumentations'), a source URL for the data (a metadata.yaml file on GitHub), and extensive renaming rules for the displayed data fields. This allows the documentation to automatically update when the source metadata changes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/dotnet-requirements.rst#2025-04-22_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"instrumentation\" section=\"instrumentations\" url=\"https://raw.githubusercontent.com/splunk/o11y-gdi-metadata/main/apm/splunk-otel-dotnet/metadata.yaml\" data-renaming='{\"keys\": \"Identifier\", \"description\": \"Description\", \"stability\": \"Stability\", \"support\": \"Support\", \"instrumented_components\": \"Components\", \"signals\": \"Signals\", \"source_href\": \"Source\", \"settings\": \"Settings\", \"dependencies\": \"Dependencies\", \"supported_versions\": \"Supported versions\", \"name\": \"Name\", \"package_href\": \"Package URL\", \"version\": \"Version\", \"instrument\": \"Type\", \"metric_name\": \"Metric name\", \"metrics\": \"Metrics\", \"env\": \"Environment variable\", \"default\": \"Default\", \"type\": \"Type\", \"category\": \"Category\"}'></div>\n```\n\n----------------------------------------\n\nTITLE: JBoss Domain Mode Java Agent Configuration\nDESCRIPTION: Configure the domain.xml in JBoss to add the javaagent option under the specific jvm-options section. This sets the Splunk Java agent path for domains.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-servers-instructions.rst#2025-04-22_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<option value=\"-javaagent:/path/to/splunk-otel-javaagent.jar\"/>\n```\n\n----------------------------------------\n\nTITLE: Adding User to Varnish Group - Bash Command\nDESCRIPTION: Command to add the Smart Agent user to the varnish group to enable varnishstat command execution.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/varnish.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nusermod -a -G varnish signalfx-agent\n```\n\n----------------------------------------\n\nTITLE: Restarting OpenTelemetry Collector Service\nDESCRIPTION: Restart the OpenTelemetry Collector service on the host. This command uses systemctl to manage the service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl restart splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Linking vCenter Receiver to Metrics Pipeline in OpenTelemetry Collector (YAML)\nDESCRIPTION: This YAML example shows how to include the 'vcenter' receiver in the 'metrics' pipeline under the 'service' section. It is required so that metrics gathered by the vCenter receiver are processed by the collector pipeline. Place this in the 'service' part of your OpenTelemetry Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/vcenter-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [vcenter]\n```\n\n----------------------------------------\n\nTITLE: Installing Git Repository and Setting Permissions\nDESCRIPTION: Commands to clone the documentation repository and set execute permissions on the start script. These commands set up the local development environment for contributing to the documentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone\n```\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x start.sh\n```\n\n----------------------------------------\n\nTITLE: Alternative Cloud Foundry Receiver Configuration\nDESCRIPTION: This example shows an alternative configuration for the Cloud Foundry receiver with different settings and naming.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/cloudfoundry-receiver.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncloudfoundry/one:\n  rlp_gateway:\n    endpoint: \"https://log-stream.sys.example.internal\"\n    shard_id: \"otel-test\"\n    timeout: \"20s\"\n    tls:\n      insecure_skip_verify: true\n  uaa:\n    endpoint: \"https://uaa.sys.example.internal\"\n    username: \"admin\"\n    password: \"test\"\n    tls:\n      insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Using Prefix Wildcard Filter in SignalFlow Data Selection - SignalFlow Language\nDESCRIPTION: This pseudo-SignalFlow example shows the application of a prefix wildcard in the filter function to select hosts matching 'kafka*'. Prefix wildcard counts are tracked against a separate, larger limit compared to generic wildcards. Proper management of prefix wildcards is necessary to avoid validation errors.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_2\n\nLANGUAGE: SignalFlow\nCODE:\n```\ndata('jvm.load', filter=filter('host', 'kafka*'))\n```\n\n----------------------------------------\n\nTITLE: Configuring Per-Alert Routing to Splunk On-Call in Dataset\nDESCRIPTION: This configuration example demonstrates how to specify the Splunk On-Call webhook as the `alertAddress` for a specific alert within the `alerts` array, overriding any globally defined `alertAddress`. This allows routing only certain alerts (like the one triggered by `count:1m(error) > 10`) to Splunk On-Call, while others use the default recipient (e.g., `email@example.com`).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/scalyr-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   alertAddress: \"email@example.com\",\n\n   alerts: [\n      // This alert will be sent to VictorOps\n      {\n         trigger: \"count:1m(error) > 10\",\n         \"alertAddress\": \"victorops:webhookUrl=https://alert.victorops.com/integrations/generic/20131114/alert/$api_key/$routing_key\"\n      },\n\n      // This alert will send notifications to email@example.com\n      {\n         trigger: \"mean:10m($source='tsdb' $serverHost='server1' metric='proc.stat.cpu_rate' type='user') > 50\"\n      }\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Splunk Synthetics Runner with Helm\nDESCRIPTION: Shell command for completely removing a Splunk Synthetics Runner Helm deployment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\nhelm uninstall my-splunk-synthetics-runner\n```\n\n----------------------------------------\n\nTITLE: Formatting Email Routing Key for Splunk On-Call (Text)\nDESCRIPTION: Demonstrates the structure of an email endpoint address used for incident routing in Splunk On-Call. The format shows the placement of the unique endpoint key and routing key within the address, with explicit highlighting of the routing key part. No software dependencies are required, but correct address generation is essential for routing to target teams. Inputs include the unique endpoint key and routing key; output is a valid On-Call email endpoint address.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/generic-email-endpoint.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n70ysj-6ks..(endpoint key)..9284\\ **+database**\\ @alert.victorops.com\n```\n\n----------------------------------------\n\nTITLE: Including HTTP Check Receiver in Metrics Pipeline\nDESCRIPTION: YAML configuration to include the HTTP check receiver in the metrics pipeline of the OpenTelemetry Collector service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/http-check-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [httpcheck]\n```\n\n----------------------------------------\n\nTITLE: Identifying Splunk Cloud Provider Throttling Metric\nDESCRIPTION: This identifier represents a Splunk metric name used to track the count of throttled service client calls for a specific cloud provider (`<cloudprovidername>`) within Splunk Infrastructure Monitoring. It helps monitor if data ingest is being limited at the source by the cloud provider, which can impact performance and potentially incur costs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/subscription-usage-overview.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.num.<cloudprovidername>ServiceClientCallCountThrottles\n```\n\n----------------------------------------\n\nTITLE: Removing Docker Container\nDESCRIPTION: Shell command to remove a stopped Docker container by its ID or name, used during the upgrade or uninstallation process for private runners.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndocker rm <container_id_or_name>\n```\n\n----------------------------------------\n\nTITLE: Basic OpenTelemetry Collector Configuration for GitLab\nDESCRIPTION: Basic configuration structure for activating GitLab monitoring in the OpenTelemetry Collector\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-gitlab/gitlab.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/gitlab:\n    type: gitlab\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Basic reStructuredText Formatting Example - reStructuredText\nDESCRIPTION: Illustrates foundational syntax and conventions of reStructuredText, including header underlining, paragraphs, unordered and ordered lists, inline code formatting, internal cross-references, invisible anchors, code blocks, and image embedding. The snippet serves as both a style guide and reference for using headings, formatting inline elements, and structuring new documentation content. No special dependencies are required; this is the standard rST markup as processed by Sphinx.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n**********************\nTitle of the page\n**********************\n\nParagraphs are written as text in separate lines.\n\n- Unordered lists use ``-`` as bullet points\n- :strong:`String formatted as bold`.\n- ``In-line code snippets are wrapped in double-ticks``\n\nHeading 1\n============================\n\nSeparate text from headings.\n\n1. First item of an ordered list\n2. Second item of an ordered list\n\nHeading 2\n----------------------------\n\nInternal links are added by referencing labels:\n\n- :ref:`label-name`\n- :ref:`Descriptive text for the link <label-name>\n\nBelow is an invisible label:\n\n.. _label-you-can-use-as-an-anchor-link:\n\nAdding a code snippet\n------------------------------\n\nThe following snippet shows a code snippet\n\n.. code-block:: java\n\n   Here is the code. Notice that space indentation matters.\n\nThe following snippet shows an image, with a path relative to the _images folder:\n\n.. image:: /_images/apm/dashboards/dashboard-gif-2.gif\n   :alt: This image shows an example APM service dashboard.\n```\n\n----------------------------------------\n\nTITLE: Starting New Podman Container\nDESCRIPTION: Starts a new container with the updated image and required configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_38\n\nLANGUAGE: shell\nCODE:\n```\npodman run --cap-add NET_ADMIN -e \"RUNNER_TOKEN=YOUR_TOKEN_HERE\" \\\nhttp://quay.io/signalfx/splunk-synthetics-runner:latest\n```\n\n----------------------------------------\n\nTITLE: Accessing Grafana UI URL Format\nDESCRIPTION: Example URL format for accessing the Grafana user interface deployed in Kubernetes. Replace `<host-name>/<IP-address>` with the appropriate node IP or hostname and `<port>` with the NodePort specified in the Service manifest (e.g., 32001).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_7\n\nLANGUAGE: none\nCODE:\n```\n        http://<host-name>/<IP-address>:<port>\n```\n\n----------------------------------------\n\nTITLE: Base64 Encoding Authentication String (Bash)\nDESCRIPTION: Linux terminal command to encode username and password combination in base64 format for API authentication.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/auth.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\necho -n 'myusername:mypassword' | base64\n```\n\n----------------------------------------\n\nTITLE: Starting PHP Application in Docker Startup Script using Shell\nDESCRIPTION: An example command to start the main PHP application within a Docker startup script, typically placed after the instrumentation setup commands. This example uses `supervisorctl` to manage the application process.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/instrumentation/instrument-php-application.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsupervisor start my-php-app\n```\n\n----------------------------------------\n\nTITLE: Specifying Future Relative Time Range in Splunk\nDESCRIPTION: Enter a relative time range unit (m, h, d, w) preceded by a plus (+) to set a time window starting from now into the future. This example sets a time window for the next 6 hours.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/use-time-range-selector.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n+6h\n```\n\n----------------------------------------\n\nTITLE: Applying Filters in SignalFlow\nDESCRIPTION: Demonstrates the correct way to apply filters in SignalFlow, ensuring compatibility with the graphical plot-builder. Filters with OR conditions must be defined inside a single filter statement.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/chart-builder.rst#2025-04-22_snippet_6\n\nLANGUAGE: SignalFlow\nCODE:\n```\nfilter(\"aws_availability_zone\", \"us-east-1a\", \"us-west-1a\")\n```\n\nLANGUAGE: SignalFlow\nCODE:\n```\nfilter(\"aws_availability_zone\", \"us-east-1a\", \"us-west-1a\") AND filter(\"aws_instance_type\", \"i3.2xlarge\")\n```\n\n----------------------------------------\n\nTITLE: Running Private Runner on Docker for Mac/Windows with Network Settings\nDESCRIPTION: Shell command to start a private runner on Docker for Mac or Windows with network shaping disabled, required due to platform limitations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -e \"DISABLE_NETWORK_SHAPING=true\" \\\n-e \"RUNNER_TOKEN=YOUR_TOKEN_HERE\" \\\nquay.io/signalfx/splunk-synthetics-runner:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk OTel Collector Proxy via Docker Run (Bash)\nDESCRIPTION: Shows command-line arguments using the `-e` flag to pass HTTP_PROXY and HTTPS_PROXY environment variables when starting a Splunk OpenTelemetry Collector container with `docker run`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/allow-services.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n-e HTTP_PROXY=<proxy.address:port>\n-e HTTPS_PROXY=<proxy.address:port>\n```\n\n----------------------------------------\n\nTITLE: Listing Splunk>VictorOps Bot Commands - Microsoft Teams (Markdown)\nDESCRIPTION: Demonstrates available command-line interface commands for the Splunk>VictorOps bot within Microsoft Teams channels. These snippets serve as quick-start references for administrators or team owners to interact with the app for help, configuration, channel mapping, and incident creation. No external dependencies are necessary; input is the command and output is respective bot responses or UI cards.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/microsoft-teams-integration-guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nhelp\n```\n\nLANGUAGE: markdown\nCODE:\n```\nconfigure\n```\n\nLANGUAGE: markdown\nCODE:\n```\nmapchannel\n```\n\nLANGUAGE: markdown\nCODE:\n```\ncreateincident\n```\n\n----------------------------------------\n\nTITLE: Aggregating Memory State Label Values\nDESCRIPTION: Combines multiple memory state values into a single 'slab' value using summation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: system.memory.usage\naction: update\noperations:\n    - action: aggregate_label_values\n      label: state\n      aggregated_values: [ slab_reclaimable, slab_unreclaimable ]\n      new_value: slab\n      aggregation_type: sum\n```\n\n----------------------------------------\n\nTITLE: Installing the Collector with Downloaded RPM Package (zypper)\nDESCRIPTION: Commands to install the required libcap-progs dependency and install the Splunk OTel Collector using a locally downloaded RPM package with zypper.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nzypper install -y libcap-progs  # Required for enabling cap_dac_read_search and cap_sys_ptrace capabilities on the Collector\nrpm -ivh <path to splunk-otel-collector rpm>\n```\n\n----------------------------------------\n\nTITLE: Embedding HTML for Phone Section in reStructuredText\nDESCRIPTION: This snippet embeds HTML content to create a section header for Phone notifications in the reStructuredText document. It includes an anchor link for easy navigation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/notifications/notification-types.rst#2025-04-22_snippet_5\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n  \n    <embed>\n      <h2>Phone<a name=\"phone\" class=\"headerlink\" href=\"#phone\" title=\"Permalink to this headline\">¶</a></h2>\n    </embed>\n```\n\n----------------------------------------\n\nTITLE: Security Policy Configuration for OpenTelemetry Kernel Collector - Bash\nDESCRIPTION: This bash command configures the Security Context Constraints (SCC) in OpenShift for the kernel collector, allowing it to run with necessary privileges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\noc adm policy add-scc-to-user privileged -z my-opentelemetry-ebpf -n <NAMESPACE>\n\n      oc adm policy add-scc-to-user anyuid -z my-opentelemetry-ebpf -n <NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Installing Zabbix Plugin via RPM Package\nDESCRIPTION: Commands to download and install the Zabbix plugin using RPM package manager for ack-back functionality.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zabbix-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-zabbix-0.18.3/victorops-zabbix-0.18.3-2.noarch.rpm\n\nsudo rpm -i victorops-zabbix-0.18.3-2.noarch.rpm\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo ./install\n```\n\n----------------------------------------\n\nTITLE: Listing All Docker Containers\nDESCRIPTION: Shell command to list all Docker containers including stopped ones, useful for identifying container IDs or names before uninstallation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ndocker ps -a\n```\n\n----------------------------------------\n\nTITLE: Adding Span Attributes in PHP using OpenTelemetry SDK\nDESCRIPTION: Illustrates creating a child span within a PHP function, setting an attribute ('dicelib.rolled') based on a random number, and properly managing the span's lifecycle (activation, ending, detaching). It notes the use of the SIGNALFX_TRACE_GLOBAL_TAGS environment variable for global tags.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/add-context-trace-span.rst#2025-04-22_snippet_6\n\nLANGUAGE: php\nCODE:\n```\n<?php\n\n// OpenTelemetry PHP\n\nuse SignalFx\\GlobalTracer;\n\nprivate function rollOnce() {\n   $parent = OpenTelemetry\\API\\Trace\\Span::getCurrent();\n   $scope = $parent->activate();\n   try {\n      $span = $this->tracer->spanBuilder(\"rollTheDice\")->startSpan();\n      $result = random_int(1, 6);\n      $span->setAttribute('dicelib.rolled', $result);\n      $span->end();\n   } finally {\n      $scope->detach();\n   }\n   return $result;\n}\n\n// You can also set global tags using the SIGNALFX_TRACE_GLOBAL_TAGS\n// environment variable, which accepts a list of comma-separated key-value\n// pairs. For example: key1:val1,key2:val2.\n?>\n```\n\n----------------------------------------\n\nTITLE: Installing VictorOps Nagios Plugin using rpm (RPM)\nDESCRIPTION: Uses the `rpm` command with the `-i` flag to install the previously downloaded VictorOps Nagios plugin `.rpm` package. Replace `<path_to_file>` with the actual path to the downloaded file. This command requires appropriate permissions, potentially `sudo`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nrpm -i <path_to_file>\n```\n\n----------------------------------------\n\nTITLE: Validating Percentile Thresholds for Sudden Change Alert\nDESCRIPTION: Specifies the required mathematical relationship between Trigger and Clear thresholds when the 'Normal based on' parameter is set to 'Percentile' for a Sudden Change alert condition. The absolute difference between the Trigger Threshold and 50 must be greater than or equal to the absolute difference between the Clear Threshold and 50 to ensure the detector functions correctly. This rule applies regardless of whether 'Alert when' is set to 'Too high' or 'Too low', as the system automatically adjusts the percentile interpretation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/alert-condition-reference/sudden-change.rst#2025-04-22_snippet_0\n\nLANGUAGE: none\nCODE:\n```\n|TriggerThreshold - 50| >= |ClearThreshold - 50|\n```\n\n----------------------------------------\n\nTITLE: Pulling Latest Images with Docker Compose\nDESCRIPTION: Shell command to pull the latest versions of all images defined in the docker-compose.yml file, used during manual upgrade of the private runner.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ndocker-compose pull\n```\n\n----------------------------------------\n\nTITLE: Including SQL Query Receiver in Metrics Pipeline\nDESCRIPTION: Configuration to include the SQL Query receiver in the metrics pipeline of the OpenTelemetry Collector service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sqlquery-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [sqlquery]\n```\n\n----------------------------------------\n\nTITLE: RestructuredText Directive for Metadata\nDESCRIPTION: RestructuredText metadata directive defining the description for the document about Collector dashboard monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-builtin-dashboard.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. meta::\n      :description: Use the built-in Collector dashboard in Splunk Observability Cloud for a better understanding of how your Collector instances are doing.\n```\n\n----------------------------------------\n\nTITLE: Viewing Custom MTS Burst/Overage Limit in Splunk\nDESCRIPTION: This metric (`sf.org.limit.customMetricTimeSeries`) likely reflects the configured burst/overage limit for active custom Metric Time Series (MTS) within the organization. It indicates the maximum allowed active custom MTS before restrictions apply.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.limit.customMetricTimeSeries\n```\n\n----------------------------------------\n\nTITLE: Configuring Socket.io with NPM Packages\nDESCRIPTION: Example of initializing socket.io instrumentation when using NPM packages. Shows how to integrate @splunk/otel-web with socket.io-client in a bundled application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-instrumentations.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport SplunkOtelWeb from '@splunk/otel-web';\nimport { io } from 'socket.io-client';\nSplunkRum.init({\n// ...\n   instrumentations: {\n      socketio: {\n         target: io,\n      },\n   },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Private Runner with Docker Run Command\nDESCRIPTION: Shell command to start a Splunk Synthetic Monitoring private runner Docker container. The command uses NET_ADMIN capability and requires a runner token for authentication.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --cap-add NET_ADMIN \\\n-e \"RUNNER_TOKEN=YOUR_TOKEN_HERE\" \\\nquay.io/signalfx/splunk-synthetics-runner:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring Specific Metric Inclusion in SignalFx Exporter YAML\nDESCRIPTION: Example showing how to configure the SignalFx exporter to send only specific metrics with particular dimension values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-exporter.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  signalfx:\n    include_metrics:\n      - metric_name: \"cpu.idle\"\n      - metric_name: \"cpu.interrupt\"\n        dimensions:\n          cpu: [\"*\"]\n```\n\n----------------------------------------\n\nTITLE: Regular Expression Pattern for Valid traceparent Values\nDESCRIPTION: This regex pattern defines the format that a valid traceparent value must match when generating Server-Timing headers. Headers that don't match this pattern will be automatically discarded.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n00-([0-9a-f]{32})-([0-9a-f]{16})-01\n```\n\n----------------------------------------\n\nTITLE: Activating the Metrics Transform Processor (YAML)\nDESCRIPTION: This snippet shows the basic configuration required to activate the `metricstransform` processor by adding it to the `processors` section of the OpenTelemetry Collector configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  metricstransform:\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for OpenTelemetry Deployment Options\nDESCRIPTION: ReStructuredText markup defining the documentation structure for OpenTelemetry Collector deployment options. Includes table of contents, deployment options list, and cross-references to platform-specific installation guides.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/otel-deployments.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _otel_deployments:\n\n****************************************************************************************************\nOther Collector deployment tools and options: ECS/EC2, Fargate, Nomad, PCF\n****************************************************************************************************\n\n.. meta::\n    :description: Options for deploying the Splunk Distribution of the OpenTelemetry Collector: ECS/EC2, Fargate, Nomad, PCF.\n\n.. toctree::\n    :maxdepth: 4\n    :titlesonly:\n    :hidden:\n\n    Amazon ECS EC2 <deployments-ecs-ec2.rst>\n    Amazon Fargate <deployments-fargate.rst>\n    Fargate scenario: Monitor Java app <deployments-fargate-java.rst>\n    Nomad <deployments-nomad.rst>\n    Pivotal Cloud Foundry <deployments-pivotal-cloudfoundry.rst>\n```\n\n----------------------------------------\n\nTITLE: Complete GitLab Services Configuration Example\nDESCRIPTION: Detailed example showing how to configure multiple GitLab services (Sidekiq and Workhorse) in the collector configuration\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-gitlab/gitlab.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/gitlab-sidekiq:\n    type: gitlab\n    host: gitlab-webservice-default.default\n    port: 3807\n  smartagent/gitlab-workhorse:\n    type: gitlab\n    host: gitlab-webservice-default.default\n    port: 9229\n\n# ... Other sections\n\nservice:\n  pipelines:\n    metrics:\n      receivers:\n        - smartagent/gitlab-sidekiq\n        - smartagent/gitlab-workhorse\n\n# ... Other sections\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Kubernetes Deployment for Splunk Synthetics Runner\nDESCRIPTION: Shell command for removing the Kubernetes deployment of Splunk Synthetics runners.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete -f deployment.yaml\n```\n\n----------------------------------------\n\nTITLE: Formatting Splunk OnCall Service Email with Routing Key\nDESCRIPTION: Example of how to format the Splunk OnCall service email address with a team routing key. The routing key is appended to the email address to direct alerts to specific teams.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/alertsite-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\ndb212e48-……8669+databaseteam@alert.victorops.com\n```\n\n----------------------------------------\n\nTITLE: Setting Server-Timing Header for Splunk APM in Bash\nDESCRIPTION: This command sets an environment variable to enable the Server-Timing header, which links spans from the browser with back-end spans and traces in Splunk APM.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nSPLUNK_TRACE_RESPONSE_HEADER_ENABLED=true\n```\n\n----------------------------------------\n\nTITLE: RST Table Structure for User Roles Documentation\nDESCRIPTION: ReStructuredText table format defining different user roles and their descriptions in Splunk On-Call platform\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/user-roles/user-roles-permissions.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 40, 60\n\n   * - :strong:`Role`\n     - :strong:`Description`\n\n   * - :ref:`Global admin <global-admin>`\n     - * The highest level of permissions in the Splunk On-Call platform \n       * Responsible for the overall workflow and management of integrations and users.\n       * Access to all functionality across the platform including scheduling, integrations, teams, and users.\n```\n\n----------------------------------------\n\nTITLE: Removing Podman Container\nDESCRIPTION: Removes a stopped Podman container by its ID or name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_37\n\nLANGUAGE: shell\nCODE:\n```\npodman rm <container_id_or_name>\n```\n\n----------------------------------------\n\nTITLE: Upgrading Helm Chart with Updated Values.yaml\nDESCRIPTION: Execute the helm upgrade command to apply changes in values.yaml. This upgrades the Grafana Helm chart, incorporating configurations for the Splunk plugin and data sources. Specify the release name when running the command.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-helm.rst#2025-04-22_snippet_1\n\nLANGUAGE: none\nCODE:\n```\nhelm upgrade <test-release> grafana/grafana -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Image Embedding Syntax in reStructuredText\nDESCRIPTION: Shows the syntax for adding images to documentation using the reStructuredText '.. image::' directive. Contributors specify the image path relative to the '_images' directory, may include width, and must provide descriptive alt text for accessibility. This snippet should be used when including screenshots or illustrations; contributors must ensure images do not contain sensitive information. No additional dependencies are required beyond the standard Sphinx build chain.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: restructuredtext\nCODE:\n```\n..  image:: /_images/<subdir>/<filename>\n    :width: 99%\n    :alt: <alt text>\n```\n\n----------------------------------------\n\nTITLE: Upgrading OpenShift Private Runner\nDESCRIPTION: Applies the deployment configuration to update an existing private runner on OpenShift. Uses latest tag with Always pull policy to update the image.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_32\n\nLANGUAGE: shell\nCODE:\n```\noc apply -f deployment.yaml\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Alerts with Wildcard Match in Splunk On-Call Rules Engine\nDESCRIPTION: This rule aggregates multiple alerts into a single incident by transforming the entity_id field for alerts related to disk problems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-transformations.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nWhen entity_id matches disk* using Wildcard Match\n\nSet entity_id to Disk Problems\n```\n\n----------------------------------------\n\nTITLE: Setting Up Unified Identity in Splunk Cloud Platform\nDESCRIPTION: Command to establish Unified Identity between Splunk Cloud Platform and Splunk Observability Cloud using the ACS CLI tool. Requires a valid Observability Cloud access token.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/centralized-rbac.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nacs observability pair --o11y-access-token \"<enter-o11y-access-token>\"\n```\n\n----------------------------------------\n\nTITLE: Starting the Splunk OpenTelemetry Collector Service\nDESCRIPTION: Command to start the Splunk OpenTelemetry Collector service using systemctl on Linux systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl start splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Specifying Future Relative Time Interval in Splunk\nDESCRIPTION: Define a relative time window scheduled for the future by specifying a start and end point relative to now, separated by 'to'. The earlier time point must come first. This example sets a future window between tomorrow (+1d) and 2 days from now (+2d), useful for muting rules.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/use-time-range-selector.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n+1d to +2d\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows IIS Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration snippet for activating the Windows IIS monitor in the OpenTelemetry Collector. Defines the receiver configuration with type windows-iis.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/microsoft-windows-iis.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/windows-iis:\n    type: windows-iis\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Configuring Consul Agent for Metrics Collection in Earlier Versions\nDESCRIPTION: Configuration to add to each Consul agent configuration file when running Consul versions earlier than 0.9.1. This enables the Consul agent to send metrics to the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/consul.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{\"telemetry\":\n   {\"statsd_address\": \"<agent host>:<agent port, default 8125>\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Google Chrome Recorder Value Change\nDESCRIPTION: Example of a value change event in Google Chrome Recorder format with selectors.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/browser-test/set-up-browser-test.rst#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n{\n// Google Chrome Recorder\n   \"type\": \"change\",\n   \"value\": \"5\",\n   \"selectors\": [\n      [\n         \"#quantity\"\n      ],\n      [\n         \"xpath///*[@id=\\\"quantity\\\"]\"\n      ]\n   ],\n   \"target\": \"main\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk On-Call Terraform Provider\nDESCRIPTION: This Terraform configuration block specifies the required providers for the project. It declares the need for the 'splunk/victorops' provider (Splunk On-Call) and pins it to version '0.1.4'. This block should be placed in a Terraform configuration file (e.g., 'sp-oncall.tf') to ensure Terraform downloads the correct provider during initialization.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/manage-splunk-oncall-using-terraform.rst#2025-04-22_snippet_0\n\nLANGUAGE: terraform\nCODE:\n```\n# Install VictorOps Terraform Provider\nterraform {\nrequired_providers {\nvictorops = {\nsource = \"splunk/victorops\"\nversion = \"0.1.4\"\n}\n}\n}\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure Definition\nDESCRIPTION: Defines the documentation structure using reStructuredText (RST) format, including table of contents, meta descriptions, and includes for auto-discovery documentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/discovery-linux.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _discovery-linux:\n\n************************************************************************\nAutomatic discovery and zero-code instrumentation for Linux\n************************************************************************\n\n.. meta:: \n    :description: Get started with automatic discovery and zero-code instrumentation for Linux environments. Deploy automatic discovery and zero-code instrumentation to automatically find services and applications running in your Linux environment and send data from them to Splunk Observability Cloud.\n\n.. toctree::\n    :hidden:\n\n    Language runtimes <linux/linux-backend>\n    Third-party applications <linux/linux-third-party>\n    Advanced customization <linux/linux-advanced-config>\n```\n\n----------------------------------------\n\nTITLE: Configuring Traces Pipeline for OTLP Export in Splunk\nDESCRIPTION: Pipeline configuration to enable trace export using the OTLP exporter in the Splunk OpenTelemetry Collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/other-ingestion-methods/grpc-data-ingest.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    traces:\n      # ...\n      exporters: [otlp]\n```\n\n----------------------------------------\n\nTITLE: Toggling Metrics Collection for SDKs (Shell)\nDESCRIPTION: Activates ('--enable-metrics') or deactivates ('--disable-metrics', default) metrics collection and exporting for activated SDKs that support it via the 'SPLUNK_METRICS_ENABLED' environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\n--[enable|disable]-metrics\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n--disable-metrics\n```\n\n----------------------------------------\n\nTITLE: Creating Image Reference in reStructuredText\nDESCRIPTION: This code block inserts an image into the document using reStructuredText syntax. It specifies the image file path, width, and alternative text for accessibility.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/mobile/mobile-settings-menu.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. image:: /_images/spoc/mob-settings1.png\n    :width: 100%\n    :alt: Settings page in the Splunk On-Call mobile app.\n```\n\n----------------------------------------\n\nTITLE: Splunk Synthetic Monitoring URL Navigation\nDESCRIPTION: Equivalent Splunk Synthetic Monitoring code for URL navigation with wait functionality.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/browser-test/set-up-browser-test.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n// Splunk Synthetic Monitoring code snippet\n\"name\": \"Go to URL\",\n\"type\": \"go_to_url\",\n\"url\": \"www.buttercupgames.com\",\n\"wait_for_nav\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading VictorOps Nagios Plugin (Debian)\nDESCRIPTION: Command to download the VictorOps Nagios plugin Debian package from GitHub.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/check_mk-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-nagios-1.4.20/victorops-nagios_1.4.20_all.deb\n```\n\n----------------------------------------\n\nTITLE: Deactivating Zero-Code Instrumentation in Kubernetes\nDESCRIPTION: Bash command to remove the auto-instrumentation annotation from a Kubernetes deployment. This command allows users to disable zero-code instrumentation for a specific application language.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nkubectl patch deployment <my-deployment> -n <my-namespace> --type=json -p='[{\"op\": \"remove\", \"path\": \"/spec/template/metadata/annotations/instrumentation.opentelemetry.io~1inject-<application_language>\"}]'\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Dependencies for Data Link Script\nDESCRIPTION: This command uses the Node Package Manager (npm) to install the required dependencies for the `createAppDLinkTerraformScript.js` Node.js script. It should be run in the directory where the script and its `package.json` file are located. Node.js and npm must be installed as prerequisites.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-datalinks-terraform/apm-create-data-links-terraform-batch.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS CloudWatch Metrics with Statistical Methods in YAML\nDESCRIPTION: This configuration defines which statistical methods (mean, upper, lower, sum, count) can be applied to various AWS service metrics. It organizes metrics by service namespace and specifies supported statistical operations for each metric, enabling proper metric collection and analysis in monitoring systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-recommended-stats.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  PercentIdleInstances:\n    - mean\n    - upper\n    - lower\n  Placement:\n    - sum\n  PlacementsCanceled:\n    - mean\n    - upper\n    - lower\n    - sum\n  PlacementsFailed:\n    - mean\n    - upper\n    - lower\n    - sum\n  PlacementsStarted:\n    - mean\n    - upper\n    - lower\n    - sum\n  PlacementsSucceeded:\n    - mean\n    - upper\n    - lower\n    - sum\n  PlacementsTimedOut:\n    - mean\n    - upper\n    - lower\n    - sum\n  PlayerSessionActivations:\n    - mean\n    - upper\n    - lower\n    - sum\n  PlayersStarted:\n    - sum\n  QueueDepth:\n    - mean\n    - upper\n    - lower\n    - sum\n  RuleEvaluationsFailed:\n    - sum\n  RuleEvaluationsPassed:\n    - sum\n  ServerProcessAbnormalTerminations:\n    - mean\n    - upper\n    - lower\n    - sum\n  ServerProcessActivations:\n    - mean\n    - upper\n    - lower\n    - sum\n  ServerProcessTerminations:\n    - mean\n    - upper\n    - lower\n    - sum\n  TicketsFailed:\n    - sum\n  TicketsStarted:\n    - sum\n  TicketsTimedOut:\n    - sum\n  TimeToMatch:\n    - mean\n    - upper\n    - lower\n    - count\n  TimeToTicketCancel:\n    - mean\n    - upper\n    - lower\n    - count\n  TimeToTicketSuccess:\n    - mean\n    - upper\n    - lower\n    - count\nAWS/KMS:\n  SecondsUntilKeyMaterialExpiration:\n    - lower\nAWS/Kinesis:\n  GetRecords.IteratorAgeMilliseconds:\n    - mean\n    - upper\n    - lower\n    - count\n  GetRecords.Latency:\n    - mean\n    - upper\n    - lower\n  GetRecords.Success:\n    - mean\n    - count\n    - sum\n  IteratorAgeMilliseconds:\n    - mean\n    - upper\n    - lower\n    - count\n  PutRecord.Latency:\n    - mean\n    - upper\n    - lower\n  PutRecord.Success:\n    - mean\n    - count\n    - sum\n  PutRecords.Latency:\n    - mean\n    - upper\n    - lower\n  PutRecords.Success:\n    - mean\n    - count\n    - sum\n  SubscribeToShardEvent.MillisBehindLatest:\n    - mean\n    - upper\n    - lower\n    - count\nAWS/KinesisAnalytics:\n  downtime:\n    - sum\n  lastCheckpointDuration:\n    - mean\n    - upper\n  lastCheckpointSize:\n    - sum\nAWS/Lambda:\n  ConcurrentExecutions:\n    - upper\n  DeadLetterErrors:\n    - sum\n  DestinationDeliveryFailures:\n    - sum\n  Duration:\n    - mean\n    - upper\n  Errors:\n    - sum\n  Invocations:\n    - sum\n  IteratorAge:\n    - mean\n    - upper\n  ProvisionedConcurrencyInvocations:\n    - sum\n  ProvisionedConcurrencySpilloverInvocations:\n    - sum\n  ProvisionedConcurrencyUtilization:\n    - upper\n  ProvisionedConcurrentExecutions:\n    - upper\n  Throttles:\n    - sum\n  UnreservedConcurrentExecutions:\n    - upper\nAWS/Logs:\n  DeliveryErrors:\n    - sum\n  DeliveryThrottling:\n    - sum\n  ForwardedBytes:\n    - sum\n  ForwardedLogEvents:\n    - sum\n  IncomingBytes:\n    - sum\n  IncomingLogEvents:\n    - sum\nAWS/NATGateway:\n  ActiveConnectionCount:\n    - upper\n  BytesInFromDestination:\n    - sum\n  BytesInFromSource:\n    - sum\n  BytesOutToDestination:\n    - sum\n  BytesOutToSource:\n    - sum\n  ConnectionAttemptCount:\n    - sum\n  ConnectionEstablishedCount:\n    - sum\n  ErrorPortAllocation:\n    - sum\n  IdleTimeoutCount:\n    - sum\n  PacketsDropCount:\n    - sum\n  PacketsInFromDestination:\n    - sum\n  PacketsInFromSource:\n    - sum\n  PacketsOutToDestination:\n    - sum\n  PacketsOutToSource:\n    - sum\nAWS/NetworkELB:\n  ActiveFlowCount:\n    - mean\n    - upper\n    - lower\n  ActiveFlowCount_TLS:\n    - mean\n    - upper\n    - lower\n  ClientTLSNegotiationErrorCount:\n    - sum\n  HealthyHostCount:\n    - upper\n    - lower\n  NewFlowCount:\n    - sum\n  NewFlowCount_TLS:\n    - sum\n  ProcessedBytes:\n    - sum\n  ProcessedBytes_TLS:\n    - sum\n  TCP_Client_Reset_Count:\n    - sum\n  TCP_ELB_Reset_Count:\n    - sum\n  TCP_Target_Reset_Count:\n    - sum\n  TargetTLSNegotiationErrorCount:\n    - sum\n  UnHealthyHostCount:\n    - upper\n    - lower\nAWS/Polly:\n  2XXCount:\n    - mean\n    - count\n    - sum\n  4XXCount:\n    - mean\n    - count\n    - sum\n  5XXCount:\n    - mean\n    - count\n    - sum\n  ResponseLatency:\n    - mean\n    - upper\n    - lower\n    - count\nAWS/Route53:\n  ChildHealthCheckHealthyCount:\n    - mean\n  ConnectionTime:\n    - mean\n  HealthCheckPercentageHealthy:\n    - mean\n    - upper\n    - lower\n  HealthCheckStatus:\n    - lower\n  SSLHandshakeTime:\n    - mean\n  TimeToFirstByte:\n    - mean\nAWS/S3:\n  AllRequests:\n    - sum\n  BucketSizeBytes:\n    - mean\n  DeleteRequests:\n    - sum\n  GetRequests:\n    - sum\n  HeadRequests:\n    - sum\n  ListRequests:\n    - sum\n  NumberOfObjects:\n    - mean\n  PostRequests:\n    - sum\n  PutRequests:\n    - sum\n  SelectRequests:\n    - sum\nAWS/SNS:\n  NumberOfMessagesPublished:\n    - sum\n  NumberOfNotificationsDelivered:\n    - sum\n  NumberOfNotificationsFailed:\n    - mean\n    - sum\n  NumberOfNotificationsFilteredOut:\n    - mean\n    - sum\n  NumberOfNotificationsFilteredOut-InvalidAttributes:\n    - mean\n    - sum\n  NumberOfNotificationsFilteredOut-NoMessageAttributes:\n    - mean\n    - sum\n  PublishSize:\n    - mean\n    - upper\n    - lower\n    - count\n  SMSMonthToDateSpentUSD:\n    - upper\n  SMSSuccessRate:\n    - mean\n    - count\n    - sum\nAWS/SWF:\n  ActivityTaskScheduleToCloseTime:\n    - mean\n    - upper\n    - lower\n  ActivityTaskScheduleToStartTime:\n    - mean\n    - upper\n    - lower\n  ActivityTaskStartToCloseTime:\n    - mean\n    - upper\n    - lower\n  ActivityTasksCanceled:\n    - sum\n  ActivityTasksCompleted:\n    - sum\n  ActivityTasksFailed:\n    - sum\n  ConsumedCapacity:\n    - sum\n  DecisionTaskScheduleToStartTime:\n    - mean\n    - upper\n    - lower\n  DecisionTaskStartToCloseTime:\n    - mean\n    - upper\n    - lower\n  DecisionTasksCompleted:\n    - sum\n  PendingTasks:\n    - sum\n  ProvisionedBucketSize:\n    - lower\n  ProvisionedRefillRate:\n    - lower\n  ScheduledActivityTasksTimedOutOnClose:\n    - sum\n  ScheduledActivityTasksTimedOutOnStart:\n    - sum\n  StartedActivityTasksTimedOutOnClose:\n    - sum\n  StartedActivityTasksTimedOutOnHeartbeat:\n    - sum\n  StartedDecisionTasksTimedOutOnClose:\n    - sum\n  ThrottledEvents:\n    - sum\n  WorkflowStartToCloseTime:\n    - mean\n    - upper\n    - lower\n  WorkflowsCanceled:\n    - sum\n  WorkflowsCompleted:\n    - sum\n  WorkflowsContinuedAsNew:\n    - sum\n  WorkflowsFailed:\n    - sum\n  WorkflowsTerminated:\n    - sum\n  WorkflowsTimedOut:\n    - sum\nAWS/SageMaker:\n  DatasetObjectsAutoAnnotated:\n    - upper\n  DatasetObjectsHumanAnnotated:\n    - upper\n  DatasetObjectsLabelingFailed:\n    - upper\n  Invocation4XXErrors:\n    - mean\n    - sum\n  Invocation5XXErrors:\n    - mean\n    - sum\n  Invocations:\n    - count\n    - sum\n  InvocationsPerInstance:\n    - sum\n  JobsFailed:\n    - count\n    - sum\n  JobsStopped:\n    - count\n    - sum\n  JobsSucceeded:\n    - count\n    - sum\n  TotalDatasetObjectsLabeled:\n    - upper\nAWS/StorageGateway:\n  CloudBytesDownloaded:\n    - count\n    - sum\n  CloudBytesUploaded:\n    - count\n    - sum\n  CloudDownloadLatency:\n    - mean\n  ReadBytes:\n    - count\n    - sum\n  ReadTime:\n    - mean\n  WriteBytes:\n    - count\n    - sum\n  WriteTime:\n    - mean\nAWS/Translate:\n  CharacterCount:\n    - mean\n    - upper\n    - lower\n    - sum\n  ResponseTime:\n    - mean\n    - count\n  ServerErrorCount:\n    - mean\n    - sum\n  SuccessfulRequestCount:\n    - mean\n    - sum\n  ThrottledCount:\n    - mean\n    - sum\n  UserErrorCount:\n    - mean\n    - sum\nGlue:\n  glue.ALL.jvm.heap.usage:\n    - mean\n  glue.ALL.jvm.heap.used:\n    - mean\n  glue.ALL.s3.filesystem.read_bytes:\n    - sum\n  glue.ALL.s3.filesystem.write_bytes:\n    - sum\n  glue.ALL.system.cpuSystemLoad:\n    - mean\n  glue.driver.BlockManager.disk.diskSpaceUsed_MB:\n    - mean\n  glue.driver.ExecutorAllocationManager.executors.numberAllExecutors:\n    - mean\n  glue.driver.ExecutorAllocationManager.executors.numberMaxNeededExecutors:\n    - upper\n  glue.driver.aggregate.bytesRead:\n    - sum\n  glue.driver.aggregate.elapsedTime:\n    - sum\n  glue.driver.aggregate.numCompletedStages:\n    - sum\n  glue.driver.aggregate.numCompletedTasks:\n    - sum\n  glue.driver.aggregate.numFailedTasks:\n    - sum\n  glue.driver.aggregate.numKilledTasks:\n    - sum\n  glue.driver.aggregate.recordsRead:\n    - sum\n  glue.driver.aggregate.shuffleBytesWritten:\n    - sum\n  glue.driver.aggregate.shuffleLocalBytesRead:\n    - sum\n  glue.driver.jvm.heap.usage:\n    - mean\n  glue.driver.jvm.heap.used:\n    - mean\n  glue.driver.s3.filesystem.read_bytes:\n    - sum\n  glue.driver.s3.filesystem.write_bytes:\n    - sum\n  glue.driver.system.cpuSystemLoad:\n    - mean\n  glue.executorId.jvm.heap.usage:\n    - mean\nWAF:\n  AllowedRequests:\n    - sum\n  BlockedRequests:\n    - sum\n  CountedRequests:\n    - sum\n  PassedRequests:\n    - sum\n```\n\n----------------------------------------\n\nTITLE: Viewing MTS Creation Per Minute Limit in Splunk\nDESCRIPTION: This metric (`sf.org.limit.metricTimeSeriesCreatedPerMinute`) likely reflects the configured limit for Metric Time Series (MTS) creations per minute for the organization. This helps understand the threshold for potential MTS dropping.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.limit.metricTimeSeriesCreatedPerMinute\n```\n\n----------------------------------------\n\nTITLE: Using Timeshift in Data Visualizations with Splunk\nDESCRIPTION: The Timeshift function is used in Splunk Infrastructure Monitoring to visualize how a metric changes over a specified period, helping in trend analysis. Essential for infrastructure and application monitoring where knowing the trend is more crucial than the metric's absolute value. Expected inputs include the metric of interest and the time range (e.g., 5 minutes, 1 week). The output is a plot reflecting the trend over the chosen timeframe.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/gain-insights-through-chart-analytics.rst#2025-04-22_snippet_0\n\nLANGUAGE: none\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Callback Function for Asynchronous Metrics in Python\nDESCRIPTION: Defines a callback function to fetch temperature data for an asynchronous observable gauge metric in Python.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef get_temp_data(options: CallbackOptions) -> Iterable[Temperature]:\n   r = requests.get(\n      \"http://weather/data/city\", timeout=options.timeout_millis / 10**3\n   )\n    for metadata in r.json():\n       yield Temperature(\n             metadata[\"temperature\"], {\"city.name\": metadata[\"temperature\"]}\n       )\n```\n\n----------------------------------------\n\nTITLE: Installing Cloud Foundry Buildpack\nDESCRIPTION: Command to create and enable the OpenTelemetry Collector buildpack in Cloud Foundry\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-pivotal-cloudfoundry.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncf create-buildpack otel_collector_buildpack . 99 --enable\n```\n\n----------------------------------------\n\nTITLE: Configuring Display Format for Custom Metric Time Series\nDESCRIPTION: This configuration option determines how custom metric time series (MTS) data is displayed. Setting it to 'No' indicates that the display should show the percentage usage rather than the absolute number of custom MTS.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/autodetect/autodetect-list.rst#2025-04-22_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nNo\n```\n\n----------------------------------------\n\nTITLE: Specifying Default Collection Interval\nDESCRIPTION: Default value for collection_interval parameter, set to 10 seconds. This determines how frequently metrics are collected by the receiver.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/collector-settings-collectioninterval.rst#2025-04-22_snippet_1\n\nLANGUAGE: go\nCODE:\n```\n10s\n```\n\n----------------------------------------\n\nTITLE: Checking Operator Logs via kubectl - Bash\nDESCRIPTION: This snippet demonstrates how to retrieve logs from the Splunk OpenTelemetry Collector operator in a Kubernetes cluster using a label selector with the kubectl CLI. It uses the kubectl logs command filtered by the app.kubernetes.io/name=operator label. It requires kubectl to be installed and configured to access the relevant Kubernetes cluster. The input is the label selector, and the expected output is the log entries from the operator pods. Ensure correct context and permissions when running this command.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/troubleshoot-zeroconfig-k8s.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs -l app.kubernetes.io/name=operator\n```\n\n----------------------------------------\n\nTITLE: RST Table Definition for OpenTelemetry Connectors\nDESCRIPTION: A reStructuredText table that documents three types of OpenTelemetry connectors: routing connector, span-metrics connector, and sum connector. Each entry includes the connector name, a description of its functionality, and the telemetry pipeline types it supports.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/collector-available-connectors.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :widths: 25 55 20\n   :header-rows: 1\n   :width: 100%\n\n   * - Name\n     - Description\n     - Pipeline types\n   * - :ref:`routing-connector` (``routing``)\n     - Routes logs, metrics or traces based on resource attributes to specific pipelines using OpenTelemetry Transformation Language (OTTL) statements as routing conditions.\n     - Traces, metrics, logs\n   * - :ref:`span-metrics-connector` (``spanmetrics``)\n     - Aggregates Request, Error and Duration (R.E.D) OpenTelemetry metrics from span data.\n     - Traces, metrics\n   * - :ref:`sum-connector` (``sum``)\n     - Sums attribute values from spans, span events, metrics, data points, and log records.\n     - Traces, metrics, logs\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL Exporter in GitLab\nDESCRIPTION: Configuration example for GitLab Postgres Prometheus exporter to allow network connections on port 9187 from any IP address\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-gitlab/gitlab.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npostgres_exporter['listen_address'] = '0.0.0.0:9187'\n```\n\nLANGUAGE: yaml\nCODE:\n```\npostgres_exporter['listen_address'] = ':9187'\n```\n\n----------------------------------------\n\nTITLE: Stopping Podman Container\nDESCRIPTION: Stops a running Podman container by its ID or name before upgrading.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_36\n\nLANGUAGE: shell\nCODE:\n```\npodman stop <container_id_or_name>\n```\n\n----------------------------------------\n\nTITLE: Server Trace Response Headers for HTTP Tracing (Plaintext)\nDESCRIPTION: This plaintext snippet shows example HTTP response headers added by the SignalFx instrumentation to enable trace context propagation between servers and Real User Monitoring (RUM) clients. The headers 'Access-Control-Expose-Headers' and 'Server-Timing' (with a 'traceparent' descriptor) help tie together front-end and back-end traces. No runtime dependencies are required to interpret these headers, but web servers and clients must support reading them. Expected inputs are HTTP requests, and outputs are HTTP responses with added tracing headers. The trace id and span id are dynamically inserted in production environments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/configuration/advanced-php-configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nAccess-Control-Expose-Headers: Server-Timing\nServer-Timing: traceparent;desc=\"00-<serverTraceId>-<serverSpanId>-01\"\n```\n\n----------------------------------------\n\nTITLE: Installing CouchDB Python Requirements in Linux\nDESCRIPTION: Command to install Python dependencies required for the CouchDB integration. This should be run after moving the plugin file to the appropriate directory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/apache-couchdb.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Embedding Network Explorer Metrics List from GitHub YAML (HTML)\nDESCRIPTION: This HTML snippet uses a `div` element with specific attributes (`class=\"metrics-yaml\"`, `url`) to dynamically embed the list of available Network Explorer metrics. The content is fetched from a YAML file located at the specified URL within the `open-telemetry/opentelemetry-ebpf` GitHub repository. This relies on custom processing by the documentation build system (like Sphinx with an extension) to render the referenced YAML content.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"metrics-yaml\" url=\"https://raw.githubusercontent.com/open-telemetry/opentelemetry-ebpf/main/docs/metrics/metrics.yaml\"></div>\n```\n\n----------------------------------------\n\nTITLE: Installing MacOS/Windows Podman Runner\nDESCRIPTION: Installs a private runner on MacOS or Windows using Podman with network shaping disabled.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_41\n\nLANGUAGE: shell\nCODE:\n```\npodman run -e \"DISABLE_NETWORK_SHAPING=true\" -e \"RUNNER_TOKEN=YOUR_TOKEN_HERE\" \\\nquay.io/signalfx/splunk-synthetics-runner:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus NGINX VTS Receiver\nDESCRIPTION: Basic configuration to activate the Prometheus NGINX VTS integration in the Collector configuration. Defines the receiver with type prometheus-nginx-vts.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-nginx-vts.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/prometheus-nginx-vts:\n    type: prometheus-nginx-vts\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Understanding the Ignoring Receiver Error Message in OpenTelemetry Collector\nDESCRIPTION: This code block shows the error message that appears when a configured receiver is not used in any pipeline. The specific example shows the smartagent/http receiver being ignored.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/missing_pipeline_configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"2021-10-19T20:18:40.556Z info builder/receivers_builder.go:112 Ignoring receiver as it is not used by any pipeline {\"kind\": \"receiver\", \"name\": \"smartagent/http\"\n```\n\n----------------------------------------\n\nTITLE: Force Removing Running Podman Container\nDESCRIPTION: Forcibly removes a running Podman container when uninstalling a private runner.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_40\n\nLANGUAGE: shell\nCODE:\n```\npodman rm -f my_running_container\n```\n\n----------------------------------------\n\nTITLE: Moving VictorOps Configuration File\nDESCRIPTION: Uses the `mv` command to relocate the `victorops.cfg` file from its default location within the installed plugin directory (`/opt/victorops/nagios_plugin/nagios_conf/`) to the Icinga configuration directory (example: `/usr/local/nagios/etc`). This makes the configuration accessible to Icinga. The destination path might vary depending on the Icinga setup.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmv /opt/victorops/nagios_plugin/nagios_conf/victorops.cfg /usr/local/nagios/etc\n```\n\n----------------------------------------\n\nTITLE: Pairing Organizations Using ACS CLI\nDESCRIPTION: Command to pair Splunk Observability Cloud and Splunk Cloud Platform organizations using the Admin Config Services (ACS) CLI. Requires an O11y access token with admin privileges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/unified-id/unified-identity.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nacs observability pair --o11y-access-token \"<enter-o11y-access-token>\"\n```\n\n----------------------------------------\n\nTITLE: Defining RST Document Structure\nDESCRIPTION: RST markup defining the document structure for OpenTelemetry Collector components documentation, including toctree directives and section references.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _otel-components:\n\n******************************************\nCollector components\n******************************************\n\n.. meta::\n    :description: Learn about the components that make up the Splunk Observability Cloud OpenTelemetry Collector.\n\n.. toctree::\n    :titlesonly:\n    :hidden:\n\n    Receivers <components/a-components-receivers.rst>\n    Processors <components/a-components-processors.rst>\n    Exporters <components/a-components-exporters.rst>\n    Extensions <components/a-components-extensions.rst>  \n    Connectors <components/a-components-connectors.rst>\n```\n\n----------------------------------------\n\nTITLE: Configuring Slow Rendering Detection Poll Interval\nDESCRIPTION: Sets the polling interval duration in milliseconds for the slow rendering detection feature, which defaults to 1 second.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/rum-android-data-model.rst#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nslowRenderingDetectionPollInterval(Duration)\n```\n\n----------------------------------------\n\nTITLE: Setting the Splunk On-Call Adapter Authentication Key (Bash)\nDESCRIPTION: Sets an environment variable 'HUBOT_VICTOROPS_KEY' with the authentication key needed for Hubot to connect securely to Splunk On-Call. This key is obtained from the Splunk On-Call integrations UI. The exported value must be kept secret, and this environment variable needs to be set before starting the Hubot process.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/hubot-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport HUBOT_VICTOROPS_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n```\n\n----------------------------------------\n\nTITLE: Splunk Synthetic Monitoring Enter Value\nDESCRIPTION: Equivalent Splunk Synthetic Monitoring code for entering a value into a form field.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/browser-test/set-up-browser-test.rst#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{\n// Splunk Synthetic Monitoring code snippet\n   \"name\": \"\",\n   \"type\": \"enter_value\",\n   \"selector_type\": \"id\",\n   \"selector\": \"quantity\",\n   \"option_selector_type\": \"index\",\n   \"option_selector\": \"5\",\n   \"wait_for_nav\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating SCOM with Splunk On-Call using PowerShell\nDESCRIPTION: This PowerShell script integrates Microsoft SCOM with Splunk On-Call by fetching alert details using the OperationsManager module, formatting them into a JSON payload, and sending them to the Splunk On-Call generic REST endpoint. It requires the SCOM Alert ID, an optional routing key (defaults to 'everyone'), and an optional Splunk On-Call API key. The script determines the alert status (CRITICAL/RECOVERY), extracts relevant details like hostname and description, constructs a JSON body, logs the event locally (Event Log), and posts the alert to Splunk On-Call via Invoke-RestMethod. Configuration requires specifying the API key and potentially hardcoding the routing key in the API URI.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/microsoft-scom-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: powershell\nCODE:\n```\n< #.SYNOPSIS Splunk On-Call Alerting from Microsoft System Center Operations Manager (SCOM) through PowerShell Call this script from SCOM's command notification channel - be sure to update the API key (parameter #2). See Example for usage. .DESCRIPTION Post alerts to Splunk On-Call from Microsoft System Center Operations Manager (SCOM) through PowerShell .EXAMPLE FULL PATH OF THE COMMAND FILE: C:\\windows\\system32\\WindowsPowerShell\\v1.0\\powershell.exe COMMAND LINE PARAMETERS: \"C:\\scripts\\VO_Send-Alert_Test.ps1\" '\"$Data[Default='Not Present']/Context/DataItem/AlertId$\"' '\"$RoutingKey\"' '\"api-key\"' # !Change $RoutingKey to the routing key you would like to pass in or remove it alltogether to default to 'everyone' # !Change api-key to the organization api key or default it in parameter #2 STARTUP FOLDER FOR THE COMMAND LINE: C:\\windows\\system32\\WindowsPowerShell\\v1.0\\ .EXAMPLE # Easier to look at: \"C:\\scripts\\VO_Send-Alerts.ps1\" ^ '\"$Data[Default='Not Present']/Context/DataItem/AlertId$\"' ^ '\"$RoutingKey\"'' ^ '\"api-key\"' # Change $RoutingKey to the routing key you would like to pass in, or remove it alltogether to default to 'everyone' #>\nParam (\n   [Parameter(Mandatory = $true , Position = 0, HelpMessage = \"Unique AlertID must be provided.\")][GUID]$AlertID,\n   [Parameter(Mandatory = $false, Position = 1, HelpMessage = \"Team routing key, optional.\")]     [String]$RoutingKey = \"everyone\",\n   [Parameter(Mandatory = $false, Position = 2, HelpMessage = \"Organization API key (see REST API Integrations settings in Splunk On-Call), optional.\")][String]$API = \"\",\n   [Parameter(Mandatory = $false, Position = 3, HelpMessage = \"Monitoring tool, optional.\")]      [String]$MonitoringTool = \"Microsoft System Center Operations Manager\"\n)\n   \n# Required: OperationsManager Module\nImport-Module OperationsManager\n   \n## Find the alert\n$Alert = Get-SCOMAlert | where { $_.id -eq $AlertID.ToString() }\n   \n# Determine the status of the alert\nswitch ($Alert.ResolutionState) {\n   0 { $Status = \"CRITICAL\" } \n   255 { $Status = \"RECOVERY\" }\n   default { $Status = \"CRITICAL\" }\n}\n   \n# workaround for unix/linux hosts - hostnames do not come through clearly in SCOM\nif ($Alert.NetbiosComputerName -ne $null) { $hstname = $alert.NetbiosComputerName }\nelseif ($Alert.MonitoringObjectPath -ne $null) { $hstname = $alert.MonitoringObjectFullName }\nelse { $hstname = $alert.MonitoringObjectName }\n   \n## Setup our Subject & StateMessage to be passed\n[String]$Subject = $Status + \": \" + $Alert.Name + \" [\" + $hstname + \"]\"\n[String]$StateMessage = \"Description: \" + $Alert.Description + \"`n\" `\n   + \"Hostname: \" + $hstname + \"`n\" `\n   + \"Timestamp: \" + $Alert.TimeRaised.ToLocalTime() + \" PST `n\" `\n   + \"Team: \" + $RoutingKey.ToUpper() + \"`n\" `\n   + \"Last modified by: \" + $Alert.LastModifiedBy + \"`n\" `\n   + \"Last modified time: \" + $Alert.LastModified + \"`n\" `\n   \n## Convert to json\n$props = @{\n   message_type        = $Status; #[String] One of the following values: INFO, WARNING, ACKNOWLEDGMENT, CRITICAL, RECOVERY\n   timestamp           = $Alert.TimeRaised.ToLocalTime(); #[Number] Timestamp of the alert in seconds since epoch. Defaults to the time the alert is received at Splunk On-Call.\n   entity_id           = $Alert.id.ToString(); #[String] The name of alerting entity. If not provided, a random name will be assigned.\n   entity_display_name = $Subject; #[String] Used within Splunk On-Call to display a human-readable name for the entity.\n   hostname            = $hstname; #[String] System hostname (set above via logic)\n   monitoring_tool     = $MonitoringTool; #[String] The name of the monitoring system software (eg. nagios, icinga, sensu, etc.)\n   state_message       = $StateMessage; #[String] Any additional status information from the alert item.\n   Subject             = $Subject;\n}\n$json = ConvertTo-Json -InputObject $props\n   \n## Log alert\n   \n# Event-log\n$CheckEventLog = (Get-EventLog -List | ? Log -EQ \"OM Alerts\")\nif ($CheckEventLog -eq $null) { \n   try { New-EventLog -LogName \"OM Alerts\" -Source \"Splunk On-Call Alerts\" }\n   catch { Write-Error \"Please rerun the script from a Windows PowerShell console with admin rights ('Run As Administrator'). Cannot continue.\"; Break }\n}\n$event_message = $StateMessage + \"`n Command: \" + \"Invoke-RestMethod -Method Post -ContentType `\"application/json`\" -Body `n$json`n -Uri `\"https://alert.victorops.com/integrations/generic/20131114/alert/$API/$RoutingKey`\"\"\n   \nWrite-EventLog -LogName \"OM Alerts\" -Source \"Splunk On-Call Alerts\" -Message $event_message -EventId 2 -EntryType Information\n   \n< # Text file $Logstring = $StateMessage.replace(\"`n\",\" \") $Logfile = \"C:\\scripts\\VO_Send-Alerts.log\" $DateTime = Get-Date -Uformat \"%y-%m-%d %H:%M:%S\" $Logstring = $DateTime + \" \" + $Logstring Add-content $Logfile -value $Logstring $json | Out-File -FilePath \"C:\\scripts\\VO_Send-Alerts.$RoutingKey.json\" #>\n   \n# Post the alert\nInvoke-RestMethod -Method Post `\n   -ContentType \"application/json\" `\n   -Body $json `\n   -Uri \"https://alert.victorops.com/integrations/generic/20131114/alert/$API/$RoutingKey\"\n```\n\n----------------------------------------\n\nTITLE: Restarting Splunk OTel Collector Service\nDESCRIPTION: This command restarts the splunk-otel-collector service using systemctl.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl restart splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Deploying BOSH Release Command\nDESCRIPTION: Command to deploy the OpenTelemetry Collector using BOSH release deployment file\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-pivotal-cloudfoundry.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbosh -d splunk-otel-collector deploy deployment.yaml\n```\n\n----------------------------------------\n\nTITLE: HTML Section Header for Edit Page Link\nDESCRIPTION: Embedded HTML markup for the edit page section header with anchor link\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/contribute.rst#2025-04-22_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<embed>\n  <h2>Edit this page link<a name=\"contribute-edit\" class=\"headerlink\" href=\"#contribute-edit\" title=\"Permalink to this headline\">¶</a></h2>\n</embed>\n```\n\n----------------------------------------\n\nTITLE: Recreating Containers with Updated Images\nDESCRIPTION: Shell command to recreate Docker Compose containers with updated images in detached mode, part of the upgrade process for private runners.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Pushing Grafana Docker Image (Shell)\nDESCRIPTION: Shell command to push the built Docker image to a Docker registry. Replace `<imagePath>`, `<imageName>`, and `<tagName>` with the appropriate values for your registry and image.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_3\n\nLANGUAGE: none\nCODE:\n```\ndocker push <imagePath/><imageName>:<tagName>\n```\n\n----------------------------------------\n\nTITLE: Sending Test Message to AlertManager\nDESCRIPTION: This shell command uses curl to send a test alert message to the AlertManager instance. It is useful for verifying AlertManager's configuration and response. It requires that the AlertManager is running and accessible at the specified URL.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/prometheus-integration.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl -H “Content-Type: application/json” -d '[{\"labels\":{\"alertname\":\"TestAlert\"}}]'  localhost:9093/api/v1/alerts\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: ReStructuredText markup defining the documentation structure for an uptime test monitoring scenario, including meta description and image references.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/synth-scenarios/uptime-test-scenario.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _uptime-test-scenario:\n\n************************************************************************************\nScenario: Monitor the performance of a user-facing application \n************************************************************************************\n\n.. meta::\n    :description: Fictional use case describing how to proactively prevent issues with a user-facing application or site using an uptime test from Splunk Synthetic Monitoring.\n\n..  image:: /_images/synthetics/Buttercup-uptime-test.png\n    :width: 100% \n    :alt: This image shows a completed browser test.\n```\n\n----------------------------------------\n\nTITLE: Monitoring Custom MTS Count Against Entitlement in Splunk\nDESCRIPTION: This metric (`sf.org.numCustomMetrics`) tracks the number of custom Metric Time Series (MTS) currently active. It is used to monitor usage against the contract entitlement limit to prevent potential overage charges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.numCustomMetrics\n```\n\n----------------------------------------\n\nTITLE: Pairing Organizations Using API Endpoint\nDESCRIPTION: Curl command to pair organizations using the API endpoint. Requires Splunk Cloud Platform admin API token, O11y API token, and instance name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/unified-id/unified-identity.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST 'https://admin.splunk.com/<enter-stack-name>/adminconfig/v2/observability/sso-pairing' \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer <enter-splunk-admin-api-token>\" \\\n-H \"o11y-access-token: <enter-o11y-api-token>\"\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Collector Configuration\nDESCRIPTION: YAML configuration for OpenTelemetry Collector setting up collectd receiver and debug exporter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/other-ingestion-collectd.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n   collectd:\n      endpoint: \"0.0.0.0:8081\"\n\nexporters:\n   debug:\n      verbosity: detailed\n\nservice:\n   pipelines:\n      metrics:\n         receivers: [collectd]\n         exporters: [debug]\n```\n\n----------------------------------------\n\nTITLE: Initializing Browser RUM with Debug Logging in HTML\nDESCRIPTION: Example configuration for initializing Splunk RUM with debug logging enabled. Includes essential parameters like beaconEndpoint, rumAccessToken, applicationName, and version.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\" crossorigin=\"anonymous\"></script>\n<script>\n      SplunkRum.init(\n      {\n         beaconEndpoint: 'https://rum-ingest.us0.signalfx.com/v1/rum'\n         rumAccessToken: 'ABC123...789',\n         applicationName: 'my-awesome-app',\n         version: '1.0.1',\n         debug: true\n      });\n</script>\n```\n\n----------------------------------------\n\nTITLE: Invalid Cloud Foundry Receiver Configuration Example\nDESCRIPTION: This configuration example demonstrates an invalid setup for the Cloud Foundry receiver with an incorrect endpoint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/cloudfoundry-receiver.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ncloudfoundry/invalid:\n  rlp_gateway:\n    endpoint: \"https://[invalid\"\n    shard_id: \"otel-test\"\n    timeout: \"20s\"\n    tls:\n      insecure_skip_verify: true\n  uaa:\n    endpoint: \"https://uaa.sys.example.internal\"\n    username: \"admin\"\n    password: \"test\"\n    tls:\n      insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Deploying Grafana Manifest to Kubernetes (Shell)\nDESCRIPTION: Shell command using kubectl to apply the Kubernetes Deployment and Service defined in the `deploy.yaml` file to the cluster.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_6\n\nLANGUAGE: none\nCODE:\n```\nkubectl create -f deploy.yaml\n```\n\n----------------------------------------\n\nTITLE: Force-Removing Running Docker Container\nDESCRIPTION: Shell command to forcefully remove a running container without stopping it first, used for quick uninstallation of private runners.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ndocker rm -f my_running_container\n```\n\n----------------------------------------\n\nTITLE: Configuring Volley HTTP Tracing\nDESCRIPTION: Example of initializing Volley HTTP tracing with Splunk RUM.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/configure-rum-android-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nVolleyTracing volleyTracing = VolleyTracing.builder(splunkRum).build();\n```\n\n----------------------------------------\n\nTITLE: Activating Elasticsearch Query Integration in Collector Configuration\nDESCRIPTION: This YAML snippet shows how to activate the Elasticsearch Query integration in the Splunk Distribution of OpenTelemetry Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/elasticsearch-query.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/elasticsearch-query:\n    type: elasticsearch-query\n    ... # Additional config\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/elasticsearch-query]\n```\n\n----------------------------------------\n\nTITLE: Adjusting Go OpenTelemetry Batch Span Processor Timeouts and Sizes (Bash)\nDESCRIPTION: Sets environment variables to adjust the batch span processor's behavior for potentially unstable connections or inadequate bandwidth. It reduces the export timeout (`OTEL_BSP_EXPORT_TIMEOUT`), increases the time between exports (`OTEL_BSP_SCHEDULE_DELAY`), increases the queue size (`OTEL_BSP_MAX_QUEUE_SIZE`), and decreases the batch size (`OTEL_BSP_MAX_EXPORT_BATCH_SIZE`). Ensure sufficient memory for the increased queue size.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/common-go-troubleshooting.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# 5s export timeout.\nexport OTEL_BSP_EXPORT_TIMEOUT=5000\n# 30s maximum time between exports.\nexport OTEL_BSP_SCHEDULE_DELAY=30000\nexport OTEL_BSP_MAX_QUEUE_SIZE=5120\nexport OTEL_BSP_MAX_EXPORT_BATCH_SIZE=128\n```\n\n----------------------------------------\n\nTITLE: Downloading SignalFx PHP Installation Script using Shell\nDESCRIPTION: Uses the `curl` command to download the SignalFx PHP tracing library setup script (`signalfx-setup.php`) from its GitHub release location. This is the first step in the manual installation process.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/instrumentation/instrument-php-application.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -LO  https://github.com/signalfx/signalfx-php-tracing/releases/latest/download/signalfx-setup.php\n```\n\n----------------------------------------\n\nTITLE: Installing Homebrew Package Manager on macOS\nDESCRIPTION: Command to install the Homebrew package manager on macOS, which is required for installing the other tools in this tutorial.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/collector-config-tutorial-start.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Web Security in Chrome\nDESCRIPTION: Flag to disable the same origin policy enforcement\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/synthetics/chrome-flags.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n--disable-web-security\n```\n\n----------------------------------------\n\nTITLE: Using Filters with Variables in SignalFlow\nDESCRIPTION: Shows how to correctly apply filters directly in the data function call, without using separate variable assignments, to ensure compatibility with the plot-builder.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/chart-builder.rst#2025-04-22_snippet_7\n\nLANGUAGE: SignalFlow\nCODE:\n```\nA = data('cpu.utilization', filter=filter('aws_availability_zone', 'us-east-1a')).publish(label='A')\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector Using a Values YAML File in Bash\nDESCRIPTION: Helm command to install the Splunk OpenTelemetry Collector using a YAML file for configuration values instead of command-line arguments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm install my-splunk-otel-collector --values my_values.yaml splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Restarting Grafana Deployment (Shell)\nDESCRIPTION: Shell command using kubectl to perform a rolling restart of the Grafana deployment (`splunk-grafana`). This is necessary to apply changes, such as mounting the newly created ConfigMap for data source provisioning.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_10\n\nLANGUAGE: none\nCODE:\n```\nkubectl rollout restart deployment my-deployment splunk-grafana\n```\n\n----------------------------------------\n\nTITLE: Updating AWS Integration Using cURL API Command\nDESCRIPTION: Command for updating an AWS integration using the Splunk Observability Cloud API. Uses cURL to send a PUT request with the integration configuration JSON.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-troubleshooting.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request PUT https://api.<realm>.signalfx.com/v2/integration/<integration-id>\n   --header \"X-SF-TOKEN:\" \\\n   --header \"Content-Type:application/json\" \\\n   --data \"@integration.json\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy Server in Chrome\nDESCRIPTION: Flag to specify a custom proxy server configuration\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/synthetics/chrome-flags.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n--proxy-server=\"foopy:8080\"\n```\n\n----------------------------------------\n\nTITLE: Uninstalling OpenShift Private Runner\nDESCRIPTION: Removes a private runner deployment from an OpenShift cluster.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_33\n\nLANGUAGE: shell\nCODE:\n```\noc delete -f deployment.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Nagios Plugin DEB Package using apt (Shell)\nDESCRIPTION: Provides an alternative method to install the downloaded Nagios plugin DEB package using the `apt install` command. Replace `<path_to_file>` with the actual path to the downloaded .deb file. This command typically handles dependencies automatically and requires root privileges via `sudo`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt install <path_to_file>\n```\n\n----------------------------------------\n\nTITLE: RST Image Inclusion\nDESCRIPTION: RST markup for including and formatting images in the documentation\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/synth-scenarios/browser-test-scenario.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n..  image:: /_images/synthetics/browser-test-one.png\n    :width: 100% \n    :alt: This image shows a completed browser test.\n```\n\n----------------------------------------\n\nTITLE: Setting Up reStructuredText References and Includes for AWS Documentation\nDESCRIPTION: This snippet contains reStructuredText directives that define references, metadata, and includes external content about AWS integrations. It uses raw HTML div tags to mark include sections and references other documentation pages.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/integrations/cloud-aws.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _cloud-aws:\n.. _aws-integrations:\n\n********************************************************************************\nAvailable Amazon Web Services integrations\n********************************************************************************\n\n.. meta::\n   :description: Landing for available AWS services.\n\nTo learn about AWS and Splunk Observability Cloud, read :ref:`get-started-aws`.\n\n\n\n.. raw:: html\n\n   <div class=\"include-start\" id=\"gdi/available-aws.rst\"></div>\n\n.. include:: /_includes/gdi/available-aws.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"gdi/available-aws.rst\"></div>\n\n```\n\n----------------------------------------\n\nTITLE: Adding Syslog Receiver to OpenTelemetry Collector Configuration\nDESCRIPTION: Demonstrates how to add a basic syslog receiver entry to the OpenTelemetry Collector configuration file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-edit.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  syslog:\n```\n\n----------------------------------------\n\nTITLE: Dropping Spans Using spanFilter in JavaScript for iOS\nDESCRIPTION: This JavaScript snippet configures the `spanFilter` option for iOS to conditionally drop spans based on their name or to redact all `http.url` attributes within spans, preventing sensitive data exposure.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/sensitive-data-rum.rst#2025-04-22_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\noptions.spanFilter = { spanData in\n      var spanData = spanData\n      if spanData.name == \"DropThis\" {\n        return nil // spans with this name aren't sent\n      }\n      var atts = spanData.attributes\n      atts[\"http.url\"] = .string(\"redacted\") // change values for all urls\n      return spanData.settingAttributes(atts)\n    }\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk Platform EKS Add-on without Secure Token Handling\nDESCRIPTION: YAML configuration for Splunk Platform with the HEC token embedded directly in the configuration. This less secure approach includes the token as plain text in the EKS Add-on configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s-addon-eks.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsplunkPlatform:\n    endpoint: http://localhost:8088/services/collector\n    token: <YOUR_HEC_TOKEN>\nclusterName: <EKS_CLUSTER_NAME>\ncloudProvider: aws\ndistribution: eks\n```\n\n----------------------------------------\n\nTITLE: Embedding External YAML Content in RST\nDESCRIPTION: This snippet demonstrates how to embed external YAML content into an RST document using raw HTML. It specifies a URL for the YAML content to be loaded dynamically.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/host-metadata.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"metrics-yaml\" url=\"https://raw.githubusercontent.com/signalfx/splunk-otel-collector/main/internal/signalfx-agent/pkg/monitors/metadata/hostmetadata/metadata.yaml\"></div>\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Macros in Nagios Configuration\nDESCRIPTION: Enables the environment macros in Nagios configuration, which is required for the shell script that sends alerts to Splunk On-Call. This directive should be added to the nagios.cfg file if it doesn't already exist.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nenable_environment_macros=1\n```\n\n----------------------------------------\n\nTITLE: Configuring Application Receivers Landing Page using reStructuredText\nDESCRIPTION: This snippet serves as a top-level landing page using reStructuredText for configuring application receivers within Splunk Observability Cloud, providing users with navigation to platform-specific guides for Heroku, Kong, and OpenStack. The snippet includes introductory descriptions, page metadata, a table of contents, and references to sub-guides. Dependencies: Sphinx/reStructuredText rendering. Inputs: None; the snippet enables structured navigation for documentation builds. Outputs: Hyperlinked indices and context for end users, with no runtime constraints.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/cloud.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _cloud:\n\n********************************************************************************\nConfigure application receivers for cloud platforms\n********************************************************************************\n\n.. meta::\n   :description: Landing for application receivers for cloud platforms in Splunk Observability Cloud.\n\n.. toctree::\n   :maxdepth: 4\n\n   monitors-cloud/heroku\n   monitors-cloud/kong\n   monitors-cloud/openstack\n\nThese application receivers gather metrics from their associated cloud platform-related applications and the hosts the applications are running on.\n\n* :ref:`heroku`\n* :ref:`kong`\n* :ref:`openstack`\n```\n\n----------------------------------------\n\nTITLE: Creating NodePort Service for Spring PetClinic\nDESCRIPTION: Commands to expose the Spring PetClinic application through a NodePort service and retrieve the port number.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-java-traces-tutorial/deploy-collector-k8s-java.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl expose deployment/spring-petclinic --type=\"NodePort\" --port 8080\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl describe svc spring-petclinic | grep \"NodePort\"\n```\n\n----------------------------------------\n\nTITLE: Alert Payload Template for Multichannel Configuration\nDESCRIPTION: JSON template for configuring webhook payload in Splunk On-Call multichannel setup\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/slack-integration-guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n{\n\"text\":\"${{ALERT.entity_display_name}},${{ALERT.entity_id}},${{ALERT.state_message}}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Note About PowerShell Requirement in reStructuredText\nDESCRIPTION: This code snippet adds a note in reStructuredText format to specify the PowerShell version requirement for the Collector installation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/requirements/collector-windows.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. note:: PowerShell 3.0 or higher is required.\n```\n\n----------------------------------------\n\nTITLE: Stopping and Removing Docker Compose Services\nDESCRIPTION: Shell command to stop and remove all Docker Compose services defined in the docker-compose.yml file, used for uninstalling the private runner.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\ndocker-compose down\n```\n\n----------------------------------------\n\nTITLE: Monitoring Custom MTS Count Against Burst Limit in Splunk\nDESCRIPTION: This metric (`sf.org.numCustomMetrics`) tracks the number of active custom Metric Time Series (MTS) within a moving 60-minute window. It is used to monitor usage against the custom MTS burst/overage limit, exceeding which stops data acceptance for *new* custom MTS.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.numCustomMetrics\n```\n\n----------------------------------------\n\nTITLE: Filtering Synthetic Runs by API Test Type in Splunk\nDESCRIPTION: Use the filter `test_type=API` with metrics such as `synthetics.run.count` to count synthetic runs specifically originating from API tests within Splunk Synthetic Monitoring. This helps track usage related to API endpoint monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/synthetics-usage.rst#2025-04-22_snippet_2\n\nLANGUAGE: Splunk Metric/Filter\nCODE:\n```\ntest_type=API\n```\n\n----------------------------------------\n\nTITLE: Disabling Proxy Server in Chrome\nDESCRIPTION: Flag to disable proxy server usage and force direct connections, overrides other proxy settings\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/synthetics/chrome-flags.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n--no-proxy-server\n```\n\n----------------------------------------\n\nTITLE: Listing China AWS Regions for Splunk Observability Cloud\nDESCRIPTION: This snippet lists the China AWS regions supported by Splunk Observability Cloud, including their region codes and corresponding geographical locations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-prereqs.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``cn-north-1``: China (Beijing)\n* ``cn-northwest-1``: China (Ningxia)\n```\n\n----------------------------------------\n\nTITLE: Constructing JSON Payload for Rollbar Resolution Webhook\nDESCRIPTION: Defines the JSON payload ('Payload' field) sent by the Splunk On-Call outgoing webhook configuration. This payload sets the 'status' field of the corresponding Rollbar item to 'resolved' when sent via the PATCH request to the Rollbar API.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rollbar-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{“status”:“resolved”}\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Smart Agent on Debian-based Linux\nDESCRIPTION: Command to remove the SignalFx Smart Agent package on Debian-based Linux distributions including Ubuntu.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nsudo dpkg --remove signalfx-agent\n```\n\n----------------------------------------\n\nTITLE: Configuring Node Filter for Agent Mode\nDESCRIPTION: Configuration example for filtering pods by node when running the Kubernetes attributes processor in agent mode.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-attributes-processor.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  containers:\n  - env:\n    - name: KUBE_NODE_NAME\n      valueFrom:\n        fieldRef:\n          apiVersion: v1\n          fieldPath: spec.nodeName\n```\n\nLANGUAGE: yaml\nCODE:\n```\nk8sattributes:\n  filter:\n    node_from_env_var: KUBE_NODE_NAME\n```\n\n----------------------------------------\n\nTITLE: Upgrading Splunk Synthetics Runner with Helm\nDESCRIPTION: Shell command for upgrading an existing Splunk Synthetics Runner Helm deployment while preserving configured values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\nhelm upgrade my-splunk-synthetics-runner synthetics-helm-charts/splunk-synthetics-runner --reuse-values\n```\n\n----------------------------------------\n\nTITLE: Creating RST List Table for Data Source Comparison\nDESCRIPTION: RST directive to create a formatted table comparing different data sources (Kubernetes, Linux, Windows) and their capabilities for providing metrics, traces, and logs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/compute/compute.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 40 20 20 20\n   :width: 100%\n   :class: monitor-table\n\n   * - :strong:`Data source`\n     - :strong:`Provides metrics`\n     - :strong:`Provides traces`\n     - :strong:`Provides logs`\n\n   * - :ref:`Kubernetes <get-started-k8s>`\n     - :strong:`X`\n     - :strong:`X`\n     - :strong:`X`\n\n   * - :ref:`Linux <get-started-linux>`\n     - :strong:`X`\n     - :strong:`X`\n     -\n\n   * - :ref:`Microsoft Windows <get-started-windows>`\n     - :strong:`X`\n     - :strong:`X`\n     -\n```\n\n----------------------------------------\n\nTITLE: Listing Podman Containers\nDESCRIPTION: Lists all Podman containers, including stopped ones, for management purposes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_39\n\nLANGUAGE: shell\nCODE:\n```\npodman ps -a\n```\n\n----------------------------------------\n\nTITLE: Searching Documentation with grep - Bash\nDESCRIPTION: Demonstrates how to use the grep command-line tool to recursively search for occurrences of the string 'OpenTelemetry' within all reStructuredText (*.rst) files in the project directory. The command outputs file names and line numbers where matches are found. Requires a Unix-like shell environment with grep installed; no special parameters beyond input string and file extension are needed. Useful for quickly locating documentation references.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngrep -inro --include \\*.rst \"OpenTelemetry\" .\n```\n\n----------------------------------------\n\nTITLE: Raw HTML Div Marker for Include Stop\nDESCRIPTION: Defines an HTML div element to mark the end of an included section with a specific ID for troubleshooting components.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/http.rst#2025-04-22_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"include-stop\" id=\"troubleshooting-components.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Instrumented Kubernetes Pod\nDESCRIPTION: Example output of a kubectl describe command for an instrumented pod. It shows the presence of an OpenTelemetry auto-instrumentation init container and various OTEL_* environment variables in the application container.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n# Name:             opentelemetry-demo-frontend-57488c7b9c-4qbfb\n# Namespace:        otel-demo\n# Annotations:      instrumentation.opentelemetry.io/inject-java: true\n# Status:           Running\n# Init Containers:\n#   opentelemetry-auto-instrumentation:\n#     Command:\n#       cp\n#       -a\n#       /autoinstrumentation/.\n#       /otel-auto-instrumentation/\n#     State:          Terminated\n#       Reason:       Completed\n#       Exit Code:    0\n# Containers:\n#   frontend:\n#     State:          Running\n#     Ready:          True\n#     Environment:\n#       FRONTEND_PORT:                              8080\n#       FRONTEND_ADDR:                              :8080\n#       AD_SERVICE_ADDR:                            opentelemetry-demo-adservice:8080\n#       CART_SERVICE_ADDR:                          opentelemetry-demo-cartservice:8080\n#       CHECKOUT_SERVICE_ADDR:                      opentelemetry-demo-checkoutservice:8080\n#       CURRENCY_SERVICE_ADDR:                      opentelemetry-demo-currencyservice:8080\n#       PRODUCT_CATALOG_SERVICE_ADDR:               opentelemetry-demo-productcatalogservice:8080\n#       RECOMMENDATION_SERVICE_ADDR:                opentelemetry-demo-recommendationservice:8080\n#       SHIPPING_SERVICE_ADDR:                      opentelemetry-demo-shippingservice:8080\n#       WEB_OTEL_SERVICE_NAME:                      frontend-web\n#       PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT:  http://localhost:8080/otlp-http/v1/traces\n#       NODE_OPTIONS:                                --require /otel-auto-instrumentation/autoinstrumentation.java\n#       SPLUNK_OTEL_AGENT:                           (v1:status.hostIP)\n#       OTEL_SERVICE_NAME:                          opentelemetry-demo-frontend\n#       OTEL_EXPORTER_OTLP_ENDPOINT:                http://$(SPLUNK_OTEL_AGENT):4317\n#       OTEL_RESOURCE_ATTRIBUTES_POD_NAME:          opentelemetry-demo-frontend-57488c7b9c-4qbfb (v1:metadata.name)\n#       OTEL_RESOURCE_ATTRIBUTES_NODE_NAME:          (v1:spec.nodeName)\n#       OTEL_PROPAGATORS:                           tracecontext,baggage,b3\n#       OTEL_RESOURCE_ATTRIBUTES:                   splunk.zc.method=autoinstrumentation-java:0.41.1,k8s.container.name=frontend,k8s.deployment.name=opentelemetry-demo-frontend,k8s.namespace.name=otel-demo,k8s.node.name=$(OTEL_RESOURCE_ATTRIBUTES_NODE_NAME),k8s.pod.name=$(OTEL_RESOURCE_ATTRIBUTES_POD_NAME),k8s.replicaset.name=opentelemetry-demo-frontend-57488c7b9c,service.version=1.5.0-frontend\n#     Mounts:\n#       /otel-auto-instrumentation from opentelemetry-auto-instrumentation (rw)\n# Volumes:\n#   opentelemetry-auto-instrumentation:\n#     Type:        EmptyDir (a temporary directory that shares a pod's lifetime)\n```\n\n----------------------------------------\n\nTITLE: Importing OpenTelemetry Python Dependencies\nDESCRIPTION: Python import statements for required OpenTelemetry packages to implement custom metric collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/other-ingestion-methods/send-custom-metrics.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry.exporter.otlp.proto.grpc.metric_exporter import (\n   OTLPMetricExporter,\n)\nfrom opentelemetry.metrics import (\n   CallbackOptions,\n   Observation,\n   get_meter_provider,\n   set_meter_provider,\n)\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\n```\n\n----------------------------------------\n\nTITLE: Setting Proxy Bypass List in Chrome\nDESCRIPTION: Flag to specify hosts that bypass the proxy server, requires --proxy-server flag to be set\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/synthetics/chrome-flags.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n--proxy-bypass-list=\"*.google.com;*foo.com;127.0.0.1:8080\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up mavenLocal Repository in build.gradle\nDESCRIPTION: Code snippet showing how to add the local Maven repository to the build.gradle file to use locally built Android RUM libraries.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_8\n\nLANGUAGE: kotlin\nCODE:\n```\nallprojects {\n   repositories {\n      google()\n//...\n      mavenLocal()\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: AWS CloudWatch Rollup Translation Table\nDESCRIPTION: Table showing the mapping between AWS CloudWatch statistics and their corresponding Splunk Infrastructure Monitoring dimension values, along with definitions of each statistic type.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/monitor/aws-infra-import.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :width: 100\n   :widths: 25 25 50\n\n   *  - :strong:`AWS statistic`\n      - :strong:`IM dimension`\n      - :strong:`Definition`\n\n   *  - Average\n      - stat:mean\n      - Mean value of metric over the sampling period\n\n   *  - Maximum\n      - stat:upper\n      - Maximum value of metric over the sampling period\n\n   *  - Minimum\n      - stat:lower\n      - Minimum value of metric over the sampling period\n\n   *  - Data Samples\n      - stat:count\n      - Number of samples over the sampling period\n\n   *  - Sum\n      - stat:sum\n      - Sum of all values that occurred over the sampling period\n```\n\n----------------------------------------\n\nTITLE: Configuring Network Scraper in YAML\nDESCRIPTION: YAML configuration for the network scraper, allowing inclusion or exclusion of specific interfaces.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/host-metrics-receiver.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nnetwork:\n  <include|exclude>:\n    interfaces: [ <interface name>, ... ]\n    match_type: <strict|regexp>\n```\n\n----------------------------------------\n\nTITLE: Configuring Insecure Origins as Secure in Chrome\nDESCRIPTION: Flag to treat specified insecure origins as secure, allowing multiple comma-separated origins\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/synthetics/chrome-flags.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n--unsafely-treat-insecure-origin-as-secure=http://a.test,http://b.test\n```\n\n----------------------------------------\n\nTITLE: Pulling Latest Podman Image\nDESCRIPTION: Updates the private runner by pulling the latest image from the registry.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_35\n\nLANGUAGE: shell\nCODE:\n```\npodman pull http://quay.io/signalfx/splunk-synthetics-runner:latest\n```\n\n----------------------------------------\n\nTITLE: Starting Riemann Client in IRB - Ruby\nDESCRIPTION: This Bash command launches an interactive Ruby session (irb) and requires the riemann/client gem, preparing the environment for sending alerts to Riemann. Prerequisite: 'riemann-client' Ruby gem must be installed. Input: none; Output: ready IRB session configured for Riemann API usage.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/riemann-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ irb -r riemann/client\n```\n\n----------------------------------------\n\nTITLE: Defining RST Document Structure\nDESCRIPTION: RST markup defining the document structure, table of contents, and metadata for Python instrumentation documentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/get-started-1x.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _get-started-python-1x:\n\n**************************************************************\nInstrument Python applications for Splunk Observability Cloud\n**************************************************************\n\n\n.. meta::\n   :description: Instrument Python applications automatically to export spans and metrics to Splunk Observability Cloud.\n\n.. toctree::\n   :hidden:\n\n   Instrument your Python application <instrument-python-application-1x>\n   Configure the Python agent <advanced-python-otel-configuration-1x>\n   Metrics and attributes <python-otel-metrics-1x>\n\n.. note:: \n   \n   The Splunk Distribution of OpenTelemetry Python version 1.X is deprecated as of February 28, 2025 and will reach end of support on February 28, 2026. Until then, only critical security fixes and bug fixes will be provided.\n\n   New customers should use the Splunk OpenTelemetry Python agent version 2.0. Existing customers should consider migrating to Splunk OpenTelemetry Python 2.0 or higher. See :ref:`python-migration-guide`.\n```\n\n----------------------------------------\n\nTITLE: Embedding Navigator List Widget Using Raw HTML in Documentation - HTML\nDESCRIPTION: This snippet injects a custom HTML widget for listing navigators by referencing an external YAML file as the data source. It uses a <div> element with custom attributes to define the YAML source URL, relevant data columns, and header labels for display in the documentation build. This approach enables dynamic population of navigator metadata into the page without requiring JavaScript or backend processing.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/navigators-list.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\\\"metrics-config\\\" url=\\\"https://raw.githubusercontent.com/splunk/o11y-gdi-metadata/main/ootb/navigators_builtin_content.yaml\\\" data-main-column=\\\"category_display_name\\\" data-secondary-column=\\\"key\\\" data-column-3=\\\"importQualifiers\\\" data-header-1=\\\"Category\\\" data-header-2=\\\"Navigator\\\" data-header-3=\\\"ImportQualifiers (Required data onboarding conditions)\\\"></div>\n```\n\n----------------------------------------\n\nTITLE: Server Timing Header Format\nDESCRIPTION: Example of the HTTP response headers added when trace response headers are enabled. Shows the format of Access-Control-Expose-Headers and Server-Timing headers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/configuration/advanced-nodejs-otel-configuration.rst#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nAccess-Control-Expose-Headers: Server-Timing\nServer-Timing: traceparent;desc=\"00-<serverTraceId>-<serverSpanId>-01\"\n```\n\n----------------------------------------\n\nTITLE: Enforcing TLS 1.2 in Windows PowerShell (.NET)\nDESCRIPTION: Sets the security protocol for .NET network requests within a PowerShell script or session to TLS 1.2. This is useful when monitoring tools leverage PowerShell scripts that need to connect to services like Splunk On-Call requiring TLS 1.2+, especially if the environment defaults to an older protocol.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/admin/get-started/tls-security-protocol.rst#2025-04-22_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\n[Net.ServicePointManager]::SecurityProtocol =\\n[Net.SecurityProtocolType]::Tls12\n```\n\n----------------------------------------\n\nTITLE: Embedding HTML in reStructuredText for Custom Headings\nDESCRIPTION: This code snippet demonstrates how to embed HTML within reStructuredText to create custom headings with anchor links.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/get-ios-data-in.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n\n   <embed>\n   <h2>Get started with Splunk RUM for Mobile<a name=\"get-started-splunk-rum-mobile\" class=\"headerlink\" href=\"#get-started-splunk-rum-mobile\" title=\"Permalink to this headline\">¶</a></h2>\n   </embed>\n```\n\n----------------------------------------\n\nTITLE: Installing Nagios Plugin via RPM Package\nDESCRIPTION: Commands to download and install the Splunk On-Call Nagios plugin using RPM package manager.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-nagios-1.4.20/victorops-nagios-1.4.20-1.noarch.rpm\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Document Structure\nDESCRIPTION: Basic document structure for Splunk mobile documentation using ReStructuredText format, including table of contents, meta description, and content organization.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/intro-to-mobile.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _intro-to-mobile:\n\n*****************************************************\nIntroduction to Splunk Observability Cloud for Mobile\n*****************************************************\n\n.. meta::\n   :description: Get started using Splunk Observability Cloud for Mobile. See features and benefits.\n\n\n.. toctree::\n   :hidden: \n\n    Download the app <download-mobile>\n    View dashboards and alerts <use-mobile>\n```\n\n----------------------------------------\n\nTITLE: Regular Expression for Validating Traceparent in Shell\nDESCRIPTION: This snippet provides a regular expression pattern for validating the traceparent value in the Server-Timing header. The pattern ensures the correct format of version, trace-id, parent-id, and trace-flags.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n00-([0-9a-f]{32})-([0-9a-f]{16})-01\n```\n\n----------------------------------------\n\nTITLE: Constructing the Grafana Access URL (URI Pattern)\nDESCRIPTION: This snippet displays the generic URL pattern for accessing a Grafana instance. Users need to substitute `<host-name>` or `<IP-address>` with the server's hostname or IP, and `<port>` with the specific port configured in the `deploy.yaml` file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_13\n\nLANGUAGE: uri\nCODE:\n```\nhttp://<host-name>/<IP-address>:<port>\n```\n\n----------------------------------------\n\nTITLE: Empty Container Div Element\nDESCRIPTION: An empty HTML div element used as a container with class 'include-stop' and id 'troubleshooting-components.rst'\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sqlquery-receiver.rst#2025-04-22_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"include-stop\" id=\"troubleshooting-components.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Uninstalling SignalFx Tracing Library in Python\nDESCRIPTION: Command to uninstall the signalfx-tracing package using pip. This is the first step in removing the deprecated SignalFx Tracing Library.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/troubleshooting/migrate-signalfx-python-agent-to-otel.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall signalfx-tracing\n```\n\n----------------------------------------\n\nTITLE: Identifying Splunk RUM Aggregate Metric Prefix\nDESCRIPTION: This prefix 'rum.' identifies metrics that represent aggregations across multiple page loads in Splunk RUM. These metrics provide a higher-level view of performance or errors across different pages or user sessions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/RUM-metrics.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nrum.\n```\n\n----------------------------------------\n\nTITLE: Filtering Synthetic Runs by Browser Test Type in Splunk\nDESCRIPTION: This filter, `test_type=browser`, is applied to metrics like `synthetics.run.count` to specifically count synthetic runs originating from Browser tests within Splunk Synthetic Monitoring. It isolates usage related to web application frontend monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/synthetics-usage.rst#2025-04-22_snippet_1\n\nLANGUAGE: Splunk Metric/Filter\nCODE:\n```\ntest_type=browser\n```\n\n----------------------------------------\n\nTITLE: RST Configuration Table Definition\nDESCRIPTION: ReStructuredText markup defining a configuration table showing the perCPU option for the load monitor\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/host-processload.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n\n   - \n\n      - Option\n      - Required\n      - Type\n      - Description\n   - \n\n      - ``perCPU``\n      - no\n      - ``bool``\n      - The default value is ``false``.\n```\n\n----------------------------------------\n\nTITLE: Configuring ActionPack Instrumentation for Rails in Ruby\nDESCRIPTION: Demonstrates how to configure ActionPack instrumentation for Rails applications to automatically add middleware for trace information.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/configuration/advanced-ruby-otel-configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\n# Rails use ActionPack\nSplunk::Otel.configure do |c|\n   c.use \"OpenTelemetry::Instrumentation::ActionPack\"\n   c.use \"Splunk::Otel::Instrumentation::ActionPack\"\nend\n```\n\n----------------------------------------\n\nTITLE: Sending Critical Alert via JSON Webhook Payload - Panopta Integration - JSON\nDESCRIPTION: This JSON payload is used to trigger a critical alert in Splunk On-Call when configured within a Panopta webhook. It requires the Panopta webhook integration to be set up, using POST as the request method, and the Splunk On-Call REST endpoint as the URL. The payload dynamically inserts entity and state details using template variables, with required fields: message_type, entity_id, state_message, and monitoring_tool. The JSON must be used in the 'Incident Webhook' configuration and expects resolved Panopta template variables as inputs, producing a REST POST body for Splunk On-Call ingestion.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/panopta-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"message_type\":\"CRITICAL\",\"entity_id\":\"$name\", \"state_message\":\"$items - $reasons\",\"monitoring_tool\":\"Panopta\"}\n```\n\n----------------------------------------\n\nTITLE: Renaming CPU State Label\nDESCRIPTION: Renames the 'state' label to 'cpu_state' for system.cpu.usage metric.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/metrics-transform-processor.rst#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: system.cpu.usage\naction: update\noperations:\n    - action: update_label\n      label: state\n      new_label: cpu_state\n```\n\n----------------------------------------\n\nTITLE: Server-Timing HTTP Response Header Format\nDESCRIPTION: Shows the format of the Server-Timing header that contains traceId and spanId in traceparent format when SPLUNK_TRACE_RESPONSE_HEADER_ENABLED is set to true.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/configuration/advanced-ruby-otel-configuration.rst#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nAccess-Control-Expose-Headers: Server-Timing\nServer-Timing: traceparent;desc=\"00-<serverTraceId>-<serverSpanId>-01\"\n```\n\n----------------------------------------\n\nTITLE: Adding Splunk OpenTelemetry Collector Heroku Buildpack\nDESCRIPTION: Command to add the latest version of the Splunk OpenTelemetry Collector Heroku buildpack to your Heroku application using the GitHub repository.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/heroku.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nheroku buildpacks:add https://github.com/signalfx/splunk-otel-collector-heroku.git#\\$(curl -s https://api.github.com/repos/signalfx/splunk-otel-collector-heroku/releases | grep '\"tag_name\"' | head -n 1 | cut -d'\"' -f4)\n```\n\n----------------------------------------\n\nTITLE: Defining reStructuredText Page Header for Host Observer Extension\nDESCRIPTION: Sets up the page header and metadata for the Host observer extension documentation using reStructuredText syntax. It includes a reference label, title, and meta description.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/host-observer-extension.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _host-observer-extension:\n\n****************************\nHost observer extension\n****************************\n\n.. meta::\n      :description: Looks at the current host for listening network endpoints.\n```\n\n----------------------------------------\n\nTITLE: Extracting Substrings with substring - Handlebars\nDESCRIPTION: Applies the substring function to extract subsequences from a string variable, starting at a minimum start index and ending at an optional end index. Outputs the resulting substring as used in the alert payload. Requires at least one parameter for start index; optionally accepts an end index.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/notif-services/splunkplatform.rst#2025-04-22_snippet_5\n\nLANGUAGE: Handlebars\nCODE:\n```\n``{{substring var 1}}``\n``{{substring var 1 3}}``\n```\n\n----------------------------------------\n\nTITLE: Adding Syslog Receiver to Logs Pipeline\nDESCRIPTION: Shows how to add the syslog receiver to the logs pipeline in the service section of the configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-edit.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n  #\n  # Other pipelines\n  #\n    logs:\n      # Add syslog at the end of the list\n      receivers: [fluentforward, otlp, syslog]\n      processors:\n      - memory_limiter\n      - batch\n      - resourcedetection\n      exporters: [splunk_hec, splunk_hec/profiling]\n```\n\n----------------------------------------\n\nTITLE: Identifying AlwaysOn Profiling Escape Hatch Activation in Logs\nDESCRIPTION: When the AlwaysOn Profiling escape hatch activates due to full buffers, it logs this specific message. This indicates that profiling data is being dropped until the buffers are empty again.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nSkipping a thread sample period, buffers are full.\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata Table for Amazon Elasticsearch Service in reStructuredText\nDESCRIPTION: This code snippet defines a table in reStructuredText format that lists the metadata properties imported by Infrastructure Monitoring for Amazon Elasticsearch Service. It includes the original Elasticsearch property name, custom property name used in Infrastructure Monitoring, and a description of the property.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/monitor/aws-infra-metadata.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 30 30 60\n   :width: 100%\n\n   *  -  :strong:`Elasticsearch Name`\n      -  :strong:`Custom Property`\n      -  :strong:`Description`\n\n   *  -  ElasticsearchVersion\n      -  aws_es_version\n      -  The Elasticsearch version, for example ``7.1``.\n```\n\n----------------------------------------\n\nTITLE: Adding DiagnosticSource Dependency for .NET Tracing (XML)\nDESCRIPTION: Adds the `System.Diagnostics.DiagnosticSource` NuGet package reference to the .NET project file. This dependency is required for manual trace instrumentation using `ActivitySource` and `Activity` as part of the OpenTelemetry .NET SDK integration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/manual-dotnet-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<PackageReference Include=\"System.Diagnostics.DiagnosticSource\" Version=\"9.0.0\" />\n```\n\n----------------------------------------\n\nTITLE: Adding Event Listeners in TypeScript for Browser RUM\nDESCRIPTION: The addEventListener and removeEventListener methods allow registering and removing event listeners. They take a string type and a Function listener as arguments.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-api-reference.rst#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nSplunkRum.addEventListener(type: string, listener: Function): void\nSplunkRum.removeEventListener(type: string, listener: Function): void\n```\n\n----------------------------------------\n\nTITLE: Activating Zero-Code Instrumentation on Linux\nDESCRIPTION: Shell command to activate the zero-code instrumentation on Linux. This loads the necessary environment settings from the installed instrumentation script.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n# Activate the zero-code instrumentation\n. $HOME/.splunk-otel-dotnet/instrument.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring JBoss EAP and WildFly with Java Agent on Windows\nDESCRIPTION: To enable the Splunk OpenTelemetry Java agent in JBoss EAP and WildFly standalone mode on Windows, append the javaagent argument to the standalone.conf.bat.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-servers-instructions.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nset \"JAVA_OPTS=%JAVA_OPTS% -javaagent:<Drive>:\\path\\to\\splunk-otel-javaagent.jar\"\n```\n\n----------------------------------------\n\nTITLE: Cleaning and Building Documentation using Make (Shell)\nDESCRIPTION: Runs the 'clean' target followed by the 'html' target defined in the Makefile, executed within the Docker container. The 'clean' target usually removes previous build artifacts, ensuring a fresh build when 'html' is subsequently executed. This is useful for avoiding issues caused by stale build files.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nmake clean html\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Pods in Namespace\nDESCRIPTION: Command to list all pods in the petclinic namespace and its sample output showing OpenTelemetry Collector components.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-java-traces-tutorial/deploy-collector-k8s-java.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pod -n petclinic\n```\n\nLANGUAGE: bash\nCODE:\n```\nNAME                                                            READY   STATUS    RESTARTS   AGE\nsplunk-otel-collector-agent-nkwwf                               1/1     Running   0          94s\nsplunk-otel-collector-certmanager-6d95596898-z7qfz              1/1     Running   0          94s\nsplunk-otel-collector-certmanager-cainjector-5c5dc4ff8f-7rlwx   1/1     Running   0          94s\nsplunk-otel-collector-certmanager-webhook-69f4ff754c-hm9m2      1/1     Running   0          94s\nsplunk-otel-collector-k8s-cluster-receiver-594fd9c8c7-6n545     1/1     Running   0          94s\nsplunk-otel-collector-operator-69d476cb7-s8hcl                  2/2     Running   0          94s\n```\n\n----------------------------------------\n\nTITLE: Creating Volley HurlStack Instance\nDESCRIPTION: Example showing how to create an instrumented HurlStack instance for Volley HTTP requests.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/configure-rum-android-instrumentation.rst#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nHurlStack hurlStack = volleyTracing.newHurlStack();\n```\n\n----------------------------------------\n\nTITLE: Including .NET Requirements Section\nDESCRIPTION: This reStructuredText snippet uses an `include` directive to insert content from another file (`/_includes/requirements/dotnet.rst`) into the current document. It's wrapped in HTML divs likely for styling or JavaScript processing during the documentation build.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/dotnet-requirements.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"include-start\" id=\"requirements/dotnet.rst\"></div>\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: /_includes/requirements/dotnet.rst\n```\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"include-stop\" id=\"requirements/dotnet.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx PHP Tracing via Apache Environment Variables\nDESCRIPTION: Demonstrates setting required SignalFx configuration as environment variables within an Apache configuration file using the `SetEnv` directive. It configures the service name, the trace endpoint URL (typically a local OTel Collector), and global tags like deployment environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/instrumentation/instrument-php-application.rst#2025-04-22_snippet_2\n\nLANGUAGE: aconf\nCODE:\n```\n# Add the following lines to your Apache configuration file\n\nSetEnv SIGNALFX_SERVICE_NAME=\"<my-service-name>\"\nSetEnv SIGNALFX_ENDPOINT_URL='http://localhost:9411/api/v2/spans'\nSetEnv SIGNALFX_TRACE_GLOBAL_TAGS=\"deployment.environment:<my_environment>\"\n```\n\n----------------------------------------\n\nTITLE: Disabling QUIC Protocol in Chrome\nDESCRIPTION: Flag to deactivate QUIC protocol and HTTP/3 support\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/synthetics/chrome-flags.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n--disable-quic\n```\n\n----------------------------------------\n\nTITLE: Creating Grafana Data Source ConfigMap (Shell)\nDESCRIPTION: Shell command using kubectl to create the Kubernetes ConfigMap defined in the `config_basic.yaml` file, which provisions the Grafana data source.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_9\n\nLANGUAGE: none\nCODE:\n```\nkubectl create -f config_basic.yaml\n```\n\n----------------------------------------\n\nTITLE: Node Down Reset Payload\nDESCRIPTION: JSON payload template for resetting node down alerts in SolarWinds. Contains recovery message type and node status information.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/solarwinds-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message_type\":\"RECOVERY\",\n  \"monitor_name\":\"SolarWinds\",\n  \"monitoring_tool\":\"SolarWinds\",\n  \"alert_rule\":\"${N=Alerting;M=AlertName}\",\n  \"state_message\":\"${NodeName} is ${Status}\",\n  \"entity_display_name\":\"${NodeName} is ${Status}\",\n  \"entity_id\":\"${N=Alerting;M=AlertObjectID}\",\n  \"host_name\":\"${NodeName}\",\n  \"ip_address\":\"${Node.IP_Address}\"\n}\n```\n\n----------------------------------------\n\nTITLE: RST Document Structure Definition\nDESCRIPTION: ReStructuredText document defining the structure and metadata for the OpenTelemetry Collector Add-on documentation\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-addon/collector-addon-intro.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _collector-addon-intro:\n\n*********************************************************************************************\nSplunk Add-On for the OpenTelemetry Collector\n*********************************************************************************************\n\n.. meta::\n   :description: Introduction to the Splunk Add-on for the Splunk Distribution of the OpenTelemetry Collector.\n\n.. toctree::\n   :maxdepth: 5\n   :hidden:\n\n   Install the Technical Add-on <collector-addon-install.rst>\n   Deployment modes <collector-addon-configure-instance.rst>\n   Configure the Technical Add-on <collector-addon-configure.rst>\n   Upgrade the Technical Add-on <collector-addon-upgrade.rst>\n   Troubleshooting <collector-addon-troubleshooting.rst>\n   Release Notes <collector-addon-release-notes.rst>\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for AWS Tutorial\nDESCRIPTION: ReStructuredText markup defining the documentation structure for AWS monitoring tutorial, including table of contents, metadata, and section headers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-tutorial/about-aws-tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _about-aws-tutorial:\n\n********************************************************************\nTutorial: Monitor your AWS environment in Splunk Observability Cloud\n********************************************************************\n\n.. meta::\n    :description: Learn how to install the Splunk Observability Cloud AWS integration and monitor you AWS services.\n\n.. toctree::\n    :hidden:\n    :maxdepth: 5\n\n    tutorial-aws-start\n    tutorial-aws-use\n```\n\n----------------------------------------\n\nTITLE: RST GUI Label Reference\nDESCRIPTION: ReStructuredText markup for referencing GUI elements and labels in the documentation\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/user-roles/user-training.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n:guilabel:`Teams`, then :guilabel:`Your Team`, and :guilabel:`On-Call Schedule`\n```\n\n----------------------------------------\n\nTITLE: Sending Delivery Insights Message to Splunk On-Call\nDESCRIPTION: This JSON payload example demonstrates how to add a Delivery Insights message to the Splunk On-Call timeline. It includes required fields such as entity_type, event_type, summary, url, action, and result, as well as optional fields like source and long_message.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/delivery-insights-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"entity_type\": \"build system\",\n   \"event_type\": \"Build\",\n   \"source\": \"Internal  Tool\",\n   \"summary\": \"new version of mobile app\",\n   \"url\": \"https://www.splunk.com\",\n   \"action\": \"deployed\",\n   \"result\": \"SUCCESS\",\n   \"long_message\": \"New build includes bug fixes for connectivity issues and that new feature\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining SSH Check Receiver Section in reStructuredText\nDESCRIPTION: This snippet defines the section for the SSH check receiver documentation using reStructuredText syntax. It includes a reference anchor, section title, and metadata for description.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sshcheck-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _sshcheck-receiver:\n\n***********************\nSSH check receiver\n***********************\n\n.. meta::\n      :description: The SSH check receiver creates stats by connecting to an SSH server which might be an SFTP server.\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Table of Contents\nDESCRIPTION: RST toctree directive defining the documentation structure with three main phases of the implementation guide.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/get-started-guide/get-started-guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :hidden:\n   :maxdepth: 3\n\n   Phase 1: Onboarding readiness <onboarding-readiness>\n   Phase 2: Initial rollout <initial-rollout>\n   Phase 3: Scaled rollout <scaled-rollout>\n```\n\n----------------------------------------\n\nTITLE: Starting Riemann Server with Custom Configuration - Bash\nDESCRIPTION: This Bash command initiates the Riemann server with a specified configuration file (usually customized for VictorOps integration). Prerequisites: Riemann must be installed and the user must execute from the Riemann directory. Input: path to the configuration file; Output: active Riemann server using the specified config. All Riemann dependencies should be resolved before executing.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/riemann-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/riemann etc/riemann.config\n```\n\n----------------------------------------\n\nTITLE: RST Include Block for Metric Definitions\nDESCRIPTION: RST include directive with raw HTML wrapper divs for metric definitions section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoopjmx.rst#2025-04-22_snippet_8\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"metric-defs.rst\"></div>\n\n.. include:: /_includes/metric-defs.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"metric-defs.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Handling Resource Loading Error in HTML\nDESCRIPTION: Example showing how Browser RUM agent handles resource loading failures for missing images or scripts.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-errors.rst#2025-04-22_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html>\n   <head>\n      [...]\n   </head>\n   <body>\n      <img src=\"/missing-image.png\" />\n   </body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Granting Execution Permissions to Start Script (Shell)\nDESCRIPTION: Uses the 'chmod' command to grant execute permissions ('+x') to the 'start.sh' script. This step is necessary before the script can be run directly using './start.sh'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nchmod +x start.sh\n```\n\n----------------------------------------\n\nTITLE: Accessing Grafana UI URL Format for Verification\nDESCRIPTION: Example URL format for accessing the Grafana user interface after configuration changes. Replace `<host-name>/<IP-address>` with the appropriate node IP or hostname and `<port>` with the NodePort specified in the Service manifest (e.g., 32001). Used here to verify data source creation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_11\n\nLANGUAGE: none\nCODE:\n```\n            http://<host-name>/<IP-address>:<port>\n```\n\n----------------------------------------\n\nTITLE: Building Documentation using Make (Shell)\nDESCRIPTION: Executes the 'html' target defined in the project's Makefile from within the Docker container. This command triggers the Sphinx build process to generate the documentation in HTML format. The output is typically placed in the '_build/html' directory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Defining Discovery Receiver Section in reStructuredText\nDESCRIPTION: This RST code defines the structure and metadata for the Discovery receiver documentation page. It sets up the page title, a description meta tag, and includes a note about the discovery mode feature.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/discovery-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _discovery-receiver:\n\n****************************\nDiscovery receiver\n****************************\n\n.. meta::\n      :description: Wraps the receiver creator to facilitate the discovery of metric collection targets. \n\nThe Splunk Distribution of the OpenTelemetry Collector supports the Discovery receiver. Documentation is planned for a future release.  \n\nTo find information about this component in the meantime, see :new-page:`Discovery receiver <https://github.com/signalfx/splunk-otel-collector/tree/main/internal/receiver/discoveryreceiver>` on GitHub.\n\n.. note:: You can also use the discovery mode of the Splunk Distribution of OpenTelemetry Collector to detect metric sources and collect metrics automatically. Learn more at :ref:`discovery_mode`.\n```\n\n----------------------------------------\n\nTITLE: Retrieving Container Names from Kubernetes Pod\nDESCRIPTION: This command retrieves the names of containers in a specified Kubernetes pod using kubectl and jq for JSON parsing.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-container.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl get --raw \"/api/v1/nodes/\"${NODE_NAME}\"/proxy/stats/summary\" | jq '.pods[] | select(.podRef.name=='\\\"$POD_NAME\\\"') | .containers[].name'\n```\n\n----------------------------------------\n\nTITLE: Adding Locally Built Library as Dependency\nDESCRIPTION: Code snippet showing how to add the locally built Android RUM library as a dependency in the build.gradle file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/install-rum-android.rst#2025-04-22_snippet_9\n\nLANGUAGE: kotlin\nCODE:\n```\ndependencies {\n   //...\n      implementation (\"com.splunk:splunk-otel-android:<version>\")\n   //...\n}\n```\n\n----------------------------------------\n\nTITLE: Monitoring Gross Content Bytes Received in Splunk RUM (Metric)\nDESCRIPTION: This metric tracks the total size (in bytes) of the content within spans received by Splunk RUM before throttling or filtering. This value contributes to assessing data volume against the Bytes Per Minute (BPM) limit.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-rum.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.rum.grossContentBytesReceived\n```\n\n----------------------------------------\n\nTITLE: Installing VictorOps Nagios Plugin using dpkg (Debian)\nDESCRIPTION: Uses the `dpkg` command with the `-i` flag to install the previously downloaded VictorOps Nagios plugin `.deb` package. Replace `<path_to_file>` with the actual path to the downloaded file. This command requires appropriate permissions, potentially `sudo`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndpkg -i <path_to_file>\n```\n\n----------------------------------------\n\nTITLE: Stopping Docker Compose Services\nDESCRIPTION: Command to stop and remove all running services and networks created by Docker Compose.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/logs-collector-splunk-tutorial/deploy-verify-environment.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose down\n```\n\n----------------------------------------\n\nTITLE: Verifying Instrumentation in Kubernetes Pod\nDESCRIPTION: Bash command to describe a Kubernetes pod, used to verify successful instrumentation. This command helps users check for the presence of the OpenTelemetry auto-instrumentation container and related environment variables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nkubectl describe pod <application_pod_name> -n <namespace>\n```\n\n----------------------------------------\n\nTITLE: Configuring Zabbix Plugin\nDESCRIPTION: Commands for configuring the Zabbix plugin including renaming configuration file, editing settings, and running setup scripts.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zabbix-integration.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmv /opt/victorops/zabbix_plugin/conf/local.zabbix.conf.example /opt/victorops/zabbix_plugin/conf/local.zabbix.conf\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo nano local.zabbix.conf\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo /opt/victorops/zabbix_plugin/bin/configure_ackback.sh\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo /opt/victorops/zabbix_plugin/bin/create_links.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring OTel Collector Pipeline for Node.js Profiling Data\nDESCRIPTION: This YAML snippet shows a sample OpenTelemetry Collector configuration for handling AlwaysOn Profiling data from Node.js applications. It defines an OTLP receiver, a Splunk HEC exporter specifically for profiling data ('splunk_hec/profiling'), and a 'logs/profiling' pipeline that routes data received via OTLP through batch and memory limiting processors to the configured Splunk HEC endpoint. Correct token and endpoint configuration are crucial.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/troubleshooting/common-nodejs-troubleshooting.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  # Profiling\n  splunk_hec/profiling:\n    token: \"${SPLUNK_ACCESS_TOKEN}\"\n    endpoint: \"${SPLUNK_INGEST_URL}/v1/log\"\n    log_data_enabled: false\n\nprocessors:\n  batch:\n  memory_limiter:\n    check_interval: 2s\n    limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}\n\nservice:\n  pipelines:\n    logs/profiling:\n      receivers: [otlp]\n      processors: [memory_limiter, batch]\n      exporters: [splunk_hec, splunk_hec/profiling]\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Logging Limits\nDESCRIPTION: JSON configuration to limit Docker logging size and rotation to prevent disk space issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_42\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"log-driver\": \"local\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: RST HTML Include Directives\nDESCRIPTION: Defines HTML div containers and includes external RST content for auto-discovery documentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/discovery-linux.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"gdi/auto-discovery-intro.rst\"></div>\n\n.. include:: /_includes/gdi/auto-discovery-intro.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"gdi/auto-discovery-intro.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Splunk On-Call RST Documentation Structure\nDESCRIPTION: ReStructuredText markup defining the documentation structure for Splunk On-Call user guide, including sections for dashboard layout, incident actions, and on-call procedures.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/user-roles/getting-started-user.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _spoc-user-guide:\n\n************************************************************************\nGetting started guide for Splunk On-Call users\n************************************************************************\n\n.. meta::\n   :description: About  Splunk On-Call.\n```\n\n----------------------------------------\n\nTITLE: Installing Zabbix Plugin via DEB Package\nDESCRIPTION: Commands to download and install the Zabbix plugin using debian package manager for ack-back functionality.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zabbix-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-zabbix-0.18.3/victorops-zabbix_0.18.3-2_all.deb\n\nsudo dpkg -i victorops-zabbix_0.18.3-2_all.deb\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo ./install\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Collector Configuration\nDESCRIPTION: RST markup defining the structure and navigation for Collector configuration documentation, including toctree directives and section references.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-common-config.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _collector-common-config:\n\n****************************************************************\nCommon configuration options\n****************************************************************\n\n.. meta::\n      :description: Learn how to perform common actions with the Collector.\n\n.. toctree::\n    :maxdepth: 4\n    :titlesonly:\n    :hidden:\n\n    Authentication settings <common-config/collector-common-config-auth.rst>\n    gRPC settings <common-config/collector-common-config-grpc.rst>\n    HTTP settings <common-config/collector-common-config-http.rst>\n    Network settings <common-config/collector-common-config-net.rst>\n    TLS settings <common-config/collector-common-config-tls.rst>\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Windows Services Monitor - Specific Service\nDESCRIPTION: Configuration snippet for monitoring a specific Windows service using the Telegraf win_services receiver in the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/win-services.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/win_services:\n    type: telegraf/win_services \n    serviceNames:\n         - exampleService1  # only monitor exampleService1\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/win_services]\n```\n\n----------------------------------------\n\nTITLE: RST Include Directive for Environment Variables\nDESCRIPTION: ReStructuredText markup for including external content about collector environment variables, wrapped in HTML div tags for content management.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/environment-variables.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"collector-env-vars.rst\"></div>\n\n.. include:: /_includes/collector-env-vars.rst\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"collector-env-vars.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Defining RST Directive for Toctree\nDESCRIPTION: RST directive to create a hidden table of contents with links to Kubernetes, Linux, and Windows setup guides.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/compute/compute.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n..\\ttoctree::\n   :hidden:\n\n   k8s\n   linux\n   windows\n```\n\n----------------------------------------\n\nTITLE: Identifying Total Synthetic Runs Metric in Splunk\nDESCRIPTION: This metric, `synthetics.run.count`, represents the total number of synthetic test runs performed within an organization in Splunk Synthetic Monitoring. It serves as a base metric for tracking overall usage and can be further filtered by test type.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/synthetics-usage.rst#2025-04-22_snippet_0\n\nLANGUAGE: Splunk Metric/Filter\nCODE:\n```\nsynthetics.run.count\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Direct Span Transmission to Splunk Observability Cloud\nDESCRIPTION: Sets the required environment variables to enable AWS Lambda functions instrumented with the Splunk OpenTelemetry Lambda layer to send spans directly to Splunk Observability Cloud. The configuration specifies the protocol and endpoint for trace data transmission.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/gdi/send-spans-directly-lambda.rst#2025-04-22_snippet_0\n\nLANGUAGE: environment-variables\nCODE:\n```\nOTEL_EXPORTER_OTLP_TRACES_PROTOCOL=http/protobuf\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://ingest.<realm>.signalfx.com/v2/trace/otlp\n```\n\n----------------------------------------\n\nTITLE: Defining Organization Reference Information Page - reStructuredText\nDESCRIPTION: This snippet defines the central reference index for Splunk Observability Cloud organization administration using reStructuredText. It includes a heading, meta description for search indexing, a hidden table of contents tree for navigating sub-sections, and a summary table linking to child reference documents (such as data retention and per-product limits). The snippet does not require external dependencies but assumes a compatible Sphinx or reST documentation pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/org-info.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _org-info:\n\n*********************************************************************\nOrg reference information\n*********************************************************************\n\n.. meta::\n   :description: Reference topics for administering your Splunk Observability Cloud org.\n\n.. toctree::\n   :hidden:\n\n   data-retention\n   organizations\n   per-product-limits\n   supported-browsers\n\n\n\n\n.. list-table::\n   :header-rows: 1\n   :widths: 50, 50\n\n   * - :strong:`For info about`\n     - :strong:`Documentation`\n\n   * - The data retention of each component of Splunk Observability Cloud\n     - See, :ref:`data-o11y`.\n  \n   * - Per product system limits\n     - See, :ref:`per-product-limits`\n\n   * - Supported browsers\n     - See, :ref:`supported-browsers`\n  \n```\n\n----------------------------------------\n\nTITLE: Note Annotation Payload - Splunk On-Call REST Endpoint - JSON\nDESCRIPTION: Provides an example JSON payload including a note annotation, using the 'vo_annotate.s.Note' field. Used to attach additional textual information to an alert; all required alert fields are present. The annotation is aggregated with the incident if the entity_id matches an open incident.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rest-endpoint-integration-guide.rst#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{ \\u201cmonitoring_tool\\u201d: \\u201cAPI\\u201d, \\u201cmessage_type\\u201d:\\u201cINFO\\u201d,\\n\\u201centity_id\\u201d:\\u201cdisk.space/db01\\u201d, \\u201centity_display_name\\u201d:\\u201cApproaching Low\\nDisk Space on DB01\\u201d, \\u201cstate_message\\u201d:\\u201cThe disk is really really full.\\nHere is a bunch of information about the problem\\u201d,\\n\\u201cvo_annotate.s.Note\\u201d:\\u201cOnce Disk Space is critically low there will be an\\nincident!\\u201d }\n```\n\n----------------------------------------\n\nTITLE: Including Metric Definitions in RST\nDESCRIPTION: This snippet demonstrates how to include metric definitions from an external file using RST directives. It uses raw HTML to wrap the include statement for better control over rendering.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/host-metadata.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"metric-defs.rst\"></div>\n\n.. include:: /_includes/metric-defs.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"metric-defs.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for Documentation Build\nDESCRIPTION: This snippet specifies the exact Python packages and their versions needed for the /splunk/public-o11y-docs project. It includes core documentation tools like Sphinx, templating engines like Jinja2, syntax highlighters like Pygments, and various Sphinx extensions for features like copy buttons, mermaid diagrams, tabs, and internationalization. Pinning versions ensures reproducible documentation builds.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nalabaster==0.7.13\nBabel==2.12.1\ncertifi==2024.7.4\ncharset-normalizer==3.2.0\ncssmin==0.2.0\nCython==3.0.0\ndocutils==0.18.1\nidna==3.7\nimagesize==1.4.1\nimportlib-metadata==6.8.0\nJinja2==3.1.4\njsmin==3.0.1\npackaging==23.1\npip==23.3\nPygments==2.16.1\npyparsing==3.1.0\npytz==2023.3\nPyYAML==6.0.1\nrequests==2.32.2\nsetuptools==70.0.0\nsnowballstemmer==2.2.0\nSphinx==5.3.0\nsphinx-copybutton==0.5.2\nsphinxcontrib-mermaid==0.9.2\nsphinx-intl==2.1.0\nsphinx-last-updated-by-git==0.3.7\nsphinx-notfound-page==0.8.3\nsphinx-tabs==3.4.1\nsphinxcontrib-applehelp==1.0.4\nsphinxcontrib-devhelp==1.0.2\nsphinxcontrib-htmlhelp==2.0.1\nsphinxcontrib-images==0.9.4\nsphinxcontrib-jsmath==1.0.1\nsphinxcontrib-qthelp==1.0.3\nsphinxcontrib-serializinghtml==1.1.5\ntyping_extensions==4.7.1\nurllib3==2.2.2\nzipp==3.19.1\n```\n\n----------------------------------------\n\nTITLE: Installing Zenoss RPM Package\nDESCRIPTION: Commands to download and install the Zenoss plugin using RPM package manager on RedHat-based systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zenoss-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-zenoss-0.22.44/victorops-zenoss-0.22.44-1.noarch.rpm\n```\n\nLANGUAGE: shell\nCODE:\n```\nrpm -i <path_to_file>\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: RST markup defining the document structure for incident response documentation, including section headers and meta description.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/practice-reliability/incident-response.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _practice-reliability-incident-response:\n\n***********************************************************************************\nRespond to incidents\n***********************************************************************************\n\n.. meta::\n   :description: This page provides an overview of the many ways you can drill down to root cause problems and decrease MTTR using the components of Splunk Observability Cloud.\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata in reStructuredText for Splunk On-Call Documentation\nDESCRIPTION: This snippet sets up the metadata for the documentation page, including the page title and description. It uses reStructuredText directives to structure the content.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/team-escalation-policy.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _team-escalation-policy:\n\n************************************************************************\nSet up an escalation policy\n************************************************************************\n\n.. meta::\n   :description: Instructions for configuring an escalation policy for Splunk On-Call\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Table of Contents in reStructuredText\nDESCRIPTION: This snippet defines a hidden table of contents for the documentation, listing related pages such as installation, configuration, and troubleshooting.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/get-ios-data-in.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :hidden:\n\n   Install the iOS RUM library <install-rum-ios>\n   Configure the instrumentation <configure-rum-ios-instrumentation>\n   Manually instrument applications <manual-rum-ios-instrumentation>\n   iOS RUM data model <rum-ios-data-model>\n   Troubleshooting <troubleshooting>\n```\n\n----------------------------------------\n\nTITLE: Initializing SignalFx PHP Instrumentation in Docker Startup Script using Shell\nDESCRIPTION: Commands intended for a Docker container's startup script (e.g., `setup.sh`). It downloads the setup script, installs the tracer for all PHP binaries, and then uses the `--update-config` flag to set INI file options for the SignalFx endpoint URL, access token, and service name, required for direct data export.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/instrumentation/instrument-php-application.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncurl -LO https://github.com/signalfx/signalfx-php-tracing/releases/latest/download/signalfx-setup.php\nphp signalfx-setup.php --php-bin=all\nphp signalfx-setup.php --update-config --signalfx.endpoint_url=https://ingest.<realm>.signalfx.com/v2/trace/signalfxv1\nphp signalfx-setup.php --update-config --signalfx.access_token=<access_token>\nphp signalfx-setup.php --update-config --signalfx.service_name=<service-name>\n```\n\n----------------------------------------\n\nTITLE: Embedding HTML for Push Notification Section in reStructuredText\nDESCRIPTION: This snippet embeds HTML content to create a section header for Push Notifications in the reStructuredText document. It includes an anchor link for easy navigation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/notifications/notification-types.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n  \n    <embed>\n      <h2>Push notification<a name=\"push-notification\" class=\"headerlink\" href=\"#push-notification\" title=\"Permalink to this headline\">¶</a></h2>\n    </embed>\n```\n\n----------------------------------------\n\nTITLE: Specifying Entity ID for Splunk On-Call SSO in Google Apps (Text)\nDESCRIPTION: Provides the Entity ID (`victorops.com`) to be entered in the Google Apps SAML configuration for Splunk On-Call. This identifier uniquely represents the Splunk On-Call service provider (SP) within the SAML transaction. This value is entered during step 6 of the configuration process.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/admin/sso/sp-sso-google.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nvictorops.com\n```\n\n----------------------------------------\n\nTITLE: RST Navigation Menu Selection\nDESCRIPTION: ReStructuredText directive for displaying menu navigation paths in the documentation\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/user-roles/user-training.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n:menuselection:`Notifications` menu and select :guilabel:`Add Splunk On-Call to Contacts`\n```\n\n----------------------------------------\n\nTITLE: Handling Console Error with Try-Catch in JavaScript\nDESCRIPTION: Example demonstrating how Browser RUM agent handles console errors within try-catch blocks.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-errors.rst#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\ntry {\n   someNull.anyField = 'value';\n} catch(e) {\n   console.error('failed to update', e);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Nagios Configuration Files for CheckMK\nDESCRIPTION: Command to create copies of Nagios and environment configuration files and create symlinks in the site Nagios directory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/check_mk-integration.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n/opt/victorops/nagios_plugin/omd_check_mk/install.sh <yoursitename>\n```\n\n----------------------------------------\n\nTITLE: Team Name Variable in Error Message\nDESCRIPTION: Template string showing how team names are referenced in error messages.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/twilio-live-call-routing-guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n\"Team ${team-name} does not exist. Please contact your administrator to fix the problem\"\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Orchestration Receivers\nDESCRIPTION: RST markup defining the documentation structure for orchestration-related application receivers, including table of contents and navigation links.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/orchestration.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _orchestration:\n\n********************************************************************************\nConfigure application receivers for orchestration\n********************************************************************************\n\n.. meta::\n   :description: Landing for application receivers for orchestration applications in Splunk Observability Cloud.\n\n.. toctree::\n   :maxdepth: 4\n   :hidden:\n\n   Istio <get-data-in/application/istio/istio>\n   monitors-orchestration/kubernetes-apiserver\n   monitors-orchestration/kubernetes-cluster\n   opentelemetry/components/kubernetes-cluster-receiver\n   monitors-orchestration/kubernetes-controller-manager\n   monitors-orchestration/kubernetes-events\n   monitors-orchestration/kubernetes-network-stats\n   opentelemetry/components/kubernetes-objects-receiver\n   monitors-orchestration/kubernetes-proxy\n   monitors-orchestration/kubernetes-scheduler\n   opentelemetry/components/kubelet-stats-receiver\n   monitors-orchestration/openshift-cluster\n```\n\n----------------------------------------\n\nTITLE: Monitoring Accepted Span Bytes Received in Splunk RUM (Metric)\nDESCRIPTION: This metric represents the size (in bytes) of the spans that Splunk RUM accepted after applying limits and throttling (such as BPM). It reflects the effective data volume processed.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-rum.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.rum.numSpanBytesReceived\n```\n\n----------------------------------------\n\nTITLE: RST Admonition Block for RBAC Preview Notice\nDESCRIPTION: ReStructuredText markup for a preview feature notice that includes disclaimer text and navigation links for role-based access control documentation\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/private-preview.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. admonition:: Preview: Role-based access\n\n    Preview features are provided by Splunk to you \"as is\" without any warranties, maintenance and support, or service level commitments. Splunk makes this preview feature available in its sole discretion and may discontinue it at any time. Use of preview features is subject to the :new-page:`Splunk General Terms <https://www.splunk.com/en_us/legal/splunk-general-terms.html>`.\n    \n    Use these links to navigate to the topics for role-based access control (RBAC):\n\n      * :ref:`roles-and-capabilities`\n      * :ref:`roles-and-capabilities-table`\n      * :ref:`users-assign-roles-ph3`\n```\n\n----------------------------------------\n\nTITLE: Removing Docker Containers with docker rm\nDESCRIPTION: Commands for removing Docker containers by ID or name, including options for force-removing running containers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\ndocker rm <container_id_or_name>\n```\n\nLANGUAGE: shell\nCODE:\n```\ndocker rm -f my_running_container\n```\n\n----------------------------------------\n\nTITLE: Defining SRE Contact Configuration in Nagios\nDESCRIPTION: Configuration for setting up an SRE team contact in Nagios that integrates with Splunk On-Call using the VictorOps_Contact template.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndefine contact{\nuse            VictorOps_Contact\nname           VictorOps_sre\ncontact_name   VictorOps_sre\nalias          VictorOps_sre\n}\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Markup for Synthetic Monitoring Concepts\nDESCRIPTION: RST markup defining a list table of key concepts and descriptions for Splunk Synthetic Monitoring documentation, including terms like synthetic monitoring, tests, metrics, and various test types.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/key-concepts.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 25, 75\n\n   * - :strong:`Concept`\n     - :strong:`Description`\n   \n   * - Synthetic Monitoring\n     - A monitoring technique to test site or application performance by generating simulated user behavior from a variety of geographic locations, devices, and connection types and by measuring response times and other performance metrics. This level of monitoring lets you proactively measure how your webpages are performing without needing to rely on data generated by real users, so that you can identify and fix problems before they affect user experience or application revenue.\n\n   * - Synthetics\n     - A common abbreviation for Synthetic Monitoring.\n```\n\n----------------------------------------\n\nTITLE: Example service account token error message in bash output\nDESCRIPTION: This error message indicates that the service account token needed for Kubernetes API access is not available, preventing metadata collection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-troubleshoot.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nError: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory\n```\n\n----------------------------------------\n\nTITLE: Retrieving ConfigMaps from Kubernetes Collector Pods\nDESCRIPTION: These commands retrieve ConfigMap information from agent and cluster receiver pods, saving them to YAML files for support analysis. This information is crucial for diagnosing configuration-related issues with the Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-support.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get cm my-splunk-otel-collector-agent -o yaml > my-splunk-otel-collector-agent-cm.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get cm my-splunk-otel-collector-k8s-cluster-receiver -o yaml > my-splunk-otel-collector-k8s-cluster-receiver-cm.yaml\n```\n\n----------------------------------------\n\nTITLE: HTML Video Embed for Documentation Tutorial\nDESCRIPTION: Embedded iframe for a YouTube video explaining documentation contribution process\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/contribute.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"videoWrapper\">\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tsvWJwcv_BQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Identifying Splunk RUM Page-Level Metric Prefix\nDESCRIPTION: This prefix 'rum.node.' identifies metrics that are specific to individual page loads in Splunk RUM. These metrics provide granular data for specific user interactions on a single page.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/RUM-metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nrum.node.\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: RST markup for structuring the documentation page, including sections, images, and references\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/synth-scenarios/browser-test-scenario.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _browser-test-scenario:\n\n******************************************************************************************\nScenario: Monitor a multi-step workflow using a Browser test \n******************************************************************************************\n\n.. meta::\n    :description: Fictional use case describing how to monitor the performance of a company website using browser test monitoring from Splunk Synthetic Monitoring.\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Document Structure with References\nDESCRIPTION: RST markup defining document structure with cross-references and metadata for a troubleshooting scenario documentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/scenarios/integration-scenario1.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _integration-scenario1:\n\n*******************************************************************************************************************\nScenario: Kai troubleshoots faster with IT Service Intelligence and Splunk Observability Cloud\n*******************************************************************************************************************\n\n.. meta::\n   :description: This scenario describes how users can use Splunk ITSI and Splunk Observability Cloud together to drill down faster on problems and reduce mean time to resolution.\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Fargate Distribution\nDESCRIPTION: YAML configuration example for running the Collector on Amazon EKS with Fargate profiles.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ndistribution: eks/fargate\n```\n\n----------------------------------------\n\nTITLE: Installing Kernel Headers on RedHat Linux/Amazon Linux\nDESCRIPTION: This bash command installs the necessary kernel development packages on RedHat Linux or Amazon Linux systems for the OpenTelemetry Collector eBPF Helm chart. It requires `yum` to be available and assumes internet access for acquiring packages.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install -y kernel-devel-$(uname -r)\n```\n\n----------------------------------------\n\nTITLE: Defining VictorOps Team Contact in Icinga\nDESCRIPTION: Configuration for defining individual team contacts in Icinga using VictorOps_Contact settings. Shows setup for DevOps, SRE, and Database teams.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndefine contact{\nuse            VictorOps_Contact\nname           VictorOps_devops\ncontact_name   VictorOps_devops\nalias          VictorOps_devops\n}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Metric Time Series with Mermaid\nDESCRIPTION: A Mermaid flowchart that illustrates a metric time series consisting of three data points for CPU utilization. Each data point shares the same metric name, type, and dimensions but has different values and timestamps.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/metrics-and-metadata/metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#FFFFFF',\n      'primaryTextColor': '#000000',\n      'primaryBorderColor': '#000000',\n      'nodeBorder':'#000000',\n      'lineColor': '#000000',\n    }\n  }\n}%%\n\n\nflowchart LR\n\n    accTitle: Metric time series diagram\n    accDescr: This example metric time series consists of 3 data points. They all share the same metric name, cpu.utilization, the same metric type, gauge, and the same set of dimension key-value pairs, which are hostname:server1 and location:Tokyo. The first data point has a value of .85 and a timestamp of 1557225030000 in UNIX time. The second data point has a value of .9 and a timestamp of 1557225030100 in UNIX time. The third data point has a value of .7 and a timestamp of 1557225030200 in UNIX time.\n    \n    %% LR indicates the direction (left-to-right)\n\n    %% You can define classes to style nodes and other elements\n    classDef default fill:#FFFFFF,stroke:#000\n    classDef name fill:#BFFFFF\n    classDef type fill:#ffdda6\n    classDef value fill:#fff7a1\n    classDef timestamp fill:#dedeff\n    classDef host fill:#BFFFBF\n    classDef location fill:#FFBFBF\n\n    %% Each subgraph determines what's in each category\n    subgraph Metric time series\n    direction LR\n    dp0~~~dp1~~~dp2\n\n    subgraph dp0[Data point 1]\n      direction LR\n      name0(name: cpu.utilization):::name\n      type0(type: gauge):::type\n      value0(value: .85):::value\n      \n      subgraph dimensions0[Dimensions]\n        direction LR\n        k0(hostname: server1):::host\n        k1(location: Tokyo):::location\n      end\n\n      timestamp0(timestamp: 1557225030000):::timestamp\n\n\n    end\n\n    subgraph dp1[Data point 2]\n      direction LR\n      name1(name: cpu.utilization):::name\n      type1(type: gauge):::type\n      value1(value: .9):::value\n\n\n      subgraph dimensions1[Dimensions]\n        direction LR\n        k2(hostname: server1):::host\n        k3(location: Tokyo):::location\n        \n      end\n      timestamp1(timestamp: 1557225030100):::timestamp\n\n    end\n\n    subgraph dp2[Data point 3]\n        direction LR\n      name2(name: cpu.utilization):::name\n      type2(type: gauge):::type\n      value2(value: .7):::value\n\n\n      subgraph dimensions2[Dimensions]\n        direction LR\n        k4(hostname: server1):::host\n        k5(location: Tokyo):::location\n        \n      end\n      timestamp2(timestamp: 1557225030200):::timestamp\n\n    end\nend\n```\n\n----------------------------------------\n\nTITLE: RST Structure for Private Connectivity Documentation\nDESCRIPTION: Defines the RST documentation structure for private connectivity options, including metadata, table of contents, and reference links.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/private-connectivity/private-connectivity-landing.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _private-connectivity-landing:\n\n**************************************\nPrivate connectivity\n**************************************\n\n.. meta::\n   :description: Options for private connectivity to send data to Splunk Observability Cloud.\n\n.. toctree::\n   :hidden:\n\n   aws-privatelink\n\nSee the following to create private connections to send data to Splunk Observability Cloud:\n\n- :ref:`aws-privatelink`\n```\n\n----------------------------------------\n\nTITLE: Restarting Collector's agent DaemonSet\nDESCRIPTION: Command to restart the OpenTelemetry Collector's agent DaemonSet after updating the access token (when agent.enabled=true).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nkubectl rollout restart DaemonSet <Release_Name>-agent\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Prometheus Configuration\nDESCRIPTION: RST formatted documentation structure defining the table of contents and configuration options for Prometheus metrics collection in Splunk Observability Cloud.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/prometheus.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _prometheus:\n\n********************************************************************************\nConfigure application receivers with Prometheus metrics\n********************************************************************************\n\n.. meta::\n   :description: Landing for application receivers for monitoring applications in Splunk Observability Cloud.\n   \n.. toctree::\n   :maxdepth: 4\n   :hidden:\n\n   monitors-prometheus/prometheus-exporter\n   monitors-prometheus/prometheus-go\n   monitors-prometheus/prometheus-nginx-ingress\n   monitors-prometheus/prometheus-nginx-vts\n   monitors-prometheus/prometheus-node\n   monitors-prometheus/prometheus-velero\n```\n\n----------------------------------------\n\nTITLE: Embedding HTML Content in reStructuredText for Enterprise Edition Notice\nDESCRIPTION: This snippet embeds HTML content within a reStructuredText document to display a notice about Enterprise Edition availability. It uses the raw directive to include HTML that cannot be directly represented in reStructuredText.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/metrics-pipeline/data-dropping-impact.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<embed>\n  <p><b>Available in Enterprise Edition</b>. For more information, see <a href=\"#sd-subscriptions\">Subscription types, expansions, renewals, and terminations</a>.</p>\n</embed>\n```\n\n----------------------------------------\n\nTITLE: Direct Configuration with Parameter Store\nDESCRIPTION: JSON configuration for deploying the OpenTelemetry Collector using AWS Systems Manager Parameter Store for configuration in AWS Fargate.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-fargate.rst#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n\"environment\": [\n  {\n    \"name\": \"SPLUNK_ACCESS_TOKEN\",\n    \"value\": \"MY_SPLUNK_ACCESS_TOKEN\"\n  },\n  {\n    \"name\": \"SPLUNK_REALM\",\n    \"value\": \"MY_SPLUNK_REALM\"\n  }\n],\n\"secrets\": [\n  {\n    \"valueFrom\": \"splunk-otel-collector-config\",\n    \"name\": \"SPLUNK_CONFIG_YAML\"\n  }\n],\n\"image\": \"quay.io/signalfx/splunk-otel-collector:0.33.0\",\n\"essential\": true,\n\"name\": \"splunk_otel_collector\"\n}\n```\n\n----------------------------------------\n\nTITLE: Extended Helm Values with CRDs Configuration\nDESCRIPTION: YAML configuration showing how to enable Custom Resource Definitions (CRDs) in the OpenTelemetry Collector deployment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nclusterName: my-cluster\n\nsplunkObservability:\n  realm: <splunk_realm>\n  accessToken: <splunk_access_token>\n\noperator:\n  enabled: true\noperatorcrds:\n  install: true\n```\n\n----------------------------------------\n\nTITLE: RST Section Headers and Documentation Structure\nDESCRIPTION: ReStructuredText markup defining the document structure including headers, meta description, and section organization for Ruby agent requirements documentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/ruby-otel-requirements.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _ruby-otel-requirements-distro:\n\n*************************************************************\nRuby agent compatibility and requirements\n*************************************************************\n\n.. meta::\n    :description: This is what you need to instrument any Ruby application using the Splunk OTel Ruby agent.\n```\n\n----------------------------------------\n\nTITLE: HTML Section Header for Contribution Guidelines\nDESCRIPTION: Embedded HTML markup for the contribution guidelines section header with anchor link\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/contribute.rst#2025-04-22_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<embed>\n  <h2>Contribution guidelines<a name=\"contribute-guidelines\" class=\"headerlink\" href=\"#contribute-guidelines\" title=\"Permalink to this headline\">¶</a></h2>\n</embed>\n```\n\n----------------------------------------\n\nTITLE: RST Include Block for Troubleshooting\nDESCRIPTION: RST include directive with raw HTML wrapper divs for troubleshooting components section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoopjmx.rst#2025-04-22_snippet_9\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"troubleshooting-components.rst\"></div>\n\n.. include:: /_includes/troubleshooting-components.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"troubleshooting-components.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Process Monitor Configuration Options in YAML\nDESCRIPTION: Configuration table showing the key settings for the host process monitor including processes list, process matching, context switch collection, and procFS path options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/host-processes.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocesses: # optional, list of strings - process names to match\nprocessMatch: # optional, map of strings for regex matching\ncollectContextSwitch: false # optional, bool for collecting context switch metrics\nprocFSPath: # optional, string for proc filesystem path\n```\n\n----------------------------------------\n\nTITLE: Running the Splunk OpenTelemetry Collector Linux Installer Script\nDESCRIPTION: Downloads and executes the Linux installer script for the Splunk OpenTelemetry Collector using environment variables for configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-start.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh;\nsudo sh /tmp/splunk-otel-collector.sh --realm $SPLUNK_REALM --memory $SPLUNK_MEMORY_TOTAL_MIB -- $SPLUNK_ACCESS_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Pairing Status Response Format\nDESCRIPTION: Example of the JSON response format when checking pairing status. Shows the pairing ID and status field.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/unified-id/unified-identity.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n\"pairingId\": \"<pairing-id>\"\n\"status\": \"SUCCESS\"\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Image Directive\nDESCRIPTION: RST markup for including and formatting images within the documentation, showing screenshots of the Splunk interface.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/scenarios/integration-scenario1.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. image:: /_images/splunkplatform/glass_table.png\n  :width: 100%\n  :alt: This screenshot shows a glass table in Splunk IT Service Intelligence that tracks service health.\n```\n\n----------------------------------------\n\nTITLE: Creating Timeout Symlink for CentOS 5\nDESCRIPTION: Commands to create and configure a timeout command symlink to avoid timeouts in CentOS 5.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nln -s /usr/share/doc/bash-3.2/scripts/timeout /usr/bin/timeout\n```\n\nLANGUAGE: bash\nCODE:\n```\nchmod 755 /usr/share/doc/bash-3.2/scripts/timeout\n```\n\n----------------------------------------\n\nTITLE: Adding URL Annotation via REST API in JSON\nDESCRIPTION: This snippet demonstrates how to add a URL annotation to an alert using a REST-style integration. It includes fields for monitoring tool, message type, entity ID, display name, state message, and the annotation URL.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/rules-engine/rules-engine-annotations.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"monitoring_tool\": \"API\", \"message_type\":\"INFO\",\n\"entity_id\":\"disk.space/db01\", \"entity_display_name\":\"Approaching Low\nDisk Space on DB01\", \"state_message\":\"The disk is really really full.\nHere is a bunch of information about the problem\",\n\"vo_annotate.u.Runbook\":\"https://help.victorops.com/knowledge-base/rest-endpoint-integration-guide/\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Nagios Plugin via DEB Package\nDESCRIPTION: Commands to download and install the Splunk On-Call Nagios plugin using DEB package manager.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/nagios-integration-guide.rst#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-nagios-1.4.20/victorops-nagios_1.4.20_all.deb\n```\n\n----------------------------------------\n\nTITLE: Defining Apteligent Integration Metadata in reStructuredText\nDESCRIPTION: This snippet sets up the document title, description, and metadata for the Apteligent integration guide using reStructuredText syntax.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/crittercism-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _apteligent-spoc:\n\nApteligent integration for Splunk On-Call\n***************************************************\n\n.. meta::\n    :description: Configure the Apteligent integration for Splunk On-Call.\n```\n\n----------------------------------------\n\nTITLE: RST Reference Links\nDESCRIPTION: RST markup for creating reference links to related documentation topics\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/synth-scenarios/browser-test-scenario.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n* :ref:`global-variables`\n* :ref:`set-up-browser-test`\n* :ref:`browser-metrics`\n* :ref:`browser-test-results`\n```\n\n----------------------------------------\n\nTITLE: Defining Permissions Matrix in RST\nDESCRIPTION: ReStructuredText table defining user role permissions for various Splunk Observability Cloud features including data links, APM metrics, business workflows, and pipeline management.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/admin/roles_data_configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n  :header-rows: 1\n  :widths: 20,20,20,20,20\n\n  * - :strong:`Permission`\n    - :strong:`admin`\n    - :strong:`power`\n    - :strong:`usage`\n    - :strong:`read_only`\n\n  * - :strong:`View Global Data Links`\n    - Yes\n    - Yes\n    - Yes\n    - Yes\n\n  * - :strong:`Create, delete, or clone Global Data Links`\n    - Yes\n    - No\n    - No\n    - No\n\n  * - :strong:`View APM MetricSets`\n    - Yes\n    - Yes\n    - Yes\n    - Yes\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Payload for Zendesk Status Update in Splunk On-Call Outgoing Webhook\nDESCRIPTION: JSON payload to be used in Splunk On-Call's outgoing webhook configuration. This payload sets a Zendesk ticket status to 'pending' when triggered, though users can customize the status value according to their preferences.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zendesk-bi-directional-integration.rst#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{ \n   \"ticket\":{ \n      \"status\": \"pending\" \n   } \n}\n```\n\n----------------------------------------\n\nTITLE: Verifying Docker Image Push (Shell)\nDESCRIPTION: Shell command to list Docker images available locally, which can be used to verify that the image was built successfully before or after pushing.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_4\n\nLANGUAGE: none\nCODE:\n```\n        docker images\n```\n\n----------------------------------------\n\nTITLE: Verify Docker Image Using Cosign\nDESCRIPTION: Command to verify the authenticity of a Collector Docker image using cosign and the public key.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/install-the-collector.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncosign verify --insecure-ignore-tlog --key cosign.pub quay.io/signalfx/splunk-otel-collector:<collector-version>\n```\n\n----------------------------------------\n\nTITLE: SignalFx Translation Rules Actions\nDESCRIPTION: Available actions for metric transformation rules including aggregation, calculation, conversion, copying, and dimension manipulation. These rules allow for customizing metrics before export.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/signalfx-exporter.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ntranslation_rules:\n  - aggregate_metric\n  - calculate_new_metric\n  - convert_values\n  - copy_metrics\n  - delta_metric\n  - divide_int\n  - drop_dimensions\n  - drop_metrics\n  - multiply_float\n  - multiply_int\n  - rename_dimension_keys\n  - rename_metrics\n  - split_metric\n```\n\n----------------------------------------\n\nTITLE: Destroying Terraform-managed Resources - Terraform - Shell Command\nDESCRIPTION: This snippet demonstrates the use of the 'terraform destroy' command to delete all resources (global data links) stored in the terraform.tf.state file. You must provide your Splunk Observability Cloud API token and realm as parameters. Ensure Terraform is installed and initialized in the directory containing your Terraform state file. Input values for <api-access-token> and <realm> are required for successful execution; failure to replace these will result in errors or unexecuted destruction.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-datalinks-terraform/apm-delete-data-links-terraform.rst#2025-04-22_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nterraform destroy -var=\\\"signalfx_auth_token=<api-access-token>\\\" -var=\\\"signalfx_api_url=https://api.<realm>.signalfx.com\\\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Default Traces Pipeline with Mermaid Diagram\nDESCRIPTION: A flowchart showing how traces data flows through the system from different receivers through processors to exporters. The diagram shows how trace data is sent to both Splunk APM and SignalFx exporters.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/collector-config-ootb.rst#2025-04-22_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n\n   accTitle: Default traces pipeline diagram\n   accDescr: Receivers send traces to the traces/memory_limiter processor. The traces/memory_limiter processor sends traces to the batch processor, and the batch processor sends traces to the resource detection processor. The resource detection processor sends traces to the Splunk APM exporter and the SignalFx exporter.\n\n   %% LR indicates the direction (left-to-right)\n\n   %% You can define classes to style nodes and other elements\n   classDef receiver fill:#00FF00\n   classDef processor fill:#FF9900\n   classDef exporter fill:#FF33FF\n\n   %% Each subgraph determines what's in each category\n   subgraph Receivers\n      direction LR\n      traces/jaeger:::receiver\n      traces/otlp:::receiver\n      traces/zipkin:::receiver\n   end\n\n   subgraph Processor\n      direction LR\n      traces/memory_limiter:::processor --> traces/batch:::processor --> traces/resourcedetection:::processor\n   end\n\n   subgraph Exporters\n      direction LR\n      traces/otlphttp:::exporter\n      traces/signalfx/out:::exporter\n   end\n\n   %% Connections beyond categories are added later\n   traces/jaeger --> traces/memory_limiter\n   traces/otlp --> traces/memory_limiter\n   traces/zipkin --> traces/memory_limiter\n   traces/resourcedetection --> traces/otlphttp\n   traces/resourcedetection --> traces/signalfx/out\n```\n\n----------------------------------------\n\nTITLE: AWS ECS Task Definition for Splunk Synthetics Runner\nDESCRIPTION: JSON configuration for creating an AWS ECS task definition for a Splunk Synthetics runner. The configuration includes memory and CPU requirements, network settings, and environment variables for authentication.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n \"requiresCompatibilities\": [\n \"EC2\"\n ],\n \"containerDefinitions\": [\n     {\n         \"name\": \"splunk-synthetics-runner\",\n         \"image\": \"quay.io/signalfx/splunk-synthetics-runner:latest\",\n         \"memory\": 7680,\n         \"cpu\": 2048,\n         \"essential\": true,\n         \"environment\": [\n           {\n               \"name\": \"RUNNER_TOKEN\",\n               \"value\": \"YOUR_TOKEN_HERE\"\n           }\n         ],\n         \"linuxParameters\": {\n               \"capabilities\": {\n                 \"add\": [\"NET_ADMIN\"]\n               }\n         }\n     }\n ],\n \"volumes\": [],\n \"networkMode\": \"none\",\n \"memory\": \"7680\",\n \"cpu\": \"2048\",\n \"placementConstraints\": [],\n \"family\": \"splunk-synthetics\"\n}\n```\n\n----------------------------------------\n\nTITLE: Uninstalling SignalFx Tracing Library for PHP using Shell\nDESCRIPTION: This shell command executes the `signalfx-setup.php` script with the `--uninstall` flag to remove the SignalFx Tracing Library for PHP from the system. It's the first step in the migration process to OpenTelemetry.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-migration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nphp signalfx-setup.php --uninstall\n```\n\n----------------------------------------\n\nTITLE: reStructuredText Directive to Include PHP Requirements\nDESCRIPTION: This reStructuredText directive includes the content of the specified file (`/_includes/requirements/php.rst`) into the current document during the build process. The actual requirements content resides in the included file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-otel-requirements.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: /_includes/requirements/php.rst\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Synthetic Monitoring Alerts\nDESCRIPTION: ReStructuredText markup defining the documentation structure for Splunk Synthetic Monitoring detectors and alerts, including section headers, meta descriptions, and formatted tables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/synth-alerts.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _synth-alerts:\n\n************************************************************\nDetectors and alerts\n************************************************************\n\n.. meta::\n    :description: How to use Splunk Observability Cloud detectors to monitor your tests for anomalies and generate alerts in Splunk Synthetic Monitoring.\n\n.. list-table::\n   :header-rows: 1\n   :widths: 25 75\n  \n   * - :strong:`Detector type`\n     - :strong:`Description`\n\n   * - Test-level detectors\n     - | Test-level detectors send alerts on metrics that correspond to an entire test.\n       | \n       | Examples: Send an alert when the count of failed runs, % uptime, or duration of the entire test exceeds a given threshold.\n```\n\n----------------------------------------\n\nTITLE: Defining ReStructuredText Document Structure for On-Call Schedules Guide\nDESCRIPTION: This snippet defines the structure of a ReStructuredText document for creating and managing on-call schedules. It includes a title, meta description, and a table of contents for related subtopics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/on-call-schedules/create-manage-on-call-schedules.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _create-manage-on-call-schedules:\n\nCreate and manage on-call schedules\n************************************************************************\n\n.. meta::\n   :description: Topics to create and manage schedules, see who's on call, and reassign shifts in Incident Intelligence in Splunk Observability Cloud.\n    \n\n.. toctree::\n   :hidden:\n\n   rotation-setup\n   schedule-examples\n   calendar-export\n   scheduled-overrides\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes Secret via kubectl Command\nDESCRIPTION: Command to apply a Kubernetes secret YAML file that contains the credentials for the Splunk OpenTelemetry Collector. This command deploys the secret defined in the YAML file to the Kubernetes cluster.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s-addon-eks.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f splunk-otel-collector-secret.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Log Observer Permissions Matrix in reStructuredText\nDESCRIPTION: This snippet defines a list-table in reStructuredText format to display Log Observer permissions for different user roles. It includes various Log Observer functions and indicates whether each role has access to that function.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/admin/roles_log_observer.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. list-table::\n  :header-rows: 1\n  :widths: 20,20,20,20,20\n\n  * - :strong:`Permission`\n    - :strong:`admin`\n    - :strong:`power`\n    - :strong:`usage`\n    - :strong:`read_only`\n\n\n  * - :strong:`View Timeline`\n    - Yes\n    - Yes\n    - Yes\n    - Yes\n\n  * - :strong:`Search and filter logs`\n    - Yes\n    - Yes\n    - Yes\n    - Yes\n\n  * - :strong:`Aggregate logs`\n    - Yes\n    - Yes\n    - Yes\n    - Yes\n\n  * - :strong:`Create and manage field aliases`\n    - Yes\n    - Yes\n    - No\n    - No\n\n  * - :strong:`View field aliases`\n    - Yes\n    - Yes\n    - Yes\n    - Yes\n\n  * - :strong:`View individual log details`\n    - Yes\n    - Yes\n    - Yes\n    - Yes\n\n  * - :strong:`Save and share Log Observer queries`\n    - Yes\n    - Yes\n    - No\n    - No\n\n  * - :strong:`Add logs data to Splunk Observability Cloud dashboards`\n    - Yes\n    - Yes\n    - Yes\n    - No\n\n  * - :strong:`View org subscription usage`\n    - Yes\n    - No\n    - Yes\n    - No\n\n  * - :strong:`Set up Log Observer Connect connection to Splunk platform`\n    - Yes\n    - No\n    - No\n    - No\n```\n\n----------------------------------------\n\nTITLE: Embedding HTML Header in RST Documentation\nDESCRIPTION: RST directive for embedding an HTML header with anchor link for the 'How to use this guide' section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/get-started-guide/get-started-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n  \n    <embed>\n      <h2>How to use this guide<a name=\"use-guide\" class=\"headerlink\" href=\"#use-guide\" title=\"Permalink to this headline\">¶</a></h2>\n    </embed>\n```\n\n----------------------------------------\n\nTITLE: Upgrading Auto Instrumentation via Zypper for Node.js (Bash)\nDESCRIPTION: Refreshes the package list and upgrades the `splunk-otel-auto-instrumentation` package using the Zypper package manager on RPM-based systems like SUSE. This is specifically shown in the context of upgrading the instrumentation for Node.js applications. Requires `sudo` privileges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nsudo zypper refresh\nsudo zypper update splunk-otel-auto-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Helm Chart Deployment Commands\nDESCRIPTION: Bash commands for deploying the OpenTelemetry Collector Helm chart with different naming and namespace options.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install splunk-otel-collector -f ./values.yaml splunk-otel-collector-chart/splunk-otel-collector\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm install otel-collector -f ./values.yaml splunk-otel-collector-chart/splunk-otel-collector --namespace o11y\n```\n\n----------------------------------------\n\nTITLE: Manually Installing Auto Instrumentation Debian Package for Node.js (Bash)\nDESCRIPTION: Installs or upgrades the `splunk-otel-auto-instrumentation` package using a downloaded Debian (`.deb`) file. Replace `<path to splunk-otel-auto-instrumentation deb>` with the actual file path. This command is used for manual upgrades on Debian-based systems in the context of Node.js instrumentation. Requires `sudo` privileges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nsudo dpkg -i <path to splunk-otel-auto-instrumentation deb>\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for GitLab Monitors in reStructuredText\nDESCRIPTION: This snippet defines a hidden table of contents for GitLab monitor documentation using reStructuredText directives. It specifies the maximum depth and includes a link to the GitLab-specific documentation file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/gitlab-monitors.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 4\n   :hidden:\n\n   monitors-gitlab/gitlab\n```\n\n----------------------------------------\n\nTITLE: Defining Rollbar API URL for Splunk On-Call Webhook\nDESCRIPTION: Specifies the target URL template for the Splunk On-Call outgoing webhook ('To' field). This URL points to the Rollbar API endpoint for updating an item's status via a PATCH request. It requires dynamic insertion of the Rollbar item ID using the `${{ALERT.rollbar_item_id}}` variable and a valid Rollbar access token appended as a query parameter.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/rollbar-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.rollbar.com/api/1/item/${{ALERT.rollbar_item_id}}?access_token=YOUR_ACCESS_TOKEN_HERE\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Compatibility Requirements\nDESCRIPTION: ReStructuredText (RST) markup defining the structure and content of the compatibility requirements documentation for Splunk Observability Cloud. Includes section headers, metadata, and include directives for modular content.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/requirements.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _requirements:\n\n*********************************************************************\nCompatibility and requirements for Splunk Observability Cloud\n*********************************************************************\n\n.. meta::\n   :description: Splunk Observability Cloud's compatibility and requirements, including infrastructure monitoring agents and application and user monitoring instrumentation compatibility information.\n```\n\n----------------------------------------\n\nTITLE: RST Table Definition for Performance KPI Chart Settings\nDESCRIPTION: A reStructuredText table defining the configuration options available for the Performance KPIs chart, including time range, segmentation, location filters, and metrics settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-status/test-kpis.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n    :header-rows: 1\n    :widths: 20 20 60\n    \n    * - :strong:`Option`\n      - :strong:`Default`\n      - :strong:`Description`\n\n    * - Time\n      - Last 8 hours\n      - Choose the amount of time shown in the chart.\n\n    * - Segment by\n      - Location\n      - | Choose whether the data points are segmented by run location or no segmentation: \n        | \n        | - Choose :strong:`No segmentation` to view data points aggregated from across all locations, pages, and synthetic transactions in your test. \n        | - Choose :strong:`Location` to compare performance across multiple test locations. \n        |\n\n    * - Locations\n      - All locations selected\n      - Choose the run locations you want to display on the chart. \n\n    * - Filter\n      - All locations selected\n      - If you have enabled segmentation by location, choose the run locations you want to display on the chart. \n\n    * - Metrics\n      - Run duration\n      - By default, the chart displays the :guilabel:`Duration` metric. Use the drop-down list to choose the metrics you want to view in the chart.\n```\n\n----------------------------------------\n\nTITLE: Opening the Kubernetes Dashboard with Minikube\nDESCRIPTION: Command to open the Kubernetes web dashboard, which provides a graphical interface for managing the local Kubernetes cluster.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/collector-config-tutorial-start.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nminikube dashboard\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Splunk Observability Onboarding\nDESCRIPTION: ReStructuredText markup defining the structure and content of the onboarding documentation for Splunk Observability Cloud, including sections for trial creation, network analysis, user access management, and team/token planning.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/get-started-guide/onboarding-readiness.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _get-started-guide-onboarding-readiness:\n\nGet started guide phase 1: Onboarding readiness \n*********************************************************\n\n.. note:: This guide is for Splunk Observability Cloud users with the admin role.\n\n.. image:: /_images/get-started/o11y_onboardingGuideFlow_1-onboarding.svg\n   :width: 100%\n   :alt: Flow showing the 3 phases of the get started journey: onboarding, initial rollout, and scaled rollout.\n```\n\n----------------------------------------\n\nTITLE: Building Grafana Docker Image (Shell)\nDESCRIPTION: Shell command to build the Docker image using the Dockerfile created in a previous step. Replace `<imageName>` and `<tagName>` with desired values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_2\n\nLANGUAGE: none\nCODE:\n```\ndocker build -t <imageName>:<tagName>\n```\n\n----------------------------------------\n\nTITLE: Configuring CPUFreq Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration example showing how to activate the CPUFreq monitor in the Collector configuration. Defines the receiver configuration with type collectd/cpufreq.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/cpufreq.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/collectd/cpufreq:\n    type: collectd/cpufreq\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: Basic Telegraf Logparser Configuration in YAML\nDESCRIPTION: Basic configuration example showing how to activate the Logparser integration in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/logparser.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/logparser:\n    type: telegraf/logparser\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Updating Detector SignalFlow Filters for OTel v2.0 Migration (SignalFlow)\nDESCRIPTION: This SignalFlow example shows how to update filters within Splunk APM detectors to accommodate OpenTelemetry Java Agent 2.0 attribute changes. By referencing both the new (`http.response.status_code`) and old (`http.status_code`) tags using an OR condition in the `filter()` function, detectors can correctly process data during the migration period. The query targets the count of successful, non-error requests for the 'adservice'.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/migrate-apm-custom-reporting.rst#2025-04-22_snippet_1\n\nLANGUAGE: signalflow\nCODE:\n```\nA = data('service.request.count', filter=filter('sf_dimensionalized', 'true') and filter('sf_service', 'adservice') and (filter('http_response_status_code', '200') or filter('http_status_code', '200')) and filter('sf_error', 'false')).publish(label='A')\n```\n\n----------------------------------------\n\nTITLE: Disabling Compression for OTLP/HTTP Exporter in YAML\nDESCRIPTION: This YAML configuration snippet shows how to disable payload compression for the `otlphttp` exporter within the OpenTelemetry Collector configuration. By setting the `compression` key to `none` under the `otlphttp` exporter settings, outgoing HTTP requests will not use gzip compression. Gzip compression is enabled by default if this setting is omitted.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/otlphttp-exporter.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otlphttp:\n    ...\n    compression: none\n```\n\n----------------------------------------\n\nTITLE: Uninstalling SignalFx Ruby Agent\nDESCRIPTION: Command to uninstall the SignalFx gem package from the Ruby environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/troubleshooting/migrate-signalfx-ruby-agent-to-otel.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngem uninstall signalfx\n```\n\n----------------------------------------\n\nTITLE: Setting Trace Propagator to b3multi (Windows PowerShell)\nDESCRIPTION: Configures the trace context propagator to 'b3multi' using an environment variable in Windows PowerShell. This ensures compatibility with systems expecting the B3 propagation format.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/configuration/advanced-java-otel-configuration.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$env:OTEL_PROPAGATORS=b3multi\n```\n\n----------------------------------------\n\nTITLE: Adding Extra Metrics to Smart Agent Monitor in YAML\nDESCRIPTION: This example shows how to ingest additional metrics using Smart Agent monitors by adding the extraMetrics field to the monitor configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/smartagent-receiver.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/postgresql:\n    type: postgresql\n    host: mypostgresinstance\n    port: 5432\n    extraMetrics:\n      - actual-name-0 # Add metrics to be ingested\n      - actual-name-1\n```\n\n----------------------------------------\n\nTITLE: Listing Regular AWS Regions for Splunk Observability Cloud\nDESCRIPTION: This snippet enumerates the regular AWS regions supported by Splunk Observability Cloud, including their region codes and corresponding geographical locations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-prereqs.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n* ``ap-northeast-1``: Asia Pacific (Tokyo)\n* ``ap-northeast-2``: Asia Pacific (Seoul)\n* ``ap-northeast-3``: Asia Pacific (Osaka)\n* ``ap-south-1``: Asia Pacific (Mumbai)\n* ``ap-southeast-1``: Asia Pacific (Singapore)\n* ``ap-southeast-2``: Asia Pacific (Sydney)\n* ``ca-central-1``: Canada (Central)\n* ``eu-central-1``: Europe (Frankfurt)\n* ``eu-north-1``: Europe (Stockholm)\n* ``eu-west-1``: Europe (Ireland)\n* ``eu-west-2``: Europe (London)\n* ``eu-west-3``: Europe (Paris)\n* ``sa-east-1``: South America (Sao Paulo)\n* ``us-east-1``: US East (N. Virginia)\n* ``us-east-2``: US East (Ohio)\n* ``us-west-1``: US West (N. California)\n* ``us-west-2``: US West (Oregon)\n```\n\n----------------------------------------\n\nTITLE: Managing Notification Recipients using reStructuredText in Sphinx Documentation - reStructuredText\nDESCRIPTION: Provides the reStructuredText markup required to generate documentation pages about managing notification subscribers in Splunk Observability Cloud. Dependencies include Sphinx or a compatible reStructuredText documentation tool. The snippet includes markup for meta descriptions, steps, UI elements, internal links, notes, and labeled anchors, enabling detailed documentation navigation and presentation. Inputs are `.rst` files, and outputs are rendered HTML or similar formats for end-user guides. Limitations: This is not executable source code, but documentation markup without live interactive features.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/manage-notifications.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _manage-notifications:\n\n*****************************************************************\nManage notification subscribers \n*****************************************************************\n\n\n\n.. meta::\n  :description: Learn how to set recipients for your Splunk Observability Cloud notifications, so that new alerts and their resolution get to the right people in your organization.\n\nWhen a :ref:`detector <create-detectors>` triggers an alert in Splunk Observability Cloud, a notification is sent to the recipients you defined. Each recipient is a subscriber to the detector that triggers the alert.\n\nYou can manage the subscribers for all notifications sent by your detectors at any time, so that new alerts and their status are sent to the right people, channels, and systems in your organization.\n\n.. _receiving-notifications:\n\nTo manage the notification recipients of existing detectors:\n\n- :ref:`Manage detector subscriptions from Alerts <manage-subs>`.\n- :ref:`Subscribe to a detector using the bell icon <subscribe>`.\n- :ref:`Edit the detector and change its rules <build-rules>`.\n\n.. note:: If you're not receiving notifications as expected, see if any detector rules have been deactivated (see :ref:`manage-rules`). Also check the |mtab| to make sure notifications haven't been :ref:`temporarily suspended <mute-notifications>`.\n\n.. _manage-subs:\n\nManage subscribers from the Detectors tab\n============================================================\n\nTo manage the subscribers to a specific detector:\n\n#. Go to :guilabel:`Alerts` and select the :guilabel:`Detectors` tab.\n#. Select the more icon (|more|) of the detector you want to edit.\n#. In the actions menu, select :menuselection:`Manage Subscriptions`.\n#. In the dialog box, select :guilabel:`Add Recipient`.\n\n.. note:: Recipients can be added separately for each rule.\n\n.. _subscribe:\n\nSubscribe to alerts using the Detector menu\n============================================================\n\nYou can manage subscriptions to any detector linked to a chart by using the detector (or bell) icon in each chart that has active detectors. To learn more, see :ref:`list-active-alerts`.\n\nTo subscribe to a linked detector from a chart:\n\n#. Select the :guilabel:`Detector` menu (bell icon).\n#. Select the detector, then select :guilabel:`Subscribe`.\n\n   .. image:: /_images/alerts-detectors-notifications/manage-notifications/detector-subscribe.png\n      :width: 75%\n      :alt: Subscribing to a detector using the Detector menu\n\n.. _remove-recipients:\n\nRemove notification recipients from a detector\n=============================================================\n\nTo stop sending notifications to a recipient, open the detector from the :guilabel:`Detectors` tab and edit each rule. \n\nYou can also :ref:`manage-subs` to see a list of current recipients and select the :guilabel:`X` next to any recipient to unsubscribe them.\n\nDo more with your notifications\n=============================================================\n\nTo further manage your subscriptions and notifications:\n\n-  :ref:`admin-notifs-index`.\n-  :ref:`admin-team-notifications`.\n-  :ref:`mute-notifications`.\n```\n\n----------------------------------------\n\nTITLE: Upgrading OpenTelemetry Collector using Zypper on RPM-based Linux Systems\nDESCRIPTION: Commands to refresh package repositories and upgrade the Splunk OpenTelemetry Collector on RPM-based systems using the zypper package manager. Requires root privileges and preserves modified configuration files.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-upgrade.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo zypper refresh\nsudo zypper update splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Printing the .NET Runtime Identifier in C#\nDESCRIPTION: This C# code snippet demonstrates how to print the application's runtime identifier (RID) to the console using `RuntimeInformation.RuntimeIdentifier`. This information is useful when configuring automatic instrumentation, ensuring the correct instrumentation package is used for the target environment. Requires the `System.Runtime.InteropServices` namespace.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_3\n\nLANGUAGE: c\nCODE:\n```\nConsole.WriteLine(RuntimeInformation.RuntimeIdentifier);\n```\n\n----------------------------------------\n\nTITLE: Tomcat and TomEE Java Agent Configuration on Windows\nDESCRIPTION: Modify the setenv.bat to include the javaagent argument specifying the Splunk Java agent path in Tomcat or TomEE for Windows, applicable only when not using Windows service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/java-servers-instructions.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nset CATALINA_OPTS=%CATALINA_OPTS% -javaagent:\"<Drive>:\\path\\to\\splunk-otel-javaagent.jar\"\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector Ansible Collection in PowerShell\nDESCRIPTION: Command to install the Splunk OpenTelemetry Collector Ansible collection from Ansible Galaxy. This is the first step to enable deploying the collector using Ansible on Windows.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/deployments-windows-ansible.rst#2025-04-22_snippet_0\n\nLANGUAGE: PowerShell\nCODE:\n```\nansible-galaxy collection install signalfx.splunk_otel_collector\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Resources\nDESCRIPTION: Commands to create Kubernetes resources from JSON or YAML files, including ConfigMap creation examples.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo kubectl create -f <file-name>\n\n# Examples\n\n# Use the subcommand configmap to create a ConfigMap from a source file \nsudo kubectl create configmap <map-name> --from-file=<file path>\n```\n\n----------------------------------------\n\nTITLE: Creating Incident Using Victor Slack Command\nDESCRIPTION: Example of using the victor-createincident slash command in Slack to create an incident\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/slack-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n/victor-createincident [this is an example message] for [victorops-username]\n```\n\n----------------------------------------\n\nTITLE: Defining Meta Description in reStructuredText\nDESCRIPTION: This snippet sets the meta description for the document using reStructuredText syntax. It provides a brief summary of the content for search engines and other indexing services.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/mobile/mobile-settings-menu.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. meta::\n   :description: The Splunk On-Call mobile app has many helpful settings to be aware of.\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure Definition\nDESCRIPTION: ReStructuredText markup defining the document structure including toctree, meta description, and includes for Windows auto-discovery documentation\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/discovery-windows.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _discovery-windows:\n\n************************************************************************\nAutomatic discovery and zero-code instrumentation for Windows\n************************************************************************\n\n.. meta:: \n    :description: Get started with automatic discovery for Windows environments. Deploy zero-code instrumentation to automatically find applications running in your Windows environment and send data from them to Splunk Observability Cloud.\n\n.. toctree::\n    :hidden:\n\n    Language runtimes <windows/windows-backend>\n\n.. raw:: html\n\n   <div class=\"include-start\" id=\"gdi/auto-discovery-intro.rst\"></div>\n\n.. include:: /_includes/gdi/auto-discovery-intro.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"gdi/auto-discovery-intro.rst\"></div>\n\n.. raw:: html\n\n    <h2>Get started</h2>\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OTel Collector with systemd Instrumentation\nDESCRIPTION: This command downloads and runs the Splunk OTel Collector installation script with systemd instrumentation. It requires replacing the realm and access token placeholders with actual values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-systemd-instrumentation --realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Activating Kubernetes Events Monitor with Helm\nDESCRIPTION: Command to enable the Kubernetes events monitor when installing with Helm chart.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-events.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n-set splunkObservability.infrastructureMonitoringEventsEnabled='true'\n```\n\n----------------------------------------\n\nTITLE: Defining ReStructuredText Metadata for SAPM Receiver Documentation\nDESCRIPTION: Sets up the ReStructuredText metadata for the SAPM receiver documentation page, including a reference label, title, and meta description. It also includes a cautionary note about the receiver's deprecation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/sapm-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _sapm-receiver:\n\n********************************************\nSplunk APM (SAPM) receiver (deprecated)\n********************************************\n\n.. meta::\n      :description: Receives traces from other collectors or from the SignalFx Smart Agent.\n\n.. caution:: The SAPM receiver is deprecated and will be removed in April 2025. To receive traces from other Collector instances use the :ref:`otlp-receiver` instead.\n```\n\n----------------------------------------\n\nTITLE: RST Metadata Definition for Log Observer Connect Limits Documentation\nDESCRIPTION: RST metadata block that defines the document description for SEO purposes, explaining that the page covers Log Observer Connect's limits on data ingestion, processing rules, and search queries.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/logs/lo-connect-limits.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. meta::\n  :description: See Log Observer Connect's limits on MB of data ingested or indexed per month, limits on the number and type of processing rules, and search query limits.\n```\n\n----------------------------------------\n\nTITLE: Embedding HTML for Email Section in reStructuredText\nDESCRIPTION: This snippet embeds HTML content to create a section header for Email notifications in the reStructuredText document. It includes an anchor link for easy navigation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/notifications/notification-types.rst#2025-04-22_snippet_4\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n  \n    <embed>\n      <h2>Email<a name=\"email\" class=\"headerlink\" href=\"#email\" title=\"Permalink to this headline\">¶</a></h2>\n    </embed>\n```\n\n----------------------------------------\n\nTITLE: Tracking Derived MTS in Aggregation via sum(by=[]) in SignalFlow - SignalFlow Language\nDESCRIPTION: This SignalFlow code illustrates how aggregating data by a field using sum(by=['host']) while calling data('jvm.load') increases the number of derived metric time series (MTS) tracked by the program. For example, if there are 20,000 unique MTS for the metric 'jvm.load', then summing by 'host' doubles the temporary MTS the function must maintain. Exceeding MTS memory limits results in program execution failure.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_3\n\nLANGUAGE: SignalFlow\nCODE:\n```\ndata('jvm.load').sum(by=['host']).publish()\n```\n\n----------------------------------------\n\nTITLE: Navigating to Docker Compose Directory\nDESCRIPTION: Shell command to change to the directory containing the docker-compose.yml file before performing Docker Compose operations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ncd /path/to/your/docker-compose-file\n```\n\n----------------------------------------\n\nTITLE: Activating AlwaysOn Profiling in Node.js\nDESCRIPTION: Activate AlwaysOn Profiling and memory profiling in Splunk OpenTelemetry for Node.js to monitor CPU and memory usage effectively. Recommended for performance optimization.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/instrumentation/instrument-nodejs-application.rst#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nstart({\n   serviceName: '<service-name>',\n   endpoint: 'collectorhost:port',\n   profiling: {                       // Activates CPU profiling\n      memoryProfilingEnabled: true    // Activates Memory profiling\n   }\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing SignalFx Tracing Library (JavaScript)\nDESCRIPTION: Shows the typical initialization pattern for the deprecated SignalFx Tracing Library for Node.js. This code needs to be replaced or updated during the migration process. It requires the 'signalfx-tracing' package.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/troubleshooting/migrate-signalfx-nodejs-agent-to-otel.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n.. code-block:: javascript\n\n  const tracer = require('signalfx-tracing').init({\n   // your options here\n  })\n```\n\n----------------------------------------\n\nTITLE: Legacy CRD template configuration for Helm values\nDESCRIPTION: YAML configuration for users who prefer to continue using Helm templates for CRD deployment (not recommended due to potential race conditions).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\noperator:\n  enabled: true\noperator:\n  crds:\n    create: true\n```\n\n----------------------------------------\n\nTITLE: Configuring IdP for Persistent Subject Format\nDESCRIPTION: Details configuration steps required to resolve a 401 error by setting the Subject attribute format to Persistent in IdP settings. This configuration ensures consistent subject format which is necessary for successful user authentication.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/SSO/sso-troubleshoot.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nPersistent\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Browser Test Results\nDESCRIPTION: ReStructuredText documentation defining the structure and content for browser test results interpretation in Splunk Synthetic Monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/browser-test/browser-test-results.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _browser-test-results:\n\n***********************************************\nInterpret Browser test results\n***********************************************\n\n.. meta::\n    :description: Understand the results of browser tests run in Splunk Synthetic Monitoring and learn how to interpret the data in visualizations, such as the waterfall chart.\n```\n\n----------------------------------------\n\nTITLE: Customizing Screen Names with Annotations in Android RUM\nDESCRIPTION: Shows how to use the @RumScreenName annotation to customize the screen name attribute for an Activity, though this feature is deprecated as of February 2025.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/manual-rum-android-instrumentation.rst#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n@RumScreenName(\"Buttercup\")\npublic class MainActivity extends Activity {\n   ...\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Configuration File for Splunk OpenTelemetry Connector\nDESCRIPTION: Optional command to specify a custom configuration file path for the Splunk OpenTelemetry Connector using the SPLUNK_CONFIG environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/heroku.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nheroku config:set SPLUNK_CONFIG=/app/mydir/myconfig.yaml\n```\n\n----------------------------------------\n\nTITLE: Specifying Custom NPM Path for Node.js Instrumentation (Shell)\nDESCRIPTION: Provides a custom path to the 'npm' executable. This is required if Node.js zero-code instrumentation is enabled and 'npm' cannot be found using 'command -v npm' or if the default installation fails. If 'npm' is not found or fails, Node.js instrumentation is not activated.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\n--npm-path <path>\n```\n\nLANGUAGE: shell\nCODE:\n```\n--npm-path /my/path/to/npm\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nnpm\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry for ASP.NET Core in IIS\nDESCRIPTION: XML configuration for ASP.NET Core applications hosted in IIS to set OpenTelemetry environment variables. This is added to the web.config file's environmentVariables section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_10\n\nLANGUAGE: xml\nCODE:\n```\n<environmentVariables>\n   <environmentVariable name=\"OTEL_SERVICE_NAME\" value=\"my-service-name\" />\n   <environmentVariable name=\"OTEL_RESOURCE_ATTRIBUTES\" value=\"deployment.environment=test,service.version=1.0.0\" />\n</environmentVariables>\n```\n\n----------------------------------------\n\nTITLE: Error Message for Missing Access Token\nDESCRIPTION: Error message displayed when SPLUNK_ACCESS_TOKEN environment variable is not set while SPLUNK_REALM is configured.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/aws/otel-lambda-layer/troubleshooting-lambda-layer.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n[ERROR] SPLUNK_REALM is set, but SPLUNK_ACCESS_TOKEN is not set. To export data to Splunk Observability Cloud, define a Splunk Access Token.\n```\n\n----------------------------------------\n\nTITLE: Injecting Service and Environment Environment Variables into Logback Log Layouts - XML\nDESCRIPTION: Used for serverless or environment-variable-based setups, this XML snippet configures Logback to include OTEL_SERVICE_NAME and OTEL_ENV_NAME in each log entry. Place in logback.xml pattern layout. Make sure environment variables are set in the runtime environment and validate substitution support.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<pattern>\\n   service: ${OTEL_SERVICE_NAME}, env: ${OTEL_ENV_NAME}: %m%n\\n</pattern>\n```\n\n----------------------------------------\n\nTITLE: Configuring React Native RUM Parameters\nDESCRIPTION: JavaScript configuration object that sets the Splunk Observability Cloud realm, RUM access token, application name, and environment for React Native RUM.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/react/install-rum-react.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst RumConfig: ReactNativeConfiguration = {\n   realm: '<realm>',\n   rumAccessToken: '<rum-access-token>',\n   applicationName: '<your-app-name>',\n   environment: '<your-environment>'\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Configuration Container Definition\nDESCRIPTION: JSON configuration for deploying the OpenTelemetry Collector with a custom configuration file in AWS Fargate.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-fargate.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n\"environment\": [\n  {\n    \"name\": \"SPLUNK_CONFIG\",\n    \"value\": \"/path/to/custom/config/file\"\n  }\n],\n\"image\": \"quay.io/signalfx/splunk-otel-collector:0.33.0\",\n\"essential\": true,\n\"name\": \"splunk_otel_collector\"\n}\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents\nDESCRIPTION: ReStructuredText toctree directive defining the navigation structure for the documentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/get-android-data-in.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :hidden:\n\n   Install the Android RUM agent <install-rum-android>\n   Configure the instrumentation <configure-rum-android-instrumentation>\n   Manually instrument applications <manual-rum-android-instrumentation>\n   Android RUM data model <rum-android-data-model>\n   Troubleshooting <troubleshooting>\n```\n\n----------------------------------------\n\nTITLE: Interpreting Trace Exporter Connection Error Log - JSON\nDESCRIPTION: This log output represents an error thrown when the OpenTelemetry OTLP exporter cannot connect to the default collector endpoint (127.0.0.1:55681). It includes stack information and diagnostic fields typical for Node.js network errors. Use these fields to check network configuration, endpoint correctness, and to troubleshoot connection refusals. Input: none; Output: logged JSON error. No direct dependencies.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/troubleshooting/common-nodejs-troubleshooting.rst#2025-04-22_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\"stack\":\"Error: connect ECONNREFUSED 127.0.0.1:55681\\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1148:16)\\n    at TCPConnectWrap.callbackTrampoline (internal/async_hooks.js:131:17)\",\"message\":\"connect ECONNREFUSED 127.0.0.1:55681\",\"errno\":\"-111\",\"code\":\"ECONNREFUSED\",\"syscall\":\"connect\",\"address\":\"127.0.0.1\",\"port\":\"55681\",\"name\":\"Error\"}\n```\n\n----------------------------------------\n\nTITLE: Adding lita-victorops Gem to Gemfile (Bash)\nDESCRIPTION: This command adds the necessary `lita-victorops` gem dependency to the `Gemfile` of a Lita instance. This step is required during the installation of the Litabot adapter for Splunk On-Call integration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/litabot-integration-guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngem “lita-victorops”\n```\n\n----------------------------------------\n\nTITLE: Executing Commands in Kubernetes Containers\nDESCRIPTION: Commands to execute operations within Kubernetes containers, including checking collector configuration and status.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nkubectl exec -it <container/pod> -- curl <commands>\n\n# Examples\n\n# Initial configuration\nkubectl exec -it my-splunk-otel-collector-agent-hg4gk -- curl http://localhost:55554/debug/configz/initial\n\n# Effective configuration\nkubectl exec -it my-splunk-otel-collector-agent-hg4gk -- curl http://localhost:55554/debug/effective\n\n# Check status of the collector\nkubectl exec -it <your-agent-pod> -- curl localhost:55679/debug/tracez | lynx -stdin\nkubectl exec -it splunk-otel-collector-agent-f4gwg -- curl localhost:55679/debug/tracez | lynx -stdin\n```\n\n----------------------------------------\n\nTITLE: Adding Velero Monitor to Service Pipeline\nDESCRIPTION: Configuration snippet showing how to add the Velero monitor to the metrics pipeline in the service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-prometheus/prometheus-velero.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/velero]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Query Counting in SignalFlow Programs - SignalFlow Language\nDESCRIPTION: This SignalFlow snippet illustrates how union and timeshift operations increase the number of queries executed by a SignalFlow program, which counts toward the maximum queries per program limit. Streams A, B, and C retrieve JVM metrics, while D unites A and B and shifts their data by one hour, causing queries A and B to rerun. This can lead to exceeding SignalFlow program query limits if used extensively.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-infra-details.rst#2025-04-22_snippet_0\n\nLANGUAGE: SignalFlow\nCODE:\n```\nA = data('jvm.a').publish('A')\\nB = data('jvm.b').publish('B')\\nC = data('jvm.c').publish('C')\\nD = union(A, B).timeshift('1h').publish('D')\n```\n\n----------------------------------------\n\nTITLE: Setting Up PHP Script for OPcache Monitoring\nDESCRIPTION: A PHP script that retrieves OPcache status information using opcache_get_status() and returns it as JSON. This script needs to be installed on your PHP web server to expose OPcache metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cache/opcache.rst#2025-04-22_snippet_0\n\nLANGUAGE: php\nCODE:\n```\n<?php\nheader('Content-Type: application/json');\n$status=opcache_get_status();\necho json_encode($status,JSON_PRETTY_PRINT);\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Collector with Custom MSI URLs\nDESCRIPTION: PowerShell command to download and run the installation script with custom URLs for the Collector and Fluentd MSI packages. This allows specifying alternative download sources for the installers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-windows/install-windows-msi.rst#2025-04-22_snippet_4\n\nLANGUAGE: PowerShell\nCODE:\n```\n& {Set-ExecutionPolicy Bypass -Scope Process -Force; $script = ((New-Object System.Net.WebClient).DownloadString('https://dl.signalfx.com/splunk-otel-collector.ps1')); $params = @{access_token = \"<SPLUNK_ACCESS_TOKEN>\"; realm = \"<SPLUNK_REALM>\"; collector_msi_url = \"<COLLECTOR_MSI_URL>\"; fluentd_msi_url = \"<FLUENTD_MSI_URL>\"}; Invoke-Command -ScriptBlock ([scriptblock]::Create(\". {$script} $(&{$args} @params)\"))}\n```\n\n----------------------------------------\n\nTITLE: Activating Only Rails Instrumentation in OpenTelemetry SDK Configuration for Ruby\nDESCRIPTION: This Ruby code shows how to configure the OpenTelemetry SDK to use only the Rails instrumentation, ignoring other available instrumentations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/ruby-manual-instrumentation.rst#2025-04-22_snippet_8\n\nLANGUAGE: ruby\nCODE:\n```\nOpenTelemetry::SDK.configure do |c|\nc.use 'OpenTelemetry::Instrumentation::Rails'\nend\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Detection Processors\nDESCRIPTION: Configuration example for resource detection processors that add environment attributes to spans. Shows how to set up system detection and environment tagging.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/splunk-collector-troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection:\n    detectors: [system,env,gce,ec2]\n    override: true\n  resource/add_environment:\n    attributes:\n      - action: insert\n        value: staging\n        key: deployment.environment\n```\n\n----------------------------------------\n\nTITLE: Adding Kubernetes Scheduler to Service Pipeline\nDESCRIPTION: Configuration showing how to add the kubernetes-scheduler monitor to the metrics pipeline in the service section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-scheduler.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n   pipelines:\n     metrics:\n       receivers: [smartagent/kubernetes-scheduler]\n```\n\n----------------------------------------\n\nTITLE: Mounting Host Socket for Systemd Monitoring in Docker\nDESCRIPTION: Example of Docker run command to mount the host's system bus socket and run the container in privileged mode for systemd monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/systemd.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndocker run ...\\\n  --privileged \\\n  -v /var/run/dbus/system_bus_socket:/var/run/dbus/system_bus_socket:ro \\\n...\n```\n\n----------------------------------------\n\nTITLE: Upgrading Zero-Code Instrumentation Package on RPM-based Systems\nDESCRIPTION: Command to upgrade the splunk-otel-auto-instrumentation RPM package using rpm. Requires sudo privileges and the package file path.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nsudo rpm -Uvh <path to splunk-otel-auto-instrumentation rpm>\n```\n\n----------------------------------------\n\nTITLE: Uninstalling .NET Instrumentation on Linux\nDESCRIPTION: This shell command removes the OpenTelemetry for .NET installation directory on Linux systems. Replace <path_of_otel_dotnet_install> with the actual installation path.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\nrm -rf <path_of_otel_dotnet_install>\n```\n\n----------------------------------------\n\nTITLE: Data Loss Detection Metrics in OpenTelemetry Collector\nDESCRIPTION: Metrics used to monitor data loss within the OpenTelemetry Collector's processing pipeline. These track dropped spans, metric points, and logs.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/metrics-internal-collector.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\notelcol_processor_dropped_spans\notelcol_processor_dropped_metric_points\notelcol_processor_dropped_logs\n```\n\n----------------------------------------\n\nTITLE: Including Troubleshooting Components in RST\nDESCRIPTION: This RST code snippet demonstrates how to include a separate troubleshooting components file into the current document using the 'include' directive with custom HTML tags for start and stop markers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/interface.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"troubleshooting-components.rst\"></div>\n\n.. include:: /_includes/troubleshooting-components.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"troubleshooting-components.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Detecting CPU Utilization Limits in Azure\nDESCRIPTION: This SignalFlow function triggers alerts when Azure elastic pool CPU utilization surpasses a designated threshold. Users can specify trigger and clear thresholds and sensitivities for precise monitoring. It requires correctly setting parameters related to utilization and alerting sensitivity.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/autodetect/autodetect-list.rst#2025-04-22_snippet_1\n\nLANGUAGE: SignalFlow\nCODE:\n```\n\"https://github.com/signalfx/signalflow-library/blob/master/library/signalfx/detectors/autodetect/infra/azure/elasticpools.flow#L48\"\n```\n\n----------------------------------------\n\nTITLE: Adding NTP Monitor to Metrics Pipeline\nDESCRIPTION: Configuration snippet showing how to add the NTP monitor to the service pipelines metrics receivers section.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/ntp.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/ntp]\n```\n\n----------------------------------------\n\nTITLE: Exporter Error Log Example\nDESCRIPTION: Example log output showing throttling errors from the OTLP exporter, including 429 response codes and queue overflow warnings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-sizing.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n2021-11-12T00:22:32.172Z\tinfo\texporterhelper/queued_retry.go:325\tExporting failed. Will retry the request after interval.\t{\"kind\": \"exporter\", \"name\": \"otlphttp\", \"error\": \"server responded with 429\", \"interval\": \"4.4850027s\"}\n2021-11-12T00:22:38.087Z\terror\texporterhelper/queued_retry.go:190\tDropping data because sending_queue is full. Try increasing queue_size.\t{\"kind\": \"exporter\", \"name\": \"otlphttp\", \"dropped_items\": 1348}\n```\n\n----------------------------------------\n\nTITLE: HTML Section Headers in RST Documentation\nDESCRIPTION: Raw HTML markup embedded in RST documentation to define section headers for the tutorial structure.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-tutorial/about-aws-tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <h2> What's in this tutorial </h2>\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <h2> How to use this tutorial </h2>\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <h2> Prerequisites </h2>\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <h2> Get started </h2>\n```\n\n----------------------------------------\n\nTITLE: Configuring Agent Mode Configuration File Path in YAML\nDESCRIPTION: Path specification for the agent configuration file in the Splunk Add-on. The default configuration file is located at /otelcol/config/ta-agent-config.yaml.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-addon/collector-addon-configure-instance.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nSplunk_config: ta-agent-config.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Agent Dependencies Post-Upgrade (Bash)\nDESCRIPTION: Navigates to the default installation directory of the Splunk OpenTelemetry Node.js agent (`/usr/lib/splunk-instrumentation/splunk-otel-js`) and installs its dependencies from the bundled tarball (`splunk-otel-js.tgz`) using `npm install`. This step is necessary after upgrading the main `splunk-otel-auto-instrumentation` package to ensure the Node.js agent is correctly set up. Requires `sudo` privileges for `npm install` in this system directory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\ncd /usr/lib/splunk-instrumentation/splunk-otel-js && \\\nsudo npm install /usr/lib/splunk-instrumentation/splunk-otel-js.tgz\n```\n\n----------------------------------------\n\nTITLE: Initializing SignalFx Tracing in Node.js - JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates the traditional method of initializing the SignalFx Tracing Library in Node.js. It requires the `signalfx-tracing` npm package and calls its `init` method with application-specific options. `signalfx-tracing` must be installed before running this. The configuration object supports various options such as service name, token, and endpoints. The code returns a tracer instance for further instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/troubleshooting/migrate-signalfx-nodejs-agent-to-otel.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst tracer = require('signalfx-tracing').init({\n // your options here\n})\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Network Receivers\nDESCRIPTION: RST (reStructuredText) code defining the documentation structure and navigation for network application receivers, including toctree directives and metadata.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/network.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _network:\n\n********************************************************************************\nConfigure application receivers for networks\n********************************************************************************\n\n.. meta::\n   :description: Landing for application receivers for network applications in Splunk Observability Cloud.\n\n.. toctree::\n   :maxdepth: 4\n   :hidden:\n\n   monitors-network/aws-appmesh\n   monitors-network/dns\n   monitors-network/logstash-tcp\n   monitors-network/net-io\n   monitors-network/network-protocols\n   monitors-network/ntp\n   monitors-network/snmp\n   monitors-network/statsd\n   monitors-network/traefik\n```\n\n----------------------------------------\n\nTITLE: Server Timing Header Format Python\nDESCRIPTION: Shows the format of Server-Timing headers used to connect RUM requests with server trace data\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/configuration/advanced-python-otel-configuration.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nAccess-Control-Expose-Headers: Server-Timing\nServer-Timing: traceparent;desc=\"00-<serverTraceId>-<serverSpanId>-01\"\n```\n\n----------------------------------------\n\nTITLE: Resource Allocation for Network Explorer - YAML\nDESCRIPTION: This snippet defines how to configure the resource allocations for a Network Explorer installation by specifying the number of shards used in various telemetry processing stages.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nreducer:\n        ingestShards: 1\n        matchingShards: 1\n        aggregationShards: 1\n\n      reducer:\n        ingestShards: 4\n        matchingShards: 4\n        aggregationShards: 4\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQ Receiver\nDESCRIPTION: Basic configuration to activate the RabbitMQ monitor in the OpenTelemetry Collector. Defines the receiver configuration with type collectd/rabbitmq.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-messaging/rabbitmq.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/rabbitmq:\n    type: collectd/rabbitmq\n    ...  # Additional config\n```\n\n----------------------------------------\n\nTITLE: Azure Documentation Include Directives\nDESCRIPTION: RST markup for including external Azure documentation content with HTML wrapper divs for content management.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/azure/azure-metrics.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"gdi/available-azure.rst\"></div>\n\n.. include:: /_includes/gdi/available-azure.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"gdi/available-azure.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Instrumentations Programmatically (JavaScript)\nDESCRIPTION: Demonstrates how to add custom or third-party OpenTelemetry JS instrumentations alongside the default ones provided by Splunk. It uses `getInstrumentations` to retrieve defaults and spreads them into the `instrumentations` array within the `tracing` configuration object passed to `start()`, followed by custom instrumentation instances.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst { start } = require('@splunk/otel');\nconst { getInstrumentations } = require('@splunk/otel/lib/instrumentations');\n\nstart({\n   tracing: {\n      instrumentations: [\n         ...getInstrumentations(), // Adds default instrumentations\n         new MyCustomInstrumentation(),\n         new AnotherInstrumentation(),\n      ],\n   },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring inputs.conf Parameters for OpenTelemetry Collector\nDESCRIPTION: Default configuration parameters for the OpenTelemetry Collector inputs.conf file, including paths, token settings, and endpoint configurations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-addon/collector-addon-install.rst#2025-04-22_snippet_0\n\nLANGUAGE: conf\nCODE:\n```\nsplunk_config = $SPLUNK_OTEL_TA_HOME/configs/ta-agent-config.yaml\ndisabled = false\nstart_by_shell = false\nsplunk_access_token_file = access_token\nsplunk_realm = us0\nsplunk_trace_ingest_url = https://ingest.us0.signalfx.com/v2/trace\n```\n\n----------------------------------------\n\nTITLE: Installing Collector with Discovery Mode\nDESCRIPTION: Script to install the Splunk OpenTelemetry Collector with automatic discovery enabled.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-third-party.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --realm <realm> – <token> --mode agent --discovery\n```\n\n----------------------------------------\n\nTITLE: AWS ECS Task Definition for Watchtower Auto-Updates\nDESCRIPTION: JSON configuration for setting up Watchtower in AWS ECS to automatically update Docker containers. The task is configured to run as a daemon within the cluster and includes volume mounts for Docker socket access.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"requiresCompatibilities\": [\n    \"EC2\"\n  ],\n  \"containerDefinitions\": [\n    {\n      \"command\": [\n        \"--label-enable\",\n        \"--cleanup\"\n      ],\n      \"name\": \"watchtower\",\n      \"image\": \"v2tec/watchtower:latest\",\n      \"memory\": \"512\",\n      \"essential\": true,\n      \"environment\": [],\n      \"linuxParameters\": null,\n      \"cpu\": \"256\",\n      \"mountPoints\": [\n        {\n          \"readOnly\": null,\n          \"containerPath\": \"/var/run/docker.sock\",\n          \"sourceVolume\": \"dockerHost\"\n        }\n      ]\n    }\n  ],\n  \"volumes\": [\n    {\n      \"name\": \"dockerHost\",\n      \"host\": {\n        \"sourcePath\": \"/var/run/docker.sock\"\n      },\n      \"dockerVolumeConfiguration\": null\n    }\n  ],\n  \"networkMode\": null,\n  \"memory\": \"512\",\n  \"cpu\": \"256\",\n  \"placementConstraints\": [],\n  \"family\": \"watchtower\"\n}\n```\n\n----------------------------------------\n\nTITLE: Exposing Grafana Service on an External IP\nDESCRIPTION: Use kubectl to expose the Grafana service on an external IP, enabling it to be accessed via a web browser. Define the external IP, target port, and external service name in the command.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-helm.rst#2025-04-22_snippet_2\n\nLANGUAGE: none\nCODE:\n```\nkubectl expose service <grafana-service-name> --target-port 3000 --name <external-service-name> --external-ip <external-IP> --port 80\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Ballast with Fixed Size in YAML\nDESCRIPTION: This configuration example sets a fixed size of 64 MiB for the memory ballast.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/memory-ballast-extension.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  memory_ballast:\n    size_mib: 64\n```\n\n----------------------------------------\n\nTITLE: Navigating to Heroku Project Directory\nDESCRIPTION: Command to navigate to the Heroku application directory before running Heroku CLI commands. This is necessary to avoid unexpected behavior when running Heroku commands.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/heroku.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd <HEROKU_APP_DIRECTORY>\n```\n\n----------------------------------------\n\nTITLE: Multiple Data Links in Terraform\nDESCRIPTION: This example demonstrates a Terraform configuration file with multiple data links to different Splunk AppDynamics tiers. Each data link specifies a unique identifier, service name, and endpoint URL to create distinct connections between services.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-datalinks-terraform/apm-create-data-links-terraform-file.rst#2025-04-22_snippet_3\n\nLANGUAGE: terraform\nCODE:\n```\n# Specify the Terraform provider and version\nterraform {\nrequired_providers {\nsignalfx = {\nsource  = \"splunk-terraform/signalfx\"\nversion = \"~> 9.6.0\"\n}\n}\n}\n\n# The following variable blocks can also be located in the variables.tf file in the same directory\nvariable \"signalfx_auth_token\" {\ndescription = \"The user API access auth token for your org\"\ntype        = string\ndefault     = \"\"\n}\nvariable \"signalfx_api_url\" {\ndescription = \"The API URL of your org\"\ntype        = string\ndefault     = \"\"\n}\n\n# Configure the Splunk Observability Cloud provider\nprovider \"signalfx\" {\nauth_token = \"${var.signalfx_auth_token}\"\napi_url    = \"${var.signalfx_api_url}\"\n}\n# A link to a Splunk AppDynamics service\nresource \"signalfx_data_link\" \"my_data_link_appd_1\" {\nproperty_name        = \"sf_service\"\nproperty_value       = \"placed_orders\"\n\ntarget_appd_url {\nname        = \"appd_url_placed_orders\"\nurl         = \"https://www.example.saas.appdynamics.com/#/application=1234&component=5678\"\n}\n}\n\n# A link to a Splunk AppDynamics service\nresource \"signalfx_data_link\" \"my_data_link_appd_2\" {\nproperty_name        = \"sf_service\"\nproperty_value       = \"returned_orders\"\n\ntarget_appd_url {\nname        = \"appd_url_returned_orders\"\nurl         = \"https://www.example.saas.appdynamics.com/#/application=4321&component=8765\"\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Alert Conditions in Splunk Detect Language\nDESCRIPTION: This code snippet demonstrates how to define trigger and detect conditions for an alert in Splunk's alerting system. It uses arbitrary data sources 'metric' and 'test' to set alert conditions with publish statements. No specific dependencies are required. Inputs include variables A and B, while outputs consist of a published alert notification when the defined conditions are met.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/alert-message-variables-reference.rst#2025-04-22_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nA = data('metric').publish('A')\nB = data('test').publish('B')\nTRIGGER_CONDITION = when(A > 100)\n\ndetect(TRIGGER_CONDITION and when(B < 500)).publish('Alert notification')\n```\n\n----------------------------------------\n\nTITLE: Required User Attributes for Microsoft Entra ID SSO Configuration\nDESCRIPTION: Specifies the essential user attributes required when configuring SSO with Microsoft Entra ID (formerly Azure Active Directory) for Splunk Observability Cloud. These attributes, such as FullName or separate FirstName/LastName, PersonImmutableID, and email, must be included in the SAML assertion sent by the IdP for successful user authentication and mapping.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/SSO/sso-about.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nFullName\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nUser.FirstName\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nUser.LastName\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nPersonImmutableID\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nUser.email\n```\n\n----------------------------------------\n\nTITLE: Implementing Exec Input in a Pipeline Configuration\nDESCRIPTION: Example showing how to include the Exec Input monitor in a logs pipeline with appropriate processor and exporter configurations to ensure proper event submission.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/exec-input.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    logs:\n      receivers:\n        - smartagent/exec\n      processors:\n        - resourcedetection\n      exporters:\n        - signalfx\n```\n\n----------------------------------------\n\nTITLE: Creating Timeout Command Symlink\nDESCRIPTION: Commands for creating and configuring a timeout command symlink to avoid CentOS 5 timeout issues.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nln -s /usr/share/doc/bash-3.2/scripts/timeout /usr/bin/timeout\n```\n\nLANGUAGE: bash\nCODE:\n```\nchmod 755 /usr/share/doc/bash-3.2/scripts/timeout\n```\n\n----------------------------------------\n\nTITLE: Example Output of Ruby Logger with Trace Correlation\nDESCRIPTION: This snippet shows an example of the log output format after configuring the logger to include trace metadata. It displays the service name, trace ID, and span ID alongside the log message.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nservice.name=basic-service trace_id=789b159aaee2b389a8771b2588278bcf span_id=6d26eba14a81f3fa\n```\n\n----------------------------------------\n\nTITLE: Example connection error message from k8s-watcher at startup\nDESCRIPTION: This error message occurs during initial startup when k8s-watcher tries to connect to k8s-relay before it's ready. These messages can be safely ignored.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-troubleshoot.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nError: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial tcp [::1]:8712: connect: connection refused\"\n```\n\n----------------------------------------\n\nTITLE: Executing Node.js Script to Create AppDynamics Data Links via Terraform\nDESCRIPTION: This command runs the Node.js script `createAppDLinkTerraformScript.js` to generate and apply a Terraform configuration for creating global data links. It requires three arguments: the path to the CSV file defining the links, the Splunk Observability Cloud API URL, and a valid Splunk Observability Cloud API access token. The script handles the interaction with Terraform to create the resources.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-datalinks-terraform/apm-create-data-links-terraform-batch.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnode createAppDLinkTerraformScript.js <csv-file-path> <o11y-api-url> <o11y-auth-token>\n```\n\n----------------------------------------\n\nTITLE: Checking AlwaysOn Profiling Configuration in Java Agent Logs (Shell)\nDESCRIPTION: Example log output from the Splunk Java agent startup showing the AlwaysOn Profiling configuration settings. Search for 'com.splunk.opentelemetry.profiler.ConfigurationLogger' in logs to find these INFO messages, which confirm settings like enablement status, recording duration, and exporter endpoints.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/common-java-troubleshooting.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n[otel.javaagent 2021-09-28 18:17:04:237 +0000] [main] INFO <snip> - -----------------------\n[otel.javaagent 2021-09-28 18:17:04:237 +0000] [main] INFO <snip> - Profiler configuration:\n[otel.javaagent 2021-09-28 18:17:04:238 +0000] [main] INFO <snip> -                 splunk.profiler.enabled : true\n[otel.javaagent 2021-09-28 18:17:04:239 +0000] [main] INFO <snip> -               splunk.profiler.directory : .\n[otel.javaagent 2021-09-28 18:17:04:244 +0000] [main] INFO <snip> -      splunk.profiler.recording.duration : 20s\n[otel.javaagent 2021-09-28 18:17:04:244 +0000] [main] INFO <snip> -              splunk.profiler.keep-files : false\n[otel.javaagent 2021-09-28 18:17:04:245 +0000] [main] INFO <snip> -           splunk.profiler.logs-endpoint : null\n[otel.javaagent 2021-09-28 18:17:04:245 +0000] [main] INFO <snip> -             otel.exporter.otlp.endpoint : http://collector:4318\n[otel.javaagent 2021-09-28 18:17:04:246 +0000] [main] INFO <snip> -   splunk.profiler.period.jdk.threaddump : null\n[otel.javaagent 2021-09-28 18:17:04:246 +0000] [main] INFO <snip> - -----------------------\n```\n\n----------------------------------------\n\nTITLE: Enabling Global Alert Sending in Kapacitor\nDESCRIPTION: Configures the Kapacitor file to send all alerts globally to Splunk On-Call by setting 'global' to true. Requires prior setup of VictorOps with an API key and routing key.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/kapacitor-integration-guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[victorops]\n   enabled = true\n   api-key = \"558e7ebc-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n   routing-key = \"Sample_route\"\n   global = true\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx PHP Tracing via Terminal Environment Variables\nDESCRIPTION: Shows how to set the required SignalFx configuration as environment variables in a shell terminal using the `export` command. This includes the service name, the trace endpoint URL, and global tags. These variables should be set globally or within the PHP application's start script.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/instrumentation/instrument-php-application.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexport SIGNALFX_SERVICE_NAME=\"<my-service-name>\"\nexport SIGNALFX_ENDPOINT_URL='http://localhost:9080/v1/trace'\nexport SIGNALFX_TRACE_GLOBAL_TAGS=\"deployment.environment:<my_environment>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Sampling by Log ID Attribute in OpenTelemetry Collector\nDESCRIPTION: This YAML configuration sets up the probabilistic sampler processor to sample logs based on a specific log ID attribute, with a 15% sampling rate.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/probabilistic-sampler-processor.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  probabilistic_sampler:\n    sampling_percentage: 15\n    attribute_source: record # possible values: one of record or traceID\n    from_attribute: logID # value is required if the source is not traceID\n```\n\n----------------------------------------\n\nTITLE: Adding Cassandra Monitor to Service Pipeline\nDESCRIPTION: This configuration adds the Cassandra monitor to the metrics pipeline service section, enabling metrics collection for the Cassandra integration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/cassandra.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/cassandra]\n```\n\n----------------------------------------\n\nTITLE: Including Python Requirements in RestructuredText\nDESCRIPTION: This snippet demonstrates how to include an external file containing Python requirements using RestructuredText directives. It uses raw HTML to define start and stop points for the included content.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/python-otel-requirements.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"requirements/python.rst\"></div>\n\n.. include:: /_includes/requirements/python.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"requirements/python.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Node.js Application Deployment YAML With OpenTelemetry Instrumentation\nDESCRIPTION: Kubernetes deployment manifest for a Node.js application with OpenTelemetry auto-instrumentation enabled through the inject-nodejs annotation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/k8s/k8s-backend.rst#2025-04-22_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nodejs-app\n  namespace: monitoring\nspec:\n  template:\n    metadata:\n      annotations:\n        instrumentation.opentelemetry.io/inject-nodejs: \"true\"\n    spec:\n      containers:\n      - name: my-nodejs-app\n        image: my-nodejs-app:latest\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Table Definition for Location Data\nDESCRIPTION: CSV table directives used to display location information for different geographical regions. Each table includes configuration for column widths and header rows.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/public-locations.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. csv-table::\n   :file: us0-public-locations.csv\n   :widths: 10, 10, 10, 10, 20\n   :header-rows: 1\n```\n\n----------------------------------------\n\nTITLE: Complete OpenTelemetry Collector Configuration with Pipelines\nDESCRIPTION: A complete YAML configuration file for the OpenTelemetry Collector that includes extensions, receivers, processors, exporters, and service pipelines. It demonstrates how to properly include the 'syslog' receiver in the logs pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/collector-config-tutorial-troubleshoot.rst#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  health_check:\n    endpoint: \"${SPLUNK_LISTEN_INTERFACE}:13133\"\n\nreceivers:\n  syslog:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: \"${SPLUNK_LISTEN_INTERFACE}:4317\"\n      http:\n        endpoint: \"${SPLUNK_LISTEN_INTERFACE}:4318\"\n\nprocessors:\n  batch:\n\nexporters:\n  otlp:\n    endpoint: \"${SPLUNK_GATEWAY_URL}:4317\"\n    tls:\n      insecure: true\n\nservice:\n  pipelines:\n    traces:\n      receivers:\n      - otlp\n      processors:\n      - batch\n      exporters:\n      - otlp\n    logs:\n      receivers:\n      - syslog\n      exporters:\n      - otlp\n  extensions: [health_check]\n```\n\n----------------------------------------\n\nTITLE: Filtering Docker cgroups Configuration\nDESCRIPTION: Example configuration showing how to filter metrics to only monitor Docker-generated cgroups.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/cgroups.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/cgroups: \n    type: cgroups\n    cgroups:\n      \"/docker/*\"\n```\n\n----------------------------------------\n\nTITLE: Running Hubot with the Splunk On-Call Adapter (Bash)\nDESCRIPTION: Starts the Hubot process using the 'victorops' adapter via a command-line invocation. Requires all dependencies to be installed and the Hubot configuration complete. This launches the chat robot, allowing it to interact with Splunk On-Call and receive commands from the incident timeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/hubot-integration.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/hubot --adapter victorops\n```\n\n----------------------------------------\n\nTITLE: Generating Test Logs for Fluentd\nDESCRIPTION: Commands to manually generate test log entries that Fluentd can monitor through syslog and journald\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/troubleshoot-logs.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\necho \"2021-03-17 02:14:44 +0000 [debug]: test\" >>/var/log/syslog.log\necho \"2021-03-17 02:14:44 +0000 [debug]: test\" | systemd-cat\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic HipChat Webhook URL in Splunk On-Call\nDESCRIPTION: This template defines the destination URL ('To address') for a Splunk On-Call Outgoing Webhook in a Multi-Room HipChat integration. It dynamically inserts the `room_id` and `auth_token` from the alert context (using variables like `{{ALERT.room_id}}` and `{{ALERT.auth_token}}`), which must be previously defined using the Splunk On-Call Rules Engine. This allows routing notifications to different HipChat rooms based on alert properties and requires Splunk On-Call Enterprise features.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/hipchat-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttps://**YOUR_DOMAIN**.hipchat.com/v2/room/:math:`{{ALERT.room\\_id}}/notification?auth\\_token=`\\ {{ALERT.auth_token}}\n```\n\n----------------------------------------\n\nTITLE: Activating Purefa Receiver in Collector Configuration (YAML)\nDESCRIPTION: This snippet shows how to activate the Purefa receiver in the Collector's configuration file. It demonstrates adding 'purefa' to the receivers section and including it in the metrics pipeline.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/purefa-receiver.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  purefa:\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [purefa]\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Environment Variables\nDESCRIPTION: ReStructuredText markup defining the documentation structure for environment variables documentation, including section headers and metadata.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/environment-variables.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _collector-env-var:\n\n*********************************************************************************\nEnvironment variables\n*********************************************************************************\n\n.. meta::\n    :description: Environment variables for the Collector.\n```\n\n----------------------------------------\n\nTITLE: Configuring Metric Activation in a Sample Receiver\nDESCRIPTION: This YAML configuration shows how to selectively enable or disable specific metrics using the 'enabled' field in a sample receiver configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/activate-deactivate-native-metrics.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  samplereceiver:\n    metrics:\n      metric-one:\n        enabled: true\n      metric-two:\n        enabled: false\n```\n\n----------------------------------------\n\nTITLE: Uninstalling SignalFx Tracing Library Using npm - Bash\nDESCRIPTION: This bash snippet shows how to uninstall the deprecated SignalFx Tracing Library (`signalfx-tracing`) from a Node.js project using npm. No dependencies except npm itself are required. The command removes the package from both the local node_modules and the project dependencies. You should run this in the root directory of your Node.js project.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/troubleshooting/migrate-signalfx-nodejs-agent-to-otel.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm uninstall --save signalfx-tracing  \n```\n\n----------------------------------------\n\nTITLE: Implementing OpenTelemetry Lambda Instrumentation\nDESCRIPTION: Example Go code showing how to implement OpenTelemetry instrumentation in an AWS Lambda function. Includes setting up the distro, configuring the tracer provider, and handling Lambda requests with instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/aws/otel-lambda-layer/instrumentation/go-lambdas.rst#2025-04-22_snippet_1\n\nLANGUAGE: go\nCODE:\n```\npackage main\n\nimport (\n   \"context\"\n   \"fmt\"\n\n   \"github.com/aws/aws-lambda-go/lambda\"\n   \"github.com/signalfx/splunk-otel-go/distro\"\n   \"go.opentelemetry.io/contrib/instrumentation/github.com/aws/aws-lambda-go/otellambda\"\n   \"go.opentelemetry.io/otel\"\n)\n\nfunc main() {\n   distro.Run()\n   flusher := otel.GetTracerProvider().(otellambda.Flusher)\n   lambda.Start(otellambda.InstrumentHandler(HandleRequest, otellambda.WithFlusher(flusher)))\n}\n\ntype MyEvent struct {\n   Name string `json:\"name\"`\n}\n\nfunc HandleRequest(ctx context.Context, name MyEvent) (string, error) {\n   return fmt.Sprintf(\"Hello %s!\", name.Name), nil\n}\n```\n\n----------------------------------------\n\nTITLE: Editing VictorOps Configuration File\nDESCRIPTION: Command to edit the VictorOps configuration file for changing service hostname if needed.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/check_mk-integration.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nvi /opt/victorops/nagios_plugin/nagios_conf/victorops.mysite.cfg\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Reference Configuration for Azure Documentation\nDESCRIPTION: Defines document references, metadata, and include directives for Azure integrations documentation. Uses reStructuredText format to structure the documentation and include external content.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/integrations/cloud-azure.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _cloud-azure:\n.. _azure-integrations:\n.. _supported-azure-services:\n\n********************************************************************************\nAvailable Azure integrations\n********************************************************************************\n\n.. meta::\n   :description: Landing for available Azure services.\n\nTo learn about Azure and Splunk Observability Cloud, read :ref:`get-started-azure`.\n\n\n\n.. raw:: html\n\n   <div class=\"include-start\" id=\"gdi/available-azure.rst\"></div>\n\n.. include:: /_includes/gdi/available-azure.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"gdi/available-azure.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Script for Dotcom Monitor Alert Options\nDESCRIPTION: Command to configure the custom script in Dotcom monitor's Alert Options. It specifies the Url_PostExecutor with the Splunk On-Call API endpoint and template ID.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/dotcommonitor-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nUrl_PostExecutor.cs \"https://alert.victorops.com/integrations/generic/20131114/alert/aa57b71c-8374-48ef-a649-fe15ed19a88ff/CURL\" \"1416\"\n```\n\n----------------------------------------\n\nTITLE: Viewing Splunk OTel Collector Service Logs\nDESCRIPTION: This command displays the logs for the splunk-otel-collector service using journalctl.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nsudo journalctl -u splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Detailed Win Performance Counters Configuration Example\nDESCRIPTION: Sample configuration showing counter settings for CPU metrics including idle time, interrupt time, privileged time, user time, and processor time.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/win_perf_counters.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n - type: telegraf/win_perf_counters\n   printValid: true\n   objects:\n    - objectName: \"Processor\"\n      instances:\n       - \"*\"\n      counters:\n       - \"% Idle Time\"\n       - \"% Interrupt Time\"\n       - \"% Privileged Time\"\n       - \"% User Time\"\n       - \"% Processor Time\"\n      includeTotal: true\n      measurement: \"win_cpu\"\n```\n\n----------------------------------------\n\nTITLE: Set Propagators for OpenTelemetry Java Agent\nDESCRIPTION: Shows how to set the trace propagators using environment variables for backward compatibility with previous versions or other agents.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/version1x/java-1x-otel-configuration.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_PROPAGATORS=b3multi\n```\n\n----------------------------------------\n\nTITLE: Deprecated path for operator auto-instrumentation configuration\nDESCRIPTION: The old YAML configuration path for operator auto-instrumentation before version 0.108.0.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\noperator:\n  instrumentation:\n    spec:\n      endpoint: XXX\n      ...\n```\n\n----------------------------------------\n\nTITLE: Specifying ACS URL for Splunk On-Call SSO in Google Apps (Text)\nDESCRIPTION: Provides the specific Assertion Consumer Service (ACS) URL (`https://sso.victorops.com:443/sp/ACS.saml2`) to be entered in the Google Apps SAML configuration for Splunk On-Call. This URL is the endpoint Splunk On-Call uses to receive SAML assertions from Google after successful authentication. This value is entered during step 6 of the configuration process.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/admin/sso/sp-sso-google.rst#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://sso.victorops.com:443/sp/ACS.saml2\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for AWS Lambda ARNs\nDESCRIPTION: GitHub-sourced YAML files containing AWS Lambda layer ARNs for different languages and architectures (Java, Python, Node.js) in x86_64 and ARM64 variants. These files specify the regional ARNs for Splunk OpenTelemetry Lambda layers.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/serverless/aws/otel-lambda-layer/instrumentation/lambda-language-layers.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Note: Actual YAML content is dynamically loaded from GitHub URLs:\n# - splunk-apm-java.md\n# - splunk-apm-java-arm.md\n# - splunk-apm-js.md\n# - splunk-apm-js-arm.md\n# - splunk-apm-python.md\n# - splunk-apm-python-arm.md\n# - splunk-apm-collector.md\n# - splunk-apm-collector-arm.md\n# - splunk-lambda-metrics.md\n# - splunk-lambda-metrics-arm.md\n```\n\n----------------------------------------\n\nTITLE: Downloading VictorOps Nagios Plugin (Debian)\nDESCRIPTION: Uses the `wget` command to download the `.deb` package file for the VictorOps Nagios plugin (compatible with Icinga) from the specified GitHub release URL. This is the first step for installing the plugin on Debian-based systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/icinga-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget https://github.com/victorops/monitoring_tool_releases/releases/download/victorops-nagios-1.4.20/victorops-nagios_1.4.20_all.deb\n```\n\n----------------------------------------\n\nTITLE: Starting New Private Runner Container with Updated Image\nDESCRIPTION: Shell command to start a new Docker container with the latest private runner image, specifying required capabilities and environment variables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --cap-add NET_ADMIN -e \"RUNNER_TOKEN=YOUR_TOKEN_HERE\" http://quay.io/signalfx/splunk-synthetics-runner:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Table Structure for Splunk Documentation\nDESCRIPTION: RST markup defining a structured table that outlines the three phases of Splunk Observability Cloud implementation, including phase descriptions, setup steps, configurations, and educational resources.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/get-started-guide/get-started-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table:: \n   :header-rows: 1\n   :widths: 10 30 30 30\n   :width: 100%\n\n   * - :strong:`Information type`\n     - :strong:`Phase 1: Onboarding readiness`\n     - :strong:`Phase 2: Initial rollout`\n     - :strong:`Phase 3: Scaled rollout`\n```\n\n----------------------------------------\n\nTITLE: Configuring Hadoop JMX for DataNode in Collector\nDESCRIPTION: YAML configuration for the Hadoop JMX integration to collect metrics from a DataNode. Specifies the host, port, and nodeType for the DataNode JMX connection.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoopjmx.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/collectd/hadoopjmx:\n    type: collectd/hadoopjmx\n    host: 127.0.0.1\n    port: 5679\n    nodeType: dataNode\n```\n\n----------------------------------------\n\nTITLE: Registering Custom .NET Meter with Auto-Instrumentation (Bash)\nDESCRIPTION: Demonstrates setting the `OTEL_DOTNET_AUTO_METRICS_ADDITIONAL_SOURCES` environment variable in a Bash shell. This configuration tells the Splunk OpenTelemetry .NET auto-instrumentation agent to listen for and collect metrics produced by the `Meter` named \"My.Application\". Multiple sources can be comma-separated.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/manual-dotnet-instrumentation.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nOTEL_DOTNET_AUTO_METRICS_ADDITIONAL_SOURCES=My.Application\n```\n\n----------------------------------------\n\nTITLE: Starting Nomad and Consul Agents\nDESCRIPTION: Commands to start local development agents for Nomad and Consul services.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-nomad.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n$ nomad agent -dev -network-interface='{{ GetPrivateInterfaces | attr \"name\" }}'\n\n$ consul agent -dev\n```\n\n----------------------------------------\n\nTITLE: Transform Kubernetes Object Logs\nDESCRIPTION: This configuration snippet shows how to edit logs received using the 'k8sobjects' receiver by replacing patterns within log attributes, useful for visualization and alert setups. The processor is set to ignore errors by default.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/transform-processor.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntransform: error_mode: ignore log_statements: - context: log statements: - replace_all_patterns(attributes, \"(object\\\\.)(.*\\\\.)\", \"object.\")\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector via Helm\nDESCRIPTION: Commands to add the Splunk OpenTelemetry Collector Helm chart repository and install the Collector with specific configuration options, including access token, realm, cluster name, and environment settings.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/collector-configuration-tutorial-k8s/collector-config-tutorial-start.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart\nhelm repo update\nhelm install --set=\"splunkObservability.accessToken=<access_token>,clusterName=splunkTutorial,splunkObservability.realm=<realm>,gateway.enabled=false,splunkObservability.profilingEnabled=true,environment=splunkTutorialEnv\" --generate-name splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Activating Debug Logging in Splunk RUM for iOS\nDESCRIPTION: Enables debug logging in the Splunk RUM iOS library by adding the debug() method to SplunkRumBuilder configuration. Debug logging helps identify issues during troubleshooting but should only be enabled when needed as it requires more resources.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: swift\nCODE:\n```\nimport SplunkOtel\n//...\nSplunkRumBuilder(realm: \"<realm>\", rumAuth: \"<rum-token>\")\n// Call functions to configure additional options\n   .debug(enabled: true)\n   .build()\n```\n\n----------------------------------------\n\nTITLE: Adding Memory Monitor to Metrics Pipeline in OpenTelemetry Collector\nDESCRIPTION: YAML configuration for integrating the memory monitor into the metrics pipeline service of the OpenTelemetry Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cache/memory.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/memory]\n```\n\n----------------------------------------\n\nTITLE: Interacting with Lita via Splunk On-Call Timeline (Bash)\nDESCRIPTION: This snippet shows an example command used in the Splunk On-Call timeline chat to interact with an integrated Lita bot. The `@lita` mention directs the command to the bot, and `karma worst` is the specific command being executed.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/litabot-integration-guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@lita karma worst\n```\n\n----------------------------------------\n\nTITLE: Adding NuGet Package for Splunk OpenTelemetry in .NET Project\nDESCRIPTION: This PowerShell command adds the Splunk OpenTelemetry AutoInstrumentation NuGet package to a .NET project. It is necessary for enabling automatic instrumentation of the application. Make sure to replace '<project>' with the actual .csproj file name in your application's root directory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_0\n\nLANGUAGE: Powershell\nCODE:\n```\ndotnet add <project> package Splunk.OpenTelemetry.AutoInstrumentation --prerelease\n```\n\n----------------------------------------\n\nTITLE: Container Security Context with NET_ADMIN and AUDIT_WRITE Configuration\nDESCRIPTION: YAML configuration for container security context with both NET_ADMIN and AUDIT_WRITE capabilities for network shaping and audit messaging.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-config/private-locations.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncontainers:\n  securityContext:\n    capabilities:\n      add:\n      - NET_ADMIN\n      - AUDIT_WRITE\n    allowPrivilegeEscalation: true\n```\n\n----------------------------------------\n\nTITLE: Verifying Collector uninstallation on Linux systems\nDESCRIPTION: Command to verify that the Splunk OpenTelemetry Collector has been successfully uninstalled by checking its systemd service status. If uninstalled, the command should return that the service could not be found.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-uninstall.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl status splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Installing the Collector with Zypper Repository\nDESCRIPTION: Commands to install the required libcap-progs dependency, set up the Splunk OTel Collector RPM repository, and install the Collector and optional zero-code instrumentation package using zypper.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nzypper install -y libcap-progs  # Required for enabling cap_dac_read_search and cap_sys_ptrace capabilities on the Collector\n\ncat <<EOH > /etc/zypp/repos.d/splunk-otel-collector.repo\n[splunk-otel-collector]\nname=Splunk OpenTelemetry Collector Repository\nbaseurl=https://splunk.jfrog.io/splunk/otel-collector-rpm/release/\\$basearch\ngpgcheck=1\ngpgkey=https://splunk.jfrog.io/splunk/otel-collector-rpm/splunk-B3CD4420.pub\nenabled=1\nEOH\n\nzypper install -y splunk-otel-collector\n\n# Optional: install Splunk OpenTelemetry zero-code instrumentation\nzypper install -y splunk-otel-auto-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata Table for Amazon Kinesis Streams in reStructuredText\nDESCRIPTION: This code snippet defines a table in reStructuredText format that lists the metadata properties imported by Infrastructure Monitoring for Amazon Kinesis Streams. It includes the original Kinesis property names, custom property names used in Infrastructure Monitoring, and descriptions of each property.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/monitor/aws-infra-metadata.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 30 30 60\n   :width: 100%\n\n   *  - :strong:`Kinesis Name`\n      - :strong:`Custom Property`\n      - :strong:`Description`\n\n   *  - StreamName\n      - aws_stream_name\n      - The name of the stream\n\n   *  - StreamStatus\n      - aws_stream_status\n      - The server-side encryption type used on the stream\n\n   *  - RetentionPeriodHours\n      - aws_retention_period_hours\n      - The current retention period, in hours\n```\n\n----------------------------------------\n\nTITLE: Auto-instrumenting Node.js Application with Splunk OTel (Bash)\nDESCRIPTION: Shows how to automatically instrument a Node.js application using the Splunk Distribution of OpenTelemetry JS without modifying the application code. This is achieved by using the Node.js '-r' flag to require the '@splunk/otel/instrument' module before running the application script.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/troubleshooting/migrate-signalfx-nodejs-agent-to-otel.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n.. code-block:: bash\n\n  node -r @splunk/otel/instrument <your-app.js>\n```\n\n----------------------------------------\n\nTITLE: Specify Skipped Instrumentations in .NET Project\nDESCRIPTION: This XML configuration allows the .NET project to skip specific instrumentation packages during the build process. This is useful if certain packages are causing conflicts or if their instrumentation is undesirable. Specify each skipped package in a semicolon-separated list.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/instrumentation/instrument-dotnet-application.rst#2025-04-22_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<PropertyGroup>\n   <SkippedInstrumentations>MongoDB.Driver.Core;StackExchange.Redis</SkippedInstrumentations>\n</PropertyGroup>\n```\n\n----------------------------------------\n\nTITLE: RST Include Directive with HTML Wrapper\nDESCRIPTION: RST markup for including external content with HTML div wrapper for content segmentation and identification.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-common-config.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"gdi/collector-common-options.rst\"></div>\n\n.. include:: /_includes/gdi/collector-common-options.rst\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"gdi/collector-common-options.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Understanding and Troubleshooting SSO Integration Errors\nDESCRIPTION: Explains common errors and their diagnostic procedures during SSO integration. It includes issues like application recognition failures, unexpected organization redirects, and common HTTP error responses such as 404, 401, and 500. Highlighted are the significance of correct configuration of parameters like Entity ID, Metadata URL, and ACS URL and how these affect SSO operations.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/authentication/SSO/sso-troubleshoot.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nApplication with identifier https://api.signalfx.com/v1/saml/metadata/EiObDvcAYAA was not found in the directory fa80159f-****-****-****-************.\n```\n\n----------------------------------------\n\nTITLE: Reporting Handled Errors in iOS RUM\nDESCRIPTION: This example shows how to report handled errors, exceptions, and messages using the reportError function. The function accepts different types including String, Error, and NSException.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/ios/manual-rum-ios-instrumentation.rst#2025-04-22_snippet_8\n\nLANGUAGE: swift\nCODE:\n```\nSplunkRum.reportError(example_error)\n```\n\n----------------------------------------\n\nTITLE: Hiding Tags for Specific Operations using APM Visibility Filter API\nDESCRIPTION: The JSON payload demonstrates how to hide specific span tags for a specific operation within a service over a designated timeframe. This approach helps manage leaked data without impacting overall monitoring visibility in Splunk APM.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/set-up-apm/sensitive-data-controls.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"description\": \"Data blocked due to leak on 04/03/21\",\n    \"startTime\": \"2021-04-03T15:00:00.073876Z\",\n    \"endTime\": \"2021-04-21T17:00:00.073876Z\",\n\n    \"matcher\": {\n\n        \"sf_service\": \"checkoutService\",\n        \"sf_operation\": \"readCartDetails\"\n    },\n    \"visibleTags\": [\"sf_environment\", \"sf_service\", \"sf_endpoint\", \"sf_operation\", \"sf_httpMethod\", \"sf_kind\", \"sf_workflow\", \"sf_failure_root_cause_service\", \"sf_error\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Logs for Splunk OpenTelemetry Connector\nDESCRIPTION: Command to check the logs of the Heroku app to verify that the Splunk OpenTelemetry Connector is running properly.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/heroku.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nheroku logs -a <app-name> --tail\n```\n\n----------------------------------------\n\nTITLE: Upgrading Auto Instrumentation via Yum for .NET (Bash)\nDESCRIPTION: Upgrades the `splunk-otel-auto-instrumentation` package using the Yum package manager on older RPM-based systems like CentOS 7. This is specifically shown in the context of upgrading the instrumentation for .NET applications. Requires `sudo` privileges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum upgrade splunk-otel-auto-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js systemd Zero-Code Instrumentation with AlwaysOn Profiling\nDESCRIPTION: Command to install the Splunk OpenTelemetry Collector with systemd Node.js instrumentation and enable AlwaysOn Profiling features for CPU, memory, and metrics.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-systemd-instrumentation --deployment-environment prod \\\n--realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN> \\\n--enable-profiler --enable-profiler-memory --enable-metrics\n```\n\n----------------------------------------\n\nTITLE: Setting Django Environment Variables for Instrumentation\nDESCRIPTION: Commands to set the DJANGO_SETTINGS_MODULE environment variable and run a Django application with OpenTelemetry instrumentation. The --noreload flag prevents Django from restarting when files change.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/instrument-python-frameworks.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport DJANGO_SETTINGS_MODULE=mydjangoproject.settings\nopentelemetry-instrument python3 ./manage.py runserver --noreload\n```\n\nLANGUAGE: powershell\nCODE:\n```\n$env:DJANGO_SETTINGS_MODULE=mydjangoproject.settings\nopentelemetry-instrument python3 ./manage.py runserver --noreload\n```\n\n----------------------------------------\n\nTITLE: Configuring Operator for Profiling in Kubernetes\nDESCRIPTION: This YAML configuration enables the Operator in Kubernetes, which is responsible for auto-instrumenting applications for profiling.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config.rst#2025-04-22_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\noperatorcrds:\n  install: true\noperator:\n  enabled: true\n```\n\n----------------------------------------\n\nTITLE: Deploying Heroku App with Splunk OpenTelemetry Connector\nDESCRIPTION: Command to deploy the Heroku app with the Splunk OpenTelemetry Connector buildpack to the Heroku platform.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-cloud/heroku.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit push heroku main\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Alert Recipients (Splunk On-Call & Email) in Dataset\nDESCRIPTION: This snippet shows the value for the `alertAddress` field when configuring Dataset to send notifications to both Splunk On-Call and multiple email addresses. Recipients are separated by commas. Replace placeholders like `$api_key`, `$routing_key`, and example emails with actual values.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/scalyr-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"alertAddress\": \"victorops:webhookUrl=https://alert.victorops.com/integrations/generic/20131114/alert/$api_key/$routing_key, foo@example.com, bar@example.com\",\n```\n\n----------------------------------------\n\nTITLE: Specifying Auto-Instrumentation Package Version (Shell)\nDESCRIPTION: Installs a specific version of the 'splunk-otel-auto-instrumentation' package instead of the latest. Minimum supported versions are 0.87.0 for Java/Node.js and 0.99.0 for .NET zero-code instrumentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\n--instrumentation-version\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nlatest\n```\n\n----------------------------------------\n\nTITLE: Deleting OpenTelemetry Collector Helm Chart\nDESCRIPTION: Command to remove the Splunk Distribution of OpenTelemetry Collector Helm chart repository.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/otel-commands.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk On-Call Zabbix Plugin via RPM Package\nDESCRIPTION: Commands to download and install the Splunk On-Call Zabbix plugin RPM package and run the installation script. The commands fetch the package from GitHub and use rpm to install it.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zabbix-integration.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nwget\nhttps://github.com/victorops/monitoring_tool_releases/releases/download/victorops-zabbix-0.17.3/victorops-zabbix-0.17.3-2.noarch.rpm\n\nsudo rpm -i victorops-zabbix-0.17.3-2.noarch.rpm\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo ./install\n```\n\n----------------------------------------\n\nTITLE: Configuring Jira Ticket Creation URL Template in Splunk On-Call\nDESCRIPTION: URL template that enables one-click Jira ticket creation from Splunk On-Call incidents. Uses template variables to automatically populate ticket fields with alert information including description, summary, and labels.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/jira-integration-guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: url\nCODE:\n```\nhttps://YOUR_DOMAIN_HERE.atlassian.net/secure/CreateIssueDetails!init.jspa?pid=10000&issuetype=10000&description=${{state_message}}&summary=${{entity_id}}&labels=${{labels}}\n```\n\n----------------------------------------\n\nTITLE: Setting up RST Table of Contents for Network Explorer Documentation\nDESCRIPTION: Configures a hidden table of contents (toctree) directive in reStructuredText that lists all the documentation pages related to Network Explorer in Splunk Infrastructure Monitoring.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    :hidden:\n\n    network-explorer-intro\n    network-explorer-setup\n    network-explorer-network-map\n    network-explorer-metrics\n    network-explorer-scenarios/network-explorer-scenarios\n    network-explorer-troubleshoot\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Ballast Extension in YAML\nDESCRIPTION: Updated configuration showing the memory ballast parameter moved from memory_limiter processor to memory_ballast extension. This change was required when upgrading from version 0.34.0 to 0.35.0.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/collector-upgrade.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  memory_ballast:\n    size_mib: ${SPLUNK_BALLAST_SIZE_MIB}\n```\n\n----------------------------------------\n\nTITLE: Including Troubleshooting Components in RST Documentation\nDESCRIPTION: This snippet demonstrates how to include a separate troubleshooting components file in an RST document using raw HTML and the include directive.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/groupbyattrs-processor.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"troubleshooting-components.rst\"></div>\n\n.. include:: /_includes/troubleshooting-components.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"troubleshooting-components.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Embedding HTML for SMS Subscription Management Section in reStructuredText\nDESCRIPTION: This snippet embeds HTML content to create a subsection header for SMS Subscription Management in the reStructuredText document. It includes an anchor link for easy navigation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/notifications/notification-types.rst#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n  \n    <embed>\n      <h3>SMS Subscription Management<a name=\"sms-subscription\" class=\"headerlink\" href=\"#sms-subscription\" title=\"Permalink to this headline\">¶</a></h3>\n    </embed>\n```\n\n----------------------------------------\n\nTITLE: Creating a Workload Rule for Log Observer Connect in Splunk Enterprise\nDESCRIPTION: Configuration for a Workload Rule in Splunk Enterprise that limits Log Observer Connect searches to 5 minutes. This prevents long-running searches from affecting system performance and ensures a responsive experience for Log Observer users.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/logs/set-up-logconnect.rst#2025-04-22_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nPredicate: user=[your_Log_Observer_Connect_service-account_name] AND runtime>5m\nSchedule: Always on\nAction: Abort search\n```\n\n----------------------------------------\n\nTITLE: Identifying JSM Access Denied Error in Java (Bash)\nDESCRIPTION: Shows a 'java.security.AccessControlException' error message that may appear in logs if the Java Security Manager (JSM) is active and denies the Splunk OTel Java agent permission to read the 'otel.javaagent.debug' property. This indicates a JSM policy configuration issue.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/troubleshooting/common-java-troubleshooting.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\njava.security.AccessControlException: access denied (\"java.util.PropertyPermission\" \"otel.javaagent.debug\" \"read\")\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata Table for AWS Lambda in reStructuredText\nDESCRIPTION: This code snippet defines a table in reStructuredText format that lists the metadata properties imported by Infrastructure Monitoring for AWS Lambda. It includes the original Lambda property names, custom property names used in Infrastructure Monitoring, and descriptions of each property.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/monitor/aws-infra-metadata.rst#2025-04-22_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :widths: 30 30 60\n   :width: 100%\n\n   *  - :strong:`AWS Lambda Filter Name`\n      - :strong:`Custom Property`\n      - :strong:`Description`\n\n\n   *  - CodeSha256\n      - aws_function_code_sha256\n      - SHA256 hash of your function deployment package\n\n   *  - CodeSize\n      - aws_function_code_size\n      - The size of the .zip file you uploaded for the function, in bytes\n\n   *  - FunctionName\n      - aws_function_name\n      - Function name\n\n   *  - MemorySize\n      - aws_function_memory_size\n      - Memory size you configured for the function, in MB\n\n   *  - Runtime\n      - aws_function_runtime\n      - Runtime environment for the function\n\n   *  - Timeout\n      - aws_function_timeout\n      - The function execution time at which AWS Lambda needs to terminate the function\n\n   *  - Version\n      - aws_function_version\n      - The function version\n\n   *  - VpcConfig.vpcId\n      - aws_function_vpc_id\n      - The Amazon Virtual Private Cloud (VPC) ID associated with your function\n```\n\n----------------------------------------\n\nTITLE: Configuring Wavefront Alert Target POST Body Template for VictorOps Integration\nDESCRIPTION: JSON template for Wavefront webhook configuration that maps Wavefront alert states to VictorOps notification format. Includes message type mapping, entity identification, state messages, and display name formatting.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/wavefront-integration-guide-victorops.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"message_type\": \"{{#endedTime}}recovery{{/endedTime}}{{^endedTime}}{{#severitySmoke}}info{{/severitySmoke}}{{#severityInfo}}info{{/severityInfo}}{{#severityWarning}}warning{{/severityWarning}}{{#severitySevere}}critical{{/severitySevere}}{{/endedTime}}\", \"entity_id\": \"{{#trimTrailingComma}}{{^endedTime}}{{#failingHosts}}{{{.}}},{{/failingHosts}}{{/endedTime}}{{#endedTime}}{{#recoveredHosts}}{{{.}}},{{/recoveredHosts}}{{/endedTime}}{{/trimTrailingComma}}\", \"state_message\": \"{{{name}}}\\n{{{url}}}\\n{{#jsonEscape}}{{{additionalInformation}}}{{/jsonEscape}}\", \"monitoring_tool\": \"Wavefront\", \"entity_display_name\": \"{{#jsonEscape}}{{{hostsFailingMessage}}}{{/jsonEscape}}\" }\n```\n\n----------------------------------------\n\nTITLE: Upgrading Auto Instrumentation via Zypper for .NET (Bash)\nDESCRIPTION: Refreshes the package list and upgrades the `splunk-otel-auto-instrumentation` package using the Zypper package manager on RPM-based systems like SUSE. This is specifically shown in the context of upgrading the instrumentation for .NET applications. Requires `sudo` privileges.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\nsudo zypper refresh\nsudo zypper update splunk-otel-auto-instrumentation\n```\n\n----------------------------------------\n\nTITLE: Configuring Interface Traffic Monitor Options in RST\nDESCRIPTION: This RST code snippet defines a table of configuration options for the interface traffic monitor. It includes options for excluding and including specific interfaces, along with their types and descriptions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/interface.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n   :widths: 18 18 18 18\n   :header-rows: 1\n\n   - \n\n      - Option\n      - Required\n      - Type\n      - Description\n   - \n\n      - ``excludedInterfaces``\n      - no\n      - ``list of strings``\n      - List of interface names to exclude from monitoring (**default:**\n         ``[/^lo\\d*$/ /^docker.*/ /^t(un|ap)\\d*$/ /^veth.*$/]``)\n   - \n\n      - ``includedInterfaces``\n      - no\n      - ``list of strings``\n      - List of all the interfaces you want to monitor, all others will\n         be ignored. If you set both ``included`` and\n         ``excludedInterfaces``, only ``includedInterfaces`` will be\n         honored.\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Protocol Flag in Chrome\nDESCRIPTION: Flag to disable HTTP/2 and force requests to use HTTP/1.1 protocol\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/synthetics/chrome-flags.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n--disable-http2\n```\n\n----------------------------------------\n\nTITLE: Initializing Terraform Directory\nDESCRIPTION: This command initializes the Terraform working directory for subsequent operations like planning and applying changes. It prepares the directory for the `terraform apply` command by downloading necessary provider plugins.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-datalinks-terraform/apm-create-data-links-terraform-file.rst#2025-04-22_snippet_4\n\nLANGUAGE: none\nCODE:\n```\nterraform init\n```\n\n----------------------------------------\n\nTITLE: Installing Rails Instrumentation Gem using Bundler in Ruby\nDESCRIPTION: This command demonstrates how to install the Rails instrumentation gem using the Bundler package manager. It specifies the gem name and version constraint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/distro/instrumentation/ruby-manual-instrumentation.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbundle add opentelemetry-instrumentation-rails --version \"~> 0.27\"\n```\n\n----------------------------------------\n\nTITLE: Additional Permissions in RST Comments\nDESCRIPTION: ReStructuredText commented sections containing additional permission definitions for business workflows, pipeline management, and RUM features.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/admin/roles_data_configuration.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n..\n  Check the following  \n\n  * - :strong:`Create APM MetricSets`\n    - Yes\n    - No\n    - No\n    - No\n\n  * - :strong:`View Business Workflow`\n    - Yes\n    - Yes\n    - Yes\n    - Yes\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk OpenTelemetry Collector with Extended Telemetry - Bash\nDESCRIPTION: This snippet provides the Helm installation command to collect Network Explorer along with other telemetry data. It enables logging and infrastructure monitoring events and sets resource limits for the gateway.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/network-explorer/network-explorer-setup.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm --namespace=<NAMESPACE> install splunk-otel-collector \\\n          --set=\"splunkObservability.realm=<REALM>\" \\\n          --set=\"splunkObservability.accessToken=<ACCESS_TOKEN>\" \\\n          --set=\"clusterName=<CLUSTER_NAME>\" \\\n          --set=\"splunkObservability.logsEnabled=true\" \\\n          --set=\"splunkObservability.infrastructureMonitoringEventsEnabled=true\" \\\n          --set=\"agent.enabled=true\" \\\n          --set=\"clusterReceiver.enabled=true\" \\\n          --set=\"gateway.replicaCount=1\" \\\n          --set=\"environment=<APM_ENV>\" \\\n          --set=\"gateway.resources.limits.cpu=500m\" \\\n          --set=\"gateway.resources.limits.memory=1Gi\" \\\n          splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: AWS Data Import Application Options Table\nDESCRIPTION: Table showing additional applications that use AWS services with links to documentation for getting data into Splunk and monitoring those applications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/infrastructure/monitor/aws-infra-import.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :width: 100\n   :widths: 30, 20, 50\n\n   *  - :strong:`Get data in`\n      - :strong:`Monitor`\n      - :strong:`Description`\n\n   *  - :ref:`get-started-k8s`\n      - :ref:`infrastructure-k8s-nav`\n      - Import metrics and logs from Kubernetes clusters running in EC2 instances or EKS.\n\n   *  -  - :ref:`get-started-linux`\n         - :ref:`get-started-windows`\n      - :ref:`infrastructure-hosts`\n      - Import metrics and logs from Linux and Windows hosts running in EC2 instances.\n\n   *  - :ref:`get-started-application`\n      - :ref:`get-started-apm`\n      - Import application metrics and spans running in hosts, Kubernetes clusters, or Lambda functions.\n```\n\n----------------------------------------\n\nTITLE: HTML Section Header for Prerequisites\nDESCRIPTION: Embedded HTML markup for the prerequisites section header with anchor link\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/get-started/contribute.rst#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<embed>\n  <h2>Prerequisites<a name=\"contribute-prereqs\" class=\"headerlink\" href=\"#contribute-prereqs\" title=\"Permalink to this headline\">¶</a></h2>\n</embed>\n```\n\n----------------------------------------\n\nTITLE: Disabling Individual Instrumentations via Environment Variables - Shell\nDESCRIPTION: This shell snippet demonstrates how to disable a specific instrumentation (such as SqlClient) by setting the corresponding OTEL_DOTNET_AUTO_TRACES_{name}_INSTRUMENTATION_ENABLED environment variable to 'false'. This action helps reduce span volume and instrumentation overhead in .NET applications instrumented with the Splunk Distribution of OpenTelemetry. Dependencies include a properly configured .NET application environment with OpenTelemetry automatic instrumentation enabled; users substitute {name} with the target instrumentation (e.g., 'SQLCLIENT'). Inputs are the variable name and value, output is that the specified instrumentation will be disabled during application run. There are no major constraints, but this only affects the specified instrumentation and must be set in the application's environment.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/performance.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nOTEL_DOTNET_AUTO_TRACES_SQLCLIENT_INSTRUMENTATION_ENABLED=false\n```\n\n----------------------------------------\n\nTITLE: Overriding Service Name for Instrumented Applications (Shell)\nDESCRIPTION: Sets a specific service name for all instrumented applications, overriding any auto-generated names. This value is assigned to the 'OTEL_SERVICE_NAME' environment variable.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux.rst#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n--service-name <name>\n```\n\n----------------------------------------\n\nTITLE: Setting Splunk Environment Variables on Linux\nDESCRIPTION: Configuration of environment variables in Linux to enable direct data transmission to Splunk Observability Cloud. Requires setting access token and realm variables.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/version-2x/instrumentation/instrument-nodejs-applications.rst#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nexport SPLUNK_ACCESS_TOKEN=<access_token>\nexport SPLUNK_REALM=<realm>\n```\n\n----------------------------------------\n\nTITLE: Removing Unsupported `publish()` Method in SignalFlow for Histograms\nDESCRIPTION: Indicates the need to remove the `publish()` method from a SignalFlow program when working with the `histogram()` function. The `publish()` method is not supported for histogram data streams.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/chart-builder.rst#2025-04-22_snippet_2\n\nLANGUAGE: SignalFlow\nCODE:\n```\npublish()\n```\n\n----------------------------------------\n\nTITLE: Installing Zero-Code Instrumentation Package on Debian\nDESCRIPTION: Command to install the splunk-otel-auto-instrumentation Debian package using dpkg. Requires root privileges and the package file path.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/install-linux-manual.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndpkg -i <path to splunk-otel-auto-instrumentation deb>\n```\n\n----------------------------------------\n\nTITLE: Enabling Centralized RBAC for Splunk Observability Cloud\nDESCRIPTION: Command to enable centralized RBAC management, making Splunk Cloud Platform the source of truth for Observability Cloud user roles. Requires an access token.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/centralized-rbac.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nacs observability enable-centralized-rbac --o11y-access-token <access-token>\n```\n\n----------------------------------------\n\nTITLE: Installing System-wide .NET Zero-Code Instrumentation\nDESCRIPTION: Command to install the Splunk OpenTelemetry Collector with system-wide .NET instrumentation using the installer script.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/linux/linux-backend.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh > /tmp/splunk-otel-collector.sh && \\\nsudo sh /tmp/splunk-otel-collector.sh --with-instrumentation --realm <SPLUNK_REALM> -- <SPLUNK_ACCESS_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Docker Build and Push Commands\nDESCRIPTION: Commands to build and push the Tomcat container image with the Java agent to Docker Hub.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-fargate-java.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker build --platform=\"linux/amd64\" -t tomcat-with-splunk-java-agent:latest --no-cache .\n\ndocker tag tomcat-with-splunk-java-agent:latest username/tomcat-with-splunk-java-agent:latest \n\ndocker push username/tomcat-with-splunk-java-agent:latest\n```\n\n----------------------------------------\n\nTITLE: Viewing Required Instrumentation Packages\nDESCRIPTION: Displays the list of instrumentation packages required for your environment without installing them. The output can be added to your project's requirements or Pipfile.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/version1x/instrument-python-application-1x.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsplunk-py-trace-bootstrap --action=requirements\n```\n\n----------------------------------------\n\nTITLE: RST Raw HTML Include for Metrics YAML\nDESCRIPTION: Includes raw HTML div to embed Hadoop JMX metrics metadata YAML from GitHub repository.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/hadoopjmx.rst#2025-04-22_snippet_7\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n      <div class=\"metrics-yaml\" url=\"https://raw.githubusercontent.com/signalfx/splunk-otel-collector/main/internal/signalfx-agent/pkg/monitors/collectd/hadoopjmx/metadata.yaml\"></div>\n```\n\n----------------------------------------\n\nTITLE: Helm Chart Deployment for Kubernetes Objects Receiver\nDESCRIPTION: Helm chart configuration for deploying the Kubernetes Objects receiver, specifying object types and collection modes.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-objects-receiver.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nclusterReceiver:\n k8sObjects:\n   - name: pods\n     mode: pull\n     label_selector: environment in (production),tier in (frontend)\n     field_selector: status.phase=Running\n     interval: 15m\n   - name: events\n     mode: watch\n     group: events.k8s.io\n     namespaces: [default]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry .NET Package with PowerShell\nDESCRIPTION: PowerShell script to install the Splunk Distribution of OpenTelemetry .NET with automatic instrumentation. The script requires access token, realm, and environment name parameters. It enables zero-code instrumentation for .NET applications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/automatic-discovery/windows/windows-backend.rst#2025-04-22_snippet_0\n\nLANGUAGE: powershell\nCODE:\n```\n& {Set-ExecutionPolicy Bypass -Scope Process -Force; `\n$script = ((New-Object System.Net.WebClient).DownloadString('https://dl.signalfx.com/splunk-otel-collector.ps1')); `\n$params = @{access_token = \"<access_token>\"; realm = \"<realm>\"; mode = \"agent\"; with_dotnet_instrumentation = \"\\$true\"; deployment_env = \"<environment_name>\"}; `\nInvoke-Command -ScriptBlock ([scriptblock]::Create(\". {$script} $(&{$args} @params)\"))}\n```\n\n----------------------------------------\n\nTITLE: Defining Billing Classes Table Structure in reStructuredText\nDESCRIPTION: A reStructuredText list table that defines the billing classes and associated metrics in Splunk Observability Cloud. The table has two columns showing the billing class name and the metrics included in each class.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/_includes/metric-classes.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. list-table:: \n  :header-rows: 1\n  :widths: 20 80\n  :width: 100%\n\n  * - :strong:`Billing class`\n    - :strong:`Metrics included`\n  * - Custom metrics\n    - Metrics reported to Splunk Observability Cloud outside of those reported by default, such as host, container, or bundled metrics. Custom metrics might result in increased data ingest costs.\n  * - APM Monitoring MetricSets\n    - Includes metrics from APM Monitoring MetricSets. See :ref:`apm-metricsets` for more information.\n  * - RUM Monitoring MetricSets\n    - Includes metrics from RUM Monitoring MetricSets. See :ref:`rum-custom-indexed-tags` for more information.\n  * - Default/bundled metrics (Infrastructure)\n    - * Host\n      * Container\n      * Bundled\n      * Additional metrics sent through infrastructure monitoring public cloud integrations that aren't attributed to specific hosts or containers.\n  * - Default/bundled metrics (APM)\n    - * Host\n      * Container\n      * Identity\n      * Bundled\n      * Tracing \n      * Runtime\n      * Synthetics\n  * - Other metrics\n    - Internal metrics\n```\n\n----------------------------------------\n\nTITLE: Statsd Metric Name Formatting\nDESCRIPTION: Example showing how to format metric names using the converter configuration with a metricName template.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-network/statsd.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nconverters:\n  - pattern: \"cluster.cds_{traffic}_{mesh}_{service}-vn_{}.{action}\"\n    metricName: \"{traffic}.{action}\"\n```\n\n----------------------------------------\n\nTITLE: Setting Pod Security Policies Manually\nDESCRIPTION: Manual installation of Pod Security Policies in pre Kubernetes 1.25 clusters. Configures a PodSecurityPolicy with specific security profiles, allowing privilege rules. Requires corresponding custom ClusterRole rules in values.yaml for application.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-config-advanced.rst#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n    cat <<EOF | kubectl apply -f -\n    apiVersion: policy/v1beta1\n    kind: PodSecurityPolicy\n    metadata:\n      name: splunk-otel-collector-psp\n      labels:\n        app: splunk-otel-collector-psp\n      annotations:\n        seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'runtime/default'\n        apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'\n        seccomp.security.alpha.kubernetes.io/defaultProfileName:  'runtime/default'\n        apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'\n    spec:\n      privileged: false\n      allowPrivilegeEscalation: false\n      hostNetwork: true\n      hostIPC: false\n      hostPID: false\n      volumes:\n      - 'configMap'\n      - 'emptyDir'\n      - 'hostPath'\n      - 'secret'\n      runAsUser:\n        rule: 'RunAsAny'\n      seLinux:\n        rule: 'RunAsAny'\n      supplementalGroups:\n        rule: 'RunAsAny'\n      fsGroup:\n        rule: 'RunAsAny'\n    EOF\n```\n\nLANGUAGE: yaml\nCODE:\n```\n    rbac:\n      customRules:\n        - apiGroups:     [extensions]\n          resources:     [podsecuritypolicies]\n          verbs:         [use]\n          resourceNames: [splunk-otel-collector-psp]\n```\n\nLANGUAGE: yaml\nCODE:\n```\n    helm install my-splunk-otel-collector -f my_values.yaml splunk-otel-collector-chart/splunk-otel-collector\n```\n\n----------------------------------------\n\nTITLE: Thread Pools Configuration\nDESCRIPTION: Example showing how to configure collection of statistics from specific thread pools.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/elasticsearch.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmonitors:\n- type: elasticsearch\n  host: localhost\n  port: 9200\n  threadPools:\n  - bulk\n  - warmer\n  - listener\n```\n\n----------------------------------------\n\nTITLE: Example Log Entry Confirming AlwaysOn Profiling Start\nDESCRIPTION: This log entry, typically found at the 'info' level in the .NET instrumentation logs, confirms that the AlwaysOn Profiling feature has successfully started its thread sampling. Checking for this message helps verify that the profiler is activated (`SPLUNK_PROFILER_ENABLED=true`).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/troubleshooting/common-dotnet-troubleshooting.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n10/12/23 12:10:31.962 PM [12096|22036] [info] ContinuousProfiler::StartThreadSampling\n```\n\n----------------------------------------\n\nTITLE: Uninstalling only the Fluentd package on Debian-based Linux systems\nDESCRIPTION: Command for uninstalling just the Fluentd (td-agent) package on Debian-based Linux systems using apt-get, leaving the Collector installed if present.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/linux-uninstall.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get purge td-agent\n```\n\n----------------------------------------\n\nTITLE: Template String Example in Twilio Error Message\nDESCRIPTION: Shows the format of a Twilio error message template using string interpolation for team name.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/twilio-live-call-routing-guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n\"Please leave a message for the ${Team} and hang up when you are finished\"\n```\n\n----------------------------------------\n\nTITLE: Complete Kubernetes Events Monitor Configuration Example\nDESCRIPTION: Full example configuration showing how to whitelist specific Kubernetes events based on reason and object kind.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-events.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n   smartagent/kubernetes-events:\n     type: kubernetes-events\n     whitelistedEvents:\n       - reason: Created\n         involvedObjectKind: Pod\n       - reason: SuccessfulCreate\n         involvedObjectKind: ReplicaSet\n```\n\n----------------------------------------\n\nTITLE: Importing OpenTelemetry Metrics API in Python\nDESCRIPTION: Imports the necessary OpenTelemetry modules for creating custom metrics in Python applications.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/python/instrumentation/python-manual-instrumentation.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import metrics\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import (\n   ConsoleMetricExporter,\n   PeriodicExportingMetricReader,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Cipher Suites in Collector TLS Configuration - YAML\nDESCRIPTION: This YAML snippet demonstrates explicit specification of cipher suites in a TLS configuration for an OpenTelemetry Collector component. The 'cipher_suites' key allows selection of allowed TLS ciphers instead of relying on defaults. No additional dependencies are required beyond YAML and Collector support for these options. The suite names must match those supported by Go's crypto/tls implementation. Input is a YAML list; output is interpreted internally by the Collector. Limitation: only available ciphers should be specified; specifying unsupported ciphers will result in errors.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/common-config/collector-common-config-tls.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncipher_suites:\n  - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n  - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\n  - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\n```\n\n----------------------------------------\n\nTITLE: Sending SignalFx PHP Data Directly via Terminal Environment Variables\nDESCRIPTION: Shows how to configure direct data export to Splunk Observability Cloud by setting environment variables in a shell terminal. It uses `export` to define the `SIGNALFX_ACCESS_TOKEN` and sets `SIGNALFX_ENDPOINT_URL` to the ingest URL, allowing the PHP tracer to send data directly without a local OTel Collector.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/instrumentation/instrument-php-application.rst#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nexport SIGNALFX_ACCESS_TOKEN=<access_token>\nexport SIGNALFX_ENDPOINT_URL=https://ingest.<realm>.signalfx.com/v2/trace/signalfxv1\n```\n\n----------------------------------------\n\nTITLE: Updating SignalFx PHP INI Settings via Setup Script using Shell\nDESCRIPTION: Provides an example of using the `signalfx-setup.php` script with the `--update-config` flag to modify PHP INI settings directly. This example sets the `signalfx.endpoint_url` INI property, which is useful for applying settings common to all PHP services on a system.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/instrumentation/instrument-php-application.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nphp signalfx-setup.php --update-config --signalfx.endpoint_url=http://172.17.0.1:9080/v1/trace\n```\n\n----------------------------------------\n\nTITLE: Example Custom Dimension Tag Identifier\nDESCRIPTION: This snippet represents an example custom dimension tag key, 'customer.id'. It is used within the documentation to illustrate how filtering by specific tag values associated with this key (like unique customer IDs) can help reduce high cardinality in Monitoring MetricSets.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/span-tags/troubleshoot-mms.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncustomer.id\n```\n\n----------------------------------------\n\nTITLE: Assigning Streams to Capital Letters in SignalFlow\nDESCRIPTION: Demonstrates how to correctly assign streams to capital letters from A to Z in SignalFlow. This approach allows for conversion to the plot-builder UI.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/chart-builder.rst#2025-04-22_snippet_4\n\nLANGUAGE: SignalFlow\nCODE:\n```\nA = data('cpu.utilization').(label='A')\nB = data('cpu.utilization').publish(label='B')\nC = (A/B+10).publish(label='C')\n```\n\n----------------------------------------\n\nTITLE: Embedding Dynamic Dependency Information\nDESCRIPTION: This HTML div is used within a reStructuredText document to dynamically embed dependency information. It specifies a section ('dependencies'), a source URL for the data (a metadata.yaml file on GitHub), and renaming rules for the displayed data fields. This indicates the documentation build process fetches and formats this information.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/otel-dotnet/dotnet-requirements.rst#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"instrumentation\" section=\"dependencies\" url=\"https://raw.githubusercontent.com/splunk/o11y-gdi-metadata/main/apm/splunk-otel-dotnet/metadata.yaml\" data-renaming='{\"name\": \"Dependency\", \"source_href\": \"Link to source\", \"version\": \"Version\", \"stability\": \"Stability\"}'></div>\n```\n\n----------------------------------------\n\nTITLE: Setting Node Name to Environment Variable\nDESCRIPTION: Sets a node name to an environment variable for use in subsequent commands that need to reference a specific node.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/k8s-troubleshooting/troubleshoot-k8s-container.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nNODE_NAME=node-1\n```\n\n----------------------------------------\n\nTITLE: Handling Null Reference Error in JavaScript\nDESCRIPTION: Example showing how Browser RUM agent handles null reference errors when attempting to access properties of null objects.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/browser/browser-rum-errors.rst#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nvar test = null;\ntest.prop1 = true;\n```\n\n----------------------------------------\n\nTITLE: Installing VictorOps Nagios Plugin (Debian)\nDESCRIPTION: Commands to install the VictorOps Nagios plugin using dpkg or apt on Debian-based systems.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/check_mk-integration.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndpkg -i <path_to_file>\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install <path_to_file>\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Package in Node.js\nDESCRIPTION: Install the @splunk/otel package to enable instrumentation of Node.js applications. This lays the groundwork for advanced metrics and tracing functionalities.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/nodejs/instrumentation/instrument-nodejs-application.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @splunk/otel\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx PHP Tracing in Kubernetes Deployment using YAML\nDESCRIPTION: A Kubernetes Deployment manifest snippet demonstrating how to inject SignalFx configuration as environment variables into application containers using the Downward API. It sets `SIGNALFX_SERVICE_NAME`, `SIGNALFX_ENDPOINT_URL` (using the host IP obtained via `fieldRef`), and `SIGNALFX_TRACE_GLOBAL_TAGS`.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/sfx/instrumentation/instrument-php-application.rst#2025-04-22_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  selector:\n    matchLabels:\n      app: your-application\n  template:\n    metadata:\n      labels:\n        app: your-application\n    spec:\n      containers:\n      - name: myapp\n        image: <image-name>\n        env:\n          - name: HOST_IP\n            valueFrom:\n              fieldRef:\n                fieldPath: status.hostIP\n          - name: SIGNALFX_SERVICE_NAME\n            value: \"<service-name>\"\n          - name: SIGNALFX_ENDPOINT_URL\n            value: \"http://$(HOST_IP):9411/api/v2/spans\"\n          - name: SIGNALFX_TRACE_GLOBAL_TAGS\n            value: \"deployment.environment:<my_environment>\"\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for OpenTelemetry Collector Tutorial\nDESCRIPTION: ReStructuredText (RST) document structure defining a tutorial for OpenTelemetry Collector configuration. Includes document title, meta description, table of contents, and section organization.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-linux/collector-configuration-tutorial/about-collector-config-tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _about-collector-configuration-tutorial:\n\n*****************************************************************************************\nTutorial: Configure the Splunk Distribution of OpenTelemetry Collector on a Linux host\n*****************************************************************************************\n\n.. meta::\n    :description: Follow this tutorial for a walkthrough of configuring the Splunk Distribution of OpenTelemetry Collector to collect telemetry in common situations.\n\n.. toctree::\n   :hidden:\n   :maxdepth: 3\n\n   collector-config-tutorial-start\n   collector-config-tutorial-edit\n   collector-config-tutorial-troubleshoot\n```\n\n----------------------------------------\n\nTITLE: RST Include Directives\nDESCRIPTION: ReStructuredText markup for including a troubleshooting components section with HTML div markers\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-hosts/host-processload.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"troubleshooting-components.rst\"></div>\n\n.. include:: /_includes/troubleshooting-components.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"troubleshooting-components.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: Identifying Troubleshooting MetricSets Cardinality in Splunk APM\nDESCRIPTION: Splunk Observability Cloud metric identifier (`sf.org.apm.numTroubleshootingMetricSets`) representing the cardinality of Troubleshooting MetricSets (TMS). This data is available per minute with a 1-minute look-back period and contributes to usage calculations for both TAPM and host-based plans, appearing in the 'Troubleshooting MetricSets' chart.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/apm-billing-usage-index.rst#2025-04-22_snippet_2\n\nLANGUAGE: metric\nCODE:\n```\nsf.org.apm.numTroubleshootingMetricSets\n```\n\n----------------------------------------\n\nTITLE: Example Grafana Access URL (URI Example)\nDESCRIPTION: Provides a specific example URL (`http://10.46.40.67:32001`) for accessing a Grafana instance. This example assumes the Grafana server is at IP `10.46.40.67` and listening on port `32001`, as defined in its deployment configuration (`deploy.yaml`).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_14\n\nLANGUAGE: uri\nCODE:\n```\nhttp://10.46.40.67:32001\n```\n\n----------------------------------------\n\nTITLE: Editing Environment Configuration File\nDESCRIPTION: Command to edit the environment configuration file for setting organization ID and key.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/check_mk-integration.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nvi /opt/victorops/nagios_plugin/conf/env.<yoursitename>.sh\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Role Definition\nDESCRIPTION: Defines the user-training role and metadata for the documentation page\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/user-roles/user-training.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _user-training:\n\n.. meta::\n   :description: About the user roll in Splunk On-Call.\n```\n\n----------------------------------------\n\nTITLE: Installing Splunk On-Call Zabbix Plugin via DEB Package\nDESCRIPTION: Commands to download and install the Splunk On-Call Zabbix plugin DEB package and run the installation script. The commands fetch the package from GitHub and use dpkg to install it.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zabbix-integration.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nwget\nhttps://github.com/victorops/monitoring_tool_releases/releases/download/victorops-zabbix-0.17.3/victorops-zabbix_0.17.3-2_all.deb\n\nsudo dpkg -i victorops-zabbix_0.17.3-2_all.deb\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo ./install\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Smart Agent on RPM-based Linux\nDESCRIPTION: Command to remove the SignalFx Smart Agent package on Red Hat, CentOS, and other RPM-based Linux distributions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nsudo rpm -e signalfx-agent\n```\n\n----------------------------------------\n\nTITLE: Querying Error Calls using SignalFlow - None\nDESCRIPTION: This SignalFlow query calculates the number of error calls by filtering spans with the 'sf_error' field set to true, counting them, and publishing the result with the label 'Error'. No dependencies are required beyond SignalFlow support in the plugin. The expected input is the 'spans' metric, and the output is a count of error calls for use in a Grafana panel.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/grafana-create-queries.rst#2025-04-22_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nA = histogram('spans', filter('sf_error', 'true')).count().publish(label='Error')\n```\n\n----------------------------------------\n\nTITLE: reStructuredText Directive for Dynamic Dependency Rendering\nDESCRIPTION: This reStructuredText directive inserts a raw HTML `<div>`. This div is processed by a custom build step or JavaScript to fetch data from the specified URL (a YAML file on GitHub containing dependency metadata) and render it dynamically on the page, likely as a table. The `data-renaming` attribute suggests column header customization.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-otel-requirements.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <div class=\"instrumentation\" section=\"dependencies\" url=\"https://raw.githubusercontent.com/splunk/o11y-gdi-metadata/main/apm/opentelemetry-php-metadata.yaml\" data-renaming='{\"name\": \"Dependency\", \"source_href\": \"Link to source\", \"version\": \"Version\", \"stability\": \"Stability\"}'></div>\n```\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"instrumentation\" section=\"dependencies\" url=\"https://raw.githubusercontent.com/splunk/o11y-gdi-metadata/main/apm/opentelemetry-php-metadata.yaml\" data-renaming='{\"name\": \"Dependency\", \"source_href\": \"Link to source\", \"version\": \"Version\", \"stability\": \"Stability\"}'></div>\n```\n\n----------------------------------------\n\nTITLE: CSV Configuration Parameters in RST\nDESCRIPTION: Documentation of CSV configuration parameters including CSVSkipRows and CSVSkipColumns options. These parameters control how CSV files are parsed by ignoring specific rows and columns.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-databases/exec-input.rst#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n- ``CSVSkipRows``\n- no\n- ``integer``\n- The number of rows to ignore before looking for headers.\n   (``csv`` only) The default value is ``0``.\n\n- ``CSVSkipColumns``\n- no\n- ``integer``\n- The number of columns to ignore before parsing data on a given\n   row. (``csv`` only) The default value is ``0``.\n```\n\n----------------------------------------\n\nTITLE: Accessing Grafana Using External IP\nDESCRIPTION: Enter the specified URL with the external IP address in a browser to access the Grafana user interface. This step validates the successful exposure of the Grafana service.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-helm.rst#2025-04-22_snippet_3\n\nLANGUAGE: none\nCODE:\n```\nhttp://<external-IP>/login\n```\n\n----------------------------------------\n\nTITLE: Configuring Passthrough Mode for Agent Collectors\nDESCRIPTION: Configuration for enabling passthrough mode in agent Collectors to forward IP addresses to gateways.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/kubernetes-attributes-processor.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nk8sattributes:\n  passthrough: true\n```\n\n----------------------------------------\n\nTITLE: Adding cgroups Monitor to Service Pipeline\nDESCRIPTION: Configuration to add the cgroups monitor to the metrics pipeline in the Collector service configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-monitoring/cgroups.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [smartagent/cgroups]\n```\n\n----------------------------------------\n\nTITLE: Implementing Percentiles in Monitoring with Splunk\nDESCRIPTION: Percentiles are used to create distributed charts for quick population overviews. By using non-stacked area charts, users can visualize different percentile values, such as p10, p50, and p90, to understand distribution in performance metrics. Inputs are typical metrics and their respective percentile values, while outputs are visual charts that provide insights into metric distribution.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/data-visualization/charts/gain-insights-through-chart-analytics.rst#2025-04-22_snippet_2\n\nLANGUAGE: none\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Docker Log Collection Command\nDESCRIPTION: Command for collecting logs from a Docker container and redirecting them to a file for analysis during migration validation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/smart-agent/smart-agent-migration-process.rst#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndocker logs my-container >my-container.log\n```\n\n----------------------------------------\n\nTITLE: Configuring Outbound Webhook Payload for Circonus Integration in JSON\nDESCRIPTION: This JSON payload is used to configure the outbound webhook in Splunk On-Call for sending acknowledgements back to Circonus. It includes alert type, Circonus alert ID, and acknowledgement author.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/circonus-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"ALERT.alert_type\": \"${{{ALERT.alert_type}}}\", \"ALERT.circonus_alert_id\": \"${{{ALERT.circonus_alert_id}}}\", \"ALERT.ack_author\": \"${{{ALERT.ack_author}}}\" }\n```\n\n----------------------------------------\n\nTITLE: Setting Clear Sensitivity for Custom MTS Usage Alerts\nDESCRIPTION: This value defines the sensitivity condition for clearing an active alert related to custom metric time series (MTS) usage percentage. An alert clears if 100% of the data points within a 30-minute window are below the clear threshold.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/autodetect/autodetect-list.rst#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\n100% of 30m\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Splunk Ruby Agent\nDESCRIPTION: Command to uninstall the Splunk Ruby agent using the gem package manager. This is the first step in migrating to the OpenTelemetry instrumentation for Ruby.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/migrate-from-splunk-ruby.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngem uninstall splunk-otel\n```\n\n----------------------------------------\n\nTITLE: Monitoring Dropped Spans due to Throttling in Splunk RUM (Metric)\nDESCRIPTION: This metric tracks the number of session events or spans dropped because the Bytes Per Minute (BPM) or Spans Per Minute (SPM) data ingestion limits were exceeded. Monitor this metric to understand the impact of throttling based on subscription entitlements.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/references/system-limits/sys-limits-rum.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsf.org.rum.numSpansDroppedThrottle\n```\n\n----------------------------------------\n\nTITLE: RST Document Structure for OpenTelemetry Ingestion Methods\nDESCRIPTION: ReStructuredText document structure defining the layout and organization of OpenTelemetry ingestion method documentation, including table of contents and meta description.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-other/otel-other-landing.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _otel-other-landing:\n\n******************************************\nOther OpenTelemetry ingestion methods\n******************************************\n\n.. meta::\n    :description: Learn about other options to send your data with OpenTelemetry to Splunk Observability Cloud OpenTelemetry Collector.\n\n.. toctree::\n    :maxdepth: 4\n    :titlesonly:\n    :hidden:\n\n    prometheus-generic\n    telegraf\n    other-ingestion-collectd\n```\n\n----------------------------------------\n\nTITLE: Creating Splunk Platform HEC Token Secret with kubectl\nDESCRIPTION: kubectl command to create a Kubernetes secret containing the Splunk Platform HTTP Event Collector (HEC) token. This command creates the secret directly without requiring a YAML file.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/install-k8s-addon-eks.rst#2025-04-22_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl create secret generic splunk-otel-collector \\\n    --from-literal=splunk_platform_hec_token=<YOUR_HEC_TOKEN> \\\n    -n splunk-monitoring\n```\n\n----------------------------------------\n\nTITLE: Test JSON Payload for Zendesk Target Configuration\nDESCRIPTION: Basic JSON payload used to test the HTTP target configuration in Zendesk. Sends an info message to Splunk On-Call to verify connectivity.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zendesk-bi-directional-integration.rst#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \n   \"message_type\":\"info\", \n   \"entity_id\":\"Test alert from Zendesk\",\n   \"state_message\":\"testing from Zendesk\"\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying \"System limits\" Heading in HTML\nDESCRIPTION: Embedded via an RST `raw:: html` directive, this HTML code displays a level 2 heading \"System limits\". The `<embed>` tag is used, likely as a structural wrapper within the documentation generator's context.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/subscription-usage-overview.rst#2025-04-22_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<embed>\n   <h2>System limits</h2>\n</embed>\n```\n\n----------------------------------------\n\nTITLE: Google Chrome Recorder Click with Navigation\nDESCRIPTION: Example of a click event with navigation in Google Chrome Recorder format, including selectors and assertions.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/browser-test/set-up-browser-test.rst#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{\n// Google Chrome Recorder\n   \"type\": \"click\",\n   \"target\": \"main\",\n   \"selectors\": [\n      [\n         \"div:nth-of-type(2) > div:nth-of-type(2) a > div\"\n      ],\n      [\n         \"xpath//html/body/main/div/div/div[2]/div[2]/div/a/div\"\n      ]\n   ],\n   \"offsetY\": 211,\n   \"offsetX\": 164,\n   \"assertedEvents\": [\n      {\n         \"type\": \"navigation\",\n         \"url\": \"www.buttercupgames.com/product/example\",\n         \"title\": \"Buttercup Games\"\n      }\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a List Table in reStructuredText for Escalation Policy Actions\nDESCRIPTION: This snippet demonstrates how to create a list table in reStructuredText to display different escalation policy actions and their descriptions. It uses the list-table directive with header rows and column widths specified.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/alerts/team-escalation-policy.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. list-table::\n   :header-rows: 1\n   :width: 100%\n   :widths: 40, 60\n\n   * - :strong:`Escalation action`\n     - :strong:`Description`\n\n   * - Execute Policy\n     - This action provides the ability to invoke a different escalation policy\n\n   * - Notify the on-duty users in rotation\n     - Our most common type of escalation action, this notifies the currently on-duty users in the rotation you specify. If there is no on-call user scheduled in a rotation at the time when this escalation action is triggered, the resulting behavior is that no page will occur in this step. The time delay before the next step will remain as configured. For example, if an incident triggers an Escalation Policy during off-hours and there is no one on call in the rotation to immediately page, the escalation policy will page no one and then wait however long is specified before executing step two.\n\n   * - Notify the next user in current on-duty shift\n     - This will notify the user(s) scheduled to be next up within the current on-duty shift.\n\n\n   * - Notify the previous user within the current on-duty shift\n     - This will notify the users that were on-duty previously within the current on-duty shift, or are currently in the schedule position to be the previous user.\n\n   * - Notify user \n     - This will notify the user of your specification regardless of the time of day\n \n   * - Notify every member of this team\n     - This will notify every member of the team that the escalation policy is created for regardless of the time of day. All users on the team will be paged for an incident, but only one user is required to ack the incident. \n\n   * - Execute webhook\n     - This will execute the escalation webhook of your choosing. See :ref:`escalation-webhooks`.\n\n   * - Send an email to email address\n     - This will send an email to the email address you specify\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Scheduler Receiver\nDESCRIPTION: Basic configuration example showing how to activate the kubernetes-scheduler integration in the Collector configuration.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/monitors-orchestration/kubernetes-scheduler.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  smartagent/kubernetes-scheduler\n    type: kubernetes-scheduler\n    ... # Additional config\n```\n\n----------------------------------------\n\nTITLE: RST Directive for Document Title\nDESCRIPTION: ReStructuredText directive defining the document reference and title for Android RUM instrumentation documentation.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/get-android-data-in.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _rum-mobile-android:\n\n**************************************************************\nInstrument Android applications for Splunk RUM\n**************************************************************\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenTelemetry Tracer in Go\nDESCRIPTION: This snippet shows how to create an OpenTelemetry tracer using otel.Tracer for instrumentation in Go. The string parameter specifies the instrumentation library name or application identifier. The returned tracer is used to initiate new spans, replacing legacy SignalFx tracing APIs. Requires the opentelemetry-go library and correct initialization of the global TracerProvider (typically via distro.Run).\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/migrate-signalfx-go-to-otel.rst#2025-04-22_snippet_4\n\nLANGUAGE: go\nCODE:\n```\n   tracer := otel.Tracer(\"app-name\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Error Budget from SLO Target (Math Notation)\nDESCRIPTION: This formula defines the error budget for a Service Level Objective (SLO) as the difference between 100% and the desired SLO target percentage. It measures the acceptable margin for errors or downtime.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/slo/burn-rate-alerts.rst#2025-04-22_snippet_0\n\nLANGUAGE: math\nCODE:\n```\n\\text{Error budget} = \\text{100%} - \\text{SLO target}\n```\n\n----------------------------------------\n\nTITLE: Resolved Alert JSON Payload for Zendesk Trigger\nDESCRIPTION: JSON payload for resolving incidents in Splunk On-Call when tickets are closed in Zendesk.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/sp-oncall/spoc-integrations/zendesk-bi-directional-integration.rst#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{ \n   \"entity_id\":\"{{ticket.id}}\", \n   \"message_type\":\"RECOVERY\",\n   \"state_message\":\"{{ticket.comments_formatted}}\",\n   \"monitoring_tool\":\"Zendesk\", \n   \"alert_url\":\"{{ticket.link}}\",\n   \"ticket_id\":\"{{ticket.id}}\", \n   \"Ticket External I.D.\":\"{{ticket.external_id}}\", \n   \"Ticket Origin\":\"{{ticket.via}}\",\n   \"Ticket Status\":\"{{ticket.status}}\", \n   \"Ticket Priority\":\"{{ticket.priority}}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Agent Deployment Commands\nDESCRIPTION: Commands to clone the repository and deploy the Collector as an agent using Nomad.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-nomad.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n$ git clone https://github.com/signalfx/splunk-otel-collector.git\n$ cd splunk-otel-collector/deployments/nomad\n$ nomad run otel-agent.nomad\n```\n\n----------------------------------------\n\nTITLE: Increasing Go OpenTelemetry Batch Span Processor Limits (Bash)\nDESCRIPTION: Sets environment variables to increase the batch span processor's maximum export batch size (`OTEL_BSP_MAX_EXPORT_BATCH_SIZE`) and queue size (`OTEL_BSP_MAX_QUEUE_SIZE`). This configuration is recommended when the system has sufficient resources and spans are being generated faster than they can be exported, potentially leading to dropped spans. Increasing the queue size requires adequate system memory.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/go/troubleshooting/common-go-troubleshooting.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OTEL_BSP_MAX_EXPORT_BATCH_SIZE=1024\nexport OTEL_BSP_MAX_QUEUE_SIZE=20480\n# Don't increase the queue size if the system has limited memory\n```\n\n----------------------------------------\n\nTITLE: Installing Sinatra Instrumentation in Ruby Gemfile\nDESCRIPTION: This snippet shows how to add the Sinatra instrumentation library to a Ruby project's Gemfile. It specifies the gem name and version constraint.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/ruby/ruby-manual-instrumentation.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngem \"opentelemetry-instrumentation-sinatra\", \"~> 0.21\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Pattern for Trace Context with JBoss LogManager - Text\nDESCRIPTION: This snippet updates JBoss LogManager's 'logging.properties' to use a pattern formatter including logger, message, trace_id, span_id, service.name, deployment.environment, and trace_flags for each log line. Supported in JBoss LogManager 1.1.0+ with the OTel Java agent passing context fields into MDC.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/java/instrumentation/connect-traces-logs.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nformatter.PATTERN=org.jboss.logmanager.formatters.PatternFormatter\\nformatter.PATTERN.properties=pattern\\nformatter.PATTERN.constructorProperties=pattern\\nformatter.PATTERN.pattern=%logger{36} - %msg trace_id=%X{trace_id} span_id=%X{span_id} service=%X{service.name}, env=%X{deployment.environment} trace_flags=%X{trace_flags}: %m%n\n```\n\n----------------------------------------\n\nTITLE: Logging into Docker Registry (Shell)\nDESCRIPTION: Shell command to authenticate with a Docker registry. This is a prerequisite step before pushing a custom Docker image.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/private-preview/splunk-o11y-plugin-for-grafana/deploy-grafana-k8s.rst#2025-04-22_snippet_1\n\nLANGUAGE: none\nCODE:\n```\ndocker login\n```\n\n----------------------------------------\n\nTITLE: RST Image Inclusion for Test Filtering\nDESCRIPTION: A reStructuredText image directive that includes a screenshot showing how to filter tests using environment tags in the Synthetic homepage interface.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/synthetics/test-status/test-status.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. image:: /_images/synthetics/syn-filter-test.png\n      :width: 60%\n      :alt: This image shows the filter env:prod for all tests on the Synthetic homepage..\n```\n\n----------------------------------------\n\nTITLE: Defining and Linking Collector Extensions - reStructuredText\nDESCRIPTION: This snippet provides the structure for a documentation file in reStructuredText (RST) format, including a title section, page metadata, a Sphinx toctree navigation block, and raw HTML hooks for dynamic content inclusion. There are no dependencies or prerequisites beyond Sphinx/RST support; the outputs are navigation and formatted documentation for end users. All inputs and navigation are specified via RST directives; no programmatic code is executed.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/components/a-components-extensions.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _otel-components-extensions:\n\n******************************************\nCollector components: Extensions\n******************************************\n\n.. meta::\n    :description: Learn about the components that make up the Splunk Observability Cloud OpenTelemetry Collector.\n\n.. toctree::\n    :maxdepth: 4\n    :titlesonly:\n    :hidden:\n\n    basic-auth-extension\n    bearertokenauth-extension\n    docker-observer-extension\n    ecs-observer-extension\n    ecstask-observer-extension\n    file-storage-extension\n    health-check-extension\n    host-observer-extension\n    http-forwarder-extension\n    kubernetes-observer-extension\n    memory-ballast-extension\n    oauth2client-extension\n    pprof-extension\n    smartagent-extension\n    zpages-extension    \n\nThe Splunk Distribution of the OpenTelemetry Collector includes and supports the extensions listed on this doc. To see other components, refer to :ref:`otel-components`.\n\n.. note:: The following list might not contain all the latest additions. For a complete list of Collector components, including components that aren't included in the Splunk Distribution of OpenTelemetry Collector, see the ``opentelemetry-contrib`` repository in GitHub.\n\nThe following extensions are available:\n\n.. raw:: html\n\n   <div class=\"include-start\" id=\"collector-available-extensions.rst\"></div>\n\n.. include:: /_includes/gdi/collector-available-extensions.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"collector-available-extensions.rst\"></div>\n\n```\n\n----------------------------------------\n\nTITLE: Including Linux Collector Requirements in RST Documentation\nDESCRIPTION: This RST code snippet demonstrates how to include external content for Linux Collector requirements in the documentation. It uses raw HTML directives to mark the beginning and end of the included content.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/compute/linux.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n   <div class=\"include-start\" id=\"requirements/collector-linux.rst\"></div>\n\n.. include:: /_includes/requirements/collector-linux.rst\n\n.. raw:: html\n\n   <div class=\"include-stop\" id=\"requirements/collector-linux.rst\"></div>\n```\n\n----------------------------------------\n\nTITLE: reStructuredText Directive for Dynamic Instrumentation Rendering\nDESCRIPTION: Similar to the dependency directive, this inserts a raw HTML `<div>` intended to dynamically load and display information about supported libraries and frameworks (instrumentations) from an external YAML file specified by the `url` attribute. The `data-renaming` attribute provides mappings for display labels.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/application/php/php-otel-requirements.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <div class=\"instrumentation\" section=\"instrumentations\" url=\"https://raw.githubusercontent.com/splunk/o11y-gdi-metadata/main/apm/opentelemetry-php-metadata.yaml\" data-renaming='{\"keys\": \"Identifier\", \"description\": \"Description\", \"stability\": \"Stability\", \"support\": \"Support\", \"instrumented_components\": \"Components\", \"signals\": \"Signals\", \"source_href\": \"Source\", \"settings\": \"Settings\", \"dependencies\": \"Dependencies\", \"supported_versions\": \"Supported versions\", \"name\": \"Name\", \"package_href\": \"Package URL\", \"version\": \"Version\", \"instrument\": \"Type\", \"metric_name\": \"Metric name\", \"metrics\": \"Metrics\"}'></div>\n```\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"instrumentation\" section=\"instrumentations\" url=\"https://raw.githubusercontent.com/splunk/o11y-gdi-metadata/main/apm/opentelemetry-php-metadata.yaml\" data-renaming='{\"keys\": \"Identifier\", \"description\": \"Description\", \"stability\": \"Stability\", \"support\": \"Support\", \"instrumented_components\": \"Components\", \"signals\": \"Signals\", \"source_href\": \"Source\", \"settings\": \"Settings\", \"dependencies\": \"Dependencies\", \"supported_versions\": \"Supported versions\", \"name\": \"Name\", \"package_href\": \"Package URL\", \"version\": \"Version\", \"instrument\": \"Type\", \"metric_name\": \"Metric name\", \"metrics\": \"Metrics\"}'></div>\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Document Structure for AWS Tutorial\nDESCRIPTION: ReStructuredText markup defining the structure and content of an AWS monitoring tutorial document, including headers, sections, images, and navigation elements.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/aws/aws-tutorial/tutorial-aws-use.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _tutorial-aws-use:\n\n**************************************************************\nPart 2: Monitor and use AWS data in Splunk Observability Cloud\n**************************************************************\n\nNow that you've integrated with your AWS services, you can access your data using navigators and dashboards, search your AWS data, and set up detectors and alerts.\n```\n\n----------------------------------------\n\nTITLE: Planning Terraform Changes\nDESCRIPTION: This command previews the changes that Terraform will apply based on the current configuration and provided variables. It outputs the plan to a file for review before execution.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/apm-data-links/apm-datalinks-terraform/apm-create-data-links-terraform-file.rst#2025-04-22_snippet_5\n\nLANGUAGE: none\nCODE:\n```\nterraform plan -var=\"signalfx_auth_token=<api-access-token>\" -var=\"signalfx_api_url=https://api.<realm>.signalfx.com\" -out=<plan-file-name>\n```\n\n----------------------------------------\n\nTITLE: Verifying cluster pod status\nDESCRIPTION: Command to check the status of the OpenTelemetry Collector pods in the Kubernetes cluster after updating the access token and restarting components.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/collector-kubernetes/kubernetes-upgrade.rst#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pod -n <Namespace> | grep <Release_Name>\n```\n\n----------------------------------------\n\nTITLE: Dropping Spans on Failed Status in JavaScript\nDESCRIPTION: This JavaScript snippet illustrates how to suppress tracing and handle span export failures in Splunk RUM. It uses the `suppressTracing` function to conditionally drop spans when exporting is unsuccessful, based on the export result code.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/rum/sensitive-data-rum.rst#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\ncontext.with(suppressTracing(context.active()), () => {\n          this._exporter.export([span], result => {\n            if (result.code !== ExportResultCode.SUCCESS) {\n              globalErrorHandler(\n                result.error ??\n                  new Error(\n                    `SimpleSpanProcessor: span export failed (status ${result})`\n                  )\n              );\n            }\n\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: ReStructuredText document structure showing section headers and metadata for SLI monitoring documentation\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/splunkplatform/practice-reliability/slis.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _practice-reliability-slis:\n\n************************************************************************************************************************\nMeasure and alert on SLIs\n************************************************************************************************************************\n\n.. meta::\n   :description: SLIs, SLOs, and SLAs in Splunk Observability Cloud\n```\n\n----------------------------------------\n\nTITLE: Displaying \"Usage and billing by source\" Heading in HTML\nDESCRIPTION: This HTML snippet, embedded using an RST `raw:: html` directive, displays a level 2 heading \"Usage and billing by source\" within the documentation. The `<embed>` tag likely serves as a wrapper in this specific RST context.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/admin/subscription-usage/subscription-usage-overview.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<embed>\n   <h2>Usage and billing by source</h2>\n</embed>\n```\n\n----------------------------------------\n\nTITLE: Embedding Raw HTML Heading in reStructuredText\nDESCRIPTION: This reStructuredText snippet uses the `raw:: html` directive to directly embed an HTML `<h2>` tag containing the text \"Continue learning about Splunk APM\" into the output document. This allows for custom HTML formatting where standard reStructuredText elements might not suffice. It relies on the document processing tool (like Sphinx) to interpret and render the raw HTML.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/apm/set-up-apm/apm.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n  <embed>\n    <h2>Continue learning about Splunk APM</h2>\n  </embed>\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Document Structure\nDESCRIPTION: RST markup defining the document structure, table of contents, and system requirements for the OpenTelemetry Collector\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/otel-requirements.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _otel-requirements:\n\n***********************************************************************************\nCollector requirements\n***********************************************************************************\n\n.. meta::\n    :description: Requirements for the Splunk Distribution of OpenTelemetry Collector.\n\n.. toctree::\n    :maxdepth: 4\n    :titlesonly:\n    :hidden:\n\n    Security guidelines <security>\n    exposed-endpoints.rst\n    sizing.rst\n    collector-architecture.rst\n```\n\n----------------------------------------\n\nTITLE: Configuring Detector Auto-Clear with Thresholds and Durations - SignalFlow\nDESCRIPTION: This snippet illustrates how to use SignalFlow to automatically clear alerts after a set period if a condition cannot be evaluated due to sparse or missing data. The detect function checks whether metric A is above a threshold (99) in at least 80% of the time windows over 1 hour, with an alert requiring a 30-minute span and at least 90% of the windows; alerts auto-resolve if not evaluatable for 1 hour as set by auto_resolve_after. Prerequisites are the presence of the metric, and alert resolution logic as described in the documentation. Inputs include the metric A and a configurable threshold, and outputs are alerts that auto-clear if required conditions aren't met for the specified interval.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/alerts-detectors-notifications/alerts-and-detectors/auto-clearing-alerts.rst#2025-04-22_snippet_1\n\nLANGUAGE: SignalFlow\nCODE:\n```\ndetect(when(A > threshold(99), lasting='1h', at_least=0.8), lasting='30m', at_least=0.9), auto_resolve_after='1h')\n```\n\n----------------------------------------\n\nTITLE: AWS Parameter Store Environment Configuration\nDESCRIPTION: JSON configuration for setting up the SPLUNK_CONFIG_YAML environment variable using AWS Parameter Store valueFrom option.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/opentelemetry/deployments/deployments-ecs-ec2.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"valueFrom\": \"splunk-otel-collector-config\",\n  \"name\": \"SPLUNK_CONFIG_YAML\"\n}\n```\n\n----------------------------------------\n\nTITLE: HTML Embed for Section Headers\nDESCRIPTION: Raw HTML embedding for section headers with anchors and permalinks.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/rum/android/get-android-data-in.rst#2025-04-22_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<embed>\n<h2>Get started with Splunk RUM for Mobile<a name=\"get-started-splunk-rum-mobile\" class=\"headerlink\" href=\"#get-started-splunk-rum-mobile\" title=\"Permalink to this headline\">¶</a></h2>\n</embed>\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Azure Integration\nDESCRIPTION: ReStructuredText markup defining the documentation structure for Azure integration setup, including toctree, sections, and formatting directives.\nSOURCE: https://github.com/splunk/public-o11y-docs/blob/main/gdi/get-data-in/connect/azure/azure-connect.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _azure-connect:\n\n**************************************************************\nConnect to Azure: Guided setup and other options \n**************************************************************\n\n.. meta::\n  :description: Connect your Microsoft Azure account to Splunk Observability Cloud: UI, API\n\n.. toctree::\n  :hidden:\n```"
  }
]